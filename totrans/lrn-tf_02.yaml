- en: Chapter 2\. Go with the Flow: Up and Running with TensorFlow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we start our journey with two working TensorFlow examples. The
    first (the traditional “hello world” program), while short and simple, includes
    many of the important elements we discuss in depth in later chapters. With the
    second, a first end-to-end machine learning model, you will embark on your journey
    toward state-of-the-art machine learning with TensorFlow.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Before getting started, we briefly walk through the installation of TensorFlow.
    In order to facilitate a quick and painless start, we install the CPU version
    only, and defer the GPU installation to later.^([1](ch02.html#idm139707904506064)) (If
    you don’t know what this means, that’s OK for the time being!) If you already
    have TensorFlow installed, skip to the second section.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Installing TensorFlow
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you are using a clean Python installation (probably set up for the purpose
    of learning TensorFlow), you can get started with the simple `pip` installation:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This approach does, however, have the drawback that TensorFlow will override
    existing packages and install specific versions to satisfy dependencies. If you
    are using this Python installation for other purposes as well, this will not do. One
    common way around this is to install TensorFlow in a virtual environment, managed
    by a utility called *virtualenv*.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on your setup, you may or may not need to install *virtualenv* on
    your machine. To install *virtualenv*, type:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: See [*http://virtualenv.pypa.io*](http://virtualenv.pypa.io) for further instructions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to install TensorFlow in a virtual environment, you must first create
    the virtual environment—in this book we choose to place these in the *~/envs*
    folder, but feel free to put them anywhere you prefer:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This will create a virtual environment named *tensorflow* in *~/envs* (which
    will manifest as the folder *~/envs/tensorflow*). To activate the environment,
    use:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The prompt should now change to indicate the activated environment:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'At this point the `pip install` command:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: will install TensorFlow into the virtual environment, without impacting other
    packages installed on your machine.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, in order to exit the virtual environment, you type:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'at which point you should get back the regular prompt:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Adding an alias to ~/.bashrc
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The process described for entering and exiting your virtual environment might
    be too cumbersome if you intend to use it often. In this case, you can simply
    append the following command to your *~/.bashrc* file:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: and use the command `tensorflow` to activate the virtual environment. To quit
    the environment, you will still use `deactivate`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a basic installation of TensorFlow, we can proceed to our first
    working examples. We will follow the well-established tradition and start with
    a “hello world” program.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Hello World
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our first example is a simple program that combines the words “Hello” and “
    World!” and displays the output—the phrase “Hello World!” While simple and straightforward,
    this example introduces many of the core elements of TensorFlow and the ways in
    which it is different from a regular Python program.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: We suggest you run this example on your machine, play around with it a bit,
    and see what works. Next, we will go over the lines of code and discuss each element
    separately.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we run a simple install and version check (if you used the virtualenv
    installation option, make sure to activate it before running TensorFlow code):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If correct, the output will be the version of TensorFlow you have installed
    on your system. Version mismatches are the most probable cause of issues down
    the line.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 2-1](#ex0201) shows the complete “hello world” example.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-1\. “Hello world” with TensorFlow
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We assume you are familiar with Python and imports, in which case the first
    line:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: requires no explanation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: IDE configuration
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you are running TensorFlow code from an IDE, then make sure to redirect
    to the virtualenv where the package is installed. Otherwise, you will get the
    following import error:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '`ImportError: No module named tensorflow`'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: In the PyCharm IDE this is done by selecting Run→Edit Configurations, then changing
    Python Interpreter to point to *~/envs/tensorflow/bin/python*, assuming you used
    *~/envs/tensorflow* as the virtualenv directory.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we define the constants `"Hello"` and `" World!"`, and combine them:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'At this point, you might wonder how (if at all) this is different from the
    simple Python code for doing this:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The key point here is what the variable `hw` contains in each case. We can
    check this using the `print` command. In the pure Python case we get this:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In the TensorFlow case, however, the output is completely different:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Probably not what you expected!
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter we explain the computation graph model of TensorFlow in
    detail, at which point this output will become completely clear. The key idea
    behind computation graphs in TensorFlow is that we first define what computations
    should take place, and then trigger the computation in an external mechanism.
    Thus, the TensorFlow line of code:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: does *not* compute the sum of `h` and `w`, but rather adds the summation operation
    to a graph of computations to be done later.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, the `Session` object acts as an interface to the external TensorFlow
    computation mechanism, and allows us to run parts of the computation graph we
    have already defined. The line:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: actually computes `hw` (as the sum of `h` and `w`, the way it was defined previously),
    following which the printing of `ans` displays the expected “Hello World!” message.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: This completes the first TensorFlow example. Next, we dive right in with a simple
    machine learning example, which already shows a great deal of the promise of the
    TensorFlow framework.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: MNIST
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *MNIST* (Mixed National Institute of Standards and Technology) handwritten
    digits dataset is one of the most researched datasets in image processing and
    machine learning, and has played an important role in the development of artificial
    neural networks (now generally referred to as *deep learning*).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: As such, it is fitting that our first machine learning example should be dedicated
    to the classification of handwritten digits ([Figure 2-1](#fig0201) shows a random
    sample from the dataset). At this point, in the interest of keeping it simple,
    we will apply a very simple classifier. This simple model will suffice to classify
    approximately 92% of the test set correctly—the best models currently available
    reach over 99.75% correct classification, but we have a few more chapters to go
    until we get there! Later in the book, we will revisit this data and use more
    sophisticated methods.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '![MNIST100](assets/letf_0201.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
- en: Figure 2-1\. 100 random MNIST images
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Softmax Regression
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example we will use a simple classifier called *softmax regression*.
    We will not go into the mathematical formulation of the model in too much detail
    (there are plenty of good resources where you can find this information, and we
    strongly suggest that you do so, if you have never seen this before). Rather,
    we will try to provide some intuition into the way the model is able to solve
    the digit recognition problem.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Put simply, the softmax regression model will figure out, for each pixel in
    the image, which digits tend to have high (or low) values in that location. For
    instance, the center of the image will tend to be white for zeros, but black for
    sixes. Thus, a black pixel in the center of an image will be evidence against
    the image containing a zero, and in favor of it containing a six.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Learning in this model consists of finding weights that tell us how to accumulate
    evidence for the existence of each of the digits. With softmax regression, we
    will not use the spatial information in the pixel layout in the image. Later on,
    when we discuss convolutional neural networks, we will see that utilizing spatial
    information is one of the key elements in making great image-processing and object-recognition
    models.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Since we are not going to use the spatial information at this point, we will
    unroll our image pixels as a single long vector denoted *x* ([Figure 2-2](#fig0202)).
    Then
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '*xw*⁰ = ∑*x*[*i*] <math alttext="w Subscript i Superscript 0"><msubsup><mi>w</mi>
    <mi>i</mi> <mn>0</mn></msubsup></math>'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: will be the evidence for the image containing the digit 0 (and in the same way
    we will have <math alttext="w Superscript d"><msup><mi>w</mi> <mi>d</mi></msup></math>
     weight vectors for each one of the other digits, <math><mrow><mi>d</mi> <mo>=</mo>
    <mn>1</mn> <mo>,</mo> <mo>.</mo> <mo>.</mo> <mo>.</mo> <mo>,</mo> <mn>9</mn></mrow></math>
    ).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '![MNIST100](assets/letf_0202.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: Figure 2-2\. MNIST image pixels unrolled to vectors and stacked as columns (sorted
    by digit from left to right). While the loss of spatial information doesn’t allow
    us to recognize the digits, the block structure evident in this figure is what
    allows the softmax model to classify images. Essentially, all zeros (leftmost
    block) share a similar pixel structure, as do all ones (second block from the
    left), etc.
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: All this means is that we sum up the pixel values, each multiplied by a weight,
    which we think of as the importance of this pixel in the overall evidence for
    the digit zero being in the image.^([2](ch02.html#idm139707905335440))
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: For instance, *w*⁰[38] will be a large positive number if the 38th pixel having
    a high intensity points strongly to the digit being a zero, a strong negative
    number if high-intensity values in this position occur mostly in other digits,
    and zero if the intensity value of the 38th pixel tells us nothing about whether
    or not this digit is a zero.^([3](ch02.html#idm139707905332608))
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Performing this calculation at once for all digits (computing the evidence for
    each of the digits appearing in the image) can be represented by a single matrix
    operation. If we place the weights for each of the digits in the columns of a
    matrix *W*, then the length-10 vector with the evidence for each of the digits
    is
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[*xw*⁰, ···, *xw*⁹] = *xW*'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'The purpose of learning a classifier is almost always to evaluate new examples.
    In this case, this means that we would like to be able to tell what digit is written
    in a new image we have not seen in our training data. In order to do this, we
    start by summing up the evidence for each of the 10 possible digits (i.e., computing
    *xW*). The final assignment will be the digit that “wins” by accumulating the
    most evidence:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '*digit* = *argmax*(*xW*)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: We start by presenting the code for this example in its entirety ([Example 2-2](#ex0202)),
    then walk through it line by line and go over the details. You may find that there
    are many novel elements or that some pieces of the puzzle are missing at this
    stage, but our advice is that you go with it for now. Everything will become clear
    in due course.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-2\. Classifying MNIST handwritten digits with softmax regression
  id: totrans-81
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'If you run the code on your machine, you should get output like this:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: That’s all it takes! If you have put similar models together before using other
    platforms, you might appreciate the simplicity and readability. However, these
    are just side bonuses, with the efficiency and flexibility gained from the computation
    graph model of TensorFlow being what we are really interested in.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: The exact accuracy value you get will be just under 92%. If you run the program
    once more, you will get another value. This sort of stochasticity is very common
    in machine learning code, and you have probably seen similar results before. In
    this case, the source is the changing order in which the handwritten digits are
    presented to the model during learning. As a result, the learned parameters following
    training are slightly different from run to run.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the same program five times might therefore produce this result:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We will now briefly go over the code for this example and see what is new from
    the previous “hello world” example. We’ll break it down line by line:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The first new element in this example is that we use external data! Rather
    than downloading the MNIST dataset (freely available at [*http://yann.lecun.com/exdb/mnist/*](http://yann.lecun.com/exdb/mnist/))
    and loading it into our program, we use a built-in utility for retrieving the
    dataset on the fly. Such utilities exist for most popular datasets, and when dealing
    with small ones (in this case only a few MB), it makes a lot of sense to do it
    this way. The second import loads the utility we will later use both to automatically
    download the data for us, and to manage and partition it as needed:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Here we define some constants that we use in our program—these will each be
    explained in the context in which they are first used:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The `read_data_sets()` method of the MNIST reading utility downloads the dataset
    and saves it locally, setting the stage for further use later in the program.
    The first argument, `DATA_DIR`, is the location we wish the data to be saved to
    locally. We set this to `'/tmp/data'`, but any other location would be just as
    good. The second argument tells the utility how we want the data to be labeled;
    we will not go into this right now.^([4](ch02.html#idm139707904192528))
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that this is what prints the first four lines of the output, indicating
    the data was obtained correctly. Now we are finally ready to set up our model:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In the previous example we saw the TensorFlow constant element—this is now
    complemented by the `placeholder` and `Variable` elements. For now, it is enough
    to know that a variable is an element manipulated by the computation, while a
    placeholder has to be supplied when triggering it. The image itself (`x`) is a
    placeholder, because it will be supplied by us when running the computation graph.
    The size [`None, 784`] means that each image is of size 784 (28×28 pixels unrolled
    into a single vector), and `None` is an indicator that we are not currently specifying
    how many of these images we will use at once:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In the next chapter these concepts will be dealt with in much more depth.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'A key concept in a large class of machine learning tasks is that we would like
    to learn a function from data examples (in our case, digit images) to their known
    labels (the identity of the digit in the image). This setting is called *supervised
    learning*. In most supervised learning models, we attempt to learn a model such
    that the true labels and the predicted labels are close in some sense. Here, `y_true`
    and `y_pred` are the elements representing the true and predicted labels, respectively:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The measure of similarity we choose for this model is what is known as *cross
    entropy*—a natural choice when the model outputs class probabilities. This element
    is often referred to as the *loss function*:^([5](ch02.html#idm139707904042960))
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The final piece of the model is how we are going to train it (i.e., how we are
    going to minimize the loss function). A very common approach is to use gradient
    descent optimization. Here, `0.5` is the learning rate, controlling how fast our
    gradient descent optimizer shifts model weights to reduce overall loss.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss optimizers and how they fit into the computation graph later
    on in the book.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Once we have defined our model, we want to define the evaluation procedure we
    will use in order to test the accuracy of the model. In this case, we are interested
    in the fraction of test examples that are correctly classified:^([6](ch02.html#idm139707903782672))
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'As with the “hello world” example, in order to make use of the computation
    graph we defined, we must create a session. The rest happens within the session:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'First, we must initialize all variables:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This carries some specific implications in the realm of machine learning and
    optimization, which we will discuss further when we use models for which initialization
    is an important issue
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The actual training of the model, in the gradient descent approach, consists
    of taking many steps in “the right direction.” The number of steps we will make,
    `NUM_STEPS`, was set to 1,000 in this case. There are more sophisticated ways
    of deciding when to stop, but more about that later! In each step we ask our data
    manager for a bunch of examples with their labels and present them to the learner.
    The `MINIBATCH_SIZE` constant controls the number of examples to use for each
    step.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we use the `feed_dict` argument of `sess.run` for the first time. Recall
    that we defined placeholder elements when constructing the model. Now, each time
    we want to run a computation that will include these elements, we must supply
    a value for them.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In order to evaluate the model we have just finished learning, we run the accuracy
    computing operation defined earlier (recall the accuracy was defined as the fraction
    of images that are correctly labeled). In this procedure, we feed a separate group
    of test images, which were never seen by the model during training:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Lastly, we print out the results as percent values.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2-3](#fig0203) shows a graph representation of our model.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/letf_0203.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: Figure 2-3\. A graph representation of the model. Rectangular elements are Variables,
    and circles are placeholders. The top-left frame represents the label prediction
    part, and the bottom-right frame the evaluation. Here, *b* is a bias term that
    could be added to the mode.
  id: totrans-123
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Model evaluation and memory errors
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When using TensorFlow, like any other system, it is important to be aware of
    the resources being used, and make sure not to exceed the capacity of the system.
    One possible pitfall is in the evaluation of models—testing their performance
    on a test set. In this example we evaluate the accuracy of the models by feeding
    all the test examples in one go:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: If all the test examples (here, `data.test.images`) are not able to fit into
    the memory in the system you are using, you will get a memory error at this point.
    This is likely to be the case, for instance, if you are running this example on
    a typical low-end GPU.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: The easy way around this (getting a machine with more memory is a temporary
    fix, since there will always be larger datasets) is to split the test procedure
    into batches, much as we did during training.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations! By now you have installed TensorFlow and taken it for a spin
    with two basic examples. You have seen some of the fundamental building blocks
    that will be used throughout the book, and have hopefully begun to get a feel
    for TensorFlow.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Next, we take a look under the hood and explore the computation graph model
    used by TensorFlow.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch02.html#idm139707904506064-marker)) We refer the reader to the official
    [TensorFlow install guide](https://www.tensorflow.org/install/) for further details,
    and especially the ever-changing details of GPU installations.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch02.html#idm139707905335440-marker)) It is common to add a “bias term,”
    which is equivalent to stating which digits we believe an image to be before seeing
    the pixel values. If you have seen this before, then try adding it to the model
    and check how it affects the results.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch02.html#idm139707905332608-marker)) If you are familiar with softmax
    regression, you probably realize this is a simplification of the way it works,
    especially when pixel values are as correlated as with digit images.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch02.html#idm139707905332608-marker)) 如果您熟悉softmax回归，您可能意识到这是它工作方式的简化，特别是当像素值与数字图像一样相关时。
- en: ^([4](ch02.html#idm139707904192528-marker)) Here and throughout, before running
    the example code, make sure `DATA_DIR` fits the operating system you are using.
    On Windows, for instance, you would probably use something like *c:\tmp\data*
    instead.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch02.html#idm139707904192528-marker)) 在整个过程中，在运行示例代码之前，请确保`DATA_DIR`适合您正在使用的操作系统。例如，在Windows上，您可能会使用类似*c:\tmp\data*的路径。
- en: ^([5](ch02.html#idm139707904042960-marker)) As of TensorFlow 1.0 this is also
    contained in `tf.losses.softmax_cross_entropy`.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch02.html#idm139707904042960-marker)) 从TensorFlow 1.0开始，这也包含在`tf.losses.softmax_cross_entropy`中。
- en: ^([6](ch02.html#idm139707903782672-marker)) As of TensorFlow 1.0 this is also
    contained in `tf.metrics.accuracy`.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch02.html#idm139707903782672-marker)) 从TensorFlow 1.0开始，这也包含在`tf.metrics.accuracy`中。
