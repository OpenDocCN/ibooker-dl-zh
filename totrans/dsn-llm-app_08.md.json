["```py\nfrom datasets import load_dataset\ntune_data = load_dataset(\"csv\", data_files='/path/to/finetune_data.csv'\n```", "```py\n# Make sure you have installed the correct version\n!pip install transformers==4.35.0\n\nfrom transformers import TrainingArguments\n```", "```py\noptim = 'adamw_bnb_8bit'\n```", "```py\noptim = \"paged_adamw_32bit\"\nlearning_rate = 3e-4\nweight_decay = 0.01\nlr_scheduler_type = 'cosine'\nwarmup_ratio = 0.03  #The proportion of training steps to be used as warmup\n```", "```py\ngradient_accumulation_steps = 4\nbf16 = True\ngradient_checkpointing = True\n```", "```py\n# Label 0 will be transformed to label_smoothing_factor/num_labels\n# Label 1 will be transformed to 1 - label_smoothing_factor +\n#label_smoothing_factor/num_labels\n\nlabel_smoothing_factor = 0.1\nneftune_noise_alpha = 5\n```", "```py\nper_device_train_batch_size = 8\nper_device_eval_batch_size = 8\n```", "```py\nmax_grad_norm=2\ngroup_by_length=True\nmax_train_epochs=3\n```", "```py\nr = 64\nlora_alpha = 8\nlora_dropout = 0.1\n```", "```py\nuse_4bit = True\nbnb_4bit_compute_dtype = 'float16'\nbnb_4bit_quant_type = 'nf4'\nuse_nested_quant = False\n```", "```py\nmax_seq_length = 128\n# Packing is used to place multiple instructions in the same input sequence\n\npacking = True\n```", "```py\n# Ensure that the specified versions of these libraries are installed.\n!pip install transformers==4.35.0 accelerate==0.24.0 peft==0.6.0\nbitsandbytes==0.41.0  trl==0.7.4\n\nfrom datasets import load_dataset\nfrom transformers import TrainingArguments, BitsAndBytesConfig\nfrom transformers import LlamaForCausalLM, LlamaTokenizer\nfrom peft import PeftModel, LoraConfig\nfrom trl import SFTTrainer\n\ntrain_params = TrainingArguments(\n    optim = \"paged_adamw_32bit\",\n    learning_rate = 3e-4,\n    weight_decay = 0.01,\n    lr_scheduler_type = 'cosine',\n    warmup_ratio = 0.03,\n    gradient_accumulation_steps = 4,\n    bf16 = True,\n    gradient_checkpointing = True,\n    label_smoothing_factor = 0.1,\n    neftune_noise_alpha = 5,\n    per_device_train_batch_size = 8,\n    per_device_eval_batch_size = 8,\n    max_grad_norm=2,\n    group_by_length=True,\n    max_train_epochs=3,\n    output_dir = '/model_outputs',\n    save_steps = 50,\n    logging_steps = 10\n    )\n\nquantize_params = BitsAndBytesConfig (\n\n    use_4bit = True,\n    bnb_4bit_compute_dtype = 'float16',\n    bnb_4bit_quant_type = 'nf4',\n    use_nested_quant = False\n    )\n\nlora_params = LoraConfig (\n    r = 64,\n    lora_alpha = 8,\n    lora_dropout = 0.1\n    )\n\nmodel = LlamaForCausalLM.from_pretrained(\n    pretrained_model_name_or_path = 'meta-llama/Llama-2-7b',\n    quantization_config=quantize_params,\n    device_map='auto'\n    )\n\ntokenizer = LlamaTokenizer.from_pretrained('meta-llama/Llama-2-7b')\n\ntune_data = load_dataset(\"csv\", data_files='/path/to/finetune_data.csv')\n\nsft = SFTTrainer (\n    model = model,\n    args = train_params,\n    train_dataset = tune_data,\n    tokenizer = tokenizer\n    peft_config = lora_params,\n    max_seq_length = 128,\n    dataset_text_field = 'text',\n    packing = True\n    )\n\nsft.train()\nsft.model.save_pretrained('/path/to/llama-2-it.csv')\n```", "```py\nfrom datasets import load_dataset\ntune_data = load_dataset(\"csv\", data_files='/path/to/finetune_data.csv')\nprint(tune_data[:2])\n```", "```py\nInput: We will support women and children and give every child the best\npossible start with $10 a day child care.\nIdentify if the above sentence represents a political promise. A political\npromise is a promise that is tangible, specific, and an action that the\ngovernment has the agency to make. Reply 'True' if the sentence represents a\npolitical promise, 'False' if not.\nOutput: True\nInput: It is time for leadership that never seeks to divide Canadians, but\ntakes every single opportunity to bring us together, including in Parliament.\nIdentify if the above sentence represents a political promise. A political\npromise is a promise that is tangible, specific, and an action that the\ngovernment has the agency to make. Reply 'True' if the sentence represents a\npolitical promise, 'False' if not.\nOutput: False\n```", "```py\n\"cnn_dailymail\": [\n  (\"Write highlights for this article:\\n\\n{text}\", \"{highlights}\"),\n  (\"Write some highlights for the following article:\\n\\n{text}\", \"{highlights}\"),\n  (\"{text}\\n\\nWrite highlights for this article.\", \"{highlights}\"),\n  (\"{text}\\n\\nWhat are highlight points for this article?\", \"{highlights}\"),\n  (\"{text}\\nSummarize the highlights of this article.\", \"{highlights}\"),\n  (\"{text}\\nWhat are the important parts of this article?\", \"{highlights}\"),\n  (\"{text}\\nHere is a summary of the highlights for this article:\",\n    \"{highlights}\"),\n  (\"Write an article using the following points:\\n\\n{highlights}\", \"{text}\"),\n  (\"Use the following highlights to write an article:\\n\\n{highlights}\",\n    \"{text}\"),\n  (\"{highlights}\\n\\nWrite an article based on these highlights.\", \"{text}\"),\n],\n```", "```py\n\"cnn_dailymail\": [\n  (\"Distill the essence of this article:\\n\\n{text}\", \"{highlights}\"),\n  (\"Give a quick rundown of this article's key points:\\n\\n{text}\",\n    \"{highlights}\"),\n  (\"Summarize the main elements of this text:\\n\\n{text}\", \"{highlights}\"),\n  (\"Highlight the primary takeaways from the following:\\n\\n{text}\",\n    \"{highlights}\"),\n  (\"Extract and summarize the top points of this article:\\n\\n{text}\",\n    \"{highlights}\"),\n  (\"Condense this article into its most important aspects:\\n\\n{text}\",\n    \"{highlights}\"),\n  (\"What are the key insights of this article?\\n\\n{text}\", \"{highlights}\"),\n      ],\n```", "```py\nInput Template:\nI want to know whether the following two sentences mean the same thing.\n{{sentence1}}\n{{sentence2}}\nDo they?\n\nTarget Template:\n{{ answer_choices[label] }}\n\nAnswer Choices Template:\nno ||| yes\n```", "```py\nDefinition\nIn this task, we ask you convert a data table of restaurant descriptions into\nfluent natural-sounding English sentences.\nThe input is a string of key-value pairs; the output should be a natural and\ngrammatical English sentence containing all the information from the input.\n\nPositive Example\n\nInput: name[Aromi], eatType[restaurant], food[English], area[city centre]\n\nOutput: Aromi is an English restaurant in the city centre.\nExplanation: The output sentence faithfully converts the data in the input\ninto a natural-sounding sentence.\n\nNegative Example\nInput: name[Blue Spice], eatType[coffee shop], priceRange[more than 00a330],\ncustomer rating[5 out of 5], ˘\narea[riverside], familyFriendly[yes], near[Avalon]\nOutput: Blue Spice is a Colombian coffee shop located by the riverside, near\nAvalon in Boston. Its prices are over\n00a330. Its customer ratings are 5 out of 5. ˘\n\nExplanation: While the output contains most of the information from the input,\nit hallucinates by adding ungrounded\ninformation such as \"Colombian\" and \"Boston\".\n\nInstance Input: name[The Mill], eatType[restaurant], area[riverside], near[The\nRice Boat]\n\nValid Output: [\"A restaurant called The Mill, can be found near the riverside `next` `to` `The` `Rice` `Boat``.``\"]`\n```"]