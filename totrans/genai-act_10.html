<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">9</span> </span> <span class="chapter-title-text">Tailoring models with model adaptation and fine-tuning</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">Basics of model adaptation and its advantages</li> 
    <li class="readable-text" id="p3">How to train an LLM</li> 
    <li class="readable-text" id="p4">How to fine-tune an LLM using both SDK and GUI</li> 
    <li class="readable-text" id="p5">Best practices for evaluation criteria and metrics for fine-tuned LLMs</li> 
    <li class="readable-text" id="p6">How to deploy a fine-tuned model for inference</li> 
    <li class="readable-text" id="p7">Gaining insight into key model adaptation techniques</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p8"> 
   <p>As we explore the intricate world of large language models (LLMs), a key aspect that stands at the forefront of practical artificial intelligence (AI) deployment is the concept of model adaptation. In the context of LLMs, model adaptation involves modifying a pretrained model such as GPT-3.5 Turbo to enhance its performance on specific tasks or datasets. This process is important because while pretrained models offer a broad understanding of language and context, they may only excel in specialized tasks with adaptation.</p> 
  </div> 
  <div class="readable-text intended-text" id="p9"> 
   <p>Model adaptation encompasses a range of techniques, each designed to tailor a model’s vast general knowledge to particular applications. The path of model adaptation is not just about enhancing performance but about transforming a generalist AI model into a specialized tool adept at handling the nuanced demands of enterprise solutions.</p> 
  </div> 
  <div class="readable-text intended-text" id="p10"> 
   <p>For enterprises, adaptation enables LLMs to handle industry-specific jargon, comply with regulatory standards in some cases, and align with businesses’ unique operational contexts. This relevance is key to deploying AI solutions that add value to enterprise environments. It is important to note that most organizations should refrain from jumping directly to fine-tuning. We need to consider this as a continuum of various techniques, stacked on and complementing one another; in addition, they are not mutually exclusive. We have already seen many such techniques in the book. For most organizations, if there is a SaaS offering such as a copilot in the application they are already using, that is the best place to start. This application uses the SaaS out-of-the-box offerings of GenAI implementation and has the maximum ROI.</p> 
  </div> 
  <div class="readable-text intended-text" id="p11"> 
   <p>In scenarios where a SaaS solution is neither available nor suitable, and a PaaS approach is preferred, it is advisable to begin with prompt engineering as the foundational step and expand on it. When we need to ground the model generations using our data, we will use retrieval-augmented generation (RAG) combined with prompt engineering, as shown in figure 9.1. When using advanced frontier models such as GPT-4, this combination solves 95% of enterprise business cases. At some point on this continuum, enterprises will reach a point where there is a need to fine-tune a model for specific requirements. Even if we fine-tune, this doesn’t eliminate the need to use prompt engineering and RAG. We will see this case in the chapter as we fine-tune and use a model that still needs prompt engineering to obtain desired results.</p> 
  </div> 
  <div class="readable-text intended-text" id="p12"> 
   <p>This chapter outlines various model adaptation techniques, helping us to understand their challenges, see how enterprises can adopt applications, and finally fine-tune and deploy a model in production. Let’s start by understanding what model adaptation is.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p13">  
   <img alt="figure" src="../Images/CH09_F01_Bahree.png" width="840" height="260"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.1</span> Model adaptation technique progression</h5>
  </div> 
  <div class="readable-text" id="p14"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_145"><span class="num-string">9.1</span> What is model adaptation?</h2> 
  </div> 
  <div class="readable-text" id="p15"> 
   <p>Model adaptation is adjusting an LLM to perform better on a specific task in a specific domain, and it is quite similar to transfer learning. Both approaches involve using a pretrained model as a starting point. These models have typically been trained on large datasets and have developed a robust understanding of various features and patterns. The key idea in model adaptation and transfer learning is to take a model trained on one task and apply it to a different but related task. This saves time and resources that would otherwise be required to train a model from scratch.</p> 
  </div> 
  <div class="readable-text intended-text" id="p16"> 
   <p> As we know, LLMs are trained on a large amount of general text data, which gives them a broad understanding of language. Still, they may not be suitable for certain tasks or domains requiring specialized knowledge or vocabulary.</p> 
  </div> 
  <div class="readable-text intended-text" id="p17"> 
   <p>The main idea behind model adaptation is that the knowledge learned from the original task can aid performance on the new task. At a high level, there are two broad categories of model adaptation—domain and task:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p18"> <em>Domain adaptation</em><em> </em>—If you have a model trained in one domain (e.g., general news articles) and want it to perform well in a different but related domain (e.g., medical news articles), you will use domain adaptation techniques. </li> 
   <li class="readable-text" id="p19"> <em>Task adaptation</em><em> </em>—If you have a model trained for one task (e.g., sentiment analysis) and you want it to perform a new but related task (e.g., emotion detection), task adaptation techniques can be utilized. </li> 
  </ul> 
  <div class="readable-text" id="p20"> 
   <p>For example, an LLM trained on Wikipedia articles might perform poorly on medical questions or legal documents. Therefore, model adaptation is needed to fine-tune the LLM on a smaller, task-specific or domain-specific dataset, which helps the model learn the relevant patterns and features for the target task or domain.</p> 
  </div> 
  <div class="readable-text" id="p21"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_146"><span class="num-string">9.1.1</span> Basics of model adaptation</h3> 
  </div> 
  <div class="readable-text" id="p22"> 
   <p>Model adaptation in LLMs involves refining a pretrained model to better fit specific tasks or data. This concept can be broadly divided into two main categories:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p23"> <em>Full fine-tuning</em><em> </em>—This approach updates all LLM parameters. It involves comprehensive retraining of the model on new data, making substantial changes to its learned patterns. </li> 
   <li class="readable-text" id="p24"> <em>Low-rank adaptation</em><em> </em>—Unlike full fine-tuning, low-rank adaptation focuses on modifying a smaller set of the model’s parameters. This method introduces trainable matrixes into each LLM layer, effectively reducing the number of parameters that need adjustment. This section will primarily focus on this category of model adaptation. </li> 
  </ul> 
  <div class="readable-text" id="p25"> 
   <p>Let’s delve into key techniques underpinning model adaptation:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p26"> <em>Transfer learning</em><em> </em>—This machine learning (ML) strategy involves applying a model trained for one task to a different but related task. For instance, a model trained on English text might be adapted to work with French text. Transfer learning is about using knowledge from one domain to improve performance in another. </li> 
   <li class="readable-text" id="p27"> <em>Fine-tuning</em><em> </em>—Fine-tuning continues training a pretrained model on a new, usually smaller, and more specialized dataset. It subtly adjusts the model’s parameters to align its knowledge with the new task or data. </li> 
  </ul> 
  <div class="readable-text" id="p28"> 
   <p>Depending on the task, data, and the specific LLM, different model adaptation techniques can be applied:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p29"> <em>Task-specific modules</em><em> </em>—This technique adds a module (such as a classifier or decoder) to the LLM, tailored to a particular task. Both the module and the LLM are then fine-tuned on task-specific data. This allows the LLM to learn the intricacies of the specific task, while maintaining its broad linguistic knowledge. </li> 
   <li class="readable-text" id="p30"> <em>Low-rank adaptation (LoRA)</em><em> </em>—LoRA applies a low-rank approximation to the LLM and fine-tunes only these components. This method reduces the number of parameters needing adjustment, while maintaining the model’s performance and flexibility. </li> 
   <li class="readable-text" id="p31"> <em>Federated learning</em><em> </em>—This approach fine-tunes the LLM across multiple distributed datasets, allowing the model to learn from diverse data, while upholding privacy. For example, federated learning could adapt BERT for medical text analysis using data from various hospitals, resulting in a specialized version such as Med-BERT. </li> 
  </ul> 
  <div class="readable-text" id="p32"> 
   <p>No single technique is universally applicable—experimentation is key. Understanding these nuances is crucial for effectively using model adaptation and fine-tuning. These methods embody transfer learning principles and provide practical ways to enhance AI models’ performance and applicability in different scenarios.</p> 
  </div> 
  <div class="readable-text" id="p33"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_147"><span class="num-string">9.1.2</span> Advantages and challenges for enterprises</h3> 
  </div> 
  <div class="readable-text" id="p34"> 
   <p>Model adaptation is increasingly crucial for enterprises in some specific industries and scenarios. It offers substantial efficiency, competitiveness, and innovation benefits. By employing adapted AI models, businesses can achieve more accurate results in less time and with fewer resources than by developing models from scratch. For example, in highly specialized domains (e.g., medical and pharmaceutical), where the margin of error needs to be closer to zero, fine-tuning a model for the specific tasks is one of the few ways to achieve the desired outcome. Other specialized areas, such as complex finance details (e.g., fraud detection) and legacy code migration (e.g., Cobol, etc.), are high-value examples where enterprises would want to consider fine-tuning a model.</p> 
  </div> 
  <div class="readable-text intended-text" id="p35"> 
   <p>Furthermore, enterprises can also perform better on specialized tasks and gain a competitive advantage, depending on the use case. This is especially true in cases where enterprises deal with unique datasets and require models to understand their specific business context. Model adaptation enables customization, improving accuracy and relevance in sentiment analysis, market trend prediction, or personalized customer interactions. By using models adapted to their specific needs, businesses can gain insights and increase efficiency, which will provide them with a competitive advantage in their market.</p> 
  </div> 
  <div class="readable-text intended-text" id="p36"> 
   <p>Enterprises can enhance efficiency and cost savings by reducing resource requirements and resource needs. Fine-tuning existing models requires significantly less computational power and data compared to training models from the ground up, which results in lower costs and quicker deployment times. For example, training Llama 2’s 70B parameter model took many months and 1,720,320 GPU hours, compared to fine-tuning a GPT-3.5 Turbo model, which takes only a few hours.</p> 
  </div> 
  <div class="readable-text intended-text" id="p37"> 
   <p>Model adaptation comes with challenges, and several key areas must be considered. First, task-specific data is crucial. It is essential to have sufficient data to fine-tune an LLM, ensuring that this data is clean, consistent, and representative of the specific task. Depending on the task and LLM characteristics, this data may require preprocessing, augmentation, or labeling. Determining how much data for fine-tuning is enough can be a nuanced process, as it varies based on several factors; at a minimum, it is a few hundred to thousand examples, depending on the model.</p> 
  </div> 
  <div class="readable-text intended-text" id="p38"> 
   <p>Determining adequate data for fine-tuning models such as OpenAI’s GPT-3.5 depends on various factors. The complexity and specificity of the task heavily influence data requirements, with more complex tasks requiring more data. However, the quality of data is crucial and often outweighs the quantity. Larger models such as GPT-3.5 can benefit from more data due to their extensive capacity, but they also can learn effectively from smaller, high-quality datasets. Organizations typically start with a baseline dataset and adjust it based on the model’s performance, which is continuously monitored for signs of overfitting or underfitting. Practical constraints such as computational resources and time also play a role in determining the dataset size. The experience and expertise of data scientists often guide the decision. Comparative analysis and continual evaluation are involved in finding the optimal balance of data quantity and quality for the specific task requirements.</p> 
  </div> 
  <div class="readable-text intended-text" id="p39"> 
   <p>Another significant challenge is related to computational resources and costs. Fine-tuning LLMs can be resource intensive and costly, often requiring substantial processing power (specifically GPUs) connected with high-speed memory. To manage this, it might be necessary to utilize cloud services, invest in specialized hardware, or employ distributed systems. Additionally, the cost of accessing pretrained LLMs can vary, depending on the provider and licensing agreements, which can add to the overall expense.</p> 
  </div> 
  <div class="readable-text intended-text" id="p40"> 
   <p>Performance and generalization are also critical considerations. Evaluating the performance of a fine-tuned LLM is imperative; it involves comparing it to other models or established baselines, which ensures that the fine-tuned LLM does not overfit the training data and can generalize well to new or unseen inputs. We cover evaluations later in this chapter, and more details on benchmarks and associated tools are covered in chapter 12.</p> 
  </div> 
  <div class="readable-text intended-text" id="p41"> 
   <p>The ethical and social implications of using fine-tuned LLMs must be addressed as well. This includes understanding potential risks and biases, such as concerns related to data privacy, model fairness, and social effects. Adhering to appropriate guidelines, standards, or regulations is necessary to ensure the ethical and responsible use of fine-tuned LLMs.</p> 
  </div> 
  <div class="readable-text intended-text" id="p42"> 
   <p>Finally, finding the right talent is critical. The need for specialized talent and expertise is a significant factor in successfully fine-tuning LLMs, which includes individuals who deeply understand ML, natural language processing (NLP), and the specific architecture of LLMs. These experts must be skilled in various areas, such as data preparation, model architecture design, training strategies, and performance evaluation. The need for skilled personnel adds another layer of challenges to the already complex process of LLM fine-tuning.</p> 
  </div> 
  <div class="readable-text" id="p43"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_148"><span class="num-string">9.2</span> When to fine-tune an LLM</h2> 
  </div> 
  <div class="readable-text" id="p44"> 
   <p>Fine-tuning is a technique to improve a model’s performance on a specific task. However, it should be the last option and used only after applying other techniques, such as prompt engineering and RAG. These techniques complement each other and should be stacked for the best output, even when using fine-tuned models. As we saw in earlier chapters, prompt engineering and RAG are not mutually exclusive but are complementary and should be stacked, even when fine-tuning. This stacked approach gives the best outputs, even when using fine-tuned models.</p> 
  </div> 
  <div class="readable-text intended-text" id="p45"> 
   <p>Once we decide to fine-tune a model, we prepare the dataset needed for training and start the fine-tuning process, which can take from a few hours to a few days. After training, we evaluate the fine-tuned model against the base model and the specific task’s baseline.</p> 
  </div> 
  <div class="readable-text intended-text" id="p46"> 
   <p>Let’s use an example to help us fine-tune and understand various aspects. Say we want to adapt a model to respond with emojis—a bot that can understand what we are asking but respond only using emojis. We will call this EmojiBot. We want to fine-tune GPT-3.5 Turbo and make it an EmojiBot. But to show that these emojis are different and specialized for a task, we don’t want the emojis that we would expect to see, say, in a chat application, on social media, or in our texts. Rather, we want the ones that follow the format used by Microsoft Teams.</p> 
  </div> 
  <div class="readable-text intended-text" id="p47"> 
   <p>Figure 9.2 shows the high-level flow for fine-tuning. First, we identify a task that would benefit from fine-tuning (such as EmojiBot). We identify which characteristics fall short of the task and create evaluation criteria. We then compare the default models’ performance against our needs. If they perform well, we establish a baseline and curate the dataset required for fine-tuning. The amount and format of data depend on the model; we’ll cover the details later. We obtain a fine-tuned model after training, which can take hours or days, depending on the task. Next, we must evaluate it against the base model and the baseline for the specific task using qualitative and quantitative measures.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p48">  
   <img alt="figure" src="../Images/CH09_F02_Bahree.png" width="919" height="516"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.2</span> Fine-tuning end-to-end flow<span class="aframe-location"/></h5>
  </div> 
  <div class="readable-text" id="p49"> 
   <p>It is quite common and almost expected that the first fine-tuned model will be worse than the default model. Usually, finding a suitable deployment model takes 10–12 training iterations. Each iteration requires tweaking the training data to address weak areas, which can take hours to days. It’s a time- and effort-consuming process that should be one of the last steps.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p50"> 
   <p><span class="print-book-callout-head">NOTE </span> Fine-tuning enhances the model’s performance on tasks similar to those outlined in the fine-tuning dataset. This process might manifest as improved accuracy, more relevant responses, or a better understanding of domain-specific language. Improved performance in terms of cheaper or faster models is a side advantage and not something guaranteed. One way to achieve this is to fine-tune a smaller model, such as GPT-3.5 Turbo, on a specific task to improve it instead of using a more expensive and powerful model, such as GPT-4.</p> 
  </div> 
  <div class="readable-text" id="p51"> 
   <p>Now that we have identified a task that makes sense to fine-tune—that is, an EmojiBot where we want to respond in emojis but in a certain pattern—let’s examine the steps needed to fine-tune an LLM such as GPT-3.5 Turbo.</p> 
  </div> 
  <div class="readable-text" id="p52"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_149"><span class="num-string">9.2.1</span> Key stages of fine-tuning an LLM</h3> 
  </div> 
  <div class="readable-text" id="p53"> 
   <p>When we want to fine-tune a model for an identified task, as outlined later in figure 9.6, section 9.3.5, there are five key stages:</p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p54"> <em>Choosing a model and fine-tuning method</em><em> </em>—To fine-tune a language model, it is necessary to choose a foundation model that suits the task and data. Various models are available, such as GPT, BERT, and RoBERTa. Consider factors such as the model’s suitability for the task, input/output size, dataset size, and technical infrastructure. Fine-tuning methods can vary based on the task and data, such as transfer learning, sequential fine-tuning, or task-specific fine-tuning. </li> 
   <li class="readable-text" id="p55"> <em>Data curation</em><em> </em>—This stage involves preparing a task-specific dataset for fine-tuning and largely involves preparing and preprocessing the dataset. This process often includes data cleaning, text normalization (e.g., tokenization), and converting the data into a format compatible with the LLM’s input requirements (e.g., data labeling). It is essential to ensure that the data represents the task and domain and covers a range of scenarios the model is expected to encounter in production. </li> 
   <li class="readable-text" id="p56"> <em>Fine-tuning</em><em> </em>—This stage is the actual process of fine-tuning and involves training the pretrained LLM on the task-specific dataset. The training process involves optimizing the model’s weights and parameters to minimize the loss function and improve its performance on the task. The fine-tuning process may involve several rounds of training on the training set, validation of the validation set, and hyperparameter tuning to optimize the model’s performance. </li> 
   <li class="readable-text" id="p57"> <em>Evaluating</em><em> </em>—Once the fine-tuning process is complete, we must evaluate the model’s performance on a test dataset. This helps to ensure that the model is generalizing well to new data and performing well on the specific task. Common metrics used for evaluation include accuracy, precision, recall, F1 score, Bilingual Evaluation Understudy (BLEU), Recall-Oriented Understudy for Gisting Evaluation (ROUGE), and so forth. This topic is covered later in detail in section 9.3.2. </li> 
   <li class="readable-text" id="p58"> <em>Deployment (inference)</em><em> </em>—Once the fine-tuned model is evaluated and we are happy with its performance, it can be deployed to production. The deployment process may involve integrating the model into a larger system, setting up the necessary infrastructure, and monitoring the model’s performance in real-world scenarios. </li> 
  </ol> 
  <div class="readable-text" id="p59"> 
   <p>Now that we have a basic concept of model adaptation and when to fine-tune, let’s see how to fine-tune.</p> 
  </div> 
  <div class="readable-text" id="p60"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_150"><span class="num-string">9.3</span> Fine-tuning OpenAI models</h2> 
  </div> 
  <div class="readable-text" id="p61"> 
   <p>Here, we’ll use an example to fine-tune OpenAI’s GPT-3.5 Turbo model. Currently, for OpenAI, only GPT-4, GPT-3.5 Turbo, GPT-3 Babbage (Babbage-002), and GPT-3 (Davinci-002) are available for fine-tuning. Several OSS LLMs, such as Meta’s Llama 2 and G42’s Falcon, can be fine-tuned. In our case, the book’s GitHub repository (<a href="https://bit.ly/GenAIBook">https://bit.ly/GenAIBook</a>) contains complete code samples and screenshots that we use and show how to fine-tune OpenAI GPT-3.5 Turbo. To make this as real for organizations as possible, we will show the process by using both Azure OpenAI and OpenAI.</p> 
  </div> 
  <div class="readable-text intended-text" id="p62"> 
   <p>We want to fine-tune GPT-3.5 Turbo and make it an EmojiBot, where the model responds in emojis only. However, as we outlined earlier, we want emojis to follow the format used by Microsoft Teams.</p> 
  </div> 
  <div class="readable-text intended-text" id="p63"> 
   <p>In Microsoft Teams, the text in parentheses, such as <code>(dog)</code>, renders the relevant emojis. We will fine-tune the model to respond to this text, which represents the specific task we want the model to improve. To understand all the different options and the corresponding text in Teams, see <a href="https://bit.ly/TeamEmojis">https://bit.ly/TeamEmojis</a>. Given that we have a task, let’s start preparing the dataset.</p> 
  </div> 
  <div class="readable-text" id="p64"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_151"><span class="num-string">9.3.1</span> Preparing a dataset for fine-tuning</h3> 
  </div> 
  <div class="readable-text" id="p65"> 
   <p>Now that we have reached a point where we have identified a task for which fine-tuning would make sense, we need to create a dataset of examples required to fine-tune. We need to create two sets of datasets: one for training and another for validation. A validation dataset is a subset of data used to evaluate the performance of a fine-tuned model on the target task. It is different from the training dataset, which is used to update the model’s parameters, and the test dataset, which is used to measure the final accuracy of the model.</p> 
  </div> 
  <div class="readable-text intended-text" id="p66"> 
   <p>A validation dataset is important for fine-tuning LLMs because it helps us to avoid overfitting, which is when the model learns the specific patterns of the training data and fails to generalize to new data. Using a validation dataset, you can monitor the model’s progress and adjust the learning rate, the number of epochs, or other hyperparameters to optimize the model’s performance.</p> 
  </div> 
  <div class="readable-text intended-text" id="p67"> 
   <p>These examples should show different ways to solve the problem and the results of each method. We also need to identify shortcomings using a base model, such as inconsistent performance on edge cases, inability to fit enough shot prompts in the context window to steer the model, high latency, and so forth.</p> 
  </div> 
  <div class="readable-text intended-text" id="p68"> 
   <p>It is highly recommended that a validation dataset be used to measure the effectiveness of fine-tuning. The training and validation datasets are in the JSONL format, with each line containing a JSON object with a text key for input text and a target key for desired output text.</p> 
  </div> 
  <div class="readable-text intended-text" id="p69"> 
   <p>Fine-tuned models are directly correlated with high-quality training data. Different models require varying amounts of training data. For effective training, we need hundreds to thousands of curated data examples. Although the API requires a minimum of 10 examples, having more is generally better. Ten examples aren’t enough to influence LLMs such as GPT-3.5 Turbo in any significant way.</p> 
  </div> 
  <div class="readable-text intended-text" id="p70"> 
   <p>OpenAI recommends having at least 50 good examples to train our model. They also recommend more good examples for better-fine-tuned models than bad ones, as those examples can negatively affect the model. Consequently, it is advisable only to use the best ones from your internal data. The following listing shows an example JSONL file for chat data.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p71"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 9.1</span> JSONL example</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">{"messages": [{"role": "system", "content": "You're a chatbot that only responds with emojis!"}, {"role": "user", "content": "I heard the koala habitat was destroyed by the fire."}, {"role": "assistant", "content": "(sadkoala)" }]}

{"messages": [{"role": "system", "content": "You're a chatbot that only responds with emojis!"}, {"role": "user", "content": "I've been working nonstop and need a break."}, {"role": "assistant", "content": "(tired)" }]}

{"messages": [{"role": "system", "content": "You're a chatbot that only responds with emojis!"}, {"role": "user", "content": "I just finished reading an amazing book!"}, {"role": "assistant", "content": "(like)" }]}</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p72"> 
   <p>As we can see, the model is being shown how to respond using emojis formatted in a certain pattern, such as <code>(sadkoala)</code>, <code>(tired)</code>, and <code>(like)</code>.</p> 
  </div> 
  <div class="readable-text" id="p73"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Basic checks</h4> 
  </div> 
  <div class="readable-text" id="p74"> 
   <p>Before fine-tuning, it’s important to perform basic checks on the training data to avoid wasting time and resources. These checks can include data readability, formatting validation, lightweight analysis for missing pairs, and token length.</p> 
  </div> 
  <div class="readable-text intended-text" id="p75"> 
   <p>We validate the data file by loading and reading it using the <code>basic_checks()</code> function. It takes a filename as input and returns the number of messages found. The messages must be in the chat completion format for fine-tuning GPT-3.5 Turbo.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p76"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 9.2</span> Dataset validation: Basic checks</h5> 
   <div class="code-area-container"> 
    <pre class="code-area"># Basic checks to ensure the data file is valid
def basic_checks(data_file):
    try:
        with open(data_file, 'r', encoding='utf-8') as f:   <span class="aframe-location"/> #1
         <span class="aframe-location"/>   dataset = [json.loads(line) for line in f]       #2

        print(f"Basic checks for file {data_file}:")
        print("Count of examples in training dataset:", len(dataset))
        print("First example:")                             <span class="aframe-location"/> #3
        for message in dataset[0]["messages"]:            <span class="aframe-location"/> #4
            print(message)
        return True
    except Exception as e:
        print(f"An error occurred in file {data_file}: {e}")
        return False</pre> 
    <div class="code-annotations-overlay-container">
     #1 Opens the file in read-mode
     <br/>#2 Loads each line of the file as a JSON object and stores it in a list
     <br/>#3 Prints the first example from the dataset and helps visually check whether things intuitively look OK
     <br/>#4 Loops through the messages in the first example and prints each one
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p77"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Format checks</h4> 
  </div> 
  <div class="readable-text" id="p78"> 
   <p>Once we have done the basic checks, the next step is to check the file for the format and ensure it is structured properly before processing it further. This is an important step, mainly because even if the format is incorrect, we won’t get an error when we start the training job, but the resulting model will be very poor, and we will only realize this posttraining when we deploy. To avoid much of this trouble, it is highly recommended that we check for formats.</p> 
  </div> 
  <div class="readable-text intended-text" id="p79"> 
   <p>Listing 9.3 shows <code>format_checks()</code>, which checks for chat completion format and pairing, with dataset and filename as its two arguments. It catches most errors but not all. The function iterates over each example in the dataset and checks for data type checks, the presence of message lists, and message keys. It validates that it has the relevant roles and content validation. This function also helps debug data-related problems.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p80"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 9.3</span> Dataset validation: Checking for format </h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">def format_checks(dataset, filename):
    # Initialize a dictionary used to track format errors
    format_errors = defaultdict(int)

    # Iterate over each example in the dataset
    for ex in dataset:
        # Check if the example is a dictionary, if not 
        # increment the corresponding error count
        if not isinstance(ex, dict):
            format_errors["data_type"] += 1
            continue

        # Check if the example has a "messages" key, 
        # if not increment the corresponding error count
        messages = ex.get("messages", None)
        if not messages:
            format_errors["missing_messages_list"] += 1
            continue

        # Iterate over each message
        for message in messages:
            # Check if the message has "role" and "content" keys,
            # if not increment the corresponding error count
            if "role" not in message or "content" not in message:
                format_errors["message_missing_key"] += 1

            # Check if the message has any unrecognized keys,
            # if so increment the corresponding error count
            if any(k not in ("role", "content", "name", 
                   <span class="">↪</span>"function_call") for k in message):
                format_errors["message_unrecognized_key"] += 1

            # Check if the role of the message is one of the recognized
            # roles, if not increment the corresponding error count
            if message.get("role", None) not in (
                "system",
                "user",
                "assistant",
                "function",
            ):
                format_errors["unrecognized_role"] += 1

            # Check if the message has either content or a function call, 
            # and if the content is a string, if not increment the 
            # corresponding error count
            content = message.get("content", None)
            function_call = message.get("function_call", None)
            if (not content and not function_call) or not 
            <span class="">↪</span>isinstance(content, str):
                format_errors["missing_content"] += 1

        # Check if there is at least one message with the role "assistant",
        # if not increment the corresponding error count
        if not any(message.get("role", None) == "assistant" 
        <span class="">↪</span>for message in messages):
            format_errors["example_missing_assistant_message"] += 1

    # If there are any format errors, print them and return False
    if format_errors:
        print(f"Formatting errors found in file {filename}:")
        for k, v in format_errors.items():
            print(f"{k}: {v}")
        return False

    print(f"No formatting errors found in file {filename}")
    return True</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p81"> 
   <p>Finally, we should also understand how the dataset performs when it comes to simple data distributions, token counts, and costs.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p82"> 
   <p><span class="print-book-callout-head">Note </span> The token count is important, not just for cost. If it is larger than the maximum number of tokens the model can handle, it will be truncated without warning. Knowing this up front is very helpful. </p> 
  </div> 
  <div class="readable-text" id="p83"> 
   <p>The following listing shows how we can finish doing the checks on the dataset.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p84"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 9.4</span> Dataset validation: Cost estimation and basic analysis </h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"># Pricing and default n_epochs estimate
MAX_TOKENS = 4096

TARGET_EPOCHS = 3
MIN_TARGET_EXAMPLES = 100
MAX_TARGET_EXAMPLES = 25000
MIN_DEFAULT_EPOCHS = 1
MAX_DEFAULT_EPOCHS = 25

def estimate_tokens(dataset, assistant_tokens):
    # Set the initial number of epochs to the target epochs
    n_epochs = TARGET_EPOCHS

    # Get the number of examples in the dataset
    n_train_examples = len(dataset)

    # If the examples total is less than the minimum target
    # adjust the epochs to ensure we have enough examples for
    # training
    if n_train_examples * TARGET_EPOCHS &lt; MIN_TARGET_EXAMPLES:
        n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES 
           <span class="">↪</span>// n_train_examples)
    # If the  number of examples is more than the maximum target
    # adjust the  epochs to ensure we don't exceed the maximum 
    # for training
    elif n_train_examples * TARGET_EPOCHS &gt; MAX_TARGET_EXAMPLES:
        n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES 
          <span class="">↪</span>// n_train_examples)

    # Calculate the total number of tokens in the dataset
    n_billing_tokens_in_dataset = sum(
        min(MAX_TOKENS, length) for length in assistant_tokens
    )

    # Print the total token count that will be charged during training
    print(
        f"Dataset has ~{n_billing_tokens_in_dataset} tokens that 
            <span class="">↪</span>will be charged for during training"
    )

    # Print the default number of epochs for training
    print(f"You will train for {n_epochs} epochs on this dataset")

    # Print the total number of tokens that will be charged during training
    print(f"You will be charged for ~{n_epochs * 
                        <span class="">↪</span>n_billing_tokens_in_dataset} tokens")

    # If the total token count exceeds the maximum tokens, print a warning 
    if n_billing_tokens_in_dataset &gt; MAX_TOKENS:
        print(
            f"WARNING: Your dataset contains examples longer than 
                       <span class="">↪</span>4K tokens by {n_billing_tokens_in_dataset – 
                       <span class="">↪</span>MAX_TOKENS} tokens."
        )
        print(
            "You will be charged for the full length of these 
             <span class="">↪</span>examples during training, but only the first
             <span class="">↪</span>4K tokens will be used for training."</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p85"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_152"><span class="num-string">9.3.2</span> LLM evaluation</h3> 
  </div> 
  <div class="readable-text" id="p86"> 
   <p>Evaluating LLMs is important for ensuring their quality, reliability, and fairness. However, evaluating LLMs is complex, as it involves multiple dimensions and challenges. Maintaining diverse automatic metrics can help efficiently track model improvements during adaptation cycles, while reducing costly manual reviews. Metrics should be customized to each adapted model’s use cases and business needs. Continuous logging from production systems enables the evaluation of real-world performance over time. </p> 
  </div> 
  <div class="readable-text intended-text" id="p87"> 
   <p>Benchmarking against baselines is an essential step in evaluating fine-tuned GPT models. It involves comparing the performance of the fine-tuned model with a preestablished standard or baseline model. This baseline could be the model’s performance before fine-tuning or a different model known for its proficiency in a similar task. The purpose of this comparison is to quantify the improvements brought by fine-tuning. For instance, a fine-tuned model might be benchmarked against a standard translation model in a language translation task to assess translation accuracy or fluency improvements. This process helps in understanding the efficacy of fine-tuning and identifying areas where the model has improved or still needs enhancement.</p> 
  </div> 
  <div class="readable-text" id="p88"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Evaluation criteria</h4> 
  </div> 
  <div class="readable-text" id="p89"> 
   <p>When preparing the fine-tuning dataset, we should also define the evaluation criteria. When fine-tuning, the evaluation process begins by establishing clear criteria critical for assessing the performance and efficacy of the model in its intended application. These criteria often include relevance, coherence, accuracy, and language fluency (table 9.1).</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p90"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 9.1</span> Fine-tuning evaluation criteria</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Evaluation criteria 
       </div></th> 
      <th> 
       <div>
         Description 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  Relevance <br/></td> 
      <td>  Gauges how well the model’s responses or outputs align with the context and intent of the input. This is especially crucial in applications such as chatbots, where providing contextually appropriate responses is key to user satisfaction. Relevance is often assessed by examining whether the model can stay on topic and provide information or responses directly applicable to the queries or tasks. <br/></td> 
     </tr> 
     <tr> 
      <td>  Coherence <br/></td> 
      <td>  Refers to the logical consistency of the model’s outputs. A fine-tuned model should generate contextually relevant, logically sound, and coherent text. This means the responses should follow a logical structure and narrative flow, making sense in the conversation or text context. Coherence is vital for maintaining user engagement and ensuring the model’s outputs are understandable and meaningful. <br/></td> 
     </tr> 
     <tr> 
      <td>  Accuracy <br/></td> 
      <td>  This particularly comes into play when the model is used for tasks involving factual information, such as educational tools, informational bots, or any application where providing correct information is critical. Accuracy is measured by how well the model’s responses align with factual correctness and objective truth. <br/></td> 
     </tr> 
     <tr> 
      <td>  Language fluency <br/></td> 
      <td>  Pertains to the grammatical and syntactical correctness of the model’s outputs. Even if a model is highly relevant, coherent, and accurate, poor language fluency can significantly detract from the user’s experience. This includes proper grammar, punctuation, and style, ensuring the text generated is correct and reads naturally to the end user. <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p91"> 
   <p>Evaluating a fine-tuned GPT model using these criteria involves a combination of automated metrics, manual review, and user feedback, ensuring that the model meets the high standards required for its specific application.</p> 
  </div> 
  <div class="readable-text" id="p92"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Choosing appropriate metrics</h4> 
  </div> 
  <div class="readable-text" id="p93"> 
   <p>When fine-tuning models, selecting the right metrics for evaluation is crucial to accurately assessing the model’s performance and improvements [1]. After fine-tuning, these metrics indicate how well the model adapts to specific tasks or domains. They provide insights into various aspects of model performance, such as prediction accuracy, language quality, and task-specific effectiveness. Enterprises should look for automated metrics evaluation where possible and have a set of quantitative and qualitative metrics.</p> 
  </div> 
  <ul> 
   <li class="readable-text buletless-item" id="p94"> Quantitative metrics: 
    <ul> 
     <li> Several metrics help measure the overlap between model outputs and human reference texts. The next section will outline some of them (BLEU, ROUGE, METEOR, etc.). </li> 
     <li> F1 score evaluates the accuracy tradeoff between precision and recall. </li> 
     <li> Perplexity assesses model uncertainty/confidence for generated text. </li> 
     <li> Task completion is used for goal-oriented dialog systems and the percentage of successful task resolution. </li> 
    </ul></li> 
   <li class="readable-text buletless-item" id="p95"> Qualitative metrics: 
    <ul> 
     <li> <em>Fluency</em><em> </em>—Rating grammaticality and readability of outputs </li> 
     <li> <em>Coherence</em><em> </em>—Logical consistency and narrative flow </li> 
     <li> <em>Conciseness</em><em> </em>—Avoiding repetitive and excessive text </li> 
     <li> <em>Factual accuracy</em><em> </em>—Avoiding objective falsehoods </li> 
    </ul></li> 
  </ul> 
  <div class="readable-text" id="p96"> 
   <p>The choice of metrics should align with the model’s intended application, whether translation, summarization, classification, or creative content generation. Metrics such as perplexity, BLEU score, ROUGE, F1 score, and human evaluation each offer a unique perspective on the model’s capabilities, helping to ensure a comprehensive and balanced evaluation of the fine-tuned model’s performance. Let’s look at each of these in more detail:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p97"> <em>Perplexity</em><em> </em>—This metric is a standard in language modeling, used to quantify how well a model predicts a sample. It measures the uncertainty of the language model in predicting the next token in a sequence [2]. A lower perplexity score indicates that the model is more confident and accurate in its predictions. This is particularly important in fine-tuning, as it can reflect how well the model has adapted to new styles or domains of text. It’s a crucial metric for assessing improvements in language generation tasks. </li> 
   <li class="readable-text" id="p98"> <em>BLEU score (Bilingual Evaluation Understudy)</em><em> </em>—The BLEU score evaluates machine translation quality by comparing it to reference translations. It counts matching word groupings and computes a score based on these matches. A higher BLEU score indicates better translation quality, but it has limitations and may not capture semantic accuracy or fluency [3].  </li> 
   <li class="readable-text" id="p99"> <em>ROUGE (Recall-Oriented Understudy for Gisting Evaluation)</em><em> </em>—ROUGE is a metric for automatic summarization evaluation. It measures the overlap between computer-generated output and reference summaries to assess the summary’s quality. Different variations of ROUGE provide insights into aspects of the summary’s quality [4].  </li> 
   <li class="readable-text" id="p100"> <em>F1 score</em><em> </em>—The F1 score is useful in classification tasks such as sentiment analysis and topic categorization. It balances the tradeoff between precision and recall, providing a single measure of a model’s accuracy in categorizing or classifying text. </li> 
   <li class="readable-text" id="p101"> <em>Human evaluation</em><em> </em>—Despite the utility of automated metrics, human judgment remains crucial, especially for tasks that require subjective assessment, such as story generation, creative writing, and conversational agents. Human evaluators can provide insights into aspects such as the naturalness of the text, its appropriateness, creativity, and even the subtleties of humor or sarcasm. This qualitative evaluation complements quantitative metrics, offering a more holistic view of the model’s performance. </li> 
  </ul> 
  <div class="readable-text" id="p102"> 
   <p>Task-specific evaluation is essential to measuring a model’s performance in its intended application. It involves using different metrics and considerations based on the task. For instance, summarization models are evaluated using ROUGE scores and human summary coherence and informativeness assessments. Similarly, question-answering models are evaluated for accuracy and relevance to the given questions. This evaluation ensures the model performs well in general metrics and is effective and reliable for its specific use case.</p> 
  </div> 
  <div class="readable-text" id="p103"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Error analysis</h4> 
  </div> 
  <div class="readable-text" id="p104"> 
   <p>Error analysis is a critical component of the evaluation process, involving a detailed examination of where and why the fine-tuned model is underperforming. This analysis helps identify patterns in the model’s mistakes, which can be categorically broken down into semantic errors, factual inaccuracies, or language inconsistencies.</p> 
  </div> 
  <div class="readable-text intended-text" id="p105"> 
   <p>For example, if a model consistently makes errors in understanding certain types of queries or generates responses with factual errors, this would be highlighted in error analysis. Understanding these error patterns is crucial for further refining the model and making targeted improvements. It also aids in understanding the model’s limitations and areas where it might require additional data or more sophisticated fine-tuning approaches. Now let’s get to fine-tuning.</p> 
  </div> 
  <div class="readable-text" id="p106"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_153"><span class="num-string">9.3.3</span> Fine-tuning</h3> 
  </div> 
  <div class="readable-text" id="p107"> 
   <p>Now that our dataset is ready and validated, we can kick off the fine-tuning process. There are two steps to perform when we need to fine-tune. First, we upload the dataset we worked on in the previous sections. When uploaded, each file gets a unique file ID that we need to save. This file ID is what we pass as one of the parameters to the fine-tuning job so it knows which file to use for which fine-tuning job.</p> 
  </div> 
  <div class="readable-text intended-text" id="p108"> 
   <p>We can use the API or GUI to do this. We will see how to achieve this using Python SDK and Azure AI Studio. I won’t show all the steps in the GUI book, but those details are available in the accompanying GitHub repo at <a href="https://bit.ly/GenAIBook">https://bit.ly/GenAIBook</a>. Let’s start by using the SDK.</p> 
  </div> 
  <div class="readable-text" id="p109"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Fine-tuning using the SDK</h4> 
  </div> 
  <div class="readable-text" id="p110"> 
   <p>The following listing shows how to use the SDK and <code>files.create()</code> method, pass in the file name, and specify the purpose of the file (<code>fine-tune</code>).</p> 
  </div> 
  <div class="browsable-container listing-container" id="p111"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 9.5</span> Uploading dataset for fine-tune </h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import os
from openai import AzureOpenAI

API_VERSION = '2023-09-15-preview'                  <span class="aframe-location"/> #1

client = AzureOpenAI(
    api_key=os.getenv('AOAI_FT_KEY'),               <span class="aframe-location"/> #2
    api_version=API_VERSION,
    azure_endpoint = os.getenv('AOAI_FT_ENDPOINT'))  #2

TRAINING_FILENAME = 'data/emoji_FT_train.jsonl'      <span class="aframe-location"/> #3

# Upload the training dataset files
file = client.files.create(
    file=open(TRAINING_FILENAME, "rb"),
    purpose="fine-tune"
)

print("Training file ID:", file.id)
print("Training file name:", file.filename)</pre> 
    <div class="code-annotations-overlay-container">
     #1 This version (or later) is required for fine-tuning.
     <br/>#2 Environment variables with the connection details
     <br/>#3 Dataset that we need to use for training
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p112"> 
   <p>When we run this snippet, we obtain the following output, with the file ID we need to be aware of when we run the second step:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p113"> 
   <div class="code-area-container"> 
    <pre class="code-area">Response:
 Training file ID: file-ca4c57d7ad814211a2db49e0382c5a77
 Training file name: emoji_FT_train.jsonl</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p114"> 
   <p>After uploading our file, we must start the fine-tuning job. When using the SDK, this is done using the <code>fine_tunings.jobs.create()</code> method. This function needs the ID of the training dataset file from the previous steps and the model to use. In our case, we want to fine-tune GPT-3.5 Turbo, specifically the 0613 model. We also specify how many epochs we need for fine-tuning. Finally, the <code>suffix</code> parameter is something we can use to help track and manage the fine-tuned model later.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p115"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 9.6</span> Starting the fine-tuning job</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import os
from openai import AzureOpenAI

API_VERSION = '2023-09-15-preview'                       <span class="aframe-location"/> #1

# Connect to the servvice
client = AzureOpenAI(
    api_key=os.getenv('AOAI_FT_KEY'),                     <span class="aframe-location"/> #2
    api_version=API_VERSION,
    azure_endpoint = os.getenv('AOAI_FT_ENDPOINT'))        #2

# Begin by creating the fine-tuning job
ft = client.fine_tuning.jobs.create(
    training_file="file-ca4c57d7ad814211a2db49e0382c5a77",  <span class="aframe-location"/> #3
    model="gpt-35-turbo-0613",
    hyperparameters={
        "n_epochs":3
    },
    suffix="emoji"
)
print("Finetuning job ID:", ft.id)</pre> 
    <div class="code-annotations-overlay-container">
     #1 This version (or later) is required for fine-tuning.
     <br/>#2 Environment variables with connection details
     <br/>#3 The file ID you see will differ from this one.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p116"> 
   <p>This snippet submits a finetuning job that gets queued up; depending on available capacity at the specific region and data center, the job will get executed. As a reminder, with Azure OpenAI, you can have multiple regions where fine-tuning is available. Our example shows the job ID from our API call:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p117"> 
   <div class="code-area-container"> 
    <pre class="code-area">Finetuning job ID: ftjob-367ee1995af740a0bf24876221585f7a</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p118"> 
   <p>Depending on the dataset size, the model we want to fine-tune, and the fine-tuning hyperparameters, the fine-tuning job can take a few hours. The fine-tuning jobs API has a function call <code>list()</code> that we can use to see all the fine-tuning jobs we have submitted. </p> 
  </div> 
  <div class="browsable-container listing-container" id="p119"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 9.7</span> Listing all the fine-tuning jobs</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import os
from openai import AzureOpenAI

API_VERSION = '2023-09-15-preview'

client = AzureOpenAI(
    api_key=os.getenv('AOAI_FT_KEY'),
    api_version=API_VERSION,
    azure_endpoint = os.getenv('AOAI_FT_ENDPOINT'))

# List all the FT jobs
ft_jobs = client.fine_tuning.jobs.list()

for ft_job in ft_jobs:
    print(ft_job.id, ft_job.status)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p120"> 
   <p>One example of this output is presented in listing 9.8. We see that we have completed two fine-tuning jobs, as shown by the <code>succeeded</code> status; one job is currently in the status <code>running</code>, which means that it has one active fine-tuning job ongoing. The last fine-tuning job (<code>ftjob-367ee1995...</code>) we have just submitted as <code>pending</code> means that the job is queued up to run at some point in the future.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p121"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 9.8</span> Output of fine-tuning jobs listing</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">ftjob-367ee1995af740a0bf24876221585f7a pending
ftjob-c41a9dc551834a1aa0be8befe788a22b running
ftjob-1a7faac8856d46e48a038c02555fe6e5 succeeded
ftjob-505d5a8bd321406dbf4605b636b0c0cd succeeded</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p122"> 
   <p>For a specific fine-tuning job, we can also see the various events related to that job. The following listing shows an example of this, again using the ID of our newly submitted job (<code>ftjob-367ee1995...)</code>.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p123"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 9.9</span> Listing events from a fine-tuning job</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import os
from openai import AzureOpenAI

API_VERSION = '2023-09-15-preview'

client = AzureOpenAI(
    api_key=os.getenv('AOAI_FT_KEY'),
    api_version=API_VERSION,
    azure_endpoint = os.getenv('AOAI_FT_ENDPOINT'))

#List all the FT events for the job from
#earier: ftjob-367ee1995af740a0bf24876221585f7a
ft_job_events = client.fine_tuning.jobs.list_events(
    fine_tuning_job_id="ftjob-367ee1995af740a0bf24876221585f7a ", 
    limit=2)

# Loop through the events and print the details
for ft_job_event in ft_job_events:
    print(ft_job_event.id, ft_job_event.message)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p124"> 
   <p>The output in this case is </p> 
  </div> 
  <div class="browsable-container listing-container" id="p125"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">ftevent-1e89dc7cc62046048bcea50de1cccbb9 Jobs ahead in queue: 1
ftevent-42649f5c7677472f83eaa6cd4cde0dba Job enqueued. 
<span class="">↪</span>Waiting for jobs ahead to complete.</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p126"> 
   <p>We can also poll to check the status of a job every few seconds and, using this, kick off another workflow. In this instance, this job ran for approximately two hours before finishing. For this, we need the <code>IPython</code> package, which can be installed in conda using <code>conda</code> <code>install</code> <code>ipython</code>, or if one is using pip, then via – <code>pip</code> <code>install</code> <code>ipython.</code></p> 
  </div> 
  <div class="browsable-container listing-container" id="p127"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 9.10</span> Polling to check fine-tuning job status</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"># Define the API version
API_VERSION = '2023-09-15-preview'

# Create an instance of the AzureOpenAI client
client = AzureOpenAI(
    api_key=os.getenv('AOAI_FT_KEY'),
    api_version=API_VERSION,
    azure_endpoint = os.getenv('AOAI_FT_ENDPOINT'))

# Define the job ID of the fine-tuning job to track
JOB_ID = "ftjob-367ee1995af740a0bf24876221585f7a"

# Record the start time of the tracking
start_time = time.time()

# Get the status of the fine-tuning job
ft_job = client.fine_tuning.jobs.retrieve(JOB_ID)
status = ft_job.status

# If the job is not yet done, continue to poll its status every 30 seconds
while status not in ["succeeded", "failed"]:
    ft_job = client.fine_tuning.jobs.retrieve(JOB_ID)
    print(ft_job)

    # Update the status
    status = ft_job.status

    # Print the elapsed time since the start of tracking
    print("Elapsed time: {} minutes {} seconds".format( 
        <span class="">↪</span>int((time.time() - start_time) // 60), 
        <span class="">↪</span>int((time.time() - start_time) % 60)))

    # Print the current status
    print(f'Status: {status}')

    # Clear the output before displaying new output – prevents flickering
    clear_output(wait=True)

    # Wait for 30 seconds before the next poll
    time.sleep(30)

# Once the job is done, print its final status
print(f'Fine-tuning job {JOB_ID} finished with status: {status}')</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p128"> 
   <p>Depending on the model and the length of the queue to schedule the fine-tuning task, one fine-tuning job can take a few hours to finish. During this time, we get training metrics that help us understand how the training goes.</p> 
  </div> 
  <div class="readable-text" id="p129"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_154"><span class="num-string">9.3.4</span> Fine-tuning training metrics</h3> 
  </div> 
  <div class="readable-text" id="p130"> 
   <p>As outlined earlier, the training can take a few hours; for more complex and bigger models, it can take a few days. The training during this time is not a black box; we can get details on key metrics during the process to get a high-level idea of what is happening. We have three key metrics that can be tracked—the training loss, mean token accuracy, and token accuracy.</p> 
  </div> 
  <div class="readable-text" id="p131"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Loss</h4> 
  </div> 
  <div class="readable-text" id="p132"> 
   <p>There are two aspects of loss: training and validation loss. Training loss measures the difference between the model’s predictions and the actual outcomes. A lower loss means the model is more accurate and has less error. Lower loss values indicate better model performance, suggesting the model’s predictions are closer to the actual data.</p> 
  </div> 
  <div class="readable-text intended-text" id="p133"> 
   <p>If we have a validation dataset (which is highly recommended), then we also have additional metrics that allow us to measure how the model is doing. The validation loss is a metric that measures the model’s error on the validation set, a portion of the dataset set aside to evaluate the model’s performance on new or unseen data. The validation loss is calculated by summing up the errors for each example in the validation set, using the same cost function as the training loss. The validation loss is usually measured after each epoch, a complete pass through the training set.</p> 
  </div> 
  <div class="readable-text intended-text" id="p134"> 
   <p>Figure 9.3 shows an example of the loss when we fine-tune using Azure OpenAI and the model performance during training. The graph in figure 9.3 showing the training loss for fine-tuning training results illustrates how well the model learns from the training data.</p> 
  </div> 
  <div class="readable-text intended-text" id="p135"> 
   <p>We see the loss value for each training step, a batch of training examples. The <em>x</em>-axis is the step number, and the <em>y</em>-axis is the loss value. The graph shows that the loss decreases as the model trains on more data, indicating that it is improving its performance. However, the loss does not reach zero, which means the model still has some errors and cannot perfectly fit the data. This is normal, as overfitting the data can lead to poor generalization of new data.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p136">  
   <img alt="figure" src="../Images/CH09_F03_Bahree.png" width="1100" height="766"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.3</span> Training loss when fine-tuning GPT-3.5 Turbo</h5>
  </div> 
  <div class="readable-text" id="p137"> 
   <p>To interpret the graph and determine whether the model is performing well, ideally for a good fit, we want both training and validation loss to decrease to stability with a minimal gap between the two, which indicates that the model is learning and generalizing well. If the training loss decreases while the validation loss increases, the model may be overfitting the training data and not generalizing well to new data. Finally, if both training and validation loss remain high, the model may be underfitting, which means it’s not learning the underlying patterns in the data well enough. The scale of the loss and the number of training steps must be considered. The model might need more training if the loss is still high or the validation loss has yet to stabilize. For those with an ML model experience or background, the overall approach for splitting between training and validating datasets and interpreting these metrics is very similar.</p> 
  </div> 
  <div class="readable-text intended-text" id="p138"> 
   <p>An interesting behavior is that the data in the loss graph fluctuates, indicating that the loss value can vary depending on the samples in each batch. It is normal for the model to be noisy; however, in fine-tuning, the model learns and improves its performance as long as the loss decreases over time.</p> 
  </div> 
  <div class="readable-text intended-text" id="p139"> 
   <p>To find whether the fine-tuning is good, we would typically look for a low and stable validation loss close to the training loss. The thresholds for what would be considered good loss values are subjective and will vary depending on the task’s complexity and the nature of the data.</p> 
  </div> 
  <div class="readable-text" id="p140"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Mean token accuracy</h4> 
  </div> 
  <div class="readable-text" id="p141"> 
   <p>Mean token accuracy measures how well a fine-tuned model correctly predicts each token in the output sequence that the model generates or predicts during training. It is reflected as a percentage, that is, the percentage of tokens the model predicts correctly in a dataset. For example, if the mean token accuracy is 90%, it means that on average, the model correctly predicts 90% of the tokens. This is an average calculated by dividing the number of correctly predicted tokens by the total number of tokens in the output.</p> 
  </div> 
  <div class="readable-text intended-text" id="p142"> 
   <p>Similar to the loss for mean token accuracy, we have two metrics: one for the training and the other for validation (assuming one has provided a validation dataset). Figure 9.4 shows the mean token accuracy of a fine-tuning job for training and validation.<span class="aframe-location"/> The training mean token accuracy is the average accuracy of the model’s predictions on the training data. It measures how well the model learns from the training data and adapts to it. A high training mean token accuracy suggests that the model learns effectively from the training data. In contrast, the validation mean token accuracy is the average accuracy of the model’s predictions on the validation data. It measures how well the model generalizes to new data it has not seen before. A high validation mean token accuracy suggests that the model does not overfit the training data and can generalize well to new data.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p143">  
   <img alt="figure" src="../Images/CH09_F04_Bahree.png" width="1100" height="767"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.4</span> Training mean token accuracy</h5>
  </div> 
  <div class="readable-text intended-text" id="p144"> 
   <p>The difference between the two metrics can help identify whether the model is overfitting to the training data. Suppose the training mean token accuracy is much higher than the validation mean token accuracy. In that case, it suggests that the model is overfitting to the training data and not generalizing well to new data. In contrast, if the validation mean token accuracy is much lower than the training mean token accuracy, it suggests that the model is underfitting the training data and not learning effectively.</p> 
  </div> 
  <div class="readable-text intended-text" id="p145"> 
   <p>This metric is useful for evaluating the performance of a fine-tuned model on the training data. A good mean token accuracy can be relative and depends on the specific task or application. Generally, a higher value (closer to 1.0) indicates better performance. However, it does not reflect how well the model generalizes to new or unseen data.</p> 
  </div> 
  <div class="readable-text intended-text" id="p146"> 
   <p>Note that the interpretation of these metrics can depend on the specific task or application. Therefore, it’s essential to consider other metrics and qualitative evaluations to get a comprehensive view of the model’s performance. The quality of mean token accuracy depends on the task’s complexity and the nature of text. Higher accuracy (closer to 100%) is expected for simpler tasks or texts with predictable patterns. A lower accuracy might still be good for more complex tasks or diverse texts.</p> 
  </div> 
  <div class="readable-text intended-text" id="p147"> 
   <p>One way to assess whether the mean token accuracy is good is to compare it with a baseline or with the performance of other models on the same task. If your model’s accuracy is higher than the baseline or similar models, it’s a positive sign. </p> 
  </div> 
  <div class="readable-text intended-text" id="p148"> 
   <p>Now that we understand the basic constructs of fine-tuning and using a CLI or code, let’s take a look at how we can achieve this using Azure OpenAI and a GUI. As stated earlier, we will use Azure OpenAI as an example, but the same process applies to OpenAI.</p> 
  </div> 
  <div class="readable-text" id="p149"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_155"><span class="num-string">9.3.5</span> Fine-tuning using Azure OpenAI</h3> 
  </div> 
  <div class="readable-text" id="p150"> 
   <p>Instead of using the SDK and the CLI, we also have a visual interface that we can employ to achieve the same outcome. Often, doing this manually would be a better approach than using code. To kick off a fine-tuning job in Azure OpenAI, when logged into the Azure Portal and in the AI Studio, under models, we choose the option to create a custom model (figure 9.5).</p> 
  </div> 
  <div class="readable-text intended-text" id="p151"> 
   <p>We go through the wizard and choose to upload the training and validation datasets, as shown in figure 9.6. Note: If these have been uploaded using the SDK, we will find them here, as long as they are in the same tenants and have the same end-point deployment.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p152">  
   <img alt="figure" src="../Images/CH09_F05_Bahree.png" width="1100" height="643"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.5</span> Azure AI Studio: Creating a custom model<span class="aframe-location"/></h5>
  </div> 
  <div class="browsable-container figure-container" id="p153">  
   <img alt="figure" src="../Images/CH09_F06_Bahree.png" width="912" height="684"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.6</span> Choosing a training and validation dataset</h5>
  </div> 
  <div class="readable-text" id="p154"> 
   <p>Figure 9.7 shows the status and details of each of our training jobs.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p155">  
   <img alt="figure" src="../Images/CH09_F07_Bahree.png" width="676" height="459"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.7</span> Training job details</h5>
  </div> 
  <div class="readable-text" id="p156"> 
   <p>Now that we have a fine-tuned model, we need to deploy it to a test environment to run an evaluation.</p> 
  </div> 
  <div class="readable-text" id="p157"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_156"><span class="num-string">9.4</span> Deployment of a fine-tuned model</h2> 
  </div> 
  <div class="readable-text" id="p158"> 
   <p>The deployment of a fine-tuned model is quite straightforward. The new fine-tuned model shows up as another model available for use in our Azure tenant or OpenAI subscription, as shown in figures 9.8 and 9.9, respectively.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p159">  
   <img alt="figure" src="../Images/CH09_F08_Bahree.png" width="677" height="538"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.8</span> Deploying fine-tuned model for inference</h5>
  </div> 
  <div class="readable-text" id="p160"> 
   <p>OpenAI has launched a feature in the playground that lets users see how a fine-tuned model differs from the base model side by side, which can be useful visually but not efficiently.<strong><span class="aframe-location"/></strong></p> 
  </div> 
  <div class="browsable-container figure-container" id="p161">  
   <img alt="figure" src="../Images/CH09_F09_Bahree.png" width="412" height="476"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.9</span> OpenAI fine-tuned model deployment</h5>
  </div> 
  <div class="readable-text" id="p162"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_157"><span class="num-string">9.4.1</span> Inference: Fine-tuned model</h3> 
  </div> 
  <div class="readable-text" id="p163"> 
   <p>Returning to our task, we now have a fine-tuned model for EmojiBot, where the bot responds in emojis using the format that Microsoft Teams uses. Figure 9.10 shows how the out-of-the-box GPT-3.5 Turbo model behaves when asked to respond with emojis; this is expected but will not work with Teams. </p> 
  </div> 
  <div class="browsable-container figure-container" id="p164">  
   <img alt="figure" src="../Images/CH09_F10_Bahree.png" width="1014" height="569"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.10</span> Response with emojis using GPT-3.5 Turbo</h5>
  </div> 
  <div class="readable-text" id="p165"> 
   <p>However, the experience for the same questions using our fine-tuned powered EmojiBot is quite different, as shown in figure 9.11. Here, for the same questions as before, we get the response in the format we’ll be able to use in Teams.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p166">  
   <img alt="figure" src="../Images/CH09_F11_Bahree.png" width="925" height="598"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.11</span> Fine-tuned EmojiBot inference<span class="aframe-location"/></h5>
  </div> 
  <div class="readable-text" id="p167"> 
   <p>However, it is easy to get completely incorrect results on the same questions from earlier and with the same parameter settings (figure 9.12). We can see the fine-tuned model answer in emojis—<code>(Pizza)</code> and <code>(Feeling tired)—</code>but the result is not what we expected.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p168">  
   <img alt="figure" src="../Images/CH09_F12_Bahree.png" width="1100" height="550"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.12</span> Fine-tuned EmojiBot with incorrect results</h5>
  </div> 
  <div class="readable-text" id="p169"> 
   <p>To resolve this, we need to tweak the system prompt to steer the model to respond using emojis where possible, which is a great way to close out by reminding that a stacked approach of prompt engineering, RAG, and fine-tuning (where the task at hand warrants) is the right approach in almost all cases.</p> 
  </div> 
  <div class="readable-text intended-text" id="p170"> 
   <p>Now that we have seen how to fine-tune a model and the steps one needs to undertake, let us switch and look at some of the underpinnings of the technology that will make this work. Strictly speaking, we do not require this to do a fine-tuning, but it will help us to understand some of the nuances to achieve better outcomes for fine-tuning. We will start by understanding how we train an LLM and, at a high level, what the steps entail.</p> 
  </div> 
  <div class="readable-text" id="p171"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_158"><span class="num-string">9.5</span> Training an LLM</h2> 
  </div> 
  <div class="readable-text" id="p172"> 
   <p>It is helpful to our understanding of model adaptation and the techniques and their associated limitations to examine what it means and what it takes to do full training for an LLM. At a high level, if we were to do full training and build an LLM from scratch, that training would involve four major stages, as shown in figure 9.13.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p173">  
   <img alt="figure" src="../Images/CH09_F13_Bahree.png" width="1010" height="484"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.13</span> Full end-to-end training of an LLM [5]</h5>
  </div> 
  <div class="readable-text" id="p174"> 
   <p>Let’s go through each stage in more detail.</p> 
  </div> 
  <div class="readable-text" id="p175"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_159"><span class="num-string">9.5.1</span> Pretraining</h3> 
  </div> 
  <div class="readable-text" id="p176"> 
   <p>Base LLMs are built during this initial stage. We touched on base LLMs in chapter 2. These are the original, pretrained models trained on a massive corpus of text data. They can generate text based on the patterns they learned during training. Some also call them raw language models. </p> 
  </div> 
  <div class="readable-text print-book-callout" id="p177"> 
   <p><span class="print-book-callout-head">Note</span>  While powerful, these base models are less suitable for general-purpose applications because they may need to align their responses with the specific intentions or instructions of the user. They are more like raw engines for text generation, lacking the refined capability to understand and adhere to the nuances of user prompts. Base models do not answer questions and often respond with more questions. In contrast, instructors are tailored to be more interactive and user-friendly, which makes them more suitable for a wide range of applications, from customer service chatbots to educational tools, where understanding and following instructions accurately is crucial.</p> 
  </div> 
  <div class="readable-text" id="p178"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_160"><span class="num-string">9.5.2</span> Supervised fine-tuning</h3> 
  </div> 
  <div class="readable-text" id="p179"> 
   <p>Supervised fine tuning (SFT) is the next stage. In this stage, the base model undergoes refining of the base model with high-quality, domain-specific data. These datasets consist of prompt–response pairs, manually created (often by human contractors), which are fewer in number than in the previous stage but of much higher quality. The contractors follow detailed documentation to create these prompt–response pairs, ensuring relevance and quality. Similar to the last pretraining stage, the SFT model is trained to predict the next token in these pairs, but these are less accurate and contextually aware when generating the response.</p> 
  </div> 
  <div class="readable-text intended-text" id="p180"> 
   <p>SFT is a technique for optimizing LLMs on labeled data for a specific downstream task, such as sentiment analysis, text summarization, or machine translation. Later in the chapter, we will cover additional details of SFT methods and approaches.</p> 
  </div> 
  <div class="readable-text" id="p181"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_161"><span class="num-string">9.5.3</span> Reward modeling</h3> 
  </div> 
  <div class="readable-text" id="p182"> 
   <p>The third phase is reward modeling, the first part of the Reinforcement Learning from Human Feedback (RLHF) process. The main goal at this stage is to develop a model that can evaluate and rank responses based on their quality and relevance. To do this, the SFT model (from the previous stage) generates multiple responses to a prompt, which human contractors then rank based on various criteria such as domain expertise, fact-checking, and code execution. These rankings train a reward model, which learns to score responses like human contractors.</p> 
  </div> 
  <div class="readable-text" id="p183"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_162"><span class="num-string">9.5.4</span> Reinforcement learning</h3> 
  </div> 
  <div class="readable-text" id="p184"> 
   <p>This is the second part of the RLHF process, and it aims to enhance the language model’s ability to generate high-quality responses through iterative feedback. In this final stage, the reward model scores responses generated by the SFT model for many prompts. These scores are used to further train the SFT model, ultimately leading to the creation of the RLHF model. The RLHF aligns the LLMs with human preferences or expectations for a given task or domain, such as chat, code, or creative writing. More details on RLHF methods will be covered later in this chapter.</p> 
  </div> 
  <div class="readable-text" id="p185"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_163"><span class="num-string">9.5.5</span> Direct policy optimization</h3> 
  </div> 
  <div class="readable-text" id="p186"> 
   <p>Direct policy optimization (DPO) [6] is another technique, which is a new type of reward model parameterization in RLHF that used for fine-tuning LLMs to align with our preferences. It exploits a relationship between reward functions and optimal policies. It allows us to skip the reward modeling step outlined earlier, as long as the human feedback can be expressed in binary terms—that is, a choice between two options. DPO can solve the reward maximization problem with constraints in a single policy training phase, essentially treating it as a classification problem. PPO (see section 9.7) requires a reward model and a complex RL-based optimization process; DPO, however, bypasses the reward modeling step and directly optimizes the language model on preference data, which can be simpler and more efficient. As DPO eliminates the need to train a reward model instead of training a reward model and optimizing a policy based on that model, we can directly optimize the policy. This characteristic makes this approach quicker, and fewer resources are used than in RLHF with PPO.</p> 
  </div> 
  <div class="readable-text" id="p187"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_164"><span class="num-string">9.6</span> Model adaptation techniques</h2> 
  </div> 
  <div class="readable-text" id="p188"> 
   <p>There are several techniques available for model adaptation, with each technique providing its unique approach and being suitable for different scenarios depending on the specific requirements (i.e., the model size, available computational resources, and the desired level of adaptation). One of the main techniques widely used for adapting LLMs is low adaptation ranking (LoRA), which will be covered in more detail in the next section. In LoRA, instead of updating all the weights in the model, only a small subset of parameters, introduced as low-rank matrixes, are modified. This approach allows efficient training and adaptation, while preserving most of the pretrained model’s structure and knowledge.</p> 
  </div> 
  <div class="readable-text intended-text" id="p189"> 
   <p>Parameter efficient fine-tuning (PEFT) is a concept in ML that refers to methods of adapting and fine-tuning large pretrained models, such as GPT-3.5, to minimize the number of parameters that need to be updated. This approach is particularly valuable when dealing with large models, as it reduces computational requirements and can mitigate problems such as overfitting. PEFT techniques are designed to make fine-tuning more accessible and efficient, especially for users with limited computational resources—LoRA is an example of the PEFT method. For more details on different types of PEFT techniques and details, see the paper “Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning” by Vladislav Lialin [7].</p> 
  </div> 
  <div class="readable-text intended-text" id="p190"> 
   <p>Catastrophic forgetting is a phenomenon where a model loses its ability to perform well on previous tasks after being fine-tuned on new tasks [8]. This can happen when the model overwrites its original parameters with task-specific ones, thus forgetting the general knowledge it learned from pretraining. When implementing PEFT to prevent catastrophic forgetting, we fine-tune only a small subset of parameters, while keeping most pretrained parameters fixed. This way, the model can retain its generalization ability and adapt to new tasks without losing its previous performance.</p> 
  </div> 
  <div class="readable-text intended-text" id="p191"> 
   <p>Supervised fine-tuning (SFT) is another type of adaptation technique; it is a specific type of fine-tuning where the model is further trained on a labeled dataset. It’s supervised because the training process uses a dataset that pairs the input data with the correct output (labels). SFT is particularly common in tasks such as classification, where the model must learn to associate specific inputs with labeled outputs. SFT is a subset of the broader fine-tuning process, specifically tailored to situations where labeled data is available.</p> 
  </div> 
  <div class="readable-text intended-text" id="p192"> 
   <p>Of course, each technique has its unique approach and is suitable for different scenarios depending on the specific requirements, such as the model size, available computational resources, and the desired level of adaptation. Table 9.2 outlines some notable ones in addition to LoRA.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p193"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 9.2</span> Model adaptation techniques</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Technique 
       </div></th> 
      <th> 
       <div>
         Description 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  Prompt tuning <br/></td> 
      <td>  Prompt tuning is a technique for adapting LLMS to different tasks by providing specific cues or prompts that guide their generation or prediction. It does not require retraining the model or updating its weights, which makes it faster and cheaper than fine-tuning. It is particularly useful for tasks where only a small amount of adaptation is required. <br/></td> 
     </tr> 
     <tr> 
      <td>  Adapter modules <br/></td> 
      <td>  Adapter models are used in LLM fine-tuning to add small and task-specific modules (small neural networks) to the pretrained model and train only these modules on the task-specific data. They are also flexible and modular, as they can be easily added or removed for different tasks without affecting the pretrained model. <br/></td> 
     </tr> 
     <tr> 
      <td>  Bias-only (BitFit) <br/></td> 
      <td>  Bias-only (BitFit) is a technique for fine-tuning LLMs by modifying only the bias terms of the model or a subset of them. It offers a minimalistic approach to adaptation, requiring even fewer trainable parameters than LoRA. BitFit is based on fine-tuning, mainly exposing the knowledge learned by pretraining rather than acquiring new task-specific knowledge. <br/></td> 
     </tr> 
     <tr> 
      <td>  Layer freezing <br/></td> 
      <td>  Layer freezing is a fine-tuning technique that keeps some of the model layers fixed and only updates the rest. This method allows for more control over which aspects of the model are adapted and can reduce training time. <br/></td> 
     </tr> 
     <tr> 
      <td>  Knowledge distillation <br/></td> 
      <td>  Involves training a smaller, more efficient model (student) to mimic the behavior of a larger pretrained model (teacher). This method is useful for deploying LLMs in resource-constrained environments. <br/></td> 
     </tr> 
     <tr> 
      <td>  Meta-learning <br/></td> 
      <td>  Focuses on training the model to learn new tasks quickly with minimal data; often involves training on various tasks so the model can more efficiently adapt to new, unseen tasks <br/></td> 
     </tr> 
     <tr> 
      <td>  Differential privacy fine-tuning <br/></td> 
      <td>  Incorporates privacy-preserving techniques during fine-tuning to protect sensitive data. This is essential for applications where data privacy and security are paramount. <br/></td> 
     </tr> 
     <tr> 
      <td>  Reinforcement learning from human feedback (RLHF) <br/></td> 
      <td>  Involves fine-tuning models based on feedback or rewards derived from human interactions or evaluations. It is useful for tasks where human judgment is crucial, such as content moderation. <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p194"> 
   <p>Each technique involves tradeoffs between the computational resources required, the level of specialization achieved, and the retention of the model’s original capabilities. The choice of technique depends on the specific application, the constraints of the deployment environment, and the goals of the model adaptation.</p> 
  </div> 
  <div class="readable-text intended-text" id="p195"> 
   <p>Now that you are more familiar with various techniques, let’s explore LoRA, the main technique for fine-tuning large models such as GPT.</p> 
  </div> 
  <div class="readable-text" id="p196"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_165"><span class="num-string">9.6.1</span> Low-rank adaptation</h3> 
  </div> 
  <div class="readable-text" id="p197"> 
   <p>LoRA, which stands for low-rank adaptation [9], is a method specifically designed for adapting LLMs. It presents an efficient alternative to traditional fine-tuning methods, which is particularly useful in scenarios where fine-tuning large models can be resource-intensive and challenging.</p> 
  </div> 
  <div class="readable-text intended-text" id="p198"> 
   <p>LoRA is based on making minimal but strategic modifications to a pretrained model without altering its entire architecture. It achieves this by introducing the notion of low-rank matrices. Instead of modifying the entire weight matrices of a neural network, LoRA inserts small, low-rank matrices into the model. These matrices are applied to the model’s layers (attention and feed-forward) during forward and backward passes, as shown in figure 9.14. As we have seen, LLMs are built on deep learning architectures consisting of multiple layers designed to process and understand human language. In addition, LoRA also retrains selectively only these low-rank matrices, while the original pretrained weights remain frozen. This selective retraining significantly reduces the computational resources needed.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p199">  
   <img alt="figure" src="../Images/CH09_F14_Bahree.png" width="305" height="266"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.14</span> LoRA reparameterization—only A and B are trained [9]</h5>
  </div> 
  <div class="readable-text intended-text" id="p200"> 
   <p>In figure 9.14, input (<strong>X</strong>) at the bottom of the diagram represents the input data fed to the layer with pretrained weights. <em>A</em> and <em>B</em> are the adaptation parameters that will be updated during fine-tuning, and <em>W</em> is the original pretrained weight that remains frozen.</p> 
  </div> 
  <div class="readable-text intended-text" id="p201"> 
   <p>When we want to fine-tune a task, we can store and load only a few task-specific parameters along with the pretrained model. This approach helps improve the efficiency during runtime for various downstream adaptations. It gives LoRA several advantages, including</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p202"> <em>Resource efficiency</em><em> </em>—LoRA requires far less computational power than traditional full-model fine-tuning, making it more accessible for adapting large models. </li> 
   <li class="readable-text" id="p203"> <em>Preservation of generalization</em><em> </em>—LoRA maintains the base LLM’s generalization abilities by not altering the original pretrained weights, while allowing specialization. </li> 
   <li class="readable-text" id="p204"> <em>Faster adaptation</em><em> </em>—The process is quicker due to fewer updated parameters, enabling rapid deployment of adapted models. </li> 
   <li class="readable-text" id="p205"> <em>Scalability</em><em> </em>—LoRA is particularly effective for large models, where full-model fine-tuning may be impractical due to resource constraints. </li> 
  </ul> 
  <div class="readable-text" id="p206"> 
   <p>LoRA is a cost-effective and efficient method for language models that allows fast switching between tasks. QLoRA is a variant of LoRA that further reduces the number of parameters by quantizing the low-rank matrices and can achieve up to 99% parameter reduction (via implementing an 8-bit optimizer for quantization), while maintaining or improving the model’s performance.</p> 
  </div> 
  <div class="readable-text" id="p207"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Quantization</h4> 
  </div> 
  <div class="readable-text" id="p208"> 
   <p>Quantization is another technique that reduces the memory and computation requirements of the model by representing the parameters with fewer bits. Quantization of a model means reducing the precision of the model’s parameters, such as weights and biases, from high-precision floating-point numbers (32 bit or 16 bit) to low-precision numbers (8 bit or 4 bit). This can reduce the model size and speed up the inference but may also affect the model accuracy.</p> 
  </div> 
  <div class="readable-text intended-text" id="p209"> 
   <p>Quantization is especially useful for LLMs, which can have billions of parameters and require a lot of memory and computation. By quantizing the model, the deployment and inference of the model can be more efficient and scalable. For example, DistilBERT is a quantized version of BERT, an LLM for NLP. It has 40% fewer parameters than BERT but retains 97% of BERT’s performance.</p> 
  </div> 
  <div class="readable-text intended-text" id="p210"> 
   <p>At face value, quantization is similar to LoRA, as both aim to improve the efficiency and scalability of LLMs. Still, they are very different in their approaches and tradeoffs:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p211"> LoRA reduces the number of trainable parameters by freezing the pretrained model weights and injecting low-rank matrices into each layer. This allows for faster fine-tuning and adaptation to new tasks. LoRA also preserves the full precision of the model weights, which means it does not reduce the model’s memory footprint or inference latency. </li> 
   <li class="readable-text" id="p212"> Quantization reduces the model’s memory and computation requirements by representing the parameters with fewer bits, such as INT4. This characteristic allows for smaller model sizes and faster inference, but it also introduces quantization errors and noise, which can degrade the model performance. Quantization also requires careful calibration and optimization to minimize the loss of accuracy and robustness. </li> 
   <li class="readable-text" id="p213"> Quantized LoRA (QLoRA) is another technique that aims to improve the parameter efficiency of fine-tuned LLMs. It extends LoRA by adding quantization to the process. This means that the LoRA adapters’ weights are quantized to a lower precision, such as 4 bit, which greatly shrinks the memory size of the model. The main benefit of QLoRA is its ability to balance performance and memory efficiency, making it a suitable option for scarce resources. Despite the decreased precision, QLoRA has been proven to keep a similar level of effectiveness to its nonquantized version, LoRA, in different tasks. This makes QLoRA an attractive choice for those who want to use powerful language models without the high computational costs. </li> 
  </ul> 
  <div class="readable-text" id="p214"> 
   <p>The main tradeoffs between LoRA and QLoRA are related to the balance of performance, memory efficiency, and computational resources. LoRA achieves a good balance between performance and efficiency, while QLoRA maximizes memory savings, which can be crucial for some use cases. The choice between the two would depend on the task’s specific needs and the deployment environment’s limitations. Experimentation is important to determine which method best fits your needs. If high accuracy is very important and computational resources are sufficient, LoRA might be the best choice. If memory efficiency is more important, then QLoRA would be better, especially if a small decrease in performance is acceptable for the application.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p215"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Teaching new knowledge using fine-tuning</h5> 
   </div> 
   <div class="readable-text" id="p216"> 
    <p>Often, there is a misconception that fine-tuning teaches the model new knowledge (or information). This is not correct. SFT does not exactly teach new knowledge to a model in the traditional sense. A fine-tuned model’s knowledge is limited to what was present in its pretraining data until its last update. It does not acquire new external knowledge during the fine-tuning process. Instead, it refines and adapts the model’s existing knowledge and capabilities to perform better on specific tasks or in certain domains. All SFT is doing is refining existing knowledge as outlined here:</p> 
   </div> 
   <ul> 
    <li class="readable-text" id="p217"> <em>Using pretrained knowledge</em>—In SFT, the model has already been trained on a large, diverse dataset. This initial training provides a broad base of general knowledge and language understanding. </li> 
    <li class="readable-text" id="p218"> <em>Focusing on specific tasks</em>—During SFT, the model’s preexisting knowledge is honed to be more effective for specific tasks. For instance, if you fine-tune a language model on medical texts, the model becomes more adept at understanding and generating language related to medicine. Still, it does not necessarily teach new facts about medicine. </li> 
    <li class="readable-text" id="p219"> <em>Adjusting weights for relevance</em>—Fine-tuning effectively adjusts the model’s internal parameters (or weights) to make certain features or patterns more prominent when making predictions or generating text. This process makes the model more sensitive to the nuances of the specific data it was fine-tuned on. </li> 
   </ul> 
   <div class="readable-text" id="p220"> 
    <p>SFT tailors a model’s existing knowledge and capabilities to be more effective for specific applications rather than teaching it new, external knowledge. The process involves adjusting the model’s internal understanding and response generation mechanisms to better align with the characteristics of the fine-tuning data.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p221"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_166"><span class="num-string">9.7</span> RLHF overview</h2> 
  </div> 
  <div class="readable-text" id="p222"> 
   <p>Reinforcement learning from human feedback (RLHF) is a sophisticated ML approach that combines reinforcement learning (RL) and supervised learning. Unlike traditional RL, which relies on predefined reward functions, RLHF integrates human judgment into learning by asking humans to evaluate the agent’s behavior and provide feedback, such as ratings, preferences, or suggestions. This feedback helps the agent to improve its performance and align with human values or preferences. Specifically, it increases helpfulness and truthfulness in the generation, while mitigating harm and bias. OpenAI’s Instruct models, now the default models, are examples of models powered by RLHF and deployed at scale [10]. Anthropic, another AI startup founded by former OpenAI employees, aims to build LLMS such as Claude that are reliable, interpretable, and steerable. They have published their approach to RLHF [11], including associated human preference data [12].</p> 
  </div> 
  <div class="readable-text intended-text" id="p223"> 
   <p> RLHF is particularly valuable when defining an explicit reward function is challenging or where human preferences, subjective judgments, and nuances are crucial. It’s used in NLP tasks such as conversation and content generation, where subjective quality matters. RLHF aids in content moderation on social media by understanding context-specific nuances. Personalized recommendation systems help tailor suggestions to individual tastes. It’s valuable for socially acceptable and comfortable robotics and human–computer interaction behaviors. Ethical decision-making guides AI in aligning with human values. RLHF enhances AI’s performance in complex games, creative endeavors such as art and music, and healthcare for personalized medical decisions. These applications highlight RLHF’s role in aligning AI with the complex and subjective nature of human preferences and judgments.</p> 
  </div> 
  <div class="readable-text intended-text" id="p224"> 
   <p>RLHF can improve LLMs’ performance, alignment, and diversity on various tasks, such as text generation, summarization, or dialogue. Instruct models are base LLMs that have been fine-tuned using the RLHF approach, and as shown in figure 9.15, they significantly outperform the base LLMs [13]. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p225">  
   <img alt="figure" src="../Images/CH09_F15_Bahree.png" width="1100" height="631"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.15</span> RLHF-trained models (PPO and PPO-ptx) significantly outperform base LLM models.</h5>
  </div> 
  <div class="readable-text print-book-callout" id="p226"> 
   <p><span class="print-book-callout-head">Note</span>  Base models are original pretrained models that have not been aligned with specific values and are not generally suitable for production use. For a reminder on the categories of LLM, see section 2.4 in chapter 2.</p> 
  </div> 
  <div class="readable-text" id="p227"> 
   <p>The RLHF framework teaches a model to perform tasks as humans would want, using human feedback as a guide. This method is especially relevant in fields where the desired output is subjective or highly context dependent, such as NLP, content generation, and decision-making systems. The key phases that comprise RLHF are outlined in figure 9.16.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p228">  
   <img alt="figure" src="../Images/CH09_F16_Bahree.png" width="1100" height="671"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.16</span> RLHF fine-tuning approaches to aligning language models [13]</h5>
  </div> 
  <div class="readable-text" id="p229"> 
   <p>These components work in tandem to create a robust learning system where a model can learn complex, human-centric tasks beyond the capabilities of traditional ML approaches. Integrating human feedback is key to bridging the gap between algorithmic decision-making and human judgment. Let’s look at the RLHF phases in more detail:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p230"> <em>Supervised fine-tuning</em><em> </em>—This phase involves training the model on a dataset of human-generated examples. These examples demonstrate the desired outcomes or behaviors, providing a baseline for the model to learn from. It helps the model understand the context and nuances of tasks as humans interpret. </li> 
   <li class="readable-text" id="p231"> <em>Reward modeling</em><em> </em>—In this step, a separate model, known as the reward model, is trained to predict the quality of outputs generated by the primary model. The reward model is trained using human judgments, often involving ratings or preferences between different outputs. This model effectively translates subjective human evaluations into quantitative feedback that the primary model can use for learning. </li> 
   <li class="readable-text" id="p232"> <em>Proximal policy optimization (PPO)</em><em> </em>—PPO is an RL algorithm that iteratively improves the primary model’s policy (decision-making process). The algorithm updates the model’s policy to maximize the rewards predicted by the reward model. PPO is chosen for its stability and efficiency in handling large and complex models. </li> 
   <li class="readable-text" id="p233"> <em>Human feedback loop</em><em> </em>—This loop involves continuous input from human evaluators who assess the quality of the model’s outputs. The feedback is used to train the reward model further, creating a dynamic learning environment where the model adapts to evolving human preferences and standards. The loop ensures that the model remains aligned with human expectations and can adapt to changes. </li> 
  </ul> 
  <div class="readable-text print-book-callout" id="p234"> 
   <p><span class="print-book-callout-head">Note</span>  PPO-ptx [13] is an adaptation of the PPO algorithm tailored for fine-tuning RLHF. It integrates a reference to the original LLM to maintain performance, while aligning the model’s outputs with human preferences. This approach helps mitigate the alignment tax, ensuring the LLM remains effective and diverse in its outputs after training. Essentially, PPO-ptx balances the model’s pretraining knowledge with the new feedback to create a high-performing LLM aligned with human values.</p> 
  </div> 
  <div class="readable-text" id="p235"> 
   <p>RLHF might seem like the silver bullet in many ways, but enterprises must be aware of some challenges. Let’s explore these.</p> 
  </div> 
  <div class="readable-text" id="p236"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_167"><span class="num-string">9.7.1</span> Challenges with RLHF</h3> 
  </div> 
  <div class="readable-text" id="p237"> 
   <p>RLHF is a powerful technique for teaching models complex tasks, but it has many practical challenges and limitations for enterprises. An RLHF system needs a lot of human preference data, which is hard to get because it involves other people who are not part of the training process. How well RLHF works depends on how good the human annotations are, which humans can write, such as when they adjust the initial LLM in InstructGPT or provide ratings of how much they like different outputs from the model. Some of these challenges are</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p238"> <em>Technical complexity</em><em> </em>—Implementing RLHF requires advanced skills and knowledge in ML, RL, and NLP. It also involves complex setup and maintenance processes, such as configuring the model architecture, reward systems, and feedback mechanisms. </li> 
   <li class="readable-text" id="p239"> <em>Computationally intensive</em><em> </em>—RLHF models need a lot of computational resources, such as GPUs and servers, which can be expensive. They also depend on the quality and quantity of human feedback, which can be hard to obtain and process. From a practical viewpoint, a lot of the human feedback is from contract workers (or gig workers) on crowdsourcing platforms where getting the right qualified people in certain domains might be challenging. Moreover, ensuring a diverse and unbiased dataset for training can be challenging and computationally heavy. </li> 
   <li class="readable-text" id="p240"> <em>Not scalable</em><em> </em>—RLHF models are difficult to scale for large-scale applications, requiring continuous human feedback and increasing computational resources. They are also hard to adapt to different domains or changing data environments, resulting in limited adaptability and customization. </li> 
   <li class="readable-text" id="p241"> <em>Quality</em><em> </em>—RLHF models are prone to bias, as they reflect human feedback providers’ subjective opinions and potential prejudices. Ensuring ethical use and unbiased outputs is a major concern. Maintaining a consistent quality of human feedback can be difficult, as human judgment can vary and affect the model’s reliability and performance. When trying to build a helpful model that avoids harm, there is an inherent tension between those two dimensions. Providing too many polite responses, such as “Sorry, I am an AI model, and I cannot help you with that,” or something similar, limits the model’s usefulness. Organizations must balance and mitigate this using additional guidance, training, and other ML techniques to create synthetic data where possible. </li> 
   <li class="readable-text" id="p242"> <em>Cost</em><em> </em>—RLHF models are costly to implement and operate. Costs include infrastructure, computational resources, data acquisition, and hiring skilled professionals. There are also ongoing operational costs related to data management, model updates, and continuous feedback integration. These costs can be substantial, especially for large-scale implementations. </li> 
   <li class="readable-text" id="p243"> <em>Data</em><em> </em>—It is hard to produce good human text that answers specific prompts because it usually means paying part-time workers (instead of product users or crowdsourcing). Luckily, the amount of data needed to train the reward model for most uses of RLHF (~50k preference labels) is not that costly. However, it is still more than what academic labs can usually afford. There is only one big dataset for RLHF on a general language model (from Anthropic) and a few smaller datasets for specific tasks (such as summarization data from OpenAI). Another problem with data for RLHF is that human annotators can disagree a lot, which makes the training data very noisy without a true answer. </li> 
  </ul> 
  <div class="readable-text" id="p244"> 
   <p>RLHF offers advanced capabilities in teaching models to perform complex tasks; however, its adoption in enterprise settings is hindered by technical complexity, resource demands, scalability challenges, ethical considerations, and high costs.</p> 
  </div> 
  <div class="readable-text intended-text" id="p245"> 
   <p>On the one hand, these barriers make it difficult for many organizations to implement and sustain RLHF systems in their operations practically. On the other hand, those who can implement this, especially some of the technical companies such as OpenAI and Anthropic, can benefit from it. Let’s see how we can scale an RLHF implementation.</p> 
  </div> 
  <div class="readable-text" id="p246"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_168"><span class="num-string">9.7.2</span> Scaling an RLHF implementation</h3> 
  </div> 
  <div class="readable-text" id="p247"> 
   <p>Scaling an RLHF implementation for LLMs involves a multifaceted approach that balances efficiency, diversity, and quality control. First, automating data collection and implementing efficient feedback mechanisms are crucial for handling large volumes of data and feedback. Automated systems can gather data from various sources or through interfaces designed for efficient human interaction. </p> 
  </div> 
  <div class="readable-text intended-text" id="p248"> 
   <p>Using a large, diverse pool of human evaluators is essential for capturing a wide range of perspectives, helping the model to be more robust and less biased. To ensure the feedback is informative, intelligent sampling strategies, such as active learning, can be used to identify and prioritize the most valuable instances for evaluation.</p> 
  </div> 
  <div class="readable-text intended-text" id="p249"> 
   <p>Parallelization and distribution of tasks among multiple evaluators can significantly speed up the feedback process. The system can handle large-scale data processing and model training with scalable infrastructure.</p> 
  </div> 
  <div class="readable-text intended-text" id="p250"> 
   <p>Implement quality control measures, such as cross-validation among evaluators and algorithms, to detect biases and maintain the quality and consistency of feedback. Regular monitoring and evaluation of the model’s performance can help you understand the effects of RLHF and guide continuous improvement.</p> 
  </div> 
  <div class="readable-text intended-text" id="p251"> 
   <p>Finally, ethical considerations and bias mitigation are crucial. Ensuring that feedback does not reinforce harmful stereotypes and actively addressing potential biases is vital for developing fair and responsible models. Overall, scaling RLHF for LLMs requires a comprehensive approach that integrates technical, logistical, and ethical strategies, aiming for a system that effectively incorporates human feedback into the model’s learning process.</p> 
  </div> 
  <div class="readable-text" id="p252"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_169">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p253"> Model adaptation should be anchored in a set of use cases, and it should be the last resort for enterprises trying to improve the model on those tasks. </li> 
   <li class="readable-text" id="p254"> Prompt engineering and RAG must work in conjunction with fine-tuning in a stacked manner. </li> 
   <li class="readable-text" id="p255"> When done correctly, fine-tuning has a high upside from enhanced efficiency and possible cost savings. </li> 
   <li class="readable-text" id="p256"> Fine-tuning has a high cost, and you should be aware of challenges such as the need for task-specific data, computational resources, performance evaluation, and ethical considerations. </li> 
   <li class="readable-text" id="p257"> Fine-tuning should be done in conjunction with evaluations and will often require multiple iterations to obtain a model ready for production deployment. </li> 
   <li class="readable-text" id="p258"> The choice of metrics for evaluating fine-tuned models largely depends on the model’s specific application and objectives. </li> 
   <li class="readable-text" id="p259"> The main model adaptation techniques that are more cost-efficient are supervised fine-tuning (SFT), parameter efficient fine-tuning (PEFT), and low-rank adaptation (LoRA). </li> 
  </ul>
 </div></div></body></html>