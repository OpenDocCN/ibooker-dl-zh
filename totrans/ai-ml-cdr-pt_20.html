<html><head></head><body><section data-pdf-bookmark="Chapter 19. Using Generative Models with Hugging Face Diffusers" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch19_using_generative_models_with_hugging_face_diffuser_1748573005765373">&#13;
      <h1><span class="label">Chapter 19. </span>Using Generative Models with <span class="keep-together">Hugging Face Diffusers</span></h1>&#13;
      <p>Over the last few chapters, we have<a contenteditable="false" data-primary="generative AI" data-secondary="diffusers library from Hugging Face" data-tertiary="about diffusion" data-type="indexterm" id="id1918"/><a contenteditable="false" data-primary="diffusers library (Hugging Face)" data-secondary="about diffusion" data-type="indexterm" id="id1919"/><a contenteditable="false" data-primary="image generator" data-secondary="diffusers library from Hugging Face" data-tertiary="about diffusion" data-type="indexterm" id="id1920"/><a contenteditable="false" data-primary="generative AI" data-secondary="stable diffusion models" data-tertiary="about diffusion" data-type="indexterm" id="id1921"/> been looking at inference on generative models and primarily using LLMs (aka text-to-text models) to explore different scenarios. However, generative AI isn’t limited just to text-based models, and another important innovation is, of course, image generation (aka text-to-image). Most image generation models today are based on a process called <em>diffusion</em>, which inspires the name <em>diffusers</em> for the Hugging Face APIs used to create images from text prompts. In this chapter, we’ll explore how diffusion models work and how to get up and running with your own apps that can generate images from prompts.</p>&#13;
      <section data-pdf-bookmark="What Are Diffusion Models?" data-type="sect1"><div class="sect1" id="ch19_what_are_diffusion_models_1748573005765584">&#13;
        <h1>What Are Diffusion Models?</h1>&#13;
        <p>By now, most of us have seen<a contenteditable="false" data-primary="diffusers library (Hugging Face)" data-secondary="diffusion models explained" data-type="indexterm" id="ch19difex"/><a contenteditable="false" data-primary="generative AI" data-secondary="diffusers library from Hugging Face" data-tertiary="diffusion models explained" data-type="indexterm" id="ch19difex2"/><a contenteditable="false" data-primary="Hugging Face Hub" data-secondary="diffusers library" data-tertiary="diffusion models explained" data-type="indexterm" id="ch19difex3"/><a contenteditable="false" data-primary="image generator" data-secondary="diffusers library from Hugging Face" data-tertiary="diffusion models explained" data-type="indexterm" id="ch19difex4"/><a contenteditable="false" data-primary="generative AI" data-secondary="stable diffusion models" data-tertiary="diffusion models explained" data-type="indexterm" id="ch19difex5"/> images that are AI created, and we’ve likely been amazed at how quickly they have grown from abstract, rough representations to near photoreal representations of what we asked for via a prompt. Because the models allow for longer prompts, with more detail, and as their training sets have grown, we’ve seen a near endless stream of improvements to what can be done with AI image generation.</p>&#13;
        <p>But how does all of this work? It starts with the idea of diffusion. </p>&#13;
        <p>You can start this process by creating a dataset of images and their associated noise. Consider <a data-type="xref" href="#ch19_figure_1_1748573005759131">Figure 19-1</a>.</p>&#13;
        <figure><div class="figure" id="ch19_figure_1_1748573005759131">&#13;
          <img src="assets/aiml_1901.png"/>&#13;
          <h6><span class="label">Figure 19-1. </span>Noising an image</h6>&#13;
        </div></figure>&#13;
        <p>Then, once you have a set of images you’ve made noisy like this, you can train a model that learns how to denoise to get the image back to its original state. Consider the noise to be the data and the original image to be the labels. So, in the case of <a data-type="xref" href="#ch19_figure_1_1748573005759131">Figure 19-1</a>, the noise on the right can be the data and the image of the puppy can be the label. At that point, you can train a model that, when it sees noise, can figure out how to turn that noise into an image. The logical extension is that you can then <em>generate</em> noise, and the model will figure out how to turn that noise into an image that will look a little bit like one of those in your training set.</p>&#13;
        <p>But, what if you go back to the step<a contenteditable="false" data-primary="diffusers library (Hugging Face)" data-secondary="stable diffusion models" data-type="indexterm" id="ch19stab"/><a contenteditable="false" data-primary="generative AI" data-secondary="stable diffusion models" data-type="indexterm" id="ch19stab2"/><a contenteditable="false" data-primary="Hugging Face Hub" data-secondary="diffusers library" data-tertiary="stable diffusion models" data-type="indexterm" id="ch19stab3"/><a contenteditable="false" data-primary="stable diffusion models" data-type="indexterm" id="ch19stab4"/><a contenteditable="false" data-primary="image generator" data-secondary="stable diffusion models" data-type="indexterm" id="ch19stab5"/> of creating the noisy image and add text to it with a very verbose description? Then, your noisy image will have a text label (represented in embeddings) attached to it (see <a data-type="xref" href="#fig-19-2">Figure 19-2</a>)!</p>&#13;
        &#13;
<figure><div class="figure" id="fig-19-2">&#13;
<img alt="" src="assets/aiml_1902.png"/>&#13;
<h6><span class="label">Figure 19-2. </span>Adding text encodings to the diffusion process</h6>&#13;
</div></figure>&#13;
&#13;
&#13;
        <p>Now, the noisy image has the embeddings describing it attached to it. In simple terms, the piece of noise is enhanced by embeddings that describe it, so the process of denoising this image back into the original image of the puppy has the extra data to guide it in how it denoises. So, again, if you train a model with the noise plus embeddings as the data and the original images as the labels, then a model can now learn more effectively how to turn noise plus embeddings into a picture.</p>&#13;
        <p>You probably see where this is going. Once that model is trained, then, in the future, if someone gives it a piece of text in a prompt, the text can be encoded into embeddings, a set of random noise can be generated, and the model can try to figure out how to take that random noise and denoise it, guided by the text, into an image. For all intents and purposes, it will create a whole new image as a result (see <a data-type="xref" href="#fig-19-3">Figure 19-3</a>). </p>&#13;
        &#13;
<figure><div class="figure" id="fig-19-3">&#13;
<img alt="" src="assets/aiml_1903.png"/>&#13;
<h6><span class="label">Figure 19-3. </span>Beginning the process of denoising an image</h6>&#13;
</div></figure>&#13;
&#13;
        <p>Here, we can start with purely random noise and a prompt. The prompt is something that likely wasn’t in the training set—there are no known images (other than AI-generated ones, of course) of teddy bears eating pizza on the surface of Mars. </p>&#13;
        <p>So a model can then denoise this over multiple steps. As you can imagine, the very first step will be random noise, the second step will be where the model tries to get the noise to match the prompt, the third step will get it a little closer, and so on.</p>&#13;
        <p>This is depicted in <a data-type="xref" href="#ch19_figure_2_1748573005759167">Figure 19-4</a>, where you can see what the image looks like with the popular <em>stable diffusion</em> models.</p>&#13;
        <figure><div class="figure" id="ch19_figure_2_1748573005759167">&#13;
          <img src="assets/aiml_1904.png"/>&#13;
          <h6><span class="label">Figure 19-4. </span>Gradually denoising an image based on a prompt</h6>&#13;
        </div></figure>&#13;
        <p>In this case, I used a diffusion model with the prompt from <a data-type="xref" href="#fig-19-3">Figure 19-3</a> about teddy bears eating pizza. You’ll see the code for this a little later in this chapter.</p>&#13;
        <p>In Step 0, you can see that we just have pure noise. In Step 1, the model has already started taking some of the stronger characteristics of the prompt—the surface of Mars—and given the image a very red hue. By Step 10, we have teddy bears and pizza, and by Step 40, the teddy bears are actually eating the pizza and the lighting has changed—presumably for dinnertime!</p>&#13;
        <p>The <em>size </em>of the image depends on the model. Many earlier models, or those designed to run on consumer hardware, will generate smaller images that they will then upscale to give the desired output. The images I have shown here were created with Stable Diffusion 3.5, which creates 1024 × 1024 images by default.<a contenteditable="false" data-primary="" data-startref="ch19stab" data-type="indexterm" id="id1922"/><a contenteditable="false" data-primary="" data-startref="ch19stab2" data-type="indexterm" id="id1923"/><a contenteditable="false" data-primary="" data-startref="ch19stab3" data-type="indexterm" id="id1924"/><a contenteditable="false" data-primary="" data-startref="ch19stab4" data-type="indexterm" id="id1925"/><a contenteditable="false" data-primary="" data-startref="ch19stab5" data-type="indexterm" id="id1926"/></p>&#13;
        <div data-type="note" epub:type="note"><h6>Note</h6>&#13;
          <p>While this chapter will focus on diffusion models, <a contenteditable="false" data-primary="generative AI" data-secondary="autoregressive image generating models" data-type="indexterm" id="id1927"/><a contenteditable="false" data-primary="autoregressive image generating models" data-type="indexterm" id="id1928"/><a contenteditable="false" data-primary="image generator" data-secondary="autoregressive models" data-type="indexterm" id="id1929"/>using them isn’t the <em>only</em> way to generate images. There are also <em>autoregressive models</em>, which learn the mappings between the tokens for the text in the description of the image and the tokens that represent the visual contents of the image. With lots of examples of these mappings, you can train a model on them. Then, you can give the model a piece of text, and it will be able to predict the tokens for that text and reassemble them into an image.<a contenteditable="false" data-primary="" data-startref="ch19difex" data-type="indexterm" id="id1930"/><a contenteditable="false" data-primary="" data-startref="ch19difex2" data-type="indexterm" id="id1931"/><a contenteditable="false" data-primary="" data-startref="ch19difex3" data-type="indexterm" id="id1932"/><a contenteditable="false" data-primary="" data-startref="ch19difex4" data-type="indexterm" id="id1933"/><a contenteditable="false" data-primary="" data-startref="ch19difex5" data-type="indexterm" id="id1934"/> </p>&#13;
        </div>&#13;
      </div></section>&#13;
      <section data-pdf-bookmark="Using Hugging Face Diffusers" data-type="sect1"><div class="sect1" id="ch19_using_hugging_face_diffusers_1748573005765652">&#13;
        <h1>Using Hugging Face Diffusers</h1>&#13;
        <p>Just as Hugging Face offers a transformers library (as we explained in <a data-type="xref" href="ch15.html#ch15_transformers_and_transformers_1748549808974580">Chapter 15</a>), it <a contenteditable="false" data-primary="diffusers library (Hugging Face)" data-secondary="using Hugging Face diffusers" data-type="indexterm" id="ch19use"/><a contenteditable="false" data-primary="generative AI" data-secondary="diffusers library from Hugging Face" data-tertiary="using Hugging Face diffusers" data-type="indexterm" id="ch19use2"/><a contenteditable="false" data-primary="Hugging Face Hub" data-secondary="diffusers library" data-tertiary="using Hugging Face diffusers" data-type="indexterm" id="ch19use3"/><a contenteditable="false" data-primary="image generator" data-secondary="using Hugging Face diffusers" data-type="indexterm" id="ch19use4"/>also offers a diffusers library to make it easier for you to use diffusion models. Diffusers abstract the complexities of using various models into an easy-to-use API. </p>&#13;
        <p>To get started with diffusers, you simply install them like this:<a contenteditable="false" data-primary="diffusers library (Hugging Face)" data-secondary="installing" data-type="indexterm" id="id1935"/><a contenteditable="false" data-primary="generative AI" data-secondary="diffusers library from Hugging Face" data-tertiary="installing" data-type="indexterm" id="id1936"/><a contenteditable="false" data-primary="Hugging Face Hub" data-secondary="diffusers library" data-tertiary="installing" data-type="indexterm" id="id1937"/><a contenteditable="false" data-primary="image generator" data-secondary="diffusers library from Hugging Face" data-tertiary="installing" data-type="indexterm" id="id1938"/></p>&#13;
        <pre data-code-language="python" data-type="programlisting"><code class="n">pip</code> <code class="n">install</code> <code class="o">-</code><code class="n">U</code> <code class="n">diffusers</code></pre>&#13;
        <p>The diffusers library manages the pipelining of model inference in the same way we experienced in earlier chapters with transformers. There are many steps involved in getting a model to render an image based on a prompt: encoding the prompt, making embeddings, passing the embeddings to the model along with any hyperparameters it needs, grabbing the output tensors, and turning them into an image. But diffusers encapsulate this for you into a pipeline, and there are a number of open source pipelines for many different models. </p>&#13;
        <p>So, for example, in the<a contenteditable="false" data-primary="stable diffusion models" data-secondary="Stable Diffusion 3.5" data-type="indexterm" id="id1939"/><a contenteditable="false" data-primary="diffusers library (Hugging Face)" data-secondary="stable diffusion models" data-tertiary="Stable Diffusion 3.5" data-type="indexterm" id="id1940"/><a contenteditable="false" data-primary="generative AI" data-secondary="stable diffusion models" data-tertiary="Stable Diffusion 3.5" data-type="indexterm" id="id1941"/><a contenteditable="false" data-primary="generative AI" data-secondary="diffusers library from Hugging Face" data-tertiary="Stable Diffusion 3.5" data-type="indexterm" id="id1942"/><a contenteditable="false" data-primary="Hugging Face Hub" data-secondary="diffusers library" data-tertiary="Stable Diffusion 3.5" data-type="indexterm" id="id1943"/><a contenteditable="false" data-primary="image generator" data-secondary="stable diffusion models" data-tertiary="Stable Diffusion 3.5" data-type="indexterm" id="id1944"/> image of teddy bears on Mars, I used Stable Diffusion 3.5 Medium, which you can find on the <a href="https://oreil.ly/liUY-">Hugging Face website</a>.</p>&#13;
        <p class="pagebreak-before">This model has limited access,<a contenteditable="false" data-primary="Hugging Face Hub" data-secondary="permission to use models" data-type="indexterm" id="id1945"/><a contenteditable="false" data-primary="Hugging Face Hub" data-secondary="token in Google Colab" data-type="indexterm" id="id1946"/><a contenteditable="false" data-primary="Google Colab" data-secondary="Hugging Face token" data-type="indexterm" id="id1947"/><a contenteditable="false" data-primary="diffusers library (Hugging Face)" data-secondary="using Hugging Face diffusers" data-tertiary="limited access" data-type="indexterm" id="id1948"/><a contenteditable="false" data-primary="image generator" data-secondary="using Hugging Face diffusers" data-tertiary="limited access" data-type="indexterm" id="id1949"/> so at the top of the Hugging Face page, you’ll see a form that you need to fill out to get permission. You’ll also need to configure your Hugging Face secret key in Colab (if you’re using Colab), which we demonstrated how to do back in <a data-type="xref" href="ch14.html#ch14_using_third_party_models_and_hubs_1748549787242797">Chapter 14</a>. </p>&#13;
        <p>If you aren’t using Colab,<a contenteditable="false" data-primary="APIs" data-secondary="Hugging Face API to sign in" data-type="indexterm" id="id1950"/><a contenteditable="false" data-primary="Hugging Face Hub" data-secondary="API to sign in" data-type="indexterm" id="id1951"/> your code will need to be signed in to Hugging Face using their API. You can do this with the following code:</p>&#13;
        <pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">huggingface_hub</code> <code class="kn">import</code> <code class="n">login</code>&#13;
 &#13;
<code class="n">login</code><code class="p">(</code><code class="n">token</code><code class="o">=</code><code class="s2">"&lt;YOUR TOKEN HERE&gt;"</code><code class="p">)</code></pre>&#13;
        <p>Once you’re signed in (or if you’re using a model that doesn’t require signing in), the process of generating an image is as follows:</p>&#13;
        <ol>&#13;
          <li>&#13;
            <p>Create a Generator object, which allows you to specify the seed.</p>&#13;
          </li>&#13;
          <li>&#13;
            <p>Create an instance of the appropriate pipeline for the model you require.</p>&#13;
          </li>&#13;
          <li>&#13;
            <p>Send that pipeline to the appropriate accelerator.</p>&#13;
          </li>&#13;
          <li>&#13;
            <p>Generate the image with the pipeline, giving it the appropriate parameters.</p>&#13;
          </li>&#13;
        </ol>&#13;
        <p>Let’s look at this step-by-step.</p>&#13;
        <p>First, you specify the generator using <code>torch.Generator</code>, where you will specify the accelerator for the generator and set the seed. You use the seed value to create the initial noise with a level of determinism. If you want to be able to <em>replicate</em> the images that are generated, despite the noise being random, you do so by guiding the noise with the seed. In other words, when the noise is generated with a seed value, the <em>same</em> noise will be generated subsequent times with the same seed. So effectively, the noise will be pseudo-random, as there will be a deterministic seed at play. On the other hand, if you don’t specify a seed, you’ll get a random value for it. Here’s an example:</p>&#13;
        <pre data-code-language="python" data-type="programlisting"><code class="c1"># Set your seed value</code>&#13;
<code class="n">seed</code> <code class="o">=</code> <code class="mi">123456</code>  <code class="c1"># You can use any integer value you want</code>&#13;
 &#13;
<code class="c1"># Create a generator with the seed</code>&#13;
<code class="n">generator</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">Generator</code><code class="p">(</code><code class="s2">"cuda"</code><code class="p">)</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="n">seed</code><code class="p">)</code></pre>&#13;
        <p>Next, you’ll specify the pipeline and instantiate it with a model:</p>&#13;
        <pre data-code-language="python" data-type="programlisting"><code class="n">pipe</code> <code class="o">=</code> <code class="n">StableDiffusion3Pipeline</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code>&#13;
           <code class="s2">"stabilityai/stable-diffusion-3.5-medium"</code><code class="p">,</code> &#13;
           <code class="n">torch_dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">bfloat16</code><code class="p">)</code>&#13;
<code class="n">pipe</code> <code class="o">=</code> <code class="n">pipe</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="s2">"cuda"</code><code class="p">)</code></pre>&#13;
        <p>Here, we’re using Stable Diffusion 3.5, <a contenteditable="false" data-primary="diffusers library (Hugging Face)" data-secondary="stable diffusion models" data-tertiary="Stable Diffusion 3.5" data-type="indexterm" id="id1952"/><a contenteditable="false" data-primary="generative AI" data-secondary="diffusers library from Hugging Face" data-tertiary="Stable Diffusion 3.5" data-type="indexterm" id="id1953"/><a contenteditable="false" data-primary="generative AI" data-secondary="stable diffusion models" data-tertiary="Stable Diffusion 3.5" data-type="indexterm" id="id1954"/><a contenteditable="false" data-primary="Hugging Face Hub" data-secondary="diffusers library" data-tertiary="Stable Diffusion 3.5" data-type="indexterm" id="id1955"/><a contenteditable="false" data-primary="stable diffusion models" data-secondary="Stable Diffusion 3.5" data-tertiary="StableDiffusion3Pipeline class" data-type="indexterm" id="id1956"/><a contenteditable="false" data-primary="image generator" data-secondary="stable diffusion models" data-tertiary="Stable Diffusion 3.5" data-type="indexterm" id="id1957"/><a contenteditable="false" data-primary="diffusers library (Hugging Face)" data-secondary="pipeline information online" data-type="indexterm" id="id1958"/><a contenteditable="false" data-primary="Hugging Face Hub" data-secondary="diffusers library" data-tertiary="pipeline information online" data-type="indexterm" id="id1959"/><a contenteditable="false" data-primary="online resources" data-secondary="pipeline information for diffusers API" data-type="indexterm" id="id1960"/><a contenteditable="false" data-primary="generative AI" data-secondary="diffusers library from Hugging Face" data-tertiary="pipeline information online" data-type="indexterm" id="id1961"/><a contenteditable="false" data-primary="image generator" data-secondary="diffusers library from Hugging Face" data-tertiary="pipeline information online" data-type="indexterm" id="id1962"/>which uses the <code>StableDiffusion3Pipeline</code> class. The diffusers API is open source, with new pipelines being added all the time. You can inspect them on <a href="https://oreil.ly/uYGnJ">GitHub</a>.</p>&#13;
        <p class="pagebreak-before">You can also browse the<a contenteditable="false" data-primary="diffusers library (Hugging Face)" data-secondary="models on Hugging Face page" data-type="indexterm" id="id1963"/><a contenteditable="false" data-primary="generative AI" data-secondary="diffusers library from Hugging Face" data-tertiary="models on Hugging Face page" data-type="indexterm" id="id1964"/><a contenteditable="false" data-primary="Hugging Face Hub" data-secondary="diffusers library" data-tertiary="models on Hugging Face page" data-type="indexterm" id="id1965"/><a contenteditable="false" data-primary="image generator" data-secondary="diffusers library from Hugging Face" data-tertiary="models on Hugging Face page" data-type="indexterm" id="id1966"/><a contenteditable="false" data-primary="online resources" data-secondary="Hugging Face image generation models" data-type="indexterm" id="id1967"/> different models on the <a href="https://oreil.ly/R4dKT">Hugging Face website</a>. Often, their landing pages will include source code about which pipeline to use and the address of the model. </p>&#13;
        <p>Once you have the pipeline, you can use it to create an image by specifying the prompt and some other model-dependent parameters that you’ll find in the model document. So, for example, for stable diffusion, you’ll specify the number of inference steps and the generator that you specified earlier. You should also specify <em>where</em> you want the pipe to execute—(in this case, it’s <code>cuda</code>, as you can see in the previous code, which uses the GPU accelerator in Colab):</p>&#13;
        <pre data-code-language="python" data-type="programlisting"><code class="n">image</code> <code class="o">=</code> <code class="n">pipe</code><code class="p">(</code>&#13;
    <code class="s2">"A photo of a group of teddy bears eating pizza on the surface of mars"</code><code class="p">,</code>&#13;
    <code class="n">num_inference_steps</code><code class="o">=</code><code class="mi">40</code><code class="p">,</code>&#13;
    <code class="n">generator</code><code class="o">=</code><code class="n">generator</code>  <code class="c1"># Add the generator here</code>&#13;
<code class="p">)</code><code class="o">.</code><code class="n">images</code>&#13;
<code class="n">image</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">save</code><code class="p">(</code><code class="s2">"teddies.png"</code><code class="p">)</code></pre>&#13;
        <p>I’ve found that the best way to experiment with this is to explore the pipeline’s source code and see the parameters that it supports. <a contenteditable="false" data-primary="stable diffusion models" data-secondary="Stable Diffusion 3.5" data-tertiary="negative prompt" data-type="indexterm" id="id1968"/><a contenteditable="false" data-primary="prompt" data-secondary="negative prompt of Stable Diffusion 3.5" data-type="indexterm" id="id1969"/>For example, with the Stable Diffusion 3 pipeline, there’s a <em>negative prompt</em> that dictates things that you do <em>not</em> want to see in the image. Often, you can use this to make images better. For example, you may have heard that image generators, particularly early ones, were very bad at drawing hands. You could use the negative prompt to have the image generator avoid this problem by saying “deformed hands” or something similar in that prompt. </p>&#13;
        <p>You can also specify things you don’t want to see in the image that are more trivial! For example, every instance of the image I drew had the teddy bears eating <em>pepperoni</em> pizza. I could remove the pepperoni from this image with this code:</p>&#13;
        <pre data-code-language="python" data-type="programlisting"><code class="n">image</code> <code class="o">=</code> <code class="n">pipe</code><code class="p">(</code>&#13;
    <code class="s2">"A photo of a group of teddy bears eating pizza on the surface of mars"</code><code class="p">,</code>&#13;
    <code class="n">negative_prompt</code><code class="o">=</code><code class="s2">"pepperoni"</code><code class="p">,</code>&#13;
    <code class="n">num_inference_steps</code><code class="o">=</code><code class="mi">40</code><code class="p">,</code>&#13;
    <code class="n">generator</code><code class="o">=</code><code class="n">generator</code>  <code class="c1"># Add the generator here</code>&#13;
<code class="p">)</code><code class="o">.</code><code class="n">images</code></pre>&#13;
        <p>The resulting image is shown in <a data-type="xref" href="#ch19_figure_3_1748573005759194">Figure 19-5</a>.</p>&#13;
        <p>The teddy on the left doesn’t look thrilled about it, but the others seem more content!</p>&#13;
        <p>In this case, we used text-to-image to create these images—but diffusion models<a contenteditable="false" data-primary="diffusers library (Hugging Face)" data-secondary="using Hugging Face diffusers" data-tertiary="image-to-image" data-type="indexterm" id="ch19i2i"/><a contenteditable="false" data-primary="generative AI" data-secondary="diffusers library from Hugging Face" data-tertiary="image-to-image" data-type="indexterm" id="ch19i2i2"/><a contenteditable="false" data-primary="Hugging Face Hub" data-secondary="diffusers library" data-tertiary="image-to-image" data-type="indexterm" id="ch19i2i3"/><a contenteditable="false" data-primary="image generator" data-secondary="diffusers library from Hugging Face" data-tertiary="image-to-image" data-type="indexterm" id="ch19i2i4"/><a contenteditable="false" data-primary="image-to-image with diffusers" data-type="indexterm" id="ch19i2i5"/><a contenteditable="false" data-primary="StableDiffusion3Img2ImgPipeline" data-type="indexterm" id="ch19i2i6"/><a contenteditable="false" data-primary="pipelines from diffusers library of Hugging Face" data-secondary="image-to-image via StableDiffusion3Img2ImgPipeline" data-type="indexterm" id="ch19i2i7"/> have become a little more advanced with add-ons for <em>image-to-image.</em> With such add-ons, instead of starting with random noise, we can begin with an existing image and then perform <em>inpainting</em>, in which we can have the model fill in new details in an existing image. We’ll explore this next.</p>&#13;
                <figure><div class="figure" id="ch19_figure_3_1748573005759194">&#13;
          <img src="assets/aiml_1905.png"/>&#13;
          <h6><span class="label">Figure 19-5. </span>Teddies that don’t like pepperoni</h6>&#13;
        </div></figure>&#13;
        <section data-pdf-bookmark="Image-to-Image with Diffusers" data-type="sect2"><div class="sect2" id="ch19_image_to_image_with_diffusers_1748573005765709">&#13;
          <h2>Image-to-Image with Diffusers</h2>&#13;
          <p>When inspecting the source code for the pipeline, you may have discovered other classes in there, such as this one: <code>StableDiffusion3Img2ImgPipeline.</code> </p>&#13;
          <p>As its name suggests, this class allows you to start with one image to create another. You can initialize it in a way that’s very similar to initializing the text-to-image <span class="keep-together">pipeline:</span></p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">diffusers</code> <code class="kn">import</code> <code class="n">StableDiffusion3Img2ImgPipeline</code>&#13;
 &#13;
<code class="c1"># Set your seed value</code>&#13;
<code class="n">seed</code> <code class="o">=</code> <code class="mi">123456</code>&#13;
 &#13;
<code class="c1"># Create a generator with the seed</code>&#13;
<code class="n">generator</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">Generator</code><code class="p">(</code><code class="s2">"cuda"</code><code class="p">)</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="n">seed</code><code class="p">)</code>&#13;
 &#13;
<code class="c1"># Load the model</code>&#13;
<code class="n">pipe</code> <code class="o">=</code> <code class="n">StableDiffusion3Img2ImgPipeline</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code>&#13;
    <code class="s2">"stabilityai/stable-diffusion-3.5-medium"</code><code class="p">,</code> &#13;
     <code class="n">torch_dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">bfloat16</code><code class="p">)</code>&#13;
 &#13;
<code class="n">pipe</code> <code class="o">=</code> <code class="n">pipe</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="s2">"cuda"</code><code class="p">)</code></pre>&#13;
          <p>Then, you’ll specify an image to use as the source image: </p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">PIL</code> <code class="kn">import</code> <code class="n">Image</code>&#13;
<code class="c1"># Load and preprocess the initial image</code>&#13;
<code class="n">init_image</code> <code class="o">=</code> <code class="n">Image</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="s2">"puppy1.jpg"</code><code class="p">)</code><code class="o">.</code><code class="n">convert</code><code class="p">(</code><code class="s2">"RGB"</code><code class="p">)</code></pre>&#13;
          <p>I’m starting with an image of a puppy (see <a data-type="xref" href="#ch19_figure_4_1748573005759214">Figure 19-6</a>).</p>&#13;
          <figure><div class="figure" id="ch19_figure_4_1748573005759214">&#13;
            <img src="assets/aiml_1906.png"/>&#13;
            <h6><span class="label">Figure 19-6. </span>Source image of a puppy</h6>&#13;
          </div></figure>&#13;
          <p>We’ll use this as the initialization image in an image-to-image pipeline with the following code. Note that the prompt is specifying a highly detailed photograph of a baby <em>dragon</em>:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Generate the image</code>&#13;
<code class="n">image</code> <code class="o">=</code> <code class="n">pipe</code><code class="p">(</code>&#13;
    <code class="n">prompt</code><code class="o">=</code><code class="s2">"A highly detailed photograph of a baby dragon"</code><code class="p">,</code>&#13;
    <code class="n">image</code><code class="o">=</code><code class="n">init_image</code><code class="p">,</code>&#13;
    <code class="n">strength</code><code class="o">=</code><code class="mf">0.7</code><code class="p">,</code>&#13;
    <code class="n">num_inference_steps</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code>&#13;
    <code class="n">generator</code><code class="o">=</code><code class="n">generator</code>&#13;
<code class="p">)</code><code class="o">.</code><code class="n">images</code></pre>&#13;
          <p>The strength parameter specifies how closely the generated image should follow the input image. At 0.0, the model won’t do anything and the output will be the input image. At 1.0, it will effectively <em>ignore</em> the input image and will just act as a text-to-image model.</p>&#13;
          <p>Under the hood, it does this with the following process. </p>&#13;
          <p>Given that the code specified a strength of 0.7, the model will add noise to the image until the image has had 70% of its pixels replaced by noise (and thus only 30% of the image is the original values).</p>&#13;
          <p>The model will then run 70 denoising steps (70% of the 100 specified), which will give an image like <a data-type="xref" href="#ch19_figure_5_1748573005759233">Figure 19-7</a>.</p>&#13;
          <p>Typically, if you use strength 0.2 to 0.4, you’ll get style transfer and other minor modifications. At 0.5 to 0.7, you’ll have basic composition maintained, but major element changes, like puppy to dragon, will be seen. Above 0.8, you’ll see almost complete regeneration, but some slight influence from the original may be retained.</p>&#13;
          <figure><div class="figure" id="ch19_figure_5_1748573005759233">&#13;
            <img src="assets/aiml_1907.png"/>&#13;
            <h6><span class="label">Figure 19-7. </span>Using image-to-image to turn a puppy into a dragon</h6>&#13;
          </div></figure>&#13;
          <p>You can see that the basic pose has been maintained, but the computer has imagined a dragon to replace the puppy as required. There’s also new foreground and background, as we didn’t specify anything about them, but they’re pretty close to the <span class="keep-together">originals.</span></p>&#13;
          <p>As an example of a different strength level, <a data-type="xref" href="#ch19_figure_6_1748573005759251">Figure 19-8</a> shows the strength at 0.4. We can also see that the basic shape of the puppy has been maintained, but it has become more dragon-like, with scaly skin and the beginnings of claws!</p>&#13;
          <figure><div class="figure" id="ch19_figure_6_1748573005759251">&#13;
            <img src="assets/aiml_1908.png"/>&#13;
            <h6><span class="label">Figure 19-8. </span>Strength level of 0.4 for the puppy to dragon image-to-image</h6>&#13;
          </div></figure>&#13;
          <p>This technique can be very useful in helping you create new images by starting from existing ones. I’ve seen it used in scenarios like filmmaking—where one can start with existing video that’s filmed in a basic, cheap locale but then enhanced with image-to-image frame by frame to get a different outcome. It’s a much cheaper way of doing postproduction by adding special effects!<a contenteditable="false" data-primary="" data-startref="ch19i2i" data-type="indexterm" id="id1970"/><a contenteditable="false" data-primary="" data-startref="ch19i2i2" data-type="indexterm" id="id1971"/><a contenteditable="false" data-primary="" data-startref="ch19i2i3" data-type="indexterm" id="id1972"/><a contenteditable="false" data-primary="" data-startref="ch19i2i4" data-type="indexterm" id="id1973"/><a contenteditable="false" data-primary="" data-startref="ch19i2i5" data-type="indexterm" id="id1974"/><a contenteditable="false" data-primary="" data-startref="ch19i2i6" data-type="indexterm" id="id1975"/><a contenteditable="false" data-primary="" data-startref="ch19i2i7" data-type="indexterm" id="id1976"/></p>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Inpainting with Diffusers" data-type="sect2"><div class="sect2" id="ch19_inpainting_with_diffusers_1748573005765759">&#13;
          <h2>Inpainting with Diffusers</h2>&#13;
          <p>Another scenario that involves using<a contenteditable="false" data-primary="diffusers library (Hugging Face)" data-secondary="using Hugging Face diffusers" data-tertiary="inpainting" data-type="indexterm" id="ch19inp"/><a contenteditable="false" data-primary="generative AI" data-secondary="diffusers library from Hugging Face" data-tertiary="inpainting" data-type="indexterm" id="ch19inp2"/><a contenteditable="false" data-primary="Hugging Face Hub" data-secondary="diffusers library" data-tertiary="inpainting" data-type="indexterm" id="ch19inp3"/><a contenteditable="false" data-primary="image generator" data-secondary="diffusers library from Hugging Face" data-tertiary="inpainting" data-type="indexterm" id="ch19inp4"/><a contenteditable="false" data-primary="inpainting with diffusers" data-type="indexterm" id="ch19inp5"/> diffusers that is supported by some models—including stable diffusion models—is the idea of <em>inpainting</em>, in which you can take an image and replace parts of it with AI-generated content. So, for example, consider the puppy from <a data-type="xref" href="#ch19_figure_4_1748573005759214">Figure 19-6</a> and how you can change the image so the little pooch is on the moon, as in <a data-type="xref" href="#ch19_figure_7_1748573005759269">Figure 19-9</a>.</p>&#13;
          <figure><div class="figure" id="ch19_figure_7_1748573005759269">&#13;
            <img src="assets/aiml_1909.png"/>&#13;
            <h6><span class="label">Figure 19-9. </span>Using inpainting to put our puppy on the moon</h6>&#13;
          </div></figure>&#13;
          <p>You can do this by using a pattern that’s similar to the previous one. First, you’ll set up the pipeline for inpainting:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">diffusers</code> <code class="kn">import</code> <code class="n">StableDiffusion3InpaintPipeline</code>&#13;
 &#13;
<code class="c1"># Load the inpainting pipeline</code>&#13;
<code class="n">pipe</code> <code class="o">=</code> <code class="n">StableDiffusion3InpaintPipeline</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code>&#13;
    <code class="s2">"stabilityai/stable-diffusion-3.5-medium"</code><code class="p">,</code>&#13;
    <code class="n">torch_dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">bfloat16</code>&#13;
<code class="p">)</code>&#13;
<code class="n">pipe</code> <code class="o">=</code> <code class="n">pipe</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="s2">"cuda"</code><code class="p">)</code></pre>&#13;
          <p>The parameters to initialize it are the same as earlier. Next, you’ll need the generator:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Set seed for reproducibility</code>&#13;
<code class="n">generator</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">Generator</code><code class="p">(</code><code class="s2">"cuda"</code><code class="p">)</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code></pre>&#13;
          <p>Then, you’ll specify the source image, which in this case is the original image of the puppy:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Load the original image and mask</code>&#13;
<code class="n">original_image</code> <code class="o">=</code> <code class="n">Image</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="s2">"puppy.jpg"</code><code class="p">)</code><code class="o">.</code><code class="n">convert</code><code class="p">(</code><code class="s2">"RGB"</code><code class="p">)</code></pre>&#13;
          <p>The complicated step is the next one, in which you specify the <em>mask</em> for the image: </p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="n">mask_image</code> <code class="o">=</code> <code class="n">Image</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="s2">"puppymask.png"</code><code class="p">)</code><code class="o">.</code><code class="n">convert</code><code class="p">(</code><code class="s2">"L"</code><code class="p">)</code>  </pre>&#13;
          <p class="pagebreak-before">A <em>mask</em> is simply an image that corresponds to the original one, in which pieces to be <em>replaced</em> are in white and pieces to be <em>preserved</em> are in black. <a data-type="xref" href="#ch19_figure_8_1748573005759284">Figure 19-10</a> shows the mask image used for the puppy. I like to think of this as similar to the green-screen process used in making movies. The white part of the image is the screen, and the black is the stuff that’s in front of it! The model will then replace the white with whatever you prompt it for.</p>&#13;
          <figure><div class="figure" id="ch19_figure_8_1748573005759284">&#13;
            <img src="assets/aiml_1910.png"/>&#13;
            <h6><span class="label">Figure 19-10. </span>Mask for the image</h6>&#13;
          </div></figure>&#13;
          <p>There are many ways to create masks. For this one, I used the Acorn 8 tool for the Mac. This tool gives you the ability to remove the background and paint it all in white, and then, for what’s left, it lets you select the pixels with a magic wand and paint them all in black. Every tool does this differently, so be sure to check the appropriate documentation.</p>&#13;
          <p>Once you have the image and the mask, you can easily use the pipeline to have the model inpaint the areas that correspond to the white part of the mask. Given that the puppy is already present, I didn’t mention it in the prompt, and I just used “on the surface of the moon” to get the image in <a data-type="xref" href="#ch19_figure_7_1748573005759269">Figure 19-9</a>:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Generate the inpainted image</code>&#13;
<code class="n">image</code> <code class="o">=</code> <code class="n">pipe</code><code class="p">(</code>&#13;
    <code class="n">prompt</code><code class="o">=</code><code class="s2">"on the surface of the moon"</code><code class="p">,</code>&#13;
    <code class="n">image</code><code class="o">=</code><code class="n">original_image</code><code class="p">,</code>&#13;
    <code class="n">mask_image</code><code class="o">=</code><code class="n">mask_image</code><code class="p">,</code>&#13;
    <code class="n">num_inference_steps</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code>&#13;
    <code class="n">generator</code><code class="o">=</code><code class="n">generator</code><code class="p">,</code>&#13;
    <code class="n">strength</code><code class="o">=</code><code class="mf">0.99</code>  <code class="c1"># How much to inpaint the masked area</code>&#13;
<code class="p">)</code><code class="o">.</code><code class="n">images</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code></pre>&#13;
          <p>The diffusers API, as you can see, gives you a very consistent approach to managing image creation, be it directly from a text prompt, starting from a source image, or inpainting a particular area.<a contenteditable="false" data-primary="" data-startref="ch19use" data-type="indexterm" id="id1977"/><a contenteditable="false" data-primary="" data-startref="ch19use2" data-type="indexterm" id="id1978"/><a contenteditable="false" data-primary="" data-startref="ch19use3" data-type="indexterm" id="id1979"/><a contenteditable="false" data-primary="" data-startref="ch19use4" data-type="indexterm" id="id1980"/><a contenteditable="false" data-primary="" data-startref="ch19inp" data-type="indexterm" id="id1981"/><a contenteditable="false" data-primary="" data-startref="ch19inp2" data-type="indexterm" id="id1982"/><a contenteditable="false" data-primary="" data-startref="ch19inp3" data-type="indexterm" id="id1983"/><a contenteditable="false" data-primary="" data-startref="ch19inp4" data-type="indexterm" id="id1984"/><a contenteditable="false" data-primary="" data-startref="ch19inp5" data-type="indexterm" id="id1985"/> </p>&#13;
        </div></section>&#13;
      </div></section>&#13;
      <section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="ch19_summary_1748573005765806">&#13;
        <h1>Summary</h1>&#13;
        <p>In this chapter, you explored how to use generative models for image creation by using the Hugging Face diffusers library. You started by looking at the fundamental underlying concepts, seeing how the idea of denoising to create new content works. </p>&#13;
        <p>You also looked into practical code-based implementation of image generation by using the diffusers API, and you focused on three main approaches:</p>&#13;
        <ol>&#13;
          <li>&#13;
            <p>You explored text-to-image by converting text prompts directly into images using the Stable Diffusion 3.5 model. You also looked at how you can control this process with parameters like the seed value and the number of inference steps.</p>&#13;
          </li>&#13;
          <li>&#13;
            <p>You explored image-to-image by starting with an existing image and transforming it by using a prompt. In particular, you saw how the <code>strength</code> hyperparameter controls the overall transformation</p>&#13;
          </li>&#13;
          <li>&#13;
            <p>You explored inpainting by preserving parts of the original image by using a mask, which allows for targeted modifications while preserving some elements.</p>&#13;
          </li>&#13;
        </ol>&#13;
        <p>You also explored hands-on, concrete code examples of each of these approaches, which showed you how to do the pipeline setup, generator initialization, and basic parameter tuning. You also saw how <em>negative</em> prompts can help you get images closer to what you really want.</p>&#13;
        <p>In the next chapter, you’ll look at LoRA (low-ranking adaptation), which lets you fine-tune diffusion models to achieve more controlled and customized images. LoRA is a powerful technique that allows for efficient model adaptation by only fine-tuning a small number of parameters, thus helping you guide the model toward specific styles, subjects, or artistic directions. You’ll explore how to implement LoRA with the diffusers library, and you’ll customize these models to create <em>specialized</em> image generators for your needs.</p>&#13;
      </div></section>&#13;
    </div></section></body></html>