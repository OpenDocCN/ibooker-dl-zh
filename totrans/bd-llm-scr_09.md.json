["```py\nprint(tokenizer.encode(\"Ak\"))\nprint(tokenizer.encode(\"w\"))\n# ...\n```", "```py\n[33901]\n[86]\n# ...\n```", "```py\nprint(tokenizer.decode([33901, 86, 343, 86, 220, 959]))\n```", "```py\n'Akwirw ier'\n```", "```py\ndataloader = create_dataloader(\n    raw_text, batch_size=4, max_length=2, stride=2\n)\n```", "```py\ntensor([[  40,  367],\n        [2885, 1464],\n        [1807, 3619],\n        [ 402,  271]])\n```", "```py\ndataloader = create_dataloader(\n    raw_text, batch_size=4, max_length=8, stride=2\n)\n```", "```py\ntensor([[   40,   367,  2885,  1464,  1807,  3619,   402,   271],\n        [ 2885,  1464,  1807,  3619,   402,   271, 10899,  2138],\n        [ 1807,  3619,   402,   271, 10899,  2138,   257,  7026],\n        [  402,   271, 10899,  2138,   257,  7026, 15632,   438]])\n```", "```py\nsa_v1.W_query = torch.nn.Parameter(sa_v2.W_query.weight.T)\nsa_v1.W_key = torch.nn.Parameter(sa_v2.W_key.weight.T)\nsa_v1.W_value = torch.nn.Parameter(sa_v2.W_value.weight.T)\n```", "```py\nd_out = 1\nmha = MultiHeadAttentionWrapper(d_in, d_out, block_size, 0.0, num_heads=2)\n```", "```py\nblock_size = 1024\nd_in, d_out = 768, 768\nnum_heads = 12\nmha = MultiHeadAttention(d_in, d_out, block_size, 0.0, num_heads)\n```", "```py\nblock = TransformerBlock(GPT_CONFIG_124M)\n\ntotal_params = sum(p.numel() for p in block.ff.parameters())\nprint(f\"Total number of parameters in feed forward module: {total_params:,}\")\n\ntotal_params = sum(p.numel() for p in block.att.parameters())\nprint(f\"Total number of parameters in attention module: {total_params:,}\")\n```", "```py\nTotal number of parameters in feed forward module: 4,722,432\nTotal number of parameters in attention module: 2,360,064\n```", "```py\nGPT_CONFIG = GPT_CONFIG_124M.copy()\nGPT_CONFIG[\"emb_dim\"] = 1600\nGPT_CONFIG[\"n_layers\"] = 48\nGPT_CONFIG[\"n_heads\"] = 25\nmodel = GPTModel(GPT_CONFIG)\n```", "```py\ngpt2-xl:\nTotal number of parameters: 1,637,792,000\nNumber of trainable parameters considering weight tying: 1,557,380,800\nTotal size of the model: 6247.68 MB\n```", "```py\nGPT_CONFIG_124M = {\n    \"vocab_size\": 50257,\n    \"context_length\": 1024,\n    \"emb_dim\": 768,\n    \"n_heads\": 12,\n    \"n_layers\": 12,\n    \"drop_rate_attn\": 0.1,      #1\n    \"drop_rate_shortcut\": 0.1,      #2\n    \"drop_rate_emb\": 0.1,      #3\n    \"qkv_bias\": False\n}\n```", "```py\nclass TransformerBlock(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.att = MultiHeadAttention(\n            d_in=cfg[\"emb_dim\"],\n            d_out=cfg[\"emb_dim\"],\n            context_length=cfg[\"context_length\"],\n            num_heads=cfg[\"n_heads\"], \n            dropout=cfg[\"drop_rate_attn\"],      #1\n            qkv_bias=cfg[\"qkv_bias\"])\n        self.ff = FeedForward(cfg)\n        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n        self.drop_shortcut = nn.Dropout(        #2\n            cfg[\"drop_rate_shortcut\"]           #2\n        )                                       #2\n\n    def forward(self, x):\n        shortcut = x\n        x = self.norm1(x)\n        x = self.att(x)\n        x = self.drop_shortcut(x)\n        x = x + shortcut\n\n        shortcut = x\n        x = self.norm2(x)\n        x = self.ff(x)\n        x = self.drop_shortcut(x)\n        x = x + shortcut\n        return x\n\nclass GPTModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.tok_emb = nn.Embedding(\n            cfg[\"vocab_size\"], cfg[\"emb_dim\"]\n        )\n        self.pos_emb = nn.Embedding(\n            cfg[\"context_length\"], cfg[\"emb_dim\"]\n        )\n        self.drop_emb = nn.Dropout(cfg[\"drop_rate_emb\"])    #3\n\n        self.trf_blocks = nn.Sequential(\n            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n\n        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n        self.out_head = nn.Linear(\n            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n        )\n\n    def forward(self, in_idx):\n        batch_size, seq_len = in_idx.shape\n        tok_embeds = self.tok_emb(in_idx)\n        pos_embeds = self.pos_emb(\n            torch.arange(seq_len, device=in_idx.device)\n        )\n        x = tok_embeds + pos_embeds\n        x = self.drop_emb(x)\n        x = self.trf_blocks(x)\n        x = self.final_norm(x)\n        logits = self.out_head(x)\n        return logitss\n```", "```py\ncheckpoint = torch.load(\"model_and_optimizer.pth\")\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.load_state_dict(checkpoint[\"model_state_dict\"])\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\noptimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n```", "```py\ntrain_loss = calc_loss_loader(train_loader, gpt, device)\nval_loss = calc_loss_loader(val_loader, gpt, device)\n```", "```py\nTraining loss: 3.754748503367106\nValidation loss: 3.559617757797241\n```", "```py\nhparams, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")\nmodel_name = \"gpt2-small (124M)\"\n```", "```py\nhparams, params = download_and_load_gpt2(model_size=\"1558M\", models_dir=\"gpt2\")\nmodel_name = \"gpt2-xl (1558M)\"\n```", "```py\ntrain_dataset = SpamDataset(..., max_length=1024, ...)\nval_dataset = SpamDataset(..., max_length=1024, ...)\ntest_dataset = SpamDataset(..., max_length=1024, ...)\n```", "```py\nfor param in model.parameters():\n    param.requires_grad = False\n```", "```py\n<user>\nIdentify the correct spelling of the following word: 'Occasion'\n\n<assistant>\nThe correct spelling is 'Occasion'.\n```", "```py\ndef format_input(entry):\n    instruction_text = (\n        f\"<|user|>\\n{entry['instruction']}\"\n    )\n    input_text = f\"\\n{entry['input']}\" if entry[\"input\"] else \"\"\n    return instruction_text + input_text\n```", "```py\nfor i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n    input_text = format_input(entry)\n    tokenizer=tokenizer\n    token_ids = generate(\n        model=model,\n        idx=text_to_token_ids(input_text, tokenizer).to(device),\n        max_new_tokens=256,\n        context_size=BASE_CONFIG[\"context_length\"],\n        eos_id=50256\n    )\n    generated_text = token_ids_to_text(token_ids, tokenizer)\n    response_text = (                       #1\n        generated_text[len(input_text):]\n        .replace(\"<|assistant|>:\", \"\")\n        .strip()\n    )\n    test_data[i][\"model_response\"] = response_text\n```", "```py\nclass InstructionDataset(Dataset):\n    def __init__(self, data, tokenizer):\n        self.data = data\n        self.instruction_lengths = []     #1\n        self.encoded_texts = []\n\n        for entry in data:\n            instruction_plus_input = format_input(entry)\n            response_text = f\"\\n\\\\n### Response:\\\\n{entry['output']}\"\n            full_text = instruction_plus_input + response_text\n\n            self.encoded_texts.append(\n                tokenizer.encode(full_text)\n            )\n            instruction_length = ( \n                len(tokenizer.encode(instruction_plus_input)\n            )\n            self.instruction_lengths.append(instruction_length)      #2\n\n    def __getitem__(self, index):    #3\n        return self.instruction_lengths[index], self.encoded_texts[index]\n\n    def __len__(self):\n        return len(self.data)\n```", "```py\ndef custom_collate_fn(\n    batch,\n    pad_token_id=50256,\n    ignore_index=-100,\n    allowed_max_length=None,\n    device=\"cpu\"\n):\n\n    batch_max_length = max(len(item)+1 for instruction_length, item in batch)\n    inputs_lst, targets_lst = [], []          #1\n\n    for instruction_length, item in batch:   \n        new_item = item.copy()\n        new_item += [pad_token_id]\n        padded = (\n            new_item + [pad_token_id] * (batch_max_length - len(new_item)\n        )\n        inputs = torch.tensor(padded[:-1])\n        targets = torch.tensor(padded[1:])\n        mask = targets == pad_token_id\n        indices = torch.nonzero(mask).squeeze()\n        if indices.numel() > 1:\n            targets[indices[1:]] = ignore_index\n\n        targets[:instruction_length-1] = -100       #2\n\n        if allowed_max_length is not None:\n            inputs = inputs[:allowed_max_length]\n            targets = targets[:allowed_max_length]\n\n        inputs_lst.append(inputs)\n        targets_lst.append(targets)\n\n    inputs_tensor = torch.stack(inputs_lst).to(device)\n    targets_tensor = torch.stack(targets_lst).to(device)\n\n    return inputs_tensor, targets_tensor\n```", "```py\nurl = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/instruction-data.json\"\n```", "```py\nurl = \"https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json\"\n```", "```py\nfrom appendix_E import LoRALayer, LinearWithLoRA, replace_linear_with_lora\n```", "```py\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total trainable parameters before: {total_params:,}\")\n\nfor param in model.parameters():\n    param.requires_grad = False\n\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total trainable parameters after: {total_params:,}\")\nreplace_linear_with_lora(model, rank=16, alpha=16)\n\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total trainable LoRA parameters: {total_params:,}\")\nmodel.to(device)\n```", "```py\nmodel = NeuralNetwork(2, 2)\nnum_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(\"Total number of trainable model parameters:\", num_params)\n```", "```py\n752\n```", "```py\na = torch.rand(100, 200)\nb = torch.rand(200, 300)\n%timeit a@b\n```", "```py\n63.8 µs ± 8.7 µs per loop\n```", "```py\na, b = a.to(\"cuda\"), b.to(\"cuda\")\n%timeit a @ b\n```", "```py\n13.8 µs ± 425 ns per loop\n```"]