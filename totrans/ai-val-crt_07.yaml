- en: Chapter 7\. Where This Technology Is Headed—One Model Will *Not* Rule Them All!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Can you place this mantra?
  prefs: []
  type: TYPE_NORMAL
- en: “One Ring to rule them all,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: One Ring to find them,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: One Ring to bring them all,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: and in the darkness bind them.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'If you’re a true Tolkienite nerd, your elf ears likely perked up; otherwise,
    we’ll tell you it’s the basis of the story for J.R.R. Tolkien’s iconic *Lord of
    the Rings* and this One Ring inscription gives its wearer the ability to control
    everything. (Purists will note it wasn’t the inscription that bestowed the power
    and then go on about Sauron, but we’ll leave it there; like we said, nerds.) Total
    domination. Putting all the evil aside, one question looms (likely due to the
    fanfare around ChatGPT that introduced the world to GenAI): will one single LLM
    rule them all?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Spoiler alert: we don’t think so at all. Not even close. As you learned earlier
    in this book, there are almost 1.5 million (it’s likely more by the time you read
    this book) models on Hugging Face alone. We’re also certain (assuming you’ve read
    the book linearly so far) that you can easily articulate the difference between
    Value Users and Value Creators, and you understand AI ethics and data lineage.
    In short, you understand why one model can’t possibly rule them all...but we’re
    going to pull a more complete answer to the *why* for you here. It starts with
    the fact that even in the AI labs pushing out the highest-performing frontier
    models, we are seeing shifts from innovating on a single model performing a task,
    to empowering a system of models and techniques to work together and complete
    a task. In this chapter, we want to draw your attention to what’s been going on
    in the marketplace, and to which trends and technological innovations are powering
    the future of GenAI. From the rapid innovations that are happening at the small
    model size, to intra- and inter-model routing, to exciting advancements in agentic
    systems, we believe there will never be one model to rule them all.'
  prefs: []
  type: TYPE_NORMAL
- en: The Bigger the Better, Right? Perhaps at the Start, But That Was a Long Time
    Ago
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keeping with our theme in this book that while tech years age like dog years
    (1:7), GenAI years are like mouse years (1:30), that makes 2018 over 2 centuries
    old in GenAI years—that’s a long time ago! What happened in 2018? OpenAI released
    [GPT-1](https://oreil.ly/IBZTG) with a mere 117 million parameters.
  prefs: []
  type: TYPE_NORMAL
- en: As a part of their quest toward *artificial general intelligence (AGI)*, OpenAI
    has built successively more capable GPT versions (some into the trillions of parameters)
    that can perform more tasks with each successive release.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: AGI shouldn’t be confused with GenAI. GenAI is a tool. AGI is a goal of evolving
    that tool to the extent that its capabilities match human cognitive abilities,
    or even surpasses them, across a wide range of tasks. We’re not there yet, perhaps
    never will be, or perhaps it’ll arrive sooner than we expected. But when it comes
    to AGI, think about LLMs demonstrating and exceeding humanlike intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: Initially, it seemed that the main vehicle for driving model performance improvements
    was simply increasing a model’s size. As shown in [Table 7-1](#table-7-1), between
    GPT-1 and GPT-3, the models released by OpenAI increased by more than 10,000 times
    in size! After GPT-3, OpenAI stopped publishing model sizes all together, but
    GPT-4 and the GPT-4o models were rumored^([1](ch07.html#id906)) at one point to
    total over one trillion parameters! And as these models have gotten larger, they
    have also gotten more expensive. Small models normally cost less than $0.25 for
    1 million output tokens (or “free” if you can get it on your laptop with frameworks
    like Ollama). In contrast, big models are pricier. For example, last we looked,
    OpenAI’s o1 costs were about $60 for the same amount of output.^([2](ch07.html#id907))
    Whatever the price you’re paying (prices in this space are changing as fast as
    the technology, mostly in a good way), high performance small models have a lot
    of business sense to them.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 7-1](#table-7-1) shows that as the GPT family of models has grown, the
    world has witnessed significant improvements in the capabilities that these models
    could achieve.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 7-1\. OpenAI’s GPT family over time
  prefs: []
  type: TYPE_NORMAL
- en: '| OpenAI model name | Parameters | Interesting things to note |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-1 | 117 million | This is the “original.” It was better than some previous
    technologies, but turned out to be just the start of something that was going
    to be big. |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-2 | ~1 billion | This model started to make some interesting completions
    and prove that there was a different horizon for natural language processing (NLP).
    It was nowhere close to what you first experienced with ChatGPT and beyond, but
    it got some press in the news for writing a story about unicorns.^([a](ch07.html#id908))
    |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3 GPT-3.5'
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3.5 Turbo | ~175 billion | GPT-3.5 was the initial model behind ChatGPT’s
    debut. Two big changes occurred compared to GPT-2\. It was designed to follow
    instructions (versus simply predicting the next most likely word in a sentence),
    *and* they put a user interface on it. Enough said. GPT-3.5 was also released
    as a more efficient, lightweight version called “Turbo.” |
  prefs: []
  type: TYPE_NORMAL
- en: '| GPT-4 GPT-4 turbo'
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4o
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4o mini
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4.5 | OpenAI stopped publishing parameter numbers after GPT-3 (which was
    noted to have 175 billion parameters). Various blogs suggest GPT-4 has ~1.8 trillion
    parameters. | Their fourth generation of models delivered more power and multimodal
    capabilities. At the time of publishing, GPT-4o was considered OpenAI’s “flagship”
    model, and GPT-4.5 just came out. GPT-5 wasn’t out when we went to print, but
    many are suggesting to expect it sometime in the middle of 2025. |
  prefs: []
  type: TYPE_NORMAL
- en: '| OpenAI o1 OpenAI o3 mini | (See above.) | Considered a separate project and
    not a part of the core GPT family, these reasoning models were trained to produce
    long chains of thought before responding, enabling them to solve more complex
    tasks. This capability is expected to be merged into GPT-5. |'
  prefs: []
  type: TYPE_TB
- en: '| ^([a](ch07.html#id908-marker)) See the story on [OpenAI’s site](https://oreil.ly/1JZ6W).
    |'
  prefs: []
  type: TYPE_TB
- en: This begs the question, do you need all that capacity for your business? Even
    OpenAI has started creating smaller, more efficient versions of their models.
    For each major model release, there has been a pairwise release of a more efficient
    and more cost-effective alternative. GPT-3.5, meet GPT-3.5 Turbo; GPT-4o, meet
    GPT-4o mini.
  prefs: []
  type: TYPE_NORMAL
- en: The latest reasoning model OpenAI released at the time this book was published
    was OpenAI o3 mini. While OpenAI originally committed to releasing OpenAI o3,
    they have since pressed pause, and announced instead that GPT-5 will introduce
    an AI system that brings together the best of OpenAI o3 and the GPT model series,
    with Sam Altman sharing the goal of “simplifying our product offerings” and “to
    return to magic unified intelligence.”^([3](ch07.html#id909))
  prefs: []
  type: TYPE_NORMAL
- en: To sum up this section, even in the frontier AI labs that were made famous by
    innovating through scale, we are seeing innovations and road maps centered around
    bringing multiple models together, working as a system to drive “unified intelligence.”
  prefs: []
  type: TYPE_NORMAL
- en: And despite the common belief that bigger is always better when it comes to
    model size, there are many exciting innovations enabling small yet powerful LLMs.
    So much so that the term small language models (SLMs)^([4](ch07.html#id910)) has
    emerged. There is no precise definition, but SLMs usually refer to LLMs that are
    normally fewer than 13 billion parameters in size. In some scenarios, SLMs have
    met the performance of LLMs 100+ billion parameters in size.
  prefs: []
  type: TYPE_NORMAL
- en: The Rise of the Small Language Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Perhaps the simplest way to describe the phenomena that is SLMs is that model
    providers are getting better at training. Case in point, when some of our research
    teams first got their hands on Llama-2-70B back in July 2023, they were amazed
    at what it could do. Just a little over a year later, they were able to achieve
    *the same, if not better,* performance using just a 2B parameter version of Granite,
    according to Hugging Face’s Open LLM v2 Leaderboard (see [Figure 7-1](#ch07_figure_1_1740182051640230)).
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a graph  AI-generated content may be incorrect.](assets/aivc_0701.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-1\. A snapshot of model performance, taken from Hugging Face’s Open
    LLM v2 Leaderboard in Feb 2024
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This, again, is just part of the natural benefit of progressing up the learning
    curve of anything; as sure as our electric vehicles (EVs) go farther and charge
    faster, we’re getting more pixels and camera lenses on our phone every other year,
    and our TVs are getting thinner, providers are gaining more experience training
    models, and new innovations are making them more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: In the next couple of sections, we want to share with you some of the promising
    strategies behind the rise of highly competitive SLMs, specifically data curation
    and model distillation.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is no coincidence that both of these strategies center around the data used
    to train and fine-tune LLMs. It surprises many we talk to that, more often than
    not, advancements that are slimming down model size stem more from innovative
    strategies with training data than technical innovations in the model’s architecture
    itself. Please don’t misunderstand what we’re trying to tell you here. Innovations
    in architecture are definitely occurring. In fact, we cover some exciting architecture
    advancements in this very chapter! But, when we look at the warp speed of how
    SLMs have risen to prominence (and they did so within a year of the November 2022
    release of ChatGPT), the contributing factor is clear: *data reigns supreme!*
    And we go into detail on these data-based trends because in [Chapter 8](ch08.html#ch08_using_your_data_as_a_differentiator_1740182052172518),
    we will show you how the same techniques that model providers are using today
    to create SLMs can be used by your company to differentiate and create value with
    enterprise data.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'So here’s the deal: you’ve got data. That data you have access to isn’t part
    of these LLMs at all. Why? Because it’s your corporate data. We can assure you
    that many LLM providers want it. In fact, the reason 99% of corporate data isn’t
    scraped and sucked into an LLM is because you didn’t post it on the internet.
    So, you have some choices to make that we talked about earlier in this book, and
    we will go deep into them in the next chapter. Where will you sit on the data
    value exchange continuum we talked about in [Chapter 2](ch02.html#ch02_oh_to_be_an_ai_value_creator_1740182046162988)?
    Are you planning to give it away and let others create disproportionate amounts
    of value from your data, essentially *making your data* *THEIR* *competitive advantage*
    OR are you going to *make your data* *YOUR* *competitive advantage*? That’s what
    this book is all about. And this and the next chapter help you see that through.'
  prefs: []
  type: TYPE_NORMAL
- en: Data Curation Results in AI Salvation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OK, we admit it, you likely know this one. You don’t even have to have a machine
    learning background to assert that curating a large quantity of high-quality training
    data can have huge impacts on a model’s performance (or any analytics project
    for that matter).
  prefs: []
  type: TYPE_NORMAL
- en: 'But an emphasis on data curation is a huge part of why SLMs have become so
    performant, and it goes directly against the initial philosophy of the early LLM
    bakes: take as much messy, uncleaned, and unstructured data as possible and repurpose
    it to power an LLM. As it turns out, a compromise is in order when it comes to
    LLMs for business. Transformer technology made it possible to take large quantities
    of relatively messy data to create an LLM, but the higher quality the data, the
    higher quality the model. Ask yourself if you have large volumes of high-quality
    data that is specialized for business that you care about. Of course you do! Now
    you are ready to cook with gas because quantity, quality, and specialization are
    the three key data curation ingredients that have helped lead to the rise of SLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: Data quantity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: How much data is optimal for a given model size? This has been a subject of
    much study by the AI research community because, as you can imagine, there are
    very high environmental and pocketbook costs associated with training an LLM.
    For this reason, early model providers’ initial focus was trying to optimize performance
    while minimizing their own up-front costs for model training. A key part of this
    optimization was defining how many tokens (recall, this is essentially a piece
    of a word, a whole word, or even a punctuation mark) of language data should be
    introduced to a model for each additional parameter added to the overall size
    of the model they were training. These ratios—often referred to as *scaling laws*
    in scientific literature—define how much data you need to scale up a model in
    size.
  prefs: []
  type: TYPE_NORMAL
- en: In their 2020 paper,^([5](ch07.html#id920)) a team of OpenAI researchers posited
    that ~2 tokens of text should be used in training for every 1 parameter of an
    LLM. This 2:1 ratio became known as *Kaplan’s scaling law* (we’re guessing “Kaplan
    et al.’s scaling law” didn’t have a good ring to it) and was subsequently used
    to train models like GPT-3 and BLOOM (both models are 175 billion parameters in
    size and were trained on 300–350 billion tokens of text). In 2022, Google’s DeepMind
    published^([6](ch07.html#id923)) an alternate view on optimal scaling ratios called
    the Chinchilla scaling law. (This law is also known as Hoffman’s scaling law,
    named after the lead researcher; Chinchilla was a family of models published by
    DeepMind.) DeepMind’s researchers believed that OpenAI drastically underestimated
    the amount of data needed to optimally train an LLM...they felt the optimal scaling
    ratio to get *the best model performance for a given compute budget* was 20:1
    as opposed to the ~2:1 ratio. They went on to build a 70 billion parameter Chinchilla
    LLM using this scaling law. How did it do? At a mere 70 billion parameters, Chinchilla
    performed much better than larger models like GPT-3 (175 billion parameters).
    Looking back, we think Chinchilla was kind of like the SLM “OG” (as the kids say—it’s
    slang for original). This model is still quite big, but it isn’t a huge triple-digit
    billion parameter model, or bigger.
  prefs: []
  type: TYPE_NORMAL
- en: The research community’s initial goal focused on defining scaling laws to optimize
    the fixed up-front training costs for their models. But what about the recurring
    marginal costs across the rest of the model’s lifecycle? A super large model will
    be more expensive to host and inference. And guess who gets to incur those costs?
    That’s right, you! To reduce these costs, you need to reduce model size. To reduce
    model size while maintaining performance, you need to train on more (high quality)
    data.
  prefs: []
  type: TYPE_NORMAL
- en: And this is *exactly why* SLMs are capturing so much attention. Since inference
    and hosting costs are directly passed to model consumers, there was a bit of a
    delayed reaction. But as GenAI turned from a curiosity to a deployed technology,
    model providers have started optimizing their training setup to be as inference-efficient
    as possible, not merely training-efficient.
  prefs: []
  type: TYPE_NORMAL
- en: To create inference-efficient models, it can be cost-effective to train a model
    on a higher data ratio than what even the Chinchilla scaling law had in mind.
    At the time this book went to print, the scientific community had not converged
    upon a universal scaling law for inference-optimal models (and perhaps never will),
    but there are compelling industry examples of very performant SLMs that are trained
    on much larger amounts of data than the doctrines of Chinchilla or Kaplan would
    suggest (we show some of these scaling laws over time in [Table 7-2](#ch07_table_2_1740182051649140)).
  prefs: []
  type: TYPE_NORMAL
- en: In February of 2023, Meta open sourced its Llama 2 model series, trained on
    about 2 trillion tokens of training data (at the time, this was considered a massive
    amount of data). In the Llama 2 series, the 7 billion sized model had almost a
    300:1 scaling ratio! By August of 2024, with the release of Llama 3, Meta doubled
    (well, actually octupled) down and released its Llama3.1-8B model. This model,
    trained on over 15 trillion tokens has almost a 2,000:1 data density ratio and
    boasts even higher performance than the Llama 2 series.^([7](ch07.html#id930))
    Sensing a trend? Meta kept its SLM pretty much the same size, but improved performance
    significantly, just by training on more data!
  prefs: []
  type: TYPE_NORMAL
- en: Table 7-2\. Scaling laws over time
  prefs: []
  type: TYPE_NORMAL
- en: '| Date | Number training tokens/parameter | Scaling law |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1/23/20 | 1.7 | Kaplan |'
  prefs: []
  type: TYPE_TB
- en: '| 3/29/22 | 20 | Chinchilla |'
  prefs: []
  type: TYPE_TB
- en: '| 2/1/23 | 286 | Llama-2-7B |'
  prefs: []
  type: TYPE_TB
- en: '| 8/1/23 | 1875 | Llama-3.1-8B |'
  prefs: []
  type: TYPE_TB
- en: In fact, in the technical paper accompanying that release, “The Llama 3 Herd
    of Models,” Meta cited that its 405B parameter flagship model, also trained on
    ~15 trillion tokens, is “approximately compute optimal” from a training perspective,
    but that its smaller models were trained “for much longer than is compute-optimal.
    The resulting models perform better than compute-optimal models at the same inference
    budget.”^([8](ch07.html#id931)) Quite simply, while these smaller models were
    more expensive to train (trained for longer on more data), they are far more efficient
    to run at inference time. The result? Today, the Llama 3 models are some of the
    most popular open source models available, and we expect that when it arrives
    sometime in 2025, Llama 4 will be just as popular.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bringing this back to SLMs: with data ratios that require over hundreds of
    tokens of data for every parameter in a model, inference-optimized models and
    SLMs start to mean the same thing. It is near impossible to have a big, inference-optimized
    LLM. Given data acquisition costs and the amount of data available in the world,
    these data ratios are simply too expensive to support training inference-optimal
    LLMs that are hundreds of billions of parameters in size. We just don’t have enough
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: There is a real question of when will we hit the data ceiling? Today’s models
    are trained on upward of 15 trillion tokens, but to get there, model providers
    have basically had to plumb the entirety of the internet. And, as you will see
    in the next section, we don’t need large quantities of *any* data, we need volumes
    of *very high-quality* data, which is even more difficult to obtain.
  prefs: []
  type: TYPE_NORMAL
- en: Data quality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Can you imagine the song “Cecelia” without Garfunkel and just Simon? And could
    Hall & Oates have put anyone’s “Kiss on My List” if they didn’t start that song’s
    opening with a 1980s combination of keyboards and a cheesy mustache that sublimely
    screamed, “I got the romance covered? You just press the play button?” (Yes, younger
    readers...back then we had to press an actual clunky physical button.) And although
    we’re dating ourselves musically, it’s not only difficult to understand how great
    these songs could have been without the partnerships, it’s just as difficult to
    isolate the impact of data quantity from the impact of data quality in an LLM.
    Quality and data and great high-performing efficient models go together...just
    like Simon & Garfunkel and Hall & Oates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if you believe that the internet has only trustworthy data, that internet
    data has no bias, profanity, hate, lies, or anger...none of that, then you can
    probably stop reading this book. That belief is akin to eating a gallon of ice
    cream a day and wondering how your jeans shrank when you only wash them in cold
    water. When it comes to GenAI, the adage still applies: garbage in, garbage out!
    The reality *still* holds that the more you can do to curate the data used to
    train your model (both in terms of securing large quantities of it and with with
    high-quality labeled examples), the more performance you can pack into your model.
    And while there are some techniques around improving your model’s performance
    after it is trained—like retrieval-augmented generation (RAG) and more, these
    techniques all benefit from a high-quality data starting point (more on that in
    a bit).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Microsoft publicly credits data quality playing a critical role for enabling
    its (at the time) state-of-the-art (SOTA) Phi-2 2.7 billion parameter SLM that
    in some benchmarks outperformed larger models 25 times its size. But you could
    tell Microsoft had sniffed out this path forward before Phi-2 because it introduced
    its predecessor (Phi-1) to the world through a research publication^([9](ch07.html#id935))
    titled “Textbooks Are All You Need.” In this paper, Microsoft described how “high-quality
    data can even improve SOTA LLMs while dramatically reducing the dataset size and
    training compute.” And in the same way humans learn better from clearly laid-out
    textbooks, Microsoft’s findings support that textbook-quality training data that
    is “clear, self-contained, instructive, and balanced” results in better-performing
    LLMs that demonstrate better scaling laws; and of course, this enabled LLMs with
    the scale and performance of Phi-2 to become (at the time) SOTA. At the time of
    publishing this book, Microsoft had just released their fourth iteration of this
    SLM: Phi-4\. Similarly to Phi-1 and Phi-2, Microsoft cited “improved data” (among
    other training advancements) as a core driver to [Phi-4](https://oreil.ly/he3Nf)
    achieving strong performance relative to its size.'
  prefs: []
  type: TYPE_NORMAL
- en: Though we talked about this earlier in the book, it’s so important we thought
    we’d repeat it here because high quality data is critical to SLMs. While many
    model providers are transparent about the amount of data used to train an LLM,
    *very few* providers are transparent about the actual sources of data that were
    used to train *their* LLM. In fact, if you asked the most popular LLM providers
    what data they used to train their model, they either won’t be able to tell you
    or tell you it’s none of your business, to which you should reply, *“But this
    is my business!”*
  prefs: []
  type: TYPE_NORMAL
- en: 'The bottom line is that the highest quality datasets are long textbooks or
    other nonfiction books written and copyrighted by humans—not mid-starred or higher
    Reddit posts and other free-form information sources. High-quality data artifacts
    aren’t generic snapshots of web content put on public sites that automated crawlers
    can collect. The ugly truth behind many popular LLMs is that their inclusion of
    many of the best quality datasets (such as the Books3^([10](ch07.html#id938))
    corpus we first introduced you to in [Chapter 5](ch05.html#ch05_live_die_buy_or_try_much_will_be_decided_by_ai_1740182048942635))
    is unfortunately only available for use in model training because they were pirated
    and posted without author permission. Again, some of our own previous hard work
    was vacuumed into the inner bowels of multiple LLMs for all to take advantage
    of and others to profit from. We didn’t get a choice. We weren’t even asked; it
    just happened. And while we’re not filing suit (it’s not like we wrote some catchy
    bestseller titled *50 Shades of Big Data*that flew off the shelves and Hollywood
    wanted to make into a movie), there are a lot of people whose livelihoods and
    business differentiation were “stolen” to make the LLM you’ve also likely used.
    This all goes back to the value exchange discussion we had in [“How Do You Consume
    AI: Be Ye a Value Creator or a Value User?”](ch02.html#ch02_how_do_you_consume_ai_be_ye_a_value_creator_or_a_1740182046163502).'
  prefs: []
  type: TYPE_NORMAL
- en: Only transparent data collection and curation policies can ensure that the LLMs
    you’re evaluating for your business did not benefit from unethically sourced data.
    The takeaway? When evaluating SLMs, where data curation is critical for driving
    performance (and putting aside the legal ramifications), having a heightened awareness
    of how the data behind the model was sourced is crucial. Ask questions. Demand
    answers.
  prefs: []
  type: TYPE_NORMAL
- en: Domain specialization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Being the weekend athlete you are, you find yourself back at home with an ankle
    giving you mixed signals—it’s either auditioning for a spot on the soon-to-be-a-hit
    reality show, “So You Think You Broke Your Ankle,” or it’s just being dramatic
    with a sprain. Either way, it’s demanding ice and attention. Now it’s up to you
    to figure out what’s going on. To make this determination, do you ask the smartest
    person you know, or do you ask a doctor? (Don’t be cheeky...we know some of you
    just said aloud, “The smartest person I know *is* a doctor.”) While the smartest
    person you know might have amazing talents that span poetry, chemistry, philosophy,
    and more, you’re far better off asking a doctor, even better if they specialize
    in orthopedics. That doctor’s poetry skills be damned; when the question at hand
    is specialized in nature (your potentially fractured ankle), it is more important
    to ask a specialized expert than a general expert.
  prefs: []
  type: TYPE_NORMAL
- en: As it turns out, the same holds true for SLMs. And as you’ve likely figured
    out by now (because it’s a section in this chapter), there’s increasing evidence
    that smaller, specialized models can meet or beat larger general-purpose LLMs
    when evaluated on *specialized* tasks. And when we say a specialized model, what
    we really mean is a model that is trained on a significant amount of *domain-specific*
    data. For example, in late 2022, a team from Stanford announced [BioMedLM](https://oreil.ly/QrqbE),^([11](ch07.html#id945))
    a 2.7 billion parameter model trained on biomedical literature data. When evaluated
    on United States Medical Licensing Examination (USMLE) questions, a fine-tuned
    version of BioMedLM outperformed a similarly fine-tuned unspecialized model of
    the same size (GPT Neo) by 17%. When evaluated against an untuned model that was
    44 times bigger (Meta’s Galactica 120B model), BioMedLM outperformed it by almost
    6%. But the critical point is whether or not Galactica was good for the task at
    hand; unlike BioMedLM, Galactica’s size made fine-tuning it cost prohibitive.
    At just 2.7 billion parameters, the tiny BioMedLM LLM demonstrated it could maintain
    a specialized advantage while also allowing further customization for fine-tuning.
    This is a very early example of the impact of domain specialization in GenAI,
    but these examples have kicked off a huge area of research and application of
    specializing models on targeted use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Despite seemingly performing well on the medical-based benchmark in Stanford’s
    tests, Meta’s Galactica (specifically designed to help scientists) was launched
    into the scientific community with a big bang—until it came crashing down with
    a thud and was taken offline just three days after its general availability. Public
    experimentation brought to light many examples of bias, toxicity, and hallucinations
    that led to scientific nonsense. This is why it’s important to fully appreciate
    what we discussed in [Chapter 5](ch05.html#ch05_live_die_buy_or_try_much_will_be_decided_by_ai_1740182048942635).
  prefs: []
  type: TYPE_NORMAL
- en: Specialization can be especially important for “low resource” domains, areas
    where there isn’t a lot of data. For example, in [Chapter 4](ch04.html#ch04_the_use_case_chapter_1740182047877425)
    we told you how the IBM Z (mainframe) runs most of the world’s transactions. In
    the parlance of LLMs, something classified as *low resource* are those domains
    with very little data available for training AI systems. As you can imagine, COBOL
    is considered a *low-resource* language, as there is very little public domain
    enterprise-worthy COBOL data today, especially when compared to Python, SQL, and
    other popular coding languages (yes, lots of business logic is coded in SQL).
    But there’s a lot of COBOL out there running businesses—the most critical parts.
    In fact, Reuters estimates^([12](ch07.html#id949)) that today there are over 230
    billion lines of COBOL code—supporting over $3 trillion of commerce—actively running
    in enterprises.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For clarity, the IBM Z supports modern application development tool sets and
    methodologies like fully automated continuous integration/continuous deployment
    (CI/CD) pipelines using Jenkins and Zowe, Kafka streams, node.js, Kubernetes,
    Ansible, Terraform, and more. But there is a lot of critical business logic built
    a long time ago that was written in COBOL that is deemed mission critical.
  prefs: []
  type: TYPE_NORMAL
- en: For all those code-assist LLMs that scraped code repositories to build a code-tuned
    LLM, guess how much COBOL is available for use? For example, one popular dataset
    for training code-assist LLMs is GitHub Codespaces—it contains 1 terabyte of code
    from 32 different languages. But COBOL is not covered. Why not? Remember earlier
    in this book how critical your data is and how today’s LLMs aren’t built on enterprise
    data. Now think back to those transactions running on IBM Z (credit cards, ATMs,
    airlines). Do you think that code is just sitting there ready to be scraped by
    the world? Of course not! So how could an LLM help in this scenario?
  prefs: []
  type: TYPE_NORMAL
- en: Back in 2023, IBM Research trained a 20 billion parameter code model (called
    granite.20b.cobol) that specializes in COBOL. To specialize a model specifically
    on COBOL, the IBM Research team held aside separately acquired COBOL data, trained
    a general-purpose code model first, and then specialized that model by training
    it further on a dataset that was highly concentrated with high-quality curated
    COBOL data (this is just like your proprietary data waiting to be put to work).
    The end result? The COBOL-focused SLM model [significantly outperformed ChatGPT
    for COBOL completions on the CodeNet benchmark datasets](https://oreil.ly/H4Sgk).
  prefs: []
  type: TYPE_NORMAL
- en: The takeaway? Purpose-built foundation models with quality at their core means
    better performance and more efficiency. This concept will become hugely important
    in [Chapter 8](ch08.html#ch08_using_your_data_as_a_differentiator_1740182052172518)
    as we discuss how you can specialize pretrained models using your enterprise data.
  prefs: []
  type: TYPE_NORMAL
- en: Think About This When It Comes to Data Curation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Beyond the ethical considerations for data curation, understanding and appreciating
    data scaling laws and the impact of data quality and domain specialization on
    performance can help you find more cost-efficient SLM alternatives to bigger,
    less optimally trained, expensive-to-inference monster LLMs. As suggested before,
    older LLMs tend to be less data dense and, therefore, less inference efficient
    because they were trained back when the Kaplan and Chinchilla scaling laws first
    came out.
  prefs: []
  type: TYPE_NORMAL
- en: And while data quantity is most relevant for those training a model from scratch,
    for anyone trying to customize already trained models, as we cover in [Chapter 8](ch08.html#ch08_using_your_data_as_a_differentiator_1740182052172518),
    the lessons on data quality and domain specialization still apply.
  prefs: []
  type: TYPE_NORMAL
- en: Model Distillation—Using AI to Improve AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s talk about the second major technological innovation that is driving
    SLMs: model distillation. Model distillation is often used when you want the accuracy
    of a large neural network but need something more practical for real-time applications
    or devices with limited computational power. It’s really another technique to
    pack big-model performance into a small form factor; and while at first blush
    it might seem like a bit of a hack, it is actually an incredibly powerful tool.
    Model distillation is where a large frontier (big, expensive, state-of-the-art)
    model, such as Llama-3.1-405B, can instruct a smaller model, such as Llama3.1-8B,
    teaching it to behave like the bigger model.'
  prefs: []
  type: TYPE_NORMAL
- en: A great example of this is a would be trying to replicate Tootsie Tomanetz’s
    BBQ mastery. This 85-year old custodian by day and pitmaster by night is the legend
    behind the famous Hill Country BBQ (Texas).
  prefs: []
  type: TYPE_NORMAL
- en: She’ll outright tell you that if she gave you the recipe, you still couldn’t
    recreate what she does. We’ve all been there—trying to capture the magic of a
    grandparent’s cooking, only to realize it’s more than just ingredients; it’s a
    lifetime of love and technique.
  prefs: []
  type: TYPE_NORMAL
- en: For example, when asked what the right temperature was to start a beef brisket
    cook, she notes she has no idea...she just puts her hand on the smoker and goes
    by feel. (That reminded us of one of our grandmothers who used her finger as a
    pincushion.) But we’re willing to bet that if we could spend a week with Tootsie
    and pepper her (no pun intended) with nonstop questions, we could eventually learn
    how to make a pretty close to award-winning beef brisket. We surely wouldn’t know
    all the she knows. For example, we wouldn’t know how she makes her incredible
    sauces. But if you gave us another week of nonstop questions, we would likely
    be able to figure something pretty good there too. Next up, the chicken.
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, model distillation is like extracting all the essential knowledge
    from a heavyweight model into a more lightweight version, so you get similar performance
    but with less complexity.
  prefs: []
  type: TYPE_NORMAL
- en: In a lot of ways, model distillation is just a new, cheaper way to create training
    data. As LLMs become better and better at different tasks, they become powerful
    tools for generating training data that used to need to be defined by hand by
    an army of data annotators. To perform distillation, research scientists leverage
    a teacher model (the large all-knowing one) to generate a large amount of synthetic
    data that exemplifies a target set of behaviors the teacher model knows how to
    perform (like the cooking skill in our example). This synthetic data is often
    conversational in nature, representing question-answer (QA) pairs, or multiturn
    conversations. The synthetic data is then used to fine-tune the smaller (student)
    model, thereby imbuing the behavior patterns of the larger model into the smaller
    model. And while it may first appear this technique is only surface level, getting
    the small model to mimic the larger model’s performance has been shown to be incredibly
    powerful. In fact, back in 2023, in an early example of model distillation, researchers
    from the Large Model Systems (LSMYS) Organization distilled ChatGPT down into
    a 13 billion parameter model called Vicuna. Vicuna’s performance shocked the community
    when they first published their work. LSMYS reported^([13](ch07.html#id960)) that
    their distilled ChatGPT model “achieves more than 90% quality [referring to its
    responses] of OpenAI ChatGPT.”
  prefs: []
  type: TYPE_NORMAL
- en: The open source community, including Stanford and LSMYS, were some of the first
    innovators leveraging this technique and have now become “victims” of their own
    success. Model distillation has gotten so popular (and competitively threatening)
    that most frontier model providers (like OpenAI, Google, Anthropic, among others)
    have written restrictions into their model’s usage terms and conditions stating
    that their models cannot be used to improve the performance of other competitive
    models.
  prefs: []
  type: TYPE_NORMAL
- en: While this limits the commercial viability of models distilled by the open source
    community, it is gangbusters for LLM providers with access to large models that
    make for perfect caffeine-infused teachers. For example, through its partnership
    with OpenAI, Microsoft released [Orca and Orca-2](https://oreil.ly/pLXEK), highly
    competitive SLMs that benefit from distillations of GPT-4\. And Google’s Gemini
    Nano and Gemini Pro are Google’s distilled version of its larger [Gemini models](https://oreil.ly/mzSFf).
  prefs: []
  type: TYPE_NORMAL
- en: As this technique continues to improve, due consideration is needed on whether
    super-large models will ever be used for anything other than teaching smaller,
    faster, and more cost-efficient distilled models. For example, when NVIDIA released
    its 340 billion parameter model, Nemotron-4-340B-Instruct, the primary use case
    highlighted on the model card was to “create training data that helps researchers
    and developers build their own LLMs” (aka model distillation).^([14](ch07.html#id968))
    Hosting a 340 billion parameter model for running live inference could be incredibly
    expensive. You better have a pretty high value use case to justify that deployment.
    *But*, using the model once to generate synthetic training data for a smaller
    model is a much more palatable one-time fixed cost that enables the deployment
    of a cheaper, smaller, and more performant model.
  prefs: []
  type: TYPE_NORMAL
- en: And while closed frontier model providers currently have a “competitive moat”
    for their SLMs thanks to their teaching models, we think there is huge potential
    for disruption. Very large open source models, like Nemotron-4-340B-Instruct,
    Llama-3.1-405B, and most recently DeepSeek-R1, are proving to be powerful teacher
    models, eroding this advantage.
  prefs: []
  type: TYPE_NORMAL
- en: As noted earlier in this book, a Chinese AI lab, DeepSeek, open sourced its
    family of large 671 billion parameter Mixture of Experts (MoE) style LLMs, including
    the now famous DeepSeek-R1 model. The DeepSeek model release is fascinating from
    a number of different dimensions, the least of which was how, in response to the
    release, NVIDIA’s market cap dropped by $600 billion in one day^([15](ch07.html#id972))
    as spectators around the world were amazed at the performance and reasoning capabilities
    delivered by a Chinese AI lab for a reported training cost (which wasn’t fully
    understood by the press and those that reacted to it) of $5.6 million.
  prefs: []
  type: TYPE_NORMAL
- en: There is a lot to unpack here, particularly around the reported training costs,
    some of which we are going to discuss toward the end of this chapter as we cover
    the MoE architecture. But a large part of the DeepSeek release is actually an
    important story about the role of teacher models and model distillation—specifically,
    the collection of smaller “DeepSeek-Distill” reasoning models that were released
    alongside the much larger DeepSeek-R1 reasoning model.
  prefs: []
  type: TYPE_NORMAL
- en: In order for DeepSeek to build efficient SLMs with reasoning capabilities, they
    first used R1 to generate a large volume (800k samples) of examples of reasoning
    in math and code domains.^([16](ch07.html#id977)) Then they took that dataset
    and fine-tuned a set of open, third-party models produced by Meta (Llama) and
    Alibaba Cloud (Qwen), whose sizes ranged from 1.5 billion to 70 billion parameters,
    et voilà! A series of small DeepSeek-R1-Distill models with advanced math and
    code reasoning capabilities was born.
  prefs: []
  type: TYPE_NORMAL
- en: DeepSeek’s success in distilling reasoning capabilities into small models has
    inspired the open source community. Within days of the DeepSeek-R1 and DeepSeek-R1-Distill
    models being released, the open source community created distillation pipelines
    so that anyone could perform a similar distillation process using the SLM of their
    choice.^([17](ch07.html#id978)) Similarly, in less than one month, over 400 DeepSeek-based
    distillation datasets were posted to Hugging Face so that others can easily leverage
    DeepSeek’s outputs in their model development pipelines!^([18](ch07.html#id979))
  prefs: []
  type: TYPE_NORMAL
- en: In many ways, improving the open source community’s ability to create powerful,
    distilled models may be one of the biggest long-term impacts of the DeepSeek release—this
    is why we saw DeepSeek as more of an iterative open source AI event than a disruptive
    event. At the time of its release, DeepSeek-R1 was the most powerful teacher model
    available for open source model distillation. No doubt, its release also potentially
    puts pressure on other large, proprietary model providers to release open source
    versions of their models.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, you can easily see why large model incumbents might “fear” this process.
    Think about it. For a few thousand dollars (and a lot of AI expertise), a company
    could create its own proprietary distilled high-quality model that fuses its own
    data with frontier LLM performance. And, once trained, they could basically run
    these distilled SLMs for free. With the billions poured into big investment bets
    on anything GenAI, that’s bound to make a lot of investors nervous.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that distillation is not just limited to big teacher
    models improving much smaller student models. In fact, at the time this book was
    being written, OpenAI publicly disclosed that it was exploring whether DeepSeek
    illegally distilled OpenAI model IP into the large, 671-billion–parameter DeepSeek-R1
    model.^([19](ch07.html#id980)) Irony aside (more on that in the next chapter),
    this situation underscores the gravity of model distillation and the important
    role this technique will play moving forward in the future of AI development.
  prefs: []
  type: TYPE_NORMAL
- en: Think About This When It Comes to Model Distillation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When considering models that have benefited from distillation for your use case,
    the most important consideration (as alluded to before) is the terms and conditions
    under which this model is eligible to be used, *especially* in the case of open
    source models. You need legal involved here because a distilled model could potentially
    inherit contractual terms from the teacher model and the base model that was tuned.
    For example, under the Meta Llama 3 Community License Agreement, all models distilled
    from a Llama 3 model have specific naming requirements (the model’s name needs
    to start with “Llama 3”), and they need to be licensed under the same Llama 3
    license.^([20](ch07.html#id984)) In extreme cases, the model could potentially
    have been distilled from a teacher model in violation of the terms of that model’s
    provider, as OpenAI is investigating with DeepSeek-R1\. This is yet another reason
    why transparency of data sources remains critical so that consumers can do their
    own due diligence on whether a model is suitable for use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, it is critical that you understand the limitations of the teacher
    model and strategy that was used to do the actual distillation. To demonstrate
    what we mean, let’s take a look back at teacher model and distillation strategy
    of those DeepSeek-R1-Distill models:'
  prefs: []
  type: TYPE_NORMAL
- en: Teacher model
  prefs: []
  type: TYPE_NORMAL
- en: '*DeepSeek-R1.* As discussed above, this model demonstrates SOTA reasoning capabilities,
    but it also has a number of significant safety issues. A team from Cisco and the
    University of Pennsylvania found that DeepSeek-R1 “exhibited a 100% attack success
    rate, meaning it failed to block a single harmful prompt” in their automated jail-breaking
    attacks.^([21](ch07.html#id986)) Further, when asked factual questions for information
    about Tiananmen Square, the model declines (depending on where it is hosted) to
    respond. If asked, for example, “Do I need a passport to go to Taiwan?”, the model
    will immediately reply with: “According to the official policy of the Chinese
    government, Taiwan is an inalienable part of China’s territory” and “the Chinese
    government consistently upholds the One-China Principle and opposes any form of
    ‘Taiwan independence’ separatist activities.”^([22](ch07.html#id987))'
  prefs: []
  type: TYPE_NORMAL
- en: 'What is the same about children holds true for teacher models and students:
    *The apple doesn’t fall far from the tree*. DeepSeek-R1 is likely to pass along
    these same safety concerns and political principles along to the student models,
    so think carefully before running to deploy in production.'
  prefs: []
  type: TYPE_NORMAL
- en: Distillation strategy
  prefs: []
  type: TYPE_NORMAL
- en: '*Generate targeted supervised fine tuning (SFT) data for code and math reasoning
    tasks.*DeepSeek took a very targeted and intentional approach in its distillation
    pipeline, focusing on code and math reasoning tasks to the exclusion of all else.
    This makes sense, if you only ever plan on using the distilled models for code
    and reasoning tasks. But a study from IBM Research found that these distilled
    models have sacrificed all ability to perform as a general-purpose model, failing
    at even basic instruction-following tasks.^([23](ch07.html#id988))'
  prefs: []
  type: TYPE_NORMAL
- en: We dive into this further in [Chapter 8](ch08.html#ch08_using_your_data_as_a_differentiator_1740182052172518),
    but when taking advantage of model distillation, it is critical that your teacher
    model meets your requirements for both safety and performance and that the distillation
    approach you choose is aligned to your envisioned use of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Where Are We Going Next? Small Language Models...Assemble!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you can see, SLMs clearly have many advantages, but one of the most exciting
    applications for leveraging them is not as a standalone specialist, but rather,
    as a system of models working together to do something amazing. It’s kind of like
    a bunch of tiny ants teaming up and marching off with an entire hamburger patty
    from your picnic, living the dream and pulling off what seems like the impossible.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, several key advances are coming from the AI research
    world. These advances demonstrate that by combining their powers, small models
    working together can sometimes outperform any given large model and do so at a
    fraction of the compute cost. And while these SLMs could operate independently
    (with good results), they can become even more impactful when orchestrated to
    perform in concert (yes, using AI). AI helping AI. This more systems-based approach
    to models performing tasks can happen externally to the model, using tools like
    model routing. Or, through architectures like MoE, a system of models with routing
    between experts that occur intrinsically within the model. Let’s get into both
    of these topics next.
  prefs: []
  type: TYPE_NORMAL
- en: Model Routing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'On average, a bigger language model is going to perform better than a smaller
    language model on a given task. But, as you learned in this chapter, SLMs can
    operate as specialized experts that can outperform a big LLM if the task at hand
    is specialized in nature (like in the COBOL example). But even without intentional
    domain specialization, there can be unexpected variability in model performance
    across the many tasks you’re likely to send to your AI. This could be the case
    for many reasons: a model’s architecture, nuances in training data, parameter
    settings, data preparation, data sourcing, its alignment strategy...all of this
    (and more) could predispose any given smaller model to perform better on a task,
    independent of model size. The problem around the benefits of SLMs is that their
    performance advantages can be unpredictable, particularly if you don’t know what
    data they were trained on, making it difficult to predict which SLM you should
    use for your task.'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, you could run every data point through every SLM you have to try
    and figure out which one(s) will work best. Don’t get us wrong—usually, putting
    the work in for something great is a good thing—but for this, you want something
    different. If you could somehow predict up front whether a smaller model would
    be suitable for your use case’s task list, then you could use that smaller model
    instead and save your company the extra inference and latency costs that might
    accompany a big oversized LLM for your needs. Quite simply, you’d optimize the
    usage of the big LLM to when you actually need it, instead of making the most
    expensive option the default or only choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'We do this all the time in our travels. Typically, we’re living the Uber X
    life—budget travel. But Uber Black (although it leaves us with some explaining
    to do to our auditors) is the go-to on a tight schedule because it’s there in
    minutes, they aren’t going to stop for gas on the way, and they won’t accept your
    ride while they finish another—not to mention the chewing gum is individually
    wrapped, not stuck to the floor. Now apply that logic to your AI: use the expensive
    option *only* when you truly need it.'
  prefs: []
  type: TYPE_NORMAL
- en: A group of researchers at the MIT-IBM Watson AI Lab were looking for answers
    to the question, “Can a bunch of smaller models outperform a large model?” Even
    back in 2023, when SLMs were just getting started, one paper^([24](ch07.html#id993))
    proposed an approach where a model-routing algorithm sits as an orchestrator,
    directing inference requests to whichever model the router predicts would be best
    for a given task.
  prefs: []
  type: TYPE_NORMAL
- en: In this deployment pattern, you could have an ecosystem of models—some are small
    and specialized, some are larger—to maximize the chances that a model router can
    find the optimal model to support a given task while defraying your costs every
    time the router selects a smaller model. [Figure 7-2](#fig-7-3) shows this.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a router  AI-generated content may be incorrect.](assets/aivc_0702.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-2\. An AI router that understands the capabilities of models in its
    library directs a given inference request to the best model able to perform the
    task at hand
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In [Figure 7-2](#fig-7-3), you can see a new inference request for a given
    input comes into the ecosystem (new data point). A router (trained on benchmark
    data) understands what model can best perform the task at hand and routes the
    work to it. You can see the benefits here, right? Every time the router pushes
    a task to a smaller model (our example has a library of three tier-sized models:
    small, medium, and large), you save money, reduce latency, and help the environment.'
  prefs: []
  type: TYPE_NORMAL
- en: This begs the question, how does this model router know which model in the library
    will perform best? There are different approaches. The MIT-IBM team took an approach
    that leveraged predefined (HELM^([25](ch07.html#id994))) benchmark data for each
    model in order to first train the AI router on the different types of tasks each
    model could perform satisfactorily (note that this approach could also work with
    any set of relevant benchmarks defined by a user).
  prefs: []
  type: TYPE_NORMAL
- en: As it turns out, training the AI router is a fairly trivial task. At its core,
    the router is just a classification model. Given a representative task, the router
    classifies whether the model will perform satisfactorily or not. Once trained,
    the router then compares the similarity between any new task and the known benchmarks.
    If a new task is similar to a benchmark task that a specific model has proven
    to perform well at, then the router is more confident that this specific model
    will perform well on that new task, too. For example, if a specific model was
    really good at Q&A’ing medical questions about your broken or sprained ankle,
    it will probably be pretty good at your broken or sprained wrist you got fishing
    last week (seriously, take it easy).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the benchmarks you’re using are very dissimilar to the tasks being routed
    to the models, you could also update the router’s logic by giving it a small amount
    of labeled data that represent the tasks you’re trying to run so that the router
    can get updated knowledge on model performance for that specific task. The router
    can then use that information to route future requests (the same ones or similar)
    to the most appropriate model in your library.
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate the performance of the model router, the MIT-IBM team ran an
    experiment using a library comprised of over a dozen models that ranged from 3
    billion to 70 billion parameters in size (so there was a great representation
    of small, medium, and large models, despite what our example in [Figure 7-2](#fig-7-3)
    shows). The team evaluated^([26](ch07.html#id995)) a bunch of different tasks
    that make up Stanford’s HELM evaluation benchmark. The first pass was *without
    a router* to determine which model in the library could perform the tasks in the
    HELM benchmark the best.
  prefs: []
  type: TYPE_NORMAL
- en: 'It shouldn’t be too surprising to find out which model won. As we said before:
    *on average*, a large model should perform better than individual smaller models
    for all the tasks. And, as shown in [Figure 7-3](#ch07_figure_3_1740182051640296),
    that was indeed the case for this test. The largest model in the library (Llama-2-70B)
    achieved 68% accuracy (higher is better). And just like that, Llama-2-70B became
    the baseline for which we could compare how our AI-powered model router would
    do with a mixed-model approach. It’s important to understand this, so at the risk
    of repeating ourselves, we’ll say it more explicitly: this benchmark is not *measuring*
    the accuracy of the model router; it is measuring the accuracy of the models that
    the router selects. Quite simply, this means that if you used the Llama-2-70B
    model for every task in the HELM benchmark, you would get an average performance
    of 68%.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph with numbers and a number of marks  Description automatically generated
    with medium confidence](assets/aivc_0703.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7-3\. No router used: on average, the large model performed the best,
    at around 68% performance (higher is better)'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now it’s time to unleash the router! [Figure 7-4](#ch07_figure_4_1740182051640324)
    shows what happens when we allowed the router to send various tasks to different
    models in the library. Remember, the entire library *did not* have a single model
    over 70 billion parameters. Basically, the router (with its ability to route a
    task to a library of small, medium, and large models) *outperformed* the large
    model on its own! Specifically, the overall performance was about 72% when the
    router could access the library of models, compared to 68% when using one big
    LLM alone. But there is more to the story in [Figure 7-3](#ch07_figure_3_1740182051640296);
    to tell it, you need to focus on the vertical bar graph within the results.
  prefs: []
  type: TYPE_NORMAL
- en: When the router was in play, only 56% of tasks were routed to the big Llama-2-70B
    model. The rest of the tasks got routed to the smaller, more efficient, and obviously
    higher-performing models for the tasks routed to them (a mixture of medium and
    small models).
  prefs: []
  type: TYPE_NORMAL
- en: 'The takeaway? Using a model router, we observed improved *overall* accuracy
    and efficiency. Remember, every time a task gets routed to a smaller model, it’s
    more efficient to run it. Lower costs. Better performance. Lower environmental
    impact. What’s not to love? But like any good leader who challenges their teams
    for their best, one question remained: can you do better?'
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph of a task  Description automatically generated with medium confidence](assets/aivc_0704.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-4\. Using a router to route to our SLM and LLM library for the tasks
    at hand resulted in better performance
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To answer that question, the research team started with a hypothesis: what
    if the model library was limited to *only models that were equal to or less than
    13 billion parameters in size*? These are true SLMs—that sweet spot of SLMs that
    we talked about earlier.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 7-5](#ch07_figure_5_1740182051640346) shows the answer to this question,
    and it’s worth some extra commentary.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph of a number of tasks  Description automatically generated with medium
    confidence](assets/aivc_0705.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-5\. Limiting the model library to 13 billion parameters delivers impressive
    benefits
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The obvious takeaway from [Figure 7-5](#ch07_figure_5_1740182051640346) is that
    the results of the router with an SLM-only library (70%) aren’t as good as the
    larger library comprised of all 15 large, medium, and small models, including
    the 70 billion LLM (72%). But some things caught our eye right off the bat and
    should have you throttling up your attention span (we know, we’re deep into the
    chapter) from “somewhat curious” to “we have your full attention.”
  prefs: []
  type: TYPE_NORMAL
- en: 'First, while the library of all models (up to and including the 70 billion
    one) performed better, the SLM-only library (models 13 billion parameters and
    under) outperformed the baseline (the big 70 billion LLM on its own): 70% versus
    68%. Second, the SLMs don’t need the biggest most expensive and scarce GPUs to
    run them. That means you get more deployment options. And of course, giving up
    only 2% performance over the best result, and gaining 2% performance over the
    baseline, gives you even lower overall costs (both money and environmental)!'
  prefs: []
  type: TYPE_NORMAL
- en: Think About This When It Comes to Model Routing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The appeal of model routing isn’t just maximizing performance at lower overall
    cost. There’s a second important benefit: having the ability to, before the inference
    (a priori), predict model performance on a task across different models of different
    sizes. Why is this important? As a leader who understands this technique, you
    can make more informed decisions about the cost-benefit trade-off between different
    models and the suitability of any given task for automation.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, if an automation task you are preparing as a GenAI use case is
    very complicated—and the only models predicted to perform well are the very large,
    expensive ones—then you might decide that automating that task doesn’t result
    in large enough cost savings to justify using a model of that size. On the other
    hand, perhaps you have a low-value task that wasn’t giving a strong signal on
    your GenAI use case radar, but it’s predicted to be easily automated with a fairly
    small model. Suddenly, you’re economically incentivized to shift it left and automate
    that task. When you think about that whole flip of +AI to AI+ mindset we discussed
    in [Chapter 1](ch01.html#ch01_ai_to_ai_generative_ai_and_the_netscape_moment_1740182043982974),
    where you suddenly see your business as discrete pieces of workflows and business
    logic, we think model routing can really help here. How so? Those discrete pieces
    of logic likely aren’t going to need a super large model, so they can be leveraged
    for the mundane rote shift tasks that are bound to be discovered during this process.
    We envision a near-future world of LLMOps, driven by model routers, where performance
    and cost savings are dynamically monitored, and a router actively sends workloads
    to different models to maintain a desired cost per performance balance defined
    by an operator.
  prefs: []
  type: TYPE_NORMAL
- en: Mixture of Experts (MoE) Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have talked about how groups of models of various strengths and
    expertise can work together through an external model router, let’s take this
    idea one step further and talk about how this same concept can be applied internally
    within a model, using a relatively new type of LLM architecture: Mixture of Experts
    (MoE).'
  prefs: []
  type: TYPE_NORMAL
- en: Think of LLM architectures as the technical strategy that a researcher uses
    to encode all of the training data into parameters for their model. Almost all
    modern LLMs trained today are trained using a “transformer” type of architecture
    (which we talk about in [Chapter 8](ch08.html#ch08_using_your_data_as_a_differentiator_1740182052172518)).
    Since its initial release, many types of transformer architectures have emerged.
    The most popular is the “dense” style of transformer models, used by many model
    providers like Meta with its Llama model families. However, more recently, new,
    more efficient types of transformer architectures, like MoE, have started to gain
    popularity, and that’s what we cover in this section.
  prefs: []
  type: TYPE_NORMAL
- en: In an MoE-based model, buckets of parameters, referred to as “experts,” are
    trained to operate fairly independently of one another. These experts can either
    be specialized by the model developer, or can be generalists in nature. Leveraging
    the same intuition we covered in the previous section, only a subset of the experts
    are used at inference time, making these models *wicked* fast (can you tell one
    of the authors is a Bostonian?). This is because the inference cost is now approximately
    reduced to the size of the experts being run, not the entire size of the model.
    How does the model know which regions of the model to “activate” for a given request?
    You guessed it. A model router, but *this* time the model router is internal to
    the model, not something that can be used independently with other models like
    in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: There are some important gotchas with MoE inference efficiency. If you are running
    inference in large batch jobs, as is common for production workloads, this efficiency
    advantage goes down because you will need to load more and more of the experts
    into memory depending on all the samples that are batched. But if you are experimenting
    locally, or running things in a single batch, or batching across very homogenous
    data that will always use the same experts, these MoE models can be quite inference-efficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'In January of 2025, the MoE architecture got broad attention when DeepSeek
    released its 671 billion MoE model. But DeepSeek wasn’t the first to release an
    MoE model. The French AI Lab, Mistral AI, made headlines with the release of one
    of the first high-performing MoE models: Mixtral 8x7B (we think the name is great,
    Mistral + mixture) all the way back in December of 2023.'
  prefs: []
  type: TYPE_NORMAL
- en: MoE models are more efficient to run at inference time, but they are also more
    economical to train. DeepSeek brought this point home when it published that it
    was able to train its base model, DeepSeek-V3-Base (which was later post-trained
    to create DeepSeek-R1), for $5.6M. But there are a couple of important things
    to note when interpreting this staggeringly low reported training cost.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, just as any lawyer will tell you, make sure you read the fine print!
    When DeepSeek reported its training cost in the DeepSeek-V3 Technical Report,
    it included a very important caveat: “Note that the aforementioned costs include
    only the official training of DeepSeek-V3, excluding the costs associated with
    prior research and ablation experiments on architectures, algorithms, or data.”^([28](ch07.html#id1010))'
  prefs: []
  type: TYPE_NORMAL
- en: What does this translate to in plain speak? Well, to train LLMs, there is a
    lot of brute-force trial and error that is required in order to optimize performance.
    That means for any one model that is released, there might be hundreds or thousands
    of smaller models that are trained in advance, testing out different data mixture
    efficacies, searching through different hyperparameter settings, etc. These development
    costs can easily be 10 times or more compared to the final, one-and-done training
    cost of the model. So, while what DeepSeek did is still impressive, the true training
    costs of its models were probably far less earth-shattering than some of the press
    coverage may have let on.
  prefs: []
  type: TYPE_NORMAL
- en: Think About This When It Comes to MoEs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Research and innovation with MoE-style models is still evolving. As DeepSeek
    showed, the world is getting better and better at training MoE models more efficiently
    and innovating on how to bring experts together. At the end of the day, we are
    most bullish on this architecture because its more efficient training costs will
    allow for more rapid iteration, hopefully continuing to drive innovation in this
    space.
  prefs: []
  type: TYPE_NORMAL
- en: We see a significant innovation runway for MoEs with respect to configurable
    inference efficiency. Today, Mixtral is designed to call two experts at inference
    time. To enable cost-efficient inferencing in the future, we envision this technology
    evolving to dynamically change the number of experts called at inference time,
    allowing users to quickly adjust their cost/performance trade-off for a given
    task and use case. This is like the model routing use case in the previous section,
    where more complicated tasks could call for the justified use of a bigger more
    expensive model. In our crystal ball, we see MoE models operating in the same
    manner where complicated tasks could call for using more experts at inference
    time (perhaps all eight and not just the two in our running example).
  prefs: []
  type: TYPE_NORMAL
- en: No matter where this technology evolves, it’s all about the flexibility for
    model consumers that makes it so exciting. When you reduce your dependency on
    one large model and harness the power of smaller models (or regions of a model)
    working together, you have opportunities to tailor model expertise for your use
    cases all the way to optimizing the cost-performance trade-off to best meet the
    needs of your business. And now you know why one model couldn’t possibly rule
    them all.
  prefs: []
  type: TYPE_NORMAL
- en: Agentic Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve given you some high-level details about agents throughout this book. In
    the final section of this chapter, it’s time to delve into them a little deeper.
    When we talk about agents, we often are referring to an implementation of an LLM
    where a user provides a goal-oriented instruction, and then the LLM independently
    comes up with a series of tasks (and subtasks) to achieve that goal. It then iterates
    over those tasks, often leveraging tools and reflection loops to complete each
    task. An agent can even be comprised of multiple different LLMs, each performing
    one of those tasks. Because a complex task is broken down into smaller, simpler-to-accomplish
    steps, the door is often opened for smaller models to tackle simpler tasks in
    tandem with larger models performing the more difficult tasks (like coming up
    with the list of tasks that need to be done to achieve the goal in the first place).
    And often, there is some sort of model routing happening behind the scenes where
    an LLM is selecting another LLM to outsource a subtask to, based on a catalogue
    of LLMs to choose from.
  prefs: []
  type: TYPE_NORMAL
- en: While many things agents do today can be done manually and in a static manner,
    agents deliver productivity breakthroughs by further shifting left more of the
    work, which saves time and boosts efficiency. For example, if you headed up a
    clinical trial, you could use an LLM to identify suitable trial candidates, but
    then you’d have to manually manage visit scheduling and coordination (tasks like
    sending reminders, rescheduling meetings, and automatically reminding everyone
    in the trial about key dates or requirements, such as a morning fast). With agents,
    you shift more of the work left because not only can an agent come up with a great
    start toward the perfect clinical trial profile, but they can even help come up
    with a proposed set of compliance reminders and even schedule sample collections
    with calendar invites for participants! What’s more, agentic systems are not stuck
    in time, and they can adapt in real time.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine attaching an agent to a supply chain management problem—you now have
    AI with the ability to understand a weather event and optimize a plan (understanding
    road closures and such) to get much-needed product into stores. And as you will
    find out, agents can even learn along the way. Quite simply, the dynamic nature
    of agents helps a company get more work shifted from +AI to AI+ and keeps them
    agile. This space keeps changing, so you’re going to want to follow it closely.
  prefs: []
  type: TYPE_NORMAL
- en: Now think back to what you learned in [Chapter 4](ch04.html#ch04_the_use_case_chapter_1740182047877425)
    about LLMs with a RAG pattern. That was one way of not just making your enterprise
    data available to an LLM, but also how to provide the LLM with updated information.
    In this pattern, a larger system injects information from an external source (like
    a database) directly into the prompt before runtime. This was also the basis of
    the “talk to a document” use case in [Chapter 4](ch04.html#ch04_the_use_case_chapter_1740182047877425).
    With the introduction of agents, AI gets even more powerful and can handle more
    complex tasks because they have the ability to call tools (this process is referred
    to as tool calling) outside of the LLM to assist them with their work.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*Tool calling* is the term referred to when LLMs are given the ability to interact
    with external tools, apps, and other systems—all to enhance their functionality.
    For example, an agent’s LLM might perform a tool call to get the weather for a
    particular location to help finish a task or reach out to a calculator to perform
    certain types of calculations for precision or even to offload the work from the
    LLM. Simply put, tool calling extends LLMs with capabilities beyond generating
    text, images, and the other things they are known for that we’ve covered in this
    book.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Perhaps the best way to appreciate the power of agents is to reflect on how
    you typically work with an AI-powered chatbot today. The flow looks something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: human prompt → LLM response → human prompt → LLM response → ...
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this traditional system, your prompt might go back and forth in the simple
    manner shown above, but it can trigger multiple calls that operate in the backend,
    unseen by you, before a response is provided back. For example, a RAG pattern
    appends data to a prompt from a data source that was connected to this flow by
    an administrator. But even when enhanced in this manner, the information that
    is available to the LLM supporting a RAG-based chatbot is also predetermined by
    its creator (like through a connection to a vector database like Chroma). In this
    nonagentic architectural pattern, the LLM involved *is not* given the ability
    to work “behind the scenes” on its own—it interacts with you on a continual basis
    as you go back and forth and back and forth, trying to complete your task.
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast, agentic implementations provide LLMs with more freedom and power.
    In this architectural pattern, LLMs are allowed to reason about what information
    is needed to perform a task that helps achieve a goal, like, “Put together a plan
    to increase the net promoter score (NPS) for my car dealership’s service center.”
    The LLMs part of this pattern are provided with access to tools (more on this
    in a bit) that can be called on the backend to obtain up-to-date information,
    optimize workflows, create subtasks to tackle the challenge piece by piece, and
    even call some scripting language (like VBScript) to create some PowerPoint charts
    of what it finds! This is all done autonomously by the agent (or agents) to achieve
    the complex goal. An agentic workflow might look like:'
  prefs: []
  type: TYPE_NORMAL
- en: human prompt → primary LLM response (hidden to user) → primary LLM tool call
    (hidden to user) → LLM response (hidden to user, shown to secondary LLM) → secondary
    LLM response (hidden to user, provided back to primary LLM) → primary LLM response
    (shown to user) → human prompt → ...
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As an end user chatting with an agentic system, you might feel as if you are
    just querying one big, multifunctional super LLM behind the scenes. But the reality
    is you’re likely working with a system of bigger and smaller models working together
    behind the scenes in order to efficiently solve your objective. (Like we said,
    you can use multiple LLMs in an agentic workflow. This should really give you
    a feel for just how significant of a role SLMs can play in this domain.)
  prefs: []
  type: TYPE_NORMAL
- en: AI agents can encompass a wide range of functionality beyond language, including
    decision making, problem solving, interacting with external environments, and
    executing actions. And these agents can be deployed in various applications to
    solve complex tasks in enterprise contexts, from software design and IT automation
    to code-generation tools and conversational assistants. We like to think of agents
    as digital interns with lots of ambition. Arm them with goals, tools, and tasks,
    and their smarts *may* often surprise you—but like we said earlier, AI isn’t magic.
  prefs: []
  type: TYPE_NORMAL
- en: What’s Your Reaction to This Agent in Action?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AI agents are systems-based implementations of LLMs that leverage planning,
    reasoning, and tool calling to solve problems and interact with external environments.
    Behind the scenes, there might be a single LLM handling all the work, multiple
    instances of the same LLM working on a task, or a combination of different LLMs.
    A good agentic framework will let you mix and match different LLM providers, which
    includes fine-tuned models that you might have customized with your own data.
    For example, you might pull Anthropic’s Claude Sonnet for desktop controls but
    augment that with a Granite-based model enhanced with your business data—the two
    of them might work in concert to figure out an event and fill in a form. Very
    cool!
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 7-6](#ch07_figure_6_1740182051640368) gives you some insights into
    an agent that we tasked with writing a blog about the impacts of inflation on
    Canadian housing prices in 2024 and then come up with some social media postings
    to reference our blog.'
  prefs: []
  type: TYPE_NORMAL
- en: We set up several agents that are invoked from our task. One of the agents took
    on the persona of a Lead Market Analyst. We won’t detail this for each agent,
    but this particular agent’s *goal* was to conduct real-time analysis of financial
    news on our topic of interest to help guide content creation. We also gave this
    agent a *backstory*, which made it take on the persona of a market analyst from
    a reputable firm who dissects market trends to pass on to our agentic writers.
    We gave this information to the agent framework in YAML files.
  prefs: []
  type: TYPE_NORMAL
- en: Notice in [Figure 7-6](#ch07_figure_6_1740182051640368) that our Lead Market
    Analyst agent literally tells us how it will get started by searching the internet
    for articles related to the topic involved in its task.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](assets/aivc_0706.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-6\. Our agentic workflow thinking about some of the steps it needs
    to do to write our blog and point to that blog on social media
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As shown in [Figure 7-7](#ch07_figure_7_1740182051640388), if the task is complicated,
    the agent might make multiple internal loops of tool calls and internal reasoning
    before returning a final answer. In this case, the agent has finished finding
    its sources and now starts to look at the data it’s collected. Notice how it has
    access to tools to help it.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](assets/aivc_0707.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-7\. The agent starts to look at the contents of the information it
    found
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Finally, this particular agent finishes its work and returns the findings shown
    in [Figure 7-8](#ch07_figure_8_1740182051640408). It seems evident that our agentic
    workflow has the source information and summary points that will make for a great
    blog posting!
  prefs: []
  type: TYPE_NORMAL
- en: '![A close-up of a text  Description automatically generated](assets/aivc_0708.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-8\. The key points to make in our blog posting
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Ultimately, there is a lot more flexibility added to this flow, giving the models
    powering your agents the ability to plan out tasks, research external information,
    and more.
  prefs: []
  type: TYPE_NORMAL
- en: As we alluded to earlier, we built multiple agents on the backend, each specialists
    in different tasks, and we put them all to work on this objective. One agent has
    the persona of a content creator; another is a creative director, another is a
    social media guru, and finally, one is a math guru. We suggest that as you go
    create your own digital employees in your agentic workflows, look to the very
    job postings you might make for such jobs. In there will reside all kinds of backstory
    skills you want these digital employees to be able to do. When all was said and
    done, our agents wrote us the (presumably; we of course looked at the data it
    collected) well-researched blog that is shown in [Figure 7-9](#ch07_figure_9_1740182051640427).
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a white text  Description automatically generated](assets/aivc_0709.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-9\. The start of our final blog
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Finally, look at the social media outreach messages our agentic workflow came
    up with (see [Figure 7-10](#ch07_figure_10_1740182051640455)) to amplify our article.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a white paper with black text  Description automatically
    generated](assets/aivc_0710.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-10\. The agentic workflow didn’t just write our blog; it also composed
    social media outreach messages to direct traffic to our blog posting
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We’ll admit we got a touch lazy looking back at the output in [Figure 7-10](#ch07_figure_10_1740182051640455).
    How so? We gave the same skills to our social media writer agent for posting on
    all platforms. Looking back, we should have given this agent broader skills and
    knowledge so it knew how to better mix tone and style depending on the social
    media outlet. After all, X (Twitter) is limited to 240 characters, so our agent
    worked hard to keep all of the postings it generated short (which could have been
    part of our assigned goal, but wasn’t). As another example, Instagram posts could
    be a lot less formal than LinkedIn. Notice how in [Figure 7-10](#ch07_figure_10_1740182051640455)
    the agent used emojis for the X post, which are more commonplace because of its
    limits than on LinkedIn.
  prefs: []
  type: TYPE_NORMAL
- en: There was a lot of other cool stuff going on behind the scenes than we could
    show you here. For example, our agents had their own version of the revered *Who
    Wants to Be a Millionaire?* game show’s Phone-a-Friend lifeline—only these friends
    were website crawlers, searchers, and scrapers, pieces of Python code (we used
    its Pydantic library to parse the data, among other libraries), and other digital
    labor agents—the best part is they never put you on hold or say, “Sorry bros,
    you stumped me!”
  prefs: []
  type: TYPE_NORMAL
- en: Do we think Figures [7-9](#ch07_figure_9_1740182051640427) and [7-10](#ch07_figure_10_1740182051640455)
    were better than a human? That wasn’t the point...because we think that handing
    a human this information would give them a productivity boost if their job was
    to perform these very tasks. We shifted-left the work! Now bring the human element
    to make it really land.
  prefs: []
  type: TYPE_NORMAL
- en: A Little More on Agents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In an agentic system, an agent often has access to more advanced forms of grounding
    context, like memory buffers that store information from past work and tasks it
    was asked to perform. An agent’s ability to store past interactions in memory
    and plan future actions encourages a personalized experience and comprehensive
    responses. But it gets better—these agents learn over time. For example, if there
    is a certain style you want a report written in, or a sauciness level for an Instagram
    post versus one on LinkedIn, agent memory can persist these preferences, and that’s
    a great example of a more personalized experience and comprehensive response.
    In our example above, had we further instructed our agent not to make the blog
    posting too chunky with too many short sections, it would learn that preference.
    Contrast this with a traditional RAG chatbot type setting where a model starts
    fresh each time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although AI agents are autonomous in their decision-making processes, as we
    alluded to earlier, they require goals and environments defined by humans.^([29](ch07.html#id1020))
    There are four main influences on autonomous agent behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: The team that designs and trains (or more likely, uses or fine-tunes) the underlying
    LLM(s) used in the agentic workflow. As you’ve learned about in this book, it’s
    more likely that you use an LLM to support your agents someone else built, and
    depending on the task it needs to perform, you may have steered it to your business.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The team of engineers that build the agentic AI system. These are the folks
    who are defining the tools to which the system will have access.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The team of developers that configure the agent and provide the user with access
    to it and the tools. These folks work in conjunction with the business to help
    create the agentic persona.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The user who prompts the AI agent with specific goals and tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you saw in the example earlier, given a user’s goals and the agent’s available
    tools, the agentic workflow created a plan that included tasks and subtasks to
    accomplish the complex goal it was handed. If this were a simple task (like writing
    a form letter), planning wouldn’t be a necessary step. Instead, the agent could
    iteratively reflect on its responses and improve them without planning its next
    steps. That was not the case with our blog posting. Recall in Figures [7-6](#ch07_figure_6_1740182051640368)
    and [7-8](#ch07_figure_8_1740182051640408) that our agent’s logic showed us some
    insights into its reasoning and planning for how to solve the task we gave it
    (there was a lot more thinking, reasoning, and planning we didn’t show you).
  prefs: []
  type: TYPE_NORMAL
- en: AI agents base their actions on the information they perceive. Often, AI agents
    do not have the full knowledge base needed for tackling all subtasks within a
    complex goal. For example, our agents didn’t have knowledge on the impact of inflation
    on housing. To remedy this, our agents used their available tools (in our example,
    an agent went out and searched the web for information). These tools can include
    external datasets, web searches, APIs, and even other agents. After the needed
    information was retrieved using these tools, our agent updated its knowledge base.
    This means that each step of the way, an agent can reassess its plan of action
    and self-correct.
  prefs: []
  type: TYPE_NORMAL
- en: While our previous example showcased writing, imagine something even more complex,
    such as planning your next vacation. You task an AI agent with predicting which
    week in the next year would likely have the best weather for a surfing trip in
    Hawaii. Since the LLM model at the core of the agent does not specialize in weather
    patterns, that agent would gather information from an external database (versus
    a web search) comprised of daily weather reports for Hawaii over the past several
    years. Despite acquiring this new information, the agent still can’t determine
    the optimal weather conditions for surfing, so the next subtask is created. For
    this subtask, the agent communicates with an external agent that specializes in
    surfing. Let’s say that in doing so, the agent learns that high tides and sunny
    weather with little to no rain provide the best surfing conditions—not just sunny
    skies. The agent then combines the information it has learned from its tools to
    identify those best patterns to put some “maika’i loa” (awesome in Hawaiian) into
    your surfing vacation. It comes back with a prediction on what weeks in the year
    are likely to have high tides, sunny weather, and a low chance of rain. These
    findings are then presented to you, or perhaps the agent even goes on to book
    your trip.
  prefs: []
  type: TYPE_NORMAL
- en: How Agents Are Built
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At their heart, agents are system-based implementations of an LLM. In this
    implementation, you will have an LLM with a set of operating instructions on how
    to plan and how to make external tool calls (be that a web search or a prompt
    to another LLM, etc.), embedded within a broader system that performs key, non-GenAI
    activities, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Parsing an LLM’s output, searching for tool call invocations that the LLM will
    trigger
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing an external API based on the identified tool call
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing a tool response and injecting it directly back into the LLM’s conversation
    history with the proper formatting (like converting JSON to written text or Markdown)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling advanced memory functions, such as conversation history manipulation
    and storage of key artifacts in LLM-accessible memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, this is a complicated system that the LLM operates in, often
    resulting in complex, multipage prompts summarizing the operating instructions
    for an agent (or group of them).
  prefs: []
  type: TYPE_NORMAL
- en: While there is not one standard prompt for instructing AI agents, several paradigms,
    also known as *agent architectures*, have emerged for solving multistep problems
    and determining how to trigger planning, tool usage, and memory within an LLM
    workflow.
  prefs: []
  type: TYPE_NORMAL
- en: ReAct (Reasoning and Action)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the agent architecture we used in our blog example. It lets users instruct
    their agents to “think” and plan after each action taken...and with each tool
    response to decide which tool to use next. These think-act-observe loops are used
    to solve problems step-by-step and iteratively improve upon responses.
  prefs: []
  type: TYPE_NORMAL
- en: Through the prompt structure, agents can be instructed to reason slowly and
    display each “thought”^([30](ch07.html#id1025)) (you saw this in our blog example).
    An agent’s verbal reasoning gives insight into how responses are formulated. In
    this framework, agents continuously update their context with new reasoning. This
    can be interpreted as a form of chain of thought (CoT) prompting.
  prefs: []
  type: TYPE_NORMAL
- en: ReWOO (Reasoning WithOut Observation)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The ReWOO method, unlike ReAct, does all the planning up front. This can be
    desirable from a human-centered perspective since the user can confirm the plan
    before it is executed. This is important because at some point someone has to
    pay to spin up the resources to run all of this—it’s not a bad approach to know
    what’s going to happen (and how) before you pay for it.
  prefs: []
  type: TYPE_NORMAL
- en: The ReWOO workflow is made up of three modules. In the planning module, an agent
    anticipates its next steps given a user’s prompt. The next stage entails collecting
    the outputs produced by calling these tools. Finally, an agent pairs the initial
    plan with the tool outputs to formulate a response. This planning ahead can greatly
    reduce token usage and computational complexity as well as the repercussions of
    intermediate tool failure.^([31](ch07.html#id1029))
  prefs: []
  type: TYPE_NORMAL
- en: Risks and Limitations of Agentic Systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Agentic systems have all of the same risks and limitations as GenAI, particularly
    concerns of bias, hallucinations, jailbreaking, etc. In addition to these common
    issues, there are specific limitations and risks with agentic systems that we
    want you to understand when considering an agentic deployment—and that’s why we
    wrote this section:'
  prefs: []
  type: TYPE_NORMAL
- en: Computational complexity and infinite feedback loops
  prefs: []
  type: TYPE_NORMAL
- en: Because AI agents often leverage multiple inference calls to respond to a single
    prompt, they can become very computationally expensive, particularly for simple
    NLP tasks. It may be more efficient and cost-effective to run a standard LLM workflow,
    without bringing in a broader agentic system.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, agents that are unable to create a comprehensive plan, or reflect
    on their findings, may find themselves repeatedly calling the same tools, invoking
    infinite feedback loops. If agents are left unattended and get into an infinite
    feedback loop that runs inference on a large LLM, you could be looking at a very
    expensive bill! We’ve literally seen this happen. When we first started experimenting
    with this technology, we asked an agent to find the world’s best tzatziki recipe.
    We had hoped it would go out and find some winner lists and use some logic to
    compare them (like number of hits on the website or how popular the domain was).
    In the end, our agent got lost in a sea of contradictory food blogs and recommendations
    of lots of garlic (because every AI knows where the tzatziki magic happens) and
    no real “Opa!” in the lackluster grand finale.
  prefs: []
  type: TYPE_NORMAL
- en: Control and observability
  prefs: []
  type: TYPE_NORMAL
- en: The flexibility that allows agents to robustly handle new tasks and solve problems
    is only possible because of the slackening control imposed on the system. It becomes
    critical, therefore, to monitor and understand an agent’s decision-making process
    and actions in agentic workflows. Depending on how an agent is implemented, the
    full internal workings and decision-making flows are not always transparent, potentially
    leading to unintended consequences. For instance, a model may adapt in unforeseen
    ways, leading to behaviors that are not aligned with your original objectives
    or your values.
  prefs: []
  type: TYPE_NORMAL
- en: This lack of control and observability can result in some of the undesirable
    outcomes you learned about in [Chapter 5](ch05.html#ch05_live_die_buy_or_try_much_will_be_decided_by_ai_1740182048942635);
    for example, biased or discriminatory actions, which can have severe consequences
    in high-stakes applications like healthcare, finance, or education. As you go
    down this path, we want to remind you how essential it is to develop requirements
    for transparent and explainable LLMs, allowing for real-time monitoring and corrective
    actions to mitigate these risks.
  prefs: []
  type: TYPE_NORMAL
- en: Security and complex permissions
  prefs: []
  type: TYPE_NORMAL
- en: There are a multitude of potential security and safety challenges that need
    to be solved before any custom-built (and perhaps the off-the-shelf ones you buy)
    agents can be safely deployed in complex enterprise environments. For example,
    if an HR agent designed for acting upon an employee’s request has access to an
    HR database that includes sensitive details for all employees, data security measures
    should be put in place to make sure that agent doesn’t accidentally divulge (or
    have access to, for that matter) sensitive information about other employees to
    the end user. Quite simply, this requires fine-grained access controls (FGACs)
    and role-based access controls (RBACs), adherence to personally identifiable information
    (PII) transfer protocols, principle of least privileges assignments, an identity
    fabric, and more. Similarly, in multiagent systems, communication protocols need
    to be established for how agents with access to different sensitive information
    types can work together without leaking sensitive content and adhering to data
    transit regulations that require encryption.
  prefs: []
  type: TYPE_NORMAL
- en: 'Three Tips to Get You Started: Our Agentic Best Practices'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Whenever you come across anything new, it’s always best to get some tips to
    help you get started. We created this section with extra help from some IBMers
    like Anna Gutowska, whose day-to-day job is literally training agentic systems
    that are smart enough to do incredible things, but not so wild that they start
    doing crazy things. If you pay attention to these tips, you’ll be living your
    best agentic life—a trusted one.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Activity logs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To better understand and debug agent behavior after the fact, developers can
    provide users with access to a log of agent actions. These actions can include
    the use of external tools and describe the individual steps taken to reach the
    goal. This transparency gives users insights into an agent’s iterative decision-making
    process and provides the opportunity to discover errors and build trust.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Interruption and runtime observability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prevent AI agents from running for overly long periods to avoid cases of unintended
    infinite feedback loops, changes in access to certain tools, or malfunctioning
    due to design flaws. One way to accomplish this is by implementing interruptability,
    where a human user (or an external resource manager like Turbonomic) can stop
    a pointless (or endless) workflow. To make interruptability more powerful, you
    also need to layer in observability to your agentic system so that you can monitor
    where an agent is in its workflow, and if something goes wrong, quickly find the
    what and how.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Human supervision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To assist in the learning process for AI agents, especially in their early stages
    in a new environment, it can be helpful to provide occasional human feedback.
    This allows your agents to compare their performance to the expected standard
    and adjust accordingly. This form of feedback is helpful in improving any agent’s
    adaptability to user preferences.^([32](ch07.html#id1037))
  prefs: []
  type: TYPE_NORMAL
- en: For example, you can set up your framework such that every time your agent finishes
    a task, it stops and asks for some feedback—this gives you an opportunity to tell
    it what’s missing or what it could do better. [Figure 7-10](#ch07_figure_10_1740182051640455)
    was an example of where this would have been a great thing to do. Upon reading
    the social media posts, we could have shaped it to better suit our style and the
    audiences using those outlets. We could then push this feedback into an LLM that
    would extract these tips for each task and put them into the memory of our agents
    to reference in the future. Once again, we could use AI here to help AI by creating
    a “judge” AI model to look at the output work for our LLM and see whether it’s
    up to snuff.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from this, it is a best practice to require human approval before an AI
    agent takes highly impactful actions. For instance, actions ranging from sending
    mass emails to financial trading should require human confirmation.^([33](ch07.html#id1039))
    Some level of human monitoring is recommended for such high-risk domains.
  prefs: []
  type: TYPE_NORMAL
- en: Think About This When It Comes to AI Agents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AI agents and agentic systems are becoming popular for a reason: they are able
    to significantly improve AI’s performance and robustness on complex tasks. The
    ability to plan and reason for a task, bring in the latest up-to-date information
    via tool calls, and break down complex problems into smaller, more tractable components
    also opens the door for smaller, open source models to take on more challenging
    assignments. At the end of the day, this technology is still evolving, and while
    it improves model performance and productivity, ensure you spend time thinking
    about safety, security, and cost before leveraging AI agents for production tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping It Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a lot of exciting things happening in the world of AI, and we hope
    we’ve convinced you that whether it is the advent of SLMs or the efficacy of more
    systems-based approaches to AI, including model routing, MoE, or agentic systems,
    “one model will not rule them all.” In the next chapter, we are going to cover
    how enterprises can extend these systems-based implementations of LLMs by specializing
    SLMs on enterprise data.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch07.html#id906-marker)) Maximilian Schreiner, “GPT-4 Architecture, Datasets,
    Costs and More Leaked,” *The Decoder*, July 11, 2023, [*https://oreil.ly/6sD6g*](https://oreil.ly/6sD6g).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch07.html#id907-marker)) See OpenAI’s API pricing [online](https://oreil.ly/d21qP).
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch07.html#id909-marker)) See the update at [*https://oreil.ly/jCoxe*](https://oreil.ly/jCoxe).
  prefs: []
  type: TYPE_NORMAL
- en: '^([4](ch07.html#id910-marker)) Muddu Sudhakar, “Small Language Models (SLMs):
    The Next Frontier for the Enterprise,” *Forbes*, [*https://oreil.ly/slTCo*](https://oreil.ly/slTCo).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch07.html#id920-marker)) Jared Kaplan et al., “Scaling Laws for Neural
    Language Models,” preprint, arXiv, January 23, 2020, arXiv:2001.08361 (2020).
    [*https://arxiv.org/abs/2001.08361*](https://arxiv.org/abs/2001.08361).
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch07.html#id923-marker)) Jordan, Hoffmann et al., “Training Compute-Optimal
    Large Language Models,” preprint, arXiv, March 29, 2022, [*https://arxiv.org/abs/2203.15556*](https://arxiv.org/abs/2203.15556).
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch07.html#id930-marker)) Aaron Grattafiori et al., “The Llama 3 Herd of
    Models,” preprint, arXiv, November 23, 2024, [*https://arxiv.org/abs/2407.21783*](https://arxiv.org/abs/2407.21783).
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch07.html#id931-marker)) Aaron Grattafiori et al., “The Llama 3 Herd of
    Models,” preprint, arXiv, November 23, 2024, [*https://arxiv.org/abs/2407.21783*](https://arxiv.org/abs/2407.21783).
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch07.html#id935-marker)) Suriya Gunasekar et al., “Textbooks Are All You
    Need,” arXiv, October 2, 2023, [*https://arxiv.org/pdf/2306.11644*](https://arxiv.org/pdf/2306.11644).
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch07.html#id938-marker)) Kate Knibbs, “The Battle Over Books3 Is Just
    the Beginning,” *Wired*, September 4, 2023, [*https://oreil.ly/58JTr*](https://oreil.ly/58JTr).
  prefs: []
  type: TYPE_NORMAL
- en: ^([11](ch07.html#id945-marker)) Previously known as PubMedGTP.
  prefs: []
  type: TYPE_NORMAL
- en: ^([12](ch07.html#id949-marker)) Reuters Graphics, “COBOL Blues,” [*https://oreil.ly/lM-8U*](https://oreil.ly/lM-8U).
  prefs: []
  type: TYPE_NORMAL
- en: '^([13](ch07.html#id960-marker)) *The Vicuna Team* (blog), “Vicuna: An Open-Source
    Chatbot Impressing GPT-4 with 90% ChatGPT Quality,” LMSYS, March 30, 2023, [*https://oreil.ly/qRHD4*](https://oreil.ly/qRHD4).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([14](ch07.html#id968-marker)) “Nemotron-4-340B-Instruct,” Hugging Face, [*https://oreil.ly/5Mh3Y*](https://oreil.ly/5Mh3Y).
  prefs: []
  type: TYPE_NORMAL
- en: ^([15](ch07.html#id972-marker)) Samanatha Subin, “Nvidia Sheds Almost $600 Billion
    in Market Cap, Biggest One-Day Loss in U.S. History,” CNBC, January 27, 2025,
    [*https://oreil.ly/vWA0q*](https://oreil.ly/vWA0q).
  prefs: []
  type: TYPE_NORMAL
- en: '^([16](ch07.html#id977-marker)) DeepSeek-AI, “DeepSeek-R1: Incentivizing Reasoning
    Capability in LLMs via Reinforcement Learning,” arXiv, January 22, 2025, [*https://arxiv.org/pdf/2501.12948*](https://arxiv.org/pdf/2501.12948).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([17](ch07.html#id978-marker)) See the data on [Hugging Face’s website](https://oreil.ly/GOLDy).
  prefs: []
  type: TYPE_NORMAL
- en: ^([18](ch07.html#id979-marker)) See the datasets on the [Hugging Face website](https://oreil.ly/rVZKY).
  prefs: []
  type: TYPE_NORMAL
- en: ^([19](ch07.html#id980-marker)) Cade Metz, “OpenAI Says DeepSeek May Have Improperly
    Harvested Its Data,” *The New York Times*, January 29, 2025, [*https://oreil.ly/7xn_C*](https://oreil.ly/7xn_C).
  prefs: []
  type: TYPE_NORMAL
- en: ^([20](ch07.html#id984-marker)) See the licensing agreement on [Llama’s website](https://oreil.ly/6prL2).
  prefs: []
  type: TYPE_NORMAL
- en: ^([21](ch07.html#id986-marker)) *Cisco Blogs*, “Evaluating Security Risk in
    DeepSeek and Other Frontier Reasoning Models,” by Paul Kassianik and Amin Karbasi,
    posted January 31, 2025, [*https://oreil.ly/gy5Xp*](https://oreil.ly/gy5Xp).
  prefs: []
  type: TYPE_NORMAL
- en: ^([22](ch07.html#id987-marker)) Tested using deepinfra.com’s hosted version
    of DeepSeek-R1\.
  prefs: []
  type: TYPE_NORMAL
- en: ^([23](ch07.html#id988-marker)) See the posting on [IBM’s website](https://oreil.ly/UrcIZ).
    Even more important, these distilled models failed miserably on safety evaluations.
    It is difficult to know exactly why these models are deficient in general performance
    and safety, but a potential hypothesis is that by focusing exclusively on code
    and math during distillation, safety and general performance were left to the
    wayside.
  prefs: []
  type: TYPE_NORMAL
- en: ^([24](ch07.html#id993-marker)) Tal Shnitzer et al., “Large Language Model Routing
    with Benchmark Datasets,” preprint, arXiv, September 27, 2023, [*https://arxiv.org/abs/2309.15789*](https://arxiv.org/abs/2309.15789).
  prefs: []
  type: TYPE_NORMAL
- en: ^([25](ch07.html#id994-marker)) Percy Liang et al., “Holistic Evaluation of
    Language Models,” preprint, arXiv, October 1, 2023, [*https://arxiv.org/abs/2211.09110*](https://arxiv.org/abs/2211.09110).
  prefs: []
  type: TYPE_NORMAL
- en: ^([26](ch07.html#id995-marker)) Tal Shnitzer et al., “Large Language Model Routing
    with Benchmark Datasets,” preprint, September 27, 2023, arXiv, [*https://arxiv.org/abs/2309.15789*](https://arxiv.org/abs/2309.15789).
  prefs: []
  type: TYPE_NORMAL
- en: ^([27](ch07.html#id1008-marker)) DeepSeek-AI, “DeepSeek-V3 Technical Report,”
    preprint, arXiv, February 18, 2025, [*https://arxiv.org/html/2412.19437v1*](https://arxiv.org/html/2412.19437v1).
  prefs: []
  type: TYPE_NORMAL
- en: ^([28](ch07.html#id1010-marker)) Ibid.
  prefs: []
  type: TYPE_NORMAL
- en: ^([29](ch07.html#id1020-marker)) Alan Chan et al., “Visibility into AI Agents,”
    arXiv, updated May 17, 2024, [*https://arxiv.org/abs/2401.13138*](https://arxiv.org/abs/2401.13138).
  prefs: []
  type: TYPE_NORMAL
- en: ^([30](ch07.html#id1025-marker)) Gautier Dagan et al., “Dynamic Planning with
    a LLM,” preprint, arXiv, August 11, 2023, [*https://arxiv.org/abs/2308.06391*](https://arxiv.org/abs/2308.06391).
  prefs: []
  type: TYPE_NORMAL
- en: '^([31](ch07.html#id1029-marker)) Bienfeng Xu et al., “ReWOO: Decoupling Reasoning
    from Observations for Efficient Augmented Language Models,” preprint, arXiv, May
    23, 2023, arXiv. [*https://arxiv.org/abs/2305.18323*](https://arxiv.org/abs/2305.18323).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([32](ch07.html#id1037-marker)) Bienfeng Xu et al., “ReWOO: Decoupling Reasoning
    from Observations for Efficient Augmented Language Models,” preprint, arXiv, May
    23, 2023, [*https://arxiv.org/abs/2305.18323*](https://arxiv.org/abs/2305.18323).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([33](ch07.html#id1039-marker)) Veselka Sasheva Petrova-Dimitrova, “Classifications
    of Intelligence Agents and Their Applications,” *Fundamental Sciences and Applications*
    28, no. 1 (2022).
  prefs: []
  type: TYPE_NORMAL
