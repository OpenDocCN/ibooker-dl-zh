- en: Chapter 7\. Where This Technology Is Headed—One Model Will *Not* Rule Them All!
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第七章\. 这项技术将走向何方——一枚戒指将无法统治一切！
- en: Can you place this mantra?
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 你能放置这个咒语吗？
- en: “One Ring to rule them all,
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “一枚戒指，召集众人，
- en: ''
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: One Ring to find them,
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一枚戒指，寻找他们，
- en: ''
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: One Ring to bring them all,
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一枚戒指，召集众人，
- en: ''
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: and in the darkness bind them.”
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在黑暗中束缚他们。”
- en: 'If you’re a true Tolkienite nerd, your elf ears likely perked up; otherwise,
    we’ll tell you it’s the basis of the story for J.R.R. Tolkien’s iconic *Lord of
    the Rings* and this One Ring inscription gives its wearer the ability to control
    everything. (Purists will note it wasn’t the inscription that bestowed the power
    and then go on about Sauron, but we’ll leave it there; like we said, nerds.) Total
    domination. Putting all the evil aside, one question looms (likely due to the
    fanfare around ChatGPT that introduced the world to GenAI): will one single LLM
    rule them all?'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是一个真正的托尔金迷，你的精灵耳朵可能竖了起来；否则，我们将告诉你这是J.R.R. 托尔金标志性作品《指环王》和这枚戒指铭文的基础，它赋予佩戴者控制一切的能力。（纯粹主义者会指出，赋予力量的不是铭文，而是关于索伦的讨论，但我们就此打住；就像我们说的，狂热者。）全面统治。抛开所有邪恶不谈，一个疑问浮现在眼前（可能是因为ChatGPT的炒作，向世界介绍了通用人工智能）：是否只有一个单一的大型语言模型能够统治一切？
- en: 'Spoiler alert: we don’t think so at all. Not even close. As you learned earlier
    in this book, there are almost 1.5 million (it’s likely more by the time you read
    this book) models on Hugging Face alone. We’re also certain (assuming you’ve read
    the book linearly so far) that you can easily articulate the difference between
    Value Users and Value Creators, and you understand AI ethics and data lineage.
    In short, you understand why one model can’t possibly rule them all...but we’re
    going to pull a more complete answer to the *why* for you here. It starts with
    the fact that even in the AI labs pushing out the highest-performing frontier
    models, we are seeing shifts from innovating on a single model performing a task,
    to empowering a system of models and techniques to work together and complete
    a task. In this chapter, we want to draw your attention to what’s been going on
    in the marketplace, and to which trends and technological innovations are powering
    the future of GenAI. From the rapid innovations that are happening at the small
    model size, to intra- and inter-model routing, to exciting advancements in agentic
    systems, we believe there will never be one model to rule them all.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 揭秘警告：我们完全不这么认为。甚至相差甚远。正如你在本书中较早了解到的，仅在Hugging Face上就有近150万个（在你阅读这本书的时候可能更多）模型。我们也确定（假设你到目前为止是线性阅读这本书的），你能够轻松地阐述价值用户和价值创造者之间的区别，你理解人工智能伦理和数据溯源。简而言之，你理解为什么一个模型不可能统治一切...但我们在这里将为你提供一个更完整的关于“为什么”的答案。这始于这样一个事实：即使在推动最高性能前沿模型的AI实验室中，我们也在看到从创新单一模型执行任务，到赋予一个模型和技术的系统协同工作并完成任务的变化。在本章中，我们希望将你的注意力引向市场上正在发生的事情，以及哪些趋势和技术创新正在推动通用人工智能的未来。从小型模型快速创新，到模型内和模型间路由，再到代理系统令人兴奋的进步，我们相信永远不会有一个模型能够统治一切。
- en: The Bigger the Better, Right? Perhaps at the Start, But That Was a Long Time
    Ago
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 越大越好，对吧？也许一开始是这样，但那已经是很久以前了
- en: Keeping with our theme in this book that while tech years age like dog years
    (1:7), GenAI years are like mouse years (1:30), that makes 2018 over 2 centuries
    old in GenAI years—that’s a long time ago! What happened in 2018? OpenAI released
    [GPT-1](https://oreil.ly/IBZTG) with a mere 117 million parameters.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 跟随本书的主题，虽然科技年像狗年一样衰老（1:7），而通用人工智能（GenAI）的年就像老鼠年（1:30），这使得2018年在GenAI年份中已经超过两个世纪——那已经是很久以前了！2018年发生了什么？OpenAI发布了仅117百万参数的[GPT-1](https://oreil.ly/IBZTG)。
- en: As a part of their quest toward *artificial general intelligence (AGI)*, OpenAI
    has built successively more capable GPT versions (some into the trillions of parameters)
    that can perform more tasks with each successive release.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 作为他们向通用人工智能（AGI）的探索的一部分，OpenAI已经构建了越来越强大的GPT版本（一些达到万亿参数），每个版本都能执行更多任务。
- en: Note
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: AGI shouldn’t be confused with GenAI. GenAI is a tool. AGI is a goal of evolving
    that tool to the extent that its capabilities match human cognitive abilities,
    or even surpasses them, across a wide range of tasks. We’re not there yet, perhaps
    never will be, or perhaps it’ll arrive sooner than we expected. But when it comes
    to AGI, think about LLMs demonstrating and exceeding humanlike intelligence.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 通用人工智能（AGI）不应与通用人工智能（GenAI）混淆。GenAI是一个工具。AGI是将这个工具发展到其能力与人类认知能力相匹配，甚至超越它们，在广泛任务上的目标。我们还没有达到那里，也许永远也不会，或者也许它比我们预期的来得更快。但当我们谈到AGI时，想想大型语言模型（LLMs）展示并超越人类智能的情况。
- en: Initially, it seemed that the main vehicle for driving model performance improvements
    was simply increasing a model’s size. As shown in [Table 7-1](#table-7-1), between
    GPT-1 and GPT-3, the models released by OpenAI increased by more than 10,000 times
    in size! After GPT-3, OpenAI stopped publishing model sizes all together, but
    GPT-4 and the GPT-4o models were rumored^([1](ch07.html#id906)) at one point to
    total over one trillion parameters! And as these models have gotten larger, they
    have also gotten more expensive. Small models normally cost less than $0.25 for
    1 million output tokens (or “free” if you can get it on your laptop with frameworks
    like Ollama). In contrast, big models are pricier. For example, last we looked,
    OpenAI’s o1 costs were about $60 for the same amount of output.^([2](ch07.html#id907))
    Whatever the price you’re paying (prices in this space are changing as fast as
    the technology, mostly in a good way), high performance small models have a lot
    of business sense to them.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，似乎推动模型性能提升的主要途径仅仅是增加模型的大小。正如[表7-1](#table-7-1)所示，在GPT-1和GPT-3之间，OpenAI发布的模型大小增加了超过10,000倍！GPT-3之后，OpenAI完全停止发布模型大小，但据传言^([1](ch07.html#id906))，GPT-4和GPT-4o模型在某一点上总参数量超过了一万亿！随着这些模型变得更大，它们也变得更贵。小型模型通常每1000万个输出标记的成本不到0.25美元（或者如果你能在笔记本电脑上使用Ollama等框架，则是“免费”）。相比之下，大型模型更贵。例如，我们最后一次查看时，OpenAI的o1成本约为相同数量的输出60美元.^([2](ch07.html#id907))
    无论你支付的价格如何（这个领域的价格变化与技术一样快，大多数情况下是好的），高性能的小型模型在商业上有很多意义。
- en: '[Table 7-1](#table-7-1) shows that as the GPT family of models has grown, the
    world has witnessed significant improvements in the capabilities that these models
    could achieve.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[表7-1](#table-7-1)显示，随着GPT系列模型的发展，世界见证了这些模型所能实现的能力的显著提升。'
- en: Table 7-1\. OpenAI’s GPT family over time
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 表7-1\. 随着时间的推移，OpenAI的GPT家族
- en: '| OpenAI model name | Parameters | Interesting things to note |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI模型名称 | 参数 | 值得注意的有趣之处 |'
- en: '| --- | --- | --- |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| GPT-1 | 117 million | This is the “original.” It was better than some previous
    technologies, but turned out to be just the start of something that was going
    to be big. |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| GPT-1 | 117 million | 这是“原始版”。它比一些之前的技术更好，但最终只是某个即将变得巨大的开始。 |'
- en: '| GPT-2 | ~1 billion | This model started to make some interesting completions
    and prove that there was a different horizon for natural language processing (NLP).
    It was nowhere close to what you first experienced with ChatGPT and beyond, but
    it got some press in the news for writing a story about unicorns.^([a](ch07.html#id908))
    |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2 | ~1 billion | 这个模型开始产生一些有趣的补全，并证明自然语言处理（NLP）存在不同的前景。它远远达不到你最初在ChatGPT中体验到的水平，但它在新闻中因撰写关于独角兽的故事而获得了一些关注.^([a](ch07.html#id908))
    |'
- en: '| GPT-3 GPT-3.5'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '| GPT-3 GPT-3.5'
- en: GPT-3.5 Turbo | ~175 billion | GPT-3.5 was the initial model behind ChatGPT’s
    debut. Two big changes occurred compared to GPT-2\. It was designed to follow
    instructions (versus simply predicting the next most likely word in a sentence),
    *and* they put a user interface on it. Enough said. GPT-3.5 was also released
    as a more efficient, lightweight version called “Turbo.” |
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3.5 Turbo | ~175 billion | GPT-3.5是ChatGPT首次亮相背后的初始模型。与GPT-2相比，发生了两个重大变化。它被设计为遵循指令（而不是简单地预测句子中下一个最可能出现的单词），*并且*它们在上面添加了用户界面。无需多言。GPT-3.5还作为一个更高效、轻量级的版本发布，称为“Turbo”。
    |
- en: '| GPT-4 GPT-4 turbo'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '| GPT-4 GPT-4 turbo'
- en: GPT-4o
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4o
- en: GPT-4o mini
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4o mini
- en: GPT-4.5 | OpenAI stopped publishing parameter numbers after GPT-3 (which was
    noted to have 175 billion parameters). Various blogs suggest GPT-4 has ~1.8 trillion
    parameters. | Their fourth generation of models delivered more power and multimodal
    capabilities. At the time of publishing, GPT-4o was considered OpenAI’s “flagship”
    model, and GPT-4.5 just came out. GPT-5 wasn’t out when we went to print, but
    many are suggesting to expect it sometime in the middle of 2025. |
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4.5 | OpenAI 在 GPT-3（其参数数量为 1750 亿）之后停止发布参数数量。各种博客建议 GPT-4 有大约 1800 亿参数。|
    他们的第四代模型提供了更多的功能和多模态能力。在本书发布时，GPT-4o 被认为是 OpenAI 的“旗舰”模型，而 GPT-4.5 刚刚发布。在我们印刷时，GPT-5
    还没有发布，但许多人预测它将在 2025 年中旬左右出现。|
- en: '| OpenAI o1 OpenAI o3 mini | (See above.) | Considered a separate project and
    not a part of the core GPT family, these reasoning models were trained to produce
    long chains of thought before responding, enabling them to solve more complex
    tasks. This capability is expected to be merged into GPT-5. |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI o1 OpenAI o3 mini | (见上文。) | 这些推理模型被视为一个独立的项目，而不是 GPT 核心家族的一部分，它们被训练在回答之前产生长链的思考，这使得它们能够解决更复杂的问题。预计这种能力将被合并到
    GPT-5 中。|'
- en: '| ^([a](ch07.html#id908-marker)) See the story on [OpenAI’s site](https://oreil.ly/1JZ6W).
    |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| ^([a](ch07.html#id908-marker)) 请参阅 [OpenAI 网站上的故事](https://oreil.ly/1JZ6W)。
    |'
- en: This begs the question, do you need all that capacity for your business? Even
    OpenAI has started creating smaller, more efficient versions of their models.
    For each major model release, there has been a pairwise release of a more efficient
    and more cost-effective alternative. GPT-3.5, meet GPT-3.5 Turbo; GPT-4o, meet
    GPT-4o mini.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这引发了一个问题，你的业务是否真的需要所有这些容量？即使是 OpenAI 也已经开始创建更小、更高效的模型版本。对于每个主要模型发布，都有一个更高效、成本效益更高的替代模型的并行发布。GPT-3.5，遇见
    GPT-3.5 Turbo；GPT-4o，遇见 GPT-4o mini。
- en: The latest reasoning model OpenAI released at the time this book was published
    was OpenAI o3 mini. While OpenAI originally committed to releasing OpenAI o3,
    they have since pressed pause, and announced instead that GPT-5 will introduce
    an AI system that brings together the best of OpenAI o3 and the GPT model series,
    with Sam Altman sharing the goal of “simplifying our product offerings” and “to
    return to magic unified intelligence.”^([3](ch07.html#id909))
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书发布时，OpenAI 发布的最新推理模型是 OpenAI o3 mini。虽然 OpenAI 最初承诺发布 OpenAI o3，但他们后来暂停了发布，并宣布
    GPT-5 将引入一个结合 OpenAI o3 和 GPT 模型系列最佳功能的 AI 系统，Sam Altman 分享了“简化我们的产品组合”和“回归魔法统一智能”的目标^([3](ch07.html#id909))。
- en: To sum up this section, even in the frontier AI labs that were made famous by
    innovating through scale, we are seeing innovations and road maps centered around
    bringing multiple models together, working as a system to drive “unified intelligence.”
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 总结本节内容，即使在那些通过规模创新而闻名的前沿人工智能实验室中，我们也看到了以将多个模型结合在一起为中心的创新和路线图，这些模型作为一个系统来推动“统一智能”。
- en: And despite the common belief that bigger is always better when it comes to
    model size, there are many exciting innovations enabling small yet powerful LLMs.
    So much so that the term small language models (SLMs)^([4](ch07.html#id910)) has
    emerged. There is no precise definition, but SLMs usually refer to LLMs that are
    normally fewer than 13 billion parameters in size. In some scenarios, SLMs have
    met the performance of LLMs 100+ billion parameters in size.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管普遍认为在模型大小方面越大越好，但有许多令人兴奋的创新使得小型但强大的 LLMs 成为可能。因此，出现了“小型语言模型”（SLMs）这一术语。虽然没有精确的定义，但
    SLMs 通常指的是参数数量通常少于 130 亿的 LLMs。在某些情况下，SLMs 已经达到了 100 多亿参数的 LLMs 的性能。
- en: The Rise of the Small Language Model
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 小型语言模型的兴起
- en: Perhaps the simplest way to describe the phenomena that is SLMs is that model
    providers are getting better at training. Case in point, when some of our research
    teams first got their hands on Llama-2-70B back in July 2023, they were amazed
    at what it could do. Just a little over a year later, they were able to achieve
    *the same, if not better,* performance using just a 2B parameter version of Granite,
    according to Hugging Face’s Open LLM v2 Leaderboard (see [Figure 7-1](#ch07_figure_1_1740182051640230)).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 描述 SLMs（小型语言模型）现象的最简单方法可能是，模型提供商在训练方面变得越来越擅长。以案为例，当我们的研究团队在 2023 年 7 月首次接触到
    Llama-2-70B 时，他们对它能做什么感到惊讶。仅仅一年多后，根据 Hugging Face 的 Open LLM v2 领导板（见[图 7-1](#ch07_figure_1_1740182051640230)），他们仅使用
    Granite 的 20 亿参数版本就能实现*相同甚至更好的*性能。
- en: '![A screenshot of a graph  AI-generated content may be incorrect.](assets/aivc_0701.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![一个图表的截图 AI生成的内容可能不正确。](assets/aivc_0701.png)'
- en: Figure 7-1\. A snapshot of model performance, taken from Hugging Face’s Open
    LLM v2 Leaderboard in Feb 2024
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-1\. 从2024年2月Hugging Face的Open LLM v2排行榜中截取的模型性能快照
- en: This, again, is just part of the natural benefit of progressing up the learning
    curve of anything; as sure as our electric vehicles (EVs) go farther and charge
    faster, we’re getting more pixels and camera lenses on our phone every other year,
    and our TVs are getting thinner, providers are gaining more experience training
    models, and new innovations are making them more efficient.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这再次只是任何事物学习曲线上升的自然益处的一部分；正如我们的电动汽车（EVs）行驶得更远、充电更快一样，我们每隔一年就能在手机上获得更多的像素和摄像头镜头，我们的电视也越来越薄，提供商在训练模型方面积累了更多经验，新的创新使它们更加高效。
- en: In the next couple of sections, we want to share with you some of the promising
    strategies behind the rise of highly competitive SLMs, specifically data curation
    and model distillation.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几个部分中，我们想要与您分享一些高度竞争性SLMs崛起背后的有希望的战略，特别是数据整理和模型蒸馏。
- en: 'It is no coincidence that both of these strategies center around the data used
    to train and fine-tune LLMs. It surprises many we talk to that, more often than
    not, advancements that are slimming down model size stem more from innovative
    strategies with training data than technical innovations in the model’s architecture
    itself. Please don’t misunderstand what we’re trying to tell you here. Innovations
    in architecture are definitely occurring. In fact, we cover some exciting architecture
    advancements in this very chapter! But, when we look at the warp speed of how
    SLMs have risen to prominence (and they did so within a year of the November 2022
    release of ChatGPT), the contributing factor is clear: *data reigns supreme!*
    And we go into detail on these data-based trends because in [Chapter 8](ch08.html#ch08_using_your_data_as_a_differentiator_1740182052172518),
    we will show you how the same techniques that model providers are using today
    to create SLMs can be used by your company to differentiate and create value with
    enterprise data.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种策略都围绕着用于训练和微调LLMs的数据展开，这并非巧合。我们与许多人交谈时，常常感到惊讶，因为模型尺寸的缩小往往更多地源于训练数据的创新策略，而不是模型架构本身的技术创新。请别误解我们在这里想要传达的信息。架构的创新确实正在发生。事实上，我们就在本章中介绍了一些令人兴奋的架构进步！但是，当我们看到SLMs如何以惊人的速度崛起（它们在2022年11月ChatGPT发布后一年内就取得了这样的成就）时，影响因素是显而易见的：*数据至高无上！*
    我们会详细讨论这些基于数据的发展趋势，因为在[第8章](ch08.html#ch08_using_your_data_as_a_differentiator_1740182052172518)中，我们将向您展示模型提供商今天用来创建SLMs的相同技术，您的公司也可以利用企业数据来区分和创造价值。
- en: Note
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'So here’s the deal: you’ve got data. That data you have access to isn’t part
    of these LLMs at all. Why? Because it’s your corporate data. We can assure you
    that many LLM providers want it. In fact, the reason 99% of corporate data isn’t
    scraped and sucked into an LLM is because you didn’t post it on the internet.
    So, you have some choices to make that we talked about earlier in this book, and
    we will go deep into them in the next chapter. Where will you sit on the data
    value exchange continuum we talked about in [Chapter 2](ch02.html#ch02_oh_to_be_an_ai_value_creator_1740182046162988)?
    Are you planning to give it away and let others create disproportionate amounts
    of value from your data, essentially *making your data* *THEIR* *competitive advantage*
    OR are you going to *make your data* *YOUR* *competitive advantage*? That’s what
    this book is all about. And this and the next chapter help you see that through.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这里是关键：您拥有数据。您能够访问的数据根本不属于这些LLMs。为什么？因为那是您的企业数据。我们可以向您保证，许多LLM提供商都想要它。事实上，99%的企业数据没有被抓取并吸入LLM的原因，是因为您没有将其发布到互联网上。因此，您需要做出一些选择，我们在这本书的前面已经讨论过，我们将在下一章深入探讨。您将在我们[第2章](ch02.html#ch02_oh_to_be_an_ai_value_creator_1740182046162988)中讨论的数据价值交换连续体上处于什么位置？您是打算将其拱手相让，让他人从您的数据中创造不成比例的价值，本质上*使您的数据*成为*他们的**竞争优势*，还是您打算*使您的数据*成为*您的**竞争优势*？这正是本书的主题。而且，这一章和下一章将帮助您看到这一点。
- en: Data Curation Results in AI Salvation
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据整理导致AI救赎
- en: OK, we admit it, you likely know this one. You don’t even have to have a machine
    learning background to assert that curating a large quantity of high-quality training
    data can have huge impacts on a model’s performance (or any analytics project
    for that matter).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我们承认，你可能知道这一点。即使你没有机器学习背景，你也可以断言，精心制作大量高质量的训练数据可以对模型的表现（或者任何分析项目）产生巨大影响。
- en: 'But an emphasis on data curation is a huge part of why SLMs have become so
    performant, and it goes directly against the initial philosophy of the early LLM
    bakes: take as much messy, uncleaned, and unstructured data as possible and repurpose
    it to power an LLM. As it turns out, a compromise is in order when it comes to
    LLMs for business. Transformer technology made it possible to take large quantities
    of relatively messy data to create an LLM, but the higher quality the data, the
    higher quality the model. Ask yourself if you have large volumes of high-quality
    data that is specialized for business that you care about. Of course you do! Now
    you are ready to cook with gas because quantity, quality, and specialization are
    the three key data curation ingredients that have helped lead to the rise of SLMs.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，对数据整理的重视是SLMs（小型语言模型）之所以表现出色的一个重要原因，这与早期LLM的初始理念直接相反：尽可能多地获取杂乱、未清洗和未结构化的数据，并重新利用它来为LLM提供动力。实际上，当涉及到商业LLM时，需要做出妥协。Transformer技术使得能够使用大量相对杂乱的数据来创建LLM成为可能，但数据质量越高，模型质量也越高。问问自己，你是否拥有大量针对你关心的商业领域的高质量、专业化的数据。当然，你有！现在你准备好加油了，因为数量、质量和专业化是三个关键的数据整理成分，这些成分有助于SLMs的兴起。
- en: Data quantity
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据量
- en: How much data is optimal for a given model size? This has been a subject of
    much study by the AI research community because, as you can imagine, there are
    very high environmental and pocketbook costs associated with training an LLM.
    For this reason, early model providers’ initial focus was trying to optimize performance
    while minimizing their own up-front costs for model training. A key part of this
    optimization was defining how many tokens (recall, this is essentially a piece
    of a word, a whole word, or even a punctuation mark) of language data should be
    introduced to a model for each additional parameter added to the overall size
    of the model they were training. These ratios—often referred to as *scaling laws*
    in scientific literature—define how much data you need to scale up a model in
    size.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定模型大小，多少数据是最优的？这个问题一直是人工智能研究社区研究的主题，因为正如你可以想象的那样，训练一个大型语言模型（LLM）与环境成本和财务成本非常高。因此，早期模型提供商的初始重点是尝试优化性能，同时最小化他们为模型训练所承担的前期成本。这个优化的关键部分是定义应该向他们正在训练的模型整体大小中添加的每个额外参数引入多少语言数据（回想一下，这本质上是一个单词的一部分，一个完整的单词，甚至是一个标点符号）。这些比率——在科学文献中通常被称为*缩放定律*——定义了你需要多少数据来扩大模型的大小。
- en: In their 2020 paper,^([5](ch07.html#id920)) a team of OpenAI researchers posited
    that ~2 tokens of text should be used in training for every 1 parameter of an
    LLM. This 2:1 ratio became known as *Kaplan’s scaling law* (we’re guessing “Kaplan
    et al.’s scaling law” didn’t have a good ring to it) and was subsequently used
    to train models like GPT-3 and BLOOM (both models are 175 billion parameters in
    size and were trained on 300–350 billion tokens of text). In 2022, Google’s DeepMind
    published^([6](ch07.html#id923)) an alternate view on optimal scaling ratios called
    the Chinchilla scaling law. (This law is also known as Hoffman’s scaling law,
    named after the lead researcher; Chinchilla was a family of models published by
    DeepMind.) DeepMind’s researchers believed that OpenAI drastically underestimated
    the amount of data needed to optimally train an LLM...they felt the optimal scaling
    ratio to get *the best model performance for a given compute budget* was 20:1
    as opposed to the ~2:1 ratio. They went on to build a 70 billion parameter Chinchilla
    LLM using this scaling law. How did it do? At a mere 70 billion parameters, Chinchilla
    performed much better than larger models like GPT-3 (175 billion parameters).
    Looking back, we think Chinchilla was kind of like the SLM “OG” (as the kids say—it’s
    slang for original). This model is still quite big, but it isn’t a huge triple-digit
    billion parameter model, or bigger.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们2020年的论文中^([5](ch07.html#id920))，一组OpenAI研究人员提出了每1个LLM参数应该使用2个文本token进行训练的观点。这个2:1的比例被称为*Kaplan的扩展定律*（我们猜测“Kaplan等人”的扩展定律听起来并不那么好）并且随后被用来训练GPT-3和BLOOM（这两个模型都是1750亿参数大小，并在300-3500亿个文本token上进行训练）。2022年，Google的DeepMind发布^([6](ch07.html#id923))了一个关于最佳扩展比例的替代观点，称为Chinchilla扩展定律。（这个定律也被称为Hoffman的扩展定律，以首席研究员的名字命名；Chinchilla是DeepMind发布的一系列模型。）DeepMind的研究人员认为，OpenAI大大低估了优化训练LLM所需的数据量……他们认为，为了在给定的计算预算中获得*最佳模型性能*的最佳扩展比例是20:1，而不是大约2:1的比例。他们继续使用这个扩展定律构建了一个700亿参数的Chinchilla
    LLM。它表现如何？在仅仅700亿参数的情况下，Chinchilla的表现比GPT-3（1750亿参数）等更大的模型要好得多。回顾过去，我们认为Chinchilla有点像是SLM的“原始”（正如孩子们说的那样——这是俚语，意为原始）。这个模型仍然相当大，但它不是一个巨大的三位数十亿参数模型，或者更大。
- en: The research community’s initial goal focused on defining scaling laws to optimize
    the fixed up-front training costs for their models. But what about the recurring
    marginal costs across the rest of the model’s lifecycle? A super large model will
    be more expensive to host and inference. And guess who gets to incur those costs?
    That’s right, you! To reduce these costs, you need to reduce model size. To reduce
    model size while maintaining performance, you need to train on more (high quality)
    data.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 研究社区的初始目标集中在定义扩展定律以优化他们模型的固定前期训练成本。但是，关于模型生命周期中其余部分的持续边际成本怎么办？一个超级大的模型在托管和推理方面将更加昂贵。猜猜谁会承担这些成本？没错，就是你！为了降低这些成本，你需要减小模型大小。为了在保持性能的同时减小模型大小，你需要使用更多（高质量）的数据。
- en: And this is *exactly why* SLMs are capturing so much attention. Since inference
    and hosting costs are directly passed to model consumers, there was a bit of a
    delayed reaction. But as GenAI turned from a curiosity to a deployed technology,
    model providers have started optimizing their training setup to be as inference-efficient
    as possible, not merely training-efficient.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是SLMs受到如此多关注的原因。由于推理和托管成本直接转嫁给模型消费者，因此反应有些滞后。但随着GenAI从一种好奇心转变为部署的技术，模型提供商开始优化他们的训练设置，使其尽可能推理高效，而不仅仅是训练高效。
- en: To create inference-efficient models, it can be cost-effective to train a model
    on a higher data ratio than what even the Chinchilla scaling law had in mind.
    At the time this book went to print, the scientific community had not converged
    upon a universal scaling law for inference-optimal models (and perhaps never will),
    but there are compelling industry examples of very performant SLMs that are trained
    on much larger amounts of data than the doctrines of Chinchilla or Kaplan would
    suggest (we show some of these scaling laws over time in [Table 7-2](#ch07_table_2_1740182051649140)).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建推理效率高的模型，在比Chinchilla扩展定律所设想的数据比率更高的数据上训练模型可能是成本效益的。在本书付印时，科学界尚未就推理最优模型的通用扩展定律达成共识（也许永远也不会），但有一些令人信服的行业例子表明，这些非常高效的SLM是在比Chinchilla或Kaplan的教条所建议的更大数量的数据上训练的（我们在[表7-2](#ch07_table_2_1740182051649140)中展示了这些扩展定律随时间的变化）。
- en: In February of 2023, Meta open sourced its Llama 2 model series, trained on
    about 2 trillion tokens of training data (at the time, this was considered a massive
    amount of data). In the Llama 2 series, the 7 billion sized model had almost a
    300:1 scaling ratio! By August of 2024, with the release of Llama 3, Meta doubled
    (well, actually octupled) down and released its Llama3.1-8B model. This model,
    trained on over 15 trillion tokens has almost a 2,000:1 data density ratio and
    boasts even higher performance than the Llama 2 series.^([7](ch07.html#id930))
    Sensing a trend? Meta kept its SLM pretty much the same size, but improved performance
    significantly, just by training on more data!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 2023 年 2 月，Meta 开源了其 Llama 2 模型系列，该系列在约 200 亿次训练数据上进行了训练（在当时，这被认为是一大批数据）。在 Llama
    2 系列中，70 亿规模的模型几乎有 300:1 的缩放比率！到 2024 年 8 月，随着 Llama 3 的发布，Meta 加倍（实际上八倍）投入并发布了
    Llama3.1-8B 模型。这个模型在超过 1500 亿次令牌上进行了训练，具有近 2000:1 的数据密度比率，并且比 Llama 2 系列表现出更高的性能。[^7](ch07.html#id930)
    感觉到趋势了吗？Meta 保持其 SLM 的大小几乎不变，但仅通过在更多数据上训练，就显著提高了性能！
- en: Table 7-2\. Scaling laws over time
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7-2\. 随时间变化的缩放定律
- en: '| Date | Number training tokens/parameter | Scaling law |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 日期 | 训练令牌/参数数量 | 缩放定律 |'
- en: '| --- | --- | --- |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1/23/20 | 1.7 | Kaplan |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 1/23/20 | 1.7 | Kaplan |'
- en: '| 3/29/22 | 20 | Chinchilla |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 3/29/22 | 20 | Chinchilla |'
- en: '| 2/1/23 | 286 | Llama-2-7B |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 2/1/23 | 286 | Llama-2-7B |'
- en: '| 8/1/23 | 1875 | Llama-3.1-8B |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 8/1/23 | 1875 | Llama-3.1-8B |'
- en: In fact, in the technical paper accompanying that release, “The Llama 3 Herd
    of Models,” Meta cited that its 405B parameter flagship model, also trained on
    ~15 trillion tokens, is “approximately compute optimal” from a training perspective,
    but that its smaller models were trained “for much longer than is compute-optimal.
    The resulting models perform better than compute-optimal models at the same inference
    budget.”^([8](ch07.html#id931)) Quite simply, while these smaller models were
    more expensive to train (trained for longer on more data), they are far more efficient
    to run at inference time. The result? Today, the Llama 3 models are some of the
    most popular open source models available, and we expect that when it arrives
    sometime in 2025, Llama 4 will be just as popular.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，在随该发布版本附带的论文《Llama 3 模型群》中，Meta 引用了其 405B 参数的旗舰模型，该模型也是在约 1500 亿次令牌上训练的，从训练角度来看，“大约是计算最优的”，但较小的模型则是“训练时间远超过计算最优”。结果是，这些模型在相同的推理预算下比计算最优模型表现更好。[^8](ch07.html#id931)
    简单来说，虽然这些较小的模型训练成本更高（在更多数据上训练时间更长），但在推理时间上效率更高。结果是？今天，Llama 3 模型是可用的最受欢迎的开源模型之一，我们预计到
    2025 年左右，Llama 4 将同样受欢迎。
- en: 'Bringing this back to SLMs: with data ratios that require over hundreds of
    tokens of data for every parameter in a model, inference-optimized models and
    SLMs start to mean the same thing. It is near impossible to have a big, inference-optimized
    LLM. Given data acquisition costs and the amount of data available in the world,
    these data ratios are simply too expensive to support training inference-optimal
    LLMs that are hundreds of billions of parameters in size. We just don’t have enough
    data.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个问题带回到 SLM 上：当模型中每个参数需要超过数百个数据令牌的数据比率时，推理优化的模型和 SLM 开始意味着同一件事。拥有一个大型、推理优化的
    LLM 几乎是不可能的。考虑到数据获取成本和世界上可用的数据量，这些数据比率对于支持训练推理优化的、参数规模达数百亿的 LLM 来说成本太高。我们真的没有足够的数据。
- en: There is a real question of when will we hit the data ceiling? Today’s models
    are trained on upward of 15 trillion tokens, but to get there, model providers
    have basically had to plumb the entirety of the internet. And, as you will see
    in the next section, we don’t need large quantities of *any* data, we need volumes
    of *very high-quality* data, which is even more difficult to obtain.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们何时达到数据上限，这是一个真正的问题？今天的模型是在超过 1500 亿次令牌上训练的，但要达到这个水平，模型提供商基本上不得不挖掘整个互联网。正如你将在下一节中看到的，我们不需要大量任何类型的数据，我们需要的是大量**非常高质量**的数据，这甚至更难获得。
- en: Data quality
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据质量
- en: Can you imagine the song “Cecelia” without Garfunkel and just Simon? And could
    Hall & Oates have put anyone’s “Kiss on My List” if they didn’t start that song’s
    opening with a 1980s combination of keyboards and a cheesy mustache that sublimely
    screamed, “I got the romance covered? You just press the play button?” (Yes, younger
    readers...back then we had to press an actual clunky physical button.) And although
    we’re dating ourselves musically, it’s not only difficult to understand how great
    these songs could have been without the partnerships, it’s just as difficult to
    isolate the impact of data quantity from the impact of data quality in an LLM.
    Quality and data and great high-performing efficient models go together...just
    like Simon & Garfunkel and Hall & Oates.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 你能想象没有加芬克尔只有西蒙的《Cecelia》这首歌吗？还有，如果Hall & Oates没有在歌曲开头用1980年代键盘和滑稽的胡须组合来完美地尖叫“我已经覆盖了浪漫？你只需按下播放按钮？”（是的，年轻的读者们...那时候我们不得不按下实际的笨重物理按钮），他们还能让任何人的“Kiss
    on My List”成为热门吗？（是的，年轻的读者们...那时候我们不得不按下实际的笨重物理按钮。）尽管我们在音乐上有些过时，但不仅难以理解如果没有这些搭档，这些歌曲会多么出色，同样难以将数据量的影响从数据质量的影响中分离出来，在LLM中也是如此。质量和数据以及高性能高效的模型是相辅相成的...就像Simon
    & Garfunkel和Hall & Oates一样。
- en: 'Now, if you believe that the internet has only trustworthy data, that internet
    data has no bias, profanity, hate, lies, or anger...none of that, then you can
    probably stop reading this book. That belief is akin to eating a gallon of ice
    cream a day and wondering how your jeans shrank when you only wash them in cold
    water. When it comes to GenAI, the adage still applies: garbage in, garbage out!
    The reality *still* holds that the more you can do to curate the data used to
    train your model (both in terms of securing large quantities of it and with with
    high-quality labeled examples), the more performance you can pack into your model.
    And while there are some techniques around improving your model’s performance
    after it is trained—like retrieval-augmented generation (RAG) and more, these
    techniques all benefit from a high-quality data starting point (more on that in
    a bit).'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你认为互联网上只有可信的数据，互联网数据没有偏见、粗话、仇恨、谎言或愤怒...没有这些，那么你可能可以停止阅读这本书了。这种信念就像每天吃一加仑冰淇淋，然后想知道为什么牛仔裤在只洗冷水的情况下会缩水。当涉及到GenAI时，这个谚语仍然适用：垃圾输入，垃圾输出！现实仍然如此：你能做更多的事情来整理用于训练你的模型的数据（无论是确保大量数据还是高质量标记的示例），你就能将更多性能打包到你的模型中。虽然有一些提高训练后模型性能的技术——比如检索增强生成（RAG）等，但这些技术都受益于高质量的数据起点（稍后还会详细介绍）。
- en: 'Microsoft publicly credits data quality playing a critical role for enabling
    its (at the time) state-of-the-art (SOTA) Phi-2 2.7 billion parameter SLM that
    in some benchmarks outperformed larger models 25 times its size. But you could
    tell Microsoft had sniffed out this path forward before Phi-2 because it introduced
    its predecessor (Phi-1) to the world through a research publication^([9](ch07.html#id935))
    titled “Textbooks Are All You Need.” In this paper, Microsoft described how “high-quality
    data can even improve SOTA LLMs while dramatically reducing the dataset size and
    training compute.” And in the same way humans learn better from clearly laid-out
    textbooks, Microsoft’s findings support that textbook-quality training data that
    is “clear, self-contained, instructive, and balanced” results in better-performing
    LLMs that demonstrate better scaling laws; and of course, this enabled LLMs with
    the scale and performance of Phi-2 to become (at the time) SOTA. At the time of
    publishing this book, Microsoft had just released their fourth iteration of this
    SLM: Phi-4\. Similarly to Phi-1 and Phi-2, Microsoft cited “improved data” (among
    other training advancements) as a core driver to [Phi-4](https://oreil.ly/he3Nf)
    achieving strong performance relative to its size.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 微软公开表示，数据质量在使其（当时）最先进的（SOTA）27亿参数的Phi-2 SLM中发挥了关键作用，在某些基准测试中，其性能超过了25倍大小的更大模型。但你可以看出，在Phi-2之前，微软就已经嗅出了这条前进的道路，因为它通过一篇题为“Textbooks
    Are All You Need.”的研究论文^([9](ch07.html#id935))向世界介绍了其前身（Phi-1）。在这篇论文中，微软描述了“高质量数据甚至可以改善SOTA
    LLM，同时大幅减少数据集大小和训练计算。”同样，人类从清晰呈现的教科书中学习得更好，微软的研究结果支持教科书质量训练数据“清晰、自包含、指导性、平衡”，这会产生性能更好的LLM，并展示出更好的扩展规律；当然，这也使得Phi-2这样的LLM在规模和性能上成为（当时）的SOTA。在本书出版时，微软刚刚发布了其第四代这种SLM：Phi-4。与Phi-1和Phi-2类似，微软将“改进数据”（以及其他训练进步）列为Phi-4相对于其规模实现强大性能的核心驱动力之一。[Phi-4](https://oreil.ly/he3Nf)。
- en: Though we talked about this earlier in the book, it’s so important we thought
    we’d repeat it here because high quality data is critical to SLMs. While many
    model providers are transparent about the amount of data used to train an LLM,
    *very few* providers are transparent about the actual sources of data that were
    used to train *their* LLM. In fact, if you asked the most popular LLM providers
    what data they used to train their model, they either won’t be able to tell you
    or tell you it’s none of your business, to which you should reply, *“But this
    is my business!”*
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在本书中之前已经讨论过这一点，但由于高质量数据对于SLM至关重要，我们认为有必要在此重复强调。虽然许多模型提供商对用于训练LLM的数据量是透明的，但*非常少*的提供商对用于训练*他们*的LLM的实际数据来源是透明的。实际上，如果你问最受欢迎的LLM提供商他们使用什么数据来训练他们的模型，他们要么无法告诉你，要么告诉你这与你无关，对此你应该回答，“但这是我的业务！”*
- en: 'The bottom line is that the highest quality datasets are long textbooks or
    other nonfiction books written and copyrighted by humans—not mid-starred or higher
    Reddit posts and other free-form information sources. High-quality data artifacts
    aren’t generic snapshots of web content put on public sites that automated crawlers
    can collect. The ugly truth behind many popular LLMs is that their inclusion of
    many of the best quality datasets (such as the Books3^([10](ch07.html#id938))
    corpus we first introduced you to in [Chapter 5](ch05.html#ch05_live_die_buy_or_try_much_will_be_decided_by_ai_1740182048942635))
    is unfortunately only available for use in model training because they were pirated
    and posted without author permission. Again, some of our own previous hard work
    was vacuumed into the inner bowels of multiple LLMs for all to take advantage
    of and others to profit from. We didn’t get a choice. We weren’t even asked; it
    just happened. And while we’re not filing suit (it’s not like we wrote some catchy
    bestseller titled *50 Shades of Big Data*that flew off the shelves and Hollywood
    wanted to make into a movie), there are a lot of people whose livelihoods and
    business differentiation were “stolen” to make the LLM you’ve also likely used.
    This all goes back to the value exchange discussion we had in [“How Do You Consume
    AI: Be Ye a Value Creator or a Value User?”](ch02.html#ch02_how_do_you_consume_ai_be_ye_a_value_creator_or_a_1740182046163502).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 核心问题是，最高质量的数据集是那些由人类编写并拥有版权的长篇教科书或其他非虚构书籍，而不是中等星级或以上的Reddit帖子和其他自由形式的信息来源。高质量的数据工件并不是放在公共网站上，自动化爬虫可以收集的网页内容的通用快照。许多流行的LLM背后令人不快的真相是，它们包含了许多最佳质量的数据集（例如，我们在第5章（ch05.html#ch05_live_die_buy_or_try_much_will_be_decided_by_ai_1740182048942635）首次向您介绍的Books3^([10](ch07.html#id938))语料库）仅可用于模型训练，因为这些数据集被非法复制并发布，没有得到作者的许可。再次强调，我们的一些前期辛勤工作被吸入了多个LLM的内部深处，供所有人利用，也让其他人从中获利。我们没有选择权。甚至没有人询问我们；这只是一场意外。虽然我们并没有提起诉讼（我们并没有写一本像《大数据的五十度灰》这样的畅销书，它从货架上飞走，好莱坞还想要将其拍成电影），但有许多人的生计和业务差异化被“窃取”，以制作出你很可能也使用过的LLM。这一切都回到了我们在“如何消费AI：成为价值创造者还是价值使用者？”（ch02.html#ch02_how_do_you_consume_ai_be_ye_a_value_creator_or_a_1740182046163502）中讨论的价值交换问题。
- en: Only transparent data collection and curation policies can ensure that the LLMs
    you’re evaluating for your business did not benefit from unethically sourced data.
    The takeaway? When evaluating SLMs, where data curation is critical for driving
    performance (and putting aside the legal ramifications), having a heightened awareness
    of how the data behind the model was sourced is crucial. Ask questions. Demand
    answers.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 透明的数据收集和整理政策可以确保您为业务评估的LLM没有从非道德来源的数据中获益。结论是？在评估SLM时，数据整理对于驱动性能至关重要（且不考虑法律后果），对模型背后的数据来源有高度的认识是至关重要的。提出问题。要求回答。
- en: Domain specialization
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 领域专业化
- en: Being the weekend athlete you are, you find yourself back at home with an ankle
    giving you mixed signals—it’s either auditioning for a spot on the soon-to-be-a-hit
    reality show, “So You Think You Broke Your Ankle,” or it’s just being dramatic
    with a sprain. Either way, it’s demanding ice and attention. Now it’s up to you
    to figure out what’s going on. To make this determination, do you ask the smartest
    person you know, or do you ask a doctor? (Don’t be cheeky...we know some of you
    just said aloud, “The smartest person I know *is* a doctor.”) While the smartest
    person you know might have amazing talents that span poetry, chemistry, philosophy,
    and more, you’re far better off asking a doctor, even better if they specialize
    in orthopedics. That doctor’s poetry skills be damned; when the question at hand
    is specialized in nature (your potentially fractured ankle), it is more important
    to ask a specialized expert than a general expert.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 作为你这样的周末运动员，你发现自己在家，脚踝给你发出了混合的信号——它要么是在为即将成为热门的真人秀节目“你认为你扭伤了脚踝吗？”试镜，要么只是因为扭伤而夸张地表现。无论如何，它都需要冰敷和关注。现在，你必须弄清楚发生了什么。为了做出这个判断，你会问你知道的最聪明的人，还是会问医生？（不要厚颜无耻……我们知道有些人可能大声说，“我知道的最聪明的人*就是*医生。”）虽然你知道的最聪明的人可能有跨越诗歌、化学、哲学等多个领域的惊人才能，但你最好还是问医生，如果他们专攻骨科就更好了。那位医生的诗歌技能无关紧要；当手头的问题具有专业性质（你可能骨折的脚踝）时，询问专业专家比询问普通专家更重要。
- en: As it turns out, the same holds true for SLMs. And as you’ve likely figured
    out by now (because it’s a section in this chapter), there’s increasing evidence
    that smaller, specialized models can meet or beat larger general-purpose LLMs
    when evaluated on *specialized* tasks. And when we say a specialized model, what
    we really mean is a model that is trained on a significant amount of *domain-specific*
    data. For example, in late 2022, a team from Stanford announced [BioMedLM](https://oreil.ly/QrqbE),^([11](ch07.html#id945))
    a 2.7 billion parameter model trained on biomedical literature data. When evaluated
    on United States Medical Licensing Examination (USMLE) questions, a fine-tuned
    version of BioMedLM outperformed a similarly fine-tuned unspecialized model of
    the same size (GPT Neo) by 17%. When evaluated against an untuned model that was
    44 times bigger (Meta’s Galactica 120B model), BioMedLM outperformed it by almost
    6%. But the critical point is whether or not Galactica was good for the task at
    hand; unlike BioMedLM, Galactica’s size made fine-tuning it cost prohibitive.
    At just 2.7 billion parameters, the tiny BioMedLM LLM demonstrated it could maintain
    a specialized advantage while also allowing further customization for fine-tuning.
    This is a very early example of the impact of domain specialization in GenAI,
    but these examples have kicked off a huge area of research and application of
    specializing models on targeted use cases.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，这一点对SLMs（自监督语言模型）同样适用。而且，你很可能已经注意到了（因为这是本章的一个部分），越来越多的证据表明，在*特定*任务上评估时，较小的、专业的模型可以与较大的通用LLMs（大型语言模型）相媲美，甚至超越它们。当我们说一个专业的模型时，我们真正指的是在大量*特定领域*数据上训练的模型。例如，在2022年底，斯坦福大学的一个团队宣布了[BioMedLM](https://oreil.ly/QrqbE)，^([11](ch07.html#id945))，这是一个在生物医学文献数据上训练的27亿参数模型。当在评估美国医学执照考试（USMLE）问题时，BioMedLM的一个微调版本比同样微调的同等规模的非专业模型（GPT
    Neo）表现好了17%。当与一个44倍大的未调优模型（Meta的Galactica 120B模型）相比时，BioMedLM几乎表现好了6%。但关键点在于Galactica是否适合当前的任务；与BioMedLM不同，Galactica的大小使得微调它成本过高。仅2.7亿参数的微型BioMedLM
    LLM展示了它能够在保持专业优势的同时，也允许进一步的定制以进行微调。这是GenAI（生成人工智能）中领域专业化影响的一个非常早期的例子，但这些例子已经开启了一个针对特定用例的专业化模型研究和应用的大领域。
- en: Note
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Despite seemingly performing well on the medical-based benchmark in Stanford’s
    tests, Meta’s Galactica (specifically designed to help scientists) was launched
    into the scientific community with a big bang—until it came crashing down with
    a thud and was taken offline just three days after its general availability. Public
    experimentation brought to light many examples of bias, toxicity, and hallucinations
    that led to scientific nonsense. This is why it’s important to fully appreciate
    what we discussed in [Chapter 5](ch05.html#ch05_live_die_buy_or_try_much_will_be_decided_by_ai_1740182048942635).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在斯坦福的基于医学的基准测试中似乎表现良好，但Meta的Galactica（专门设计来帮助科学家）在科学界引起了巨大的轰动——直到它突然崩溃并在线下线，仅仅在通用可用后的三天。公开实验揭示了众多偏见、毒性和幻觉的例子，导致了科学上的胡言乱语。这就是为什么充分理解我们在[第5章](ch05.html#ch05_live_die_buy_or_try_much_will_be_decided_by_ai_1740182048942635)中讨论的内容非常重要。
- en: Specialization can be especially important for “low resource” domains, areas
    where there isn’t a lot of data. For example, in [Chapter 4](ch04.html#ch04_the_use_case_chapter_1740182047877425)
    we told you how the IBM Z (mainframe) runs most of the world’s transactions. In
    the parlance of LLMs, something classified as *low resource* are those domains
    with very little data available for training AI systems. As you can imagine, COBOL
    is considered a *low-resource* language, as there is very little public domain
    enterprise-worthy COBOL data today, especially when compared to Python, SQL, and
    other popular coding languages (yes, lots of business logic is coded in SQL).
    But there’s a lot of COBOL out there running businesses—the most critical parts.
    In fact, Reuters estimates^([12](ch07.html#id949)) that today there are over 230
    billion lines of COBOL code—supporting over $3 trillion of commerce—actively running
    in enterprises.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在“低资源”领域，专业化可能特别重要，这些领域的数据量并不多。例如，在[第4章](ch04.html#ch04_the_use_case_chapter_1740182047877425)中，我们告诉你们IBM
    Z（大型机）运行着世界上大部分的交易。在LLM的术语中，被归类为“低资源”的是那些为训练AI系统提供的数据非常少的领域。正如你可以想象的那样，COBOL被认为是一种“低资源”语言，因为今天几乎没有公开领域的、值得企业使用的COBOL数据，尤其是与Python、SQL和其他流行的编程语言（是的，大量的业务逻辑是用SQL编写的）相比。但有很多COBOL代码正在运行业务——最关键的部分。事实上，路透社估计^([12](ch07.html#id949))，今天有超过2300亿行COBOL代码——支持着超过30万亿美元的贸易——正在企业中积极运行。
- en: Note
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: For clarity, the IBM Z supports modern application development tool sets and
    methodologies like fully automated continuous integration/continuous deployment
    (CI/CD) pipelines using Jenkins and Zowe, Kafka streams, node.js, Kubernetes,
    Ansible, Terraform, and more. But there is a lot of critical business logic built
    a long time ago that was written in COBOL that is deemed mission critical.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清晰起见，IBM Z支持现代应用程序开发工具集和方法，如使用Jenkins和Zowe的完全自动化的持续集成/持续部署（CI/CD）管道、Kafka流、node.js、Kubernetes、Ansible、Terraform等。但有很多关键的业务逻辑是在很久以前用COBOL编写的，被认为是至关重要的。
- en: For all those code-assist LLMs that scraped code repositories to build a code-tuned
    LLM, guess how much COBOL is available for use? For example, one popular dataset
    for training code-assist LLMs is GitHub Codespaces—it contains 1 terabyte of code
    from 32 different languages. But COBOL is not covered. Why not? Remember earlier
    in this book how critical your data is and how today’s LLMs aren’t built on enterprise
    data. Now think back to those transactions running on IBM Z (credit cards, ATMs,
    airlines). Do you think that code is just sitting there ready to be scraped by
    the world? Of course not! So how could an LLM help in this scenario?
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有那些通过抓取代码仓库来构建代码调优的LLM（代码辅助语言模型），猜猜有多少COBOL代码可供使用？例如，一个用于训练代码辅助LLM的流行数据集是GitHub
    Codespaces——它包含了来自32种不同语言的1TB代码。但COBOL并未包含在内。为什么？记得在这本书的早期部分，我们讨论了你的数据是多么关键，以及今天的LLM并不是建立在企业数据之上的。现在回想一下在IBM
    Z（大型机）上运行的那些交易（信用卡、ATM、航空公司）。你认为这些代码只是在那里等着被世界抓取吗？当然不是！那么LLM在这种情况下能提供什么帮助呢？
- en: Back in 2023, IBM Research trained a 20 billion parameter code model (called
    granite.20b.cobol) that specializes in COBOL. To specialize a model specifically
    on COBOL, the IBM Research team held aside separately acquired COBOL data, trained
    a general-purpose code model first, and then specialized that model by training
    it further on a dataset that was highly concentrated with high-quality curated
    COBOL data (this is just like your proprietary data waiting to be put to work).
    The end result? The COBOL-focused SLM model [significantly outperformed ChatGPT
    for COBOL completions on the CodeNet benchmark datasets](https://oreil.ly/H4Sgk).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 回到2023年，IBM研究团队训练了一个200亿参数的代码模型（称为granite.20b.cobol），该模型专注于COBOL。为了专门针对COBOL进行模型训练，IBM研究团队将单独获取的COBOL数据保留在一边，首先训练了一个通用代码模型，然后通过在高度集中且高质量整理的COBOL数据集上进一步训练，将该模型专业化（这就像你的专有数据等待投入使用一样）。最终结果？专注于COBOL的SLM模型在CodeNet基准数据集上的COBOL补全任务中[显著优于ChatGPT](https://oreil.ly/H4Sgk)。
- en: The takeaway? Purpose-built foundation models with quality at their core means
    better performance and more efficiency. This concept will become hugely important
    in [Chapter 8](ch08.html#ch08_using_your_data_as_a_differentiator_1740182052172518)
    as we discuss how you can specialize pretrained models using your enterprise data.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 吸取的教训？以质量为核心定制的基座模型意味着更好的性能和更高的效率。这个概念在我们讨论如何使用企业数据专门化预训练模型时，在[第8章](ch08.html#ch08_using_your_data_as_a_differentiator_1740182052172518)中将变得极其重要。
- en: Think About This When It Comes to Data Curation
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 当涉及到数据整理时，请考虑以下几点
- en: Beyond the ethical considerations for data curation, understanding and appreciating
    data scaling laws and the impact of data quality and domain specialization on
    performance can help you find more cost-efficient SLM alternatives to bigger,
    less optimally trained, expensive-to-inference monster LLMs. As suggested before,
    older LLMs tend to be less data dense and, therefore, less inference efficient
    because they were trained back when the Kaplan and Chinchilla scaling laws first
    came out.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数据整理的伦理考量之外，理解和欣赏数据缩放定律以及数据质量和领域专业化对性能的影响，可以帮助你找到更经济的SLM替代方案，以替代更大、训练不那么优化、推理成本高昂的巨无霸LLM。正如之前所提到的，较老的LLM往往数据密度较低，因此推理效率也较低，因为它们是在Kaplan和Chinchilla缩放定律首次发布时进行训练的。
- en: And while data quantity is most relevant for those training a model from scratch,
    for anyone trying to customize already trained models, as we cover in [Chapter 8](ch08.html#ch08_using_your_data_as_a_differentiator_1740182052172518),
    the lessons on data quality and domain specialization still apply.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然数据量对于从头开始训练模型的人来说最为相关，但对于任何试图定制已训练模型的人来说，正如我们在[第8章](ch08.html#ch08_using_your_data_as_a_differentiator_1740182052172518)中讨论的那样，关于数据质量和领域专业化的教训仍然适用。
- en: Model Distillation—Using AI to Improve AI
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型蒸馏——利用AI提升AI
- en: 'Let’s talk about the second major technological innovation that is driving
    SLMs: model distillation. Model distillation is often used when you want the accuracy
    of a large neural network but need something more practical for real-time applications
    or devices with limited computational power. It’s really another technique to
    pack big-model performance into a small form factor; and while at first blush
    it might seem like a bit of a hack, it is actually an incredibly powerful tool.
    Model distillation is where a large frontier (big, expensive, state-of-the-art)
    model, such as Llama-3.1-405B, can instruct a smaller model, such as Llama3.1-8B,
    teaching it to behave like the bigger model.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们谈谈推动SLM的第二个主要技术创新：模型蒸馏。模型蒸馏通常用于当你想要大型神经网络的准确性，但又需要更实用的实时应用或计算能力有限的设备时。这实际上是一种将大模型性能打包到小尺寸的技术；虽然乍一看可能像是一种小技巧，但实际上它是一个非常强大的工具。模型蒸馏是大型前沿（大、昂贵、最先进）模型，如Llama-3.1-405B，可以指导一个较小的模型，如Llama3.1-8B，教它表现得像更大的模型。
- en: A great example of this is a would be trying to replicate Tootsie Tomanetz’s
    BBQ mastery. This 85-year old custodian by day and pitmaster by night is the legend
    behind the famous Hill Country BBQ (Texas).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这的一个很好的例子是试图复制Tootsie Tomanetz的烧烤大师技艺。这位85岁的日间看护员和夜间烧烤大师是著名希尔乡村烧烤（德克萨斯州）背后的传奇人物。
- en: She’ll outright tell you that if she gave you the recipe, you still couldn’t
    recreate what she does. We’ve all been there—trying to capture the magic of a
    grandparent’s cooking, only to realize it’s more than just ingredients; it’s a
    lifetime of love and technique.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 她会直接告诉你，即使她给了你食谱，你也无法复制她所做的。我们都有过这样的经历——试图捕捉祖父母烹饪的魔法，结果发现这不仅仅是配料；这是一生的爱和技巧。
- en: For example, when asked what the right temperature was to start a beef brisket
    cook, she notes she has no idea...she just puts her hand on the smoker and goes
    by feel. (That reminded us of one of our grandmothers who used her finger as a
    pincushion.) But we’re willing to bet that if we could spend a week with Tootsie
    and pepper her (no pun intended) with nonstop questions, we could eventually learn
    how to make a pretty close to award-winning beef brisket. We surely wouldn’t know
    all the she knows. For example, we wouldn’t know how she makes her incredible
    sauces. But if you gave us another week of nonstop questions, we would likely
    be able to figure something pretty good there too. Next up, the chicken.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当被问及开始炖牛肉块的正确温度时，她表示她不知道……她只是把手放在烟熏炉上，凭感觉。 (这让我们想起了我们的一位祖母，她用手指当针垫。) 但我们愿意打赌，如果我们能和Tootsie待上一周，不停地问她问题（无意中打趣），我们最终可以学会如何制作接近获奖水平的牛肉块。我们当然不会知道她所知道的一切。例如，我们不会知道她如何制作她那令人难以置信的酱汁。但如果再给我们一周的时间不停地提问，我们很可能也能在那里找到一些很好的东西。接下来是鸡肉。
- en: Essentially, model distillation is like extracting all the essential knowledge
    from a heavyweight model into a more lightweight version, so you get similar performance
    but with less complexity.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，模型蒸馏就像是从一个重型模型中提取所有必要知识到一个更轻量级的版本，这样你就能获得相似的性能，但复杂性更低。
- en: In a lot of ways, model distillation is just a new, cheaper way to create training
    data. As LLMs become better and better at different tasks, they become powerful
    tools for generating training data that used to need to be defined by hand by
    an army of data annotators. To perform distillation, research scientists leverage
    a teacher model (the large all-knowing one) to generate a large amount of synthetic
    data that exemplifies a target set of behaviors the teacher model knows how to
    perform (like the cooking skill in our example). This synthetic data is often
    conversational in nature, representing question-answer (QA) pairs, or multiturn
    conversations. The synthetic data is then used to fine-tune the smaller (student)
    model, thereby imbuing the behavior patterns of the larger model into the smaller
    model. And while it may first appear this technique is only surface level, getting
    the small model to mimic the larger model’s performance has been shown to be incredibly
    powerful. In fact, back in 2023, in an early example of model distillation, researchers
    from the Large Model Systems (LSMYS) Organization distilled ChatGPT down into
    a 13 billion parameter model called Vicuna. Vicuna’s performance shocked the community
    when they first published their work. LSMYS reported^([13](ch07.html#id960)) that
    their distilled ChatGPT model “achieves more than 90% quality [referring to its
    responses] of OpenAI ChatGPT.”
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在很多方面，模型蒸馏仅仅是一种新的、更便宜的方式来创建训练数据。随着大型语言模型（LLMs）在各项任务上变得越来越出色，它们成为了生成训练数据的强大工具，这些训练数据过去需要由一群数据标注员手动定义。为了进行蒸馏，研究科学家利用一个教师模型（即那个无所不知的大型模型）生成大量合成数据，这些数据体现了教师模型知道如何执行的目标行为集合（比如我们例子中的烹饪技能）。这种合成数据通常是会话式的，代表问答（QA）对或多轮对话。然后，这些合成数据被用来微调较小的（学生）模型，从而将较大模型的模式行为注入到较小的模型中。虽然这种技术最初可能看起来只是表面上的，但让小模型模仿大模型的性能已被证明是非常强大的。事实上，早在2023年，在模型蒸馏的早期例子中，来自大型模型系统（LSMYS）组织的研究人员将ChatGPT蒸馏成了一个拥有130亿参数的模型，名为Vicuna。当他们在首次发布他们的工作时，Vicuna的表现震惊了整个社区。LSMYS报告称^([13](ch07.html#id960))，他们的蒸馏ChatGPT模型“在质量上超过了90%[指其回答]，与OpenAI的ChatGPT相当。”
- en: The open source community, including Stanford and LSMYS, were some of the first
    innovators leveraging this technique and have now become “victims” of their own
    success. Model distillation has gotten so popular (and competitively threatening)
    that most frontier model providers (like OpenAI, Google, Anthropic, among others)
    have written restrictions into their model’s usage terms and conditions stating
    that their models cannot be used to improve the performance of other competitive
    models.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 开源社区，包括斯坦福和LSMYS，是第一批利用这项技术的创新者之一，现在已经成为他们自己成功的“受害者”。模型蒸馏变得如此流行（并且具有竞争威胁性），以至于大多数前沿模型提供商（如OpenAI、Google、Anthropic等）都在他们的模型使用条款和条件中写入了限制，声明他们的模型不能用于提高其他竞争模型的性能。
- en: While this limits the commercial viability of models distilled by the open source
    community, it is gangbusters for LLM providers with access to large models that
    make for perfect caffeine-infused teachers. For example, through its partnership
    with OpenAI, Microsoft released [Orca and Orca-2](https://oreil.ly/pLXEK), highly
    competitive SLMs that benefit from distillations of GPT-4\. And Google’s Gemini
    Nano and Gemini Pro are Google’s distilled version of its larger [Gemini models](https://oreil.ly/mzSFf).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这限制了开源社区蒸馏模型的商业可行性，但对于能够利用大型模型作为完美咖啡因注入型教师的LLM提供商来说，这是一个巨大的机遇。例如，通过与其合作伙伴OpenAI的合作，微软发布了[Orca和Orca-2](https://oreil.ly/pLXEK)，这些具有高度竞争力的SLM从GPT-4的蒸馏中受益。而Google的Gemini
    Nano和Gemini Pro则是Google对其更大[Gemini模型](https://oreil.ly/mzSFf)的蒸馏版本。
- en: As this technique continues to improve, due consideration is needed on whether
    super-large models will ever be used for anything other than teaching smaller,
    faster, and more cost-efficient distilled models. For example, when NVIDIA released
    its 340 billion parameter model, Nemotron-4-340B-Instruct, the primary use case
    highlighted on the model card was to “create training data that helps researchers
    and developers build their own LLMs” (aka model distillation).^([14](ch07.html#id968))
    Hosting a 340 billion parameter model for running live inference could be incredibly
    expensive. You better have a pretty high value use case to justify that deployment.
    *But*, using the model once to generate synthetic training data for a smaller
    model is a much more palatable one-time fixed cost that enables the deployment
    of a cheaper, smaller, and more performant model.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 随着这项技术的持续改进，我们需要认真考虑超级大型模型是否会被用于除了教授更小、更快、更经济的蒸馏模型之外的其他用途。例如，当NVIDIA发布了340亿参数的模型Nemotron-4-340B-Instruct时，在模型卡片上突出显示的主要用途是“创建帮助研究人员和开发者构建他们自己的LLM的训练数据”（即模型蒸馏）^([14](ch07.html#id968))。运行一个340亿参数模型进行实时推理可能会非常昂贵。你最好有一个相当有价值的用例来证明这种部署的合理性。“但是”，使用该模型一次来为较小的模型生成合成训练数据，这是一个更令人满意的单一固定成本，使得部署更便宜、更小、性能更优的模型成为可能。
- en: And while closed frontier model providers currently have a “competitive moat”
    for their SLMs thanks to their teaching models, we think there is huge potential
    for disruption. Very large open source models, like Nemotron-4-340B-Instruct,
    Llama-3.1-405B, and most recently DeepSeek-R1, are proving to be powerful teacher
    models, eroding this advantage.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管封闭的前沿模型提供商目前由于他们的教学模型而对其SLM拥有“竞争优势”，但我们认为颠覆的潜力巨大。像Nemotron-4-340B-Instruct、Llama-3.1-405B和最近最著名的DeepSeek-R1这样的大型开源模型正在证明自己是强大的教师模型，侵蚀了这种优势。
- en: As noted earlier in this book, a Chinese AI lab, DeepSeek, open sourced its
    family of large 671 billion parameter Mixture of Experts (MoE) style LLMs, including
    the now famous DeepSeek-R1 model. The DeepSeek model release is fascinating from
    a number of different dimensions, the least of which was how, in response to the
    release, NVIDIA’s market cap dropped by $600 billion in one day^([15](ch07.html#id972))
    as spectators around the world were amazed at the performance and reasoning capabilities
    delivered by a Chinese AI lab for a reported training cost (which wasn’t fully
    understood by the press and those that reacted to it) of $5.6 million.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如本书前面所述，一家中国人工智能实验室DeepSeek开源了其家族中的大型6710亿参数混合专家（MoE）风格的LLM，包括现在著名的DeepSeek-R1模型。DeepSeek模型的发布从多个不同维度来看都令人着迷，其中最不重要的是，在发布后，NVIDIA的市值在一天内下降了6000亿美元^([15](ch07.html#id972))，因为全世界的观众都对一家中国人工智能实验室提供的性能和推理能力感到惊讶，据报道的训练成本为560万美元（这些成本并未被媒体和对此作出反应的人完全理解）。
- en: There is a lot to unpack here, particularly around the reported training costs,
    some of which we are going to discuss toward the end of this chapter as we cover
    the MoE architecture. But a large part of the DeepSeek release is actually an
    important story about the role of teacher models and model distillation—specifically,
    the collection of smaller “DeepSeek-Distill” reasoning models that were released
    alongside the much larger DeepSeek-R1 reasoning model.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有很多东西需要解释，尤其是关于报告的训练成本，其中一些我们将在本章末尾讨论MoE架构时进行讨论。但DeepSeek发布的大部分内容实际上是一个关于教师模型和模型精炼作用的重要故事——特别是与更大的DeepSeek-R1推理模型一起发布的较小“DeepSeek-Distill”推理模型集合。
- en: In order for DeepSeek to build efficient SLMs with reasoning capabilities, they
    first used R1 to generate a large volume (800k samples) of examples of reasoning
    in math and code domains.^([16](ch07.html#id977)) Then they took that dataset
    and fine-tuned a set of open, third-party models produced by Meta (Llama) and
    Alibaba Cloud (Qwen), whose sizes ranged from 1.5 billion to 70 billion parameters,
    et voilà! A series of small DeepSeek-R1-Distill models with advanced math and
    code reasoning capabilities was born.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让DeepSeek构建具有推理能力的有效SLM，他们首先使用R1生成大量（800k个样本）数学和代码领域的推理示例.^([16](ch07.html#id977))
    然后，他们使用这个数据集微调了一系列由Meta（Llama）和阿里巴巴云（Qwen）生产的公开第三方模型，这些模型的大小从15亿到700亿参数不等，就这样！一系列具有高级数学和代码推理能力的DeepSeek-R1-Distill小模型诞生了。
- en: DeepSeek’s success in distilling reasoning capabilities into small models has
    inspired the open source community. Within days of the DeepSeek-R1 and DeepSeek-R1-Distill
    models being released, the open source community created distillation pipelines
    so that anyone could perform a similar distillation process using the SLM of their
    choice.^([17](ch07.html#id978)) Similarly, in less than one month, over 400 DeepSeek-based
    distillation datasets were posted to Hugging Face so that others can easily leverage
    DeepSeek’s outputs in their model development pipelines!^([18](ch07.html#id979))
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: DeepSeek将推理能力精炼到小型模型中的成功激励了开源社区。在DeepSeek-R1和DeepSeek-R1-Distill模型发布后的几天内，开源社区创建了精炼管道，使得任何人都可以使用他们选择的SLM执行类似的精炼过程.^([17](ch07.html#id978))
    同样，不到一个月的时间，就有超过400个基于DeepSeek的精炼数据集发布到Hugging Face，以便其他人可以轻松地在他们的模型开发管道中利用DeepSeek的输出！^([18](ch07.html#id979))
- en: In many ways, improving the open source community’s ability to create powerful,
    distilled models may be one of the biggest long-term impacts of the DeepSeek release—this
    is why we saw DeepSeek as more of an iterative open source AI event than a disruptive
    event. At the time of its release, DeepSeek-R1 was the most powerful teacher model
    available for open source model distillation. No doubt, its release also potentially
    puts pressure on other large, proprietary model providers to release open source
    versions of their models.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多方面，提高开源社区创建强大、精炼模型的能力可能是DeepSeek发布的最重大长期影响之一——这就是为什么我们将DeepSeek视为一个迭代开源AI事件，而不是一个颠覆性事件。在其发布时，DeepSeek-R1是开源模型精炼中最强大的教师模型。毫无疑问，其发布也可能会给其他大型、专有模型提供商带来压力，迫使他们发布自己模型的开源版本。
- en: Of course, you can easily see why large model incumbents might “fear” this process.
    Think about it. For a few thousand dollars (and a lot of AI expertise), a company
    could create its own proprietary distilled high-quality model that fuses its own
    data with frontier LLM performance. And, once trained, they could basically run
    these distilled SLMs for free. With the billions poured into big investment bets
    on anything GenAI, that’s bound to make a lot of investors nervous.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你可以轻易地看出为什么大型模型巨头可能会“害怕”这个过程。想想看。只需几千美元（以及大量的AI专业知识），一家公司就能创建自己的专有高质模型，将自身数据与前沿的LLM性能融合。一旦训练完成，他们基本上可以免费运行这些精炼的SLM。鉴于数十亿资金投入到了对任何GenAI的大规模投资赌注中，这无疑会让许多投资者感到紧张。
- en: It is important to note that distillation is not just limited to big teacher
    models improving much smaller student models. In fact, at the time this book was
    being written, OpenAI publicly disclosed that it was exploring whether DeepSeek
    illegally distilled OpenAI model IP into the large, 671-billion–parameter DeepSeek-R1
    model.^([19](ch07.html#id980)) Irony aside (more on that in the next chapter),
    this situation underscores the gravity of model distillation and the important
    role this technique will play moving forward in the future of AI development.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Think About This When It Comes to Model Distillation
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When considering models that have benefited from distillation for your use case,
    the most important consideration (as alluded to before) is the terms and conditions
    under which this model is eligible to be used, *especially* in the case of open
    source models. You need legal involved here because a distilled model could potentially
    inherit contractual terms from the teacher model and the base model that was tuned.
    For example, under the Meta Llama 3 Community License Agreement, all models distilled
    from a Llama 3 model have specific naming requirements (the model’s name needs
    to start with “Llama 3”), and they need to be licensed under the same Llama 3
    license.^([20](ch07.html#id984)) In extreme cases, the model could potentially
    have been distilled from a teacher model in violation of the terms of that model’s
    provider, as OpenAI is investigating with DeepSeek-R1\. This is yet another reason
    why transparency of data sources remains critical so that consumers can do their
    own due diligence on whether a model is suitable for use.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, it is critical that you understand the limitations of the teacher
    model and strategy that was used to do the actual distillation. To demonstrate
    what we mean, let’s take a look back at teacher model and distillation strategy
    of those DeepSeek-R1-Distill models:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Teacher model
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '*DeepSeek-R1.* As discussed above, this model demonstrates SOTA reasoning capabilities,
    but it also has a number of significant safety issues. A team from Cisco and the
    University of Pennsylvania found that DeepSeek-R1 “exhibited a 100% attack success
    rate, meaning it failed to block a single harmful prompt” in their automated jail-breaking
    attacks.^([21](ch07.html#id986)) Further, when asked factual questions for information
    about Tiananmen Square, the model declines (depending on where it is hosted) to
    respond. If asked, for example, “Do I need a passport to go to Taiwan?”, the model
    will immediately reply with: “According to the official policy of the Chinese
    government, Taiwan is an inalienable part of China’s territory” and “the Chinese
    government consistently upholds the One-China Principle and opposes any form of
    ‘Taiwan independence’ separatist activities.”^([22](ch07.html#id987))'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'What is the same about children holds true for teacher models and students:
    *The apple doesn’t fall far from the tree*. DeepSeek-R1 is likely to pass along
    these same safety concerns and political principles along to the student models,
    so think carefully before running to deploy in production.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 对于孩子和教师模型以及学生来说，相同的原则也适用：*苹果不会从树上掉得太远*。DeepSeek-R1可能会将这些相同的安全担忧和政治原则传递给学生模型，因此在部署到生产之前请仔细思考。
- en: Distillation strategy
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 蒸馏策略
- en: '*Generate targeted supervised fine tuning (SFT) data for code and math reasoning
    tasks.*DeepSeek took a very targeted and intentional approach in its distillation
    pipeline, focusing on code and math reasoning tasks to the exclusion of all else.
    This makes sense, if you only ever plan on using the distilled models for code
    and reasoning tasks. But a study from IBM Research found that these distilled
    models have sacrificed all ability to perform as a general-purpose model, failing
    at even basic instruction-following tasks.^([23](ch07.html#id988))'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*为代码和数学推理任务生成针对性的监督微调（SFT）数据。*DeepSeek在其蒸馏流程中采取了非常针对性和有意的做法，专注于代码和数学推理任务，排除了所有其他任务。如果你只打算使用蒸馏模型进行代码和推理任务，这很有道理。但IBM研究的一项研究发现，这些蒸馏模型已经牺牲了作为通用模型的所有能力，甚至在基本指令遵循任务上都失败了。[23](ch07.html#id988)'
- en: We dive into this further in [Chapter 8](ch08.html#ch08_using_your_data_as_a_differentiator_1740182052172518),
    but when taking advantage of model distillation, it is critical that your teacher
    model meets your requirements for both safety and performance and that the distillation
    approach you choose is aligned to your envisioned use of the model.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第8章](ch08.html#ch08_using_your_data_as_a_differentiator_1740182052172518)中进一步探讨这个问题，但当你利用模型蒸馏时，至关重要的是你的教师模型必须满足你对安全和性能的要求，并且你选择的蒸馏方法必须与你对模型预期用途的设想相一致。
- en: Where Are We Going Next? Small Language Models...Assemble!
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们下一步将去哪里？小型语言模型……集合！
- en: As you can see, SLMs clearly have many advantages, but one of the most exciting
    applications for leveraging them is not as a standalone specialist, but rather,
    as a system of models working together to do something amazing. It’s kind of like
    a bunch of tiny ants teaming up and marching off with an entire hamburger patty
    from your picnic, living the dream and pulling off what seems like the impossible.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，SLMs显然具有许多优势，但利用它们的最大兴奋应用并不是作为一个独立的专家，而是作为一个模型系统协同工作以完成一些惊人的事情。这有点像一群小蚂蚁团结起来，从野餐中带走整个汉堡肉饼，实现梦想，完成看似不可能的事情。
- en: At the time of writing, several key advances are coming from the AI research
    world. These advances demonstrate that by combining their powers, small models
    working together can sometimes outperform any given large model and do so at a
    fraction of the compute cost. And while these SLMs could operate independently
    (with good results), they can become even more impactful when orchestrated to
    perform in concert (yes, using AI). AI helping AI. This more systems-based approach
    to models performing tasks can happen externally to the model, using tools like
    model routing. Or, through architectures like MoE, a system of models with routing
    between experts that occur intrinsically within the model. Let’s get into both
    of these topics next.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，人工智能研究领域正涌现出几个关键进展。这些进展表明，通过结合它们的实力，协同工作的小型模型有时甚至可以超越任何特定的大型模型，并且只需其计算成本的一小部分。尽管这些小型语言模型（SLMs）可以独立运行（并取得良好效果），但当它们被编排在一起协同工作时（是的，使用AI）会产生更大的影响。AI帮助AI。这种更系统的方法，即模型执行任务的方式，可以在模型外部发生，使用诸如模型路由等工具。或者，通过如MoE（多专家架构）这样的架构，一个具有模型之间路由的专家系统，这种路由内在地发生在模型内部。接下来，让我们深入探讨这两个主题。
- en: Model Routing
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型路由
- en: 'On average, a bigger language model is going to perform better than a smaller
    language model on a given task. But, as you learned in this chapter, SLMs can
    operate as specialized experts that can outperform a big LLM if the task at hand
    is specialized in nature (like in the COBOL example). But even without intentional
    domain specialization, there can be unexpected variability in model performance
    across the many tasks you’re likely to send to your AI. This could be the case
    for many reasons: a model’s architecture, nuances in training data, parameter
    settings, data preparation, data sourcing, its alignment strategy...all of this
    (and more) could predispose any given smaller model to perform better on a task,
    independent of model size. The problem around the benefits of SLMs is that their
    performance advantages can be unpredictable, particularly if you don’t know what
    data they were trained on, making it difficult to predict which SLM you should
    use for your task.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 平均而言，在特定任务上，更大的语言模型通常会比较小的语言模型表现更好。但是，正如你在本章中学到的，SLMs可以作为专门的专家运行，如果手头的任务本质上是专业的（比如在COBOL示例中），它们甚至可以超越大型LLM。即使没有故意的领域专业化，模型在许多你可能会发送给AI的任务中的性能也可能存在意外的可变性。这可能是许多原因造成的：模型的架构、训练数据中的细微差别、参数设置、数据准备、数据来源、其对齐策略……所有这些（以及更多）都可能使任何给定的较小模型在任务上表现更好，而与模型大小无关。关于SLMs的益处的问题在于，它们的性能优势可能是不可预测的，尤其是如果你不知道它们是在什么数据上训练的，这使得预测你应该为你的任务使用哪个SLM变得困难。
- en: Of course, you could run every data point through every SLM you have to try
    and figure out which one(s) will work best. Don’t get us wrong—usually, putting
    the work in for something great is a good thing—but for this, you want something
    different. If you could somehow predict up front whether a smaller model would
    be suitable for your use case’s task list, then you could use that smaller model
    instead and save your company the extra inference and latency costs that might
    accompany a big oversized LLM for your needs. Quite simply, you’d optimize the
    usage of the big LLM to when you actually need it, instead of making the most
    expensive option the default or only choice.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你可以将每个数据点都通过你拥有的每个SLM来尝试，以找出哪个（些）会工作得最好。不要误解我们——通常，为了一项伟大的事业而努力是好事——但在这个例子中，你需要的是不同的东西。如果你能事先预测一个较小的模型是否适合你用例的任务列表，那么你可以使用那个较小的模型，并为你公司节省额外的推理和延迟成本，这些成本可能伴随着为你的需求而配备的大型超大规模LLM。简单地说，你会优化大LLM的使用，使其在真正需要时使用，而不是将最昂贵的选项作为默认或唯一的选择。
- en: 'We do this all the time in our travels. Typically, we’re living the Uber X
    life—budget travel. But Uber Black (although it leaves us with some explaining
    to do to our auditors) is the go-to on a tight schedule because it’s there in
    minutes, they aren’t going to stop for gas on the way, and they won’t accept your
    ride while they finish another—not to mention the chewing gum is individually
    wrapped, not stuck to the floor. Now apply that logic to your AI: use the expensive
    option *only* when you truly need it.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在旅行中经常这样做。通常，我们过着Uber X的生活——预算旅行。但Uber Black（尽管这让我们在审计员面前有些解释要做）在时间紧迫时是首选，因为它几分钟内就能到达，他们不会在路上加油，而且他们不会在你完成另一项任务时接受你的搭车——更不用说口香糖是单独包装的，而不是粘在地上了。现在将这个逻辑应用到你的AI上：只有在真正需要时才使用昂贵的选项。
- en: A group of researchers at the MIT-IBM Watson AI Lab were looking for answers
    to the question, “Can a bunch of smaller models outperform a large model?” Even
    back in 2023, when SLMs were just getting started, one paper^([24](ch07.html#id993))
    proposed an approach where a model-routing algorithm sits as an orchestrator,
    directing inference requests to whichever model the router predicts would be best
    for a given task.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 麻省理工学院-IBM Watson AI实验室的一组研究人员正在寻找答案：“一组较小的模型能否超越大型模型？”甚至早在2023年，当SLMs刚开始起步时，有一篇论文^([24](ch07.html#id993))提出了一种方法，其中模型路由算法作为一个协调者，将推理请求指向路由器预测的针对特定任务最佳的模型。
- en: In this deployment pattern, you could have an ecosystem of models—some are small
    and specialized, some are larger—to maximize the chances that a model router can
    find the optimal model to support a given task while defraying your costs every
    time the router selects a smaller model. [Figure 7-2](#fig-7-3) shows this.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种部署模式中，你可以有一个模型生态系统——一些是小型的和专业的，一些是较大的——以最大化模型路由器找到支持特定任务的最佳模型的机会，同时每次路由器选择较小的模型时都降低你的成本。[图7-2](#fig-7-3)展示了这一点。
- en: '![A diagram of a router  AI-generated content may be incorrect.](assets/aivc_0702.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![路由器图示 AI生成的内容可能不正确。](assets/aivc_0702.png)'
- en: Figure 7-2\. An AI router that understands the capabilities of models in its
    library directs a given inference request to the best model able to perform the
    task at hand
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-2\. 理解其库中模型能力的AI路由器将给定的推理请求路由到能够执行当前任务的最佳模型
- en: 'In [Figure 7-2](#fig-7-3), you can see a new inference request for a given
    input comes into the ecosystem (new data point). A router (trained on benchmark
    data) understands what model can best perform the task at hand and routes the
    work to it. You can see the benefits here, right? Every time the router pushes
    a task to a smaller model (our example has a library of three tier-sized models:
    small, medium, and large), you save money, reduce latency, and help the environment.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图7-2](#fig-7-3)中，你可以看到一个针对给定输入的新推理请求进入生态系统（新的数据点）。一个路由器（在基准数据上训练）理解哪个模型可以最好地执行当前任务，并将工作路由到它。你在这里看到了好处，对吧？每次路由器将任务推送到一个较小的模型（我们的示例有一个包含三个层大小模型的库：小型、中型和大型），你就可以省钱，减少延迟，并帮助环境。
- en: This begs the question, how does this model router know which model in the library
    will perform best? There are different approaches. The MIT-IBM team took an approach
    that leveraged predefined (HELM^([25](ch07.html#id994))) benchmark data for each
    model in order to first train the AI router on the different types of tasks each
    model could perform satisfactorily (note that this approach could also work with
    any set of relevant benchmarks defined by a user).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这就引出了一个问题，这个模型路由器是如何知道库中哪个模型会表现最好的？有几种不同的方法。麻省理工学院-IBM团队采取了一种方法，该方法利用了为每个模型预定义的（HELM^([25](ch07.html#id994)））基准数据，以便首先训练AI路由器，使其能够满意地完成每个模型可能执行的不同类型任务（请注意，这种方法也可以与用户定义的任何相关基准集一起工作）。
- en: As it turns out, training the AI router is a fairly trivial task. At its core,
    the router is just a classification model. Given a representative task, the router
    classifies whether the model will perform satisfactorily or not. Once trained,
    the router then compares the similarity between any new task and the known benchmarks.
    If a new task is similar to a benchmark task that a specific model has proven
    to perform well at, then the router is more confident that this specific model
    will perform well on that new task, too. For example, if a specific model was
    really good at Q&A’ing medical questions about your broken or sprained ankle,
    it will probably be pretty good at your broken or sprained wrist you got fishing
    last week (seriously, take it easy).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，训练AI路由器是一个相当简单的工作。在本质上，路由器只是一个分类模型。给定一个代表性任务，路由器会判断模型是否能满意地完成。一旦训练完成，路由器就会比较任何新任务与已知基准之间的相似性。如果一个新任务与特定模型已经证明能够很好地完成的基准任务相似，那么路由器更有信心这个特定模型在新任务上也能表现良好。例如，如果一个特定模型在回答关于你扭伤或挫伤的脚踝的医疗问题方面非常出色，那么它可能在你上周钓鱼时扭伤或挫伤的手腕上也会表现得相当不错（认真地说，要小心）。
- en: Note
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If the benchmarks you’re using are very dissimilar to the tasks being routed
    to the models, you could also update the router’s logic by giving it a small amount
    of labeled data that represent the tasks you’re trying to run so that the router
    can get updated knowledge on model performance for that specific task. The router
    can then use that information to route future requests (the same ones or similar)
    to the most appropriate model in your library.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的基准与要路由到模型的任务非常不同，你也可以通过给它一小部分代表你试图运行的任务的标记数据来更新路由器的逻辑，这样路由器就可以获得关于该特定任务模型性能的更新知识。然后，路由器可以使用这些信息将未来的请求（相同的或类似的）路由到库中最合适的模型。
- en: To demonstrate the performance of the model router, the MIT-IBM team ran an
    experiment using a library comprised of over a dozen models that ranged from 3
    billion to 70 billion parameters in size (so there was a great representation
    of small, medium, and large models, despite what our example in [Figure 7-2](#fig-7-3)
    shows). The team evaluated^([26](ch07.html#id995)) a bunch of different tasks
    that make up Stanford’s HELM evaluation benchmark. The first pass was *without
    a router* to determine which model in the library could perform the tasks in the
    HELM benchmark the best.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示模型路由器的性能，麻省理工学院-IBM团队使用了一个包含超过12个模型的库进行了实验，这些模型的大小从30亿到700亿参数不等（尽管我们的示例[图7-2](#fig-7-3)显示了情况，但仍然很好地代表了小型、中型和大型模型）。团队评估了组成斯坦福大学HELM评估基准的一组不同任务。第一次尝试是*没有路由器*，以确定库中哪个模型能最好地完成HELM基准中的任务。
- en: 'It shouldn’t be too surprising to find out which model won. As we said before:
    *on average*, a large model should perform better than individual smaller models
    for all the tasks. And, as shown in [Figure 7-3](#ch07_figure_3_1740182051640296),
    that was indeed the case for this test. The largest model in the library (Llama-2-70B)
    achieved 68% accuracy (higher is better). And just like that, Llama-2-70B became
    the baseline for which we could compare how our AI-powered model router would
    do with a mixed-model approach. It’s important to understand this, so at the risk
    of repeating ourselves, we’ll say it more explicitly: this benchmark is not *measuring*
    the accuracy of the model router; it is measuring the accuracy of the models that
    the router selects. Quite simply, this means that if you used the Llama-2-70B
    model for every task in the HELM benchmark, you would get an average performance
    of 68%.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 发现哪个模型获胜并不令人惊讶。正如我们之前所说：**平均而言**，大型模型在所有任务上应该比单个较小的模型表现更好。正如[图7-3](#ch07_figure_3_1740182051640296)所示，这种情况在本测试中确实如此。库中最大的模型（Llama-2-70B）达到了68%的准确率（越高越好）。就这样，Llama-2-70B成为了我们比较我们的AI模型路由器采用混合模型方法的表现的基准。理解这一点很重要，所以为了避免重复，我们将更明确地说：这个基准**不是**在**测量**模型路由器的准确率；它是在测量路由器选择的模型的准确率。简单来说，这意味着如果你在HELM基准测试中的每个任务上使用Llama-2-70B模型，你将得到平均68%的性能。
- en: '![A graph with numbers and a number of marks  Description automatically generated
    with medium confidence](assets/aivc_0703.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![一个带有数字和多个标记的图表  描述自动生成，置信度中等](assets/aivc_0703.png)'
- en: 'Figure 7-3\. No router used: on average, the large model performed the best,
    at around 68% performance (higher is better)'
  id: totrans-131
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-3。未使用路由器：平均而言，大型模型表现最佳，大约68%的性能（越高越好）
- en: Now it’s time to unleash the router! [Figure 7-4](#ch07_figure_4_1740182051640324)
    shows what happens when we allowed the router to send various tasks to different
    models in the library. Remember, the entire library *did not* have a single model
    over 70 billion parameters. Basically, the router (with its ability to route a
    task to a library of small, medium, and large models) *outperformed* the large
    model on its own! Specifically, the overall performance was about 72% when the
    router could access the library of models, compared to 68% when using one big
    LLM alone. But there is more to the story in [Figure 7-3](#ch07_figure_3_1740182051640296);
    to tell it, you need to focus on the vertical bar graph within the results.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候释放路由器了！[图7-4](#ch07_figure_4_1740182051640324)显示了当我们允许路由器将各种任务发送到库中的不同模型时会发生什么。记住，整个库**没有**一个超过70亿参数的模型。基本上，路由器（凭借其将任务路由到包含小型、中型和大型模型的库的能力）**优于**单独的大模型！具体来说，当路由器可以访问模型库时，整体性能约为72%，而单独使用一个大LLM时为68%。但在[图7-3](#ch07_figure_3_1740182051640296)中还有更多故事；要讲述这个故事，你需要关注结果中的垂直条形图。
- en: When the router was in play, only 56% of tasks were routed to the big Llama-2-70B
    model. The rest of the tasks got routed to the smaller, more efficient, and obviously
    higher-performing models for the tasks routed to them (a mixture of medium and
    small models).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 当路由器在发挥作用时，只有56%的任务被路由到大型Llama-2-70B模型。其余的任务被路由到更小、更高效、显然表现更好的模型（混合了中型和小型模型）。
- en: 'The takeaway? Using a model router, we observed improved *overall* accuracy
    and efficiency. Remember, every time a task gets routed to a smaller model, it’s
    more efficient to run it. Lower costs. Better performance. Lower environmental
    impact. What’s not to love? But like any good leader who challenges their teams
    for their best, one question remained: can you do better?'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下？使用模型路由器，我们观察到了**整体**准确性和效率的提升。记住，每次任务被路由到更小的模型时，运行它都更加高效。成本更低。性能更好。环境影响更小。有什么不令人爱的地方呢？但就像任何一位挑战团队以追求最佳表现的优秀领导者一样，一个问题仍然存在：你能做得更好吗？
- en: '![A graph of a task  Description automatically generated with medium confidence](assets/aivc_0704.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![一个任务图表  描述自动生成，置信度中等](assets/aivc_0704.png)'
- en: Figure 7-4\. Using a router to route to our SLM and LLM library for the tasks
    at hand resulted in better performance
  id: totrans-136
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-4。使用路由器将任务路由到我们的SLM和LLM库，结果提高了性能
- en: 'To answer that question, the research team started with a hypothesis: what
    if the model library was limited to *only models that were equal to or less than
    13 billion parameters in size*? These are true SLMs—that sweet spot of SLMs that
    we talked about earlier.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这个问题，研究团队从一个假设开始：如果模型库仅限于*大小等于或小于130亿参数的模型*，会怎样？这些是真正的SLM——我们之前提到的SLM的甜蜜点。
- en: '[Figure 7-5](#ch07_figure_5_1740182051640346) shows the answer to this question,
    and it’s worth some extra commentary.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-5](#ch07_figure_5_1740182051640346)展示了这个问题的答案，并且值得一些额外的评论。'
- en: '![A graph of a number of tasks  Description automatically generated with medium
    confidence](assets/aivc_0705.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![一个任务数量的图表  描述由中等置信度自动生成](assets/aivc_0705.png)'
- en: Figure 7-5\. Limiting the model library to 13 billion parameters delivers impressive
    benefits
  id: totrans-140
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-5\. 将模型库限制在130亿参数可以带来令人印象深刻的效益
- en: The obvious takeaway from [Figure 7-5](#ch07_figure_5_1740182051640346) is that
    the results of the router with an SLM-only library (70%) aren’t as good as the
    larger library comprised of all 15 large, medium, and small models, including
    the 70 billion LLM (72%). But some things caught our eye right off the bat and
    should have you throttling up your attention span (we know, we’re deep into the
    chapter) from “somewhat curious” to “we have your full attention.”
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 从[图7-5](#ch07_figure_5_1740182051640346)中显而易见的结论是，仅包含SLM的模型库（70%）的结果并不如包含所有15个大型、中型和小型模型（包括70亿参数的LLM）的大型库（72%）。但有些事情一开始就引起了我们的注意，应该让你从“有点好奇”转变为“全神贯注”（我们知道，我们已经深入到本章）。
- en: 'First, while the library of all models (up to and including the 70 billion
    one) performed better, the SLM-only library (models 13 billion parameters and
    under) outperformed the baseline (the big 70 billion LLM on its own): 70% versus
    68%. Second, the SLMs don’t need the biggest most expensive and scarce GPUs to
    run them. That means you get more deployment options. And of course, giving up
    only 2% performance over the best result, and gaining 2% performance over the
    baseline, gives you even lower overall costs (both money and environmental)!'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，虽然所有模型库（包括70亿个模型）的表现更好，但仅包含SLM的模型库（参数量在130亿以下）在性能上超过了基线（仅有的70亿参数的大型LLM）：70%对68%。其次，SLM不需要最大的、最昂贵且稀缺的GPU来运行。这意味着你将获得更多的部署选项。当然，只牺牲了2%的性能以获得最佳结果，并在基线上提高了2%的性能，这使你的总体成本（包括金钱和环境）更低！
- en: Think About This When It Comes to Model Routing
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在考虑模型路由时，请考虑以下几点
- en: 'The appeal of model routing isn’t just maximizing performance at lower overall
    cost. There’s a second important benefit: having the ability to, before the inference
    (a priori), predict model performance on a task across different models of different
    sizes. Why is this important? As a leader who understands this technique, you
    can make more informed decisions about the cost-benefit trade-off between different
    models and the suitability of any given task for automation.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 模型路由的吸引力不仅在于在更低的总体成本下最大化性能。还有一个重要的好处：在推理之前（事前），能够预测不同大小模型在特定任务上的性能。为什么这很重要？作为一个理解这种技术的领导者，你可以更明智地决定不同模型之间的成本效益权衡，以及任何给定任务自动化的适用性。
- en: For example, if an automation task you are preparing as a GenAI use case is
    very complicated—and the only models predicted to perform well are the very large,
    expensive ones—then you might decide that automating that task doesn’t result
    in large enough cost savings to justify using a model of that size. On the other
    hand, perhaps you have a low-value task that wasn’t giving a strong signal on
    your GenAI use case radar, but it’s predicted to be easily automated with a fairly
    small model. Suddenly, you’re economically incentivized to shift it left and automate
    that task. When you think about that whole flip of +AI to AI+ mindset we discussed
    in [Chapter 1](ch01.html#ch01_ai_to_ai_generative_ai_and_the_netscape_moment_1740182043982974),
    where you suddenly see your business as discrete pieces of workflows and business
    logic, we think model routing can really help here. How so? Those discrete pieces
    of logic likely aren’t going to need a super large model, so they can be leveraged
    for the mundane rote shift tasks that are bound to be discovered during this process.
    We envision a near-future world of LLMOps, driven by model routers, where performance
    and cost savings are dynamically monitored, and a router actively sends workloads
    to different models to maintain a desired cost per performance balance defined
    by an operator.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你正在准备的一个自动化任务作为通用人工智能（GenAI）用例非常复杂——并且预测表现良好的唯一模型都是非常大、昂贵的模型——那么你可能决定自动化这个任务不会带来足够大的成本节约，从而证明使用这种规模的模型是合理的。另一方面，也许你有一个价值低的任务，它没有在你的GenAI用例雷达上发出强烈的信号，但它预测可以很容易地用相对较小的模型自动化。突然之间，你从经济上受到激励，将其左移并自动化这个任务。当你思考我们在[第1章](ch01.html#ch01_ai_to_ai_generative_ai_and_the_netscape_moment_1740182043982974)中讨论的整个从+AI到AI+思维模式的转变时，你突然将你的业务视为工作流程和商业逻辑的离散部分，我们认为模型路由在这里真的可以大有帮助。如何做到这一点呢？这些离散的逻辑部分可能不需要一个超级大的模型，因此它们可以被用于日常的例行任务，这些任务在这个过程中很可能会被发现。我们设想一个由模型路由器驱动的LLMOps的近未来世界，其中性能和成本节约是动态监控的，并且路由器会主动将工作负载发送到不同的模型，以维持由操作员定义的期望的成本与性能平衡。
- en: Mixture of Experts (MoE) Architecture
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 专家混合（MoE）架构
- en: 'Now that we have talked about how groups of models of various strengths and
    expertise can work together through an external model router, let’s take this
    idea one step further and talk about how this same concept can be applied internally
    within a model, using a relatively new type of LLM architecture: Mixture of Experts
    (MoE).'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了各种强度和专长的模型组如何通过外部模型路由器协同工作，那么让我们进一步探讨这个概念如何应用于模型内部，使用一种相对较新的LLM架构：专家混合（MoE）。
- en: Think of LLM architectures as the technical strategy that a researcher uses
    to encode all of the training data into parameters for their model. Almost all
    modern LLMs trained today are trained using a “transformer” type of architecture
    (which we talk about in [Chapter 8](ch08.html#ch08_using_your_data_as_a_differentiator_1740182052172518)).
    Since its initial release, many types of transformer architectures have emerged.
    The most popular is the “dense” style of transformer models, used by many model
    providers like Meta with its Llama model families. However, more recently, new,
    more efficient types of transformer architectures, like MoE, have started to gain
    popularity, and that’s what we cover in this section.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 将LLM架构视为研究人员使用的技术策略，将所有训练数据编码到模型的参数中。今天几乎所有训练的现代LLM都是使用“变换器”类型的架构进行训练的（我们将在[第8章](ch08.html#ch08_using_your_data_as_a_differentiator_1740182052172518)中讨论）。自从其最初发布以来，已经出现了许多类型的变换器架构。最受欢迎的是“密集”风格的变换器模型，由许多模型提供商使用，例如Meta的Llama模型系列。然而，最近，新的、更高效的变换器架构类型，如MoE，开始变得流行，这就是我们本节要介绍的内容。
- en: In an MoE-based model, buckets of parameters, referred to as “experts,” are
    trained to operate fairly independently of one another. These experts can either
    be specialized by the model developer, or can be generalists in nature. Leveraging
    the same intuition we covered in the previous section, only a subset of the experts
    are used at inference time, making these models *wicked* fast (can you tell one
    of the authors is a Bostonian?). This is because the inference cost is now approximately
    reduced to the size of the experts being run, not the entire size of the model.
    How does the model know which regions of the model to “activate” for a given request?
    You guessed it. A model router, but *this* time the model router is internal to
    the model, not something that can be used independently with other models like
    in the previous section.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于MoE的模型中，参数桶，被称为“专家”，被训练以相对独立地操作。这些专家可以是模型开发者的专业化的，或者本质上可以是通才。利用我们在上一节中讨论的相同直觉，推理时只使用专家子集，这使得这些模型*非常快*（你能看出一位作者是波士顿人吗？）。这是因为推理成本现在大约减少到正在运行的专家的大小，而不是整个模型的大小。模型是如何知道对于给定的请求应该“激活”模型的哪个区域的？答案是。一个模型路由器，但*这次*模型路由器是模型内部的，而不是像上一节中那样可以独立与其他模型一起使用的。
- en: There are some important gotchas with MoE inference efficiency. If you are running
    inference in large batch jobs, as is common for production workloads, this efficiency
    advantage goes down because you will need to load more and more of the experts
    into memory depending on all the samples that are batched. But if you are experimenting
    locally, or running things in a single batch, or batching across very homogenous
    data that will always use the same experts, these MoE models can be quite inference-efficient.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在MoE推理效率方面有一些重要的陷阱。如果你在运行大型批处理作业中的推理，这在生产负载中很常见，这种效率优势会降低，因为你需要根据所有批处理的样本加载越来越多的专家到内存中。但是，如果你是在本地进行实验，或者在一个单独的批次中运行，或者批处理非常同质化的数据，这些MoE模型可以非常高效地进行推理。
- en: 'In January of 2025, the MoE architecture got broad attention when DeepSeek
    released its 671 billion MoE model. But DeepSeek wasn’t the first to release an
    MoE model. The French AI Lab, Mistral AI, made headlines with the release of one
    of the first high-performing MoE models: Mixtral 8x7B (we think the name is great,
    Mistral + mixture) all the way back in December of 2023.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 2025年1月，当DeepSeek发布了其6710亿个MoE模型时，MoE架构引起了广泛关注。但DeepSeek并不是第一个发布MoE模型的公司。法国人工智能实验室Mistral
    AI在2023年12月发布了第一个高性能MoE模型Mixtral 8x7B（我们认为这个名字很棒，Mistral + mixture），引起了人们的关注。
- en: MoE models are more efficient to run at inference time, but they are also more
    economical to train. DeepSeek brought this point home when it published that it
    was able to train its base model, DeepSeek-V3-Base (which was later post-trained
    to create DeepSeek-R1), for $5.6M. But there are a couple of important things
    to note when interpreting this staggeringly low reported training cost.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: MoE模型在推理时的运行效率更高，但它们在训练时也更加经济。当DeepSeek发布其基础模型DeepSeek-V3-Base（后来经过后训练以创建DeepSeek-R1）的训练成本为5.6百万美元时，这一点得到了证实。但在解读这个惊人的低报告训练成本时，有几个重要的事项需要注意。
- en: 'First, just as any lawyer will tell you, make sure you read the fine print!
    When DeepSeek reported its training cost in the DeepSeek-V3 Technical Report,
    it included a very important caveat: “Note that the aforementioned costs include
    only the official training of DeepSeek-V3, excluding the costs associated with
    prior research and ablation experiments on architectures, algorithms, or data.”^([28](ch07.html#id1010))'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，就像任何律师都会告诉你的那样，确保你阅读了细印！当DeepSeek在DeepSeek-V3技术报告中报告其训练成本时，它包含了一个非常重要的警告：“请注意，上述成本仅包括DeepSeek-V3的官方训练，不包括与架构、算法或数据相关的先前研究和消融实验的成本。”^([28](ch07.html#id1010))
- en: What does this translate to in plain speak? Well, to train LLMs, there is a
    lot of brute-force trial and error that is required in order to optimize performance.
    That means for any one model that is released, there might be hundreds or thousands
    of smaller models that are trained in advance, testing out different data mixture
    efficacies, searching through different hyperparameter settings, etc. These development
    costs can easily be 10 times or more compared to the final, one-and-done training
    cost of the model. So, while what DeepSeek did is still impressive, the true training
    costs of its models were probably far less earth-shattering than some of the press
    coverage may have let on.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这在普通语言中是什么意思呢？嗯，为了训练大型语言模型（LLMs），需要大量的暴力尝试和错误来优化性能。这意味着对于任何发布的模型，可能事先训练了数百或数千个较小的模型，测试不同的数据混合效率，搜索不同的超参数设置等等。这些开发成本可能比模型的最终一次性训练成本高出10倍或更多。因此，虽然DeepSeek所做的事情仍然令人印象深刻，但其模型的真正训练成本可能远不如一些媒体报道的那么震撼。
- en: Think About This When It Comes to MoEs
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 谈到MoEs，请考虑以下几点
- en: Research and innovation with MoE-style models is still evolving. As DeepSeek
    showed, the world is getting better and better at training MoE models more efficiently
    and innovating on how to bring experts together. At the end of the day, we are
    most bullish on this architecture because its more efficient training costs will
    allow for more rapid iteration, hopefully continuing to drive innovation in this
    space.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MoE风格的模型进行的研究和创新仍在不断发展。正如DeepSeek所展示的，世界在更有效地训练MoE模型和不断创新如何将专家聚集在一起方面正在变得越来越好。最终，我们对这种架构持最乐观的态度，因为其更有效的训练成本将允许更快的迭代，希望继续推动这一领域的创新。
- en: We see a significant innovation runway for MoEs with respect to configurable
    inference efficiency. Today, Mixtral is designed to call two experts at inference
    time. To enable cost-efficient inferencing in the future, we envision this technology
    evolving to dynamically change the number of experts called at inference time,
    allowing users to quickly adjust their cost/performance trade-off for a given
    task and use case. This is like the model routing use case in the previous section,
    where more complicated tasks could call for the justified use of a bigger more
    expensive model. In our crystal ball, we see MoE models operating in the same
    manner where complicated tasks could call for using more experts at inference
    time (perhaps all eight and not just the two in our running example).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在可配置推理效率方面看到了MoEs（Mixture of Experts）的巨大创新空间。今天，Mixtral被设计成在推理时调用两个专家。为了在未来实现成本效益高的推理，我们设想这项技术将演变成动态改变推理时调用的专家数量，使用户能够快速调整给定任务和用例的成本/性能权衡。这就像上一节中提到的模型路由用例，更复杂的任务可能需要使用更大、更昂贵的模型。在我们的水晶球中，我们看到MoE模型将以同样的方式运行，复杂任务可能需要在推理时使用更多的专家（也许是我们运行示例中的所有八个，而不仅仅是两个）。
- en: No matter where this technology evolves, it’s all about the flexibility for
    model consumers that makes it so exciting. When you reduce your dependency on
    one large model and harness the power of smaller models (or regions of a model)
    working together, you have opportunities to tailor model expertise for your use
    cases all the way to optimizing the cost-performance trade-off to best meet the
    needs of your business. And now you know why one model couldn’t possibly rule
    them all.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 无论这项技术如何发展，它之所以如此令人兴奋，都是因为它为模型消费者提供了灵活性。当你减少对一个大模型的依赖，并利用较小模型（或模型的部分）共同的力量时，你就有机会为你的用例定制模型专业知识，甚至优化成本/性能权衡，以最好地满足你企业的需求。现在你知道为什么一个模型不可能统治所有这些了。
- en: Agentic Systems
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代理系统
- en: We’ve given you some high-level details about agents throughout this book. In
    the final section of this chapter, it’s time to delve into them a little deeper.
    When we talk about agents, we often are referring to an implementation of an LLM
    where a user provides a goal-oriented instruction, and then the LLM independently
    comes up with a series of tasks (and subtasks) to achieve that goal. It then iterates
    over those tasks, often leveraging tools and reflection loops to complete each
    task. An agent can even be comprised of multiple different LLMs, each performing
    one of those tasks. Because a complex task is broken down into smaller, simpler-to-accomplish
    steps, the door is often opened for smaller models to tackle simpler tasks in
    tandem with larger models performing the more difficult tasks (like coming up
    with the list of tasks that need to be done to achieve the goal in the first place).
    And often, there is some sort of model routing happening behind the scenes where
    an LLM is selecting another LLM to outsource a subtask to, based on a catalogue
    of LLMs to choose from.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们已经为您提供了关于代理的一些高级细节。在本章的最后部分，我们将更深入地探讨它们。当我们谈论代理时，我们通常指的是一种LLM的实现，用户提供以目标为导向的指令，然后LLM独立地提出一系列任务（和子任务）来实现该目标。然后它遍历这些任务，通常利用工具和反思循环来完成每个任务。一个代理甚至可以由多个不同的LLM组成，每个LLM执行其中一项任务。由于复杂任务被分解成更小、更易于完成的步骤，因此通常为较小的模型打开了与较大的模型协同处理更简单任务的大门（例如，提出实现目标所需完成的任务列表）。而且，通常在幕后发生某种类型的模型路由，其中LLM根据可选择的LLM目录选择另一个LLM来外包子任务。
- en: While many things agents do today can be done manually and in a static manner,
    agents deliver productivity breakthroughs by further shifting left more of the
    work, which saves time and boosts efficiency. For example, if you headed up a
    clinical trial, you could use an LLM to identify suitable trial candidates, but
    then you’d have to manually manage visit scheduling and coordination (tasks like
    sending reminders, rescheduling meetings, and automatically reminding everyone
    in the trial about key dates or requirements, such as a morning fast). With agents,
    you shift more of the work left because not only can an agent come up with a great
    start toward the perfect clinical trial profile, but they can even help come up
    with a proposed set of compliance reminders and even schedule sample collections
    with calendar invites for participants! What’s more, agentic systems are not stuck
    in time, and they can adapt in real time.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管现在许多代理可以手动以静态方式完成，但通过将更多工作向左移动，代理实现了生产力的突破，这节省了时间并提高了效率。例如，如果你领导一项临床试验，你可以使用大型语言模型（LLM）来识别合适的试验候选人，但随后你必须手动管理访问安排和协调（例如发送提醒、重新安排会议，以及自动提醒试验中的每个人关于关键日期或要求，如早晨禁食）。有了代理，你可以将更多的工作向左移动，因为不仅代理可以提出一个完美的临床试验档案的起点，他们甚至可以帮助制定合规提醒的提案，甚至为参与者安排样本收集的日历邀请！更重要的是，代理系统不受时间的限制，并且可以实时适应。
- en: Imagine attaching an agent to a supply chain management problem—you now have
    AI with the ability to understand a weather event and optimize a plan (understanding
    road closures and such) to get much-needed product into stores. And as you will
    find out, agents can even learn along the way. Quite simply, the dynamic nature
    of agents helps a company get more work shifted from +AI to AI+ and keeps them
    agile. This space keeps changing, so you’re going to want to follow it closely.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下将一个代理附加到供应链管理问题中——你现在拥有能够理解天气事件并优化计划（理解道路关闭等情况）以将急需的产品送入商店的AI。而且正如你将发现的，代理甚至可以在过程中学习。简单来说，代理的动态特性帮助公司将更多的工作从+AI转移到AI+，并保持其敏捷性。这个领域不断变化，所以你将想要密切关注它。
- en: Now think back to what you learned in [Chapter 4](ch04.html#ch04_the_use_case_chapter_1740182047877425)
    about LLMs with a RAG pattern. That was one way of not just making your enterprise
    data available to an LLM, but also how to provide the LLM with updated information.
    In this pattern, a larger system injects information from an external source (like
    a database) directly into the prompt before runtime. This was also the basis of
    the “talk to a document” use case in [Chapter 4](ch04.html#ch04_the_use_case_chapter_1740182047877425).
    With the introduction of agents, AI gets even more powerful and can handle more
    complex tasks because they have the ability to call tools (this process is referred
    to as tool calling) outside of the LLM to assist them with their work.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在回想一下你在[第4章](ch04.html#ch04_the_use_case_chapter_1740182047877425)中学到的关于具有RAG模式的LLM的知识。那是一种不仅使你的企业数据对LLM可用，而且还能为LLM提供更新信息的途径。在这个模式中，一个更大的系统在运行前直接将来自外部源（如数据库）的信息注入到提示中。这也是[第4章](ch04.html#ch04_the_use_case_chapter_1740182047877425)中“与文档对话”用例的基础。随着代理的引入，AI变得更加强大，能够处理更复杂的任务，因为它们有调用工具（这个过程被称为工具调用）的能力，以协助它们完成工作。
- en: Note
  id: totrans-164
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '*Tool calling* is the term referred to when LLMs are given the ability to interact
    with external tools, apps, and other systems—all to enhance their functionality.
    For example, an agent’s LLM might perform a tool call to get the weather for a
    particular location to help finish a task or reach out to a calculator to perform
    certain types of calculations for precision or even to offload the work from the
    LLM. Simply put, tool calling extends LLMs with capabilities beyond generating
    text, images, and the other things they are known for that we’ve covered in this
    book.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '*工具调用*是指当LLM被赋予与外部工具、应用程序和其他系统交互的能力时使用的术语，所有这些都是为了增强其功能。例如，一个代理的LLM可能会进行工具调用以获取特定位置的天气信息，以帮助完成任务，或者联系计算器执行某些类型的计算以提高精度，甚至将工作从LLM卸载。简单来说，工具调用扩展了LLM的能力，使其不仅限于生成文本、图像以及其他我们在本书中介绍过的东西。'
- en: 'Perhaps the best way to appreciate the power of agents is to reflect on how
    you typically work with an AI-powered chatbot today. The flow looks something
    like this:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 也许是最好的方式来欣赏代理的力量，就是反思你今天通常如何与一个由AI驱动的聊天机器人一起工作。流程看起来可能像这样：
- en: human prompt → LLM response → human prompt → LLM response → ...
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: human prompt → LLM response → human prompt → LLM response → ...
- en: In this traditional system, your prompt might go back and forth in the simple
    manner shown above, but it can trigger multiple calls that operate in the backend,
    unseen by you, before a response is provided back. For example, a RAG pattern
    appends data to a prompt from a data source that was connected to this flow by
    an administrator. But even when enhanced in this manner, the information that
    is available to the LLM supporting a RAG-based chatbot is also predetermined by
    its creator (like through a connection to a vector database like Chroma). In this
    nonagentic architectural pattern, the LLM involved *is not* given the ability
    to work “behind the scenes” on its own—it interacts with you on a continual basis
    as you go back and forth and back and forth, trying to complete your task.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个传统系统中，你的提示可能以上述简单的方式来回传递，但在提供响应之前，它可能触发多个在后台操作、你无法看到的调用。例如，RAG模式的应用程序会从通过管理员连接到该流程的数据源中向提示添加数据。但即使以这种方式增强，支持基于RAG的聊天机器人的LLM可用的信息也由其创建者预先确定（例如，通过连接到像Chroma这样的向量数据库）。在这个非代理架构模式中，涉及的LLM*没有*被赋予在幕后独立工作的能力——它在与你不断往返的过程中与你互动，试图完成你的任务。
- en: 'In contrast, agentic implementations provide LLMs with more freedom and power.
    In this architectural pattern, LLMs are allowed to reason about what information
    is needed to perform a task that helps achieve a goal, like, “Put together a plan
    to increase the net promoter score (NPS) for my car dealership’s service center.”
    The LLMs part of this pattern are provided with access to tools (more on this
    in a bit) that can be called on the backend to obtain up-to-date information,
    optimize workflows, create subtasks to tackle the challenge piece by piece, and
    even call some scripting language (like VBScript) to create some PowerPoint charts
    of what it finds! This is all done autonomously by the agent (or agents) to achieve
    the complex goal. An agentic workflow might look like:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，智能实现为LLM提供了更多的自由和权力。在这个架构模式中，LLM被允许推理出完成任务以实现目标所需的信息，例如，“制定一个计划来提高我的汽车经销商服务中心的净推荐分数（NPS）。”这个模式中的LLM部分提供了访问工具（稍后会更详细地介绍）的权限，这些工具可以在后端调用以获取最新信息、优化工作流程、创建子任务逐步解决挑战，甚至调用一些脚本语言（如VBScript）来创建一些PowerPoint图表来展示其发现！所有这些都是由代理（或代理）自主完成的，以实现复杂的目标。一个智能工作流程可能看起来像：
- en: human prompt → primary LLM response (hidden to user) → primary LLM tool call
    (hidden to user) → LLM response (hidden to user, shown to secondary LLM) → secondary
    LLM response (hidden to user, provided back to primary LLM) → primary LLM response
    (shown to user) → human prompt → ...
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '人类提示 → 主要LLM响应（对用户隐藏）→ 主要LLM工具调用（对用户隐藏）→ LLM响应（对用户隐藏，显示给次要LLM）→ 次要LLM响应（对用户隐藏，提供给主要LLM）→
    主要LLM响应（显示给用户）→ 人类提示 → ... '
- en: As an end user chatting with an agentic system, you might feel as if you are
    just querying one big, multifunctional super LLM behind the scenes. But the reality
    is you’re likely working with a system of bigger and smaller models working together
    behind the scenes in order to efficiently solve your objective. (Like we said,
    you can use multiple LLMs in an agentic workflow. This should really give you
    a feel for just how significant of a role SLMs can play in this domain.)
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 作为与智能代理系统聊天的最终用户，你可能感觉就像是在幕后查询一个庞大、多功能的超级LLM。但现实情况是，你很可能在与一个由大小不同的模型协同工作以高效解决你目标的大系统打交道。（正如我们所说，你可以在智能工作流程中使用多个LLM。这应该真的让你感受到SLM在这个领域可以发挥多么重要的作用。）
- en: AI agents can encompass a wide range of functionality beyond language, including
    decision making, problem solving, interacting with external environments, and
    executing actions. And these agents can be deployed in various applications to
    solve complex tasks in enterprise contexts, from software design and IT automation
    to code-generation tools and conversational assistants. We like to think of agents
    as digital interns with lots of ambition. Arm them with goals, tools, and tasks,
    and their smarts *may* often surprise you—but like we said earlier, AI isn’t magic.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: AI代理可以涵盖除语言之外广泛的功能，包括决策、问题解决、与外部环境交互和执行动作。这些代理可以部署在各种应用中，以解决企业环境中的复杂任务，从软件开发和IT自动化到代码生成工具和对话助手。我们喜欢将代理视为充满雄心的数字实习生。给他们目标、工具和任务，他们的智慧可能会经常让你感到惊讶——但正如我们之前所说，AI并非魔法。
- en: What’s Your Reaction to This Agent in Action?
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你对这个正在行动的代理有何反应？
- en: AI agents are systems-based implementations of LLMs that leverage planning,
    reasoning, and tool calling to solve problems and interact with external environments.
    Behind the scenes, there might be a single LLM handling all the work, multiple
    instances of the same LLM working on a task, or a combination of different LLMs.
    A good agentic framework will let you mix and match different LLM providers, which
    includes fine-tuned models that you might have customized with your own data.
    For example, you might pull Anthropic’s Claude Sonnet for desktop controls but
    augment that with a Granite-based model enhanced with your business data—the two
    of them might work in concert to figure out an event and fill in a form. Very
    cool!
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: AI代理是基于LLM的系统实现，利用规划、推理和工具调用解决问题并与外部环境交互。幕后，可能有一个LLM处理所有工作，多个相同LLM的实例在执行任务，或者不同LLM的组合。一个好的智能框架将允许你混合和匹配不同的LLM提供商，这包括你可能用自己数据微调的模型。例如，你可能会使用Anthropic的Claude
    Sonnet进行桌面控制，但用增强你业务数据的基于Granite的模型来补充——这两个模型可能协同工作来分析一个事件并填写表格。非常酷！
- en: '[Figure 7-6](#ch07_figure_6_1740182051640368) gives you some insights into
    an agent that we tasked with writing a blog about the impacts of inflation on
    Canadian housing prices in 2024 and then come up with some social media postings
    to reference our blog.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-6](#ch07_figure_6_1740182051640368)为您揭示了一个我们指派撰写关于2024年加拿大房价通胀影响的博客，并随后提出一些社交媒体帖子以引用我们的博客的代理。'
- en: We set up several agents that are invoked from our task. One of the agents took
    on the persona of a Lead Market Analyst. We won’t detail this for each agent,
    but this particular agent’s *goal* was to conduct real-time analysis of financial
    news on our topic of interest to help guide content creation. We also gave this
    agent a *backstory*, which made it take on the persona of a market analyst from
    a reputable firm who dissects market trends to pass on to our agentic writers.
    We gave this information to the agent framework in YAML files.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设置了几个从我们的任务中调用的代理。其中一位代理扮演了首席市场分析师的角色。我们不会为每个代理详细说明，但这个特定代理的*目标*是实时分析我们感兴趣主题的财经新闻，以帮助指导内容创作。我们还为这个代理提供了一个*背景故事*，使其扮演一个来自知名公司的市场分析师，分析市场趋势以传递给我们的代理作者。我们将这些信息通过YAML文件传递给代理框架。
- en: Notice in [Figure 7-6](#ch07_figure_6_1740182051640368) that our Lead Market
    Analyst agent literally tells us how it will get started by searching the internet
    for articles related to the topic involved in its task.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在[图7-6](#ch07_figure_6_1740182051640368)中，我们的首席市场分析师代理实际上通过在互联网上搜索与其任务相关的主题文章，向我们展示了它将如何开始。
- en: '![A screenshot of a computer  Description automatically generated](assets/aivc_0706.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![计算机的截图  自动生成的描述](assets/aivc_0706.png)'
- en: Figure 7-6\. Our agentic workflow thinking about some of the steps it needs
    to do to write our blog and point to that blog on social media
  id: totrans-179
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-6\. 我们代理工作流程思考它需要执行的一些步骤来撰写博客并在社交媒体上指向该博客
- en: As shown in [Figure 7-7](#ch07_figure_7_1740182051640388), if the task is complicated,
    the agent might make multiple internal loops of tool calls and internal reasoning
    before returning a final answer. In this case, the agent has finished finding
    its sources and now starts to look at the data it’s collected. Notice how it has
    access to tools to help it.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图7-7](#ch07_figure_7_1740182051640388)所示，如果任务复杂，代理在返回最终答案之前可能会进行多次内部循环的工具调用和内部推理。在这种情况下，代理已经完成了其源信息的查找，现在开始查看收集到的数据。注意它如何能够访问帮助其的工具。
- en: '![A screenshot of a computer  Description automatically generated](assets/aivc_0707.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![计算机的截图  自动生成的描述](assets/aivc_0707.png)'
- en: Figure 7-7\. The agent starts to look at the contents of the information it
    found
  id: totrans-182
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-7\. 代理开始查看它找到的信息内容
- en: Finally, this particular agent finishes its work and returns the findings shown
    in [Figure 7-8](#ch07_figure_8_1740182051640408). It seems evident that our agentic
    workflow has the source information and summary points that will make for a great
    blog posting!
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这个特定代理完成了其工作，并返回了[图7-8](#ch07_figure_8_1740182051640408)中显示的发现。显然，我们的代理工作流程具有源信息和总结要点，这将使博客帖子非常出色！
- en: '![A close-up of a text  Description automatically generated](assets/aivc_0708.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![文本的特写  自动生成的描述](assets/aivc_0708.png)'
- en: Figure 7-8\. The key points to make in our blog posting
  id: totrans-185
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-8\. 我们博客帖子中需要强调的关键点
- en: Ultimately, there is a lot more flexibility added to this flow, giving the models
    powering your agents the ability to plan out tasks, research external information,
    and more.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们为此流程增加了更多的灵活性，使得驱动代理的模型能够规划任务、研究外部信息等。
- en: As we alluded to earlier, we built multiple agents on the backend, each specialists
    in different tasks, and we put them all to work on this objective. One agent has
    the persona of a content creator; another is a creative director, another is a
    social media guru, and finally, one is a math guru. We suggest that as you go
    create your own digital employees in your agentic workflows, look to the very
    job postings you might make for such jobs. In there will reside all kinds of backstory
    skills you want these digital employees to be able to do. When all was said and
    done, our agents wrote us the (presumably; we of course looked at the data it
    collected) well-researched blog that is shown in [Figure 7-9](#ch07_figure_9_1740182051640427).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，我们在后台构建了多个代理，每个代理都擅长不同的任务，并将它们全部用于这个目标。一个代理扮演内容创作者的角色；另一个是创意总监，另一个是社交媒体大师，最后一个是数学大师。我们建议，当你创建自己的代理工作流程中的数字员工时，看看你可能会发布的这类工作的职位描述。在那里将驻留所有你希望这些数字员工能够做到的背景技能。最终，我们的代理为我们写了一篇（我们当然查看了它收集的数据）经过充分研究的博客，如图[图7-9](#ch07_figure_9_1740182051640427)所示。
- en: '![A screenshot of a white text  Description automatically generated](assets/aivc_0709.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![一张带有白色文字的截图  自动生成的描述](assets/aivc_0709.png)'
- en: Figure 7-9\. The start of our final blog
  id: totrans-189
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-9\. 我们最终博客的开始
- en: Finally, look at the social media outreach messages our agentic workflow came
    up with (see [Figure 7-10](#ch07_figure_10_1740182051640455)) to amplify our article.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，看看我们的代理工作流程提出的社交媒体推广信息（见图[图7-10](#ch07_figure_10_1740182051640455)），以放大我们的文章。
- en: '![A screenshot of a white paper with black text  Description automatically
    generated](assets/aivc_0710.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![一张带有黑色文字的白色纸张截图  自动生成的描述](assets/aivc_0710.png)'
- en: Figure 7-10\. The agentic workflow didn’t just write our blog; it also composed
    social media outreach messages to direct traffic to our blog posting
  id: totrans-192
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-10\. 代理工作流程不仅写了我们的博客，还编写了社交媒体推广信息，以引导流量到我们的博客帖子
- en: We’ll admit we got a touch lazy looking back at the output in [Figure 7-10](#ch07_figure_10_1740182051640455).
    How so? We gave the same skills to our social media writer agent for posting on
    all platforms. Looking back, we should have given this agent broader skills and
    knowledge so it knew how to better mix tone and style depending on the social
    media outlet. After all, X (Twitter) is limited to 240 characters, so our agent
    worked hard to keep all of the postings it generated short (which could have been
    part of our assigned goal, but wasn’t). As another example, Instagram posts could
    be a lot less formal than LinkedIn. Notice how in [Figure 7-10](#ch07_figure_10_1740182051640455)
    the agent used emojis for the X post, which are more commonplace because of its
    limits than on LinkedIn.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们承认，回顾[图7-10](#ch07_figure_10_1740182051640455)中的输出时，我们有点偷懒。怎么会这样？我们给了我们的社交媒体写手代理在所有平台上发布内容的相同技能。回顾起来，我们应该给这个代理更广泛的能力和知识，以便它知道如何根据不同的社交媒体平台更好地混合语气和风格。毕竟，X（Twitter）的限制是240个字符，所以我们的代理努力使它生成的所有帖子都保持简短（这可能是我们分配的目标的一部分，但并不是）。作为另一个例子，Instagram帖子可能比LinkedIn更不正式。注意在[图7-10](#ch07_figure_10_1740182051640455)中，代理为X帖子使用了表情符号，这在限制条件下比LinkedIn更常见。
- en: There was a lot of other cool stuff going on behind the scenes than we could
    show you here. For example, our agents had their own version of the revered *Who
    Wants to Be a Millionaire?* game show’s Phone-a-Friend lifeline—only these friends
    were website crawlers, searchers, and scrapers, pieces of Python code (we used
    its Pydantic library to parse the data, among other libraries), and other digital
    labor agents—the best part is they never put you on hold or say, “Sorry bros,
    you stumped me!”
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后还有许多其他有趣的事情，我们无法在这里展示。例如，我们的代理有自己的版本，类似于备受尊敬的“谁想成为百万富翁？”游戏节目的“求助朋友”生命线——只是这些朋友是网站爬虫、搜索者和抓取器，Python代码的一部分（我们使用了Pydantic库来解析数据，以及其他库），以及其他数字劳动力代理——最好的部分是它们永远不会让你等待，也不会说，“抱歉，兄弟们，我困惑了！”
- en: Do we think Figures [7-9](#ch07_figure_9_1740182051640427) and [7-10](#ch07_figure_10_1740182051640455)
    were better than a human? That wasn’t the point...because we think that handing
    a human this information would give them a productivity boost if their job was
    to perform these very tasks. We shifted-left the work! Now bring the human element
    to make it really land.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为图[7-9](#ch07_figure_9_1740182051640427)和[7-10](#ch07_figure_10_1740182051640455)比人类做得更好吗？这并不是重点...因为我们认为，如果他们的工作就是执行这些任务，那么将这项信息交给人类将提高他们的生产力。我们向左移动了工作！现在引入人类元素，使其真正落地。
- en: A Little More on Agents
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于代理的更多内容
- en: In an agentic system, an agent often has access to more advanced forms of grounding
    context, like memory buffers that store information from past work and tasks it
    was asked to perform. An agent’s ability to store past interactions in memory
    and plan future actions encourages a personalized experience and comprehensive
    responses. But it gets better—these agents learn over time. For example, if there
    is a certain style you want a report written in, or a sauciness level for an Instagram
    post versus one on LinkedIn, agent memory can persist these preferences, and that’s
    a great example of a more personalized experience and comprehensive response.
    In our example above, had we further instructed our agent not to make the blog
    posting too chunky with too many short sections, it would learn that preference.
    Contrast this with a traditional RAG chatbot type setting where a model starts
    fresh each time.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在代理系统中，代理通常可以访问更高级的扎根上下文形式，例如存储过去工作和被要求执行的任务信息的内存缓冲区。代理将过去交互存储在记忆中并规划未来行动的能力鼓励了个性化的体验和全面的响应。而且，这些代理会随着时间的推移而学习。例如，如果你想要报告以某种特定风格编写，或者Instagram帖子与LinkedIn帖子相比的辣味水平，代理记忆可以持续这些偏好，这是一个更个性化体验和全面响应的绝佳例子。在我们的例子中，如果我们进一步指示我们的代理不要让博客帖子过于零散，包含太多短节，它就会学会这个偏好。与此相对比的是传统的RAG聊天机器人设置，其中模型每次都是从零开始。
- en: 'Although AI agents are autonomous in their decision-making processes, as we
    alluded to earlier, they require goals and environments defined by humans.^([29](ch07.html#id1020))
    There are four main influences on autonomous agent behavior:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管AI代理在决策过程中是自主的，如我们之前所提到的，它们需要由人类定义的目标和环境。[^([29](ch07.html#id1020))] 有四个主要因素影响自主代理的行为：
- en: The team that designs and trains (or more likely, uses or fine-tunes) the underlying
    LLM(s) used in the agentic workflow. As you’ve learned about in this book, it’s
    more likely that you use an LLM to support your agents someone else built, and
    depending on the task it needs to perform, you may have steered it to your business.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计和训练（或更可能的是，使用或微调）在代理工作流程中使用的底层LLM（大型语言模型）的团队。正如你在本书中学到的，你更有可能使用别人构建的LLM来支持你的代理，并且根据它需要执行的任务，你可能已经将其引导到你的业务中。
- en: The team of engineers that build the agentic AI system. These are the folks
    who are defining the tools to which the system will have access.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建代理人工智能系统的工程师团队。这些人正在定义系统将能够访问的工具。
- en: The team of developers that configure the agent and provide the user with access
    to it and the tools. These folks work in conjunction with the business to help
    create the agentic persona.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置代理并提供用户访问权限及其工具的开发者团队。这些人与业务部门合作，帮助创建代理角色。
- en: The user who prompts the AI agent with specific goals and tasks.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提出具体目标和任务的AI代理的用户。
- en: As you saw in the example earlier, given a user’s goals and the agent’s available
    tools, the agentic workflow created a plan that included tasks and subtasks to
    accomplish the complex goal it was handed. If this were a simple task (like writing
    a form letter), planning wouldn’t be a necessary step. Instead, the agent could
    iteratively reflect on its responses and improve them without planning its next
    steps. That was not the case with our blog posting. Recall in Figures [7-6](#ch07_figure_6_1740182051640368)
    and [7-8](#ch07_figure_8_1740182051640408) that our agent’s logic showed us some
    insights into its reasoning and planning for how to solve the task we gave it
    (there was a lot more thinking, reasoning, and planning we didn’t show you).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如你之前在示例中看到的，给定用户的目标和代理可用的工具，代理工作流程创建了一个包含任务和子任务以完成所赋予的复杂目标的计划。如果这是一个简单任务（如撰写格式化信函），规划就不是必要步骤。相反，代理可以迭代地反思其响应并改进它们，而不需要规划其下一步行动。我们的博客帖子并不是这样。回想一下图[7-6](#ch07_figure_6_1740182051640368)和[7-8](#ch07_figure_8_1740182051640408)，我们的代理的逻辑向我们展示了其推理和规划如何解决我们赋予它的任务（我们没有向你展示更多思考、推理和规划）的见解。
- en: AI agents base their actions on the information they perceive. Often, AI agents
    do not have the full knowledge base needed for tackling all subtasks within a
    complex goal. For example, our agents didn’t have knowledge on the impact of inflation
    on housing. To remedy this, our agents used their available tools (in our example,
    an agent went out and searched the web for information). These tools can include
    external datasets, web searches, APIs, and even other agents. After the needed
    information was retrieved using these tools, our agent updated its knowledge base.
    This means that each step of the way, an agent can reassess its plan of action
    and self-correct.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: AI代理基于它们感知到的信息采取行动。通常，AI代理没有解决复杂目标中所有子任务所需的完整知识库。例如，我们的代理没有关于通货膨胀对住房影响的知识。为了解决这个问题，我们的代理使用了他们可用的工具（在我们的例子中，一个代理出去在网上搜索信息）。这些工具可以包括外部数据集、网络搜索、API，甚至其他代理。使用这些工具检索到所需信息后，我们的代理更新了其知识库。这意味着在每个步骤中，代理都可以重新评估其行动计划并自我纠正。
- en: While our previous example showcased writing, imagine something even more complex,
    such as planning your next vacation. You task an AI agent with predicting which
    week in the next year would likely have the best weather for a surfing trip in
    Hawaii. Since the LLM model at the core of the agent does not specialize in weather
    patterns, that agent would gather information from an external database (versus
    a web search) comprised of daily weather reports for Hawaii over the past several
    years. Despite acquiring this new information, the agent still can’t determine
    the optimal weather conditions for surfing, so the next subtask is created. For
    this subtask, the agent communicates with an external agent that specializes in
    surfing. Let’s say that in doing so, the agent learns that high tides and sunny
    weather with little to no rain provide the best surfing conditions—not just sunny
    skies. The agent then combines the information it has learned from its tools to
    identify those best patterns to put some “maika’i loa” (awesome in Hawaiian) into
    your surfing vacation. It comes back with a prediction on what weeks in the year
    are likely to have high tides, sunny weather, and a low chance of rain. These
    findings are then presented to you, or perhaps the agent even goes on to book
    your trip.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们之前的例子展示了写作，但想象一下更复杂的情况，比如规划你的下一次假期。你让一个AI代理预测下一年中哪一周在夏威夷进行冲浪旅行的天气最有可能是最好的。由于该代理核心的LLM模型并不专门研究天气模式，该代理会从包含过去几年夏威夷每日天气报告的外部数据库（而不是网络搜索）中收集信息。尽管获得了这些新信息，代理仍然无法确定最佳的冲浪天气条件，因此创建了下一个子任务。对于这个子任务，代理与一个专门从事冲浪的外部代理进行沟通。假设在这个过程中，代理了解到大潮和晴朗的天气，几乎没有降雨，提供了最佳的冲浪条件——不仅仅是晴朗的天空。然后，代理将其从工具中学到的信息结合起来，以确定那些最佳模式，为你的冲浪假期增添一些“maika’i
    loa”（夏威夷语中的“棒极了”）。它将预测哪些周可能会有大潮、晴朗的天气和低降雨概率带回来。然后，这些发现将展示给你，或者也许代理甚至会帮你预订旅行。
- en: How Agents Are Built
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代理是如何构建的
- en: 'At their heart, agents are system-based implementations of an LLM. In this
    implementation, you will have an LLM with a set of operating instructions on how
    to plan and how to make external tool calls (be that a web search or a prompt
    to another LLM, etc.), embedded within a broader system that performs key, non-GenAI
    activities, such as:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在本质上，代理是LLM的系统化实现。在这个实现中，你将有一个LLM，它包含一组操作指令，说明如何规划和如何调用外部工具（无论是网络搜索还是对另一个LLM的提示等），嵌入在一个更广泛的系统中，该系统执行关键的非GenAI活动，例如：
- en: Parsing an LLM’s output, searching for tool call invocations that the LLM will
    trigger
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解析LLM的输出，搜索LLM将触发的工具调用
- en: Processing an external API based on the identified tool call
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据识别的工具调用处理外部API
- en: Processing a tool response and injecting it directly back into the LLM’s conversation
    history with the proper formatting (like converting JSON to written text or Markdown)
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理工具响应，并以适当的格式（如将JSON转换为文本或Markdown）将其直接注入LLM的对话历史中
- en: Handling advanced memory functions, such as conversation history manipulation
    and storage of key artifacts in LLM-accessible memory
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理高级记忆功能，例如操纵对话历史和将关键工件存储在LLM可访问的内存中
- en: As you can see, this is a complicated system that the LLM operates in, often
    resulting in complex, multipage prompts summarizing the operating instructions
    for an agent (or group of them).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这是一个复杂的系统，LLM在其中运行，通常会产生复杂的多页提示，总结代理（或一组代理）的操作指令。
- en: While there is not one standard prompt for instructing AI agents, several paradigms,
    also known as *agent architectures*, have emerged for solving multistep problems
    and determining how to trigger planning, tool usage, and memory within an LLM
    workflow.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管没有一种标准的提示来指导AI代理，但已经出现了几种范式，也称为*代理架构*，用于解决多步骤问题以及确定如何在LLM工作流程中触发规划、工具使用和记忆。
- en: ReAct (Reasoning and Action)
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ReAct（推理和行动）
- en: This is the agent architecture we used in our blog example. It lets users instruct
    their agents to “think” and plan after each action taken...and with each tool
    response to decide which tool to use next. These think-act-observe loops are used
    to solve problems step-by-step and iteratively improve upon responses.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们博客示例中使用的代理架构。它允许用户指导他们的代理在每个行动之后“思考”和规划...并且根据每个工具的响应来决定下一个要使用的工具。这些思考-行动-观察循环用于逐步解决问题并迭代改进响应。
- en: Through the prompt structure, agents can be instructed to reason slowly and
    display each “thought”^([30](ch07.html#id1025)) (you saw this in our blog example).
    An agent’s verbal reasoning gives insight into how responses are formulated. In
    this framework, agents continuously update their context with new reasoning. This
    can be interpreted as a form of chain of thought (CoT) prompting.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提示结构，可以指导代理缓慢地进行推理并展示每个“思考”^([30](ch07.html#id1025))（你可以在我们的博客示例中看到这一点）。代理的口头推理可以揭示响应是如何形成的。在这个框架中，代理会不断用新的推理更新其上下文。这可以解释为一种形式的思维链（CoT）提示。
- en: ReWOO (Reasoning WithOut Observation)
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ReWOO（无需观察的推理）
- en: The ReWOO method, unlike ReAct, does all the planning up front. This can be
    desirable from a human-centered perspective since the user can confirm the plan
    before it is executed. This is important because at some point someone has to
    pay to spin up the resources to run all of this—it’s not a bad approach to know
    what’s going to happen (and how) before you pay for it.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 与ReAct不同，ReWOO方法在开始时完成所有规划。从以人为本的角度来看，这可能是有吸引力的，因为用户可以在执行之前确认计划。这很重要，因为总有一天有人需要付费来启动运行所有这些资源的资源——在付费之前知道将要发生什么（以及如何发生）并不是一个坏的方法。
- en: The ReWOO workflow is made up of three modules. In the planning module, an agent
    anticipates its next steps given a user’s prompt. The next stage entails collecting
    the outputs produced by calling these tools. Finally, an agent pairs the initial
    plan with the tool outputs to formulate a response. This planning ahead can greatly
    reduce token usage and computational complexity as well as the repercussions of
    intermediate tool failure.^([31](ch07.html#id1029))
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ReWOO工作流程由三个模块组成。在规划模块中，代理根据用户的提示预测其下一步行动。下一阶段包括收集调用这些工具产生的输出。最后，代理将初始计划与工具输出配对，以形成响应。这种提前规划可以大大减少令牌使用量、计算复杂度以及中间工具失败的影响.^([31](ch07.html#id1029))
- en: Risks and Limitations of Agentic Systems
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代理系统的风险和限制
- en: 'Agentic systems have all of the same risks and limitations as GenAI, particularly
    concerns of bias, hallucinations, jailbreaking, etc. In addition to these common
    issues, there are specific limitations and risks with agentic systems that we
    want you to understand when considering an agentic deployment—and that’s why we
    wrote this section:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 代理系统具有与GenAI相同的所有风险和限制，特别是关于偏见、幻觉、越狱等问题。除了这些常见问题之外，代理系统还有我们希望你在考虑代理部署时了解的具体限制和风险——这就是我们编写本节的原因：
- en: Computational complexity and infinite feedback loops
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 计算复杂性和无限反馈循环
- en: Because AI agents often leverage multiple inference calls to respond to a single
    prompt, they can become very computationally expensive, particularly for simple
    NLP tasks. It may be more efficient and cost-effective to run a standard LLM workflow,
    without bringing in a broader agentic system.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 由于AI代理通常需要多次推理调用来响应单个提示，因此它们可能变得非常计算密集，尤其是对于简单的NLP任务。运行标准的LLM工作流程可能更高效、成本效益更高，而不需要引入更广泛的代理系统。
- en: In addition, agents that are unable to create a comprehensive plan, or reflect
    on their findings, may find themselves repeatedly calling the same tools, invoking
    infinite feedback loops. If agents are left unattended and get into an infinite
    feedback loop that runs inference on a large LLM, you could be looking at a very
    expensive bill! We’ve literally seen this happen. When we first started experimenting
    with this technology, we asked an agent to find the world’s best tzatziki recipe.
    We had hoped it would go out and find some winner lists and use some logic to
    compare them (like number of hits on the website or how popular the domain was).
    In the end, our agent got lost in a sea of contradictory food blogs and recommendations
    of lots of garlic (because every AI knows where the tzatziki magic happens) and
    no real “Opa!” in the lackluster grand finale.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，无法制定全面计划或反思其发现的代理可能会发现自己反复调用相同的工具，引发无限反馈循环。如果代理被忽视并陷入无限反馈循环，对大型LLM进行推理，你可能会面临一笔非常昂贵的账单！我们确实见过这种情况发生。当我们最初开始尝试这项技术时，我们让一个代理寻找世界上最好的tzatziki食谱。我们希望它会出去找到一些赢家名单，并使用一些逻辑进行比较（比如网站上的点击次数或域名的流行程度）。最后，我们的代理迷失在一片矛盾的食物博客和大量大蒜推荐的海洋中（因为每个AI都知道tzatziki魔法发生的地方），在乏味的最终决赛中没有任何真正的“Opa！”
- en: Control and observability
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 控制和可观察性
- en: The flexibility that allows agents to robustly handle new tasks and solve problems
    is only possible because of the slackening control imposed on the system. It becomes
    critical, therefore, to monitor and understand an agent’s decision-making process
    and actions in agentic workflows. Depending on how an agent is implemented, the
    full internal workings and decision-making flows are not always transparent, potentially
    leading to unintended consequences. For instance, a model may adapt in unforeseen
    ways, leading to behaviors that are not aligned with your original objectives
    or your values.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 允许代理稳健地处理新任务和解决问题的灵活性，仅因系统上施加的控制放松而成为可能。因此，监控和理解代理在代理工作流程中的决策过程和行动变得至关重要。根据代理的实现方式，其完整的内部运作和决策流程并不总是透明的，这可能导致意想不到的后果。例如，一个模型可能会以不可预见的方式适应，导致的行为与你的原始目标或价值观不一致。
- en: This lack of control and observability can result in some of the undesirable
    outcomes you learned about in [Chapter 5](ch05.html#ch05_live_die_buy_or_try_much_will_be_decided_by_ai_1740182048942635);
    for example, biased or discriminatory actions, which can have severe consequences
    in high-stakes applications like healthcare, finance, or education. As you go
    down this path, we want to remind you how essential it is to develop requirements
    for transparent and explainable LLMs, allowing for real-time monitoring and corrective
    actions to mitigate these risks.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这种缺乏控制和可观察性可能导致你在[第5章](ch05.html#ch05_live_die_buy_or_try_much_will_be_decided_by_ai_1740182048942635)中学到的某些不良后果；例如，具有偏见或歧视性的行为，在医疗保健、金融或教育等高风险应用中可能产生严重后果。随着你沿着这条道路前进，我们想提醒你，开发对透明和可解释的LLM的要求是多么重要，这允许实时监控和纠正措施，以减轻这些风险。
- en: Security and complex permissions
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 安全性和复杂权限
- en: There are a multitude of potential security and safety challenges that need
    to be solved before any custom-built (and perhaps the off-the-shelf ones you buy)
    agents can be safely deployed in complex enterprise environments. For example,
    if an HR agent designed for acting upon an employee’s request has access to an
    HR database that includes sensitive details for all employees, data security measures
    should be put in place to make sure that agent doesn’t accidentally divulge (or
    have access to, for that matter) sensitive information about other employees to
    the end user. Quite simply, this requires fine-grained access controls (FGACs)
    and role-based access controls (RBACs), adherence to personally identifiable information
    (PII) transfer protocols, principle of least privileges assignments, an identity
    fabric, and more. Similarly, in multiagent systems, communication protocols need
    to be established for how agents with access to different sensitive information
    types can work together without leaking sensitive content and adhering to data
    transit regulations that require encryption.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何定制的（以及可能购买的现成）代理可以在复杂的企业环境中安全部署之前，需要解决许多潜在的安全和挑战。例如，如果一个旨在响应员工请求的人力资源代理可以访问包含所有员工敏感细节的人力资源数据库，那么应该实施数据安全措施，以确保该代理不会意外泄露（或访问）有关其他员工的敏感信息给最终用户。简单来说，这需要细粒度访问控制（FGACs）、基于角色的访问控制（RBACs）、遵守个人身份信息（PII）传输协议、最小权限分配原则、身份编织以及更多。同样，在多代理系统中，需要建立通信协议，以便具有不同敏感信息类型访问权限的代理可以协同工作，而不会泄露敏感内容，并遵守需要加密的数据传输规定。
- en: 'Three Tips to Get You Started: Our Agentic Best Practices'
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 三条启动小贴士：我们的代理最佳实践
- en: Whenever you come across anything new, it’s always best to get some tips to
    help you get started. We created this section with extra help from some IBMers
    like Anna Gutowska, whose day-to-day job is literally training agentic systems
    that are smart enough to do incredible things, but not so wild that they start
    doing crazy things. If you pay attention to these tips, you’ll be living your
    best agentic life—a trusted one.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 每当你遇到任何新事物时，总是最好获取一些小贴士来帮助你开始。我们在这个部分中得到了一些IBM员工，如Anna Gutowska的帮助，她的日常工作就是训练足够智能以完成令人难以置信的事情的代理系统，但又不至于过于狂野而开始做疯狂的事情。如果你注意这些小贴士，你将过上最好的代理生活——一个值得信赖的生活。
- en: 1\. Activity logs
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1. 活动日志
- en: To better understand and debug agent behavior after the fact, developers can
    provide users with access to a log of agent actions. These actions can include
    the use of external tools and describe the individual steps taken to reach the
    goal. This transparency gives users insights into an agent’s iterative decision-making
    process and provides the opportunity to discover errors and build trust.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解和调试代理行为，开发者可以向用户提供访问代理操作日志的权限。这些操作可以包括使用外部工具，并描述为达到目标所采取的各个单独步骤。这种透明度使用户能够深入了解代理的迭代决策过程，并有机会发现错误并建立信任。
- en: 2\. Interruption and runtime observability
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2. 中断和运行时可观察性
- en: Prevent AI agents from running for overly long periods to avoid cases of unintended
    infinite feedback loops, changes in access to certain tools, or malfunctioning
    due to design flaws. One way to accomplish this is by implementing interruptability,
    where a human user (or an external resource manager like Turbonomic) can stop
    a pointless (or endless) workflow. To make interruptability more powerful, you
    also need to layer in observability to your agentic system so that you can monitor
    where an agent is in its workflow, and if something goes wrong, quickly find the
    what and how.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 防止AI代理运行过长时间，以避免意外无限反馈循环、访问某些工具的变更或由于设计缺陷而导致的故障。实现这一目标的一种方法是通过实现中断性，使人类用户（或像Turbonomic这样的外部资源管理器）能够停止无意义（或无休止）的工作流程。为了使中断性更强大，你还需要将可观察性层叠到你的代理系统中，这样你就可以监控代理在其工作流程中的位置，并在出现问题的情况下，快速找到原因和方式。
- en: 3\. Human supervision
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3. 人类监督
- en: To assist in the learning process for AI agents, especially in their early stages
    in a new environment, it can be helpful to provide occasional human feedback.
    This allows your agents to compare their performance to the expected standard
    and adjust accordingly. This form of feedback is helpful in improving any agent’s
    adaptability to user preferences.^([32](ch07.html#id1037))
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助人工智能代理的学习过程，尤其是在它们在新环境中的早期阶段，提供偶尔的人类反馈可能会有所帮助。这允许您的代理将它们的性能与预期标准进行比较，并相应地进行调整。这种反馈形式有助于提高任何代理对用户偏好的适应性.^([32](ch07.html#id1037))。
- en: For example, you can set up your framework such that every time your agent finishes
    a task, it stops and asks for some feedback—this gives you an opportunity to tell
    it what’s missing or what it could do better. [Figure 7-10](#ch07_figure_10_1740182051640455)
    was an example of where this would have been a great thing to do. Upon reading
    the social media posts, we could have shaped it to better suit our style and the
    audiences using those outlets. We could then push this feedback into an LLM that
    would extract these tips for each task and put them into the memory of our agents
    to reference in the future. Once again, we could use AI here to help AI by creating
    a “judge” AI model to look at the output work for our LLM and see whether it’s
    up to snuff.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您可以设置您的框架，以便每次您的代理完成一项任务时，它就会停下来并请求一些反馈——这给了您机会告诉它缺少什么或它能做得更好。[图7-10](#ch07_figure_10_1740182051640455)
    就是这样一个例子，这样做会非常好。在阅读社交媒体帖子后，我们可以将其调整得更适合我们的风格和那些渠道的受众。然后我们可以将这种反馈推送到一个LLM，它会提取每个任务的这些提示并将其放入我们代理的未来参考记忆中。再次强调，我们可以使用AI来帮助AI，通过创建一个“裁判”AI模型来查看我们LLM的输出工作，看看它是否合格。
- en: Apart from this, it is a best practice to require human approval before an AI
    agent takes highly impactful actions. For instance, actions ranging from sending
    mass emails to financial trading should require human confirmation.^([33](ch07.html#id1039))
    Some level of human monitoring is recommended for such high-risk domains.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这一点，在人工智能代理采取高度影响性行动之前要求人类批准是一种最佳实践。例如，从发送大量电子邮件到金融交易等行动都应该需要人类确认.^([33](ch07.html#id1039))
    对于这样的高风险领域，建议进行一定水平的人类监控。
- en: Think About This When It Comes to AI Agents
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 当涉及到人工智能代理时，请思考以下问题
- en: 'AI agents and agentic systems are becoming popular for a reason: they are able
    to significantly improve AI’s performance and robustness on complex tasks. The
    ability to plan and reason for a task, bring in the latest up-to-date information
    via tool calls, and break down complex problems into smaller, more tractable components
    also opens the door for smaller, open source models to take on more challenging
    assignments. At the end of the day, this technology is still evolving, and while
    it improves model performance and productivity, ensure you spend time thinking
    about safety, security, and cost before leveraging AI agents for production tasks.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能代理和代理系统之所以越来越受欢迎，是有原因的：它们能够显著提高人工智能在复杂任务上的性能和鲁棒性。能够为任务进行规划和推理，通过工具调用引入最新的信息，并将复杂问题分解成更小、更易于处理的组件，这也为小型、开源模型承担更具挑战性的任务打开了大门。最终，这项技术仍在不断发展，尽管它提高了模型性能和生产力，但在利用人工智能代理进行生产任务之前，确保您花时间考虑安全性、安全性和成本。
- en: Wrapping It Up
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: There are a lot of exciting things happening in the world of AI, and we hope
    we’ve convinced you that whether it is the advent of SLMs or the efficacy of more
    systems-based approaches to AI, including model routing, MoE, or agentic systems,
    “one model will not rule them all.” In the next chapter, we are going to cover
    how enterprises can extend these systems-based implementations of LLMs by specializing
    SLMs on enterprise data.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能的世界正在发生许多令人兴奋的事情，我们希望我们已经说服您，无论是SLMs的兴起还是更系统化的AI方法的有效性，包括模型路由、MoE或代理系统，“一个模型不会统治一切”。在下一章中，我们将介绍企业如何通过在企业数据上专门化SLMs来扩展LLMs的系统化实现。
- en: ^([1](ch07.html#id906-marker)) Maximilian Schreiner, “GPT-4 Architecture, Datasets,
    Costs and More Leaked,” *The Decoder*, July 11, 2023, [*https://oreil.ly/6sD6g*](https://oreil.ly/6sD6g).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch07.html#id906-marker)) Maximilian Schreiner，“GPT-4架构、数据集、成本及其他泄露”，*The
    Decoder*，2023年7月11日，[*https://oreil.ly/6sD6g*](https://oreil.ly/6sD6g)。
- en: ^([2](ch07.html#id907-marker)) See OpenAI’s API pricing [online](https://oreil.ly/d21qP).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch07.html#id907-marker)) 查看OpenAI的API定价 [在线](https://oreil.ly/d21qP)。
- en: ^([3](ch07.html#id909-marker)) See the update at [*https://oreil.ly/jCoxe*](https://oreil.ly/jCoxe).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch07.html#id909-marker)) 查看更新 [*https://oreil.ly/jCoxe*](https://oreil.ly/jCoxe)。
- en: '^([4](ch07.html#id910-marker)) Muddu Sudhakar, “Small Language Models (SLMs):
    The Next Frontier for the Enterprise,” *Forbes*, [*https://oreil.ly/slTCo*](https://oreil.ly/slTCo).'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '^([4](ch07.html#id910-marker)) Muddu Sudhakar, “Small Language Models (SLMs):
    The Next Frontier for the Enterprise,” *Forbes*, [*https://oreil.ly/slTCo*](https://oreil.ly/slTCo).'
- en: ^([5](ch07.html#id920-marker)) Jared Kaplan et al., “Scaling Laws for Neural
    Language Models,” preprint, arXiv, January 23, 2020, arXiv:2001.08361 (2020).
    [*https://arxiv.org/abs/2001.08361*](https://arxiv.org/abs/2001.08361).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch07.html#id920-marker)) Jared Kaplan等人， “Scaling Laws for Neural Language
    Models,” 预印本，arXiv，January 23, 2020, arXiv:2001.08361 (2020). [*https://arxiv.org/abs/2001.08361*](https://arxiv.org/abs/2001.08361).
- en: ^([6](ch07.html#id923-marker)) Jordan, Hoffmann et al., “Training Compute-Optimal
    Large Language Models,” preprint, arXiv, March 29, 2022, [*https://arxiv.org/abs/2203.15556*](https://arxiv.org/abs/2203.15556).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch07.html#id923-marker)) Jordan, Hoffmann等人， “Training Compute-Optimal
    Large Language Models,” 预印本，arXiv，March 29, 2022, [*https://arxiv.org/abs/2203.15556*](https://arxiv.org/abs/2203.15556).
- en: ^([7](ch07.html#id930-marker)) Aaron Grattafiori et al., “The Llama 3 Herd of
    Models,” preprint, arXiv, November 23, 2024, [*https://arxiv.org/abs/2407.21783*](https://arxiv.org/abs/2407.21783).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch07.html#id930-marker)) Aaron Grattafiori等人， “The Llama 3 Herd of Models,”
    预印本，arXiv，November 23, 2024, [*https://arxiv.org/abs/2407.21783*](https://arxiv.org/abs/2407.21783).
- en: ^([8](ch07.html#id931-marker)) Aaron Grattafiori et al., “The Llama 3 Herd of
    Models,” preprint, arXiv, November 23, 2024, [*https://arxiv.org/abs/2407.21783*](https://arxiv.org/abs/2407.21783).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch07.html#id931-marker)) Aaron Grattafiori等人， “The Llama 3 Herd of Models,”
    预印本，arXiv，November 23, 2024, [*https://arxiv.org/abs/2407.21783*](https://arxiv.org/abs/2407.21783).
- en: ^([9](ch07.html#id935-marker)) Suriya Gunasekar et al., “Textbooks Are All You
    Need,” arXiv, October 2, 2023, [*https://arxiv.org/pdf/2306.11644*](https://arxiv.org/pdf/2306.11644).
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch07.html#id935-marker)) Suriya Gunasekar等人， “Textbooks Are All You Need,”
    arXiv，October 2, 2023, [*https://arxiv.org/pdf/2306.11644*](https://arxiv.org/pdf/2306.11644).
- en: ^([10](ch07.html#id938-marker)) Kate Knibbs, “The Battle Over Books3 Is Just
    the Beginning,” *Wired*, September 4, 2023, [*https://oreil.ly/58JTr*](https://oreil.ly/58JTr).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch07.html#id938-marker)) Kate Knibbs， “The Battle Over Books3 Is Just
    the Beginning,” *Wired*，September 4, 2023, [*https://oreil.ly/58JTr*](https://oreil.ly/58JTr).
- en: ^([11](ch07.html#id945-marker)) Previously known as PubMedGTP.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch07.html#id945-marker)) 之前被称为PubMedGTP。
- en: ^([12](ch07.html#id949-marker)) Reuters Graphics, “COBOL Blues,” [*https://oreil.ly/lM-8U*](https://oreil.ly/lM-8U).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch07.html#id949-marker)) Reuters Graphics, “COBOL Blues,” [*https://oreil.ly/lM-8U*](https://oreil.ly/lM-8U).
- en: '^([13](ch07.html#id960-marker)) *The Vicuna Team* (blog), “Vicuna: An Open-Source
    Chatbot Impressing GPT-4 with 90% ChatGPT Quality,” LMSYS, March 30, 2023, [*https://oreil.ly/qRHD4*](https://oreil.ly/qRHD4).'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '^([13](ch07.html#id960-marker)) *The Vicuna Team* (博客), “Vicuna: An Open-Source
    Chatbot Impressing GPT-4 with 90% ChatGPT Quality,” LMSYS, March 30, 2023, [*https://oreil.ly/qRHD4*](https://oreil.ly/qRHD4).'
- en: ^([14](ch07.html#id968-marker)) “Nemotron-4-340B-Instruct,” Hugging Face, [*https://oreil.ly/5Mh3Y*](https://oreil.ly/5Mh3Y).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch07.html#id968-marker)) “Nemotron-4-340B-Instruct,” Hugging Face, [*https://oreil.ly/5Mh3Y*](https://oreil.ly/5Mh3Y).
- en: ^([15](ch07.html#id972-marker)) Samanatha Subin, “Nvidia Sheds Almost $600 Billion
    in Market Cap, Biggest One-Day Loss in U.S. History,” CNBC, January 27, 2025,
    [*https://oreil.ly/vWA0q*](https://oreil.ly/vWA0q).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch07.html#id972-marker)) Samanatha Subin, “Nvidia Sheds Almost $600 Billion
    in Market Cap, Biggest One-Day Loss in U.S. History,” CNBC, January 27, 2025,
    [*https://oreil.ly/vWA0q*](https://oreil.ly/vWA0q).
- en: '^([16](ch07.html#id977-marker)) DeepSeek-AI, “DeepSeek-R1: Incentivizing Reasoning
    Capability in LLMs via Reinforcement Learning,” arXiv, January 22, 2025, [*https://arxiv.org/pdf/2501.12948*](https://arxiv.org/pdf/2501.12948).'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '^([16](ch07.html#id977-marker)) DeepSeek-AI， “DeepSeek-R1: Incentivizing Reasoning
    Capability in LLMs via Reinforcement Learning,” arXiv，January 22, 2025, [*https://arxiv.org/pdf/2501.12948*](https://arxiv.org/pdf/2501.12948).'
- en: ^([17](ch07.html#id978-marker)) See the data on [Hugging Face’s website](https://oreil.ly/GOLDy).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ^([17](ch07.html#id978-marker)) 请参阅[Hugging Face网站](https://oreil.ly/GOLDy)上的数据。
- en: ^([18](ch07.html#id979-marker)) See the datasets on the [Hugging Face website](https://oreil.ly/rVZKY).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ^([18](ch07.html#id979-marker)) 请参阅[Hugging Face网站](https://oreil.ly/rVZKY)上的数据集。
- en: ^([19](ch07.html#id980-marker)) Cade Metz, “OpenAI Says DeepSeek May Have Improperly
    Harvested Its Data,” *The New York Times*, January 29, 2025, [*https://oreil.ly/7xn_C*](https://oreil.ly/7xn_C).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ^([19](ch07.html#id980-marker)) Cade Metz， “OpenAI Says DeepSeek May Have Improperly
    Harvested Its Data,” *The New York Times*，January 29, 2025, [*https://oreil.ly/7xn_C*](https://oreil.ly/7xn_C).
- en: ^([20](ch07.html#id984-marker)) See the licensing agreement on [Llama’s website](https://oreil.ly/6prL2).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: ^([20](ch07.html#id984-marker)) 请参阅[Llama网站](https://oreil.ly/6prL2)上的许可协议。
- en: ^([21](ch07.html#id986-marker)) *Cisco Blogs*, “Evaluating Security Risk in
    DeepSeek and Other Frontier Reasoning Models,” by Paul Kassianik and Amin Karbasi,
    posted January 31, 2025, [*https://oreil.ly/gy5Xp*](https://oreil.ly/gy5Xp).
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ^([21](ch07.html#id986-marker)) *思科博客*，“评估DeepSeek和其他前沿推理模型的安全风险”，作者Paul Kassianik和Amin
    Karbasi，发布于2025年1月31日，[*https://oreil.ly/gy5Xp*](https://oreil.ly/gy5Xp).
- en: ^([22](ch07.html#id987-marker)) Tested using deepinfra.com’s hosted version
    of DeepSeek-R1\.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ^([22](ch07.html#id987-marker)) 使用deepinfra.com提供的DeepSeek-R1版本进行测试。
- en: ^([23](ch07.html#id988-marker)) See the posting on [IBM’s website](https://oreil.ly/UrcIZ).
    Even more important, these distilled models failed miserably on safety evaluations.
    It is difficult to know exactly why these models are deficient in general performance
    and safety, but a potential hypothesis is that by focusing exclusively on code
    and math during distillation, safety and general performance were left to the
    wayside.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ^([23](ch07.html#id988-marker)) 请参阅[IBM网站上的帖子](https://oreil.ly/UrcIZ)。更重要的是，这些蒸馏模型在安全性评估中彻底失败。很难确切知道为什么这些模型在总体性能和安全性方面存在缺陷，但一个可能的假设是，在蒸馏过程中只专注于代码和数学，导致安全性和总体性能被忽视。
- en: ^([24](ch07.html#id993-marker)) Tal Shnitzer et al., “Large Language Model Routing
    with Benchmark Datasets,” preprint, arXiv, September 27, 2023, [*https://arxiv.org/abs/2309.15789*](https://arxiv.org/abs/2309.15789).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ^([24](ch07.html#id993-marker)) Tal Shnitzer等人，“使用基准数据集的大语言模型路由”，预印本，arXiv，2023年9月27日，[*https://arxiv.org/abs/2309.15789*](https://arxiv.org/abs/2309.15789).
- en: ^([25](ch07.html#id994-marker)) Percy Liang et al., “Holistic Evaluation of
    Language Models,” preprint, arXiv, October 1, 2023, [*https://arxiv.org/abs/2211.09110*](https://arxiv.org/abs/2211.09110).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ^([25](ch07.html#id994-marker)) 李航等人，“语言模型的整体评估”，预印本，arXiv，2023年10月1日，[*https://arxiv.org/abs/2211.09110*](https://arxiv.org/abs/2211.09110).
- en: ^([26](ch07.html#id995-marker)) Tal Shnitzer et al., “Large Language Model Routing
    with Benchmark Datasets,” preprint, September 27, 2023, arXiv, [*https://arxiv.org/abs/2309.15789*](https://arxiv.org/abs/2309.15789).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: ^([26](ch07.html#id995-marker)) Tal Shnitzer等人，“使用基准数据集的大语言模型路由”，预印本，2023年9月27日，arXiv，[*https://arxiv.org/abs/2309.15789*](https://arxiv.org/abs/2309.15789).
- en: ^([27](ch07.html#id1008-marker)) DeepSeek-AI, “DeepSeek-V3 Technical Report,”
    preprint, arXiv, February 18, 2025, [*https://arxiv.org/html/2412.19437v1*](https://arxiv.org/html/2412.19437v1).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ^([27](ch07.html#id1008-marker)) DeepSeek-AI，“DeepSeek-V3 技术报告”，预印本，arXiv，2025年2月18日，[*https://arxiv.org/html/2412.19437v1*](https://arxiv.org/html/2412.19437v1).
- en: ^([28](ch07.html#id1010-marker)) Ibid.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ^([28](ch07.html#id1010-marker)) 同上。
- en: ^([29](ch07.html#id1020-marker)) Alan Chan et al., “Visibility into AI Agents,”
    arXiv, updated May 17, 2024, [*https://arxiv.org/abs/2401.13138*](https://arxiv.org/abs/2401.13138).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: ^([29](ch07.html#id1020-marker)) Alan Chan等人，“AI代理的可见性”，arXiv，更新于2024年5月17日，[*https://arxiv.org/abs/2401.13138*](https://arxiv.org/abs/2401.13138).
- en: ^([30](ch07.html#id1025-marker)) Gautier Dagan et al., “Dynamic Planning with
    a LLM,” preprint, arXiv, August 11, 2023, [*https://arxiv.org/abs/2308.06391*](https://arxiv.org/abs/2308.06391).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: ^([30](ch07.html#id1025-marker)) Gautier Dagan等人，“使用LLM进行动态规划”，预印本，arXiv，2023年8月11日，[*https://arxiv.org/abs/2308.06391*](https://arxiv.org/abs/2308.06391).
- en: '^([31](ch07.html#id1029-marker)) Bienfeng Xu et al., “ReWOO: Decoupling Reasoning
    from Observations for Efficient Augmented Language Models,” preprint, arXiv, May
    23, 2023, arXiv. [*https://arxiv.org/abs/2305.18323*](https://arxiv.org/abs/2305.18323).'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ^([31](ch07.html#id1029-marker)) 徐海滨等人，“ReWOO：从观察中解耦推理以提高增强语言模型的效率”，预印本，arXiv，2023年5月23日，arXiv.
    [*https://arxiv.org/abs/2305.18323*](https://arxiv.org/abs/2305.18323).
- en: '^([32](ch07.html#id1037-marker)) Bienfeng Xu et al., “ReWOO: Decoupling Reasoning
    from Observations for Efficient Augmented Language Models,” preprint, arXiv, May
    23, 2023, [*https://arxiv.org/abs/2305.18323*](https://arxiv.org/abs/2305.18323).'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: ^([32](ch07.html#id1037-marker)) 徐海滨等人，“ReWOO：从观察中解耦推理以提高增强语言模型的效率”，预印本，arXiv，2023年5月23日，[*https://arxiv.org/abs/2305.18323*](https://arxiv.org/abs/2305.18323).
- en: ^([33](ch07.html#id1039-marker)) Veselka Sasheva Petrova-Dimitrova, “Classifications
    of Intelligence Agents and Their Applications,” *Fundamental Sciences and Applications*
    28, no. 1 (2022).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: ^([33](ch07.html#id1039-marker)) Veselka Sasheva Petrova-Dimitrova，“智能代理的分类及其应用”，*基础科学与应用*
    28，第1期（2022年）。
