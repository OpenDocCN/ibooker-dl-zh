<html><head></head><body>
<div id="sbo-rt-content"><div class="readable-text" id="p1">
<h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">10</span> </span> <span class="chapter-title-text">Identification and the causal hierarchy</span></h1>
</div>
<div class="introduction-summary">
<h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3>
<ul>
<li class="readable-text" id="p2">Motivating examples for identification</li>
<li class="readable-text" id="p3">Using y0 for identification and deriving estimands</li>
<li class="readable-text" id="p4">How to derive counterfactual graphs in y0</li>
<li class="readable-text" id="p5">Deriving SWIGs for graph-based counterfactual identification</li>
</ul>
</div>
<div class="readable-text" id="p6">
<p>The practice of advancing machine learning often relies on a blind confidence that more data and the right architecture can solve any task. For tasks with causal elements, <em>causal identification</em> can make that less of a matter of faith and more of a science. It can tell us when more data won’t help, and what types of inductive biases are needed for the algorithm to work.</p>
</div>
<div class="readable-text intended-text" id="p7">
<p>Causal identification is the task of determining when we can make a causal inference from purely observational data or a counterfactual inference from observational or experimental data. In statistics and data science, it is the theory that allows us to distill causation from correlation and estimate causal effects in the presence of confounders. But causal identification has applications in AI. For example, suppose a deep learning algorithm achieves high performance on a particular causal reasoning benchmark. The ideas behind causal identification tell us that certain causal inductive biases must be baked into the model architecture, training data, training procedure, hyperparameters (e.g., prompts), and/or benchmark data. By tracking down that causal information, we can make sure the algorithm can consistently achieve that benchmark performance in new scenarios. </p>
</div>
<div class="readable-text intended-text" id="p8">
<p>Identification is a theory-heavy part of causal inference. Fortunately, we can rely on libraries to do the theoretical heavy lifting for us and focus on skill-building with these libraries. In this chapter, we’ll focus on a library called y0 (pronounced why-not), which implements algorithms for identification using graphs. By the end of the chapter, we’ll have demystified causal identification and you’ll know how to apply y0’s identification algorithms.</p>
</div>
<div class="readable-text" id="p9">
<h2 class="readable-text-h2" id="sigil_toc_id_229"><span class="num-string">10.1</span> The causal hierarchy</h2>
</div>
<div class="readable-text" id="p10">
<p>The <em>causal hierarchy</em>, also known as <em>Pearl’s hierarchy</em> or the <em>ladder of causation</em>, is a three-level hierarchy over the types of causal questions we ask, models we build, data we acquire, and causal inferences we make.</p>
</div>
<div class="readable-text intended-text" id="p11">
<p>The causal hierarchy consists of three levels: </p>
</div>
<ol>
<li class="readable-text" id="p12"> Association </li>
<li class="readable-text" id="p13"> Intervention </li>
<li class="readable-text" id="p14"> Counterfactual </li>
</ol>
<div class="readable-text" id="p15">
<p>When we do a statistical or causal analysis, we are reasoning at one of these three levels. When we know at what level we are reasoning, we can determine what kind of assumptions and data we need to rely on to do that reasoning correctly.</p>
</div>
<div class="readable-text" id="p16">
<h3 class="readable-text-h3" id="sigil_toc_id_230"><span class="num-string">10.1.1</span> Where questions and queries fall on the hierarchy</h3>
</div>
<div class="readable-text" id="p17">
<p>The questions we ask of our causal model, and the causal queries we formalize from those questions, fall at different levels of the hierarchy. First, level 1 (the association level) is concerned with “What is…?” questions. Let’s illustrate with the online gaming example, shown again in figure 10.1.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p18">
<img alt="figure" height="195" src="../Images/CH10_F01_Ness.png" width="361"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.1</span> The DAG for the online gaming example</h5>
</div>
<div class="readable-text intended-text" id="p19">
<p>An example level 1 question and associated query is</p>
</div>
<div class="readable-text" id="p20">
<blockquote>
<div>
     “What are in-game purchase amounts for players highly engaged in side-quests?” 
     <em>P</em> ( 
     <em>I</em>| 
     <em>E</em>=“high”)
    </div>
</blockquote>
</div>
<div class="readable-text" id="p21">
<p>Reasoning at this level aims to describe, model, or detect dependence between variables. At this level, we’re not reasoning about any causal relationships between the variables.</p>
</div>
<div class="readable-text intended-text" id="p22">
<p>Questions at level 2 (the intervention level) involve non-counterfactual hypothetical conditions, such as</p>
</div>
<div class="readable-text" id="p23">
<blockquote>
<div>
     “What would in-game purchases be for a player if side-quest engagement were high?” 
     <em>P</em> ( 
     <em>I</em>  
     <sub><em>E</em></sub>
<sub>=“high”</sub>)
    </div>
</blockquote>
</div>
<div class="readable-text" id="p24">
<p>At level 2, we formalize such questions with the ideal intervention. Note that any query derived from a level 2 query is also a level 2 query, such as ATEs, (e.g., <em>E</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=“high”</sub> – <em>I</em><sub><em>E</em></sub><sub>=“low”</sub>)) and CATEs.</p>
</div>
<div class="readable-text intended-text" id="p25">
<p>Finally, counterfactual questions and queries fall at level 3 (the counterfactual level): </p>
</div>
<div class="readable-text" id="p26">
<blockquote>
<div>
     “Given this player had low side-quest engagement and low purchases, what would their level of purchases have been if they were more engaged?” 
     <em>P</em>( 
     <em>I</em>
<sub><em>E</em></sub>
<sub>=“high”</sub>| 
     <em>E</em>=“low”, 
     <em>I</em>=“low”)
    </div>
</blockquote>
</div>
<div class="readable-text" id="p27">
<p>As with level 2 queries, any query we derive from a level 3 query also falls at level 3. For example, a causal attribution query designed to answer “Why did this player have low purchases” would be a level 3 query if it were a function of level 3 queries like the probabilities of causation described in section 8.3.</p>
</div>
<div class="readable-text intended-text" id="p28">
<p>In identification, we work directly with queries. The y0 library in Python gives us a domain specific language for representing queries. The following code implements the query <em>P</em>(<em>I</em><sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub>).</p>
</div>
<div class="browsable-container listing-container" id="p29">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.1</span> Creating a query in y0</h5>
<div class="code-area-container">
<pre class="code-area">!pip install git+https://github.com/y0-causal-inference/y0.git@v0.2.0
from y0.dsl import P, Variable     <span class="aframe-location"/> #1
E = Variable("E")    <span class="aframe-location"/> #2
I = Variable("I")     #2
query = P[E](I)    <span class="aframe-location"/> #3
query     <span class="aframe-location"/> #4</pre>
<div class="code-annotations-overlay-container">
     #1 “P” is for probability distributions, and “Variable” is for defining. variables.
     <br/>#2 Define variables G (guild membership), E (side-quest engagement), and I (in-game purchases).
     <br/>#3 Define the distributional query P(I
     <sub>E</sub>).
     <br/>#4 If running in a notebook environment, this will show a rendered image of P(I
     <sub>E</sub>).
     <br/>
</div>
</div>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p30">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Setting up your environment</h5>
</div>
<div class="readable-text" id="p31">
<p>In this chapter, I rely on version 0.2.0 of the y0 library. As it is a relatively new library, the library’s API is in development and recent versions will deviate slightly from what is shown here. Check out the library’s tutorials for recent developments.</p>
</div>
<div class="readable-text" id="p32">
<p>Again, we rely on Graphviz and some custom utilities for plotting DAGs. The Graphviz installation depends on your environment. I am using Ubuntu 22.04 and install Graphviz via libgraphviz-dev. Then I install Python libraries graphviz version 0.20.3, and PyGraphviz version 1.13. The Graphviz code is for plotting only, so if you get stuck, you could forgo plotting for the rest of the code.</p>
</div>
</div>
<div class="readable-text" id="p33">
<p>The <code>query</code> object is an object of the class <code>Probability</code>. The class’s <code>__repr__</code> method (which tells Python what to return in the terminal when you call it directly) is implemented such that when we evaluate the object in the last line of the preceding code in a Jupyter notebook, it will display rendered LaTeX (a typesetting/markup language with a focus on math notation), as in figure 10.2.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p34">
<img alt="figure" height="138" src="../Images/CH10_F02_Ness.png" width="279"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.2</span> The rendered math image returned when you evaluate the <code>query</code> object in listing 10.1</h5>
</div>
<div class="readable-text intended-text" id="p35">
<p>The causal hierarchy applies to models and data as well.</p>
</div>
<div class="readable-text" id="p36">
<h3 class="readable-text-h3" id="sigil_toc_id_231"><span class="num-string">10.1.2</span> Where models and assumptions fall on the hierarchy</h3>
</div>
<div class="readable-text" id="p37">
<p>A “model” is a set of assumptions about the data generating process (DGP). Those assumptions live at various levels of the hierarchy. </p>
</div>
<div class="readable-text" id="p38">
<h4 class="readable-text-h4 sigil_not_in_toc">Level 1 assumptions</h4>
</div>
<div class="readable-text" id="p39">
<p>Models at the associational level have statistical but non-causal assumptions. For example, suppose we’re interested in <em>P</em>(<em>I</em><em>  </em>|<em>E</em><em>  </em>=<em> </em><em>e</em><em> </em>), for either value (“low”, “high”) that <em>e </em>might take. We might fit a linear model to regress in-game purchases <em>I </em>against side-quest engagement <em>E</em>. Or we might train a neural network that maps <em>E</em> to <em>I</em>. These are two statistical models with different parameterizations. In other words, they differ in non-causal, statistical assumptions placed on <em>P</em>(<em>I</em>|<em>E</em>). Once we add causal assumptions, we move to a higher level of the hierarchy.</p>
</div>
<div class="readable-text" id="p40">
<h4 class="readable-text-h4 sigil_not_in_toc">Level 2 assumptions</h4>
</div>
<div class="readable-text" id="p41">
<p>Assumptions that we can represent with a causal DAG are level 2 (interventional) assumptions. An example of a level 2 model would be a causal graphical model (aka, a causal Bayesian network)—a probabilistic model trained on a causal DAG. A causal DAG by itself is a level 2 set of assumptions; assumptions about what causes what. Generally, assumptions that let you deduce the consequences of an intervention are level 2 assumptions.</p>
</div>
<div class="readable-text" id="p42">
<h4 class="readable-text-h4 sigil_not_in_toc">Level 3 Assumptions</h4>
</div>
<div class="readable-text" id="p43">
<p>The canonical example of a level 3 model is a structural causal model. But more generally, assumptions about mechanism—<em>how </em>variables affect one another—are level 3 (counterfactual) assumptions.</p>
</div>
<div class="readable-text intended-text" id="p44">
<p>One way to think about this is that any causal assumption you cannot represent in the structure of the DAG is, by process of elimination, a level 3 assumption. For example, suppose your DAG has the edge <em>X</em><em> </em>→<em>Y</em>. Further, you believe the causal relationship between <em>X</em> and <em>Y</em> is naturally linear. You can’t “see” linearity on the DAG structure, so linearity is a level 3 assumption.</p>
</div>
<div class="readable-text" id="p45">
<h3 class="readable-text-h3" id="sigil_toc_id_232"><span class="num-string">10.1.3</span> Where data falls on the hierarchy</h3>
</div>
<div class="readable-text" id="p46">
<p>Recall the differences between observational data and interventional data. Observational data is passively observed; as a result, it captures statistical associations resulting from dependence between variables in the DGP.</p>
</div>
<div class="readable-text" id="p47">
<h4 class="readable-text-h4 sigil_not_in_toc">Level 1 data</h4>
</div>
<div class="readable-text" id="p48">
<p>In our online gaming example, the level 1 data was logged examples of side-quest engagement and in-game purchases pulled by a database query. Observational data lives at level 1 of the causal hierarchy.</p>
</div>
<div class="readable-text" id="p49">
<h4 class="readable-text-h4 sigil_not_in_toc">Level 2 data</h4>
</div>
<div class="readable-text" id="p50">
<p>Interventional data is generated as the result of applying an intervention, such as data collected from a randomized experiment. In the gaming example, this was the data created because of an A/B test that randomly assigned players to different groups where they are coerced into different fixed side-quest engagement levels. Intervention data lives at level 2 of the hierarchy.</p>
</div>
<div class="readable-text" id="p51">
<h4 class="readable-text-h4 sigil_not_in_toc">Level 3 data</h4>
</div>
<div class="readable-text" id="p52">
<p>Counterfactual data, which lives at level 3 of the hierarchy, is the odd case. Counterfactual data would contain data from across possible worlds. In most domains, we only have data from one world—one <em>potential outcome </em>for each unit of observation in the data.</p>
</div>
<div class="readable-text intended-text" id="p53">
<p>However, there are special cases where counterfactual data exists. For example, cloud service providers use complex but deterministic policies for allocating resources in the cloud, given various constraints. For one example with a given allocation outcome in the log, we could generate a counterfactual outcome for that example by applying a different allocation policy to that example. Similarly, given data produced by simulation software, we could generate counterfactual data by changing the simulation to reflect a <em>hypothetical condition</em> and then rerunning it with the same initial conditions as the original data.</p>
</div>
<div class="readable-text" id="p54">
<h3 class="readable-text-h3" id="sigil_toc_id_233"><span class="num-string">10.1.4</span> The causal hierarchy theorem</h3>
</div>
<div class="readable-text" id="p55">
<p>The causal hierarchy offers us a key insight from something called the <em>causal hierarchy theorem</em>. That insight is this: “You cannot answer a level <em>k</em> question without level <em>k</em> assumptions.” For example, if you want a causal effect, you need a DAG or some other level 2 (or level 3) assumptions. If you want to answer a counterfactual question, you need level 3 assumptions. And even the most cutting-edge of deep learning models can’t answer level <em>k</em> questions reliably unless they encode a representation of level <em>k</em> assumptions.</p>
</div>
<div class="readable-text intended-text" id="p56">
<p>More formally, the causal hierarchy theorem establishes that the three layers of the causal hierarchy are, in mathematical jargon, “almost always separate.” Roughly speaking, “separate” means that data from a lower level of the hierarchy is insufficient to infer a query from a higher level of the hierarchy. And “almost always” means this statement is true except in cases so rare that we can dismiss them as practically unimportant.</p>
</div>
<div class="readable-text intended-text" id="p57">
<p>Aside from this insight, the causal hierarchy makes understanding identification—perhaps the hardest topic in all causal inference—much easier, as we’ll see in the rest of the chapter. </p>
</div>
<div class="readable-text" id="p58">
<h2 class="readable-text-h2" id="sigil_toc_id_234"><span class="num-string">10.2</span> Identification and the causal inference workflow</h2>
</div>
<div class="readable-text" id="p59">
<p>In this section, we’ll look at the workflow for posing and answering causal questions and the role that identification plays in that workflow. We’ll use the online gaming DAG introduced in chapter 7 as an example. Let’s start by building the DAG with y0.</p>
</div>
<div class="browsable-container listing-container" id="p60">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.2</span> Building the online gaming DAG in y0</h5>
<div class="code-area-container">
<pre class="code-area">import requests    <span class="aframe-location"/> #1
def download_code(url):   #1
    response = requests.get(url)   #1
    if response.status_code == 200:    #1
        code_content = response.text   #1
        print("Code fetched successfully.")  #1
        return code_content    #1
    else:   #1
        print("Failed to fetch code.")    #1
        return None   #1
url = (   #1
    "https://raw.githubusercontent.com/altdeep/"   #1
    "causalML/master/book/chapter%2010/id_utilities.py"   #1
)   #1
utilities_code = download_code(url)   #1
print(utilities_code)   <span class="aframe-location"/> #2
# After checking, uncomment the exec call to load utilities
#exec(utilities_code)    #2

from y0.graph import NxMixedGraph as Y0Graph   <span class="aframe-location"/> #3
from y0.dsl import P, Variable   <span class="aframe-location"/> #4
G = Variable("G")   #4
E = Variable("E")     #4
I = Variable("I")    #4
dag = Y0Graph.from_edges(    #4
    directed=[     #4
        (G, E),   #4
        (G, I),     #4
        (E, I)     #4
    ]     #4
)     #4
gv_draw(dag)    <span class="aframe-location"/> #5</pre>
<div class="code-annotations-overlay-container">
     #1 Install Graphviz for DAG visualization. Download some helper functions for identification and visualization that convert some y0 abstractions into abstractions we’re familiar with.
     <br/>#2 Inspect the downloaded code before executing as a matter of good security practice. Then uncomment the last line and execute.
     <br/>#3 y0 works with a custom graph class called NxMixedGraph. To avoid confusion, we’ll call it a Y0Graph and use it to implement DAGs.
     <br/>#4 Build the graph.
     <br/>#5 Draw the graph with a Graphviz helper function.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p61">
<p>This produces the graph in figure 10.3.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p62">
<img alt="figure" height="232" src="../Images/CH10_F03_Ness.png" width="123"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.3</span> Drawing the online gaming graph with y0</h5>
</div>
<div class="readable-text intended-text" id="p63">
<p>Our goal in chapter 7 was to use our model of <em>P</em>(<em>G</em>, <em>E</em>, <em>I</em><em> </em>) to simulate from <em>P</em>(<em>I</em><sub><em>E</em></sub><sub>=“high”</sub>) using the intervention operator. In sections 7.1.6 and 7.2.6, we did this simulation and saw empirical evidence that it works for this online game example. Identification means showing that it works in general, based on your model and assumptions. Formally, we want to be sure that level 1 distribution <em>P</em>(<em>G</em>, <em>E</em>, <em>I</em>), or data from that distribution, combined with our DAG,</p>
</div>
<div class="readable-text" id="p64">
<p>is enough to simulate from level 2 distribution <em>P</em>(<em>I</em><sub><em>E</em></sub><sub>=“high”</sub>). Identification with y0 confirms that this is indeed possible.</p>
</div>
<div class="browsable-container listing-container" id="p65">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.3</span> Checking identification of <em>P</em>(<em>I</em><em><sub>E</sub></em><sub>=“high”</sub>)<sub> </sub>from <em>P</em>(<em>G</em>, <em>E</em>,<em> </em><em>I</em>)</h5>
<div class="code-area-container">
<pre class="code-area">e = E   <span class="aframe-location"/> #1
check_identifiable(    <span class="aframe-location"/> #2
    dag,     #2
    query=P(I @ e),     #2
    distribution=P(G, E, I)    #2
)     #2</pre>
<div class="code-annotations-overlay-container">
     #1 Make a lowercase “e” to represent an intervention value.
     <br/>#2 Check identifiability given the DAG, a distribution, and a target query. Y0 represents ideal interventions with @, so we write P(I
     <sub>E=e</sub> as P(I @ e).
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p66">
<p>This will return <code>True</code>, but what if we didn’t have any observations of guild membership <em>G</em><em> </em>? We can use y0 to test if we have identification for <em>P</em>(<em>I</em><sub><em>E</em></sub><sub>=“high”</sub>) from <em>P</em>(<em>E</em>, <em>I</em><em> </em>). In other words, test if it is possible to infer <em>P</em>(<em>I</em><sub><em>E</em></sub><sub>=“high”</sub>) from observations of <em>E</em> and <em>I</em> only.</p>
</div>
<div class="browsable-container listing-container" id="p67">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.4</span> Checking identification of <em>P</em>(<em>I</em><em><sub>E</sub></em><sub>=“high”</sub>)<sub> </sub>from <em>P</em>(<em>E</em>,<em> </em><em>I</em>)</h5>
<div class="code-area-container">
<pre class="code-area">check_identifiable(
    dag,
    query=P(I @ e),
    distribution=P(E, I)
)</pre>
</div>
</div>
<div class="readable-text" id="p68">
<p>This will return <code>False</code>, because we don’t have identification for <em>P</em>(<em>I</em><sub><em>E</em></sub><em> </em><sub>=</sub><em> </em><sub><em>e</em></sub><em> </em>) from the DAG and <em>P</em>(<em>E</em>, <em>I</em>) given our graphical assumptions.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p69">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Lack of identification and misguided probabilistic ML</h5>
</div>
<div class="readable-text" id="p70">
<p>Y0 shows us that <em>P</em>(<em>I</em><em><sub>E</sub></em><sub>=</sub><em><sub>e</sub></em>) is not identified from <em>P</em>(<em>E</em>, <em>I</em>) given our online game DAG. Consider the implications of this result from the perspective of probabilistic machine learning (ML). As experts in probabilistic ML, given <em>G</em> is unmeasured, we might be inclined to train a latent variable model on <em>P</em>(<em>E</em>, <em>I</em>) where <em>G</em> is the latent variable. Once we’ve learned that model, we could implement the intervention with graph surgery setting <em>E</em>=<em>e</em>, and then sampling <em>I</em> from the transformed model.</p>
</div>
<div class="readable-text" id="p71">
<p>This algorithm would <em>run</em>; it would generate samples. But the lack of identification result from y0 proves that, given only the assumptions in our DAG, we could not consider these to be valid samples from <em>P</em>(<em>I</em><em><sub>E</sub></em><sub>=</sub><em><sub>e</sub></em>). And training on more data wouldn’t help. The only way this could work is if there were additional causal assumptions constraining inference beyond the assumptions encoded by the DAG.</p>
</div>
</div>
<div class="readable-text" id="p72">
<p>Given this introduction, let’s define identification.</p>
</div>
<div class="readable-text" id="p73">
<h3 class="readable-text-h3" id="sigil_toc_id_235"><span class="num-string">10.2.1</span> Defining identification</h3>
</div>
<div class="readable-text" id="p74">
<p>Suppose I were to randomly choose a pair of numbers, <em>X</em> and <em>Y</em>, and add them together to get <em>Z</em>. Then, I tell you what <em>Z</em> was and ask you to infer the values of <em>X</em> and <em>Y</em>. Could you do it? Not without more information. So, what if I gave you millions of examples of feature <em>Z</em> and label {<em>X</em>, <em>Y</em><em>  </em>}. Could you train a deep learning model to predict label {<em>X</em>, <em>Y</em><em>  </em>} from input feature <em>Z</em><em>  </em>? Again, no, at least not without strong assumptions on the possible values of {<em>X</em>, <em>Y</em><em>  </em>}. What if, instead of millions, I gave you billions of examples? No; more data would not help. In statistics, we would say the prediction target {<em>X</em>, <em>Y</em><em>  </em>} is not <em>identified</em>.</p>
</div>
<div class="readable-text intended-text" id="p75">
<p>In other words, you want to infer something, and you have an algorithm (e.g., a deep net) that takes in data and produces an answer. That answer will usually be a bit different than the true value because of statistical variation in the input data. If your inference objective is identified, then the more data you input to the algorithm, the more that variance will shrink and your algorithm’s answer will converge to the true answer. If your inference objective is not identified, then more data will not reduce your algorithm’s errors.</p>
</div>
<div class="readable-text intended-text" id="p76">
<p><em>Causal identification</em> is just statistical identification across levels of the causal hierarchy. A causal query is identified when your causal assumptions enable you to infer that query using data from a lower level on the hierarchy.</p>
</div>
<div class="readable-text" id="p77">
<h3 class="readable-text-h3" id="sigil_toc_id_236"><span class="num-string">10.2.2</span> The causal inference workflow</h3>
</div>
<div class="readable-text" id="p78">
<p>Now that we have defined identification, we can define a full workflow for causal inference. Figure 10.4 shows the full workflow.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p79">
<img alt="figure" height="158" src="../Images/CH10_F04_Ness.png" width="740"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.4</span> The causal inference workflow. The identification step is an essential step in the workflow.</h5>
</div>
<div class="readable-text" id="p80">
<p>Identification is a key step in the workflow. Let’s walk through each of the steps.</p>
</div>
<div class="readable-text" id="p81">
<h4 class="readable-text-h4 sigil_not_in_toc">Step 1: Pose your query</h4>
</div>
<div class="readable-text" id="p82">
<p>First, we pose our causal question as a query. For example, given our question “What would in-game purchases be for a player if side-quest engagement was high?” our query is <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=“high”</sub>).<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p83">
<img alt="figure" height="195" src="../Images/CH10_F05_Ness.png" width="361"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.5</span> Step 2: Build the model to capture your causal assumptions relative to your query. For the query <em>P</em>(<em>I</em><em><sub>E</sub></em><sub>=“high”</sub>), this is our online gaming DAG.</h5>
</div>
<div class="readable-text" id="p84">
<h4 class="readable-text-h4 sigil_not_in_toc">Step 2: Build your model</h4>
</div>
<div class="readable-text" id="p85">
<p>Next, build a causal model that captures your basic causal assumptions. Our model will be the online game causal DAG, shown again in figure 10.5.</p>
</div>
<div class="readable-text intended-text" id="p86">
<p>Your model’s assumptions should at least match the level of your query in the causal hierarchy. For example, the query <em>P</em>(<em>I</em><sub><em>E</em></sub><sub>=“high”</sub>) is a level 2 query, so we need at least some level 2 assumptions. The causal DAG is a level 2 causal model, so in our analysis, the DAG provides the necessary level 2 assumptions.</p>
</div>
<div class="readable-text" id="p87">
<h4 class="readable-text-h4 sigil_not_in_toc">Step 3: Check identification</h4>
</div>
<div class="readable-text" id="p88">
<p>Evaluate whether you have identification for your query, given your model assumptions and your available data. If you don’t have identification, you must either observe additional variables in your data or change your assumptions. For example, we could modify our online gaming DAG (changing level 2 assumptions). Or simply stop and conclude you can’t answer the question given your data and knowledge about the problem, and devote your attention elsewhere.</p>
</div>
<div class="readable-text" id="p89">
<h4 class="readable-text-h4 sigil_not_in_toc">Step 4: Estimate your query</h4>
</div>
<div class="readable-text" id="p90">
<p>Once you know you have identification for your query, you can run statistical inference on, or “estimate,” your query. There are a variety of estimation methods and algorithms, from Bayesian inference to linear regression to propensity scores to double machine learning. We’ll review some estimation methods in the next chapter.</p>
</div>
<div class="readable-text" id="p91">
<h4 class="readable-text-h4 sigil_not_in_toc">Step 5: Refute your causal inference</h4>
</div>
<div class="readable-text" id="p92">
<p>Refutation is a final step where we conduct sensitivity analysis to evaluate how sensitive our results from step 4 are to violations of our assumptions, including the assumptions that enabled identification. We’ll see examples of this in chapter 11. </p>
</div>
<div class="readable-text" id="p93">
<h3 class="readable-text-h3" id="sigil_toc_id_237"><span class="num-string">10.2.3</span> Separating identification and estimation</h3>
</div>
<div class="readable-text" id="p94">
<p>In many texts, identification and estimation are combined in one step by matching the estimators and practical scenarios where those estimators will work. In this book, we’ll highlight the separation of identification and estimation for several reasons:</p>
</div>
<ul>
<li class="readable-text" id="p95"> The separation lets us shunt all the causal considerations into the identification step. This helps us be explicit about what causal assumptions we are relying on for estimation to work and builds intuition for when our analysis might fail. </li>
<li class="readable-text" id="p96"> The estimation step thus simplifies to purely statistical questions, where we consider the usual statistical trade-offs (bias vs. variance, uncertainty quantification, how well it scales, etc.). </li>
<li class="readable-text" id="p97"> The separation also allows us to handle estimation with the automatic differentiation capabilities that power cutting-edge deep learning libraries without worrying whether these learning procedures will get the causality wrong. </li>
</ul>
<div class="readable-text" id="p98">
<p>Next, we’ll dive into the most common identification strategy: backdoor adjustment.</p>
</div>
<div class="readable-text" id="p99">
<h2 class="readable-text-h2" id="sigil_toc_id_238"><span class="num-string">10.3</span> Identification with backdoor adjustment</h2>
</div>
<div class="readable-text" id="p100">
<p>Suppose we want to determine the causal effect of engagement on in-game purchases, i.e., <em>E</em><em>  </em>(<em>I</em><sub><em>E</em></sub><em> </em><sub>=</sub><em> </em><sub>“high”</sub> – <em>I</em><sub><em>E</em></sub><em> </em><sub>=</sub><em> </em><sub>“low”</sub>). We can derive this expectation from the query <em>E</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub><em> </em>=<em> </em><em>i</em><em> </em>), so we focus on <em>P</em><em> </em>(<em>I</em><em> </em><sub><em>E</em></sub><em> </em><sub>=</sub><em> </em><sub><em>e</em></sub><em> </em>=<em> </em><em>i</em><em>  </em>). We can use the online gaming DAG to prove the following is true:</p>
</div>
<div class="readable-text" id="p101">
<p><span class="aframe-location"/>We’ll see how to derive this equation in the next section. The right side of this equation is a level 1 quantity called an <em>estimand </em>that we can derive from the joint distribution <em>P</em><em> </em>(<em>I</em>, <em>E</em>, <em>G</em><em> </em>).</p>
</div>
<div class="browsable-container figure-container" id="p102">
<img alt="figure" height="56" src="../Images/ness-ch10-eqs-0x.png" width="604"/>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p103">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Queries, estimands, and estimators</h5>
</div>
<div class="readable-text" id="p104">
<p>In statistics, the <em>estimand</em> is the thing the statistical algorithm (the <em>estimator</em>) estimates. The task of identification is finding (identifying) an estimand for your query. In terms of the causal hierarchy, causal identification is about finding a lower-level estimand for a higher-level query. </p>
</div>
<div class="readable-text" id="p105">
<p>In the online gaming backdoor identification example, <em>P</em>(<em>I</em><sub>E=“high”</sub>=i) is a level 2 query, and <span class="regular-symbol">∑</span><em><sub>g</sub></em><em>P</em>(<em>I</em>=<em>i</em>|<em>E</em>=“high”, <em>G</em>=<em>g</em>)<em>P</em>(<em>G</em>=<em>g</em>) is the level 1 estimand called the <em>backdoor adjustment estimand</em>. Backdoor adjustment is an operation we apply to <em>P</em>(<em>E</em>, <em>I</em>, <em>G</em>), where we sum out (or integrate out in the continuous case) the common cause <em>G</em>. In some cases, we’ll see we don’t need to know the estimand explicitly, only that it exists. </p>
</div>
</div>
<div class="readable-text" id="p106">
<p>We passed our DAG and the intervention-level query <em>P</em><em> </em>(<em>I</em><em> </em><sub><em>E</em></sub><em> </em><sub>=</sub><em> </em><sub>“high”</sub>) to y0, and it told us it identified an estimand, an operation applied to <em>P</em><em> </em>(<em>E</em>, <em>I</em>, <em>G</em><em>  </em>) that is equivalent to <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><em> </em><sub>=</sub><em> </em><sub>“high”</sub>). Let’s have y0 display that estimand.</p>
</div>
<div class="browsable-container listing-container" id="p107">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.5</span> Deriving the estimand to get <em>P</em>(<em>I</em><em><sub>E</sub></em><sub>=“high”</sub>) from <em>P</em>(<em>E</em>, <em>I</em>, <em>G</em>) </h5>
<div class="code-area-container">
<pre class="code-area">from y0.graph import NxMixedGraph as Y0Graph
from y0.dsl import P, Variable
from y0.algorithm.identify import Identification, identify

query = P(I @ e)
base_distribution = P(I, E, G)

identification_task = Identification.from_expression(
    graph=dag,
    query=query,
    estimand=base_distribution)

identify(identification_task)</pre>
</div>
</div>
<div class="readable-text" id="p108">
<p>This returns the expression in figure 10.6.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p109">
<img alt="figure" height="25" src="../Images/ness-ch10-eqs-1x.png" width="333"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.6</span> Output of y0’s identify function</h5>
</div>
<div class="readable-text intended-text" id="p110">
<p>In our notation, this is <span class="regular-symbol">∑</span><em> </em><sub><em>g</em></sub><em> </em><em>P</em><em> </em>(<em>I</em><em> </em>=<em> </em><em>i</em><em> </em>|<em>E</em><em>  </em>=“high”, <em>G</em><em>  </em>=<em> </em><em>g</em>) <span class="regular-symbol">∑</span><em class="obliqued"><sub>ε</sub></em><sub>,</sub><em class="obliqued"><sub>i</sub></em><em>P</em>(<em>E</em><em>  </em>=<em> </em><em class="obliqued">ε</em>, <em>G</em><em>  </em>=<em> </em><em>g</em>, <em>I</em><em>  </em>=<em> </em><em>i</em>), which simplifies to <span class="regular-symbol">∑</span><em> </em><sub><em>g</em></sub><em> </em><em>P</em><em> </em>(<em>I</em><em> </em>=<em> </em><em>i</em><em> </em>|<em>E</em><em>  </em>=<em> </em>“high”, <em>G</em><em>  </em>=<em> </em><em>g</em><em> </em>) <em>P</em>(<em>G</em><em>  </em>=<em> </em><em>g</em><em> </em>). This is the <em>backdoor adjustment estimand</em>. We’ll see at a high level how y0 derives this estimand. But first, let’s look a bit more closely at this estimand.</p>
</div>
<div class="readable-text" id="p111">
<h3 class="readable-text-h3" id="sigil_toc_id_239"><span class="num-string">10.3.1</span> The backdoor adjustment formula</h3>
</div>
<div class="readable-text" id="p112">
<p>In general terms, suppose <em>X</em> is a cause of <em>Y</em>, and we are interested in the intervention-level query <em>P</em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub>). In that case, the <em>backdoor adjustment estimand </em>is <span class="regular-symbol">∑</span><em><sub>g</sub></em><em>P</em>(<em>X</em><em> </em>=<em> </em><em>x</em>, <em>Z</em><em>  </em>=<em> </em><em>z</em>) <em>P</em><em> </em>(<em>Z</em><em>  </em>=<em> </em><em>z</em><em> </em>). The <em>backdoor adjustment formula </em>equates the causal query <em>P</em>(<em>X</em><em><sub>X</sub></em><sub>=</sub><em><sub>x</sub></em>) with its estimand:<em> </em> <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p113">
<img alt="figure" height="52" src="../Images/ness-ch10-eqs-12x.png" width="534"/>
</div>
<div class="readable-text" id="p114">
<p>Here, <em>Z</em> is a set of variables called the adjustment set. The summation is shorthand for summation and integration—you sum over discrete variables in the adjustment set and integrate over continuous variables. The adjustment set is defined as fa set of variables that satisfies the <em>backdoor criterion</em>—(1) the set collectively <em>d</em>-separates all <em>backdoor paths</em> from <em>X</em> to <em>Y</em>, and (2) it contains no descendants of <em>X</em>.</p>
</div>
<div class="readable-text intended-text" id="p115">
<p> To understand why we want to d-separate backdoor paths between <em>X</em> and <em>Y</em>, consider again our DAG for our online gaming example in figure 10.7.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p116">
<img alt="figure" height="195" src="../Images/CH10_F07_Ness.png" width="361"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.7</span> The online gaming DAG</h5>
</div>
<div class="readable-text" id="p117">
<p> What is the difference between <em>P</em>(<em>I</em><em> </em>|<em>E</em><em>  </em>=“high”) and <em>P</em>(<em>I</em><sub><em>E</em></sub><sub>=“high”</sub>)? Consider the two paths between <em>E</em> and <em>I</em> in figure 10.8. In the case of <em>P</em>(<em>I</em><em> </em>|<em>E</em><em>  </em>=“high”), observing <em>E</em>=“high” gives us information about <em>I</em> by way of its direct causal impact on <em>I</em>, i.e., through path <em>E</em><em>  </em>→<em>I</em>. But observing <em>E</em><em>   </em>=<em> </em>“high” also gives us information about <em>G</em>, and subsequently about <em>I</em> through the <em>backdoor path</em> <em>E</em><em> </em>←<em>G</em><em> </em>→<em>I</em>. A <em>backdoor path</em> between two variables is a <em>d</em>-connected path between a common cause. In the case of <em>P</em>(<em>I</em><em><sub>E</sub></em><sub>=“high”</sub>), we only want the impact on<em> I</em> through the direct path <em>E</em>→<em>I</em>.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p118">
<img alt="figure" height="191" src="../Images/CH10_F08_Ness.png" width="497"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.8</span> <em>E</em>←<em>G</em>→<em>I</em> is a backdoor path where <em>G</em> is a “confounder” that is a common cause of <em>E</em> and <em>I</em>. We are interested in the statistical signal flowing along the causal path from <em>E</em> to <em>I</em>, but that signal is “confounded” by the noncausal noise from additional statistical information through <em>G</em> on the backdoor path <em>E</em>←<em>G</em>→<em>I</em>.</h5>
</div>
<div class="readable-text" id="p119">
<p>We call <em>G</em> a <em>confounder</em>, because the statistical “signal” flowing along the causal path from <em>E</em> to <em>I</em> is “confounded” by the noncausal “noise” from additional statistical information through <em>G</em> on the <em>backdoor path</em> <em>E</em><em> </em>←<em>G</em><em> </em>→<em>I</em>. To address this problem, we seek to d-separate this backdoor path by blocking on <em>G</em>.</p>
</div>
<div class="readable-text intended-text" id="p120">
<p>We want to identify a backdoor estimand for the query <em>P</em>(<em>I</em><sub><em>E</em></sub><sub>=“high”</sub>). So we substitute <em>I</em> for <em>Y</em>, and <em>E</em><em> </em> for <em>X</em> in the backdoor adjustment formula. <em>G</em> blocks the backdoor path <em>E</em> <em>G</em> <em>I</em>, so the set <em>G</em> becomes our adjustment set:</p>
</div>
<div class="readable-text intended-text" id="p121">
<p><em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub><em> </em>=<em> </em><em>i</em>) = <span class="regular-symbol">∑</span><sub><em>g</em></sub><em> </em><em>P</em><em> </em>(<em>I</em><em>  </em>=<em> </em><em>i</em>|<em>E</em><em>  </em>=<em> </em><em>e</em>, <em>G</em><em>  </em>=<em> </em><em>g</em><em> </em>)<em>P</em>(<em>G</em><em>  </em>=<em> </em><em>g</em><em> </em>)</p>
</div>
<div class="readable-text" id="p122">
<p>The backdoor adjustment formula d-separates the backdoor paths by summing out/integrating over, or in other words, “adjusting for” the backdoor statistical signal, leaving only the signal derived from the direct causal relationship.</p>
</div>
<div class="readable-text print-book-callout" id="p123">
<p><span class="print-book-callout-head">NOTE</span>  Some texts refer to the G-formula instead of backdoor adjustment formula. The backdoor adjustment formula is just the G-formula where the adjustment set is defined in terms of the backdoor criterion.</p>
</div>
<div class="readable-text print-book-callout" id="p124">
<p>While an adjustment set can include non-confounders, in practice, excluding all but a minimal set of backdoor-blocking confounders cuts down on complexity and statistical variation. We dive into the statistical considerations of backdoor adjustment in chapter 11.</p>
</div>
<div class="readable-text" id="p125">
<h3 class="readable-text-h3" id="sigil_toc_id_240"><span class="num-string">10.3.2</span> Demystifying the back door</h3>
</div>
<div class="readable-text" id="p126">
<p>So where does the backdoor adjustment estimand come from? Let’s consider our online gaming example again. The query is <em>P</em>(<em>I</em><em> </em><sub><em>E</em></sub><sub>=</sub><em> </em><sub><em>e</em></sub>) where <em>e</em> is “high” or “low.” In counterfactual terms, let’s consider two possible worlds, one with our original DAG, and one where we apply the intervention to side-quest engagement (<em>E</em>). Let’s view the parallel world graph in figure 10.9.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p127">
<img alt="figure" height="404" src="../Images/CH10_F09_Ness.png" width="301"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.9</span> We have two parallel worlds: world A where <em>E</em> is not intervened upon, and world B where <em>E</em> is intervened upon.</h5>
</div>
<div class="readable-text intended-text" id="p128">
<p>If you squint hard enough at this graph, you’ll notice that it implies that <em>E</em> is conditionally independent from <em>I</em><em> </em><sub><em>E</em></sub><sub>=</sub><em> </em><sub><em>e</em></sub> given <em>G</em>. We’ll use some d-separation–based reasoning to see this. Remember that, in general, we can’t use d-separation to reason across worlds on a parallel world graph because the d-separation rules don’t account for nodes that are equivalent across worlds (like <em>G</em>). But we’ll use a trick where we reason about conditional independence between <em>E</em> and <em>I</em><em> </em><sub><em>E</em></sub><sub>=</sub><em> </em><sub><em>e</em></sub> by looking at a d-connected path from <em>E</em> to <em>G</em> in world A, and then extend that d-connected path <em>from </em>the equivalent <em>G</em> in world B to <em>I</em><em> </em><sub><em>E</em></sub><sub>=</sub><em> </em><sub><em>e</em></sub>.</p>
</div>
<div class="readable-text intended-text" id="p129">
<p>First, consider that paths from <em>E</em> in world A to world B have to cross one of two bridges between worlds, <em>N</em><sub><em>G</em></sub> and <em>N</em><sub><em>I</em></sub>. But the two paths to <em>N</em><sub><em>I</em></sub> (<em>E</em> → <em>I</em> ← <em>N</em><sub><em>I</em></sub>, <em>E</em> ← <em>G </em>→ <em>I</em> ← <em>N</em><sub><em>I</em></sub>) are both d-separated due to the collider on <em>I</em>.</p>
</div>
<div class="readable-text intended-text" id="p130">
<p>So we have one d-connected path to world B (<em>E</em> ← <em>G</em> ← <em>N</em><sub><em>G</em></sub>). Now suppose we look at <em>G</em> in world B; from world B’s <em>G</em>, it is one step to <em>I</em><em> </em><sub><em>E</em></sub><sub>=</sub><em> </em><sub><em>e</em></sub>. But we know that, by the law of consistency, the value of <em>G</em> in both worlds must be the same; both <em>G</em>s are the same deterministic function of <em>N</em><sub><em>G</em></sub>, and neither <em>G</em> is affected by an intervention. So, for convenience, we’ll collapse the two <em>G</em>s into one node in the parallel world graph (figure 10.10). Looking now at the path <em>E</em> ← <em>G</em> → <em>I</em><em> </em><sub><em>E</em></sub><sub>=</sub><em> </em><sub><em>e</em></sub>, we can see this path is d-separated by <em>G</em>. Hence, we can conclude <em>E</em> ⊥ <em>I</em><em> </em><sub><em>E</em></sub><sub>=</sub><em> </em><sub><em>e</em></sub> | <em>G</em>.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p131">
<img alt="figure" height="444" src="../Images/CH10_F10_Ness.png" width="310"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.10</span> Collapsing <em>G</em> across worlds reveals <em>G</em> d-separates <em>E</em> and <em>I</em><em><sub>E</sub></em><sub>=</sub><em><sub>e</sub></em>.</h5>
</div>
<div class="readable-text intended-text" id="p132">
<p><strong> </strong>In causal inference jargon, this simplification is called <em>ignorability</em>. <em>Ignorability </em>means the causal variable <em>E</em> and the counterfactual potential outcomes like <em>I</em><em> </em><sub><em>E</em></sub><sub>=</sub><em> </em><sub><em>e</em></sub> are conditionally independent given confounders. Ignorability is a common assumption made in causal inference. We can use this ignorability assumption in deriving the backdoor estimand.</p>
</div>
<div class="readable-text intended-text" id="p133">
<p> Before we start, let’s recall a key definitional fact of conditional independence: if two variables <em>U</em> and <em>V</em> are conditionally independent given <em>Z</em>, then <em>P</em><em> </em>(<em>U</em><em>  </em>|<em>Z</em><em>  </em>=<em> </em><em>z</em>, <em>V</em><em>  </em>=<em> </em><em>v</em><em> </em>) = <em>P</em><em> </em>(<em>U</em><em>  </em>|<em>Z</em><em>  </em>=<em> </em><em>z</em>). Flipping that around, <em>P</em><em> </em>(<em>U</em><em>  </em>|<em>Z</em><em>  </em>=<em> </em><em>z</em><em> </em>) = <em>P</em><em> </em>(<em>U</em><em>  </em>|<em>Z</em><em>  </em>=<em> </em><em>z</em>, <em>V</em><em>  </em>=<em> </em><em>v</em><em> </em>). In other words, <em>P</em><em> </em>(<em>U</em><em>  </em>|<em>Z</em><em>   </em>=<em> </em><em>z</em>) = <em>P</em><em> </em>(<em>U</em><em>  </em>|<em>Z</em><em>   </em>=<em>  </em><em>z</em>, <em>V</em><em>   </em>=<em> </em>“apples”) = <em>P</em><em> </em>(<em>U</em><em>  </em>|<em>Z</em><em>   </em>=<em> </em><em>z</em>, <em>V</em><em>  </em>=<em>  </em>“oranges”); it doesn’t matter what value <em>V</em> takes because, since <em>Z</em> rendered it independent from <em>U</em>, its value has no bearing on <em>U</em>. Introducing <em>V</em> and giving it whatever value we want is the trick that makes the derivation work. Also, recall the <em>law of total probability</em> says that we can marginalize a variable out of a joint distribution by summing (or integrating) over that variable, as in <em>P</em><em> </em>(<em>U</em><em>  </em>=<em> </em><em>u</em>) = <span class="regular-symbol">∑</span><sub><em>v</em></sub><em>P</em><em> </em>(<em>U</em><em>  </em>=<em> </em><em>u</em>, <em>V</em><em>  </em>=<em> </em><em>v</em><em> </em>). The same is true when the joint distribution is subject to intervention, as in <em>P</em><em> </em>(<em>U</em><em><sub>W</sub></em><sub>=</sub><em><sub>w</sub></em><em> </em>=<em> </em><em>u</em>) = <span class="regular-symbol">∑</span><sub><em>v</em></sub><em>P</em><em> </em>(<em>U</em><em><sub>W</sub></em><sub>=</sub><em><sub>w</sub></em><em> </em>=<em> </em><em>u</em>, <em>V</em><em><sub>W</sub></em><sub>=</sub><em><sub>w</sub></em><em> </em>=<em> </em><em>v</em><em> </em>).</p>
</div>
<div class="readable-text intended-text" id="p134">
<p>Now let’s start with the causal query <em>P</em><em> </em>(<em>I</em><em> </em><sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub><em> </em>) and see how to equate it with the backdoor estimand <span class="regular-symbol">∑</span><sub><em>g</em></sub> <em>P</em><em> </em>(<em>I</em><em>  </em>|<em>E</em><em>  </em>=<em> </em><em>e</em>, <em>G</em><em>  </em>=<em> </em><em>g</em>)<em>P</em><em> </em>(<em>G</em><em>  </em>=<em> </em><em>g</em><em> </em>). </p>
</div>
<ol>
<li class="readable-text" id="p135"> For some value of in-game purchases <em>i</em>, <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub><em> </em>=<em> </em><em>i</em><em> </em>) = <span class="regular-symbol">∑</span><sub><em>g</em></sub> <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub><em> </em>=<em> </em><em>i</em>, <em>G</em><sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub><em> </em>=<em> </em><em>g</em><em> </em>) by the law of total probability. </li>
<li class="readable-text" id="p136"> <span class="regular-symbol">∑</span><sub><em>g</em></sub> <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub>=<em>i</em>, <em>G</em><sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub>=<em>g</em><em> </em>) = <span class="regular-symbol">∑</span><sub><em>g</em></sub> <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub>=<em>i</em>, <em>G</em><em> </em>=<em>g</em><em> </em>), because we know from our original DAG that <em>G</em> is not affected by the intervention on <em>E</em>. </li>
<li class="readable-text" id="p137"> Next we use the chain rule to factorize <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub><em> </em>=<em> </em><em>i</em>, <em>G</em><em>  </em>=<em> </em><em>g</em><em> </em>): <span class="regular-symbol">∑</span><sub><em>g</em></sub> <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub><em> </em>=<em> </em><em>i</em>, <em>G</em><em>  </em>=<em> </em><em>g</em><em> </em>) = <span class="regular-symbol">∑</span><sub><em>g</em></sub> <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub><em> </em>=<em> </em><em>i</em><em> </em>| <em>G</em><em>  </em>=<em> </em><em>g</em>)<em>P</em><em> </em>(<em>G</em><em>  </em>=<em> </em><em>g</em><em> </em>). </li>
<li class="readable-text" id="p138"> Now we come to the trick—<em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub><em> </em>=<em> </em><em>i</em><em>  </em>|<em>G</em><em>  </em>=<em> </em><em>g</em><em> </em>) = <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub><em> </em>=<em> </em><em>i</em><em>  </em>|<em>E</em><em>  </em>=<em> </em><em>e</em>, <em>G</em><em>  </em>=<em> </em><em>g</em><em> </em>) for any value of <em>e</em>, because once we condition on <em>G</em>=<em>g</em>, <em>E</em>=<em>e </em>and <em>I</em><sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub> are independent. So in our derivation, we can replace <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub><em> </em>=<em> </em><em>i</em><em>  </em>|<em>G</em><em>  </em>=<em> </em><em>g</em><em> </em>) with <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub><em> </em>=<em> </em><em>i</em><em>  </em>|<em>E</em><em>  </em>=<em> </em><em>e</em>, <em>G</em><em>  </em>=<em> </em><em>g</em><em> </em>). </li>
<li class="readable-text" id="p139"> Once we condition that <em>E</em><em>  </em>=<em> </em><em>e</em>, we can use the law of consistency to drop the subscript: <span class="regular-symbol">∑</span><sub><em>g</em></sub> <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub><em> </em>=<em> </em><em>i</em><em>  </em>|<em>E</em><em>  </em>=<em> </em><em>e</em>, <em>G</em><em>  </em>=<em> </em><em>g</em><em>  </em>)<em>P</em><em> </em>(<em>G</em><em>  </em>=<em> </em><em>g</em><em>  </em>) = <span class="regular-symbol">∑</span><sub><em>g</em></sub> <em>P</em><em> </em>(<em>I</em><em>  </em>=<em> </em><em>i</em><em>  </em>|<em>E</em><em>  </em>=<em> </em><em>e</em>, <em>G</em><em>  </em>=<em> </em><em>g</em><em> </em>)<em>P</em><em> </em>(<em>G</em><em>  </em>=<em> </em><em>g</em><em>  </em>). </li>
</ol>
<div class="readable-text" id="p140">
<p>Let’s explain steps 4 and 5. Our ignorability result shows that <em>I</em><sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub> and <em>E</em> are conditionally independent given <em>G</em>. So in step 4 we apply the independence trick that lets us introduce <em>E</em>. Further, we set the value of <em>E </em>to be <em>e</em> so it matches the subscript <sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub>. This allows us to apply the law of consistency from chapter 8 and drop the subscript <sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub>.</p>
</div>
<div class="readable-text intended-text" id="p141">
<p>Voila, we’ve identified a backdoor estimand, an estimand from level 1 of the causal hierarchy, for a level 2 causal query <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub>) using level 2 assumptions encoded in a DAG. Causal identification is just coming up with derivations like this. Much, if not most, of traditional causal inference research boils down to doing this kind of math, or writing algorithms that do it for you.</p>
</div>
<div class="readable-text intended-text" id="p142">
<p>Next, we’ll look at the do-calculus, which provides simple graph-based rules for identification that we can use in identification algorithms.</p>
</div>
<div class="readable-text" id="p143">
<h2 class="readable-text-h2" id="sigil_toc_id_241"><span class="num-string">10.4</span> Graphical identification with the do-calculus</h2>
</div>
<div class="readable-text" id="p144">
<p><em>Graphical identification</em> (sometimes called <em>nonparametric identification</em>) refers to identification techniques that rely on reasoning over the DAG. One of the most well-known approaches to graphical identification is the <em>do-calculus</em>, a set of three rules used for identification with causal graphs. The rules use graph surgery and d-separation to determine cases when you can replace causal terms like <em>I</em><sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub> with non-causal terms like <em>I</em><em>  </em>|<em>E</em>=<em>e</em>. Starting with a query on a higher level of the causal hierarchy, we can apply these rules in sequence to derive a lower-level estimand.</p>
</div>
<div class="readable-text" id="p145">
<h3 class="readable-text-h3" id="sigil_toc_id_242"><span class="num-string">10.4.1</span> Demystifying the do-calculus</h3>
</div>
<div class="readable-text" id="p146">
<p>Recall high school geometry, where you saw if-then statements like this:</p>
</div>
<div class="readable-text" id="p147">
<blockquote>
<div>
<em>If </em>the shape is a square, 
     <em>then </em>all the sides are equal.
    </div>
</blockquote>
</div>
<div class="readable-text" id="p148">
<p>When you were trying to solve a geometry problem, you used facts like this in the steps of your solution.</p>
</div>
<div class="readable-text intended-text" id="p149">
<p>Similarly, the do-calculus consists of three rules (if-then statements) of the following form:</p>
</div>
<div class="readable-text" id="p150">
<blockquote>
<div>
<em>If</em> certain variables are d-separated after applying graph surgery to the DAG, 
     <em>then</em> probability query 
     <em>A</em> equals probability query 
     <em>B</em>.
    </div>
</blockquote>
</div>
<div class="readable-text" id="p151">
<h4 class="readable-text-h4 sigil_not_in_toc">The rules of the do-calculus are not intuitive</h4>
</div>
<div class="readable-text" id="p152">
<p>The three rules of the do-calculus are not intuitive upon reading them, just as geometric rules like cos<sup>2</sup><em>x</em> + sin<sup>2</sup><em>x</em> = 1 were not intuitive when you first saw them in high school. But like those geometric rules, we derive the rules of the do-calculus from simpler familiar concepts, namely d-separation, ideal interventions, and the rules of probability. And like the rules of geometry, we can use the rules of the do-calculus to prove that a causal query from one level of the hierarchy is equivalent to one from another level.</p>
</div>
<div class="readable-text intended-text" id="p153">
<p>Practically speaking, we can rely either on software libraries that implement the do-calculus in graphical identification algorithms (like y0) or simply hard-code well-known identification results like the backdoor adjustment estimand. To take away some of the mystery, I’ll introduce the rules and show how they can derive the backdoor estimand. The goal here is not to memorize these rules, but rather to see how they work in a derivation of the backdoor estimand that contrasts with the derivation in the previous section. </p>
</div>
<div class="readable-text intended-text" id="p154">
<p>In defining these rules, we’ll focus on the target distribution <em>Y</em> under an intervention on <em>X</em>. We want to generalize to all DAGs, so we’ll name two other nodes, <em>Z</em> and <em>W</em>. <em>Z</em> and <em>W</em> will allow us to cover cases where we have another potential intervention target <em>Z</em> and any node <em>W</em> we’d like to condition upon. Further, while I’ll often refer to individual variables, keep in mind that the rules apply when <em>X</em>, <em>Y</em>, <em>Z</em>, and <em>W</em> are sets of variables.</p>
</div>
<div class="readable-text" id="p155">
<h4 class="readable-text-h4 sigil_not_in_toc">Rule 1: Insertion or removal of observations</h4>
</div>
<div class="readable-text" id="p156">
<blockquote>
<div>
     If 
     <em>Y</em> and 
     <em>Z</em> are d-separated in your DAG by 
     <em>X</em> and 
     <em>W</em> after the incoming edges to 
     <em>X</em> are removed . . . 
    </div>
</blockquote>
</div>
<div class="readable-text" id="p157">
<blockquote>
<div>
<em>Then</em>
<em>P</em> ( 
     <em>Y</em>
<sub><em>X</em></sub>
<sub>=</sub>
<sub><em>x</em></sub>=  
     <em>y</em> | 
     <em>Z</em>  =  
     <em>z</em>, 
     <em>W</em>  =  
     <em>w</em>) = 
     <em>P</em>( 
     <em>Y</em>
<sub><em>X</em></sub>
<sub>=</sub>
<sub><em>x</em></sub> =  
     <em>y</em> | 
     <em>W</em>  =  
     <em>w</em>).
    </div>
</blockquote>
</div>
<div class="readable-text" id="p158">
<p>This is called “insertion or removal” because we can remove <em>Z</em><em> </em>=<em>z</em> from <em>P</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub>=<em>y</em><em> </em>|<em>Z</em><em>  </em>=<em> </em><em>z</em>, <em>W</em><em>   </em>=<em> </em><em>w</em>) to get <em>P</em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub><em> </em>=<em>y </em>| <em>W</em><em>  </em>=<em> </em><em>w</em>) and vice versa.</p>
</div>
<div class="readable-text" id="p159">
<h4 class="readable-text-h4 sigil_not_in_toc">Rule 2: Exchange of an intervention for an observation</h4>
</div>
<div class="readable-text" id="p160">
<blockquote>
<div>
<em>If</em>
<em>Y</em> and 
     <em>Z</em> are d-separated in your DAG by 
     <em>X</em> and 
     <em>W</em> after incoming edges in 
     <em>X</em> and outgoing edges from 
     <em>Z</em> have been removed . . . 
    </div>
</blockquote>
</div>
<div class="readable-text" id="p161">
<blockquote>
<div>
<em>then P</em> ( 
     <em>Y</em>
<sub><em>X</em></sub>
<sub>=</sub>
<sub><em>x</em></sub>
<sub>, </sub>
<sub><em>Z</em></sub>
<sub>=</sub>
<sub><em>z</em></sub>= 
     <em>y</em> | 
     <em>W</em> =  
     <em>w</em>) = 
     <em>P</em>( 
     <em>Y</em>
<sub><em>X</em></sub>
<sub>=</sub>
<sub><em>x</em></sub> =  
     <em>y</em> | 
     <em>Z</em>  =   
     <em>z</em>, 
     <em>W</em>  =   
     <em>w</em>).
    </div>
</blockquote>
</div>
<div class="readable-text" id="p162">
<p>Here we can either <em>exchange </em>the intervention <sub><em>Z</em></sub><sub>=</sub><sub><em>z</em></sub> in <em>P</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub><sub>, </sub><sub><em>Z</em></sub><sub>=</sub><sub><em>z</em></sub><em> </em>=<em> </em><em>y</em> | <em>W</em><em>   </em>=<em> </em><em>w</em>) for conditioning on the observation <em>Z</em><em>  </em>=<em> </em><em>z</em> to get <em>P</em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub>=<em>y</em> | <em>Z</em><em>  </em>=<em> </em><em>z</em>, <em>W</em><em>  </em>=<em> </em><em>w</em>), or vice versa. </p>
</div>
<div class="readable-text" id="p163">
<h4 class="readable-text-h4 sigil_not_in_toc">Rule 3: Insertion or removal of interventions</h4>
</div>
<div class="readable-text" id="p164">
<p>For rule 3, we are going to define <em>Z</em> as a set of nodes, and <em>Z</em>(<em>W</em><em>  </em>) as the subset of <em>Z</em> that are not ancestors of <em>W</em>.</p>
</div>
<div class="readable-text" id="p165">
<blockquote>
<div>
     If 
     <em>Y</em> and 
     <em>Z</em> are d-separated in your DAG by 
     <em>X</em> and 
     <em>W</em> after you remove all incoming edges to 
     <em>X</em> and 
     <em>Z</em>( 
     <em>W</em>) . . .
    </div>
</blockquote>
</div>
<div class="readable-text" id="p166">
<blockquote>
<div>
<em>then</em>
<em>P</em>( 
     <em>Y</em>
<sub><em>X</em></sub>
<sub>=</sub>
<sub><em>x</em></sub>
<sub>, </sub>
<sub><em>Z</em></sub>
<sub>=</sub>
<sub><em>z</em></sub> =  
     <em>y </em>| 
     <em>W</em> =  
     <em>w</em>) = 
     <em>P</em>( 
     <em>Y</em>
<sub><em>X</em></sub>
<sub>=</sub>
<sub><em>x</em></sub> =  
     <em>y </em>| 
     <em>W</em> =  
     <em>w</em>).
    </div>
</blockquote>
</div>
<div class="readable-text" id="p167">
<p>This rule allows you to insert <sub><em>Z</em></sub><sub>=</sub><sub><em>z</em></sub> into <em>P</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub><em> </em>=<em> </em><em>y </em>| <em>W</em><em>  </em>=<em> </em><em>w</em>) to get <em>P</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub><sub>, </sub><sub><em>Z</em></sub><sub>=</sub><sub><em>z</em></sub><em> </em>=<em> </em><em>y </em>| <em>W</em><em>  </em>=<em> </em><em>w</em>) or remove <sub><em>Z</em></sub><sub>=</sub><sub><em>z</em></sub> from <em>P</em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub><sub>, </sub><sub><em>Z</em></sub><sub>=</sub><sub><em>z</em></sub>=<em> </em><em>y </em>| <em>W</em>=<em>w</em>) to get <em>P</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub>=<em>y </em>| <em>W</em><em>  </em>=<em> </em><em>w</em>).</p>
</div>
<div class="readable-text" id="p168">
<h3 class="readable-text-h3" id="sigil_toc_id_243"><span class="num-string">10.4.2</span> Using the do-calculus for backdoor identification </h3>
</div>
<div class="readable-text" id="p169">
<p>Now we’ll use the do-calculus to provide an alternative derivation of the backdoor estimand that differs from our “ignorability”-based definition. Again, I include this derivation to demystify the application of the do-calculus. Don’t worry if you don’t completely follow each step:</p>
</div>
<ol>
<li class="readable-text" id="p170"> <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub><em> </em>=<em> </em><em>i</em><em> </em>) = <span class="regular-symbol">∑</span><sub><em>g</em></sub> <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub><em> </em>=<em> </em><em>i</em>, <em>G</em><sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub><em> </em>=<em> </em><em>g</em><em> </em>) by the law of total probability. </li>
<li class="readable-text" id="p171"> <span class="regular-symbol">∑</span><sub><em>g</em></sub> <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub><em> </em>=<em>i</em>, <em>G</em><sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub><em> </em>=<em> </em><em>g</em><em> </em>) = <span class="regular-symbol">∑</span><sub><em>g</em></sub> <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub><sub>, </sub><sub><em>G</em></sub><sub>=</sub><sub><em>g</em></sub><em> </em>=<em> </em><em>i</em><em> </em>)<em>P</em><em>  </em>(<em>G</em><em> </em><sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub><sub>, </sub><sub><em>I</em></sub><sub>=</sub><sub><em>i</em></sub><em> </em>=<em> </em><em>g</em><em> </em>) by way of <em>c-component factorization</em>. </li>
<li class="readable-text" id="p172"> <em>P</em><em> </em>(I<sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub><sub>, </sub><sub><em>G</em></sub><sub>=</sub><sub><em>g</em></sub><em> </em>=<em> </em>i) = <em>P</em><em> </em>(<em>I</em><em> </em>=<em> </em><em>i</em><em>  </em>|<em>E</em><em>  </em>=<em> </em><em>e</em>, <em>G</em><em>  </em>=<em> </em><em>g</em><em> </em>) by rule 2 of the do-calculus. </li>
<li class="readable-text" id="p173"> <em>P</em>(G<sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub><sub>, </sub><sub><em>I</em></sub><sub>=</sub><sub><em>i</em></sub><em> </em>=<em> </em>g) = <em>P</em><em> </em>(<em>G</em><em>  </em>=<em> </em><em>g</em>) by rule 3 of the do-calculus. </li>
<li class="readable-text" id="p174"> Therefore, <em>P</em><em> </em>(I<sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub><em> </em>=<em> </em>i)<em>  </em>= <span class="regular-symbol">∑</span><sub><em>g</em></sub> <em>P</em><em> </em>(<em>I</em><em>  </em>=<em> </em><em>i</em><em>  </em>|<em>E</em><em>  </em>=<em> </em><em>e</em>, <em>G</em><em>  </em>=<em> </em><em>g</em><em> </em>) <em>P</em><em> </em>(<em>G</em><em>  </em>=<em> </em><em>g</em><em> </em>) by plugging 3 and 4 into 2. </li>
</ol>
<div class="readable-text" id="p175">
<p>The do-calculus rules are applied in steps 3 and 4. </p>
</div>
<div class="readable-text print-book-callout" id="p176">
<p><span class="print-book-callout-head">Note</span>  Step 2 uses a factorization rule called <em>c-component factorization</em>. A c-component (confounded component) is a set of nodes in a DAG where each pair of observable nodes is connected by a path with edges that always point toward, never away from, the observable nodes (these are the “orphaned cousins” mentioned in chapter 4). The joint probability of the observed variables can be factorized into c-components, and this fact enabled step 2. Factorizing over c-components is common in identification algorithms. See the references in the chapter notes at <a href="https://www.altdeep.ai/p/causalaibook">https://www.altdeep.ai/p/causalaibook</a>.</p>
</div>
<div class="readable-text" id="p177">
<p>This do-calculus-based derivation is far less intuitive than our “ignorability”-based derivation. There are two advantages we get in exchange for that of intuition. First, the do-calculus is <em>complete</em>, meaning that if a query has an identifiable estimand using graphical assumptions alone, it can be derived using the do-calculus. Second, we have algorithms that leverage the do-calculus to automate graphical identification.</p>
</div>
<div class="readable-text" id="p178">
<h2 class="readable-text-h2" id="sigil_toc_id_244"><span class="num-string">10.5</span> Graphical identification algorithms</h2>
</div>
<div class="readable-text" id="p179">
<p>Graphical identification algorithms, often called <em>ID algorithms</em>, automate the application of graph-based identification systems like the do-calculus. When we used y0 to check for identification of <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub><em> </em>) and to derive the backdoor estimand, it was using its implementation of graphical identification algorithms. In this section, we’ll see how we can use these algorithms to identify another useful estimand called the <em>front-door estimand</em>.</p>
</div>
<div class="readable-text" id="p180">
<h3 class="readable-text-h3" id="sigil_toc_id_245"><span class="num-string">10.5.1</span> Case study: The front-door estimand</h3>
</div>
<div class="readable-text" id="p181">
<p>In our online gaming example, suppose we were not able to observe guild membership. Then we would not have backdoor identification of <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=</sub><sub><em>e</em></sub>). However, suppose we had a <em>mediator </em>between side-quest engagement (<em>E</em><em> </em>) and in-game purchases (<em>I</em><em>  </em>)—a node on the graph between <em>E</em> and <em>I</em>. Specifically, our mediator represents <em>won items </em>(<em>W</em><em>  </em>), as seen in figure 10.11. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p182">
<img alt="figure" height="361" src="../Images/CH10_F11_Ness.png" width="870"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.11</span> Side-quest engagement leads to winning virtual items like this magic bow. Won items drive more in-game purchases, such as magic arrows for the magic bow, so we introduce a mediator “won items” on the causal path between side-quest engagement and in-game purchases.</h5>
</div>
<div class="readable-text" id="p183">
<p>The idea of won items is as follows. When a player successfully completes a side-quest, they win a virtual item. The more side-quests they finish, the more items they earn. Those <em>won </em>virtual items and <em>purchased </em>virtual items can complement one another. For example, winning a magic bow motivates purchases of magical arrows. Thus, the amount of won items a player has influences the amount of virtual items they purchase.</p>
</div>
<div class="readable-text intended-text" id="p184">
<p>Given this graph, we can use y0’s implementation of graphical identification algorithms to derive the front-door estimand.</p>
</div>
<div class="browsable-container listing-container" id="p185">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.6</span> Deriving the front-door estimand in y0</h5>
<div class="code-area-container">
<pre class="code-area">from y0.graph import NxMixedGraph as Y0Graph
from y0.dsl import P, Variable
G = Variable("G")    <span class="aframe-location"/> #1
E = Variable("E")     #1
I = Variable("I")    #1
W = Variable("W")     #1
e = E     #1
dag = Y0Graph.from_edges(     #1
    directed=[     #1
        (G, E),    #1
        (G, I),     #1
        (E, W),   #1
        (W, I)    #1
    ]     #1
)     #1

query=P(I @ e)    <span class="aframe-location"/> #2
base_distribution = P(I, E, W)   <span class="aframe-location"/> #3

identification_task = Identification.from_expression(   <span class="aframe-location"/> #4
    graph=dag,    #4
    query=query,     #4
    estimand=base_distribution)     #4
identify(identification_task)     #4</pre>
<div class="code-annotations-overlay-container">
     #1 Build a new graph with the mediator variable.
     <br/>#2 Still the same query as in listing 10.5, P(I_{E=e})
     <br/>#3 But now we observe I, E, and W
     <br/>#4 Finally, we check if the query is identified given the DAG and observational distribution.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p186">
<p>This code will return the output in figure 10.12.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p187">
<img alt="figure" height="25" src="../Images/ness-ch10-eqs-3x.png" width="548"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.12</span> Y0 renders a math figure as output of identification.</h5>
</div>
<div class="readable-text" id="p188">
<p>Rearranging the output, and in our notation, this is the result:<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p189">
<img alt="figure" height="56" src="../Images/ness-ch10-eqs-4x.png" width="882"/>
</div>
<div class="readable-text" id="p190">
<p>Simplifying as before, we get the front-door estimand:<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p191">
<img alt="figure" height="52" src="../Images/ness-ch10-eqs-5x.png" width="723"/>
</div>
<div class="readable-text" id="p192">
<p>Note that there is an outer summation over <em>W</em> and an inner summation over all values of <em>E</em> (with each value of <em>E</em> denoted as <em class="obliqued">ε</em>, distinct from the intervention value <em>e</em>).</p>
</div>
<div class="readable-text" id="p193">
<h3 class="readable-text-h3" id="sigil_toc_id_246"><span class="num-string">10.5.2</span> Demystifying the front door</h3>
</div>
<div class="readable-text" id="p194">
<p>Like the backdoor estimand, the do-calculus derivation of the front-door estimand involves repeated substitutions using rules 2 and 3. The rough intuition behind the front-door estimand is that the statistical association between side-quest engagement and in-game purchases comes from both the direct causal path and the path through the backdoor confounder guild membership (<em>G</em><em> </em>). The front-door estimand uses the mediator to determine how much of that association is due to the direct causal path; the mediator acts as a gauge of the flow of statistical information through that direct causal path.</p>
</div>
<div class="readable-text intended-text" id="p195">
<p>A key benefit of the estimand is that it does not require observing a set of confounders that block all possible backdoor paths. Avoiding backdoor adjustment is useful when you have many confounders, are unable to adjust due to latent confounders, or are concerned that there might be some unknown confounders.</p>
</div>
<div class="readable-text intended-text" id="p196">
<p>Next, we’ll examine how to identify counterfactuals. </p>
</div>
<div class="readable-text" id="p197">
<h2 class="readable-text-h2" id="sigil_toc_id_247"><span class="num-string">10.6</span> General counterfactual identification</h2>
</div>
<div class="readable-text" id="p198">
<p>The causal DAG is a level 2 modeling assumption. The causal hierarchy theorem tells us that the graph in general is not sufficient to identify level 3 counterfactual queries. For counterfactual identification from level 1 or level 2 distributions, you need level 3 assumptions. In simple terms, a level 3 assumption is any causal assumption that you can’t represent with a simple causal DAG.</p>
</div>
<div class="readable-text intended-text" id="p199">
<p>In chapter 9, I introduced the general algorithm for counterfactual inference. The algorithm requires a structural causal model (SCM), which is a level 3 model; it encapsulates level 3 assumptions. With an SCM, the algorithm can infer <em>all </em>counterfactual queries that can be defined on its underlying variables. The cost of this ability is that the SCM must encapsulate <em>all </em>the assumptions needed to answer all those queries. Many of these assumptions cannot be validated with level 1 or level 2 data. </p>
</div>
<div class="readable-text intended-text" id="p200">
<p>The more assumptions you make, the more vulnerable your inferences are to violations of these assumptions. For this reason, we seek identification techniques that target specific counterfactual queries (rather than every counterfactual query) with the minimal set of level 3 assumptions possible.</p>
</div>
<div class="readable-text" id="p201">
<h3 class="readable-text-h3" id="sigil_toc_id_248"><span class="num-string">10.6.1</span> The problem with the general algorithm for counterfactual inference</h3>
</div>
<div class="readable-text" id="p202">
<p>We can see the problem with the general algorithm for counterfactual inference when we apply it to two similar SCMs. Let’s suppose there is a ground-truth SCM that differs from the SCM you are using to run the algorithm. Suppose both SCMs have the exact same underlying DAG and the same statistical fit on observational and experimental data; in other words, the SCMs provide the same inferences for all level 1 and level 2 queries. Your SCM could still produce different (inaccurate) counterfactual inferences relative to the ground-truth SCM.</p>
</div>
<div class="readable-text intended-text" id="p203">
<p>To see why, recall the stick-breaking example from chapter 6. I posed two similar but different SCMs. This was the first:<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p204">
<img alt="figure" height="141" src="../Images/ness-ch10-eqs-10x.png" width="315"/>
</div>
<div class="readable-text" id="p205">
<p>And this was the second:<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p206">
<img alt="figure" height="141" src="../Images/ness-ch10-eqs-11x.png" width="314"/>
</div>
<div class="readable-text" id="p207">
<p>Figure 10.13 visualizes sampling a single value from these models.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p208">
<img alt="figure" height="502" src="../Images/CH10_F13_Ness.png" width="793"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.13</span> Two different SCMs encode the exact same observational and interventional distributions, but given the same exogenous variable value, you can get two different values of the corresponding endogenous variable in each model.</h5>
</div>
<div class="readable-text" id="p209">
<p>Figure 10.13 shows how, given a value of <em>n</em><sub><em>y</em></sub> = .15, the sticks break at the .15 meters point, but the first stick will break in region 2, returning a value of 2, while the second stick will break in region 3, returning a 3. They produce different outcomes given the same random input because they differ in a level 3 assumption, i.e., <em>how </em>they process the input.</p>
</div>
<div class="readable-text intended-text" id="p210">
<p>For this reason, when we go in the opposite direction and apply the abduction step in the general counterfactual inference algorithm, we can get different results across these models. For a given value of the endogenous variable, we can get different posterior distributions on the exogenous variable. </p>
</div>
<div class="readable-text intended-text" id="p211">
<p>Figure 10.14 illustrates how the two models, for an observed outcome of 3, would produce different inferences on <em>N</em><sub><em>y</em></sub>. For the first SCM, a value of <em>y</em><em> </em>=<em> </em>3 means <em>P</em><em> </em>(<em>N</em><sub><em>y</em></sub><em> </em>|<em>Y</em><em>  </em>=<em>  </em>3) is a continuous uniform distribution on the range (<em>p</em><sub><em>x</em></sub><sub>1</sub> + <em>p</em><sub><em>x</em></sub><sub>2</sub>) to 1, and for the second SCM, it is a continuous uniform distribution on the range 0 to <em>p</em><sub><em>x</em></sub><sub>3</sub>. These different distributions of <em>P</em><em> </em>(<em>N</em><sub><em>y</em></sub><em> </em>|<em>Y</em><em>  </em>=<em>  </em>3) would lead to different results from the counterfactual inference algorithm. Now suppose SCM 2 is right and SCM 1 is wrong. If we choose SCM 1, our counterfactual inferences will be inaccurate. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p212">
<img alt="figure" height="562" src="../Images/CH10_F14_Ness.png" width="778"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.14</span> The two SCMs, despite encoding the same set of observational and interventional distributions, would produce two different posteriors of <em>N</em><em><sub>y</sub></em> given <em>Y</em>=3 in the abduction step. Therefore, they encode different counterfactual distributions and can produce different counterfactual inferences.</h5>
</div>
<div class="readable-text" id="p213">
<p>The general case is even harder; there can be many SCMs entailing the same level 1 and 2 assumptions but have different level 3 assumptions. You might learn one of those SCMs by, for example, using a deep neural network-based approach to learn a deep SCM from level 1 and level 2 data. But the deep SCM might not be the <em>right </em>SCM with respect to the counterfactual inferences you want to make.</p>
</div>
<div class="readable-text intended-text" id="p214">
<p>The general algorithm for counterfactual inference is ideal if you are confident in the ground-truth SCM. But in cases where you aren’t, you can look toward counterfactual identification, where you specify a <em>minimal </em>set of level 3 assumptions that enable you to identify a target counterfactual query.</p>
</div>
<div class="readable-text" id="p215">
<h3 class="readable-text-h3" id="sigil_toc_id_249"><span class="num-string">10.6.2</span> Example: Monotonicity and the probabilities of causation</h3>
</div>
<div class="readable-text" id="p216">
<p>Monotonicity is an example of a powerful level 3 assumption. Monotonicity is the simple assumption that the relationship between a cause <em>X</em> and an outcome <em>Y</em> is monotonic: <em>E</em><em> </em>(<em>Y</em><em>  </em>|<em>X</em><em> </em>=<em>x</em><em> </em>) either never increases or never decreases as <em>x</em> increases. Note that linearity is a special case of monotonicity.</p>
</div>
<div class="readable-text intended-text" id="p217">
<p>An intuitive example of monotonicity and non-monotonicity is in the dosage of medicine. In a monotonic dose-response relationship, taking more of the medicine either helps or does nothing. In a non-monotonic dose-response relationship, taking the medicine might help at a normal dose, but taking an overdose might cause the problem to get worse. Monotinicity helps identification by eliminating counterfactual possibilities; if the dose-response relationship is monotonic, when you imagine what would have happened if you took a stronger dose, you can eliminate the possiblity that you would have gotten worse.</p>
</div>
<div class="readable-text intended-text" id="p218">
<p>Recall the probabilities of causation we saw in chapter 8:</p>
</div>
<ul>
<li class="readable-text" id="p219"> Probability of necessity (PN): <em>P</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub>0</sub><em> </em>=<em> </em>0|<em>X</em><em>  </em>=<em> </em>1, <em>Y</em><em>  </em>=<em> </em>1) </li>
<li class="readable-text" id="p220"> Probability of sufficiency (PS): <em>P</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=1</sub><em> </em>=<em> </em>1|<em>X</em><em>  </em>=<em> </em>0, <em>Y</em><em>   </em>=<em> </em>0) </li>
<li class="readable-text" id="p221"> Probability of necessity and sufficiency (PNS): <em>P</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=1</sub><em> </em>=<em> </em>1, <em>Y</em><sub><em>X</em></sub><sub>=0</sub><em> </em>=<em> </em>0) </li>
</ul>
<div class="readable-text" id="p222">
<p>Given monotonicity, we can identify the following level 2 estimands for the probabilities of causation.</p>
</div>
<ul>
<li class="readable-text" id="p223"> PN = (<em>P</em>(<em>Y</em><em>  </em>=<em> </em>1) – <em>P</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=0</sub><em> </em>=<em> </em>1))/<em>P</em><em> </em>(<em>X</em>=1, <em>Y</em>=1) </li>
<li class="readable-text" id="p224"> PS = (<em>P</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=1</sub><em> </em>=<em> </em>1) – <em>P</em><em> </em>(<em>Y</em><em>  </em>=<em> </em>1))/<em>P</em><em> </em>(<em>X</em><em>  </em>=<em> </em>0, <em>Y</em><em>  </em>=<em> </em>0) </li>
<li class="readable-text" id="p225"> PNS = <em>P</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=1</sub><em> </em>=<em> </em>1) – <em>P</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=0</sub><em> </em>=<em> </em>1) </li>
</ul>
<div class="readable-text" id="p226">
<p>We can estimate these level 2 estimands from level 2 data, such as a randomized experiment. And, of course, if we only have observational data, we can use backdoor or front-door adjustment or another identification strategy to infer <em>P</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=0</sub><em> </em>=1) and <em>P</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=1</sub><em> </em>=1) from that data.</p>
</div>
<div class="readable-text intended-text" id="p227">
<p>We could derive these estimands by hand again, but instead, let’s think about the monotonicity enabled this identification by eliminating counterfactual possibilities. To see this, consider our uplift modeling question in chapter 8. There, <em>X</em> was whether we sent a promotion, and <em>Y</em> was whether the customer remained a paying subscriber (<em>Y</em><em>  </em>=<em> </em>1) or “churned” (unsubscribed; <em>Y</em><em>  </em>=<em> </em>0). We segmented the subscribers as follows:</p>
</div>
<ul>
<li class="readable-text" id="p228"> <em>Persuadables</em><em> </em>—Subscribers whose chance of remaining increases when you send a promotion </li>
<li class="readable-text" id="p229"> <em>Sure things</em><em> </em>—Subscribers who have a high chance of remaining regardless of whether you send a promotion </li>
<li class="readable-text" id="p230"> <em>Lost causes</em><em> </em>—Subscribers who have a low chance of remaining regardless of whether you send a promotion </li>
<li class="readable-text" id="p231"> <em>Sleeping dogs</em>: Subscribers whose chances of remaining <em>go down </em>when you send a promotion </li>
</ul>
<div class="readable-text" id="p232">
<p>If you assume monotonicity, you are assuming that sending the promotion either does nothing or increases the chances of remaining. It assumes there are no users who will respond poorly to the promotion. In other words, assuming monotonicity means you assume there are no sleeping dogs.</p>
</div>
<div class="readable-text intended-text" id="p233">
<p>Now let’s consider how this narrows things down. Suppose you have the following question:</p>
</div>
<div class="readable-text" id="p234">
<blockquote>
<div>
     I failed to send a promotion to a customer and they churned. Would they have remained had I sent the promotion? 
     <em>P</em> ( 
     <em>Y</em>
<sub><em>X</em></sub>
<sub>=1</sub> = 1| 
     <em>X</em>  = 0, 
     <em>Y</em>  = 0)
    </div>
</blockquote>
</div>
<div class="readable-text" id="p235">
<p>This counterfactual query is the probability of sufficiency. We want to know if sending the promotion would have increased the chances of their remaining. Thinking through the question,</p>
</div>
<ul>
<li class="readable-text" id="p236"> If the customer was a persuadable, sending the promotion would have increased their chances of remaining. </li>
<li class="readable-text" id="p237"> If the customer was a lost cause, sending the promotion would have had no effect. </li>
<li class="readable-text" id="p238"> If the customer was a sleeping dog, sending the promotion would have made them <em>even less </em>likely to remain. </li>
</ul>
<div class="readable-text" id="p239">
<p>It’s hard to determine if we should have sent the promotion if being a persuadable and being a sleeping dog were both possible for this customer, in one case the promotion would have helped and in the other it would have made churning even more certain. But if we assume monotonicity, we eliminate the possibility that they were a sleeping dog, and can conclude sending the promotion would have helped or, at least, not have hurt their chances of staying.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p240">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Bayesian modeling and counterfactual identification</h5>
</div>
<div class="readable-text" id="p241">
<p>Although the graphical identification algorithms will work with some counterfactual queries, we don’t have general algorithms for counterfactual identification. But given our focus on the tools of probabilistic ML, we can look to Bayesian modeling for a path forward.</p>
</div>
<div class="readable-text" id="p242">
<p>Identification is fundamentally about uncertainty. For example, in the counterfactual case, a lack of identification means that even with infinite level 1 and level 2 data, you can’t be certain about the true value of the level 3 query. From a Bayesian perspective, we can use probability to handle that uncertainty.</p>
</div>
<div class="readable-text" id="p243">
<p>Suppose you have a set of causal assumptions, including non-graphical assumptions, and some level 1 and 2 data. You can take the following Bayesian approach to test whether your assumptions and data are sufficient to identify your counterfactual query:</p>
</div>
<ol>
<li class="readable-text" id="p244"> Specify a set of SCMs that are diverse yet all consistent with your causal assumptions. </li>
<li class="readable-text" id="p245"> Place a prior distribution over this set, such that more plausible models get higher prior probability values. </li>
<li class="readable-text" id="p246"> Obtain a posterior distribution on the SCMs given observational (level 1) and interventional (level 2) data. </li>
<li class="readable-text" id="p247"> Sample SCMs from the posterior distribution, and for each sample SCM, you apply the general algorithm for counterfactual inference for a specific counterfactual query. </li>
</ol>
<div class="readable-text" id="p248">
<p>The result would constitute the posterior distribution over this counterfactual inference. If your causal assumptions and your data are enough to identify the counterfactual query, the posterior on the counterfactual inference will converge to the true value as the size of your data increases. (Successful convergence assumes typical “regularity” conditions for Bayesian estimation. Results will depend on the quality of the prior.) But even if it doesn’t converge to the true value, your assumptions might still enable convergence to a ballpark region around the true value that is small enough to be useful (this is called partial identification, as described in section 10.9).</p>
</div>
<div class="readable-text" id="p249">
<p>The Pyro library, and its causality-focused extension ChiRho, facilitate combining Bayesian and causal ideas in this way.</p>
</div>
</div>
<div class="readable-text" id="p250">
<p>There are generalizations of monotonicity from binary actions (like sending or not sending a promotion) to multiple actions as in a decision or reinforcement learning problem, see the course notes at <a href="https://www.altdeep.ai/p/causalaibook">https://www.altdeep.ai/p/causalaibook</a> for references.</p>
</div>
<div class="readable-text" id="p251">
<h2 class="readable-text-h2" id="sigil_toc_id_250"><span class="num-string">10.7</span> Graphical counterfactual identification</h2>
</div>
<div class="readable-text" id="p252">
<p>A conventional causal DAG only encodes level 2 assumptions, but there are graphical techniques for reasoning about counterfactuals. Graphical counterfactual inference only works in special cases, but these cases are quite practical. Further, working with graphs enables us to automate identification with algorithms. To illustrate graphical counterfactual identification, we’ll introduce a new case study.</p>
</div>
<div class="readable-text intended-text" id="p253">
<p>When you open Netflix, you see the Netflix dashboard, which shows several forms of recommended content. Two of these are “Top Picks For You,” which is a personalized selection of shows and movies that Netflix’s algorithms predict you will enjoy based on your past viewing behavior and ratings, and “Because You Watched,” which recommends content based on things you watched recently. The model of this system includes the following variables:</p>
</div>
<ul>
<li class="readable-text" id="p254"> <em>T</em><em> </em>—A variable for the recommendation policy that selects a subscriber’s “Top Picks for You” content. For simplicity, we’ll consider a policy, “+<em>t</em><em>  </em>”, that is currently in production. We’ll use “–<em>t</em><em>  </em>”, meaning “not <em>t</em><em>  </em>”, to represent alternative policies. </li>
<li class="readable-text" id="p255"> <em>B</em><em> </em>—A variable for the recommendation policy that selects a subscriber’s “Because You Watched” content. Again, we’ll simplify this to a binary variable with policy “+<em>b</em><em> </em>”, representing the policy in production, and all alternative policies “–<em>b</em><em> </em>”, as in “not <em>b</em><em> </em>.” </li>
<li class="readable-text" id="p256"> <em>V</em><em> </em>—The amount of engagement that a subscriber has with the content recommended by “Because You Watched.” </li>
<li class="readable-text" id="p257"> <em>W</em><em> </em>—The amount of engagement that a subscriber has with the content recommended by “Top Picks for You.” </li>
<li class="readable-text" id="p258"> <em>A</em><em> </em>—Attrition, meaning whether a subscriber eventually leaves Netflix. </li>
<li class="readable-text" id="p259"> <em>C</em><em> </em>—Subscriber context, meaning the type of subscriber (location, demographics, preferences, etc.) we are dealing with. </li>
</ul>
<div class="readable-text" id="p260">
<p>Recommendation algorithms always take the profile of the subscriber into account, along with the viewership history, so subscriber profile <em>C</em> is a cause of both recommendation policy variables <em>T</em> and <em>B</em>.</p>
</div>
<div class="readable-text intended-text" id="p261">
<p>In this section, we’ll use y0 to analyze this problem at various levels of the hierarchy. We’ll start by visualizing the graph.</p>
</div>
<div class="browsable-container listing-container" id="p262">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.7</span> Plot the recommendation DAG</h5>
<div class="code-area-container">
<pre class="code-area">T = Variable("T")    <span class="aframe-location"/> #1
W = Variable("W")    #1
B = Variable("B")    #1
V = Variable("V")    #1
C = Variable("C")    #1
A = Variable("A")    #1
t, a, w, v, b = T, A, W, V, B   #1
dag = Y0Graph.from_edges(directed=[    <span class="aframe-location"/> #2
    (T, W),    #2
    (W, A),    #2
    (B, V),    #2
    (V, A),    #2
    (C, T),    #2
    (C, A),   <span class="aframe-location"/> #2
    (C, B)    #2
])    #2
gv_draw(dag)   <span class="aframe-location"/> #3</pre>
<div class="code-annotations-overlay-container">
     #1 Define variables for the model.
     <br/>#2 Create the graph.
     <br/>#3 Plot the graph.
     <br/>
</div>
</div>
</div>
<div class="browsable-container figure-container" id="p263">
<img alt="figure" height="370" src="../Images/CH10_F15_Ness.png" width="199"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.15</span> Causal DAG for the recommendation algorithm problem</h5>
</div>
<div class="readable-text" id="p264">
<p>This generates the DAG in figure 10.15. </p>
</div>
<div class="readable-text intended-text" id="p265">
<p>As a preliminary investigation, you might look at the average treatment effect (ATE, a level 2 query) of the “Top Picks for You” content on attrition <em>E</em><em> </em>(<em>A</em><sub><em>T</em></sub><sub>=+</sub><sub><em>t</em></sub> – <em>A</em><sub><em>T</em></sub><sub>=–</sub><sub><em>t</em></sub>). Given that attrition <em>A</em> has a binary outcome, we can write this as <em>P</em><em> </em>(<em>A</em><sub><em>T</em></sub><sub>=+</sub><sub><em>t</em></sub><em> </em>=+<em>a</em><em> </em>) – <em>P</em><em> </em>(<em>A</em><sub><em>T</em></sub><sub>=–</sub><sub><em>t</em></sub><em> </em>=<em> </em>+<em>a</em><em> </em>). Focusing on <em>P</em><em> </em>(<em>A</em><sub><em>T</em></sub><sub>=–</sub><sub><em>t</em></sub><em> </em>=<em> </em>+<em>a</em><em> </em>), we know right away that we can identify this via both the (level 2) backdoor and the front door. So let’s move on to an interesting (level 3) counterfactual query called <em>effect of treatment on the treated </em>(ETT).</p>
</div>
<div class="readable-text" id="p266">
<h3 class="readable-text-h3" id="sigil_toc_id_251"><span class="num-string">10.7.1</span> Effect of treatment on the treated</h3>
</div>
<div class="readable-text" id="p267">
<p>Recall that you get the ATE directly (without needing to identify and estimate a level 1 estimand) from a randomized experiment. Suppose you ran such an experiment on a cohort of users, and it showed a favorable ATE, such as that +<em>t</em> has a favorable impact on <em>W</em> and <em>A</em> relative to –<em>t</em>. So your team deploys the policy.</p>
</div>
<div class="readable-text intended-text" id="p268">
<p>Suppose the +<em>t</em> policy works best with users who have watched a lot of movies and thus have more viewing data. For this reason, when the policy is deployed to production, such users are more likely to get assigned the policy. But since they are so highly engaged, they are unlikely to leave, regardless of whether they are assigned the +<em>t</em> or –<em>t</em> policy. We could have a situation where the +<em>t</em> policy looks effective in an experiment where people are assigned policies randomly, regardless of their level of engagement, but in production the assignment is biased to highly engaged people who are indifferent to the policy. </p>
</div>
<div class="readable-text intended-text" id="p269">
<p>The level 3 query that addresses this is a counterfactual version of the ATE called effect of treatment on the treated (ETT, or sometimes ATT, as in <em>average treatment effect on the treated</em>). We write this as counterfactual query <em>E</em><em> </em>(<em>A</em><sub><em>T</em></sub><sub>=+</sub><sub><em>t</em></sub> – <em>A</em><sub><em>T</em></sub><sub>=–</sub><sub><em>t</em></sub><em> </em>|<em>T</em><em>  </em>=<em>  </em>+<em>t</em><em> </em>), as in “for people who saw policy +<em>t</em>, how much more attrition do they have relative to what they would have if they had seen –<em>t</em><em> </em>?”<em> </em>Decomposing for binary <em>A</em> as we did with the ATE, we can write this as <em>P</em>(<em>A</em><sub><em>T</em></sub><sub>=+</sub><sub><em>t</em></sub>=+<em>a</em>|<em>T</em><em>  </em>=<em> </em>+<em>t</em>) – <em>P</em>(<em>A</em><sub><em>T</em></sub><sub>=–</sub><sub><em>t</em></sub><em> </em>=<em> </em>+<em>a</em>|<em>T</em><em>  </em>=<em> </em>+<em>t</em>). <em>P</em>(<em>A</em><sub><em>T</em></sub><sub>=+</sub><sub><em>t</em></sub><em> </em>=<em> </em>+<em>a</em><em> </em>|<em>T</em><em>  </em>=<em> </em>+<em>t</em><em> </em>) simplifies to <em>P</em><em> </em>(<em>A</em><em>  </em>=<em> </em>+<em>a</em><em> </em>|<em>T</em><em>  </em>=<em> </em>+<em>t</em><em> </em>) by the law of consistency. So we can focus on the second term, <em>P</em><em> </em>(<em>A</em><sub><em>T</em></sub><sub>=–</sub><sub><em>t</em></sub>=+<em>a</em><em> </em>|<em>T</em>=+<em>t</em><em> </em>).</p>
</div>
<div class="readable-text intended-text" id="p270">
<p>In this special case of binary <em>A</em>, we can identify the ETT using graphical identification (for non-binary <em>A</em>, more level 3 assumptions are needed). To do graphical identification for counterfactuals, we can use graphical identification algorithms with counterfactual graphs.</p>
</div>
<div class="readable-text" id="p271">
<h3 class="readable-text-h3" id="sigil_toc_id_252"><span class="num-string">10.7.2</span> Identification over the counterfactual graph</h3>
</div>
<div class="readable-text" id="p272">
<p>Y0 can derive an estimand for ETT using a graphical identification algorithm called “IDC*” (pronounced I-D-C-star). </p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p273">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Graph ID algorithms, ID, IDC, ID*, IDC*, in y0</h5>
</div>
<div class="readable-text" id="p274">
<p>Some of the core graphical identification algorithms implemented in y0 are ID, ID*, IDC, and IDC*. ID identifies interventional (level 2) queries from a DAG and observational (level 1) data. ID* identifies counterfactual (level 3) queries from observational and experimental (level 1 and level 2) data. IDC and IDC* extend ID and ID* to work on queries that condition on evidence, such as ETT.</p>
</div>
<div class="readable-text" id="p275">
<p>The algorithms use the structure of the causal graph to recursively simplify the identification problem by removing irrelevant variables and decomposing the graph into c-component subgraphs. They apply the rules of do-calculus to reduce intervention terms, block confounding backdoor paths, and factorize the query into simpler subqueries. If no further simplification is possible due to the graph's structure, the algorithms return a 'non-identifiable' result.</p>
</div>
<div class="readable-text" id="p276">
<p>This chapter's code relies on Y0’s implementations of these algorithms, though Y0 implements other graphical identification algorithms as well.</p>
</div>
</div>
<div class="browsable-container listing-container" id="p277">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.8</span> Identifying ETT with a graphical identification algorithm</h5>
<div class="code-area-container">
<pre class="code-area">from y0.algorithm.identify.idc_star import idc_star

idc_star(
    dag,
    outcomes={A @ -t: +a},    <span class="aframe-location"/> #1
    conditions={T: +t}   <span class="aframe-location"/> #2
)</pre>
<div class="code-annotations-overlay-container">
     #1 Hypothetical outcome A
     <sub>T=–t</sub> = +a
     <br/>#2 Factual condition T = +t
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p278">
<p>This will produce a rather verbose level 2 estimand. We can then apply level 2 graphical identification algorithms to get a level 1 estimand, which will simplify to the following:<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p279">
<img alt="figure" height="52" src="../Images/ness-ch10-eqs-6x.png" width="717"/>
</div>
<div class="readable-text" id="p280">
<p>I’ll show a simple derivation in the next section.</p>
</div>
<div class="readable-text intended-text" id="p281">
<p>For now, the intuition is that we are applying graphical identification algorithms over something called a counterfactual graph. Up until now, our graph of choice for counterfactual reasoning was the parallel world graph. Indeed, we can have y0 make a parallel world graph for us.</p>
</div>
<div class="browsable-container listing-container" id="p282">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.9</span> Plotting the parallel world graph with y0</h5>
<div class="code-area-container">
<pre class="code-area">from y0.algorithm.identify.cg import make_parallel_worlds_graph
parallel_world_graph = make_parallel_worlds_graph(   <span class="aframe-location"/> #1
    dag,  #1
     {frozenset([+t])}    #1
)    #1
gv_draw(parallel_world_graph)    <span class="aframe-location"/> #2</pre>
<div class="code-annotations-overlay-container">
     #1 The make_parallel_worlds_graph method takes an input DAG and sets of interventions. It constructs a new world for each set.
     <br/>#2 The helper function visualizes the graph in a familiar way.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p283">
<p>This graph differs slightly from the ones I’ve drawn because the algorithm applies the subscript for an intervention to every node in the world where the intervention occurred; the subscript indexes all the variables in a world. It’s up to us to reason that <em>C</em> from one world and <em>C</em><sub>+</sub><sub><em>t</em></sub> from another must have the same outcomes, since <em>C</em><sub>+</sub><sub><em>t</em></sub> is not affected by its world’s intervention do(<em>T</em><em>  </em>=<em> </em>+<em>t</em><em> </em>).</p>
</div>
<div class="readable-text intended-text" id="p284">
<p>Now recall that the problem with the parallel world graph is that d-separation won’t work with it. For example, in figure 10.16, d-separation suggests that <em>C</em> and <em>C</em><sub>+</sub><sub><em>t</em></sub> are conditionally independent given their common exogenous parent <em>N</em><sub><em>C</em></sub>, but we just articulated that <em>C</em> and C<sub>+</sub><sub><em>t</em></sub> must be the same; if C has a value, <em>C</em><sub>+</sub><sub><em>t</em></sub> must have the same value, so they are perfectly dependent.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p285">
<img alt="figure" height="554" src="../Images/CH10_F16_Ness.png" width="790"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.16</span> A parallel world graph drawn by y0 (and Graphviz). In this version of the parallel world graph, the subscripts indicate a world. For example, +<em>t</em> indicates the world where the intervention do(<em>T</em>=+<em>t</em>) is applied. To prevent confusion, the exogenous variables use superscripts instead of subscripts to indicate their child endogenous variables (e.g., <em>N</em><em><sup>C</sup></em><em> </em>is the parent of <em>C</em> (and <em>C</em><sub>+t</sub>).</h5>
</div>
<div class="readable-text" id="p286">
<p>We can remedy this with the <em>counterfactual graph</em>. A counterfactual graph is created by using a parallel world graph and the counterfactual query to understand which nodes across worlds in the parallel world graph are equivalent, and then collapsing equivalent nodes into one. The resulting graph contains nodes across parallel worlds that are relevant to the events in the query. Unlike parallel world graphs, you can use d-separation to reason about counterfactual graphs. We can use y0 to create a counterfactual graph for events <em>A</em><sub><em>T</em></sub><sub>=–</sub><sub><em>t</em></sub><em>  </em>=<em>  </em>+<em>a</em> and <em>T</em><em>  </em>=<em>  </em>+<em>t</em>.</p>
</div>
<div class="browsable-container listing-container" id="p287">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.10</span> Listing 10.10 Counterfactual graph events <em>A</em><em><sub>T</sub></em><sub>=</sub><sub>–</sub><em><sub>t</sub></em>=+<em>a</em> and <em>T</em>=+<em>t</em></h5>
<div class="code-area-container">
<pre class="code-area">from y0.algorithm.identify.cg import make_counterfactual_graph

events = {A @ -t: +a, T: +t}   <span class="aframe-location"/> #1
cf_graph, _ = make_counterfactual_graph(dag, events)
gv_draw(cf_graph)</pre>
<div class="code-annotations-overlay-container">
     #1 Counterfactual graphs work with event outcomes in the query. For P(A
     <sub>T=–t</sub>=+a|T=+t), we want events A
     <sub>T=–t</sub> =+a and T=+t.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p288">
<p>This creates the counterfactual graph in figure 10.17.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p289">
<img alt="figure" height="480" src="../Images/CH10_F17_Ness.png" width="493"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.17</span> Counterfactual graph for events, produced by y0 (and Graphviz). <em>T</em><em><sub>–t</sub></em><sub> </sub>corresponds to the intervention do(<em>T</em>=–<em>t</em>).</h5>
</div>
<div class="readable-text" id="p290">
<p>At a high level, graphical identification algorithms in y0 do counterfactual identification by working with counterfactual graphs in lieu of conventional DAGs. First, it finds a level 2 estimand for a level 3 query. From there, you can use experimental data to answer the level 2 terms in the estimand, or you can attempt to further derive them to level 1 estimands from the level 2 terms.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p291">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Graphs alone won’t work when you condition on outcome!</h5>
</div>
<div class="readable-text" id="p292">
<p>Suppose that instead of the ETT term <em>P</em>(<em>A</em><em><sub>T</sub></em><sub>=–</sub><em><sub>t</sub></em>=+<em>a</em>|<em>T</em>=+<em>t</em>), you were interested in <em>P</em>(<em>A</em><em><sub>T</sub></em><sub>=–</sub><em><sub>t </sub></em>=+<em>a</em>|<em>T</em>=+<em>t</em>, <em>A</em>=+<em>a</em>), answering the question “Given a subscriber exposed to policy +<em>t</em> and later unsubscribed, would they still have unsubscribed had they not been exposed to that policy?” Or you could be interested in <em>E</em>(<em>A</em><em><sub>T</sub></em><sub>=–</sub><em><sub>t</sub></em> – <em>A</em><em><sub>T</sub></em><sub>=+</sub><em><sub>t</sub></em>|<em>T</em>=+<em>t</em>, <em>A</em>=+<em>a</em>) sometimes called <em>counterfactual regret</em>, which captures the amount the policy +<em>t</em> contributed to an unsubscribed individual’s decision to unsubscribe.</p>
</div>
<div class="readable-text" id="p293">
<p><em>P</em>(<em>A</em><em><sub>T</sub></em><sub>=–</sub><em><sub>t</sub></em>=+<em>a</em>|<em>T</em>=+<em>t</em>, <em>A</em>=+<em>a</em>) is an example of a query where the hypothetical outcomes and factual conditions are in conflict. In this case, the factual conditions contain an outcome for <em>A</em>, and the hypothetical condition contains an interventional outcome for <em>A</em>. The graphical counterfactual identification techniques mentioned in this section will not work for this type of query. Identification in this case requires additional level 3 assumptions.</p>
</div>
<div class="readable-text" id="p294">
<p>This is unfortunate, because this type of counterfactual is precisely the kind of “how might things have turned out differently?” counterfactual questions that are the most interesting, and the most central to how humans reason and make decisions.</p>
</div>
</div>
<div class="readable-text" id="p295">
<p>We can also use graphical identification for more advanced queries. For example, suppose you want to isolate how <em>T </em>affects <em>A</em> from how <em>B</em> affects <em>A</em>. You want to focus on users where <em>B</em> was –<em>b</em>. You find the data from a past experiment where “Because you watched . . .” policy <em>B</em> was randomized. You take that data and zoom in on participants in the experiment who were assigned –<em>b</em>. The outcome of interest in that experiment was <em>V</em>, the amount of engagement with the content recommended in the “Because you Watched” box. So you have the outcomes of <em>V</em><sub><em>B</em></sub><sub>=–</sub><sub><em>b</em></sub> for those subscribers of interest. With this new data, you expand your query from <em>P</em><em> </em>(<em>A</em><sub><em>T</em></sub><sub>=–</sub><sub><em>t</em></sub><em> </em>=<em> </em>+<em>a</em><em> </em>|<em>T</em><em>  </em>=<em> </em>+<em>t</em>) to <em>P</em><em> </em>(<em>A</em><sub><em>T</em></sub><sub>=–</sub><sub><em>t</em></sub><em> </em>=<em>  </em>+<em>a</em><em> </em>|<em>T</em><em>  </em>=<em>  </em>+<em>t</em>, <em>B</em><em>   </em>=<em>  </em>–<em>b</em>, <em>V</em><sub><em>B</em></sub><sub>=–</sub><sub><em>b</em></sub><em> </em>=<em> </em><em>v</em><em> </em>), including <em>V</em><sub><em>B</em></sub><sub>=–</sub><sub><em>b</em></sub>=<em>v</em> because it is helpful in predicting attrition. Now you have three parallel worlds to reason over: the actual world, the world with do(<em>T</em><em>   </em>=<em>  </em>+<em>t</em><em> </em>), and the world with do(<em>B</em><em>   </em>=<em>  </em>–<em>b</em><em> </em>).</p>
</div>
<div class="browsable-container listing-container" id="p296">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.11</span> Create a parallel world graph for do(<em>T</em>=+<em>t</em>) and do(<em>B</em>=–<em>b</em>)</h5>
<div class="code-area-container">
<pre class="code-area">parallel_world_graph = make_parallel_worlds_graph(
    dag,
   {frozenset([-t]), frozenset([-b])}     <span class="aframe-location"/> #1
)
gv_draw(parallel_world_graph)</pre>
<div class="code-annotations-overlay-container">
     #1 The second argument enumerates the hypothetical conditions.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p297">
<p>This code creates this three-world parallel world graph seen in figure 10.18.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p298">
<img alt="figure" height="433" src="../Images/CH10_F18_Ness.png" width="811"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.18</span> A parallel world graph with the actual world <em>T</em>=+<em>t</em> and hypothetical worlds do(<em>T</em>=–<em>t</em>) and do(<em>B</em>=–<em>b</em>). The dashed lines are edges from exogenous variables (dark gray).</h5>
</div>
<div class="readable-text" id="p299">
<p>Notably, the query <em>P</em><em> </em>(<em>A</em><sub><em>T</em></sub><sub>=–</sub><sub><em>t</em></sub><em>  </em>=<em>  </em>+<em>a</em><em> </em>|<em>T</em><em>  </em>=<em> </em>+<em>t</em>, <em>B</em><em>  </em>=<em>  </em>–<em>b</em>, <em>V</em><sub><em>B</em></sub><sub>=–</sub><sub><em>b</em></sub><em> </em>=<em> </em><em>v</em><em> </em>) collapses the parallel world graph to the same counterfactual graph as <em>P</em><em> </em>(<em>A</em><sub><em>T</em></sub><sub>=–</sub><sub><em>t</em></sub><em> </em>=<em> </em>+<em>a</em><em> </em>|<em>T</em><em>   </em>=<em>  </em>+<em>t</em><em> </em>).</p>
</div>
<div class="browsable-container listing-container" id="p300">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.12</span> Counterfactual graph for expanded expression</h5>
<div class="code-area-container">
<pre class="code-area">joint_query = {A @ -t: +a, T: +t, B: -b, V @ -b: +v}
cf_graph, _ = make_counterfactual_graph(dag, joint_query)
gv_draw(cf_graph)</pre>
</div>
</div>
<div class="readable-text" id="p301">
<p>This gives us the counterfactual graph in figure 10.19, which is the same as the graph in figure 10.17.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p302">
<img alt="figure" height="521" src="../Images/CH10_F19_Ness.png" width="573"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.19</span> The counterfactual graph for <em>P</em>(<em>A</em><em><sub>T</sub></em><sub>=–</sub><em><sub>t</sub></em>=+<em>a</em>|<em>T</em>=+<em>t</em>, <em>B</em>=–<em>b</em>, <em>V</em><em><sub>B</sub></em><sub>=–</sub><em><sub>b</sub></em>=<em>v</em>) is the same as for <em>P</em>(<em>A</em><em><sub>T</sub></em><sub>=–</sub><em><sub>t</sub></em>=+<em>a</em>|<em>T</em>=+<em>t</em>).</h5>
</div>
<div class="readable-text" id="p303">
<p>Next, we’ll look at another graph-based approach called single-world intervention graphs.</p>
</div>
<div class="readable-text" id="p304">
<h3 class="readable-text-h3" id="sigil_toc_id_253"><span class="num-string">10.7.3</span> Counterfactual identification with single-world intervention graphs</h3>
</div>
<div class="readable-text" id="p305">
<p>Single-world intervention graphs (SWIGs) provide an alternative to counterfactual identification with counterfactual graphs. Like a counterfactual graph, we construct a SWIG using the original causal DAG and the causal query. We’ll use the Netflix recommendation example to construct a SWIG for the interventions do(<em>T</em><em>  </em>=<em>  </em>–<em>t</em><em> </em>) and do (<em>B</em><em>  </em>=<em>  </em>–<em>b</em><em> </em>). Let’s construct a SWIG from a causal DAG.</p>
</div>
<div class="readable-text" id="p306">
<h4 class="readable-text-h4 sigil_not_in_toc">Node-spitting operation</h4>
</div>
<div class="readable-text" id="p307">
<p>We have the intervention that targets do(<em>T</em><em>   </em>=<em>  </em>+<em>t</em><em> </em>), and we can implement it with a special kind of graph surgery called a <em>node-splitting operation</em>. We split a new node off the intervention target <em>T</em><em>, as i</em>n figure 10.20. <em>T</em> still represents the same variable as in the original graph, but the new node represents a constant, the intervention value +<em>t</em>. <em>T</em> keeps its parents (in this case <em>C</em><em> </em>) but loses its children (in this case <em>W</em><em>  </em>) to the new node.</p>
</div>
<div class="readable-text" id="p308">
<h4 class="readable-text-h4 sigil_not_in_toc">Subscript inheritance</h4>
</div>
<div class="readable-text" id="p309">
<p>Next, every node downstream of the new node inherits the new node’s values as a subscript. For example, in figure 10.21, <em>W</em> and <em>A</em> are downstream of the intervention, so the subscript <em><sub>T</sub></em><sub>= </sub><sub>–</sub><em><sub>t</sub></em> is appended to these nodes, so they become <em>W</em><sub><em>T</em></sub><sub>=</sub><em> </em><sub>-</sub><sub><em>t</em></sub> and <em>A</em><sub><em>T</em></sub><sub>=</sub><em> </em><sub>-</sub><sub><em>t</em></sub>.</p>
</div>
<div class="browsable-container figure-container" id="p310">
<img alt="figure" height="341" src="../Images/CH10_F20_Ness.png" width="483"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.20</span> The intervention operator for a SWIG is the node-splitting operation.</h5>
</div>
<div class="browsable-container figure-container" id="p311">
<img alt="figure" height="365" src="../Images/CH10_F21_Ness.png" width="442"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.21</span> Every node downstream of the intervention gets the intervention subscript.</h5>
</div>
<div class="readable-text" id="p312">
<h4 class="readable-text-h4 sigil_not_in_toc">Repeat for each intervention</h4>
</div>
<div class="readable-text" id="p313">
<p>We repeat this process for each intervention. In figure 10.22, we apply do(<em>B</em><em>  </em>=<em> </em>–<em>b</em>), and split <em>B</em> and we convert <em>V</em> to <em>V</em><sub><em>B</em></sub><sub>=–</sub><sub><em>b</em></sub> and <em>A</em><sub><em>T</em></sub><sub>=-</sub><sub><em>t</em></sub> to <em>A</em><sub><em>T</em></sub><sub>=–</sub><sub><em>t</em></sub><sub>,</sub><sub><em>B</em></sub><sub>=–</sub><sub><em>b</em></sub>. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p314">
<img alt="figure" height="342" src="../Images/CH10_F22_Ness.png" width="718"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.22</span> A node takes the subscript of all its upstream interventions.</h5>
</div>
<div class="readable-text" id="p315">
<p>Like the counterfactual graph, the SWIG contains counterfactual variables and admits d-separation. With these properties, we can do identification.</p>
</div>
<div class="readable-text" id="p316">
<h3 class="readable-text-h3" id="sigil_toc_id_254"><span class="num-string">10.7.4</span> Identification with SWIGs</h3>
</div>
<div class="readable-text" id="p317">
<p>Suppose we are interested in ETT and want to identify <em>P</em><em> </em>(<em>A</em><sub><em>T</em></sub><sub>=</sub><em> </em><sub>–</sub><sub><em>t</em></sub><em> </em>=<em>  </em>+<em>a</em><em> </em>|<em>T</em><em>  </em>=<em>  </em>+<em>t</em><em> </em>). We derive the SWIG in figure 10.23.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p318">
<img alt="figure" height="311" src="../Images/CH10_F23_Ness.png" width="252"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.23</span> We can use the SWIG to derive ETT using the ignorability trick.</h5>
</div>
<div class="readable-text intended-text" id="p319">
<p>With this graph, we can identify <em>P</em><em> </em>(<em>A</em><sub><em>T</em></sub><sub>=–</sub><sub><em>t</em></sub><em> </em>=<em>  </em>+<em>a</em><em> </em>|<em>T</em><em>  </em>=<em> </em>+<em>t</em><em> </em>) using the ignorability trick I introduced in section 10.4:</p>
</div>
<ol>
<li class="readable-text" id="p320"> <em>P</em>(<em>A</em><sub><em>T</em></sub><sub>=–</sub><sub><em>t</em></sub><em> </em>=<em> </em>+<em>a</em>|<em>T</em><em>  </em>=<em> </em>+<em>t</em>) = <span class="regular-symbol">∑</span><sub><em>c</em></sub> <em>P</em><em> </em>(<em>A</em><sub><em>T</em></sub><sub>=–</sub><sub><em>t</em></sub><em> </em>=<em> </em>+<em>a</em>, <em>C</em><sub><em>T</em></sub><sub>=–</sub><sub><em>t</em></sub>=<em>c</em><em> </em>|<em>T</em><em>  </em>=<em>  </em>+<em>t</em><em> </em>) by the law of total probability. </li>
<li class="readable-text" id="p321"> <span class="regular-symbol">∑</span><sub><em>c</em></sub> <em>P</em><em> </em>(<em>A</em><sub><em>T</em></sub><sub>=</sub><em> </em><sub>–</sub><sub><em>t</em></sub><em> </em>=<em>  </em>+<em>a</em>, <em>C</em><sub><em>T</em></sub><sub>=–</sub><sub><em>t</em></sub><em> </em>=<em>c</em><em>  </em>|<em>T</em><em>  </em>=<em> </em>+<em>t</em><em> </em>) = <span class="regular-symbol">∑</span><sub><em>c</em></sub> <em>P</em><em> </em>(<em>A</em><sub><em>T</em></sub><sub>=–</sub><sub><em>t</em></sub><em> </em>=<em> </em>+<em>a</em>, <em>C</em> <em> </em>=<em> </em><em>c</em><em> </em>|<em>T</em><em> </em>=+<em>t</em><em> </em>), since <em>C</em> is not affected by interventions on <em>T</em>. </li>
<li class="readable-text" id="p322"> <span class="regular-symbol">∑</span><sub><em>c</em></sub> <em>P</em><em> </em>(<em>A</em><sub><em>T</em></sub><sub>=–</sub><sub><em>t</em></sub><em> </em>=<em>  </em>+<em>a</em>, <em>C</em><em>  </em>=<em> </em><em>c</em><em> </em>|<em>T</em><em>  </em>=<em> </em>+<em>t</em><em> </em>) factorizes into <span class="regular-symbol">∑</span><sub><em>c</em></sub> <em>P</em><em> </em>(<em>A</em><sub><em>T</em></sub><sub>=–</sub><sub><em>t </em></sub>=+<em>a</em><em> </em>|<em>C</em><em>  </em>=<em> </em><em>c</em>, <em>T</em><em> </em>=<em> </em>+<em>t</em><em> </em>) <em>P</em><em> </em>(<em>C</em><em>  </em>=<em> </em><em>c</em> | <em>T</em><em>  </em>=<em>  </em>+<em>t</em><em> </em>) by the chain rule of probability. </li>
<li class="readable-text" id="p323"> <em>P</em><em> </em>(<em>A</em><sub><em>T</em></sub><sub>=–</sub><sub><em>t</em></sub><em> </em>=<em>  </em>+<em>a</em><em> </em>|<em>C</em><em> </em>=<em> </em><em>c</em>, <em>T</em><em>  </em>=<em>  </em>+<em>t</em><em> </em>) = <em>P</em><em> </em>(<em>A</em><sub><em>T</em></sub><sub>=–</sub><sub><em>t</em></sub><em> </em>=<em>  </em>+<em>a</em><em> </em>|<em>C</em><em>  </em>=<em> </em><em>c</em>, <em>T</em><em>  </em>=<em>  </em>–<em>t</em><em> </em>), again by the ignorability trick. </li>
<li class="readable-text" id="p324"> And as before, <em>P</em><em> </em>(<em>A</em><sub><em>T</em></sub><sub>=–</sub><sub><em>t</em></sub><em> </em>=<em> </em>+<em>a</em><em> </em>|<em>C</em><em>  </em>=<em> </em><em>c</em>, <em>T</em><em>  </em>=<em> </em>–<em>t</em>) = <em>P</em><em> </em>(<em>A</em> =<em>  </em>+<em>a</em><em> </em>|<em>C</em><em>  </em>=<em>c</em><em> </em>, <em>T</em><em>  </em>=<em>  </em>–<em>t</em><em> </em>) by the law of consistency. Thus, <em>P</em><em> </em>(<em>A</em><sub><em>T</em></sub><sub>=–</sub><sub><em>t</em></sub><em> </em>=<em> </em>+<em>a</em><em> </em>|<em>T</em><em>  </em>=<em>  </em>+<em>t</em><em> </em>) = <span class="regular-symbol">∑</span><sub><em>c</em></sub> <em>P</em>(<em>A</em> =<em>  </em>+<em>a</em><em> </em>|<em>C</em><em>  </em>=<em> </em><em>c</em>, <em>T</em><em>  </em>=<em>  </em>–<em>t</em><em> </em>) <em>P</em>(<em>C</em><em>  </em>=<em> </em><em>c</em> | <em>T</em><em>  </em>=<em> </em>+<em>t</em><em> </em>) </li>
</ol>
<div class="readable-text" id="p325">
<p>The magic happens in the ignorability trick in step 4, where <em>C</em>’s d-separation of <em>A</em><sub><em>T</em></sub><sub>=–</sub><sub><em>t</em></sub> and <em>T</em> lets us change <em>T</em><em>  </em>=<em>  </em>+<em>t</em> to <em>T</em><em>  </em>=<em>  </em>–<em>t</em>. Notice that the same d-separation exists in the counterfactual graph we derived for <em>P</em><em> </em>(<em>A</em><sub><em>T</em></sub><sub>=–</sub><sub><em>t</em></sub><em> </em>=<em> </em>+<em>a</em><em> </em>|<em>T</em><em>  </em>=<em>  </em>+<em>t</em><em> </em>), shown in figure 10.17. The difference is that deriving the SWIG is easy, while deriving the counterfactual graph is nuanced, and one generally uses an algorithm like <code>make_counterfactual_grap</code> in y0.</p>
</div>
<div class="readable-text" id="p326">
<h3 class="readable-text-h3" id="sigil_toc_id_255"><span class="num-string">10.7.5</span> The single-world assumption</h3>
</div>
<div class="readable-text" id="p327">
<p>The node-splitting operation relies on a new level 3 assumption. If you are going to node-split a variable <em>X</em>, then you are assuming it is possible to know what value <em>X</em> would naturally take without the intervention and that it would be possible for you intervene before it realized that value. Imagine in our Netflix example that, given a subscriber had profile <em>C</em>=<em>c</em>, the recommendation algorithm was about to assign the subscriber a policy +<em>t</em> for recommending “Top picks for you,” but before that policy went into effect, you intervened and artificially changed it to –<em>t</em><em> </em>. It’s possible that the way you forced the policy to be –<em>t</em> had some side effects that changed the recommendation system in some fundamental way, such that in this new system, <em>T</em> would not have been <em>+t</em>, in the first place. With the single-world assumption, you assume you can know T’s natural value would have been <em>+t</em>, and that your intervention wouldn’t change the system in a way that would affect <em>T</em> taking that natural value. You are implicitly making this assumption when you reason with SWIGs. </p>
</div>
<div class="readable-text intended-text" id="p328">
<p>That assumption allows you to avoid the need to create additional worlds to reason over. You can condition on outcome <em>T</em><em> </em><em> </em>=<em> </em>+<em>t</em><em> </em> and intervene do(<em>T</em><em> </em><em> </em>=<em> </em>–<em>t</em><em> </em>) in a “single world.” You can also run experiments, where you apply the intervention do(<em>T</em><em> </em><em> </em>=<em> </em>–<em>t</em><em> </em>) and test if <em>T</em> (where you know T’s “natural values”) is conditionally independent of <em>A</em>(<em>T</em><em> </em><em> </em>=<em> </em><em> </em>–<em>t</em><em> </em>) given <em>C</em>. This reduces the number of counterfactual queries you can answer, but proponents of SWIGs suggest this is a strength, because it limits you to counterfactuals that can be validated by experiments.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p329">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Contrasting counterfactual graphs and SWIGs</h5>
</div>
<div class="readable-text" id="p330">
<p><em>Counterfactual graphs and SWIGs are similar in function, but they are distinctly different artifacts.</em></p>
</div>
<ul>
<li class="readable-text" id="p331"> <em>Counterfactual graphs</em><em> </em>—The counterfactual graph works by collapsing equivalent parallel world graph nodes over possible worlds. They only contain nodes relevant to the specific query. They are defined for binary events like {<em>T</em>=+<em>t</em>} and {<em>T</em>=–<em>t</em>}—this works well even with continuous variables, because counterfactual language typically compares one <em>hypothetical condition</em> to one <em>factual condition</em> (e.g., “We invested 1 million; what if we had invested {2/more/half/…}?”). </li>
<li class="readable-text" id="p332"> <em>Single-world intervention graphs (SWIGs) </em><em>—</em>The SWIG works by applying a node-splitting type of graph surgery. Unlike counterfactual graphs, they work with general variables (rather than just binary events) and are not query-specific (all original nodes are present). However, they rely on a single-world assumption—that it is possible to know with certainty what value a variable would have taken had it not been intervened upon.  </li>
</ul>
<div class="readable-text" id="p333">
<p>The primary use case for both graphs is identification. Neither counterfactual graphs nor SWIGs enable identification from level 1 or 2 data of counterfactual queries such as <em>P</em>(<em>A</em><em><sub>T</sub></em><sub>=–</sub><em><sub>t</sub></em>=+<em>a</em>|<em>T</em>=+<em>t</em>, <em>A</em>=+<em>a</em>) where the same variable appears in the hypothetical outcome and the factual condition. But you can still <em>derive</em> the counterfactual graph for such queries; this is not true for SWIGs. That is useful if you want to reason about independence across worlds in cases of queries such as <em>P</em>(<em>A</em><em><sub>T</sub></em><sub>=–</sub><em><sub>t</sub></em>=+<em>a</em>|<em>T</em>=+<em>t</em>, <em>A</em>=+<em>a</em>).</p>
</div>
</div>
<div class="readable-text" id="p334">
<h2 class="readable-text-h2" id="sigil_toc_id_256"><span class="num-string">10.8</span> Identification and probabilistic inference</h2>
</div>
<div class="readable-text" id="p335">
<p>We’ve seen that a core part of the identification task is deriving an estimand. How does that estimand mesh with a probabilistic machine learning approach?</p>
</div>
<div class="readable-text intended-text" id="p336">
<p>Consider, for example, our online game model, where ETT = <em>E</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=“high”</sub> – <em>I</em><sub><em>E</em></sub><sub>=“low”</sub><em> </em>|<em>E</em><em>   </em>=<em> </em>“high”) = <em>E</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=“high”</sub>|<em>E</em><em>  </em>=<em> </em>“high”) – <em>E</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=“low”</sub>|<em>E</em><em>  </em>=<em> </em>“high”). We need to identify <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=“high”</sub>|<em>E</em><em>  </em>=<em> </em>“high”) and <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=“low”</sub>|<em>E</em><em>   </em>=<em> </em>“high”). Recall that <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=“high”</sub>|<em>E</em><em>  </em>=<em> </em>“high”) simplifies to the level 1 query <em>P</em><em> </em>(<em>I</em><em>  </em>|<em>E</em><em> </em>=“high”) by the law of consistency, so the challenge lies with identifying the counterfactual distribution <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=“low”</sub>|<em>E</em><em>   </em>=“high”).</p>
</div>
<div class="readable-text intended-text" id="p337">
<p>Using a probabilistic machine learning approach with Pyro, we know we can infer <em>P</em><em> </em>(<em>I</em><em>  </em>|<em>E</em><em>   </em>=<em> </em>“high”) by using <code>pyro.condition</code> to condition on <em>E</em><em>  </em>=<em> </em>“high” and then running inference. The question is how we’ll infer the counterfactual distribution <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=“low”</sub>|<em>E</em><em>  </em>=<em> </em>“high”).</p>
</div>
<div class="readable-text intended-text" id="p338">
<p>In the previous section, we saw that we can identify this query with a SWIG (assuming the single-world assumption holds). We used the SWIG to derive the following estimand for <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=0</sub>=<em>i</em><em>  </em>|<em>E</em><em>  </em>=<em> </em>“high”):</p>
</div>
<div class="browsable-container figure-container" id="p339">
<img alt="figure" height="56" src="../Images/ness-ch10-eqs-7x.png" width="483"/>
</div>
<div class="readable-text" id="p340">
<p>But what do we do with this estimand with respect to building a model in Pyro? We could construct two Pyro models, one for <em>P</em>(<em>G</em><em> </em>|<em>E</em><em> </em>) and one for <em>P</em>(<em>I</em><em>  </em>|<em>G</em>, <em>E</em><em> </em>), infer <em>P</em><em> </em>(<em>I</em><em>   </em>=<em> </em><em>i</em><em>  </em>| <em>G</em><em>   </em>=<em> </em><em>g</em>, <em>E</em><em>   </em>=<em> </em>“low”) and <em>P</em><em> </em>(<em>G</em><em>   </em>=<em> </em><em>g</em> | <em>E</em><em>   </em>=<em> </em>“high”) and then do the summation. But this is inelegant relative to our regular approach to probabilistic inference with a causal generative model: </p>
</div>
<ol>
<li class="readable-text" id="p341"> Implement the full causal generative model. </li>
<li class="readable-text" id="p342"> Train its parameters on data. </li>
<li class="readable-text" id="p343"> Apply the intervention operator to simulate an intervention. </li>
<li class="readable-text" id="p344"> Run an inference algorithm. </li>
</ol>
<div class="readable-text" id="p345">
<p>In this approach, we build one causal model—we don’t build separate models for the estimand's components <em>P</em><em> </em>(<em>G</em><em>  </em>|<em>E</em><em> </em>) and <em>P</em><em> </em>(<em>I</em><em>   </em>|<em>G</em>, <em>E</em><em>  </em>). Nonetheless, our regular approach to probabilistic inference with a causal generative model does work if we have identification, given the causal assumptions we implement in step 1 and the data we train on in step 2. We don’t even need to know the estimand explicitly; it is enough to know it exists—in other words, that the query is identified (e.g., by using Y0’s <code>check_ identifiable</code> function). With identification, steps 2–4 collectively become just another estimator for that estimand.</p>
</div>
<div class="readable-text intended-text" id="p346">
<p>To illustrate, let’s consider how we’d sample from <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=“low”</sub>|<em>E</em><em>  </em>=<em> </em>“high”) using a Pyro model of our online gaming example. For simplicity, let’s replace <em>E</em><em>   </em>=<em> </em>“high” and <em>E</em><em>   </em>=<em> </em>“low” with <em>E</em><em>   </em>=<em> </em>1 and <em>E</em><em>   </em>=<em>  </em>0 respectively. We know <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=0</sub>|<em>E</em><em>  </em>=<em> </em>1) is identified given our causal DAG and the single-word assumption. Fortunately, Pyro’s (and ChiRho’s) <code>do</code> intervention operator implements the SWIG’s node-splitting operation by default (if you used <code>pyro.render_model</code> to visualize an intervention and didn’t get what you expected, this is why). For ordinary interventional queries on a causal DAG, there is no difference between this and the ordinary graph surgery approach to interventions. But when we want to condition on <em>E</em><em>   </em>=<em> </em>1 and intervene to set <em>E</em><em>   </em>=<em>  </em>0, Pyro will accommodate us. We’ll use this approach to sample from <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=0</sub>|<em>E</em><em>   </em>=<em> </em>1). As a sanity check, we’ll also sample from the plain vanilla intervention distribution <em>P</em>(<em>I</em><sub><em>E</em></sub><sub>=0</sub>) and contrast those samples with samples from <em>P</em>(<em>I</em><sub><em>E</em></sub><sub>=0</sub>|<em>E</em><em>  </em>=<em> </em>1).</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p347">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Setting up your environment</h5>
</div>
<div class="readable-text" id="p348">
<p>As a change in pace, I’ll illustrate this example using NumPyro instead of Pyro, though the code will work in Pyro with small tweaks. We’ll use NumPyro version 0.15.0. We’ll also use an inference library meant to complement NumPyro and Pyro called Funsor, version 0.4.5. We’ll also use Matplotlib for plotting.</p>
</div>
</div>
<div class="readable-text" id="p349">
<p>First, let’s build the model.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p350">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">NumPyro vs. Pyro</h5>
</div>
<div class="readable-text" id="p351">
<p>Pyro extends PyTorch, while NumPyro extends NumPy and automatic differentiation with JAX. The user interfaces are quite similar. If you are less comfortable with PyTorch abstractions and debugging PyTorch errors, or you prefer MCMC-based inference with the Bayesian programming patterns one uses in Stan or PyMC, then you might prefer NumPyro.</p>
</div>
</div>
<div class="browsable-container listing-container" id="p352">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.13</span> Generating from <em>P</em>(<em>I</em><em><sub>E</sub></em><sub>=</sub><sub>0</sub>) vs. <em>P</em>(<em>I</em><em><sub>E</sub></em><sub>=</sub><sub>0</sub>|<em>E</em>=1) in Pyro</h5>
<div class="code-area-container">
<pre class="code-area">import jax.numpy as np
from jax import random
from numpyro import sample
from numpyro.handlers import condition, do
from numpyro.distributions import Bernoulli, Normal
from numpyro.infer import MCMC, NUTS
import matplotlib.pyplot as plt

rng = random.PRNGKey(1)

def model():   <span class="aframe-location"/> #1
    p_member = 0.5  #1
    is_guild_member = sample(    #1
        "Guild Membership",    #1
        Bernoulli(p_member)   #1
    )    #1
    p_engaged = (0.8*is_guild_member + 0.2*(1-is_guild_member))    #1
    is_highly_engaged = sample(    #1
        "Side-quest Engagement",  #1
        Bernoulli(p_engaged)   #1
    )    #1
    p_won_engaged = (.9*is_highly_engaged + .1*(1-is_highly_engaged))    #1
    high_won_items = sample("Won Items", Bernoulli(p_won_engaged))    #1
    mu = (    #1
        37.95*(1-is_guild_member)*(1-high_won_items) +    #1
        54.92*(1-is_guild_member)*high_won_items +    #1
        223.71*(is_guild_member)*(1-high_won_items) +    #1
        125.50*(is_guild_member)*high_won_items    #1
    )   #1
    sigma = (   #1
        23.80*(1-is_guild_member)*(1-high_won_items) +    #1
        4.92*(1-is_guild_member)*high_won_items +    #1
        5.30*(is_guild_member)*(1-high_won_items) +    #1
        53.49*(is_guild_member)*high_won_items    #1
    )  #1
    norm_dist = Normal(mu, sigma)#1
    in_game_purchases = sample("In-game Purchases", norm_dist)    #1</pre>
<div class="code-annotations-overlay-container">
     #1 A version of the online gaming model. The weights are estimates from the data (learning procedure not shown here).
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p353">
<p>Next, we’ll apply the intervention and run inference to sample from <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=</sub><sub>0</sub>).</p>
</div>
<div class="browsable-container listing-container" id="p354">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.14</span> Apply intervention do(<em>E</em>=0) and infer from <em>P</em>(<em>I</em><em><sub>E</sub></em><sub>=</sub><sub>0</sub>)</h5>
<div class="code-area-container">
<pre class="code-area">intervention_model = do(    <span class="aframe-location"/> #1
    model,    #1
    {"Side-quest Engagement": np.array(0.)})    #1
intervention_kernel = NUTS(intervention_model)   <span class="aframe-location"/> #2
intervention_model_sampler = MCMC(    #2
    intervention_kernel,   #2
    num_samples=5000,  #2
    num_warmup=200     #2
)     #2
intervention_model_sampler.run(rng)    #2
intervention_samples = intervention_model_sampler.get_samples()    #2
int_purchases_samples = intervention_samples["In-game Purchases"] #2</pre>
<div class="code-annotations-overlay-container">
     #1 Apply the do operator to the model.
     <br/>#2 Apply inference to sample from P(I
     <sub>E=0</sub>).
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p355">
<p>We’ll contrast these samples from <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=</sub><sub>0</sub>) with samples we’ll draw from <em>P</em><em> </em>(<em>I</em><em><sub>E</sub></em><sub>=</sub><sub>0</sub>|<em>E</em><em>  </em>=<em> </em>1). To infer <em>P</em><em> </em>(<em>I</em><em><sub>E</sub></em><sub>=</sub><sub>0</sub><em> </em>|<em>E</em><em>   </em>=<em> </em>1), we’ll condition <code>intervention_model</code> on the factual condition <em>E</em><em>   </em>=<em> </em>1. Then we’ll run inference again on this conditioned-upon intervened-upon model.</p>
</div>
<div class="browsable-container listing-container" id="p356">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.15</span> Condition intervention model and infer <em>P</em>(<em>I</em><em><sub>E</sub></em><sub>=</sub><sub>0</sub>|<em>E</em>=1)</h5>
<div class="code-area-container">
<pre class="code-area">cond_and_int_model = condition(   <span class="aframe-location"/> #1
    intervention_model,   #1
     {"Side-quest Engagement": np.array(1.)}   #1
)    #1
int_cond_kernel = NUTS(cond_and_int_model)   <span class="aframe-location"/> #2
int_cond_model_sampler = MCMC(     #2
    int_cond_kernel,     #2
    num_samples=5000,     #2
    num_warmup=200   #2
)     #2
int_cond_model_sampler.run(rng)     #2
int_cond_samples = int_cond_model_sampler.get_samples()   #2
int_cond_purchases_samples = int_cond_samples["In-game Purchases"]  #2</pre>
<div class="code-annotations-overlay-container">
     #1 Now apply the condition operator to sample from P(I
     <sub>E=0</sub>|E=1).
     <br/>#2 Apply inference to sample from P(I
     <sub>E=0</sub>|E=1).
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p357">
<p>Note that Pyro’s <code>do</code><em> </em>and <code>condition</code><em> </em>subroutines mutually compose; i.e., for a model with a variable <em>X</em>, <code>do(condition(model,</code> <code>{"X": 1.}),</code> <code>{"X":</code> <code>0.})</code> is equivalent to <code>condition(do(model,</code> <code>{"X":</code> <code>0.}),</code> <code>{"X":</code> <code>1.})</code>.</p>
</div>
<div class="readable-text intended-text" id="p358">
<p>Finally, we’ll plot samples from <em>P</em><em> </em>(<em>I</em><em><sub>E</sub></em><sub>=0</sub>) and <em>P</em><em> </em>(<em>I</em><em><sub>E</sub></em><sub>=0</sub>|<em>E</em><em>   </em>=<em> </em>1) and evaluate the difference in these distributions.</p>
</div>
<div class="browsable-container listing-container" id="p359">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.16</span> Plot samples from <em>P</em>(<em>I</em><em><sub>E</sub></em><sub>=</sub><sub>0</sub>) and <em>P</em>(<em>I</em><em><sub>E</sub></em><sub>=</sub><sub>0</sub>|<em>E</em>=1)</h5>
<div class="code-area-container">
<pre class="code-area">plt.hist(    <span class="aframe-location"/> #1
    int_purchases_samples,   #1
    bins=30,    #1
    alpha=0.5,    #1
    label='$P(I_{E=0})$'   #1
)     #1
plt.hist(    <span class="aframe-location"/> #2
    int_cond_purchases_samples,    #2
    bins=30,     #2
    alpha=0.5,     #2
    label='$P(I_{E=0}|E=1)$'     #2
)     #2
plt.legend(loc='upper left')    #2
plt.show()    #2</pre>
<div class="code-annotations-overlay-container">
     #1 Plot a histogram of samples from P(I
     <sub>E=0</sub>).
     <br/>#2 Plot a histogram of samples from P(I
     <sub>E=0</sub>|E=1).
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p360">
<p>This code generates the histograms in figure 10.24.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p361">
<img alt="figure" height="811" src="../Images/CH10_F24_Ness.png" width="1100"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.24</span> Histograms of samples from <em>P</em>(<em>I</em><em><sub>E</sub></em><sub>=0</sub>) and <em>P</em>(<em>I</em><em><sub>E</sub></em><sub>=0</sub>|<em>E</em>=1) generated in Pyro</h5>
</div>
<div class="readable-text" id="p362">
<p>In this example, the parameters were given. In chapter 11, where we’ll look at estimation, we’ll seamlessly combine this query inference with Bayesian parameter inference from data.</p>
</div>
<div class="readable-text" id="p363">
<h2 class="readable-text-h2" id="sigil_toc_id_257"><span class="num-string">10.9</span> Partial identification</h2>
</div>
<div class="readable-text" id="p364">
<p>We’ll close this chapter with a quick note on partial identification. Sometimes a query is not identified, given your assumptions, but it may be <em>partially identifiable</em>. Partial identifiability means you can identify estimands for an upper and lower bound of your query. Partial identification is highly relevant to causal AI because machine learning algorithms often rely on finding and optimizing bounds on objective functions. Let’s walk through a few examples.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p365">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">MCMC vs. SVI</h5>
</div>
<div class="readable-text" id="p366">
<p>Here we used Markov chain Monte Carlo (MCMC), but both Pyro and NumPyro have abstractions for stochastic variational inference (SVI). In this example, the parameters (<code>p_member</code>, <code>p_engaged</code>, etc.) of the model are specified. We could also make the parameters unknown variables with Bayesian priors and do the inference on these causal queries <em>P</em>(<em>I</em><em><sub>E</sub></em><sub>=0</sub>) and <em>P</em>(<em>I</em><em><sub>E</sub></em><sub>=0</sub>|<em>E</em>=1); in this case, we’d be doing Bayesian inference of these queries.</p>
</div>
<div class="readable-text" id="p367">
<p>But for this we’d need <em>N</em> IID samples from an observational distribution where we had graphical identification (<em>P</em>(<em>G</em>, <em>E</em>, <em>W</em>, <em>I</em>), <em>P</em>(<em>G</em>, <em>E</em>, <em>I</em>), or <em>P</em>(<em>E</em>, <em>W</em>, <em>I</em>)). In the case of <em>P</em>(<em>G</em>, <em>E</em>, <em>W</em>, <em>I</em>), where all the variables in the DAG are observed, the number of unknown variables is just the number of parameters. But in the latter two cases, of <em>P</em>(<em>G</em>, <em>E</em>, <em>I</em>) or <em>P</em>(<em>E</em>, <em>W</em>, <em>I</em>), where there is a latent <em>G</em><em> </em>or <em>W</em> of size <em>N</em>, the number of unknowns grows with <em>N</em>. In this case, SVI will scale better with large <em>N</em>. We’ll see an example in chapter 11.</p>
</div>
</div>
<div class="readable-text" id="p368">
<p>Suppose in our online gaming example you ran an experiment where you randomly assigned players to a treatment or control group. Players in the treatment group are exposed to a policy that encourages more side-quest engagement. You reason that since you can’t actually force players to engage in side-quests, it’s better to have this randomized treatment/control variable as a parent of our side-quest engagement variable, as seen in the DAG in figure 10.25.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p369">
<img alt="figure" height="231" src="../Images/CH10_F25_Ness.png" width="844"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.25</span> We don’t have identification for the ATE of <em>E</em> on <em>I</em> because <em>G</em> and <em>W</em> are unobserved. But we have partial identification given variable <em>A</em>, representing gamers’ assignments in a randomized experiment.</h5>
</div>
<div class="readable-text" id="p370">
<p>For this new variable <em>A</em>, let <em>A</em><em>  </em>=<em> </em>1 refer to the treatment group and <em>A</em>=0 refer to the control group. We have this new variable <em>A</em>, and the average treatment effect of the policy on in-game purchases <em>E</em>(<em>I</em><sub><em>A</em></sub><sub>=1</sub> – <em>I</em><sub><em>A</em></sub><sub>=0</sub>) is an interesting query. But suppose we’re still ultimately interested in knowing the average treatment effect of side-quest engagement <em>itself </em>on purchases, i.e., <em>E</em>(<em>I</em><sub><em>E</em></sub><sub>=“high”</sub> – <em>I</em><sub><em>E</em></sub><sub>=“low”</sub>).</p>
</div>
<div class="readable-text intended-text" id="p371">
<p>If guild membership (<em>G</em><em> </em>) were observed, we’d have identification through backdoor adjustment. If won items (<em>W</em><em>  </em>) were observed, we could use front-door adjustment. But suppose that in this scenario you observe neither. In this case, observing the side-quest group assignment variable would give you partial identification. Suppose that the in-game purchases variable was a binary 1 for “high” and 0 for “low” instead of a continuous value. Then the bounds on <em>E</em>(<em>I</em><sub><em>E</em></sub><sub>=“high” </sub>– <em>I</em><sub><em>E</em></sub><sub>=“low”</sub>) are<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p372">
<img alt="figure" height="134" src="../Images/ness-ch10-eqs-8x.png" width="679"/>
</div>
<div class="readable-text" id="p373">
<p>These bounds can be the next best thing to having full identification, especially if the bounds are tight. Alternatively, perhaps it is enough to know that the lower bound on the ATE for side-quest engagement is significantly greater than 0.</p>
</div>
<div class="readable-text intended-text" id="p374">
<p>Similarly, general bounds exist for common counterfactual queries, such as probabilities of causation. For example, suppose you wanted to know if high side-quest engagement was a necessary and sufficient condition of high in-game purchases. You can construct the following bounds on the probability of necessity and sufficiency (PNS): <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p375">
<img alt="figure" height="98" src="../Images/ness-ch10-eqs-9x.png" width="572"/>
</div>
<div class="readable-text" id="p376">
<p>These bounds consist of level 2 quantities like <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=e</sub>=<em> </em><em>i</em><em> </em>), and you can go on to identify level 1 estimands if possible given your assumptions.</p>
</div>
<div class="readable-text intended-text" id="p377">
<p>Remember that partial identification bounds are highly specific to your causal assumptions (like the DAG) and the parameterization of the variables; for example, the preceding examples are specific to binary variables. See the chapter notes at <a href="https://www.altdeep.ai/p/causalaibook">https://www.altdeep.ai/p/causalaibook</a> for links to papers that derived these bounds as well as bounds for other practical sets of assumptions.</p>
</div>
<div class="readable-text" id="p378">
<h2 class="readable-text-h2" id="sigil_toc_id_258">Summary</h2>
</div>
<ul>
<li class="readable-text" id="p379"> The importance of causal identification has increased in the AI era as we seek to understand the causal inductive bias in deep learning architectures. </li>
<li class="readable-text" id="p380"> Libraries like y0 implement strategies for algorithmic identification. </li>
<li class="readable-text" id="p381"> The causal hierarchy is a three-tiered structure that categorizes the causal questions we pose, the models we develop, and the causal inferences we draw. These levels are association, intervention, and counterfactual. </li>
<li class="readable-text" id="p382"> Association-level reasoning addresses “what is” questions and models that answer these questions with basic statistical assumptions. </li>
<li class="readable-text" id="p383"> Interventional or counterfactual queries fall on their corresponding level of the hierarchy. </li>
<li class="readable-text" id="p384"> Observational data falls on the associational level, and experimental data falls on the interventional level of the hierarchy. Counterfactual data arises in situations where the modeler can control a deterministic data generating process (DGP). </li>
<li class="readable-text" id="p385"> Causal identification is the procedure of discerning when causal inferences can be drawn from experimental or observational data. It is done by determining if data at a lower level of the hierarchy can be used to infer a query at a higher level of the hierarchy. </li>
<li class="readable-text" id="p386"> An example of a causal identification result is the backdoor formula, which equates intervention level query <em>P</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub>) to association level quantity <span class="regular-symbol">∑</span><sub><em>z</em></sub><em>P</em><em> </em>(<em>Y</em><em>  </em>|<em>X</em><em>  </em>=<em> </em><em>x</em>, <em>Z</em><em>  </em>=<em> </em>z)<em>P</em><em> </em>(<em>Z</em><em>   </em>=<em> </em>z), where <em>Z</em> is a set of common causes. </li>
<li class="readable-text" id="p387"> The causal hierarchy theorem shows how lower-level data is insufficient to infer a distribution at a higher level without higher-level modeling assumptions. </li>
<li class="readable-text" id="p388"> The do-calculus has three rules that can be used for graph-based identification. </li>
<li class="readable-text" id="p389"> A counterfactual graph is a DAG that includes variables across counterfactual worlds on one graph. Unlike the parallel world graph, it admits d-separation. We derive the counterfactual graph from the parallel world graph and the target query. </li>
<li class="readable-text" id="p390"> Graphical identification algorithms automate identification with graphs using rules such as the do-calculus. </li>
<li class="readable-text" id="p391"> Nonparametric identification is identification with non-graphical assumptions, such as assumptions about the functional relationships between variables in the model. </li>
<li class="readable-text" id="p392"> The ignorability assumption is that the causal variable and the potential outcomes are conditionally independent given confounders. </li>
<li class="readable-text" id="p393"> Effect of treatment on the treated (ETT) evaluates the effect of a cause on the subset of the population that was exposed to the cause. </li>
<li class="readable-text" id="p394"> Single world intervention graphs (SWIGs) provide an intuitive alternative to counterfactual identification with do-calculus and counterfactual graphs. They are constructed by applying a node-splitting operation to the original causal DAG. SWIGs use a “single-world” assumption, which assumes it’s possible to know a variable’s natural value while also intervening on it before it realizes that value without any side-effects that would affect that natural value. </li>
<li class="readable-text" id="p395"> SWIGs work with variables and a narrow set of counterfactuals under the single-world assumption, while counterfactual graphs can accommodate queries that cannot be graphically identified. </li>
<li class="readable-text" id="p396"> Pyro implements the SWIG’s node-splitting model of intervention, which enables probabilistic inference of SWIG-identified quantities. </li>
<li class="readable-text" id="p397"> Inference of causal queries using a causal graphical model and probabilistic inference algorithms is possible as long as the query is identified, given the model’s assumptions and training data. </li>
<li class="readable-text" id="p398"> Partial identification means you can at least identify estimands for bounds on a target query. This can be quite useful if you lack full identification, especially since machine learning often works by optimizing bounds on objective functions. </li>
</ul>
</div></body></html>