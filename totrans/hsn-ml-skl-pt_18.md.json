["```py\nimport torch\nimport torch.nn as nn\n\nclass PatchEmbedding(nn.Module):\n    def __init__(self, in_channels, embed_dim, patch_size=16):\n        super().__init__()\n        self.conv2d = nn.Conv2d(embed_dim, in_channels,\n                                kernel_size=patch_size, stride=patch_size)\n    def forward(self, X):\n        X = self.conv2d(X)  # shape [B=Batch, C=Channels, H=Height, W=Width]\n        X = X.flatten(start_dim=2)  # shape [B, C, H * W]\n        return X.transpose(1, 2)  # shape [B, H * W, C]\n```", "```py\nclass ViT(nn.Module):\n    def __init__(self, img_size=224, patch_size=16, in_channels=3,\n                 num_classes=1000, embed_dim=768, depth=12, num_heads=12,\n                 ff_dim=3072, dropout=0.1):\n        super().__init__()\n        self.patch_embed = PatchEmbedding(embed_dim, in_channels, patch_size)\n        cls_init = torch.randn(1, 1, embed_dim) * 0.02\n        self.cls_token = nn.Parameter(cls_init)  # shape [1, 1, E=embed_dim]\n        num_patches = (img_size // patch_size) ** 2  # num_patches (denoted L)\n        pos_init = torch.randn(1, num_patches + 1, embed_dim) * 0.02\n        self.pos_embed = nn.Parameter(pos_init)  # shape [1, 1 + L, E]\n        self.dropout = nn.Dropout(p=dropout)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=embed_dim, nhead=num_heads, dim_feedforward=ff_dim,\n            dropout=dropout, activation=\"gelu\", batch_first=True)\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n        self.layer_norm = nn.LayerNorm(embed_dim)\n        self.output = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, X):\n        Z = self.patch_embed(X)  # shape [B, L, E]\n        cls_expd = self.cls_token.expand(Z.shape[0], -1, -1)  # shape [B, 1, E]\n        Z = torch.cat((cls_expd, Z), dim=1)  # shape [B, 1 + L, E]\n        Z = Z + self.pos_embed\n        Z = self.dropout(Z)\n        Z = self.encoder(Z)  # shape [B, 1 + L, E]\n        Z = self.layer_norm(Z[:, 0])  # shape [B, E]\n        logits = self.output(Z) # shape [B, C]\n        return logits\n```", "```py\nvit_model = ViT(\n    img_size=224, patch_size=16, in_channels=3, num_classes=1000, embed_dim=768,\n    depth=12, num_heads=12, ff_dim=3072, dropout=0.1)\nbatch = torch.randn(4, 3, 224, 224)\nlogits = vit_model(batch)  # shape [4, 1000]\n```", "```py\nfrom datasets import load_dataset\n\npets = load_dataset(\"timm/oxford-iiit-pet\")\n```", "```py\nfrom transformers import ViTForImageClassification, AutoImageProcessor\n\nmodel_id = \"google/vit-base-patch16-224-in21k\"\nvit_model = ViTForImageClassification.from_pretrained(model_id, num_labels=37)\nvit_processor = AutoImageProcessor.from_pretrained(model_id, use_fast=True)\n```", "```py\ndef vit_collate_fn(batch):\n    images = [example[\"image\"] for example in batch]\n    labels = [example[\"label\"] for example in batch]\n    inputs = vit_processor(images, return_tensors=\"pt\", do_convert_rgb=True)\n    inputs[\"labels\"] = torch.tensor(labels)\n    return inputs\n```", "```py\nfrom transformers import Trainer, TrainingArguments\n\nargs = TrainingArguments(\"my_pets_vit\", per_device_train_batch_size=16,\n                         eval_strategy=\"epoch\", num_train_epochs=3,\n                         remove_unused_columns=False)\ntrainer = Trainer(model=vit_model, args=args, data_collator=vit_collate_fn,\n                  train_dataset=pets[\"train\"], eval_dataset=pets[\"test\"])\ntrain_output = trainer.train()\n```", "```py\nfrom transformers import pipeline\n\nmodel_id = \"openai/clip-vit-base-patch32\"\nclip_pipeline = pipeline(task=\"zero-shot-image-classification\", model=model_id,\n                         device_map=\"auto\", dtype=\"auto\")\ncandidate_labels = [\"cricket\", \"ladybug\", \"spider\"]\nimage_url = \"https://homl.info/ladybug\"  # a photo of a ladybug on a dandelion\nresults = clip_pipeline(image_url, candidate_labels=candidate_labels,\n                        hypothesis_template=\"This is a photo of a {}.\")\n```", "```py\n[{'score': 0.9972853660583496, 'label': 'ladybug'},\n {'score': 0.0016511697322130203, 'label': 'spider'},\n {'score': 0.0010634352220222354, 'label': 'cricket'}]\n```", "```py\nimport PIL\nimport urllib.request\nfrom transformers import CLIPProcessor, CLIPModel\n\nclip_processor = CLIPProcessor.from_pretrained(model_id)\nclip_model = CLIPModel.from_pretrained(model_id)\nimage = PIL.Image.open(urllib.request.urlopen(image_url)).convert(\"RGB\")\ncaptions = [f\"This is a photo of a {label}.\" for label in candidate_labels]\ninputs = clip_processor(text=captions, images=[image], return_tensors=\"pt\",\n                        padding=True)\nwith torch.no_grad():\n    outputs = clip_model(**inputs)\n\ntext_features = outputs.text_embeds    # shape [3, 512]  # 3 captions\nimage_features = outputs.image_embeds  # shape [1, 512]  # 1 image (ladybug)\n```", "```py\n>>> similarities = image_features @ text_features.T  # shape [1, 3] `>>>` `similarities` `` `tensor([[0.2337, 0.3021, 0.2381]])` ``\n```", "```py```", "````` This works because matrix multiplication computes the dot products of every row vector in the first matrix with every column vector in the second, and each dot product is equal to the cosine of the angle between the vectors multiplied by the norms of the vectors. Since the vectors have been ℓ[2] normalized in this case, the norms are equal to 1, so the result is just the cosine of the angle, which is the similarity score we’re after. As you can see, the most similar representation is the second one, for the ladybug class. If you prefer estimated probabilities rather than similarity scores, you must first rescale the similarities using the model’s learned temperature, then pass the result through the softmax function (it’s nice to see that we get the same result as the pipeline):    ```py >>> temperature = clip_model.logit_scale.detach().exp() `>>>` `rescaled_similarities` `=` `similarities` `*` `temperature` ```` `>>>` `probabilities` `=` `torch``.``nn``.``functional``.``softmax``(``rescaled_similarities` `,` `dim``=``1``)` ```py `>>>` `probabilities` `` `tensor([[0.0011, 0.9973, 0.0017]])` `` ``` ```py` ```   ```py `` `CLIP wasn’t the only surprise OpenAI had in stock in 2021\\. Just the following month, OpenAI announced DALL·E, which can generate impressive images given a text description. Let’s discuss it now.` `` ``` ```py`` `````", "``````py``` ``````", "``````py` ## DALL·E: Generating Images from Text Prompts    OpenAI [DALL·E](https://homl.info/dalle),⁠^([28](ch16.html#id3883)) released in February 2021, is a model capable of generating images based on text prompts, such as “an armchair in the shape of an avocado”. Its architecture is quite simple (see the lefthand side of [Figure 16-14](#dalle_diagram)): a GPT-like model trained to predict the next token, but unlike GPT, it was pretrained on millions of image-caption pairs, and fed input sequences composed on text tokens followed by visual tokens. At inference time, you only feed it the text tokens, and the model then generates the visual tokens, one at a time, until you get the full image. The visual tokens are generated by a dVAE model, which takes an image and outputs a sequence of tokens from a fixed vocabulary. Sadly, the model was never released to the public, but the paper was detailed enough so some open source replications are available, such as [DALL·E mini](https://huggingface.co/dalle-mini), also known as Craiyon.    One year later, in April 2022, OpenAI released [DALL·E 2](https://homl.info/dalle2),⁠^([29](ch16.html#id3886)) able to generate even higher quality images. Its architecture is actually very different: the text is fed to a CLIP model which outputs a text embedding, then this text embedding is fed to a *diffusion model* which uses it to guide its image generation process (we will discuss diffusion models in [Chapter 18](ch18.html#autoencoders_chapter)). The model is not open source, but it’s available through a paid API, and via some products such as Microsoft Designer, Bing Image Creator, Canva, ChatGPT, and more.  ![Diagram comparing the architectures of DALL·E and DALL·E 2, illustrating the transition from text embedding via a decoder and dVAE (for DALL·E) to CLIP and diffusion models (for DALL·E 2) in generating the image of a Chinese tower.](assets/hmls_1614.png)  ###### Figure 16-14\\. DALL·E (left) and DALL·E 2 (right)    DALL·E 3 was released in October 2023\\. Sadly, by then OpenAI had fully shifted away from its initial openness: there was no peer-reviewed paper, no code, no weights, no data. Like the previous version, DALL·E 3 is available through an API and via some products. We know it’s diffusion-based, it doesn’t use CLIP, and it’s tightly integrated with GPT-4, which rewrites the prompt before generating the image. It works impressively well: it outputs stunning images which match the prompts much more precisely than previous versions. The difference is particularly striking for *compositional prompts* (e.g., “A fluffy white cat sitting on a red velvet cushion, with a vase of sunflowers behind it, bathed in golden hour light. The cat is looking directly at the viewer”.). DALL·E 1 and 2 would generally follow only one or two elements of such prompts, whereas DALL·E 3 follows instructions much more closely. The image quality, realism, artistic style, and consistency are astounding. Lastly, DALL·E 3 also integrates some moderation capabilities.    The next landmark in our multimodal journey came one month after the first DALL·E model: the Perceiver.    ## Perceiver: Bridging High-Resolution Modalities with Latent Spaces    Every transformer so far has required chopping the inputs into meaningful tokens. In the case of text, tokens represent words or subwords. In the case of ViTs, they represent 16 × 16 pixel patches. In VideoBERT, it’s short 1.5-second clips. In audio transformers, it’s short audio clips. If we fed individual characters, pixels, or audio frames directly into a transformer, the input sequence would be extremely long, and we would run into the quadratic attention problem. Also, we would lose important inductive biases: for example, by chopping an image into patches, we enforce a strong inductive bias toward proximity (i.e., nearby pixels are assumed to be more strongly correlated than distant pixels).    However, such tokenization is modality-specific, which makes it harder to deal with new modalities or mix them in the model. Moreover, inductive biases are great when you don’t have a lot of training data (assuming the biases are correct), but if your dataset is large, you will often get better performance by using unbiased models with very few implicit assumptions. Sure, the model will have to figure out on its own that nearby pixels are generally related, but on the other hand, it will be flexible enough to discover patterns that might otherwise go unnoticed.    This is why DeepMind introduced the [*Perceiver*](https://homl.info/perceiver)⁠^([30](ch16.html#id3898)) in March 2021\\. This architecture is capable of directly handling any modality at the lowest level: characters, pixels, audio frames, and more. Moreover, it does so with a modality-agnostic design, so the same model can handle different modalities. The Perceiver architecture is shown in [Figure 16-15](#perceiver_diagram).  ![Diagram of the Perceiver architecture showing the flow of input pixels through Fourier positional encoding, linear encoding, and conversion into pixel tokens, which interact with latent tokens through cross-attention layers, leading to a classification head.](assets/hmls_1615.png)  ###### Figure 16-15\\. Perceiver architecture: inputs are ingested through cross-attention layers, while the main input is a sequence of learned latent tokens    Let’s walk through this architecture:    *   The input is first chopped into its smallest constituents. In this example, the input is an image, so it is chopped into individual pixels: we now have a sequence of 3D vectors (red, green, blue).           *   Positional encodings are concatenated to these feature vectors. Perceiver uses Fourier positional encodings, which are very similar to the sinusoidal positional encodings of the original Transformer, except they encode all of the input’s dimensions. Since an image is 2D, each pixel’s horizontal and vertical coordinates are encoded; for example, if a pixel is located at coordinates *x* and *y* (normalized between –1 and 1), then the positional encoding vector will include *x* and *y*, followed by sin(π_fx_), sin(π_fy_), and cos(π_fx_), cos(π_fy_) repeated *K* times (typically 6) with the frequency *f* starting at 1 and going up to *μ* / 2 (spaced equally), where *μ* is the target resolution (e.g., if the image is 224 × 224 pixels, then *μ* = 224).⁠^([31](ch16.html#id3901)) The dimensionality of the positional encoding vector is *d*(2_K_ + 1), where *d* is the number of input dimensions (i.e., 1 for audio, 2 for images, 3 for videos, etc.).           *   The pixel tokens now have 3 + 2 × (2 × 6 + 1) = 29 dimensions. We then pass them through a linear layer to project them to the Perceiver’s dimensionality (e.g., 512).           *   The Perceiver’s architecture itself is composed of repeated processing blocks (e.g., eight), where each block is composed of a single cross-attention multi-head attention layer (MHA) followed by a regular transformer encoder (e.g., with six encoder layers). The final block is composed of a single cross-attention MHA layer and an average pooling layer to reduce the input sequence into a single vector, which is then fed to a classification head (i.e., linear plus softmax).           *   The pixel tokens are fed to the Perceiver exclusively through the MHA layers, and they play the role of the keys and values. In other words, the Perceiver attends to the pixel tokens through cross-attention only.           *   Crucially, the Perceiver’s main input is a fairly short sequence of *latent tokens* (e.g., 512). These tokens are similar to an RNN’s hidden state: an initial sequence (learned during training) is fed to the Perceiver, and it gradually gets updated as the model learns more and more about the pixel tokens via cross-attention. Since it’s a short sequence, it doesn’t suffer much from the quadratic attention problem. This is called the *latent bottleneck trick*, and is the key to the success of the Perceiver.           *   The authors experimented sharing weights across processing blocks (excluding the first cross-attention layer), and they got good results. When the processing blocks share the same weights, the Perceiver is effectively a recurrent neural network, and the latent tokens really are its hidden state.              ###### Note    As we saw in [Chapter 7](ch07.html#dimensionality_chapter), the manifold hypothesis states that most real-world data lives near a low-dimensional manifold, much like a rolled piece of paper lives in 3D but is essentially a 2D object. This 2D space is latent (i.e., hidden, potential) until we unroll the paper. Similarly, the Perceiver’s goal is to “unroll” its high-dimensional inputs so the model can work in the latent space, using low-dimensional representations.    Importantly, this architecture can efficiently process high-resolution inputs. For example, a 224 × 224 image has 50,176 pixels, so if we tried to feed such a long sequence of pixel tokens directly to a regular encoder, each self-attention layer would have to compute 50,176² ≈ 2.5 billion attention scores! But since the Perceiver only attends to the pixel tokens through cross-attention, it just needs to compute 50,176 times the number of latent tokens. Even for the biggest Perceiver variant, that’s just a total of 50,176 × 512 ≈ 25.7 million attention scores, which is roughly 100 times less compute.    ###### Note    Thanks to the latent bottleneck, the Perceiver scales linearly with the number of pixel tokens, instead of quadratically.    The authors trained the Perceiver using regular supervised learning on various classification tasks across several modalities, including image-only (ImageNet), audio plus video (AudioSet),⁠^([32](ch16.html#id3910)) or point clouds (ModelNet40),⁠^([33](ch16.html#id3911)) all using the same model architecture. They got competitive results, in some cases even reaching the state of the art.    The videos in the AudioSet dataset were downsampled to 224 × 224 pixels at 25 frames per second (fps), with a 48 kHz audio sample rate. You could theoretically feed each pixel and each audio frame individually to the Perceiver, but this would be a bit extreme, as each 10s video would be represented as a sequence of 224 × 224 × 25 × 10 ≈ 12.5 million pixel tokens, and 48,000 × 10 = 480,000 audio tokens.    So the authors had to compromise. They trained on 32-frame clips (at 25 fps, that’s 1.28s each, instead of 10s) and they chopped the video into 2 × 8 × 8 patches (i.e., 2 frames × 8 × 8 pixels), resulting in 224 × 224 × 32 / (2 × 8 × 8) = 12,544 video tokens of 128 RGB pixels each (plus the position encoding). They also chopped the audio into clips of 128 frames each, resulting in 480 audio tokens. They also tried converting the audio to a mel spectrogram (which resulted in 4,800 audio tokens). Using a spectrogram instead of raw audio is a standard practice in audio processing, but it made very little difference to the model’s performance, which shows that the Perceiver is able to extract useful features from the raw data without any help.    Then they simply concatenated the video and audio token sequences (after positional encoding), and also concatenated a modality embedding to help the model distinguish the modalities.    One limitation of the Perceiver architecture is that it was only designed for multimodal classification. That said, instead of averaging the latent tokens and feeding them to a classification head, we could try to use them for other downstream tasks. Of course, the DeepMind researchers thought of that, and just a few months later they published the Perceiver IO architecture.    ## Perceiver IO: A Flexible Output Mechanism for the Perceiver    DeepMind released [Perceiver IO](https://homl.info/perceiverio) in July 2021.⁠^([34](ch16.html#id3915)) It can perform classification tasks like the Perceiver, but also many other tasks such as masked language modeling (MLM) better than BERT, *optical flow* (i.e., predicting where each pixel will move in the next video frame), actually beating the state of the art, and even playing StarCraft II.    The model is identical to Perceiver up to the output latent tokens, but the pooling layer and the classification head are replaced by a very flexible output mechanism (see [Figure 16-16](#perceiverio_diagram)):    *   A new cross-attention layer is added, which acts as a decoder by attending to the output latent tokens and producing the final output representations. These output representations can then go through a task-specific head, or even multiple heads if we’re doing multitask learning.           *   The number and nature of the output tokens is task-specific:               *   For classification, we only need one output vector, which we can feed to a classification head. Therefore, we need one output query token, which can just be a learned embedding.                       *   For masked language modeling, we can use one output query token per masked token, and add a classification head on top of the output representations (i.e., linear plus softmax) to get one estimated token probability for each masked token. To help the model locate each masked token, the output query tokens are learnable positional embeddings based on the masked token’s position. For example, given the masked sentence “The dog [MASK] the [MASK]”, the masked tokens are located at positions #2 and #4, so we use the positional embedding #2 as the first output query token, and #4 as the second output query token. This same approach works for any other modality: just predict the masked tokens. It can also be extended to multiple modalities at once, typically by adding a modality embedding to the output query token before feeding it to the output cross-attention layer.                       *   For optical flow, the authors actually used one output token per pixel, using the same pixel representations both as the inputs to the Perceiver and as the output query tokens. This representation includes a Fourier positional encoding.                    ![Diagram of Perceiver IO architecture showing input tokens processed into latent tokens, which connect through a cross-attention layer to output query tokens, leading to task-specific head(s).](assets/hmls_1616.png)  ###### Figure 16-16\\. Perceiver IO architecture: one output query token per desired output token is fed to a cross-attention layer that attends to the Perceiver’s output latent tokens    ###### Note    Because the output query tokens only ever attend to the latent tokens, the Perceiver IO can handle a very large number of output query tokens. The latent bottleneck allows the model to scale linearly for both the inputs and outputs.    The Perceiver IO is a bidirectional architecture; there’s no causal masking, so it’s not well suited for autoregressive tasks. In particular, it cannot efficiently perform next token prediction, so it’s not well suited for text generation tasks such as image captioning. Sure, you could feed it an image and some text with a mask token at the end, and make it predict which token was masked, then start over to get the next token, and so on, but it would be horribly inefficient compared to a causal model (which can cache the previous state).    For this reason, Google and DeepMind researchers released the [Perceiver AR architecture](https://homl.info/perceiverar) in February 2022 to address this limitation (AR stands for autoregressive). The model works very much like the Perceiver, except the last tokens of the input sequence are used as the latent tokens, the model is causal over these latent tokens, and it is trained using next token prediction. Perceiver AR didn’t quite have the same impact as Perceiver and Perceiver IO, but it got excellent results on very long input sequences, thanks to its linear scaling capability.    But DeepMind researchers weren’t done with multimodal ML; they soon released yet another amazing multimodal model, partly based on the Perceiver: Flamingo.    ## Flamingo: Open-Ended Visual Dialogue    DeepMind’s [Flamingo paper](https://homl.info/flamingo), published in April 2022, introduced a visual-language model (VLM) that can take arbitrary sequences of text and images as input and generate coherent free-form text. Most importantly, its few-shot performance is excellent on a wide variety of tasks.    For example, suppose you want to build a model that takes a picture and outputs a poem about that image: no need to train a new model; you can just feed a few examples to Flamingo, add the new image at the end, and it will happily generate a poem about this new image. If you want it to detect license plate numbers on car photos, just give it a few photos along with the corresponding license plate numbers (as text), then add a new car photo, and Flamingo will output its license plate number. You can just as easily use Flamingo for image captioning. Or visual question answering. Or you can ask it to compare two images. In fact, you can even give the model several frames from a video and ask it to describe the action. It’s an incredibly versatile and powerful model out of the box, without any fine-tuning.    Let’s look at Flamingo’s architecture (see [Figure 16-17](#flamingo_diagram)):    *   Instead of starting from scratch, Flamingo is based on two large pretrained models, which are both frozen: a vision model and a decoder-only language model. The authors used Chinchilla and CLIP, respectively, but many other powerful models would work fine too.           *   Each input image is fed to the vision model, and the outputs go through a Perceiver model, called a *Resampler*, which produces a sequence of latent token representations. This ensures that every image gets represented as a fairly short sequence of latent representations (typically much shorter than the output of the vision model). This works around the quadratic attention problem.           *   The sequences output by the Resampler are fed as the keys/values to many *gated xattn-dense* modules, which are inserted before every block in the frozen LLM:               *   Each gated xattn-dense module is composed of a masked multi-head attention layer followed by a feedforward module, with a skip connection each, just like the cross-attention half of a vanilla Transformer’s decoder layer.                       *   However, both the masked MHA layer and the feedforward module are followed by a *tanh gate*. These gates multiply their input by tanh(*α*), where *α* is a learnable scalar parameter initialized to 0 (one per gate). Since tan(0) = 0, training starts with all gates closed, so the inputs can only flow through the skip connections, and the gated xattn-dense modules have no impact on the LLM. But as training progresses, the model gradually learns to open the gates, allowing the gated modules to influence the LLM’s outputs.                       *   In the gated xattn-dense module, each text token can only attend to visual tokens from the closest image located before it; visual tokens from all other images are masked. For example, the last text token (“is”) can only attend to the Chinese tower photo, it cannot directly attend to the flower photo. However, since previous text tokens have information about the flower photo, the last token does have indirect access to the flower photo via the frozen LLM’s self-attention layers.                   *   The text is tokenized as the LLM expects (e.g., Chinchilla expects start-of-sequence and end-of-sequence tokens, which I denoted as <s> and </s>), but a couple new special tokens are added. Each image-text chunk ends with an end-of-chunk token (which I denoted as </c>), and each image is replaced with an image token (which I denoted as <i>). Both are represented using trainable embeddings.            ![Diagram illustrating the Flamingo model architecture, showing the flow of image and text inputs through a vision encoder, Resampler, and gated xattn-dense modules, which integrate into the LLM blocks.](assets/hmls_1617.png)  ###### Figure 16-17\\. Flamingo takes any sequence of text and images, and outputs coherent free-form text    The bad news is that DeepMind did not release Flamingo to the public. The good news is that open source replications and variants are available:    *   [OpenFlamingo](https://homl.info/openflamingo), created by the MLFoundations team, which is part of the non-profit organization LAION. It is fully open source and available on the Hugging Face Hub (e.g., openflamingo/OpenFlamingo-9B-vitl-mpt7b, based on a CLIP ViT-L/14 vision encoder and a MPT-7B LLM).           *   [IDEFICS](https://homl.info/idefics) by Hugging Face, trained on a huge dataset named OBELICS,⁠^([35](ch16.html#id3931)) composed of 141 million interleaved text-image documents gathered from Common Crawl (including 350 million images and 115 billion text tokens). Both IDEFICS and OBELICS are available on the hub (e.g., Idefics3-8B-Llama3 and OBELICS by HuggingFaceM4). The architecture includes a few improvements over Flamingo; for example, you can more easily swap in different LLMs or vision encoders. IDEFICS itself is open source, but the models it is based on may have licensing limitations. In particular, IDEFICS 1 and 3 are based on Llama, which has some limitations for commercial use, while IDEFICS 2 is based on Mistral, which is fully open source.           *   [AudioFlamingo](https://homl.info/audioflamingo) by Nvidia, which is very similar to Flamingo but handles audio instead of images.           *   Other variants are available, such as domain-specific models like [Med-Flamingo](https://homl.info/medflamingo), an OpenFlamingo model trained on medical documents.              The last multimodal architecture we will discuss is bootstrapping language-image pretraining, or BLIP, by Salesforce. Its second version, BLIP-2, also successfully reuses two large pretrained models—a vision model and an LLM—to create a VLM that can ingest both images and text, and generate free-form text. Let’s see how.    ## BLIP and BLIP-2    The original [BLIP model](https://homl.info/blip) is an excellent visual-language model released by Salesforce in January 2022.⁠^([36](ch16.html#id3938)) Its architecture is a *mixture of encoder-decoder* (MED) composed of a text-only encoder, a vision-only encoder, an image-grounded text encoder, and an image-grounded text decoder, sharing many layers. This flexible architecture made it possible to train the model simultaneously on three distinct objectives: *image-text matching* (ITM), an *image-text contrastive* (ITC) loss to align image and text representations (similar to CLIP), and language modeling (LM) where the model must try to generate the caption using next token prediction.    Another important reason for BLIP’s success is the fact that it was pretrained on a very large and clean dataset. To build this dataset, the authors simultaneously trained a *captioning module* to generate synthetic captions for images, and a *filtering module* to remove noisy data. This approach, named *CapFilt*, removed poor quality captions from the original web-scraped dataset, and added many new high-quality synthetic captions. After this bootstrapping stage, the authors trained the final model on the large and clean dataset they had just built. It’s a two-stage process, hence the name BLIP: *bootstrapping language-image pretraining*.    One year later, in January 2023, Salesforce released [BLIP-2](https://homl.info/blip2),⁠^([37](ch16.html#id3951)) which is based on the same core ideas but greatly improves the model’s performance by reusing two large pretrained models, one vision model and one language model, both frozen. BLIP-2 even outperformed Flamingo with a much smaller model.    Training is split in two stages. BLIP-2’s architecture during the first stage is shown in [Figure 16-18](#blip2_stage1_diagram).  ![Diagram illustrating BLIP-2's Stage 1 pretraining architecture, focusing on the Q-Former module integrating vision and text sequences through various attention and processing layers.](assets/hmls_1618.png)  ###### Figure 16-18\\. BLIP-2 pretraining, Stage 1: training the Q-Former    *   The central component is called the *Q-Former* (querying transformer). Its architecture is the same as BERT-base, and in fact it’s even initialized using BERT-base’s pretrained weights, but it also has some extra cross-attention layers that let it attend to visual tokens produced by the pretrained visual encoder. The cross-attention layers are inserted in every other encoder layer, between the self-attention layer and the feedforward module, and they are initialized randomly.           *   The Q-Former processes three sequences: a sequence of text tokens (using BERT tokenization and token embeddings), a sequence of visual tokens produced by the pretrained vision encoder, and lastly a sequence of trainable Perceiver-style latent tokens. In BLIP-2, the latent tokens are called *query tokens* because their output representations will later be used to query the pretrained LLM.           *   The Q-Former is trained with the same three objectives as BLIP: ITM, ITC, and LM. For each objective, a different mask is used:               *   For ITM, query tokens and text tokens can attend to each other. In other words, the output representations for the query tokens represent text-grounded visual features, and the output representations for the text tokens represent image-grounded text features. The query token outputs go through a linear layer which produces two logits per query token (image-text match or mismatch), and the model computes the mean logits across all query tokens, then computes the binary cross-entropy.                       *   For ITC, query tokens and text tokens cannot attend to each other. In other words, the Q-Former’s outputs represent visual-only features and text-only features. For each possible image/caption pair in the batch, the model computes the maximum similarity between the query token outputs and the class token output. We get a matrix of maximum similarities, and the loss pushes the values toward +1 on the main diagonal, and pushes the other values toward 0, much like CLIP.                       *   For LM, text tokens can only attend previous tokens (i.e., we use a causal mask), but they can attend all query tokens. However, query tokens cannot attend any text token. In other words, the query token outputs represent visual-only features, while text token outputs represent image-grounded causal text features. The model is trained using next token prediction: each text token’s output goes through a classification head which must predict the next token in the caption.                      You may be surprised that the Q-Former is used to encode text (for ITM and ITC) and also to generate text (for LM). Since the Q-Former is initialized using the weights of a pretrained BERT-base model, it’s pretty good at text encoding right from the start of training, but it initially doesn’t know that it has to predict the next token for the LM task. Luckily, it can learn fairly fast since it’s not starting from scratch; it has good BERT features to work with. However, we need to tell it whether we want it to encode the text or predict the next token. For this, we replace the class token with a *decode token* during LM.⁠^([38](ch16.html#id3959))    Once stage 1 is finished, the Q-Former is already a powerful model that can encode images and text into the same space, so a photo of a chimpanzee produces a very similar output representation as the caption “A photo of a chimpanzee”. But it’s even better than that: the query token outputs were trained to be most helpful for next token prediction.    ###### Tip    To produce negative examples for ITM, one strategy is to randomly pick a caption in the same batch, excluding the image’s true caption. However, this makes the task too easy, so the model doesn’t learn much. Instead, the authors used a *hard negative mining* strategy, where difficult captions are more likely to be sampled. For example, given a photo of a chimpanzee, the caption “A gorilla” is more likely to be sampled than “A spacecraft”. To find difficult captions, the algorithm uses the similarity scores from the ITC task.    So it’s time for the second stage of training (see [Figure 16-19](#blip2_stage2_diagram)):    *   We keep the vision transformer and the Q-Former, but we drop the rest and we add a new linear layer, initialized randomly, on top of the Q-Former.           *   For each image/caption pair, the Q-Former attends to the visual features produced by the pretrained vision encoder, and the outputs go through the linear layer to produce a sequence of visual query tokens.           *   The visual query tokens and the text token representations are concatenated and fed to the (frozen) pretrained LLM. We train BLIP-2 to predict the next caption token.              During stage 2, the model learns to properly map the visual query tokens to the LLM’s input space. Once trained, the model can be used like in stage 2, generating visual-grounded text.  ![Diagram illustrating the BLIP-2 pretraining process, showing how visual features from a vision encoder are processed by a Q-former and a linear layer to generate visual query tokens, which are then combined with text tokens and fed to a large language model (LLM).](assets/hmls_1619.png)  ###### Figure 16-19\\. BLIP-2 pretraining, Stage 2: training the linear layer to map the query tokens to the LLM’s input space    Let’s use BLIP-2 to generate a caption for an image:    ``` from transformers import Blip2Processor, Blip2ForConditionalGeneration  model_id = \"Salesforce/blip2-opt-2.7b\" blip2_processor = Blip2Processor.from_pretrained(model_id) blip2_model = Blip2ForConditionalGeneration.from_pretrained(     model_id, device_map=device, dtype=torch.float16)  image_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"  # two cats image = Image.open(urllib.request.urlopen(image_url)) inputs = blip2_processor(images=image, return_tensors=\"pt\") inputs = inputs.to(device, dtype=torch.float16) with torch.no_grad():     generated_ids = blip2_model.generate(**inputs)  generated_text = blip2_processor.batch_decode(generated_ids) ```py    What did BLIP-2 see?    ``` >>> generated_text `['<image><image><image><image>[...]<image></s>two cats laying on a couch\\n']` ```py   ``````", "``` >>> generated_text = blip2_processor.batch_decode(generated_ids, `... `                                              `skip_special_tokens``=``True``)` ```", "```` `>>>` `generated_text` ```py `>>>` `` `['two cats laying on a couch\\n']` `` ``` ```py` ````", "```py   ```", "```py ```", "```py ```", "```py` ```", "```py`` ```", "```py```", "``````py ````` ```py`# Other Multimodal Models    We’ve covered quite a few multimodal models, with very different architectures and pretraining techniques, but of course there are many others. Here is a quick overview of some of the most notable ones:    LayoutLM (Microsoft, Dec. 2019)      Document understanding based on text, vision, and document layout. Version 3 was released in April 2022.      GLIP (Microsoft, Dec. 2021)      A vision-language model for visual grounding and object detection. GLIP-2 was released in 2022.      Stable Diffusion (Stability AI, Dec. 2021)      A powerful text-to-image model.      OFA (Microsoft, Feb. 2022)      Unified (one for all) vision-language pretraining framework handling various vision-language tasks.      CoCa (Google, May 2022)      A vision-language model pretrained using contrastive and captioning objectives. CoCa influenced later models like PaLI-X and Flamingo-2.      PaLI (Google, Sep. 2022)      Multilingual multimodal models for vision-language tasks like VQA and captioning, with strong zero-shot performance. The next versions, PaLI-X and PaLI-3, were released in 2023, and PaliGemma in May 2024.      Kosmos-1 (Microsoft, Feb. 2023)      A vision-language model with strong support for visual grounding. Kosmos-2 and Kosmos-2.5 came out in 2023.      PaLM-E (Google, Mar. 2023)      PaLM-E extends Google’s PaLM series with visual inputs and embodied sensor data. A decoder-only LLM generates text commands like “grab the hammer”, which are interpreted and executed by a robot via a downstream system.      LLaVA (H. Liu et al., Apr. 2023)      Among the best open source vision-language chat models.      ImageBind (Meta, May 2023)      A CLIP-style model extended to six modalities (image, text, audio, IMU,⁠^([39](ch16.html#id3976)) depth, and thermal).      RT-2 (DeepMind, Jul. 2023)      A vision-language model capable of robotic control as well, trained on a large-scale instruction-following dataset.      SeamlessM4T (Meta, Aug. 2023)      A single model that can perform speech-to-text, speech-to-speech, text-to-speech, and text-to-text translation across close to 100 languages.      Qwen-VL (Alibaba, Sep. 2023)      Open vision-language family (7B to 72B) that became one of the strongest open multimodal baselines. Led to Qwen2-VL (Aug. 2024) and Qwen3-Omni (Sep. 2025), which expanded to video and audio and reached trillion-parameter scale.      Fuyu (Adept AI, Oct. 2023)      Processes interleaved image and text in real time with a unified transformer.      EMO (Alibaba, Feb. 2024)      Takes an image of a person, plus an audio recording of someone speaking or singing, and the model generates a video of that person, matching the audio. EMO-2 was released in January 2025.      GLaMM (H. Rasheed et al., Jun. 2024)      A visual dialogue model which generates text responses mixed with object segmentation masks.      LaViDa (UCLA, Panasonic, Adobe, Salesforce, May 2025)      A family of open, diffusion-based vision-language models.      ###### Tip    I’ve created homl.info short links for all the models discussed in this chapter; just use the lowercase name without hyphens, for example, [*https://homl.info/qwen2vl*](https://homl.info/qwen2vl).    There are also several commercial multimodal models whose detailed architectures were not disclosed, such as GPT-4.1 and Sora by OpenAI, Gemini 2.5 Pro by Google, Veo-3 by DeepMind, and Claude 4 Opus by Anthropic. To access these models, you first need to create an account and get a subscription (or use the free tier), then you can either use the provided apps (e.g., Google AI Studio, [*https://aistudio.google.com*](https://aistudio.google.com)), or query the model via an API. For example, following is a short code example showing how to query Gemini 2.5 Pro via the API. You first need to get an API key in Google AI Studio, then you can use any secret management method you prefer to store it and load it in your code (e.g., if you are using Colab, I recommend you use Colab’s secret manager, as we saw in [Chapter 15](ch15.html#transformer_chapter)).    ``` from google import genai  gemini_api_key = [...]  # load from Colab secrets, or from a file, or hardcode gemini_client = genai.Client(api_key=gemini_api_key) cats_photo = gemini_client.files.upload(file=\"my_cats_photo.jpg\") question = \"What animal and how many? Format: [animal, number]\" response = gemini_client.models.generate_content(     model=\"gemini-2.5-flash\",  # or \"gemini-2.5-pro\"     contents=[cats_photo, question]) print(response.text)  # prints: \"[cat, 2]\" ```py    This code uses the `google-genai` library, which is already installed on Colab. It also assumes that a file named *my_cats_photo.jpg* is present in the same directory as the notebook.    This wraps up this chapter; I hope you enjoyed it. Transformers can now see, hear, touch, and more! In the next chapter, we will explore some fairly advanced techniques designed to speed up and scale transformers. As Daft Punk put it: harder, better, faster, stronger.    # Exercises    1.  Can you describe the original ViT’s architecture? Why does it matter?           2.  What tasks are regular ViTs (meaning nonhierarchical) best used for? What are their limitations?           3.  What is the main innovation in DeiT? Is this idea generalizable to other architectures?           4.  What are some examples of hierarchical ViTs? What kind of tasks are they good for?           5.  How do PVTs and Swin Transformers reduce the computational cost of processing high-resolution images?           6.  How does DINO work? What changed in DINOv2? When would you want to use DINOv2?           7.  What is the objective of the JEPA architecture? How does it work?           8.  What is a multimodal model? Can you give five examples of multimodal tasks?           9.  Explain what the fusion and alignment problems are in multimodal learning. Why are transformers well suited to tackle them?           10.  Can you write a one-line summary of the main ideas in VideoBERT, ViLBERT, CLIP, DALL·E, Perceiver IO, Flamingo, and BLIP-2?           11.  If you are using a Perceiver IO model and you double the length of the inputs and the outputs, approximately how much more computation will be required?           12.  Try fine-tuning a pretrained ViT model on the [Food 101 dataset](https://homl.info/food101) (`torchvision.datasets.Food101`). What accuracy can you reach? How about using a CLIP model, zero-shot?           13.  Create a simple search engine for your own photos: first, write a function that uses a CLIP model to embed all of your photos and saves the resulting vectors. Next, write a function that takes a search query (text or image), embeds it using CLIP, then finds the most similar photo embeddings and displays the corresponding photos. You can manually implement the similarity search algorithm, or a dedicated library such as the [FAISS library](https://github.com/facebookresearch/faiss) or even a full-blown vector database.           14.  Use BLIP-2 to automatically caption all of your photos.              Solutions to these exercises are available at the end of this chapter’s notebook, at [*https://homl.info/colab-p*](https://homl.info/colab-p).    ^([1](ch16.html#id3722-marker)) Kelvin Xu et al., “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention”, *Proceedings of the 32nd International Conference on Machine Learning* (2015): 2048–2057.    ^([2](ch16.html#id3724-marker)) This is a part of Figure 3 from the paper. It is reproduced with the kind authorization of the authors.    ^([3](ch16.html#id3728-marker)) Marco Tulio Ribeiro et al., “‘Why Should I Trust You?’: Explaining the Predictions of Any Classifier”, *Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining* (2016): 1135–1144.    ^([4](ch16.html#id3740-marker)) Nicolas Carion et al., “End-to-End Object Detection with Transformers”, arXiv preprint arXiv:2005.12872 (2020).    ^([5](ch16.html#id3741-marker)) Alexey Dosovitskiy et al., “An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale”, arXiv preprint arXiv:2010.11929 (2020).    ^([6](ch16.html#id3760-marker)) Hugo Touvron et al., “Training Data-Efficient Image Transformers & Distillation Through Attention”, arXiv preprint arXiv:2012.12877 (2020).    ^([7](ch16.html#id3765-marker)) Wenhai Wang et al., “Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions”, arXiv preprint arXiv:2102.12122 (2021).    ^([8](ch16.html#id3776-marker)) Ze Liu et al., “Swin Transformer: Hierarchical Vision Transformer using Shifted Windows”, arXiv preprint arXiv:2103.14030 (2021).    ^([9](ch16.html#id3781-marker)) Ze Liu et al., “Swin Transformer V2: Scaling Up Capacity and Resolution”, arXiv preprint arXiv:2111.09883 (2021).    ^([10](ch16.html#id3788-marker)) Mathilde Caron et al., “Emerging Properties in Self-Supervised Vision Transformers”, arXiv preprint arXiv:2104.14294 (2021).    ^([11](ch16.html#id3797-marker)) This is the righthand part of Figure 3 of the DINO paper, reproduced with the kind authorization of the authors.    ^([12](ch16.html#id3799-marker)) Yangtao Wang et al., “TokenCut: Segmenting Objects in Images and Videos with Self-supervised Transformer and Normalized Cut”, arXiv preprint arXiv:2209.00383 (2022).    ^([13](ch16.html#id3801-marker)) “DINOv2: Learning Robust Visual Features without Supervision”, arXiv preprint arXiv:2304.07193 (2023).    ^([14](ch16.html#id3806-marker)) Xiaohua Zhai et al., “Scaling Vision Transformers”, arXiv preprint arXiv:2106.04560 (2021).    ^([15](ch16.html#id3808-marker)) Hangbo Bao et al., “BEiT: BERT Pre-Training of Image Transformers”, arXiv preprint arXiv:2106.08254 (2021).    ^([16](ch16.html#id3816-marker)) Kaiming He et al., “Masked Autoencoders Are Scalable Vision Learners”, arXiv preprint arXiv:2111.06377 (2021).    ^([17](ch16.html#id3820-marker)) Mitchell Wortsman et al., “Model Soups: Averaging Weights of Multiple Fine-tuned Models Improves Accuracy Without Increasing Inference Time”, arXiv preprint arXiv:2203.05482 (2022).    ^([18](ch16.html#id3821-marker)) Yuxin Fang et al., “EVA: Exploring the Limits of Masked Visual Representation Learning at Scale”, arXiv preprint arXiv:2211.07636 (2022).    ^([19](ch16.html#id3823-marker)) “Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture”, arXiv preprint arXiv:2301.08243 (2023).    ^([20](ch16.html#id3826-marker)) Yann LeCun, “A Path Towards Autonomous Machine Intelligence” (2022).    ^([21](ch16.html#id3840-marker)) Chen Sun et al., “VideoBERT: A Joint Model for Video and Language Representation Learning”, arXiv preprint arXiv:1904.01766 (2019).    ^([22](ch16.html#id3846-marker)) L. Zhou, Y. Zhou et al., “End-to-End Dense Video Captioning with Masked Transformer”, *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition* (2018).    ^([23](ch16.html#id3852-marker)) Jiasen Lu et al., “ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks”, *Advances in Neural Information Processing Systems* 32 (2019).    ^([24](ch16.html#id3853-marker)) Jize Cao et al. later provided some empirical evidence supporting this claim in their paper [“Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models”](https://homl.info/probing): in particular, they found that more attention heads focus on the text modality than on the visual modality.    ^([25](ch16.html#id3870-marker)) Alec Radford et al., “Learning Transferable Visual Models From Natural Language Supervision”, arXiv preprint arXiv:2103.00020 (2021).    ^([26](ch16.html#id3873-marker)) The training code and data were not released by OpenAI, but Gabriel Ilharco et al. created [OpenCLIP](https://homl.info/openclip) which is a flexible open source replication of CLIP with the full training code and data.    ^([27](ch16.html#id3874-marker)) This contrastive loss was first introduced as the *multiclass n-pair loss* in a [2016 paper by Kihyuk Sohn](https://homl.info/npairloss), then used for contrastive representation learning and renamed to *InfoNCE* (information noise-contrastive estimation) in a [2018 paper by Aaron van den Oord et al](https://homl.info/infonce).    ^([28](ch16.html#id3883-marker)) Aditya Ramesh et al., “Zero-Shot Text-to-Image Generation”, arXiv preprint arXiv:2102.12092 (2021).    ^([29](ch16.html#id3886-marker)) Aditya Ramesh et al., “Hierarchical Text-Conditional Image Generation with CLIP Latents”, arXiv preprint arXiv:2204.06125 (2022).    ^([30](ch16.html#id3898-marker)) Andrew Jaegle et al., “Perceiver: General Perception with Iterative Attention”, arXiv preprint arXiv:2103.03206 (2021).    ^([31](ch16.html#id3901-marker)) If Δ is the spacing between samples, then the Nyquist–Shannon sampling theorem tells us that the maximum frequency we can measure is *f* = 1 / 2Δ. This is why *f* stops at *μ* / 2 rather than *μ*: sampling at a higher resolution would not add any information, and it might introduce aliasing artifacts.    ^([32](ch16.html#id3910-marker)) [AudioSet](https://homl.info/audioset) contains over 2 million video segments of 10s each, sorted into over 500 classes.    ^([33](ch16.html#id3911-marker)) [ModelNet40](https://homl.info/modelnet) is a synthetic dataset of 3D point clouds of various shapes, such as airplanes or cars. A common source of point clouds in real life is LiDAR sensors.    ^([34](ch16.html#id3915-marker)) Andrew Jaegle et al., “Perceiver IO: A General Architecture for Structured Inputs & Outputs”, arXiv preprint arXiv:2107.14795 (2021).    ^([35](ch16.html#id3931-marker)) In the French comic series *Astérix*, Obélix is a big and friendly Gaul, and Idéfix is his clever little dog.    ^([36](ch16.html#id3938-marker)) Junnan Li et al., “BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation”, arXiv preprint arXiv:2201.12086 (2022).    ^([37](ch16.html#id3951-marker)) Junnan Li et al., “BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models”, arXiv preprint arXiv:2301.12597 (2023).    ^([38](ch16.html#id3959-marker)) The idea of training a single model capable of both encoding and generating text was introduced in 2019 by Microsoft researchers Li Dong et al. with their [UniLM model](https://homl.info/unilm).    ^([39](ch16.html#id3976-marker)) Most modern smartphones contain an inertial measurement unit (IMU) sensor: it measures acceleration, angular velocity, and often the magnetic field strength.```` ```py`` ``````"]