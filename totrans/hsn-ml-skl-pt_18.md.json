["```py\nimport torch\nimport torch.nn as nn\n\nclass PatchEmbedding(nn.Module):\n    def __init__(self, in_channels, embed_dim, patch_size=16):\n        super().__init__()\n        self.conv2d = nn.Conv2d(embed_dim, in_channels,\n                                kernel_size=patch_size, stride=patch_size)\n    def forward(self, X):\n        X = self.conv2d(X)  # shape [B=Batch, C=Channels, H=Height, W=Width]\n        X = X.flatten(start_dim=2)  # shape [B, C, H * W]\n        return X.transpose(1, 2)  # shape [B, H * W, C]\n```", "```py\nclass ViT(nn.Module):\n    def __init__(self, img_size=224, patch_size=16, in_channels=3,\n                 num_classes=1000, embed_dim=768, depth=12, num_heads=12,\n                 ff_dim=3072, dropout=0.1):\n        super().__init__()\n        self.patch_embed = PatchEmbedding(embed_dim, in_channels, patch_size)\n        cls_init = torch.randn(1, 1, embed_dim) * 0.02\n        self.cls_token = nn.Parameter(cls_init)  # shape [1, 1, E=embed_dim]\n        num_patches = (img_size // patch_size) ** 2  # num_patches (denoted L)\n        pos_init = torch.randn(1, num_patches + 1, embed_dim) * 0.02\n        self.pos_embed = nn.Parameter(pos_init)  # shape [1, 1 + L, E]\n        self.dropout = nn.Dropout(p=dropout)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=embed_dim, nhead=num_heads, dim_feedforward=ff_dim,\n            dropout=dropout, activation=\"gelu\", batch_first=True)\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n        self.layer_norm = nn.LayerNorm(embed_dim)\n        self.output = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, X):\n        Z = self.patch_embed(X)  # shape [B, L, E]\n        cls_expd = self.cls_token.expand(Z.shape[0], -1, -1)  # shape [B, 1, E]\n        Z = torch.cat((cls_expd, Z), dim=1)  # shape [B, 1 + L, E]\n        Z = Z + self.pos_embed\n        Z = self.dropout(Z)\n        Z = self.encoder(Z)  # shape [B, 1 + L, E]\n        Z = self.layer_norm(Z[:, 0])  # shape [B, E]\n        logits = self.output(Z) # shape [B, C]\n        return logits\n```", "```py\nvit_model = ViT(\n    img_size=224, patch_size=16, in_channels=3, num_classes=1000, embed_dim=768,\n    depth=12, num_heads=12, ff_dim=3072, dropout=0.1)\nbatch = torch.randn(4, 3, 224, 224)\nlogits = vit_model(batch)  # shape [4, 1000]\n```", "```py\nfrom datasets import load_dataset\n\npets = load_dataset(\"timm/oxford-iiit-pet\")\n```", "```py\nfrom transformers import ViTForImageClassification, AutoImageProcessor\n\nmodel_id = \"google/vit-base-patch16-224-in21k\"\nvit_model = ViTForImageClassification.from_pretrained(model_id, num_labels=37)\nvit_processor = AutoImageProcessor.from_pretrained(model_id, use_fast=True)\n```", "```py\ndef vit_collate_fn(batch):\n    images = [example[\"image\"] for example in batch]\n    labels = [example[\"label\"] for example in batch]\n    inputs = vit_processor(images, return_tensors=\"pt\", do_convert_rgb=True)\n    inputs[\"labels\"] = torch.tensor(labels)\n    return inputs\n```", "```py\nfrom transformers import Trainer, TrainingArguments\n\nargs = TrainingArguments(\"my_pets_vit\", per_device_train_batch_size=16,\n                         eval_strategy=\"epoch\", num_train_epochs=3,\n                         remove_unused_columns=False)\ntrainer = Trainer(model=vit_model, args=args, data_collator=vit_collate_fn,\n                  train_dataset=pets[\"train\"], eval_dataset=pets[\"test\"])\ntrain_output = trainer.train()\n```", "```py\nfrom transformers import pipeline\n\nmodel_id = \"openai/clip-vit-base-patch32\"\nclip_pipeline = pipeline(task=\"zero-shot-image-classification\", model=model_id,\n                         device_map=\"auto\", dtype=\"auto\")\ncandidate_labels = [\"cricket\", \"ladybug\", \"spider\"]\nimage_url = \"https://homl.info/ladybug\"  # a photo of a ladybug on a dandelion\nresults = clip_pipeline(image_url, candidate_labels=candidate_labels,\n                        hypothesis_template=\"This is a photo of a {}.\")\n```", "```py\n[{'score': 0.9972853660583496, 'label': 'ladybug'},\n {'score': 0.0016511697322130203, 'label': 'spider'},\n {'score': 0.0010634352220222354, 'label': 'cricket'}]\n```", "```py\nimport PIL\nimport urllib.request\nfrom transformers import CLIPProcessor, CLIPModel\n\nclip_processor = CLIPProcessor.from_pretrained(model_id)\nclip_model = CLIPModel.from_pretrained(model_id)\nimage = PIL.Image.open(urllib.request.urlopen(image_url)).convert(\"RGB\")\ncaptions = [f\"This is a photo of a {label}.\" for label in candidate_labels]\ninputs = clip_processor(text=captions, images=[image], return_tensors=\"pt\",\n                        padding=True)\nwith torch.no_grad():\n    outputs = clip_model(**inputs)\n\ntext_features = outputs.text_embeds    # shape [3, 512]  # 3 captions\nimage_features = outputs.image_embeds  # shape [1, 512]  # 1 image (ladybug)\n```", "```py\n>>> similarities = image_features @ text_features.T  # shape [1, 3]\n>>> similarities\ntensor([[0.2337, 0.3021, 0.2381]])\n```", "```py\n>>> temperature = clip_model.logit_scale.detach().exp()\n>>> rescaled_similarities = similarities * temperature\n>>> probabilities = torch.nn.functional.softmax(rescaled_similarities , dim=1)\n>>> probabilities\ntensor([[0.0011, 0.9973, 0.0017]])\n```", "```py\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration\n\nmodel_id = \"Salesforce/blip2-opt-2.7b\"\nblip2_processor = Blip2Processor.from_pretrained(model_id)\nblip2_model = Blip2ForConditionalGeneration.from_pretrained(\n    model_id, device_map=device, dtype=torch.float16)\n\nimage_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"  # two cats\nimage = Image.open(urllib.request.urlopen(image_url))\ninputs = blip2_processor(images=image, return_tensors=\"pt\")\ninputs = inputs.to(device, dtype=torch.float16)\nwith torch.no_grad():\n    generated_ids = blip2_model.generate(**inputs)\n\ngenerated_text = blip2_processor.batch_decode(generated_ids)\n```", "```py\n>>> generated_text\n['<image><image><image><image>[...]<image></s>two cats laying on a couch\\n']\n```", "```py\n>>> generated_text = blip2_processor.batch_decode(generated_ids,\n...                                               skip_special_tokens=True)\n...\n>>> generated_text\n>>>\n['two cats laying on a couch\\n']\n```", "```py\nfrom google import genai\n\ngemini_api_key = [...]  # load from Colab secrets, or from a file, or hardcode\ngemini_client = genai.Client(api_key=gemini_api_key)\ncats_photo = gemini_client.files.upload(file=\"my_cats_photo.jpg\")\nquestion = \"What animal and how many? Format: [animal, number]\"\nresponse = gemini_client.models.generate_content(\n    model=\"gemini-2.5-flash\",  # or \"gemini-2.5-pro\"\n    contents=[cats_photo, question])\nprint(response.text)  # prints: \"[cat, 2]\"\n```"]