<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Image classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Image classification</h1>
<blockquote>原文：<a href="https://deeplearningwithpython.io/chapters/chapter08_image-classification">https://deeplearningwithpython.io/chapters/chapter08_image-classification</a></blockquote>


<aside>
<p>This chapter covers
</p>
<ul>
<li>Understanding convolutional neural networks (ConvNets)</li>
<li>Using data augmentation to mitigate overfitting</li>
<li>Using a pretrained ConvNet for feature extraction</li>
<li>Fine-tuning a pretrained ConvNet</li>
</ul>
</aside>

<p>Computer vision was the first big success story of deep learning.
It led to the initial rise of deep learning between 2011 and 2015.
A type of deep learning called <em>convolutional neural networks</em>
started getting remarkably good results on image classification competitions
around that time, first with Dan Ciresan winning two niche competitions
(the ICDAR 2011 Chinese character
recognition competition and the IJCNN 2011 German traffic signs recognition competition)
and then, more notably, in fall 2012, with Hinton’s group winning the high-profile ImageNet
large-scale visual recognition challenge. Many more promising results quickly started
bubbling up in other computer vision tasks.</p>
<p>Interestingly, these early successes weren’t quite enough to make deep learning
mainstream at the time — it took a few years. The computer vision research
community had spent many years investing in methods other than neural networks,
and it wasn’t quite ready to give up on them just because there was a new kid on the block.
In 2013 and 2014, deep learning still faced intense skepticism from many senior
computer vision researchers. It was only in 2016 that it finally became dominant.
One author remembers exhorting an ex-professor, in February 2014,
to pivot to deep learning. “It’s the next big thing!” he would say.
“Well, maybe it’s just a fad,” the professor would reply. By 2016, his entire lab was doing
deep learning. There’s no stopping an idea whose time has come.</p>
<p>Today, you’re constantly interacting with deep learning–based vision models —
via Google Photos, Google image search, the camera on your phone,
YouTube, OCR software, and many more.
These models are also at the heart of cutting-edge research in autonomous driving,
robotics, AI-assisted medical diagnosis, autonomous retail checkout systems,
and even autonomous farming.</p>
<p>This chapter introduces convolutional neural networks, also known as
<em>ConvNets</em> or <em>CNNs</em>, the type of deep-learning model that is used by most computer
vision applications. You’ll learn to apply ConvNets to image classification
problems — in particular, those involving small training datasets, which are the
most common use case if you aren’t a large tech company.</p>
<h2 id="introduction-to-convnets">Introduction to ConvNets</h2>
<p>We’re about to dive into the theory of what ConvNets are
and why they have been so successful at computer vision tasks.
But first, let’s take a practical look at a simple
ConvNet example. It uses a ConvNet to classify MNIST digits, a task we
performed in chapter 2 using a densely connected network (our test accuracy
then was 97.8%). Even though the ConvNet will be basic, its accuracy will blow
out of the water that of the densely connected model from chapter 2.</p>
<p>The following lines of code show you what a basic ConvNet looks like. It’s a
stack of <code>Conv2D</code> and <code>MaxPooling2D</code> layers. You’ll see in a minute exactly
what they do. We’ll build the model using the Functional API,
which we introduced in the previous chapter.</p>
<figure id="listing-8-1">
<pre><code class="language-python">import keras
from keras import layers

inputs = keras.Input(shape=(28, 28, 1))
x = layers.Conv2D(filters=64, kernel_size=3, activation="relu")(inputs)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=128, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.GlobalAveragePooling2D()(x)
outputs = layers.Dense(10, activation="softmax")(x)
model = keras.Model(inputs=inputs, outputs=outputs)
</code></pre>
<figcaption>
<a href="#listing-8-1">Listing 8.1</a>: Instantiating a small ConvNet
</figcaption>
</figure>

<p>Importantly, a ConvNet takes as input tensors of shape <code>(image_height,
image_width, image_channels)</code> (not including the batch dimension). In this
case, we’ll configure the ConvNet to process inputs of size <code>(28, 28, 1)</code>,
which is the format of MNIST images.</p>
<p>Let’s display the architecture of our ConvNet. </p>
<figure id="listing-8-2">
<pre><code class="language-python">&gt;&gt;&gt; model.summary()</code>
<code class="language-output">Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ input_layer (InputLayer)          │ (None, 28, 28, 1)        │             0 │
├───────────────────────────────────┼──────────────────────────┼───────────────┤
│ conv2d (Conv2D)                   │ (None, 26, 26, 64)       │           640 │
├───────────────────────────────────┼──────────────────────────┼───────────────┤
│ max_pooling2d (MaxPooling2D)      │ (None, 13, 13, 64)       │             0 │
├───────────────────────────────────┼──────────────────────────┼───────────────┤
│ conv2d_1 (Conv2D)                 │ (None, 11, 11, 128)      │        73,856 │
├───────────────────────────────────┼──────────────────────────┼───────────────┤
│ max_pooling2d_1 (MaxPooling2D)    │ (None, 5, 5, 128)        │             0 │
├───────────────────────────────────┼──────────────────────────┼───────────────┤
│ conv2d_2 (Conv2D)                 │ (None, 3, 3, 256)        │       295,168 │
├───────────────────────────────────┼──────────────────────────┼───────────────┤
│ global_average_pooling2d          │ (None, 256)              │             0 │
│ (GlobalAveragePooling2D)          │                          │               │
├───────────────────────────────────┼──────────────────────────┼───────────────┤
│ dense (Dense)                     │ (None, 10)               │         2,570 │
└───────────────────────────────────┴──────────────────────────┴───────────────┘
 Total params: 372,234 (1.42 MB)
 Trainable params: 372,234 (1.42 MB)
 Non-trainable params: 0 (0.00 B)</code></pre>
<figcaption>
<a href="#listing-8-2">Listing 8.2</a>: Displaying the model’s summary
</figcaption>
</figure>

<p>You can see that the output of every <code>Conv2D</code> and <code>MaxPooling2D</code> layer is a 3D
tensor of shape <code>(height, width, channels)</code>. The width and height dimensions
tend to shrink as you go deeper in the model. The number of channels is
controlled by the first argument passed to the <code>Conv2D</code> layers (64, 128, or 256).</p>
<aside>
<p><span class="note-title">Image data formats in deep learning frameworks</span></p>
<p>Some deep learning libraries flip the location of channels in image tensors to
the first rank (notably much of the PyTorch ecosystem). Rather than passing
images with shape <code>(height, width, channels)</code> you would pass
<code>(channels, height, width)</code>.</p>
<p>This is purely a matter of convention, and in Keras is configurable. You can
call <code>keras.config.set_image_data_format("channels_first")</code> to change Keras’
default, or pass a <code>data_format</code> argument to any conv or pooling layer. In
general, you can leave the default as is unless you have a specific need for
<code>"channels_first"</code>.</p>
</aside>

<p>After the last <code>Conv2D</code> layer, we end up with an output of shape
<code>(3, 3, 256)</code> — a 3 × 3 feature map of 256 channels.
The next step is to feed this output into a
densely connected classifier like those you’re already familiar with:
a stack of <code>Dense</code> layers. These classifiers process vectors, which are 1D,
whereas the current output is a rank-3 tensor. To bridge the gap, we flatten the 3D
outputs to 1D with a <code>GlobalAveragePooling2D</code> layer before adding the <code>Dense</code> layers.
This layer will take the average of each 3 × 3 feature map in the tensor of shape <code>(3, 3, 256)</code>,
resulting in an output vector of shape <code>(256,)</code>. Finally, we’ll do 10-way classification, so our last layer has 10 outputs and a
softmax activation.</p>
<p>Now, let’s train the ConvNet on the MNIST digits. We’ll reuse a lot of the code
from the MNIST example in chapter 2. Because we’re doing 10-way classification
with a softmax output, we’ll use the categorical crossentropy loss, and
because our labels are integers, we’ll use the sparse version,
<code>sparse_categorical_crossentropy</code>.</p>
<figure id="listing-8-3">
<pre><code class="language-python">from keras.datasets import mnist

(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
train_images = train_images.reshape((60000, 28, 28, 1))
train_images = train_images.astype("float32") / 255
test_images = test_images.reshape((10000, 28, 28, 1))
test_images = test_images.astype("float32") / 255
model.compile(
    optimizer="adam",
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"],
)
model.fit(train_images, train_labels, epochs=5, batch_size=64)
</code></pre>
<figcaption>
<a href="#listing-8-3">Listing 8.3</a>: Training the ConvNet on MNIST images
</figcaption>
</figure>

<p>Let’s evaluate the model on the test data.</p>
<figure id="listing-8-4">
<pre><code class="language-python">&gt;&gt;&gt; test_loss, test_acc = model.evaluate(test_images, test_labels)
&gt;&gt;&gt; print(f"Test accuracy: {test_acc:.3f}")</code>
<code class="language-output">Test accuracy: 0.991</code></pre>
<figcaption>
<a href="#listing-8-4">Listing 8.4</a>: Evaluating the ConvNet
</figcaption>
</figure>

<p>Whereas the densely connected model from chapter 2 had a test accuracy of
97.8%, the basic ConvNet has a test accuracy of 99.1%: we decreased the error
rate by about 60% (relative). Not bad!</p>
<p>But why does this simple ConvNet work so well, compared to a densely connected
model? To answer this, let’s dive into what the <code>Conv2D</code> and
<code>MaxPooling2D</code> layers do.</p>
<h3 id="the-convolution-operation">The convolution operation</h3>
<p>The fundamental difference between a densely connected layer and a convolution
layer is this: <code>Dense</code> layers learn global patterns in their input feature space
(for example, for a MNIST digit, patterns involving all pixels),
whereas convolution layers learn local patterns (see figure 8.1):
in the case of images, patterns found in small 2D windows of the inputs.
In the previous example, these windows were all 3 × 3.</p>
<figure id="figure-8-1">
<img src="../Images/1d29271b09d5f37759936df42a9f5aca.png" data-original-src="https://deeplearningwithpython.io/images/ch08/local_patterns.b72668dd.jpg"/>
<figcaption>
<a href="#figure-8-1">Figure 8.1</a>: Images can be broken into local patterns such as edges, textures, and so on.
</figcaption>
</figure>

<p>This key characteristic gives ConvNets two interesting properties:</p>
<ul>
<li><em>The patterns they learn are translation invariant</em>. After
learning a certain pattern in the
lower-right corner of a picture, a ConvNet can recognize it anywhere: for
example, in the upper-left corner. A densely connected model would have to
learn the pattern anew if it appeared at a new location. This makes ConvNets
data efficient when processing images —
because <em>the visual world is fundamentally translation invariant</em>.
They need fewer training samples to
learn representations that have generalization power.</li>
</ul>
<ul>
<li><em>They can learn spatial hierarchies of patterns (see figure 8.2)</em>. A first
convolution layer will learn small local patterns such as edges, a second
convolution layer will learn larger patterns made of the features of the first
layers, and so on. This allows ConvNets to efficiently learn increasingly
complex and abstract visual concepts —
because <em>the visual world is fundamentally spatially hierarchical</em>.</li>
</ul>
<figure id="figure-8-2">
<img src="../Images/fd7bec6c7a9246339ceec605eca1d67b.png" data-original-src="https://deeplearningwithpython.io/images/ch08/visual_hierarchy_hires.40ec558e.png"/>
<figcaption>
<a href="#figure-8-2">Figure 8.2</a>: The visual world forms a spatial hierarchy of visual modules: elementary lines or textures combine into simple objects such as eyes or ears, which combine into high-level concepts such as “cat.”
</figcaption>
</figure>

<p>Convolutions operate over rank-3 tensors, called <em>feature maps</em>, with
two spatial axes (<em>height</em> and <em>width</em>) as well as a <em>depth</em> axis (also
called the <em>channels</em> axis). For an RGB image, the dimension of the depth axis
is 3, because the image has three color channels: red, green, and blue. For a
black-and-white picture, like the MNIST digits, the depth is 1 (levels of
gray). The convolution operation extracts patches from its input feature map
and applies the same transformation to all of these patches, producing an
<em>output feature map</em>. This output feature map is still a rank-3 tensor: it has a
width and a height. Its depth can be arbitrary because the output depth is a
parameter of the layer, and the different channels in that depth axis no
longer stand for specific colors as in RGB input; rather, they stand for
<em>filters</em>. Filters encode specific aspects of the input data: at a high level,
a single filter could encode the concept “presence of a face in the input,”
for instance.</p>
<p>In the MNIST example, the first convolution layer takes a feature map of size
<code>(28, 28, 1)</code> and outputs a feature map of size <code>(26, 26, 64)</code>: it computes 64
filters over its input. Each of these 64 output channels contains a 26 × 26
grid of values, which is a <em>response map</em> of the filter over the input,
indicating the response of that filter pattern at different locations in the
input (see figure 8.3).
That is what the term <em>feature map</em> means: every dimension in the depth axis is
a feature (or filter), and the rank-2 tensor <code>output[:, :, n]</code> is the 2D spatial
<em>map</em> of the response of this filter over the input.</p>
<figure id="figure-8-3" class="small-image">
<img src="../Images/d506560758d6cee6f0fc59966eb67536.png" data-original-src="https://deeplearningwithpython.io/images/ch08/response_map_hires.ab2ee335.png"/>
<figcaption>
<a href="#figure-8-3">Figure 8.3</a>: The concept of a response map: a 2D map of the presence of a pattern at different locations in an input
</figcaption>
</figure>

<p>Convolutions are defined by two key parameters:</p>
<ul>
<li><em>Size of the patches extracted from the inputs</em>  —  These are typically 3 × 3
or 5 × 5. In the example, they were 3 × 3, which is a common choice.</li>
</ul>
<ul>
<li><em>Depth of the output feature map</em>  —  The number of filters computed by the
convolution. The example started with a depth of 32 and ended with a depth of
64.</li>
</ul>
<p>In Keras <code>Conv2D</code> layers, these parameters are the first
arguments passed to the layer:
<code>Conv2D(output_depth, (window_height, window_width))</code>.</p>
<p>A convolution works by <em>sliding</em> these windows of size 3 × 3 or 5 × 5 over the
3D input feature map, stopping at every possible location, and extracting the
3D patch of surrounding features of shape <code>(window_height, window_width, input_depth)</code>.
Each such 3D patch is then transformed into a 1D vector of shape <code>(output_depth,)</code>,
which is done via a tensor product
with a learned weight matrix, called the <em>convolution kernel</em> —
the same kernel is reused across every patch.
All of these vectors (one per patch) are then spatially
reassembled into a 3D output map of shape <code>(height, width, output_depth)</code>.
Every spatial location in the output feature map corresponds to the same
location in the input feature map (for example, the lower-right corner of the
output contains information about the lower-right corner of the input). For
instance, with 3 × 3 windows, the vector <code>output[i, j, :]</code> comes from the 3D
patch <code>input[i-1:i+1, j-1:j+1, :]</code>. The full process is detailed in figure 8.4.</p>
<figure id="figure-8-4">
<img src="../Images/934c8e82aac0247164ceb48739a78d9d.png" data-original-src="https://deeplearningwithpython.io/images/ch08/how_convolution_works.fb611af4.png"/>
<figcaption>
<a href="#figure-8-4">Figure 8.4</a>: How convolution works
</figcaption>
</figure>

<p>Note that the output width and height may differ from the input width and
height. They may differ for two reasons:</p>
<ul>
<li>Border effects, which can be countered by padding the input feature map</li>
<li>The use of <em>strides</em>, which we’ll define in a second</li>
</ul>
<p>Let’s take a deeper look at these notions.</p>
<h4 id="understanding-border-effects-and-padding">Understanding border effects and padding</h4>
<p>Consider a 5 × 5 feature map (25 tiles total). There are only 9
tiles around which you can center a 3 × 3 window, forming a 3 × 3 grid (see
figure 8.5). Hence, the output feature map will be 3 × 3. It shrinks a little:
by exactly two tiles alongside each dimension, in this case. You can see this
border effect in action in the earlier example: you start with 28 × 28 inputs,
which become 26 × 26 after the first convolution layer.</p>
<figure id="figure-8-5">
<img src="../Images/c33e84c0fad9d7d5fd595d8177a06c46.png" data-original-src="https://deeplearningwithpython.io/images/ch08/3x3_patches_in_5x5_input.3954b81b.png"/>
<figcaption>
<a href="#figure-8-5">Figure 8.5</a>: Valid locations of 3 × 3 patches in a 5 × 5 input feature map
</figcaption>
</figure>

<p>If you want to get an output feature map with the same spatial dimensions as
the input, you can use <em>padding</em>. Padding consists of adding an appropriate
number of rows and columns on each side of the input feature map so as to make
it possible to fit centered convolution windows around every input tile. For a 3
× 3 window, you add one column on the right, one column on the left, one row
at the top, and one row at the bottom. For a 5 × 5 window, you add two rows
(see figure 8.6).</p>
<figure id="figure-8-6">
<img src="../Images/c875426d9fe089ca3ef09266ff1bdd92.png" data-original-src="https://deeplearningwithpython.io/images/ch08/padding_of_5x5_input.fb864a53.png"/>
<figcaption>
<a href="#figure-8-6">Figure 8.6</a>: Padding a 5 × 5 input to be able to extract 25 3 × 3 patches
</figcaption>
</figure>

<p>In <code>Conv2D</code> layers, padding is configurable via the <code>padding</code> argument, which
takes two values: <code>"valid"</code>, which means no padding (only valid window
locations will be used); and <code>"same"</code>, which means “pad in such a way as to
have an output with the same width and height as the input.” The <code>padding</code>
argument defaults to <code>"valid"</code>.</p>
<h4 id="understanding-convolution-strides">Understanding convolution strides</h4>
<p>The other factor that can influence output size is the
notion of <em>strides</em>. The description of convolution so far has assumed that
the center tiles of the convolution windows are all contiguous. But the
distance between two successive windows is a parameter of the convolution,
called its <em>stride</em>, which defaults to 1. It’s possible to
have <em>strided convolutions</em>: convolutions with a stride higher
than 1. In figure 8.7, you can see the patches extracted by a 3 × 3
convolution with stride 2 over a 5 × 5 input (without padding)</p>
<figure id="figure-8-7">
<img src="../Images/fa3a3d514d7e28b55abc09e59d9e9dfd.png" data-original-src="https://deeplearningwithpython.io/images/ch08/strides.78c3a935.png"/>
<figcaption>
<a href="#figure-8-7">Figure 8.7</a>: 3 × 3 convolution patches with 2 × 2 strides
</figcaption>
</figure>

<p>Using stride 2 means the width and height of the feature map are downsampled by
a factor of 2 (in addition to any changes induced by border effects). Strided
convolutions are rarely used in classification models, but they come in handy for
some types of models, as you will find out in the next chapter.</p>
<p>In classification models, instead of strides, we tend to use the
<em>max-pooling</em> operation to downsample feature maps — which you saw in action in
our first ConvNet example. Let’s look at it in more depth.</p>
<h3 id="the-max-pooling-operation">The max-pooling operation</h3>
<p>In the ConvNet example, you may have noticed that
the size of the feature maps is halved after every <code>MaxPooling2D</code> layer. For
instance, before the first <code>MaxPooling2D</code> layers, the
feature map is 26 × 26, but the max-pooling operation halves it to 13
× 13. That’s the role of max pooling: to aggressively downsample feature maps,
much like strided convolutions.</p>
<p>Max pooling consists of extracting windows from the input feature maps and
outputting the max value of each channel. It’s conceptually similar to
convolution, except that instead of transforming local patches via a learned
linear transformation (the convolution kernel), they’re transformed via a
hardcoded <code>max</code> tensor operation. A big difference from convolution is that
max pooling is usually done with 2 × 2 windows and stride 2 to
downsample the feature maps by a factor of 2. On the other hand, convolution
is typically done with 3 × 3 windows and no stride (stride 1).</p>
<p>Why downsample feature maps this way? Why not remove the max-pooling layers and
keep fairly large feature maps all the way up? Let’s look at this option.
Our model would then look like this.</p>
<figure id="listing-8-5">
<pre><code class="language-python">inputs = keras.Input(shape=(28, 28, 1))
x = layers.Conv2D(filters=64, kernel_size=3, activation="relu")(inputs)
x = layers.Conv2D(filters=128, kernel_size=3, activation="relu")(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.GlobalAveragePooling2D()(x)
outputs = layers.Dense(10, activation="softmax")(x)
model_no_max_pool = keras.Model(inputs=inputs, outputs=outputs)
</code></pre>
<figcaption>
<a href="#listing-8-5">Listing 8.5</a>: An incorrectly structured ConvNet missing its max-pooling layers
</figcaption>
</figure>

<p>Here’s a summary of the model:</p>
<figure>
<pre><code class="language-python">&gt;&gt;&gt; model_no_max_pool.summary()</code>
<code class="language-output">Model: "functional_1"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ input_layer_1 (InputLayer)        │ (None, 28, 28, 1)        │             0 │
├───────────────────────────────────┼──────────────────────────┼───────────────┤
│ conv2d_3 (Conv2D)                 │ (None, 26, 26, 64)       │           640 │
├───────────────────────────────────┼──────────────────────────┼───────────────┤
│ conv2d_4 (Conv2D)                 │ (None, 24, 24, 128)      │        73,856 │
├───────────────────────────────────┼──────────────────────────┼───────────────┤
│ conv2d_5 (Conv2D)                 │ (None, 22, 22, 256)      │       295,168 │
├───────────────────────────────────┼──────────────────────────┼───────────────┤
│ global_average_pooling2d_1        │ (None, 256)              │             0 │
│ (GlobalAveragePooling2D)          │                          │               │
├───────────────────────────────────┼──────────────────────────┼───────────────┤
│ dense_1 (Dense)                   │ (None, 10)               │         2,570 │
└───────────────────────────────────┴──────────────────────────┴───────────────┘
 Total params: 372,234 (1.42 MB)
 Trainable params: 372,234 (1.42 MB)
 Non-trainable params: 0 (0.00 B)</code></pre>
</figure>

<p>What’s wrong with this setup? Two things:</p>
<ul>
<li>It isn’t conducive to learning a spatial hierarchy of features. The 3 × 3
windows in the third layer will only contain information coming from 7 × 7
windows in the initial input. The high-level patterns learned by the ConvNet
will still be very small with regard to the initial input, which may not be
enough to learn to classify digits (try recognizing a digit by only looking at
it through windows that are 7 × 7 pixels!). We need the features from the last
convolution layer to contain information about the totality of the input.</li>
</ul>
<ul>
<li>The final feature map has dimensions 22 × 22. That’s huge — when you take the
average of each 22 × 22 feature map, you are going to be destroying a lot of information
compared to when your feature maps were only 3 × 3.</li>
</ul>
<p>In short, the reason to use downsampling is to reduce the size of the feature maps,
making the information they contain increasingly less spatially distributed and increasingly
contained in the channels, while also inducing spatial-filter hierarchies by
making successive convolution layers “look” at increasingly large windows (in
terms of the fraction of the original input image they cover).</p>
<p>Note that max pooling isn’t the only way you can achieve such downsampling. As
you already know, you can also use strides in the prior convolution layer. And
you can use average pooling instead of max pooling, where each local input
patch is transformed by taking the average value of each channel over the
patch, rather than the max. But max pooling tends to work better than these
alternative solutions. In a nutshell, the reason is that features tend to
encode the spatial presence of some pattern or concept over the different
tiles of the feature map (hence the term <em>feature map</em>),
and it’s more informative to look at the <em>maximal presence</em> of different
features than at their <em>average presence</em>. So the most reasonable subsampling
strategy is to first produce dense maps of features (via unstrided
convolutions) and then look at the maximal activation of the features over
small patches, rather than looking at sparser windows of the inputs (via
strided convolutions) or averaging input patches, which could cause you to
miss or dilute feature-presence information.</p>
<p>At this point, you should understand the basics of ConvNets — feature maps,
convolution, and max pooling — and you know how to build a small ConvNet to
solve a toy problem such as MNIST digits classification. Now let’s move on to
more useful, practical applications.</p>
<h2 id="training-a-convnet-from-scratch-on-a-small-dataset">Training a ConvNet from scratch on a small dataset</h2>
<p>Having to train an
image-classification model using very little data is a common situation, which
you’ll likely encounter in practice if you ever do computer vision in a
professional context. A “few” samples can mean anywhere from a few hundred to
a few tens of thousands of images. As a practical example, we’ll focus on
classifying images as dogs or cats. We’ll work with a dataset containing
5,000 pictures of cats and dogs (2,500 cats, 2,500 dogs), taken from the original Kaggle dataset.
We’ll use 2,000 pictures for
training, 1,000 for validation, and 2,000 for testing.</p>
<p>In this section, we’ll review one basic strategy to tackle this problem:
training a new model from scratch using what little data we have. We’ll
start by naively training a small ConvNet on the 2,000 training samples,
without any regularization, to set a baseline for what can be achieved. This
will get us to a classification accuracy of about 80%. At that point, the main
issue will be overfitting. Then we’ll introduce <em>data augmentation</em>, a
powerful technique for mitigating overfitting in
computer vision. By using data augmentation, we’ll improve the model to
reach a test accuracy of about 84%.</p>
<p>In the next section, we’ll review two more essential techniques for applying
deep learning to small datasets: <em>feature extraction with a pretrained model</em>
and <em>fine-tuning a pretrained model</em> (which will get us to a final
accuracy of 98.5%). Together, these three strategies — training a small model from
scratch, doing feature extraction using a pretrained model, and fine-tuning a
pretrained model — will constitute your future toolbox for tackling the problem
of performing image classification with small datasets.</p>
<h3 id="the-relevance-of-deep-learning-for-small-data-problems">The relevance of deep learning for small-data problems</h3>
<p>What qualifies as “enough samples” to train a model is relative — relative to the
size and depth of the model you’re trying to train, for starters. It isn’t
possible to train a ConvNet to solve a complex problem with just a few tens of
samples, but a few hundred can potentially suffice if the model is small and
well regularized and the task is simple. Because ConvNets learn local,
translation-invariant features, they’re highly data efficient on perceptual
problems. Training a ConvNet from scratch on a very small image dataset will
still yield reasonable results despite a relative lack of data, without the
need for any custom feature engineering. You’ll see this in action in this
section.</p>
<p>What’s more, deep learning models are by nature highly repurposable: you can
take, say, an image-classification or speech-to-text model trained on a
large-scale dataset and reuse it on a significantly different problem with
only minor changes. Specifically, in the case of computer vision, many
pretrained classification models are publicly
available for download and can be used to bootstrap powerful vision models out
of very little data. This is one of the greatest strengths of deep learning:
feature reuse. You’ll explore this in the next section.</p>
<p>Let’s start by getting our hands on the data.</p>
<h3 id="downloading-the-data">Downloading the data</h3>
<p>The Dogs vs. Cats
dataset that we will use isn’t packaged with Keras. It was made available by
Kaggle as part of a computer-vision competition in late 2013, back when
ConvNets weren’t mainstream. You can download the original dataset from
<code>www.kaggle.com/c/dogs-vs-cats/data</code> (you’ll need to create a Kaggle account if
you don’t already have one — don’t worry, the process is painless). You
can also use the Kaggle API to download the dataset in Colab.</p>
<aside>
<p><span class="note-title">Downloading a Kaggle dataset in Google Colaboratory</span></p>
<p>Kaggle makes available an easy-to-use API to programmatically download
Kaggle-hosted datasets. You can use it to download the Dogs vs. Cats dataset
to a Colab notebook, for instance. This API is available via the <code>kagglehub</code>
package, which is preinstalled on Colab.</p>
<p>Before we can download the dataset, we will need to do two things:</p>
<ol>
<li>Go to <a href="https://www.kaggle.com/">https://www.kaggle.com/</a> and sign in.</li>
<li>Go to <a href="https://www.kaggle.com/c/dogs-vs-cats/data">https://www.kaggle.com/c/dogs-vs-cats/data</a>, scroll down and click to Join the Competition.</li>
<li>Go to <a href="https://www.kaggle.com/settings">https://www.kaggle.com/settings</a> and generate a Kaggle API key.</li>
</ol>
<p>With that we are ready to download the data in our notebook. First, log in with
your Kaggle API key:</p>
<figure>
<pre><code class="language-python">import kagglehub

kagglehub.login()
</code></pre>
</figure>

<p>Then, download the competition data:</p>
<figure>
<pre><code class="language-python">download_path = kagglehub.competition_download("dogs-vs-cats")
</code></pre>
</figure>

<p>This downloads two new files, <code>train.zip</code> (the training data) and
<code>test1.zip</code> (the test data). We’ll only use the training data here.
Let’s unzip it:</p>
<figure>
<pre><code class="language-python">import zipfile

with zipfile.ZipFile(download_path + "/train.zip", "r") as zip_ref:
    zip_ref.extractall(".")
</code></pre>
</figure>

<p>All done!</p>
</aside>

<p>The pictures in our dataset are medium-resolution color JPEGs.
Figure 8.8 shows some examples.</p>
<figure id="figure-8-8">
<img src="../Images/f90c51c5a2e790835b3e0c4525ac4835.png" data-original-src="https://deeplearningwithpython.io/images/ch08/dog_and_cat_samples.d2409a95.png"/>
<figcaption>
<a href="#figure-8-8">Figure 8.8</a>: Samples from the Dogs vs. Cats dataset. Sizes weren’t modified: the samples come in different sizes, colors, backgrounds, etc.
</figcaption>
</figure>

<p>Unsurprisingly, the original dogs-versus-cats Kaggle competition, all the way
back in 2013, was won by entrants who used ConvNets. The best entries achieved
up to 95% accuracy. In this example, we will get fairly close to this accuracy
(in the next section), even though we will train our models on less than 10%
of the data that was available to the competitors.</p>
<p>This dataset contains 25,000 images of dogs and cats (12,500 from each class)
and is 543 MB (compressed). After downloading and uncompressing the data, we’ll
create a new dataset containing three subsets: a training set with 1,000
samples of each class, a validation set with 500 samples of each class, and a
test set with 1,000 samples of each class. Why do this? Because many of the
image datasets you’ll encounter in your career only contain a few thousand
samples, not tens of thousands. Having more data available would make the
problem easier — so it’s good practice to learn with a small dataset.</p>
<p>The subsampled dataset we will work with will have the following directory
structure:</p>
<figure>
<pre><code class="language-text">dogs_vs_cats_small/
...train/
# Contains 1,000 cat images
......cat/
# Contains 1,000 dog images
......dog/
...validation/
# Contains 500 cat images
......cat/
# Contains 500 dog images
......dog/
...test/
# Contains 1,000 cat images
......cat/
# Contains 1,000 dog images
......dog/
</code></pre>
</figure>

<p>Let’s make it happen in a coupl of calls to <code>shutil</code>, a Python library for
running shell-like commands.</p>
<figure id="listing-8-6">
<pre><code class="language-python">import os, shutil, pathlib

# Path to the directory where the original dataset was uncompressed
original_dir = pathlib.Path("train")
# Directory where we will store our smaller dataset
new_base_dir = pathlib.Path("dogs_vs_cats_small")

# Utility function to copy cat (respectively, dog) images from index
# `start_index` to index `end_index` to the subdirectory
# `new_base_dir/{subset_name}/cat` (respectively, dog). "subset_name"
# will be either "train," "validation," or "test."
def make_subset(subset_name, start_index, end_index):
    for category in ("cat", "dog"):
        dir = new_base_dir / subset_name / category
        os.makedirs(dir)
        fnames = [f"{category}.{i}.jpg" for i in range(start_index, end_index)]
        for fname in fnames:
            shutil.copyfile(src=original_dir / fname, dst=dir / fname)

# Creates the training subset with the first 1,000 images of each
# category
make_subset("train", start_index=0, end_index=1000)
# Creates the validation subset with the next 500 images of each
# category
make_subset("validation", start_index=1000, end_index=1500)
# Creates the test subset with the next 1,000 images of each category
make_subset("test", start_index=1500, end_index=2500)
</code></pre>
<figcaption>
<a href="#listing-8-6">Listing 8.6</a>: Copying images to training, validation, and test directories
</figcaption>
</figure>

<p>We now have 2,000 training images, 1,000 validation images, and 2,000
test images. Each split contains the same number of samples from each class:
this is a balanced binary classification problem, which means classification
accuracy will be an appropriate measure of success.</p>
<h3 id="building-your-model">Building your model</h3>
<p>We will reuse the same general model structure you saw in the first example:
the ConvNet will be a stack of alternated <code>Conv2D</code> (with <code>relu</code> activation)
and <code>MaxPooling2D</code> layers.</p>
<p>But because we’re dealing with bigger images and a more complex problem,
we’ll make our model larger, accordingly: it will have two more <code>Conv2D</code> +
<code>MaxPooling2D</code> stages. This serves both to augment the capacity of the model
and to further reduce the size of the feature maps so they aren’t overly large
when we reach the pooling layer. Here, because we start
from inputs of size 180  × 180 pixels (a somewhat arbitrary choice),
we end up with feature maps of size 7 × 7 just before the
<code>GlobalAveragePooling2D</code> layer.</p>
<aside>
<p>The depth of the feature maps progressively increases in the model
(from 32 to 512), whereas the size of the feature maps decreases (from 180 ×
180 to 7 × 7). This is a pattern you’ll see in almost all ConvNets.</p>
</aside>

<p>Because we’re looking at a binary classification problem, we’ll end the
model with a single unit (a <code>Dense</code> layer of size 1) and a <code>sigmoid</code>
activation. This unit will encode the probability that the model is looking
at one class or the other.</p>
<p>One last small difference: we will start the model with a <code>Rescaling</code>
layer, which will rescale image inputs (whose values are originally in the
[0, 255] range) to the [0, 1] range.</p>
<figure id="listing-8-7">
<pre><code class="language-python">import keras
from keras import layers

# The model expects RGB images of size 180 x 180.
inputs = keras.Input(shape=(180, 180, 3))
# Rescales inputs to the [0, 1] range by dividing them by 255
x = layers.Rescaling(1.0 / 255)(inputs)
x = layers.Conv2D(filters=32, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=64, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=128, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=512, kernel_size=3, activation="relu")(x)
# Flattens the 3D activations with shape (height, width, 512) into 1D
# activations with shape (512,) by averaging them over spatial
# dimensions
x = layers.GlobalAveragePooling2D()(x)
outputs = layers.Dense(1, activation="sigmoid")(x)
model = keras.Model(inputs=inputs, outputs=outputs)
</code></pre>
<figcaption>
<a href="#listing-8-7">Listing 8.7</a>: Instantiating a small ConvNet for dogs vs. cats classification
</figcaption>
</figure>

<p>Let’s look at how the dimensions of the feature maps change with every
successive layer:</p>
<figure>
<pre><code class="language-python">&gt;&gt;&gt; model.summary()</code>
<code class="language-output">Model: "functional_2"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ input_layer_2 (InputLayer)        │ (None, 180, 180, 3)      │             0 │
├───────────────────────────────────┼──────────────────────────┼───────────────┤
│ rescaling (Rescaling)             │ (None, 180, 180, 3)      │             0 │
├───────────────────────────────────┼──────────────────────────┼───────────────┤
│ conv2d_6 (Conv2D)                 │ (None, 178, 178, 32)     │           896 │
├───────────────────────────────────┼──────────────────────────┼───────────────┤
│ max_pooling2d_2 (MaxPooling2D)    │ (None, 89, 89, 32)       │             0 │
├───────────────────────────────────┼──────────────────────────┼───────────────┤
│ conv2d_7 (Conv2D)                 │ (None, 87, 87, 64)       │        18,496 │
├───────────────────────────────────┼──────────────────────────┼───────────────┤
│ max_pooling2d_3 (MaxPooling2D)    │ (None, 43, 43, 64)       │             0 │
├───────────────────────────────────┼──────────────────────────┼───────────────┤
│ conv2d_8 (Conv2D)                 │ (None, 41, 41, 128)      │        73,856 │
├───────────────────────────────────┼──────────────────────────┼───────────────┤
│ max_pooling2d_4 (MaxPooling2D)    │ (None, 20, 20, 128)      │             0 │
├───────────────────────────────────┼──────────────────────────┼───────────────┤
│ conv2d_9 (Conv2D)                 │ (None, 18, 18, 256)      │       295,168 │
├───────────────────────────────────┼──────────────────────────┼───────────────┤
│ max_pooling2d_5 (MaxPooling2D)    │ (None, 9, 9, 256)        │             0 │
├───────────────────────────────────┼──────────────────────────┼───────────────┤
│ conv2d_10 (Conv2D)                │ (None, 7, 7, 512)        │     1,180,160 │
├───────────────────────────────────┼──────────────────────────┼───────────────┤
│ global_average_pooling2d_2        │ (None, 512)              │             0 │
│ (GlobalAveragePooling2D)          │                          │               │
├───────────────────────────────────┼──────────────────────────┼───────────────┤
│ dense_2 (Dense)                   │ (None, 1)                │           513 │
└───────────────────────────────────┴──────────────────────────┴───────────────┘
 Total params: 1,569,089 (5.99 MB)
 Trainable params: 1,569,089 (5.99 MB)
 Non-trainable params: 0 (0.00 B)</code></pre>
</figure>

<p>For the compilation step, you’ll go with the <code>adam</code> optimizer, as usual.
Because you ended the model with a single sigmoid unit, you’ll use binary
crossentropy as the loss (as a reminder, check out table 6.1 in chapter 6
  for a cheat sheet on what loss function to use in various situations).</p>
<figure id="listing-8-8">
<pre><code class="language-python">model.compile(
    loss="binary_crossentropy",
    optimizer="adam",
    metrics=["accuracy"],
)
</code></pre>
<figcaption>
<a href="#listing-8-8">Listing 8.8</a>: Configuring the model for training
</figcaption>
</figure>

<h3 id="data-preprocessing">Data preprocessing</h3>
<p>As you know by now, data should be formatted
into appropriately preprocessed floating-point
tensors before being fed into the model. Currently, the data sits on a drive
as JPEG files, so the steps for getting it into the model are roughly as
follows:</p>
<ol>
<li>Read the picture files.</li>
<li>Decode the JPEG content to RGB grids of pixels.</li>
<li>Convert these into floating-point tensors.</li>
<li>Resize them to a shared size (we’ll use 180 x 180).</li>
<li>Pack them into batches (we’ll use batches of 32 images).</li>
</ol>
<p>This may seem a bit daunting, but fortunately Keras has utilities to take care of
these steps automatically.
In particular, Keras features the utility function
<code>image_dataset_from_directory</code>, which lets you quickly set up a data pipeline
that can automatically turn image files on disk into batches of preprocessed tensors.
This is what you’ll use here.</p>
<p>Calling <code>image_dataset_from_directory(directory)</code> will first
list the subdirectories of <code>directory</code> and assume each one contains images
from one of your classes. It will then index the image files in each subdirectory.
Finally, it will create and return a <code>tf.data.Dataset</code> object
configured to read these files, shuffle them, decode them to tensors,
resize them to a shared size, and pack them into batches.</p>
<figure id="listing-8-9">
<pre><code class="language-python">from keras.utils import image_dataset_from_directory

batch_size = 64
image_size = (180, 180)
train_dataset = image_dataset_from_directory(
    new_base_dir / "train", image_size=image_size, batch_size=batch_size
)
validation_dataset = image_dataset_from_directory(
    new_base_dir / "validation", image_size=image_size, batch_size=batch_size
)
test_dataset = image_dataset_from_directory(
    new_base_dir / "test", image_size=image_size, batch_size=batch_size
)
</code></pre>
<figcaption>
<a href="#listing-8-9">Listing 8.9</a>: Using <code>image_dataset_from_directory</code> to read images from directories
</figcaption>
</figure>

<h4 id="understanding-tensorflow-dataset-objects">Understanding TensorFlow Dataset objects</h4>
<p>TensorFlow makes available the <code>tf.data</code> API to create efficient input pipelines
for machine learning models. Its core class is <code>tf.data.Dataset</code>.</p>
<p>The <code>Dataset</code> class can be used for data loading and preprocessing in any framework
— not just TensorFlow. You can use it together with JAX or PyTorch.
When you use it with a Keras model, it works the same, independently of the backend
you’re currently using.</p>
<p>A <code>Dataset</code> object is an iterator: you can use it in a <code>for</code> loop. It will
typically return batches of input data and labels. You can pass a <code>Dataset</code>
object directly to the <code>fit()</code> method of a Keras model.</p>
<p>The <code>Dataset</code> class handles many key features that would otherwise be cumbersome
to implement yourself, in particular parallelization of the preprocessing logic
across multiple CPU cores, as well as asynchronous data prefetching
(preprocessing the next batch of data while the previous one is being handled
by the model, which keeps execution flowing without interruptions).</p>
<p>The <code>Dataset</code> class also exposes a functional-style API for modifying datasets.
Here’s a quick example: let’s create a <code>Dataset</code> instance from a NumPy array
of random numbers. We’ll consider 1,000 samples, where each sample is a vector
of size 16.</p>
<figure id="listing-8-10">
<pre><code class="language-python">import numpy as np
import tensorflow as tf

random_numbers = np.random.normal(size=(1000, 16))
# The from_tensor_slices() class method can be used to create a Dataset
# from a NumPy array or a tuple or dict of NumPy arrays.
dataset = tf.data.Dataset.from_tensor_slices(random_numbers)
</code></pre>
<figcaption>
<a href="#listing-8-10">Listing 8.10</a>: Instantiating a <code>Dataset</code> from a NumPy array
</figcaption>
</figure>

<p>At first, our dataset just yields single samples.</p>
<figure id="listing-8-11">
<pre><code class="language-python">&gt;&gt;&gt; for i, element in enumerate(dataset):
&gt;&gt;&gt;     print(element.shape)
&gt;&gt;&gt;     if i &gt;= 2:
&gt;&gt;&gt;         break</code>
<code class="language-output">(16,)
(16,)
(16,)</code></pre>
<figcaption>
<a href="#listing-8-11">Listing 8.11</a>: Iterating on a dataset
</figcaption>
</figure>

<p>You can use the <code>.batch()</code> method to batch the data.</p>
<figure id="listing-8-12">
<pre><code class="language-python">&gt;&gt;&gt; batched_dataset = dataset.batch(32)
&gt;&gt;&gt; for i, element in enumerate(batched_dataset):
&gt;&gt;&gt;     print(element.shape)
&gt;&gt;&gt;     if i &gt;= 2:
&gt;&gt;&gt;         break</code>
<code class="language-output">(32, 16)
(32, 16)
(32, 16)</code></pre>
<figcaption>
<a href="#listing-8-12">Listing 8.12</a>: Batching a dataset
</figcaption>
</figure>

<p>More broadly, you have access to a range of useful dataset methods, such as these:</p>
<ul>
<li><code>.shuffle(buffer_size)</code> will shuffle elements within a buffer.</li>
<li><code>.prefetch(buffer_size)</code> will prefetch a buffer of elements in GPU memory
to achieve better device utilization.</li>
<li><code>.map(callable)</code> will apply an arbitrary transformation to each element of the dataset
  (the function <code>callable</code>, expected to take as input a single element yielded by the dataset).</li>
</ul>
<p>The method <code>.map(function, num_parallel_calls)</code> in particular is one that you will use often. Here’s an
example: let’s use it to reshape the elements in our toy dataset from shape <code>(16,)</code>
to shape <code>(4, 4)</code>.</p>
<figure id="listing-8-13">
<pre><code class="language-python">&gt;&gt;&gt; reshaped_dataset = dataset.map(
...     lambda x: tf.reshape(x, (4, 4)),
...     num_parallel_calls=8)
&gt;&gt;&gt; for i, element in enumerate(reshaped_dataset):
...     print(element.shape)
...     if i &gt;= 2:
...         break</code>
<code class="language-output">(4, 4)
(4, 4)
(4, 4)</code></pre>
<figcaption>
<a href="#listing-8-13">Listing 8.13</a>: Applying a transformation to <code>Dataset</code> elements using <code>map()</code>
</figcaption>
</figure>

<p>You’re about to see more <code>map()</code> action over the next chapters.</p>
<h4 id="fitting-the-model">Fitting the model</h4>
<p>Let’s look at the output of one of these <code>Dataset</code> objects: it yields batches of
180 × 180 RGB images (shape <code>(32, 180, 180, 3)</code>) and integer labels
(shape <code>(32,)</code>). There are 32 samples in each batch (the batch size).</p>
<figure id="listing-8-14">
<pre><code class="language-python">&gt;&gt;&gt; for data_batch, labels_batch in train_dataset:
&gt;&gt;&gt;     print("data batch shape:", data_batch.shape)
&gt;&gt;&gt;     print("labels batch shape:", labels_batch.shape)
&gt;&gt;&gt;     break</code>
<code class="language-output">data batch shape: (32, 180, 180, 3)
labels batch shape: (32,)</code></pre>
<figcaption>
<a href="#listing-8-14">Listing 8.14</a>: Displaying the shapes yielded by the <code>Dataset</code>
</figcaption>
</figure>

<p>Let’s fit the model on our dataset. We use the <code>validation_data</code> argument
in <code>fit()</code> to monitor validation metrics on a separate <code>Dataset</code> object.</p>
<p>Note that we also use a <code>ModelCheckpoint</code> callback to save the model
after each epoch. We configure it with the path where to save the file, as
well as the arguments <code>save_best_only=True</code> and <code>monitor="val_loss"</code>: they
tell the callback to only save a new file (overwriting any previous one)
when the current value of the <code>val_loss</code> metric is lower than at any previous
time during training. This guarantees that your saved file will always
contain the state of the model corresponding to its best-performing training
epoch, in terms of its performance on the validation data.
As a result, we won’t have to retrain a new model for a lower number of epochs
if we start overfitting: we can just reload our saved file.</p>
<figure id="listing-8-15">
<pre><code class="language-python">callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath="convnet_from_scratch.keras",
        save_best_only=True,
        monitor="val_loss",
    )
]
history = model.fit(
    train_dataset,
    epochs=50,
    validation_data=validation_dataset,
    callbacks=callbacks,
)
</code></pre>
<figcaption>
<a href="#listing-8-15">Listing 8.15</a>: Fitting the model using a <code>Dataset</code>
</figcaption>
</figure>

<p>Let’s plot the loss and accuracy of the model over the training and validation
data during training (see figure 8.9).</p>
<figure id="listing-8-16">
<pre><code class="language-python">import matplotlib.pyplot as plt

accuracy = history.history["accuracy"]
val_accuracy = history.history["val_accuracy"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]
epochs = range(1, len(accuracy) + 1)

plt.plot(epochs, accuracy, "r--", label="Training accuracy")
plt.plot(epochs, val_accuracy, "b", label="Validation accuracy")
plt.title("Training and validation accuracy")
plt.legend()
plt.figure()

plt.plot(epochs, loss, "r--", label="Training loss")
plt.plot(epochs, val_loss, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.legend()
plt.show()
</code></pre>
<figcaption>
<a href="#listing-8-16">Listing 8.16</a>: Displaying curves of loss and accuracy during training
</figcaption>
</figure>

<figure id="figure-8-9">
<img src="../Images/e154cf5f9475f3c0d64a49453174de12.png" data-original-src="https://deeplearningwithpython.io/images/ch08/cats-and-dogs-1-training-and-validation-acc.c0b7aa87.png"/>
<img src="../Images/19d507efdd59c5b2077f28641a60ec3a.png" data-original-src="https://deeplearningwithpython.io/images/ch08/cats-and-dogs-1-training-and-validation-loss.cbe4e0a3.png"/>
<figcaption>
<a href="#figure-8-9">Figure 8.9</a>: Training and validation metrics for a simple ConvNet
</figcaption>
</figure>

<p>These plots are characteristic of overfitting. The training accuracy increases
linearly over time, until it reaches nearly 100%, whereas the validation
accuracy peaks around 80%. The validation loss reaches its minimum after only
10 epochs and then stalls, whereas the training loss keeps decreasing
linearly as training proceeds.</p>
<p>Let’s check the test accuracy. We’ll reload the model from its saved file
to evaluate it as it was before it started overfitting.</p>
<figure id="listing-8-17">
<pre><code class="language-python">test_model = keras.models.load_model("convnet_from_scratch.keras")
test_loss, test_acc = test_model.evaluate(test_dataset)
print(f"Test accuracy: {test_acc:.3f}")
</code></pre>
<figcaption>
<a href="#listing-8-17">Listing 8.17</a>: Evaluating the model on the test set
</figcaption>
</figure>

<p>We get a test accuracy of 78.6% (due to the randomness of neural network
initializations, you may get numbers within a few percentage points of that).</p>
<p>Because you have relatively few training samples (2,000), overfitting will be
your number-one concern. You already know about a number of techniques that
can help mitigate overfitting, such as dropout and weight decay (L2
regularization). We’re now going to work with a new one, specific to computer
vision and used almost universally when processing images with deep learning
models: <em>data augmentation</em>.</p>
<h3 id="using-data-augmentation">Using data augmentation</h3>
<p>Overfitting is caused by having too few samples to learn from,
rendering you unable to train a model that can generalize to new data.
Given infinite data, your model would be exposed to every possible aspect
of the data distribution at hand: you would never overfit.
Data augmentation takes the approach of generating
more training data from existing training samples, by <em>augmenting</em> the samples
via a number of random transformations that yield believable-looking images.
The goal is that at training time, your model will never see the exact same
picture twice. This helps expose the model to more aspects of the data and
generalize better.</p>
<p>In Keras, this can be done via <em>data augmentation layers</em>. Such layers
could be added in one of two ways:</p>
<ul>
<li><em>At the start of the model</em> — <em>Inside</em> the model. In our case, the layers would
come right before the <code>Rescaling</code> layer.</li>
<li><em>Inside the data pipeline</em> — <em>Outside</em> the model. In our case, we’d apply them to our
<code>Dataset</code> via a <code>map()</code> call.</li>
</ul>
<p>The main difference between these two options is that data augmentation done inside the model would be running on the GPU,
just like the rest of the model. Meanwhile, data augmentation done in the data pipeline would be running on
the CPU, typically in a parallel way on multiple CPU cores. Sometimes, there can be performance benefits to doing the former, but
the latter is usually the better option. So let’s go with that!</p>
<figure id="listing-8-18">
<pre><code class="language-python"># Defines the transformations to apply as a list
data_augmentation_layers = [
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.2),
]

# Creates a function that applies them sequentially
def data_augmentation(images, targets):
    for layer in data_augmentation_layers:
        images = layer(images)
    return images, targets

# Maps this function into the dataset
augmented_train_dataset = train_dataset.map(
    data_augmentation, num_parallel_calls=8
)
# Enables prefetching of batches on GPU memory; important for best
# performance
augmented_train_dataset = augmented_train_dataset.prefetch(tf.data.AUTOTUNE)
</code></pre>
<figcaption>
<a href="#listing-8-18">Listing 8.18</a>: Defining a data augmentation stage
</figcaption>
</figure>

<p>These are just a few of the layers available (for more, see the Keras
documentation). Let’s quickly go over this code:</p>
<ul>
<li><code>RandomFlip("horizontal")</code> will apply horizontal flipping to a random 50%
of the images that go through it.</li>
<li><code>RandomRotation(0.1)</code> will rotate the input images by a random value
in the range [–10%, +10%] (these are fractions of a full circle — in degrees
  the range would be [–36 degrees, +36 degrees]).</li>
<li><code>RandomZoom(0.2)</code> will zoom in or out of the image by a random factor in the
range [–20%, +20%].</li>
</ul>
<p>Let’s look at the augmented images (see figure 8.10).</p>
<figure id="listing-8-19">
<pre><code class="language-python">plt.figure(figsize=(10, 10))
# You can use take(N) to only sample N batches from the dataset. This
# is equivalent to inserting a break in the loop after the Nth batch.
for image_batch, _ in train_dataset.take(1):
    image = image_batch[0]
    for i in range(9):
        ax = plt.subplot(3, 3, i + 1)
        augmented_image, _ = data_augmentation(image, None)
        augmented_image = keras.ops.convert_to_numpy(augmented_image)
        # Displays the first image in the output batch. For each of the
        # nine iterations, this is a different augmentation of the same
        # image.
        plt.imshow(augmented_image.astype("uint8"))
        plt.axis("off")
</code></pre>
<figcaption>
<a href="#listing-8-19">Listing 8.19</a>: Displaying some randomly augmented training images
</figcaption>
</figure>

<figure id="figure-8-10">
<img src="../Images/c23ec10d204d38425efe734fd98e79ec.png" data-original-src="https://deeplearningwithpython.io/images/ch08/augmented_data.63e74cdb.png"/>
<figcaption>
<a href="#figure-8-10">Figure 8.10</a>: Generating variations of a very good boy via random data augmentation
</figcaption>
</figure>

<p>If you train a new model using this data augmentation configuration, the
model will never see the same input twice. But the inputs it sees are still
heavily intercorrelated, because they come from a small number of original
images — you can’t produce new information; you can only remix existing
information. As such, this may not be enough to completely get rid of
overfitting. To further fight overfitting, you’ll also add a <code>Dropout</code> layer
to your model, right before the densely connected
classifier.</p>
<figure id="listing-8-20">
<pre><code class="language-python">inputs = keras.Input(shape=(180, 180, 3))
x = layers.Rescaling(1.0 / 255)(inputs)
x = layers.Conv2D(filters=32, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=64, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=128, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=512, kernel_size=3, activation="relu")(x)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dropout(0.25)(x)
outputs = layers.Dense(1, activation="sigmoid")(x)
model = keras.Model(inputs=inputs, outputs=outputs)

model.compile(
    loss="binary_crossentropy",
    optimizer="adam",
    metrics=["accuracy"],
)
</code></pre>
<figcaption>
<a href="#listing-8-20">Listing 8.20</a>: Defining a new ConvNet that includes dropout
</figcaption>
</figure>

<p>Let’s train the model using data augmentation and dropout. Because we expect
overfitting to occur much later during training, we will train for twice as
many epochs —  100. Note that we evaluate on images that aren’t augmented
— data augmentation is usually only performed at training time, as it is a regularization technique.</p>
<figure id="listing-8-21">
<pre><code class="language-python">callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath="convnet_from_scratch_with_augmentation.keras",
        save_best_only=True,
        monitor="val_loss",
    )
]
history = model.fit(
    augmented_train_dataset,
    # Since we expect the model to overfit slower, we train for more
    # epochs.
    epochs=100,
    validation_data=validation_dataset,
    callbacks=callbacks,
)
</code></pre>
<figcaption>
<a href="#listing-8-21">Listing 8.21</a>: Training the regularized ConvNet on augmented images
</figcaption>
</figure>

<p>Let’s plot the results again; see figure 8.11. Thanks to data
augmentation and dropout, we start overfitting much later, around epochs 60–70
(compared to epoch 10 for the original model). The validation accuracy ends up
peaking above 85% — a big improvement over our first try.</p>
<figure id="figure-8-11">
<img src="../Images/b2977f5c12049346adc8bb619d2010b7.png" data-original-src="https://deeplearningwithpython.io/images/ch08/cats-and-dogs-1-training-and-validation-da-acc.95f4446c.png"/>
<img src="../Images/7377fd180ae66d4f52ab60a6e968fd80.png" data-original-src="https://deeplearningwithpython.io/images/ch08/cats-and-dogs-1-training-and-validation-da-loss.fb77981b.png"/>
<figcaption>
<a href="#figure-8-11">Figure 8.11</a>: Training and validation metrics with data augmentation
</figcaption>
</figure>

<p>Let’s check the test accuracy.</p>
<figure id="listing-8-22">
<pre><code class="language-python">test_model = keras.models.load_model(
    "convnet_from_scratch_with_augmentation.keras"
)
test_loss, test_acc = test_model.evaluate(test_dataset)
print(f"Test accuracy: {test_acc:.3f}")
</code></pre>
<figcaption>
<a href="#listing-8-22">Listing 8.22</a>: Evaluating the model on the test set
</figcaption>
</figure>

<p>We get a test accuracy of 83.9%. It’s starting to look good! If you’re using
Colab, make sure to download the saved file (<code>convnet_from_scratch_with_augmentation.keras</code>),
as we will use it for some experiments in the next chapter.</p>
<p>By further tuning the model’s
configuration (such as the number of filters per convolution layer or the number
of layers in the model), you may be able to get an even better accuracy,
likely up to 90%. But it would prove difficult to go any higher just by
training your own ConvNet from scratch because you have so little data to
work with. As a next step to improve your accuracy on this problem, you’ll
have to use a pretrained model, which is the focus of the next two sections.</p>
<h2 id="using-a-pretrained-model">Using a pretrained model</h2>
<p>A common and highly effective approach to deep
learning on small image datasets is to use a
pretrained model. A <em>pretrained model</em> is a model that
was previously trained on a large dataset, typically on a large-scale
image-classification task. If this original dataset is large enough and
general enough, then the spatial hierarchy of features learned by the
pretrained model can effectively act as a generic model of the visual world,
and hence its features can prove useful for many different computer vision
problems, even though these new problems may involve completely different
classes than those of the original task. For instance, you might train a
model on ImageNet (where classes are mostly animals and everyday objects)
and then repurpose this trained model for something as remote as identifying
furniture items in images. Such portability of learned features across
different problems is a key advantage of deep learning compared to many older,
shallow learning approaches, and it makes deep learning very effective for
small-data problems.</p>
<p>In this case, let’s consider a large ConvNet trained on the ImageNet dataset
(1.4 million labeled images and 1,000 different classes). ImageNet contains
many animal classes, including different species of cats and dogs, and you can
thus expect it to perform well on the dogs-versus-cats classification problem.</p>
<p>We’ll use the Xception architecture. This may be your first encounter with one of
these cutesy model names — Xception, ResNet, EfficientNet,
and so on; you’ll get used to them if you keep doing deep learning for computer vision because they will come up frequently. You’ll learn about the architectural
details of Xception in the next chapter.</p>
<p>There are two ways to use a pretrained model: <em>feature extraction</em> and
<em>fine-tuning</em>. We’ll cover both of them. Let’s start with feature extraction.</p>
<h3 id="feature-extraction-with-a-pretrained-model">Feature extraction with a pretrained model</h3>
<p>Feature extraction consists of using the representations learned by
a previously trained model to extract interesting features from new samples.
These features are then run through a new classifier, which is trained from scratch.</p>
<p>As you saw previously, ConvNets used for image classification comprise two
parts: they start with a series of pooling and convolution layers, and they
end with a densely connected classifier. The first part is
called the <em>convolutional base</em> or <em>backbone</em> of the model. In the
case of ConvNets, feature extraction consists of taking the convolutional base
of a previously trained network, running the new data through it, and
training a new classifier on top of the output (see figure 8.12).</p>
<figure id="figure-8-12">
<img src="../Images/6ae9c33c2b3199361221a86298064605.png" data-original-src="https://deeplearningwithpython.io/images/ch08/swapping_fc_classifier.6e525b7a.png"/>
<figcaption>
<a href="#figure-8-12">Figure 8.12</a>: Swapping classifiers while keeping the same convolutional base
</figcaption>
</figure>

<p>Why only reuse the convolutional base? Could you reuse
the densely connected classifier as well? In general, doing so should be
avoided. The reason is that the representations learned by the convolutional
base are likely to be more generic and therefore more reusable: the feature
maps of a ConvNet are presence maps of generic concepts over a picture, which
is likely to be useful regardless of the computer vision problem at hand. But
the representations learned by the classifier will necessarily be specific to
the set of classes on which the model was trained — they will only contain
information about the presence probability of this or that class in the entire
picture. Additionally, representations found in densely connected layers no
longer contain any information about where objects are located in the input
image: these layers get rid of the notion of space, whereas the object
location is still described by convolutional feature maps. For problems where
object location matters, densely connected features are largely useless.</p>
<p>Note that the level of generality (and therefore reusability) of the
representations extracted by specific convolution layers depends on the depth
of the layer in the model. Layers that come earlier in the model extract
local, highly generic feature maps (such as visual edges, colors, and
textures), whereas layers that are higher up extract more abstract concepts
(such as “cat ear” or “dog eye”). So if your new dataset differs a lot from
the dataset on which the original model was trained, you may be better off
using only the first few layers of the model to do feature extraction, rather
than using the entire convolutional base.</p>
<p>In this case, because the ImageNet class set contains multiple dog and cat
classes, it’s likely to be beneficial to reuse the information contained in
the densely connected layers of the original model. But we’ll choose not to,
so we can cover the more general case where the class set of the new problem
doesn’t overlap the class set of the original model. Let’s put this in
practice by using the convolutional base of our pretrained model
to extract interesting features from cat and dog images and then
train a dogs-versus-cats classifier on top of these features.</p>
<p>We will use the <em>KerasHub</em> library to create all pretrained models used in
this book. KerasHub contains Keras implementations of popular pretrained model
architectures paired with pretrained weights that can be downloaded to your
machine. It contains a number of ConvNets like Xception, ResNet, EfficientNet
and MobileNet, as well as larger, generative models we will use in
the later chapters of this book. Let’s try using it to instantiate the
Xception model trained on the ImageNet dataset.</p>
<aside>
<p>KerasHub comes as a separate package from Keras. This package is preinstalled
in Colab and Kaggle notebooks, but if you want to use it outside these
environments you can install it yourself with <code>pip install keras-hub</code>.</p>
</aside>

<figure id="listing-8-23">
<pre><code class="language-python">import keras_hub

conv_base = keras_hub.models.Backbone.from_preset("xception_41_imagenet")
</code></pre>
<figcaption>
<a href="#listing-8-23">Listing 8.23</a>: Instantiating the Xception convolutional base
</figcaption>
</figure>

<p>You’ll note a couple of things. First, KerasHub uses the term <em>backbone</em> to refer
to the underlying feature extractor network without the classification head
(it’s a little easier to type than “convolutional base”). It also uses a
special constructor called <code>from_preset()</code> that will download the configuration
and weights for the Xception model.</p>
<p>What’s that “41” in the name of the model we are using? Pretrained ConvNets are
by convention often named by how “deep” they are. In this case, the 41 means
that our Xception model has 41 trainable layers (conv and dense layers) stacked
on top of each other. It’s the “deepest” model we’ve used so far in the book
by a good margin.</p>
<p>There’s one more missing piece we need before we can use this model. Every
pretrained ConvNet will do some rescaling and resizing of images before
pretraining. It’s important to make sure our input images <em>match</em>; otherwise, our
model will need to relearn how to extract features from images
with a totally different input range. Rather than keep track of which pretrained
models use a <code>[0, 1]</code> input range for pixel values and which use a <code>[-1, 1]</code>
range, we can use a KerasHub layer called <code>ImageConverter</code> that will rescale our
images to match our pretrained checkpoint. It has the same special
<code>from_preset()</code> constructor as the backbone class.</p>
<figure id="listing-8-24">
<pre><code class="language-python">preprocessor = keras_hub.layers.ImageConverter.from_preset(
    "xception_41_imagenet",
    image_size=(180, 180),
)
</code></pre>
<figcaption>
<a href="#listing-8-24">Listing 8.24</a>: Instantiating the preprocessing paired with the Xception model
</figcaption>
</figure>

<p>At this point, there are two ways you could proceed:</p>
<ul>
<li>Running the convolutional base over your dataset, recording its output to a
NumPy array on disk, and then using this data as input to a standalone,
densely connected classifier similar to those you saw in chapters 4 and 5.
This solution is fast and cheap to run, because it only requires running the
convolutional base once for every input image, and the convolutional base is
by far the most expensive part of the pipeline. But for the same reason, this
technique won’t allow you to use data augmentation.</li>
</ul>
<ul>
<li>Extending the model you have (<code>conv_base</code>) by adding <code>Dense</code> layers on top
and running the whole thing end to end on the input data. This will allow you
to use data augmentation because every input image goes through the
convolutional base every time it’s seen by the model. But for the same reason,
this technique is far more expensive than the first.</li>
</ul>
<p>We’ll cover both techniques. Let’s walk through the code required to set up the
first one: recording the output of <code>conv_base</code> on your data and using these
outputs as inputs to a new model.</p>
<h4 id="fast-feature-extraction-without-data-augmentation">Fast feature extraction without data augmentation</h4>
<p>We’ll start by extracting features as NumPy arrays, by calling
the <code>predict()</code> method of the <code>conv_base</code> model on our training, validation,
and testing datasets.
Let’s iterate over our datasets to extract
the pretrained model’s features.</p>
<figure id="listing-8-25">
<pre><code class="language-python">def get_features_and_labels(dataset):
    all_features = []
    all_labels = []
    for images, labels in dataset:
        preprocessed_images = preprocessor(images)
        features = conv_base.predict(preprocessed_images, verbose=0)
        all_features.append(features)
        all_labels.append(labels)
    return np.concatenate(all_features), np.concatenate(all_labels)

train_features, train_labels = get_features_and_labels(train_dataset)
val_features, val_labels = get_features_and_labels(validation_dataset)
test_features, test_labels = get_features_and_labels(test_dataset)
</code></pre>
<figcaption>
<a href="#listing-8-25">Listing 8.25</a>: Extracting the image features and corresponding labels
</figcaption>
</figure>

<p>Importantly, <code>predict()</code> only expects images, not labels, but our current
dataset yields batches that contain both images and their labels.</p>
<p>The extracted features are currently of shape <code>(samples, 6, 6, 2048)</code>:</p>
<figure>
<pre><code class="language-python">&gt;&gt;&gt; train_features.shape</code>
<code class="language-output">(2000, 6, 6, 2048)</code></pre>
</figure>

<p>At this point, you can define your densely connected classifier (note the use
of dropout for regularization) and train it on the data and labels that you
just recorded.</p>
<figure id="listing-8-26">
<pre><code class="language-python">inputs = keras.Input(shape=(6, 6, 2048))
# Averages spatial dimensions to flatten the feature map
x = layers.GlobalAveragePooling2D()(inputs)
x = layers.Dense(256, activation="relu")(x)
x = layers.Dropout(0.25)(x)
outputs = layers.Dense(1, activation="sigmoid")(x)
model = keras.Model(inputs, outputs)
model.compile(
    loss="binary_crossentropy",
    optimizer="adam",
    metrics=["accuracy"],
)

callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath="feature_extraction.keras",
        save_best_only=True,
        monitor="val_loss",
    )
]
history = model.fit(
    train_features,
    train_labels,
    epochs=10,
    validation_data=(val_features, val_labels),
    callbacks=callbacks,
)
</code></pre>
<figcaption>
<a href="#listing-8-26">Listing 8.26</a>: Defining and training the densely connected classifier
</figcaption>
</figure>

<p>Training is very fast because you only have to deal with two <code>Dense</code> layers
— an epoch takes less than 1 second even on CPU.</p>
<p>Let’s look at the loss and accuracy curves during training (see figure 8.13).</p>
<figure id="listing-8-27">
<pre><code class="language-python">import matplotlib.pyplot as plt

acc = history.history["accuracy"]
val_acc = history.history["val_accuracy"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]
epochs = range(1, len(acc) + 1)
plt.plot(epochs, acc, "r--", label="Training accuracy")
plt.plot(epochs, val_acc, "b", label="Validation accuracy")
plt.title("Training and validation accuracy")
plt.legend()
plt.figure()
plt.plot(epochs, loss, "r--", label="Training loss")
plt.plot(epochs, val_loss, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.legend()
plt.show()
</code></pre>
<figcaption>
<a href="#listing-8-27">Listing 8.27</a>: Plotting the results
</figcaption>
</figure>

<figure id="figure-8-13">
<img src="../Images/605acfe8452fb67a1808d09146951f69.png" data-original-src="https://deeplearningwithpython.io/images/ch08/training-and-validation-fe-acc.2e8c417c.png"/>
<img src="../Images/df8ababe79bc755a8d10cdfc5323b506.png" data-original-src="https://deeplearningwithpython.io/images/ch08/training-and-validation-fe-loss.49f7ffe0.png"/>
<figcaption>
<a href="#figure-8-13">Figure 8.13</a>: Training and validation metrics for plain feature extraction
</figcaption>
</figure>

<p>You reach a validation accuracy of slightly over 98% — much better than you achieved in
the previous section with the small model trained from scratch. This is a bit
of an unfair comparison, however, because ImageNet contains many dog and cat
instances, which means that our pretrained model already has the exact
knowledge required for the task at hand. This won’t always be the case when you
use pretrained features.</p>
<p>However, the plots also indicate that you’re overfitting almost from the start
— despite using dropout with a fairly large rate.
That’s because this technique doesn’t use data augmentation, which is essential
for preventing overfitting with small image datasets.</p>
<p>Let’s check the test accuracy:</p>
<figure>
<pre><code class="language-python">test_model = keras.models.load_model("feature_extraction.keras")
test_loss, test_acc = test_model.evaluate(test_features, test_labels)
print(f"Test accuracy: {test_acc:.3f}")
</code></pre>
</figure>

<p>We get test accuracy of 98.1% — a very nice improvement over training a model from scratch!</p>
<h4 id="feature-extraction-together-with-data-augmentation">Feature extraction together with data augmentation</h4>
<p>Now, let’s review the second technique we mentioned for doing feature extraction,
which is much slower and more expensive but allows you to use data augmentation
during training: creating a model that chains the <code>conv_base</code> with a new dense
classifier and training it end to end on the inputs.</p>
<p>To do this, we will first freeze the convolutional base.
<em>Freezing</em> a layer or set of layers means preventing their weights from being
updated during training. Here, if you don’t do this, then the representations that
were previously learned by the convolutional base will be modified during training.
Because the <code>Dense</code> layers on top are randomly initialized, very large weight
updates would be propagated through the network, effectively destroying the
representations previously learned.</p>
<p>In Keras, you freeze a layer or model by setting its <code>trainable</code> attribute to <code>False</code>.</p>
<figure id="listing-8-28">
<pre><code class="language-python">import keras_hub

conv_base = keras_hub.models.Backbone.from_preset(
    "xception_41_imagenet",
    trainable=False,
)
</code></pre>
<figcaption>
<a href="#listing-8-28">Listing 8.28</a>: Creating the frozen convolutional base
</figcaption>
</figure>

<p>Setting <code>trainable</code> to <code>False</code> empties the list of trainable weights of the layer
or model.</p>
<figure id="listing-8-29">
<pre><code class="language-python">&gt;&gt;&gt; conv_base.trainable = True
&gt;&gt;&gt; # The number of trainable weights before freezing the conv base
&gt;&gt;&gt; len(conv_base.trainable_weights)</code>
<code class="language-output">154</code>
<code class="language-python">&gt;&gt;&gt; conv_base.trainable = False
&gt;&gt;&gt; # The number of trainable weights after freezing the conv base
&gt;&gt;&gt; len(conv_base.trainable_weights)</code>
<code class="language-output">0</code></pre>
<figcaption>
<a href="#listing-8-29">Listing 8.29</a>: Printing the list of trainable weights before and after freezing
</figcaption>
</figure>

<p>Now, we can just create a new model that chains together our frozen convolutional base
and a dense classifier, like this:</p>
<figure>
<pre><code class="language-python">inputs = keras.Input(shape=(180, 180, 3))
x = preprocessor(inputs)
x = conv_base(x)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(256)(x)
x = layers.Dropout(0.25)(x)
outputs = layers.Dense(1, activation="sigmoid")(x)
model = keras.Model(inputs, outputs)
model.compile(
    loss="binary_crossentropy",
    optimizer="adam",
    metrics=["accuracy"],
)
</code></pre>
</figure>

<p>With this setup, only the weights from the two <code>Dense</code> layers that you added
will be trained. That’s a total of four weight tensors: two per layer (the
main weight matrix and the bias vector). Note that for these changes
to take effect, you must first compile the model. If you ever modify weight
trainability after compilation, you should then recompile the model, or these
changes will be ignored.</p>
<p>Let’s train our model. We’ll reuse our augmented dataset <code>augmented_train_dataset</code>.
Thanks to data augmentation, it will
take much longer for the model to start overfitting, so we can train for more
epochs — let’s do 30:</p>
<figure>
<pre><code class="language-python">callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath="feature_extraction_with_data_augmentation.keras",
        save_best_only=True,
        monitor="val_loss",
    )
]
history = model.fit(
    augmented_train_dataset,
    epochs=30,
    validation_data=validation_dataset,
    callbacks=callbacks,
)
</code></pre>
</figure>

<aside>
<p>This technique is expensive enough that you should only attempt it if you
have access to a GPU (such as the free GPU available in Colab) —
it’s intractable on CPU. If you can’t run your
code on GPU, then the previous technique is the way to go.</p>
</aside>

<p>Let’s plot the results again (see figure 8.14). This model reaches a validation
accuracy of 98.2%.</p>
<figure id="figure-8-14">
<img src="../Images/8e3a8a5c36838e7499ef2d4ccb9849c0.png" data-original-src="https://deeplearningwithpython.io/images/ch08/training-and-validation-feda-acc.b0c05268.png"/>
<img src="../Images/ae553ac8e7101ab0653eeff0e1edba20.png" data-original-src="https://deeplearningwithpython.io/images/ch08/training-and-validation-feda-loss.69d30842.png"/>
<figcaption>
<a href="#figure-8-14">Figure 8.14</a>: Training and validation metrics for feature extraction with data augmentation
</figcaption>
</figure>

<p>Let’s check the test accuracy.</p>
<figure id="listing-8-30">
<pre><code class="language-python">test_model = keras.models.load_model(
    "feature_extraction_with_data_augmentation.keras"
)
test_loss, test_acc = test_model.evaluate(test_dataset)
print(f"Test accuracy: {test_acc:.3f}")
</code></pre>
<figcaption>
<a href="#listing-8-30">Listing 8.30</a>: Evaluating the model on the test set
</figcaption>
</figure>

<p>We get a test accuracy of 98.4%. This is not an improvement over the previous model,
which is a bit disappointing. This could be a sign that our data augmentation configuration
does not exactly match the distribution of the test data.
Let’s see if we can do better with our latest attempt.</p>
<h3 id="fine-tuning-a-pretrained-model">Fine-tuning a pretrained model</h3>
<p>Another widely used technique for model
reuse, complementary to feature extraction, is <em>fine-tuning</em> (see figure
8.15). Fine-tuning consists of unfreezing the frozen
model base used for feature extraction and jointly training both the newly
added part of the model (in this case, the fully connected classifier) and
the base model. This is called <em>fine-tuning</em> because it slightly adjusts the
more abstract representations of the model being reused to make them
more relevant for the problem at hand.</p>
<p>We stated earlier that it’s necessary to freeze the pretrained convolution base first
to be able to train a randomly initialized classifier on top. For the
same reason, it’s only possible to fine-tune the
convolutional base once the classifier on top has already been trained. If the
classifier isn’t already trained, then the error signal propagating through
the network during training will be too large, and the representations
previously learned by the layers being fine-tuned will be destroyed. Thus, the
steps for fine-tuning a network are as follows:</p>
<ol>
<li>Add your custom network on top of an already trained base network.</li>
<li>Freeze the base network.</li>
<li>Train the part you added.</li>
<li>Unfreeze the base network.</li>
<li>Jointly train both these layers and the part you added.</li>
</ol>
<p>Note that you should not unfreeze “batch normalization” layers (<code>BatchNormalization</code>).
Batch normalization and its effect on fine-tuning is explained in the next chapter.</p>
<p>You already completed the first three steps when doing feature extraction.
Let’s proceed with step 4: you’ll unfreeze your <code>conv_base</code>.</p>
<aside>
<p><span class="note-title">Partial fine-tuning</span></p>
<p>In this case, we chose to unfreeze and fine-tune all of the Xception
convolutional base. However, when dealing with large pretrained models, you may sometimes
only unfreeze some of the top layers of the convolutional base, and leave the lower layers
frozen. You’re probably wondering, why only fine-tune some of the layers? Why the top ones specifically?
Here’s why:</p>
<ul>
<li>Earlier layers in the convolutional base encode more-generic, reusable
features, whereas layers higher up encode more-specialized features. It’s more
useful to fine-tune the more specialized features because these are the ones
that need to be repurposed on your new problem. There would be fast-decreasing
returns in fine-tuning lower layers.</li>
</ul>
<ul>
<li>The more parameters you’re training, the more you’re at risk of overfitting.
The convolutional base has 15 million parameters, so it would be risky to
attempt to train it on your small dataset.</li>
</ul>
<p>Thus, it can be a good strategy to fine-tune only the top three or
four layers in the convolutional base. You’d do something like this:</p>
<figure>
<pre><code class="language-python">conv_base.trainable = True
for layer in conv_base.layers[:-4]:
    layer.trainable = False
</code></pre>
</figure>
</aside>

<p>Let’s start fine-tuning the model using a very low learning rate. The reason for using a low
learning rate is that you want to limit the magnitude of the modifications you
make to the representations of the layers you’re fine-tuning. Updates
that are too large may harm these representations.</p>
<figure id="listing-8-31">
<pre><code class="language-python">model.compile(
    loss="binary_crossentropy",
    optimizer=keras.optimizers.Adam(learning_rate=1e-5),
    metrics=["accuracy"],
)

callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath="fine_tuning.keras",
        save_best_only=True,
        monitor="val_loss",
    )
]
history = model.fit(
    augmented_train_dataset,
    epochs=30,
    validation_data=validation_dataset,
    callbacks=callbacks,
)
</code></pre>
<figcaption>
<a href="#listing-8-31">Listing 8.31</a>: Fine-tuning the model
</figcaption>
</figure>

<p>You can now finally evaluate this model on the test data (see figure 8.15):</p>
<figure>
<pre><code class="language-python">model = keras.models.load_model("fine_tuning.keras")
test_loss, test_acc = model.evaluate(test_dataset)
print(f"Test accuracy: {test_acc:.3f}")
</code></pre>
</figure>

<figure id="figure-8-15">
<img src="../Images/a4eed2da3b1ba70dcee533725f83fcc8.png" data-original-src="https://deeplearningwithpython.io/images/ch08/training-and-validation-ft-acc.7ec17959.png"/>
<img src="../Images/6549d0428cc9b3aa20c7b92abef1c8e1.png" data-original-src="https://deeplearningwithpython.io/images/ch08/training-and-validation-ft-loss.3c4293eb.png"/>
<figcaption>
<a href="#figure-8-15">Figure 8.15</a>: Training and validation metrics for fine-tuning
</figcaption>
</figure>

<p>Here, you get a test accuracy of 98.6% (again, your own results may be within half a percentage point).
In the original Kaggle competition around this dataset,
this would have been one of the top results. It’s not quite a fair
comparison, however, since you used pretrained features that already contained
prior knowledge about cats and dogs, which competitors couldn’t use at the time.</p>
<p>On the positive side, by using modern
deep learning techniques, you managed to reach this result using only a small
fraction of the training data that was available for the competition (about 10%).
There is a huge difference between being able to train on 20,000 samples
compared to 2,000 samples!</p>
<p>Now you have a solid set of tools for dealing with image-classification
problems — in particular, with small datasets.</p>
<h2 id="summary">Summary</h2>
<ul>
<li>ConvNets excel at computer vision tasks. It’s possible to train one from scratch, even on a very small dataset,
with decent results.</li>
<li>ConvNets work by learning a hierarchy of modular patterns and concepts to
represent the visual world.</li>
<li>On a small dataset, overfitting will be the main issue. Data augmentation is
a powerful way to fight overfitting when you’re working with image data.</li>
<li>It’s easy to reuse an existing ConvNet on a new dataset via feature
extraction. This is a valuable technique for working with small image datasets.</li>
<li>As a complement to feature extraction, you can use fine-tuning, which adapts
to a new problem some of the representations previously learned by an existing
model. This pushes performance a bit further.</li>
</ul>
    
</body>
</html>