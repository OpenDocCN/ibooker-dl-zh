- en: Chapter 7\. Energy-Based Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。基于能量的模型
- en: Energy-based models are a broad class of generative model that borrow a key
    idea from modeling physical systems—namely, that the probability of an event can
    be expressed using a Boltzmann distribution, a specific function that normalizes
    a real-valued energy function between 0 and 1\. This distribution was originally
    formulated in 1868 by Ludwig Boltzmann, who used it to describe gases in thermal
    equilibrium.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 基于能量的模型是一类广泛的生成模型，借鉴了建模物理系统的一个关键思想——即，事件的概率可以用玻尔兹曼分布来表示，这是一个特定函数，将实值能量函数归一化在0到1之间。这个分布最初是由路德维希·玻尔兹曼在1868年提出的，他用它来描述处于热平衡状态的气体。
- en: In this chapter, we will see how we can use this idea to train a generative
    model that can be used to produce images of handwritten digits. We will explore
    several new concepts, including contrastive divergence for training the EBM and
    Langevin dynamics for sampling.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看到如何使用这个想法来训练一个生成模型，用于生成手写数字的图像。我们将探索几个新概念，包括用于训练EBM的对比散度和用于采样的朗之万动力学。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: We will begin with a short story to illustrate the key concepts behind energy-based
    models.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个简短的故事开始，以说明基于能量的模型背后的关键概念。
- en: The story of Diane Mixx and the Long-au-Vin running club captures the key ideas
    behind energy-based modeling. Let’s now explore the theory in more detail, before
    we implement a practical example using Keras.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Diane Mixx和Long-au-Vin跑步俱乐部的故事捕捉了基于能量建模的关键思想。现在让我们更详细地探讨理论，然后我们将使用Keras实现一个实际示例。
- en: Energy-Based Models
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于能量的模型
- en: Energy-based models attempt to model the true data-generating distribution using
    a *Boltzmann distribution* ([Equation 7-1](#boltzmann_equation)) where <math alttext="upper
    E left-parenthesis x right-parenthesis"><mrow><mi>E</mi> <mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow></math> is know as the *energy function* (or *score*) of an observation
    <math alttext="x"><mi>x</mi></math> .
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 基于能量的模型试图使用*玻尔兹曼分布*（[方程式7-1](#boltzmann_equation)）来建模真实的数据生成分布，其中 <math alttext="upper
    E left-parenthesis x right-parenthesis"><mrow><mi>E</mi> <mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow></math> 被称为观察结果 <math alttext="x"><mi>x</mi></math> 的*能量函数*（或*分数*）。
- en: Equation 7-1\. Boltzmann distribution
  id: totrans-8
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式7-1。玻尔兹曼分布
- en: <math alttext="p left-parenthesis bold x right-parenthesis equals StartFraction
    e Superscript minus upper E left-parenthesis bold x right-parenthesis Baseline
    Over integral Underscript ModifyingAbove bold x With bold caret element-of bold
    upper X Endscripts e Superscript minus upper E left-parenthesis ModifyingAbove
    bold x With bold caret right-parenthesis Baseline EndFraction" display="block"><mrow><mi>p</mi>
    <mrow><mo>(</mo> <mi>𝐱</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><msup><mi>e</mi>
    <mrow><mo>-</mo><mi>E</mi><mo>(</mo><mi>𝐱</mi><mo>)</mo></mrow></msup> <mrow><msub><mo>∫</mo>
    <mrow><mover accent="true"><mi>𝐱</mi> <mo>^</mo></mover><mo>∈</mo><mi>𝐗</mi></mrow></msub>
    <msup><mi>e</mi> <mrow><mo>-</mo><mi>E</mi><mo>(</mo><mover accent="true"><mi>𝐱</mi>
    <mo>^</mo></mover><mo>)</mo></mrow></msup></mrow></mfrac></mrow></math>
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="p left-parenthesis bold x right-parenthesis equals StartFraction
    e Superscript minus upper E left-parenthesis bold x right-parenthesis Baseline
    Over integral Underscript ModifyingAbove bold x With bold caret element-of bold
    upper X Endscripts e Superscript minus upper E left-parenthesis ModifyingAbove
    bold x With bold caret right-parenthesis Baseline EndFraction" display="block"><mrow><mi>p</mi>
    <mrow><mo>(</mo> <mi>𝐱</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><msup><mi>e</mi>
    <mrow><mo>-</mo><mi>E</mi><mo>(</mo><mi>𝐱</mi><mo>)</mo></mrow></msup> <mrow><msub><mo>∫</mo>
    <mrow><mover accent="true"><mi>𝐱</mi> <mo>^</mo></mover><mo>∈</mo><mi>𝐗</mi></mrow></msub>
    <msup><mi>e</mi> <mrow><mo>-</mo><mi>E</mi><mo>(</mo><mover accent="true"><mi>𝐱</mi>
    <mo>^</mo></mover><mo>)</mo></mrow></msup></mrow></mfrac></mrow></math>
- en: In practice, this amounts to training a neural network <math alttext="upper
    E left-parenthesis x right-parenthesis"><mrow><mi>E</mi> <mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow></math> to output low scores for likely observations (so <math
    alttext="p bold x"><mrow><mi>p</mi> <mi>𝐱</mi></mrow></math> is close to 1) and
    high scores for unlikely observations (so <math alttext="p bold x"><mrow><mi>p</mi>
    <mi>𝐱</mi></mrow></math> is close to 0).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这意味着训练一个神经网络 <math alttext="upper E left-parenthesis x right-parenthesis"><mrow><mi>E</mi>
    <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math>，对可能的观察输出低分数（使得 <math alttext="p
    bold x"><mrow><mi>p</mi> <mi>𝐱</mi></mrow></math> 接近于1），对不太可能的观察输出高分数（使得 <math
    alttext="p bold x"><mrow><mi>p</mi> <mi>𝐱</mi></mrow></math> 接近于0）。
- en: There are two challenges with modeling the data in this way. Firstly, it is
    not clear how we should use our model for sampling new observations—we can use
    it to generate a score given an observation, but how do we generate an observation
    that has a low score (i.e., a plausible observation)?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 用这种方式建模数据存在两个挑战。首先，我们不清楚如何使用我们的模型来采样新的观察结果——我们可以用它来生成给定观察结果的分数，但如何生成一个得分低的观察结果（即，一个可信的观察结果）？
- en: Secondly, the normalizing denominator of [Equation 7-1](#boltzmann_equation)
    contains an integral that is intractable for all but the simplest of problems.
    If we cannot calculate this integral, then we cannot use maximum likelihood estimation
    to train the model, as this requires that <math alttext="p bold x"><mrow><mi>p</mi>
    <mi>𝐱</mi></mrow></math> is a valid probability distribution.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，[方程式7-1](#boltzmann_equation)的标准化分母包含一个对于除了最简单的问题外都难以计算的积分。如果我们无法计算这个积分，那么我们就无法使用最大似然估计来训练模型，因为这要求
    <math alttext="p bold x"><mrow><mi>p</mi> <mi>𝐱</mi></mrow></math> 是一个有效的概率分布。
- en: The key idea behind an energy-based model is that we can use approximation techniques
    to ensure we never need to calculate the intractable denominator. This is in contrast
    to, say, a normalizing flow, where we go to great lengths to ensure that the transformations
    that we apply to our standard Gaussian distribution do not change the fact that
    the output is still a valid probability distribution.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 基于能量的模型的关键思想是，我们可以使用近似技术来确保我们永远不需要计算难以计算的分母。这与标准化流形形成对比，标准化流形需要我们付出很大的努力，以确保我们对标准高斯分布应用的变换不会改变输出仍然是有效的概率分布的事实。
- en: We sidestep the tricky intractable denominator problem by using a technique
    called contrastive divergence (for training) and a technique called Langevin dynamics
    (for sampling), following the ideas from Du and Mordatch’s 2019 paper “Implicit
    Generation and Modeling with Energy-Based Models.”^([1](ch07.xhtml#idm45387012482384))
    We shall explore these techniques in detail while building our own EBM later in
    the chapter.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过使用一种称为对比散度（用于训练）和一种称为朗之万动力学（用于采样）的技术来避开棘手的难以计算的分母问题，这些技术遵循了杜和莫达奇在2019年的论文“基于能量的模型的隐式生成和建模”的思想。我们将在本章后面详细探讨这些技术，同时构建我们自己的EBM。
- en: First, let’s get set up with a dataset and design a simple neural network that
    will represent our real-valued energy function <math alttext="upper E left-parenthesis
    x right-parenthesis"><mrow><mi>E</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math>
    .
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们准备一个数据集并设计一个简单的神经网络，来表示我们的实值能量函数 <math alttext="upper E left-parenthesis
    x right-parenthesis"><mrow><mi>E</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math>。
- en: Running the Code for This Example
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行此示例的代码
- en: The code for this example can be found in the Jupyter notebook located at *notebooks/07_ebm/01_ebm/ebm.ipynb*
    in the book repository.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例的代码可以在书籍存储库中的*notebooks/07_ebm/01_ebm/ebm.ipynb*中找到。
- en: The code is adapted from the excellent [tutorial on deep energy-based generative
    models](https://oreil.ly/kyO9B) by Phillip Lippe.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码是根据Philip Lippe的优秀教程“深度基于能量的生成模型”进行调整的。
- en: The MNIST Dataset
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MNIST数据集
- en: We’ll be using the standard [MNIST dataset](https://oreil.ly/mSvhc), consisting
    of grayscale images of handwritten digits. Some example images from the dataset
    are shown in [Figure 7-2](#mnist_examples).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用标准的[MNIST数据集](https://oreil.ly/mSvhc)，其中包含手写数字的灰度图像。数据集中的一些示例图像显示在[图7-2](#mnist_examples)中。
- en: '![](Images/gdl2_0702.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0702.png)'
- en: Figure 7-2\. Examples of images from the MNIST dataset
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-2。MNIST数据集中的图像示例
- en: The dataset comes prepackaged with TensorFlow, so it can be downloaded as shown
    in [Example 7-1](#mnist-ex).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集已经预先打包到TensorFlow中，因此可以按照[示例7-1](#mnist-ex)中所示下载。
- en: Example 7-1\. Loading the MNIST dataset
  id: totrans-24
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例7-1。加载MNIST数据集
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As usual, we’ll scale the pixel values to the range [-1, 1] and add some padding
    to make the images 32 × 32 pixels in size. We also convert it to a TensorFlow
    Dataset, as shown in [Example 7-2](#mnist-preprocessing-ex).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，我们将像素值缩放到范围[-1, 1]，并添加一些填充使图像大小为32×32像素。我们还将其转换为TensorFlow数据集，如[示例7-2](#mnist-preprocessing-ex)中所示。
- en: Example 7-2\. Preprocessing the MNIST dataset
  id: totrans-27
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例7-2。预处理MNIST数据集
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now that we have our dataset, we can build the neural network that will represent
    our energy function <math alttext="upper E left-parenthesis x right-parenthesis"><mrow><mi>E</mi>
    <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math> .
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了数据集，我们可以构建代表我们能量函数<math alttext="上标E左括号x右括号"><mrow><mi>E</mi> <mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow></math>的神经网络。
- en: The Energy Function
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 能量函数
- en: The energy function <math alttext="upper E Subscript theta Baseline left-parenthesis
    x right-parenthesis"><mrow><msub><mi>E</mi> <mi>θ</mi></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow></mrow></math> is a neural network with parameters
    <math alttext="theta"><mi>θ</mi></math> that can transform an input image <math
    alttext="x"><mi>x</mi></math> into a scalar value. Throughout this network, we
    make use of an activation function called *swish*, as described in the following
    sidebar.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 能量函数<math alttext="上标E下标θ左括号x右括号"><mrow><msub><mi>E</mi> <mi>θ</mi></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow></mrow></math>是一个具有参数<math alttext="θ"><mi>θ</mi></math>的神经网络，可以将输入图像<math
    alttext="x"><mi>x</mi></math>转换为标量值。在整个网络中，我们使用了一个称为*swish*的激活函数，如下面的侧边栏所述。
- en: The network is a set of stacked `Conv2D` layers that gradually reduce the size
    of the image while increasing the number of channels. The final layer is a single
    fully connected unit with linear activation, so the network can output values
    in the range ( <math alttext="negative normal infinity"><mrow><mo>-</mo> <mi>∞</mi></mrow></math>
    , <math alttext="normal infinity"><mi>∞</mi></math> ). The code to build it is
    given in [Example 7-3](#ebm-model).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 网络是一组堆叠的`Conv2D`层，逐渐减小图像的尺寸同时增加通道数。最后一层是一个具有线性激活的单个完全连接单元，因此网络可以输出范围内的值（<math
    alttext="负无穷"><mrow><mo>-</mo> <mi>∞</mi></mrow></math>，<math alttext="正无穷"><mi>∞</mi></math>）。构建它的代码在[示例7-3](#ebm-model)中给出。
- en: Example 7-3\. Building the energy function <math alttext="upper E left-parenthesis
    x right-parenthesis"><mrow><mi>E</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math>
    neural network
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例7-3。构建能量函数<math alttext="上标E左括号x右括号"><mrow><mi>E</mi> <mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow></math>神经网络
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[![1](Images/1.png)](#co_energy_based_models_CO1-1)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_energy_based_models_CO1-1)'
- en: The energy function is a set of stacked `Conv2D` layers, with swish activation.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 能量函数是一组堆叠的`Conv2D`层，带有swish激活。
- en: '[![2](Images/2.png)](#co_energy_based_models_CO1-2)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_energy_based_models_CO1-2)'
- en: The final layer is a single fully connected unit, with a linear activation function.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一层是一个单个完全连接单元，具有线性激活函数。
- en: '[![3](Images/3.png)](#co_energy_based_models_CO1-3)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_energy_based_models_CO1-3)'
- en: A Keras `Model` that converts the input image into a scalar energy value.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一个将输入图像转换为标量能量值的Keras `Model`。
- en: Sampling Using Langevin Dynamics
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Langevin动力学进行采样
- en: The energy function only outputs a score for a given input—how can we use this
    function to generate new samples that have a low energy score?
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 能量函数只为给定输入输出一个分数——我们如何使用这个函数生成能量分数低的新样本？
- en: We will use a technique called *Langevin dynamics*, which makes use of the fact
    that we can compute the gradient of the energy function with respect to its input.
    If we start from a random point in the sample space and take small steps in the
    opposite direction of the calculated gradient, we will gradually reduce the energy
    function. If our neural network is trained correctly, then the random noise should
    transform into an image that resembles an observation from the training set before
    our eyes!
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一种称为*Langevin动力学*的技术，利用了我们可以计算能量函数相对于其输入的梯度的事实。如果我们从样本空间中的一个随机点开始，并朝着计算出的梯度的相反方向迈出小步，我们将逐渐减小能量函数。如果我们的神经网络训练正确，那么随机噪声应该在我们眼前转变成类似于训练集中的观察结果的图像！
- en: Stochastic Gradient Langevin Dynamics
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机梯度Langevin动力学
- en: Importantly, we must also add a small amount of random noise to the input as
    we travel across the sample space; otherwise, there is a risk of falling into
    local minima. The technique is therefore known as stochastic gradient Langevin
    dynamics.^([3](ch07.xhtml#idm45387012049200))
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，当我们穿越样本空间时，我们还必须向输入添加少量随机噪声；否则，有可能陷入局部最小值。因此，该技术被称为随机梯度Langevin动力学。^([3](ch07.xhtml#idm45387012049200))
- en: We can visualize this gradient descent as shown in [Figure 7-4](#langevin_diagram),
    for a two-dimensional space with the energy function value on the third dimension.
    The path is a noisy descent downhill, following the negative gradient of the energy
    function <math alttext="upper E left-parenthesis x right-parenthesis"><mrow><mi>E</mi>
    <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math> with respect to the input <math
    alttext="x"><mi>x</mi></math> . In the MNIST image dataset, we have 1,024 pixels
    so are navigating a 1,024-dimensional space, but the same principles apply!
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这种梯度下降可视化为 [图7-4](#langevin_diagram) 中所示，对于一个具有能量函数值的三维空间。路径是一个嘈杂的下降，沿着能量函数
    <math alttext="upper E left-parenthesis x right-parenthesis"><mrow><mi>E</mi>
    <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math> 的负梯度相对于输入 <math alttext="x"><mi>x</mi></math>
    下降。在MNIST图像数据集中，我们有1,024个像素，因此在一个1,024维空间中导航，但是相同的原则适用！
- en: '![https://www.math3d.org/C1sQPhizZ](Images/gdl2_0704.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![https://www.math3d.org/C1sQPhizZ](Images/gdl2_0704.png)'
- en: Figure 7-4\. Gradient descent using Langevin dynamics
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-4\. 使用朗之万动力学的梯度下降
- en: It is worth noting the difference between this kind of gradient descent and
    the kind of gradient descent we normally use to train a neural network.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，这种梯度下降与我们通常用来训练神经网络的梯度下降之间的区别。
- en: When training a neural network, we calculate the gradient of the *loss function*
    with respect to the *parameters* of the network (i.e., the weights) using backpropagation.
    Then we update the parameters a small amount in the direction of the negative
    gradient, so that over many iterations, we gradually minimize the loss.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练神经网络时，我们使用反向传播计算*损失函数*相对于网络的*参数*（即权重）的梯度。然后我们将参数在负梯度方向上微调，这样经过多次迭代，我们逐渐最小化损失。
- en: With Langevin dynamics, we keep the neural network weights *fixed* and calculate
    the gradient of the *output* with respect to the *input*. Then we update the input
    a small amount in the direction of the negative gradient, so that over many iterations,
    we gradually minimize the output (the energy score).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 使用朗之万动力学，我们保持神经网络权重*固定*，计算*输出*相对于*输入*的梯度。然后我们将输入在负梯度方向上微调，这样经过多次迭代，我们逐渐最小化输出（能量分数）。
- en: Both processes utilize the same idea (gradient descent), but are applied to
    different functions and with respect to different entities.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个过程都利用了相同的思想（梯度下降），但是应用于不同的函数，并且针对不同的实体。
- en: 'Formally, Langevin dynamics can be described by the following equation:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，朗之万动力学可以用以下方程描述：
- en: <math alttext="x Superscript k Baseline equals x Superscript k minus 1 Baseline
    minus eta normal nabla Subscript x Baseline upper E Subscript theta Baseline left-parenthesis
    x Superscript k minus 1 Baseline right-parenthesis plus omega" display="block"><mrow><msup><mi>x</mi>
    <mi>k</mi></msup> <mo>=</mo> <msup><mi>x</mi> <mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msup>
    <mo>-</mo> <mi>η</mi> <msub><mi>∇</mi> <mi>x</mi></msub> <msub><mi>E</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msup>
    <mo>)</mo></mrow> <mo>+</mo> <mi>ω</mi></mrow></math>
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="x Superscript k Baseline equals x Superscript k minus 1 Baseline
    minus eta normal nabla Subscript x Baseline upper E Subscript theta Baseline left-parenthesis
    x Superscript k minus 1 Baseline right-parenthesis plus omega" display="block"><mrow><msup><mi>x</mi>
    <mi>k</mi></msup> <mo>=</mo> <msup><mi>x</mi> <mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msup>
    <mo>-</mo> <mi>η</mi> <msub><mi>∇</mi> <mi>x</mi></msub> <msub><mi>E</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msup>
    <mo>)</mo></mrow> <mo>+</mo> <mi>ω</mi></mrow></math>
- en: where <math alttext="omega tilde script upper N left-parenthesis 0 comma sigma
    right-parenthesis"><mrow><mi>ω</mi> <mo>∼</mo> <mi>𝒩</mi> <mo>(</mo> <mn>0</mn>
    <mo>,</mo> <mi>σ</mi> <mo>)</mo></mrow></math> and <math alttext="x Superscript
    0 Baseline tilde script upper U"><mrow><msup><mi>x</mi> <mn>0</mn></msup> <mo>∼</mo>
    <mi>𝒰</mi></mrow></math> (–1,1). <math alttext="eta"><mi>η</mi></math> is the
    step size hyperparameter that must be tuned—too large and the steps jump over
    minima, too small and the algorithm will be too slow to converge.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math alttext="omega tilde script upper N left-parenthesis 0 comma sigma
    right-parenthesis"><mrow><mi>ω</mi> <mo>∼</mo> <mi>𝒩</mi> <mo>(</mo> <mn>0</mn>
    <mo>,</mo> <mi>σ</mi> <mo>)</mo></mrow></math> 和 <math alttext="x Superscript
    0 Baseline tilde script upper U"><mrow><msup><mi>x</mi> <mn>0</mn></msup> <mo>∼</mo>
    <mi>𝒰</mi></mrow></math>（-1,1）。<math alttext="eta"><mi>η</mi></math> 是必须调整的步长超参数——太大会导致步骤跳过最小值，太小则算法收敛速度太慢。
- en: Tip
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: <math alttext="x Superscript 0 Baseline tilde script upper U"><mrow><msup><mi>x</mi>
    <mn>0</mn></msup> <mo>∼</mo> <mi>𝒰</mi></mrow></math> (–1,1) is the uniform distribution
    on the range [–1, 1].
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="x Superscript 0 Baseline tilde script upper U"><mrow><msup><mi>x</mi>
    <mn>0</mn></msup> <mo>∼</mo> <mi>𝒰</mi></mrow></math>（-1,1）是范围[-1,1]上的均匀分布。
- en: We can code up our Langevin sampling function as illustrated in [Example 7-4](#langevin-sampler).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以编写我们的朗之万采样函数，如 [示例7-4](#langevin-sampler) 所示。
- en: Example 7-4\. The Langevin sampling function
  id: totrans-59
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例7-4\. 朗之万采样函数
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[![1](Images/1.png)](#co_energy_based_models_CO2-1)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_energy_based_models_CO2-1)'
- en: Loop over given number of steps.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 循环执行给定数量的步骤。
- en: '[![2](Images/2.png)](#co_energy_based_models_CO2-2)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_energy_based_models_CO2-2)'
- en: Add a small amount of noise to the image.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 向图像中添加少量噪音。
- en: '[![3](Images/3.png)](#co_energy_based_models_CO2-3)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_energy_based_models_CO2-3)'
- en: Pass the image through the model to obtain the energy score.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 通过模型传递图像以获得能量分数。
- en: '[![4](Images/4.png)](#co_energy_based_models_CO2-4)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_energy_based_models_CO2-4)'
- en: Calculate the gradient of the output with respect to the input.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 计算输出相对于输入的梯度。
- en: '[![5](Images/5.png)](#co_energy_based_models_CO2-5)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_energy_based_models_CO2-5)'
- en: Add a small amount of the gradient to the input image.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 向输入图像中添加少量梯度。
- en: Training with Contrastive Divergence
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用对比散度进行训练
- en: Now that we know how to sample a novel low-energy point from the sample space,
    let’s turn our attention to training the model.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何从样本空间中采样一个新的低能量点，让我们将注意力转向训练模型。
- en: We cannot apply maximum likelihood estimation, because the energy function does
    not output a probability; it outputs a score that does not integrate to 1 across
    the sample space. Instead, we will apply a technique first proposed in 2002 by
    Geoffrey Hinton, called *contrastive divergence*, for training unnormalized scoring
    models.^([4](ch07.xhtml#idm45387011649104))
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法应用最大似然估计，因为能量函数不输出概率；它输出的是一个在样本空间中不积分为1的分数。相反，我们将应用 Geoffrey Hinton 在2002年首次提出的一种技术，称为*对比散度*，用于训练非归一化评分模型。^([4](ch07.xhtml#idm45387011649104))
- en: 'The value that we want to minimize (as always) is the negative log-likelihood
    of the data:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望最小化的值（一如既往）是数据的负对数似然：
- en: <math alttext="script upper L equals minus double-struck upper E Subscript x
    tilde normal d normal a normal t normal a Baseline left-bracket log p Subscript
    theta Baseline left-parenthesis bold x right-parenthesis right-bracket" display="block"><mrow><mi>ℒ</mi>
    <mo>=</mo> <mo>-</mo> <msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo> <mi>data</mi></mrow></msub>
    <mfenced separators="" open="[" close="]"><mo form="prefix">log</mo> <msub><mi>p</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>𝐱</mi> <mo>)</mo></mrow></mfenced></mrow></math>
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="script upper L equals minus double-struck upper E Subscript x
    tilde normal d normal a normal t normal a Baseline left-bracket log p Subscript
    theta Baseline left-parenthesis bold x right-parenthesis right-bracket" display="block"><mrow><mi>ℒ</mi>
    <mo>=</mo> <mo>-</mo> <msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo> <mi>data</mi></mrow></msub>
    <mfenced separators="" open="[" close="]"><mo form="prefix">log</mo> <msub><mi>p</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>𝐱</mi> <mo>)</mo></mrow></mfenced></mrow></math>
- en: When <math alttext="p Subscript theta Baseline left-parenthesis bold x right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math>
    has the form of a Boltzmann distribution, with energy function <math alttext="upper
    E Subscript theta Baseline left-parenthesis bold x right-parenthesis"><mrow><msub><mi>E</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math>
    , it can be shown that the gradient of this value can be written as follows (Oliver
    Woodford’s “Notes on Contrastive Divergence” for the full derivation):^([5](ch07.xhtml#idm45387011627440))
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当 <math alttext="p Subscript theta Baseline left-parenthesis bold x right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math>
    具有玻尔兹曼分布的形式，能量函数为 <math alttext="upper E Subscript theta Baseline left-parenthesis
    bold x right-parenthesis"><mrow><msub><mi>E</mi> <mi>θ</mi></msub> <mrow><mo>(</mo>
    <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math> ，可以证明该值的梯度可以写成以下形式（Oliver Woodford的“对比散度笔记”进行完整推导）：^([5](ch07.xhtml#idm45387011627440))
- en: <math alttext="StartLayout 1st Row 1st Column normal nabla Subscript theta Baseline
    script upper L 2nd Column equals 3rd Column double-struck upper E Subscript x
    tilde normal d normal a normal t normal a Baseline left-bracket normal nabla Subscript
    theta Baseline upper E Subscript theta Baseline left-parenthesis bold x right-parenthesis
    right-bracket minus double-struck upper E Subscript x tilde normal m normal o
    normal d normal e normal l Baseline left-bracket normal nabla Subscript theta
    Baseline upper E Subscript theta Baseline left-parenthesis bold x right-parenthesis
    right-bracket EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><msub><mi>∇</mi> <mi>θ</mi></msub> <mi>ℒ</mi></mrow></mtd>
    <mtd><mo>=</mo></mtd> <mtd columnalign="left"><mrow><msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo>
    <mi>data</mi></mrow></msub> <mfenced separators="" open="[" close="]"><msub><mi>∇</mi>
    <mi>θ</mi></msub> <msub><mi>E</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>𝐱</mi>
    <mo>)</mo></mrow></mfenced> <mo>-</mo> <msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo>
    <mi>model</mi></mrow></msub> <mfenced separators="" open="[" close="]"><msub><mi>∇</mi>
    <mi>θ</mi></msub> <msub><mi>E</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>𝐱</mi>
    <mo>)</mo></mrow></mfenced></mrow></mtd></mtr></mtable></math>
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column normal nabla Subscript theta Baseline
    script upper L 2nd Column equals 3rd Column double-struck upper E Subscript x
    tilde normal d normal a normal t normal a Baseline left-bracket normal nabla Subscript
    theta Baseline upper E Subscript theta Baseline left-parenthesis bold x right-parenthesis
    right-bracket minus double-struck upper E Subscript x tilde normal m normal o
    normal d normal e normal l Baseline left-bracket normal nabla Subscript theta
    Baseline upper E Subscript theta Baseline left-parenthesis bold x right-parenthesis
    right-bracket EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><msub><mi>∇</mi> <mi>θ</mi></msub> <mi>ℒ</mi></mrow></mtd>
    <mtd><mo>=</mo></mtd> <mtd columnalign="left"><mrow><msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo>
    <mi>data</mi></mrow></msub> <mfenced separators="" open="[" close="]"><msub><mi>∇</mi>
    <mi>θ</mi></msub> <msub><mi>E</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>𝐱</mi>
    <mo>)</mo></mrow></mfenced> <mo>-</mo> <msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo>
    <mi>model</mi></mrow></msub> <mfenced separators="" open="[" close="]"><msub><mi>∇</mi>
    <mi>θ</mi></msub> <msub><mi>E</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>𝐱</mi>
    <mo>)</mo></mrow></mfenced></mrow></mtd></mtr></mtable></math>
- en: This intuitively makes a lot of sense—we want to train the model to output large
    negative energy scores for real observations and large positive energy scores
    for generated fake observations so that the contrast between these two extremes
    is as large as possible.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这在直觉上是有很多意义的-我们希望训练模型输出真实观察的大负能量分数，并为生成的假观察输出大正能量分数，以便这两个极端之间的对比尽可能大。
- en: In other words, we can calculate the difference between the energy scores of
    real and fake samples and use this as our loss function.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们可以计算真实和假样本的能量分数之间的差异，并将其用作我们的损失函数。
- en: To calculate the energy scores of fake samples, we would need to be able to
    sample exactly from the distribution <math alttext="p Subscript theta Baseline
    left-parenthesis bold x right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math> , which isn’t possible
    due to the intractable denominator. Instead, we can use our Langevin sampling
    procedure to generate a set of observations with low energy scores. The process
    would need to run for infinitely many steps to produce a perfect sample (which
    is obviously impractical), so instead we run for some small number of steps, on
    the assumption that this is good enough to produce a meaningful loss function.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算假样本的能量分数，我们需要能够从分布 <math alttext="p Subscript theta Baseline left-parenthesis
    bold x right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo>
    <mi>𝐱</mi> <mo>)</mo></mrow></mrow></math> 中精确抽样，但由于不可解的分母，这是不可能的。相反，我们可以使用Langevin采样过程生成一组能量分数较低的观察。这个过程需要运行无限多步才能产生完美样本（显然是不切实际的），因此我们运行一些小步数，假设这足以产生有意义的损失函数。
- en: We also maintain a buffer of samples from previous iterations, so that we can
    use this as the starting point for the next batch, rather than pure random noise.
    The code to produce the sampling buffer is shown in [Example 7-5](#sampling_buffer).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还维护一个来自先前迭代的样本缓冲区，这样我们可以将其用作下一批的起点，而不是纯随机噪声。生成采样缓冲区的代码如[示例7-5](#sampling_buffer)所示。
- en: Example 7-5\. The `Buffer`
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例7-5。`缓冲区`
- en: '[PRE4]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](Images/1.png)](#co_energy_based_models_CO3-1)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_energy_based_models_CO3-1)'
- en: The sampling buffer is initialized with a batch of random noise.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 采样缓冲区用一批随机噪声初始化。
- en: '[![2](Images/2.png)](#co_energy_based_models_CO3-2)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_energy_based_models_CO3-2)'
- en: On average, 5% of observations are generated from scratch (i.e., random noise)
    each time.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 平均而言，每次有5%的观察是从头开始生成的（即，随机噪声）。
- en: '[![3](Images/3.png)](#co_energy_based_models_CO3-3)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_energy_based_models_CO3-3)'
- en: The rest are pulled at random from the existing buffer.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的随机从现有缓冲区中提取。
- en: '[![4](Images/4.png)](#co_energy_based_models_CO3-4)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_energy_based_models_CO3-4)'
- en: The observations are concatenated and run through the Langevin sampler.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这些观察被连接并通过Langevin采样器运行。
- en: '[![5](Images/5.png)](#co_energy_based_models_CO3-5)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_energy_based_models_CO3-5)'
- en: The resulting sample is added to the buffer, which is trimmed to a max length
    of 8,192 observations.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的样本被添加到缓冲区中，缓冲区被修剪为最多8,192个观察。
- en: '[Figure 7-5](#contrastive_divergence) shows one training step of contrastive
    divergence. The scores of real observations are pushed down by the algorithm and
    the scores of fake observations are pulled up, without caring about normalizing
    these scores after each step.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-5](#contrastive_divergence)显示了对比散度的一个训练步骤。真实观察的分数被算法推低，而假观察的分数被拉高，每一步之后都不考虑对这些分数进行归一化。'
- en: '![](Images/gdl2_0705.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0705.png)'
- en: Figure 7-5\. One step of contrastive divergence
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-5。对比散度的一步
- en: We can code up the training step of the contrastive divergence algorithm within
    a custom Keras model as shown in [Example 7-6](#contrastive_divergence_python).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '我们可以编写对比散度算法的训练步骤，如[示例7-6](#contrastive_divergence_python)所示，在自定义Keras模型中。 '
- en: Example 7-6\. EBM trained using contrastive divergence
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例7-6。使用对比散度训练的EBM
- en: '[PRE5]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](Images/1.png)](#co_energy_based_models_CO4-1)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_energy_based_models_CO4-1)'
- en: A small amount of random noise is added to the real images, to avoid the model
    overfitting to the training set.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为真实图像添加少量随机噪声，以避免模型过度拟合训练集。
- en: '[![2](Images/2.png)](#co_energy_based_models_CO4-2)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_energy_based_models_CO4-2)'
- en: A set of fake images are sampled from the buffer.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 一组假图像从缓冲区中抽样。
- en: '[![3](Images/3.png)](#co_energy_based_models_CO4-3)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_energy_based_models_CO4-3)'
- en: The real and fake images are run through the model to produce real and fake
    scores.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 真实和假图像通过模型运行以产生真实和假分数。
- en: '[![4](Images/4.png)](#co_energy_based_models_CO4-4)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_energy_based_models_CO4-4)'
- en: The contrastive divergence loss is simply the difference between the scores
    of real and fake observations.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 对比散度损失简单地是真实和假观察的分数之间的差异。
- en: '[![5](Images/5.png)](#co_energy_based_models_CO4-5)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_energy_based_models_CO4-5)'
- en: A regularization loss is added to avoid the scores becoming too large.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 添加正则化损失以避免分数变得过大。
- en: '[![6](Images/6.png)](#co_energy_based_models_CO4-6)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_energy_based_models_CO4-6)'
- en: Gradients of the loss function with respect to the weights of the network are
    calculated for backpropagation.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 通过反向传播计算网络权重相对于损失函数的梯度。
- en: '[![7](Images/7.png)](#co_energy_based_models_CO4-7)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](Images/7.png)](#co_energy_based_models_CO4-7)'
- en: The `test_step` is used during validation and calculates the contrastive divergence
    between the scores of a set of random noise and data from the training set. It
    can be used as a measure for how well the model is training (see the following
    section).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`test_step` 用于在验证过程中计算一组随机噪声和训练集中的数据之间的对比散度。它可以作为衡量模型训练效果的指标（见下一节）。'
- en: Analysis of the Energy-Based Model
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 能量基模型的分析
- en: The loss curves and supporting metrics from the training process are shown in
    [Figure 7-6](#ebm_loss_curves).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程中的损失曲线和支持指标显示在 [Figure 7-6](#ebm_loss_curves) 中。
- en: '![](Images/gdl2_0706.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0706.png)'
- en: Figure 7-6\. Loss curves and metrics for the training process of the EBM
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-6\. EBM 训练过程的损失曲线和指标
- en: Firstly, notice that the loss calculated during the training step is approximately
    constant and small across epochs. While the model is constantly improving, so
    is the quality of generated images in the buffer that it is required to compare
    against real images from the training set, so we shouldn’t expect the training
    loss to fall significantly.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，注意到在训练步骤中计算的损失在各个周期中大致保持不变且较小。虽然模型不断改进，但与训练集中的真实图像进行比较的缓冲区中生成的图像质量也在提高，因此我们不应该期望训练损失显著下降。
- en: Therefore, to judge model performance, we also set up a validation process that
    doesn’t sample from the buffer, but instead scores a sample of random noise and
    compares this against the scores of examples from the training set. If the model
    is improving, we should see that the contrastive divergence falls over the epochs
    (i.e., it is getting better at distinguishing random noise from real images),
    as can be seen in [Figure 7-6](#ebm_loss_curves).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了评估模型性能，我们还建立了一个验证过程，该过程不从缓冲区中采样，而是对一组随机噪声进行评分，并将其与训练集中的示例进行比较。如果模型正在改进，我们应该看到对比散度随着周期的增加而下降（即，它在区分随机噪声和真实图像方面变得更好），如
    [Figure 7-6](#ebm_loss_curves) 中所示。
- en: Generating new samples from the EBM is simply a case of running the Langevin
    sampler for a large number of steps, from a standing start (random noise), as
    shown in [Example 7-7](#generating_ebm). The observation is forced *downhill*,
    following the gradients of the scoring function with respect to the input, so
    that out of the noise, a plausible observation appears.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 从 EBM 生成新样本只需运行 Langevin 采样器进行大量步骤，从一个静止状态（随机噪声）开始，如 [Example 7-7](#generating_ebm)
    中所示。观测被迫 *下坡*，沿着相对于输入的评分函数的梯度，以便在噪声中出现一个合理的观测。
- en: Example 7-7\. Generating new observations using the EBM
  id: totrans-121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-7\. 使用 EBM 生成新观测
- en: '[PRE6]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Some examples of observations produced by the sampler after 50 epochs of training
    are shown in [Figure 7-7](#ebm_examples).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在经过 50 个周期的训练后，采样器生成的一些观测示例显示在 [Figure 7-7](#ebm_examples) 中。
- en: '![](Images/gdl2_0707.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0707.png)'
- en: Figure 7-7\. Examples produced by the Langevin sampler using the EBM model to
    direct the gradient descent
  id: totrans-125
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-7\. 使用 EBM 模型的 Langevin 采样器生成的示例以指导梯度下降
- en: We can even show a replay of how a single observation is generated by taking
    snapshots of the current observations during the Langevin sampling process—this
    is shown in [Figure 7-8](#langevin_examples).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们甚至可以展示单个观测是如何通过在 Langevin 采样过程中拍摄当前观测的快照生成的——这在 [Figure 7-8](#langevin_examples)
    中展示。
- en: '![](Images/gdl2_0708.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0708.png)'
- en: Figure 7-8\. Snapshots of an observation at different steps of the Langevin
    sampling process
  id: totrans-128
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-8\. Langevin 采样过程中不同步骤的观测快照
- en: Other Energy-Based Models
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他能量基模型
- en: In the previous example we made use of a deep EBM trained using contrastive
    divergence with a Langevin dynamics sampler. However, early EBM models did not
    make use of Langevin sampling, but instead relied on other techniques and architectures.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们使用了使用对比散度和 Langevin 动力学采样器训练的深度 EBM。然而，早期的 EBM 模型并没有使用 Langevin 采样，而是依赖于其他技术和架构。
- en: 'One of the earliest examples of an EBM was the *Boltzmann machine*.^([6](ch07.xhtml#idm45387010130976))
    This is a fully connected, undirected neural network, where binary units are either
    *visible* (*v*) or *hidden* (*h*). The energy of a given configuration of the
    network is defined as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 最早的能量基模型之一是 *Boltzmann 机*。^([6](ch07.xhtml#idm45387010130976)) 这是一个全连接的无向神经网络，其中二进制单元要么是
    *可见*（*v*），要么是 *隐藏*（*h*）。网络的给定配置的能量定义如下：
- en: <math alttext="upper E Subscript theta Baseline left-parenthesis v comma h right-parenthesis
    equals minus one-half left-parenthesis v Superscript upper T Baseline upper L
    v plus h Superscript upper T Baseline upper J h plus v Superscript upper T Baseline
    upper W h right-parenthesis" display="block"><mrow><msub><mi>E</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <mi>v</mi> <mo>,</mo> <mi>h</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mo>-</mo> <mfrac><mn>1</mn> <mn>2</mn></mfrac> <mfenced separators="" open="("
    close=")"><msup><mi>v</mi> <mi>T</mi></msup> <mi>L</mi> <mi>v</mi> <mo>+</mo>
    <msup><mi>h</mi> <mi>T</mi></msup> <mi>J</mi> <mi>h</mi> <mo>+</mo> <msup><mi>v</mi>
    <mi>T</mi></msup> <mi>W</mi> <mi>h</mi></mfenced></mrow></math>
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper E Subscript theta Baseline left-parenthesis v comma h right-parenthesis
    equals minus one-half left-parenthesis v Superscript upper T Baseline upper L
    v plus h Superscript upper T Baseline upper J h plus v Superscript upper T Baseline
    upper W h right-parenthesis" display="block"><mrow><msub><mi>E</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <mi>v</mi> <mo>,</mo> <mi>h</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mo>-</mo> <mfrac><mn>1</mn> <mn>2</mn></mfrac> <mfenced separators="" open="("
    close=")"><msup><mi>v</mi> <mi>T</mi></msup> <mi>L</mi> <mi>v</mi> <mo>+</mo>
    <msup><mi>h</mi> <mi>T</mi></msup> <mi>J</mi> <mi>h</mi> <mo>+</mo> <msup><mi>v</mi>
    <mi>T</mi></msup> <mi>W</mi> <mi>h</mi></mfenced></mrow></math>
- en: where <math alttext="upper W comma upper L comma upper J"><mrow><mi>W</mi> <mo>,</mo>
    <mi>L</mi> <mo>,</mo> <mi>J</mi></mrow></math> are the weights matrices that are
    learned by the model. Training is achieved by contrastive divergence, but using
    Gibbs sampling to alternate between the visible and hidden layers until an equilibrium
    is found. In practice this is very slow and not scalable to large numbers of hidden
    units.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math alttext="upper W comma upper L comma upper J"><mrow><mi>W</mi> <mo>,</mo>
    <mi>L</mi> <mo>,</mo> <mi>J</mi></mrow></math> 是模型学习的权重矩阵。训练通过对比散度实现，但使用 Gibbs
    采样在可见层和隐藏层之间交替，直到找到平衡。实际上，这是非常缓慢的，不适用于大量隐藏单元。
- en: Tip
  id: totrans-134
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: See Jessica Stringham’s blog post [“Gibbs Sampling in Python”](https://oreil.ly/tXmOq)
    for an excellent simple example of Gibbs sampling.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看 Jessica Stringham 的博客文章 [“Python 中的 Gibbs 采样”](https://oreil.ly/tXmOq) 以获取
    Gibbs 采样的优秀简单示例。
- en: An extension to this model, the *restricted Boltzmann machine* (RBM), removes
    the connections between units of the same type, therefore creating a two-layer
    bipartite graph. This allows RBMs to be stacked into *deep belief networks* to
    model more complex distributions. However, modeling high-dimensional data with
    RBMs remains impractical, due to the fact that Gibbs sampling with long mixing
    times is still required.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型的扩展，*受限玻尔兹曼机*（RBM），移除了相同类型单元之间的连接，因此创建了一个两层的二部图。这使得RBM可以堆叠成*深度信念网络*，以建模更复杂的分布。然而，使用RBM对高维数据进行建模仍然是不切实际的，因为仍然需要长混合时间的吉布斯采样。
- en: It was only in the late 2000s that EBMs were shown to have potential for modeling
    more high-dimensional datasets and a framework for building deep EBMs was established.^([7](ch07.xhtml#idm45387010525504))
    Langevin dynamics became the preferred sampling method for EBMs, which later evolved
    into a training technique known as *score matching*. This further developed into
    a model class known as *Denoising Diffusion Probabilistic Models*, which power
    state-of-the-art generative models such as DALL.E 2 and ImageGen. We will explore
    diffusion models in more detail in [Chapter 8](ch08.xhtml#chapter_diffusion).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 直到2000年代末，EBM才被证明具有对更高维数据集进行建模的潜力，并建立了一个构建深度EBM的框架。^([7](ch07.xhtml#idm45387010525504))
    Langevin动力学成为EBM的首选采样方法，后来演变成一种称为*得分匹配*的训练技术。这进一步发展成一种称为*去噪扩散概率模型*的模型类，为DALL.E
    2和ImageGen等最先进的生成模型提供动力。我们将在[第8章](ch08.xhtml#chapter_diffusion)中更详细地探讨扩散模型。
- en: Summary
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Energy-based models are a class of generative model that make use of an energy
    scoring function—a neural network that is trained to output low scores for real
    observations and high scores for generated observations. Calculating the probability
    distribution given by this score function would require normalizing by an intractable
    denominator. EBMs avoid this problem by utilizing two tricks: contrastive divergence
    for training the network and Langevin dynamics for sampling new observations.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 基于能量的模型是一类生成模型，利用能量评分函数——一个经过训练的神经网络，用于为真实观察输出低分数，为生成观察输出高分数。计算由该得分函数给出的概率分布需要通过一个难以处理的分母进行归一化。EBM通过利用两个技巧来避免这个问题：对比散度用于训练网络，Langevin动力学用于采样新观察。
- en: The energy function is trained by minimizing the difference between the generated
    sample scores and the scores of the training data, a technique known as contrastive
    divergence. This can be shown to be equivalent to minimizing the negative log-likelihood,
    as required by maximum likelihood estimation, but does not require us to calculate
    the intractable normalizing denominator. In practice, we approximate the sampling
    process for the fake samples to ensure the algorithm remains efficient.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 能量函数通过最小化生成样本得分与训练数据得分之间的差异来进行训练，这种技术称为对比散度。可以证明这等价于最小化负对数似然，这是最大似然估计所要求的，但不需要我们计算难以处理的归一化分母。在实践中，我们近似为假样本的采样过程，以确保算法保持高效。
- en: Sampling of deep EBMs is achieved through Langevin dynamics, a technique that
    uses the gradient of the score with respect to the input image to gradually transform
    random noise into a plausible observation by updating the input in small steps,
    following the gradient downhill. This improves upon earlier methods such as Gibbs
    sampling, which is utilized by restricted Boltzmann machines.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 深度EBM的采样是通过Langevin动力学实现的，这是一种利用得分相对于输入图像的梯度逐渐将随机噪声转化为合理观察的技术，通过更新输入进行小步骤，沿着梯度下降。这改进了早期的方法，如受限玻尔兹曼机使用的吉布斯采样。
- en: ^([1](ch07.xhtml#idm45387012482384-marker)) Yilun Du and Igor Mordatch, “Implicit
    Generation and Modeling with Energy-Based Models,” March 20, 2019, [*https://arxiv.org/abs/1903.08689*](https://arxiv.org/abs/1903.08689).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch07.xhtml#idm45387012482384-marker)) 杜一伦和伊戈尔·莫达奇，“基于能量的模型的隐式生成和建模”，2019年3月20日，[*https://arxiv.org/abs/1903.08689*](https://arxiv.org/abs/1903.08689)。
- en: ^([2](ch07.xhtml#idm45387012241152-marker)) Prajit Ramachandran et al., “Searching
    for Activation Functions,” October 16, 2017, [*https://arxiv.org/abs/1710.05941v2*](https://arxiv.org/abs/1710.05941v2).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch07.xhtml#idm45387012241152-marker)) Prajit Ramachandran等人，“搜索激活函数”，2017年10月16日，[*https://arxiv.org/abs/1710.05941v2*](https://arxiv.org/abs/1710.05941v2)。
- en: ^([3](ch07.xhtml#idm45387012049200-marker)) Max Welling and Yee Whye Teh, “Bayesian
    Learning via Stochastic Gradient Langevin Dynamics,” 2011, *[*https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf*](https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf)*
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch07.xhtml#idm45387012049200-marker)) Max Welling和Yee Whye Teh，“通过随机梯度Langevin动力学进行贝叶斯学习”，2011年，[*https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf*](https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf)
- en: ^([4](ch07.xhtml#idm45387011649104-marker)) Geoffrey E. Hinton, “Training Products
    of Experts by Minimizing Contrastive Divergence,” 2002, *[*https://www.cs.toronto.edu/~hinton/absps/tr00-004.pdf*](https://www.cs.toronto.edu/~hinton/absps/tr00-004.pdf)*.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch07.xhtml#idm45387011649104-marker)) Geoffrey E. Hinton，“通过最小化对比散度训练专家乘积”，2002年，[*https://www.cs.toronto.edu/~hinton/absps/tr00-004.pdf*](https://www.cs.toronto.edu/~hinton/absps/tr00-004.pdf)。
- en: ^([5](ch07.xhtml#idm45387011627440-marker)) Oliver Woodford, “Notes on Contrastive
    Divergence,” 2006, [*https://www.robots.ox.ac.uk/~ojw/files/NotesOnCD.pdf*](https://www.robots.ox.ac.uk/~ojw/files/NotesOnCD.pdf).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch07.xhtml#idm45387011627440-marker)) Oliver Woodford，“对比散度笔记”，2006年，[*https://www.robots.ox.ac.uk/~ojw/files/NotesOnCD.pdf*](https://www.robots.ox.ac.uk/~ojw/files/NotesOnCD.pdf)。
- en: ^([6](ch07.xhtml#idm45387010130976-marker)) David H. Ackley et al., “A Learning
    Algorithm for Boltzmann Machines,” 1985, *Cognitive Science* 9(1), 147-165.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch07.xhtml#idm45387010130976-marker)) David H. Ackley等人，“玻尔兹曼机的学习算法”，1985年，*认知科学*
    9(1), 147-165。
- en: ^([7](ch07.xhtml#idm45387010525504-marker)) Yann Lecun et al., “A Tutorial on
    Energy-Based Learning,” 2006, *[*https://www.researchgate.net/publication/200744586_A_tutorial_on_energy-based_learning*](https://www.researchgate.net/publication/200744586_A_tutorial_on_energy-based_learning)*.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch07.xhtml#idm45387010525504-marker)) Yann Lecun等人，“基于能量的学习教程”，2006年，*[*https://www.researchgate.net/publication/200744586_A_tutorial_on_energy-based_learning*](https://www.researchgate.net/publication/200744586_A_tutorial_on_energy-based_learning)*.
