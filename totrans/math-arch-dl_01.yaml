- en: 2 Vectors, matrices, and tensors in machine learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 机器学习中的向量、矩阵和张量
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Vectors and matrices and their role in datascience
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量和矩阵及其在数据科学中的作用
- en: Working with eigenvalues and eigenvectors
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用特征值和特征向量工作
- en: Finding the axes of a hyper-ellipse
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找超椭圆的轴
- en: At its core, machine learning, and indeed all computer software, is about number
    crunching. We input a set of numbers into the machine and get back a different
    set of numbers as output. However, this cannot be done randomly. It is important
    to organize these numbers appropriately and group them into meaningful objects
    that go into and come out of the machine. This is where vectors and matrices come
    in. These are concepts that mathematicians have been using for centuries—we are
    simply reusing them in machine learning.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本质上，机器学习，以及所有计算机软件，都是关于数字处理的。我们将一组数字输入到机器中，并得到一组不同的数字作为输出。然而，这不能是随机的。适当地组织这些数字并将它们分组成有意义的对象，这些对象进入并从机器中出来，这是非常重要的。这就是向量矩阵发挥作用的地方。这些是数学家使用了几百年的概念——我们只是在机器学习中重新使用它们。
- en: In this chapter, we will study vectors and matrices, primarily from a machine
    learning point of view. Starting from the basics, we will quickly graduate to
    advanced concepts, restricting ourselves to topics relevant to machine learning.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将从机器学习角度研究向量和矩阵，从基础知识开始，迅速过渡到高级概念，限制 ourselves 到与机器学习相关的主题。
- en: We provide Jupyter Notebook-based Python implementations for most of the concepts
    discussed in this and other chapters. Complete, fully functional code that can
    be downloaded and executed (after installing Python and Jupyter Notebook) can
    be found at [http://mng.bz/KMQ4](http://mng.bz/KMQ4). The code relevant to this
    chapter can be found at [http://mng.bz/d4nz](http://mng.bz/d4nz).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为本书及本章讨论的大部分概念提供了基于 Jupyter Notebook 的 Python 实现。完整的、功能齐全的代码（在安装 Python 和
    Jupyter Notebook 后）可以下载并执行，地址为 [http://mng.bz/KMQ4](http://mng.bz/KMQ4)。本章相关的代码可以在
    [http://mng.bz/d4nz](http://mng.bz/d4nz) 找到。
- en: 2.1 Vectors and their role in machine learning
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 向量和它们在机器学习中的作用
- en: Let’s revisit the machine learning model for a cat brain introduced in section
    [1.3](../Text/01.xhtml#sec-cat_brain). It takes two numbers as input, representing
    the hardness and sharpness of the object in front of the cat. The cat brain processes
    the input and generates an output threat score that leads to a decision to *run
    away* or *ignore* or *approach and purr*. The two input numbers usually appear
    together, and it will be handy to group them into a single object. This object
    will be an ordered sequence of two numbers, the first representing hardness and
    the second representing sharpness. Such an object is a perfect example of a vector.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下第 [1.3](../Text/01.xhtml#sec-cat_brain) 节中介绍的猫脑机器学习模型。它接受两个数字作为输入，代表猫面前物体的硬度和锋利度。猫脑处理输入并生成一个输出威胁分数，这将导致决定
    *逃跑*、*忽略* 或 *靠近并咕噜咕噜叫*。这两个输入数字通常一起出现，将它们组合成一个单一对象将非常方便。这个对象将是两个数字的有序序列，第一个代表硬度，第二个代表锋利度。这样的对象是向量的一个完美例子。
- en: Thus, a *vector* can be thought of as an ordered sequence of two or more numbers,
    also known as an *array* of numbers.[¹](02.xhtml#fn4) Vectors constitute a compact
    way of denoting a set of numbers that together represent some entity. In this
    book, vectors are represented by lowercase letters with an overhead arrow and
    arrays by square brackets. For instance, the input to the cat brain model in section
    [1.3](../Text/01.xhtml#sec-cat_brain) was a vector ![](../../OEBPS/Images/eq_02-00-a2.png),
    where *x*[0] represented hardness and *x*[1] represented sharpness.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，可以将 *向量* 理解为两个或更多数字的有序序列，也称为数字的 *数组*。[¹](02.xhtml#fn4) 向量构成了一种紧凑的方式来表示一组数字，这些数字共同代表某个实体。在本书中，向量用带顶箭头的低字母表示，数组用方括号表示。例如，第
    [1.3](../Text/01.xhtml#sec-cat_brain) 节中猫脑模型的输入是一个向量 ![](../../OEBPS/Images/eq_02-00-a2.png)，其中
    *x*[0] 代表硬度，*x*[1] 代表锋利度。
- en: Outputs to machine learning models are also often represented as vectors. For
    instance, consider an object recognition model that takes an image as input and
    emits a set of numbers indicating the probabilities that the image contains a
    dog, human, or cat, respectively. The output of such a model is a three element
    vector ![](../../OEBPS/Images/eq_02-00-b2.png), where the number *y*[0] denotes
    the probability that the image contains a dog, *y*[1] denotes the probability
    that the image contains a human, and *y*[2] denotes the probability that the image
    contains a cat. Figure [2.1](02.xhtml#fig-vec_out) shows some possible input images
    and corresponding output vectors.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型的输出也常常表示为向量。例如，考虑一个以图像为输入并输出一系列数字的对象识别模型，这些数字表示图像包含狗、人类或猫的概率。这种模型的输出是一个三元素向量
    ![](../../OEBPS/Images/eq_02-00-b2.png)，其中数字 *y*[0] 表示图像包含狗的概率，*y*[1] 表示图像包含人类的概率，*y*[2]
    表示图像包含猫的概率。图[2.1](02.xhtml#fig-vec_out)显示了可能的输入图像和相应的输出向量。
- en: '![](../../OEBPS/Images/CH02_F01_Chaudhury.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH02_F01_Chaudhury.png)'
- en: Figure 2.1 Input images and corresponding output vectors denoting probabilities
    that the image contains a dog and/or human and/or cat, respectively. Example output
    vectors are shown.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 输入图像及其对应的输出向量，表示图像包含狗、人类和/或猫的概率。显示了示例输出向量。
- en: In multilayered machines like neural networks, the input and output to a layer
    can be vectors. We also typically represent the parameters of the model function
    (see section [1.3](../Text/01.xhtml#sec-cat_brain)) as vectors. This is illustrated
    in section [2.3](02.xhtml#sec-matrices).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在像神经网络这样的多层机器中，层的输入和输出可以是向量。我们通常也将模型函数的参数（参见第[1.3](../Text/01.xhtml#sec-cat_brain)节）表示为向量。这一点在第[2.3](02.xhtml#sec-matrices)节中有说明。
- en: Table 2.1 Toy documents and corresponding feature vectors describing them. Words
    eligible for the feature vector are bold. The first element of the feature vector
    indicates the number of occurrences of the word *gun* and the second *violence*.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.1 玩具文档及其对应的描述它们的特征向量。适合特征向量的单词是粗体的。特征向量的第一个元素表示单词**枪**出现的次数，第二个元素是**暴力**。
- en: '| Docid | Document | Feature vector |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| Docid | 文档 | 特征向量 |'
- en: '| --- | --- | --- |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| *d*[0] | Roses are lovely. Nobody hates roses. | [0 0] |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| *d*[0] | 玫瑰很漂亮。没有人讨厌玫瑰。 | [0 0] |'
- en: '| *d*[1] | **Gun violence** has reached an epidemic proportion in America.
    | [1 1] |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| *d*[1] | **枪支暴力**在美国已经达到了流行病的比例。 | [1 1] |'
- en: '| *d*[2] | The issue of **gun violence** is really over-hyped. One can find
    many instances of **violence**, where no **guns** were involved. | [2 2] |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| *d*[2] | **枪支暴力**的问题实际上被过度炒作。可以找到许多涉及**暴力**的例子，但没有**枪支**。 | [2 2] |'
- en: '| *d*[3] | **Guns** are for **violence** prone people. **Violence** begets
    **guns**. **Guns** beget **violence**. | [3 3] |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| *d*[3] | **枪支**是为**暴力**倾向的人准备的。**暴力**滋生**枪支**。**枪支**滋生**暴力**。 | [3 3] |'
- en: '| *d*[4] | I like **guns** but I hate **violence**. I have never been involved
    in **violence**. But I own many **guns**. **Gun violence** is incomprehensible
    to me. I do believe **gun** owners are the most anti **violence** people on the
    planet. He who never uses a **gun** will be prone to senseless **violence**. |
    [5 5] |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| *d*[4] | 我喜欢**枪支**但讨厌**暴力**。我从未参与过**暴力**。但我拥有许多**枪支**。**枪支暴力**对我来说是无法理解的。我确实相信**枪支**拥有者是地球上最反**暴力**的人。从不使用**枪支**的人容易陷入无意义的**暴力**。
    | [5 5] |'
- en: '| *d*[5] | **Guns** were used in a armed robbery in San Francisco last night.
    | [1 0] |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| *d*[5] | 昨晚旧金山发生了一起武装抢劫，使用了**枪支**。 | [1 0] |'
- en: '| *d*[6] | Acts of **violence** usually involves a weapon. | [0 1] |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| *d*[6] | **暴力**行为通常涉及武器。 | [0 1] |'
- en: 'One particularly significant notion in machine learning and data science is
    the idea of a *feature vector*. This is essentially a vector that describes various
    properties of the object being dealt with in a particular machine learning problem.
    We will illustrate the idea with an example from the world of natural language
    processing (NLP). Suppose we have a set of documents. We want to create a document
    retrieval system where, given a new document, we have to retrieve similar documents
    in the system. This essentially boils down to estimating the similarity between
    documents in a quantitative fashion. We will study this problem in detail later,
    but for now, we want to note that the most natural way to approach this is to
    create feature vectors for each document that quantitatively describe the document.
    In section [2.5.6](02.xhtml#subsection-dot_product), we will see how to measure
    the similarity between these vectors; here, let’s focus on simply creating descriptor
    vectors for the documents. A popular way to do this is to choose a set of interesting
    words (we typically exclude words like “and,” “if,” and “to” that are present
    in all documents from this list), count the number of occurrences of those interesting
    words in each document, and make a vector of those values. Table [2.1](02.xhtml#tab-doc-vec)
    shows a toy example with six documents and corresponding feature vectors. For
    simplicity, we have considered only two of the possible set of words: *gun* and
    *violence*, plural or singular, uppercase or lowercase.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习和数据科学中，一个特别重要的概念是 *特征向量* 的概念。这本质上是一个向量，它描述了在特定机器学习问题中处理的对象的各种属性。我们将通过自然语言处理（NLP）领域的一个例子来说明这个概念。假设我们有一组文档。我们想要创建一个文档检索系统，其中，给定一个新的文档，我们必须检索系统中相似的文档。这本质上归结为以定量方式估计文档之间的相似性。我们将在稍后详细研究这个问题，但现在，我们想要指出，处理这个问题的最自然的方法是为每个文档创建特征向量，这些向量定量描述了文档。在
    [2.5.6](02.xhtml#subsection-dot_product) 节中，我们将看到如何测量这些向量之间的相似性；在这里，让我们专注于简单地为文档创建描述向量。做这件事的一个流行方法是选择一组有趣的词（我们通常排除像“and”、“if”和“to”这样的词，这些词在所有文档中都存在），计算每个文档中这些有趣词的出现次数，并创建一个包含这些值的向量。表
    [2.1](02.xhtml#tab-doc-vec) 展示了一个包含六个文档及其相应特征向量的玩具示例。为了简单起见，我们只考虑了可能的一组单词中的两个：*枪*
    和 *暴力*，无论是复数还是单数，大写还是小写。
- en: As a different example, the sequence of pixels in an image can also be viewed
    as a feature vector. Neural networks in computer vision tasks usually expect this
    feature vector.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 作为另一个例子，图像中的像素序列也可以被视为一个特征向量。在计算机视觉任务中，神经网络通常期望这个特征向量。
- en: 2.1.1 The geometric view of vectors and its significance in machine learning
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.1 向量的几何视角及其在机器学习中的重要性
- en: Vectors can also be viewed geometrically. The simplest example is a two-element
    vector ![](../../OEBPS/Images/eq_02-00-c2.png). Its two elements can be taken
    to be *x* and *y*, Cartesian coordinates in a two-dimensional space, in which
    case the vector corresponds to a point in that space. *Vectors with n elements
    represent points in an n-dimensional space*. The ability to see inputs and outputs
    of a machine learning model as points allows us to view the model itself as a
    geometric transformation that maps input points to output points in some high-dimensional
    space. We have already seen this in section [1.4](../Text/01.xhtml#sec-geom-view-ml).
    It is an enormously powerful concept we will use throughout the book.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 向量也可以从几何学的角度来理解。最简单的例子是一个二维向量 ![二维向量](../../OEBPS/Images/eq_02-00-c2.png)。它的两个元素可以被认为是
    *x* 和 *y*，这是二维空间中的笛卡尔坐标，在这种情况下，向量对应于该空间中的一个点。*具有 n 个元素的向量代表 n 维空间中的点*。将机器学习模型的输入和输出视为点的能力使我们能够将模型本身视为一种几何变换，它将输入点映射到某些高维空间中的输出点。我们已经在
    [1.4](../Text/01.xhtml#sec-geom-view-ml) 节中看到了这一点。这是一个我们将在整本书中使用的极其强大的概念。
- en: A vector represents a point in space. Also, an array of coordinate values like
    ![](../../OEBPS/Images/eq_02-00-d2.png) describes the position of one point *in
    a given coordinate system*. Hence, an array (of coordinate values) can be viewed
    as the quantitative representation of a vector. See figure [2.2](02.xhtml#fig-vector_diagram)
    to get an intuitive understanding of this.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 向量表示空间中的一个点。此外，一个坐标值数组，例如 ![坐标值数组](../../OEBPS/Images/eq_02-00-d2.png)，描述了在给定坐标系中一个点的位置
    *in a given coordinate system*。因此，一个坐标值数组（的集合）可以被视为向量的定量表示。参见图 [2.2](02.xhtml#fig-vector_diagram)
    以获得对此的直观理解。
- en: '![](../../OEBPS/Images/CH02_F02_Chaudhury.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![查德胡里图像](../../OEBPS/Images/CH02_F02_Chaudhury.png)'
- en: 'Figure 2.2 A vector describing the position of point P with respect to point
    O. The basic mental picture is an arrowed line. This agrees with the definition
    of a vector that you may have learned in high school: a vector has a magnitude
    (length of the arrowed line) and direction (indicated by the arrow). On a plane,
    this is equivalent to the ordered pair of numbers *x*, *y*, where the geometric
    interpretations of *x* and *y* are as shown in the figure. In this context, it
    is worthwhile to note that only the relative positions of the points O and P matter.
    If both the points are moved, keeping their relationship intact, the vector does
    not change.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 描述点P相对于点O的位置的向量。基本的思维图是一个带箭头的线。这与你在高中可能学过的向量的定义一致：向量有一个大小（箭头线的长度）和方向（由箭头指示）。在平面上，这相当于有序对数字
    *x*，*y*，其中 *x* 和 *y* 的几何解释如图所示。在此背景下，值得注意的是，只有点O和P的相对位置才是重要的。如果两个点都移动，保持它们的关系不变，向量不会改变。
- en: For a real life example, consider the plane of a page of this book. Suppose
    we want to reach the top-right corner point of the page from the bottom-left corner.
    Let’s call the bottom-left corner *O* and the top-right corner *P*. We can travel
    the width (8.5 inches) to the right to reach the bottom-left corner and then travel
    the height (11 inches) upward to reach the top-right corner. Thus, if we choose
    a coordinate system with the bottom-left corner as the origin and the *X*-axis
    along the width, and the *Y*-axis along the height, point *P* corresponds to the
    array representation ![](../../OEBPS/Images/eq_02-00-e2.png). But we could also
    travel along the diagonal from the bottom-left to the top-right corner to reach
    *P* from *O*. Either way, we end up at the same point *P*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个现实生活中的例子，考虑这本书的一页的平面。假设我们想从左下角到达页面的右上角。让我们称左下角为 *O*，右上角为 *P*。我们可以向右移动宽度（8.5英寸）到达左下角，然后向上移动高度（11英寸）到达右上角。因此，如果我们选择一个以左下角为原点，*X*轴沿着宽度，*Y*轴沿着高度的坐标系，点
    *P* 对应于数组表示 ![](../../OEBPS/Images/eq_02-00-e2.png)。但我们也可能沿着对角线从左下角到右上角移动到达 *P*。无论哪种方式，我们最终都会到达同一个点
    *P*。
- en: This leads to a conundrum. The vector ![](../../OEBPS/Images/AR_OP.png) represents
    the abstract geometric notion “position of *P* with respect to *O*” independent
    of our choice of coordinate axes. On the other hand, the array representation
    depends on the choice of a coordinate system. For example, the array ![](../../OEBPS/Images/eq_02-00-e2.png)
    represents the top-right corner point *P* only under a specific choice of coordinate
    axes (parallel to the sides of the page) and a reference point (bottom-left corner).
    Ideally, to be unambiguous, we should specify the coordinate system along with
    the array representation. Why don’t we ever do this in machine learning? Because
    in machine learning, it doesn’t exactly matter what the coordinate system is as
    long as we stick to any fixed coordinate system. Machine learning is about minimizing
    loss functions (which we will study later). As such, absolute positions of point
    are immaterial, only relative positions matter.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了一个难题。向量 ![](../../OEBPS/Images/AR_OP.png) 代表了抽象的几何概念“*P*相对于*O*的位置”，与我们选择的坐标轴无关。另一方面，数组表示依赖于坐标系的选择。例如，数组
    ![](../../OEBPS/Images/eq_02-00-e2.png) 仅在特定的坐标轴选择（与页面边缘平行）和参考点（左下角）下表示右上角点 *P*。理想情况下，为了明确，我们应该指定坐标系以及数组表示。为什么在机器学习中我们从不这样做呢？因为在机器学习中，只要我们坚持任何固定的坐标系，坐标系统的具体选择并不重要。机器学习是关于最小化损失函数（我们将在后面学习）。因此，点的绝对位置无关紧要，只有相对位置才是重要的。
- en: There are explicit rules (which we will study later) that state how the vector
    transforms when the coordinate system changes. We will invoke them when necessary.
    All vectors used in a machine learning computation must consistently use the same
    coordinate system or be transformed appropriately.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 存在明确的规则（我们将在后面学习），这些规则说明了当坐标系改变时向量如何变换。在需要时我们将调用这些规则。在机器学习计算中使用的所有向量必须始终使用相同的坐标系或适当地进行变换。
- en: 'One other point: planar spaces, such as the plane of the paper on which this
    book is written, are two-dimensional (2D). The mechanical world we live in is
    three-dimensional (3D). Human imagination usually fails to see higher dimensions.
    In machine learning and data science, we often talk of spaces with thousands of
    dimensions. You may not be able to see those spaces in your mind, but that is
    not a crippling limitation. You can use 3D analogues in your head. They work in
    a surprisingly large variety of cases. However, it is important to bear in mind
    that this is not always true. Some examples where the lower-dimensional intuitions
    fail at higher dimensions will be shown later.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 另一点：平面空间，例如本书所写的纸张平面，是二维的（2D）。我们生活的机械世界是三维的（3D）。人类的想象力通常无法看到更高维度。在机器学习和数据科学中，我们经常谈论具有数千维度的空间。你可能无法在心中看到这些空间，但这并不是一个致命的限制。你可以在心中使用三维类比。它们在许多情况下都有效。然而，重要的是要记住，这并不总是正确的。稍后将会展示一些在更高维度中低维直觉失效的例子。
- en: 2.2 PyTorch code for vector manipulations
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 使用PyTorch进行向量操作代码
- en: PyTorch is an open source machine learning library developed by Facebook’s artificial
    intelligence group. It is one of the most elegant practical tools for developing
    deep learning applications at present. In this book, we aim to familiarize you
    with PyTorch and similar programming paradigms alongside the relevant mathematics.
    Knowledge of Python basics will be assumed. You are strongly encouraged to try
    out all the code snippets in this book (after installing the appropriate packages
    like PyTorch, that is).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch是由Facebook的人工智能团队开发的开源机器学习库。它是目前开发深度学习应用中最优雅的实用工具之一。在本书中，我们旨在使你熟悉PyTorch以及类似的编程范式，同时介绍相关的数学知识。假设你具备Python基础知识。强烈建议你尝试本书中的所有代码片段（在安装了适当的包，如PyTorch之后）。
- en: All the Python code in this book is produced via Jupyter Notebook. A summary
    of the theoretical material presented in the code is provided before the code
    snippet.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的所有Python代码都是通过Jupyter Notebook生成的。在代码片段之前提供了所展示的理论材料的摘要。
- en: 2.2.1 PyTorch code for the introduction to vectors
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1 使用PyTorch的向量介绍代码
- en: Listing 2.1 shows how to create and access vectors and subvectors and slice
    and dice vectors using PyTorch.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.1展示了如何使用PyTorch创建和访问向量和子向量，以及如何切片和切块向量。
- en: NOTE Fully functional code demonstrating how to create a vector and access its
    elements, executable via Jupyter Notebook, can be found at [http://mng.bz/xm8q](http://mng.bz/xm8q).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：一个完全功能的代码示例，展示如何创建向量及其元素访问，可通过Jupyter Notebook执行，可以在[http://mng.bz/xm8q](http://mng.bz/xm8q)找到。
- en: Listing 2.1 Introduction to vectors via PyTorch
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.1 通过PyTorch介绍向量
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① torch.tensor represents a multidimensional array. The vector is a 1D tensor
    that can be initialized by directly specifying values.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ① `torch.tensor` 表示一个多维数组。向量是一个一维张量，可以通过直接指定值来初始化。
- en: ② Tensor elements are floats by default. We can force tensors to be other types
    such as float64 (double).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ② 默认情况下，张量元素是浮点数。我们可以强制张量成为其他类型，如float64（双精度）。
- en: ③ The square bracket operator lets us access individual vector elements.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 方括号运算符让我们能够访问单个向量元素。
- en: ④ Negative indices count from the end of the array. –1 denotes the last element.
    -2 denotes the second-to-last element.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 负索引从数组的末尾开始计数。-1表示最后一个元素。-2表示倒数第二个元素。
- en: ⑤ The colon operator slices off a range of elements from the vector.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 冒号运算符从向量中切片掉一系列元素。
- en: ⑥ Nothing before a colon denotes the beginning of~the~array. Nothing after a
    colon denotes the end~of~the array.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 冒号之前的内容表示数组的开始。冒号之后的内容表示数组的结束。
- en: ⑦ Torch tensors can be initialized from NumPy arrays.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 火炬张量可以从NumPy数组初始化。
- en: ⑧ The difference between the Torch tensor and its NumPy version is zero.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 火炬张量与其NumPy版本之间的差异为零。
- en: ⑨ Torch tensors can be converted to NumPy arrays.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 火炬张量可以转换为NumPy数组。
- en: 2.3 Matrices and their role in machine learning
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 矩阵及其在机器学习中的作用
- en: Sometimes it is not sufficient to group a set of numbers into a vector. We have
    to collect several vectors into another group. For instance, consider the input
    to training a machine learning model. Here we have several input instances, each
    consisting of a sequence of numbers. As seen in section [2.1](02.xhtml#sec-vectors),
    the sequence of numbers belonging to a single input instance can be grouped into
    a vector. How do we represent the entire collection of input instances? This is
    where the concept of matrices comes in handy from the world of mathematics. A
    *matrix* can be viewed as a rectangular array of numbers arranged in a fixed count
    of rows and columns. Each row of a matrix is a vector, and so is each column.
    Thus a matrix can be thought of as a collection of row vectors. It can also be
    viewed as a collection of column vectors. We can represent the entire set of numbers
    that constitute the training input to a machine learning model as a matrix, with
    each row vector corresponding to a single training instance.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，将一组数字分组成一个向量是不够的。我们必须将几个向量收集到另一个组中。例如，考虑训练机器学习模型的输入。在这里，我们有几个输入实例，每个实例由一系列数字组成。如第
    [2.1](02.xhtml#sec-vectors) 节所述，单个输入实例的数字序列可以分组成一个向量。我们如何表示整个输入实例集合？这就是数学世界中矩阵概念派上用场的地方。一个
    *矩阵* 可以看作是按固定行数和列数排列的数字的矩形数组。矩阵的每一行是一个向量，每一列也是一个向量。因此，矩阵可以被视为行向量的集合。它也可以被视为列向量的集合。我们可以将构成机器学习模型训练输入的整个数字集合表示为一个矩阵，其中每一行向量对应一个单独的训练实例。
- en: Consider our familiar cat-brain problem again. As stated earlier, a single input
    instance to the machine is a vector ![](../../OEBPS/Images/eq_02-00-f2.png), where
    *x*[0] describes the hardness of the object in front of the cat. Now consider
    a training dataset with many such input instances, each with a known output threat
    score. You might recall from section [1.1](../Text/01.xhtml#sec-paradigm-shift)
    that the goal in machine learning is to create a function that maps these inputs
    to their respective outputs with as little overall error as possible. Our training
    data may look as shown in table [2.2](02.xhtml#tab-cat-brain-training-data) (note
    that in real-life problems, the training dataset is usually large—often millions
    of input-output pairs—but in this toy problem, we will have 8 training data instances).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 再次考虑我们熟悉的猫脑问题。如前所述，机器的单个输入实例是一个向量 ![图片](../../OEBPS/Images/eq_02-00-f2.png)，其中
    *x*[0] 描述了猫面前物体的硬度。现在考虑一个包含许多此类输入实例的训练数据集，每个实例都有一个已知的输出威胁分数。你可能还记得第 [1.1](../Text/01.xhtml#sec-paradigm-shift)
    节中提到的，机器学习的目标是创建一个函数，以尽可能少的总体误差将这些输入映射到相应的输出。我们的训练数据可能看起来像表 [2.2](02.xhtml#tab-cat-brain-training-data)
    所示（注意，在实际问题中，训练数据集通常很大——经常是数百万个输入输出对，但在这个玩具问题中，我们将有 8 个训练数据实例）。
- en: Table 2.2 Example training dataset for our toy machine learning–based cat brain
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.2 我们玩具机器学习猫脑的示例训练数据集
- en: '|  | Input value: Hardness | Input value: Sharpness | Output: Threat score
    |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | 输入值：硬度 | 输入值：锋利度 | 输出：威胁分数 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 0 | 0.11 | 0.09 | −0.8 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.11 | 0.09 | −0.8 |'
- en: '| 1 | 0.01 | 0.02 | −0.97 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.01 | 0.02 | −0.97 |'
- en: '| 2 | 0.98 | 0.91 | 0.89 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.98 | 0.91 | 0.89 |'
- en: '| 3 | 0.12 | 0.21 | −0.68 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.12 | 0.21 | −0.68 |'
- en: '| 4 | 0.98 | 0.99 | 0.95 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.98 | 0.99 | 0.95 |'
- en: '| 5 | 0.85 | 0.87 | 0.74 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.85 | 0.87 | 0.74 |'
- en: '| 6 | 0.03 | 0.14 | −0.88 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 0.03 | 0.14 | −0.88 |'
- en: '| 7 | 0.55 | 0.45 | 0.00 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 0.55 | 0.45 | 0.00 |'
- en: From table [2.2](02.xhtml#tab-cat-brain-training-data), we can collect the columns
    corresponding to hardness and sharpness into a matrix, as shown in equation [2.1](02.xhtml#eq-cat-brain-toy-training-dataset)—this
    is a compact representation of the training dataset for this problem. [²](02.xhtml#fn5)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 从表 [2.2](02.xhtml#tab-cat-brain-training-data) 中，我们可以将对应硬度和锋利的列收集到一个矩阵中，如图 [2.1](02.xhtml#eq-cat-brain-toy-training-dataset)
    所示——这是该问题的训练数据集的紧凑表示。[²](02.xhtml#fn5)
- en: '![](../../OEBPS/Images/eq_02-01.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_02-01.png)'
- en: Equation 2.1
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2.1
- en: Each row of matrix *X* is a particular input instance. Different rows represent
    different input instances. On the other hand, different columns represent different
    feature elements. For example, the 0th row of matrix *X* is the vector [*x*[00]    *x*[01]]
    representing the 0th input instance. Its elements, *x*[00] and *x*[01] represent
    different feature elements, hardness and sharpness respectively of the 0th training
    input instance.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 *X* 的每一行是特定的输入实例。不同的行代表不同的输入实例。另一方面，不同的列代表不同的特征元素。例如，矩阵 *X* 的第0行是向量 [*x*[00]    *x*[01]]，代表第0个输入实例。其元素
    *x*[00] 和 *x*[01] 代表不同的特征元素，分别是第0个训练输入实例的硬度和锐度。
- en: 2.3.1 Matrix representation of digital images
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 数字图像的矩阵表示
- en: 'Digital images are also often represented as matrices. Here, each element represents
    the brightness at a specific pixel position (*x*, *y* coordinate) of the image.
    Typically, the brightness value is normalized to an integer in the range 0 to
    255. 0 is black, 255 is white, and 128 is gray.[³](02.xhtml#fn6) Following is
    an example of a tiny image, 9 pixels wide and 4 pixels high:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 数字图像也经常表示为矩阵。在这里，每个元素代表图像在特定像素位置（*x*，*y*坐标）的亮度。通常，亮度值被归一化到0到255的整数范围内。0是黑色，255是白色，128是灰色。[³](02.xhtml#fn6)以下是一个小图像的例子，宽度为9像素，高度为4像素：
- en: '![](../../OEBPS/Images/eq_02-02.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-02.png)'
- en: Equation 2.2
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 公式2.2
- en: The brightness increases gradually from left to right and also from top to bottom.
    *I*[00] represents the top-left pixel, which is black. *I*[3, 8] represents the
    bottom-right pixel, which is white. The intermediate pixels are various shades
    of gray between black and white. The actual image is shown in figure [2.2](02.xhtml#fig-tiny_im).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 亮度从左到右和从上到下逐渐增加。*I*[00] 代表左上角的像素，是黑色。*I*[3, 8] 代表右下角的像素，是白色。中间的像素是介于黑白之间的各种灰度。实际图像显示在图[2.2](02.xhtml#fig-tiny_im)中。
- en: '![](../../OEBPS/Images/CH02_F03_Chaudhury.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH02_F03_Chaudhury.png)'
- en: Figure 2.3 Image corresponding to matrix *I*[4, 9] in equation [2.2](02.xhtml#eq-tiny_im)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 公式[2.2](02.xhtml#eq-tiny_im)中矩阵 *I*[4, 9] 对应的图像
- en: '2.4 Python code: Introducing matrices, tensors, and images via PyTorch'
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 使用PyTorch介绍矩阵、张量和图像的Python代码
- en: For programming purposes, you can think of tensors as multidimensional arrays.
    Scalars are zero-dimensional tensors. Vectors are one-dimensional tensors. Matrices
    are two-dimensional tensors. RGB images are three-dimensional tensors (*colorchannels*
    × *height* × *width*). A batch of 64 images is a four-dimensional tensor (64 ×
    *colorchannels* × *height* × *width*).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了编程目的，你可以将张量视为多维数组。标量是零维张量。向量是一维张量。矩阵是二维张量。RGB图像是三维张量（*颜色通道* × *高度* × *宽度*）。64张图像的一批是四维张量（64
    × *颜色通道* × *高度* × *宽度*）。
- en: Listing 2.2 Introducing matrices via PyTorch
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.2 使用PyTorch介绍矩阵
- en: '[PRE1]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '① A matrix is a 2D array of numbers: i.e., a 2D tensor. The entire training
    data input set for a machine-learning model can be viewed as a matrix. Each input
    instance is one row. Row count ≡ number of training examples, column count ≡ training
    instance size'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ① 矩阵是一个数字的二维数组：即，一个二维张量。一个机器学习模型的整个训练数据输入集可以被视为一个矩阵。每个输入实例是一行。行数 ≡ 训练示例数，列数
    ≡ 训练实例大小
- en: '② Cat-brain training data input: 8 examples, each with two values (hardness,
    sharpness). An 8 × 2 tensor is created by specifying values.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ② 猫脑训练数据输入：8个示例，每个示例有两个值（硬度，锐度）。通过指定值创建了一个8 × 2的张量。
- en: ③ The shape of a tensor is a list. For a matrix, the first list element is num
    rows; the second list element is num columns.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 张量的形状是一个列表。对于一个矩阵，第一个列表元素是行数；第二个列表元素是列数。
- en: ④ Square brackets extract individual matrix elements.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 方括号提取单个矩阵元素。
- en: ⑤ A standalone colon operator denotes all possible indices.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 独立的冒号运算符表示所有可能的索引。
- en: ⑥ The colon operator denotes the range of indices.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 冒号运算符表示索引的范围。
- en: ⑦ 0th column
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 第0列
- en: ⑧ 1st column
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 第1列
- en: Listing 2.3 Slicing and dicing matrices
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.3 矩阵的切片和切块
- en: '[PRE2]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ① Ranges of rows and columns can be specified via the colon operator to slice
    off (extract) submatrices.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ① 可以通过冒号运算符指定行和列的范围来切片（提取）子矩阵。
- en: ② Extracts the first three training examples (rows)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ② 提取前三个训练示例（行）
- en: ③ Extracts the sharpness feature for the 5th to 7th training examples
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 提取第5到第7个训练示例的锐度特征
- en: Listing 2.4 Tensors and images in PyTorch
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.4 PyTorch中的张量和图像
- en: '[PRE3]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① PyTorch tensors can be used to represent tensors. A vector is a 1-tensor,
    a matrix is a 2-tensor, and a scalar is a 0-tensor.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ① PyTorch 张量可以用来表示张量。一个向量是一个 1-张量，一个矩阵是一个 2-张量，一个标量是一个 0-张量。
- en: ② Creates a random tensor of specified dimensions
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ② 创建一个指定维度的随机张量
- en: ③ All images are tensors. An RGB image of height H, width W is a 3-tensor of
    shape [3, H, W].
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 所有图像都是张量。高度为 H、宽度为 W 的 RGB 图像是一个形状为 [3, H, W] 的 3-张量。
- en: ④ 4 × 9 single-channel image shown in figure [2.3](02.xhtml#fig-tiny_im)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 图 [2.3](02.xhtml#fig-tiny_im) 中显示的 4 × 9 单通道图像
- en: ⑤ Reads a 199 × 256 × 3 image from disk
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 从磁盘读取 199 × 256 × 3 的图像
- en: ⑥ Usual slicing dicing operators work. Extracts the red, green, and blue channels
    of the image as shown in figure [2.4](02.xhtml#fig-numpy-dog-grid).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 常规的切片切割操作有效。提取图像中如图 [2.4](02.xhtml#fig-numpy-dog-grid) 所示的红、绿、蓝通道。
- en: ⑦ Crops out a 100 × 100 subimage as shown in figure [2.5](02.xhtml#fig-numpy-crop-dog)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 裁剪出如图 [2.5](02.xhtml#fig-numpy-crop-dog) 所示的 100 × 100 子图像。
- en: '![](../../OEBPS/Images/CH02_F04_Chaudhury.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH02_F04_Chaudhury.png)'
- en: Figure 2.4 Tensors and images in PyTorch
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4 PyTorch 中的张量和图像
- en: '![](../../OEBPS/Images/CH02_F05_Chaudhury.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH02_F05_Chaudhury.png)'
- en: Figure 2.5 Cropped image of dog
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5 狗的裁剪图像
- en: 2.5 Basic vector and matrix operations in machine learning
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 机器学习中的基本向量和矩阵运算
- en: In this section, we introduce several basic vector and matrix operations along
    with examples to demonstrate their significance in image processing, computer
    vision, and machine learning. It is meant to be an application-centric introduction
    to linear algebra. But it is *not* meant to be a comprehensive review of matrix
    and vector operations, for which you are referred to a textbook on linear algebra.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了几个基本的向量和矩阵运算及其在图像处理、计算机视觉和机器学习中的重要性，并通过示例进行演示。本节旨在提供一个以应用为中心的线性代数入门。但并非旨在对矩阵和向量运算进行全面回顾，对于这些内容，请参考线性代数教科书。
- en: '![](../../OEBPS/Images/CH02_F06_Chaudhury.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH02_F06_Chaudhury.png)'
- en: Figure 2.6 Image corresponding to the transpose of matrix *I*[4, 9] shown in
    equation [2.3](02.xhtml#eq-tiny_im_transpose). This is equivalent to rotating
    the image by 90°.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6 方程 [2.3](02.xhtml#eq-tiny_im_transpose) 中显示的矩阵 *I*[4, 9] 的转置对应的图像。这相当于将图像旋转
    90°。
- en: 2.5.1 Matrix and vector transpose
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.1 矩阵和向量转置
- en: 'In equation [2.2](02.xhtml#eq-tiny_im), we encountered the matrix *I*[4, 9]
    depicting a tiny image. Suppose we want to rotate the image by 90° so it looks
    like figure [2.5](02.xhtml#fig-tiny_im-transpose). The original matrix *I*[4,
    9] and its transpose *I*[4,]*^T*[9] = *I*[9, 4] are shown here:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程 [2.2](02.xhtml#eq-tiny_im) 中，我们遇到了表示微小图像的矩阵 *I*[4, 9]。假设我们想要将图像旋转 90°，使其看起来像图
    [2.5](02.xhtml#fig-tiny_im-transpose)。原始矩阵 *I*[4, 9] 和其转置 *I*[4,]*^T*[9] = *I*[9,
    4] 如下所示：
- en: '![](../../OEBPS/Images/eq_02-03.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_02-03.png)'
- en: Equation 2.3
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2.3
- en: By comparing equation [2.2](02.xhtml#eq-tiny_im) and equation [2.3](02.xhtml#eq-tiny_im_transpose),
    you can easily see that one can be obtained from the other by interchanging the
    row and column indices. This operation is generally known as *matrix transposition*.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较方程 [2.2](02.xhtml#eq-tiny_im) 和方程 [2.3](02.xhtml#eq-tiny_im_transpose)，你可以很容易地看出，一个可以通过交换行和列索引从另一个获得。这种操作通常被称为
    *矩阵转置*。
- en: Formally, the transpose of a matrix *A[m, n]* with *m* rows and *n* columns
    is another matrix with *n* rows and *m* columns. This transposed matrix, denoted
    *A[n,]^T[m]*, is such that *A^T*[*i*, *j*] = *A*[*j*, *i*]. For instance, the
    value at row 0 column 6 in matrix *I*[4, 9] is 48; in the transposed matrix, the
    same value appears in row 6 and column 0. In matrix parlance, *I*[4, 9][0,6] =
    *I*[9,]*^T*[4][6,0] = 48.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，一个具有 *m* 行和 *n* 列的矩阵 *A[m, n]* 的转置是一个具有 *n* 行和 *m* 列的另一个矩阵。这个转置矩阵，表示为 *A[n,]^T[m]*，满足
    *A^T*[*i*, *j*] = *A*[*j*, *i*]。例如，矩阵 *I*[4, 9] 中第 0 行第 6 列的值是 48；在转置矩阵中，相同的值出现在第
    6 行和第 0 列。在矩阵术语中，*I*[4, 9][0,6] = *I*[9,]*^T*[4][6,0] = 48。
- en: 'Vector transposition is a special case of matrix transposition (since all vectors
    are matrices—a column vector with *n* elements is an *n* × 1 matrix). For instance,
    an arbitrary vector and its transpose are shown next:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 向量转置是矩阵转置的特殊情况（因为所有向量都是矩阵——一个具有 *n* 个元素的列向量是一个 *n* × 1 的矩阵）。例如，一个任意向量及其转置如下所示：
- en: '![](../../OEBPS/Images/eq_02-04.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_02-04.png)'
- en: Equation 2.4
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2.4
- en: '![](../../OEBPS/Images/eq_02-05.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_02-05.png)'
- en: Equation 2.5
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2.5
- en: 2.5.2 Dot product of two vectors and its role in machine learning
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.2 两个向量的点积及其在机器学习中的作用
- en: In section [1.3](../Text/01.xhtml#sec-cat_brain), we saw the simplest of machine
    learning models where the output is generated by taking a weighted sum of the
    inputs (and then adding a constant bias value). This model/machine is characterized
    by the weights *w*[0], *w*[1], and bias *b*. Take the rows of table [2.2](02.xhtml#tab-cat-brain-training-data).
    For example, for row 0, the input values are the hardness of the approaching object
    = 0.11 and softness = 0.09. The corresponding model output will be *y* = *w*[0]
    × 0.11 + *w*[1] × 0.09 + *b*. In fact, the goal of training is to choose *w*[0],
    *w*[1], and *b* such that model outputs are as close as possible to the known
    outputs; that is, *y* = *w*[0] × 0.11 + *w*[1] × 0.09 + *b* should be as close
    to −0.8 as possible, *y* = *w*[0] × 0.01 + *w*[1] × 0.02 + *b* should be as close
    to −0.97 as possible, that is,  in general, given an input instance ![](../../OEBPS/Images/eq_02-05-a.png),
    the model output is *y* = *x*[0]*w*[0] + *x*[1]*w*[1] + *b*.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在[1.3](../Text/01.xhtml#sec-cat_brain)节中，我们看到了最简单的机器学习模型，其中输出是通过取输入的加权总和（然后添加一个常数偏置值）来生成的。这个模型/机器的特征是权重
    *w*[0]、*w*[1] 和偏置 *b*。以表[2.2](02.xhtml#tab-cat-brain-training-data)的行为例。例如，对于第0行，输入值是接近物体的硬度=0.11和柔软度=0.09。相应的模型输出将是
    *y* = *w*[0] × 0.11 + *w*[1] × 0.09 + *b*。实际上，训练的目标是选择 *w*[0]、*w*[1] 和 *b*，使得模型输出尽可能接近已知的输出；也就是说，*y*
    = *w*[0] × 0.11 + *w*[1] × 0.09 + *b* 应尽可能接近 -0.8，*y* = *w*[0] × 0.01 + *w*[1]
    × 0.02 + *b* 应尽可能接近 -0.97，也就是说，一般来说，给定一个输入实例 ![](../../OEBPS/Images/eq_02-05-a.png)，模型输出是
    *y* = *x*[0]*w*[0] + *x*[1]*w*[1] + *b*。
- en: 'We will keep returning to this model throughout the chapter. But first, let’s
    consider a different question. In this toy example, we have only three model parameters:
    two weights, *w*[0], *w*[1], and one bias *b*. Hence it is not very messy to write
    the model output flat out as *y* = *x*[0]*w*[0] + *x*[1]*w*[1] + *b*. But, with
    longer feature vectors (that is, more weights) it will become unwieldy. Is there
    a compact way to represent the model output for a specific input instance, irrespective
    of the size of the input?'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中反复回到这个模型。但首先，让我们考虑一个不同的问题。在这个玩具示例中，我们只有三个模型参数：两个权重，*w*[0] 和 *w*[1]，以及一个偏置
    *b*。因此，将模型输出直接写成 *y* = *x*[0]*w*[0] + *x*[1]*w*[1] + *b* 并不是很混乱。但是，对于更长的特征向量（即更多的权重），它将变得难以处理。对于特定的输入实例，有没有一种紧凑的方式来表示模型输出，而不论输入的大小如何？
- en: Turns out the answer is yes—we can use an operation called *dot product* from
    the world of mathematics. We have already seen in section [2.1](02.xhtml#sec-vectors)
    that an individual instance of model input can be compactly represented by a vector,
    say ![](../../OEBPS/Images/AR_x.png) (it can have any number of input values).
    We can also represent the set of weights as vector ![](../../OEBPS/Images/AR_w.png)—it
    will have the same number of items as the input vector. The dot product is simply
    the element-wise multiplication of the two vectors ![](../../OEBPS/Images/AR_x.png)
    and ![](../../OEBPS/Images/AR_w.png). Formally, given two vectors and ![](../../OEBPS/Images/eq_02-05-b2.png)
    and ![](../../OEBPS/Images/eq_02-05-c2.png), the dot product of the two vectors
    is defined as
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是肯定的——我们可以使用来自数学世界的称为“点积”的操作。我们已经在[2.1](02.xhtml#sec-vectors)节中看到，模型输入的单个实例可以简洁地表示为一个向量，例如
    ![](../../OEBPS/Images/AR_x.png)（它可以有任意数量的输入值）。我们也可以将权重集合表示为向量 ![](../../OEBPS/Images/AR_w.png)——它将具有与输入向量相同数量的项。点积是两个向量
    ![](../../OEBPS/Images/AR_x.png) 和 ![](../../OEBPS/Images/AR_w.png) 的逐元素乘积。形式上，给定两个向量
    ![](../../OEBPS/Images/eq_02-05-b2.png) 和 ![](../../OEBPS/Images/eq_02-05-c2.png)，两个向量的点积定义为
- en: '![](../../OEBPS/Images/eq_02-06.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-06.png)'
- en: Equation 2.6
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式2.6
- en: In other words, the sum of the products of corresponding elements of the two
    vectors is the dot product of the two vectors, denoted ![](../../OEBPS/Images/AR_a.png)
    ⋅ ![](../../OEBPS/Images/AR_b.png).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，两个向量对应元素的乘积之和是两个向量的点积，表示为 ![](../../OEBPS/Images/AR_a.png) ⋅ ![](../../OEBPS/Images/AR_b.png)。
- en: NOTE The dot product notation can compactly represent the model output as *y*
    = ![](../../OEBPS/Images/AR_w.png) ⋅ ![](../../OEBPS/Images/AR_x.png) + *b*. The
    representation does not increase in size even when the number of inputs and weights
    is large.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：点积符号可以简洁地表示模型输出为 *y* = ![](../../OEBPS/Images/AR_w.png) ⋅ ![](../../OEBPS/Images/AR_x.png)
    + *b*。即使输入和权重的数量很大，这种表示法的大小也不会增加。
- en: Consider our (by now familiar) cat-brain example again. Suppose the weight vector
    is ![](../../OEBPS/Images/eq_02-06-a2.png) and the bias value *b* = 5. Then the
    model output for the 0th input instance from table [2.2](02.xhtml#tab-cat-brain-training-data)
    will be ![](../../OEBPS/Images/eq_02-06-b2.png). It is another matter that these
    are bad choices for weight and bias parameters, since the model output 5.51 is
    a far cry from the desired output −0.89. We will soon see how to obtain better
    parameter values. For now, we just need to note that the dot product offers a
    neat way to represent the simple weighted sum model output.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 再次考虑我们（现在已经很熟悉的）猫脑示例。假设权重向量是 ![](../../OEBPS/Images/eq_02-06-a2.png) 且偏置值 *b*
    = 5。那么从表 [2.2](02.xhtml#tab-cat-brain-training-data) 中 0 号输入实例的模型输出将是 ![](../../OEBPS/Images/eq_02-06-b2.png)。这些权重和偏置参数的选择并不理想，因为模型输出
    5.51 与期望的输出 -0.89 相去甚远。我们很快就会看到如何获得更好的参数值。现在，我们只需注意点积提供了一种简洁的方式来表示简单的加权求和模型输出。
- en: NOTE The dot product is defined only if the vectors have the same dimensions.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：点积仅在向量具有相同维度时才定义。
- en: Sometimes the dot product is also referred to as *inner product*, denoted ⟨![](../../OEBPS/Images/AR_a.png),
    ![](../../OEBPS/Images/AR_b.png)⟩. Strictly speaking, the phrase *inner product*
    is a bit more general; it applies to infinite-dimensional vectors as well. In
    this book, we will often use the terms interchangeably, sacrificing mathematical
    rigor for enhanced understanding.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 有时点积也被称为 *内积*，表示为 ⟨![](../../OEBPS/Images/AR_a.png), ![](../../OEBPS/Images/AR_b.png)⟩。严格来说，短语“内积”更为通用；它也适用于无限维向量。在这本书中，我们将经常互换使用这些术语，牺牲数学严谨性以增强理解。
- en: 2.5.3 Matrix multiplication and machine learning
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.3 矩阵乘法和机器学习
- en: Vectors are special cases of matrices. Hence, matrix-vector multiplication is
    a special case of matrix-matrix multiplication. We will start with that.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 向量是矩阵的特殊情况。因此，矩阵-向量乘法是矩阵-矩阵乘法的特殊情况。我们将从这里开始。
- en: Matrix-vector multiplication
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵-向量乘法
- en: In section [2.5.2](02.xhtml#subsec-dotprod-ml), we saw that given a weight vector,
    say ![](../../OEBPS/Images/eq_02-06-c2.png), and the bias value *b* = 5, the weighted
    sum model output upon a single input instance, say ![](../../OEBPS/Images/eq_02-06-d2.png),
    can be represented using a vector-vector dot product ![](../../OEBPS/Images/eq_02-06-e2.png).
    As depicted in equation [2.1](02.xhtml#eq-cat-brain-toy-training-dataset), during
    training, we are dealing with many training data instances at the same time. In
    real life, we typically deal with hundreds of thousands of input instances, each
    having hundreds of values. Is there a way to represent the model output for the
    entire training dataset compactly, such that it is independent of the count of
    input instances and their sizes?
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [2.5.2](02.xhtml#subsec-dotprod-ml) 节中，我们看到了给定一个权重向量，比如 ![](../../OEBPS/Images/eq_02-06-c2.png)，以及偏置值
    *b* = 5，单个输入实例上的加权求和模型输出可以通过向量-向量点积 ![](../../OEBPS/Images/eq_02-06-e2.png) 表示。如方程
    [2.1](02.xhtml#eq-cat-brain-toy-training-dataset) 所示，在训练过程中，我们同时处理许多训练数据实例。在现实生活中，我们通常处理成千上万的输入实例，每个实例有数百个值。有没有一种方法可以紧凑地表示整个训练数据集的模型输出，使其独立于输入实例的数量和大小？
- en: 'The answer turns out to be yes. We can use the idea of matrix-vector multiplication
    from the world of mathematics. The product of a matrix *X* and column vector ![](../../OEBPS/Images/AR_w.png)
    is another vector, denoted *X*![](../../OEBPS/Images/AR_w.png). Its elements are
    the dot products between the row vectors of *X* and the column vector ![](../../OEBPS/Images/AR_w.png).
    For example, given the model weight vector ![](../../OEBPS/Images/eq_02-06-f2.png)
    and the bias value *b* = 5, the outputs on the toy training dataset of our familiar
    cat-brain model (equation [2.1](02.xhtml#eq-cat-brain-toy-training-dataset)) can
    be obtained via the following steps:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是肯定的。我们可以从数学世界的矩阵-向量乘法中借用这个想法。矩阵 *X* 和列向量 ![](../../OEBPS/Images/AR_w.png)
    的乘积是另一个向量，表示为 *X*![](../../OEBPS/Images/AR_w.png)。其元素是 *X* 的行向量与列向量 ![](../../OEBPS/Images/AR_w.png)
    之间的点积。例如，给定模型权重向量 ![](../../OEBPS/Images/eq_02-06-f2.png) 和偏置值 *b* = 5，我们熟悉的猫脑模型（方程
    [2.1](02.xhtml#eq-cat-brain-toy-training-dataset)）在玩具训练数据集上的输出可以通过以下步骤获得：
- en: '![](../../OEBPS/Images/eq_02-07.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_02-07.png)'
- en: Equation 2.7
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2.7
- en: Adding the bias value of 5, the model output on the toy training dataset is
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 添加偏置值 5，玩具训练数据集上的模型输出为
- en: '![](../../OEBPS/Images/eq_02-08.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_02-08.png)'
- en: Equation 2.8
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2.8
- en: In general, the output of our simple model (biased weighted sum of input elements)
    can be expressed compactly as ![](../../OEBPS/Images/AR_y.png) = *X*![](../../OEBPS/Images/AR_w.png)
    + ![](../../OEBPS/Images/AR_b.png).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们简单模型的输出（输入元素的偏置加权求和）可以紧凑地表示为![](../../OEBPS/Images/AR_y.png) = *X*![](../../OEBPS/Images/AR_w.png)
    + ![](../../OEBPS/Images/AR_b.png)。
- en: Matrix-matrix multiplication
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵-矩阵乘法
- en: 'Generalizing the notion of matrix times vector, we can define matrix times
    matrix. A matrix with *m* rows and *p* columns, say *A[m, p]*, can be multiplied
    with another matrix with *p* rows and *n* columns, say *B[p, n]*, to generate
    a matrix with *m* rows and *n* columns, say *C[m, n]*: for example, *C[m, n]*
    = *A[m, p]* *B[p, n]*. Note that the number of columns in the left matrix must
    match the number of rows in the right matrix. Element *i, j* of the result matrix,
    *C[i, j]*, is obtained by point-wise multiplication of the elements of the *i*th
    row vector of *A* and the *j*th column vector of *B*. The following example illustrates
    the idea:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 将矩阵乘以向量的概念推广后，我们可以定义矩阵乘以矩阵。一个有*m*行和*p*列的矩阵，例如*A[m, p]*，可以与另一个有*p*行和*n*列的矩阵相乘，例如*B[p,
    n]*，生成一个有*m*行和*n*列的矩阵，例如*C[m, n]*：例如，*C[m, n]* = *A[m, p]* *B[p, n]*。注意，左矩阵的列数必须与右矩阵的行数相匹配。结果矩阵的元素*i,
    j*，即*C[i, j]*，是通过*A*的第*i*行向量的元素与*B*的第*j*列向量的元素逐点相乘得到的。以下示例说明了这个概念：
- en: '![](../../OEBPS/Images/eq_02-08-a.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-08-a.png)'
- en: The computation for *C*[2, 1] is shown via bolding by way of example.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 通过加粗的方式举例说明了*C*[2, 1]的计算。
- en: NOTE Matrix multiplication is not commutative. In general, *AB* ≠ *BA*.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：矩阵乘法不是交换的。一般来说，*AB* ≠ *BA*。
- en: At this point, the astute reader may already have noted that the dot product
    is a special case of matrix multiplication. For instance, the dot product between
    two vectors ![](../../OEBPS/Images/eq_02-08-b2.png) and ![](../../OEBPS/Images/eq_02-08-c2.png)
    is equivalent to transposing either of the two vectors and then doing a matrix
    multiplication with the other. In other words,
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，敏锐的读者可能已经注意到了点积是矩阵乘法的一种特殊情况。例如，两个向量![](../../OEBPS/Images/eq_02-08-b2.png)和![](../../OEBPS/Images/eq_02-08-c2.png)的点积等价于将其中一个向量转置后，再与另一个向量进行矩阵乘法。换句话说，
- en: '![](../../OEBPS/Images/eq_02-08-d.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-08-d.png)'
- en: The idea works in higher dimensions, too. In general, given two vectors ![](../../OEBPS/Images/eq_02-08-e2.png)
    and ![](../../OEBPS/Images/eq_02-08-f2.png), the dot product of the two vectors
    is defined as
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法在更高维的情况下也适用。一般来说，给定两个向量![](../../OEBPS/Images/eq_02-08-e2.png)和![](../../OEBPS/Images/eq_02-08-f2.png)，这两个向量的点积定义为
- en: '![](../../OEBPS/Images/eq_02-09.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-09.png)'
- en: Equation 2.9
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 方程2.9
- en: Another special case of matrix multiplication is row-vector matrix multiplication.
    For example, ![](../../OEBPS/Images/AR_b.png)*^TA* = ![](../../OEBPS/Images/AR_c.png)
    or
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法的另一个特殊情况是行向量矩阵乘法。例如，![](../../OEBPS/Images/AR_b.png)*^TA* = ![](../../OEBPS/Images/AR_c.png)或者
- en: '![](../../OEBPS/Images/eq_02-09-a.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-09-a.png)'
- en: Transpose of matrix products
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘积的转置
- en: 'Given two matrices *A* and *B*, where the number of columns in *A* matches
    the number of rows in *B* (that is, it is possible to multiply them), the transpose
    of the product is the product of the individual transposes, in reversed order.
    The rule also applies to matrix-vector multiplication. The following equations
    capture this rule:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 给定两个矩阵*A*和*B*，其中*A*的列数与*B*的行数相匹配（也就是说，可以相乘），它们的乘积的转置是各自转置的乘积，顺序相反。这个规则也适用于矩阵-向量乘法。以下方程捕捉了这个规则：
- en: '![](../../OEBPS/Images/eq_02-10.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-10.png)'
- en: Equation 2.10
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 方程2.10
- en: '2.5.4 Length of a vector (L2 norm): Model error'
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.4 向量的长度（L2范数）：模型误差
- en: Imagine that a machine learning model is supposed to output a target value *ȳ*,
    but it outputs *y* instead. We are interested in the *error* made by the model.
    The error is the difference between the target and the actual outputs.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个机器学习模型应该输出一个目标值*ȳ*，但它输出*y*。我们感兴趣的是模型犯的*错误*。错误是目标值与实际输出之间的差异。
- en: Squared error
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 平方误差
- en: When a computing error occurs, we are only interested in how far the computed
    value is from ideal. We do not care whether the computed value is bigger or smaller
    than ideal. For instance, if the target (ideal) value is 2, the computed values
    1.5 and 2.5 are equally in error—we are equally happy or unhappy with either of
    them. Hence, it is common practice to *square* error values. Thus for instance,
    if the target value is 2 and the computed value is 1.5, the error is (1.5 − 2)²
    = 0.25. If the target value is 2.5, the error is (2.5 − 2)² = 0.25. The squaring
    operation essentially eliminates the sign of the error value. We can then follow
    it up with a square root, but it is OK not to.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 当发生计算错误时，我们只关心计算值与理想值之间的差距。我们并不关心计算值是大于还是小于理想值。例如，如果目标（理想）值是 2，那么计算值 1.5 和 2.5
    的误差是相同的——我们对这两个值都同样满意或不满意。因此，通常的做法是将误差值进行平方。例如，如果目标值是 2，计算值是 1.5，那么误差是 (1.5 −
    2)² = 0.25。如果目标值是 2.5，误差是 (2.5 − 2)² = 0.25。平方操作本质上消除了误差值的符号。然后我们可以跟一个平方根，但也可以不这样做。
- en: 'You might ask, “But wait: squaring alters the value of the quantity. Don’t
    we care about the exact value of the error?” The answer is, we usually don’t;
    we only care about *relative* values of errors. If the target is 2, we want the
    error for an output value of, say, 2.1 to be less than the error for an output
    value of 2.5; the exact values of the errors do not matter.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问，“但是等等：平方改变了数量的值。我们难道不在乎误差的精确值吗？” 答案是，我们通常不在乎；我们只关心误差的 *相对* 值。如果目标是 2，我们希望输出值为
    2.1 的误差小于输出值为 2.5 的误差；误差的精确值并不重要。
- en: Let’s apply this idea of squaring to machine learning model error. As seen earlier
    in section [2.5.3](02.xhtml#subsec-matmul-ml), given a model weight vector, say
    ![](../../OEBPS/Images/eq_02-10-a2.png), and the bias value *b* = 5, the weighted
    sum model output upon a single input instance, say ![](../../OEBPS/Images/eq_02-10-b2.png),
    is ![](../../OEBPS/Images/eq_02-10-c2.png). The corresponding target (ideal) output,
    from table [2.2](02.xhtml#tab-cat-brain-training-data), is −0.8. The squared error
    *e*² = (−0.8−5.51)² = 39.82 gives us an idea of how good or bad the model parameters
    3, 2, 5 are. For instance, if we instead use a weight vector ![](../../OEBPS/Images/eq_02-10-d2.png)
    and bias value −1, we get model output ![](../../OEBPS/Images/eq_02-10-e2.png).
    The output is exactly the same as the target. The corresponding squared error
    *e*² = (−0.8−(−0.8))² = 0. This (zero error) immediately tells us that 1, 1, −1
    are much better choices of model parameters than 3, 2, 5.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这种平方的思想应用到机器学习模型误差中。如前文第 [2.5.3](02.xhtml#subsec-matmul-ml) 节所述，给定一个模型权重向量，比如
    ![](../../OEBPS/Images/eq_02-10-a2.png)，以及偏差值 *b* = 5，单个输入实例的加权求和模型输出，比如 ![](../../OEBPS/Images/eq_02-10-b2.png)，是
    ![](../../OEBPS/Images/eq_02-10-c2.png)。相应的目标（理想）输出，来自表 [2.2](02.xhtml#tab-cat-brain-training-data)，是
    −0.8。平方误差 *e*² = (−0.8−5.51)² = 39.82 给出了模型参数 3, 2, 5 的好坏程度。例如，如果我们改用一个权重向量 ![](../../OEBPS/Images/eq_02-10-d2.png)
    和偏差值 −1，我们得到模型输出 ![](../../OEBPS/Images/eq_02-10-e2.png)。输出与目标完全相同。相应的平方误差 *e*²
    = (−0.8−(−0.8))² = 0。这个（零误差）立即告诉我们，1, 1, −1 作为模型参数的选择比 3, 2, 5 要好得多。
- en: In general, the error made by a biased weighted sum model can be expressed as
    follows. If ![](../../OEBPS/Images/AR_w.png) denotes the weight vector and ![](../../OEBPS/Images/AR_b.png)
    denotes the bias, the output corresponding to an input instance ![](../../OEBPS/Images/AR_x.png)
    can be expressed as *y* = ![](../../OEBPS/Images/AR_w.png) ⋅ ![](../../OEBPS/Images/AR_x.png)
    + *b*. Let *ȳ* denote the corresponding target (ground truth). Then the error
    is defined as *e* = (*y*−*ȳ*)².
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，一个有偏重的加权求和模型所犯的错误可以表示如下。如果用 ![](../../OEBPS/Images/AR_w.png) 表示权重向量，用
    ![](../../OEBPS/Images/AR_b.png) 表示偏差，则对应于输入实例 ![](../../OEBPS/Images/AR_x.png)
    的输出可以表示为 *y* = ![](../../OEBPS/Images/AR_w.png) ⋅ ![](../../OEBPS/Images/AR_x.png)
    + *b*。用 *ȳ* 表示相应的目标（真实值）。那么错误被定义为 *e* = (*y*−*ȳ*)²。
- en: 'Thus we see that we can compute the error on a single training instance by
    taking the difference between the model output and the ground truth and squaring
    it. How do we extend this concept over the entire training dataset? The set of
    outputs corresponding to the entire set of training inputs can be expressed as
    the output vector *y* = *X*![](../../OEBPS/Images/AR_w.png) + ![](../../OEBPS/Images/AR_b.png).
    The corresponding target output vector, consisting of the entire set of ground
    truths can be expressed as ![](../../OEBPS/Images/AR_y2.png). The differences
    between the target and model output over the entire training set can be expressed
    as another vector ![](../../OEBPS/Images/AR_y2.png) - ![](../../OEBPS/Images/AR_y.png).
    In our particular example:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们看到我们可以通过取模型输出和真实值之间的差异并对其进行平方来计算单个训练实例的误差。我们如何将这个概念扩展到整个训练数据集？对应于整个训练输入集的输出集可以表示为输出向量
    *y* = *X*![图片](../../OEBPS/Images/AR_w.png) + ![图片](../../OEBPS/Images/AR_b.png)。相应的目标输出向量，由整个真实值集组成，可以表示为![图片](../../OEBPS/Images/AR_y2.png)。整个训练集中目标和模型输出之间的差异可以表示为另一个向量![图片](../../OEBPS/Images/AR_y2.png)
    - ![图片](../../OEBPS/Images/AR_y.png)。在我们的特定例子中：
- en: '![](../../OEBPS/Images/eq_02-10-f.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_02-10-f.png)'
- en: 'Thus the total error over the entire training dataset is obtained by taking
    the difference between the output and the ground truth vector, squaring its elements
    and adding them up. Recalling equation [2.9](02.xhtml#eq-dotprod-matmul), this
    is exactly what will happen if we take the *dot product of the difference vector
    with itself*. That happens to be the definition of the *squared magnitude* or
    *length* or *L2 norm* of a vector: the dot product of the vector with itself.
    In the previous example, the overall training (squared) error is:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，整个训练数据集的总误差是通过取输出和真实向量之间的差异，平方其元素并将它们相加来获得的。回忆方程[2.9](02.xhtml#eq-dotprod-matmul)，这正是如果我们取差异向量的*点积*会发生的事情。这恰好是向量的*平方模*或*长度*或*L2范数*的定义：向量与自身的点积。在先前的例子中，整体训练（平方）误差是：
- en: '![](../../OEBPS/Images/eq_02-10-g.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_02-10-g.png)'
- en: Formally, the length of a vector ![](../../OEBPS/Images/eq_02-10-h2.png), denoted
    ||![](../../OEBPS/Images/AR_v.png)||, is defined as ![](../../OEBPS/Images/eq_02-10-i2.png).
    This quantity is sometimes called the L2 norm of the vector.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，向量![图片](../../OEBPS/Images/eq_02-10-h2.png)的长度，表示为||![图片](../../OEBPS/Images/AR_v.png)||，定义为![图片](../../OEBPS/Images/eq_02-10-i2.png)。这个量有时被称为向量的L2范数。
- en: In particular, given a machine learning model with output vector ![](../../OEBPS/Images/AR_y.png)
    and a target vector ![](../../OEBPS/Images/AR_y2.png), the error is the same as
    the magnitude or L2 norm of the difference vector
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，给定一个输出向量为![图片](../../OEBPS/Images/AR_y.png)的机器学习模型和一个目标向量为![图片](../../OEBPS/Images/AR_y2.png)的向量，错误等于差异向量的模或L2范数
- en: '![](../../OEBPS/Images/eq_02-10-j.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_02-10-j.png)'
- en: 2.5.5 Geometric intuitions for vector length
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.5 向量长度的几何直觉
- en: For a 2D vector ![](../../OEBPS/Images/eq_02-10-k2.png), as seen in figure [2.2](02.xhtml#fig-vector_diagram),
    the L2 norm ![](../../OEBPS/Images/eq_02-10-l2.png) is nothing but the hypotenuse
    of the right-angled triangle whose sides are elements of the vector. The same
    intuition holds in higher dimensions.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个二维向量![图片](../../OEBPS/Images/eq_02-10-k2.png)，如图[2.2](02.xhtml#fig-vector_diagram)所示，L2范数![图片](../../OEBPS/Images/eq_02-10-l2.png)不过是直角三角形的斜边，其边是向量的元素。在更高维的情况下，这种直觉同样适用。
- en: A *unit vector* is a vector whose length is 1\. Given any vector ![](../../OEBPS/Images/AR_v.png),
    the corresponding unit vector can be obtained by dividing every element by the
    length of that vector. For example, given ![](../../OEBPS/Images/eq_02-10-m2.png),
    length ![](../../OEBPS/Images/eq_02-10-n2.png) and the corresponding unit vector
    ![](../../OEBPS/Images/eq_02-10-o2.png). Unit vectors typically represent a direction.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*单位向量*是一个长度为1的向量。对于任何向量![图片](../../OEBPS/Images/AR_v.png)，相应的单位向量可以通过将该向量的每个元素除以该向量的长度来获得。例如，给定![图片](../../OEBPS/Images/eq_02-10-m2.png)，长度![图片](../../OEBPS/Images/eq_02-10-n2.png)和相应的单位向量![图片](../../OEBPS/Images/eq_02-10-o2.png)。单位向量通常表示一个方向。
- en: NOTE Unit vectors are conventionally depicted with the hat symbol as opposed
    to the little overhead arrow, as in *û^Tû* = 1.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：单位向量通常用帽子符号表示，而不是小箭头，如*û^Tû* = 1。
- en: In machine learning, the goal of training is often to minimize the length of
    the error vector (the difference between the model output vector and the target
    ground truth vector).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，训练的目标通常是使错误向量（模型输出向量与目标真实向量之间的差异）的长度最小化。
- en: '2.5.6 Geometric intuitions for the dot product: Feature similarity'
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.6 点积的几何直觉：特征相似度
- en: Consider the document retrieval problem depicted in table [2.1](02.xhtml#tab-doc-vec)
    one more time. We have a set of documents, each described by its own feature vector.
    Given a pair of such documents, we must find their similarity. This essentially
    boils down to estimating the similarity between two feature vectors. In this section,
    we will see that the dot product between a pair of vectors can be used as a measure
    of similarity between them.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 再次考虑表 [2.1](02.xhtml#tab-doc-vec) 中描述的文档检索问题。我们有一组文档，每个文档都由其自身的特征向量描述。给定这样一对文档，我们必须找到它们的相似度。这本质上归结为估计两个特征向量之间的相似度。在本节中，我们将看到一对向量的点积可以用作它们之间相似度的度量。
- en: For instance, consider the feature vectors corresponding to *d*[5] and *d*[6]
    in table [2.1](02.xhtml#tab-doc-vec). They are ![](../../OEBPS/Images/eq_02-10-p2.png)
    and ![](../../OEBPS/Images/eq_02-10-q2.png). The dot product between them is 1
    × 0 + 0 × 1 = 0. This is low and agrees with our intuition that there is no common
    word of interest between them, so the documents are very dissimilar. On the other
    hand, the dot product between feature vectors of *d*[3] and *d*[4] is ![](../../OEBPS/Images/eq_02-10-r2.png).
    This is high and agrees with our intuition that they have many commonalities in
    words of interest and are similar documents. Thus, we get the first glimpse of
    an important concept. Loosely speaking, *similar vectors have larger dot products,
    and dissimilar vectors have near-zero dot products.*
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑表 [2.1](02.xhtml#tab-doc-vec) 中对应于 *d*[5] 和 *d*[6] 的特征向量。它们是 ![](../../OEBPS/Images/eq_02-10-p2.png)
    和 ![](../../OEBPS/Images/eq_02-10-q2.png)。它们之间的点积是 1 × 0 + 0 × 1 = 0。这是很低的，与我们直觉认为它们之间没有共同的感兴趣词汇相符，因此文档非常不相似。另一方面，*d*[3]
    和 *d*[4] 的特征向量之间的点积是 ![](../../OEBPS/Images/eq_02-10-r2.png)。这是很高的，与我们直觉认为它们在感兴趣词汇上有许多共同之处，是相似文档相符。因此，我们得到了一个重要概念的初步认识。简单来说，*相似的向量有较大的点积，不相似的向量有接近零的点积*。
- en: We will keep revisiting this problem of estimating similarity between feature
    vectors and solve it with more and more finesse. As a first attempt, we will now
    study in greater detail how dot products measure similarities between vectors.
    First we will show that the component of a vector along another is yielded by
    the dot product. Using this, we will show that the “similarity/agreement” between
    a pair of vectors can be estimated using the dot product between them. In particular,
    we will see that if the vectors point in more or less the same direction, their
    dot products are higher than when the vectors are perpendicular to each other.
    If the vectors point in opposite directions, their dot product is negative.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将不断回顾这个估计特征向量之间相似度的问题，并使用越来越巧妙的方法来解决它。作为一个初步尝试，我们现在将更详细地研究点积如何衡量向量之间的相似度。首先，我们将证明一个向量在另一个向量上的分量是由点积产生的。利用这一点，我们将展示一对向量的“相似度/一致性”可以通过它们之间的点积来估计。特别是，我们将看到如果向量指向大致相同的方向，它们的点积将高于当向量相互垂直时的情况。如果向量指向相反的方向，它们的点积将是负数。
- en: Dot product measures the component of one vector along another
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 点积衡量一个向量在另一个向量上的分量
- en: 'Let’s examine a special case first: the component of a vector along a coordinate
    axis. This can be obtained by multiplying the length of the vector with the cosine
    of the angle between the vector and the relevant coordinate axis. As shown for
    2D in figure [2.7a](02.xhtml#ch2fig-vec_component), a vector ![](../../OEBPS/Images/AR_v.png)
    can be broken into two components along the *X* and *Y* axes as'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 首先考察一个特殊情况：向量在坐标轴上的分量。这可以通过将向量的长度与向量与相关坐标轴之间的角度的余弦值相乘来获得。如图 [2.7a](02.xhtml#ch2fig-vec_component)
    所示，一个向量 ![](../../OEBPS/Images/AR_v.png) 可以沿 *X* 和 *Y* 轴分解为两个分量，如下
- en: '![](../../OEBPS/Images/eq_02-10-s.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-10-s.png)'
- en: 'Note how the length of the vector is preserved:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 注意向量长度的保持：
- en: '![](../../OEBPS/Images/eq_02-10-t.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-10-t.png)'
- en: '![](../../OEBPS/Images/CH02_F07a_Chaudhury.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH02_F07a_Chaudhury.png)'
- en: (a) Components of a 2D vector along coordinate axes. Note that ||![](../../OEBPS/Images/AR_a.png)||
    is the length of hypotenuse.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 2D向量在坐标轴上的分量。注意 ||![](../../OEBPS/Images/AR_a.png)|| 是斜边的长度。
- en: '![](../../OEBPS/Images/CH02_F07b_Chaudhury.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH02_F07b_Chaudhury.png)'
- en: (b) Dot product as a component of one vector along another ![](../../OEBPS/Images/AR_a.png)
    ⋅ ![](../../OEBPS/Images/AR_b.png) = ![](../../OEBPS/Images/AR_a.png)*^T*![](../../OEBPS/Images/AR_b.png)
    = *a[x]b[x]* + *a[y]b[y]* = ||![](../../OEBPS/Images/AR_a.png)|| ||![](../../OEBPS/Images/AR_b.png)||*cos*(*θ*).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 点积作为另一个向量上的一个向量的分量 ![](../../OEBPS/Images/AR_a.png) ⋅ ![](../../OEBPS/Images/AR_b.png)
    = ![](../../OEBPS/Images/AR_a.png)*^T*![](../../OEBPS/Images/AR_b.png) = *a[x]b[x]*
    + *a[y]b[y]* = ||![](../../OEBPS/Images/AR_a.png)|| ||![](../../OEBPS/Images/AR_b.png)||*cos*(*θ*)。
- en: Figure 2.7 Vector components and dot product
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7 向量分量和点积
- en: Now let’s study the more general case of the component of one vector in the
    direction of another arbitrary vector (figure [2.7b](02.xhtml#ch2fig-vec_component)).
    The component of a vector ![](../../OEBPS/Images/AR_a.png) along another vector
    ![](../../OEBPS/Images/AR_b.png) is ![](../../OEBPS/Images/AR_a.png) ⋅ ![](../../OEBPS/Images/AR_b.png)
    = ![](../../OEBPS/Images/AR_a.png)*^T*![](../../OEBPS/Images/AR_b.png). This is
    equivalent to ||![](../../OEBPS/Images/AR_a.png)|| ||![](../../OEBPS/Images/AR_b.png)||*cos*(*θ*),
    where *θ* is the angle between the vectors ![](../../OEBPS/Images/AR_a.png) and
    ![](../../OEBPS/Images/AR_b.png). (This has been proven for the two-dimension
    case discussed in section [A.1](../Text/A.xhtml#sec-dotprod-cosine-proof) of the
    appendix. You can read it if you would like deeper intuition.)
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来研究一个向量在另一个任意向量方向上的分量的一般情况（见图[2.7b](02.xhtml#ch2fig-vec_component)）。一个向量
    ![](../../OEBPS/Images/AR_a.png) 在另一个向量 ![](../../OEBPS/Images/AR_b.png) 上的分量是
    ![](../../OEBPS/Images/AR_a.png) ⋅ ![](../../OEBPS/Images/AR_b.png) = ![](../../OEBPS/Images/AR_a.png)*^T*![](../../OEBPS/Images/AR_b.png)。这相当于
    ||![](../../OEBPS/Images/AR_a.png)|| ||![](../../OEBPS/Images/AR_b.png)||*cos*(*θ*)，其中
    *θ* 是向量 ![](../../OEBPS/Images/AR_a.png) 和 ![](../../OEBPS/Images/AR_b.png) 之间的角度。（这已在附录中讨论的二维情况
    [A.1](../Text/A.xhtml#sec-dotprod-cosine-proof) 中得到证明。如果您想获得更深的直觉，可以阅读它。）
- en: Dot product measures the agreement between two vectors
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 点积衡量两个向量之间的协议
- en: The dot product can be expressed using the cosine of the angle between the vectors.
    Given two vectors ![](../../OEBPS/Images/AR_a.png) and ![](../../OEBPS/Images/AR_b.png),
    if *θ* is the angle between them, we have see figure [2.7b](02.xhtml#ch2fig-vec_component))
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 点积可以用向量之间角度的余弦来表示。给定两个向量 ![](../../OEBPS/Images/AR_a.png) 和 ![](../../OEBPS/Images/AR_b.png)，如果
    *θ* 是它们之间的角度，我们得到（见图[2.7b](02.xhtml#ch2fig-vec_component)）
- en: '![](../../OEBPS/Images/eq_02-11.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_02-11.png)'
- en: Equation 2.11
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 方程2.11
- en: Expressing the dot product using cosines makes it easier to see that it measures
    the *agreement* (aka *correlation*) between two vectors. If the vectors have the
    same direction, the angle between them is 0 and the cosine is 1, implying maximum
    agreement. The cosine becomes progressively smaller as the angle between the vectors
    increases, until the two vectors become perpendicular to each other and the cosine
    is zero, implying no correlation—the vectors are independent of each other. If
    the angle between them is 180°, the cosine is −1, implying that the vectors are
    anti-correlated. Thus, the dot product of two vectors is proportional to their
    directional agreement.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 使用余弦表达点积使其更容易看出它衡量的是两个向量之间的*协议*（也称为*相关性*）。如果向量具有相同的方向，它们之间的角度是0，余弦是1，这意味着最大协议。随着向量之间角度的增加，余弦值逐渐减小，直到两个向量相互垂直且余弦值为零，这意味着没有相关性——向量相互独立。如果它们之间的角度是180°，余弦值为-1，这意味着向量是反相关的。因此，两个向量的点积与它们的方向协议成正比。
- en: What role do the vector lengths play in all this? The dot product between two
    vectors is also proportional to the lengths of the vectors. This means agreement
    scores between bigger vectors are higher (an agreement between the US president
    and the German chancellor counts more than an agreement between you and me).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些中，向量长度扮演什么角色？两个向量的点积也正比于向量的长度。这意味着较大向量之间的同意度得分更高（美国总统和德国总理之间的协议比您和我之间的协议更重要）。
- en: 'If you want the agreement score to be neutral to the vector length, you can
    use a normalized dot product between unit-length vectors along the same directions:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想让协议得分对向量长度保持中性，您可以使用沿相同方向单位长度的向量的归一化点积：
- en: '![](../../OEBPS/Images/eq_02-11-a.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_02-11-a.png)'
- en: Dot product and the difference between two unit vectors
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 点积和两个单位向量之间的差异
- en: To obtain further insight into how the dot product indicates agreement or correlation
    between two directions, consider the two unit vectors ![](../../OEBPS/Images/eq_02-11-b2.png)
    and ![](../../OEBPS/Images/eq_02-11-c2.png). The difference between them is ![](../../OEBPS/Images/eq_02-11-d2.png).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入了解点积如何指示两个方向之间的共识或相关性，考虑两个单位向量 ![](../../OEBPS/Images/eq_02-11-b2.png) 和
    ![](../../OEBPS/Images/eq_02-11-c2.png)。它们之间的差异是 ![](../../OEBPS/Images/eq_02-11-d2.png)。
- en: Note that since they are unit vectors, ![](../../OEBPS/Images/eq_02-11-e2.png).
    The length of the difference vector
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于它们是单位向量，![](../../OEBPS/Images/eq_02-11-e2.png)。差异向量的长度
- en: '![](../../OEBPS/Images/eq_02-11-f.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-11-f.png)'
- en: 'From the last equality, it is evident that a larger dot product implies a smaller
    difference: that is, more agreement between the vectors.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 从最后一个等式可以看出，较大的点积意味着较小的差异：也就是说，向量之间有更多的共识。
- en: 2.6 Orthogonality of vectors and its physical significance
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.6 向量的正交性及其物理意义
- en: Try moving an object at right angles to the direction in which you are pushing
    it. You will find it impossible. The larger the angle, the less effective your
    force vector becomes (finally becoming totally ineffective at a 90° angle). This
    is why it is easy to walk on a horizontal surface (you are moving at right angles
    to the direction of gravitational pull, so the gravity vector is ineffective)
    but harder on an upward incline (the gravity vector is having some effect against
    you).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试将物体移动到与您推的方向垂直的方向。你会发现这是不可能的。角度越大，你的力向量变得越不有效（最终在 90° 角时完全无效）。这就是为什么在水平面上行走很容易（你是在与重力吸引方向垂直的方向上移动，所以重力向量无效），但在向上倾斜的表面上行走则更难（重力向量对你产生了一些反作用）。
- en: These physical notions are captured mathematically in the notion of a dot product.
    The dot product between two vectors ![](../../OEBPS/Images/AR_a.png) (say, the
    push vector) and ![](../../OEBPS/Images/AR_b.png) (say, the displacement of the
    pushed object vector) is ||![](../../OEBPS/Images/AR_a.png)|| ||![](../../OEBPS/Images/AR_b.png)||*cosθ*,
    where *θ* is the angle between the two vectors. When *θ* is 0 (the two vectors
    are aligned), *cosθ* = 1, the maximum possible value of *cosθ*, so push is maximally
    effective. As *θ* increases, *cosθ* decreases, and push becomes less and less
    effective. Finally, at *θ* = 90°, *cosθ* = 0, and push becomes completely ineffective.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这些物理概念在点积的概念中被数学化地捕捉。两个向量 ![](../../OEBPS/Images/AR_a.png)（例如，推力向量）和 ![](../../OEBPS/Images/AR_b.png)（例如，被推物体的位移向量）的点积是
    ||![](../../OEBPS/Images/AR_a.png)|| ||![](../../OEBPS/Images/AR_b.png)||*cosθ*，其中
    *θ* 是两个向量之间的角度。当 *θ* 为 0（两个向量对齐）时，*cosθ* = 1，是 *cosθ* 的最大可能值，因此推力是最有效的。随着 *θ*
    的增加，*cosθ* 减少，推力变得越来越不有效。最后，当 *θ* = 90° 时，*cosθ* = 0，推力变得完全无效。
- en: 'Two vectors are orthogonal if their dot product is zero. Geometrically, this
    means the vectors are perpendicular to each other. Physically, this means the
    two vectors are independent: one cannot influence the other. You can say there
    is nothing in common between orthogonal vectors. For instance, the feature vector
    for *d*[5] is ![](../../OEBPS/Images/eq_02-10-p2.png) and that for *d*[6] is ![](../../OEBPS/Images/eq_02-10-q2.png)
    in table [2.1](02.xhtml#tab-doc-vec). These are orthogonal (dot product is zero),
    and you can easily see that none of the feature words (*gun*, *violence*) are
    common to both documents.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个向量的点积为零，则这两个向量是正交的。从几何上看，这意味着向量彼此垂直。从物理上看，这意味着两个向量是独立的：一个不能影响另一个。你可以这样说，正交向量之间没有共同之处。例如，*d*[5]
    的特征向量是 ![](../../OEBPS/Images/eq_02-10-p2.png)，而 *d*[6] 的特征向量是 ![](../../OEBPS/Images/eq_02-10-q2.png)（见表
    [2.1](02.xhtml#tab-doc-vec)）。这些是正交的（点积为零），你可以很容易地看到，两个文档中都没有共同的特征词（*枪*，*暴力*）。
- en: '2.7 Python code: Basic vector and matrix operations via PyTorch'
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.7 Python 代码：通过 PyTorch 的基本向量和矩阵运算
- en: In this section, we use Python PyTorch code to illustrate many of the concepts
    discussed earlier.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用 Python PyTorch 代码来阐述前面讨论的许多概念。
- en: NOTE Fully functional code for this section, executable via Jupyter Notebook,
    can be found at [http://mng.bz/ryzE](https://github.com/krishnonwork/mathematical-methods-in-deep-learning-ipython/blob/master/python/ch2/2.7-transpose-dot-matmul.ipynb).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: NOTE 完整功能的代码可在 Jupyter Notebook 中执行，可在[http://mng.bz/ryzE](https://github.com/krishnonwork/mathematical-methods-in-deep-learning-ipython/blob/master/python/ch2/2.7-transpose-dot-matmul.ipynb)找到。
- en: 2.7.1 PyTorch code for a matrix transpose
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.7.1 矩阵转置的 PyTorch 代码
- en: The following listing shows the PyTorch code for a matrix transpose.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了矩阵转置的 PyTorch 代码。
- en: Listing 2.5 Transpose
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.5 转置
- en: '[PRE4]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ① The torch.arange function creates a vector whose elements go from *start*
    to *stop* in increments of *step*. Here we create a 4 × 9 image corresponding
    to *I*[4,9] in equation [2.2](02.xhtml#eq-tiny_im), shown in figure [2.3](02.xhtml#fig-tiny_im).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ① torch.arange 函数创建一个元素从 *start* 到 *stop* 以 *step* 为增量的向量。这里我们创建一个 4 × 9 的图像，对应于方程
    [2.2](02.xhtml#eq-tiny_im) 中的 *I*[4,9]，如图 [2.3](02.xhtml#fig-tiny_im) 所示。
- en: ② The transpose operator interchanges rows and columns. The 4 × 9 image becomes
    a 9 × 4 image (see figure [2.6](02.xhtml#fig-tiny_im-transpose). The element at
    position (*i, j*) is interchanged with the element at position (*j, i*).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ② 转置运算符交换行和列。4 × 9 的图像变为 9 × 4 的图像（见图 [2.6](02.xhtml#fig-tiny_im-transpose)。位置
    (*i, j*) 的元素与位置 (*j, i*) 的元素交换。
- en: ③ Interchanged elements of the original and transposed matrix are equal.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 原始矩阵和转置矩阵的交换元素相等。
- en: ④ The .T operator retrieves the transpose of an array.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ④ .T 运算符检索数组的转置。
- en: 2.7.2 PyTorch code for a dot product
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.7.2 PyTorch 点积的代码
- en: The dot product of two vectors ![](../../OEBPS/Images/AR_a.png) and ![](../../OEBPS/Images/AR_b.png)
    represents the components of one vector along the other. Consider two vectors
    ![](../../OEBPS/Images/AR_a.png) = [*a*[1] *a*[2] *a*[3]] and ![](../../OEBPS/Images/AR_b.png)
    = [*b*[1] *b*[2] *b*[3]]. Then ![](../../OEBPS/Images/AR_a.png).![](../../OEBPS/Images/AR_b.png)
    = *a*[1]*b*[1] + *a*[2]*b*[2] + *a*[3]*b*[3].
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 两个向量 ![](../../OEBPS/Images/AR_a.png) 和 ![](../../OEBPS/Images/AR_b.png) 的点积表示一个向量沿另一个向量的分量。考虑两个向量
    ![](../../OEBPS/Images/AR_a.png) = [*a*[1] *a*[2] *a*[3]] 和 ![](../../OEBPS/Images/AR_b.png)
    = [*b*[1] *b*[2] *b*[3]]。那么 ![](../../OEBPS/Images/AR_a.png).![](../../OEBPS/Images/AR_b.png)
    = *a*[1]*b*[1] + *a*[2]*b*[2] + *a*[3]*b*[3]。
- en: Listing 2.6 Dot product
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.6 点积
- en: '[PRE5]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '① Outputs 32: 1 ∗ 4 + 2 ∗ 5 + 3 ∗ 6'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ① 输出 32：1 ∗ 4 + 2 ∗ 5 + 3 ∗ 6
- en: '② Outputs 0: 1 ∗ 0 + 0 ∗ 1'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ② 输出 0：1 ∗ 0 + 0 ∗ 1
- en: 2.7.3 PyTorch code for matrix vector multiplication
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.7.3 PyTorch 矩阵-向量乘法的代码
- en: Consider a matrix *A[m, n]* with *m* rows and *n* columns that is multiplied
    with a vector *![](../../OEBPS/Images/AR_b.png)[n]* with *n* elements. The result
    is a *m* element column vector *![](../../OEBPS/Images/AR_c.png)[m]* . In the
    following example, *m* = 3 and *n* = 2.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个具有 *m* 行和 *n* 列的矩阵 *A[m, n]*，它与一个具有 *n* 个元素的向量 *![](../../OEBPS/Images/AR_b.png)[n]*
    相乘。结果是 *m* 个元素的列向量 *![](../../OEBPS/Images/AR_c.png)[m]*。在以下示例中，*m* = 3，*n* =
    2。
- en: '![](../../OEBPS/Images/eq_02-11-g.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-11-g.png)'
- en: In general,
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，
- en: '![](../../OEBPS/Images/eq_02-11-h.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-11-h.png)'
- en: Listing 2.7 Matrix vector multiplication
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.7 矩阵-向量乘法
- en: '[PRE6]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ① A linear model comprises a weight vector ![](../../OEBPS/Images/AR_w.png)
    and bias *b*. For each training data instance *![](../../OEBPS/Images/AR_x.png)[i]*,
    the model outputs *y**[i]* = ![](../../OEBPS/Images/AR_x.png)*[i]^T*![](../../OEBPS/Images/AR_w.png)
    + *b*. For the training data matrix *X* (whose rows are training data instances),
    the model outputs *X*![](../../OEBPS/Images/AR_w.png) + ![](../../OEBPS/Images/AR_b.png)
    = ![](../../OEBPS/Images/AR_y.png)
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ① 线性模型由权重向量 ![](../../OEBPS/Images/AR_w.png) 和偏置 *b* 组成。对于每个训练数据实例 *![](../../OEBPS/Images/AR_x.png)[i]*，模型输出
    *y**[i]* = ![](../../OEBPS/Images/AR_x.png)*[i]^T*![](../../OEBPS/Images/AR_w.png)
    + *b*。对于训练数据矩阵 *X*（其行是训练数据实例），模型输出 *X*![](../../OEBPS/Images/AR_w.png) + ![](../../OEBPS/Images/AR_b.png)
    = ![](../../OEBPS/Images/AR_y.png)
- en: ② Cat-brain 15 × 2 training data matrix (equation [2.7](02.xhtml#eq-cat-brain-nobiased))
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ② Cat-brain 15 × 2 训练数据矩阵（方程 [2.7](02.xhtml#eq-cat-brain-nobiased)）
- en: ③ Random initialization of weight vector
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 权重向量的随机初始化
- en: '④ Model training output: ![](../../OEBPS/Images/AR_y.png) = *X*![](../../OEBPS/Images/AR_w.png)
    + *b*. The scalar *b* is automatically replicated to create a vector.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 模型训练输出：![](../../OEBPS/Images/AR_y.png) = *X*![](../../OEBPS/Images/AR_w.png)
    + *b*。标量 *b* 会自动复制以创建一个向量。
- en: 2.7.4 PyTorch code for matrix-matrix multiplication
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.7.4 PyTorch 矩阵-矩阵乘法的代码
- en: 'Consider a matrix *A[m, p]* with *m* rows and *p* columns. Let’s multiply it
    with another matrix *B[p, n]* with *p* rows and *n* columns. The resultant matrix
    *C[m, n]* contains *m* rows and *n* columns. Note that the number of columns in
    the left matrix *A* should match the number of rows in the right matrix *B*:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个具有 *m* 行和 *p* 列的矩阵 *A[m, p]*。让我们用另一个具有 *p* 行和 *n* 列的矩阵 *B[p, n]* 相乘。结果矩阵
    *C[m, n]* 包含 *m* 行和 *n* 列。请注意，左矩阵 *A* 的列数应与右矩阵 *B* 的行数相匹配：
- en: '![](../../OEBPS/Images/eq_02-11-i.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-11-i.png)'
- en: In general,
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，
- en: '![](../../OEBPS/Images/eq_02-11-j.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-11-j.png)'
- en: Listing 2.8 Matrix-matrix multiplication
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.8 矩阵-矩阵乘法
- en: '[PRE7]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ① *C* = *AB* ⟹ *C*[*i*, *j*] is the dot product of the *i*th row of *A* and
    *j*th column of B.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ① *C* = *AB* ⟹ *C*[*i*, *j*] 是 *A* 的第 *i* 行与 *B* 的第 *j* 列的点积。
- en: ② ![](../../OEBPS/Images/eq_02-11-k.png)
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ② ![多维直线方程图](../../OEBPS/Images/eq_02-11-k.png)
- en: ③ The dot product can be viewed as a row matrix multiplied by a column matrix.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 点积可以看作是一个行矩阵乘以一个列矩阵。
- en: 2.7.5 PyTorch code for the transpose of a matrix product
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.7.5 PyTorch 中矩阵乘积转置的代码
- en: 'Given two matrices *A* and *B*, where the number of columns in *A* matches
    the number of rows in *B*, the transpose of their product is the product of the
    individual transposes *in reversed order*: (*AB*)*^T* = *B^TA^T*.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 给定两个矩阵 *A* 和 *B*，其中 *A* 的列数与 *B* 的行数相匹配，它们的乘积的转置是各自转置的乘积（顺序相反）：(*AB*)*^T* =
    *B^TA^T*。
- en: Listing 2.9 Transpose of a matrix product
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.9 矩阵乘积的转置
- en: '[PRE8]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ① Asserts equality between (*AB*)*^T* and *B^TA^T*
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ① 声明 (*AB*)*^T* 和 *B^TA^T* 相等
- en: '② Applies to matrix-vector multiplication, too: (*A^T*![](../../OEBPS/Images/AR_x.png))*^T*
    = ![](../../OEBPS/Images/AR_x.png)*^TA*'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ② 也适用于矩阵-向量乘法：(*A^T*![向量图](../../OEBPS/Images/AR_x.png))*^T* = ![向量图](../../OEBPS/Images/AR_x.png)*^TA*
- en: 2.8 Multidimensional line and plane equations and machine learning
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.8 多维直线和平面方程与机器学习
- en: Geometrically speaking, what does a machine learning classifier really do? We
    provided the outline of an answer in section [1.4](../Text/01.xhtml#sec-geom-view-ml).
    You are invited to review that and especially figures [1.2](../Text/01.xhtml#fig-geometrical_view)
    and [1.3](../Text/01.xhtml#fig-ml-as-mapping). We will briefly summarize here.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 从几何学的角度来看，机器学习分类器实际上做什么？我们在第 [1.4](../Text/01.xhtml#sec-geom-view-ml) 节中提供了答案的概要。请回顾该内容，特别是图
    [1.2](../Text/01.xhtml#fig-geometrical_view) 和 [1.3](../Text/01.xhtml#fig-ml-as-mapping)。我们在这里将简要总结。
- en: Inputs to a classifier are feature vectors. These vectors can be viewed as points
    in some multidimensional feature space. The task of classification then boils
    down to separating the points belonging to different classes. The points may be
    all jumbled up in the input space. It is the model’s job to transform them into
    a different (output) space where it is easier to separate the classes. A visual
    example of this was provided in figure [1.3](../Text/01.xhtml#fig-ml-as-mapping).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器的输入是特征向量。这些向量可以看作是多维特征空间中的点。分类的任务就是将属于不同类的点分开。这些点可能在输入空间中杂乱无章。这是模型的工作，将它们转换到不同的（输出）空间中，在那里更容易分开类。这种转换的视觉示例已在图
    [1.3](../Text/01.xhtml#fig-ml-as-mapping) 中提供。
- en: What is the geometrical nature of the separator? In a very simple situation,
    such as the one depicted in figure [1.2](../Text/01.xhtml#fig-geometrical_view),
    the separator is a line in 2D space. In real-life situations, the separator is
    often a line or a plane in a high-dimensional space. In more complicated situations,
    the separator is a curved surface, as depicted in figure [1.4](../Text/01.xhtml#fig-non_linear_separator).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 分离器的几何性质是什么？在非常简单的情况下，例如图 [1.2](../Text/01.xhtml#fig-geometrical_view) 所示的情况，分离器是二维空间中的一条线。在现实生活中的情况下，分离器通常是高维空间中的一条线或一个平面。在更复杂的情况下，分离器是一个曲面，如图
    [1.4](../Text/01.xhtml#fig-non_linear_separator) 所示。
- en: In this section, we will study the mathematics and geometry behind two types
    of separators, lines, and planes in high-dimensional spaces, aka hyperlines and
    hyperplanes.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将研究高维空间中两种类型分离器（直线和平面，也称为超线和超平面）背后的数学和几何。
- en: 2.8.1 Multidimensional line equation
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.8.1 多维直线方程
- en: In high school geometry, we learned *y* = *mx* + *c* as the equation of a line.
    But this does not lend itself readily to higher dimensions. Here we will study
    a better representation of a straight line that works equally well for any finite-dimensional
    space.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在高中几何中，我们学习了 *y* = *mx* + *c* 作为直线的方程。但这并不容易应用于更高维度。在这里，我们将研究一种更好的直线表示方法，它对任何有限维空间都同样有效。
- en: As shown in figure [2.8](02.xhtml#fig-multi-dim-lineeq), a line joining vectors
    ![](../../OEBPS/Images/AR_a.png) and ![](../../OEBPS/Images/AR_b.png) can be viewed
    as the set of points we will encounter if we
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [2.8](02.xhtml#fig-multi-dim-lineeq) 所示，连接向量 ![起点向量图](../../OEBPS/Images/AR_a.png)
    和 ![终点向量图](../../OEBPS/Images/AR_b.png) 的直线可以看作是我们将遇到的点的集合，如果我们
- en: Start at point ![](../../OEBPS/Images/AR_a.png)
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从点 ![起点向量图](../../OEBPS/Images/AR_a.png) 开始
- en: Travel along the direction ![](../../OEBPS/Images/AR_b.png) − ![](../../OEBPS/Images/AR_a.png)
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 沿着方向 ![方向向量图](../../OEBPS/Images/AR_b.png) − ![起点向量图](../../OEBPS/Images/AR_a.png)
    移动
- en: '![](../../OEBPS/Images/CH02_F08_Chaudhury.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![章节图](../../OEBPS/Images/CH02_F08_Chaudhury.png)'
- en: Figure 2.8 Any point ![](../../OEBPS/Images/AR_x.png) on the line joining two
    vectors ![](../../OEBPS/Images/AR_a.png), ![](../../OEBPS/Images/AR_b.png) is
    given by ![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_a.png) +
    *α*(![](../../OEBPS/Images/AR_b.png)−![](../../OEBPS/Images/AR_a.png)).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.8 在连接两个向量 ![](../../OEBPS/Images/AR_a.png) 和 ![](../../OEBPS/Images/AR_b.png)
    的直线上，任意点 ![](../../OEBPS/Images/AR_x.png) 可以表示为 ![](../../OEBPS/Images/AR_x.png)
    = ![](../../OEBPS/Images/AR_a.png) + *α*(![](../../OEBPS/Images/AR_b.png)−![](../../OEBPS/Images/AR_a.png))。
- en: Different points on the line are obtained by traveling different distances.
    Denoting this arbitrary distance by *α*, the equation of the line joining vectors
    ![](../../OEBPS/Images/AR_a.png) and ![](../../OEBPS/Images/AR_b.png) can be expressed
    as
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 通过旅行不同的距离，可以得到线上的不同点。用 *α* 表示这个任意距离，连接向量 ![](../../OEBPS/Images/AR_a.png) 和
    ![](../../OEBPS/Images/AR_b.png) 的直线方程可以表示为
- en: '![](../../OEBPS/Images/eq_02-12.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-12.png)'
- en: Equation 2.12
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 2.12
- en: Equation [2.12](02.xhtml#eq-collinearity) says that any point on the line joining
    ![](../../OEBPS/Images/AR_a.png) and ![](../../OEBPS/Images/AR_b.png) can be obtained
    as a weighted combination of ![](../../OEBPS/Images/AR_a.png) and ![](../../OEBPS/Images/AR_b.png),
    the weights being *α* and 1 − *α*. By varying *α*, we obtain different points
    on the line. Also, different ranges of *α* values yield different segments on
    the line. As shown in figure [2.8](02.xhtml#fig-multi-dim-lineeq), values of *α*
    between 0 and 1 yield points between ![](../../OEBPS/Images/AR_a.png) and ![](../../OEBPS/Images/AR_b.png).
    Negative values of *α* yield points to the left of ![](../../OEBPS/Images/AR_a.png).
    Values of *α* greater than 1 yield points to the right of ![](../../OEBPS/Images/AR_b.png).
    This equation for a line works for any dimensions, not just two.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 [2.12](02.xhtml#eq-collinearity) 表示，连接 ![](../../OEBPS/Images/AR_a.png)
    和 ![](../../OEBPS/Images/AR_b.png) 的直线上的任意点都可以表示为 ![](../../OEBPS/Images/AR_a.png)
    和 ![](../../OEBPS/Images/AR_b.png) 的加权组合，权重为 *α* 和 1 − *α*。通过改变 *α*，我们可以得到直线上的不同点。不同的
    *α* 值范围产生直线上的不同线段。如图 [2.8](02.xhtml#fig-multi-dim-lineeq) 所示，*α* 在 0 和 1 之间的值产生在
    ![](../../OEBPS/Images/AR_a.png) 和 ![](../../OEBPS/Images/AR_b.png) 之间的点。*α* 的负值产生在
    ![](../../OEBPS/Images/AR_a.png) 的左侧。*α* 大于 1 的值产生在 ![](../../OEBPS/Images/AR_b.png)
    的右侧。这个直线方程适用于任何维度，而不仅仅是二维。
- en: 2.8.2 Multidimensional planes and their role in machine learning
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.8.2 多维平面及其在机器学习中的作用
- en: 'In section [1.5](../Text/01.xhtml#sec-regression-vs-classification), we encountered
    classifiers. Let’s take another look at them. Suppose we want to create a classifier
    that helps us make *buy* or *no-buy* decisions on stocks based on only three input
    variables: 1) *momentum*, or the rate at which the stock price is changing positive
    momentum means the stock price is increasing and vice versa); 2) the *dividend*
    paid last quarter; and (3) *volatility*, or how much the price has fluctuated
    in the last quarter. Let’s plot all training points in the feature space with
    coordinate axes corresponding to the variables *momentum*, *dividend*, *volatility*.
    Figure [2.9](02.xhtml#fig-planar-classifier) shows that the classes can be separated
    by a plane in the three-dimensional feature space.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [1.5](../Text/01.xhtml#sec-regression-vs-classification) 节中，我们遇到了分类器。让我们再看看它们。假设我们想要创建一个分类器，帮助我们根据仅有的三个输入变量：1)
    *动量*，即股价变化的速率（正动量意味着股价在上升，反之亦然）；2) 上个季度支付的 *股息*；以及 3) *波动性*，即股价在上个季度的波动幅度。让我们在特征空间中绘制所有训练点，坐标轴对应于变量
    *动量*、*股息*、*波动性*。图 [2.9](02.xhtml#fig-planar-classifier) 显示，类别可以通过三维特征空间中的一个平面来分离。
- en: '![](../../OEBPS/Images/CH02_F09_Chaudhury.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH02_F09_Chaudhury.png)'
- en: 'Figure 2.9 A toy machine learning classifier for stock buy vs. no-buy decision-making.
    A plus (+) indicates no-buy, and a dash (-) indicates buy. The decision is made
    based on three input variables: momentum, dividend, and volatility.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.9 一个玩具机器学习分类器，用于股票买入与不买入决策。加号 (+) 表示不买入，减号 (-) 表示买入。决策是基于三个输入变量：动量、股息和波动性。
- en: Geometrically speaking, our model simply corresponds to this plane. Input points
    above the plane indicate buy decisions (dashes [-]), and input points indicate
    no-buy decisions (pluses [+]). In general, you want to buy high-positive-momentum
    stocks, so points at the higher end of the momentum axis are likelier to be *buy*.
    However, this is not the only indicator. For more volatile stocks, we demand higher
    *momentum* to switch from *no-buy* to *buy*. This is why the plane slopes upward
    (higher *momentum*) as we move rightward (higher *volatility*). Also, we demand
    less *momentum* for stocks with higher *dividends*. This is why the plane slopes
    downward (lower *momentum*) as we go toward higher *dividends*.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 从几何学的角度来看，我们的模型简单地对应于这个平面。位于平面之上的输入点表示买入决策（破折号 [-]），而位于平面之下的输入点表示不买入决策（加号 [+])。一般来说，你想要买入高动量股票，因此动量轴较高端的点更有可能被标记为
    *买入*。然而，这并不是唯一的指标。对于更波动的股票，我们要求更高的 *动量* 来从 *不买入* 切换到 *买入*。这就是为什么随着我们向右移动（波动性更高），平面向上倾斜（动量更高）。此外，对于股息率更高的股票，我们要求更低的
    *动量*。这就是为什么随着我们向更高的 *股息率* 移动，平面向下倾斜（动量更低）。
- en: Real problems have more dimensions, of course (since many more inputs are involved
    in the decision), and the separator becomes a hyperplane. Also, in real-life problems,
    the points are often too intertwined in the input space for any separator to work.
    We first have to apply a transformation that maps the point to an output space
    where it is easier to separate. Given their significance as class separators in
    machine learning, we will study hyperplanes in this section.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 实际问题通常有更多的维度（因为决策中涉及更多的输入），分隔符变成超平面。此外，在实际问题中，点在输入空间中通常交织得太紧密，以至于任何分隔符都无法工作。我们首先必须应用一个变换，将点映射到一个更容易分离的输出空间。鉴于它们在机器学习中的类别分隔符的重要性，我们将在本节中研究超平面。
- en: In high school 3D geometry, we learned *ax* + *by* + *cz* + *d* = 0 as the equation
    of a plane. Now we will study a version of it that works in higher dimensions.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在高中三维几何中，我们学习了 *ax* + *by* + *cz* + *d* = 0 作为平面的方程。现在我们将研究它在更高维度上工作的版本。
- en: Geometrically speaking, given a plane (in any dimension), we can find a direction
    called the *normal direction*, denoted *n̂*, such that
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 从几何学的角度来看，给定一个平面（在任何维度上），我们可以找到一个称为 *法线方向* 的方向，表示为 *n̂*，使得
- en: If we take any pair of points on the plane, say ![](../../OEBPS/Images/AR_x.png)[0]
    and ![](../../OEBPS/Images/AR_x.png), …
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们在平面上取任意一对点，比如说 ![](../../OEBPS/Images/AR_x.png)[0] 和 ![](../../OEBPS/Images/AR_x.png)，
    …
- en: The line joining ![](../../OEBPS/Images/AR_x.png) and ![](../../OEBPS/Images/AR_x.png)[0]—i.e.,
    the vector ![](../../OEBPS/Images/AR_x.png) − ![](../../OEBPS/Images/AR_x.png)[0]—is
    orthogonal to *n̂*.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接 ![](../../OEBPS/Images/AR_x.png) 和 ![](../../OEBPS/Images/AR_x.png)[0] 的线，即向量
    ![](../../OEBPS/Images/AR_x.png) − ![](../../OEBPS/Images/AR_x.png)[0]，与 *n̂*
    正交。
- en: Thus, if we know a fixed point on the plane, say ![](../../OEBPS/Images/AR_x.png)[0],
    then all points on the plane will satisfy
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们知道平面上的一个固定点，比如说 ![](../../OEBPS/Images/AR_x.png)[0]，那么平面上的所有点都将满足
- en: '*n̂* · (![](../../OEBPS/Images/AR_x.png) − ![](../../OEBPS/Images/AR_x.png)[0])
    = 0'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '*n̂* · (![](../../OEBPS/Images/AR_x.png) − ![](../../OEBPS/Images/AR_x.png)[0])
    = 0'
- en: or
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 或者
- en: '*n̂^T*(![](../../OEBPS/Images/AR_x.png) − ![](../../OEBPS/Images/AR_x.png)[0])
    = 0'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '*n̂^T*(![](../../OEBPS/Images/AR_x.png) − ![](../../OEBPS/Images/AR_x.png)[0])
    = 0'
- en: Thus we can express the equation of a plane as
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将平面的方程表示为
- en: '*n̂^T*![](../../OEBPS/Images/AR_x.png) − *n̂^T*![](../../OEBPS/Images/AR_x.png)[0]
    = 0'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '*n̂^T*![](../../OEBPS/Images/AR_x.png) − *n̂^T*![](../../OEBPS/Images/AR_x.png)[0]
    = 0'
- en: Equation 2.13
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2.13
- en: Equation [2.13](02.xhtml#eq-plane-0) is depicted pictorially in figure [2.10](02.xhtml#fig-multi-dim-planeeq).
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 [2.13](02.xhtml#eq-plane-0) 在图 [2.10](02.xhtml#fig-multi-dim-planeeq) 中以图形方式表示。
- en: '![](../../OEBPS/Images/CH02_F10_Chaudhury.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH02_F10_Chaudhury.png)'
- en: Figure 2.10 The normal to the plane is the same at all points on the plane.
    This is the fundamental property of a plane. *n̂* depicts that normal direction.
    Let ![](../../OEBPS/Images/AR_x.png)[0] be a point on the plane. All other points
    on the plane, depicted as ![](../../OEBPS/Images/AR_x.png), will satisfy the equation
    (![](../../OEBPS/Images/AR_x.png)−![](../../OEBPS/Images/AR_x.png)[0]) ⋅ *n̂*
    = 0. This physically says that the line joining a known point ![](../../OEBPS/Images/AR_x.png)[0]
    on the plane and any other arbitrary point ![](../../OEBPS/Images/AR_x.png) on
    the plane is at right angles to the normal *n̂*. This formulation works for any
    dimension.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: 'In section [1.3](../Text/01.xhtml#sec-cat_brain), equation [1.3](../Text/01.xhtml#eq-linear-predictor),
    we encountered the simplest machine learning model: a weighted sum of inputs along
    with a bias. Denoting the inputs as ![](../../OEBPS/Images/AR_x.png), the weights
    as ![](../../OEBPS/Images/AR_w.png), and the bias as *b*, this model was depicted
    as'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png) + *b*
    = 0'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Equation 2.14
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparing equations [2.13](02.xhtml#eq-plane-0) and [2.14](02.xhtml#eq-plane-1)
    , we get the geometric significance: the simple model of equation [1.3](../Text/01.xhtml#eq-linear-predictor)
    is nothing but a planar separator. Its weight vector ![](../../OEBPS/Images/AR_w.png)
    corresponds to the plane’s orientation (normal). The bias *b* corresponds to the
    plane’s location (a fixed point on the plane). During training, we are learning
    the weights and biases—this is essentially learning the orientation and position
    of the optimal plane that will separate the training inputs. To be consistent
    with the machine learning paradigm, henceforth we will write the equation of a
    hyperplane as equation [2.14](02.xhtml#eq-plane-1) for some constant ![](../../OEBPS/Images/AR_w.png)
    and *b*.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Note that ![](../../OEBPS/Images/AR_w.png) need not be a unit-length vector.
    Since the right-hand side is zero, if necessary, we can divide both sides by ||![](../../OEBPS/Images/AR_w.png)||
    to convert to a form like equation [2.13](02.xhtml#eq-plane-0).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: The sign of the expression ![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png)
    + *b* has special significance. All points ![](../../OEBPS/Images/AR_x.png) for
    which ![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png) + *b*
    < 0 lie on the same side of the hyperplane. All points ![](../../OEBPS/Images/AR_x.png)
    for which ![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png)
    + *b* > 0 lie on the other side of the hyperplane. And of course, all points ![](../../OEBPS/Images/AR_x.png)
    for which ![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png)
    + *b* = 0 lie on the hyperplane.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that the 3D equation *ax* + *by* + *cz* + *d* = 0 is a special
    case of equation [2.14](02.xhtml#eq-plane-1) because *ax* + *by* + *cz* + *d*
    = 0 can be rewritten as
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-14-a.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
- en: which is same as ![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png)
    + *b* = 0 with ![](../../OEBPS/Images/eq_02-14-b2.png) and ![](../../OEBPS/Images/eq_02-14-c2.png).
    Incidentally, this tells us that in 3D, the normal to the plane *ax* + *by* +
    *cz* + *d* = 0 is ![](../../OEBPS/Images/eq_02-14-d2.png) .
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: 2.9 Linear combinations, vector spans, basis vectors, and collinearity preservation
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: by now, it should be clear that machine learning and data science are all about
    points in high-dimensional spaces. Consequently, it behooves us to have a decent
    understanding of these spaces. For instance, given a space, we may need to ask,
    “Would it be possible to express all points in the space in terms of a set of
    a few vectors? What is the smallest set of vectors we really need for that purpose?”
    This section is devoted to the study of these questions.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: 2.9.1 Linear dependence
  id: totrans-303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Consider the vectors (points) shown in figure [2.11](02.xhtml#fig-lin-dep).
    The corresponding vectors in 2D are
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-14-e.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
- en: We can find four scalars *α*[0] = 2, *α*[1] = 2, *α*[2] = 2, and *α*[3] = −3
    such that
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-14-f.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
- en: If we can find such scalars, not all zero, we say the vectors ![](../../OEBPS/Images/AR_v.png)[0],
    ![](../../OEBPS/Images/AR_v.png)[1], ![](../../OEBPS/Images/AR_v.png)[2], and
    ![](../../OEBPS/Images/AR_v.png)[3] are *linearly dependent*. The geometric picture
    to keep in mind is that points corresponding to linearly dependent vectors lie
    on a single straight line in the space containing them.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH02_F11_Chaudhury.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
- en: Figure 2.11 Linearly dependent points in a 2D plane
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: Collinearity implies linear dependence
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 'Proof: Let ![](../../OEBPS/Images/AR_a.png), ![](../../OEBPS/Images/AR_b.png)
    and ![](../../OEBPS/Images/AR_c.png) be three collinear vectors. From equation
    [2.12](02.xhtml#eq-collinearity), there exists some *α* ∈ ℝ such that'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_c.png) = (1−*α*)![](../../OEBPS/Images/AR_a.png)
    + *α*![](../../OEBPS/Images/AR_b.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
- en: This equation can be rewritten as
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '*α*[1]![](../../OEBPS/Images/AR_a.png) + *α*[2]![](../../OEBPS/Images/AR_b.png)
    + *α*[3]![](../../OEBPS/Images/AR_c.png) = 0'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: where *α*[1] = (1−*α*), *α*[2] = *α* and *α*[3] = −1. Thus we have proven that
    three collinear vectors ![](../../OEBPS/Images/AR_a.png), ![](../../OEBPS/Images/AR_b.png),
    and ![](../../OEBPS/Images/AR_c.png) must also be linearly dependent.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Linear combination
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: Given a set of vectors ![](../../OEBPS/Images/AR_v.png)[1], ![](../../OEBPS/Images/AR_v.png)[2],
    …. ![](../../OEBPS/Images/AR_v.png)*[n]* and a set of scalar weights *α*[1], *α*[2],
    …*α[n]*, the weighted sum *α*[1]![](../../OEBPS/Images/AR_v.png)[1] + *α*[2]![](../../OEBPS/Images/AR_v.png)[2]
    + + … *α**[n]*![](../../OEBPS/Images/AR_v.png)*[n]* is called a *linear combination*.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Generic multidimensional definition of linear dependence
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: A set of vectors ![](../../OEBPS/Images/AR_v.png)[1], ![](../../OEBPS/Images/AR_v.png)[2],
    …. ![](../../OEBPS/Images/AR_v.png)*[n]* are *linearly dependent* if there exists
    a set of weights *α*[1], *α*[2], …*α[n]*, not all zeros, such that *α*[1]![](../../OEBPS/Images/AR_v.png)[1]
    + *α*[2]![](../../OEBPS/Images/AR_v.png)[2] + + … *α**[n]*![](../../OEBPS/Images/AR_v.png)*[n]*
    = 0. For example, the row vectors [1   1] and [2   2] are linearly dependent,
    since –2[1   1] + [2   2] = 0.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 2.9.2 Span of a set of vectors
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given a set of vectors ![](../../OEBPS/Images/AR_v.png)[1], ![](../../OEBPS/Images/AR_v.png)[2],
    …. ![](../../OEBPS/Images/AR_v.png)*[n]*, their *span* is defined as the set of
    all vectors that are linear combinations of the original set . This includes the
    original vectors.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider the two vectors ![](../../OEBPS/Images/eq_02-14-i1b.png)
    and ![](../../OEBPS/Images/eq_02-14-i2b.png). The span of these two vectors is
    the entire plane containing the two vectors. Any vector, for instance, the vector
    ![](../../OEBPS/Images/eq_02-14-j2.png) can be expressed as a weighted sum 18![](../../OEBPS/Images/AR_v.png)[*x*⊥]
    + 97![](../../OEBPS/Images/AR_v.png)[*y*⊥].
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: You can probably recognize that ![](../../OEBPS/Images/eq_02-14-k1b.png) and
    ![](../../OEBPS/Images/eq_02-14-k2b.png) are the familiar Cartesian coordinate
    axes (*X*-axis and *Y*-axis, respectively) in the 2D plane.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: 2.9.3 Vector spaces, basis vectors, and closure
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have been talking informally about vector spaces. It is time to define them
    more precisely.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: Vector spaces
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: A set of vectors (points) in *n* dimensions form a *vector space* if and only
    if the operations of *addition* and *scalar multiplication* are defined on the
    set. In particular, this implies that it is possible to take linear combinations
    of members of a vector space.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: Basis vectors
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Given a vector space, a set of vectors that span the space is called a *basis*
    for the space. For instance, for the space ℝ², the two vectors ![](../../OEBPS/Images/eq_02-14-k1.png)
    and ![](../../OEBPS/Images/eq_02-14-k2.png) are basis vectors. This essentially
    means any vector in ℝ² can be expressed as a linear combination of these two.
    The notion can be extended to higher dimensions. For ℝ*^n*, the vectors ![](../../OEBPS/Images/eq_02-14-l.png)
    form a basis.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: The alert reader has probably guessed by now that the basis vectors are related
    to coordinate axes. In fact, the basis vectors just described constitute the Cartesian
    coordinate axes.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have only seen examples of basis vectors that are mutually orthogonal,
    such as the dot product of the two basis vectors in ℝ² shown earlier: ![](../../OEBPS/Images/eq_02-14-m.png).
    However, basis vectors do not have to be orthogonal. Any pair of linearly independent
    vectors forms a basis in ℝ². Basis vectors, then, are by no means unique. That
    said, orthogonal vectors are most convenient, as we shall see later.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Minimal and complete basis
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Exactly *n* vectors are needed to span a space with dimensionality *n*. This
    means the basis set for a space will have at least as many elements as the dimensionality
    of the space. That many basis vectors are also sufficient to form a basis. For
    instance, exactly *n* vectors are needed to form a basis in (that is, span) ℝ*^n*.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: A related fact is that in ℝ*^n*, any set of *m* vectors with *m* > *n* will
    be linearly dependent. In other words, the largest size of a set of linearly independent
    vectors in an *n*-dimensional space is *n*.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: Closure
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 'A set of vectors is said to be *closed* under linear combination if and only
    if the linear combination of any pair of vectors in the set also belongs to the
    same set. Consider the set of points ℝ². Recall that this is the set of vectors
    with two real elements. Take any pair of vectors ![](../../OEBPS/Images/AR_a.png)
    and ![](../../OEBPS/Images/AR_b.png) in ℝ²: for instance, ![](../../OEBPS/Images/eq_02-14-n1.png)
    and ![](../../OEBPS/Images/eq_02-14-n2.png). Any linear combination of these two
    vectors will also comprise two real numbers—that is, will belong to ℝ². We say
    ℝ² is a *vector space* since it is *closed* under linear combination.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: Consider the space ℝ². Geometrically speaking, this represents a two dimensional
    plane. Let’s take two points on this plane, ![](../../OEBPS/Images/AR_a.png) and
    ![](../../OEBPS/Images/AR_b.png). Linear combinations of ![](../../OEBPS/Images/AR_a.png),
    ![](../../OEBPS/Images/AR_b.png) geometrically correspond to points on the line
    joining them. We know that if two points lie on a plane, the entire line will
    also lie on the plane. Thus, in two dimensions, a plane is closed under linear
    combinations. This is the geometrical intuition behind the notion of closure on
    vector spaces. It can be extended to arbitrary dimensions.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the set of points on the surface of a sphere is *not* closed
    under linear combination because the line joining an arbitrary pair of points
    on this set will not wholly lie on the surface of that sphere.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '2.10 Linear transforms: Geometric and algebraic interpretations'
  id: totrans-340
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Inputs to a machine learning or data science system are typically feature vectors
    (introduced in section [2.1](02.xhtml#sec-vectors)) in high-dimensional spaces.
    Each individual dimension of the feature vector corresponds to a particular property
    of the input. Thus, the feature vector is a descriptor for the particular input
    instance. It can be viewed as a point in the feature space. We usually transform
    the points to a friendlier space where it is easier to perform the analysis we
    are trying to do. For instance, if we are building a classifier, we try to transform
    the input into a space where the points belonging to different classes are more
    segregated (see section [1.3](../Text/01.xhtml#fig-ml-as-mapping) in general and
    figure [1.3](../Text/01.xhtml#fig-ml-as-mapping) in particular for simple examples).
    Sometimes we transform to simplify the data, eliminating axes along which there
    is scant variation in the data. Given their significance in machine learning,
    in this section we will study the basics of transforms.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: Informally, a transform is an operation that maps a set of points vectors) to
    another. Given a set *S* of *n* × 1 vectors, any *m* × *n* matrix *T* can be viewed
    as a transform. If ![](../../OEBPS/Images/AR_v.png) belongs to the set *S*, multiplication
    with the matrix *T* will map (transform) ![](../../OEBPS/Images/AR_v.png) to a
    vector *T*![](../../OEBPS/Images/AR_v.png). We will later see that matrix multiplication
    is a subclass of transforms that preserve collinearity—points that lie on a straight
    line before the transformation will continue to lie on a (possibly different)
    straight line post the transformation. For instance, consider the matrix
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-14-o.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
- en: 'In section [2.14](02.xhtml#sec-rotation-matrices-eigen), we will see that this
    is a special kind of matrix called a rotation matrix; for now, simply consider
    it an example of a matrix. *R* is a transformation operator that maps a point
    in a 2D plane to another point in the same plane. In mathematical notation, *R*
    : ℝ² → ℝ². In fact, as depicted in figure [2.14](02.xhtml#fig-rotation_matrix_diagram),
    this transformation (multiplication by matrix *R*) rotates the position vector
    of a point in the 2D plane by an angle of 45°.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: The output and input points may belong to different spaces in such transforms.
    For instance, consider the matrix
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-14-p.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
- en: 'It is not hard to see that this matrix projects 3D points to the 2D X-Y plane:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-14-q.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
- en: 'Hence, this transformation (multiplication by matrix *P*) projects points from
    three to two dimensions. In mathematical parlance, *P* : ℝ³ → ℝ².'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: 'The transforms *R* and *P* share a common property: *they preserve collinearity*.
    This means a set of vectors (points) ![](../../OEBPS/Images/AR_a.png), ![](../../OEBPS/Images/AR_b.png),
    ![](../../OEBPS/Images/AR_c.png), ⋯ that originally lay on a straight line remain
    so after the transformation.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s check this out for the rotation transformation in the example from section
    [2.9](02.xhtml#sec-lin-dep). There we saw four vectors:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-14-r.png)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
- en: 'These vectors all lie on a straight *L* : *x* = *y*. The rotation transformed
    versions of these vectors are'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-14-s.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
- en: It is trivial to see that the transformed vectors also lie on a (different)
    straight line. In fact, ![](../../OEBPS/Images/AR_osm.png)^′, ![](../../OEBPS/Images/AR_a.png)^′,
    ![](../../OEBPS/Images/AR_b.png)^′, ![](../../OEBPS/Images/AR_c.png)^′ lie on
    the *Y*-axis, which is the 45° rotated version of the original line *y* = *x*.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: It is trivial to see that the transformed vectors also lie on a (different)
    straight line. In fact, ![](../../OEBPS/Images/AR_osm.png)^′, ![](../../OEBPS/Images/AR_a.png)^′,
    ![](../../OEBPS/Images/AR_b.png)^′, ![](../../OEBPS/Images/AR_c.png)^′ lie on
    the *Y*-axis, which is the 45° rotated version of the original line *y* = *x*.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: 'The projection transform represented by matrix *P* also preserves collinearity.
    Consider four collinear vectors in 3D:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-14-t.png)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
- en: The corresponding transformed vectors
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-14-u.png)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
- en: also lie on a straight line in 2D.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: The class of transforms that preserves collinearity are known as *linear transforms*.
    They can always be represented as a matrix multiplication. Conversely, all matrix
    multiplications represent a linear transformation. A more formal definition is
    provided later.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: 2.10.1 Generic multidimensional definition of linear transforms
  id: totrans-363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A function *ϕ* is a linear transform if and only if it satisfies
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '*ϕ*(*α*![](../../OEBPS/Images/AR_a.png) + *β*![](../../OEBPS/Images/AR_b.png))
    = *αϕ*(![](../../OEBPS/Images/AR_a.png)) + *βϕ*(![](../../OEBPS/Images/AR_b.png))
    ∀ *α*, *β* ∈ ℝ'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: Equation 2.15
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, *a transform is linear if and only if the transform of the
    linear combination of two vectors is the same as the linear combination (with
    the same weights) of the transforms of individual vectors*. (This can be remembered
    as: *Linear transform means transforms of linear combinations are same as linear
    combinations of transforms*.) Multiplication with a rotation or projection matrix
    (shown earlier) is a linear transform.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: 2.10.2 All matrix-vector multiplications are linear transforms
  id: totrans-368
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s verify that matrix multiplication satisfies the definition of linear mapping
    (equation [2.15](02.xhtml#eq-lin-map)). Let ![](../../OEBPS/Images/AR_a.png),
    ![](../../OEBPS/Images/AR_b.png) ∈ ℝ*^n* be two arbitrary *n*-dimensional vectors
    and *A[m, n]* be an arbitrary matrix with *n* columns. Then following the standard
    rules of matrix-vector multiplication,
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '*A*(*α*![](../../OEBPS/Images/AR_a.png) + *β*![](../../OEBPS/Images/AR_b.png))
    = *α*(*A*![](../../OEBPS/Images/AR_a.png)) + *β*(*A*![](../../OEBPS/Images/AR_b.png))'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: which mimics equation [2.15](02.xhtml#eq-lin-map) with *ϕ* replaced with matrix
    *A*. Thus we have proven that all matrix multiplications are linear transforms.
    The reverse is not true. In particular, linear transforms that operate on infinite-dimensional
    vectors are not matrices. But all linear transforms that operate on finite-dimensional
    vectors can be expressed as matrices. (The proof is a bit more complicated and
    will be skipped.)
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: Thus, in finite dimensions, multiplication with a matrix and linear transformation
    are one and the same thing. In section [2.3](02.xhtml#sec-matrices), we saw the
    array view of matrices. The corresponding geometric view, that all matrices represent
    linear transformation, was presented in this section.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: Let’s finish this section by studying an example of a transform that is *not*
    linear. Consider the function
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '*ϕ*(![](../../OEBPS/Images/AR_x.png)) = ||![](../../OEBPS/Images/AR_x.png)||'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: 'for ![](../../OEBPS/Images/AR_x.png) ∈ ℝ*^n*. This function *ϕ* maps *n*-dimensional
    vectors to a scalar that is the length of the vector, *ϕ* : ℝ*^n* → ℝ. We will
    examine if it satisfies equation [2.15](02.xhtml#eq-lin-map) with *α*[1] = *α*[2]
    = 1. For two specific vectors ![](../../OEBPS/Images/AR_a.png), ![](../../OEBPS/Images/AR_b.png)
    ∈ ℝ*^n*,'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-15-a1.png)'
  id: totrans-376
  prefs: []
  type: TYPE_IMG
- en: Now
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-15-b.png)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
- en: and
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-15-c.png)'
  id: totrans-380
  prefs: []
  type: TYPE_IMG
- en: 'Clearly, these two are not equal; hence, we have violated equation [2.15](02.xhtml#eq-lin-map):
    *ϕ* is a nonlinear mapping.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: 2.11 Multidimensional arrays, multilinear transforms, and tensors
  id: totrans-382
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We often hear the term *tensor* in connection with machine learning. Google’s
    famous machine learning platform is named *TensorFlow*. In this section, we will
    introduce you to the concept of a tensor.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: '2.11.1 Array view: Multidimensional arrays of numbers'
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A tensor may be viewed as a generalized *n*-dimensional array—although, strictly
    speaking, not all multidimensional arrays are tensors. We will learn more about
    the distinction between multidimensional arrays and tensors when we study multilinear
    transforms. For now, we will not worry too much about the distinction. A vector
    can be viewed as a 1 tensor, a matrix is a 2 tensor, and a scalar is a 0 tensor.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: 'In section [2.3](02.xhtml#sec-matrices), we saw that digital images are represented
    as 2D arrays (matrices). A color image—where each pixel is represented by three
    colors, R, G, and B (red, green, and blue)—is an example of a multidimensional
    array or tensor. This is because it can be viewed as a combination of three images:
    the R, G, and B images, respectively.'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: The inputs and outputs to each layer in a neural network are also tensors.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: 2.12 Linear systems and matrix inverse
  id: totrans-388
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Machine learning today is usually an iterative process. Given a set of training
    data, you want to estimate a set of machine parameters that will yield target
    values (or close approximations to them) on training inputs. The number of training
    inputs and the size of the parameter set are often very large. This makes it impossible
    to have a closed-form solution where we solve for the unknown parameters in a
    single step. Solutions are usually iterative. We start with a guessed set of values
    for the parameters and iteratively improve the guess by processing training data.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: Having said that, we often encounter smaller problems in real life. We are better
    off using more traditional closed-form techniques here since they are much faster
    and more accurate. This section is devoted to gaining some insights into these
    techniques.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go back to our familiar cat-brain problem and refer to its training data
    in table [2.2](02.xhtml#tab-cat-brain-training-data). As before, we are still
    talking about a weighted sum model with three parameters: weights *w*[0], *w*[1]
    and bias *b*. Let’s focus on the top three rows from the table, repeated here
    in table [2.2](02.xhtml#tab-cat-brain-training-data-trunc) for convenience.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: Table 2.3 Example training dataset for our toy machine learning–based cat brain
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Input value: Hardness | Input value: Sharpness | Output: Threat score
    |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0.11 | 0.09 | −0.8 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.01 | 0.02 | −0.97 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.98 | 0.91 | 0.89 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
- en: The training data says that with a hardness value 0.11 and a sharpness value
    0.09, we expect the system’s output to match or closely approximate) the target
    value −0.8, and so on. In other words, our estimated values for parameters *w*[0],
    *w*[1], *b* should ideally satisfy
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: 0.11*w*[0] + 0.09*w*[1] + b = –0.8
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: 0.01*w*[0] + 0.02*w*[1] + b = –0.97
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: 0.98*w*[0] + 0.91 *w*[1] + b = 0.89
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: 'We can express this via matrix multiplication as the following equation:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-15-d.png)'
  id: totrans-403
  prefs: []
  type: TYPE_IMG
- en: How do we obtain the values of *w*[0], *w*[1], *b* that make this equation true?
    That is, how do we solve this equation? There are formal methods (discussed later)
    to directly solve such equations for *w*[0], *w*[1], and *b* (in this very simple
    example, you might just “see” that *w*[0] = 1, *w*[1] = 1, *b* = −1 solves the
    equation, but we need a general method).
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: This equation is an example of a class of equations called a *linear system*.
    A linear system in *n* unknowns *x*[1], *x*[2], *x*[3], ⋯, *x[n]*,
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: '*a*[11]*x*[1] + *a*[12]*x*[2] + *a*[13]*x*[3] + … + *a*[1*n*]*x[n]* = *b*[1]'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: '*a*[21]*x*[1] + *a*[22]*x*[2] + *a*[23]*x*[3] + … + *a*[2*n*]*x[n]* = *b*[2]'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: ⁞
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: '*a*[*n*1]*x*[1] + *a*[*n*2]*x*[2] + *a*[*n*3]*x*[3] + … + *a[nn]**x[n]* = *b[n]*'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: can be expressed via matrix and vectors as
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: '*A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: where
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-15-e.png)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
- en: 'Although equivalent, the matrix depiction is more compact and dimension-independent.
    In machine learning, we usually have many variables (thousands), so this compactness
    makes a significant difference. Also, *A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)
    looks similar to the one-variable equation we know so well: *ax* = *b*. In fact,
    many intuitions can be transferred from 1D to higher dimensions.'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: 'What is the solution to the 1D equation? You may have learned it in fifth grade:
    The solution of *ax* = *b* is *x* = *a*^(−1)*b* where *a*^(−1) = 1/*a*, *a* ≠
    0.'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the same notation in all dimensions. The solution of *A*![](../../OEBPS/Images/AR_x.png)
    = ![](../../OEBPS/Images/AR_b.png) is ![](../../OEBPS/Images/AR_x.png) = *A*^(−1)![](../../OEBPS/Images/AR_b.png),
    where *A*^(−1) is the matrix inverse. The inverse matrix *A*^(−1) has the determinant
    of the matrix, 1/*det*(*A*), as a factor. We will not discuss determinant and
    inverse matrix computation here—you can obtain that in any standard linear algebra
    textbook—but will state some facts that lend insights into determinants and inverse
    matrices:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: The inverse matrix *A*^(−1) is related to matrix *A* in the same way the scalar
    *a*^(−1) is related to the scalar *a*. *a*^(−1) exists if and only if *a* ≠ 0.
    Analogously, *A*^(−1) exists if *det*(*A*) ≠ 0, where *det*(*A*) refers to the
    determinant of a matrix.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The product of a scalar *a* and its inverse *a*^(−1) is 1. Analogously, *AA*^(−1)
    = *A*^(−1)*A* = **I**, where **I** denotes the identity matrix that is the higher-dimension
    analog for 1 in scalar arithmetic. It is a matrix in which the diagonal terms
    are 1 and all other terms are 0. The *n*-dimensional identity matrix is as follows:'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-15-f.png)'
  id: totrans-419
  prefs: []
  type: TYPE_IMG
- en: When there is no subscript, the dimensionality can be inferred from the context.
    For any matrix *A*, **I***A* = *A***I** = *A*. For any vector ![](../../OEBPS/Images/AR_a.png),
    **I**![](../../OEBPS/Images/AR_a.png) = ![](../../OEBPS/Images/AR_a.png)*^T***I**
    = ![](../../OEBPS/Images/AR_a.png). These can be easily verified using the rules
    of matrix multiplication.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: There are completely precise but tedious rules for computing determinants and
    matrix inverses. Despite the importance of the concept, we rarely need to compute
    them in life as all linear algebra software packages provide routines to do this.
    Furthermore, computing matrix inverses is not good programming practice because
    it is numerically unstable. We will not discuss the direct computation of determinant
    or matrix inverse here (except that in section [A.2](../Text/A.xhtml#sec-det2x2)
    of the appendix, we show how to compute the determinant of a 2 × 2 matrix). We
    will discuss pseudo-inverses, which have more significance in machine learning.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: 2.12.1 Linear systems with zero or near-zero determinants,and ill-conditioned
    systems
  id: totrans-422
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We saw earlier that a linear system *A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)
    has the solution ![](../../OEBPS/Images/AR_x.png) = *A*^(−1)![](../../OEBPS/Images/AR_b.png).
    But *A*^(−1) has 1/*det*(*A*) as a factor. What if the determinant is zero?
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: 'The short answer: when the determinant is zero, the linear system cannot be
    exactly solved. We may still attempt to come up with an approximate answer (see
    section [2.12.3](02.xhtml#subsec-over-under-determined-linsys)), but an exact
    solution is not possible.'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s examine the situation a bit more closely with the aid of an example.
    Consider the following system of equations:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: '*  x*[1] + *x*[2]   = 2'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: 2*x*[1] + 2*x*[2]  = 4
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: 'It can be rewritten as a linear system with a square matrix:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-15-g.png)'
  id: totrans-429
  prefs: []
  type: TYPE_IMG
- en: 'But you can quickly see that the system of equations cannot be solved. The
    second equation is really the same as the first. In fact, we can obtain the second
    by multiplying the first by a scalar, 2. Hence, we don’t really have two equations:
    we have only one, so the system cannot be solved. Now examine the row vectors
    of matrix *A*. They are [1   1] and [2   2]. They are linearly dependent because
    −2[1   1] + [2   2] = 0. Now examine the determinant of matrix *A* (section [A.2](../Text/A.xhtml#sec-det2x2)
    of the appendix shows how to compute the determinant of a 2 × 2 matrix). It is
    2 × 1 − 1 × 2 = 0. These results are not coincidences. Any one of them implies
    the other. In fact, the following statements about the linear system *A*![](../../OEBPS/Images/AR_x.png)
    = ![](../../OEBPS/Images/AR_b.png) (with a square matrix) are equivalent:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: Matrix *A* has a row/column that can be expressed as a weighted sum of the others.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matrix *A* has linearly dependent rows or columns.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matrix *A* has zero determinant (such matrices are called *singular* matrices).
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The inverse of matrix *A* (i.e., *A*^(−1)) does not exist. *A* is called *singular*.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The linear system cannot be solved.
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The system is trying to tell you that you have fewer equations than you think
    you have, and you cannot solve the system of equations.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes the determinant is not exactly zero but close to zero. Although solvable
    in theory, such systems are *numerically unstable*. Small changes in input cause
    the result to change drastically. For instance, consider this nearly singular
    matrix:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-16.png)'
  id: totrans-438
  prefs: []
  type: TYPE_IMG
- en: Equation 2.16
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: Its determinant is 0.002, close to zero. Let ![](../../OEBPS/Images/eq_02-16-a.png)
    be a vector.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-17.png)'
  id: totrans-441
  prefs: []
  type: TYPE_IMG
- en: Equation 2.17
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: (Note how large the elements of *A*^(−1) are. This is due to division by an
    extremely small determinant and, in turn, causes the instability illustrated next.)
    The solution to the equation ![](../../OEBPS/Images/eq_02-17-a.png). But if we
    change ![](../../OEBPS/Images/AR_b.png) just a little and make ![](../../OEBPS/Images/eq_02-17-b.png),
    the solution changes to a drastically different ![](../../OEBPS/Images/eq_02-17-c.png).
    This is inherently unstable and arises from the near singularity of the matrix
    *A*. Such linear systems are called *ill-conditioned*.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: 2.12.2 PyTorch code for inverse, determinant, and singularity testing of matrices
  id: totrans-444
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Inverting a matrix and computing its determinant can be done with a single function
    call from the linear algebra package linalg.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.10 Matrix inverse for an invertible matrix (nonzero determinant)
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ① ![](../../OEBPS/Images/eq_02-17-d.png)
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: ② ![](../../OEBPS/Images/eq_02-17-e.png)
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: ③ The PyTorch function *torch*.*eye*(*n*) generates an identity matrix *I* of
    size *n*
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: ④ Verify ![](../../OEBPS/Images/eq_02-17-f.png)
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ **I** is like 1. Verify *A***I** = **I***A* = *A*
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: A singular matrix is a matrix whose determinant is zero. Such matrices are non-invertible.
    Linear systems of equations with singular matrices cannot be solved.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.11 Singular matrix
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ① ![](../../OEBPS/Images/eq_02-17-g.png)
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: ② Determinant = 1 × 2 − 2 × 1 = 0. Singular matrix; attempting to compute the
    inverse causes a runtime error.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: 2.12.3 Over- and under-determined linear systems in machine learning
  id: totrans-458
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'What if the matrix *A* is *not* square? This implies that the number of equations
    does not match the number of unknowns. Does such a system even make sense? Surprisingly,
    it does. As a rule, machine learning systems fall in this category: the number
    of equations corresponds to the number of training data instances collected, while
    the number of unknowns is a function of the number of weights in the model which
    is a function of the particular model family chosen to represent the system. These
    are independent of each other. As stated earlier, we often solve these systems
    iteratively. Nonetheless, it is important to understand linear systems with nonsquare
    matrices *A*, to gain insight.'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two possible cases, assuming that the matrix *A* is *m* × *n* (*m*
    rows and *n* columns):'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: 'Case 1: *m* > *n* (more equations than unknowns; overdetermined system)'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Case 2: *m* < *n* (fewer equations than unknown; underdetermined system)'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For instance, table [2.2](02.xhtml#tab-cat-brain-training-data) leads to an
    overdetermined linear system. Let’s write the system of equations:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: 0.11 *w*[0] + 0.09 *w*[1] + b = –0.8
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: 0.01 *w*[0] + 0.02 *w*[1] + b = –0.97
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: 0.98 *w*[0] + 0.91 *w*[1] + b =   0.89
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: 0.12 *w*[0] + 0.21 *w*[1] + b = –0.68
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: 0.98 *w*[0] + 0.99 *w*[1] + b =   0.95
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: 0.85 *w*[0] + 0.87 *w*[1] + b =   0.74
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: 0.03 *w*[0] + 0.14 *w*[1] + b = –0.88
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: 0.55 *w*[0] + 0.45 *w*[1] + b =   0.00
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: 'These yield the following overdetermined linear system:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-18.png)'
  id: totrans-473
  prefs: []
  type: TYPE_IMG
- en: Equation 2.18
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a nonsquare 15 × 3 linear system. There are only 3 unknowns to solve
    for (*w*[0], *w*[1], *b*), and there are 15 equations. This is highly redundant:
    we needed only three equations and could have solved it via linear system solution
    techniques (section [2.12](02.xhtml#sec-lin_systems)). But the important thing
    to note is this: *the equations are not fully consistent*. There is no single
    set of values for the unknown that will satisfy all of them. In other words, the
    training data is noisy—an almost universal occurrence in real-life machine learning
    systems. Consequently, we have to find a solution that is optimal (causes as little
    error as possible) over all the equations.'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: We want to solve it such that the overall error ||*A*![](../../OEBPS/Images/AR_x.png)
    − ![](../../OEBPS/Images/AR_b.png)|| is minimized. In other words, we are looking
    for ![](../../OEBPS/Images/AR_x.png) such that *A*![](../../OEBPS/Images/AR_x.png)
    is as close to ![](../../OEBPS/Images/AR_b.png) as possible. This closed-form
    that is, non-iterative) method is an extremely important precursor to machine
    learning and data science. We will revisit this multiple times, most notably in
    sections [2.12.4](02.xhtml#subsec-moore-penrose-pseudoinverse) and [4.5](../Text/04.xhtml#sec-svd).
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: 2.12.4 Moore Penrose pseudo-inverse of a matrix
  id: totrans-477
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The pseudo-inverse is a handy technique to solve over- or under-determined
    linear systems. Suppose we have an overdetermined system with the not-necessarily
    square *m* × *n* matrix *A*:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: '*A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: 'Since *A* is not guaranteed to be square, we can take neither the determinant
    nor the inverse in general. So the usual *A*^(−1)![](../../OEBPS/Images/AR_b.png)
    does not work. At this point, we observe that although the inverse cannot be taken,
    transposing the matrix is always possible. Let’s multiply both sides of the equation
    with *A^T*:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: '*A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png) ⇔ *A^TA*![](../../OEBPS/Images/AR_x.png)
    = *A^T*![](../../OEBPS/Images/AR_b.png)'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that *A^TA* is a square matrix: its dimensions are (*m*×*n*) × (*n*×*m*)
    = *m* × *m*. Let’s assume, without proof for the moment, that it is invertible.
    Then'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: '*A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png) ⇔ *A^TA*![](../../OEBPS/Images/AR_x.png)
    = *A^T*![](../../OEBPS/Images/AR_b.png) ⇔ ![](../../OEBPS/Images/AR_x.png) = (*A^TA*)^(−1)*A^T*![](../../OEBPS/Images/AR_b.png)'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: Hmmm, not bad; we seem to be onto something. In fact, we just derived the *pseudo-inverse*
    of matrix *A*, denoted *A* ^+ = (*A^TA*)^(−1)*A^T*. Unlike the inverse, the pseudo-inverse
    does not need the matrix to be square with linearly independent rows. Much like
    the regular linear system, we get the solution of the (possibly nonsquare) system
    of equations as *A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)
    ⇔ ![](../../OEBPS/Images/AR_x.png) = *A* ^+ ![](../../OEBPS/Images/AR_b.png).
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: The pseudo-inverse-based solution actually minimizes the error ||*A*![](../../OEBPS/Images/AR_x.png)
    − ![](../../OEBPS/Images/AR_b.png)||. We will provide an intuitive proof of that
    in section [2.12.5](02.xhtml#subsec-pseudo-inv-geometric-intuition). Meanwhile,
    you are encouraged to write the Python code to evaluate (*A^TA*)^(−1)*A^T*![](../../OEBPS/Images/AR_b.png)
    and verify that it approximately yields the expected answer ![](../../OEBPS/Images/eq_02-18-a.png)
    for equation [2.18](02.xhtml#eq-overdetemined_system_example).
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: '2.12.5 Pseudo-inverse of a matrix: A beautiful geometric intuition'
  id: totrans-486
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A matrix *A*[*m* × *n*] can be rewritten in terms of its column vectors as [![](../../OEBPS/Images/AR_a.png)[1],
    ![](../../OEBPS/Images/AR_a.png)[2], … ![](../../OEBPS/Images/AR_a.png)*[n]*],
    where ![](../../OEBPS/Images/AR_a.png)[1] … ![](../../OEBPS/Images/AR_a.png)*[n]*
    are all *m*-dimensional vectors. Then if ![](../../OEBPS/Images/eq_02-18-b.png),
    we get *A*![](../../OEBPS/Images/AR_x.png) = *x*[1]![](../../OEBPS/Images/AR_a.png)[1]
    + *x*[2]![](../../OEBPS/Images/AR_a.png)[2] + ⋯ *x[n]*![](../../OEBPS/Images/AR_a.png)*[n]*.
    In other words, *A*![](../../OEBPS/Images/AR_x.png) is just a linear combination
    of the column vectors of *A* with the elements of ![](../../OEBPS/Images/AR_x.png)
    as the weights (you are encouraged to write out a small 3 × 3 system and verify
    this). The space of all vectors of the form *A*![](../../OEBPS/Images/AR_x.png)
    (that is, the linear span of the column vectors of *A*) is known as the *column
    space* of *A*.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution to the linear system of equations *A*![](../../OEBPS/Images/AR_x.png)
    = ![](../../OEBPS/Images/AR_b.png) can be viewed as finding the ![](../../OEBPS/Images/AR_x.png)
    that minimizes the difference of *A*![](../../OEBPS/Images/AR_x.png) and ![](../../OEBPS/Images/AR_b.png):
    that is, minimizes ||*A*![](../../OEBPS/Images/AR_x.png) − ![](../../OEBPS/Images/AR_b.png)||.
    This means we are trying to find a point in the column space of *A* that is closest
    to the point ![](../../OEBPS/Images/AR_b.png). Note that this interpretation does
    not assume a square matrix *A*. Nor does it assume a nonzero determinant. In the
    friendly case where the matrix *A* is square and invertible, we can find a vector
    ![](../../OEBPS/Images/AR_x.png) such that *A*![](../../OEBPS/Images/AR_x.png)
    becomes exactly equal to ![](../../OEBPS/Images/AR_b.png), which makes ||*A*![](../../OEBPS/Images/AR_x.png)
    − ![](../../OEBPS/Images/AR_b.png)|| = 0. If *A* is not square, we will try to
    find ![](../../OEBPS/Images/AR_x.png) such that *A*![](../../OEBPS/Images/AR_x.png)
    is closer to ![](../../OEBPS/Images/AR_b.png) than any other vector in the column
    space of *A*. Mathematically speaking, [⁴](02.xhtml#fn7)'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-19.png)'
  id: totrans-489
  prefs: []
  type: TYPE_IMG
- en: Equation 2.19
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: From geometry, we intuitively know that the closest point to ![](../../OEBPS/Images/AR_b.png)
    in the column space of *A* is obtained by dropping a perpendicular from ![](../../OEBPS/Images/AR_b.png)
    to the column space of *A* (see figure [2.12](02.xhtml#fig-nonsquare_linear_system_diagram)).
    The point where this perpendicular meets the column space is called the *projection*
    of ![](../../OEBPS/Images/AR_b.png) on the column space of *A*. The solution vector
    ![](../../OEBPS/Images/AR_x.png) to equation [2.19](02.xhtml#eq-linear_system_minimization_problem)
    that we are looking for should correspond to the projection of ![](../../OEBPS/Images/AR_b.png)
    on the column space of *A*. This in turn means ![](../../OEBPS/Images/AR_b.png)
    − *A*![](../../OEBPS/Images/AR_x.png) is orthogonal (perpendicular) to all vectors
    in the column space of *A* (see figure [2.12](02.xhtml#fig-nonsquare_linear_system_diagram)).
    We represent arbitrary vectors in the column space of *A* as *A*![](../../OEBPS/Images/AR_y.png)
    for arbitrary ![](../../OEBPS/Images/AR_y.png). Hence, for all such ![](../../OEBPS/Images/AR_y.png),
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-19-a.png)'
  id: totrans-492
  prefs: []
  type: TYPE_IMG
- en: For the previous equation to be true for all vectors ![](../../OEBPS/Images/AR_y.png),
    we must have *A^T*(![](../../OEBPS/Images/AR_b.png)−*A*![](../../OEBPS/Images/AR_x.png))
    = 0.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we have
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-19-b.png)'
  id: totrans-495
  prefs: []
  type: TYPE_IMG
- en: which is exactly the Moore-Penrose pseudo-inverse.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH02_F12_Chaudhury.png)'
  id: totrans-497
  prefs: []
  type: TYPE_IMG
- en: Figure 2.12 Solving a linear system *A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)
    is equivalent to finding the point on the column space of *A* that is closest
    to ![](../../OEBPS/Images/AR_b.png). This means we have to drop a perpendicular
    from ![](../../OEBPS/Images/AR_b.png) to column space of *A*. If *A*![](../../OEBPS/Images/AR_x.png)
    represents the point where that perpendicular meets the column space (aka projection),
    the difference vector ![](../../OEBPS/Images/AR_b.png) − *A*![](../../OEBPS/Images/AR_x.png)
    corresponds to the line joining ![](../../OEBPS/Images/AR_b.png) and its projection
    *A*![](../../OEBPS/Images/AR_x.png). This line will be perpendicular to all vectors
    in the column space of *A*. Equivalently, it is perpendicular to *A*![](../../OEBPS/Images/AR_y.png)
    for any arbitrary ![](../../OEBPS/Images/AR_y.png).
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: For a machine learning-centric example, consider the overdetermined system corresponding
    to the cat brain earlier in the chapter. There are 15 training examples, each
    with input and desired outputs specified.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: Our goal is to determine three unknowns *w*[0], *w*[1], and *b* such that for
    each training input ![](../../OEBPS/Images/eq_02-19-c.png), the model output
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-20.png)'
  id: totrans-501
  prefs: []
  type: TYPE_IMG
- en: Equation 2.20
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
- en: matches the desired output (aka ground truth) *ȳ[i]* as closely as possible.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE We employed a neat trick here: we added a 1 to the right of the input,
    which allows us to depict the entire system (including the bias) in a single compact
    matrix-vector multiplication. We call this *augmentation*—we augment the input
    row vector with an extra 1 on the right.'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: Collating all the training examples together, we get
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-21.png)'
  id: totrans-506
  prefs: []
  type: TYPE_IMG
- en: Equation 2.21
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: which can be expressed compactly as
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
- en: '*X*![](../../OEBPS/Images/AR_w.png) = ![](../../OEBPS/Images/AR_y.png)'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: where *X* is the augmented input matrix with a rightmost column of all 1s. The
    goal is to minimize ||![](../../OEBPS/Images/AR_y.png) – ![](../../OEBPS/Images/AR_y2.png)||.
    To this end, we formulate the over-determined linear system
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
- en: '*X*![](../../OEBPS/Images/AR_w.png) = ![](../../OEBPS/Images/AR_y2.png)'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
- en: Note that *this is not a classic system of equations—it has more equations than
    unknowns*. We cannot solve this via matrix inversion. We *can*, however, use the
    pseudo-inverse mechanism to solve it. The resulting solution yields the “best
    fit” or “best effort” solution, which minimizes the total error over all the training
    examples.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: The exact numerical system (repeated here for ease of reference) is
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-22.png)'
  id: totrans-514
  prefs: []
  type: TYPE_IMG
- en: Equation 2.22
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
- en: We solve for ![](../../OEBPS/Images/AR_w.png) using the pseudo-inverse formula
    ![](../../OEBPS/Images/AR_w.png) = (*X^TX*)^(–1)*X^T*![](../../OEBPS/Images/AR_y2.png)
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: 2.12.6 PyTorch code to solve overdetermined systems
  id: totrans-517
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: NOTE Fully functional code for this section, executable via Jupyter Notebook,
    can be found at [http://mng.bz/PPJ2](http://mng.bz/PPJ2).
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.12 Solving an overdetermined system using the pseudo-inverse
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-520
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ① X is the augmented data matrix from equation [2.22](02.xhtml#eq-lin-model)
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
- en: ② The Pytorch column stack operator
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
- en: adds a column to a matrix. Here, the added column is all 1s
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
- en: '③ It is easy to verify that the solution to equation [2.22](02.xhtml#eq-lin-model)
    is roughly *w*[0] = 1, *w*[1] = 1, *b* = −1. But the equations are not consistent:
    no one solution perfectly fits all of them. The pseudo-inverse finds the “best
    fit” solution: it minimizes total error for all the equations.'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
- en: ④ Expect the solution to be close to [1, 1, −1]
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ The solution is [1.08, 0.90, −0.96]
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
- en: '2.13 Eigenvalues and eigenvectors: Swiss Army knives of machine learning'
  id: totrans-527
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Machine learning and data science are all about finding patterns in large volumes
    of high-dimensional data. The inputs are feature vectors introduced in section
    [2.1](02.xhtml#sec-vectors)) in high-dimensional spaces. Each feature vector can
    be viewed as a point in the feature space descriptor for an input instance. Sometimes
    we transform these feature vectors—map the feature points to a friendlier space—to
    simplify the data by reducing dimensionality. This is done by eliminating axes
    along which there is scant variation in the data. Eigenvalues and eigenvectors
    are invaluable tools in the arsenal of a machine learning engineer or a data scientist
    for this purpose. In chapter [4](../Text/04.xhtml#chap-linalg-tools-ml), we will
    study how to use these tools to simplify and find broad patterns in a large volume
    of multidimensional data.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take an informal look at eigenvectors first. They are properties of square
    matrices. As seen earlier, matrices can be viewed as linear transforms which map
    vectors (points) in one space to different vectors (points) in the same or a different
    space. But a typical linear transform leaves a few points in the space (almost)
    unaffected. These points are called *eigenvectors*. They are important physical
    aspects of the transform. Let’s look at a simple example. Suppose we are *rotating*
    points in 3D space about the *Z*-axis (see figure [2.13](02.xhtml#fig-rotation_about_z_diagram)).
    The points on the *Z*-axis will stay where they were despite the rotation. In
    general, points on the axis of rotation (*Z* in this case) do not go anywhere
    after rotation. The axis of rotation is an eigenvector of the rotation transformation.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH02_F13_Chaudhury.png)'
  id: totrans-530
  prefs: []
  type: TYPE_IMG
- en: Figure 2.13 During rotation, points on the axis of rotation do not change position.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
- en: Extending this idea, when *transforming* vectors ![](../../OEBPS/Images/AR_x.png)
    with a matrix *A*, are there vectors that do not change, at least in direction?
    Turns out the answer is yes. These are the so-called *eigenvectors*—they do not
    change direction when undergoing linear transformation by a matrix *A*. To be
    precise, if ![](../../OEBPS/Images/AR_e.png) is an eigenvector of the square matrix
    *A*, [⁵](02.xhtml#fn8) then
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: '*A![](../../OEBPS/Images/AR_e.png)* = *λ![](../../OEBPS/Images/AR_e.png)*'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
- en: Thus the linear transformation (that is, multiplication by matrix *A*) has changed
    the length but not the direction of ![](../../OEBPS/Images/AR_e.png) because *λ*![](../../OEBPS/Images/AR_e.png)
    is parallel to ![](../../OEBPS/Images/AR_e.png).
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: How do we obtain *λ* and ![](../../OEBPS/Images/AR_e.png)? Well,
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
- en: '*A*![](../../OEBPS/Images/AR_e.png) = *λ* ![](../../OEBPS/Images/AR_e.png)'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: ⇔ *A*![](../../OEBPS/Images/AR_e.png) - *λ*![](../../OEBPS/Images/AR_e.png)
    = ![](../../OEBPS/Images/AR_0.png)
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
- en: ⇔ (*A* - *λ* **I**)![](../../OEBPS/Images/AR_e.png) = ![](../../OEBPS/Images/AR_0.png)
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
- en: where **I** denotes the identity matrix.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
- en: Of course, we are only interested in nontrivial solutions, where ![](../../OEBPS/Images/AR_e.png)
    ≠ ![](../../OEBPS/Images/AR_0.png). In that case, *A* – *λ***I** cannot be invertible,
    because if it were, we could obtain the contradictory solution ![](../../OEBPS/Images/AR_e.png)
    = (*A* – *λ* **I**)^(–1) ![](../../OEBPS/Images/AR_0.png) = ![](../../OEBPS/Images/AR_0.png).
    Thus, (*A* − *λ***I**) is non-invertible, implying the determinant
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
- en: '*det*(*A* − *λ***I**) = 0'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
- en: For an *n* × *n* matrix *A*, this yields an *n*th-degree polynomial equation
    with *n* solutions for the unknown *λ*. *Thus, an *n* × *n* matrix has n eigenvalues,
    not necessarily all distinct*.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s compute eigenvalues and eigenvectors of a 3 × 3 matrix, just for kicks.
    The matrix we use is carefully chosen, as will be evident soon. But for now, think
    of it as an arbitrary matrix:'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-23.png)'
  id: totrans-544
  prefs: []
  type: TYPE_IMG
- en: Equation 2.23
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
- en: 'We will compute the eigenvalues and eigenvectors of *A*:'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-23-a.png)'
  id: totrans-547
  prefs: []
  type: TYPE_IMG
- en: Thus,
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-23-b.png)'
  id: totrans-549
  prefs: []
  type: TYPE_IMG
- en: Here, *i* = √–1. If necessary, you are encouraged to refresh your memory of
    imaginary and complex numbers from high school algebra.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we have found (as expected) three eigenvalues: 1, *e*^(*i* *π*/4), and
    *e*^(–*i* *π*/4). Each of them will yield one eigenvector. Let’s compute the eigenvector
    corresponding to the eigenvalue of 1 by way of example:'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-23-c.png)'
  id: totrans-552
  prefs: []
  type: TYPE_IMG
- en: Thus, ![](../../OEBPS/Images/eq_02-23-d.png) is an eigenvector for the eigenvalue
    1 for matrix A. So is ![](../../OEBPS/Images/eq_02-23-e.png) for any real *k*.
    In fact, if *λ*, ![](../../OEBPS/Images/AR_e.png) is an eigenvalue, eigenvector
    pair for matrix *A*, then
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
- en: '*A![](../../OEBPS/Images/AR_e.png)* = *λ![](../../OEBPS/Images/AR_e.png)* ⇔
    *A*(*k![](../../OEBPS/Images/AR_e.png)*) = *λ*(*k![](../../OEBPS/Images/AR_e.png)*)'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
- en: That is, *λ*, (*k![](../../OEBPS/Images/AR_e.png)*) is also an eigenvalue, eigenvector
    pair of *A*. In other words, we can only determine the eigenvector up to a fixed
    scale factor. We take the eigenvector to be of unit length (*![](../../OEBPS/Images/AR_e.png)^T![](../../OEBPS/Images/AR_e.png)*
    = 1) without loss of generality.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
- en: The eigenvector for our example matrix turns out to be the *Z*-axis. This is
    not an accident. Our matrix *A* was, in fact, a rotation about the *Z*-axis. *A
    rotation matrix will always have 1 as an eigenvalue. The corresponding eigenvector
    will be the axis of rotation. In 3D, the other two eigenvalues will be complex
    numbers yielding the angle of rotation.* This is detailed in section [2.14](02.xhtml#sec-rotation-matrices-eigen).
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
- en: 2.13.1 Eigenvectors and linear independence
  id: totrans-557
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Two eigenvectors of a matrix corresponding to unequal eigenvalues are linearly
    independent. Let’s prove this to get some insights. Let *λ*[1], ![](../../OEBPS/Images/AR_e.png)[1]
    and *λ*[2], ![](../../OEBPS/Images/AR_e.png)[2] be eigenvalue, eigenvector pairs
    for a matrix *A* with *λ*[1] ≠ *λ*[2]. Then
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
- en: '*A*![](../../OEBPS/Images/AR_e.png)[1] = *λ*[1]![](../../OEBPS/Images/AR_e.png)[1]'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
- en: '*A*![](../../OEBPS/Images/AR_e.png)[2] = *λ*[2]![](../../OEBPS/Images/AR_e.png)[2]'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
- en: If possible, let there be two constants *α*[1] and *α*[2] such that
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
- en: '*α*[1]![](../../OEBPS/Images/AR_e.png)[1] + *α*[2]![](../../OEBPS/Images/AR_e.png)[2]
    = 0'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
- en: Equation 2.24
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
- en: In other words, suppose the two eigenvectors are linearly dependent. We will
    show that this assumption leads to an impossibility.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
- en: Multiplying equation [2.24](02.xhtml#eq-linear-dep-vectors) by *A*, we get
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
- en: '*α*[1]*A*![](../../OEBPS/Images/AR_e.png)[1] + *α*[2]*A*![](../../OEBPS/Images/AR_e.png)[2]
     = 0'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
- en: ⇔ *α*[1]*λ*[1]![](../../OEBPS/Images/AR_e.png)[1] + *α*[2]*λ*[2]![](../../OEBPS/Images/AR_e.png)[2]  =
    0
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
- en: Also, we can multiply equation [2.24](02.xhtml#eq-linear-dep-vectors) by *λ*[2].
    Thus we get
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
- en: '*α*[1]*λ*[1]![](../../OEBPS/Images/AR_e.png)[1] + *α*[2]*λ*[2]![](../../OEBPS/Images/AR_e.png)[2]
    = 0'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
- en: '*α*[1]*λ*[2]![](../../OEBPS/Images/AR_e.png)[1] + *α*[2]*λ*[2]![](../../OEBPS/Images/AR_e.png)[2]
    = 0'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
- en: Subtracting, we get
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
- en: '*α*[1](*λ*[1]-*λ*[2])![](../../OEBPS/Images/AR_e.png)[1] = 0'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
- en: By assumption, *α*[1] ≠ 0, *λ*[1] ≠ *λ*[2] and ![](../../OEBPS/Images/AR_e.png)[1]
    is not all zeros. Thus it is impossible for their product to be zero. Our original
    assumption (the two eigenvectors are linearly dependent) must have been wrong.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
- en: 2.13.2 Symmetric matrices and orthogonal eigenvectors
  id: totrans-574
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Two eigenvectors of a symmetric matrix that correspond to different eigenvalues
    are mutually orthogonal. Let’s prove this to get additional insight. A matrix
    *A* is symmetric if *A^T* = *A*. If *λ*[1], ![](../../OEBPS/Images/AR_e.png)[1]
    and *λ*[2], ![](../../OEBPS/Images/AR_e.png)[2] are eigenvalue, eigenvector pairs
    for a symmetric matrix *A*, then
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
- en: '*A*![](../../OEBPS/Images/AR_e.png)[1] = *λ*[1]![](../../OEBPS/Images/AR_e.png)[1]'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
- en: Equation 2.25
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
- en: '*A*![](../../OEBPS/Images/AR_e.png)[2] = *λ*[2]![](../../OEBPS/Images/AR_e.png)[2]'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
- en: Equation 2.26
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
- en: Transposing equation [2.25](02.xhtml#eq-eigen-vec1),
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_e.png)[1]*^T A^T* = *λ*[1]![](../../OEBPS/Images/AR_e.png)[1]^T'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
- en: Right-multiplying by ![](../../OEBPS/Images/AR_e.png)[2], we get
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_e.png)[1]^T *A^T*![](../../OEBPS/Images/AR_e.png)[2]
    = *λ*[1]![](../../OEBPS/Images/AR_e.png)[1]*^T ![](../../OEBPS/Images/AR_e.png)*[2]'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
- en: ⇔ ![](../../OEBPS/Images/AR_e.png)[1]^T *A*![](../../OEBPS/Images/AR_e.png)[2
      ] = *λ*[1]![](../../OEBPS/Images/AR_e.png)[1]*^T ![](../../OEBPS/Images/AR_e.png)*[2]
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
- en: where the last equation follows from the matrix symmetry. Also, left-multiplying
    equation [2.26](02.xhtml#eq-eigen-vec2) by ![](../../OEBPS/Images/AR_e.png)[1]*^T*,
    we get
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_e.png)[1]*^T A*![](../../OEBPS/Images/AR_e.png)[2]
    = *λ*[2]![](../../OEBPS/Images/AR_e.png)[1]*^T ![](../../OEBPS/Images/AR_e.png)*[2]'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
- en: Thus
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_e.png)[1]*^T A*![](../../OEBPS/Images/AR_e.png)[2]
    = *λ*[1]![](../../OEBPS/Images/AR_e.png)[1]*^T ![](../../OEBPS/Images/AR_e.png)*[2]'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_e.png)[1]*^T A*![](../../OEBPS/Images/AR_e.png)[2]
    = *λ*[2]![](../../OEBPS/Images/AR_e.png)[1]^T ![](../../OEBPS/Images/AR_e.png)[2]'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
- en: Subtracting the equations, we get
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
- en: 0 = (*λ*[1] - *λ*[2]) ![](../../OEBPS/Images/AR_e.png)[1]*^T ![](../../OEBPS/Images/AR_e.png)*[2]
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
- en: Since *λ*[1] ≠ *λ*[2], we must have ![](../../OEBPS/Images/AR_e.png)[1]*^T ![](../../OEBPS/Images/AR_e.png)*[2]
    = 0, which means the two eigenvectors are orthogonal. Thus, if *A* is an *n* ×
    *n* symmetric matrix with eigenvectors ![](../../OEBPS/Images/AR_e.png)[1], ![](../../OEBPS/Images/AR_e.png)[2],
    … ![](../../OEBPS/Images/AR_e.png)*[n]*, then ![](../../OEBPS/Images/AR_e.png)*[i]^T![](../../OEBPS/Images/AR_e.png)[j]*
    = 0 for all *i*, *j* satisfying *λ[i]* ≠ *λ[j]*.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
- en: 2.13.3 PyTorch code to compute eigenvectors and eigenvalues
  id: totrans-593
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: NOTE Fully functional code for this section, executable via Jupyter Notebook,
    can be found at [http://mng.bz/1rEZ](http://mng.bz/1rEZ).
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.13 Eigenvalues and vectors
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-596
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ① ![](../../OEBPS/Images/eq_02-23-f.png) Rotates points in 3D space around the
    *Z*-axis.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
- en: 'The axis of rotation is the *Z*-axis: [0  0  1]*^T*'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
- en: ② Function eig() in the torch linalg package computes eigenvalues and vectors.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
- en: ③ Eigenvalues or vectors can contain complex numbers involving *j* = √-1
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
- en: 2.14 Orthogonal (rotation) matrices and their eigenvalues and eigenvectors
  id: totrans-601
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Of all the transforms, rotation transforms have a special intuitive appeal because
    of their highly observable behavior in the mechanical world. Furthermore, they
    play a significant role in developing and analyzing several machine learning tools.
    In this section, we overview rotation (aka orthogonal) matrices. (Fully functional
    code for the Jupyter notebook for this section can be found at [http://mng.bz/2eNN](http://mng.bz/2eNN).)
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
- en: 2.14.1 Rotation matrices
  id: totrans-603
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Figure [2.14](02.xhtml#fig-rotation_matrix_diagram) shows a point (*x*, *y*)
    rotated about the origin by an angle *θ*. The original point’s position vector
    made an angle *α* with the *X*-axis. Post-rotation, the point’s new coordinates
    are (*x*^′, *y*^′). Note that by definition, rotation does not change the distance
    from the center of rotation; that is what the circle indicates.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH02_F14_Chaudhury.png)'
  id: totrans-605
  prefs: []
  type: TYPE_IMG
- en: Figure 2.14 Rotation in a plane about the origin. By definition, rotation does
    not change the distance from the center of rotation (indicated by the circle).
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
- en: 'Some well-known rotation matrices are as follows:'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
- en: '**Planar rotation by angle *θ* about the origin** (see figure [2.14](02.xhtml#fig-rotation_matrix_diagram)):'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-27.png)'
  id: totrans-609
  prefs: []
  type: TYPE_IMG
- en: Equation 2.27
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
- en: '**Rotation by angle *θ* in 3D space about the *Z*-axis**:'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-28.png)'
  id: totrans-612
  prefs: []
  type: TYPE_IMG
- en: Equation 2.28
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the *z* coordinate remains unaffected by this rotation:'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-28-a.png)'
  id: totrans-615
  prefs: []
  type: TYPE_IMG
- en: This rotation matrix has an eigenvalue of 1, and the corresponding eigenvector
    is the *Z*-axis—you should verify this. This implies that a point on the *Z*-axis
    maps to itself when transformed (rotated) by the previous matrix, which is in
    keeping with the property that the *z* coordinate remains unchanged by this rotation.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
- en: '**Rotation by angle *θ* in 3D space about the *X*-axis**:'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-29.png)'
  id: totrans-618
  prefs: []
  type: TYPE_IMG
- en: Equation 2.29
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the *X* coordinate remains unaffected by this rotation and the *X*-axis
    is an eigenvector of this matrix:'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-29-a.png)'
  id: totrans-621
  prefs: []
  type: TYPE_IMG
- en: '**Rotation by angle *θ* in 3D space about the *Y*-axis**:'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-30.png)'
  id: totrans-623
  prefs: []
  type: TYPE_IMG
- en: Equation 2.30
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the *Y* coordinate remains unaffected by this rotation and the *Y*-axis
    is an eigenvector of this matrix:'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-30-a.png)'
  id: totrans-626
  prefs: []
  type: TYPE_IMG
- en: Listing 2.14 Rotation matrices
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-628
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ① Returns the matrix that performs in-plane 2D rotation by angle theta about
    the origin. Thus, multiplication with this matrix moves a point to a new location.
    The angle between the position vectors of the original and new points is theta
    (figure [2.14](02.xhtml#fig-rotation_matrix_diagram)).
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
- en: ② Returns the matrix that rotates a point in 3D space about the chosen axis
    by angle theta degrees. The axis of rotation can be 0, 1, or 2, corresponding
    to the X-, Y-, or Z-axis, respectively.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
- en: ③ *R*[3*dx*] from equation [2.29](02.xhtml#eq-rot3d-x)
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
- en: ④ *R*[3*dy*] from equation [2.30](02.xhtml#eq-rot3d-y)
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ *R*[3*dz*] from equation [2.28](02.xhtml#eq-rot3d-z)
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.15 Applying rotation matrices
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-635
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ① Creates vector ![](../../OEBPS/Images/AR_u.png) (see figure [2.15](02.xhtml#fig-numpy-rotations))
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
- en: ② *R*[3*dz*] from equation [2.28](02.xhtml#eq-rot3d-z), 45° about *Z*-axis
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
- en: ③ ![](../../OEBPS/Images/AR_v.png) (see figure [2.15](02.xhtml#fig-numpy-rotations))
    is ![](../../OEBPS/Images/AR_u.png) rotated by *R*[3*dz*].
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
- en: ④ *R*[3*dx*] from equation [2.28](02.xhtml#eq-rot3d-z), 45° about *X*-axis
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ ![](../../OEBPS/Images/AR_w.png) (see figure [2.15](02.xhtml#fig-numpy-rotations))
    is ![](../../OEBPS/Images/AR_v.png) rotated by *R*[3*dx*].
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH02_F15_Chaudhury.png)'
  id: totrans-641
  prefs: []
  type: TYPE_IMG
- en: Figure 2.15 Rotation visualized. Here the original vector u is first rotated
    by 45 degrees around the Z-axis to get vector v, which is subsequently rotated
    again by 45 degrees around the X-axis to get vector w.
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
- en: 2.14.2 Orthogonality of rotation matrices
  id: totrans-643
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A matrix *R* is *orthogonal* if and only if it its transpose is also its inverse:
    that is, *R^TR* = *RR^T* = **I**. *All rotations matrices are orthogonal matrices.
    All orthogonal matrices represent some rotation.* For instance:'
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-30-b1.png)'
  id: totrans-645
  prefs: []
  type: TYPE_IMG
- en: You are encouraged to verify, likewise, that all the rotation matrices shown
    here are orthogonal.
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
- en: Orthogonality and length-preservation
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
- en: Orthogonality implies that rotation is length-preserving. Given any vector ![](../../OEBPS/Images/AR_x.png)
    and rotation matrix *R*, let ![](../../OEBPS/Images/AR_y.png) = *R*![](../../OEBPS/Images/AR_x.png)
    be the rotated vector. The lengths (magnitudes) of the two vectors ![](../../OEBPS/Images/AR_x.png),
    ![](../../OEBPS/Images/AR_y.png) are equal since it is easy to see that
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
- en: '||![](../../OEBPS/Images/AR_y.png)|| = ![](../../OEBPS/Images/AR_y.png)*^T*![](../../OEBPS/Images/AR_y.png)
    = (*R*![](../../OEBPS/Images/AR_x.png))*^T*(*R*![](../../OEBPS/Images/AR_x.png))
    = ![](../../OEBPS/Images/AR_x.png)*^TR^TR*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_x.png)*^T***I**![](../../OEBPS/Images/AR_x.png)
    = ![](../../OEBPS/Images/AR_x.png)*^T*![](../../OEBPS/Images/AR_x.png) = ||![](../../OEBPS/Images/AR_x.png)||'
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
- en: From elementary matrix theory, we know that
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
- en: (*AB*)*^T* = *B^TA^T*
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
- en: Negating the angle of rotation
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
- en: Negating the angle of rotation is equivalent to inverting the rotation matrix,
    which is equivalent to transposing the rotation matrix. For instance, consider
    in-plane rotation. Say a point ![](../../OEBPS/Images/AR_x.png) is rotated about
    the origin to vector ![](../../OEBPS/Images/AR_y.png) via matrix *R*. Thus
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-30-c.png)'
  id: totrans-654
  prefs: []
  type: TYPE_IMG
- en: Now we can go back from ![](../../OEBPS/Images/AR_y.png) to ![](../../OEBPS/Images/AR_x.png)
    by rotating by −*θ*. The corresponding rotation matrix is
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-30-d.png)'
  id: totrans-656
  prefs: []
  type: TYPE_IMG
- en: 'In other words, *R^T* inverts the rotation: that is, rotates by the negative
    angle.'
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
- en: 2.14.3 PyTorch code for orthogonality of rotation matrices
  id: totrans-658
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s verify the orthogonality of the rotation matrix by creating one in PyTorch,
    imparting a transpose to it, and verifying that the product of the original matrix
    and the transpose is the identity matrix.
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.16 Orthogonality of rotation matrices
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-661
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ① Creates a rotation matrix, *R*[30]
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
- en: ② The inverse of a rotation matrix is the same as its transpose.
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
- en: ③ Multiplying a rotation matrix and its inverse yields the identity matrix.
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
- en: ④ A vector ![](../../OEBPS/Images/AR_u.png) rotated by matrix *R*[30] to yield
    vector ![](../../OEBPS/Images/AR_v.png), *R*[30]![](../../OEBPS/Images/AR_u.png)
    = ![](../../OEBPS/Images/AR_v.png).
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ The norm of a vector is the same as its length. Rotation preserves the length
    of a vector ||*R![](../../OEBPS/Images/AR_u.png)*|| = ||![](../../OEBPS/Images/AR_u.png)||.
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Rotation by an angle followed by rotation by the negative of that angle takes
    the vector back to its original position. Rotation by a negative angle is equivalent
    to inverse rotation.
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ A matrix that rotates by an angle is the inverse of the matrix that rotates
    by the negative of the same angle.
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
- en: 2.14.4 Eigenvalues and eigenvectors of a rotation matrix:Finding the axis of
    rotation
  id: totrans-669
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let *λ*, ![](../../OEBPS/Images/AR_e.png) be an eigenvalue, eigenvector pair
    of a rotation matrix *R*. Then *R![](../../OEBPS/Images/AR_e.png)* = *λ![](../../OEBPS/Images/AR_e.png)*
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
- en: Transposing both sides,
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
- en: '*![](../../OEBPS/Images/AR_e.png)^TR^T* = *λ![](../../OEBPS/Images/AR_e.png)^T*'
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
- en: Multiplying the left and right sides, respectively, with the equivalent entities
    *R![](../../OEBPS/Images/AR_e.png)* and *λ![](../../OEBPS/Images/AR_e.png)*, we
    get
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_e.png)*^TR^T*(*R*![](../../OEBPS/Images/AR_e.png))
    = *λ![](../../OEBPS/Images/AR_e.png)^T*(*λ*![](../../OEBPS/Images/AR_e.png)) ⇔
    ![](../../OEBPS/Images/AR_e.png)*^T(R^TR*)![](../../OEBPS/Images/AR_e.png) = *λ*²![](../../OEBPS/Images/AR_e.png)*^T![](../../OEBPS/Images/AR_e.png)
    ⇔ ![](../../OEBPS/Images/AR_e.png)^T*(**I**)![](../../OEBPS/Images/AR_e.png) =
    *λ*²![](../../OEBPS/Images/AR_e.png)^T![](../../OEBPS/Images/AR_e.png)'
  id: totrans-674
  prefs: []
  type: TYPE_IMG
- en: ⇔ ![](../../OEBPS/Images/AR_e.png)^T![](../../OEBPS/Images/AR_e.png) = *λ*²![](../../OEBPS/Images/AR_e.png)^T![](../../OEBPS/Images/AR_e.png)
    ⇔ *λ*² = 1 ⇔ *λ* = 1
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
- en: '(The negative solution *λ* = −1 corresponds to reflection.) Thus, all rotation
    matrices have 1 as one of their eigenvalues. The corresponding eigenvector ![](../../OEBPS/Images/AR_e.png)
    satisfies *R![](../../OEBPS/Images/AR_e.png)* = ![](../../OEBPS/Images/AR_e.png).
    This is the axis of rotation: the set of points that stay where they were post-rotation.'
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
- en: 2.14.5 PyTorch code for eigenvalues and vectors of rotation matrices
  id: totrans-677
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following listing shows the code for the axis of rotation.
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.17 Axis of rotation
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-680
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ① ![](../../OEBPS/Images/eq_02-30-e.png)
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
- en: about the *Z*-axis. All rotation matrices will havean eigenvalue 1. The corresponding
    eigenvector
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
- en: is the axis of rotation (here, the *Z*-axis).
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
- en: ② The PyTorch function eig() computes eigenvalues and eigenvectors.
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
- en: ③ The PyTorch function where() returns the indices at which the specified condition
    is true.
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
- en: ④ Obtains the eigenvector for eigenvalue 1
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ The axis of rotation is the *Z*-axis.
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Takes a random point
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
- en: on this axis and applies the rotation to this point; its position does not change.
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
- en: 2.15 Matrix diagonalization
  id: totrans-690
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In section [2.12](02.xhtml#sec-lin_systems), we studied linear systems and their
    importance in machine learning. We also remarked that the standard mathematical
    process of solving linear systems via matrix inversion is not very desirable from
    a machine learning point of view. In this section, we will see one method of solving
    linear systems without matrix inversion. In addition, this section will help us
    develop the insights necessary to understand quadratic forms and, eventually,
    principal component analysis (PCA), one of the most important tools in data science.
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
- en: Consider an *n* × *n* matrix *A* with *n* linearly independent eigenvectors.
    Let *S* be a matrix with these eigenvectors as its columns. That is,
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-30-f.png)'
  id: totrans-693
  prefs: []
  type: TYPE_IMG
- en: and
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-30-g.png)'
  id: totrans-695
  prefs: []
  type: TYPE_IMG
- en: Then
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-30-h.png)'
  id: totrans-697
  prefs: []
  type: TYPE_IMG
- en: where
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-30-i.png)'
  id: totrans-699
  prefs: []
  type: TYPE_IMG
- en: is a diagonal matrix with the eigenvalues of *A* on the diagonal and 0 everywhere
    else.
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we have
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
- en: '*AS* = *S*Λ'
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
- en: which leads to
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
- en: '*A* = *S*Λ*S*^(−1)'
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
- en: and
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
- en: Λ = *S*^(−1)*AS*
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
- en: 'If *A* is symmetric, then its eigenvectors are orthogonal. Then *S^TS* = *SS^T*
    = **I** ⇔ *S*^(−1) = *S^T*, and we get the diagonalization of *A*: *A* = *S*Λ*S^T*
    Note that diagonalization is not unique: a given matrix may be diagonalized in
    multiple ways.'
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
- en: 2.15.1 PyTorch code for matrix diagonalization
  id: totrans-708
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we will study the PyTorch implementation of the math we learned in section
    [2.15](02.xhtml#sec-mat-diagonalization). As usual, we will only show the directly
    relevant bit of code here.
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional code for this section, executable via Jupyter Notebook,
    can be found at [http://mng.bz/RXJn](http://mng.bz/RXJn).
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.18 Diagonalization of a matrix
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-712
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ① Diagonalization is factorizing a matrix *A* = *SΣS*^(−1). *S* is a matrix
    with eigenvectors of *A* as columns. Σ is a diagonal matrix with eigenvalues of
    *A* in the diagonal.
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
- en: ② The PyTorch function eig() returns eigenvalues and vectors.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
- en: ③ The PyTorch function diag() creates a diagonal matrix of given values.
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
- en: ④ Returns the three factors
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Creates a matrix *A*
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Reconstructs *A* from its factors
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Verifies that the reconstructed matrix is the same as the original
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
- en: 2.15.2 Solving linear systems without inversion via diagonalization
  id: totrans-720
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Diagonalization has many practical applications. Let’s study one now. In general,
    matrix inversion (that is, computing *A*^(−1)) is a very complex process that
    is numerically unstable. Hence, solving *A*![](../../OEBPS/Images/AR_x.png) =
    ![](../../OEBPS/Images/AR_b.png) via ![](../../OEBPS/Images/AR_x.png) = *A*^(−1)![](../../OEBPS/Images/AR_b.png)
    is to be avoided when possible. In the case of a square symmetric matrix with
    *n* distinct eigenvalues, diagonalization can come to the rescue. We can solve
    this in multiple steps. We first diagonalize *A*:'
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
- en: '*A* = *S*Λ*S^T*'
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
- en: Then
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
- en: '*A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)'
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
- en: 'can be written as:'
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
- en: '*S*Λ*S^T*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)'
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
- en: 'where *S* is the matrix with eigenvectors of *A* as its columns:'
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
- en: '*S* = [![](../../OEBPS/Images/AR_e.png)[1]   ![](../../OEBPS/Images/AR_e.png)[2]  
    … ![](../../OEBPS/Images/AR_e.png)*[n]*]'
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
- en: '(Since *A* is symmetric, these eigenvectors are orthogonal. Hence *S^TS* =
    *SS^T* = **I**.) The solution can be obtained in a series of very simple steps:'
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-30-j.png)'
  id: totrans-730
  prefs: []
  type: TYPE_IMG
- en: First solve
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
- en: '*S*![](../../OEBPS/Images/AR_y.png)[1] = ![](../../OEBPS/Images/AR_b.png)'
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
- en: as
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_y.png)[1] = *S^T* ![](../../OEBPS/Images/AR_b.png)'
  id: totrans-734
  prefs: []
  type: TYPE_IMG
- en: Notice that both the transpose and matrix-vector multiplications are simple
    and numerically stable operations, unlike matrix inversion. Then we get
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
- en: Λ(*S^T*![](../../OEBPS/Images/AR_x.png)) = ![](../../OEBPS/Images/AR_y.png)[1]
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
- en: Now solve
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
- en: Λ![](../../OEBPS/Images/AR_y.png)[2] = ![](../../OEBPS/Images/AR_y.png)[1]
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
- en: as
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_y.png)[2] = Λ^(–1)![](../../OEBPS/Images/AR_y.png)[1]'
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that since Λ is a diagonal matrix, inverting it is trivial:'
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-31.png)'
  id: totrans-742
  prefs: []
  type: TYPE_IMG
- en: Equation 2.31
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
- en: As a final step, solve
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
- en: '*S^T*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_y.png)[2]'
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
- en: as
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_x.png) = *S*![](../../OEBPS/Images/AR_y.png)[2]'
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
- en: Thus we have obtained ![](../../OEBPS/Images/AR_x.png) without a single complex
    or unstable step.
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
- en: 2.15.3 PyTorch code for solving linear systems via diagonalization
  id: totrans-749
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s try solving the following set of equations:'
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
- en: '*x* +   *y* +  *z* = 8'
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
- en: 2*x* + 2*y* + 3*z* = 15
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
- en: '*x* + 3*y* + 3*z* = 16'
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
- en: This can be written using matrices and vectors as
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
- en: '*A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)'
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
- en: where
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-31-a.png)'
  id: totrans-757
  prefs: []
  type: TYPE_IMG
- en: Note that *A* is a symmetric matrix. It has orthogonal eigenvectors. The matrix
    with eigenvectors of *A* in columns is orthogonal. Its transpose and inverse are
    the same.
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.19 Solving linear systems using diagonalization
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-760
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ① Creates a symmetric matrix *A*
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
- en: ② Asserts that *A* may be symmetric
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
- en: ③ Creates a vector ![](../../OEBPS/Images/AR_b.png)
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
- en: ④ Solves *A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)
    using matrix inversion, ![](../../OEBPS/Images/AR_x.png) = *A*^(−1)![](../../OEBPS/Images/AR_b.png).
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: matrix inversion is numerically unstable.'
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Solves *A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)
    via diagonalization. *A* = *S*Σ*S^T*.
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-31-b.png).'
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
- en: '⑥ 1\. Solve: *S*![](../../OEBPS/Images/AR_y.png)[1] = ![](../../OEBPS/Images/AR_b.png)
    as ![](../../OEBPS/Images/AR_y.png)[1] = *S^T* ![](../../OEBPS/Images/AR_b.png)
    (no matrix inversion)'
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
- en: '⑦ 2\. Solve: Λ ![](../../OEBPS/Images/AR_y.png)[2] = ![](../../OEBPS/Images/AR_y.png)[1]
    as ![](../../OEBPS/Images/AR_y.png)[2] = Λ^(-1)![](../../OEBPS/Images/AR_y.png)[1]
    (inverting a diagonal matrix is easy; see equation [2.31](02.xhtml#eq-diag-inv).)'
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
- en: '⑧ 3\. Solve: *S^T*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_y.png)[2]
    as ![](../../OEBPS/Images/AR_x.png) = *S*![](../../OEBPS/Images/AR_y.png)[2] (no
    matrix inversion)'
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
- en: ⑨ Verifies that the two solutions are the same
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
- en: ⑨ Verifies that the two solutions are the same
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
- en: 2.15.4 Matrix powers using diagonalization
  id: totrans-773
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If matrix *A* can be diagonalized, then
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-31-c.png)'
  id: totrans-775
  prefs: []
  type: TYPE_IMG
- en: For a diagonal matrix
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-31-d.png)'
  id: totrans-777
  prefs: []
  type: TYPE_IMG
- en: the *n*th power is simply
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-31-e.png)'
  id: totrans-779
  prefs: []
  type: TYPE_IMG
- en: If we need to compute various powers of an *m* × *m* matrix A at different times,
    we should precompute the matrix S and compute any power with only *O*(*m*) operations—compared
    to the (*nm*³) operations necessary for naive computations.
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
- en: 2.16 Spectral decomposition of a symmetric matrix
  id: totrans-781
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have seen in section [2.15](02.xhtml#sec-mat-diagonalization) that a square
    symmetric matrix with distinct eigenvalues can be decomposed as
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
- en: '*A* = *S*Λ*S^T*'
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
- en: where
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
- en: '*A* = [![](../../OEBPS/Images/AR_e.png)[1] ![](../../OEBPS/Images/AR_e.png)[2]
    … ![](../../OEBPS/Images/AR_e.png)[n]]'
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
- en: Thus,
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-31-f.png)'
  id: totrans-787
  prefs: []
  type: TYPE_IMG
- en: This equation can be rewritten as
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
- en: '*A* = *λ*[1]![](../../OEBPS/Images/AR_e.png)[1]![](../../OEBPS/Images/AR_e.png)[1]^T
    + *λ*[2]![](../../OEBPS/Images/AR_e.png)[2]![](../../OEBPS/Images/AR_e.png)[2]*^T*
    + … + *λ[n]![](../../OEBPS/Images/AR_e.png)[n]*![](../../OEBPS/Images/AR_e.png)*[n]^T*'
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
- en: Equation 2.32
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
- en: Thus a square symmetric matrix can be written in terms of its eigenvalues and
    eigenvectors. This is the spectral resolution theorem.
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
- en: 2.16.1 PyTorch code for the spectral decomposition of a matrix
  id: totrans-792
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following listing shows the relevant code for this section.
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.20 Spectral decomposition of a matrix
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-795
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ① Asserts that A is a 2D tensor (i.e., matrix)
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
- en: '② A is square: i.e., *A*.*shape*[0] (num rows) ≜ *A*.*shape*[1] (num columns)'
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
- en: '③ Asserts that A is symmetric: i.e., *A* = = *A^T*'
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
- en: ④ The PyTorch function eig() returns eigenvectors and values.
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Defines a 3D tensor *C* of shape *n* × *n* × *n* to hold the *n* components
    from equation [2.32](02.xhtml#eq-spectral-decomp). Each term *λ[i]![](../../OEBPS/Images/AR_e.png)[i]![](../../OEBPS/Images/AR_e.png)**^T*
    is an *n* × *n* matrix. There are *n* such terms, all compactly held in tensor
    *C*.
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Computes *C*[*i*] = *λ[i]![](../../OEBPS/Images/AR_e.png)[i]![](../../OEBPS/Images/AR_e.png)**^T*
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Reconstructs *A* by adding its components stored in *C*
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Verifies that the matrix reconstructed from spectral components matches the
    original
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
- en: '2.17 An application relevant to machine learning: Finding the axes of a hyperellipse'
  id: totrans-804
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The notion of an ellipse in high-dimensional space (aka hyperellipse) keeps
    coming back in various forms in machine learning. Here we will make a preliminary
    review of them. We will revisit these concepts later.
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall the equation of an ellipse from high school math:'
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-32-a.png)'
  id: totrans-807
  prefs: []
  type: TYPE_IMG
- en: 'This is a rather simple ellipse: it is two-dimensional and centered at the
    origin, and its major and minor axes are aligned with the coordinate axes. Denoting
    ![](../../OEBPS/Images/eq_02-32-b.png) as the position vector, the same equation
    can be written as'
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_x.png)*^TΛ*![](../../OEBPS/Images/AR_x.png) = 1'
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
- en: where ![](../../OEBPS/Images/eq_02-32-c.png) is a diagonal matrix. Written in
    this form, the equation can be extended beyond 2D to an *n*-dimensional axis-aligned
    ellipse centered at the origin. Now let’s apply a rotation *R* to the axis. Then
    every vector ![](../../OEBPS/Images/AR_x.png) transforms to *R*![](../../OEBPS/Images/AR_x.png).
    The equation of the ellipse in the new (rotated) coordinate system is
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
- en: (*R![](../../OEBPS/Images/AR_x.png)^T Λ* (*R![](../../OEBPS/Images/AR_x.png)*)
    = 1
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
- en: ⇔ ![](../../OEBPS/Images/AR_x.png)^T (*R^T ΛR*) ![](../../OEBPS/Images/AR_x.png)
    = 1
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
- en: where
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
- en: '*A* = (*R^TΛR*).'
  id: totrans-814
  prefs: []
  type: TYPE_NORMAL
- en: The generalized equation of the ellipse is
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_x.png)*^TA*![](../../OEBPS/Images/AR_x.png) = 1'
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
- en: 'Note the following:'
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
- en: The ellipse is no longer axis aligned.
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The matrix *A* is no longer diagonal.
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A* is symmetric. We can easily verify that *A^T* = (*R^T*Λ*R*)*^T* = *R^T*Λ*^TR*
    = *R^T*Λ*R* (remember, the transpose of a diagonal matrix is itself).'
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If, in addition, we want to get rid of the “centered at the origin” assumption,
    we get
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
- en: (![](../../OEBPS/Images/AR_x.png)−*μ*)*^TA*(![](../../OEBPS/Images/AR_x.png)−*μ*)
    = 1
  id: totrans-822
  prefs: []
  type: TYPE_NORMAL
- en: Equation 2.33
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s flip the problem around. Suppose we have a generic *n*-dimensional
    ellipse. How do we compute its axes’ directions?
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, if we can rotate the coordinate system so that the matrix in the middle
    is diagonal, we are done. Diagonalization (see section [2.15](02.xhtml#sec-mat-diagonalization))
    is the answer. Specifically, we find the matrix *S* with eigenvectors of *A* in
    its columns. This is a rotation matrix (being orthogonal, since *A* is symmetric).
    We transform rotate) the coordinate system by applying this matrix. In this new
    coordinate system, the ellipse is axis aligned. Stated another way, the new coordinate
    axes—these are the eigenvectors of *A*—yield the axes of the ellipse.
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
- en: 2.17.1 PyTorch code for hyperellipses
  id: totrans-826
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s try finding the axes of the hyperellipse described by the equation 5*x*²
    + 6*xy* + 5*y*² = 20. Note that the actual ellipse we use as an example is 2D
    (to facilitate visualization), but the code we develop will be general and extensible
    to multiple dimensions.
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
- en: The ellipse equation can be written using matrices and vectors as ![](../../OEBPS/Images/AR_x.png)*^TA*![](../../OEBPS/Images/AR_x.png)
    = 1, where
  id: totrans-828
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-33-a.png)'
  id: totrans-829
  prefs: []
  type: TYPE_IMG
- en: 'To find the axes of the hyperellipse, we need to transform the coordinate system
    so that the matrix in the middle becomes diagonal. Here is how this can be done:
    if we diagonalize *A* into *S*Σ*S*^(−1), then the ellipse equation becomes ![](../../OEBPS/Images/AR_x.png)*^TS*Σ*S*^(−1)![](../../OEBPS/Images/AR_x.png)
    = 1, where Σ is a diagonal matrix. Since *A* is symmetric, its eigenvectors are
    orthogonal. Hence, the matrix containing these eigenvectors as columns is orthogonal:
    i.e., *S*^(−1) = *S^T*. In other words, *S* is a rotation matrix. So the ellipse
    equation becomes ![](../../OEBPS/Images/AR_x.png)*^TS*Σ*S^T*![](../../OEBPS/Images/AR_x.png)
    = 1 or (![](../../OEBPS/Images/AR_x.png)*^TS*)Σ(*S^T*![](../../OEBPS/Images/AR_x.png))
    = 1 or ![](../../OEBPS/Images/AR_y.png)*^T*Σ![](../../OEBPS/Images/AR_y.png) =
    1 where ![](../../OEBPS/Images/AR_y.png) = *S^T*![](../../OEBPS/Images/AR_x.png).
    This is of the desired form since Σ is a diagonal matrix. Remember, *S* is a rotation
    matrix. Thus, rotating the coordinate system by *S* aligns the coordinate axes
    with the ellipse axes.'
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.21 Axes of a hyperellipse
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-832
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '① Equation of the ellipse: 5*x*² + 6*x**y* + 5*y*² = 20 or ![](../../OEBPS/Images/AR_x.png)*^TA*![](../../OEBPS/Images/AR_x.png)
    = 20, where ![](../../OEBPS/Images/eq_02-33-b.png)'
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
- en: ② X-axis vector
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
- en: ③ Major axis of the ellipse
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
- en: ④ The dot product between two vectors is the cosine of the angle between them.
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
- en: '⑤ The angle between the ellipse’s major axis and the X-axis: 45° (see figure
    [2.16](02.xhtml#fig-numpy-hyper-ellipse))'
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH02_F16_Chaudhury.png)'
  id: totrans-838
  prefs: []
  type: TYPE_IMG
- en: Figure 2.16 Note that the ellipse’s major axis forms an angle of 45 degrees
    with the X-axis. Rotating the coordinate system by this angle will align the ellipse
    axes with the coordinate axes. Subsequently, the first principal vector will also
    lie along this direction.
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-840
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In machine learning, a vector is a one-dimensional array of numbers and a matrix
    is a two-dimensional array of numbers. Inputs and outputs of machine learning
    models are typically represented as vectors or matrices. In multilayered models,
    inputs and outputs of each individual layer are also represented as vectors or
    matrices. Images are two-dimensional arrays of numbers corresponding to pixel
    color values. As such, they are represented as matrices.
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An *n*-dimensional vector can be viewed as point in ℝ*^n* space. All models
    can be viewed as functions that map points from input to output space. The model
    is designed so that it is easier to solve the problem of interest in the output
    space.
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A dot product between a pair of vectors ![](../../OEBPS/Images/AR_x.png) = [*x*[1]
       *x*[2]   …   *x[n]*] and ![](../../OEBPS/Images/AR_y.png) = [*y*[1]    *y*[2]
      …   *y[n]*] is the scalar quantity ![](../../OEBPS/Images/AR_x.png) ⋅ ![](../../OEBPS/Images/AR_y.png)
    = *x*[1]*y*[1] + *x*[2]*y*[2] + ⋯ + *x[n]**y**[n]*. It is a measure of how similar
    the vectors are. Dot products are widely used in machine learning. For instance,
    in supervised machine learning, we train the model so that its output is as similar
    as possible to the known output for a sample set of input points known as training
    data. Here, some variant of the dot product is often used to measure the similarity
    of the model output and the known output. Two vectors are orthogonal if their
    dot product is zero. This means the vectors have no similarity and are independent
    of each other.
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A vector’s dot product with itself is the square of the magnitude or length
    of the vector ![](../../OEBPS/Images/AR_x.png) ⋅ ![](../../OEBPS/Images/AR_x.png)
    = ||![](../../OEBPS/Images/AR_x.png)||² = *x*[1]*x*[1] + *x*[2]*x*[2] + ⋯ + *x[n]x[n]*.
  id: totrans-844
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Given a set of vectors ![](../../OEBPS/Images/AR_x.png)[1], ![](../../OEBPS/Images/AR_x.png)[2],
    ⋯, ![](../../OEBPS/Images/AR_x.png)*[n]*, the weighted sum *a*[1]![](../../OEBPS/Images/AR_x.png)[1]
    + *a*[2]![](../../OEBPS/Images/AR_x.png)[2] + ⋯ + *a[n]*![](../../OEBPS/Images/AR_x.png)*[n]*
    where *a*[1], *a*[2], ⋯, *a[n]* are arbitrary scalars) is known as a linear combination.
    In particular, if the coefficients *a*[1], *a*[2], ⋯, *a[n]* are non-negative
    and they sum to 1, the linear combination is called a *convex* combination.
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If it is possible to find a set of coefficients *a*[1], *a*[2], ⋯, *a[n]*, not
    all zero, such that the linear combination is a null vector (meaning all its elements
    are zeros), then the vectors ![](../../OEBPS/Images/AR_x.png)[1], ![](../../OEBPS/Images/AR_x.png)[2],
    ⋯, ![](../../OEBPS/Images/AR_x.png)*[n]* are said to *linearly dependent*. On
    the other hand, if the only way to obtain a linear combination that is a null
    vector is to make every coefficient zero, the vectors are said to be *linearly
    independent*.
  id: totrans-846
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: One important application of matrices and vectors is to solve a system of linear
    equations. Such a system can be expressed in matrix vector terms as *A*![](../../OEBPS/Images/AR_x.png)
    = ![](../../OEBPS/Images/AR_b.png), where we solve for an unknown vector ![](../../OEBPS/Images/AR_x.png)
    satisfying the equation. This system has an exact solution if and only if *A*
    is invertible. This means *A* is a square matrix (the number of rows equals the
    number of columns) and the row vectors are linearly independent. If the row vectors
    are linearly independent, so are the column vectors, and vice versa. If the rows
    and columns are linearly independent, the determinant of *A* is guaranteed to
    be nonzero. Hence, linear independence of rows/columns and nonzero determinant
    are equivalent conditions. If any one of them is satisfied, the linear system
    has an exact and unique solution.
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In practice, this requirement is often not met, and we have an over- or under-determined
    system. In such situations, the Moore-Penrose inverse leads to a form of best
    approximation. Geometrically, the Moore-Penrose method yields the point that is
    closest to ![](../../OEBPS/Images/AR_b.png) in the space of vectors spanned by
    columns of *A*. Equivalently, the Moore-Penrose solution ![](../../OEBPS/Images/AR_x.png)[*]
    yields the point closest to ![](../../OEBPS/Images/AR_b.png) on the space of vectors
    spanned by the columns of *A*.
  id: totrans-848
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For a square matrix *A*, if and only if *Aê* = *λê*, we say *λ* is an eigenvalue
    (a scalar) and *ê* is an eigenvector (a unit vector) of *A*. Physically, the eigenvector
    *ê* is a unit vector whose direction does not change when transformed by the matrix
    *A*. The transform can magnify its length by the scalar scale factor *λ*, which
    is the eigenvalue.
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An *n* × *n* matrix *A* has *n* eigenvalue/eigenvector pairs. The eigenvalues
    need not all be unique. The eigenvectors corresponding to different eigenvalues
    are linearly independent. If the matrix *A* is symmetric, satisfying *A^T* = *A*,
    the eigenvectors corresponding to different eigenvalues are orthogonal. A rotation
    matrix is a matrix in which the rows are orthogonal to each other and so are the
    columns. Such a matrix is also known as an orthogonal matrix. An orthogonal matrix
    *R* satisfies the equation *R^TR* = **I**, where **I** is the identity matrix.
    In the special case when the matrix *A* is a rotation matrix *R*, one of the eigenvalues
    is always 1. The corresponding eigenvector is the axis of rotation. A matrix *A*
    with *n* linearly independent eigenvectors can be decomposed as *A* = *S*Λ*S*^(−1),
    where *S* =[![](../../OEBPS/Images/AR_e.png)[1]    ![](../../OEBPS/Images/AR_e.png)[2]
      …   *![](../../OEBPS/Images/AR_e.png)[n]*] is the matrix with eigenvectors of
    *A* as its columns and Λ is a diagonal matrix with the eigenvalues of *A* as its
    diagonal. This decomposition is called matrix diagonalization and leads to a numerically
    stable way to solve linear systems.
  id: totrans-850
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A square symmetric matrix *A* can be expressed in terms of its eigenvectors
    and eigenvalues as *A* = *λ*[1]![](../../OEBPS/Images/AR_e.png)[1]![](../../OEBPS/Images/AR_e.png)[1]*^T*
    + *λ*[2]![](../../OEBPS/Images/AR_e.png)[2]![](../../OEBPS/Images/AR_e.png)[2]*^T*
    +   …  + *λ[n]![](../../OEBPS/Images/AR_e.png)[n]*![](../../OEBPS/Images/AR_e.png)*[n]^T*.
    This is known as the spectral decomposition of the matrix *A*.
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
- en: ¹  In mathematics, vectors can have an infinite number of elements. Such vectors
    cannot be expressed as arrays—but we will mostly ignore them in this book. [↩](02.xhtml#fnref4)
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
- en: ²  We usually use uppercase letters to symbolize matrices. [↩](02.xhtml#fnref5)
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
- en: ³  In digital computers, numbers in the range 0..255 can be represented with
    a single byte of storage; hence this choice. [↩](02.xhtml#fnref6)
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
- en: ⁴  The mathematical symbol ∀ stands for “for all.” Thus, ∀![](../../OEBPS/Images/AR_y.png)
    ∈ ℜ*^n* means “all vectors y in the *n*-dimensional space.” [↩](02.xhtml#fnref7)
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
- en: ⁵  You can compute eigenvectors and eigenvalues only of square matrices. [↩](02.xhtml#fnref8)
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
