- en: 2 Vectors, matrices, and tensors in machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Vectors and matrices and their role in datascience
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with eigenvalues and eigenvectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding the axes of a hyper-ellipse
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At its core, machine learning, and indeed all computer software, is about number
    crunching. We input a set of numbers into the machine and get back a different
    set of numbers as output. However, this cannot be done randomly. It is important
    to organize these numbers appropriately and group them into meaningful objects
    that go into and come out of the machine. This is where vectors and matrices come
    in. These are concepts that mathematicians have been using for centuries—we are
    simply reusing them in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will study vectors and matrices, primarily from a machine
    learning point of view. Starting from the basics, we will quickly graduate to
    advanced concepts, restricting ourselves to topics relevant to machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: We provide Jupyter Notebook-based Python implementations for most of the concepts
    discussed in this and other chapters. Complete, fully functional code that can
    be downloaded and executed (after installing Python and Jupyter Notebook) can
    be found at [http://mng.bz/KMQ4](http://mng.bz/KMQ4). The code relevant to this
    chapter can be found at [http://mng.bz/d4nz](http://mng.bz/d4nz).
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Vectors and their role in machine learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s revisit the machine learning model for a cat brain introduced in section
    [1.3](../Text/01.xhtml#sec-cat_brain). It takes two numbers as input, representing
    the hardness and sharpness of the object in front of the cat. The cat brain processes
    the input and generates an output threat score that leads to a decision to *run
    away* or *ignore* or *approach and purr*. The two input numbers usually appear
    together, and it will be handy to group them into a single object. This object
    will be an ordered sequence of two numbers, the first representing hardness and
    the second representing sharpness. Such an object is a perfect example of a vector.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, a *vector* can be thought of as an ordered sequence of two or more numbers,
    also known as an *array* of numbers.[¹](02.xhtml#fn4) Vectors constitute a compact
    way of denoting a set of numbers that together represent some entity. In this
    book, vectors are represented by lowercase letters with an overhead arrow and
    arrays by square brackets. For instance, the input to the cat brain model in section
    [1.3](../Text/01.xhtml#sec-cat_brain) was a vector ![](../../OEBPS/Images/eq_02-00-a2.png),
    where *x*[0] represented hardness and *x*[1] represented sharpness.
  prefs: []
  type: TYPE_NORMAL
- en: Outputs to machine learning models are also often represented as vectors. For
    instance, consider an object recognition model that takes an image as input and
    emits a set of numbers indicating the probabilities that the image contains a
    dog, human, or cat, respectively. The output of such a model is a three element
    vector ![](../../OEBPS/Images/eq_02-00-b2.png), where the number *y*[0] denotes
    the probability that the image contains a dog, *y*[1] denotes the probability
    that the image contains a human, and *y*[2] denotes the probability that the image
    contains a cat. Figure [2.1](02.xhtml#fig-vec_out) shows some possible input images
    and corresponding output vectors.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH02_F01_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 Input images and corresponding output vectors denoting probabilities
    that the image contains a dog and/or human and/or cat, respectively. Example output
    vectors are shown.
  prefs: []
  type: TYPE_NORMAL
- en: In multilayered machines like neural networks, the input and output to a layer
    can be vectors. We also typically represent the parameters of the model function
    (see section [1.3](../Text/01.xhtml#sec-cat_brain)) as vectors. This is illustrated
    in section [2.3](02.xhtml#sec-matrices).
  prefs: []
  type: TYPE_NORMAL
- en: Table 2.1 Toy documents and corresponding feature vectors describing them. Words
    eligible for the feature vector are bold. The first element of the feature vector
    indicates the number of occurrences of the word *gun* and the second *violence*.
  prefs: []
  type: TYPE_NORMAL
- en: '| Docid | Document | Feature vector |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| *d*[0] | Roses are lovely. Nobody hates roses. | [0 0] |'
  prefs: []
  type: TYPE_TB
- en: '| *d*[1] | **Gun violence** has reached an epidemic proportion in America.
    | [1 1] |'
  prefs: []
  type: TYPE_TB
- en: '| *d*[2] | The issue of **gun violence** is really over-hyped. One can find
    many instances of **violence**, where no **guns** were involved. | [2 2] |'
  prefs: []
  type: TYPE_TB
- en: '| *d*[3] | **Guns** are for **violence** prone people. **Violence** begets
    **guns**. **Guns** beget **violence**. | [3 3] |'
  prefs: []
  type: TYPE_TB
- en: '| *d*[4] | I like **guns** but I hate **violence**. I have never been involved
    in **violence**. But I own many **guns**. **Gun violence** is incomprehensible
    to me. I do believe **gun** owners are the most anti **violence** people on the
    planet. He who never uses a **gun** will be prone to senseless **violence**. |
    [5 5] |'
  prefs: []
  type: TYPE_TB
- en: '| *d*[5] | **Guns** were used in a armed robbery in San Francisco last night.
    | [1 0] |'
  prefs: []
  type: TYPE_TB
- en: '| *d*[6] | Acts of **violence** usually involves a weapon. | [0 1] |'
  prefs: []
  type: TYPE_TB
- en: 'One particularly significant notion in machine learning and data science is
    the idea of a *feature vector*. This is essentially a vector that describes various
    properties of the object being dealt with in a particular machine learning problem.
    We will illustrate the idea with an example from the world of natural language
    processing (NLP). Suppose we have a set of documents. We want to create a document
    retrieval system where, given a new document, we have to retrieve similar documents
    in the system. This essentially boils down to estimating the similarity between
    documents in a quantitative fashion. We will study this problem in detail later,
    but for now, we want to note that the most natural way to approach this is to
    create feature vectors for each document that quantitatively describe the document.
    In section [2.5.6](02.xhtml#subsection-dot_product), we will see how to measure
    the similarity between these vectors; here, let’s focus on simply creating descriptor
    vectors for the documents. A popular way to do this is to choose a set of interesting
    words (we typically exclude words like “and,” “if,” and “to” that are present
    in all documents from this list), count the number of occurrences of those interesting
    words in each document, and make a vector of those values. Table [2.1](02.xhtml#tab-doc-vec)
    shows a toy example with six documents and corresponding feature vectors. For
    simplicity, we have considered only two of the possible set of words: *gun* and
    *violence*, plural or singular, uppercase or lowercase.'
  prefs: []
  type: TYPE_NORMAL
- en: As a different example, the sequence of pixels in an image can also be viewed
    as a feature vector. Neural networks in computer vision tasks usually expect this
    feature vector.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1 The geometric view of vectors and its significance in machine learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vectors can also be viewed geometrically. The simplest example is a two-element
    vector ![](../../OEBPS/Images/eq_02-00-c2.png). Its two elements can be taken
    to be *x* and *y*, Cartesian coordinates in a two-dimensional space, in which
    case the vector corresponds to a point in that space. *Vectors with n elements
    represent points in an n-dimensional space*. The ability to see inputs and outputs
    of a machine learning model as points allows us to view the model itself as a
    geometric transformation that maps input points to output points in some high-dimensional
    space. We have already seen this in section [1.4](../Text/01.xhtml#sec-geom-view-ml).
    It is an enormously powerful concept we will use throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: A vector represents a point in space. Also, an array of coordinate values like
    ![](../../OEBPS/Images/eq_02-00-d2.png) describes the position of one point *in
    a given coordinate system*. Hence, an array (of coordinate values) can be viewed
    as the quantitative representation of a vector. See figure [2.2](02.xhtml#fig-vector_diagram)
    to get an intuitive understanding of this.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH02_F02_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.2 A vector describing the position of point P with respect to point
    O. The basic mental picture is an arrowed line. This agrees with the definition
    of a vector that you may have learned in high school: a vector has a magnitude
    (length of the arrowed line) and direction (indicated by the arrow). On a plane,
    this is equivalent to the ordered pair of numbers *x*, *y*, where the geometric
    interpretations of *x* and *y* are as shown in the figure. In this context, it
    is worthwhile to note that only the relative positions of the points O and P matter.
    If both the points are moved, keeping their relationship intact, the vector does
    not change.'
  prefs: []
  type: TYPE_NORMAL
- en: For a real life example, consider the plane of a page of this book. Suppose
    we want to reach the top-right corner point of the page from the bottom-left corner.
    Let’s call the bottom-left corner *O* and the top-right corner *P*. We can travel
    the width (8.5 inches) to the right to reach the bottom-left corner and then travel
    the height (11 inches) upward to reach the top-right corner. Thus, if we choose
    a coordinate system with the bottom-left corner as the origin and the *X*-axis
    along the width, and the *Y*-axis along the height, point *P* corresponds to the
    array representation ![](../../OEBPS/Images/eq_02-00-e2.png). But we could also
    travel along the diagonal from the bottom-left to the top-right corner to reach
    *P* from *O*. Either way, we end up at the same point *P*.
  prefs: []
  type: TYPE_NORMAL
- en: This leads to a conundrum. The vector ![](../../OEBPS/Images/AR_OP.png) represents
    the abstract geometric notion “position of *P* with respect to *O*” independent
    of our choice of coordinate axes. On the other hand, the array representation
    depends on the choice of a coordinate system. For example, the array ![](../../OEBPS/Images/eq_02-00-e2.png)
    represents the top-right corner point *P* only under a specific choice of coordinate
    axes (parallel to the sides of the page) and a reference point (bottom-left corner).
    Ideally, to be unambiguous, we should specify the coordinate system along with
    the array representation. Why don’t we ever do this in machine learning? Because
    in machine learning, it doesn’t exactly matter what the coordinate system is as
    long as we stick to any fixed coordinate system. Machine learning is about minimizing
    loss functions (which we will study later). As such, absolute positions of point
    are immaterial, only relative positions matter.
  prefs: []
  type: TYPE_NORMAL
- en: There are explicit rules (which we will study later) that state how the vector
    transforms when the coordinate system changes. We will invoke them when necessary.
    All vectors used in a machine learning computation must consistently use the same
    coordinate system or be transformed appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: 'One other point: planar spaces, such as the plane of the paper on which this
    book is written, are two-dimensional (2D). The mechanical world we live in is
    three-dimensional (3D). Human imagination usually fails to see higher dimensions.
    In machine learning and data science, we often talk of spaces with thousands of
    dimensions. You may not be able to see those spaces in your mind, but that is
    not a crippling limitation. You can use 3D analogues in your head. They work in
    a surprisingly large variety of cases. However, it is important to bear in mind
    that this is not always true. Some examples where the lower-dimensional intuitions
    fail at higher dimensions will be shown later.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 PyTorch code for vector manipulations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PyTorch is an open source machine learning library developed by Facebook’s artificial
    intelligence group. It is one of the most elegant practical tools for developing
    deep learning applications at present. In this book, we aim to familiarize you
    with PyTorch and similar programming paradigms alongside the relevant mathematics.
    Knowledge of Python basics will be assumed. You are strongly encouraged to try
    out all the code snippets in this book (after installing the appropriate packages
    like PyTorch, that is).
  prefs: []
  type: TYPE_NORMAL
- en: All the Python code in this book is produced via Jupyter Notebook. A summary
    of the theoretical material presented in the code is provided before the code
    snippet.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1 PyTorch code for the introduction to vectors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Listing 2.1 shows how to create and access vectors and subvectors and slice
    and dice vectors using PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional code demonstrating how to create a vector and access its
    elements, executable via Jupyter Notebook, can be found at [http://mng.bz/xm8q](http://mng.bz/xm8q).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.1 Introduction to vectors via PyTorch
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: ① torch.tensor represents a multidimensional array. The vector is a 1D tensor
    that can be initialized by directly specifying values.
  prefs: []
  type: TYPE_NORMAL
- en: ② Tensor elements are floats by default. We can force tensors to be other types
    such as float64 (double).
  prefs: []
  type: TYPE_NORMAL
- en: ③ The square bracket operator lets us access individual vector elements.
  prefs: []
  type: TYPE_NORMAL
- en: ④ Negative indices count from the end of the array. –1 denotes the last element.
    -2 denotes the second-to-last element.
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ The colon operator slices off a range of elements from the vector.
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Nothing before a colon denotes the beginning of~the~array. Nothing after a
    colon denotes the end~of~the array.
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Torch tensors can be initialized from NumPy arrays.
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ The difference between the Torch tensor and its NumPy version is zero.
  prefs: []
  type: TYPE_NORMAL
- en: ⑨ Torch tensors can be converted to NumPy arrays.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Matrices and their role in machine learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes it is not sufficient to group a set of numbers into a vector. We have
    to collect several vectors into another group. For instance, consider the input
    to training a machine learning model. Here we have several input instances, each
    consisting of a sequence of numbers. As seen in section [2.1](02.xhtml#sec-vectors),
    the sequence of numbers belonging to a single input instance can be grouped into
    a vector. How do we represent the entire collection of input instances? This is
    where the concept of matrices comes in handy from the world of mathematics. A
    *matrix* can be viewed as a rectangular array of numbers arranged in a fixed count
    of rows and columns. Each row of a matrix is a vector, and so is each column.
    Thus a matrix can be thought of as a collection of row vectors. It can also be
    viewed as a collection of column vectors. We can represent the entire set of numbers
    that constitute the training input to a machine learning model as a matrix, with
    each row vector corresponding to a single training instance.
  prefs: []
  type: TYPE_NORMAL
- en: Consider our familiar cat-brain problem again. As stated earlier, a single input
    instance to the machine is a vector ![](../../OEBPS/Images/eq_02-00-f2.png), where
    *x*[0] describes the hardness of the object in front of the cat. Now consider
    a training dataset with many such input instances, each with a known output threat
    score. You might recall from section [1.1](../Text/01.xhtml#sec-paradigm-shift)
    that the goal in machine learning is to create a function that maps these inputs
    to their respective outputs with as little overall error as possible. Our training
    data may look as shown in table [2.2](02.xhtml#tab-cat-brain-training-data) (note
    that in real-life problems, the training dataset is usually large—often millions
    of input-output pairs—but in this toy problem, we will have 8 training data instances).
  prefs: []
  type: TYPE_NORMAL
- en: Table 2.2 Example training dataset for our toy machine learning–based cat brain
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Input value: Hardness | Input value: Sharpness | Output: Threat score
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0.11 | 0.09 | −0.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.01 | 0.02 | −0.97 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.98 | 0.91 | 0.89 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.12 | 0.21 | −0.68 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0.98 | 0.99 | 0.95 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 0.85 | 0.87 | 0.74 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 0.03 | 0.14 | −0.88 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 0.55 | 0.45 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: From table [2.2](02.xhtml#tab-cat-brain-training-data), we can collect the columns
    corresponding to hardness and sharpness into a matrix, as shown in equation [2.1](02.xhtml#eq-cat-brain-toy-training-dataset)—this
    is a compact representation of the training dataset for this problem. [²](02.xhtml#fn5)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-01.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 2.1
  prefs: []
  type: TYPE_NORMAL
- en: Each row of matrix *X* is a particular input instance. Different rows represent
    different input instances. On the other hand, different columns represent different
    feature elements. For example, the 0th row of matrix *X* is the vector [*x*[00]    *x*[01]]
    representing the 0th input instance. Its elements, *x*[00] and *x*[01] represent
    different feature elements, hardness and sharpness respectively of the 0th training
    input instance.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.1 Matrix representation of digital images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Digital images are also often represented as matrices. Here, each element represents
    the brightness at a specific pixel position (*x*, *y* coordinate) of the image.
    Typically, the brightness value is normalized to an integer in the range 0 to
    255. 0 is black, 255 is white, and 128 is gray.[³](02.xhtml#fn6) Following is
    an example of a tiny image, 9 pixels wide and 4 pixels high:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-02.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 2.2
  prefs: []
  type: TYPE_NORMAL
- en: The brightness increases gradually from left to right and also from top to bottom.
    *I*[00] represents the top-left pixel, which is black. *I*[3, 8] represents the
    bottom-right pixel, which is white. The intermediate pixels are various shades
    of gray between black and white. The actual image is shown in figure [2.2](02.xhtml#fig-tiny_im).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH02_F03_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 Image corresponding to matrix *I*[4, 9] in equation [2.2](02.xhtml#eq-tiny_im)
  prefs: []
  type: TYPE_NORMAL
- en: '2.4 Python code: Introducing matrices, tensors, and images via PyTorch'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For programming purposes, you can think of tensors as multidimensional arrays.
    Scalars are zero-dimensional tensors. Vectors are one-dimensional tensors. Matrices
    are two-dimensional tensors. RGB images are three-dimensional tensors (*colorchannels*
    × *height* × *width*). A batch of 64 images is a four-dimensional tensor (64 ×
    *colorchannels* × *height* × *width*).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.2 Introducing matrices via PyTorch
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '① A matrix is a 2D array of numbers: i.e., a 2D tensor. The entire training
    data input set for a machine-learning model can be viewed as a matrix. Each input
    instance is one row. Row count ≡ number of training examples, column count ≡ training
    instance size'
  prefs: []
  type: TYPE_NORMAL
- en: '② Cat-brain training data input: 8 examples, each with two values (hardness,
    sharpness). An 8 × 2 tensor is created by specifying values.'
  prefs: []
  type: TYPE_NORMAL
- en: ③ The shape of a tensor is a list. For a matrix, the first list element is num
    rows; the second list element is num columns.
  prefs: []
  type: TYPE_NORMAL
- en: ④ Square brackets extract individual matrix elements.
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ A standalone colon operator denotes all possible indices.
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ The colon operator denotes the range of indices.
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ 0th column
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ 1st column
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.3 Slicing and dicing matrices
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: ① Ranges of rows and columns can be specified via the colon operator to slice
    off (extract) submatrices.
  prefs: []
  type: TYPE_NORMAL
- en: ② Extracts the first three training examples (rows)
  prefs: []
  type: TYPE_NORMAL
- en: ③ Extracts the sharpness feature for the 5th to 7th training examples
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.4 Tensors and images in PyTorch
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: ① PyTorch tensors can be used to represent tensors. A vector is a 1-tensor,
    a matrix is a 2-tensor, and a scalar is a 0-tensor.
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates a random tensor of specified dimensions
  prefs: []
  type: TYPE_NORMAL
- en: ③ All images are tensors. An RGB image of height H, width W is a 3-tensor of
    shape [3, H, W].
  prefs: []
  type: TYPE_NORMAL
- en: ④ 4 × 9 single-channel image shown in figure [2.3](02.xhtml#fig-tiny_im)
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Reads a 199 × 256 × 3 image from disk
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Usual slicing dicing operators work. Extracts the red, green, and blue channels
    of the image as shown in figure [2.4](02.xhtml#fig-numpy-dog-grid).
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Crops out a 100 × 100 subimage as shown in figure [2.5](02.xhtml#fig-numpy-crop-dog)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH02_F04_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 Tensors and images in PyTorch
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH02_F05_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 Cropped image of dog
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Basic vector and matrix operations in machine learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we introduce several basic vector and matrix operations along
    with examples to demonstrate their significance in image processing, computer
    vision, and machine learning. It is meant to be an application-centric introduction
    to linear algebra. But it is *not* meant to be a comprehensive review of matrix
    and vector operations, for which you are referred to a textbook on linear algebra.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH02_F06_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 Image corresponding to the transpose of matrix *I*[4, 9] shown in
    equation [2.3](02.xhtml#eq-tiny_im_transpose). This is equivalent to rotating
    the image by 90°.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.1 Matrix and vector transpose
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In equation [2.2](02.xhtml#eq-tiny_im), we encountered the matrix *I*[4, 9]
    depicting a tiny image. Suppose we want to rotate the image by 90° so it looks
    like figure [2.5](02.xhtml#fig-tiny_im-transpose). The original matrix *I*[4,
    9] and its transpose *I*[4,]*^T*[9] = *I*[9, 4] are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-03.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 2.3
  prefs: []
  type: TYPE_NORMAL
- en: By comparing equation [2.2](02.xhtml#eq-tiny_im) and equation [2.3](02.xhtml#eq-tiny_im_transpose),
    you can easily see that one can be obtained from the other by interchanging the
    row and column indices. This operation is generally known as *matrix transposition*.
  prefs: []
  type: TYPE_NORMAL
- en: Formally, the transpose of a matrix *A[m, n]* with *m* rows and *n* columns
    is another matrix with *n* rows and *m* columns. This transposed matrix, denoted
    *A[n,]^T[m]*, is such that *A^T*[*i*, *j*] = *A*[*j*, *i*]. For instance, the
    value at row 0 column 6 in matrix *I*[4, 9] is 48; in the transposed matrix, the
    same value appears in row 6 and column 0. In matrix parlance, *I*[4, 9][0,6] =
    *I*[9,]*^T*[4][6,0] = 48.
  prefs: []
  type: TYPE_NORMAL
- en: 'Vector transposition is a special case of matrix transposition (since all vectors
    are matrices—a column vector with *n* elements is an *n* × 1 matrix). For instance,
    an arbitrary vector and its transpose are shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-04.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 2.4
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-05.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 2.5
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.2 Dot product of two vectors and its role in machine learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In section [1.3](../Text/01.xhtml#sec-cat_brain), we saw the simplest of machine
    learning models where the output is generated by taking a weighted sum of the
    inputs (and then adding a constant bias value). This model/machine is characterized
    by the weights *w*[0], *w*[1], and bias *b*. Take the rows of table [2.2](02.xhtml#tab-cat-brain-training-data).
    For example, for row 0, the input values are the hardness of the approaching object
    = 0.11 and softness = 0.09. The corresponding model output will be *y* = *w*[0]
    × 0.11 + *w*[1] × 0.09 + *b*. In fact, the goal of training is to choose *w*[0],
    *w*[1], and *b* such that model outputs are as close as possible to the known
    outputs; that is, *y* = *w*[0] × 0.11 + *w*[1] × 0.09 + *b* should be as close
    to −0.8 as possible, *y* = *w*[0] × 0.01 + *w*[1] × 0.02 + *b* should be as close
    to −0.97 as possible, that is,  in general, given an input instance ![](../../OEBPS/Images/eq_02-05-a.png),
    the model output is *y* = *x*[0]*w*[0] + *x*[1]*w*[1] + *b*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will keep returning to this model throughout the chapter. But first, let’s
    consider a different question. In this toy example, we have only three model parameters:
    two weights, *w*[0], *w*[1], and one bias *b*. Hence it is not very messy to write
    the model output flat out as *y* = *x*[0]*w*[0] + *x*[1]*w*[1] + *b*. But, with
    longer feature vectors (that is, more weights) it will become unwieldy. Is there
    a compact way to represent the model output for a specific input instance, irrespective
    of the size of the input?'
  prefs: []
  type: TYPE_NORMAL
- en: Turns out the answer is yes—we can use an operation called *dot product* from
    the world of mathematics. We have already seen in section [2.1](02.xhtml#sec-vectors)
    that an individual instance of model input can be compactly represented by a vector,
    say ![](../../OEBPS/Images/AR_x.png) (it can have any number of input values).
    We can also represent the set of weights as vector ![](../../OEBPS/Images/AR_w.png)—it
    will have the same number of items as the input vector. The dot product is simply
    the element-wise multiplication of the two vectors ![](../../OEBPS/Images/AR_x.png)
    and ![](../../OEBPS/Images/AR_w.png). Formally, given two vectors and ![](../../OEBPS/Images/eq_02-05-b2.png)
    and ![](../../OEBPS/Images/eq_02-05-c2.png), the dot product of the two vectors
    is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-06.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 2.6
  prefs: []
  type: TYPE_NORMAL
- en: In other words, the sum of the products of corresponding elements of the two
    vectors is the dot product of the two vectors, denoted ![](../../OEBPS/Images/AR_a.png)
    ⋅ ![](../../OEBPS/Images/AR_b.png).
  prefs: []
  type: TYPE_NORMAL
- en: NOTE The dot product notation can compactly represent the model output as *y*
    = ![](../../OEBPS/Images/AR_w.png) ⋅ ![](../../OEBPS/Images/AR_x.png) + *b*. The
    representation does not increase in size even when the number of inputs and weights
    is large.
  prefs: []
  type: TYPE_NORMAL
- en: Consider our (by now familiar) cat-brain example again. Suppose the weight vector
    is ![](../../OEBPS/Images/eq_02-06-a2.png) and the bias value *b* = 5. Then the
    model output for the 0th input instance from table [2.2](02.xhtml#tab-cat-brain-training-data)
    will be ![](../../OEBPS/Images/eq_02-06-b2.png). It is another matter that these
    are bad choices for weight and bias parameters, since the model output 5.51 is
    a far cry from the desired output −0.89. We will soon see how to obtain better
    parameter values. For now, we just need to note that the dot product offers a
    neat way to represent the simple weighted sum model output.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE The dot product is defined only if the vectors have the same dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes the dot product is also referred to as *inner product*, denoted ⟨![](../../OEBPS/Images/AR_a.png),
    ![](../../OEBPS/Images/AR_b.png)⟩. Strictly speaking, the phrase *inner product*
    is a bit more general; it applies to infinite-dimensional vectors as well. In
    this book, we will often use the terms interchangeably, sacrificing mathematical
    rigor for enhanced understanding.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.3 Matrix multiplication and machine learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vectors are special cases of matrices. Hence, matrix-vector multiplication is
    a special case of matrix-matrix multiplication. We will start with that.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix-vector multiplication
  prefs: []
  type: TYPE_NORMAL
- en: In section [2.5.2](02.xhtml#subsec-dotprod-ml), we saw that given a weight vector,
    say ![](../../OEBPS/Images/eq_02-06-c2.png), and the bias value *b* = 5, the weighted
    sum model output upon a single input instance, say ![](../../OEBPS/Images/eq_02-06-d2.png),
    can be represented using a vector-vector dot product ![](../../OEBPS/Images/eq_02-06-e2.png).
    As depicted in equation [2.1](02.xhtml#eq-cat-brain-toy-training-dataset), during
    training, we are dealing with many training data instances at the same time. In
    real life, we typically deal with hundreds of thousands of input instances, each
    having hundreds of values. Is there a way to represent the model output for the
    entire training dataset compactly, such that it is independent of the count of
    input instances and their sizes?
  prefs: []
  type: TYPE_NORMAL
- en: 'The answer turns out to be yes. We can use the idea of matrix-vector multiplication
    from the world of mathematics. The product of a matrix *X* and column vector ![](../../OEBPS/Images/AR_w.png)
    is another vector, denoted *X*![](../../OEBPS/Images/AR_w.png). Its elements are
    the dot products between the row vectors of *X* and the column vector ![](../../OEBPS/Images/AR_w.png).
    For example, given the model weight vector ![](../../OEBPS/Images/eq_02-06-f2.png)
    and the bias value *b* = 5, the outputs on the toy training dataset of our familiar
    cat-brain model (equation [2.1](02.xhtml#eq-cat-brain-toy-training-dataset)) can
    be obtained via the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-07.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 2.7
  prefs: []
  type: TYPE_NORMAL
- en: Adding the bias value of 5, the model output on the toy training dataset is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-08.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 2.8
  prefs: []
  type: TYPE_NORMAL
- en: In general, the output of our simple model (biased weighted sum of input elements)
    can be expressed compactly as ![](../../OEBPS/Images/AR_y.png) = *X*![](../../OEBPS/Images/AR_w.png)
    + ![](../../OEBPS/Images/AR_b.png).
  prefs: []
  type: TYPE_NORMAL
- en: Matrix-matrix multiplication
  prefs: []
  type: TYPE_NORMAL
- en: 'Generalizing the notion of matrix times vector, we can define matrix times
    matrix. A matrix with *m* rows and *p* columns, say *A[m, p]*, can be multiplied
    with another matrix with *p* rows and *n* columns, say *B[p, n]*, to generate
    a matrix with *m* rows and *n* columns, say *C[m, n]*: for example, *C[m, n]*
    = *A[m, p]* *B[p, n]*. Note that the number of columns in the left matrix must
    match the number of rows in the right matrix. Element *i, j* of the result matrix,
    *C[i, j]*, is obtained by point-wise multiplication of the elements of the *i*th
    row vector of *A* and the *j*th column vector of *B*. The following example illustrates
    the idea:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-08-a.png)'
  prefs: []
  type: TYPE_IMG
- en: The computation for *C*[2, 1] is shown via bolding by way of example.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Matrix multiplication is not commutative. In general, *AB* ≠ *BA*.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, the astute reader may already have noted that the dot product
    is a special case of matrix multiplication. For instance, the dot product between
    two vectors ![](../../OEBPS/Images/eq_02-08-b2.png) and ![](../../OEBPS/Images/eq_02-08-c2.png)
    is equivalent to transposing either of the two vectors and then doing a matrix
    multiplication with the other. In other words,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-08-d.png)'
  prefs: []
  type: TYPE_IMG
- en: The idea works in higher dimensions, too. In general, given two vectors ![](../../OEBPS/Images/eq_02-08-e2.png)
    and ![](../../OEBPS/Images/eq_02-08-f2.png), the dot product of the two vectors
    is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-09.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 2.9
  prefs: []
  type: TYPE_NORMAL
- en: Another special case of matrix multiplication is row-vector matrix multiplication.
    For example, ![](../../OEBPS/Images/AR_b.png)*^TA* = ![](../../OEBPS/Images/AR_c.png)
    or
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-09-a.png)'
  prefs: []
  type: TYPE_IMG
- en: Transpose of matrix products
  prefs: []
  type: TYPE_NORMAL
- en: 'Given two matrices *A* and *B*, where the number of columns in *A* matches
    the number of rows in *B* (that is, it is possible to multiply them), the transpose
    of the product is the product of the individual transposes, in reversed order.
    The rule also applies to matrix-vector multiplication. The following equations
    capture this rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-10.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 2.10
  prefs: []
  type: TYPE_NORMAL
- en: '2.5.4 Length of a vector (L2 norm): Model error'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imagine that a machine learning model is supposed to output a target value *ȳ*,
    but it outputs *y* instead. We are interested in the *error* made by the model.
    The error is the difference between the target and the actual outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Squared error
  prefs: []
  type: TYPE_NORMAL
- en: When a computing error occurs, we are only interested in how far the computed
    value is from ideal. We do not care whether the computed value is bigger or smaller
    than ideal. For instance, if the target (ideal) value is 2, the computed values
    1.5 and 2.5 are equally in error—we are equally happy or unhappy with either of
    them. Hence, it is common practice to *square* error values. Thus for instance,
    if the target value is 2 and the computed value is 1.5, the error is (1.5 − 2)²
    = 0.25. If the target value is 2.5, the error is (2.5 − 2)² = 0.25. The squaring
    operation essentially eliminates the sign of the error value. We can then follow
    it up with a square root, but it is OK not to.
  prefs: []
  type: TYPE_NORMAL
- en: 'You might ask, “But wait: squaring alters the value of the quantity. Don’t
    we care about the exact value of the error?” The answer is, we usually don’t;
    we only care about *relative* values of errors. If the target is 2, we want the
    error for an output value of, say, 2.1 to be less than the error for an output
    value of 2.5; the exact values of the errors do not matter.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s apply this idea of squaring to machine learning model error. As seen earlier
    in section [2.5.3](02.xhtml#subsec-matmul-ml), given a model weight vector, say
    ![](../../OEBPS/Images/eq_02-10-a2.png), and the bias value *b* = 5, the weighted
    sum model output upon a single input instance, say ![](../../OEBPS/Images/eq_02-10-b2.png),
    is ![](../../OEBPS/Images/eq_02-10-c2.png). The corresponding target (ideal) output,
    from table [2.2](02.xhtml#tab-cat-brain-training-data), is −0.8. The squared error
    *e*² = (−0.8−5.51)² = 39.82 gives us an idea of how good or bad the model parameters
    3, 2, 5 are. For instance, if we instead use a weight vector ![](../../OEBPS/Images/eq_02-10-d2.png)
    and bias value −1, we get model output ![](../../OEBPS/Images/eq_02-10-e2.png).
    The output is exactly the same as the target. The corresponding squared error
    *e*² = (−0.8−(−0.8))² = 0. This (zero error) immediately tells us that 1, 1, −1
    are much better choices of model parameters than 3, 2, 5.
  prefs: []
  type: TYPE_NORMAL
- en: In general, the error made by a biased weighted sum model can be expressed as
    follows. If ![](../../OEBPS/Images/AR_w.png) denotes the weight vector and ![](../../OEBPS/Images/AR_b.png)
    denotes the bias, the output corresponding to an input instance ![](../../OEBPS/Images/AR_x.png)
    can be expressed as *y* = ![](../../OEBPS/Images/AR_w.png) ⋅ ![](../../OEBPS/Images/AR_x.png)
    + *b*. Let *ȳ* denote the corresponding target (ground truth). Then the error
    is defined as *e* = (*y*−*ȳ*)².
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus we see that we can compute the error on a single training instance by
    taking the difference between the model output and the ground truth and squaring
    it. How do we extend this concept over the entire training dataset? The set of
    outputs corresponding to the entire set of training inputs can be expressed as
    the output vector *y* = *X*![](../../OEBPS/Images/AR_w.png) + ![](../../OEBPS/Images/AR_b.png).
    The corresponding target output vector, consisting of the entire set of ground
    truths can be expressed as ![](../../OEBPS/Images/AR_y2.png). The differences
    between the target and model output over the entire training set can be expressed
    as another vector ![](../../OEBPS/Images/AR_y2.png) - ![](../../OEBPS/Images/AR_y.png).
    In our particular example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-10-f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus the total error over the entire training dataset is obtained by taking
    the difference between the output and the ground truth vector, squaring its elements
    and adding them up. Recalling equation [2.9](02.xhtml#eq-dotprod-matmul), this
    is exactly what will happen if we take the *dot product of the difference vector
    with itself*. That happens to be the definition of the *squared magnitude* or
    *length* or *L2 norm* of a vector: the dot product of the vector with itself.
    In the previous example, the overall training (squared) error is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-10-g.png)'
  prefs: []
  type: TYPE_IMG
- en: Formally, the length of a vector ![](../../OEBPS/Images/eq_02-10-h2.png), denoted
    ||![](../../OEBPS/Images/AR_v.png)||, is defined as ![](../../OEBPS/Images/eq_02-10-i2.png).
    This quantity is sometimes called the L2 norm of the vector.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, given a machine learning model with output vector ![](../../OEBPS/Images/AR_y.png)
    and a target vector ![](../../OEBPS/Images/AR_y2.png), the error is the same as
    the magnitude or L2 norm of the difference vector
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-10-j.png)'
  prefs: []
  type: TYPE_IMG
- en: 2.5.5 Geometric intuitions for vector length
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For a 2D vector ![](../../OEBPS/Images/eq_02-10-k2.png), as seen in figure [2.2](02.xhtml#fig-vector_diagram),
    the L2 norm ![](../../OEBPS/Images/eq_02-10-l2.png) is nothing but the hypotenuse
    of the right-angled triangle whose sides are elements of the vector. The same
    intuition holds in higher dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: A *unit vector* is a vector whose length is 1\. Given any vector ![](../../OEBPS/Images/AR_v.png),
    the corresponding unit vector can be obtained by dividing every element by the
    length of that vector. For example, given ![](../../OEBPS/Images/eq_02-10-m2.png),
    length ![](../../OEBPS/Images/eq_02-10-n2.png) and the corresponding unit vector
    ![](../../OEBPS/Images/eq_02-10-o2.png). Unit vectors typically represent a direction.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Unit vectors are conventionally depicted with the hat symbol as opposed
    to the little overhead arrow, as in *û^Tû* = 1.
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, the goal of training is often to minimize the length of
    the error vector (the difference between the model output vector and the target
    ground truth vector).
  prefs: []
  type: TYPE_NORMAL
- en: '2.5.6 Geometric intuitions for the dot product: Feature similarity'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Consider the document retrieval problem depicted in table [2.1](02.xhtml#tab-doc-vec)
    one more time. We have a set of documents, each described by its own feature vector.
    Given a pair of such documents, we must find their similarity. This essentially
    boils down to estimating the similarity between two feature vectors. In this section,
    we will see that the dot product between a pair of vectors can be used as a measure
    of similarity between them.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, consider the feature vectors corresponding to *d*[5] and *d*[6]
    in table [2.1](02.xhtml#tab-doc-vec). They are ![](../../OEBPS/Images/eq_02-10-p2.png)
    and ![](../../OEBPS/Images/eq_02-10-q2.png). The dot product between them is 1
    × 0 + 0 × 1 = 0. This is low and agrees with our intuition that there is no common
    word of interest between them, so the documents are very dissimilar. On the other
    hand, the dot product between feature vectors of *d*[3] and *d*[4] is ![](../../OEBPS/Images/eq_02-10-r2.png).
    This is high and agrees with our intuition that they have many commonalities in
    words of interest and are similar documents. Thus, we get the first glimpse of
    an important concept. Loosely speaking, *similar vectors have larger dot products,
    and dissimilar vectors have near-zero dot products.*
  prefs: []
  type: TYPE_NORMAL
- en: We will keep revisiting this problem of estimating similarity between feature
    vectors and solve it with more and more finesse. As a first attempt, we will now
    study in greater detail how dot products measure similarities between vectors.
    First we will show that the component of a vector along another is yielded by
    the dot product. Using this, we will show that the “similarity/agreement” between
    a pair of vectors can be estimated using the dot product between them. In particular,
    we will see that if the vectors point in more or less the same direction, their
    dot products are higher than when the vectors are perpendicular to each other.
    If the vectors point in opposite directions, their dot product is negative.
  prefs: []
  type: TYPE_NORMAL
- en: Dot product measures the component of one vector along another
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s examine a special case first: the component of a vector along a coordinate
    axis. This can be obtained by multiplying the length of the vector with the cosine
    of the angle between the vector and the relevant coordinate axis. As shown for
    2D in figure [2.7a](02.xhtml#ch2fig-vec_component), a vector ![](../../OEBPS/Images/AR_v.png)
    can be broken into two components along the *X* and *Y* axes as'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-10-s.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note how the length of the vector is preserved:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-10-t.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](../../OEBPS/Images/CH02_F07a_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Components of a 2D vector along coordinate axes. Note that ||![](../../OEBPS/Images/AR_a.png)||
    is the length of hypotenuse.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH02_F07b_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Dot product as a component of one vector along another ![](../../OEBPS/Images/AR_a.png)
    ⋅ ![](../../OEBPS/Images/AR_b.png) = ![](../../OEBPS/Images/AR_a.png)*^T*![](../../OEBPS/Images/AR_b.png)
    = *a[x]b[x]* + *a[y]b[y]* = ||![](../../OEBPS/Images/AR_a.png)|| ||![](../../OEBPS/Images/AR_b.png)||*cos*(*θ*).
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.7 Vector components and dot product
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s study the more general case of the component of one vector in the
    direction of another arbitrary vector (figure [2.7b](02.xhtml#ch2fig-vec_component)).
    The component of a vector ![](../../OEBPS/Images/AR_a.png) along another vector
    ![](../../OEBPS/Images/AR_b.png) is ![](../../OEBPS/Images/AR_a.png) ⋅ ![](../../OEBPS/Images/AR_b.png)
    = ![](../../OEBPS/Images/AR_a.png)*^T*![](../../OEBPS/Images/AR_b.png). This is
    equivalent to ||![](../../OEBPS/Images/AR_a.png)|| ||![](../../OEBPS/Images/AR_b.png)||*cos*(*θ*),
    where *θ* is the angle between the vectors ![](../../OEBPS/Images/AR_a.png) and
    ![](../../OEBPS/Images/AR_b.png). (This has been proven for the two-dimension
    case discussed in section [A.1](../Text/A.xhtml#sec-dotprod-cosine-proof) of the
    appendix. You can read it if you would like deeper intuition.)
  prefs: []
  type: TYPE_NORMAL
- en: Dot product measures the agreement between two vectors
  prefs: []
  type: TYPE_NORMAL
- en: The dot product can be expressed using the cosine of the angle between the vectors.
    Given two vectors ![](../../OEBPS/Images/AR_a.png) and ![](../../OEBPS/Images/AR_b.png),
    if *θ* is the angle between them, we have see figure [2.7b](02.xhtml#ch2fig-vec_component))
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-11.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 2.11
  prefs: []
  type: TYPE_NORMAL
- en: Expressing the dot product using cosines makes it easier to see that it measures
    the *agreement* (aka *correlation*) between two vectors. If the vectors have the
    same direction, the angle between them is 0 and the cosine is 1, implying maximum
    agreement. The cosine becomes progressively smaller as the angle between the vectors
    increases, until the two vectors become perpendicular to each other and the cosine
    is zero, implying no correlation—the vectors are independent of each other. If
    the angle between them is 180°, the cosine is −1, implying that the vectors are
    anti-correlated. Thus, the dot product of two vectors is proportional to their
    directional agreement.
  prefs: []
  type: TYPE_NORMAL
- en: What role do the vector lengths play in all this? The dot product between two
    vectors is also proportional to the lengths of the vectors. This means agreement
    scores between bigger vectors are higher (an agreement between the US president
    and the German chancellor counts more than an agreement between you and me).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want the agreement score to be neutral to the vector length, you can
    use a normalized dot product between unit-length vectors along the same directions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-11-a.png)'
  prefs: []
  type: TYPE_IMG
- en: Dot product and the difference between two unit vectors
  prefs: []
  type: TYPE_NORMAL
- en: To obtain further insight into how the dot product indicates agreement or correlation
    between two directions, consider the two unit vectors ![](../../OEBPS/Images/eq_02-11-b2.png)
    and ![](../../OEBPS/Images/eq_02-11-c2.png). The difference between them is ![](../../OEBPS/Images/eq_02-11-d2.png).
  prefs: []
  type: TYPE_NORMAL
- en: Note that since they are unit vectors, ![](../../OEBPS/Images/eq_02-11-e2.png).
    The length of the difference vector
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-11-f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the last equality, it is evident that a larger dot product implies a smaller
    difference: that is, more agreement between the vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.6 Orthogonality of vectors and its physical significance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Try moving an object at right angles to the direction in which you are pushing
    it. You will find it impossible. The larger the angle, the less effective your
    force vector becomes (finally becoming totally ineffective at a 90° angle). This
    is why it is easy to walk on a horizontal surface (you are moving at right angles
    to the direction of gravitational pull, so the gravity vector is ineffective)
    but harder on an upward incline (the gravity vector is having some effect against
    you).
  prefs: []
  type: TYPE_NORMAL
- en: These physical notions are captured mathematically in the notion of a dot product.
    The dot product between two vectors ![](../../OEBPS/Images/AR_a.png) (say, the
    push vector) and ![](../../OEBPS/Images/AR_b.png) (say, the displacement of the
    pushed object vector) is ||![](../../OEBPS/Images/AR_a.png)|| ||![](../../OEBPS/Images/AR_b.png)||*cosθ*,
    where *θ* is the angle between the two vectors. When *θ* is 0 (the two vectors
    are aligned), *cosθ* = 1, the maximum possible value of *cosθ*, so push is maximally
    effective. As *θ* increases, *cosθ* decreases, and push becomes less and less
    effective. Finally, at *θ* = 90°, *cosθ* = 0, and push becomes completely ineffective.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two vectors are orthogonal if their dot product is zero. Geometrically, this
    means the vectors are perpendicular to each other. Physically, this means the
    two vectors are independent: one cannot influence the other. You can say there
    is nothing in common between orthogonal vectors. For instance, the feature vector
    for *d*[5] is ![](../../OEBPS/Images/eq_02-10-p2.png) and that for *d*[6] is ![](../../OEBPS/Images/eq_02-10-q2.png)
    in table [2.1](02.xhtml#tab-doc-vec). These are orthogonal (dot product is zero),
    and you can easily see that none of the feature words (*gun*, *violence*) are
    common to both documents.'
  prefs: []
  type: TYPE_NORMAL
- en: '2.7 Python code: Basic vector and matrix operations via PyTorch'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we use Python PyTorch code to illustrate many of the concepts
    discussed earlier.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional code for this section, executable via Jupyter Notebook,
    can be found at [http://mng.bz/ryzE](https://github.com/krishnonwork/mathematical-methods-in-deep-learning-ipython/blob/master/python/ch2/2.7-transpose-dot-matmul.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: 2.7.1 PyTorch code for a matrix transpose
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following listing shows the PyTorch code for a matrix transpose.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.5 Transpose
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: ① The torch.arange function creates a vector whose elements go from *start*
    to *stop* in increments of *step*. Here we create a 4 × 9 image corresponding
    to *I*[4,9] in equation [2.2](02.xhtml#eq-tiny_im), shown in figure [2.3](02.xhtml#fig-tiny_im).
  prefs: []
  type: TYPE_NORMAL
- en: ② The transpose operator interchanges rows and columns. The 4 × 9 image becomes
    a 9 × 4 image (see figure [2.6](02.xhtml#fig-tiny_im-transpose). The element at
    position (*i, j*) is interchanged with the element at position (*j, i*).
  prefs: []
  type: TYPE_NORMAL
- en: ③ Interchanged elements of the original and transposed matrix are equal.
  prefs: []
  type: TYPE_NORMAL
- en: ④ The .T operator retrieves the transpose of an array.
  prefs: []
  type: TYPE_NORMAL
- en: 2.7.2 PyTorch code for a dot product
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The dot product of two vectors ![](../../OEBPS/Images/AR_a.png) and ![](../../OEBPS/Images/AR_b.png)
    represents the components of one vector along the other. Consider two vectors
    ![](../../OEBPS/Images/AR_a.png) = [*a*[1] *a*[2] *a*[3]] and ![](../../OEBPS/Images/AR_b.png)
    = [*b*[1] *b*[2] *b*[3]]. Then ![](../../OEBPS/Images/AR_a.png).![](../../OEBPS/Images/AR_b.png)
    = *a*[1]*b*[1] + *a*[2]*b*[2] + *a*[3]*b*[3].
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.6 Dot product
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '① Outputs 32: 1 ∗ 4 + 2 ∗ 5 + 3 ∗ 6'
  prefs: []
  type: TYPE_NORMAL
- en: '② Outputs 0: 1 ∗ 0 + 0 ∗ 1'
  prefs: []
  type: TYPE_NORMAL
- en: 2.7.3 PyTorch code for matrix vector multiplication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Consider a matrix *A[m, n]* with *m* rows and *n* columns that is multiplied
    with a vector *![](../../OEBPS/Images/AR_b.png)[n]* with *n* elements. The result
    is a *m* element column vector *![](../../OEBPS/Images/AR_c.png)[m]* . In the
    following example, *m* = 3 and *n* = 2.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-11-g.png)'
  prefs: []
  type: TYPE_IMG
- en: In general,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-11-h.png)'
  prefs: []
  type: TYPE_IMG
- en: Listing 2.7 Matrix vector multiplication
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: ① A linear model comprises a weight vector ![](../../OEBPS/Images/AR_w.png)
    and bias *b*. For each training data instance *![](../../OEBPS/Images/AR_x.png)[i]*,
    the model outputs *y**[i]* = ![](../../OEBPS/Images/AR_x.png)*[i]^T*![](../../OEBPS/Images/AR_w.png)
    + *b*. For the training data matrix *X* (whose rows are training data instances),
    the model outputs *X*![](../../OEBPS/Images/AR_w.png) + ![](../../OEBPS/Images/AR_b.png)
    = ![](../../OEBPS/Images/AR_y.png)
  prefs: []
  type: TYPE_NORMAL
- en: ② Cat-brain 15 × 2 training data matrix (equation [2.7](02.xhtml#eq-cat-brain-nobiased))
  prefs: []
  type: TYPE_NORMAL
- en: ③ Random initialization of weight vector
  prefs: []
  type: TYPE_NORMAL
- en: '④ Model training output: ![](../../OEBPS/Images/AR_y.png) = *X*![](../../OEBPS/Images/AR_w.png)
    + *b*. The scalar *b* is automatically replicated to create a vector.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.7.4 PyTorch code for matrix-matrix multiplication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Consider a matrix *A[m, p]* with *m* rows and *p* columns. Let’s multiply it
    with another matrix *B[p, n]* with *p* rows and *n* columns. The resultant matrix
    *C[m, n]* contains *m* rows and *n* columns. Note that the number of columns in
    the left matrix *A* should match the number of rows in the right matrix *B*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-11-i.png)'
  prefs: []
  type: TYPE_IMG
- en: In general,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-11-j.png)'
  prefs: []
  type: TYPE_IMG
- en: Listing 2.8 Matrix-matrix multiplication
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: ① *C* = *AB* ⟹ *C*[*i*, *j*] is the dot product of the *i*th row of *A* and
    *j*th column of B.
  prefs: []
  type: TYPE_NORMAL
- en: ② ![](../../OEBPS/Images/eq_02-11-k.png)
  prefs: []
  type: TYPE_NORMAL
- en: ③ The dot product can be viewed as a row matrix multiplied by a column matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 2.7.5 PyTorch code for the transpose of a matrix product
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Given two matrices *A* and *B*, where the number of columns in *A* matches
    the number of rows in *B*, the transpose of their product is the product of the
    individual transposes *in reversed order*: (*AB*)*^T* = *B^TA^T*.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.9 Transpose of a matrix product
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: ① Asserts equality between (*AB*)*^T* and *B^TA^T*
  prefs: []
  type: TYPE_NORMAL
- en: '② Applies to matrix-vector multiplication, too: (*A^T*![](../../OEBPS/Images/AR_x.png))*^T*
    = ![](../../OEBPS/Images/AR_x.png)*^TA*'
  prefs: []
  type: TYPE_NORMAL
- en: 2.8 Multidimensional line and plane equations and machine learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Geometrically speaking, what does a machine learning classifier really do? We
    provided the outline of an answer in section [1.4](../Text/01.xhtml#sec-geom-view-ml).
    You are invited to review that and especially figures [1.2](../Text/01.xhtml#fig-geometrical_view)
    and [1.3](../Text/01.xhtml#fig-ml-as-mapping). We will briefly summarize here.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs to a classifier are feature vectors. These vectors can be viewed as points
    in some multidimensional feature space. The task of classification then boils
    down to separating the points belonging to different classes. The points may be
    all jumbled up in the input space. It is the model’s job to transform them into
    a different (output) space where it is easier to separate the classes. A visual
    example of this was provided in figure [1.3](../Text/01.xhtml#fig-ml-as-mapping).
  prefs: []
  type: TYPE_NORMAL
- en: What is the geometrical nature of the separator? In a very simple situation,
    such as the one depicted in figure [1.2](../Text/01.xhtml#fig-geometrical_view),
    the separator is a line in 2D space. In real-life situations, the separator is
    often a line or a plane in a high-dimensional space. In more complicated situations,
    the separator is a curved surface, as depicted in figure [1.4](../Text/01.xhtml#fig-non_linear_separator).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will study the mathematics and geometry behind two types
    of separators, lines, and planes in high-dimensional spaces, aka hyperlines and
    hyperplanes.
  prefs: []
  type: TYPE_NORMAL
- en: 2.8.1 Multidimensional line equation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In high school geometry, we learned *y* = *mx* + *c* as the equation of a line.
    But this does not lend itself readily to higher dimensions. Here we will study
    a better representation of a straight line that works equally well for any finite-dimensional
    space.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in figure [2.8](02.xhtml#fig-multi-dim-lineeq), a line joining vectors
    ![](../../OEBPS/Images/AR_a.png) and ![](../../OEBPS/Images/AR_b.png) can be viewed
    as the set of points we will encounter if we
  prefs: []
  type: TYPE_NORMAL
- en: Start at point ![](../../OEBPS/Images/AR_a.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Travel along the direction ![](../../OEBPS/Images/AR_b.png) − ![](../../OEBPS/Images/AR_a.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH02_F08_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8 Any point ![](../../OEBPS/Images/AR_x.png) on the line joining two
    vectors ![](../../OEBPS/Images/AR_a.png), ![](../../OEBPS/Images/AR_b.png) is
    given by ![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_a.png) +
    *α*(![](../../OEBPS/Images/AR_b.png)−![](../../OEBPS/Images/AR_a.png)).
  prefs: []
  type: TYPE_NORMAL
- en: Different points on the line are obtained by traveling different distances.
    Denoting this arbitrary distance by *α*, the equation of the line joining vectors
    ![](../../OEBPS/Images/AR_a.png) and ![](../../OEBPS/Images/AR_b.png) can be expressed
    as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-12.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 2.12
  prefs: []
  type: TYPE_NORMAL
- en: Equation [2.12](02.xhtml#eq-collinearity) says that any point on the line joining
    ![](../../OEBPS/Images/AR_a.png) and ![](../../OEBPS/Images/AR_b.png) can be obtained
    as a weighted combination of ![](../../OEBPS/Images/AR_a.png) and ![](../../OEBPS/Images/AR_b.png),
    the weights being *α* and 1 − *α*. By varying *α*, we obtain different points
    on the line. Also, different ranges of *α* values yield different segments on
    the line. As shown in figure [2.8](02.xhtml#fig-multi-dim-lineeq), values of *α*
    between 0 and 1 yield points between ![](../../OEBPS/Images/AR_a.png) and ![](../../OEBPS/Images/AR_b.png).
    Negative values of *α* yield points to the left of ![](../../OEBPS/Images/AR_a.png).
    Values of *α* greater than 1 yield points to the right of ![](../../OEBPS/Images/AR_b.png).
    This equation for a line works for any dimensions, not just two.
  prefs: []
  type: TYPE_NORMAL
- en: 2.8.2 Multidimensional planes and their role in machine learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In section [1.5](../Text/01.xhtml#sec-regression-vs-classification), we encountered
    classifiers. Let’s take another look at them. Suppose we want to create a classifier
    that helps us make *buy* or *no-buy* decisions on stocks based on only three input
    variables: 1) *momentum*, or the rate at which the stock price is changing positive
    momentum means the stock price is increasing and vice versa); 2) the *dividend*
    paid last quarter; and (3) *volatility*, or how much the price has fluctuated
    in the last quarter. Let’s plot all training points in the feature space with
    coordinate axes corresponding to the variables *momentum*, *dividend*, *volatility*.
    Figure [2.9](02.xhtml#fig-planar-classifier) shows that the classes can be separated
    by a plane in the three-dimensional feature space.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH02_F09_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.9 A toy machine learning classifier for stock buy vs. no-buy decision-making.
    A plus (+) indicates no-buy, and a dash (-) indicates buy. The decision is made
    based on three input variables: momentum, dividend, and volatility.'
  prefs: []
  type: TYPE_NORMAL
- en: Geometrically speaking, our model simply corresponds to this plane. Input points
    above the plane indicate buy decisions (dashes [-]), and input points indicate
    no-buy decisions (pluses [+]). In general, you want to buy high-positive-momentum
    stocks, so points at the higher end of the momentum axis are likelier to be *buy*.
    However, this is not the only indicator. For more volatile stocks, we demand higher
    *momentum* to switch from *no-buy* to *buy*. This is why the plane slopes upward
    (higher *momentum*) as we move rightward (higher *volatility*). Also, we demand
    less *momentum* for stocks with higher *dividends*. This is why the plane slopes
    downward (lower *momentum*) as we go toward higher *dividends*.
  prefs: []
  type: TYPE_NORMAL
- en: Real problems have more dimensions, of course (since many more inputs are involved
    in the decision), and the separator becomes a hyperplane. Also, in real-life problems,
    the points are often too intertwined in the input space for any separator to work.
    We first have to apply a transformation that maps the point to an output space
    where it is easier to separate. Given their significance as class separators in
    machine learning, we will study hyperplanes in this section.
  prefs: []
  type: TYPE_NORMAL
- en: In high school 3D geometry, we learned *ax* + *by* + *cz* + *d* = 0 as the equation
    of a plane. Now we will study a version of it that works in higher dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Geometrically speaking, given a plane (in any dimension), we can find a direction
    called the *normal direction*, denoted *n̂*, such that
  prefs: []
  type: TYPE_NORMAL
- en: If we take any pair of points on the plane, say ![](../../OEBPS/Images/AR_x.png)[0]
    and ![](../../OEBPS/Images/AR_x.png), …
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The line joining ![](../../OEBPS/Images/AR_x.png) and ![](../../OEBPS/Images/AR_x.png)[0]—i.e.,
    the vector ![](../../OEBPS/Images/AR_x.png) − ![](../../OEBPS/Images/AR_x.png)[0]—is
    orthogonal to *n̂*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, if we know a fixed point on the plane, say ![](../../OEBPS/Images/AR_x.png)[0],
    then all points on the plane will satisfy
  prefs: []
  type: TYPE_NORMAL
- en: '*n̂* · (![](../../OEBPS/Images/AR_x.png) − ![](../../OEBPS/Images/AR_x.png)[0])
    = 0'
  prefs: []
  type: TYPE_NORMAL
- en: or
  prefs: []
  type: TYPE_NORMAL
- en: '*n̂^T*(![](../../OEBPS/Images/AR_x.png) − ![](../../OEBPS/Images/AR_x.png)[0])
    = 0'
  prefs: []
  type: TYPE_NORMAL
- en: Thus we can express the equation of a plane as
  prefs: []
  type: TYPE_NORMAL
- en: '*n̂^T*![](../../OEBPS/Images/AR_x.png) − *n̂^T*![](../../OEBPS/Images/AR_x.png)[0]
    = 0'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 2.13
  prefs: []
  type: TYPE_NORMAL
- en: Equation [2.13](02.xhtml#eq-plane-0) is depicted pictorially in figure [2.10](02.xhtml#fig-multi-dim-planeeq).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH02_F10_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 The normal to the plane is the same at all points on the plane.
    This is the fundamental property of a plane. *n̂* depicts that normal direction.
    Let ![](../../OEBPS/Images/AR_x.png)[0] be a point on the plane. All other points
    on the plane, depicted as ![](../../OEBPS/Images/AR_x.png), will satisfy the equation
    (![](../../OEBPS/Images/AR_x.png)−![](../../OEBPS/Images/AR_x.png)[0]) ⋅ *n̂*
    = 0. This physically says that the line joining a known point ![](../../OEBPS/Images/AR_x.png)[0]
    on the plane and any other arbitrary point ![](../../OEBPS/Images/AR_x.png) on
    the plane is at right angles to the normal *n̂*. This formulation works for any
    dimension.
  prefs: []
  type: TYPE_NORMAL
- en: 'In section [1.3](../Text/01.xhtml#sec-cat_brain), equation [1.3](../Text/01.xhtml#eq-linear-predictor),
    we encountered the simplest machine learning model: a weighted sum of inputs along
    with a bias. Denoting the inputs as ![](../../OEBPS/Images/AR_x.png), the weights
    as ![](../../OEBPS/Images/AR_w.png), and the bias as *b*, this model was depicted
    as'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png) + *b*
    = 0'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 2.14
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparing equations [2.13](02.xhtml#eq-plane-0) and [2.14](02.xhtml#eq-plane-1)
    , we get the geometric significance: the simple model of equation [1.3](../Text/01.xhtml#eq-linear-predictor)
    is nothing but a planar separator. Its weight vector ![](../../OEBPS/Images/AR_w.png)
    corresponds to the plane’s orientation (normal). The bias *b* corresponds to the
    plane’s location (a fixed point on the plane). During training, we are learning
    the weights and biases—this is essentially learning the orientation and position
    of the optimal plane that will separate the training inputs. To be consistent
    with the machine learning paradigm, henceforth we will write the equation of a
    hyperplane as equation [2.14](02.xhtml#eq-plane-1) for some constant ![](../../OEBPS/Images/AR_w.png)
    and *b*.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that ![](../../OEBPS/Images/AR_w.png) need not be a unit-length vector.
    Since the right-hand side is zero, if necessary, we can divide both sides by ||![](../../OEBPS/Images/AR_w.png)||
    to convert to a form like equation [2.13](02.xhtml#eq-plane-0).
  prefs: []
  type: TYPE_NORMAL
- en: The sign of the expression ![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png)
    + *b* has special significance. All points ![](../../OEBPS/Images/AR_x.png) for
    which ![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png) + *b*
    < 0 lie on the same side of the hyperplane. All points ![](../../OEBPS/Images/AR_x.png)
    for which ![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png)
    + *b* > 0 lie on the other side of the hyperplane. And of course, all points ![](../../OEBPS/Images/AR_x.png)
    for which ![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png)
    + *b* = 0 lie on the hyperplane.
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that the 3D equation *ax* + *by* + *cz* + *d* = 0 is a special
    case of equation [2.14](02.xhtml#eq-plane-1) because *ax* + *by* + *cz* + *d*
    = 0 can be rewritten as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-14-a.png)'
  prefs: []
  type: TYPE_IMG
- en: which is same as ![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png)
    + *b* = 0 with ![](../../OEBPS/Images/eq_02-14-b2.png) and ![](../../OEBPS/Images/eq_02-14-c2.png).
    Incidentally, this tells us that in 3D, the normal to the plane *ax* + *by* +
    *cz* + *d* = 0 is ![](../../OEBPS/Images/eq_02-14-d2.png) .
  prefs: []
  type: TYPE_NORMAL
- en: 2.9 Linear combinations, vector spans, basis vectors, and collinearity preservation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: by now, it should be clear that machine learning and data science are all about
    points in high-dimensional spaces. Consequently, it behooves us to have a decent
    understanding of these spaces. For instance, given a space, we may need to ask,
    “Would it be possible to express all points in the space in terms of a set of
    a few vectors? What is the smallest set of vectors we really need for that purpose?”
    This section is devoted to the study of these questions.
  prefs: []
  type: TYPE_NORMAL
- en: 2.9.1 Linear dependence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Consider the vectors (points) shown in figure [2.11](02.xhtml#fig-lin-dep).
    The corresponding vectors in 2D are
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-14-e.png)'
  prefs: []
  type: TYPE_IMG
- en: We can find four scalars *α*[0] = 2, *α*[1] = 2, *α*[2] = 2, and *α*[3] = −3
    such that
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-14-f.png)'
  prefs: []
  type: TYPE_IMG
- en: If we can find such scalars, not all zero, we say the vectors ![](../../OEBPS/Images/AR_v.png)[0],
    ![](../../OEBPS/Images/AR_v.png)[1], ![](../../OEBPS/Images/AR_v.png)[2], and
    ![](../../OEBPS/Images/AR_v.png)[3] are *linearly dependent*. The geometric picture
    to keep in mind is that points corresponding to linearly dependent vectors lie
    on a single straight line in the space containing them.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH02_F11_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.11 Linearly dependent points in a 2D plane
  prefs: []
  type: TYPE_NORMAL
- en: Collinearity implies linear dependence
  prefs: []
  type: TYPE_NORMAL
- en: 'Proof: Let ![](../../OEBPS/Images/AR_a.png), ![](../../OEBPS/Images/AR_b.png)
    and ![](../../OEBPS/Images/AR_c.png) be three collinear vectors. From equation
    [2.12](02.xhtml#eq-collinearity), there exists some *α* ∈ ℝ such that'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_c.png) = (1−*α*)![](../../OEBPS/Images/AR_a.png)
    + *α*![](../../OEBPS/Images/AR_b.png)'
  prefs: []
  type: TYPE_IMG
- en: This equation can be rewritten as
  prefs: []
  type: TYPE_NORMAL
- en: '*α*[1]![](../../OEBPS/Images/AR_a.png) + *α*[2]![](../../OEBPS/Images/AR_b.png)
    + *α*[3]![](../../OEBPS/Images/AR_c.png) = 0'
  prefs: []
  type: TYPE_NORMAL
- en: where *α*[1] = (1−*α*), *α*[2] = *α* and *α*[3] = −1. Thus we have proven that
    three collinear vectors ![](../../OEBPS/Images/AR_a.png), ![](../../OEBPS/Images/AR_b.png),
    and ![](../../OEBPS/Images/AR_c.png) must also be linearly dependent.
  prefs: []
  type: TYPE_NORMAL
- en: Linear combination
  prefs: []
  type: TYPE_NORMAL
- en: Given a set of vectors ![](../../OEBPS/Images/AR_v.png)[1], ![](../../OEBPS/Images/AR_v.png)[2],
    …. ![](../../OEBPS/Images/AR_v.png)*[n]* and a set of scalar weights *α*[1], *α*[2],
    …*α[n]*, the weighted sum *α*[1]![](../../OEBPS/Images/AR_v.png)[1] + *α*[2]![](../../OEBPS/Images/AR_v.png)[2]
    + + … *α**[n]*![](../../OEBPS/Images/AR_v.png)*[n]* is called a *linear combination*.
  prefs: []
  type: TYPE_NORMAL
- en: Generic multidimensional definition of linear dependence
  prefs: []
  type: TYPE_NORMAL
- en: A set of vectors ![](../../OEBPS/Images/AR_v.png)[1], ![](../../OEBPS/Images/AR_v.png)[2],
    …. ![](../../OEBPS/Images/AR_v.png)*[n]* are *linearly dependent* if there exists
    a set of weights *α*[1], *α*[2], …*α[n]*, not all zeros, such that *α*[1]![](../../OEBPS/Images/AR_v.png)[1]
    + *α*[2]![](../../OEBPS/Images/AR_v.png)[2] + + … *α**[n]*![](../../OEBPS/Images/AR_v.png)*[n]*
    = 0. For example, the row vectors [1   1] and [2   2] are linearly dependent,
    since –2[1   1] + [2   2] = 0.
  prefs: []
  type: TYPE_NORMAL
- en: 2.9.2 Span of a set of vectors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given a set of vectors ![](../../OEBPS/Images/AR_v.png)[1], ![](../../OEBPS/Images/AR_v.png)[2],
    …. ![](../../OEBPS/Images/AR_v.png)*[n]*, their *span* is defined as the set of
    all vectors that are linear combinations of the original set . This includes the
    original vectors.
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider the two vectors ![](../../OEBPS/Images/eq_02-14-i1b.png)
    and ![](../../OEBPS/Images/eq_02-14-i2b.png). The span of these two vectors is
    the entire plane containing the two vectors. Any vector, for instance, the vector
    ![](../../OEBPS/Images/eq_02-14-j2.png) can be expressed as a weighted sum 18![](../../OEBPS/Images/AR_v.png)[*x*⊥]
    + 97![](../../OEBPS/Images/AR_v.png)[*y*⊥].
  prefs: []
  type: TYPE_NORMAL
- en: You can probably recognize that ![](../../OEBPS/Images/eq_02-14-k1b.png) and
    ![](../../OEBPS/Images/eq_02-14-k2b.png) are the familiar Cartesian coordinate
    axes (*X*-axis and *Y*-axis, respectively) in the 2D plane.
  prefs: []
  type: TYPE_NORMAL
- en: 2.9.3 Vector spaces, basis vectors, and closure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have been talking informally about vector spaces. It is time to define them
    more precisely.
  prefs: []
  type: TYPE_NORMAL
- en: Vector spaces
  prefs: []
  type: TYPE_NORMAL
- en: A set of vectors (points) in *n* dimensions form a *vector space* if and only
    if the operations of *addition* and *scalar multiplication* are defined on the
    set. In particular, this implies that it is possible to take linear combinations
    of members of a vector space.
  prefs: []
  type: TYPE_NORMAL
- en: Basis vectors
  prefs: []
  type: TYPE_NORMAL
- en: Given a vector space, a set of vectors that span the space is called a *basis*
    for the space. For instance, for the space ℝ², the two vectors ![](../../OEBPS/Images/eq_02-14-k1.png)
    and ![](../../OEBPS/Images/eq_02-14-k2.png) are basis vectors. This essentially
    means any vector in ℝ² can be expressed as a linear combination of these two.
    The notion can be extended to higher dimensions. For ℝ*^n*, the vectors ![](../../OEBPS/Images/eq_02-14-l.png)
    form a basis.
  prefs: []
  type: TYPE_NORMAL
- en: The alert reader has probably guessed by now that the basis vectors are related
    to coordinate axes. In fact, the basis vectors just described constitute the Cartesian
    coordinate axes.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have only seen examples of basis vectors that are mutually orthogonal,
    such as the dot product of the two basis vectors in ℝ² shown earlier: ![](../../OEBPS/Images/eq_02-14-m.png).
    However, basis vectors do not have to be orthogonal. Any pair of linearly independent
    vectors forms a basis in ℝ². Basis vectors, then, are by no means unique. That
    said, orthogonal vectors are most convenient, as we shall see later.'
  prefs: []
  type: TYPE_NORMAL
- en: Minimal and complete basis
  prefs: []
  type: TYPE_NORMAL
- en: Exactly *n* vectors are needed to span a space with dimensionality *n*. This
    means the basis set for a space will have at least as many elements as the dimensionality
    of the space. That many basis vectors are also sufficient to form a basis. For
    instance, exactly *n* vectors are needed to form a basis in (that is, span) ℝ*^n*.
  prefs: []
  type: TYPE_NORMAL
- en: A related fact is that in ℝ*^n*, any set of *m* vectors with *m* > *n* will
    be linearly dependent. In other words, the largest size of a set of linearly independent
    vectors in an *n*-dimensional space is *n*.
  prefs: []
  type: TYPE_NORMAL
- en: Closure
  prefs: []
  type: TYPE_NORMAL
- en: 'A set of vectors is said to be *closed* under linear combination if and only
    if the linear combination of any pair of vectors in the set also belongs to the
    same set. Consider the set of points ℝ². Recall that this is the set of vectors
    with two real elements. Take any pair of vectors ![](../../OEBPS/Images/AR_a.png)
    and ![](../../OEBPS/Images/AR_b.png) in ℝ²: for instance, ![](../../OEBPS/Images/eq_02-14-n1.png)
    and ![](../../OEBPS/Images/eq_02-14-n2.png). Any linear combination of these two
    vectors will also comprise two real numbers—that is, will belong to ℝ². We say
    ℝ² is a *vector space* since it is *closed* under linear combination.'
  prefs: []
  type: TYPE_NORMAL
- en: Consider the space ℝ². Geometrically speaking, this represents a two dimensional
    plane. Let’s take two points on this plane, ![](../../OEBPS/Images/AR_a.png) and
    ![](../../OEBPS/Images/AR_b.png). Linear combinations of ![](../../OEBPS/Images/AR_a.png),
    ![](../../OEBPS/Images/AR_b.png) geometrically correspond to points on the line
    joining them. We know that if two points lie on a plane, the entire line will
    also lie on the plane. Thus, in two dimensions, a plane is closed under linear
    combinations. This is the geometrical intuition behind the notion of closure on
    vector spaces. It can be extended to arbitrary dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the set of points on the surface of a sphere is *not* closed
    under linear combination because the line joining an arbitrary pair of points
    on this set will not wholly lie on the surface of that sphere.
  prefs: []
  type: TYPE_NORMAL
- en: '2.10 Linear transforms: Geometric and algebraic interpretations'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Inputs to a machine learning or data science system are typically feature vectors
    (introduced in section [2.1](02.xhtml#sec-vectors)) in high-dimensional spaces.
    Each individual dimension of the feature vector corresponds to a particular property
    of the input. Thus, the feature vector is a descriptor for the particular input
    instance. It can be viewed as a point in the feature space. We usually transform
    the points to a friendlier space where it is easier to perform the analysis we
    are trying to do. For instance, if we are building a classifier, we try to transform
    the input into a space where the points belonging to different classes are more
    segregated (see section [1.3](../Text/01.xhtml#fig-ml-as-mapping) in general and
    figure [1.3](../Text/01.xhtml#fig-ml-as-mapping) in particular for simple examples).
    Sometimes we transform to simplify the data, eliminating axes along which there
    is scant variation in the data. Given their significance in machine learning,
    in this section we will study the basics of transforms.
  prefs: []
  type: TYPE_NORMAL
- en: Informally, a transform is an operation that maps a set of points vectors) to
    another. Given a set *S* of *n* × 1 vectors, any *m* × *n* matrix *T* can be viewed
    as a transform. If ![](../../OEBPS/Images/AR_v.png) belongs to the set *S*, multiplication
    with the matrix *T* will map (transform) ![](../../OEBPS/Images/AR_v.png) to a
    vector *T*![](../../OEBPS/Images/AR_v.png). We will later see that matrix multiplication
    is a subclass of transforms that preserve collinearity—points that lie on a straight
    line before the transformation will continue to lie on a (possibly different)
    straight line post the transformation. For instance, consider the matrix
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-14-o.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In section [2.14](02.xhtml#sec-rotation-matrices-eigen), we will see that this
    is a special kind of matrix called a rotation matrix; for now, simply consider
    it an example of a matrix. *R* is a transformation operator that maps a point
    in a 2D plane to another point in the same plane. In mathematical notation, *R*
    : ℝ² → ℝ². In fact, as depicted in figure [2.14](02.xhtml#fig-rotation_matrix_diagram),
    this transformation (multiplication by matrix *R*) rotates the position vector
    of a point in the 2D plane by an angle of 45°.'
  prefs: []
  type: TYPE_NORMAL
- en: The output and input points may belong to different spaces in such transforms.
    For instance, consider the matrix
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-14-p.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It is not hard to see that this matrix projects 3D points to the 2D X-Y plane:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-14-q.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Hence, this transformation (multiplication by matrix *P*) projects points from
    three to two dimensions. In mathematical parlance, *P* : ℝ³ → ℝ².'
  prefs: []
  type: TYPE_NORMAL
- en: 'The transforms *R* and *P* share a common property: *they preserve collinearity*.
    This means a set of vectors (points) ![](../../OEBPS/Images/AR_a.png), ![](../../OEBPS/Images/AR_b.png),
    ![](../../OEBPS/Images/AR_c.png), ⋯ that originally lay on a straight line remain
    so after the transformation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s check this out for the rotation transformation in the example from section
    [2.9](02.xhtml#sec-lin-dep). There we saw four vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-14-r.png)'
  prefs: []
  type: TYPE_IMG
- en: 'These vectors all lie on a straight *L* : *x* = *y*. The rotation transformed
    versions of these vectors are'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-14-s.png)'
  prefs: []
  type: TYPE_IMG
- en: It is trivial to see that the transformed vectors also lie on a (different)
    straight line. In fact, ![](../../OEBPS/Images/AR_osm.png)^′, ![](../../OEBPS/Images/AR_a.png)^′,
    ![](../../OEBPS/Images/AR_b.png)^′, ![](../../OEBPS/Images/AR_c.png)^′ lie on
    the *Y*-axis, which is the 45° rotated version of the original line *y* = *x*.
  prefs: []
  type: TYPE_NORMAL
- en: It is trivial to see that the transformed vectors also lie on a (different)
    straight line. In fact, ![](../../OEBPS/Images/AR_osm.png)^′, ![](../../OEBPS/Images/AR_a.png)^′,
    ![](../../OEBPS/Images/AR_b.png)^′, ![](../../OEBPS/Images/AR_c.png)^′ lie on
    the *Y*-axis, which is the 45° rotated version of the original line *y* = *x*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The projection transform represented by matrix *P* also preserves collinearity.
    Consider four collinear vectors in 3D:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-14-t.png)'
  prefs: []
  type: TYPE_IMG
- en: The corresponding transformed vectors
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-14-u.png)'
  prefs: []
  type: TYPE_IMG
- en: also lie on a straight line in 2D.
  prefs: []
  type: TYPE_NORMAL
- en: The class of transforms that preserves collinearity are known as *linear transforms*.
    They can always be represented as a matrix multiplication. Conversely, all matrix
    multiplications represent a linear transformation. A more formal definition is
    provided later.
  prefs: []
  type: TYPE_NORMAL
- en: 2.10.1 Generic multidimensional definition of linear transforms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A function *ϕ* is a linear transform if and only if it satisfies
  prefs: []
  type: TYPE_NORMAL
- en: '*ϕ*(*α*![](../../OEBPS/Images/AR_a.png) + *β*![](../../OEBPS/Images/AR_b.png))
    = *αϕ*(![](../../OEBPS/Images/AR_a.png)) + *βϕ*(![](../../OEBPS/Images/AR_b.png))
    ∀ *α*, *β* ∈ ℝ'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 2.15
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, *a transform is linear if and only if the transform of the
    linear combination of two vectors is the same as the linear combination (with
    the same weights) of the transforms of individual vectors*. (This can be remembered
    as: *Linear transform means transforms of linear combinations are same as linear
    combinations of transforms*.) Multiplication with a rotation or projection matrix
    (shown earlier) is a linear transform.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.10.2 All matrix-vector multiplications are linear transforms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s verify that matrix multiplication satisfies the definition of linear mapping
    (equation [2.15](02.xhtml#eq-lin-map)). Let ![](../../OEBPS/Images/AR_a.png),
    ![](../../OEBPS/Images/AR_b.png) ∈ ℝ*^n* be two arbitrary *n*-dimensional vectors
    and *A[m, n]* be an arbitrary matrix with *n* columns. Then following the standard
    rules of matrix-vector multiplication,
  prefs: []
  type: TYPE_NORMAL
- en: '*A*(*α*![](../../OEBPS/Images/AR_a.png) + *β*![](../../OEBPS/Images/AR_b.png))
    = *α*(*A*![](../../OEBPS/Images/AR_a.png)) + *β*(*A*![](../../OEBPS/Images/AR_b.png))'
  prefs: []
  type: TYPE_NORMAL
- en: which mimics equation [2.15](02.xhtml#eq-lin-map) with *ϕ* replaced with matrix
    *A*. Thus we have proven that all matrix multiplications are linear transforms.
    The reverse is not true. In particular, linear transforms that operate on infinite-dimensional
    vectors are not matrices. But all linear transforms that operate on finite-dimensional
    vectors can be expressed as matrices. (The proof is a bit more complicated and
    will be skipped.)
  prefs: []
  type: TYPE_NORMAL
- en: Thus, in finite dimensions, multiplication with a matrix and linear transformation
    are one and the same thing. In section [2.3](02.xhtml#sec-matrices), we saw the
    array view of matrices. The corresponding geometric view, that all matrices represent
    linear transformation, was presented in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s finish this section by studying an example of a transform that is *not*
    linear. Consider the function
  prefs: []
  type: TYPE_NORMAL
- en: '*ϕ*(![](../../OEBPS/Images/AR_x.png)) = ||![](../../OEBPS/Images/AR_x.png)||'
  prefs: []
  type: TYPE_NORMAL
- en: 'for ![](../../OEBPS/Images/AR_x.png) ∈ ℝ*^n*. This function *ϕ* maps *n*-dimensional
    vectors to a scalar that is the length of the vector, *ϕ* : ℝ*^n* → ℝ. We will
    examine if it satisfies equation [2.15](02.xhtml#eq-lin-map) with *α*[1] = *α*[2]
    = 1. For two specific vectors ![](../../OEBPS/Images/AR_a.png), ![](../../OEBPS/Images/AR_b.png)
    ∈ ℝ*^n*,'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-15-a1.png)'
  prefs: []
  type: TYPE_IMG
- en: Now
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-15-b.png)'
  prefs: []
  type: TYPE_IMG
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-15-c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Clearly, these two are not equal; hence, we have violated equation [2.15](02.xhtml#eq-lin-map):
    *ϕ* is a nonlinear mapping.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.11 Multidimensional arrays, multilinear transforms, and tensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We often hear the term *tensor* in connection with machine learning. Google’s
    famous machine learning platform is named *TensorFlow*. In this section, we will
    introduce you to the concept of a tensor.
  prefs: []
  type: TYPE_NORMAL
- en: '2.11.1 Array view: Multidimensional arrays of numbers'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A tensor may be viewed as a generalized *n*-dimensional array—although, strictly
    speaking, not all multidimensional arrays are tensors. We will learn more about
    the distinction between multidimensional arrays and tensors when we study multilinear
    transforms. For now, we will not worry too much about the distinction. A vector
    can be viewed as a 1 tensor, a matrix is a 2 tensor, and a scalar is a 0 tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'In section [2.3](02.xhtml#sec-matrices), we saw that digital images are represented
    as 2D arrays (matrices). A color image—where each pixel is represented by three
    colors, R, G, and B (red, green, and blue)—is an example of a multidimensional
    array or tensor. This is because it can be viewed as a combination of three images:
    the R, G, and B images, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: The inputs and outputs to each layer in a neural network are also tensors.
  prefs: []
  type: TYPE_NORMAL
- en: 2.12 Linear systems and matrix inverse
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Machine learning today is usually an iterative process. Given a set of training
    data, you want to estimate a set of machine parameters that will yield target
    values (or close approximations to them) on training inputs. The number of training
    inputs and the size of the parameter set are often very large. This makes it impossible
    to have a closed-form solution where we solve for the unknown parameters in a
    single step. Solutions are usually iterative. We start with a guessed set of values
    for the parameters and iteratively improve the guess by processing training data.
  prefs: []
  type: TYPE_NORMAL
- en: Having said that, we often encounter smaller problems in real life. We are better
    off using more traditional closed-form techniques here since they are much faster
    and more accurate. This section is devoted to gaining some insights into these
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go back to our familiar cat-brain problem and refer to its training data
    in table [2.2](02.xhtml#tab-cat-brain-training-data). As before, we are still
    talking about a weighted sum model with three parameters: weights *w*[0], *w*[1]
    and bias *b*. Let’s focus on the top three rows from the table, repeated here
    in table [2.2](02.xhtml#tab-cat-brain-training-data-trunc) for convenience.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 2.3 Example training dataset for our toy machine learning–based cat brain
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Input value: Hardness | Input value: Sharpness | Output: Threat score
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0.11 | 0.09 | −0.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.01 | 0.02 | −0.97 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.98 | 0.91 | 0.89 |'
  prefs: []
  type: TYPE_TB
- en: The training data says that with a hardness value 0.11 and a sharpness value
    0.09, we expect the system’s output to match or closely approximate) the target
    value −0.8, and so on. In other words, our estimated values for parameters *w*[0],
    *w*[1], *b* should ideally satisfy
  prefs: []
  type: TYPE_NORMAL
- en: 0.11*w*[0] + 0.09*w*[1] + b = –0.8
  prefs: []
  type: TYPE_NORMAL
- en: 0.01*w*[0] + 0.02*w*[1] + b = –0.97
  prefs: []
  type: TYPE_NORMAL
- en: 0.98*w*[0] + 0.91 *w*[1] + b = 0.89
  prefs: []
  type: TYPE_NORMAL
- en: 'We can express this via matrix multiplication as the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-15-d.png)'
  prefs: []
  type: TYPE_IMG
- en: How do we obtain the values of *w*[0], *w*[1], *b* that make this equation true?
    That is, how do we solve this equation? There are formal methods (discussed later)
    to directly solve such equations for *w*[0], *w*[1], and *b* (in this very simple
    example, you might just “see” that *w*[0] = 1, *w*[1] = 1, *b* = −1 solves the
    equation, but we need a general method).
  prefs: []
  type: TYPE_NORMAL
- en: This equation is an example of a class of equations called a *linear system*.
    A linear system in *n* unknowns *x*[1], *x*[2], *x*[3], ⋯, *x[n]*,
  prefs: []
  type: TYPE_NORMAL
- en: '*a*[11]*x*[1] + *a*[12]*x*[2] + *a*[13]*x*[3] + … + *a*[1*n*]*x[n]* = *b*[1]'
  prefs: []
  type: TYPE_NORMAL
- en: '*a*[21]*x*[1] + *a*[22]*x*[2] + *a*[23]*x*[3] + … + *a*[2*n*]*x[n]* = *b*[2]'
  prefs: []
  type: TYPE_NORMAL
- en: ⁞
  prefs: []
  type: TYPE_NORMAL
- en: '*a*[*n*1]*x*[1] + *a*[*n*2]*x*[2] + *a*[*n*3]*x*[3] + … + *a[nn]**x[n]* = *b[n]*'
  prefs: []
  type: TYPE_NORMAL
- en: can be expressed via matrix and vectors as
  prefs: []
  type: TYPE_NORMAL
- en: '*A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)'
  prefs: []
  type: TYPE_NORMAL
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-15-e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Although equivalent, the matrix depiction is more compact and dimension-independent.
    In machine learning, we usually have many variables (thousands), so this compactness
    makes a significant difference. Also, *A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)
    looks similar to the one-variable equation we know so well: *ax* = *b*. In fact,
    many intuitions can be transferred from 1D to higher dimensions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'What is the solution to the 1D equation? You may have learned it in fifth grade:
    The solution of *ax* = *b* is *x* = *a*^(−1)*b* where *a*^(−1) = 1/*a*, *a* ≠
    0.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the same notation in all dimensions. The solution of *A*![](../../OEBPS/Images/AR_x.png)
    = ![](../../OEBPS/Images/AR_b.png) is ![](../../OEBPS/Images/AR_x.png) = *A*^(−1)![](../../OEBPS/Images/AR_b.png),
    where *A*^(−1) is the matrix inverse. The inverse matrix *A*^(−1) has the determinant
    of the matrix, 1/*det*(*A*), as a factor. We will not discuss determinant and
    inverse matrix computation here—you can obtain that in any standard linear algebra
    textbook—but will state some facts that lend insights into determinants and inverse
    matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: The inverse matrix *A*^(−1) is related to matrix *A* in the same way the scalar
    *a*^(−1) is related to the scalar *a*. *a*^(−1) exists if and only if *a* ≠ 0.
    Analogously, *A*^(−1) exists if *det*(*A*) ≠ 0, where *det*(*A*) refers to the
    determinant of a matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The product of a scalar *a* and its inverse *a*^(−1) is 1. Analogously, *AA*^(−1)
    = *A*^(−1)*A* = **I**, where **I** denotes the identity matrix that is the higher-dimension
    analog for 1 in scalar arithmetic. It is a matrix in which the diagonal terms
    are 1 and all other terms are 0. The *n*-dimensional identity matrix is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-15-f.png)'
  prefs: []
  type: TYPE_IMG
- en: When there is no subscript, the dimensionality can be inferred from the context.
    For any matrix *A*, **I***A* = *A***I** = *A*. For any vector ![](../../OEBPS/Images/AR_a.png),
    **I**![](../../OEBPS/Images/AR_a.png) = ![](../../OEBPS/Images/AR_a.png)*^T***I**
    = ![](../../OEBPS/Images/AR_a.png). These can be easily verified using the rules
    of matrix multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: There are completely precise but tedious rules for computing determinants and
    matrix inverses. Despite the importance of the concept, we rarely need to compute
    them in life as all linear algebra software packages provide routines to do this.
    Furthermore, computing matrix inverses is not good programming practice because
    it is numerically unstable. We will not discuss the direct computation of determinant
    or matrix inverse here (except that in section [A.2](../Text/A.xhtml#sec-det2x2)
    of the appendix, we show how to compute the determinant of a 2 × 2 matrix). We
    will discuss pseudo-inverses, which have more significance in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 2.12.1 Linear systems with zero or near-zero determinants,and ill-conditioned
    systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We saw earlier that a linear system *A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)
    has the solution ![](../../OEBPS/Images/AR_x.png) = *A*^(−1)![](../../OEBPS/Images/AR_b.png).
    But *A*^(−1) has 1/*det*(*A*) as a factor. What if the determinant is zero?
  prefs: []
  type: TYPE_NORMAL
- en: 'The short answer: when the determinant is zero, the linear system cannot be
    exactly solved. We may still attempt to come up with an approximate answer (see
    section [2.12.3](02.xhtml#subsec-over-under-determined-linsys)), but an exact
    solution is not possible.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s examine the situation a bit more closely with the aid of an example.
    Consider the following system of equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '*  x*[1] + *x*[2]   = 2'
  prefs: []
  type: TYPE_NORMAL
- en: 2*x*[1] + 2*x*[2]  = 4
  prefs: []
  type: TYPE_NORMAL
- en: 'It can be rewritten as a linear system with a square matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-15-g.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But you can quickly see that the system of equations cannot be solved. The
    second equation is really the same as the first. In fact, we can obtain the second
    by multiplying the first by a scalar, 2. Hence, we don’t really have two equations:
    we have only one, so the system cannot be solved. Now examine the row vectors
    of matrix *A*. They are [1   1] and [2   2]. They are linearly dependent because
    −2[1   1] + [2   2] = 0. Now examine the determinant of matrix *A* (section [A.2](../Text/A.xhtml#sec-det2x2)
    of the appendix shows how to compute the determinant of a 2 × 2 matrix). It is
    2 × 1 − 1 × 2 = 0. These results are not coincidences. Any one of them implies
    the other. In fact, the following statements about the linear system *A*![](../../OEBPS/Images/AR_x.png)
    = ![](../../OEBPS/Images/AR_b.png) (with a square matrix) are equivalent:'
  prefs: []
  type: TYPE_NORMAL
- en: Matrix *A* has a row/column that can be expressed as a weighted sum of the others.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matrix *A* has linearly dependent rows or columns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matrix *A* has zero determinant (such matrices are called *singular* matrices).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The inverse of matrix *A* (i.e., *A*^(−1)) does not exist. *A* is called *singular*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The linear system cannot be solved.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The system is trying to tell you that you have fewer equations than you think
    you have, and you cannot solve the system of equations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes the determinant is not exactly zero but close to zero. Although solvable
    in theory, such systems are *numerically unstable*. Small changes in input cause
    the result to change drastically. For instance, consider this nearly singular
    matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-16.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 2.16
  prefs: []
  type: TYPE_NORMAL
- en: Its determinant is 0.002, close to zero. Let ![](../../OEBPS/Images/eq_02-16-a.png)
    be a vector.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-17.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 2.17
  prefs: []
  type: TYPE_NORMAL
- en: (Note how large the elements of *A*^(−1) are. This is due to division by an
    extremely small determinant and, in turn, causes the instability illustrated next.)
    The solution to the equation ![](../../OEBPS/Images/eq_02-17-a.png). But if we
    change ![](../../OEBPS/Images/AR_b.png) just a little and make ![](../../OEBPS/Images/eq_02-17-b.png),
    the solution changes to a drastically different ![](../../OEBPS/Images/eq_02-17-c.png).
    This is inherently unstable and arises from the near singularity of the matrix
    *A*. Such linear systems are called *ill-conditioned*.
  prefs: []
  type: TYPE_NORMAL
- en: 2.12.2 PyTorch code for inverse, determinant, and singularity testing of matrices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Inverting a matrix and computing its determinant can be done with a single function
    call from the linear algebra package linalg.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.10 Matrix inverse for an invertible matrix (nonzero determinant)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: ① ![](../../OEBPS/Images/eq_02-17-d.png)
  prefs: []
  type: TYPE_NORMAL
- en: ② ![](../../OEBPS/Images/eq_02-17-e.png)
  prefs: []
  type: TYPE_NORMAL
- en: ③ The PyTorch function *torch*.*eye*(*n*) generates an identity matrix *I* of
    size *n*
  prefs: []
  type: TYPE_NORMAL
- en: ④ Verify ![](../../OEBPS/Images/eq_02-17-f.png)
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ **I** is like 1. Verify *A***I** = **I***A* = *A*
  prefs: []
  type: TYPE_NORMAL
- en: A singular matrix is a matrix whose determinant is zero. Such matrices are non-invertible.
    Linear systems of equations with singular matrices cannot be solved.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.11 Singular matrix
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: ① ![](../../OEBPS/Images/eq_02-17-g.png)
  prefs: []
  type: TYPE_NORMAL
- en: ② Determinant = 1 × 2 − 2 × 1 = 0. Singular matrix; attempting to compute the
    inverse causes a runtime error.
  prefs: []
  type: TYPE_NORMAL
- en: 2.12.3 Over- and under-determined linear systems in machine learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'What if the matrix *A* is *not* square? This implies that the number of equations
    does not match the number of unknowns. Does such a system even make sense? Surprisingly,
    it does. As a rule, machine learning systems fall in this category: the number
    of equations corresponds to the number of training data instances collected, while
    the number of unknowns is a function of the number of weights in the model which
    is a function of the particular model family chosen to represent the system. These
    are independent of each other. As stated earlier, we often solve these systems
    iteratively. Nonetheless, it is important to understand linear systems with nonsquare
    matrices *A*, to gain insight.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two possible cases, assuming that the matrix *A* is *m* × *n* (*m*
    rows and *n* columns):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Case 1: *m* > *n* (more equations than unknowns; overdetermined system)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Case 2: *m* < *n* (fewer equations than unknown; underdetermined system)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For instance, table [2.2](02.xhtml#tab-cat-brain-training-data) leads to an
    overdetermined linear system. Let’s write the system of equations:'
  prefs: []
  type: TYPE_NORMAL
- en: 0.11 *w*[0] + 0.09 *w*[1] + b = –0.8
  prefs: []
  type: TYPE_NORMAL
- en: 0.01 *w*[0] + 0.02 *w*[1] + b = –0.97
  prefs: []
  type: TYPE_NORMAL
- en: 0.98 *w*[0] + 0.91 *w*[1] + b =   0.89
  prefs: []
  type: TYPE_NORMAL
- en: 0.12 *w*[0] + 0.21 *w*[1] + b = –0.68
  prefs: []
  type: TYPE_NORMAL
- en: 0.98 *w*[0] + 0.99 *w*[1] + b =   0.95
  prefs: []
  type: TYPE_NORMAL
- en: 0.85 *w*[0] + 0.87 *w*[1] + b =   0.74
  prefs: []
  type: TYPE_NORMAL
- en: 0.03 *w*[0] + 0.14 *w*[1] + b = –0.88
  prefs: []
  type: TYPE_NORMAL
- en: 0.55 *w*[0] + 0.45 *w*[1] + b =   0.00
  prefs: []
  type: TYPE_NORMAL
- en: 'These yield the following overdetermined linear system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-18.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 2.18
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a nonsquare 15 × 3 linear system. There are only 3 unknowns to solve
    for (*w*[0], *w*[1], *b*), and there are 15 equations. This is highly redundant:
    we needed only three equations and could have solved it via linear system solution
    techniques (section [2.12](02.xhtml#sec-lin_systems)). But the important thing
    to note is this: *the equations are not fully consistent*. There is no single
    set of values for the unknown that will satisfy all of them. In other words, the
    training data is noisy—an almost universal occurrence in real-life machine learning
    systems. Consequently, we have to find a solution that is optimal (causes as little
    error as possible) over all the equations.'
  prefs: []
  type: TYPE_NORMAL
- en: We want to solve it such that the overall error ||*A*![](../../OEBPS/Images/AR_x.png)
    − ![](../../OEBPS/Images/AR_b.png)|| is minimized. In other words, we are looking
    for ![](../../OEBPS/Images/AR_x.png) such that *A*![](../../OEBPS/Images/AR_x.png)
    is as close to ![](../../OEBPS/Images/AR_b.png) as possible. This closed-form
    that is, non-iterative) method is an extremely important precursor to machine
    learning and data science. We will revisit this multiple times, most notably in
    sections [2.12.4](02.xhtml#subsec-moore-penrose-pseudoinverse) and [4.5](../Text/04.xhtml#sec-svd).
  prefs: []
  type: TYPE_NORMAL
- en: 2.12.4 Moore Penrose pseudo-inverse of a matrix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The pseudo-inverse is a handy technique to solve over- or under-determined
    linear systems. Suppose we have an overdetermined system with the not-necessarily
    square *m* × *n* matrix *A*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since *A* is not guaranteed to be square, we can take neither the determinant
    nor the inverse in general. So the usual *A*^(−1)![](../../OEBPS/Images/AR_b.png)
    does not work. At this point, we observe that although the inverse cannot be taken,
    transposing the matrix is always possible. Let’s multiply both sides of the equation
    with *A^T*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png) ⇔ *A^TA*![](../../OEBPS/Images/AR_x.png)
    = *A^T*![](../../OEBPS/Images/AR_b.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that *A^TA* is a square matrix: its dimensions are (*m*×*n*) × (*n*×*m*)
    = *m* × *m*. Let’s assume, without proof for the moment, that it is invertible.
    Then'
  prefs: []
  type: TYPE_NORMAL
- en: '*A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png) ⇔ *A^TA*![](../../OEBPS/Images/AR_x.png)
    = *A^T*![](../../OEBPS/Images/AR_b.png) ⇔ ![](../../OEBPS/Images/AR_x.png) = (*A^TA*)^(−1)*A^T*![](../../OEBPS/Images/AR_b.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Hmmm, not bad; we seem to be onto something. In fact, we just derived the *pseudo-inverse*
    of matrix *A*, denoted *A* ^+ = (*A^TA*)^(−1)*A^T*. Unlike the inverse, the pseudo-inverse
    does not need the matrix to be square with linearly independent rows. Much like
    the regular linear system, we get the solution of the (possibly nonsquare) system
    of equations as *A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)
    ⇔ ![](../../OEBPS/Images/AR_x.png) = *A* ^+ ![](../../OEBPS/Images/AR_b.png).
  prefs: []
  type: TYPE_NORMAL
- en: The pseudo-inverse-based solution actually minimizes the error ||*A*![](../../OEBPS/Images/AR_x.png)
    − ![](../../OEBPS/Images/AR_b.png)||. We will provide an intuitive proof of that
    in section [2.12.5](02.xhtml#subsec-pseudo-inv-geometric-intuition). Meanwhile,
    you are encouraged to write the Python code to evaluate (*A^TA*)^(−1)*A^T*![](../../OEBPS/Images/AR_b.png)
    and verify that it approximately yields the expected answer ![](../../OEBPS/Images/eq_02-18-a.png)
    for equation [2.18](02.xhtml#eq-overdetemined_system_example).
  prefs: []
  type: TYPE_NORMAL
- en: '2.12.5 Pseudo-inverse of a matrix: A beautiful geometric intuition'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A matrix *A*[*m* × *n*] can be rewritten in terms of its column vectors as [![](../../OEBPS/Images/AR_a.png)[1],
    ![](../../OEBPS/Images/AR_a.png)[2], … ![](../../OEBPS/Images/AR_a.png)*[n]*],
    where ![](../../OEBPS/Images/AR_a.png)[1] … ![](../../OEBPS/Images/AR_a.png)*[n]*
    are all *m*-dimensional vectors. Then if ![](../../OEBPS/Images/eq_02-18-b.png),
    we get *A*![](../../OEBPS/Images/AR_x.png) = *x*[1]![](../../OEBPS/Images/AR_a.png)[1]
    + *x*[2]![](../../OEBPS/Images/AR_a.png)[2] + ⋯ *x[n]*![](../../OEBPS/Images/AR_a.png)*[n]*.
    In other words, *A*![](../../OEBPS/Images/AR_x.png) is just a linear combination
    of the column vectors of *A* with the elements of ![](../../OEBPS/Images/AR_x.png)
    as the weights (you are encouraged to write out a small 3 × 3 system and verify
    this). The space of all vectors of the form *A*![](../../OEBPS/Images/AR_x.png)
    (that is, the linear span of the column vectors of *A*) is known as the *column
    space* of *A*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution to the linear system of equations *A*![](../../OEBPS/Images/AR_x.png)
    = ![](../../OEBPS/Images/AR_b.png) can be viewed as finding the ![](../../OEBPS/Images/AR_x.png)
    that minimizes the difference of *A*![](../../OEBPS/Images/AR_x.png) and ![](../../OEBPS/Images/AR_b.png):
    that is, minimizes ||*A*![](../../OEBPS/Images/AR_x.png) − ![](../../OEBPS/Images/AR_b.png)||.
    This means we are trying to find a point in the column space of *A* that is closest
    to the point ![](../../OEBPS/Images/AR_b.png). Note that this interpretation does
    not assume a square matrix *A*. Nor does it assume a nonzero determinant. In the
    friendly case where the matrix *A* is square and invertible, we can find a vector
    ![](../../OEBPS/Images/AR_x.png) such that *A*![](../../OEBPS/Images/AR_x.png)
    becomes exactly equal to ![](../../OEBPS/Images/AR_b.png), which makes ||*A*![](../../OEBPS/Images/AR_x.png)
    − ![](../../OEBPS/Images/AR_b.png)|| = 0. If *A* is not square, we will try to
    find ![](../../OEBPS/Images/AR_x.png) such that *A*![](../../OEBPS/Images/AR_x.png)
    is closer to ![](../../OEBPS/Images/AR_b.png) than any other vector in the column
    space of *A*. Mathematically speaking, [⁴](02.xhtml#fn7)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-19.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 2.19
  prefs: []
  type: TYPE_NORMAL
- en: From geometry, we intuitively know that the closest point to ![](../../OEBPS/Images/AR_b.png)
    in the column space of *A* is obtained by dropping a perpendicular from ![](../../OEBPS/Images/AR_b.png)
    to the column space of *A* (see figure [2.12](02.xhtml#fig-nonsquare_linear_system_diagram)).
    The point where this perpendicular meets the column space is called the *projection*
    of ![](../../OEBPS/Images/AR_b.png) on the column space of *A*. The solution vector
    ![](../../OEBPS/Images/AR_x.png) to equation [2.19](02.xhtml#eq-linear_system_minimization_problem)
    that we are looking for should correspond to the projection of ![](../../OEBPS/Images/AR_b.png)
    on the column space of *A*. This in turn means ![](../../OEBPS/Images/AR_b.png)
    − *A*![](../../OEBPS/Images/AR_x.png) is orthogonal (perpendicular) to all vectors
    in the column space of *A* (see figure [2.12](02.xhtml#fig-nonsquare_linear_system_diagram)).
    We represent arbitrary vectors in the column space of *A* as *A*![](../../OEBPS/Images/AR_y.png)
    for arbitrary ![](../../OEBPS/Images/AR_y.png). Hence, for all such ![](../../OEBPS/Images/AR_y.png),
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-19-a.png)'
  prefs: []
  type: TYPE_IMG
- en: For the previous equation to be true for all vectors ![](../../OEBPS/Images/AR_y.png),
    we must have *A^T*(![](../../OEBPS/Images/AR_b.png)−*A*![](../../OEBPS/Images/AR_x.png))
    = 0.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-19-b.png)'
  prefs: []
  type: TYPE_IMG
- en: which is exactly the Moore-Penrose pseudo-inverse.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH02_F12_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.12 Solving a linear system *A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)
    is equivalent to finding the point on the column space of *A* that is closest
    to ![](../../OEBPS/Images/AR_b.png). This means we have to drop a perpendicular
    from ![](../../OEBPS/Images/AR_b.png) to column space of *A*. If *A*![](../../OEBPS/Images/AR_x.png)
    represents the point where that perpendicular meets the column space (aka projection),
    the difference vector ![](../../OEBPS/Images/AR_b.png) − *A*![](../../OEBPS/Images/AR_x.png)
    corresponds to the line joining ![](../../OEBPS/Images/AR_b.png) and its projection
    *A*![](../../OEBPS/Images/AR_x.png). This line will be perpendicular to all vectors
    in the column space of *A*. Equivalently, it is perpendicular to *A*![](../../OEBPS/Images/AR_y.png)
    for any arbitrary ![](../../OEBPS/Images/AR_y.png).
  prefs: []
  type: TYPE_NORMAL
- en: For a machine learning-centric example, consider the overdetermined system corresponding
    to the cat brain earlier in the chapter. There are 15 training examples, each
    with input and desired outputs specified.
  prefs: []
  type: TYPE_NORMAL
- en: Our goal is to determine three unknowns *w*[0], *w*[1], and *b* such that for
    each training input ![](../../OEBPS/Images/eq_02-19-c.png), the model output
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-20.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 2.20
  prefs: []
  type: TYPE_NORMAL
- en: matches the desired output (aka ground truth) *ȳ[i]* as closely as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE We employed a neat trick here: we added a 1 to the right of the input,
    which allows us to depict the entire system (including the bias) in a single compact
    matrix-vector multiplication. We call this *augmentation*—we augment the input
    row vector with an extra 1 on the right.'
  prefs: []
  type: TYPE_NORMAL
- en: Collating all the training examples together, we get
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-21.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 2.21
  prefs: []
  type: TYPE_NORMAL
- en: which can be expressed compactly as
  prefs: []
  type: TYPE_NORMAL
- en: '*X*![](../../OEBPS/Images/AR_w.png) = ![](../../OEBPS/Images/AR_y.png)'
  prefs: []
  type: TYPE_NORMAL
- en: where *X* is the augmented input matrix with a rightmost column of all 1s. The
    goal is to minimize ||![](../../OEBPS/Images/AR_y.png) – ![](../../OEBPS/Images/AR_y2.png)||.
    To this end, we formulate the over-determined linear system
  prefs: []
  type: TYPE_NORMAL
- en: '*X*![](../../OEBPS/Images/AR_w.png) = ![](../../OEBPS/Images/AR_y2.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Note that *this is not a classic system of equations—it has more equations than
    unknowns*. We cannot solve this via matrix inversion. We *can*, however, use the
    pseudo-inverse mechanism to solve it. The resulting solution yields the “best
    fit” or “best effort” solution, which minimizes the total error over all the training
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: The exact numerical system (repeated here for ease of reference) is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-22.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 2.22
  prefs: []
  type: TYPE_NORMAL
- en: We solve for ![](../../OEBPS/Images/AR_w.png) using the pseudo-inverse formula
    ![](../../OEBPS/Images/AR_w.png) = (*X^TX*)^(–1)*X^T*![](../../OEBPS/Images/AR_y2.png)
  prefs: []
  type: TYPE_NORMAL
- en: 2.12.6 PyTorch code to solve overdetermined systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: NOTE Fully functional code for this section, executable via Jupyter Notebook,
    can be found at [http://mng.bz/PPJ2](http://mng.bz/PPJ2).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.12 Solving an overdetermined system using the pseudo-inverse
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: ① X is the augmented data matrix from equation [2.22](02.xhtml#eq-lin-model)
  prefs: []
  type: TYPE_NORMAL
- en: ② The Pytorch column stack operator
  prefs: []
  type: TYPE_NORMAL
- en: adds a column to a matrix. Here, the added column is all 1s
  prefs: []
  type: TYPE_NORMAL
- en: '③ It is easy to verify that the solution to equation [2.22](02.xhtml#eq-lin-model)
    is roughly *w*[0] = 1, *w*[1] = 1, *b* = −1. But the equations are not consistent:
    no one solution perfectly fits all of them. The pseudo-inverse finds the “best
    fit” solution: it minimizes total error for all the equations.'
  prefs: []
  type: TYPE_NORMAL
- en: ④ Expect the solution to be close to [1, 1, −1]
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ The solution is [1.08, 0.90, −0.96]
  prefs: []
  type: TYPE_NORMAL
- en: '2.13 Eigenvalues and eigenvectors: Swiss Army knives of machine learning'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Machine learning and data science are all about finding patterns in large volumes
    of high-dimensional data. The inputs are feature vectors introduced in section
    [2.1](02.xhtml#sec-vectors)) in high-dimensional spaces. Each feature vector can
    be viewed as a point in the feature space descriptor for an input instance. Sometimes
    we transform these feature vectors—map the feature points to a friendlier space—to
    simplify the data by reducing dimensionality. This is done by eliminating axes
    along which there is scant variation in the data. Eigenvalues and eigenvectors
    are invaluable tools in the arsenal of a machine learning engineer or a data scientist
    for this purpose. In chapter [4](../Text/04.xhtml#chap-linalg-tools-ml), we will
    study how to use these tools to simplify and find broad patterns in a large volume
    of multidimensional data.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take an informal look at eigenvectors first. They are properties of square
    matrices. As seen earlier, matrices can be viewed as linear transforms which map
    vectors (points) in one space to different vectors (points) in the same or a different
    space. But a typical linear transform leaves a few points in the space (almost)
    unaffected. These points are called *eigenvectors*. They are important physical
    aspects of the transform. Let’s look at a simple example. Suppose we are *rotating*
    points in 3D space about the *Z*-axis (see figure [2.13](02.xhtml#fig-rotation_about_z_diagram)).
    The points on the *Z*-axis will stay where they were despite the rotation. In
    general, points on the axis of rotation (*Z* in this case) do not go anywhere
    after rotation. The axis of rotation is an eigenvector of the rotation transformation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH02_F13_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.13 During rotation, points on the axis of rotation do not change position.
  prefs: []
  type: TYPE_NORMAL
- en: Extending this idea, when *transforming* vectors ![](../../OEBPS/Images/AR_x.png)
    with a matrix *A*, are there vectors that do not change, at least in direction?
    Turns out the answer is yes. These are the so-called *eigenvectors*—they do not
    change direction when undergoing linear transformation by a matrix *A*. To be
    precise, if ![](../../OEBPS/Images/AR_e.png) is an eigenvector of the square matrix
    *A*, [⁵](02.xhtml#fn8) then
  prefs: []
  type: TYPE_NORMAL
- en: '*A![](../../OEBPS/Images/AR_e.png)* = *λ![](../../OEBPS/Images/AR_e.png)*'
  prefs: []
  type: TYPE_NORMAL
- en: Thus the linear transformation (that is, multiplication by matrix *A*) has changed
    the length but not the direction of ![](../../OEBPS/Images/AR_e.png) because *λ*![](../../OEBPS/Images/AR_e.png)
    is parallel to ![](../../OEBPS/Images/AR_e.png).
  prefs: []
  type: TYPE_NORMAL
- en: How do we obtain *λ* and ![](../../OEBPS/Images/AR_e.png)? Well,
  prefs: []
  type: TYPE_NORMAL
- en: '*A*![](../../OEBPS/Images/AR_e.png) = *λ* ![](../../OEBPS/Images/AR_e.png)'
  prefs: []
  type: TYPE_NORMAL
- en: ⇔ *A*![](../../OEBPS/Images/AR_e.png) - *λ*![](../../OEBPS/Images/AR_e.png)
    = ![](../../OEBPS/Images/AR_0.png)
  prefs: []
  type: TYPE_NORMAL
- en: ⇔ (*A* - *λ* **I**)![](../../OEBPS/Images/AR_e.png) = ![](../../OEBPS/Images/AR_0.png)
  prefs: []
  type: TYPE_NORMAL
- en: where **I** denotes the identity matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, we are only interested in nontrivial solutions, where ![](../../OEBPS/Images/AR_e.png)
    ≠ ![](../../OEBPS/Images/AR_0.png). In that case, *A* – *λ***I** cannot be invertible,
    because if it were, we could obtain the contradictory solution ![](../../OEBPS/Images/AR_e.png)
    = (*A* – *λ* **I**)^(–1) ![](../../OEBPS/Images/AR_0.png) = ![](../../OEBPS/Images/AR_0.png).
    Thus, (*A* − *λ***I**) is non-invertible, implying the determinant
  prefs: []
  type: TYPE_NORMAL
- en: '*det*(*A* − *λ***I**) = 0'
  prefs: []
  type: TYPE_NORMAL
- en: For an *n* × *n* matrix *A*, this yields an *n*th-degree polynomial equation
    with *n* solutions for the unknown *λ*. *Thus, an *n* × *n* matrix has n eigenvalues,
    not necessarily all distinct*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s compute eigenvalues and eigenvectors of a 3 × 3 matrix, just for kicks.
    The matrix we use is carefully chosen, as will be evident soon. But for now, think
    of it as an arbitrary matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-23.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 2.23
  prefs: []
  type: TYPE_NORMAL
- en: 'We will compute the eigenvalues and eigenvectors of *A*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-23-a.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-23-b.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *i* = √–1. If necessary, you are encouraged to refresh your memory of
    imaginary and complex numbers from high school algebra.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we have found (as expected) three eigenvalues: 1, *e*^(*i* *π*/4), and
    *e*^(–*i* *π*/4). Each of them will yield one eigenvector. Let’s compute the eigenvector
    corresponding to the eigenvalue of 1 by way of example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-23-c.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, ![](../../OEBPS/Images/eq_02-23-d.png) is an eigenvector for the eigenvalue
    1 for matrix A. So is ![](../../OEBPS/Images/eq_02-23-e.png) for any real *k*.
    In fact, if *λ*, ![](../../OEBPS/Images/AR_e.png) is an eigenvalue, eigenvector
    pair for matrix *A*, then
  prefs: []
  type: TYPE_NORMAL
- en: '*A![](../../OEBPS/Images/AR_e.png)* = *λ![](../../OEBPS/Images/AR_e.png)* ⇔
    *A*(*k![](../../OEBPS/Images/AR_e.png)*) = *λ*(*k![](../../OEBPS/Images/AR_e.png)*)'
  prefs: []
  type: TYPE_NORMAL
- en: That is, *λ*, (*k![](../../OEBPS/Images/AR_e.png)*) is also an eigenvalue, eigenvector
    pair of *A*. In other words, we can only determine the eigenvector up to a fixed
    scale factor. We take the eigenvector to be of unit length (*![](../../OEBPS/Images/AR_e.png)^T![](../../OEBPS/Images/AR_e.png)*
    = 1) without loss of generality.
  prefs: []
  type: TYPE_NORMAL
- en: The eigenvector for our example matrix turns out to be the *Z*-axis. This is
    not an accident. Our matrix *A* was, in fact, a rotation about the *Z*-axis. *A
    rotation matrix will always have 1 as an eigenvalue. The corresponding eigenvector
    will be the axis of rotation. In 3D, the other two eigenvalues will be complex
    numbers yielding the angle of rotation.* This is detailed in section [2.14](02.xhtml#sec-rotation-matrices-eigen).
  prefs: []
  type: TYPE_NORMAL
- en: 2.13.1 Eigenvectors and linear independence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Two eigenvectors of a matrix corresponding to unequal eigenvalues are linearly
    independent. Let’s prove this to get some insights. Let *λ*[1], ![](../../OEBPS/Images/AR_e.png)[1]
    and *λ*[2], ![](../../OEBPS/Images/AR_e.png)[2] be eigenvalue, eigenvector pairs
    for a matrix *A* with *λ*[1] ≠ *λ*[2]. Then
  prefs: []
  type: TYPE_NORMAL
- en: '*A*![](../../OEBPS/Images/AR_e.png)[1] = *λ*[1]![](../../OEBPS/Images/AR_e.png)[1]'
  prefs: []
  type: TYPE_NORMAL
- en: '*A*![](../../OEBPS/Images/AR_e.png)[2] = *λ*[2]![](../../OEBPS/Images/AR_e.png)[2]'
  prefs: []
  type: TYPE_NORMAL
- en: If possible, let there be two constants *α*[1] and *α*[2] such that
  prefs: []
  type: TYPE_NORMAL
- en: '*α*[1]![](../../OEBPS/Images/AR_e.png)[1] + *α*[2]![](../../OEBPS/Images/AR_e.png)[2]
    = 0'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 2.24
  prefs: []
  type: TYPE_NORMAL
- en: In other words, suppose the two eigenvectors are linearly dependent. We will
    show that this assumption leads to an impossibility.
  prefs: []
  type: TYPE_NORMAL
- en: Multiplying equation [2.24](02.xhtml#eq-linear-dep-vectors) by *A*, we get
  prefs: []
  type: TYPE_NORMAL
- en: '*α*[1]*A*![](../../OEBPS/Images/AR_e.png)[1] + *α*[2]*A*![](../../OEBPS/Images/AR_e.png)[2]
     = 0'
  prefs: []
  type: TYPE_NORMAL
- en: ⇔ *α*[1]*λ*[1]![](../../OEBPS/Images/AR_e.png)[1] + *α*[2]*λ*[2]![](../../OEBPS/Images/AR_e.png)[2]  =
    0
  prefs: []
  type: TYPE_NORMAL
- en: Also, we can multiply equation [2.24](02.xhtml#eq-linear-dep-vectors) by *λ*[2].
    Thus we get
  prefs: []
  type: TYPE_NORMAL
- en: '*α*[1]*λ*[1]![](../../OEBPS/Images/AR_e.png)[1] + *α*[2]*λ*[2]![](../../OEBPS/Images/AR_e.png)[2]
    = 0'
  prefs: []
  type: TYPE_NORMAL
- en: '*α*[1]*λ*[2]![](../../OEBPS/Images/AR_e.png)[1] + *α*[2]*λ*[2]![](../../OEBPS/Images/AR_e.png)[2]
    = 0'
  prefs: []
  type: TYPE_NORMAL
- en: Subtracting, we get
  prefs: []
  type: TYPE_NORMAL
- en: '*α*[1](*λ*[1]-*λ*[2])![](../../OEBPS/Images/AR_e.png)[1] = 0'
  prefs: []
  type: TYPE_NORMAL
- en: By assumption, *α*[1] ≠ 0, *λ*[1] ≠ *λ*[2] and ![](../../OEBPS/Images/AR_e.png)[1]
    is not all zeros. Thus it is impossible for their product to be zero. Our original
    assumption (the two eigenvectors are linearly dependent) must have been wrong.
  prefs: []
  type: TYPE_NORMAL
- en: 2.13.2 Symmetric matrices and orthogonal eigenvectors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Two eigenvectors of a symmetric matrix that correspond to different eigenvalues
    are mutually orthogonal. Let’s prove this to get additional insight. A matrix
    *A* is symmetric if *A^T* = *A*. If *λ*[1], ![](../../OEBPS/Images/AR_e.png)[1]
    and *λ*[2], ![](../../OEBPS/Images/AR_e.png)[2] are eigenvalue, eigenvector pairs
    for a symmetric matrix *A*, then
  prefs: []
  type: TYPE_NORMAL
- en: '*A*![](../../OEBPS/Images/AR_e.png)[1] = *λ*[1]![](../../OEBPS/Images/AR_e.png)[1]'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 2.25
  prefs: []
  type: TYPE_NORMAL
- en: '*A*![](../../OEBPS/Images/AR_e.png)[2] = *λ*[2]![](../../OEBPS/Images/AR_e.png)[2]'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 2.26
  prefs: []
  type: TYPE_NORMAL
- en: Transposing equation [2.25](02.xhtml#eq-eigen-vec1),
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_e.png)[1]*^T A^T* = *λ*[1]![](../../OEBPS/Images/AR_e.png)[1]^T'
  prefs: []
  type: TYPE_NORMAL
- en: Right-multiplying by ![](../../OEBPS/Images/AR_e.png)[2], we get
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_e.png)[1]^T *A^T*![](../../OEBPS/Images/AR_e.png)[2]
    = *λ*[1]![](../../OEBPS/Images/AR_e.png)[1]*^T ![](../../OEBPS/Images/AR_e.png)*[2]'
  prefs: []
  type: TYPE_NORMAL
- en: ⇔ ![](../../OEBPS/Images/AR_e.png)[1]^T *A*![](../../OEBPS/Images/AR_e.png)[2
      ] = *λ*[1]![](../../OEBPS/Images/AR_e.png)[1]*^T ![](../../OEBPS/Images/AR_e.png)*[2]
  prefs: []
  type: TYPE_NORMAL
- en: where the last equation follows from the matrix symmetry. Also, left-multiplying
    equation [2.26](02.xhtml#eq-eigen-vec2) by ![](../../OEBPS/Images/AR_e.png)[1]*^T*,
    we get
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_e.png)[1]*^T A*![](../../OEBPS/Images/AR_e.png)[2]
    = *λ*[2]![](../../OEBPS/Images/AR_e.png)[1]*^T ![](../../OEBPS/Images/AR_e.png)*[2]'
  prefs: []
  type: TYPE_NORMAL
- en: Thus
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_e.png)[1]*^T A*![](../../OEBPS/Images/AR_e.png)[2]
    = *λ*[1]![](../../OEBPS/Images/AR_e.png)[1]*^T ![](../../OEBPS/Images/AR_e.png)*[2]'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_e.png)[1]*^T A*![](../../OEBPS/Images/AR_e.png)[2]
    = *λ*[2]![](../../OEBPS/Images/AR_e.png)[1]^T ![](../../OEBPS/Images/AR_e.png)[2]'
  prefs: []
  type: TYPE_NORMAL
- en: Subtracting the equations, we get
  prefs: []
  type: TYPE_NORMAL
- en: 0 = (*λ*[1] - *λ*[2]) ![](../../OEBPS/Images/AR_e.png)[1]*^T ![](../../OEBPS/Images/AR_e.png)*[2]
  prefs: []
  type: TYPE_NORMAL
- en: Since *λ*[1] ≠ *λ*[2], we must have ![](../../OEBPS/Images/AR_e.png)[1]*^T ![](../../OEBPS/Images/AR_e.png)*[2]
    = 0, which means the two eigenvectors are orthogonal. Thus, if *A* is an *n* ×
    *n* symmetric matrix with eigenvectors ![](../../OEBPS/Images/AR_e.png)[1], ![](../../OEBPS/Images/AR_e.png)[2],
    … ![](../../OEBPS/Images/AR_e.png)*[n]*, then ![](../../OEBPS/Images/AR_e.png)*[i]^T![](../../OEBPS/Images/AR_e.png)[j]*
    = 0 for all *i*, *j* satisfying *λ[i]* ≠ *λ[j]*.
  prefs: []
  type: TYPE_NORMAL
- en: 2.13.3 PyTorch code to compute eigenvectors and eigenvalues
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: NOTE Fully functional code for this section, executable via Jupyter Notebook,
    can be found at [http://mng.bz/1rEZ](http://mng.bz/1rEZ).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.13 Eigenvalues and vectors
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: ① ![](../../OEBPS/Images/eq_02-23-f.png) Rotates points in 3D space around the
    *Z*-axis.
  prefs: []
  type: TYPE_NORMAL
- en: 'The axis of rotation is the *Z*-axis: [0  0  1]*^T*'
  prefs: []
  type: TYPE_NORMAL
- en: ② Function eig() in the torch linalg package computes eigenvalues and vectors.
  prefs: []
  type: TYPE_NORMAL
- en: ③ Eigenvalues or vectors can contain complex numbers involving *j* = √-1
  prefs: []
  type: TYPE_NORMAL
- en: 2.14 Orthogonal (rotation) matrices and their eigenvalues and eigenvectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Of all the transforms, rotation transforms have a special intuitive appeal because
    of their highly observable behavior in the mechanical world. Furthermore, they
    play a significant role in developing and analyzing several machine learning tools.
    In this section, we overview rotation (aka orthogonal) matrices. (Fully functional
    code for the Jupyter notebook for this section can be found at [http://mng.bz/2eNN](http://mng.bz/2eNN).)
  prefs: []
  type: TYPE_NORMAL
- en: 2.14.1 Rotation matrices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Figure [2.14](02.xhtml#fig-rotation_matrix_diagram) shows a point (*x*, *y*)
    rotated about the origin by an angle *θ*. The original point’s position vector
    made an angle *α* with the *X*-axis. Post-rotation, the point’s new coordinates
    are (*x*^′, *y*^′). Note that by definition, rotation does not change the distance
    from the center of rotation; that is what the circle indicates.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH02_F14_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.14 Rotation in a plane about the origin. By definition, rotation does
    not change the distance from the center of rotation (indicated by the circle).
  prefs: []
  type: TYPE_NORMAL
- en: 'Some well-known rotation matrices are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Planar rotation by angle *θ* about the origin** (see figure [2.14](02.xhtml#fig-rotation_matrix_diagram)):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-27.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 2.27
  prefs: []
  type: TYPE_NORMAL
- en: '**Rotation by angle *θ* in 3D space about the *Z*-axis**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-28.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 2.28
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the *z* coordinate remains unaffected by this rotation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-28-a.png)'
  prefs: []
  type: TYPE_IMG
- en: This rotation matrix has an eigenvalue of 1, and the corresponding eigenvector
    is the *Z*-axis—you should verify this. This implies that a point on the *Z*-axis
    maps to itself when transformed (rotated) by the previous matrix, which is in
    keeping with the property that the *z* coordinate remains unchanged by this rotation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Rotation by angle *θ* in 3D space about the *X*-axis**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-29.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 2.29
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the *X* coordinate remains unaffected by this rotation and the *X*-axis
    is an eigenvector of this matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-29-a.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Rotation by angle *θ* in 3D space about the *Y*-axis**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-30.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 2.30
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the *Y* coordinate remains unaffected by this rotation and the *Y*-axis
    is an eigenvector of this matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-30-a.png)'
  prefs: []
  type: TYPE_IMG
- en: Listing 2.14 Rotation matrices
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: ① Returns the matrix that performs in-plane 2D rotation by angle theta about
    the origin. Thus, multiplication with this matrix moves a point to a new location.
    The angle between the position vectors of the original and new points is theta
    (figure [2.14](02.xhtml#fig-rotation_matrix_diagram)).
  prefs: []
  type: TYPE_NORMAL
- en: ② Returns the matrix that rotates a point in 3D space about the chosen axis
    by angle theta degrees. The axis of rotation can be 0, 1, or 2, corresponding
    to the X-, Y-, or Z-axis, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: ③ *R*[3*dx*] from equation [2.29](02.xhtml#eq-rot3d-x)
  prefs: []
  type: TYPE_NORMAL
- en: ④ *R*[3*dy*] from equation [2.30](02.xhtml#eq-rot3d-y)
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ *R*[3*dz*] from equation [2.28](02.xhtml#eq-rot3d-z)
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.15 Applying rotation matrices
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: ① Creates vector ![](../../OEBPS/Images/AR_u.png) (see figure [2.15](02.xhtml#fig-numpy-rotations))
  prefs: []
  type: TYPE_NORMAL
- en: ② *R*[3*dz*] from equation [2.28](02.xhtml#eq-rot3d-z), 45° about *Z*-axis
  prefs: []
  type: TYPE_NORMAL
- en: ③ ![](../../OEBPS/Images/AR_v.png) (see figure [2.15](02.xhtml#fig-numpy-rotations))
    is ![](../../OEBPS/Images/AR_u.png) rotated by *R*[3*dz*].
  prefs: []
  type: TYPE_NORMAL
- en: ④ *R*[3*dx*] from equation [2.28](02.xhtml#eq-rot3d-z), 45° about *X*-axis
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ ![](../../OEBPS/Images/AR_w.png) (see figure [2.15](02.xhtml#fig-numpy-rotations))
    is ![](../../OEBPS/Images/AR_v.png) rotated by *R*[3*dx*].
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH02_F15_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.15 Rotation visualized. Here the original vector u is first rotated
    by 45 degrees around the Z-axis to get vector v, which is subsequently rotated
    again by 45 degrees around the X-axis to get vector w.
  prefs: []
  type: TYPE_NORMAL
- en: 2.14.2 Orthogonality of rotation matrices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A matrix *R* is *orthogonal* if and only if it its transpose is also its inverse:
    that is, *R^TR* = *RR^T* = **I**. *All rotations matrices are orthogonal matrices.
    All orthogonal matrices represent some rotation.* For instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-30-b1.png)'
  prefs: []
  type: TYPE_IMG
- en: You are encouraged to verify, likewise, that all the rotation matrices shown
    here are orthogonal.
  prefs: []
  type: TYPE_NORMAL
- en: Orthogonality and length-preservation
  prefs: []
  type: TYPE_NORMAL
- en: Orthogonality implies that rotation is length-preserving. Given any vector ![](../../OEBPS/Images/AR_x.png)
    and rotation matrix *R*, let ![](../../OEBPS/Images/AR_y.png) = *R*![](../../OEBPS/Images/AR_x.png)
    be the rotated vector. The lengths (magnitudes) of the two vectors ![](../../OEBPS/Images/AR_x.png),
    ![](../../OEBPS/Images/AR_y.png) are equal since it is easy to see that
  prefs: []
  type: TYPE_NORMAL
- en: '||![](../../OEBPS/Images/AR_y.png)|| = ![](../../OEBPS/Images/AR_y.png)*^T*![](../../OEBPS/Images/AR_y.png)
    = (*R*![](../../OEBPS/Images/AR_x.png))*^T*(*R*![](../../OEBPS/Images/AR_x.png))
    = ![](../../OEBPS/Images/AR_x.png)*^TR^TR*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_x.png)*^T***I**![](../../OEBPS/Images/AR_x.png)
    = ![](../../OEBPS/Images/AR_x.png)*^T*![](../../OEBPS/Images/AR_x.png) = ||![](../../OEBPS/Images/AR_x.png)||'
  prefs: []
  type: TYPE_NORMAL
- en: From elementary matrix theory, we know that
  prefs: []
  type: TYPE_NORMAL
- en: (*AB*)*^T* = *B^TA^T*
  prefs: []
  type: TYPE_NORMAL
- en: Negating the angle of rotation
  prefs: []
  type: TYPE_NORMAL
- en: Negating the angle of rotation is equivalent to inverting the rotation matrix,
    which is equivalent to transposing the rotation matrix. For instance, consider
    in-plane rotation. Say a point ![](../../OEBPS/Images/AR_x.png) is rotated about
    the origin to vector ![](../../OEBPS/Images/AR_y.png) via matrix *R*. Thus
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-30-c.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we can go back from ![](../../OEBPS/Images/AR_y.png) to ![](../../OEBPS/Images/AR_x.png)
    by rotating by −*θ*. The corresponding rotation matrix is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-30-d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In other words, *R^T* inverts the rotation: that is, rotates by the negative
    angle.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.14.3 PyTorch code for orthogonality of rotation matrices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s verify the orthogonality of the rotation matrix by creating one in PyTorch,
    imparting a transpose to it, and verifying that the product of the original matrix
    and the transpose is the identity matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.16 Orthogonality of rotation matrices
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: ① Creates a rotation matrix, *R*[30]
  prefs: []
  type: TYPE_NORMAL
- en: ② The inverse of a rotation matrix is the same as its transpose.
  prefs: []
  type: TYPE_NORMAL
- en: ③ Multiplying a rotation matrix and its inverse yields the identity matrix.
  prefs: []
  type: TYPE_NORMAL
- en: ④ A vector ![](../../OEBPS/Images/AR_u.png) rotated by matrix *R*[30] to yield
    vector ![](../../OEBPS/Images/AR_v.png), *R*[30]![](../../OEBPS/Images/AR_u.png)
    = ![](../../OEBPS/Images/AR_v.png).
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ The norm of a vector is the same as its length. Rotation preserves the length
    of a vector ||*R![](../../OEBPS/Images/AR_u.png)*|| = ||![](../../OEBPS/Images/AR_u.png)||.
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Rotation by an angle followed by rotation by the negative of that angle takes
    the vector back to its original position. Rotation by a negative angle is equivalent
    to inverse rotation.
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ A matrix that rotates by an angle is the inverse of the matrix that rotates
    by the negative of the same angle.
  prefs: []
  type: TYPE_NORMAL
- en: 2.14.4 Eigenvalues and eigenvectors of a rotation matrix:Finding the axis of
    rotation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let *λ*, ![](../../OEBPS/Images/AR_e.png) be an eigenvalue, eigenvector pair
    of a rotation matrix *R*. Then *R![](../../OEBPS/Images/AR_e.png)* = *λ![](../../OEBPS/Images/AR_e.png)*
  prefs: []
  type: TYPE_NORMAL
- en: Transposing both sides,
  prefs: []
  type: TYPE_NORMAL
- en: '*![](../../OEBPS/Images/AR_e.png)^TR^T* = *λ![](../../OEBPS/Images/AR_e.png)^T*'
  prefs: []
  type: TYPE_NORMAL
- en: Multiplying the left and right sides, respectively, with the equivalent entities
    *R![](../../OEBPS/Images/AR_e.png)* and *λ![](../../OEBPS/Images/AR_e.png)*, we
    get
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_e.png)*^TR^T*(*R*![](../../OEBPS/Images/AR_e.png))
    = *λ![](../../OEBPS/Images/AR_e.png)^T*(*λ*![](../../OEBPS/Images/AR_e.png)) ⇔
    ![](../../OEBPS/Images/AR_e.png)*^T(R^TR*)![](../../OEBPS/Images/AR_e.png) = *λ*²![](../../OEBPS/Images/AR_e.png)*^T![](../../OEBPS/Images/AR_e.png)
    ⇔ ![](../../OEBPS/Images/AR_e.png)^T*(**I**)![](../../OEBPS/Images/AR_e.png) =
    *λ*²![](../../OEBPS/Images/AR_e.png)^T![](../../OEBPS/Images/AR_e.png)'
  prefs: []
  type: TYPE_IMG
- en: ⇔ ![](../../OEBPS/Images/AR_e.png)^T![](../../OEBPS/Images/AR_e.png) = *λ*²![](../../OEBPS/Images/AR_e.png)^T![](../../OEBPS/Images/AR_e.png)
    ⇔ *λ*² = 1 ⇔ *λ* = 1
  prefs: []
  type: TYPE_NORMAL
- en: '(The negative solution *λ* = −1 corresponds to reflection.) Thus, all rotation
    matrices have 1 as one of their eigenvalues. The corresponding eigenvector ![](../../OEBPS/Images/AR_e.png)
    satisfies *R![](../../OEBPS/Images/AR_e.png)* = ![](../../OEBPS/Images/AR_e.png).
    This is the axis of rotation: the set of points that stay where they were post-rotation.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.14.5 PyTorch code for eigenvalues and vectors of rotation matrices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following listing shows the code for the axis of rotation.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.17 Axis of rotation
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: ① ![](../../OEBPS/Images/eq_02-30-e.png)
  prefs: []
  type: TYPE_NORMAL
- en: about the *Z*-axis. All rotation matrices will havean eigenvalue 1. The corresponding
    eigenvector
  prefs: []
  type: TYPE_NORMAL
- en: is the axis of rotation (here, the *Z*-axis).
  prefs: []
  type: TYPE_NORMAL
- en: ② The PyTorch function eig() computes eigenvalues and eigenvectors.
  prefs: []
  type: TYPE_NORMAL
- en: ③ The PyTorch function where() returns the indices at which the specified condition
    is true.
  prefs: []
  type: TYPE_NORMAL
- en: ④ Obtains the eigenvector for eigenvalue 1
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ The axis of rotation is the *Z*-axis.
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Takes a random point
  prefs: []
  type: TYPE_NORMAL
- en: on this axis and applies the rotation to this point; its position does not change.
  prefs: []
  type: TYPE_NORMAL
- en: 2.15 Matrix diagonalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In section [2.12](02.xhtml#sec-lin_systems), we studied linear systems and their
    importance in machine learning. We also remarked that the standard mathematical
    process of solving linear systems via matrix inversion is not very desirable from
    a machine learning point of view. In this section, we will see one method of solving
    linear systems without matrix inversion. In addition, this section will help us
    develop the insights necessary to understand quadratic forms and, eventually,
    principal component analysis (PCA), one of the most important tools in data science.
  prefs: []
  type: TYPE_NORMAL
- en: Consider an *n* × *n* matrix *A* with *n* linearly independent eigenvectors.
    Let *S* be a matrix with these eigenvectors as its columns. That is,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-30-f.png)'
  prefs: []
  type: TYPE_IMG
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-30-g.png)'
  prefs: []
  type: TYPE_IMG
- en: Then
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-30-h.png)'
  prefs: []
  type: TYPE_IMG
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-30-i.png)'
  prefs: []
  type: TYPE_IMG
- en: is a diagonal matrix with the eigenvalues of *A* on the diagonal and 0 everywhere
    else.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we have
  prefs: []
  type: TYPE_NORMAL
- en: '*AS* = *S*Λ'
  prefs: []
  type: TYPE_NORMAL
- en: which leads to
  prefs: []
  type: TYPE_NORMAL
- en: '*A* = *S*Λ*S*^(−1)'
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: Λ = *S*^(−1)*AS*
  prefs: []
  type: TYPE_NORMAL
- en: 'If *A* is symmetric, then its eigenvectors are orthogonal. Then *S^TS* = *SS^T*
    = **I** ⇔ *S*^(−1) = *S^T*, and we get the diagonalization of *A*: *A* = *S*Λ*S^T*
    Note that diagonalization is not unique: a given matrix may be diagonalized in
    multiple ways.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.15.1 PyTorch code for matrix diagonalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we will study the PyTorch implementation of the math we learned in section
    [2.15](02.xhtml#sec-mat-diagonalization). As usual, we will only show the directly
    relevant bit of code here.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional code for this section, executable via Jupyter Notebook,
    can be found at [http://mng.bz/RXJn](http://mng.bz/RXJn).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.18 Diagonalization of a matrix
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: ① Diagonalization is factorizing a matrix *A* = *SΣS*^(−1). *S* is a matrix
    with eigenvectors of *A* as columns. Σ is a diagonal matrix with eigenvalues of
    *A* in the diagonal.
  prefs: []
  type: TYPE_NORMAL
- en: ② The PyTorch function eig() returns eigenvalues and vectors.
  prefs: []
  type: TYPE_NORMAL
- en: ③ The PyTorch function diag() creates a diagonal matrix of given values.
  prefs: []
  type: TYPE_NORMAL
- en: ④ Returns the three factors
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Creates a matrix *A*
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Reconstructs *A* from its factors
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Verifies that the reconstructed matrix is the same as the original
  prefs: []
  type: TYPE_NORMAL
- en: 2.15.2 Solving linear systems without inversion via diagonalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Diagonalization has many practical applications. Let’s study one now. In general,
    matrix inversion (that is, computing *A*^(−1)) is a very complex process that
    is numerically unstable. Hence, solving *A*![](../../OEBPS/Images/AR_x.png) =
    ![](../../OEBPS/Images/AR_b.png) via ![](../../OEBPS/Images/AR_x.png) = *A*^(−1)![](../../OEBPS/Images/AR_b.png)
    is to be avoided when possible. In the case of a square symmetric matrix with
    *n* distinct eigenvalues, diagonalization can come to the rescue. We can solve
    this in multiple steps. We first diagonalize *A*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A* = *S*Λ*S^T*'
  prefs: []
  type: TYPE_NORMAL
- en: Then
  prefs: []
  type: TYPE_NORMAL
- en: '*A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '*S*Λ*S^T*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'where *S* is the matrix with eigenvectors of *A* as its columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '*S* = [![](../../OEBPS/Images/AR_e.png)[1]   ![](../../OEBPS/Images/AR_e.png)[2]  
    … ![](../../OEBPS/Images/AR_e.png)*[n]*]'
  prefs: []
  type: TYPE_NORMAL
- en: '(Since *A* is symmetric, these eigenvectors are orthogonal. Hence *S^TS* =
    *SS^T* = **I**.) The solution can be obtained in a series of very simple steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-30-j.png)'
  prefs: []
  type: TYPE_IMG
- en: First solve
  prefs: []
  type: TYPE_NORMAL
- en: '*S*![](../../OEBPS/Images/AR_y.png)[1] = ![](../../OEBPS/Images/AR_b.png)'
  prefs: []
  type: TYPE_NORMAL
- en: as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_y.png)[1] = *S^T* ![](../../OEBPS/Images/AR_b.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that both the transpose and matrix-vector multiplications are simple
    and numerically stable operations, unlike matrix inversion. Then we get
  prefs: []
  type: TYPE_NORMAL
- en: Λ(*S^T*![](../../OEBPS/Images/AR_x.png)) = ![](../../OEBPS/Images/AR_y.png)[1]
  prefs: []
  type: TYPE_NORMAL
- en: Now solve
  prefs: []
  type: TYPE_NORMAL
- en: Λ![](../../OEBPS/Images/AR_y.png)[2] = ![](../../OEBPS/Images/AR_y.png)[1]
  prefs: []
  type: TYPE_NORMAL
- en: as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_y.png)[2] = Λ^(–1)![](../../OEBPS/Images/AR_y.png)[1]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that since Λ is a diagonal matrix, inverting it is trivial:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-31.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 2.31
  prefs: []
  type: TYPE_NORMAL
- en: As a final step, solve
  prefs: []
  type: TYPE_NORMAL
- en: '*S^T*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_y.png)[2]'
  prefs: []
  type: TYPE_NORMAL
- en: as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_x.png) = *S*![](../../OEBPS/Images/AR_y.png)[2]'
  prefs: []
  type: TYPE_NORMAL
- en: Thus we have obtained ![](../../OEBPS/Images/AR_x.png) without a single complex
    or unstable step.
  prefs: []
  type: TYPE_NORMAL
- en: 2.15.3 PyTorch code for solving linear systems via diagonalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s try solving the following set of equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '*x* +   *y* +  *z* = 8'
  prefs: []
  type: TYPE_NORMAL
- en: 2*x* + 2*y* + 3*z* = 15
  prefs: []
  type: TYPE_NORMAL
- en: '*x* + 3*y* + 3*z* = 16'
  prefs: []
  type: TYPE_NORMAL
- en: This can be written using matrices and vectors as
  prefs: []
  type: TYPE_NORMAL
- en: '*A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)'
  prefs: []
  type: TYPE_NORMAL
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-31-a.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that *A* is a symmetric matrix. It has orthogonal eigenvectors. The matrix
    with eigenvectors of *A* in columns is orthogonal. Its transpose and inverse are
    the same.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.19 Solving linear systems using diagonalization
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: ① Creates a symmetric matrix *A*
  prefs: []
  type: TYPE_NORMAL
- en: ② Asserts that *A* may be symmetric
  prefs: []
  type: TYPE_NORMAL
- en: ③ Creates a vector ![](../../OEBPS/Images/AR_b.png)
  prefs: []
  type: TYPE_NORMAL
- en: ④ Solves *A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)
    using matrix inversion, ![](../../OEBPS/Images/AR_x.png) = *A*^(−1)![](../../OEBPS/Images/AR_b.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: matrix inversion is numerically unstable.'
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Solves *A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)
    via diagonalization. *A* = *S*Σ*S^T*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-31-b.png).'
  prefs: []
  type: TYPE_NORMAL
- en: '⑥ 1\. Solve: *S*![](../../OEBPS/Images/AR_y.png)[1] = ![](../../OEBPS/Images/AR_b.png)
    as ![](../../OEBPS/Images/AR_y.png)[1] = *S^T* ![](../../OEBPS/Images/AR_b.png)
    (no matrix inversion)'
  prefs: []
  type: TYPE_NORMAL
- en: '⑦ 2\. Solve: Λ ![](../../OEBPS/Images/AR_y.png)[2] = ![](../../OEBPS/Images/AR_y.png)[1]
    as ![](../../OEBPS/Images/AR_y.png)[2] = Λ^(-1)![](../../OEBPS/Images/AR_y.png)[1]
    (inverting a diagonal matrix is easy; see equation [2.31](02.xhtml#eq-diag-inv).)'
  prefs: []
  type: TYPE_NORMAL
- en: '⑧ 3\. Solve: *S^T*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_y.png)[2]
    as ![](../../OEBPS/Images/AR_x.png) = *S*![](../../OEBPS/Images/AR_y.png)[2] (no
    matrix inversion)'
  prefs: []
  type: TYPE_NORMAL
- en: ⑨ Verifies that the two solutions are the same
  prefs: []
  type: TYPE_NORMAL
- en: ⑨ Verifies that the two solutions are the same
  prefs: []
  type: TYPE_NORMAL
- en: 2.15.4 Matrix powers using diagonalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If matrix *A* can be diagonalized, then
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-31-c.png)'
  prefs: []
  type: TYPE_IMG
- en: For a diagonal matrix
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-31-d.png)'
  prefs: []
  type: TYPE_IMG
- en: the *n*th power is simply
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-31-e.png)'
  prefs: []
  type: TYPE_IMG
- en: If we need to compute various powers of an *m* × *m* matrix A at different times,
    we should precompute the matrix S and compute any power with only *O*(*m*) operations—compared
    to the (*nm*³) operations necessary for naive computations.
  prefs: []
  type: TYPE_NORMAL
- en: 2.16 Spectral decomposition of a symmetric matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have seen in section [2.15](02.xhtml#sec-mat-diagonalization) that a square
    symmetric matrix with distinct eigenvalues can be decomposed as
  prefs: []
  type: TYPE_NORMAL
- en: '*A* = *S*Λ*S^T*'
  prefs: []
  type: TYPE_NORMAL
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: '*A* = [![](../../OEBPS/Images/AR_e.png)[1] ![](../../OEBPS/Images/AR_e.png)[2]
    … ![](../../OEBPS/Images/AR_e.png)[n]]'
  prefs: []
  type: TYPE_NORMAL
- en: Thus,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-31-f.png)'
  prefs: []
  type: TYPE_IMG
- en: This equation can be rewritten as
  prefs: []
  type: TYPE_NORMAL
- en: '*A* = *λ*[1]![](../../OEBPS/Images/AR_e.png)[1]![](../../OEBPS/Images/AR_e.png)[1]^T
    + *λ*[2]![](../../OEBPS/Images/AR_e.png)[2]![](../../OEBPS/Images/AR_e.png)[2]*^T*
    + … + *λ[n]![](../../OEBPS/Images/AR_e.png)[n]*![](../../OEBPS/Images/AR_e.png)*[n]^T*'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 2.32
  prefs: []
  type: TYPE_NORMAL
- en: Thus a square symmetric matrix can be written in terms of its eigenvalues and
    eigenvectors. This is the spectral resolution theorem.
  prefs: []
  type: TYPE_NORMAL
- en: 2.16.1 PyTorch code for the spectral decomposition of a matrix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following listing shows the relevant code for this section.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.20 Spectral decomposition of a matrix
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: ① Asserts that A is a 2D tensor (i.e., matrix)
  prefs: []
  type: TYPE_NORMAL
- en: '② A is square: i.e., *A*.*shape*[0] (num rows) ≜ *A*.*shape*[1] (num columns)'
  prefs: []
  type: TYPE_NORMAL
- en: '③ Asserts that A is symmetric: i.e., *A* = = *A^T*'
  prefs: []
  type: TYPE_NORMAL
- en: ④ The PyTorch function eig() returns eigenvectors and values.
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Defines a 3D tensor *C* of shape *n* × *n* × *n* to hold the *n* components
    from equation [2.32](02.xhtml#eq-spectral-decomp). Each term *λ[i]![](../../OEBPS/Images/AR_e.png)[i]![](../../OEBPS/Images/AR_e.png)**^T*
    is an *n* × *n* matrix. There are *n* such terms, all compactly held in tensor
    *C*.
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Computes *C*[*i*] = *λ[i]![](../../OEBPS/Images/AR_e.png)[i]![](../../OEBPS/Images/AR_e.png)**^T*
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Reconstructs *A* by adding its components stored in *C*
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Verifies that the matrix reconstructed from spectral components matches the
    original
  prefs: []
  type: TYPE_NORMAL
- en: '2.17 An application relevant to machine learning: Finding the axes of a hyperellipse'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The notion of an ellipse in high-dimensional space (aka hyperellipse) keeps
    coming back in various forms in machine learning. Here we will make a preliminary
    review of them. We will revisit these concepts later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall the equation of an ellipse from high school math:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-32-a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is a rather simple ellipse: it is two-dimensional and centered at the
    origin, and its major and minor axes are aligned with the coordinate axes. Denoting
    ![](../../OEBPS/Images/eq_02-32-b.png) as the position vector, the same equation
    can be written as'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_x.png)*^TΛ*![](../../OEBPS/Images/AR_x.png) = 1'
  prefs: []
  type: TYPE_NORMAL
- en: where ![](../../OEBPS/Images/eq_02-32-c.png) is a diagonal matrix. Written in
    this form, the equation can be extended beyond 2D to an *n*-dimensional axis-aligned
    ellipse centered at the origin. Now let’s apply a rotation *R* to the axis. Then
    every vector ![](../../OEBPS/Images/AR_x.png) transforms to *R*![](../../OEBPS/Images/AR_x.png).
    The equation of the ellipse in the new (rotated) coordinate system is
  prefs: []
  type: TYPE_NORMAL
- en: (*R![](../../OEBPS/Images/AR_x.png)^T Λ* (*R![](../../OEBPS/Images/AR_x.png)*)
    = 1
  prefs: []
  type: TYPE_NORMAL
- en: ⇔ ![](../../OEBPS/Images/AR_x.png)^T (*R^T ΛR*) ![](../../OEBPS/Images/AR_x.png)
    = 1
  prefs: []
  type: TYPE_NORMAL
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: '*A* = (*R^TΛR*).'
  prefs: []
  type: TYPE_NORMAL
- en: The generalized equation of the ellipse is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_x.png)*^TA*![](../../OEBPS/Images/AR_x.png) = 1'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The ellipse is no longer axis aligned.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The matrix *A* is no longer diagonal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A* is symmetric. We can easily verify that *A^T* = (*R^T*Λ*R*)*^T* = *R^T*Λ*^TR*
    = *R^T*Λ*R* (remember, the transpose of a diagonal matrix is itself).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If, in addition, we want to get rid of the “centered at the origin” assumption,
    we get
  prefs: []
  type: TYPE_NORMAL
- en: (![](../../OEBPS/Images/AR_x.png)−*μ*)*^TA*(![](../../OEBPS/Images/AR_x.png)−*μ*)
    = 1
  prefs: []
  type: TYPE_NORMAL
- en: Equation 2.33
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s flip the problem around. Suppose we have a generic *n*-dimensional
    ellipse. How do we compute its axes’ directions?
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, if we can rotate the coordinate system so that the matrix in the middle
    is diagonal, we are done. Diagonalization (see section [2.15](02.xhtml#sec-mat-diagonalization))
    is the answer. Specifically, we find the matrix *S* with eigenvectors of *A* in
    its columns. This is a rotation matrix (being orthogonal, since *A* is symmetric).
    We transform rotate) the coordinate system by applying this matrix. In this new
    coordinate system, the ellipse is axis aligned. Stated another way, the new coordinate
    axes—these are the eigenvectors of *A*—yield the axes of the ellipse.
  prefs: []
  type: TYPE_NORMAL
- en: 2.17.1 PyTorch code for hyperellipses
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s try finding the axes of the hyperellipse described by the equation 5*x*²
    + 6*xy* + 5*y*² = 20. Note that the actual ellipse we use as an example is 2D
    (to facilitate visualization), but the code we develop will be general and extensible
    to multiple dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: The ellipse equation can be written using matrices and vectors as ![](../../OEBPS/Images/AR_x.png)*^TA*![](../../OEBPS/Images/AR_x.png)
    = 1, where
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_02-33-a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To find the axes of the hyperellipse, we need to transform the coordinate system
    so that the matrix in the middle becomes diagonal. Here is how this can be done:
    if we diagonalize *A* into *S*Σ*S*^(−1), then the ellipse equation becomes ![](../../OEBPS/Images/AR_x.png)*^TS*Σ*S*^(−1)![](../../OEBPS/Images/AR_x.png)
    = 1, where Σ is a diagonal matrix. Since *A* is symmetric, its eigenvectors are
    orthogonal. Hence, the matrix containing these eigenvectors as columns is orthogonal:
    i.e., *S*^(−1) = *S^T*. In other words, *S* is a rotation matrix. So the ellipse
    equation becomes ![](../../OEBPS/Images/AR_x.png)*^TS*Σ*S^T*![](../../OEBPS/Images/AR_x.png)
    = 1 or (![](../../OEBPS/Images/AR_x.png)*^TS*)Σ(*S^T*![](../../OEBPS/Images/AR_x.png))
    = 1 or ![](../../OEBPS/Images/AR_y.png)*^T*Σ![](../../OEBPS/Images/AR_y.png) =
    1 where ![](../../OEBPS/Images/AR_y.png) = *S^T*![](../../OEBPS/Images/AR_x.png).
    This is of the desired form since Σ is a diagonal matrix. Remember, *S* is a rotation
    matrix. Thus, rotating the coordinate system by *S* aligns the coordinate axes
    with the ellipse axes.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.21 Axes of a hyperellipse
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '① Equation of the ellipse: 5*x*² + 6*x**y* + 5*y*² = 20 or ![](../../OEBPS/Images/AR_x.png)*^TA*![](../../OEBPS/Images/AR_x.png)
    = 20, where ![](../../OEBPS/Images/eq_02-33-b.png)'
  prefs: []
  type: TYPE_NORMAL
- en: ② X-axis vector
  prefs: []
  type: TYPE_NORMAL
- en: ③ Major axis of the ellipse
  prefs: []
  type: TYPE_NORMAL
- en: ④ The dot product between two vectors is the cosine of the angle between them.
  prefs: []
  type: TYPE_NORMAL
- en: '⑤ The angle between the ellipse’s major axis and the X-axis: 45° (see figure
    [2.16](02.xhtml#fig-numpy-hyper-ellipse))'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH02_F16_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.16 Note that the ellipse’s major axis forms an angle of 45 degrees
    with the X-axis. Rotating the coordinate system by this angle will align the ellipse
    axes with the coordinate axes. Subsequently, the first principal vector will also
    lie along this direction.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In machine learning, a vector is a one-dimensional array of numbers and a matrix
    is a two-dimensional array of numbers. Inputs and outputs of machine learning
    models are typically represented as vectors or matrices. In multilayered models,
    inputs and outputs of each individual layer are also represented as vectors or
    matrices. Images are two-dimensional arrays of numbers corresponding to pixel
    color values. As such, they are represented as matrices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An *n*-dimensional vector can be viewed as point in ℝ*^n* space. All models
    can be viewed as functions that map points from input to output space. The model
    is designed so that it is easier to solve the problem of interest in the output
    space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A dot product between a pair of vectors ![](../../OEBPS/Images/AR_x.png) = [*x*[1]
       *x*[2]   …   *x[n]*] and ![](../../OEBPS/Images/AR_y.png) = [*y*[1]    *y*[2]
      …   *y[n]*] is the scalar quantity ![](../../OEBPS/Images/AR_x.png) ⋅ ![](../../OEBPS/Images/AR_y.png)
    = *x*[1]*y*[1] + *x*[2]*y*[2] + ⋯ + *x[n]**y**[n]*. It is a measure of how similar
    the vectors are. Dot products are widely used in machine learning. For instance,
    in supervised machine learning, we train the model so that its output is as similar
    as possible to the known output for a sample set of input points known as training
    data. Here, some variant of the dot product is often used to measure the similarity
    of the model output and the known output. Two vectors are orthogonal if their
    dot product is zero. This means the vectors have no similarity and are independent
    of each other.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A vector’s dot product with itself is the square of the magnitude or length
    of the vector ![](../../OEBPS/Images/AR_x.png) ⋅ ![](../../OEBPS/Images/AR_x.png)
    = ||![](../../OEBPS/Images/AR_x.png)||² = *x*[1]*x*[1] + *x*[2]*x*[2] + ⋯ + *x[n]x[n]*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Given a set of vectors ![](../../OEBPS/Images/AR_x.png)[1], ![](../../OEBPS/Images/AR_x.png)[2],
    ⋯, ![](../../OEBPS/Images/AR_x.png)*[n]*, the weighted sum *a*[1]![](../../OEBPS/Images/AR_x.png)[1]
    + *a*[2]![](../../OEBPS/Images/AR_x.png)[2] + ⋯ + *a[n]*![](../../OEBPS/Images/AR_x.png)*[n]*
    where *a*[1], *a*[2], ⋯, *a[n]* are arbitrary scalars) is known as a linear combination.
    In particular, if the coefficients *a*[1], *a*[2], ⋯, *a[n]* are non-negative
    and they sum to 1, the linear combination is called a *convex* combination.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If it is possible to find a set of coefficients *a*[1], *a*[2], ⋯, *a[n]*, not
    all zero, such that the linear combination is a null vector (meaning all its elements
    are zeros), then the vectors ![](../../OEBPS/Images/AR_x.png)[1], ![](../../OEBPS/Images/AR_x.png)[2],
    ⋯, ![](../../OEBPS/Images/AR_x.png)*[n]* are said to *linearly dependent*. On
    the other hand, if the only way to obtain a linear combination that is a null
    vector is to make every coefficient zero, the vectors are said to be *linearly
    independent*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: One important application of matrices and vectors is to solve a system of linear
    equations. Such a system can be expressed in matrix vector terms as *A*![](../../OEBPS/Images/AR_x.png)
    = ![](../../OEBPS/Images/AR_b.png), where we solve for an unknown vector ![](../../OEBPS/Images/AR_x.png)
    satisfying the equation. This system has an exact solution if and only if *A*
    is invertible. This means *A* is a square matrix (the number of rows equals the
    number of columns) and the row vectors are linearly independent. If the row vectors
    are linearly independent, so are the column vectors, and vice versa. If the rows
    and columns are linearly independent, the determinant of *A* is guaranteed to
    be nonzero. Hence, linear independence of rows/columns and nonzero determinant
    are equivalent conditions. If any one of them is satisfied, the linear system
    has an exact and unique solution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In practice, this requirement is often not met, and we have an over- or under-determined
    system. In such situations, the Moore-Penrose inverse leads to a form of best
    approximation. Geometrically, the Moore-Penrose method yields the point that is
    closest to ![](../../OEBPS/Images/AR_b.png) in the space of vectors spanned by
    columns of *A*. Equivalently, the Moore-Penrose solution ![](../../OEBPS/Images/AR_x.png)[*]
    yields the point closest to ![](../../OEBPS/Images/AR_b.png) on the space of vectors
    spanned by the columns of *A*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For a square matrix *A*, if and only if *Aê* = *λê*, we say *λ* is an eigenvalue
    (a scalar) and *ê* is an eigenvector (a unit vector) of *A*. Physically, the eigenvector
    *ê* is a unit vector whose direction does not change when transformed by the matrix
    *A*. The transform can magnify its length by the scalar scale factor *λ*, which
    is the eigenvalue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An *n* × *n* matrix *A* has *n* eigenvalue/eigenvector pairs. The eigenvalues
    need not all be unique. The eigenvectors corresponding to different eigenvalues
    are linearly independent. If the matrix *A* is symmetric, satisfying *A^T* = *A*,
    the eigenvectors corresponding to different eigenvalues are orthogonal. A rotation
    matrix is a matrix in which the rows are orthogonal to each other and so are the
    columns. Such a matrix is also known as an orthogonal matrix. An orthogonal matrix
    *R* satisfies the equation *R^TR* = **I**, where **I** is the identity matrix.
    In the special case when the matrix *A* is a rotation matrix *R*, one of the eigenvalues
    is always 1. The corresponding eigenvector is the axis of rotation. A matrix *A*
    with *n* linearly independent eigenvectors can be decomposed as *A* = *S*Λ*S*^(−1),
    where *S* =[![](../../OEBPS/Images/AR_e.png)[1]    ![](../../OEBPS/Images/AR_e.png)[2]
      …   *![](../../OEBPS/Images/AR_e.png)[n]*] is the matrix with eigenvectors of
    *A* as its columns and Λ is a diagonal matrix with the eigenvalues of *A* as its
    diagonal. This decomposition is called matrix diagonalization and leads to a numerically
    stable way to solve linear systems.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A square symmetric matrix *A* can be expressed in terms of its eigenvectors
    and eigenvalues as *A* = *λ*[1]![](../../OEBPS/Images/AR_e.png)[1]![](../../OEBPS/Images/AR_e.png)[1]*^T*
    + *λ*[2]![](../../OEBPS/Images/AR_e.png)[2]![](../../OEBPS/Images/AR_e.png)[2]*^T*
    +   …  + *λ[n]![](../../OEBPS/Images/AR_e.png)[n]*![](../../OEBPS/Images/AR_e.png)*[n]^T*.
    This is known as the spectral decomposition of the matrix *A*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: ¹  In mathematics, vectors can have an infinite number of elements. Such vectors
    cannot be expressed as arrays—but we will mostly ignore them in this book. [↩](02.xhtml#fnref4)
  prefs: []
  type: TYPE_NORMAL
- en: ²  We usually use uppercase letters to symbolize matrices. [↩](02.xhtml#fnref5)
  prefs: []
  type: TYPE_NORMAL
- en: ³  In digital computers, numbers in the range 0..255 can be represented with
    a single byte of storage; hence this choice. [↩](02.xhtml#fnref6)
  prefs: []
  type: TYPE_NORMAL
- en: ⁴  The mathematical symbol ∀ stands for “for all.” Thus, ∀![](../../OEBPS/Images/AR_y.png)
    ∈ ℜ*^n* means “all vectors y in the *n*-dimensional space.” [↩](02.xhtml#fnref7)
  prefs: []
  type: TYPE_NORMAL
- en: ⁵  You can compute eigenvectors and eigenvalues only of square matrices. [↩](02.xhtml#fnref8)
  prefs: []
  type: TYPE_NORMAL
