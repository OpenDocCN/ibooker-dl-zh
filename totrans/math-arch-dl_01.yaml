- en: 2 Vectors, matrices, and tensors in machine learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 机器学习中的向量、矩阵和张量
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Vectors and matrices and their role in datascience
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量和矩阵及其在数据科学中的作用
- en: Working with eigenvalues and eigenvectors
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用特征值和特征向量工作
- en: Finding the axes of a hyper-ellipse
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找超椭圆的轴
- en: At its core, machine learning, and indeed all computer software, is about number
    crunching. We input a set of numbers into the machine and get back a different
    set of numbers as output. However, this cannot be done randomly. It is important
    to organize these numbers appropriately and group them into meaningful objects
    that go into and come out of the machine. This is where vectors and matrices come
    in. These are concepts that mathematicians have been using for centuries—we are
    simply reusing them in machine learning.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本质上，机器学习，以及所有计算机软件，都是关于数字处理的。我们将一组数字输入到机器中，并得到一组不同的数字作为输出。然而，这不能是随机的。适当地组织这些数字并将它们分组成有意义的对象，这些对象进入并从机器中出来，这是非常重要的。这就是向量矩阵发挥作用的地方。这些是数学家使用了几百年的概念——我们只是在机器学习中重新使用它们。
- en: In this chapter, we will study vectors and matrices, primarily from a machine
    learning point of view. Starting from the basics, we will quickly graduate to
    advanced concepts, restricting ourselves to topics relevant to machine learning.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将从机器学习角度研究向量和矩阵，从基础知识开始，迅速过渡到高级概念，限制 ourselves 到与机器学习相关的主题。
- en: We provide Jupyter Notebook-based Python implementations for most of the concepts
    discussed in this and other chapters. Complete, fully functional code that can
    be downloaded and executed (after installing Python and Jupyter Notebook) can
    be found at [http://mng.bz/KMQ4](http://mng.bz/KMQ4). The code relevant to this
    chapter can be found at [http://mng.bz/d4nz](http://mng.bz/d4nz).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为本书及本章讨论的大部分概念提供了基于 Jupyter Notebook 的 Python 实现。完整的、功能齐全的代码（在安装 Python 和
    Jupyter Notebook 后）可以下载并执行，地址为 [http://mng.bz/KMQ4](http://mng.bz/KMQ4)。本章相关的代码可以在
    [http://mng.bz/d4nz](http://mng.bz/d4nz) 找到。
- en: 2.1 Vectors and their role in machine learning
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 向量和它们在机器学习中的作用
- en: Let’s revisit the machine learning model for a cat brain introduced in section
    [1.3](../Text/01.xhtml#sec-cat_brain). It takes two numbers as input, representing
    the hardness and sharpness of the object in front of the cat. The cat brain processes
    the input and generates an output threat score that leads to a decision to *run
    away* or *ignore* or *approach and purr*. The two input numbers usually appear
    together, and it will be handy to group them into a single object. This object
    will be an ordered sequence of two numbers, the first representing hardness and
    the second representing sharpness. Such an object is a perfect example of a vector.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下第 [1.3](../Text/01.xhtml#sec-cat_brain) 节中介绍的猫脑机器学习模型。它接受两个数字作为输入，代表猫面前物体的硬度和锋利度。猫脑处理输入并生成一个输出威胁分数，这将导致决定
    *逃跑*、*忽略* 或 *靠近并咕噜咕噜叫*。这两个输入数字通常一起出现，将它们组合成一个单一对象将非常方便。这个对象将是两个数字的有序序列，第一个代表硬度，第二个代表锋利度。这样的对象是向量的一个完美例子。
- en: Thus, a *vector* can be thought of as an ordered sequence of two or more numbers,
    also known as an *array* of numbers.[¹](02.xhtml#fn4) Vectors constitute a compact
    way of denoting a set of numbers that together represent some entity. In this
    book, vectors are represented by lowercase letters with an overhead arrow and
    arrays by square brackets. For instance, the input to the cat brain model in section
    [1.3](../Text/01.xhtml#sec-cat_brain) was a vector ![](../../OEBPS/Images/eq_02-00-a2.png),
    where *x*[0] represented hardness and *x*[1] represented sharpness.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，可以将 *向量* 理解为两个或更多数字的有序序列，也称为数字的 *数组*。[¹](02.xhtml#fn4) 向量构成了一种紧凑的方式来表示一组数字，这些数字共同代表某个实体。在本书中，向量用带顶箭头的低字母表示，数组用方括号表示。例如，第
    [1.3](../Text/01.xhtml#sec-cat_brain) 节中猫脑模型的输入是一个向量 ![](../../OEBPS/Images/eq_02-00-a2.png)，其中
    *x*[0] 代表硬度，*x*[1] 代表锋利度。
- en: Outputs to machine learning models are also often represented as vectors. For
    instance, consider an object recognition model that takes an image as input and
    emits a set of numbers indicating the probabilities that the image contains a
    dog, human, or cat, respectively. The output of such a model is a three element
    vector ![](../../OEBPS/Images/eq_02-00-b2.png), where the number *y*[0] denotes
    the probability that the image contains a dog, *y*[1] denotes the probability
    that the image contains a human, and *y*[2] denotes the probability that the image
    contains a cat. Figure [2.1](02.xhtml#fig-vec_out) shows some possible input images
    and corresponding output vectors.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型的输出也常常表示为向量。例如，考虑一个以图像为输入并输出一系列数字的对象识别模型，这些数字表示图像包含狗、人类或猫的概率。这种模型的输出是一个三元素向量
    ![](../../OEBPS/Images/eq_02-00-b2.png)，其中数字 *y*[0] 表示图像包含狗的概率，*y*[1] 表示图像包含人类的概率，*y*[2]
    表示图像包含猫的概率。图[2.1](02.xhtml#fig-vec_out)显示了可能的输入图像和相应的输出向量。
- en: '![](../../OEBPS/Images/CH02_F01_Chaudhury.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH02_F01_Chaudhury.png)'
- en: Figure 2.1 Input images and corresponding output vectors denoting probabilities
    that the image contains a dog and/or human and/or cat, respectively. Example output
    vectors are shown.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 输入图像及其对应的输出向量，表示图像包含狗、人类和/或猫的概率。显示了示例输出向量。
- en: In multilayered machines like neural networks, the input and output to a layer
    can be vectors. We also typically represent the parameters of the model function
    (see section [1.3](../Text/01.xhtml#sec-cat_brain)) as vectors. This is illustrated
    in section [2.3](02.xhtml#sec-matrices).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在像神经网络这样的多层机器中，层的输入和输出可以是向量。我们通常也将模型函数的参数（参见第[1.3](../Text/01.xhtml#sec-cat_brain)节）表示为向量。这一点在第[2.3](02.xhtml#sec-matrices)节中有说明。
- en: Table 2.1 Toy documents and corresponding feature vectors describing them. Words
    eligible for the feature vector are bold. The first element of the feature vector
    indicates the number of occurrences of the word *gun* and the second *violence*.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.1 玩具文档及其对应的描述它们的特征向量。适合特征向量的单词是粗体的。特征向量的第一个元素表示单词**枪**出现的次数，第二个元素是**暴力**。
- en: '| Docid | Document | Feature vector |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| Docid | 文档 | 特征向量 |'
- en: '| --- | --- | --- |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| *d*[0] | Roses are lovely. Nobody hates roses. | [0 0] |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| *d*[0] | 玫瑰很漂亮。没有人讨厌玫瑰。 | [0 0] |'
- en: '| *d*[1] | **Gun violence** has reached an epidemic proportion in America.
    | [1 1] |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| *d*[1] | **枪支暴力**在美国已经达到了流行病的比例。 | [1 1] |'
- en: '| *d*[2] | The issue of **gun violence** is really over-hyped. One can find
    many instances of **violence**, where no **guns** were involved. | [2 2] |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| *d*[2] | **枪支暴力**的问题实际上被过度炒作。可以找到许多涉及**暴力**的例子，但没有**枪支**。 | [2 2] |'
- en: '| *d*[3] | **Guns** are for **violence** prone people. **Violence** begets
    **guns**. **Guns** beget **violence**. | [3 3] |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| *d*[3] | **枪支**是为**暴力**倾向的人准备的。**暴力**滋生**枪支**。**枪支**滋生**暴力**。 | [3 3] |'
- en: '| *d*[4] | I like **guns** but I hate **violence**. I have never been involved
    in **violence**. But I own many **guns**. **Gun violence** is incomprehensible
    to me. I do believe **gun** owners are the most anti **violence** people on the
    planet. He who never uses a **gun** will be prone to senseless **violence**. |
    [5 5] |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| *d*[4] | 我喜欢**枪支**但讨厌**暴力**。我从未参与过**暴力**。但我拥有许多**枪支**。**枪支暴力**对我来说是无法理解的。我确实相信**枪支**拥有者是地球上最反**暴力**的人。从不使用**枪支**的人容易陷入无意义的**暴力**。
    | [5 5] |'
- en: '| *d*[5] | **Guns** were used in a armed robbery in San Francisco last night.
    | [1 0] |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| *d*[5] | 昨晚旧金山发生了一起武装抢劫，使用了**枪支**。 | [1 0] |'
- en: '| *d*[6] | Acts of **violence** usually involves a weapon. | [0 1] |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| *d*[6] | **暴力**行为通常涉及武器。 | [0 1] |'
- en: 'One particularly significant notion in machine learning and data science is
    the idea of a *feature vector*. This is essentially a vector that describes various
    properties of the object being dealt with in a particular machine learning problem.
    We will illustrate the idea with an example from the world of natural language
    processing (NLP). Suppose we have a set of documents. We want to create a document
    retrieval system where, given a new document, we have to retrieve similar documents
    in the system. This essentially boils down to estimating the similarity between
    documents in a quantitative fashion. We will study this problem in detail later,
    but for now, we want to note that the most natural way to approach this is to
    create feature vectors for each document that quantitatively describe the document.
    In section [2.5.6](02.xhtml#subsection-dot_product), we will see how to measure
    the similarity between these vectors; here, let’s focus on simply creating descriptor
    vectors for the documents. A popular way to do this is to choose a set of interesting
    words (we typically exclude words like “and,” “if,” and “to” that are present
    in all documents from this list), count the number of occurrences of those interesting
    words in each document, and make a vector of those values. Table [2.1](02.xhtml#tab-doc-vec)
    shows a toy example with six documents and corresponding feature vectors. For
    simplicity, we have considered only two of the possible set of words: *gun* and
    *violence*, plural or singular, uppercase or lowercase.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习和数据科学中，一个特别重要的概念是 *特征向量* 的概念。这本质上是一个向量，它描述了在特定机器学习问题中处理的对象的各种属性。我们将通过自然语言处理（NLP）领域的一个例子来说明这个概念。假设我们有一组文档。我们想要创建一个文档检索系统，其中，给定一个新的文档，我们必须检索系统中相似的文档。这本质上归结为以定量方式估计文档之间的相似性。我们将在稍后详细研究这个问题，但现在，我们想要指出，处理这个问题的最自然的方法是为每个文档创建特征向量，这些向量定量描述了文档。在
    [2.5.6](02.xhtml#subsection-dot_product) 节中，我们将看到如何测量这些向量之间的相似性；在这里，让我们专注于简单地为文档创建描述向量。做这件事的一个流行方法是选择一组有趣的词（我们通常排除像“and”、“if”和“to”这样的词，这些词在所有文档中都存在），计算每个文档中这些有趣词的出现次数，并创建一个包含这些值的向量。表
    [2.1](02.xhtml#tab-doc-vec) 展示了一个包含六个文档及其相应特征向量的玩具示例。为了简单起见，我们只考虑了可能的一组单词中的两个：*枪*
    和 *暴力*，无论是复数还是单数，大写还是小写。
- en: As a different example, the sequence of pixels in an image can also be viewed
    as a feature vector. Neural networks in computer vision tasks usually expect this
    feature vector.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 作为另一个例子，图像中的像素序列也可以被视为一个特征向量。在计算机视觉任务中，神经网络通常期望这个特征向量。
- en: 2.1.1 The geometric view of vectors and its significance in machine learning
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.1 向量的几何视角及其在机器学习中的重要性
- en: Vectors can also be viewed geometrically. The simplest example is a two-element
    vector ![](../../OEBPS/Images/eq_02-00-c2.png). Its two elements can be taken
    to be *x* and *y*, Cartesian coordinates in a two-dimensional space, in which
    case the vector corresponds to a point in that space. *Vectors with n elements
    represent points in an n-dimensional space*. The ability to see inputs and outputs
    of a machine learning model as points allows us to view the model itself as a
    geometric transformation that maps input points to output points in some high-dimensional
    space. We have already seen this in section [1.4](../Text/01.xhtml#sec-geom-view-ml).
    It is an enormously powerful concept we will use throughout the book.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 向量也可以从几何学的角度来理解。最简单的例子是一个二维向量 ![二维向量](../../OEBPS/Images/eq_02-00-c2.png)。它的两个元素可以被认为是
    *x* 和 *y*，这是二维空间中的笛卡尔坐标，在这种情况下，向量对应于该空间中的一个点。*具有 n 个元素的向量代表 n 维空间中的点*。将机器学习模型的输入和输出视为点的能力使我们能够将模型本身视为一种几何变换，它将输入点映射到某些高维空间中的输出点。我们已经在
    [1.4](../Text/01.xhtml#sec-geom-view-ml) 节中看到了这一点。这是一个我们将在整本书中使用的极其强大的概念。
- en: A vector represents a point in space. Also, an array of coordinate values like
    ![](../../OEBPS/Images/eq_02-00-d2.png) describes the position of one point *in
    a given coordinate system*. Hence, an array (of coordinate values) can be viewed
    as the quantitative representation of a vector. See figure [2.2](02.xhtml#fig-vector_diagram)
    to get an intuitive understanding of this.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 向量表示空间中的一个点。此外，一个坐标值数组，例如 ![坐标值数组](../../OEBPS/Images/eq_02-00-d2.png)，描述了在给定坐标系中一个点的位置
    *in a given coordinate system*。因此，一个坐标值数组（的集合）可以被视为向量的定量表示。参见图 [2.2](02.xhtml#fig-vector_diagram)
    以获得对此的直观理解。
- en: '![](../../OEBPS/Images/CH02_F02_Chaudhury.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![查德胡里图像](../../OEBPS/Images/CH02_F02_Chaudhury.png)'
- en: 'Figure 2.2 A vector describing the position of point P with respect to point
    O. The basic mental picture is an arrowed line. This agrees with the definition
    of a vector that you may have learned in high school: a vector has a magnitude
    (length of the arrowed line) and direction (indicated by the arrow). On a plane,
    this is equivalent to the ordered pair of numbers *x*, *y*, where the geometric
    interpretations of *x* and *y* are as shown in the figure. In this context, it
    is worthwhile to note that only the relative positions of the points O and P matter.
    If both the points are moved, keeping their relationship intact, the vector does
    not change.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 描述点P相对于点O的位置的向量。基本的思维图是一个带箭头的线。这与你在高中可能学过的向量的定义一致：向量有一个大小（箭头线的长度）和方向（由箭头指示）。在平面上，这相当于有序对数字
    *x*，*y*，其中 *x* 和 *y* 的几何解释如图所示。在此背景下，值得注意的是，只有点O和P的相对位置才是重要的。如果两个点都移动，保持它们的关系不变，向量不会改变。
- en: For a real life example, consider the plane of a page of this book. Suppose
    we want to reach the top-right corner point of the page from the bottom-left corner.
    Let’s call the bottom-left corner *O* and the top-right corner *P*. We can travel
    the width (8.5 inches) to the right to reach the bottom-left corner and then travel
    the height (11 inches) upward to reach the top-right corner. Thus, if we choose
    a coordinate system with the bottom-left corner as the origin and the *X*-axis
    along the width, and the *Y*-axis along the height, point *P* corresponds to the
    array representation ![](../../OEBPS/Images/eq_02-00-e2.png). But we could also
    travel along the diagonal from the bottom-left to the top-right corner to reach
    *P* from *O*. Either way, we end up at the same point *P*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个现实生活中的例子，考虑这本书的一页的平面。假设我们想从左下角到达页面的右上角。让我们称左下角为 *O*，右上角为 *P*。我们可以向右移动宽度（8.5英寸）到达左下角，然后向上移动高度（11英寸）到达右上角。因此，如果我们选择一个以左下角为原点，*X*轴沿着宽度，*Y*轴沿着高度的坐标系，点
    *P* 对应于数组表示 ![](../../OEBPS/Images/eq_02-00-e2.png)。但我们也可能沿着对角线从左下角到右上角移动到达 *P*。无论哪种方式，我们最终都会到达同一个点
    *P*。
- en: This leads to a conundrum. The vector ![](../../OEBPS/Images/AR_OP.png) represents
    the abstract geometric notion “position of *P* with respect to *O*” independent
    of our choice of coordinate axes. On the other hand, the array representation
    depends on the choice of a coordinate system. For example, the array ![](../../OEBPS/Images/eq_02-00-e2.png)
    represents the top-right corner point *P* only under a specific choice of coordinate
    axes (parallel to the sides of the page) and a reference point (bottom-left corner).
    Ideally, to be unambiguous, we should specify the coordinate system along with
    the array representation. Why don’t we ever do this in machine learning? Because
    in machine learning, it doesn’t exactly matter what the coordinate system is as
    long as we stick to any fixed coordinate system. Machine learning is about minimizing
    loss functions (which we will study later). As such, absolute positions of point
    are immaterial, only relative positions matter.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了一个难题。向量 ![](../../OEBPS/Images/AR_OP.png) 代表了抽象的几何概念“*P*相对于*O*的位置”，与我们选择的坐标轴无关。另一方面，数组表示依赖于坐标系的选择。例如，数组
    ![](../../OEBPS/Images/eq_02-00-e2.png) 仅在特定的坐标轴选择（与页面边缘平行）和参考点（左下角）下表示右上角点 *P*。理想情况下，为了明确，我们应该指定坐标系以及数组表示。为什么在机器学习中我们从不这样做呢？因为在机器学习中，只要我们坚持任何固定的坐标系，坐标系统的具体选择并不重要。机器学习是关于最小化损失函数（我们将在后面学习）。因此，点的绝对位置无关紧要，只有相对位置才是重要的。
- en: There are explicit rules (which we will study later) that state how the vector
    transforms when the coordinate system changes. We will invoke them when necessary.
    All vectors used in a machine learning computation must consistently use the same
    coordinate system or be transformed appropriately.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 存在明确的规则（我们将在后面学习），这些规则说明了当坐标系改变时向量如何变换。在需要时我们将调用这些规则。在机器学习计算中使用的所有向量必须始终使用相同的坐标系或适当地进行变换。
- en: 'One other point: planar spaces, such as the plane of the paper on which this
    book is written, are two-dimensional (2D). The mechanical world we live in is
    three-dimensional (3D). Human imagination usually fails to see higher dimensions.
    In machine learning and data science, we often talk of spaces with thousands of
    dimensions. You may not be able to see those spaces in your mind, but that is
    not a crippling limitation. You can use 3D analogues in your head. They work in
    a surprisingly large variety of cases. However, it is important to bear in mind
    that this is not always true. Some examples where the lower-dimensional intuitions
    fail at higher dimensions will be shown later.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 另一点：平面空间，例如本书所写的纸张平面，是二维的（2D）。我们生活的机械世界是三维的（3D）。人类的想象力通常无法看到更高维度。在机器学习和数据科学中，我们经常谈论具有数千维度的空间。你可能无法在心中看到这些空间，但这并不是一个致命的限制。你可以在心中使用三维类比。它们在许多情况下都有效。然而，重要的是要记住，这并不总是正确的。稍后将会展示一些在更高维度中低维直觉失效的例子。
- en: 2.2 PyTorch code for vector manipulations
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 使用PyTorch进行向量操作代码
- en: PyTorch is an open source machine learning library developed by Facebook’s artificial
    intelligence group. It is one of the most elegant practical tools for developing
    deep learning applications at present. In this book, we aim to familiarize you
    with PyTorch and similar programming paradigms alongside the relevant mathematics.
    Knowledge of Python basics will be assumed. You are strongly encouraged to try
    out all the code snippets in this book (after installing the appropriate packages
    like PyTorch, that is).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch是由Facebook的人工智能团队开发的开源机器学习库。它是目前开发深度学习应用中最优雅的实用工具之一。在本书中，我们旨在使你熟悉PyTorch以及类似的编程范式，同时介绍相关的数学知识。假设你具备Python基础知识。强烈建议你尝试本书中的所有代码片段（在安装了适当的包，如PyTorch之后）。
- en: All the Python code in this book is produced via Jupyter Notebook. A summary
    of the theoretical material presented in the code is provided before the code
    snippet.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的所有Python代码都是通过Jupyter Notebook生成的。在代码片段之前提供了所展示的理论材料的摘要。
- en: 2.2.1 PyTorch code for the introduction to vectors
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1 使用PyTorch的向量介绍代码
- en: Listing 2.1 shows how to create and access vectors and subvectors and slice
    and dice vectors using PyTorch.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.1展示了如何使用PyTorch创建和访问向量和子向量，以及如何切片和切块向量。
- en: NOTE Fully functional code demonstrating how to create a vector and access its
    elements, executable via Jupyter Notebook, can be found at [http://mng.bz/xm8q](http://mng.bz/xm8q).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：一个完全功能的代码示例，展示如何创建向量及其元素访问，可通过Jupyter Notebook执行，可以在[http://mng.bz/xm8q](http://mng.bz/xm8q)找到。
- en: Listing 2.1 Introduction to vectors via PyTorch
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.1 通过PyTorch介绍向量
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① torch.tensor represents a multidimensional array. The vector is a 1D tensor
    that can be initialized by directly specifying values.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ① `torch.tensor` 表示一个多维数组。向量是一个一维张量，可以通过直接指定值来初始化。
- en: ② Tensor elements are floats by default. We can force tensors to be other types
    such as float64 (double).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ② 默认情况下，张量元素是浮点数。我们可以强制张量成为其他类型，如float64（双精度）。
- en: ③ The square bracket operator lets us access individual vector elements.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 方括号运算符让我们能够访问单个向量元素。
- en: ④ Negative indices count from the end of the array. –1 denotes the last element.
    -2 denotes the second-to-last element.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 负索引从数组的末尾开始计数。-1表示最后一个元素。-2表示倒数第二个元素。
- en: ⑤ The colon operator slices off a range of elements from the vector.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 冒号运算符从向量中切片掉一系列元素。
- en: ⑥ Nothing before a colon denotes the beginning of~the~array. Nothing after a
    colon denotes the end~of~the array.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 冒号之前的内容表示数组的开始。冒号之后的内容表示数组的结束。
- en: ⑦ Torch tensors can be initialized from NumPy arrays.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 火炬张量可以从NumPy数组初始化。
- en: ⑧ The difference between the Torch tensor and its NumPy version is zero.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 火炬张量与其NumPy版本之间的差异为零。
- en: ⑨ Torch tensors can be converted to NumPy arrays.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 火炬张量可以转换为NumPy数组。
- en: 2.3 Matrices and their role in machine learning
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 矩阵及其在机器学习中的作用
- en: Sometimes it is not sufficient to group a set of numbers into a vector. We have
    to collect several vectors into another group. For instance, consider the input
    to training a machine learning model. Here we have several input instances, each
    consisting of a sequence of numbers. As seen in section [2.1](02.xhtml#sec-vectors),
    the sequence of numbers belonging to a single input instance can be grouped into
    a vector. How do we represent the entire collection of input instances? This is
    where the concept of matrices comes in handy from the world of mathematics. A
    *matrix* can be viewed as a rectangular array of numbers arranged in a fixed count
    of rows and columns. Each row of a matrix is a vector, and so is each column.
    Thus a matrix can be thought of as a collection of row vectors. It can also be
    viewed as a collection of column vectors. We can represent the entire set of numbers
    that constitute the training input to a machine learning model as a matrix, with
    each row vector corresponding to a single training instance.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，将一组数字分组成一个向量是不够的。我们必须将几个向量收集到另一个组中。例如，考虑训练机器学习模型的输入。在这里，我们有几个输入实例，每个实例由一系列数字组成。如第
    [2.1](02.xhtml#sec-vectors) 节所述，单个输入实例的数字序列可以分组成一个向量。我们如何表示整个输入实例集合？这就是数学世界中矩阵概念派上用场的地方。一个
    *矩阵* 可以看作是按固定行数和列数排列的数字的矩形数组。矩阵的每一行是一个向量，每一列也是一个向量。因此，矩阵可以被视为行向量的集合。它也可以被视为列向量的集合。我们可以将构成机器学习模型训练输入的整个数字集合表示为一个矩阵，其中每一行向量对应一个单独的训练实例。
- en: Consider our familiar cat-brain problem again. As stated earlier, a single input
    instance to the machine is a vector ![](../../OEBPS/Images/eq_02-00-f2.png), where
    *x*[0] describes the hardness of the object in front of the cat. Now consider
    a training dataset with many such input instances, each with a known output threat
    score. You might recall from section [1.1](../Text/01.xhtml#sec-paradigm-shift)
    that the goal in machine learning is to create a function that maps these inputs
    to their respective outputs with as little overall error as possible. Our training
    data may look as shown in table [2.2](02.xhtml#tab-cat-brain-training-data) (note
    that in real-life problems, the training dataset is usually large—often millions
    of input-output pairs—but in this toy problem, we will have 8 training data instances).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 再次考虑我们熟悉的猫脑问题。如前所述，机器的单个输入实例是一个向量 ![图片](../../OEBPS/Images/eq_02-00-f2.png)，其中
    *x*[0] 描述了猫面前物体的硬度。现在考虑一个包含许多此类输入实例的训练数据集，每个实例都有一个已知的输出威胁分数。你可能还记得第 [1.1](../Text/01.xhtml#sec-paradigm-shift)
    节中提到的，机器学习的目标是创建一个函数，以尽可能少的总体误差将这些输入映射到相应的输出。我们的训练数据可能看起来像表 [2.2](02.xhtml#tab-cat-brain-training-data)
    所示（注意，在实际问题中，训练数据集通常很大——经常是数百万个输入输出对，但在这个玩具问题中，我们将有 8 个训练数据实例）。
- en: Table 2.2 Example training dataset for our toy machine learning–based cat brain
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.2 我们玩具机器学习猫脑的示例训练数据集
- en: '|  | Input value: Hardness | Input value: Sharpness | Output: Threat score
    |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | 输入值：硬度 | 输入值：锋利度 | 输出：威胁分数 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 0 | 0.11 | 0.09 | −0.8 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.11 | 0.09 | −0.8 |'
- en: '| 1 | 0.01 | 0.02 | −0.97 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.01 | 0.02 | −0.97 |'
- en: '| 2 | 0.98 | 0.91 | 0.89 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.98 | 0.91 | 0.89 |'
- en: '| 3 | 0.12 | 0.21 | −0.68 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.12 | 0.21 | −0.68 |'
- en: '| 4 | 0.98 | 0.99 | 0.95 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.98 | 0.99 | 0.95 |'
- en: '| 5 | 0.85 | 0.87 | 0.74 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.85 | 0.87 | 0.74 |'
- en: '| 6 | 0.03 | 0.14 | −0.88 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 0.03 | 0.14 | −0.88 |'
- en: '| 7 | 0.55 | 0.45 | 0.00 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 0.55 | 0.45 | 0.00 |'
- en: From table [2.2](02.xhtml#tab-cat-brain-training-data), we can collect the columns
    corresponding to hardness and sharpness into a matrix, as shown in equation [2.1](02.xhtml#eq-cat-brain-toy-training-dataset)—this
    is a compact representation of the training dataset for this problem. [²](02.xhtml#fn5)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 从表 [2.2](02.xhtml#tab-cat-brain-training-data) 中，我们可以将对应硬度和锋利的列收集到一个矩阵中，如图 [2.1](02.xhtml#eq-cat-brain-toy-training-dataset)
    所示——这是该问题的训练数据集的紧凑表示。[²](02.xhtml#fn5)
- en: '![](../../OEBPS/Images/eq_02-01.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_02-01.png)'
- en: Equation 2.1
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2.1
- en: Each row of matrix *X* is a particular input instance. Different rows represent
    different input instances. On the other hand, different columns represent different
    feature elements. For example, the 0th row of matrix *X* is the vector [*x*[00]    *x*[01]]
    representing the 0th input instance. Its elements, *x*[00] and *x*[01] represent
    different feature elements, hardness and sharpness respectively of the 0th training
    input instance.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 *X* 的每一行是特定的输入实例。不同的行代表不同的输入实例。另一方面，不同的列代表不同的特征元素。例如，矩阵 *X* 的第0行是向量 [*x*[00]    *x*[01]]，代表第0个输入实例。其元素
    *x*[00] 和 *x*[01] 代表不同的特征元素，分别是第0个训练输入实例的硬度和锐度。
- en: 2.3.1 Matrix representation of digital images
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 数字图像的矩阵表示
- en: 'Digital images are also often represented as matrices. Here, each element represents
    the brightness at a specific pixel position (*x*, *y* coordinate) of the image.
    Typically, the brightness value is normalized to an integer in the range 0 to
    255. 0 is black, 255 is white, and 128 is gray.[³](02.xhtml#fn6) Following is
    an example of a tiny image, 9 pixels wide and 4 pixels high:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 数字图像也经常表示为矩阵。在这里，每个元素代表图像在特定像素位置（*x*，*y*坐标）的亮度。通常，亮度值被归一化到0到255的整数范围内。0是黑色，255是白色，128是灰色。[³](02.xhtml#fn6)以下是一个小图像的例子，宽度为9像素，高度为4像素：
- en: '![](../../OEBPS/Images/eq_02-02.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-02.png)'
- en: Equation 2.2
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 公式2.2
- en: The brightness increases gradually from left to right and also from top to bottom.
    *I*[00] represents the top-left pixel, which is black. *I*[3, 8] represents the
    bottom-right pixel, which is white. The intermediate pixels are various shades
    of gray between black and white. The actual image is shown in figure [2.2](02.xhtml#fig-tiny_im).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 亮度从左到右和从上到下逐渐增加。*I*[00] 代表左上角的像素，是黑色。*I*[3, 8] 代表右下角的像素，是白色。中间的像素是介于黑白之间的各种灰度。实际图像显示在图[2.2](02.xhtml#fig-tiny_im)中。
- en: '![](../../OEBPS/Images/CH02_F03_Chaudhury.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH02_F03_Chaudhury.png)'
- en: Figure 2.3 Image corresponding to matrix *I*[4, 9] in equation [2.2](02.xhtml#eq-tiny_im)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 公式[2.2](02.xhtml#eq-tiny_im)中矩阵 *I*[4, 9] 对应的图像
- en: '2.4 Python code: Introducing matrices, tensors, and images via PyTorch'
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 使用PyTorch介绍矩阵、张量和图像的Python代码
- en: For programming purposes, you can think of tensors as multidimensional arrays.
    Scalars are zero-dimensional tensors. Vectors are one-dimensional tensors. Matrices
    are two-dimensional tensors. RGB images are three-dimensional tensors (*colorchannels*
    × *height* × *width*). A batch of 64 images is a four-dimensional tensor (64 ×
    *colorchannels* × *height* × *width*).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了编程目的，你可以将张量视为多维数组。标量是零维张量。向量是一维张量。矩阵是二维张量。RGB图像是三维张量（*颜色通道* × *高度* × *宽度*）。64张图像的一批是四维张量（64
    × *颜色通道* × *高度* × *宽度*）。
- en: Listing 2.2 Introducing matrices via PyTorch
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.2 使用PyTorch介绍矩阵
- en: '[PRE1]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '① A matrix is a 2D array of numbers: i.e., a 2D tensor. The entire training
    data input set for a machine-learning model can be viewed as a matrix. Each input
    instance is one row. Row count ≡ number of training examples, column count ≡ training
    instance size'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ① 矩阵是一个数字的二维数组：即，一个二维张量。一个机器学习模型的整个训练数据输入集可以被视为一个矩阵。每个输入实例是一行。行数 ≡ 训练示例数，列数
    ≡ 训练实例大小
- en: '② Cat-brain training data input: 8 examples, each with two values (hardness,
    sharpness). An 8 × 2 tensor is created by specifying values.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ② 猫脑训练数据输入：8个示例，每个示例有两个值（硬度，锐度）。通过指定值创建了一个8 × 2的张量。
- en: ③ The shape of a tensor is a list. For a matrix, the first list element is num
    rows; the second list element is num columns.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 张量的形状是一个列表。对于一个矩阵，第一个列表元素是行数；第二个列表元素是列数。
- en: ④ Square brackets extract individual matrix elements.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 方括号提取单个矩阵元素。
- en: ⑤ A standalone colon operator denotes all possible indices.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 独立的冒号运算符表示所有可能的索引。
- en: ⑥ The colon operator denotes the range of indices.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 冒号运算符表示索引的范围。
- en: ⑦ 0th column
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 第0列
- en: ⑧ 1st column
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 第1列
- en: Listing 2.3 Slicing and dicing matrices
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.3 矩阵的切片和切块
- en: '[PRE2]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ① Ranges of rows and columns can be specified via the colon operator to slice
    off (extract) submatrices.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ① 可以通过冒号运算符指定行和列的范围来切片（提取）子矩阵。
- en: ② Extracts the first three training examples (rows)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ② 提取前三个训练示例（行）
- en: ③ Extracts the sharpness feature for the 5th to 7th training examples
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 提取第5到第7个训练示例的锐度特征
- en: Listing 2.4 Tensors and images in PyTorch
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.4 PyTorch中的张量和图像
- en: '[PRE3]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① PyTorch tensors can be used to represent tensors. A vector is a 1-tensor,
    a matrix is a 2-tensor, and a scalar is a 0-tensor.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ① PyTorch 张量可以用来表示张量。一个向量是一个 1-张量，一个矩阵是一个 2-张量，一个标量是一个 0-张量。
- en: ② Creates a random tensor of specified dimensions
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ② 创建一个指定维度的随机张量
- en: ③ All images are tensors. An RGB image of height H, width W is a 3-tensor of
    shape [3, H, W].
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 所有图像都是张量。高度为 H、宽度为 W 的 RGB 图像是一个形状为 [3, H, W] 的 3-张量。
- en: ④ 4 × 9 single-channel image shown in figure [2.3](02.xhtml#fig-tiny_im)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 图 [2.3](02.xhtml#fig-tiny_im) 中显示的 4 × 9 单通道图像
- en: ⑤ Reads a 199 × 256 × 3 image from disk
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 从磁盘读取 199 × 256 × 3 的图像
- en: ⑥ Usual slicing dicing operators work. Extracts the red, green, and blue channels
    of the image as shown in figure [2.4](02.xhtml#fig-numpy-dog-grid).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 常规的切片切割操作有效。提取图像中如图 [2.4](02.xhtml#fig-numpy-dog-grid) 所示的红、绿、蓝通道。
- en: ⑦ Crops out a 100 × 100 subimage as shown in figure [2.5](02.xhtml#fig-numpy-crop-dog)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 裁剪出如图 [2.5](02.xhtml#fig-numpy-crop-dog) 所示的 100 × 100 子图像。
- en: '![](../../OEBPS/Images/CH02_F04_Chaudhury.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH02_F04_Chaudhury.png)'
- en: Figure 2.4 Tensors and images in PyTorch
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4 PyTorch 中的张量和图像
- en: '![](../../OEBPS/Images/CH02_F05_Chaudhury.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH02_F05_Chaudhury.png)'
- en: Figure 2.5 Cropped image of dog
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5 狗的裁剪图像
- en: 2.5 Basic vector and matrix operations in machine learning
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 机器学习中的基本向量和矩阵运算
- en: In this section, we introduce several basic vector and matrix operations along
    with examples to demonstrate their significance in image processing, computer
    vision, and machine learning. It is meant to be an application-centric introduction
    to linear algebra. But it is *not* meant to be a comprehensive review of matrix
    and vector operations, for which you are referred to a textbook on linear algebra.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了几个基本的向量和矩阵运算及其在图像处理、计算机视觉和机器学习中的重要性，并通过示例进行演示。本节旨在提供一个以应用为中心的线性代数入门。但并非旨在对矩阵和向量运算进行全面回顾，对于这些内容，请参考线性代数教科书。
- en: '![](../../OEBPS/Images/CH02_F06_Chaudhury.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH02_F06_Chaudhury.png)'
- en: Figure 2.6 Image corresponding to the transpose of matrix *I*[4, 9] shown in
    equation [2.3](02.xhtml#eq-tiny_im_transpose). This is equivalent to rotating
    the image by 90°.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6 方程 [2.3](02.xhtml#eq-tiny_im_transpose) 中显示的矩阵 *I*[4, 9] 的转置对应的图像。这相当于将图像旋转
    90°。
- en: 2.5.1 Matrix and vector transpose
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.1 矩阵和向量转置
- en: 'In equation [2.2](02.xhtml#eq-tiny_im), we encountered the matrix *I*[4, 9]
    depicting a tiny image. Suppose we want to rotate the image by 90° so it looks
    like figure [2.5](02.xhtml#fig-tiny_im-transpose). The original matrix *I*[4,
    9] and its transpose *I*[4,]*^T*[9] = *I*[9, 4] are shown here:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程 [2.2](02.xhtml#eq-tiny_im) 中，我们遇到了表示微小图像的矩阵 *I*[4, 9]。假设我们想要将图像旋转 90°，使其看起来像图
    [2.5](02.xhtml#fig-tiny_im-transpose)。原始矩阵 *I*[4, 9] 和其转置 *I*[4,]*^T*[9] = *I*[9,
    4] 如下所示：
- en: '![](../../OEBPS/Images/eq_02-03.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_02-03.png)'
- en: Equation 2.3
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2.3
- en: By comparing equation [2.2](02.xhtml#eq-tiny_im) and equation [2.3](02.xhtml#eq-tiny_im_transpose),
    you can easily see that one can be obtained from the other by interchanging the
    row and column indices. This operation is generally known as *matrix transposition*.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较方程 [2.2](02.xhtml#eq-tiny_im) 和方程 [2.3](02.xhtml#eq-tiny_im_transpose)，你可以很容易地看出，一个可以通过交换行和列索引从另一个获得。这种操作通常被称为
    *矩阵转置*。
- en: Formally, the transpose of a matrix *A[m, n]* with *m* rows and *n* columns
    is another matrix with *n* rows and *m* columns. This transposed matrix, denoted
    *A[n,]^T[m]*, is such that *A^T*[*i*, *j*] = *A*[*j*, *i*]. For instance, the
    value at row 0 column 6 in matrix *I*[4, 9] is 48; in the transposed matrix, the
    same value appears in row 6 and column 0. In matrix parlance, *I*[4, 9][0,6] =
    *I*[9,]*^T*[4][6,0] = 48.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，一个具有 *m* 行和 *n* 列的矩阵 *A[m, n]* 的转置是一个具有 *n* 行和 *m* 列的另一个矩阵。这个转置矩阵，表示为 *A[n,]^T[m]*，满足
    *A^T*[*i*, *j*] = *A*[*j*, *i*]。例如，矩阵 *I*[4, 9] 中第 0 行第 6 列的值是 48；在转置矩阵中，相同的值出现在第
    6 行和第 0 列。在矩阵术语中，*I*[4, 9][0,6] = *I*[9,]*^T*[4][6,0] = 48。
- en: 'Vector transposition is a special case of matrix transposition (since all vectors
    are matrices—a column vector with *n* elements is an *n* × 1 matrix). For instance,
    an arbitrary vector and its transpose are shown next:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 向量转置是矩阵转置的特殊情况（因为所有向量都是矩阵——一个具有 *n* 个元素的列向量是一个 *n* × 1 的矩阵）。例如，一个任意向量及其转置如下所示：
- en: '![](../../OEBPS/Images/eq_02-04.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_02-04.png)'
- en: Equation 2.4
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2.4
- en: '![](../../OEBPS/Images/eq_02-05.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_02-05.png)'
- en: Equation 2.5
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2.5
- en: 2.5.2 Dot product of two vectors and its role in machine learning
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.2 两个向量的点积及其在机器学习中的作用
- en: In section [1.3](../Text/01.xhtml#sec-cat_brain), we saw the simplest of machine
    learning models where the output is generated by taking a weighted sum of the
    inputs (and then adding a constant bias value). This model/machine is characterized
    by the weights *w*[0], *w*[1], and bias *b*. Take the rows of table [2.2](02.xhtml#tab-cat-brain-training-data).
    For example, for row 0, the input values are the hardness of the approaching object
    = 0.11 and softness = 0.09. The corresponding model output will be *y* = *w*[0]
    × 0.11 + *w*[1] × 0.09 + *b*. In fact, the goal of training is to choose *w*[0],
    *w*[1], and *b* such that model outputs are as close as possible to the known
    outputs; that is, *y* = *w*[0] × 0.11 + *w*[1] × 0.09 + *b* should be as close
    to −0.8 as possible, *y* = *w*[0] × 0.01 + *w*[1] × 0.02 + *b* should be as close
    to −0.97 as possible, that is,  in general, given an input instance ![](../../OEBPS/Images/eq_02-05-a.png),
    the model output is *y* = *x*[0]*w*[0] + *x*[1]*w*[1] + *b*.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在[1.3](../Text/01.xhtml#sec-cat_brain)节中，我们看到了最简单的机器学习模型，其中输出是通过取输入的加权总和（然后添加一个常数偏置值）来生成的。这个模型/机器的特征是权重
    *w*[0]、*w*[1] 和偏置 *b*。以表[2.2](02.xhtml#tab-cat-brain-training-data)的行为例。例如，对于第0行，输入值是接近物体的硬度=0.11和柔软度=0.09。相应的模型输出将是
    *y* = *w*[0] × 0.11 + *w*[1] × 0.09 + *b*。实际上，训练的目标是选择 *w*[0]、*w*[1] 和 *b*，使得模型输出尽可能接近已知的输出；也就是说，*y*
    = *w*[0] × 0.11 + *w*[1] × 0.09 + *b* 应尽可能接近 -0.8，*y* = *w*[0] × 0.01 + *w*[1]
    × 0.02 + *b* 应尽可能接近 -0.97，也就是说，一般来说，给定一个输入实例 ![](../../OEBPS/Images/eq_02-05-a.png)，模型输出是
    *y* = *x*[0]*w*[0] + *x*[1]*w*[1] + *b*。
- en: 'We will keep returning to this model throughout the chapter. But first, let’s
    consider a different question. In this toy example, we have only three model parameters:
    two weights, *w*[0], *w*[1], and one bias *b*. Hence it is not very messy to write
    the model output flat out as *y* = *x*[0]*w*[0] + *x*[1]*w*[1] + *b*. But, with
    longer feature vectors (that is, more weights) it will become unwieldy. Is there
    a compact way to represent the model output for a specific input instance, irrespective
    of the size of the input?'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中反复回到这个模型。但首先，让我们考虑一个不同的问题。在这个玩具示例中，我们只有三个模型参数：两个权重，*w*[0] 和 *w*[1]，以及一个偏置
    *b*。因此，将模型输出直接写成 *y* = *x*[0]*w*[0] + *x*[1]*w*[1] + *b* 并不是很混乱。但是，对于更长的特征向量（即更多的权重），它将变得难以处理。对于特定的输入实例，有没有一种紧凑的方式来表示模型输出，而不论输入的大小如何？
- en: Turns out the answer is yes—we can use an operation called *dot product* from
    the world of mathematics. We have already seen in section [2.1](02.xhtml#sec-vectors)
    that an individual instance of model input can be compactly represented by a vector,
    say ![](../../OEBPS/Images/AR_x.png) (it can have any number of input values).
    We can also represent the set of weights as vector ![](../../OEBPS/Images/AR_w.png)—it
    will have the same number of items as the input vector. The dot product is simply
    the element-wise multiplication of the two vectors ![](../../OEBPS/Images/AR_x.png)
    and ![](../../OEBPS/Images/AR_w.png). Formally, given two vectors and ![](../../OEBPS/Images/eq_02-05-b2.png)
    and ![](../../OEBPS/Images/eq_02-05-c2.png), the dot product of the two vectors
    is defined as
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是肯定的——我们可以使用来自数学世界的称为“点积”的操作。我们已经在[2.1](02.xhtml#sec-vectors)节中看到，模型输入的单个实例可以简洁地表示为一个向量，例如
    ![](../../OEBPS/Images/AR_x.png)（它可以有任意数量的输入值）。我们也可以将权重集合表示为向量 ![](../../OEBPS/Images/AR_w.png)——它将具有与输入向量相同数量的项。点积是两个向量
    ![](../../OEBPS/Images/AR_x.png) 和 ![](../../OEBPS/Images/AR_w.png) 的逐元素乘积。形式上，给定两个向量
    ![](../../OEBPS/Images/eq_02-05-b2.png) 和 ![](../../OEBPS/Images/eq_02-05-c2.png)，两个向量的点积定义为
- en: '![](../../OEBPS/Images/eq_02-06.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-06.png)'
- en: Equation 2.6
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式2.6
- en: In other words, the sum of the products of corresponding elements of the two
    vectors is the dot product of the two vectors, denoted ![](../../OEBPS/Images/AR_a.png)
    ⋅ ![](../../OEBPS/Images/AR_b.png).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，两个向量对应元素的乘积之和是两个向量的点积，表示为 ![](../../OEBPS/Images/AR_a.png) ⋅ ![](../../OEBPS/Images/AR_b.png)。
- en: NOTE The dot product notation can compactly represent the model output as *y*
    = ![](../../OEBPS/Images/AR_w.png) ⋅ ![](../../OEBPS/Images/AR_x.png) + *b*. The
    representation does not increase in size even when the number of inputs and weights
    is large.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：点积符号可以简洁地表示模型输出为 *y* = ![](../../OEBPS/Images/AR_w.png) ⋅ ![](../../OEBPS/Images/AR_x.png)
    + *b*。即使输入和权重的数量很大，这种表示法的大小也不会增加。
- en: Consider our (by now familiar) cat-brain example again. Suppose the weight vector
    is ![](../../OEBPS/Images/eq_02-06-a2.png) and the bias value *b* = 5. Then the
    model output for the 0th input instance from table [2.2](02.xhtml#tab-cat-brain-training-data)
    will be ![](../../OEBPS/Images/eq_02-06-b2.png). It is another matter that these
    are bad choices for weight and bias parameters, since the model output 5.51 is
    a far cry from the desired output −0.89. We will soon see how to obtain better
    parameter values. For now, we just need to note that the dot product offers a
    neat way to represent the simple weighted sum model output.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 再次考虑我们（现在已经很熟悉的）猫脑示例。假设权重向量是 ![](../../OEBPS/Images/eq_02-06-a2.png) 且偏置值 *b*
    = 5。那么从表 [2.2](02.xhtml#tab-cat-brain-training-data) 中 0 号输入实例的模型输出将是 ![](../../OEBPS/Images/eq_02-06-b2.png)。这些权重和偏置参数的选择并不理想，因为模型输出
    5.51 与期望的输出 -0.89 相去甚远。我们很快就会看到如何获得更好的参数值。现在，我们只需注意点积提供了一种简洁的方式来表示简单的加权求和模型输出。
- en: NOTE The dot product is defined only if the vectors have the same dimensions.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：点积仅在向量具有相同维度时才定义。
- en: Sometimes the dot product is also referred to as *inner product*, denoted ⟨![](../../OEBPS/Images/AR_a.png),
    ![](../../OEBPS/Images/AR_b.png)⟩. Strictly speaking, the phrase *inner product*
    is a bit more general; it applies to infinite-dimensional vectors as well. In
    this book, we will often use the terms interchangeably, sacrificing mathematical
    rigor for enhanced understanding.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 有时点积也被称为 *内积*，表示为 ⟨![](../../OEBPS/Images/AR_a.png), ![](../../OEBPS/Images/AR_b.png)⟩。严格来说，短语“内积”更为通用；它也适用于无限维向量。在这本书中，我们将经常互换使用这些术语，牺牲数学严谨性以增强理解。
- en: 2.5.3 Matrix multiplication and machine learning
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.3 矩阵乘法和机器学习
- en: Vectors are special cases of matrices. Hence, matrix-vector multiplication is
    a special case of matrix-matrix multiplication. We will start with that.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 向量是矩阵的特殊情况。因此，矩阵-向量乘法是矩阵-矩阵乘法的特殊情况。我们将从这里开始。
- en: Matrix-vector multiplication
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵-向量乘法
- en: In section [2.5.2](02.xhtml#subsec-dotprod-ml), we saw that given a weight vector,
    say ![](../../OEBPS/Images/eq_02-06-c2.png), and the bias value *b* = 5, the weighted
    sum model output upon a single input instance, say ![](../../OEBPS/Images/eq_02-06-d2.png),
    can be represented using a vector-vector dot product ![](../../OEBPS/Images/eq_02-06-e2.png).
    As depicted in equation [2.1](02.xhtml#eq-cat-brain-toy-training-dataset), during
    training, we are dealing with many training data instances at the same time. In
    real life, we typically deal with hundreds of thousands of input instances, each
    having hundreds of values. Is there a way to represent the model output for the
    entire training dataset compactly, such that it is independent of the count of
    input instances and their sizes?
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [2.5.2](02.xhtml#subsec-dotprod-ml) 节中，我们看到了给定一个权重向量，比如 ![](../../OEBPS/Images/eq_02-06-c2.png)，以及偏置值
    *b* = 5，单个输入实例上的加权求和模型输出可以通过向量-向量点积 ![](../../OEBPS/Images/eq_02-06-e2.png) 表示。如方程
    [2.1](02.xhtml#eq-cat-brain-toy-training-dataset) 所示，在训练过程中，我们同时处理许多训练数据实例。在现实生活中，我们通常处理成千上万的输入实例，每个实例有数百个值。有没有一种方法可以紧凑地表示整个训练数据集的模型输出，使其独立于输入实例的数量和大小？
- en: 'The answer turns out to be yes. We can use the idea of matrix-vector multiplication
    from the world of mathematics. The product of a matrix *X* and column vector ![](../../OEBPS/Images/AR_w.png)
    is another vector, denoted *X*![](../../OEBPS/Images/AR_w.png). Its elements are
    the dot products between the row vectors of *X* and the column vector ![](../../OEBPS/Images/AR_w.png).
    For example, given the model weight vector ![](../../OEBPS/Images/eq_02-06-f2.png)
    and the bias value *b* = 5, the outputs on the toy training dataset of our familiar
    cat-brain model (equation [2.1](02.xhtml#eq-cat-brain-toy-training-dataset)) can
    be obtained via the following steps:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是肯定的。我们可以从数学世界的矩阵-向量乘法中借用这个想法。矩阵 *X* 和列向量 ![](../../OEBPS/Images/AR_w.png)
    的乘积是另一个向量，表示为 *X*![](../../OEBPS/Images/AR_w.png)。其元素是 *X* 的行向量与列向量 ![](../../OEBPS/Images/AR_w.png)
    之间的点积。例如，给定模型权重向量 ![](../../OEBPS/Images/eq_02-06-f2.png) 和偏置值 *b* = 5，我们熟悉的猫脑模型（方程
    [2.1](02.xhtml#eq-cat-brain-toy-training-dataset)）在玩具训练数据集上的输出可以通过以下步骤获得：
- en: '![](../../OEBPS/Images/eq_02-07.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_02-07.png)'
- en: Equation 2.7
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2.7
- en: Adding the bias value of 5, the model output on the toy training dataset is
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 添加偏置值 5，玩具训练数据集上的模型输出为
- en: '![](../../OEBPS/Images/eq_02-08.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_02-08.png)'
- en: Equation 2.8
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2.8
- en: In general, the output of our simple model (biased weighted sum of input elements)
    can be expressed compactly as ![](../../OEBPS/Images/AR_y.png) = *X*![](../../OEBPS/Images/AR_w.png)
    + ![](../../OEBPS/Images/AR_b.png).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们简单模型的输出（输入元素的偏置加权求和）可以紧凑地表示为![](../../OEBPS/Images/AR_y.png) = *X*![](../../OEBPS/Images/AR_w.png)
    + ![](../../OEBPS/Images/AR_b.png)。
- en: Matrix-matrix multiplication
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵-矩阵乘法
- en: 'Generalizing the notion of matrix times vector, we can define matrix times
    matrix. A matrix with *m* rows and *p* columns, say *A[m, p]*, can be multiplied
    with another matrix with *p* rows and *n* columns, say *B[p, n]*, to generate
    a matrix with *m* rows and *n* columns, say *C[m, n]*: for example, *C[m, n]*
    = *A[m, p]* *B[p, n]*. Note that the number of columns in the left matrix must
    match the number of rows in the right matrix. Element *i, j* of the result matrix,
    *C[i, j]*, is obtained by point-wise multiplication of the elements of the *i*th
    row vector of *A* and the *j*th column vector of *B*. The following example illustrates
    the idea:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 将矩阵乘以向量的概念推广后，我们可以定义矩阵乘以矩阵。一个有*m*行和*p*列的矩阵，例如*A[m, p]*，可以与另一个有*p*行和*n*列的矩阵相乘，例如*B[p,
    n]*，生成一个有*m*行和*n*列的矩阵，例如*C[m, n]*：例如，*C[m, n]* = *A[m, p]* *B[p, n]*。注意，左矩阵的列数必须与右矩阵的行数相匹配。结果矩阵的元素*i,
    j*，即*C[i, j]*，是通过*A*的第*i*行向量的元素与*B*的第*j*列向量的元素逐点相乘得到的。以下示例说明了这个概念：
- en: '![](../../OEBPS/Images/eq_02-08-a.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-08-a.png)'
- en: The computation for *C*[2, 1] is shown via bolding by way of example.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 通过加粗的方式举例说明了*C*[2, 1]的计算。
- en: NOTE Matrix multiplication is not commutative. In general, *AB* ≠ *BA*.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：矩阵乘法不是交换的。一般来说，*AB* ≠ *BA*。
- en: At this point, the astute reader may already have noted that the dot product
    is a special case of matrix multiplication. For instance, the dot product between
    two vectors ![](../../OEBPS/Images/eq_02-08-b2.png) and ![](../../OEBPS/Images/eq_02-08-c2.png)
    is equivalent to transposing either of the two vectors and then doing a matrix
    multiplication with the other. In other words,
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，敏锐的读者可能已经注意到了点积是矩阵乘法的一种特殊情况。例如，两个向量![](../../OEBPS/Images/eq_02-08-b2.png)和![](../../OEBPS/Images/eq_02-08-c2.png)的点积等价于将其中一个向量转置后，再与另一个向量进行矩阵乘法。换句话说，
- en: '![](../../OEBPS/Images/eq_02-08-d.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-08-d.png)'
- en: The idea works in higher dimensions, too. In general, given two vectors ![](../../OEBPS/Images/eq_02-08-e2.png)
    and ![](../../OEBPS/Images/eq_02-08-f2.png), the dot product of the two vectors
    is defined as
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法在更高维的情况下也适用。一般来说，给定两个向量![](../../OEBPS/Images/eq_02-08-e2.png)和![](../../OEBPS/Images/eq_02-08-f2.png)，这两个向量的点积定义为
- en: '![](../../OEBPS/Images/eq_02-09.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-09.png)'
- en: Equation 2.9
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 方程2.9
- en: Another special case of matrix multiplication is row-vector matrix multiplication.
    For example, ![](../../OEBPS/Images/AR_b.png)*^TA* = ![](../../OEBPS/Images/AR_c.png)
    or
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法的另一个特殊情况是行向量矩阵乘法。例如，![](../../OEBPS/Images/AR_b.png)*^TA* = ![](../../OEBPS/Images/AR_c.png)或者
- en: '![](../../OEBPS/Images/eq_02-09-a.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-09-a.png)'
- en: Transpose of matrix products
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘积的转置
- en: 'Given two matrices *A* and *B*, where the number of columns in *A* matches
    the number of rows in *B* (that is, it is possible to multiply them), the transpose
    of the product is the product of the individual transposes, in reversed order.
    The rule also applies to matrix-vector multiplication. The following equations
    capture this rule:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 给定两个矩阵*A*和*B*，其中*A*的列数与*B*的行数相匹配（也就是说，可以相乘），它们的乘积的转置是各自转置的乘积，顺序相反。这个规则也适用于矩阵-向量乘法。以下方程捕捉了这个规则：
- en: '![](../../OEBPS/Images/eq_02-10.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-10.png)'
- en: Equation 2.10
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 方程2.10
- en: '2.5.4 Length of a vector (L2 norm): Model error'
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.4 向量的长度（L2范数）：模型误差
- en: Imagine that a machine learning model is supposed to output a target value *ȳ*,
    but it outputs *y* instead. We are interested in the *error* made by the model.
    The error is the difference between the target and the actual outputs.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个机器学习模型应该输出一个目标值*ȳ*，但它输出*y*。我们感兴趣的是模型犯的*错误*。错误是目标值与实际输出之间的差异。
- en: Squared error
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 平方误差
- en: When a computing error occurs, we are only interested in how far the computed
    value is from ideal. We do not care whether the computed value is bigger or smaller
    than ideal. For instance, if the target (ideal) value is 2, the computed values
    1.5 and 2.5 are equally in error—we are equally happy or unhappy with either of
    them. Hence, it is common practice to *square* error values. Thus for instance,
    if the target value is 2 and the computed value is 1.5, the error is (1.5 − 2)²
    = 0.25. If the target value is 2.5, the error is (2.5 − 2)² = 0.25. The squaring
    operation essentially eliminates the sign of the error value. We can then follow
    it up with a square root, but it is OK not to.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 当发生计算错误时，我们只关心计算值与理想值之间的差距。我们并不关心计算值是大于还是小于理想值。例如，如果目标（理想）值是 2，那么计算值 1.5 和 2.5
    的误差是相同的——我们对这两个值都同样满意或不满意。因此，通常的做法是将误差值进行平方。例如，如果目标值是 2，计算值是 1.5，那么误差是 (1.5 −
    2)² = 0.25。如果目标值是 2.5，误差是 (2.5 − 2)² = 0.25。平方操作本质上消除了误差值的符号。然后我们可以跟一个平方根，但也可以不这样做。
- en: 'You might ask, “But wait: squaring alters the value of the quantity. Don’t
    we care about the exact value of the error?” The answer is, we usually don’t;
    we only care about *relative* values of errors. If the target is 2, we want the
    error for an output value of, say, 2.1 to be less than the error for an output
    value of 2.5; the exact values of the errors do not matter.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问，“但是等等：平方改变了数量的值。我们难道不在乎误差的精确值吗？” 答案是，我们通常不在乎；我们只关心误差的 *相对* 值。如果目标是 2，我们希望输出值为
    2.1 的误差小于输出值为 2.5 的误差；误差的精确值并不重要。
- en: Let’s apply this idea of squaring to machine learning model error. As seen earlier
    in section [2.5.3](02.xhtml#subsec-matmul-ml), given a model weight vector, say
    ![](../../OEBPS/Images/eq_02-10-a2.png), and the bias value *b* = 5, the weighted
    sum model output upon a single input instance, say ![](../../OEBPS/Images/eq_02-10-b2.png),
    is ![](../../OEBPS/Images/eq_02-10-c2.png). The corresponding target (ideal) output,
    from table [2.2](02.xhtml#tab-cat-brain-training-data), is −0.8. The squared error
    *e*² = (−0.8−5.51)² = 39.82 gives us an idea of how good or bad the model parameters
    3, 2, 5 are. For instance, if we instead use a weight vector ![](../../OEBPS/Images/eq_02-10-d2.png)
    and bias value −1, we get model output ![](../../OEBPS/Images/eq_02-10-e2.png).
    The output is exactly the same as the target. The corresponding squared error
    *e*² = (−0.8−(−0.8))² = 0. This (zero error) immediately tells us that 1, 1, −1
    are much better choices of model parameters than 3, 2, 5.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这种平方的思想应用到机器学习模型误差中。如前文第 [2.5.3](02.xhtml#subsec-matmul-ml) 节所述，给定一个模型权重向量，比如
    ![](../../OEBPS/Images/eq_02-10-a2.png)，以及偏差值 *b* = 5，单个输入实例的加权求和模型输出，比如 ![](../../OEBPS/Images/eq_02-10-b2.png)，是
    ![](../../OEBPS/Images/eq_02-10-c2.png)。相应的目标（理想）输出，来自表 [2.2](02.xhtml#tab-cat-brain-training-data)，是
    −0.8。平方误差 *e*² = (−0.8−5.51)² = 39.82 给出了模型参数 3, 2, 5 的好坏程度。例如，如果我们改用一个权重向量 ![](../../OEBPS/Images/eq_02-10-d2.png)
    和偏差值 −1，我们得到模型输出 ![](../../OEBPS/Images/eq_02-10-e2.png)。输出与目标完全相同。相应的平方误差 *e*²
    = (−0.8−(−0.8))² = 0。这个（零误差）立即告诉我们，1, 1, −1 作为模型参数的选择比 3, 2, 5 要好得多。
- en: In general, the error made by a biased weighted sum model can be expressed as
    follows. If ![](../../OEBPS/Images/AR_w.png) denotes the weight vector and ![](../../OEBPS/Images/AR_b.png)
    denotes the bias, the output corresponding to an input instance ![](../../OEBPS/Images/AR_x.png)
    can be expressed as *y* = ![](../../OEBPS/Images/AR_w.png) ⋅ ![](../../OEBPS/Images/AR_x.png)
    + *b*. Let *ȳ* denote the corresponding target (ground truth). Then the error
    is defined as *e* = (*y*−*ȳ*)².
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，一个有偏重的加权求和模型所犯的错误可以表示如下。如果用 ![](../../OEBPS/Images/AR_w.png) 表示权重向量，用
    ![](../../OEBPS/Images/AR_b.png) 表示偏差，则对应于输入实例 ![](../../OEBPS/Images/AR_x.png)
    的输出可以表示为 *y* = ![](../../OEBPS/Images/AR_w.png) ⋅ ![](../../OEBPS/Images/AR_x.png)
    + *b*。用 *ȳ* 表示相应的目标（真实值）。那么错误被定义为 *e* = (*y*−*ȳ*)²。
- en: 'Thus we see that we can compute the error on a single training instance by
    taking the difference between the model output and the ground truth and squaring
    it. How do we extend this concept over the entire training dataset? The set of
    outputs corresponding to the entire set of training inputs can be expressed as
    the output vector *y* = *X*![](../../OEBPS/Images/AR_w.png) + ![](../../OEBPS/Images/AR_b.png).
    The corresponding target output vector, consisting of the entire set of ground
    truths can be expressed as ![](../../OEBPS/Images/AR_y2.png). The differences
    between the target and model output over the entire training set can be expressed
    as another vector ![](../../OEBPS/Images/AR_y2.png) - ![](../../OEBPS/Images/AR_y.png).
    In our particular example:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们看到我们可以通过取模型输出和真实值之间的差异并对其进行平方来计算单个训练实例的误差。我们如何将这个概念扩展到整个训练数据集？对应于整个训练输入集的输出集可以表示为输出向量
    *y* = *X*![图片](../../OEBPS/Images/AR_w.png) + ![图片](../../OEBPS/Images/AR_b.png)。相应的目标输出向量，由整个真实值集组成，可以表示为![图片](../../OEBPS/Images/AR_y2.png)。整个训练集中目标和模型输出之间的差异可以表示为另一个向量![图片](../../OEBPS/Images/AR_y2.png)
    - ![图片](../../OEBPS/Images/AR_y.png)。在我们的特定例子中：
- en: '![](../../OEBPS/Images/eq_02-10-f.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_02-10-f.png)'
- en: 'Thus the total error over the entire training dataset is obtained by taking
    the difference between the output and the ground truth vector, squaring its elements
    and adding them up. Recalling equation [2.9](02.xhtml#eq-dotprod-matmul), this
    is exactly what will happen if we take the *dot product of the difference vector
    with itself*. That happens to be the definition of the *squared magnitude* or
    *length* or *L2 norm* of a vector: the dot product of the vector with itself.
    In the previous example, the overall training (squared) error is:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，整个训练数据集的总误差是通过取输出和真实向量之间的差异，平方其元素并将它们相加来获得的。回忆方程[2.9](02.xhtml#eq-dotprod-matmul)，这正是如果我们取差异向量的*点积*会发生的事情。这恰好是向量的*平方模*或*长度*或*L2范数*的定义：向量与自身的点积。在先前的例子中，整体训练（平方）误差是：
- en: '![](../../OEBPS/Images/eq_02-10-g.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_02-10-g.png)'
- en: Formally, the length of a vector ![](../../OEBPS/Images/eq_02-10-h2.png), denoted
    ||![](../../OEBPS/Images/AR_v.png)||, is defined as ![](../../OEBPS/Images/eq_02-10-i2.png).
    This quantity is sometimes called the L2 norm of the vector.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，向量![图片](../../OEBPS/Images/eq_02-10-h2.png)的长度，表示为||![图片](../../OEBPS/Images/AR_v.png)||，定义为![图片](../../OEBPS/Images/eq_02-10-i2.png)。这个量有时被称为向量的L2范数。
- en: In particular, given a machine learning model with output vector ![](../../OEBPS/Images/AR_y.png)
    and a target vector ![](../../OEBPS/Images/AR_y2.png), the error is the same as
    the magnitude or L2 norm of the difference vector
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，给定一个输出向量为![图片](../../OEBPS/Images/AR_y.png)的机器学习模型和一个目标向量为![图片](../../OEBPS/Images/AR_y2.png)的向量，错误等于差异向量的模或L2范数
- en: '![](../../OEBPS/Images/eq_02-10-j.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_02-10-j.png)'
- en: 2.5.5 Geometric intuitions for vector length
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.5 向量长度的几何直觉
- en: For a 2D vector ![](../../OEBPS/Images/eq_02-10-k2.png), as seen in figure [2.2](02.xhtml#fig-vector_diagram),
    the L2 norm ![](../../OEBPS/Images/eq_02-10-l2.png) is nothing but the hypotenuse
    of the right-angled triangle whose sides are elements of the vector. The same
    intuition holds in higher dimensions.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个二维向量![图片](../../OEBPS/Images/eq_02-10-k2.png)，如图[2.2](02.xhtml#fig-vector_diagram)所示，L2范数![图片](../../OEBPS/Images/eq_02-10-l2.png)不过是直角三角形的斜边，其边是向量的元素。在更高维的情况下，这种直觉同样适用。
- en: A *unit vector* is a vector whose length is 1\. Given any vector ![](../../OEBPS/Images/AR_v.png),
    the corresponding unit vector can be obtained by dividing every element by the
    length of that vector. For example, given ![](../../OEBPS/Images/eq_02-10-m2.png),
    length ![](../../OEBPS/Images/eq_02-10-n2.png) and the corresponding unit vector
    ![](../../OEBPS/Images/eq_02-10-o2.png). Unit vectors typically represent a direction.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*单位向量*是一个长度为1的向量。对于任何向量![图片](../../OEBPS/Images/AR_v.png)，相应的单位向量可以通过将该向量的每个元素除以该向量的长度来获得。例如，给定![图片](../../OEBPS/Images/eq_02-10-m2.png)，长度![图片](../../OEBPS/Images/eq_02-10-n2.png)和相应的单位向量![图片](../../OEBPS/Images/eq_02-10-o2.png)。单位向量通常表示一个方向。
- en: NOTE Unit vectors are conventionally depicted with the hat symbol as opposed
    to the little overhead arrow, as in *û^Tû* = 1.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：单位向量通常用帽子符号表示，而不是小箭头，如*û^Tû* = 1。
- en: In machine learning, the goal of training is often to minimize the length of
    the error vector (the difference between the model output vector and the target
    ground truth vector).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，训练的目标通常是使错误向量（模型输出向量与目标真实向量之间的差异）的长度最小化。
- en: '2.5.6 Geometric intuitions for the dot product: Feature similarity'
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.6 点积的几何直觉：特征相似度
- en: Consider the document retrieval problem depicted in table [2.1](02.xhtml#tab-doc-vec)
    one more time. We have a set of documents, each described by its own feature vector.
    Given a pair of such documents, we must find their similarity. This essentially
    boils down to estimating the similarity between two feature vectors. In this section,
    we will see that the dot product between a pair of vectors can be used as a measure
    of similarity between them.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 再次考虑表 [2.1](02.xhtml#tab-doc-vec) 中描述的文档检索问题。我们有一组文档，每个文档都由其自身的特征向量描述。给定这样一对文档，我们必须找到它们的相似度。这本质上归结为估计两个特征向量之间的相似度。在本节中，我们将看到一对向量的点积可以用作它们之间相似度的度量。
- en: For instance, consider the feature vectors corresponding to *d*[5] and *d*[6]
    in table [2.1](02.xhtml#tab-doc-vec). They are ![](../../OEBPS/Images/eq_02-10-p2.png)
    and ![](../../OEBPS/Images/eq_02-10-q2.png). The dot product between them is 1
    × 0 + 0 × 1 = 0. This is low and agrees with our intuition that there is no common
    word of interest between them, so the documents are very dissimilar. On the other
    hand, the dot product between feature vectors of *d*[3] and *d*[4] is ![](../../OEBPS/Images/eq_02-10-r2.png).
    This is high and agrees with our intuition that they have many commonalities in
    words of interest and are similar documents. Thus, we get the first glimpse of
    an important concept. Loosely speaking, *similar vectors have larger dot products,
    and dissimilar vectors have near-zero dot products.*
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑表 [2.1](02.xhtml#tab-doc-vec) 中对应于 *d*[5] 和 *d*[6] 的特征向量。它们是 ![](../../OEBPS/Images/eq_02-10-p2.png)
    和 ![](../../OEBPS/Images/eq_02-10-q2.png)。它们之间的点积是 1 × 0 + 0 × 1 = 0。这是很低的，与我们直觉认为它们之间没有共同的感兴趣词汇相符，因此文档非常不相似。另一方面，*d*[3]
    和 *d*[4] 的特征向量之间的点积是 ![](../../OEBPS/Images/eq_02-10-r2.png)。这是很高的，与我们直觉认为它们在感兴趣词汇上有许多共同之处，是相似文档相符。因此，我们得到了一个重要概念的初步认识。简单来说，*相似的向量有较大的点积，不相似的向量有接近零的点积*。
- en: We will keep revisiting this problem of estimating similarity between feature
    vectors and solve it with more and more finesse. As a first attempt, we will now
    study in greater detail how dot products measure similarities between vectors.
    First we will show that the component of a vector along another is yielded by
    the dot product. Using this, we will show that the “similarity/agreement” between
    a pair of vectors can be estimated using the dot product between them. In particular,
    we will see that if the vectors point in more or less the same direction, their
    dot products are higher than when the vectors are perpendicular to each other.
    If the vectors point in opposite directions, their dot product is negative.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将不断回顾这个估计特征向量之间相似度的问题，并使用越来越巧妙的方法来解决它。作为一个初步尝试，我们现在将更详细地研究点积如何衡量向量之间的相似度。首先，我们将证明一个向量在另一个向量上的分量是由点积产生的。利用这一点，我们将展示一对向量的“相似度/一致性”可以通过它们之间的点积来估计。特别是，我们将看到如果向量指向大致相同的方向，它们的点积将高于当向量相互垂直时的情况。如果向量指向相反的方向，它们的点积将是负数。
- en: Dot product measures the component of one vector along another
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 点积衡量一个向量在另一个向量上的分量
- en: 'Let’s examine a special case first: the component of a vector along a coordinate
    axis. This can be obtained by multiplying the length of the vector with the cosine
    of the angle between the vector and the relevant coordinate axis. As shown for
    2D in figure [2.7a](02.xhtml#ch2fig-vec_component), a vector ![](../../OEBPS/Images/AR_v.png)
    can be broken into two components along the *X* and *Y* axes as'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 首先考察一个特殊情况：向量在坐标轴上的分量。这可以通过将向量的长度与向量与相关坐标轴之间的角度的余弦值相乘来获得。如图 [2.7a](02.xhtml#ch2fig-vec_component)
    所示，一个向量 ![](../../OEBPS/Images/AR_v.png) 可以沿 *X* 和 *Y* 轴分解为两个分量，如下
- en: '![](../../OEBPS/Images/eq_02-10-s.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-10-s.png)'
- en: 'Note how the length of the vector is preserved:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 注意向量长度的保持：
- en: '![](../../OEBPS/Images/eq_02-10-t.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-10-t.png)'
- en: '![](../../OEBPS/Images/CH02_F07a_Chaudhury.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH02_F07a_Chaudhury.png)'
- en: (a) Components of a 2D vector along coordinate axes. Note that ||![](../../OEBPS/Images/AR_a.png)||
    is the length of hypotenuse.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 2D向量在坐标轴上的分量。注意 ||![](../../OEBPS/Images/AR_a.png)|| 是斜边的长度。
- en: '![](../../OEBPS/Images/CH02_F07b_Chaudhury.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH02_F07b_Chaudhury.png)'
- en: (b) Dot product as a component of one vector along another ![](../../OEBPS/Images/AR_a.png)
    ⋅ ![](../../OEBPS/Images/AR_b.png) = ![](../../OEBPS/Images/AR_a.png)*^T*![](../../OEBPS/Images/AR_b.png)
    = *a[x]b[x]* + *a[y]b[y]* = ||![](../../OEBPS/Images/AR_a.png)|| ||![](../../OEBPS/Images/AR_b.png)||*cos*(*θ*).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 点积作为另一个向量上的一个向量的分量 ![](../../OEBPS/Images/AR_a.png) ⋅ ![](../../OEBPS/Images/AR_b.png)
    = ![](../../OEBPS/Images/AR_a.png)*^T*![](../../OEBPS/Images/AR_b.png) = *a[x]b[x]*
    + *a[y]b[y]* = ||![](../../OEBPS/Images/AR_a.png)|| ||![](../../OEBPS/Images/AR_b.png)||*cos*(*θ*)。
- en: Figure 2.7 Vector components and dot product
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7 向量分量和点积
- en: Now let’s study the more general case of the component of one vector in the
    direction of another arbitrary vector (figure [2.7b](02.xhtml#ch2fig-vec_component)).
    The component of a vector ![](../../OEBPS/Images/AR_a.png) along another vector
    ![](../../OEBPS/Images/AR_b.png) is ![](../../OEBPS/Images/AR_a.png) ⋅ ![](../../OEBPS/Images/AR_b.png)
    = ![](../../OEBPS/Images/AR_a.png)*^T*![](../../OEBPS/Images/AR_b.png). This is
    equivalent to ||![](../../OEBPS/Images/AR_a.png)|| ||![](../../OEBPS/Images/AR_b.png)||*cos*(*θ*),
    where *θ* is the angle between the vectors ![](../../OEBPS/Images/AR_a.png) and
    ![](../../OEBPS/Images/AR_b.png). (This has been proven for the two-dimension
    case discussed in section [A.1](../Text/A.xhtml#sec-dotprod-cosine-proof) of the
    appendix. You can read it if you would like deeper intuition.)
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来研究一个向量在另一个任意向量方向上的分量的一般情况（见图[2.7b](02.xhtml#ch2fig-vec_component)）。一个向量
    ![](../../OEBPS/Images/AR_a.png) 在另一个向量 ![](../../OEBPS/Images/AR_b.png) 上的分量是
    ![](../../OEBPS/Images/AR_a.png) ⋅ ![](../../OEBPS/Images/AR_b.png) = ![](../../OEBPS/Images/AR_a.png)*^T*![](../../OEBPS/Images/AR_b.png)。这相当于
    ||![](../../OEBPS/Images/AR_a.png)|| ||![](../../OEBPS/Images/AR_b.png)||*cos*(*θ*)，其中
    *θ* 是向量 ![](../../OEBPS/Images/AR_a.png) 和 ![](../../OEBPS/Images/AR_b.png) 之间的角度。（这已在附录中讨论的二维情况
    [A.1](../Text/A.xhtml#sec-dotprod-cosine-proof) 中得到证明。如果您想获得更深的直觉，可以阅读它。）
- en: Dot product measures the agreement between two vectors
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 点积衡量两个向量之间的协议
- en: The dot product can be expressed using the cosine of the angle between the vectors.
    Given two vectors ![](../../OEBPS/Images/AR_a.png) and ![](../../OEBPS/Images/AR_b.png),
    if *θ* is the angle between them, we have see figure [2.7b](02.xhtml#ch2fig-vec_component))
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 点积可以用向量之间角度的余弦来表示。给定两个向量 ![](../../OEBPS/Images/AR_a.png) 和 ![](../../OEBPS/Images/AR_b.png)，如果
    *θ* 是它们之间的角度，我们得到（见图[2.7b](02.xhtml#ch2fig-vec_component)）
- en: '![](../../OEBPS/Images/eq_02-11.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_02-11.png)'
- en: Equation 2.11
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 方程2.11
- en: Expressing the dot product using cosines makes it easier to see that it measures
    the *agreement* (aka *correlation*) between two vectors. If the vectors have the
    same direction, the angle between them is 0 and the cosine is 1, implying maximum
    agreement. The cosine becomes progressively smaller as the angle between the vectors
    increases, until the two vectors become perpendicular to each other and the cosine
    is zero, implying no correlation—the vectors are independent of each other. If
    the angle between them is 180°, the cosine is −1, implying that the vectors are
    anti-correlated. Thus, the dot product of two vectors is proportional to their
    directional agreement.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 使用余弦表达点积使其更容易看出它衡量的是两个向量之间的*协议*（也称为*相关性*）。如果向量具有相同的方向，它们之间的角度是0，余弦是1，这意味着最大协议。随着向量之间角度的增加，余弦值逐渐减小，直到两个向量相互垂直且余弦值为零，这意味着没有相关性——向量相互独立。如果它们之间的角度是180°，余弦值为-1，这意味着向量是反相关的。因此，两个向量的点积与它们的方向协议成正比。
- en: What role do the vector lengths play in all this? The dot product between two
    vectors is also proportional to the lengths of the vectors. This means agreement
    scores between bigger vectors are higher (an agreement between the US president
    and the German chancellor counts more than an agreement between you and me).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些中，向量长度扮演什么角色？两个向量的点积也正比于向量的长度。这意味着较大向量之间的同意度得分更高（美国总统和德国总理之间的协议比您和我之间的协议更重要）。
- en: 'If you want the agreement score to be neutral to the vector length, you can
    use a normalized dot product between unit-length vectors along the same directions:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想让协议得分对向量长度保持中性，您可以使用沿相同方向单位长度的向量的归一化点积：
- en: '![](../../OEBPS/Images/eq_02-11-a.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_02-11-a.png)'
- en: Dot product and the difference between two unit vectors
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 点积和两个单位向量之间的差异
- en: To obtain further insight into how the dot product indicates agreement or correlation
    between two directions, consider the two unit vectors ![](../../OEBPS/Images/eq_02-11-b2.png)
    and ![](../../OEBPS/Images/eq_02-11-c2.png). The difference between them is ![](../../OEBPS/Images/eq_02-11-d2.png).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入了解点积如何指示两个方向之间的共识或相关性，考虑两个单位向量 ![](../../OEBPS/Images/eq_02-11-b2.png) 和
    ![](../../OEBPS/Images/eq_02-11-c2.png)。它们之间的差异是 ![](../../OEBPS/Images/eq_02-11-d2.png)。
- en: Note that since they are unit vectors, ![](../../OEBPS/Images/eq_02-11-e2.png).
    The length of the difference vector
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于它们是单位向量，![](../../OEBPS/Images/eq_02-11-e2.png)。差异向量的长度
- en: '![](../../OEBPS/Images/eq_02-11-f.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-11-f.png)'
- en: 'From the last equality, it is evident that a larger dot product implies a smaller
    difference: that is, more agreement between the vectors.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 从最后一个等式可以看出，较大的点积意味着较小的差异：也就是说，向量之间有更多的共识。
- en: 2.6 Orthogonality of vectors and its physical significance
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.6 向量的正交性及其物理意义
- en: Try moving an object at right angles to the direction in which you are pushing
    it. You will find it impossible. The larger the angle, the less effective your
    force vector becomes (finally becoming totally ineffective at a 90° angle). This
    is why it is easy to walk on a horizontal surface (you are moving at right angles
    to the direction of gravitational pull, so the gravity vector is ineffective)
    but harder on an upward incline (the gravity vector is having some effect against
    you).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试将物体移动到与您推的方向垂直的方向。你会发现这是不可能的。角度越大，你的力向量变得越不有效（最终在 90° 角时完全无效）。这就是为什么在水平面上行走很容易（你是在与重力吸引方向垂直的方向上移动，所以重力向量无效），但在向上倾斜的表面上行走则更难（重力向量对你产生了一些反作用）。
- en: These physical notions are captured mathematically in the notion of a dot product.
    The dot product between two vectors ![](../../OEBPS/Images/AR_a.png) (say, the
    push vector) and ![](../../OEBPS/Images/AR_b.png) (say, the displacement of the
    pushed object vector) is ||![](../../OEBPS/Images/AR_a.png)|| ||![](../../OEBPS/Images/AR_b.png)||*cosθ*,
    where *θ* is the angle between the two vectors. When *θ* is 0 (the two vectors
    are aligned), *cosθ* = 1, the maximum possible value of *cosθ*, so push is maximally
    effective. As *θ* increases, *cosθ* decreases, and push becomes less and less
    effective. Finally, at *θ* = 90°, *cosθ* = 0, and push becomes completely ineffective.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这些物理概念在点积的概念中被数学化地捕捉。两个向量 ![](../../OEBPS/Images/AR_a.png)（例如，推力向量）和 ![](../../OEBPS/Images/AR_b.png)（例如，被推物体的位移向量）的点积是
    ||![](../../OEBPS/Images/AR_a.png)|| ||![](../../OEBPS/Images/AR_b.png)||*cosθ*，其中
    *θ* 是两个向量之间的角度。当 *θ* 为 0（两个向量对齐）时，*cosθ* = 1，是 *cosθ* 的最大可能值，因此推力是最有效的。随着 *θ*
    的增加，*cosθ* 减少，推力变得越来越不有效。最后，当 *θ* = 90° 时，*cosθ* = 0，推力变得完全无效。
- en: 'Two vectors are orthogonal if their dot product is zero. Geometrically, this
    means the vectors are perpendicular to each other. Physically, this means the
    two vectors are independent: one cannot influence the other. You can say there
    is nothing in common between orthogonal vectors. For instance, the feature vector
    for *d*[5] is ![](../../OEBPS/Images/eq_02-10-p2.png) and that for *d*[6] is ![](../../OEBPS/Images/eq_02-10-q2.png)
    in table [2.1](02.xhtml#tab-doc-vec). These are orthogonal (dot product is zero),
    and you can easily see that none of the feature words (*gun*, *violence*) are
    common to both documents.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个向量的点积为零，则这两个向量是正交的。从几何上看，这意味着向量彼此垂直。从物理上看，这意味着两个向量是独立的：一个不能影响另一个。你可以这样说，正交向量之间没有共同之处。例如，*d*[5]
    的特征向量是 ![](../../OEBPS/Images/eq_02-10-p2.png)，而 *d*[6] 的特征向量是 ![](../../OEBPS/Images/eq_02-10-q2.png)（见表
    [2.1](02.xhtml#tab-doc-vec)）。这些是正交的（点积为零），你可以很容易地看到，两个文档中都没有共同的特征词（*枪*，*暴力*）。
- en: '2.7 Python code: Basic vector and matrix operations via PyTorch'
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.7 Python 代码：通过 PyTorch 的基本向量和矩阵运算
- en: In this section, we use Python PyTorch code to illustrate many of the concepts
    discussed earlier.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用 Python PyTorch 代码来阐述前面讨论的许多概念。
- en: NOTE Fully functional code for this section, executable via Jupyter Notebook,
    can be found at [http://mng.bz/ryzE](https://github.com/krishnonwork/mathematical-methods-in-deep-learning-ipython/blob/master/python/ch2/2.7-transpose-dot-matmul.ipynb).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: NOTE 完整功能的代码可在 Jupyter Notebook 中执行，可在[http://mng.bz/ryzE](https://github.com/krishnonwork/mathematical-methods-in-deep-learning-ipython/blob/master/python/ch2/2.7-transpose-dot-matmul.ipynb)找到。
- en: 2.7.1 PyTorch code for a matrix transpose
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.7.1 矩阵转置的 PyTorch 代码
- en: The following listing shows the PyTorch code for a matrix transpose.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了矩阵转置的 PyTorch 代码。
- en: Listing 2.5 Transpose
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.5 转置
- en: '[PRE4]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ① The torch.arange function creates a vector whose elements go from *start*
    to *stop* in increments of *step*. Here we create a 4 × 9 image corresponding
    to *I*[4,9] in equation [2.2](02.xhtml#eq-tiny_im), shown in figure [2.3](02.xhtml#fig-tiny_im).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ① torch.arange 函数创建一个元素从 *start* 到 *stop* 以 *step* 为增量的向量。这里我们创建一个 4 × 9 的图像，对应于方程
    [2.2](02.xhtml#eq-tiny_im) 中的 *I*[4,9]，如图 [2.3](02.xhtml#fig-tiny_im) 所示。
- en: ② The transpose operator interchanges rows and columns. The 4 × 9 image becomes
    a 9 × 4 image (see figure [2.6](02.xhtml#fig-tiny_im-transpose). The element at
    position (*i, j*) is interchanged with the element at position (*j, i*).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ② 转置运算符交换行和列。4 × 9 的图像变为 9 × 4 的图像（见图 [2.6](02.xhtml#fig-tiny_im-transpose)。位置
    (*i, j*) 的元素与位置 (*j, i*) 的元素交换。
- en: ③ Interchanged elements of the original and transposed matrix are equal.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 原始矩阵和转置矩阵的交换元素相等。
- en: ④ The .T operator retrieves the transpose of an array.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ④ .T 运算符检索数组的转置。
- en: 2.7.2 PyTorch code for a dot product
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.7.2 PyTorch 点积的代码
- en: The dot product of two vectors ![](../../OEBPS/Images/AR_a.png) and ![](../../OEBPS/Images/AR_b.png)
    represents the components of one vector along the other. Consider two vectors
    ![](../../OEBPS/Images/AR_a.png) = [*a*[1] *a*[2] *a*[3]] and ![](../../OEBPS/Images/AR_b.png)
    = [*b*[1] *b*[2] *b*[3]]. Then ![](../../OEBPS/Images/AR_a.png).![](../../OEBPS/Images/AR_b.png)
    = *a*[1]*b*[1] + *a*[2]*b*[2] + *a*[3]*b*[3].
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 两个向量 ![](../../OEBPS/Images/AR_a.png) 和 ![](../../OEBPS/Images/AR_b.png) 的点积表示一个向量沿另一个向量的分量。考虑两个向量
    ![](../../OEBPS/Images/AR_a.png) = [*a*[1] *a*[2] *a*[3]] 和 ![](../../OEBPS/Images/AR_b.png)
    = [*b*[1] *b*[2] *b*[3]]。那么 ![](../../OEBPS/Images/AR_a.png).![](../../OEBPS/Images/AR_b.png)
    = *a*[1]*b*[1] + *a*[2]*b*[2] + *a*[3]*b*[3]。
- en: Listing 2.6 Dot product
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.6 点积
- en: '[PRE5]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '① Outputs 32: 1 ∗ 4 + 2 ∗ 5 + 3 ∗ 6'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ① 输出 32：1 ∗ 4 + 2 ∗ 5 + 3 ∗ 6
- en: '② Outputs 0: 1 ∗ 0 + 0 ∗ 1'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ② 输出 0：1 ∗ 0 + 0 ∗ 1
- en: 2.7.3 PyTorch code for matrix vector multiplication
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.7.3 PyTorch 矩阵-向量乘法的代码
- en: Consider a matrix *A[m, n]* with *m* rows and *n* columns that is multiplied
    with a vector *![](../../OEBPS/Images/AR_b.png)[n]* with *n* elements. The result
    is a *m* element column vector *![](../../OEBPS/Images/AR_c.png)[m]* . In the
    following example, *m* = 3 and *n* = 2.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个具有 *m* 行和 *n* 列的矩阵 *A[m, n]*，它与一个具有 *n* 个元素的向量 *![](../../OEBPS/Images/AR_b.png)[n]*
    相乘。结果是 *m* 个元素的列向量 *![](../../OEBPS/Images/AR_c.png)[m]*。在以下示例中，*m* = 3，*n* =
    2。
- en: '![](../../OEBPS/Images/eq_02-11-g.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-11-g.png)'
- en: In general,
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，
- en: '![](../../OEBPS/Images/eq_02-11-h.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-11-h.png)'
- en: Listing 2.7 Matrix vector multiplication
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.7 矩阵-向量乘法
- en: '[PRE6]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ① A linear model comprises a weight vector ![](../../OEBPS/Images/AR_w.png)
    and bias *b*. For each training data instance *![](../../OEBPS/Images/AR_x.png)[i]*,
    the model outputs *y**[i]* = ![](../../OEBPS/Images/AR_x.png)*[i]^T*![](../../OEBPS/Images/AR_w.png)
    + *b*. For the training data matrix *X* (whose rows are training data instances),
    the model outputs *X*![](../../OEBPS/Images/AR_w.png) + ![](../../OEBPS/Images/AR_b.png)
    = ![](../../OEBPS/Images/AR_y.png)
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ① 线性模型由权重向量 ![](../../OEBPS/Images/AR_w.png) 和偏置 *b* 组成。对于每个训练数据实例 *![](../../OEBPS/Images/AR_x.png)[i]*，模型输出
    *y**[i]* = ![](../../OEBPS/Images/AR_x.png)*[i]^T*![](../../OEBPS/Images/AR_w.png)
    + *b*。对于训练数据矩阵 *X*（其行是训练数据实例），模型输出 *X*![](../../OEBPS/Images/AR_w.png) + ![](../../OEBPS/Images/AR_b.png)
    = ![](../../OEBPS/Images/AR_y.png)
- en: ② Cat-brain 15 × 2 training data matrix (equation [2.7](02.xhtml#eq-cat-brain-nobiased))
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ② Cat-brain 15 × 2 训练数据矩阵（方程 [2.7](02.xhtml#eq-cat-brain-nobiased)）
- en: ③ Random initialization of weight vector
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 权重向量的随机初始化
- en: '④ Model training output: ![](../../OEBPS/Images/AR_y.png) = *X*![](../../OEBPS/Images/AR_w.png)
    + *b*. The scalar *b* is automatically replicated to create a vector.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 模型训练输出：![](../../OEBPS/Images/AR_y.png) = *X*![](../../OEBPS/Images/AR_w.png)
    + *b*。标量 *b* 会自动复制以创建一个向量。
- en: 2.7.4 PyTorch code for matrix-matrix multiplication
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.7.4 PyTorch 矩阵-矩阵乘法的代码
- en: 'Consider a matrix *A[m, p]* with *m* rows and *p* columns. Let’s multiply it
    with another matrix *B[p, n]* with *p* rows and *n* columns. The resultant matrix
    *C[m, n]* contains *m* rows and *n* columns. Note that the number of columns in
    the left matrix *A* should match the number of rows in the right matrix *B*:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个具有 *m* 行和 *p* 列的矩阵 *A[m, p]*。让我们用另一个具有 *p* 行和 *n* 列的矩阵 *B[p, n]* 相乘。结果矩阵
    *C[m, n]* 包含 *m* 行和 *n* 列。请注意，左矩阵 *A* 的列数应与右矩阵 *B* 的行数相匹配：
- en: '![](../../OEBPS/Images/eq_02-11-i.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-11-i.png)'
- en: In general,
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，
- en: '![](../../OEBPS/Images/eq_02-11-j.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-11-j.png)'
- en: Listing 2.8 Matrix-matrix multiplication
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.8 矩阵-矩阵乘法
- en: '[PRE7]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ① *C* = *AB* ⟹ *C*[*i*, *j*] is the dot product of the *i*th row of *A* and
    *j*th column of B.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ① *C* = *AB* ⟹ *C*[*i*, *j*] 是 *A* 的第 *i* 行与 *B* 的第 *j* 列的点积。
- en: ② ![](../../OEBPS/Images/eq_02-11-k.png)
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ② ![多维直线方程图](../../OEBPS/Images/eq_02-11-k.png)
- en: ③ The dot product can be viewed as a row matrix multiplied by a column matrix.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 点积可以看作是一个行矩阵乘以一个列矩阵。
- en: 2.7.5 PyTorch code for the transpose of a matrix product
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.7.5 PyTorch 中矩阵乘积转置的代码
- en: 'Given two matrices *A* and *B*, where the number of columns in *A* matches
    the number of rows in *B*, the transpose of their product is the product of the
    individual transposes *in reversed order*: (*AB*)*^T* = *B^TA^T*.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 给定两个矩阵 *A* 和 *B*，其中 *A* 的列数与 *B* 的行数相匹配，它们的乘积的转置是各自转置的乘积（顺序相反）：(*AB*)*^T* =
    *B^TA^T*。
- en: Listing 2.9 Transpose of a matrix product
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.9 矩阵乘积的转置
- en: '[PRE8]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ① Asserts equality between (*AB*)*^T* and *B^TA^T*
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ① 声明 (*AB*)*^T* 和 *B^TA^T* 相等
- en: '② Applies to matrix-vector multiplication, too: (*A^T*![](../../OEBPS/Images/AR_x.png))*^T*
    = ![](../../OEBPS/Images/AR_x.png)*^TA*'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ② 也适用于矩阵-向量乘法：(*A^T*![向量图](../../OEBPS/Images/AR_x.png))*^T* = ![向量图](../../OEBPS/Images/AR_x.png)*^TA*
- en: 2.8 Multidimensional line and plane equations and machine learning
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.8 多维直线和平面方程与机器学习
- en: Geometrically speaking, what does a machine learning classifier really do? We
    provided the outline of an answer in section [1.4](../Text/01.xhtml#sec-geom-view-ml).
    You are invited to review that and especially figures [1.2](../Text/01.xhtml#fig-geometrical_view)
    and [1.3](../Text/01.xhtml#fig-ml-as-mapping). We will briefly summarize here.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 从几何学的角度来看，机器学习分类器实际上做什么？我们在第 [1.4](../Text/01.xhtml#sec-geom-view-ml) 节中提供了答案的概要。请回顾该内容，特别是图
    [1.2](../Text/01.xhtml#fig-geometrical_view) 和 [1.3](../Text/01.xhtml#fig-ml-as-mapping)。我们在这里将简要总结。
- en: Inputs to a classifier are feature vectors. These vectors can be viewed as points
    in some multidimensional feature space. The task of classification then boils
    down to separating the points belonging to different classes. The points may be
    all jumbled up in the input space. It is the model’s job to transform them into
    a different (output) space where it is easier to separate the classes. A visual
    example of this was provided in figure [1.3](../Text/01.xhtml#fig-ml-as-mapping).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器的输入是特征向量。这些向量可以看作是多维特征空间中的点。分类的任务就是将属于不同类的点分开。这些点可能在输入空间中杂乱无章。这是模型的工作，将它们转换到不同的（输出）空间中，在那里更容易分开类。这种转换的视觉示例已在图
    [1.3](../Text/01.xhtml#fig-ml-as-mapping) 中提供。
- en: What is the geometrical nature of the separator? In a very simple situation,
    such as the one depicted in figure [1.2](../Text/01.xhtml#fig-geometrical_view),
    the separator is a line in 2D space. In real-life situations, the separator is
    often a line or a plane in a high-dimensional space. In more complicated situations,
    the separator is a curved surface, as depicted in figure [1.4](../Text/01.xhtml#fig-non_linear_separator).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 分离器的几何性质是什么？在非常简单的情况下，例如图 [1.2](../Text/01.xhtml#fig-geometrical_view) 所示的情况，分离器是二维空间中的一条线。在现实生活中的情况下，分离器通常是高维空间中的一条线或一个平面。在更复杂的情况下，分离器是一个曲面，如图
    [1.4](../Text/01.xhtml#fig-non_linear_separator) 所示。
- en: In this section, we will study the mathematics and geometry behind two types
    of separators, lines, and planes in high-dimensional spaces, aka hyperlines and
    hyperplanes.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将研究高维空间中两种类型分离器（直线和平面，也称为超线和超平面）背后的数学和几何。
- en: 2.8.1 Multidimensional line equation
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.8.1 多维直线方程
- en: In high school geometry, we learned *y* = *mx* + *c* as the equation of a line.
    But this does not lend itself readily to higher dimensions. Here we will study
    a better representation of a straight line that works equally well for any finite-dimensional
    space.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在高中几何中，我们学习了 *y* = *mx* + *c* 作为直线的方程。但这并不容易应用于更高维度。在这里，我们将研究一种更好的直线表示方法，它对任何有限维空间都同样有效。
- en: As shown in figure [2.8](02.xhtml#fig-multi-dim-lineeq), a line joining vectors
    ![](../../OEBPS/Images/AR_a.png) and ![](../../OEBPS/Images/AR_b.png) can be viewed
    as the set of points we will encounter if we
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [2.8](02.xhtml#fig-multi-dim-lineeq) 所示，连接向量 ![起点向量图](../../OEBPS/Images/AR_a.png)
    和 ![终点向量图](../../OEBPS/Images/AR_b.png) 的直线可以看作是我们将遇到的点的集合，如果我们
- en: Start at point ![](../../OEBPS/Images/AR_a.png)
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从点 ![起点向量图](../../OEBPS/Images/AR_a.png) 开始
- en: Travel along the direction ![](../../OEBPS/Images/AR_b.png) − ![](../../OEBPS/Images/AR_a.png)
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 沿着方向 ![方向向量图](../../OEBPS/Images/AR_b.png) − ![起点向量图](../../OEBPS/Images/AR_a.png)
    移动
- en: '![](../../OEBPS/Images/CH02_F08_Chaudhury.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![章节图](../../OEBPS/Images/CH02_F08_Chaudhury.png)'
- en: Figure 2.8 Any point ![](../../OEBPS/Images/AR_x.png) on the line joining two
    vectors ![](../../OEBPS/Images/AR_a.png), ![](../../OEBPS/Images/AR_b.png) is
    given by ![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_a.png) +
    *α*(![](../../OEBPS/Images/AR_b.png)−![](../../OEBPS/Images/AR_a.png)).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.8 在连接两个向量 ![](../../OEBPS/Images/AR_a.png) 和 ![](../../OEBPS/Images/AR_b.png)
    的直线上，任意点 ![](../../OEBPS/Images/AR_x.png) 可以表示为 ![](../../OEBPS/Images/AR_x.png)
    = ![](../../OEBPS/Images/AR_a.png) + *α*(![](../../OEBPS/Images/AR_b.png)−![](../../OEBPS/Images/AR_a.png))。
- en: Different points on the line are obtained by traveling different distances.
    Denoting this arbitrary distance by *α*, the equation of the line joining vectors
    ![](../../OEBPS/Images/AR_a.png) and ![](../../OEBPS/Images/AR_b.png) can be expressed
    as
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 通过旅行不同的距离，可以得到线上的不同点。用 *α* 表示这个任意距离，连接向量 ![](../../OEBPS/Images/AR_a.png) 和
    ![](../../OEBPS/Images/AR_b.png) 的直线方程可以表示为
- en: '![](../../OEBPS/Images/eq_02-12.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-12.png)'
- en: Equation 2.12
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 2.12
- en: Equation [2.12](02.xhtml#eq-collinearity) says that any point on the line joining
    ![](../../OEBPS/Images/AR_a.png) and ![](../../OEBPS/Images/AR_b.png) can be obtained
    as a weighted combination of ![](../../OEBPS/Images/AR_a.png) and ![](../../OEBPS/Images/AR_b.png),
    the weights being *α* and 1 − *α*. By varying *α*, we obtain different points
    on the line. Also, different ranges of *α* values yield different segments on
    the line. As shown in figure [2.8](02.xhtml#fig-multi-dim-lineeq), values of *α*
    between 0 and 1 yield points between ![](../../OEBPS/Images/AR_a.png) and ![](../../OEBPS/Images/AR_b.png).
    Negative values of *α* yield points to the left of ![](../../OEBPS/Images/AR_a.png).
    Values of *α* greater than 1 yield points to the right of ![](../../OEBPS/Images/AR_b.png).
    This equation for a line works for any dimensions, not just two.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 [2.12](02.xhtml#eq-collinearity) 表示，连接 ![](../../OEBPS/Images/AR_a.png)
    和 ![](../../OEBPS/Images/AR_b.png) 的直线上的任意点都可以表示为 ![](../../OEBPS/Images/AR_a.png)
    和 ![](../../OEBPS/Images/AR_b.png) 的加权组合，权重为 *α* 和 1 − *α*。通过改变 *α*，我们可以得到直线上的不同点。不同的
    *α* 值范围产生直线上的不同线段。如图 [2.8](02.xhtml#fig-multi-dim-lineeq) 所示，*α* 在 0 和 1 之间的值产生在
    ![](../../OEBPS/Images/AR_a.png) 和 ![](../../OEBPS/Images/AR_b.png) 之间的点。*α* 的负值产生在
    ![](../../OEBPS/Images/AR_a.png) 的左侧。*α* 大于 1 的值产生在 ![](../../OEBPS/Images/AR_b.png)
    的右侧。这个直线方程适用于任何维度，而不仅仅是二维。
- en: 2.8.2 Multidimensional planes and their role in machine learning
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.8.2 多维平面及其在机器学习中的作用
- en: 'In section [1.5](../Text/01.xhtml#sec-regression-vs-classification), we encountered
    classifiers. Let’s take another look at them. Suppose we want to create a classifier
    that helps us make *buy* or *no-buy* decisions on stocks based on only three input
    variables: 1) *momentum*, or the rate at which the stock price is changing positive
    momentum means the stock price is increasing and vice versa); 2) the *dividend*
    paid last quarter; and (3) *volatility*, or how much the price has fluctuated
    in the last quarter. Let’s plot all training points in the feature space with
    coordinate axes corresponding to the variables *momentum*, *dividend*, *volatility*.
    Figure [2.9](02.xhtml#fig-planar-classifier) shows that the classes can be separated
    by a plane in the three-dimensional feature space.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [1.5](../Text/01.xhtml#sec-regression-vs-classification) 节中，我们遇到了分类器。让我们再看看它们。假设我们想要创建一个分类器，帮助我们根据仅有的三个输入变量：1)
    *动量*，即股价变化的速率（正动量意味着股价在上升，反之亦然）；2) 上个季度支付的 *股息*；以及 3) *波动性*，即股价在上个季度的波动幅度。让我们在特征空间中绘制所有训练点，坐标轴对应于变量
    *动量*、*股息*、*波动性*。图 [2.9](02.xhtml#fig-planar-classifier) 显示，类别可以通过三维特征空间中的一个平面来分离。
- en: '![](../../OEBPS/Images/CH02_F09_Chaudhury.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH02_F09_Chaudhury.png)'
- en: 'Figure 2.9 A toy machine learning classifier for stock buy vs. no-buy decision-making.
    A plus (+) indicates no-buy, and a dash (-) indicates buy. The decision is made
    based on three input variables: momentum, dividend, and volatility.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.9 一个玩具机器学习分类器，用于股票买入与不买入决策。加号 (+) 表示不买入，减号 (-) 表示买入。决策是基于三个输入变量：动量、股息和波动性。
- en: Geometrically speaking, our model simply corresponds to this plane. Input points
    above the plane indicate buy decisions (dashes [-]), and input points indicate
    no-buy decisions (pluses [+]). In general, you want to buy high-positive-momentum
    stocks, so points at the higher end of the momentum axis are likelier to be *buy*.
    However, this is not the only indicator. For more volatile stocks, we demand higher
    *momentum* to switch from *no-buy* to *buy*. This is why the plane slopes upward
    (higher *momentum*) as we move rightward (higher *volatility*). Also, we demand
    less *momentum* for stocks with higher *dividends*. This is why the plane slopes
    downward (lower *momentum*) as we go toward higher *dividends*.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 从几何学的角度来看，我们的模型简单地对应于这个平面。位于平面之上的输入点表示买入决策（破折号 [-]），而位于平面之下的输入点表示不买入决策（加号 [+])。一般来说，你想要买入高动量股票，因此动量轴较高端的点更有可能被标记为
    *买入*。然而，这并不是唯一的指标。对于更波动的股票，我们要求更高的 *动量* 来从 *不买入* 切换到 *买入*。这就是为什么随着我们向右移动（波动性更高），平面向上倾斜（动量更高）。此外，对于股息率更高的股票，我们要求更低的
    *动量*。这就是为什么随着我们向更高的 *股息率* 移动，平面向下倾斜（动量更低）。
- en: Real problems have more dimensions, of course (since many more inputs are involved
    in the decision), and the separator becomes a hyperplane. Also, in real-life problems,
    the points are often too intertwined in the input space for any separator to work.
    We first have to apply a transformation that maps the point to an output space
    where it is easier to separate. Given their significance as class separators in
    machine learning, we will study hyperplanes in this section.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 实际问题通常有更多的维度（因为决策中涉及更多的输入），分隔符变成超平面。此外，在实际问题中，点在输入空间中通常交织得太紧密，以至于任何分隔符都无法工作。我们首先必须应用一个变换，将点映射到一个更容易分离的输出空间。鉴于它们在机器学习中的类别分隔符的重要性，我们将在本节中研究超平面。
- en: In high school 3D geometry, we learned *ax* + *by* + *cz* + *d* = 0 as the equation
    of a plane. Now we will study a version of it that works in higher dimensions.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在高中三维几何中，我们学习了 *ax* + *by* + *cz* + *d* = 0 作为平面的方程。现在我们将研究它在更高维度上工作的版本。
- en: Geometrically speaking, given a plane (in any dimension), we can find a direction
    called the *normal direction*, denoted *n̂*, such that
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 从几何学的角度来看，给定一个平面（在任何维度上），我们可以找到一个称为 *法线方向* 的方向，表示为 *n̂*，使得
- en: If we take any pair of points on the plane, say ![](../../OEBPS/Images/AR_x.png)[0]
    and ![](../../OEBPS/Images/AR_x.png), …
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们在平面上取任意一对点，比如说 ![](../../OEBPS/Images/AR_x.png)[0] 和 ![](../../OEBPS/Images/AR_x.png)，
    …
- en: The line joining ![](../../OEBPS/Images/AR_x.png) and ![](../../OEBPS/Images/AR_x.png)[0]—i.e.,
    the vector ![](../../OEBPS/Images/AR_x.png) − ![](../../OEBPS/Images/AR_x.png)[0]—is
    orthogonal to *n̂*.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接 ![](../../OEBPS/Images/AR_x.png) 和 ![](../../OEBPS/Images/AR_x.png)[0] 的线，即向量
    ![](../../OEBPS/Images/AR_x.png) − ![](../../OEBPS/Images/AR_x.png)[0]，与 *n̂*
    正交。
- en: Thus, if we know a fixed point on the plane, say ![](../../OEBPS/Images/AR_x.png)[0],
    then all points on the plane will satisfy
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们知道平面上的一个固定点，比如说 ![](../../OEBPS/Images/AR_x.png)[0]，那么平面上的所有点都将满足
- en: '*n̂* · (![](../../OEBPS/Images/AR_x.png) − ![](../../OEBPS/Images/AR_x.png)[0])
    = 0'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '*n̂* · (![](../../OEBPS/Images/AR_x.png) − ![](../../OEBPS/Images/AR_x.png)[0])
    = 0'
- en: or
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 或者
- en: '*n̂^T*(![](../../OEBPS/Images/AR_x.png) − ![](../../OEBPS/Images/AR_x.png)[0])
    = 0'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '*n̂^T*(![](../../OEBPS/Images/AR_x.png) − ![](../../OEBPS/Images/AR_x.png)[0])
    = 0'
- en: Thus we can express the equation of a plane as
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将平面的方程表示为
- en: '*n̂^T*![](../../OEBPS/Images/AR_x.png) − *n̂^T*![](../../OEBPS/Images/AR_x.png)[0]
    = 0'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '*n̂^T*![](../../OEBPS/Images/AR_x.png) − *n̂^T*![](../../OEBPS/Images/AR_x.png)[0]
    = 0'
- en: Equation 2.13
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2.13
- en: Equation [2.13](02.xhtml#eq-plane-0) is depicted pictorially in figure [2.10](02.xhtml#fig-multi-dim-planeeq).
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 [2.13](02.xhtml#eq-plane-0) 在图 [2.10](02.xhtml#fig-multi-dim-planeeq) 中以图形方式表示。
- en: '![](../../OEBPS/Images/CH02_F10_Chaudhury.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH02_F10_Chaudhury.png)'
- en: Figure 2.10 The normal to the plane is the same at all points on the plane.
    This is the fundamental property of a plane. *n̂* depicts that normal direction.
    Let ![](../../OEBPS/Images/AR_x.png)[0] be a point on the plane. All other points
    on the plane, depicted as ![](../../OEBPS/Images/AR_x.png), will satisfy the equation
    (![](../../OEBPS/Images/AR_x.png)−![](../../OEBPS/Images/AR_x.png)[0]) ⋅ *n̂*
    = 0. This physically says that the line joining a known point ![](../../OEBPS/Images/AR_x.png)[0]
    on the plane and any other arbitrary point ![](../../OEBPS/Images/AR_x.png) on
    the plane is at right angles to the normal *n̂*. This formulation works for any
    dimension.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.10 平面上的法线在平面的所有点上都是相同的。这是平面的基本性质。*n̂* 表示法线方向。设 ![](../../OEBPS/Images/AR_x.png)[0]
    为平面上的一个点。平面上所有其他点，表示为 ![](../../OEBPS/Images/AR_x.png)，都将满足方程 (![](../../OEBPS/Images/AR_x.png)−![](../../OEBPS/Images/AR_x.png)[0])
    ⋅ *n̂* = 0。这从物理上说明，连接平面上的已知点 ![](../../OEBPS/Images/AR_x.png)[0] 和平面上的任意其他任意点
    ![](../../OEBPS/Images/AR_x.png) 的直线与法线 *n̂* 成直角。这种表述适用于任何维度。
- en: 'In section [1.3](../Text/01.xhtml#sec-cat_brain), equation [1.3](../Text/01.xhtml#eq-linear-predictor),
    we encountered the simplest machine learning model: a weighted sum of inputs along
    with a bias. Denoting the inputs as ![](../../OEBPS/Images/AR_x.png), the weights
    as ![](../../OEBPS/Images/AR_w.png), and the bias as *b*, this model was depicted
    as'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [1.3](../Text/01.xhtml#sec-cat_brain) 节中，我们遇到了最简单的机器学习模型：输入的加权和以及偏差。将输入表示为
    ![](../../OEBPS/Images/AR_x.png)，权重表示为 ![](../../OEBPS/Images/AR_w.png)，偏差表示为
    *b*，该模型被描述为
- en: '![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png) + *b*
    = 0'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png) + *b*
    = 0'
- en: Equation 2.14
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2.14
- en: 'Comparing equations [2.13](02.xhtml#eq-plane-0) and [2.14](02.xhtml#eq-plane-1)
    , we get the geometric significance: the simple model of equation [1.3](../Text/01.xhtml#eq-linear-predictor)
    is nothing but a planar separator. Its weight vector ![](../../OEBPS/Images/AR_w.png)
    corresponds to the plane’s orientation (normal). The bias *b* corresponds to the
    plane’s location (a fixed point on the plane). During training, we are learning
    the weights and biases—this is essentially learning the orientation and position
    of the optimal plane that will separate the training inputs. To be consistent
    with the machine learning paradigm, henceforth we will write the equation of a
    hyperplane as equation [2.14](02.xhtml#eq-plane-1) for some constant ![](../../OEBPS/Images/AR_w.png)
    and *b*.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 比较方程 [2.13](02.xhtml#eq-plane-0) 和 [2.14](02.xhtml#eq-plane-1)，我们得到几何意义：方程 [1.3](../Text/01.xhtml#eq-linear-predictor)
    的简单模型实际上是一个平面分离器。其权重向量 ![](../../OEBPS/Images/AR_w.png) 对应于平面的方向（法线）。偏差 *b* 对应于平面的位置（平面上的一个固定点）。在训练过程中，我们正在学习权重和偏差——这本质上是在学习将训练输入分开的最优平面的方向和位置。为了与机器学习范式保持一致，从现在起我们将超平面的方程写作方程
    [2.14](02.xhtml#eq-plane-1)，其中包含某个常数 ![](../../OEBPS/Images/AR_w.png) 和 *b*。
- en: Note that ![](../../OEBPS/Images/AR_w.png) need not be a unit-length vector.
    Since the right-hand side is zero, if necessary, we can divide both sides by ||![](../../OEBPS/Images/AR_w.png)||
    to convert to a form like equation [2.13](02.xhtml#eq-plane-0).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，![](../../OEBPS/Images/AR_w.png) 不一定是单位长度的向量。由于右侧为零，如果需要，我们可以将两边都除以 ||![](../../OEBPS/Images/AR_w.png)||
    来转换为类似于方程 [2.13](02.xhtml#eq-plane-0) 的形式。
- en: The sign of the expression ![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png)
    + *b* has special significance. All points ![](../../OEBPS/Images/AR_x.png) for
    which ![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png) + *b*
    < 0 lie on the same side of the hyperplane. All points ![](../../OEBPS/Images/AR_x.png)
    for which ![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png)
    + *b* > 0 lie on the other side of the hyperplane. And of course, all points ![](../../OEBPS/Images/AR_x.png)
    for which ![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png)
    + *b* = 0 lie on the hyperplane.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 表达式 ![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png) + *b*
    的符号具有特殊意义。对于所有满足 ![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png)
    + *b* < 0 的点 ![](../../OEBPS/Images/AR_x.png)，它们位于超平面的同一侧。对于所有满足 ![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png)
    + *b* > 0 的点 ![](../../OEBPS/Images/AR_x.png)，它们位于超平面的另一侧。当然，对于所有满足 ![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png)
    + *b* = 0 的点 ![](../../OEBPS/Images/AR_x.png)，它们位于超平面上。
- en: It should be noted that the 3D equation *ax* + *by* + *cz* + *d* = 0 is a special
    case of equation [2.14](02.xhtml#eq-plane-1) because *ax* + *by* + *cz* + *d*
    = 0 can be rewritten as
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 应该注意的是，三维方程 *ax* + *by* + *cz* + *d* = 0 是方程 [2.14](02.xhtml#eq-plane-1) 的一个特例，因为
    *ax* + *by* + *cz* + *d* = 0 可以重写为
- en: '![](../../OEBPS/Images/eq_02-14-a.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-14-a.png)'
- en: which is same as ![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png)
    + *b* = 0 with ![](../../OEBPS/Images/eq_02-14-b2.png) and ![](../../OEBPS/Images/eq_02-14-c2.png).
    Incidentally, this tells us that in 3D, the normal to the plane *ax* + *by* +
    *cz* + *d* = 0 is ![](../../OEBPS/Images/eq_02-14-d2.png) .
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这与 ![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png) + *b*
    = 0 相同，其中 ![](../../OEBPS/Images/eq_02-14-b2.png) 和 ![](../../OEBPS/Images/eq_02-14-c2.png)。顺便提一下，这告诉我们，在三维空间中，平面
    *ax* + *by* + *cz* + *d* = 0 的法向量是 ![](../../OEBPS/Images/eq_02-14-d2.png)。
- en: 2.9 Linear combinations, vector spans, basis vectors, and collinearity preservation
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.9 线性组合，向量张成，基向量，和共线性保持
- en: by now, it should be clear that machine learning and data science are all about
    points in high-dimensional spaces. Consequently, it behooves us to have a decent
    understanding of these spaces. For instance, given a space, we may need to ask,
    “Would it be possible to express all points in the space in terms of a set of
    a few vectors? What is the smallest set of vectors we really need for that purpose?”
    This section is devoted to the study of these questions.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，应该很清楚，机器学习和数据科学都是关于高维空间中的点。因此，我们有必要对这些空间有一个相当的了解。例如，给定一个空间，我们可能需要问，“是否可以用一组少数几个向量来表示空间中的所有点？我们真正需要多少个向量才能达到这个目的？”本节致力于研究这些问题。
- en: 2.9.1 Linear dependence
  id: totrans-303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.9.1 线性相关
- en: Consider the vectors (points) shown in figure [2.11](02.xhtml#fig-lin-dep).
    The corresponding vectors in 2D are
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑图 [2.11](02.xhtml#fig-lin-dep) 中显示的向量（点）。在 2D 中的对应向量是
- en: '![](../../OEBPS/Images/eq_02-14-e.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-14-e.png)'
- en: We can find four scalars *α*[0] = 2, *α*[1] = 2, *α*[2] = 2, and *α*[3] = −3
    such that
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以找到四个标量 *α*[0] = 2, *α*[1] = 2, *α*[2] = 2, 和 *α*[3] = −3，使得
- en: '![](../../OEBPS/Images/eq_02-14-f.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-14-f.png)'
- en: If we can find such scalars, not all zero, we say the vectors ![](../../OEBPS/Images/AR_v.png)[0],
    ![](../../OEBPS/Images/AR_v.png)[1], ![](../../OEBPS/Images/AR_v.png)[2], and
    ![](../../OEBPS/Images/AR_v.png)[3] are *linearly dependent*. The geometric picture
    to keep in mind is that points corresponding to linearly dependent vectors lie
    on a single straight line in the space containing them.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能找到这样的标量，它们不全为零，我们说向量 ![](../../OEBPS/Images/AR_v.png)[0], ![](../../OEBPS/Images/AR_v.png)[1],
    ![](../../OEBPS/Images/AR_v.png)[2], 和 ![](../../OEBPS/Images/AR_v.png)[3] 是 *线性相关*
    的。需要记住的几何图像是，对应于线性相关向量的点位于包含它们的单一直线上。
- en: '![](../../OEBPS/Images/CH02_F11_Chaudhury.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH02_F11_Chaudhury.png)'
- en: Figure 2.11 Linearly dependent points in a 2D plane
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.11 2D 平面中的线性相关点
- en: Collinearity implies linear dependence
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 共线性意味着线性相关
- en: 'Proof: Let ![](../../OEBPS/Images/AR_a.png), ![](../../OEBPS/Images/AR_b.png)
    and ![](../../OEBPS/Images/AR_c.png) be three collinear vectors. From equation
    [2.12](02.xhtml#eq-collinearity), there exists some *α* ∈ ℝ such that'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 证明：设 ![](../../OEBPS/Images/AR_a.png), ![](../../OEBPS/Images/AR_b.png) 和 ![](../../OEBPS/Images/AR_c.png)
    是三个共线向量。根据方程 [2.12](02.xhtml#eq-collinearity)，存在某个 *α* ∈ ℝ，使得
- en: '![](../../OEBPS/Images/AR_c.png) = (1−*α*)![](../../OEBPS/Images/AR_a.png)
    + *α*![](../../OEBPS/Images/AR_b.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/AR_c.png) = (1−*α*)![](../../OEBPS/Images/AR_a.png)
    + *α*![](../../OEBPS/Images/AR_b.png)'
- en: This equation can be rewritten as
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程可以重写为
- en: '*α*[1]![](../../OEBPS/Images/AR_a.png) + *α*[2]![](../../OEBPS/Images/AR_b.png)
    + *α*[3]![](../../OEBPS/Images/AR_c.png) = 0'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '*α*[1]![](../../OEBPS/Images/AR_a.png) + *α*[2]![](../../OEBPS/Images/AR_b.png)
    + *α*[3]![](../../OEBPS/Images/AR_c.png) = 0'
- en: where *α*[1] = (1−*α*), *α*[2] = *α* and *α*[3] = −1. Thus we have proven that
    three collinear vectors ![](../../OEBPS/Images/AR_a.png), ![](../../OEBPS/Images/AR_b.png),
    and ![](../../OEBPS/Images/AR_c.png) must also be linearly dependent.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *α*[1] = (1−*α*), *α*[2] = *α* 和 *α*[3] = −1。因此，我们已经证明了三个共线的向量 ![](../../OEBPS/Images/AR_a.png),
    ![](../../OEBPS/Images/AR_b.png), 和 ![](../../OEBPS/Images/AR_c.png) 也必须是线性相关的。
- en: Linear combination
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 线性组合
- en: Given a set of vectors ![](../../OEBPS/Images/AR_v.png)[1], ![](../../OEBPS/Images/AR_v.png)[2],
    …. ![](../../OEBPS/Images/AR_v.png)*[n]* and a set of scalar weights *α*[1], *α*[2],
    …*α[n]*, the weighted sum *α*[1]![](../../OEBPS/Images/AR_v.png)[1] + *α*[2]![](../../OEBPS/Images/AR_v.png)[2]
    + + … *α**[n]*![](../../OEBPS/Images/AR_v.png)*[n]* is called a *linear combination*.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一组向量 ![](../../OEBPS/Images/AR_v.png)[1], ![](../../OEBPS/Images/AR_v.png)[2],
    …. ![](../../OEBPS/Images/AR_v.png)*[n]* 和一组标量权重 *α*[1], *α*[2], …*α[n]*，它们的加权求和
    *α*[1]![](../../OEBPS/Images/AR_v.png)[1] + *α*[2]![](../../OEBPS/Images/AR_v.png)[2]
    + + … *α**[n]*![](../../OEBPS/Images/AR_v.png)*[n]* 被称为 *线性组合*。
- en: Generic multidimensional definition of linear dependence
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 线性相关的多维定义
- en: A set of vectors ![](../../OEBPS/Images/AR_v.png)[1], ![](../../OEBPS/Images/AR_v.png)[2],
    …. ![](../../OEBPS/Images/AR_v.png)*[n]* are *linearly dependent* if there exists
    a set of weights *α*[1], *α*[2], …*α[n]*, not all zeros, such that *α*[1]![](../../OEBPS/Images/AR_v.png)[1]
    + *α*[2]![](../../OEBPS/Images/AR_v.png)[2] + + … *α**[n]*![](../../OEBPS/Images/AR_v.png)*[n]*
    = 0. For example, the row vectors [1   1] and [2   2] are linearly dependent,
    since –2[1   1] + [2   2] = 0.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 一组向量 ![向量1](../../OEBPS/Images/AR_v.png)[1]、![向量2](../../OEBPS/Images/AR_v.png)[2]、……
    ![向量n](../../OEBPS/Images/AR_v.png)*[n]* 如果存在一组权重 *α*[1]、*α*[2]、…… *α[n]*（不全为零），使得
    *α*[1]![向量1](../../OEBPS/Images/AR_v.png)[1] + *α*[2]![向量2](../../OEBPS/Images/AR_v.png)[2]
    + + … *α**[n]*![向量n](../../OEBPS/Images/AR_v.png)*[n]* = 0，则称这些向量**线性相关**。例如，行向量
    [1   1] 和 [2   2] 是线性相关的，因为 –2[1   1] + [2   2] = 0。
- en: 2.9.2 Span of a set of vectors
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.9.2 向量集合的张量积
- en: Given a set of vectors ![](../../OEBPS/Images/AR_v.png)[1], ![](../../OEBPS/Images/AR_v.png)[2],
    …. ![](../../OEBPS/Images/AR_v.png)*[n]*, their *span* is defined as the set of
    all vectors that are linear combinations of the original set . This includes the
    original vectors.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一组向量 ![向量1](../../OEBPS/Images/AR_v.png)[1]、![向量2](../../OEBPS/Images/AR_v.png)[2]、……
    ![向量n](../../OEBPS/Images/AR_v.png)*[n]*，它们的**张量积**定义为所有可以由原始集合的线性组合构成的向量集合。这包括原始向量。
- en: For example, consider the two vectors ![](../../OEBPS/Images/eq_02-14-i1b.png)
    and ![](../../OEBPS/Images/eq_02-14-i2b.png). The span of these two vectors is
    the entire plane containing the two vectors. Any vector, for instance, the vector
    ![](../../OEBPS/Images/eq_02-14-j2.png) can be expressed as a weighted sum 18![](../../OEBPS/Images/AR_v.png)[*x*⊥]
    + 97![](../../OEBPS/Images/AR_v.png)[*y*⊥].
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑两个向量 ![向量1](../../OEBPS/Images/eq_02-14-i1b.png) 和 ![向量2](../../OEBPS/Images/eq_02-14-i2b.png)。这两个向量的张量积是包含这两个向量的整个平面。任何向量，例如向量
    ![向量2](../../OEBPS/Images/eq_02-14-j2.png)，都可以表示为加权求和 18![向量x](../../OEBPS/Images/AR_v.png)[*x*⊥]
    + 97![向量y](../../OEBPS/Images/AR_v.png)[*y*⊥]。
- en: You can probably recognize that ![](../../OEBPS/Images/eq_02-14-k1b.png) and
    ![](../../OEBPS/Images/eq_02-14-k2b.png) are the familiar Cartesian coordinate
    axes (*X*-axis and *Y*-axis, respectively) in the 2D plane.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经认出 ![向量1](../../OEBPS/Images/eq_02-14-k1b.png) 和 ![向量2](../../OEBPS/Images/eq_02-14-k2b.png)
    是熟悉的二维平面中的笛卡尔坐标轴（*X*-轴和*Y*-轴，分别）。
- en: 2.9.3 Vector spaces, basis vectors, and closure
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.9.3 向量空间、基向量和封闭性
- en: We have been talking informally about vector spaces. It is time to define them
    more precisely.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一直在非正式地讨论向量空间。现在是时候更精确地定义它们了。
- en: Vector spaces
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 向量空间
- en: A set of vectors (points) in *n* dimensions form a *vector space* if and only
    if the operations of *addition* and *scalar multiplication* are defined on the
    set. In particular, this implies that it is possible to take linear combinations
    of members of a vector space.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *n* 维度中的向量（点）集合形成一个**向量空间**，当且仅当该集合上定义了**加法**和**标量乘法**运算。特别是，这意味着可以取向量空间成员的线性组合。
- en: Basis vectors
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 基向量
- en: Given a vector space, a set of vectors that span the space is called a *basis*
    for the space. For instance, for the space ℝ², the two vectors ![](../../OEBPS/Images/eq_02-14-k1.png)
    and ![](../../OEBPS/Images/eq_02-14-k2.png) are basis vectors. This essentially
    means any vector in ℝ² can be expressed as a linear combination of these two.
    The notion can be extended to higher dimensions. For ℝ*^n*, the vectors ![](../../OEBPS/Images/eq_02-14-l.png)
    form a basis.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个向量空间，一组张量积空间中的向量被称为该空间的**基**。例如，对于空间ℝ²，两个向量 ![向量1](../../OEBPS/Images/eq_02-14-k1.png)
    和 ![向量2](../../OEBPS/Images/eq_02-14-k2.png) 是基向量。这本质上意味着ℝ²中的任何向量都可以表示为这两个向量的线性组合。这个概念可以扩展到更高维度。对于ℝ*^n*，向量
    ![向量n](../../OEBPS/Images/eq_02-14-l.png) 构成一个基。
- en: The alert reader has probably guessed by now that the basis vectors are related
    to coordinate axes. In fact, the basis vectors just described constitute the Cartesian
    coordinate axes.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 警觉的读者可能已经猜到了，基向量与坐标轴有关。事实上，上面描述的基向量构成了笛卡尔坐标轴。
- en: 'So far, we have only seen examples of basis vectors that are mutually orthogonal,
    such as the dot product of the two basis vectors in ℝ² shown earlier: ![](../../OEBPS/Images/eq_02-14-m.png).
    However, basis vectors do not have to be orthogonal. Any pair of linearly independent
    vectors forms a basis in ℝ². Basis vectors, then, are by no means unique. That
    said, orthogonal vectors are most convenient, as we shall see later.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只看到了基向量的例子，这些向量是相互正交的，例如前面展示的ℝ²中的两个基向量的点积：![向量点积](../../OEBPS/Images/eq_02-14-m.png)。然而，基向量不必是正交的。任何一对线性无关的向量在ℝ²中形成一个基。因此，基向量绝不是唯一的。尽管如此，正交向量是最方便的，我们将在后面看到。
- en: Minimal and complete basis
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 最小和完备基
- en: Exactly *n* vectors are needed to span a space with dimensionality *n*. This
    means the basis set for a space will have at least as many elements as the dimensionality
    of the space. That many basis vectors are also sufficient to form a basis. For
    instance, exactly *n* vectors are needed to form a basis in (that is, span) ℝ*^n*.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 需要**n**个向量来张成具有**n**维度的空间。这意味着空间的基集合将至少包含与空间维度一样多的元素。那么多的基向量也足以形成一个基。例如，需要恰好**n**个向量来形成（即张成）ℝ*^n*的基。
- en: A related fact is that in ℝ*^n*, any set of *m* vectors with *m* > *n* will
    be linearly dependent. In other words, the largest size of a set of linearly independent
    vectors in an *n*-dimensional space is *n*.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 一个相关的事实是，在ℝ*^n*中，任何包含*m*个向量且*m* > *n*的集合将是线性相关的。换句话说，在*n*-维空间中，线性无关向量集合的最大大小是*n*。
- en: Closure
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 封闭性
- en: 'A set of vectors is said to be *closed* under linear combination if and only
    if the linear combination of any pair of vectors in the set also belongs to the
    same set. Consider the set of points ℝ². Recall that this is the set of vectors
    with two real elements. Take any pair of vectors ![](../../OEBPS/Images/AR_a.png)
    and ![](../../OEBPS/Images/AR_b.png) in ℝ²: for instance, ![](../../OEBPS/Images/eq_02-14-n1.png)
    and ![](../../OEBPS/Images/eq_02-14-n2.png). Any linear combination of these two
    vectors will also comprise two real numbers—that is, will belong to ℝ². We say
    ℝ² is a *vector space* since it is *closed* under linear combination.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个向量集合中的任意一对向量的线性组合也属于该集合，则称该向量集合在**线性组合**下是**封闭**的。考虑向量集合ℝ²。回想一下，这是包含两个实数元素的向量的集合。取ℝ²中的任意一对向量
    ![图片](../../OEBPS/Images/AR_a.png) 和 ![图片](../../OEBPS/Images/AR_b.png)：例如，![图片](../../OEBPS/Images/eq_02-14-n1.png)
    和 ![图片](../../OEBPS/Images/eq_02-14-n2.png)。这两个向量的任意线性组合也将包含两个实数——也就是说，将属于ℝ²。我们称ℝ²是一个**向量空间**，因为它在**线性组合**下是**封闭**的。
- en: Consider the space ℝ². Geometrically speaking, this represents a two dimensional
    plane. Let’s take two points on this plane, ![](../../OEBPS/Images/AR_a.png) and
    ![](../../OEBPS/Images/AR_b.png). Linear combinations of ![](../../OEBPS/Images/AR_a.png),
    ![](../../OEBPS/Images/AR_b.png) geometrically correspond to points on the line
    joining them. We know that if two points lie on a plane, the entire line will
    also lie on the plane. Thus, in two dimensions, a plane is closed under linear
    combinations. This is the geometrical intuition behind the notion of closure on
    vector spaces. It can be extended to arbitrary dimensions.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑空间ℝ²。从几何上讲，这代表一个二维平面。让我们在这个平面上取两个点 ![图片](../../OEBPS/Images/AR_a.png) 和 ![图片](../../OEBPS/Images/AR_b.png)。![图片](../../OEBPS/Images/AR_a.png)
    和 ![图片](../../OEBPS/Images/AR_b.png) 的线性组合在几何上对应于连接它们的直线上的点。我们知道，如果两个点位于一个平面上，那么整个直线也将位于该平面上。因此，在二维中，平面在**线性组合**下是封闭的。这是向量空间中**封闭性**概念的几何直觉。它可以扩展到任意维度。
- en: On the other hand, the set of points on the surface of a sphere is *not* closed
    under linear combination because the line joining an arbitrary pair of points
    on this set will not wholly lie on the surface of that sphere.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，球面上点的集合在**线性组合**下**不是**封闭的，因为连接该集合中任意一对点的直线不会完全位于该球面的表面上。
- en: '2.10 Linear transforms: Geometric and algebraic interpretations'
  id: totrans-340
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.10 线性变换：几何和代数解释
- en: Inputs to a machine learning or data science system are typically feature vectors
    (introduced in section [2.1](02.xhtml#sec-vectors)) in high-dimensional spaces.
    Each individual dimension of the feature vector corresponds to a particular property
    of the input. Thus, the feature vector is a descriptor for the particular input
    instance. It can be viewed as a point in the feature space. We usually transform
    the points to a friendlier space where it is easier to perform the analysis we
    are trying to do. For instance, if we are building a classifier, we try to transform
    the input into a space where the points belonging to different classes are more
    segregated (see section [1.3](../Text/01.xhtml#fig-ml-as-mapping) in general and
    figure [1.3](../Text/01.xhtml#fig-ml-as-mapping) in particular for simple examples).
    Sometimes we transform to simplify the data, eliminating axes along which there
    is scant variation in the data. Given their significance in machine learning,
    in this section we will study the basics of transforms.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习或数据科学系统的输入通常是特征向量（在第 [2.1](02.xhtml#sec-vectors) 节中介绍），它们位于高维空间中。特征向量的每个单独维度对应于输入的一个特定属性。因此，特征向量是特定输入实例的描述符。它可以被视为特征空间中的一个点。我们通常将点转换到更容易进行我们试图进行的分析的空间中。例如，如果我们正在构建一个分类器，我们试图将输入转换到不同类别的点更加分离的空间中（参见一般性的第
    [1.3](../Text/01.xhtml#fig-ml-as-mapping) 节和特定的图 [1.3](../Text/01.xhtml#fig-ml-as-mapping)）。有时我们进行变换以简化数据，消除数据中变化很少的轴。鉴于它们在机器学习中的重要性，在本节中，我们将研究变换的基本知识。
- en: Informally, a transform is an operation that maps a set of points vectors) to
    another. Given a set *S* of *n* × 1 vectors, any *m* × *n* matrix *T* can be viewed
    as a transform. If ![](../../OEBPS/Images/AR_v.png) belongs to the set *S*, multiplication
    with the matrix *T* will map (transform) ![](../../OEBPS/Images/AR_v.png) to a
    vector *T*![](../../OEBPS/Images/AR_v.png). We will later see that matrix multiplication
    is a subclass of transforms that preserve collinearity—points that lie on a straight
    line before the transformation will continue to lie on a (possibly different)
    straight line post the transformation. For instance, consider the matrix
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 非正式地说，变换是一种将一组点（向量）映射到另一组点的操作。给定一个由 *n* × 1 向量组成的集合 *S*，任何 *m* × *n* 矩阵 *T*
    都可以被视为一个变换。如果 ![](../../OEBPS/Images/AR_v.png) 属于集合 *S*，则与矩阵 *T* 的乘法将（变换）![](../../OEBPS/Images/AR_v.png)
    映射到向量 *T*![](../../OEBPS/Images/AR_v.png)。我们稍后将会看到，矩阵乘法是保持共线性的变换的一个子类——变换前位于直线上的点在变换后将继续位于（可能不同的）直线上。例如，考虑以下矩阵
- en: '![](../../OEBPS/Images/eq_02-14-o.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-14-o.png)'
- en: 'In section [2.14](02.xhtml#sec-rotation-matrices-eigen), we will see that this
    is a special kind of matrix called a rotation matrix; for now, simply consider
    it an example of a matrix. *R* is a transformation operator that maps a point
    in a 2D plane to another point in the same plane. In mathematical notation, *R*
    : ℝ² → ℝ². In fact, as depicted in figure [2.14](02.xhtml#fig-rotation_matrix_diagram),
    this transformation (multiplication by matrix *R*) rotates the position vector
    of a point in the 2D plane by an angle of 45°.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '在第 [2.14](02.xhtml#sec-rotation-matrices-eigen) 节中，我们将看到这是一种称为旋转矩阵的特殊类型的矩阵；现在，只需将其视为矩阵的一个例子。*R*
    是一个变换算子，它将二维平面上的一个点映射到同一平面上的另一个点。用数学符号表示，*R* : ℝ² → ℝ²。实际上，如图 [2.14](02.xhtml#fig-rotation_matrix_diagram)
    所示，这种变换（通过矩阵 *R* 的乘法）将二维平面上的一个点的位置向量旋转了45°。'
- en: The output and input points may belong to different spaces in such transforms.
    For instance, consider the matrix
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种变换中，输出点和输入点可能属于不同的空间。例如，考虑以下矩阵
- en: '![](../../OEBPS/Images/eq_02-14-p.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-14-p.png)'
- en: 'It is not hard to see that this matrix projects 3D points to the 2D X-Y plane:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易看出，这个矩阵将三维点投影到二维 X-Y 平面上：
- en: '![](../../OEBPS/Images/eq_02-14-q.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-14-q.png)'
- en: 'Hence, this transformation (multiplication by matrix *P*) projects points from
    three to two dimensions. In mathematical parlance, *P* : ℝ³ → ℝ².'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，这种变换（通过矩阵 *P* 的乘法）将点从三维投影到二维。在数学术语中，*P* : ℝ³ → ℝ²。'
- en: 'The transforms *R* and *P* share a common property: *they preserve collinearity*.
    This means a set of vectors (points) ![](../../OEBPS/Images/AR_a.png), ![](../../OEBPS/Images/AR_b.png),
    ![](../../OEBPS/Images/AR_c.png), ⋯ that originally lay on a straight line remain
    so after the transformation.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 变换 *R* 和 *P* 具有共同属性：*它们保持共线性*。这意味着一组原本位于直线上的向量（点）![](../../OEBPS/Images/AR_a.png)、![](../../OEBPS/Images/AR_b.png)、![](../../OEBPS/Images/AR_c.png)、⋯
    在变换后仍然保持直线。
- en: 'Let’s check this out for the rotation transformation in the example from section
    [2.9](02.xhtml#sec-lin-dep). There we saw four vectors:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下在[2.9](02.xhtml#sec-lin-dep)节中的例子中的旋转变换。在那里我们看到了四个向量：
- en: '![](../../OEBPS/Images/eq_02-14-r.png)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-14-r.png)'
- en: 'These vectors all lie on a straight *L* : *x* = *y*. The rotation transformed
    versions of these vectors are'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '这些向量都位于一条直线上 *L* : *x* = *y*。这些向量的旋转变换版本是'
- en: '![](../../OEBPS/Images/eq_02-14-s.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-14-s.png)'
- en: It is trivial to see that the transformed vectors also lie on a (different)
    straight line. In fact, ![](../../OEBPS/Images/AR_osm.png)^′, ![](../../OEBPS/Images/AR_a.png)^′,
    ![](../../OEBPS/Images/AR_b.png)^′, ![](../../OEBPS/Images/AR_c.png)^′ lie on
    the *Y*-axis, which is the 45° rotated version of the original line *y* = *x*.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易看出，变换后的向量也位于一条（不同的）直线上。事实上，![](../../OEBPS/Images/AR_osm.png)^′, ![](../../OEBPS/Images/AR_a.png)^′,
    ![](../../OEBPS/Images/AR_b.png)^′, ![](../../OEBPS/Images/AR_c.png)^′ 位于 *Y*
    轴上，这是原始直线 *y* = *x* 的 45° 旋转版本。
- en: It is trivial to see that the transformed vectors also lie on a (different)
    straight line. In fact, ![](../../OEBPS/Images/AR_osm.png)^′, ![](../../OEBPS/Images/AR_a.png)^′,
    ![](../../OEBPS/Images/AR_b.png)^′, ![](../../OEBPS/Images/AR_c.png)^′ lie on
    the *Y*-axis, which is the 45° rotated version of the original line *y* = *x*.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易看出，变换后的向量也位于一条（不同的）直线上。事实上，![](../../OEBPS/Images/AR_osm.png)^′, ![](../../OEBPS/Images/AR_a.png)^′,
    ![](../../OEBPS/Images/AR_b.png)^′, ![](../../OEBPS/Images/AR_c.png)^′ 位于 *Y*
    轴上，这是原始直线 *y* = *x* 的 45° 旋转版本。
- en: 'The projection transform represented by matrix *P* also preserves collinearity.
    Consider four collinear vectors in 3D:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 由矩阵 *P* 表示的投影变换也保持共线性。考虑三维空间中的四个共线向量：
- en: '![](../../OEBPS/Images/eq_02-14-t.png)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-14-t.png)'
- en: The corresponding transformed vectors
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 对应的变换向量
- en: '![](../../OEBPS/Images/eq_02-14-u.png)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-14-u.png)'
- en: also lie on a straight line in 2D.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 也位于二维空间中的一条直线上。
- en: The class of transforms that preserves collinearity are known as *linear transforms*.
    They can always be represented as a matrix multiplication. Conversely, all matrix
    multiplications represent a linear transformation. A more formal definition is
    provided later.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 保持共线性的变换类被称为 *线性变换*。它们总是可以表示为矩阵乘法。相反，所有的矩阵乘法都代表线性变换。稍后提供更正式的定义。
- en: 2.10.1 Generic multidimensional definition of linear transforms
  id: totrans-363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.10.1 线性变换的多维通用定义
- en: A function *ϕ* is a linear transform if and only if it satisfies
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 一个函数 *ϕ* 是线性变换当且仅当它满足
- en: '*ϕ*(*α*![](../../OEBPS/Images/AR_a.png) + *β*![](../../OEBPS/Images/AR_b.png))
    = *αϕ*(![](../../OEBPS/Images/AR_a.png)) + *βϕ*(![](../../OEBPS/Images/AR_b.png))
    ∀ *α*, *β* ∈ ℝ'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '*ϕ*(*α*![](../../OEBPS/Images/AR_a.png) + *β*![](../../OEBPS/Images/AR_b.png))
    = *αϕ*(![](../../OEBPS/Images/AR_a.png)) + *βϕ*(![](../../OEBPS/Images/AR_b.png))
    ∀ *α*, *β* ∈ ℝ'
- en: Equation 2.15
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2.15
- en: 'In other words, *a transform is linear if and only if the transform of the
    linear combination of two vectors is the same as the linear combination (with
    the same weights) of the transforms of individual vectors*. (This can be remembered
    as: *Linear transform means transforms of linear combinations are same as linear
    combinations of transforms*.) Multiplication with a rotation or projection matrix
    (shown earlier) is a linear transform.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，*一个变换是线性的当且仅当两个向量的线性组合的变换与单个向量的变换的线性组合（具有相同的权重）相同*。（这可以记住为：*线性变换意味着线性组合的变换与变换的线性组合相同*。）与旋转或投影矩阵（前面已展示）的乘法是一个线性变换。
- en: 2.10.2 All matrix-vector multiplications are linear transforms
  id: totrans-368
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.10.2 所有矩阵-向量乘法都是线性变换
- en: Let’s verify that matrix multiplication satisfies the definition of linear mapping
    (equation [2.15](02.xhtml#eq-lin-map)). Let ![](../../OEBPS/Images/AR_a.png),
    ![](../../OEBPS/Images/AR_b.png) ∈ ℝ*^n* be two arbitrary *n*-dimensional vectors
    and *A[m, n]* be an arbitrary matrix with *n* columns. Then following the standard
    rules of matrix-vector multiplication,
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们验证矩阵乘法是否满足线性映射的定义（方程[2.15](02.xhtml#eq-lin-map)）。设 ![](../../OEBPS/Images/AR_a.png),
    ![](../../OEBPS/Images/AR_b.png) ∈ ℝ*^n* 是两个任意的 *n*-维向量，*A[m, n]* 是一个任意的具有 *n*
    列的矩阵。然后按照矩阵-向量乘法的标准规则，
- en: '*A*(*α*![](../../OEBPS/Images/AR_a.png) + *β*![](../../OEBPS/Images/AR_b.png))
    = *α*(*A*![](../../OEBPS/Images/AR_a.png)) + *β*(*A*![](../../OEBPS/Images/AR_b.png))'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '*A*(*α*![](../../OEBPS/Images/AR_a.png) + *β*![](../../OEBPS/Images/AR_b.png))
    = *α*(*A*![](../../OEBPS/Images/AR_a.png)) + *β*(*A*![](../../OEBPS/Images/AR_b.png))'
- en: which mimics equation [2.15](02.xhtml#eq-lin-map) with *ϕ* replaced with matrix
    *A*. Thus we have proven that all matrix multiplications are linear transforms.
    The reverse is not true. In particular, linear transforms that operate on infinite-dimensional
    vectors are not matrices. But all linear transforms that operate on finite-dimensional
    vectors can be expressed as matrices. (The proof is a bit more complicated and
    will be skipped.)
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 它模仿了方程 [2.15](02.xhtml#eq-lin-map)，其中 *ϕ* 被矩阵 *A* 替换。因此，我们已经证明了所有矩阵乘法都是线性变换。反之则不成立。特别是，作用在无限维向量上的线性变换不是矩阵。但所有作用在有限维向量上的线性变换都可以表示为矩阵。（证明稍微复杂一些，将省略。）
- en: Thus, in finite dimensions, multiplication with a matrix and linear transformation
    are one and the same thing. In section [2.3](02.xhtml#sec-matrices), we saw the
    array view of matrices. The corresponding geometric view, that all matrices represent
    linear transformation, was presented in this section.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在有限维度中，矩阵乘法和线性变换是同一件事。在 [2.3](02.xhtml#sec-matrices) 节中，我们看到了矩阵的数组视图。相应的几何视图，即所有矩阵都表示线性变换，在本节中已经介绍。
- en: Let’s finish this section by studying an example of a transform that is *not*
    linear. Consider the function
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过研究一个 *不是* 线性的变换的例子来结束本节。考虑以下函数
- en: '*ϕ*(![](../../OEBPS/Images/AR_x.png)) = ||![](../../OEBPS/Images/AR_x.png)||'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '*ϕ*(![](../../OEBPS/Images/AR_x.png)) = ||![](../../OEBPS/Images/AR_x.png)||'
- en: 'for ![](../../OEBPS/Images/AR_x.png) ∈ ℝ*^n*. This function *ϕ* maps *n*-dimensional
    vectors to a scalar that is the length of the vector, *ϕ* : ℝ*^n* → ℝ. We will
    examine if it satisfies equation [2.15](02.xhtml#eq-lin-map) with *α*[1] = *α*[2]
    = 1. For two specific vectors ![](../../OEBPS/Images/AR_a.png), ![](../../OEBPS/Images/AR_b.png)
    ∈ ℝ*^n*,'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '对于 ![](../../OEBPS/Images/AR_x.png) ∈ ℝ*^n*。这个函数 *ϕ* 将 *n* 维向量映射到一个标量，即向量的长度，*ϕ*
    : ℝ*^n* → ℝ。我们将检查它是否满足方程 [2.15](02.xhtml#eq-lin-map) 中 *α*[1] = *α*[2] = 1 的条件。对于两个特定的向量
    ![](../../OEBPS/Images/AR_a.png)，![](../../OEBPS/Images/AR_b.png) ∈ ℝ*^n*，'
- en: '![](../../OEBPS/Images/eq_02-15-a1.png)'
  id: totrans-376
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_02-15-a1.png)'
- en: Now
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 现在
- en: '![](../../OEBPS/Images/eq_02-15-b.png)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_02-15-b.png)'
- en: and
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '![](../../OEBPS/Images/eq_02-15-c.png)'
  id: totrans-380
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_02-15-c.png)'
- en: 'Clearly, these two are not equal; hence, we have violated equation [2.15](02.xhtml#eq-lin-map):
    *ϕ* is a nonlinear mapping.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这两个不相等；因此，我们违反了方程 [2.15](02.xhtml#eq-lin-map)：*ϕ* 是一个非线性映射。
- en: 2.11 Multidimensional arrays, multilinear transforms, and tensors
  id: totrans-382
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.11 多维数组、多元线性变换和张量
- en: We often hear the term *tensor* in connection with machine learning. Google’s
    famous machine learning platform is named *TensorFlow*. In this section, we will
    introduce you to the concept of a tensor.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常在机器学习的背景下听到 *张量* 这个词。谷歌著名的机器学习平台被命名为 *TensorFlow*。在本节中，我们将向您介绍张量的概念。
- en: '2.11.1 Array view: Multidimensional arrays of numbers'
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.11.1 数组视图：数字的多维数组
- en: A tensor may be viewed as a generalized *n*-dimensional array—although, strictly
    speaking, not all multidimensional arrays are tensors. We will learn more about
    the distinction between multidimensional arrays and tensors when we study multilinear
    transforms. For now, we will not worry too much about the distinction. A vector
    can be viewed as a 1 tensor, a matrix is a 2 tensor, and a scalar is a 0 tensor.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 张量可以被视为一个广义的 *n* 维数组——尽管严格来说，并非所有多维数组都是张量。当我们学习多元线性变换时，我们将了解更多关于多维数组和张量之间区别的信息。现在，我们不必过于担心这种区别。一个向量可以被视为一个
    1 维张量，一个矩阵是一个 2 维张量，而一个标量是一个 0 维张量。
- en: 'In section [2.3](02.xhtml#sec-matrices), we saw that digital images are represented
    as 2D arrays (matrices). A color image—where each pixel is represented by three
    colors, R, G, and B (red, green, and blue)—is an example of a multidimensional
    array or tensor. This is because it can be viewed as a combination of three images:
    the R, G, and B images, respectively.'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [2.3](02.xhtml#sec-matrices) 节中，我们了解到数字图像被表示为二维数组（矩阵）。一个彩色图像——其中每个像素由三种颜色，R、G
    和 B（红色、绿色和蓝色）表示——是一个多维数组或张量的例子。这是因为它可以被视为三个图像的组合：R、G 和 B 图像，分别。
- en: The inputs and outputs to each layer in a neural network are also tensors.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中每一层的输入和输出也是张量。
- en: 2.12 Linear systems and matrix inverse
  id: totrans-388
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.12 线性系统和矩阵逆
- en: Machine learning today is usually an iterative process. Given a set of training
    data, you want to estimate a set of machine parameters that will yield target
    values (or close approximations to them) on training inputs. The number of training
    inputs and the size of the parameter set are often very large. This makes it impossible
    to have a closed-form solution where we solve for the unknown parameters in a
    single step. Solutions are usually iterative. We start with a guessed set of values
    for the parameters and iteratively improve the guess by processing training data.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的机器学习通常是一个迭代过程。给定一组训练数据，你希望估计一组机器参数，这些参数将在训练输入上产生目标值（或接近它们的近似值）。训练输入的数量和参数集的大小通常非常大。这使得我们无法有一个闭式解，其中我们一次性求解未知参数。解决方案通常是迭代的。我们从参数的猜测值集合开始，通过处理训练数据来迭代地改进猜测。
- en: Having said that, we often encounter smaller problems in real life. We are better
    off using more traditional closed-form techniques here since they are much faster
    and more accurate. This section is devoted to gaining some insights into these
    techniques.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，在现实生活中，我们经常遇到更小的问题。我们最好使用更传统的闭式技术，因为它们要快得多，也更准确。本节致力于对这些技术进行一些了解。
- en: 'Let’s go back to our familiar cat-brain problem and refer to its training data
    in table [2.2](02.xhtml#tab-cat-brain-training-data). As before, we are still
    talking about a weighted sum model with three parameters: weights *w*[0], *w*[1]
    and bias *b*. Let’s focus on the top three rows from the table, repeated here
    in table [2.2](02.xhtml#tab-cat-brain-training-data-trunc) for convenience.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们熟悉的猫脑问题，并参考其训练数据表 [2.2](02.xhtml#tab-cat-brain-training-data)。和之前一样，我们仍在讨论一个具有三个参数的加权求和模型：权重
    *w*[0]、*w*[1] 和偏置 *b*。让我们专注于表中的前三行，为了方便起见，这里再次以表 [2.2](02.xhtml#tab-cat-brain-training-data-trunc)
    的形式呈现。
- en: Table 2.3 Example training dataset for our toy machine learning–based cat brain
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.3 基于玩具机器学习的猫脑示例训练数据集
- en: '|  | Input value: Hardness | Input value: Sharpness | Output: Threat score
    |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '|  | 输入值：硬度 | 输入值：锋利度 | 输出：威胁分数 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 0 | 0.11 | 0.09 | −0.8 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.11 | 0.09 | −0.8 |'
- en: '| 1 | 0.01 | 0.02 | −0.97 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.01 | 0.02 | −0.97 |'
- en: '| 2 | 0.98 | 0.91 | 0.89 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.98 | 0.91 | 0.89 |'
- en: The training data says that with a hardness value 0.11 and a sharpness value
    0.09, we expect the system’s output to match or closely approximate) the target
    value −0.8, and so on. In other words, our estimated values for parameters *w*[0],
    *w*[1], *b* should ideally satisfy
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据表明，当硬度值为 0.11 且锋利度值为 0.09 时，我们期望系统的输出与目标值 -0.8 匹配或接近，等等。换句话说，我们估计的参数 *w*[0]、*w*[1]、*b*
    的值应理想地满足
- en: 0.11*w*[0] + 0.09*w*[1] + b = –0.8
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 0.11*w*[0] + 0.09*w*[1] + b = –0.8
- en: 0.01*w*[0] + 0.02*w*[1] + b = –0.97
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 0.01*w*[0] + 0.02*w*[1] + b = –0.97
- en: 0.98*w*[0] + 0.91 *w*[1] + b = 0.89
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 0.98*w*[0] + 0.91 *w*[1] + b = 0.89
- en: 'We can express this via matrix multiplication as the following equation:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过矩阵乘法表达如下方程：
- en: '![](../../OEBPS/Images/eq_02-15-d.png)'
  id: totrans-403
  prefs: []
  type: TYPE_IMG
  zh: '![方程图](../../OEBPS/Images/eq_02-15-d.png)'
- en: How do we obtain the values of *w*[0], *w*[1], *b* that make this equation true?
    That is, how do we solve this equation? There are formal methods (discussed later)
    to directly solve such equations for *w*[0], *w*[1], and *b* (in this very simple
    example, you might just “see” that *w*[0] = 1, *w*[1] = 1, *b* = −1 solves the
    equation, but we need a general method).
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何获得使这个方程成立的 *w*[0]、*w*[1]、*b* 的值？也就是说，我们如何解这个方程？有正式的方法（稍后讨论）可以直接解这类方程，以求解
    *w*[0]、*w*[1] 和 *b*（在这个非常简单的例子中，你可能“看到”*w*[0] = 1、*w*[1] = 1、*b* = −1 可以解这个方程，但我们需要一个通用方法）。
- en: This equation is an example of a class of equations called a *linear system*.
    A linear system in *n* unknowns *x*[1], *x*[2], *x*[3], ⋯, *x[n]*,
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程是称为线性系统的一类方程的例子。在 *n* 个未知数 *x*[1]、*x*[2]、*x*[3]、⋯、*x[n]* 中的线性系统，
- en: '*a*[11]*x*[1] + *a*[12]*x*[2] + *a*[13]*x*[3] + … + *a*[1*n*]*x[n]* = *b*[1]'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '*a*[11]*x*[1] + *a*[12]*x*[2] + *a*[13]*x*[3] + … + *a*[1*n*]*x[n]* = *b*[1]'
- en: '*a*[21]*x*[1] + *a*[22]*x*[2] + *a*[23]*x*[3] + … + *a*[2*n*]*x[n]* = *b*[2]'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '*a*[21]*x*[1] + *a*[22]*x*[2] + *a*[23]*x*[3] + … + *a*[2*n*]*x[n]* = *b*[2]'
- en: ⁞
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: ⁞
- en: '*a*[*n*1]*x*[1] + *a*[*n*2]*x*[2] + *a*[*n*3]*x*[3] + … + *a[nn]**x[n]* = *b[n]*'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '*a*[*n*1]*x*[1] + *a*[*n*2]*x*[2] + *a*[*n*3]*x*[3] + … + *a[nn]**x[n]* = *b[n]*'
- en: can be expressed via matrix and vectors as
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过矩阵和向量表示为
- en: '*A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '*A*![相关系数](../../OEBPS/Images/AR_x.png) = ![相关系数](../../OEBPS/Images/AR_b.png)'
- en: where
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '![](../../OEBPS/Images/eq_02-15-e.png)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
  zh: '![方程图](../../OEBPS/Images/eq_02-15-e.png)'
- en: 'Although equivalent, the matrix depiction is more compact and dimension-independent.
    In machine learning, we usually have many variables (thousands), so this compactness
    makes a significant difference. Also, *A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)
    looks similar to the one-variable equation we know so well: *ax* = *b*. In fact,
    many intuitions can be transferred from 1D to higher dimensions.'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然等价，但矩阵表示更紧凑且与维度无关。在机器学习中，我们通常有很多变量（数千个），这种紧凑性产生了显著差异。此外，*A*![](../../OEBPS/Images/AR_x.png)
    = ![](../../OEBPS/Images/AR_b.png) 看起来与我们非常熟悉的单变量方程相似：*ax* = *b*。事实上，许多直觉可以从一维转移到高维。
- en: 'What is the solution to the 1D equation? You may have learned it in fifth grade:
    The solution of *ax* = *b* is *x* = *a*^(−1)*b* where *a*^(−1) = 1/*a*, *a* ≠
    0.'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 一维方程的解是什么？你可能在上五年级时就学过了：*ax* = *b* 的解是 *x* = *a*^(−1)*b*，其中 *a*^(−1) = 1/*a*，*a*
    ≠ 0。
- en: 'We can use the same notation in all dimensions. The solution of *A*![](../../OEBPS/Images/AR_x.png)
    = ![](../../OEBPS/Images/AR_b.png) is ![](../../OEBPS/Images/AR_x.png) = *A*^(−1)![](../../OEBPS/Images/AR_b.png),
    where *A*^(−1) is the matrix inverse. The inverse matrix *A*^(−1) has the determinant
    of the matrix, 1/*det*(*A*), as a factor. We will not discuss determinant and
    inverse matrix computation here—you can obtain that in any standard linear algebra
    textbook—but will state some facts that lend insights into determinants and inverse
    matrices:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在所有维度中使用相同的符号。*A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)
    的解是 ![](../../OEBPS/Images/AR_x.png) = *A*^(−1)![](../../OEBPS/Images/AR_b.png)，其中
    *A*^(−1) 是矩阵的逆。逆矩阵 *A*^(−1) 有矩阵的行列式，1/*det*(*A*), 作为因子。我们不会在这里讨论行列式和逆矩阵的计算——你可以在任何标准的线性代数教科书中找到这些——但会陈述一些有助于理解行列式和逆矩阵的事实：
- en: The inverse matrix *A*^(−1) is related to matrix *A* in the same way the scalar
    *a*^(−1) is related to the scalar *a*. *a*^(−1) exists if and only if *a* ≠ 0.
    Analogously, *A*^(−1) exists if *det*(*A*) ≠ 0, where *det*(*A*) refers to the
    determinant of a matrix.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逆矩阵 *A*^(−1) 与矩阵 *A* 的关系与标量 *a*^(−1) 与标量 *a* 的关系相同。*a*^(−1) 存在当且仅当 *a* ≠ 0。类似地，*A*^(−1)
    存在当 *det*(*A*) ≠ 0，其中 *det*(*A*) 指的是矩阵的行列式。
- en: 'The product of a scalar *a* and its inverse *a*^(−1) is 1. Analogously, *AA*^(−1)
    = *A*^(−1)*A* = **I**, where **I** denotes the identity matrix that is the higher-dimension
    analog for 1 in scalar arithmetic. It is a matrix in which the diagonal terms
    are 1 and all other terms are 0. The *n*-dimensional identity matrix is as follows:'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标量 *a* 和其逆 *a*^(−1) 的乘积是 1。类似地，*AA*^(−1) = *A*^(−1)*A* = **I**，其中 **I** 表示单位矩阵，它是标量算术中
    1 的高维类似物。它是一个对角线项为 1，其他项为 0 的矩阵。*n*-维单位矩阵如下：
- en: '![](../../OEBPS/Images/eq_02-15-f.png)'
  id: totrans-419
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-15-f.png)'
- en: When there is no subscript, the dimensionality can be inferred from the context.
    For any matrix *A*, **I***A* = *A***I** = *A*. For any vector ![](../../OEBPS/Images/AR_a.png),
    **I**![](../../OEBPS/Images/AR_a.png) = ![](../../OEBPS/Images/AR_a.png)*^T***I**
    = ![](../../OEBPS/Images/AR_a.png). These can be easily verified using the rules
    of matrix multiplication.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 当没有下标时，可以从上下文中推断出维度。对于任何矩阵 *A*，**I***A* = *A***I** = *A*。对于任何向量 ![](../../OEBPS/Images/AR_a.png)，**I**![](../../OEBPS/Images/AR_a.png)
    = ![](../../OEBPS/Images/AR_a.png)*^T***I** = ![](../../OEBPS/Images/AR_a.png)。这些可以通过矩阵乘法的规则轻松验证。
- en: There are completely precise but tedious rules for computing determinants and
    matrix inverses. Despite the importance of the concept, we rarely need to compute
    them in life as all linear algebra software packages provide routines to do this.
    Furthermore, computing matrix inverses is not good programming practice because
    it is numerically unstable. We will not discuss the direct computation of determinant
    or matrix inverse here (except that in section [A.2](../Text/A.xhtml#sec-det2x2)
    of the appendix, we show how to compute the determinant of a 2 × 2 matrix). We
    will discuss pseudo-inverses, which have more significance in machine learning.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 计算行列式和矩阵逆有完全精确但繁琐的规则。尽管这个概念很重要，但在生活中我们很少需要计算它们，因为所有线性代数软件包都提供了执行此操作的例程。此外，计算矩阵逆不是好的编程实践，因为它在数值上是不稳定的。我们不会在这里讨论行列式或矩阵逆的直接计算（除了附录
    [A.2](../Text/A.xhtml#sec-det2x2) 中，我们展示了如何计算 2 × 2 矩阵的行列式）。我们将讨论伪逆，这在机器学习中具有更重要的意义。
- en: 2.12.1 Linear systems with zero or near-zero determinants,and ill-conditioned
    systems
  id: totrans-422
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.12.1 行列式为零或接近零的线性系统，以及病态系统
- en: We saw earlier that a linear system *A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)
    has the solution ![](../../OEBPS/Images/AR_x.png) = *A*^(−1)![](../../OEBPS/Images/AR_b.png).
    But *A*^(−1) has 1/*det*(*A*) as a factor. What if the determinant is zero?
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看到，线性系统 *A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)
    的解是 ![](../../OEBPS/Images/AR_x.png) = *A*^(−1)![](../../OEBPS/Images/AR_b.png)。但
    *A*^(−1) 有 1/*det*(*A*) 作为因子。如果行列式为零怎么办？
- en: 'The short answer: when the determinant is zero, the linear system cannot be
    exactly solved. We may still attempt to come up with an approximate answer (see
    section [2.12.3](02.xhtml#subsec-over-under-determined-linsys)), but an exact
    solution is not possible.'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 简短的回答：当行列式为零时，线性系统无法精确求解。我们仍然可以尝试找到一个近似答案（见第 [2.12.3](02.xhtml#subsec-over-under-determined-linsys)
    节），但精确解是不可能的。
- en: 'Let’s examine the situation a bit more closely with the aid of an example.
    Consider the following system of equations:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们借助一个例子更仔细地考察一下这种情况。考虑以下方程组：
- en: '*  x*[1] + *x*[2]   = 2'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '*  x*[1] + *x*[2]   = 2'
- en: 2*x*[1] + 2*x*[2]  = 4
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 2*x*[1] + 2*x*[2]  = 4
- en: 'It can be rewritten as a linear system with a square matrix:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以重写为一个具有方阵的线性系统：
- en: '![](../../OEBPS/Images/eq_02-15-g.png)'
  id: totrans-429
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-15-g.png)'
- en: 'But you can quickly see that the system of equations cannot be solved. The
    second equation is really the same as the first. In fact, we can obtain the second
    by multiplying the first by a scalar, 2. Hence, we don’t really have two equations:
    we have only one, so the system cannot be solved. Now examine the row vectors
    of matrix *A*. They are [1   1] and [2   2]. They are linearly dependent because
    −2[1   1] + [2   2] = 0. Now examine the determinant of matrix *A* (section [A.2](../Text/A.xhtml#sec-det2x2)
    of the appendix shows how to compute the determinant of a 2 × 2 matrix). It is
    2 × 1 − 1 × 2 = 0. These results are not coincidences. Any one of them implies
    the other. In fact, the following statements about the linear system *A*![](../../OEBPS/Images/AR_x.png)
    = ![](../../OEBPS/Images/AR_b.png) (with a square matrix) are equivalent:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 但你可以很快看出，这个方程组无法求解。第二个方程实际上与第一个方程相同。事实上，我们可以通过将第一个方程乘以一个标量，2，来得到第二个方程。因此，我们实际上只有一个方程，而不是两个，所以这个方程组无法求解。现在考察矩阵
    *A* 的行向量。它们是 [1   1] 和 [2   2]。它们是线性相关的，因为 −2[1   1] + [2   2] = 0。现在考察矩阵 *A*
    的行列式（附录 [A.2](../Text/A.xhtml#sec-det2x2) 展示了如何计算 2 × 2 矩阵的行列式）。它是 2 × 1 − 1 ×
    2 = 0。这些结果不是巧合。任何一个都意味着另一个。事实上，以下关于线性系统 *A*![](../../OEBPS/Images/AR_x.png) =
    ![](../../OEBPS/Images/AR_b.png)（对于方阵）的陈述是等价的：
- en: Matrix *A* has a row/column that can be expressed as a weighted sum of the others.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵 *A* 有一个行/列可以表示为其他行的加权求和。
- en: Matrix *A* has linearly dependent rows or columns.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵 *A* 的行或列线性相关。
- en: Matrix *A* has zero determinant (such matrices are called *singular* matrices).
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵 *A* 的行列式为零（这样的矩阵被称为 *奇异矩阵*）。
- en: The inverse of matrix *A* (i.e., *A*^(−1)) does not exist. *A* is called *singular*.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵 *A* 的逆（即 *A*^(−1)）不存在。*A* 被称为 *奇异的*。
- en: The linear system cannot be solved.
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个线性系统无法求解。
- en: The system is trying to tell you that you have fewer equations than you think
    you have, and you cannot solve the system of equations.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 系统试图告诉你，你拥有的方程比你想的少，你无法求解这个方程组。
- en: 'Sometimes the determinant is not exactly zero but close to zero. Although solvable
    in theory, such systems are *numerically unstable*. Small changes in input cause
    the result to change drastically. For instance, consider this nearly singular
    matrix:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 有时行列式不是正好为零，而是接近于零。虽然理论上可以求解，但这样的系统是 *数值不稳定的*。输入的微小变化会导致结果发生剧烈变化。例如，考虑这个几乎奇异的矩阵：
- en: '![](../../OEBPS/Images/eq_02-16.png)'
  id: totrans-438
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-16.png)'
- en: Equation 2.16
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2.16
- en: Its determinant is 0.002, close to zero. Let ![](../../OEBPS/Images/eq_02-16-a.png)
    be a vector.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 它的行列式是 0.002，接近于零。设 ![](../../OEBPS/Images/eq_02-16-a.png) 为一个向量。
- en: '![](../../OEBPS/Images/eq_02-17.png)'
  id: totrans-441
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-17.png)'
- en: Equation 2.17
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2.17
- en: (Note how large the elements of *A*^(−1) are. This is due to division by an
    extremely small determinant and, in turn, causes the instability illustrated next.)
    The solution to the equation ![](../../OEBPS/Images/eq_02-17-a.png). But if we
    change ![](../../OEBPS/Images/AR_b.png) just a little and make ![](../../OEBPS/Images/eq_02-17-b.png),
    the solution changes to a drastically different ![](../../OEBPS/Images/eq_02-17-c.png).
    This is inherently unstable and arises from the near singularity of the matrix
    *A*. Such linear systems are called *ill-conditioned*.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: （注意* A^(-1) *的元素有多大。这是由于除以一个非常小的行列式，进而导致下文所示的不稳定性。）方程的解为 ![](../../OEBPS/Images/eq_02-17-a.png)。但如果我们将![](../../OEBPS/Images/AR_b.png)稍微改变一下，使其变为![](../../OEBPS/Images/eq_02-17-b.png)，解就会变成一个截然不同的![](../../OEBPS/Images/eq_02-17-c.png)。这是固有的不稳定性，并源于矩阵*
    A *的近奇异性。这样的线性系统被称为*病态的*。
- en: 2.12.2 PyTorch code for inverse, determinant, and singularity testing of matrices
  id: totrans-444
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.12.2 矩阵的逆、行列式和奇异性测试的PyTorch代码
- en: Inverting a matrix and computing its determinant can be done with a single function
    call from the linear algebra package linalg.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 通过线性代数包linalg的单个函数调用即可求逆矩阵和计算行列式。
- en: Listing 2.10 Matrix inverse for an invertible matrix (nonzero determinant)
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.10 可逆矩阵的矩阵逆（非零行列式）
- en: '[PRE9]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ① ![](../../OEBPS/Images/eq_02-17-d.png)
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: ① ![](../../OEBPS/Images/eq_02-17-d.png)
- en: ② ![](../../OEBPS/Images/eq_02-17-e.png)
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: ② ![](../../OEBPS/Images/eq_02-17-e.png)
- en: ③ The PyTorch function *torch*.*eye*(*n*) generates an identity matrix *I* of
    size *n*
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: ③ PyTorch函数*torch.eye(* n *)生成一个大小为* n *的单位矩阵* I *
- en: ④ Verify ![](../../OEBPS/Images/eq_02-17-f.png)
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 验证![](../../OEBPS/Images/eq_02-17-f.png)
- en: ⑤ **I** is like 1. Verify *A***I** = **I***A* = *A*
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ ** I **就像1。验证*A**I = **I**A = * A *
- en: A singular matrix is a matrix whose determinant is zero. Such matrices are non-invertible.
    Linear systems of equations with singular matrices cannot be solved.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 奇异矩阵是指行列式为零的矩阵。这样的矩阵是不可逆的。具有奇异矩阵的线性方程组无法求解。
- en: Listing 2.11 Singular matrix
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.11 奇异矩阵
- en: '[PRE10]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ① ![](../../OEBPS/Images/eq_02-17-g.png)
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: ① ![](../../OEBPS/Images/eq_02-17-g.png)
- en: ② Determinant = 1 × 2 − 2 × 1 = 0. Singular matrix; attempting to compute the
    inverse causes a runtime error.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: ② 行列式 = 1 × 2 - 2 × 1 = 0。奇异矩阵；尝试计算逆矩阵会导致运行时错误。
- en: 2.12.3 Over- and under-determined linear systems in machine learning
  id: totrans-458
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.12.3 机器学习中的超定和欠定线性系统
- en: 'What if the matrix *A* is *not* square? This implies that the number of equations
    does not match the number of unknowns. Does such a system even make sense? Surprisingly,
    it does. As a rule, machine learning systems fall in this category: the number
    of equations corresponds to the number of training data instances collected, while
    the number of unknowns is a function of the number of weights in the model which
    is a function of the particular model family chosen to represent the system. These
    are independent of each other. As stated earlier, we often solve these systems
    iteratively. Nonetheless, it is important to understand linear systems with nonsquare
    matrices *A*, to gain insight.'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 如果矩阵* A *不是方阵呢？这意味着方程的数量不等于未知数的数量。这样的系统有意义吗？出人意料的是，它是有意义的。一般来说，机器学习系统属于这一类：方程的数量对应于收集的训练数据实例的数量，而未知数的数量是模型中权重的函数，这是特定模型家族选择来表示系统的函数。这些是相互独立的。如前所述，我们通常迭代地解决这些系统。然而，了解非方阵的线性系统*
    A *是很重要的，以获得洞察。
- en: 'There are two possible cases, assuming that the matrix *A* is *m* × *n* (*m*
    rows and *n* columns):'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 假设矩阵* A *是* m * × * n *（* m *行和* n *列），有两种可能的情况：
- en: 'Case 1: *m* > *n* (more equations than unknowns; overdetermined system)'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情况1：* m * > * n *（方程多于未知数；超定系统）
- en: 'Case 2: *m* < *n* (fewer equations than unknown; underdetermined system)'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情况2：* m * < * n *（方程少于未知数；欠定系统）
- en: 'For instance, table [2.2](02.xhtml#tab-cat-brain-training-data) leads to an
    overdetermined linear system. Let’s write the system of equations:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，表[2.2](02.xhtml#tab-cat-brain-training-data)导致一个超定线性系统。让我们写下方程组：
- en: 0.11 *w*[0] + 0.09 *w*[1] + b = –0.8
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 0.11 * w*[0] + 0.09 * w*[1] + b = –0.8
- en: 0.01 *w*[0] + 0.02 *w*[1] + b = –0.97
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 0.01 * w*[0] + 0.02 * w*[1] + b = –0.97
- en: 0.98 *w*[0] + 0.91 *w*[1] + b =   0.89
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 0.98 * w*[0] + 0.91 * w*[1] + b = 0.89
- en: 0.12 *w*[0] + 0.21 *w*[1] + b = –0.68
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 0.12 * w*[0] + 0.21 * w*[1] + b = –0.68
- en: 0.98 *w*[0] + 0.99 *w*[1] + b =   0.95
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 0.98 * w*[0] + 0.99 * w*[1] + b = 0.95
- en: 0.85 *w*[0] + 0.87 *w*[1] + b =   0.74
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 0.85 * w*[0] + 0.87 * w*[1] + b = 0.74
- en: 0.03 *w*[0] + 0.14 *w*[1] + b = –0.88
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 0.03 * w*[0] + 0.14 * w*[1] + b = –0.88
- en: 0.55 *w*[0] + 0.45 *w*[1] + b =   0.00
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 0.55 * w*[0] + 0.45 * w*[1] + b = 0.00
- en: 'These yield the following overdetermined linear system:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下超定线性系统：
- en: '![](../../OEBPS/Images/eq_02-18.png)'
  id: totrans-473
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-18.png)'
- en: Equation 2.18
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 2.18
- en: 'This is a nonsquare 15 × 3 linear system. There are only 3 unknowns to solve
    for (*w*[0], *w*[1], *b*), and there are 15 equations. This is highly redundant:
    we needed only three equations and could have solved it via linear system solution
    techniques (section [2.12](02.xhtml#sec-lin_systems)). But the important thing
    to note is this: *the equations are not fully consistent*. There is no single
    set of values for the unknown that will satisfy all of them. In other words, the
    training data is noisy—an almost universal occurrence in real-life machine learning
    systems. Consequently, we have to find a solution that is optimal (causes as little
    error as possible) over all the equations.'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非方阵的 15 × 3 线性系统。只有 3 个未知数需要求解 (*w*[0], *w*[1], *b*)，而方程有 15 个。这非常冗余：我们只需要三个方程，就可以通过线性系统求解技术（第
    [2.12](02.xhtml#sec-lin_systems) 节）来解决这个问题。但重要的是要注意这一点：*方程并不完全一致*。没有一组单一的未知数值可以满足所有方程。换句话说，训练数据是嘈杂的——这是现实生活中的机器学习系统的一个几乎普遍现象。因此，我们必须找到一个解决方案，在整个方程中都是最优的（尽可能减少错误）。
- en: We want to solve it such that the overall error ||*A*![](../../OEBPS/Images/AR_x.png)
    − ![](../../OEBPS/Images/AR_b.png)|| is minimized. In other words, we are looking
    for ![](../../OEBPS/Images/AR_x.png) such that *A*![](../../OEBPS/Images/AR_x.png)
    is as close to ![](../../OEBPS/Images/AR_b.png) as possible. This closed-form
    that is, non-iterative) method is an extremely important precursor to machine
    learning and data science. We will revisit this multiple times, most notably in
    sections [2.12.4](02.xhtml#subsec-moore-penrose-pseudoinverse) and [4.5](../Text/04.xhtml#sec-svd).
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望求解它，使得整体误差 ||*A*![](../../OEBPS/Images/AR_x.png) − ![](../../OEBPS/Images/AR_b.png)||
    最小化。换句话说，我们正在寻找 ![](../../OEBPS/Images/AR_x.png)，使得 *A*![](../../OEBPS/Images/AR_x.png)
    尽可能接近 ![](../../OEBPS/Images/AR_b.png)。这种闭式（即非迭代）方法是机器学习和数据科学的一个极其重要的先导。我们将在多个地方重新讨论这个问题，特别是在第
    [2.12.4](02.xhtml#subsec-moore-penrose-pseudoinverse) 节和 [4.5](../Text/04.xhtml#sec-svd)
    节。
- en: 2.12.4 Moore Penrose pseudo-inverse of a matrix
  id: totrans-477
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.12.4 矩阵的摩尔-彭若斯伪逆
- en: 'The pseudo-inverse is a handy technique to solve over- or under-determined
    linear systems. Suppose we have an overdetermined system with the not-necessarily
    square *m* × *n* matrix *A*:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 伪逆是一种解决超定或欠定线性系统的便捷技术。假设我们有一个超定系统，其非必要方阵 *m* × *n* 矩阵 *A*：
- en: '*A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: '*A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)'
- en: 'Since *A* is not guaranteed to be square, we can take neither the determinant
    nor the inverse in general. So the usual *A*^(−1)![](../../OEBPS/Images/AR_b.png)
    does not work. At this point, we observe that although the inverse cannot be taken,
    transposing the matrix is always possible. Let’s multiply both sides of the equation
    with *A^T*:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 *A* 不保证是方阵，我们通常不能取其行列式或逆。所以，通常的 *A*^(−1)![](../../OEBPS/Images/AR_b.png)
    是不工作的。在这个时候，我们注意到，尽管不能取逆，但矩阵转置总是可能的。让我们将方程的两边乘以 *A^T*：
- en: '*A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png) ⇔ *A^TA*![](../../OEBPS/Images/AR_x.png)
    = *A^T*![](../../OEBPS/Images/AR_b.png)'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '*A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png) ⇔ *A^TA*![](../../OEBPS/Images/AR_x.png)
    = *A^T*![](../../OEBPS/Images/AR_b.png)'
- en: 'Notice that *A^TA* is a square matrix: its dimensions are (*m*×*n*) × (*n*×*m*)
    = *m* × *m*. Let’s assume, without proof for the moment, that it is invertible.
    Then'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 *A^TA* 是一个方阵：其维度是 (*m*×*n*) × (*n*×*m*) = *m* × *m*。暂时不进行证明，我们假设它是可逆的。那么
- en: '*A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png) ⇔ *A^TA*![](../../OEBPS/Images/AR_x.png)
    = *A^T*![](../../OEBPS/Images/AR_b.png) ⇔ ![](../../OEBPS/Images/AR_x.png) = (*A^TA*)^(−1)*A^T*![](../../OEBPS/Images/AR_b.png)'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: '*A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png) ⇔ *A^TA*![](../../OEBPS/Images/AR_x.png)
    = *A^T*![](../../OEBPS/Images/AR_b.png) ⇔ ![](../../OEBPS/Images/AR_x.png) = (*A^TA*)^(−1)*A^T*![](../../OEBPS/Images/AR_b.png)'
- en: Hmmm, not bad; we seem to be onto something. In fact, we just derived the *pseudo-inverse*
    of matrix *A*, denoted *A* ^+ = (*A^TA*)^(−1)*A^T*. Unlike the inverse, the pseudo-inverse
    does not need the matrix to be square with linearly independent rows. Much like
    the regular linear system, we get the solution of the (possibly nonsquare) system
    of equations as *A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)
    ⇔ ![](../../OEBPS/Images/AR_x.png) = *A* ^+ ![](../../OEBPS/Images/AR_b.png).
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯嗯，还不错；我们似乎找到了一些东西。事实上，我们刚刚推导出了矩阵 *A* 的 *伪逆*，表示为 *A* ^+ = (*A^TA*)^(−1)*A^T*。与逆矩阵不同，伪逆不需要矩阵是方阵且行线性无关。与常规线性系统类似，我们得到（可能非方阵）方程组的解为
    *A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png) ⇔ ![](../../OEBPS/Images/AR_x.png)
    = *A* ^+ ![](../../OEBPS/Images/AR_b.png)。
- en: The pseudo-inverse-based solution actually minimizes the error ||*A*![](../../OEBPS/Images/AR_x.png)
    − ![](../../OEBPS/Images/AR_b.png)||. We will provide an intuitive proof of that
    in section [2.12.5](02.xhtml#subsec-pseudo-inv-geometric-intuition). Meanwhile,
    you are encouraged to write the Python code to evaluate (*A^TA*)^(−1)*A^T*![](../../OEBPS/Images/AR_b.png)
    and verify that it approximately yields the expected answer ![](../../OEBPS/Images/eq_02-18-a.png)
    for equation [2.18](02.xhtml#eq-overdetemined_system_example).
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 基于伪逆的解实际上最小化了误差 ||*A*![](../../OEBPS/Images/AR_x.png) − ![](../../OEBPS/Images/AR_b.png)||。我们将在第
    [2.12.5](02.xhtml#subsec-pseudo-inv-geometric-intuition) 节中提供一个直观的证明。同时，鼓励你编写
    Python 代码来评估 (*A^TA*)^(−1)*A^T*![](../../OEBPS/Images/AR_b.png) 并验证它大约给出了方程 [2.18](02.xhtml#eq-overdetemined_system_example)
    预期的答案 ![](../../OEBPS/Images/eq_02-18-a.png)。
- en: '2.12.5 Pseudo-inverse of a matrix: A beautiful geometric intuition'
  id: totrans-486
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.12.5 矩阵的伪逆：一种美丽的几何直觉
- en: A matrix *A*[*m* × *n*] can be rewritten in terms of its column vectors as [![](../../OEBPS/Images/AR_a.png)[1],
    ![](../../OEBPS/Images/AR_a.png)[2], … ![](../../OEBPS/Images/AR_a.png)*[n]*],
    where ![](../../OEBPS/Images/AR_a.png)[1] … ![](../../OEBPS/Images/AR_a.png)*[n]*
    are all *m*-dimensional vectors. Then if ![](../../OEBPS/Images/eq_02-18-b.png),
    we get *A*![](../../OEBPS/Images/AR_x.png) = *x*[1]![](../../OEBPS/Images/AR_a.png)[1]
    + *x*[2]![](../../OEBPS/Images/AR_a.png)[2] + ⋯ *x[n]*![](../../OEBPS/Images/AR_a.png)*[n]*.
    In other words, *A*![](../../OEBPS/Images/AR_x.png) is just a linear combination
    of the column vectors of *A* with the elements of ![](../../OEBPS/Images/AR_x.png)
    as the weights (you are encouraged to write out a small 3 × 3 system and verify
    this). The space of all vectors of the form *A*![](../../OEBPS/Images/AR_x.png)
    (that is, the linear span of the column vectors of *A*) is known as the *column
    space* of *A*.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 *A*[*m* × *n*] 可以用其列向量表示为 [![](../../OEBPS/Images/AR_a.png)[1], ![](../../OEBPS/Images/AR_a.png)[2],
    … ![](../../OEBPS/Images/AR_a.png)*[n]*]，其中 ![](../../OEBPS/Images/AR_a.png)[1]
    … ![](../../OEBPS/Images/AR_a.png)*[n]* 都是 *m*-维向量。然后如果 ![](../../OEBPS/Images/eq_02-18-b.png)，我们得到
    *A*![](../../OEBPS/Images/AR_x.png) = *x*[1]![](../../OEBPS/Images/AR_a.png)[1]
    + *x*[2]![](../../OEBPS/Images/AR_a.png)[2] + ⋯ *x[n]*![](../../OEBPS/Images/AR_a.png)*[n]*。换句话说，*A*![](../../OEBPS/Images/AR_x.png)
    只是 *A* 的列向量的线性组合，其中 ![](../../OEBPS/Images/AR_x.png) 的元素作为权重（鼓励你写出一个小的 3 × 3 系统并验证这一点）。所有形式为
    *A*![](../../OEBPS/Images/AR_x.png) 的向量空间（即 *A* 的列向量的线性空间）被称为 *A* 的 *列空间*。
- en: 'The solution to the linear system of equations *A*![](../../OEBPS/Images/AR_x.png)
    = ![](../../OEBPS/Images/AR_b.png) can be viewed as finding the ![](../../OEBPS/Images/AR_x.png)
    that minimizes the difference of *A*![](../../OEBPS/Images/AR_x.png) and ![](../../OEBPS/Images/AR_b.png):
    that is, minimizes ||*A*![](../../OEBPS/Images/AR_x.png) − ![](../../OEBPS/Images/AR_b.png)||.
    This means we are trying to find a point in the column space of *A* that is closest
    to the point ![](../../OEBPS/Images/AR_b.png). Note that this interpretation does
    not assume a square matrix *A*. Nor does it assume a nonzero determinant. In the
    friendly case where the matrix *A* is square and invertible, we can find a vector
    ![](../../OEBPS/Images/AR_x.png) such that *A*![](../../OEBPS/Images/AR_x.png)
    becomes exactly equal to ![](../../OEBPS/Images/AR_b.png), which makes ||*A*![](../../OEBPS/Images/AR_x.png)
    − ![](../../OEBPS/Images/AR_b.png)|| = 0. If *A* is not square, we will try to
    find ![](../../OEBPS/Images/AR_x.png) such that *A*![](../../OEBPS/Images/AR_x.png)
    is closer to ![](../../OEBPS/Images/AR_b.png) than any other vector in the column
    space of *A*. Mathematically speaking, [⁴](02.xhtml#fn7)'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 线性方程组 *A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)
    的解可以看作是寻找 ![](../../OEBPS/Images/AR_x.png)，使其最小化 *A*![](../../OEBPS/Images/AR_x.png)
    和 ![](../../OEBPS/Images/AR_b.png) 之间的差异：即最小化 ||*A*![](../../OEBPS/Images/AR_x.png)
    − ![](../../OEBPS/Images/AR_b.png)||。这意味着我们正在尝试找到一个点，该点在 *A* 的列空间中，并且离点 ![](../../OEBPS/Images/AR_b.png)
    最近的。请注意，这种解释并不假设矩阵 *A* 是方阵。它也不假设行列式不为零。在友好的情况下，当矩阵 *A* 是方阵且可逆时，我们可以找到一个向量 ![](../../OEBPS/Images/AR_x.png)，使得
    *A*![](../../OEBPS/Images/AR_x.png) 完全等于 ![](../../OEBPS/Images/AR_b.png)，这使得
    ||*A*![](../../OEBPS/Images/AR_x.png) − ![](../../OEBPS/Images/AR_b.png)|| = 0。如果
    *A* 不是方阵，我们将尝试找到一个 ![](../../OEBPS/Images/AR_x.png)，使得 *A*![](../../OEBPS/Images/AR_x.png)
    比任何其他在 *A* 的列空间中的向量更接近 ![](../../OEBPS/Images/AR_b.png)。从数学上讲，[⁴](02.xhtml#fn7)
- en: '![](../../OEBPS/Images/eq_02-19.png)'
  id: totrans-489
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 02-19](../../OEBPS/Images/eq_02-19.png)'
- en: Equation 2.19
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2.19
- en: From geometry, we intuitively know that the closest point to ![](../../OEBPS/Images/AR_b.png)
    in the column space of *A* is obtained by dropping a perpendicular from ![](../../OEBPS/Images/AR_b.png)
    to the column space of *A* (see figure [2.12](02.xhtml#fig-nonsquare_linear_system_diagram)).
    The point where this perpendicular meets the column space is called the *projection*
    of ![](../../OEBPS/Images/AR_b.png) on the column space of *A*. The solution vector
    ![](../../OEBPS/Images/AR_x.png) to equation [2.19](02.xhtml#eq-linear_system_minimization_problem)
    that we are looking for should correspond to the projection of ![](../../OEBPS/Images/AR_b.png)
    on the column space of *A*. This in turn means ![](../../OEBPS/Images/AR_b.png)
    − *A*![](../../OEBPS/Images/AR_x.png) is orthogonal (perpendicular) to all vectors
    in the column space of *A* (see figure [2.12](02.xhtml#fig-nonsquare_linear_system_diagram)).
    We represent arbitrary vectors in the column space of *A* as *A*![](../../OEBPS/Images/AR_y.png)
    for arbitrary ![](../../OEBPS/Images/AR_y.png). Hence, for all such ![](../../OEBPS/Images/AR_y.png),
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 从几何学上，我们直观地知道，在 *A* 的列空间中，离 ![](../../OEBPS/Images/AR_b.png) 最近的点是通过对 ![](../../OEBPS/Images/AR_b.png)
    从垂直于 *A* 的列空间（见图 [2.12](02.xhtml#fig-nonsquare_linear_system_diagram)）进行投影得到的。这个垂直线与列空间的交点被称为
    ![](../../OEBPS/Images/AR_b.png) 在 *A* 的列空间上的 *投影*。我们正在寻找的方程 [2.19](02.xhtml#eq-linear_system_minimization_problem)
    的解向量 ![](../../OEBPS/Images/AR_x.png) 应该对应于 ![](../../OEBPS/Images/AR_b.png) 在
    *A* 的列空间上的投影。这反过来意味着 ![](../../OEBPS/Images/AR_b.png) − *A*![](../../OEBPS/Images/AR_x.png)
    与 *A* 的列空间中的所有向量正交（垂直）。我们用 *A*![](../../OEBPS/Images/AR_y.png) 表示 *A* 的列空间中的任意向量，对于任意的
    ![](../../OEBPS/Images/AR_y.png)。因此，对于所有这样的 ![](../../OEBPS/Images/AR_y.png)，
- en: '![](../../OEBPS/Images/eq_02-19-a.png)'
  id: totrans-492
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 02-19-a](../../OEBPS/Images/eq_02-19-a.png)'
- en: For the previous equation to be true for all vectors ![](../../OEBPS/Images/AR_y.png),
    we must have *A^T*(![](../../OEBPS/Images/AR_b.png)−*A*![](../../OEBPS/Images/AR_x.png))
    = 0.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使前一个方程对所有向量 ![](../../OEBPS/Images/AR_y.png) 都成立，我们必须有 *A^T*(![](../../OEBPS/Images/AR_b.png)−*A*![](../../OEBPS/Images/AR_x.png))
    = 0。
- en: Thus, we have
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有
- en: '![](../../OEBPS/Images/eq_02-19-b.png)'
  id: totrans-495
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 02-19-b](../../OEBPS/Images/eq_02-19-b.png)'
- en: which is exactly the Moore-Penrose pseudo-inverse.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是摩尔-彭罗斯伪逆。
- en: '![](../../OEBPS/Images/CH02_F12_Chaudhury.png)'
  id: totrans-497
  prefs: []
  type: TYPE_IMG
  zh: '![第二章图 12](../../OEBPS/Images/CH02_F12_Chaudhury.png)'
- en: Figure 2.12 Solving a linear system *A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)
    is equivalent to finding the point on the column space of *A* that is closest
    to ![](../../OEBPS/Images/AR_b.png). This means we have to drop a perpendicular
    from ![](../../OEBPS/Images/AR_b.png) to column space of *A*. If *A*![](../../OEBPS/Images/AR_x.png)
    represents the point where that perpendicular meets the column space (aka projection),
    the difference vector ![](../../OEBPS/Images/AR_b.png) − *A*![](../../OEBPS/Images/AR_x.png)
    corresponds to the line joining ![](../../OEBPS/Images/AR_b.png) and its projection
    *A*![](../../OEBPS/Images/AR_x.png). This line will be perpendicular to all vectors
    in the column space of *A*. Equivalently, it is perpendicular to *A*![](../../OEBPS/Images/AR_y.png)
    for any arbitrary ![](../../OEBPS/Images/AR_y.png).
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.12 解线性系统 *A*![图片](../../OEBPS/Images/AR_x.png) = ![图片](../../OEBPS/Images/AR_b.png)
    等价于找到 *A* 的列空间中与 ![图片](../../OEBPS/Images/AR_b.png) 最近的点。这意味着我们必须从 ![图片](../../OEBPS/Images/AR_b.png)
    向 *A* 的列空间引一条垂线。如果 *A*![图片](../../OEBPS/Images/AR_x.png) 表示该垂线与列空间的交点（即投影），则差向量
    ![图片](../../OEBPS/Images/AR_b.png) − *A*![图片](../../OEBPS/Images/AR_x.png) 对应于连接
    ![图片](../../OEBPS/Images/AR_b.png) 和其投影 *A*![图片](../../OEBPS/Images/AR_x.png)
    的线。这条线将与 *A* 的列空间中的所有向量垂直。等价地，它对于任意 ![图片](../../OEBPS/Images/AR_y.png) 都与 *A*![图片](../../OEBPS/Images/AR_y.png)
    垂直。
- en: For a machine learning-centric example, consider the overdetermined system corresponding
    to the cat brain earlier in the chapter. There are 15 training examples, each
    with input and desired outputs specified.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个以机器学习为中心的示例，考虑本章前面提到的猫脑对应超定系统。有15个训练示例，每个示例都指定了输入和期望输出。
- en: Our goal is to determine three unknowns *w*[0], *w*[1], and *b* such that for
    each training input ![](../../OEBPS/Images/eq_02-19-c.png), the model output
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是确定三个未知数 *w*[0]、*w*[1] 和 *b*，使得对于每个训练输入 ![图片](../../OEBPS/Images/eq_02-19-c.png)，模型输出
- en: '![](../../OEBPS/Images/eq_02-20.png)'
  id: totrans-501
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_02-20.png)'
- en: Equation 2.20
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2.20
- en: matches the desired output (aka ground truth) *ȳ[i]* as closely as possible.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 尽可能接近期望输出（即真实值）*ȳ[i]*。
- en: 'NOTE We employed a neat trick here: we added a 1 to the right of the input,
    which allows us to depict the entire system (including the bias) in a single compact
    matrix-vector multiplication. We call this *augmentation*—we augment the input
    row vector with an extra 1 on the right.'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在这里我们使用了一个巧妙的方法：我们在输入的右侧添加了1，这使得我们可以用一个紧凑的矩阵-向量乘法来表示整个系统（包括偏差）。我们称之为*增广*——我们在输入行向量右侧添加一个额外的1。
- en: Collating all the training examples together, we get
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有训练示例整理在一起，我们得到
- en: '![](../../OEBPS/Images/eq_02-21.png)'
  id: totrans-506
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_02-21.png)'
- en: Equation 2.21
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2.21
- en: which can be expressed compactly as
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以紧凑地表示为
- en: '*X*![](../../OEBPS/Images/AR_w.png) = ![](../../OEBPS/Images/AR_y.png)'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: '*X*![图片](../../OEBPS/Images/AR_w.png) = ![图片](../../OEBPS/Images/AR_y.png)'
- en: where *X* is the augmented input matrix with a rightmost column of all 1s. The
    goal is to minimize ||![](../../OEBPS/Images/AR_y.png) – ![](../../OEBPS/Images/AR_y2.png)||.
    To this end, we formulate the over-determined linear system
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *X* 是右侧列全为1的增广输入矩阵。目标是使 ||![图片](../../OEBPS/Images/AR_y.png) – ![图片](../../OEBPS/Images/AR_y2.png)||
    最小化。为此，我们构建了超定线性系统
- en: '*X*![](../../OEBPS/Images/AR_w.png) = ![](../../OEBPS/Images/AR_y2.png)'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: '*X*![图片](../../OEBPS/Images/AR_w.png) = ![图片](../../OEBPS/Images/AR_y2.png)'
- en: Note that *this is not a classic system of equations—it has more equations than
    unknowns*. We cannot solve this via matrix inversion. We *can*, however, use the
    pseudo-inverse mechanism to solve it. The resulting solution yields the “best
    fit” or “best effort” solution, which minimizes the total error over all the training
    examples.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，*这并不是一个经典的方程组——方程的数量多于未知数*。我们不能通过矩阵求逆来解这个方程组。然而，我们可以使用伪逆机制来解它。得到的解提供了“最佳拟合”或“最佳努力”解，它最小化了所有训练示例的总误差。
- en: The exact numerical system (repeated here for ease of reference) is
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 精确的数值系统（为方便参考在此重复）如下
- en: '![](../../OEBPS/Images/eq_02-22.png)'
  id: totrans-514
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_02-22.png)'
- en: Equation 2.22
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2.22
- en: We solve for ![](../../OEBPS/Images/AR_w.png) using the pseudo-inverse formula
    ![](../../OEBPS/Images/AR_w.png) = (*X^TX*)^(–1)*X^T*![](../../OEBPS/Images/AR_y2.png)
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用伪逆公式求解 ![图片](../../OEBPS/Images/AR_w.png) = (*X^TX*)^(–1)*X^T*![图片](../../OEBPS/Images/AR_y2.png)
- en: 2.12.6 PyTorch code to solve overdetermined systems
  id: totrans-517
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.12.6 使用 PyTorch 代码解决超定系统
- en: NOTE Fully functional code for this section, executable via Jupyter Notebook,
    can be found at [http://mng.bz/PPJ2](http://mng.bz/PPJ2).
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本节的完整功能代码，可通过 Jupyter Notebook 执行，可在[http://mng.bz/PPJ2](http://mng.bz/PPJ2)找到。
- en: Listing 2.12 Solving an overdetermined system using the pseudo-inverse
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.12 使用伪逆求解超定系统
- en: '[PRE11]'
  id: totrans-520
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ① X is the augmented data matrix from equation [2.22](02.xhtml#eq-lin-model)
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: ① X 是方程 [2.22](02.xhtml#eq-lin-model) 中的增广数据矩阵
- en: ② The Pytorch column stack operator
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: ② Pytorch 列堆叠操作符
- en: adds a column to a matrix. Here, the added column is all 1s
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 向矩阵中添加一列。在这里，添加的列全是 1
- en: '③ It is easy to verify that the solution to equation [2.22](02.xhtml#eq-lin-model)
    is roughly *w*[0] = 1, *w*[1] = 1, *b* = −1. But the equations are not consistent:
    no one solution perfectly fits all of them. The pseudo-inverse finds the “best
    fit” solution: it minimizes total error for all the equations.'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 容易验证方程 [2.22](02.xhtml#eq-lin-model) 的解大致为 *w*[0] = 1, *w*[1] = 1, *b* = −1。但方程并不一致：没有一个解能完美地适应所有这些方程。伪逆找到“最佳拟合”解：它最小化了所有方程的总误差。
- en: ④ Expect the solution to be close to [1, 1, −1]
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 预期解接近 [1, 1, −1]
- en: ⑤ The solution is [1.08, 0.90, −0.96]
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 解为 [1.08, 0.90, −0.96]
- en: '2.13 Eigenvalues and eigenvectors: Swiss Army knives of machine learning'
  id: totrans-527
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.13 特征值和特征向量：机器学习的瑞士军刀
- en: Machine learning and data science are all about finding patterns in large volumes
    of high-dimensional data. The inputs are feature vectors introduced in section
    [2.1](02.xhtml#sec-vectors)) in high-dimensional spaces. Each feature vector can
    be viewed as a point in the feature space descriptor for an input instance. Sometimes
    we transform these feature vectors—map the feature points to a friendlier space—to
    simplify the data by reducing dimensionality. This is done by eliminating axes
    along which there is scant variation in the data. Eigenvalues and eigenvectors
    are invaluable tools in the arsenal of a machine learning engineer or a data scientist
    for this purpose. In chapter [4](../Text/04.xhtml#chap-linalg-tools-ml), we will
    study how to use these tools to simplify and find broad patterns in a large volume
    of multidimensional data.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习和数据科学都是关于在大量高维数据中寻找模式。输入是第 [2.1](02.xhtml#sec-vectors) 节中介绍的高维空间中的特征向量。每个特征向量可以看作是输入实例的特征空间描述符中的一个点。有时我们会转换这些特征向量——将特征点映射到一个更友好的空间——通过降低维度来简化数据。这是通过消除数据中变化很少的轴来实现的。特征值和特征向量是机器学习工程师或数据科学家在简化数据和寻找大量多维数据中的广泛模式时的宝贵工具。在第
    [4](../Text/04.xhtml#chap-linalg-tools-ml) 章中，我们将研究如何使用这些工具来简化数据和在大量多维数据中寻找广泛模式。
- en: Let’s take an informal look at eigenvectors first. They are properties of square
    matrices. As seen earlier, matrices can be viewed as linear transforms which map
    vectors (points) in one space to different vectors (points) in the same or a different
    space. But a typical linear transform leaves a few points in the space (almost)
    unaffected. These points are called *eigenvectors*. They are important physical
    aspects of the transform. Let’s look at a simple example. Suppose we are *rotating*
    points in 3D space about the *Z*-axis (see figure [2.13](02.xhtml#fig-rotation_about_z_diagram)).
    The points on the *Z*-axis will stay where they were despite the rotation. In
    general, points on the axis of rotation (*Z* in this case) do not go anywhere
    after rotation. The axis of rotation is an eigenvector of the rotation transformation.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先非正式地看看特征向量。它们是方阵的性质。如前所述，矩阵可以看作是线性变换，它将一个空间中的向量（点）映射到相同或不同空间中的不同向量（点）。但典型的线性变换会留下一些空间中的点（几乎）不受影响。这些点被称为
    *特征向量*。它们是变换的重要物理方面。让我们看一个简单的例子。假设我们在 3D 空间中围绕 *Z*-轴（见图 [2.13](02.xhtml#fig-rotation_about_z_diagram)）旋转点。*Z*-轴上的点在旋转后仍将保持在原地。一般来说，旋转轴上的点在旋转后不会移动。旋转轴是旋转变换的特征向量。
- en: '![](../../OEBPS/Images/CH02_F13_Chaudhury.png)'
  id: totrans-530
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH02_F13_Chaudhury.png)'
- en: Figure 2.13 During rotation, points on the axis of rotation do not change position.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.13 在旋转过程中，旋转轴上的点位置不变。
- en: Extending this idea, when *transforming* vectors ![](../../OEBPS/Images/AR_x.png)
    with a matrix *A*, are there vectors that do not change, at least in direction?
    Turns out the answer is yes. These are the so-called *eigenvectors*—they do not
    change direction when undergoing linear transformation by a matrix *A*. To be
    precise, if ![](../../OEBPS/Images/AR_e.png) is an eigenvector of the square matrix
    *A*, [⁵](02.xhtml#fn8) then
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个想法扩展，当用矩阵 *A* 对向量 ![](../../OEBPS/Images/AR_x.png) 进行 *变换* 时，是否存在不改变（至少在方向上）的向量？结果是肯定的。这些就是所谓的
    *特征向量*——当通过矩阵 *A* 进行线性变换时，它们不会改变方向。为了更精确，如果 ![](../../OEBPS/Images/AR_e.png) 是方阵
    *A* 的特征向量，[⁵](02.xhtml#fn8) 则有
- en: '*A![](../../OEBPS/Images/AR_e.png)* = *λ![](../../OEBPS/Images/AR_e.png)*'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: '*A![](../../OEBPS/Images/AR_e.png)* = *λ![](../../OEBPS/Images/AR_e.png)*'
- en: Thus the linear transformation (that is, multiplication by matrix *A*) has changed
    the length but not the direction of ![](../../OEBPS/Images/AR_e.png) because *λ*![](../../OEBPS/Images/AR_e.png)
    is parallel to ![](../../OEBPS/Images/AR_e.png).
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，线性变换（即，矩阵 *A* 的乘法）改变了 ![](../../OEBPS/Images/AR_e.png) 的长度，但没有改变其方向，因为 *λ*![](../../OEBPS/Images/AR_e.png)
    与 ![](../../OEBPS/Images/AR_e.png) 平行。
- en: How do we obtain *λ* and ![](../../OEBPS/Images/AR_e.png)? Well,
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何获得 *λ* 和 ![](../../OEBPS/Images/AR_e.png)？嗯，
- en: '*A*![](../../OEBPS/Images/AR_e.png) = *λ* ![](../../OEBPS/Images/AR_e.png)'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '*A*![](../../OEBPS/Images/AR_e.png) = *λ* ![](../../OEBPS/Images/AR_e.png)'
- en: ⇔ *A*![](../../OEBPS/Images/AR_e.png) - *λ*![](../../OEBPS/Images/AR_e.png)
    = ![](../../OEBPS/Images/AR_0.png)
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: ⇔ *A*![](../../OEBPS/Images/AR_e.png) - *λ*![](../../OEBPS/Images/AR_e.png)
    = ![](../../OEBPS/Images/AR_0.png)
- en: ⇔ (*A* - *λ* **I**)![](../../OEBPS/Images/AR_e.png) = ![](../../OEBPS/Images/AR_0.png)
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: ⇔ (*A* - *λ* **I**)![](../../OEBPS/Images/AR_e.png) = ![](../../OEBPS/Images/AR_0.png)
- en: where **I** denotes the identity matrix.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 **I** 表示单位矩阵。
- en: Of course, we are only interested in nontrivial solutions, where ![](../../OEBPS/Images/AR_e.png)
    ≠ ![](../../OEBPS/Images/AR_0.png). In that case, *A* – *λ***I** cannot be invertible,
    because if it were, we could obtain the contradictory solution ![](../../OEBPS/Images/AR_e.png)
    = (*A* – *λ* **I**)^(–1) ![](../../OEBPS/Images/AR_0.png) = ![](../../OEBPS/Images/AR_0.png).
    Thus, (*A* − *λ***I**) is non-invertible, implying the determinant
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们只对非平凡解感兴趣，其中 ![](../../OEBPS/Images/AR_e.png) ≠ ![](../../OEBPS/Images/AR_0.png)。在这种情况下，*A*
    – *λ***I** 不能是可逆的，因为如果是的话，我们可以得到矛盾的解 ![](../../OEBPS/Images/AR_e.png) = (*A* –
    *λ* **I**)^(–1) ![](../../OEBPS/Images/AR_0.png) = ![](../../OEBPS/Images/AR_0.png)。因此，(*A*
    − *λ***I**) 是不可逆的，这意味着行列式
- en: '*det*(*A* − *λ***I**) = 0'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: '*det*(*A* − *λ***I**) = 0'
- en: For an *n* × *n* matrix *A*, this yields an *n*th-degree polynomial equation
    with *n* solutions for the unknown *λ*. *Thus, an *n* × *n* matrix has n eigenvalues,
    not necessarily all distinct*.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个 *n* × *n* 矩阵 *A*，这会产生一个 *n* 次的多项式方程，对于未知数 *λ* 有 *n* 个解。*因此，一个 *n* × *n*
    矩阵有 n 个特征值，不一定都是不同的*。
- en: 'Let’s compute eigenvalues and eigenvectors of a 3 × 3 matrix, just for kicks.
    The matrix we use is carefully chosen, as will be evident soon. But for now, think
    of it as an arbitrary matrix:'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算一个 3 × 3 矩阵的特征值和特征向量，只是为了好玩。我们使用的矩阵是精心选择的，这一点很快就会变得明显。但就现在而言，把它看作是一个任意矩阵：
- en: '![](../../OEBPS/Images/eq_02-23.png)'
  id: totrans-544
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-23.png)'
- en: Equation 2.23
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2.23
- en: 'We will compute the eigenvalues and eigenvectors of *A*:'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将计算矩阵 *A* 的特征值和特征向量：
- en: '![](../../OEBPS/Images/eq_02-23-a.png)'
  id: totrans-547
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-23-a.png)'
- en: Thus,
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，
- en: '![](../../OEBPS/Images/eq_02-23-b.png)'
  id: totrans-549
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-23-b.png)'
- en: Here, *i* = √–1. If necessary, you are encouraged to refresh your memory of
    imaginary and complex numbers from high school algebra.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*i* = √–1。如果需要，鼓励您从高中代数中复习一下虚数和复数的知识。
- en: 'Thus, we have found (as expected) three eigenvalues: 1, *e*^(*i* *π*/4), and
    *e*^(–*i* *π*/4). Each of them will yield one eigenvector. Let’s compute the eigenvector
    corresponding to the eigenvalue of 1 by way of example:'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们找到了（正如预期的那样）三个特征值：1，*e*^(*i* *π*/4)，和 *e*^(–*i* *π*/4)。每个都会产生一个特征向量。以下是一个例子，让我们计算对应于特征值
    1 的特征向量：
- en: '![](../../OEBPS/Images/eq_02-23-c.png)'
  id: totrans-552
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-23-c.png)'
- en: Thus, ![](../../OEBPS/Images/eq_02-23-d.png) is an eigenvector for the eigenvalue
    1 for matrix A. So is ![](../../OEBPS/Images/eq_02-23-e.png) for any real *k*.
    In fact, if *λ*, ![](../../OEBPS/Images/AR_e.png) is an eigenvalue, eigenvector
    pair for matrix *A*, then
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，![](../../OEBPS/Images/eq_02-23-d.png) 是矩阵 A 对应于特征值 1 的一个特征向量。同样，对于任何实数 *k*，![](../../OEBPS/Images/eq_02-23-e.png)
    也是。实际上，如果 *λ*，![](../../OEBPS/Images/AR_e.png) 是矩阵 *A* 的一个特征值，特征向量对，那么
- en: '*A![](../../OEBPS/Images/AR_e.png)* = *λ![](../../OEBPS/Images/AR_e.png)* ⇔
    *A*(*k![](../../OEBPS/Images/AR_e.png)*) = *λ*(*k![](../../OEBPS/Images/AR_e.png)*)'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: '*A![](../../OEBPS/Images/AR_e.png)* = *λ![](../../OEBPS/Images/AR_e.png)* ⇔
    *A*(*k![](../../OEBPS/Images/AR_e.png)*) = *λ*(*k![](../../OEBPS/Images/AR_e.png)*)'
- en: That is, *λ*, (*k![](../../OEBPS/Images/AR_e.png)*) is also an eigenvalue, eigenvector
    pair of *A*. In other words, we can only determine the eigenvector up to a fixed
    scale factor. We take the eigenvector to be of unit length (*![](../../OEBPS/Images/AR_e.png)^T![](../../OEBPS/Images/AR_e.png)*
    = 1) without loss of generality.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 即，*λ*，(*k![](../../OEBPS/Images/AR_e.png)*) 也是一个矩阵 *A* 的特征值，特征向量对。换句话说，我们只能确定特征向量到一个固定比例因子。我们可以将特征向量取为单位长度（*![](../../OEBPS/Images/AR_e.png)^T![](../../OEBPS/Images/AR_e.png)*
    = 1），这样不失一般性。
- en: The eigenvector for our example matrix turns out to be the *Z*-axis. This is
    not an accident. Our matrix *A* was, in fact, a rotation about the *Z*-axis. *A
    rotation matrix will always have 1 as an eigenvalue. The corresponding eigenvector
    will be the axis of rotation. In 3D, the other two eigenvalues will be complex
    numbers yielding the angle of rotation.* This is detailed in section [2.14](02.xhtml#sec-rotation-matrices-eigen).
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 我们示例矩阵的特征向量实际上是 *Z*-轴。这不是一个巧合。我们的矩阵 *A* 实际上是在 *Z*-轴上的旋转。*旋转矩阵的特征值总是 1。相应的特征向量将是旋转轴。在
    3D 中，其他两个特征值将是复数，产生旋转角度。* 这在 [2.14](02.xhtml#sec-rotation-matrices-eigen) 节中有详细说明。
- en: 2.13.1 Eigenvectors and linear independence
  id: totrans-557
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.13.1 特征向量和线性无关
- en: Two eigenvectors of a matrix corresponding to unequal eigenvalues are linearly
    independent. Let’s prove this to get some insights. Let *λ*[1], ![](../../OEBPS/Images/AR_e.png)[1]
    and *λ*[2], ![](../../OEBPS/Images/AR_e.png)[2] be eigenvalue, eigenvector pairs
    for a matrix *A* with *λ*[1] ≠ *λ*[2]. Then
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 对应于不等特征值的矩阵的特征向量是线性无关的。让我们来证明这一点以获得一些洞察。设 *λ*[1]，![](../../OEBPS/Images/AR_e.png)[1]
    和 *λ*[2]，![](../../OEBPS/Images/AR_e.png)[2] 是矩阵 *A* 的特征值，特征向量对，其中 *λ*[1] ≠ *λ*[2]。那么
- en: '*A*![](../../OEBPS/Images/AR_e.png)[1] = *λ*[1]![](../../OEBPS/Images/AR_e.png)[1]'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: '*A*![](../../OEBPS/Images/AR_e.png)[1] = *λ*[1]![](../../OEBPS/Images/AR_e.png)[1]'
- en: '*A*![](../../OEBPS/Images/AR_e.png)[2] = *λ*[2]![](../../OEBPS/Images/AR_e.png)[2]'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: '*A*![](../../OEBPS/Images/AR_e.png)[2] = *λ*[2]![](../../OEBPS/Images/AR_e.png)[2]'
- en: If possible, let there be two constants *α*[1] and *α*[2] such that
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可能的话，让存在两个常数 *α*[1] 和 *α*[2] 使得
- en: '*α*[1]![](../../OEBPS/Images/AR_e.png)[1] + *α*[2]![](../../OEBPS/Images/AR_e.png)[2]
    = 0'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: '*α*[1]![](../../OEBPS/Images/AR_e.png)[1] + *α*[2]![](../../OEBPS/Images/AR_e.png)[2]
    = 0'
- en: Equation 2.24
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 2.24
- en: In other words, suppose the two eigenvectors are linearly dependent. We will
    show that this assumption leads to an impossibility.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，假设两个特征向量是线性相关的。我们将证明这个假设会导致不可能的情况。
- en: Multiplying equation [2.24](02.xhtml#eq-linear-dep-vectors) by *A*, we get
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 将方程 [2.24](02.xhtml#eq-linear-dep-vectors) 乘以 *A*，我们得到
- en: '*α*[1]*A*![](../../OEBPS/Images/AR_e.png)[1] + *α*[2]*A*![](../../OEBPS/Images/AR_e.png)[2]
     = 0'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: '*α*[1]*A*![](../../OEBPS/Images/AR_e.png)[1] + *α*[2]*A*![](../../OEBPS/Images/AR_e.png)[2]
     = 0'
- en: ⇔ *α*[1]*λ*[1]![](../../OEBPS/Images/AR_e.png)[1] + *α*[2]*λ*[2]![](../../OEBPS/Images/AR_e.png)[2]  =
    0
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: ⇔ *α*[1]*λ*[1]![](../../OEBPS/Images/AR_e.png)[1] + *α*[2]*λ*[2]![](../../OEBPS/Images/AR_e.png)[2]  =
    0
- en: Also, we can multiply equation [2.24](02.xhtml#eq-linear-dep-vectors) by *λ*[2].
    Thus we get
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还可以将方程 [2.24](02.xhtml#eq-linear-dep-vectors) 乘以 *λ*[2]。因此我们得到
- en: '*α*[1]*λ*[1]![](../../OEBPS/Images/AR_e.png)[1] + *α*[2]*λ*[2]![](../../OEBPS/Images/AR_e.png)[2]
    = 0'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: '*α*[1]*λ*[1]![](../../OEBPS/Images/AR_e.png)[1] + *α*[2]*λ*[2]![](../../OEBPS/Images/AR_e.png)[2]
    = 0'
- en: '*α*[1]*λ*[2]![](../../OEBPS/Images/AR_e.png)[1] + *α*[2]*λ*[2]![](../../OEBPS/Images/AR_e.png)[2]
    = 0'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: '*α*[1]*λ*[2]![](../../OEBPS/Images/AR_e.png)[1] + *α*[2]*λ*[2]![](../../OEBPS/Images/AR_e.png)[2]
    = 0'
- en: Subtracting, we get
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 相减，我们得到
- en: '*α*[1](*λ*[1]-*λ*[2])![](../../OEBPS/Images/AR_e.png)[1] = 0'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: '*α*[1](*λ*[1]-*λ*[2])![](../../OEBPS/Images/AR_e.png)[1] = 0'
- en: By assumption, *α*[1] ≠ 0, *λ*[1] ≠ *λ*[2] and ![](../../OEBPS/Images/AR_e.png)[1]
    is not all zeros. Thus it is impossible for their product to be zero. Our original
    assumption (the two eigenvectors are linearly dependent) must have been wrong.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 根据假设，*α*[1] ≠ 0，*λ*[1] ≠ *λ*[2] 并且 ![](../../OEBPS/Images/AR_e.png)[1] 不是全零。因此，它们的乘积不可能为零。我们的原始假设（两个特征向量线性相关）一定是错误的。
- en: 2.13.2 Symmetric matrices and orthogonal eigenvectors
  id: totrans-574
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.13.2 对称矩阵和正交特征向量
- en: Two eigenvectors of a symmetric matrix that correspond to different eigenvalues
    are mutually orthogonal. Let’s prove this to get additional insight. A matrix
    *A* is symmetric if *A^T* = *A*. If *λ*[1], ![](../../OEBPS/Images/AR_e.png)[1]
    and *λ*[2], ![](../../OEBPS/Images/AR_e.png)[2] are eigenvalue, eigenvector pairs
    for a symmetric matrix *A*, then
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 对称矩阵的两个对应于不同特征值的特征向量是互相正交的。让我们来证明这一点以获得更多的洞察。一个矩阵 *A* 是对称的当且仅当 *A^T* = *A*。如果
    *λ*[1]，![](../../OEBPS/Images/AR_e.png)[1] 和 *λ*[2]，![](../../OEBPS/Images/AR_e.png)[2]
    是对称矩阵 *A* 的特征值，特征向量对，那么
- en: '*A*![](../../OEBPS/Images/AR_e.png)[1] = *λ*[1]![](../../OEBPS/Images/AR_e.png)[1]'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: '*A*![](../../OEBPS/Images/AR_e.png)[1] = *λ*[1]![](../../OEBPS/Images/AR_e.png)[1]'
- en: Equation 2.25
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 2.25
- en: '*A*![](../../OEBPS/Images/AR_e.png)[2] = *λ*[2]![](../../OEBPS/Images/AR_e.png)[2]'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: '*A*![](../../OEBPS/Images/AR_e.png)[2] = *λ*[2]![](../../OEBPS/Images/AR_e.png)[2]'
- en: Equation 2.26
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 2.26
- en: Transposing equation [2.25](02.xhtml#eq-eigen-vec1),
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 对方程 [2.25](02.xhtml#eq-eigen-vec1) 进行转置，
- en: '![](../../OEBPS/Images/AR_e.png)[1]*^T A^T* = *λ*[1]![](../../OEBPS/Images/AR_e.png)[1]^T'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../../OEBPS/Images/AR_e.png)[1]*^T A^T* = *λ*[1]![](../../OEBPS/Images/AR_e.png)[1]^T'
- en: Right-multiplying by ![](../../OEBPS/Images/AR_e.png)[2], we get
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 从右乘以 ![](../../OEBPS/Images/AR_e.png)[2]，我们得到
- en: '![](../../OEBPS/Images/AR_e.png)[1]^T *A^T*![](../../OEBPS/Images/AR_e.png)[2]
    = *λ*[1]![](../../OEBPS/Images/AR_e.png)[1]*^T ![](../../OEBPS/Images/AR_e.png)*[2]'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: '![图像](../../OEBPS/Images/AR_e.png)[1]^T *A^T*![图像](../../OEBPS/Images/AR_e.png)[2]
    = *λ*[1]![图像](../../OEBPS/Images/AR_e.png)[1]*^T ![图像](../../OEBPS/Images/AR_e.png)*[2]'
- en: ⇔ ![](../../OEBPS/Images/AR_e.png)[1]^T *A*![](../../OEBPS/Images/AR_e.png)[2
      ] = *λ*[1]![](../../OEBPS/Images/AR_e.png)[1]*^T ![](../../OEBPS/Images/AR_e.png)*[2]
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: ⇔ ![图像](../../OEBPS/Images/AR_e.png)[1]^T *A*![图像](../../OEBPS/Images/AR_e.png)[2  ]
    = *λ*[1]![图像](../../OEBPS/Images/AR_e.png)[1]*^T ![图像](../../OEBPS/Images/AR_e.png)*[2]
- en: where the last equation follows from the matrix symmetry. Also, left-multiplying
    equation [2.26](02.xhtml#eq-eigen-vec2) by ![](../../OEBPS/Images/AR_e.png)[1]*^T*,
    we get
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 其中最后一个等式来源于矩阵的对称性。此外，将方程 [2.26](02.xhtml#eq-eigen-vec2) 左乘以 ![图像](../../OEBPS/Images/AR_e.png)[1]*^T*，我们得到
- en: '![](../../OEBPS/Images/AR_e.png)[1]*^T A*![](../../OEBPS/Images/AR_e.png)[2]
    = *λ*[2]![](../../OEBPS/Images/AR_e.png)[1]*^T ![](../../OEBPS/Images/AR_e.png)*[2]'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: '![图像](../../OEBPS/Images/AR_e.png)[1]*^T A*![图像](../../OEBPS/Images/AR_e.png)[2]
    = *λ*[2]![图像](../../OEBPS/Images/AR_e.png)[1]*^T ![图像](../../OEBPS/Images/AR_e.png)*[2]'
- en: Thus
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 因此
- en: '![](../../OEBPS/Images/AR_e.png)[1]*^T A*![](../../OEBPS/Images/AR_e.png)[2]
    = *λ*[1]![](../../OEBPS/Images/AR_e.png)[1]*^T ![](../../OEBPS/Images/AR_e.png)*[2]'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: '![图像](../../OEBPS/Images/AR_e.png)[1]*^T A*![图像](../../OEBPS/Images/AR_e.png)[2]
    = *λ*[1]![图像](../../OEBPS/Images/AR_e.png)[1]*^T ![图像](../../OEBPS/Images/AR_e.png)*[2]'
- en: '![](../../OEBPS/Images/AR_e.png)[1]*^T A*![](../../OEBPS/Images/AR_e.png)[2]
    = *λ*[2]![](../../OEBPS/Images/AR_e.png)[1]^T ![](../../OEBPS/Images/AR_e.png)[2]'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: '![图像](../../OEBPS/Images/AR_e.png)[1]*^T A*![图像](../../OEBPS/Images/AR_e.png)[2]
    = *λ*[2]![图像](../../OEBPS/Images/AR_e.png)[1]^T ![图像](../../OEBPS/Images/AR_e.png)[2]'
- en: Subtracting the equations, we get
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 相减得到
- en: 0 = (*λ*[1] - *λ*[2]) ![](../../OEBPS/Images/AR_e.png)[1]*^T ![](../../OEBPS/Images/AR_e.png)*[2]
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 0 = (*λ*[1] - *λ*[2]) ![图像](../../OEBPS/Images/AR_e.png)[1]*^T ![图像](../../OEBPS/Images/AR_e.png)*[2]
- en: Since *λ*[1] ≠ *λ*[2], we must have ![](../../OEBPS/Images/AR_e.png)[1]*^T ![](../../OEBPS/Images/AR_e.png)*[2]
    = 0, which means the two eigenvectors are orthogonal. Thus, if *A* is an *n* ×
    *n* symmetric matrix with eigenvectors ![](../../OEBPS/Images/AR_e.png)[1], ![](../../OEBPS/Images/AR_e.png)[2],
    … ![](../../OEBPS/Images/AR_e.png)*[n]*, then ![](../../OEBPS/Images/AR_e.png)*[i]^T![](../../OEBPS/Images/AR_e.png)[j]*
    = 0 for all *i*, *j* satisfying *λ[i]* ≠ *λ[j]*.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 *λ*[1] ≠ *λ*[2]，我们必须有 ![图像](../../OEBPS/Images/AR_e.png)[1]*^T ![图像](../../OEBPS/Images/AR_e.png)*[2]
    = 0，这意味着这两个特征向量是正交的。因此，如果 *A* 是一个 *n* × *n* 的对称矩阵，其特征向量为 ![图像](../../OEBPS/Images/AR_e.png)[1]，![图像](../../OEBPS/Images/AR_e.png)[2]，…
    ![图像](../../OEBPS/Images/AR_e.png)*[n]*，那么 ![图像](../../OEBPS/Images/AR_e.png)*[i]^T![图像](../../OEBPS/Images/AR_e.png)[j]*
    = 0 对于所有满足 *λ[i]* ≠ *λ[j]* 的 *i*，*j*。
- en: 2.13.3 PyTorch code to compute eigenvectors and eigenvalues
  id: totrans-593
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.13.3 计算特征向量和特征值的 PyTorch 代码
- en: NOTE Fully functional code for this section, executable via Jupyter Notebook,
    can be found at [http://mng.bz/1rEZ](http://mng.bz/1rEZ).
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本节的完整功能代码，可通过 Jupyter Notebook 执行，可在 [http://mng.bz/1rEZ](http://mng.bz/1rEZ)
    找到。
- en: Listing 2.13 Eigenvalues and vectors
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.13 特征值和向量
- en: '[PRE12]'
  id: totrans-596
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ① ![](../../OEBPS/Images/eq_02-23-f.png) Rotates points in 3D space around the
    *Z*-axis.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: ① ![图像](../../OEBPS/Images/eq_02-23-f.png) 在三维空间中绕 *Z* 轴旋转点。
- en: 'The axis of rotation is the *Z*-axis: [0  0  1]*^T*'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 旋转轴是 *Z* 轴：[0  0  1]*^T*
- en: ② Function eig() in the torch linalg package computes eigenvalues and vectors.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: ② torch.linalg 包中的 eig() 函数计算特征值和向量。
- en: ③ Eigenvalues or vectors can contain complex numbers involving *j* = √-1
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 特征值或向量可能包含涉及 *j* = √-1 的复数
- en: 2.14 Orthogonal (rotation) matrices and their eigenvalues and eigenvectors
  id: totrans-601
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.14 正交（旋转）矩阵及其特征值和特征向量
- en: Of all the transforms, rotation transforms have a special intuitive appeal because
    of their highly observable behavior in the mechanical world. Furthermore, they
    play a significant role in developing and analyzing several machine learning tools.
    In this section, we overview rotation (aka orthogonal) matrices. (Fully functional
    code for the Jupyter notebook for this section can be found at [http://mng.bz/2eNN](http://mng.bz/2eNN).)
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有变换中，旋转变换由于其机械世界中的高度可观察行为而具有特殊的直观吸引力。此外，它们在开发和分析多个机器学习工具中发挥着重要作用。在本节中，我们概述了旋转（即正交）矩阵。（本节的
    Jupyter Notebook 的完整功能代码可在 [http://mng.bz/2eNN](http://mng.bz/2eNN) 找到。）
- en: 2.14.1 Rotation matrices
  id: totrans-603
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.14.1 旋转矩阵
- en: Figure [2.14](02.xhtml#fig-rotation_matrix_diagram) shows a point (*x*, *y*)
    rotated about the origin by an angle *θ*. The original point’s position vector
    made an angle *α* with the *X*-axis. Post-rotation, the point’s new coordinates
    are (*x*^′, *y*^′). Note that by definition, rotation does not change the distance
    from the center of rotation; that is what the circle indicates.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [2.14](02.xhtml#fig-rotation_matrix_diagram) 显示了一个点 (*x*, *y*) 围绕原点以角度 *θ*
    旋转。原始点的位置向量与 *X*-轴形成角度 *α*。旋转后，点的新的坐标是 (*x*^′, *y*^′)。请注意，根据定义，旋转不会改变旋转中心的距离；这正是圆圈所表示的。
- en: '![](../../OEBPS/Images/CH02_F14_Chaudhury.png)'
  id: totrans-605
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH02_F14_Chaudhury.png)'
- en: Figure 2.14 Rotation in a plane about the origin. By definition, rotation does
    not change the distance from the center of rotation (indicated by the circle).
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.14 在原点周围平面的旋转。根据定义，旋转不会改变旋转中心（由圆圈指示）的距离。
- en: 'Some well-known rotation matrices are as follows:'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 一些著名的旋转矩阵如下：
- en: '**Planar rotation by angle *θ* about the origin** (see figure [2.14](02.xhtml#fig-rotation_matrix_diagram)):'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**围绕原点以角度 *θ* 进行平面旋转**（见图 [2.14](02.xhtml#fig-rotation_matrix_diagram)）：'
- en: '![](../../OEBPS/Images/eq_02-27.png)'
  id: totrans-609
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-27.png)'
- en: Equation 2.27
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2.27
- en: '**Rotation by angle *θ* in 3D space about the *Z*-axis**:'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在 3D 空间中围绕 *Z*-轴以角度 *θ* 进行旋转**：'
- en: '![](../../OEBPS/Images/eq_02-28.png)'
  id: totrans-612
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-28.png)'
- en: Equation 2.28
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2.28
- en: 'Note that the *z* coordinate remains unaffected by this rotation:'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，*z* 坐标在此旋转中不受影响：
- en: '![](../../OEBPS/Images/eq_02-28-a.png)'
  id: totrans-615
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-28-a.png)'
- en: This rotation matrix has an eigenvalue of 1, and the corresponding eigenvector
    is the *Z*-axis—you should verify this. This implies that a point on the *Z*-axis
    maps to itself when transformed (rotated) by the previous matrix, which is in
    keeping with the property that the *z* coordinate remains unchanged by this rotation.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 这个旋转矩阵有一个特征值为 1，对应的特征向量是 *Z*-轴——你应该验证这一点。这意味着当通过前面的矩阵（旋转）变换时，*Z*-轴上的点映射到自身，这与
    *z* 坐标在此旋转中保持不变的属性相一致。
- en: '**Rotation by angle *θ* in 3D space about the *X*-axis**:'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在 3D 空间中围绕 *X*-轴以角度 *θ* 进行旋转**：'
- en: '![](../../OEBPS/Images/eq_02-29.png)'
  id: totrans-618
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-29.png)'
- en: Equation 2.29
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2.29
- en: 'Note that the *X* coordinate remains unaffected by this rotation and the *X*-axis
    is an eigenvector of this matrix:'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，*X* 坐标在此旋转中不受影响，*X*-轴是该矩阵的特征向量：
- en: '![](../../OEBPS/Images/eq_02-29-a.png)'
  id: totrans-621
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-29-a.png)'
- en: '**Rotation by angle *θ* in 3D space about the *Y*-axis**:'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在 3D 空间中围绕 *Y*-轴以角度 *θ* 进行旋转**：'
- en: '![](../../OEBPS/Images/eq_02-30.png)'
  id: totrans-623
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-30.png)'
- en: Equation 2.30
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2.30
- en: 'Note that the *Y* coordinate remains unaffected by this rotation and the *Y*-axis
    is an eigenvector of this matrix:'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，*Y* 坐标在此旋转中不受影响，*Y*-轴是该矩阵的特征向量：
- en: '![](../../OEBPS/Images/eq_02-30-a.png)'
  id: totrans-626
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-30-a.png)'
- en: Listing 2.14 Rotation matrices
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.14 旋转矩阵
- en: '[PRE13]'
  id: totrans-628
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ① Returns the matrix that performs in-plane 2D rotation by angle theta about
    the origin. Thus, multiplication with this matrix moves a point to a new location.
    The angle between the position vectors of the original and new points is theta
    (figure [2.14](02.xhtml#fig-rotation_matrix_diagram)).
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: ① 返回在原点周围以角度 theta 进行的平面 2D 旋转的矩阵。因此，与该矩阵的乘法将点移动到新位置。原始点和新点位置向量之间的角度是 theta（见图
    [2.14](02.xhtml#fig-rotation_matrix_diagram)）。
- en: ② Returns the matrix that rotates a point in 3D space about the chosen axis
    by angle theta degrees. The axis of rotation can be 0, 1, or 2, corresponding
    to the X-, Y-, or Z-axis, respectively.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: ② 返回一个矩阵，该矩阵将点在 3D 空间中围绕所选轴旋转 theta 度。旋转轴可以是 0、1 或 2，分别对应于 X-、Y- 或 Z-轴。
- en: ③ *R*[3*dx*] from equation [2.29](02.xhtml#eq-rot3d-x)
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: ③ *R*[3*dx*] 来自方程 [2.29](02.xhtml#eq-rot3d-x)
- en: ④ *R*[3*dy*] from equation [2.30](02.xhtml#eq-rot3d-y)
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: ④ *R*[3*dy*] 来自方程 [2.30](02.xhtml#eq-rot3d-y)
- en: ⑤ *R*[3*dz*] from equation [2.28](02.xhtml#eq-rot3d-z)
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ *R*[3*dz*] 来自方程 [2.28](02.xhtml#eq-rot3d-z)
- en: Listing 2.15 Applying rotation matrices
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.15 应用旋转矩阵
- en: '[PRE14]'
  id: totrans-635
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ① Creates vector ![](../../OEBPS/Images/AR_u.png) (see figure [2.15](02.xhtml#fig-numpy-rotations))
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: ① 创建向量 ![](../../OEBPS/Images/AR_u.png)（见图 [2.15](02.xhtml#fig-numpy-rotations)）
- en: ② *R*[3*dz*] from equation [2.28](02.xhtml#eq-rot3d-z), 45° about *Z*-axis
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: ② *R*[3*dz*] 来自方程 [2.28](02.xhtml#eq-rot3d-z)，45°关于 *Z*-轴
- en: ③ ![](../../OEBPS/Images/AR_v.png) (see figure [2.15](02.xhtml#fig-numpy-rotations))
    is ![](../../OEBPS/Images/AR_u.png) rotated by *R*[3*dz*].
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: ③ ![](../../OEBPS/Images/AR_v.png)（见图 [2.15](02.xhtml#fig-numpy-rotations)）是通过
    *R*[3*dz*] 旋转的 ![](../../OEBPS/Images/AR_u.png)。
- en: ④ *R*[3*dx*] from equation [2.28](02.xhtml#eq-rot3d-z), 45° about *X*-axis
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: ④ *R*[3*dx*] 来自方程 [2.28](02.xhtml#eq-rot3d-z)，45°关于 *X*-轴
- en: ⑤ ![](../../OEBPS/Images/AR_w.png) (see figure [2.15](02.xhtml#fig-numpy-rotations))
    is ![](../../OEBPS/Images/AR_v.png) rotated by *R*[3*dx*].
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ ![](../../OEBPS/Images/AR_w.png)（见图 [2.15](02.xhtml#fig-numpy-rotations)）是通过
    *R*[3*dx*] 旋转 ![](../../OEBPS/Images/AR_v.png) 得到的。
- en: '![](../../OEBPS/Images/CH02_F15_Chaudhury.png)'
  id: totrans-641
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH02_F15_Chaudhury.png)'
- en: Figure 2.15 Rotation visualized. Here the original vector u is first rotated
    by 45 degrees around the Z-axis to get vector v, which is subsequently rotated
    again by 45 degrees around the X-axis to get vector w.
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.15 旋转可视化。在这里，原始向量 u 首先绕 Z 轴旋转 45 度得到向量 v，然后再次绕 X 轴旋转 45 度得到向量 w。
- en: 2.14.2 Orthogonality of rotation matrices
  id: totrans-643
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.14.2 旋转矩阵的正交性
- en: 'A matrix *R* is *orthogonal* if and only if it its transpose is also its inverse:
    that is, *R^TR* = *RR^T* = **I**. *All rotations matrices are orthogonal matrices.
    All orthogonal matrices represent some rotation.* For instance:'
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 一个矩阵 *R* 是 *正交的* 当且仅当它的转置也是它的逆：即，*R^TR* = *RR^T* = **I**。*所有旋转矩阵都是正交矩阵。所有正交矩阵都表示某种旋转。*
    例如：
- en: '![](../../OEBPS/Images/eq_02-30-b1.png)'
  id: totrans-645
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-30-b1.png)'
- en: You are encouraged to verify, likewise, that all the rotation matrices shown
    here are orthogonal.
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓励您同样验证，这里显示的所有旋转矩阵都是正交的。
- en: Orthogonality and length-preservation
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 正交性和长度保持
- en: Orthogonality implies that rotation is length-preserving. Given any vector ![](../../OEBPS/Images/AR_x.png)
    and rotation matrix *R*, let ![](../../OEBPS/Images/AR_y.png) = *R*![](../../OEBPS/Images/AR_x.png)
    be the rotated vector. The lengths (magnitudes) of the two vectors ![](../../OEBPS/Images/AR_x.png),
    ![](../../OEBPS/Images/AR_y.png) are equal since it is easy to see that
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 正交性意味着旋转保持长度。给定任意向量 ![](../../OEBPS/Images/AR_x.png) 和旋转矩阵 *R*，令 ![](../../OEBPS/Images/AR_y.png)
    = *R*![](../../OEBPS/Images/AR_x.png) 为旋转后的向量。由于很容易看出这两个向量的长度（大小）相等，因此 ![](../../OEBPS/Images/AR_x.png)，![](../../OEBPS/Images/AR_y.png)
    相等。
- en: '||![](../../OEBPS/Images/AR_y.png)|| = ![](../../OEBPS/Images/AR_y.png)*^T*![](../../OEBPS/Images/AR_y.png)
    = (*R*![](../../OEBPS/Images/AR_x.png))*^T*(*R*![](../../OEBPS/Images/AR_x.png))
    = ![](../../OEBPS/Images/AR_x.png)*^TR^TR*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_x.png)*^T***I**![](../../OEBPS/Images/AR_x.png)
    = ![](../../OEBPS/Images/AR_x.png)*^T*![](../../OEBPS/Images/AR_x.png) = ||![](../../OEBPS/Images/AR_x.png)||'
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: '||![](../../OEBPS/Images/AR_y.png)|| = ![](../../OEBPS/Images/AR_y.png)*^T*![](../../OEBPS/Images/AR_y.png)
    = (*R*![](../../OEBPS/Images/AR_x.png))*^T*(*R*![](../../OEBPS/Images/AR_x.png))
    = ![](../../OEBPS/Images/AR_x.png)*^TR^TR*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_x.png)*^T***I**![](../../OEBPS/Images/AR_x.png)
    = ![](../../OEBPS/Images/AR_x.png)*^T*![](../../OEBPS/Images/AR_x.png) = ||![](../../OEBPS/Images/AR_x.png)||'
- en: From elementary matrix theory, we know that
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 从基本的矩阵理论中，我们知道
- en: (*AB*)*^T* = *B^TA^T*
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: (*AB*)*^T* = *B^TA^T*
- en: Negating the angle of rotation
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 取消旋转角度
- en: Negating the angle of rotation is equivalent to inverting the rotation matrix,
    which is equivalent to transposing the rotation matrix. For instance, consider
    in-plane rotation. Say a point ![](../../OEBPS/Images/AR_x.png) is rotated about
    the origin to vector ![](../../OEBPS/Images/AR_y.png) via matrix *R*. Thus
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 取消旋转角度相当于反转旋转矩阵，这相当于转置旋转矩阵。例如，考虑平面内的旋转。假设一个点 ![](../../OEBPS/Images/AR_x.png)
    通过矩阵 *R* 绕原点旋转到向量 ![](../../OEBPS/Images/AR_y.png)。因此
- en: '![](../../OEBPS/Images/eq_02-30-c.png)'
  id: totrans-654
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-30-c.png)'
- en: Now we can go back from ![](../../OEBPS/Images/AR_y.png) to ![](../../OEBPS/Images/AR_x.png)
    by rotating by −*θ*. The corresponding rotation matrix is
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过旋转 −*θ* 从 ![](../../OEBPS/Images/AR_y.png) 回到 ![](../../OEBPS/Images/AR_x.png)。相应的旋转矩阵是
- en: '![](../../OEBPS/Images/eq_02-30-d.png)'
  id: totrans-656
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-30-d.png)'
- en: 'In other words, *R^T* inverts the rotation: that is, rotates by the negative
    angle.'
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，*R^T* 反转了旋转：即，以负角度旋转。
- en: 2.14.3 PyTorch code for orthogonality of rotation matrices
  id: totrans-658
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.14.3 PyTorch 代码用于旋转矩阵的正交性
- en: Let’s verify the orthogonality of the rotation matrix by creating one in PyTorch,
    imparting a transpose to it, and verifying that the product of the original matrix
    and the transpose is the identity matrix.
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过在 PyTorch 中创建一个旋转矩阵，给它一个转置，并验证原始矩阵和转置的乘积是单位矩阵来验证旋转矩阵的正交性。
- en: Listing 2.16 Orthogonality of rotation matrices
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.16 旋转矩阵的正交性
- en: '[PRE15]'
  id: totrans-661
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ① Creates a rotation matrix, *R*[30]
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: ① 创建一个旋转矩阵，*R*[30]
- en: ② The inverse of a rotation matrix is the same as its transpose.
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: ② 旋转矩阵的逆矩阵与它的转置相同。
- en: ③ Multiplying a rotation matrix and its inverse yields the identity matrix.
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 乘以旋转矩阵及其逆矩阵得到单位矩阵。
- en: ④ A vector ![](../../OEBPS/Images/AR_u.png) rotated by matrix *R*[30] to yield
    vector ![](../../OEBPS/Images/AR_v.png), *R*[30]![](../../OEBPS/Images/AR_u.png)
    = ![](../../OEBPS/Images/AR_v.png).
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 一个向量![单位向量](../../OEBPS/Images/AR_u.png)被矩阵*R*[30]旋转以得到向量![向量](../../OEBPS/Images/AR_v.png)，*R*[30]![单位向量](../../OEBPS/Images/AR_u.png)
    = ![向量](../../OEBPS/Images/AR_v.png).
- en: ⑤ The norm of a vector is the same as its length. Rotation preserves the length
    of a vector ||*R![](../../OEBPS/Images/AR_u.png)*|| = ||![](../../OEBPS/Images/AR_u.png)||.
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 向量的范数与它的长度相同。旋转保持向量的长度 ||*R![单位向量](../../OEBPS/Images/AR_u.png)*|| = ||![单位向量](../../OEBPS/Images/AR_u.png)||.
- en: ⑥ Rotation by an angle followed by rotation by the negative of that angle takes
    the vector back to its original position. Rotation by a negative angle is equivalent
    to inverse rotation.
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 先旋转一个角度，然后旋转该角度的负值，将向量带回到其原始位置。负角度的旋转等同于逆旋转。
- en: ⑦ A matrix that rotates by an angle is the inverse of the matrix that rotates
    by the negative of the same angle.
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 旋转一个角度的矩阵是旋转相同角度负值的矩阵的逆。
- en: 2.14.4 Eigenvalues and eigenvectors of a rotation matrix:Finding the axis of
    rotation
  id: totrans-669
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.14.4 旋转矩阵的特征值和特征向量：寻找旋转轴
- en: Let *λ*, ![](../../OEBPS/Images/AR_e.png) be an eigenvalue, eigenvector pair
    of a rotation matrix *R*. Then *R![](../../OEBPS/Images/AR_e.png)* = *λ![](../../OEBPS/Images/AR_e.png)*
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 设*λ*，![矩阵转置](../../OEBPS/Images/AR_e.png)为旋转矩阵*R*的一个特征值，特征向量对。那么*R![矩阵转置](../../OEBPS/Images/AR_e.png)*
    = *λ![矩阵转置](../../OEBPS/Images/AR_e.png)*
- en: Transposing both sides,
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 对等式的两边进行转置，
- en: '*![](../../OEBPS/Images/AR_e.png)^TR^T* = *λ![](../../OEBPS/Images/AR_e.png)^T*'
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: '*![矩阵转置](../../OEBPS/Images/AR_e.png)^TR^T* = *λ![矩阵转置](../../OEBPS/Images/AR_e.png)*'
- en: Multiplying the left and right sides, respectively, with the equivalent entities
    *R![](../../OEBPS/Images/AR_e.png)* and *λ![](../../OEBPS/Images/AR_e.png)*, we
    get
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: 分别将等式的左右两边乘以等价的实体*R![矩阵转置](../../OEBPS/Images/AR_e.png)*和*λ![矩阵转置](../../OEBPS/Images/AR_e.png)*，我们得到
- en: '![](../../OEBPS/Images/AR_e.png)*^TR^T*(*R*![](../../OEBPS/Images/AR_e.png))
    = *λ![](../../OEBPS/Images/AR_e.png)^T*(*λ*![](../../OEBPS/Images/AR_e.png)) ⇔
    ![](../../OEBPS/Images/AR_e.png)*^T(R^TR*)![](../../OEBPS/Images/AR_e.png) = *λ*²![](../../OEBPS/Images/AR_e.png)*^T![](../../OEBPS/Images/AR_e.png)
    ⇔ ![](../../OEBPS/Images/AR_e.png)^T*(**I**)![](../../OEBPS/Images/AR_e.png) =
    *λ*²![](../../OEBPS/Images/AR_e.png)^T![](../../OEBPS/Images/AR_e.png)'
  id: totrans-674
  prefs: []
  type: TYPE_IMG
  zh: '![矩阵转置](../../OEBPS/Images/AR_e.png)*^TR^T*(*R*![矩阵转置](../../OEBPS/Images/AR_e.png))
    = *λ![矩阵转置](../../OEBPS/Images/AR_e.png)*(*λ*![矩阵转置](../../OEBPS/Images/AR_e.png))
    ⇔ ![矩阵转置](../../OEBPS/Images/AR_e.png)*^T(R^TR*)![矩阵转置](../../OEBPS/Images/AR_e.png)
    = *λ*²![矩阵转置](../../OEBPS/Images/AR_e.png)*^T![矩阵转置](../../OEBPS/Images/AR_e.png)
    ⇔ ![矩阵转置](../../OEBPS/Images/AR_e.png)^T*(**I**)[!矩阵转置](../../OEBPS/Images/AR_e.png)
    = *λ*²![矩阵转置](../../OEBPS/Images/AR_e.png)*^T![矩阵转置](../../OEBPS/Images/AR_e.png)'
- en: ⇔ ![](../../OEBPS/Images/AR_e.png)^T![](../../OEBPS/Images/AR_e.png) = *λ*²![](../../OEBPS/Images/AR_e.png)^T![](../../OEBPS/Images/AR_e.png)
    ⇔ *λ*² = 1 ⇔ *λ* = 1
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: ⇔ ![矩阵转置](../../OEBPS/Images/AR_e.png)^T![矩阵转置](../../OEBPS/Images/AR_e.png)
    = *λ*²![矩阵转置](../../OEBPS/Images/AR_e.png)^T![矩阵转置](../../OEBPS/Images/AR_e.png)
    ⇔ *λ*² = 1 ⇔ *λ* = 1
- en: '(The negative solution *λ* = −1 corresponds to reflection.) Thus, all rotation
    matrices have 1 as one of their eigenvalues. The corresponding eigenvector ![](../../OEBPS/Images/AR_e.png)
    satisfies *R![](../../OEBPS/Images/AR_e.png)* = ![](../../OEBPS/Images/AR_e.png).
    This is the axis of rotation: the set of points that stay where they were post-rotation.'
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: （负解*λ* = −1对应于反射。）因此，所有旋转矩阵都有一个特征值1。相应的特征向量![矩阵转置](../../OEBPS/Images/AR_e.png)满足*R![矩阵转置](../../OEBPS/Images/AR_e.png)*
    = ![矩阵转置](../../OEBPS/Images/AR_e.png)。这是旋转轴：旋转后保持原位的点的集合。
- en: 2.14.5 PyTorch code for eigenvalues and vectors of rotation matrices
  id: totrans-677
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.14.5 旋转矩阵的特征值和向量的PyTorch代码
- en: The following listing shows the code for the axis of rotation.
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了旋转轴的代码。
- en: Listing 2.17 Axis of rotation
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.17 旋转轴
- en: '[PRE16]'
  id: totrans-680
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ① ![](../../OEBPS/Images/eq_02-30-e.png)
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: ① ![方程式](../../OEBPS/Images/eq_02-30-e.png)
- en: about the *Z*-axis. All rotation matrices will havean eigenvalue 1. The corresponding
    eigenvector
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: 关于*Z*轴旋转。所有旋转矩阵都将有一个特征值1。相应的特征向量
- en: is the axis of rotation (here, the *Z*-axis).
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 是旋转轴（在这里，是*Z*轴）。
- en: ② The PyTorch function eig() computes eigenvalues and eigenvectors.
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: ② PyTorch函数eig()计算特征值和特征向量。
- en: ③ The PyTorch function where() returns the indices at which the specified condition
    is true.
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: ③ PyTorch函数where()返回指定条件为真的索引。
- en: ④ Obtains the eigenvector for eigenvalue 1
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 获取特征值1的特征向量
- en: ⑤ The axis of rotation is the *Z*-axis.
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 旋转轴是*Z*轴。
- en: ⑥ Takes a random point
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 取一个随机点
- en: on this axis and applies the rotation to this point; its position does not change.
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个轴上并对此点应用旋转；其位置不改变。
- en: 2.15 Matrix diagonalization
  id: totrans-690
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.15 矩阵对角化
- en: In section [2.12](02.xhtml#sec-lin_systems), we studied linear systems and their
    importance in machine learning. We also remarked that the standard mathematical
    process of solving linear systems via matrix inversion is not very desirable from
    a machine learning point of view. In this section, we will see one method of solving
    linear systems without matrix inversion. In addition, this section will help us
    develop the insights necessary to understand quadratic forms and, eventually,
    principal component analysis (PCA), one of the most important tools in data science.
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2.12 节中，我们研究了线性系统及其在机器学习中的重要性。我们还指出，通过矩阵求逆解决线性系统的标准数学过程从机器学习的角度来看并不理想。在本节中，我们将看到一种不通过矩阵求逆解决线性系统的方法。此外，本节将帮助我们发展理解二次型以及最终主成分分析（PCA）所必需的洞察力，PCA
    是数据科学中最重要工具之一。
- en: Consider an *n* × *n* matrix *A* with *n* linearly independent eigenvectors.
    Let *S* be a matrix with these eigenvectors as its columns. That is,
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个 *n* × *n* 矩阵 *A*，它具有 *n* 个线性无关的特征向量。设 *S* 是一个矩阵，其列由这些特征向量组成。也就是说，
- en: '![](../../OEBPS/Images/eq_02-30-f.png)'
  id: totrans-693
  prefs: []
  type: TYPE_IMG
  zh: '![方程式](../../OEBPS/Images/eq_02-30-f.png)'
- en: and
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '![](../../OEBPS/Images/eq_02-30-g.png)'
  id: totrans-695
  prefs: []
  type: TYPE_IMG
  zh: '![方程式](../../OEBPS/Images/eq_02-30-g.png)'
- en: Then
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 然后
- en: '![](../../OEBPS/Images/eq_02-30-h.png)'
  id: totrans-697
  prefs: []
  type: TYPE_IMG
  zh: '![方程式](../../OEBPS/Images/eq_02-30-h.png)'
- en: where
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '![](../../OEBPS/Images/eq_02-30-i.png)'
  id: totrans-699
  prefs: []
  type: TYPE_IMG
  zh: '![方程式](../../OEBPS/Images/eq_02-30-i.png)'
- en: is a diagonal matrix with the eigenvalues of *A* on the diagonal and 0 everywhere
    else.
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 是一个对角矩阵，其对角线上的元素是 *A* 的特征值，其余位置都是 0。
- en: Thus, we have
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有
- en: '*AS* = *S*Λ'
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: '*AS* = *S*Λ'
- en: which leads to
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致
- en: '*A* = *S*Λ*S*^(−1)'
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: '*A* = *S*Λ*S*^(−1)'
- en: and
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: Λ = *S*^(−1)*AS*
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: Λ = *S*^(−1)*AS*
- en: 'If *A* is symmetric, then its eigenvectors are orthogonal. Then *S^TS* = *SS^T*
    = **I** ⇔ *S*^(−1) = *S^T*, and we get the diagonalization of *A*: *A* = *S*Λ*S^T*
    Note that diagonalization is not unique: a given matrix may be diagonalized in
    multiple ways.'
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 *A* 是对称的，那么它的特征向量是正交的。那么 *S^TS* = *SS^T* = **I** ⇔ *S*^(−1) = *S^T*，我们得到
    *A* 的对角化：*A* = *S*Λ*S^T* 注意，对角化不是唯一的：一个给定的矩阵可能以多种方式对角化。
- en: 2.15.1 PyTorch code for matrix diagonalization
  id: totrans-708
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.15.1 矩阵对角化的 PyTorch 代码
- en: Now we will study the PyTorch implementation of the math we learned in section
    [2.15](02.xhtml#sec-mat-diagonalization). As usual, we will only show the directly
    relevant bit of code here.
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将研究 PyTorch 对 2.15 节中我们所学数学的实现。像往常一样，我们在这里只展示直接相关的代码部分。
- en: NOTE Fully functional code for this section, executable via Jupyter Notebook,
    can be found at [http://mng.bz/RXJn](http://mng.bz/RXJn).
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本节完全功能的代码，可通过 Jupyter Notebook 执行，可在[http://mng.bz/RXJn](http://mng.bz/RXJn)找到。
- en: Listing 2.18 Diagonalization of a matrix
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.18 矩阵的对角化
- en: '[PRE17]'
  id: totrans-712
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ① Diagonalization is factorizing a matrix *A* = *SΣS*^(−1). *S* is a matrix
    with eigenvectors of *A* as columns. Σ is a diagonal matrix with eigenvalues of
    *A* in the diagonal.
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: ① 对角化是将矩阵 *A* = *SΣS*^(−1) 分解。*S* 是一个矩阵，其列由 *A* 的特征向量组成。Σ 是一个对角矩阵，其对角线上的元素是
    *A* 的特征值。
- en: ② The PyTorch function eig() returns eigenvalues and vectors.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: ② PyTorch 函数 eig() 返回特征值和向量。
- en: ③ The PyTorch function diag() creates a diagonal matrix of given values.
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: ③ PyTorch 函数 diag() 创建一个给定值的对角矩阵。
- en: ④ Returns the three factors
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 返回三个因子
- en: ⑤ Creates a matrix *A*
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 创建矩阵 *A*
- en: ⑥ Reconstructs *A* from its factors
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 从其因子重建 *A*
- en: ⑦ Verifies that the reconstructed matrix is the same as the original
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 验证重建的矩阵与原始矩阵相同
- en: 2.15.2 Solving linear systems without inversion via diagonalization
  id: totrans-720
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.15.2 通过对角化解决线性系统而不进行求逆
- en: 'Diagonalization has many practical applications. Let’s study one now. In general,
    matrix inversion (that is, computing *A*^(−1)) is a very complex process that
    is numerically unstable. Hence, solving *A*![](../../OEBPS/Images/AR_x.png) =
    ![](../../OEBPS/Images/AR_b.png) via ![](../../OEBPS/Images/AR_x.png) = *A*^(−1)![](../../OEBPS/Images/AR_b.png)
    is to be avoided when possible. In the case of a square symmetric matrix with
    *n* distinct eigenvalues, diagonalization can come to the rescue. We can solve
    this in multiple steps. We first diagonalize *A*:'
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 对角化有许多实际应用。现在让我们研究一个。一般来说，矩阵求逆（即计算 *A*^(−1)）是一个非常复杂的过程，数值上不稳定。因此，当可能时，应避免通过
    ![](../../OEBPS/Images/AR_x.png) = *A*^(−1)![](../../OEBPS/Images/AR_b.png) 解决
    *A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)。在具有 *n*
    个不同特征值的平方对称矩阵的情况下，对角化可以提供帮助。我们可以分多步解决这个问题。我们首先对 *A* 进行对角化：
- en: '*A* = *S*Λ*S^T*'
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: '*A* = *S*Λ*S^T*'
- en: Then
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: 然后
- en: '*A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)'
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: '*A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)'
- en: 'can be written as:'
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: 可以表示为：
- en: '*S*Λ*S^T*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)'
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: '*S*Λ*S^T*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)'
- en: 'where *S* is the matrix with eigenvectors of *A* as its columns:'
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *S* 是以 *A* 的特征向量为列的矩阵：
- en: '*S* = [![](../../OEBPS/Images/AR_e.png)[1]   ![](../../OEBPS/Images/AR_e.png)[2]  
    … ![](../../OEBPS/Images/AR_e.png)*[n]*]'
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: '*S* = [![](../../OEBPS/Images/AR_e.png)[1]   ![](../../OEBPS/Images/AR_e.png)[2]  
    … ![](../../OEBPS/Images/AR_e.png)*[n]*]'
- en: '(Since *A* is symmetric, these eigenvectors are orthogonal. Hence *S^TS* =
    *SS^T* = **I**.) The solution can be obtained in a series of very simple steps:'
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: （由于 *A* 是对称的，这些特征向量是正交的。因此 *S^TS* = *SS^T* = **I**。）解可以通过一系列非常简单的步骤获得：
- en: '![](../../OEBPS/Images/eq_02-30-j.png)'
  id: totrans-730
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-30-j.png)'
- en: First solve
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: 首先解决
- en: '*S*![](../../OEBPS/Images/AR_y.png)[1] = ![](../../OEBPS/Images/AR_b.png)'
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: '*S*![](../../OEBPS/Images/AR_y.png)[1] = ![](../../OEBPS/Images/AR_b.png)'
- en: as
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: as
- en: '![](../../OEBPS/Images/AR_y.png)[1] = *S^T* ![](../../OEBPS/Images/AR_b.png)'
  id: totrans-734
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/AR_y.png)[1] = *S^T* ![](../../OEBPS/Images/AR_b.png)'
- en: Notice that both the transpose and matrix-vector multiplications are simple
    and numerically stable operations, unlike matrix inversion. Then we get
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，转置和矩阵-向量乘法都是简单且数值稳定的操作，与矩阵求逆不同。然后我们得到
- en: Λ(*S^T*![](../../OEBPS/Images/AR_x.png)) = ![](../../OEBPS/Images/AR_y.png)[1]
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: Λ(*S^T*![](../../OEBPS/Images/AR_x.png)) = ![](../../OEBPS/Images/AR_y.png)[1]
- en: Now solve
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: 现在解决
- en: Λ![](../../OEBPS/Images/AR_y.png)[2] = ![](../../OEBPS/Images/AR_y.png)[1]
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: Λ![](../../OEBPS/Images/AR_y.png)[2] = ![](../../OEBPS/Images/AR_y.png)[1]
- en: as
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: as
- en: '![](../../OEBPS/Images/AR_y.png)[2] = Λ^(–1)![](../../OEBPS/Images/AR_y.png)[1]'
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../../OEBPS/Images/AR_y.png)[2] = Λ^(–1)![](../../OEBPS/Images/AR_y.png)[1]'
- en: 'Note that since Λ is a diagonal matrix, inverting it is trivial:'
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于 Λ 是对角矩阵，求逆是平凡的：
- en: '![](../../OEBPS/Images/eq_02-31.png)'
  id: totrans-742
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-31.png)'
- en: Equation 2.31
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2.31
- en: As a final step, solve
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后一步，解决
- en: '*S^T*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_y.png)[2]'
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: '*S^T*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_y.png)[2]'
- en: as
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: as
- en: '![](../../OEBPS/Images/AR_x.png) = *S*![](../../OEBPS/Images/AR_y.png)[2]'
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../../OEBPS/Images/AR_x.png) = *S*![](../../OEBPS/Images/AR_y.png)[2]'
- en: Thus we have obtained ![](../../OEBPS/Images/AR_x.png) without a single complex
    or unstable step.
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们没有经过任何复杂的或不稳定的步骤就获得了 ![](../../OEBPS/Images/AR_x.png)。
- en: 2.15.3 PyTorch code for solving linear systems via diagonalization
  id: totrans-749
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.15.3 使用对角化解决线性系统的 PyTorch 代码
- en: 'Let’s try solving the following set of equations:'
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试解决以下方程组：
- en: '*x* +   *y* +  *z* = 8'
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: '*x* +   *y* +   *z* = 8'
- en: 2*x* + 2*y* + 3*z* = 15
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: 2*x* + 2*y* + 3*z* = 15
- en: '*x* + 3*y* + 3*z* = 16'
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: '*x* + 3*y* + 3*z* = 16'
- en: This can be written using matrices and vectors as
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以用矩阵和向量表示为
- en: '*A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)'
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: '*A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)'
- en: where
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '![](../../OEBPS/Images/eq_02-31-a.png)'
  id: totrans-757
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-31-a.png)'
- en: Note that *A* is a symmetric matrix. It has orthogonal eigenvectors. The matrix
    with eigenvectors of *A* in columns is orthogonal. Its transpose and inverse are
    the same.
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，*A* 是对称矩阵。它有正交的特征向量。以 *A* 的特征向量为列的矩阵是正交的。它的转置和逆是相同的。
- en: Listing 2.19 Solving linear systems using diagonalization
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.19 使用对角化解决线性系统
- en: '[PRE18]'
  id: totrans-760
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ① Creates a symmetric matrix *A*
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: ① 创建一个对称矩阵 *A*
- en: ② Asserts that *A* may be symmetric
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: ② 声明 *A* 可能是对称的
- en: ③ Creates a vector ![](../../OEBPS/Images/AR_b.png)
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 创建一个向量 ![](../../OEBPS/Images/AR_b.png)
- en: ④ Solves *A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)
    using matrix inversion, ![](../../OEBPS/Images/AR_x.png) = *A*^(−1)![](../../OEBPS/Images/AR_b.png).
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 使用矩阵求逆解决 *A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)，![](../../OEBPS/Images/AR_x.png)
    = *A*^(−1)![](../../OEBPS/Images/AR_b.png)。
- en: 'Note: matrix inversion is numerically unstable.'
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：矩阵求逆是数值不稳定的。
- en: ⑤ Solves *A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)
    via diagonalization. *A* = *S*Σ*S^T*.
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 通过对角化解决 *A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)。*A*
    = *S*Σ*S^T*。
- en: '![](../../OEBPS/Images/eq_02-31-b.png).'
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../../OEBPS/Images/eq_02-31-b.png).'
- en: '⑥ 1\. Solve: *S*![](../../OEBPS/Images/AR_y.png)[1] = ![](../../OEBPS/Images/AR_b.png)
    as ![](../../OEBPS/Images/AR_y.png)[1] = *S^T* ![](../../OEBPS/Images/AR_b.png)
    (no matrix inversion)'
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 1. 解决：*S*![](../../OEBPS/Images/AR_y.png)[1] = ![](../../OEBPS/Images/AR_b.png)
    作为 ![](../../OEBPS/Images/AR_y.png)[1] = *S^T* ![](../../OEBPS/Images/AR_b.png)（无需矩阵求逆）
- en: '⑦ 2\. Solve: Λ ![](../../OEBPS/Images/AR_y.png)[2] = ![](../../OEBPS/Images/AR_y.png)[1]
    as ![](../../OEBPS/Images/AR_y.png)[2] = Λ^(-1)![](../../OEBPS/Images/AR_y.png)[1]
    (inverting a diagonal matrix is easy; see equation [2.31](02.xhtml#eq-diag-inv).)'
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 2. 解决：Λ ![](../../OEBPS/Images/AR_y.png)[2] = ![](../../OEBPS/Images/AR_y.png)[1]
    作为 ![](../../OEBPS/Images/AR_y.png)[2] = Λ^(-1)![](../../OEBPS/Images/AR_y.png)[1]（对角矩阵求逆很容易；见方程
    [2.31](02.xhtml#eq-diag-inv)。）
- en: '⑧ 3\. Solve: *S^T*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_y.png)[2]
    as ![](../../OEBPS/Images/AR_x.png) = *S*![](../../OEBPS/Images/AR_y.png)[2] (no
    matrix inversion)'
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 3. 解：*S^T*![图片](../../OEBPS/Images/AR_x.png) = ![图片](../../OEBPS/Images/AR_y.png)[2]
    作为 ![图片](../../OEBPS/Images/AR_x.png) = *S*![图片](../../OEBPS/Images/AR_y.png)[2]（无需矩阵求逆）
- en: ⑨ Verifies that the two solutions are the same
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 验证两个解相同
- en: ⑨ Verifies that the two solutions are the same
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 验证两个解相同
- en: 2.15.4 Matrix powers using diagonalization
  id: totrans-773
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.15.4 使用对角化计算矩阵幂
- en: If matrix *A* can be diagonalized, then
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: 如果矩阵 *A* 可以对角化，那么
- en: '![](../../OEBPS/Images/eq_02-31-c.png)'
  id: totrans-775
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_02-31-c.png)'
- en: For a diagonal matrix
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: 对于对角矩阵
- en: '![](../../OEBPS/Images/eq_02-31-d.png)'
  id: totrans-777
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_02-31-d.png)'
- en: the *n*th power is simply
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: 第 *n* 次幂简单地是
- en: '![](../../OEBPS/Images/eq_02-31-e.png)'
  id: totrans-779
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_02-31-e.png)'
- en: If we need to compute various powers of an *m* × *m* matrix A at different times,
    we should precompute the matrix S and compute any power with only *O*(*m*) operations—compared
    to the (*nm*³) operations necessary for naive computations.
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要在不同时间计算矩阵 A 的各种幂，我们应该预先计算矩阵 S，然后只需进行 *O*(*m*) 次操作即可计算任何幂——与原始计算所需的 (*nm*³)
    次操作相比。
- en: 2.16 Spectral decomposition of a symmetric matrix
  id: totrans-781
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.16 对称矩阵的谱分解
- en: We have seen in section [2.15](02.xhtml#sec-mat-diagonalization) that a square
    symmetric matrix with distinct eigenvalues can be decomposed as
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 [2.15](02.xhtml#sec-mat-diagonalization) 节中看到，具有不同特征值的平方对称矩阵可以分解为
- en: '*A* = *S*Λ*S^T*'
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: '*A* = *S*Λ*S^T*'
- en: where
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '*A* = [![](../../OEBPS/Images/AR_e.png)[1] ![](../../OEBPS/Images/AR_e.png)[2]
    … ![](../../OEBPS/Images/AR_e.png)[n]]'
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: '*A* = [![图片](../../OEBPS/Images/AR_e.png)[1] ![图片](../../OEBPS/Images/AR_e.png)[2]
    … ![图片](../../OEBPS/Images/AR_e.png)[n]]'
- en: Thus,
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，
- en: '![](../../OEBPS/Images/eq_02-31-f.png)'
  id: totrans-787
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_02-31-f.png)'
- en: This equation can be rewritten as
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程可以重写为
- en: '*A* = *λ*[1]![](../../OEBPS/Images/AR_e.png)[1]![](../../OEBPS/Images/AR_e.png)[1]^T
    + *λ*[2]![](../../OEBPS/Images/AR_e.png)[2]![](../../OEBPS/Images/AR_e.png)[2]*^T*
    + … + *λ[n]![](../../OEBPS/Images/AR_e.png)[n]*![](../../OEBPS/Images/AR_e.png)*[n]^T*'
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: '*A* = *λ*[1]![图片](../../OEBPS/Images/AR_e.png)[1]![图片](../../OEBPS/Images/AR_e.png)[1]^T
    + *λ*[2]![图片](../../OEBPS/Images/AR_e.png)[2]![图片](../../OEBPS/Images/AR_e.png)[2]*^T*
    + … + *λ[n]![图片](../../OEBPS/Images/AR_e.png)[n]*![图片](../../OEBPS/Images/AR_e.png)*[n]^T*'
- en: Equation 2.32
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2.32
- en: Thus a square symmetric matrix can be written in terms of its eigenvalues and
    eigenvectors. This is the spectral resolution theorem.
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个平方对称矩阵可以用其特征值和特征向量来表示。这是谱分解定理。
- en: 2.16.1 PyTorch code for the spectral decomposition of a matrix
  id: totrans-792
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.16.1 矩阵谱分解的 PyTorch 代码
- en: The following listing shows the relevant code for this section.
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了本节的相关代码。
- en: Listing 2.20 Spectral decomposition of a matrix
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.20 矩阵的谱分解
- en: '[PRE19]'
  id: totrans-795
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ① Asserts that A is a 2D tensor (i.e., matrix)
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: ① 断言 A 是一个二维张量（即，矩阵）
- en: '② A is square: i.e., *A*.*shape*[0] (num rows) ≜ *A*.*shape*[1] (num columns)'
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
  zh: ② A 是平方的：即，*A*.*shape*[0]（行数）≜ *A*.*shape*[1]（列数）
- en: '③ Asserts that A is symmetric: i.e., *A* = = *A^T*'
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 断言 A 是对称的：即，*A* = = *A^T*
- en: ④ The PyTorch function eig() returns eigenvectors and values.
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: ④ PyTorch 函数 eig() 返回特征向量和值。
- en: ⑤ Defines a 3D tensor *C* of shape *n* × *n* × *n* to hold the *n* components
    from equation [2.32](02.xhtml#eq-spectral-decomp). Each term *λ[i]![](../../OEBPS/Images/AR_e.png)[i]![](../../OEBPS/Images/AR_e.png)**^T*
    is an *n* × *n* matrix. There are *n* such terms, all compactly held in tensor
    *C*.
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 定义一个形状为 *n* × *n* × *n* 的三维张量 *C* 来存储方程 [2.32](02.xhtml#eq-spectral-decomp)
    中的 *n* 个分量。每个项 *λ[i]![图片](../../OEBPS/Images/AR_e.png)[i]![图片](../../OEBPS/Images/AR_e.png)**^T*
    是一个 *n* × *n* 矩阵。这里有 *n* 个这样的项，所有这些项都紧凑地存储在张量 *C* 中。
- en: ⑥ Computes *C*[*i*] = *λ[i]![](../../OEBPS/Images/AR_e.png)[i]![](../../OEBPS/Images/AR_e.png)**^T*
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 计算 *C*[*i*] = *λ[i]![图片](../../OEBPS/Images/AR_e.png)[i]![图片](../../OEBPS/Images/AR_e.png)**^T*
- en: ⑦ Reconstructs *A* by adding its components stored in *C*
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 通过添加存储在 *C* 中的分量来重建 *A*
- en: ⑧ Verifies that the matrix reconstructed from spectral components matches the
    original
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 验证从谱分量重建的矩阵与原始矩阵匹配
- en: '2.17 An application relevant to machine learning: Finding the axes of a hyperellipse'
  id: totrans-804
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.17 与机器学习相关的一个应用：寻找超椭圆的轴
- en: The notion of an ellipse in high-dimensional space (aka hyperellipse) keeps
    coming back in various forms in machine learning. Here we will make a preliminary
    review of them. We will revisit these concepts later.
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，高维空间中椭圆的概念（也称为超椭圆）以各种形式不断出现。在这里，我们将对这些概念进行初步回顾。我们稍后还会重新审视这些概念。
- en: 'Recall the equation of an ellipse from high school math:'
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下高中数学中椭圆的方程：
- en: '![](../../OEBPS/Images/eq_02-32-a.png)'
  id: totrans-807
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_02-32-a.png)'
- en: 'This is a rather simple ellipse: it is two-dimensional and centered at the
    origin, and its major and minor axes are aligned with the coordinate axes. Denoting
    ![](../../OEBPS/Images/eq_02-32-b.png) as the position vector, the same equation
    can be written as'
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相当简单的椭圆：它是二维的，以原点为中心，其主轴和副轴与坐标轴对齐。将 ![](../../OEBPS/Images/eq_02-32-b.png)
    表示为位置向量，相同的方程可以写成
- en: '![](../../OEBPS/Images/AR_x.png)*^TΛ*![](../../OEBPS/Images/AR_x.png) = 1'
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../../OEBPS/Images/AR_x.png)*^TΛ*![](../../OEBPS/Images/AR_x.png) = 1'
- en: where ![](../../OEBPS/Images/eq_02-32-c.png) is a diagonal matrix. Written in
    this form, the equation can be extended beyond 2D to an *n*-dimensional axis-aligned
    ellipse centered at the origin. Now let’s apply a rotation *R* to the axis. Then
    every vector ![](../../OEBPS/Images/AR_x.png) transforms to *R*![](../../OEBPS/Images/AR_x.png).
    The equation of the ellipse in the new (rotated) coordinate system is
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](../../OEBPS/Images/eq_02-32-c.png) 是一个对角矩阵。写成这种形式，方程可以扩展到二维以上的 *n*-维轴对齐椭圆，以原点为中心。现在让我们对轴应用一个旋转
    *R*。然后每个向量 ![](../../OEBPS/Images/AR_x.png) 都会变换为 *R*![](../../OEBPS/Images/AR_x.png)。在新（旋转）坐标系中椭圆的方程是
- en: (*R![](../../OEBPS/Images/AR_x.png)^T Λ* (*R![](../../OEBPS/Images/AR_x.png)*)
    = 1
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
  zh: (*R![](../../OEBPS/Images/AR_x.png)^T Λ* (*R![](../../OEBPS/Images/AR_x.png)*)
    = 1
- en: ⇔ ![](../../OEBPS/Images/AR_x.png)^T (*R^T ΛR*) ![](../../OEBPS/Images/AR_x.png)
    = 1
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
  zh: ⇔ ![](../../OEBPS/Images/AR_x.png)^T (*R^T ΛR*) ![](../../OEBPS/Images/AR_x.png)
    = 1
- en: where
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '*A* = (*R^TΛR*).'
  id: totrans-814
  prefs: []
  type: TYPE_NORMAL
  zh: '*A* = (*R^TΛR*).'
- en: The generalized equation of the ellipse is
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
  zh: 椭圆的广义方程是
- en: '![](../../OEBPS/Images/AR_x.png)*^TA*![](../../OEBPS/Images/AR_x.png) = 1'
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../../OEBPS/Images/AR_x.png)*^TA*![](../../OEBPS/Images/AR_x.png) = 1'
- en: 'Note the following:'
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: 注意以下内容：
- en: The ellipse is no longer axis aligned.
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 椭圆不再是轴对齐的。
- en: The matrix *A* is no longer diagonal.
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵 *A* 不再是对角的。
- en: '*A* is symmetric. We can easily verify that *A^T* = (*R^T*Λ*R*)*^T* = *R^T*Λ*^TR*
    = *R^T*Λ*R* (remember, the transpose of a diagonal matrix is itself).'
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*A* 是对称的。我们可以很容易地验证 *A^T* = (*R^T*Λ*R*)*^T* = *R^T*Λ*^TR* = *R^T*Λ*R*（记住，对角矩阵的转置是其本身）。'
- en: If, in addition, we want to get rid of the “centered at the origin” assumption,
    we get
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们还想摆脱“以原点为中心”的假设，我们得到
- en: (![](../../OEBPS/Images/AR_x.png)−*μ*)*^TA*(![](../../OEBPS/Images/AR_x.png)−*μ*)
    = 1
  id: totrans-822
  prefs: []
  type: TYPE_NORMAL
  zh: (![](../../OEBPS/Images/AR_x.png)−*μ*)*^TA*(![](../../OEBPS/Images/AR_x.png)−*μ*)
    = 1
- en: Equation 2.33
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2.33
- en: Now let’s flip the problem around. Suppose we have a generic *n*-dimensional
    ellipse. How do we compute its axes’ directions?
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们反过来思考这个问题。假设我们有一个通用的 *n*-维椭圆。我们如何计算其轴的方向？
- en: Clearly, if we can rotate the coordinate system so that the matrix in the middle
    is diagonal, we are done. Diagonalization (see section [2.15](02.xhtml#sec-mat-diagonalization))
    is the answer. Specifically, we find the matrix *S* with eigenvectors of *A* in
    its columns. This is a rotation matrix (being orthogonal, since *A* is symmetric).
    We transform rotate) the coordinate system by applying this matrix. In this new
    coordinate system, the ellipse is axis aligned. Stated another way, the new coordinate
    axes—these are the eigenvectors of *A*—yield the axes of the ellipse.
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，如果我们能旋转坐标系，使得中间的矩阵是对角的，我们就完成了。对角化（见第 [2.15](02.xhtml#sec-mat-diagonalization)
    节）是答案。具体来说，我们找到具有 *A* 的特征向量的矩阵 *S* 作为其列。这是一个旋转矩阵（因为 *A* 是对称的）。我们通过应用这个矩阵来旋转坐标系。在这个新的坐标系中，椭圆是轴对齐的。换句话说，新的坐标轴——这些是
    *A* 的特征向量——给出了椭圆的轴。
- en: 2.17.1 PyTorch code for hyperellipses
  id: totrans-826
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.17.1 用于超椭圆的 PyTorch 代码
- en: Let’s try finding the axes of the hyperellipse described by the equation 5*x*²
    + 6*xy* + 5*y*² = 20. Note that the actual ellipse we use as an example is 2D
    (to facilitate visualization), but the code we develop will be general and extensible
    to multiple dimensions.
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试找到由方程 5*x*² + 6*xy* + 5*y*² = 20 描述的超椭圆的轴。请注意，我们用作示例的实际椭圆是二维的（以便于可视化），但我们开发的代码将是通用的，并可以扩展到多个维度。
- en: The ellipse equation can be written using matrices and vectors as ![](../../OEBPS/Images/AR_x.png)*^TA*![](../../OEBPS/Images/AR_x.png)
    = 1, where
  id: totrans-828
  prefs: []
  type: TYPE_NORMAL
  zh: 椭圆方程可以用矩阵和向量表示为 ![](../../OEBPS/Images/AR_x.png)*^TA*![](../../OEBPS/Images/AR_x.png)
    = 1，其中
- en: '![](../../OEBPS/Images/eq_02-33-a.png)'
  id: totrans-829
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_02-33-a.png)'
- en: 'To find the axes of the hyperellipse, we need to transform the coordinate system
    so that the matrix in the middle becomes diagonal. Here is how this can be done:
    if we diagonalize *A* into *S*Σ*S*^(−1), then the ellipse equation becomes ![](../../OEBPS/Images/AR_x.png)*^TS*Σ*S*^(−1)![](../../OEBPS/Images/AR_x.png)
    = 1, where Σ is a diagonal matrix. Since *A* is symmetric, its eigenvectors are
    orthogonal. Hence, the matrix containing these eigenvectors as columns is orthogonal:
    i.e., *S*^(−1) = *S^T*. In other words, *S* is a rotation matrix. So the ellipse
    equation becomes ![](../../OEBPS/Images/AR_x.png)*^TS*Σ*S^T*![](../../OEBPS/Images/AR_x.png)
    = 1 or (![](../../OEBPS/Images/AR_x.png)*^TS*)Σ(*S^T*![](../../OEBPS/Images/AR_x.png))
    = 1 or ![](../../OEBPS/Images/AR_y.png)*^T*Σ![](../../OEBPS/Images/AR_y.png) =
    1 where ![](../../OEBPS/Images/AR_y.png) = *S^T*![](../../OEBPS/Images/AR_x.png).
    This is of the desired form since Σ is a diagonal matrix. Remember, *S* is a rotation
    matrix. Thus, rotating the coordinate system by *S* aligns the coordinate axes
    with the ellipse axes.'
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到超椭圆的轴，我们需要变换坐标系，使得中间的矩阵变为对角矩阵。以下是实现方法：如果我们把 *A* 对角化成 *S*Σ*S*^(−1)，那么椭圆方程变为
    ![](../../OEBPS/Images/AR_x.png)*^TS*Σ*S*^(−1)![](../../OEBPS/Images/AR_x.png)
    = 1，其中 Σ 是对角矩阵。由于 *A* 是对称的，其特征向量是正交的。因此，包含这些特征向量作为列的矩阵是正交的：即，*S*^(−1) = *S^T*.换句话说，*S*
    是一个旋转矩阵。因此，椭圆方程变为 ![](../../OEBPS/Images/AR_x.png)*^TS*Σ*S^T*![](../../OEBPS/Images/AR_x.png)
    = 1 或 (![](../../OEBPS/Images/AR_x.png)*^TS*)Σ(*S^T*![](../../OEBPS/Images/AR_x.png))
    = 1 或 ![](../../OEBPS/Images/AR_y.png)*^T*Σ![](../../OEBPS/Images/AR_y.png) =
    1，其中 ![](../../OEBPS/Images/AR_y.png) = *S^T*![](../../OEBPS/Images/AR_x.png)。由于
    Σ 是对角矩阵，这是期望的形式。记住，*S* 是一个旋转矩阵。因此，通过 *S* 旋转坐标系，坐标轴与椭圆轴对齐。
- en: Listing 2.21 Axes of a hyperellipse
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.21 超椭圆的轴
- en: '[PRE20]'
  id: totrans-832
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '① Equation of the ellipse: 5*x*² + 6*x**y* + 5*y*² = 20 or ![](../../OEBPS/Images/AR_x.png)*^TA*![](../../OEBPS/Images/AR_x.png)
    = 20, where ![](../../OEBPS/Images/eq_02-33-b.png)'
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
  zh: ① 椭圆的方程：5*x*² + 6*x**y* + 5*y*² = 20 或 ![](../../OEBPS/Images/AR_x.png)*^TA*![](../../OEBPS/Images/AR_x.png)
    = 20，其中 ![](../../OEBPS/Images/eq_02-33-b.png)
- en: ② X-axis vector
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
  zh: ② X轴向量
- en: ③ Major axis of the ellipse
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 椭圆的主轴
- en: ④ The dot product between two vectors is the cosine of the angle between them.
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 两个向量之间的点积是它们之间角度的余弦值。
- en: '⑤ The angle between the ellipse’s major axis and the X-axis: 45° (see figure
    [2.16](02.xhtml#fig-numpy-hyper-ellipse))'
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 椭圆主轴与X轴之间的角度：45°（见图 [2.16](02.xhtml#fig-numpy-hyper-ellipse)）
- en: '![](../../OEBPS/Images/CH02_F16_Chaudhury.png)'
  id: totrans-838
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH02_F16_Chaudhury.png)'
- en: Figure 2.16 Note that the ellipse’s major axis forms an angle of 45 degrees
    with the X-axis. Rotating the coordinate system by this angle will align the ellipse
    axes with the coordinate axes. Subsequently, the first principal vector will also
    lie along this direction.
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.16 注意到椭圆的主轴与X轴形成45度的角度。通过这个角度旋转坐标系，将椭圆轴与坐标轴对齐。随后，第一个主向量也将沿着这个方向。
- en: Summary
  id: totrans-840
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: In machine learning, a vector is a one-dimensional array of numbers and a matrix
    is a two-dimensional array of numbers. Inputs and outputs of machine learning
    models are typically represented as vectors or matrices. In multilayered models,
    inputs and outputs of each individual layer are also represented as vectors or
    matrices. Images are two-dimensional arrays of numbers corresponding to pixel
    color values. As such, they are represented as matrices.
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在机器学习中，向量是一维数字数组，矩阵是二维数字数组。机器学习模型的输入和输出通常表示为向量或矩阵。在多层模型中，每个单独层的输入和输出也以向量或矩阵的形式表示。图像是表示像素颜色值的二维数字数组。因此，它们以矩阵的形式表示。
- en: An *n*-dimensional vector can be viewed as point in ℝ*^n* space. All models
    can be viewed as functions that map points from input to output space. The model
    is designed so that it is easier to solve the problem of interest in the output
    space.
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 *n*-维向量可以看作是 ℝ*^n* 空间中的一个点。所有模型都可以看作是从输入空间映射到输出空间的函数。模型设计得使得在输出空间中解决感兴趣的问题更容易。
- en: A dot product between a pair of vectors ![](../../OEBPS/Images/AR_x.png) = [*x*[1]
       *x*[2]   …   *x[n]*] and ![](../../OEBPS/Images/AR_y.png) = [*y*[1]    *y*[2]
      …   *y[n]*] is the scalar quantity ![](../../OEBPS/Images/AR_x.png) ⋅ ![](../../OEBPS/Images/AR_y.png)
    = *x*[1]*y*[1] + *x*[2]*y*[2] + ⋯ + *x[n]**y**[n]*. It is a measure of how similar
    the vectors are. Dot products are widely used in machine learning. For instance,
    in supervised machine learning, we train the model so that its output is as similar
    as possible to the known output for a sample set of input points known as training
    data. Here, some variant of the dot product is often used to measure the similarity
    of the model output and the known output. Two vectors are orthogonal if their
    dot product is zero. This means the vectors have no similarity and are independent
    of each other.
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个向量 ![](../../OEBPS/Images/AR_x.png) = [*x*[1]    *x*[2]   …   *x[n]*] 和 ![](../../OEBPS/Images/AR_y.png)
    = [*y*[1]    *y*[2]   …   *y[n]*] 的点积是一个标量量 ![](../../OEBPS/Images/AR_x.png) ⋅
    ![](../../OEBPS/Images/AR_y.png) = *x*[1]*y*[1] + *x*[2]*y*[2] + ⋯ + *x[n]**y**[n]*。它是衡量向量相似程度的一个指标。点积在机器学习中得到了广泛的应用。例如，在监督机器学习中，我们训练模型，使其输出尽可能接近已知输出，这些已知输出是针对一组已知输入点（称为训练数据）的。在这里，点积的某种变体通常用于衡量模型输出和已知输出的相似度。如果两个向量的点积为零，则这两个向量是正交的。这意味着向量之间没有相似性，彼此独立。
- en: A vector’s dot product with itself is the square of the magnitude or length
    of the vector ![](../../OEBPS/Images/AR_x.png) ⋅ ![](../../OEBPS/Images/AR_x.png)
    = ||![](../../OEBPS/Images/AR_x.png)||² = *x*[1]*x*[1] + *x*[2]*x*[2] + ⋯ + *x[n]x[n]*.
  id: totrans-844
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个向量与其自身的点积是向量的模长或长度的平方 ![](../../OEBPS/Images/AR_x.png) ⋅ ![](../../OEBPS/Images/AR_x.png)
    = ||![](../../OEBPS/Images/AR_x.png)||² = *x*[1]*x*[1] + *x*[2]*x*[2] + ⋯ + *x[n]x[n]*。
- en: Given a set of vectors ![](../../OEBPS/Images/AR_x.png)[1], ![](../../OEBPS/Images/AR_x.png)[2],
    ⋯, ![](../../OEBPS/Images/AR_x.png)*[n]*, the weighted sum *a*[1]![](../../OEBPS/Images/AR_x.png)[1]
    + *a*[2]![](../../OEBPS/Images/AR_x.png)[2] + ⋯ + *a[n]*![](../../OEBPS/Images/AR_x.png)*[n]*
    where *a*[1], *a*[2], ⋯, *a[n]* are arbitrary scalars) is known as a linear combination.
    In particular, if the coefficients *a*[1], *a*[2], ⋯, *a[n]* are non-negative
    and they sum to 1, the linear combination is called a *convex* combination.
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定一组向量 ![](../../OEBPS/Images/AR_x.png)[1], ![](../../OEBPS/Images/AR_x.png)[2],
    ⋯, ![](../../OEBPS/Images/AR_x.png)*[n]*，其加权求和 *a*[1]![](../../OEBPS/Images/AR_x.png)[1]
    + *a*[2]![](../../OEBPS/Images/AR_x.png)[2] + ⋯ + *a[n]*![](../../OEBPS/Images/AR_x.png)*[n]*（其中
    *a*[1], *a*[2], ⋯, *a[n]* 是任意标量）被称为线性组合。特别是，如果系数 *a*[1], *a*[2], ⋯, *a[n]* 非负且它们的和为
    1，则这种线性组合被称为 *凸* 组合。
- en: If it is possible to find a set of coefficients *a*[1], *a*[2], ⋯, *a[n]*, not
    all zero, such that the linear combination is a null vector (meaning all its elements
    are zeros), then the vectors ![](../../OEBPS/Images/AR_x.png)[1], ![](../../OEBPS/Images/AR_x.png)[2],
    ⋯, ![](../../OEBPS/Images/AR_x.png)*[n]* are said to *linearly dependent*. On
    the other hand, if the only way to obtain a linear combination that is a null
    vector is to make every coefficient zero, the vectors are said to be *linearly
    independent*.
  id: totrans-846
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果可以找到一组系数 *a*[1], *a*[2], ⋯, *a[n]*（不全为零），使得线性组合是一个零向量（意味着所有元素都是零），则称向量 ![](../../OEBPS/Images/AR_x.png)[1],
    ![](../../OEBPS/Images/AR_x.png)[2], ⋯, ![](../../OEBPS/Images/AR_x.png)*[n]*
    是 *线性相关* 的。另一方面，如果唯一获得零向量线性组合的方法是使所有系数为零，则称这些向量是 *线性无关* 的。
- en: One important application of matrices and vectors is to solve a system of linear
    equations. Such a system can be expressed in matrix vector terms as *A*![](../../OEBPS/Images/AR_x.png)
    = ![](../../OEBPS/Images/AR_b.png), where we solve for an unknown vector ![](../../OEBPS/Images/AR_x.png)
    satisfying the equation. This system has an exact solution if and only if *A*
    is invertible. This means *A* is a square matrix (the number of rows equals the
    number of columns) and the row vectors are linearly independent. If the row vectors
    are linearly independent, so are the column vectors, and vice versa. If the rows
    and columns are linearly independent, the determinant of *A* is guaranteed to
    be nonzero. Hence, linear independence of rows/columns and nonzero determinant
    are equivalent conditions. If any one of them is satisfied, the linear system
    has an exact and unique solution.
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵和向量的一个重要应用是解线性方程组。这样的系统可以用矩阵向量形式表示为 *A*![](../../OEBPS/Images/AR_x.png) =
    ![](../../OEBPS/Images/AR_b.png)，其中我们求解满足该方程的未知向量 ![](../../OEBPS/Images/AR_x.png)。当且仅当
    *A* 可逆时，该系统有精确解。这意味着 *A* 是一个方阵（行数等于列数）且行向量线性无关。如果行向量线性无关，则列向量也线性无关，反之亦然。如果行和列线性无关，则
    *A* 的行列式保证不为零。因此，行/列的线性无关性和非零行列式是等价条件。如果满足其中任何一个条件，线性系统就有精确且唯一的解。
- en: In practice, this requirement is often not met, and we have an over- or under-determined
    system. In such situations, the Moore-Penrose inverse leads to a form of best
    approximation. Geometrically, the Moore-Penrose method yields the point that is
    closest to ![](../../OEBPS/Images/AR_b.png) in the space of vectors spanned by
    columns of *A*. Equivalently, the Moore-Penrose solution ![](../../OEBPS/Images/AR_x.png)[*]
    yields the point closest to ![](../../OEBPS/Images/AR_b.png) on the space of vectors
    spanned by the columns of *A*.
  id: totrans-848
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在实践中，这种要求往往不满足，我们得到的是一个超定或欠定系统。在这种情况下，Moore-Penrose 逆导出一种最佳逼近形式。从几何上看，Moore-Penrose
    方法得到的是在由 *A* 的列向量张成的向量空间中，最接近 ![](../../OEBPS/Images/AR_b.png) 的点。等价地，Moore-Penrose
    解 ![](../../OEBPS/Images/AR_x.png)[*] 得到的是在由 *A* 的列向量张成的向量空间中，最接近 ![](../../OEBPS/Images/AR_b.png)
    的点。
- en: For a square matrix *A*, if and only if *Aê* = *λê*, we say *λ* is an eigenvalue
    (a scalar) and *ê* is an eigenvector (a unit vector) of *A*. Physically, the eigenvector
    *ê* is a unit vector whose direction does not change when transformed by the matrix
    *A*. The transform can magnify its length by the scalar scale factor *λ*, which
    is the eigenvalue.
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于一个方阵 *A*，如果且仅如果 *Aê* = *λê*，我们称 *λ* 为 *A* 的特征值（一个标量），*ê* 为 *A* 的特征向量（一个单位向量）。在物理上，特征向量
    *ê* 是一个单位向量，其方向在矩阵 *A* 变换时不会改变。变换可以通过标量尺度因子 *λ*（即特征值）放大其长度。
- en: An *n* × *n* matrix *A* has *n* eigenvalue/eigenvector pairs. The eigenvalues
    need not all be unique. The eigenvectors corresponding to different eigenvalues
    are linearly independent. If the matrix *A* is symmetric, satisfying *A^T* = *A*,
    the eigenvectors corresponding to different eigenvalues are orthogonal. A rotation
    matrix is a matrix in which the rows are orthogonal to each other and so are the
    columns. Such a matrix is also known as an orthogonal matrix. An orthogonal matrix
    *R* satisfies the equation *R^TR* = **I**, where **I** is the identity matrix.
    In the special case when the matrix *A* is a rotation matrix *R*, one of the eigenvalues
    is always 1. The corresponding eigenvector is the axis of rotation. A matrix *A*
    with *n* linearly independent eigenvectors can be decomposed as *A* = *S*Λ*S*^(−1),
    where *S* =[![](../../OEBPS/Images/AR_e.png)[1]    ![](../../OEBPS/Images/AR_e.png)[2]
      …   *![](../../OEBPS/Images/AR_e.png)[n]*] is the matrix with eigenvectors of
    *A* as its columns and Λ is a diagonal matrix with the eigenvalues of *A* as its
    diagonal. This decomposition is called matrix diagonalization and leads to a numerically
    stable way to solve linear systems.
  id: totrans-850
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个 *n* × *n* 的矩阵 *A* 有 *n* 个特征值/特征向量对。特征值不一定都是唯一的。对应于不同特征值的特征向量是线性无关的。如果矩阵 *A*
    是对称的，满足 *A^T* = *A*，则对应于不同特征值的特征向量是正交的。旋转矩阵是一个行向量彼此正交且列向量也彼此正交的矩阵。这样的矩阵也被称为正交矩阵。一个正交矩阵
    *R* 满足方程 *R^TR* = **I**，其中 **I** 是单位矩阵。在矩阵 *A* 是旋转矩阵 *R* 的特殊情况下，一个特征值总是 1。对应的特征向量是旋转轴。一个具有
    *n* 个线性无关特征向量的矩阵 *A* 可以分解为 *A* = *S*Λ*S*^(−1)，其中 *S* = [![](../../OEBPS/Images/AR_e.png)[1]   ![](../../OEBPS/Images/AR_e.png)[2]   …   *![](../../OEBPS/Images/AR_e.png)[n]*]
    是以 *A* 的特征向量作为其列的矩阵，Λ 是对角矩阵，其对角线上的元素是 *A* 的特征值。这种分解称为矩阵对角化，并导致求解线性系统的一种数值稳定方法。
- en: A square symmetric matrix *A* can be expressed in terms of its eigenvectors
    and eigenvalues as *A* = *λ*[1]![](../../OEBPS/Images/AR_e.png)[1]![](../../OEBPS/Images/AR_e.png)[1]*^T*
    + *λ*[2]![](../../OEBPS/Images/AR_e.png)[2]![](../../OEBPS/Images/AR_e.png)[2]*^T*
    +   …  + *λ[n]![](../../OEBPS/Images/AR_e.png)[n]*![](../../OEBPS/Images/AR_e.png)*[n]^T*.
    This is known as the spectral decomposition of the matrix *A*.
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个平方对称矩阵 *A* 可以用其特征向量和特征值来表示，即 *A* = *λ*[1]![](../../OEBPS/Images/AR_e.png)[1]![](../../OEBPS/Images/AR_e.png)[1]*^T*
    + *λ*[2]![](../../OEBPS/Images/AR_e.png)[2]![](../../OEBPS/Images/AR_e.png)[2]*^T*
    + ... + *λ[n]![](../../OEBPS/Images/AR_e.png)[n]*![](../../OEBPS/Images/AR_e.png)*[n]^T*。这被称为矩阵
    *A* 的谱分解。
- en: '* * *'
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ¹  In mathematics, vectors can have an infinite number of elements. Such vectors
    cannot be expressed as arrays—but we will mostly ignore them in this book. [↩](02.xhtml#fnref4)
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
  zh: ¹  在数学中，向量可以有无穷多个元素。这样的向量不能表示为数组——但在这本书中我们主要会忽略它们。[↩](02.xhtml#fnref4)
- en: ²  We usually use uppercase letters to symbolize matrices. [↩](02.xhtml#fnref5)
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: ²  我们通常使用大写字母来表示矩阵。[↩](02.xhtml#fnref5)
- en: ³  In digital computers, numbers in the range 0..255 can be represented with
    a single byte of storage; hence this choice. [↩](02.xhtml#fnref6)
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: ³  在数字计算机中，范围在0..255之间的数字可以用一个字节的存储来表示；因此选择了这种方案。[↩](02.xhtml#fnref6)
- en: ⁴  The mathematical symbol ∀ stands for “for all.” Thus, ∀![](../../OEBPS/Images/AR_y.png)
    ∈ ℜ*^n* means “all vectors y in the *n*-dimensional space.” [↩](02.xhtml#fnref7)
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴  数学符号∀代表“对所有”。因此，∀![](../../OEBPS/Images/AR_y.png) ∈ ℜ*^n* 表示“*n*-维空间中的所有向量y”。[↩](02.xhtml#fnref7)
- en: ⁵  You can compute eigenvectors and eigenvalues only of square matrices. [↩](02.xhtml#fnref8)
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
  zh: ⁵  你只能计算方阵的特征向量和特征值。[↩](02.xhtml#fnref8)
