- en: Chapter 14\. Deep Computer Vision Using Convolutional Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第14章。使用卷积神经网络进行深度计算机视觉
- en: 'Although IBM’s Deep Blue supercomputer beat the chess world champion Garry
    Kasparov back in 1996, it wasn’t until fairly recently that computers were able
    to reliably perform seemingly trivial tasks such as detecting a puppy in a picture
    or recognizing spoken words. Why are these tasks so effortless to us humans? The
    answer lies in the fact that perception largely takes place outside the realm
    of our consciousness, within specialized visual, auditory, and other sensory modules
    in our brains. By the time sensory information reaches our consciousness, it is
    already adorned with high-level features; for example, when you look at a picture
    of a cute puppy, you cannot choose *not* to see the puppy, *not* to notice its
    cuteness. Nor can you explain *how* you recognize a cute puppy; it’s just obvious
    to you. Thus, we cannot trust our subjective experience: perception is not trivial
    at all, and to understand it we must look at how our sensory modules work.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管IBM的Deep Blue超级计算机在1996年击败了国际象棋世界冠军加里·卡斯帕罗夫，但直到最近计算机才能可靠地执行看似微不足道的任务，比如在图片中检测小狗或识别口语。为什么这些任务对我们人类来说如此轻松？答案在于感知主要发生在我们的意识领域之外，在我们大脑中专门的视觉、听觉和其他感觉模块中。当感觉信息达到我们的意识时，它已经被赋予高级特征；例如，当你看到一张可爱小狗的图片时，你无法选择*不*看到小狗，*不*注意到它的可爱。你也无法解释*如何*识别一个可爱的小狗；对你来说这是显而易见的。因此，我们不能信任我们的主观经验：感知并不是微不足道的，要理解它，我们必须看看我们的感觉模块是如何工作的。
- en: '*Convolutional neural networks* (CNNs) emerged from the study of the brain’s
    visual cortex, and they have been used in computer image recognition since the
    1980s. Over the last 10 years, thanks to the increase in computational power,
    the amount of available training data, and the tricks presented in [Chapter 11](ch11.html#deep_chapter)
    for training deep nets, CNNs have managed to achieve superhuman performance on
    some complex visual tasks. They power image search services, self-driving cars,
    automatic video classification systems, and more. Moreover, CNNs are not restricted
    to visual perception: they are also successful at many other tasks, such as voice
    recognition and natural language processing. However, we will focus on visual
    applications for now.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*卷积神经网络*（CNNs）起源于对大脑视觉皮层的研究，自上世纪80年代以来就被用于计算机图像识别。在过去的10年里，由于计算能力的增加、可用训练数据的增加，以及[第11章](ch11.html#deep_chapter)中介绍的用于训练深度网络的技巧，CNNs已经成功在一些复杂的视觉任务上实现了超人类表现。它们驱动着图像搜索服务、自动驾驶汽车、自动视频分类系统等。此外，CNNs并不局限于视觉感知：它们在许多其他任务上也取得了成功，比如语音识别和自然语言处理。然而，我们现在将专注于视觉应用。'
- en: In this chapter we will explore where CNNs came from, what their building blocks
    look like, and how to implement them using Keras. Then we will discuss some of
    the best CNN architectures, as well as other visual tasks, including object detection
    (classifying multiple objects in an image and placing bounding boxes around them)
    and semantic segmentation (classifying each pixel according to the class of the
    object it belongs to).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨CNNs的起源，它们的构建模块是什么样的，以及如何使用Keras实现它们。然后我们将讨论一些最佳的CNN架构，以及其他视觉任务，包括目标检测（对图像中的多个对象进行分类并在其周围放置边界框）和语义分割（根据对象所属的类别对每个像素进行分类）。
- en: The Architecture of the Visual Cortex
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视觉皮层的结构
- en: David H. Hubel and Torsten Wiesel performed a series of experiments on cats
    in [1958](https://homl.info/71)⁠^([1](ch14.html#idm45720185768624)) and [1959](https://homl.info/72)⁠^([2](ch14.html#idm45720185766752))
    (and a [few years later on monkeys](https://homl.info/73)⁠^([3](ch14.html#idm45720185764704))),
    giving crucial insights into the structure of the visual cortex (the authors received
    the Nobel Prize in Physiology or Medicine in 1981 for their work). In particular,
    they showed that many neurons in the visual cortex have a small *local receptive
    field*, meaning they react only to visual stimuli located in a limited region
    of the visual field (see [Figure 14-1](#cat_visual_cortex_diagram), in which the
    local receptive fields of five neurons are represented by dashed circles). The
    receptive fields of different neurons may overlap, and together they tile the
    whole visual field.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: David H. Hubel和Torsten Wiesel在[1958年](https://homl.info/71)对猫进行了一系列实验⁠^([1](ch14.html#idm45720185768624))，[1959年](https://homl.info/72)（以及[几年后对猴子进行的实验](https://homl.info/73)⁠^([3](ch14.html#idm45720185764704)）），为视觉皮层的结构提供了关键见解（这两位作者因其工作于1981年获得了诺贝尔生理学或医学奖）。特别是，他们表明视觉皮层中许多神经元具有小的*局部感受野*，这意味着它们只对位于视觉场有限区域内的视觉刺激做出反应（见[图14-1](#cat_visual_cortex_diagram)，其中五个神经元的局部感受野由虚线圈表示）。不同神经元的感受野可能重叠，它们共同覆盖整个视觉场。
- en: '![mls3 1401](assets/mls3_1401.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1401](assets/mls3_1401.png)'
- en: Figure 14-1\. Biological neurons in the visual cortex respond to specific patterns
    in small regions of the visual field called receptive fields; as the visual signal
    makes its way through consecutive brain modules, neurons respond to more complex
    patterns in larger receptive fields
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-1\. 视觉皮层中的生物神经元对视觉场中称为感受野的小区域中的特定模式做出反应；随着视觉信号通过连续的大脑模块，神经元对更大感受野中的更复杂模式做出反应
- en: Moreover, the authors showed that some neurons react only to images of horizontal
    lines, while others react only to lines with different orientations (two neurons
    may have the same receptive field but react to different line orientations). They
    also noticed that some neurons have larger receptive fields, and they react to
    more complex patterns that are combinations of the lower-level patterns. These
    observations led to the idea that the higher-level neurons are based on the outputs
    of neighboring lower-level neurons (in [Figure 14-1](#cat_visual_cortex_diagram),
    notice that each neuron is connected only to nearby neurons from the previous
    layer). This powerful architecture is able to detect all sorts of complex patterns
    in any area of the visual field.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，作者们表明，一些神经元只对水平线的图像做出反应，而另一些只对具有不同方向的线做出反应（两个神经元可能具有相同的感受野，但对不同的线方向做出反应）。他们还注意到一些神经元具有更大的感受野，它们对更复杂的模式做出反应，这些模式是低级模式的组合。这些观察结果导致了这样一个想法，即高级神经元基于相邻低级神经元的输出（在[图14-1](#cat_visual_cortex_diagram)中，注意到每个神经元只连接到前一层附近的神经元）。这种强大的架构能够在视觉领域的任何区域检测各种复杂的模式。
- en: 'These studies of the visual cortex inspired the [neocognitron](https://homl.info/74),⁠^([4](ch14.html#idm45720185754832))
    introduced in 1980, which gradually evolved into what we now call convolutional
    neural networks. An important milestone was a [1998 paper](https://homl.info/75)⁠^([5](ch14.html#idm45720185753008))
    by Yann LeCun et al. that introduced the famous *LeNet-5* architecture, which
    became widely used by banks to recognize handwritten digits on checks. This architecture
    has some building blocks that you already know, such as fully connected layers
    and sigmoid activation functions, but it also introduces two new building blocks:
    *convolutional layers* and *pooling layers*. Let’s look at them now.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这些对视觉皮层的研究启发了1980年引入的[neocognitron](https://homl.info/74)，逐渐演变成我们现在称之为卷积神经网络的东西。一个重要的里程碑是Yann
    LeCun等人在1998年发表的一篇[论文](https://homl.info/75)，介绍了著名的*LeNet-5*架构，这种架构被银行广泛用于识别支票上的手写数字。这种架构具有一些你已经了解的构建块，如全连接层和Sigmoid激活函数，但它还引入了两个新的构建块：*卷积层*和*池化层*。现在让我们来看看它们。
- en: Note
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Why not simply use a deep neural network with fully connected layers for image
    recognition tasks? Unfortunately, although this works fine for small images (e.g.,
    MNIST), it breaks down for larger images because of the huge number of parameters
    it requires. For example, a 100 × 100–pixel image has 10,000 pixels, and if the
    first layer has just 1,000 neurons (which already severely restricts the amount
    of information transmitted to the next layer), this means a total of 10 million
    connections. And that’s just the first layer. CNNs solve this problem using partially
    connected layers and weight sharing.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不简单地使用具有全连接层的深度神经网络来进行图像识别任务呢？不幸的是，尽管这对于小图像（例如MNIST）效果很好，但对于较大的图像来说，由于需要的参数数量巨大，它会崩溃。例如，一个100×100像素的图像有10,000个像素，如果第一层只有1,000个神经元（这已经严重限制了传递到下一层的信息量），这意味着总共有1千万个连接。而这只是第一层。CNN通过部分连接的层和权重共享来解决这个问题。
- en: Convolutional Layers
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积层
- en: The most important building block of a CNN is the *convolutional layer*:⁠^([6](ch14.html#idm45720185745312))
    neurons in the first convolutional layer are not connected to every single pixel
    in the input image (like they were in the layers discussed in previous chapters),
    but only to pixels in their receptive fields (see [Figure 14-2](#cnn_layers_diagram)).
    In turn, each neuron in the second convolutional layer is connected only to neurons
    located within a small rectangle in the first layer. This architecture allows
    the network to concentrate on small low-level features in the first hidden layer,
    then assemble them into larger higher-level features in the next hidden layer,
    and so on. This hierarchical structure is common in real-world images, which is
    one of the reasons why CNNs work so well for image recognition.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: CNN最重要的构建块是*卷积层*：第一个卷积层中的神经元不与输入图像中的每个像素相连接（就像在前几章讨论的层中那样），而只与其感受野中的像素相连接（参见[图14-2](#cnn_layers_diagram)）。反过来，第二个卷积层中的每个神经元只与第一层中一个小矩形内的神经元相连接。这种架构允许网络在第一个隐藏层集中于小的低级特征，然后在下一个隐藏层中将它们组合成更大的高级特征，依此类推。这种分层结构在现实世界的图像中很常见，这也是CNN在图像识别方面表现出色的原因之一。
- en: '![mls3 1402](assets/mls3_1402.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1402](assets/mls3_1402.png)'
- en: Figure 14-2\. CNN layers with rectangular local receptive fields
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-2。具有矩形局部感受野的CNN层
- en: Note
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: All the multilayer neural networks we’ve looked at so far had layers composed
    of a long line of neurons, and we had to flatten input images to 1D before feeding
    them to the neural network. In a CNN each layer is represented in 2D, which makes
    it easier to match neurons with their corresponding inputs.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们看到的所有多层神经网络都由一长串神经元组成，我们必须在将输入图像馈送到神经网络之前将其展平为1D。在CNN中，每一层都以2D表示，这使得更容易将神经元与其对应的输入匹配。
- en: A neuron located in row *i*, column *j* of a given layer is connected to the
    outputs of the neurons in the previous layer located in rows *i* to *i* + *f*[*h*]
    – 1, columns *j* to *j* + *f*[*w*] – 1, where *f*[*h*] and *f*[*w*] are the height
    and width of the receptive field (see [Figure 14-3](#slide_and_padding_diagram)).
    In order for a layer to have the same height and width as the previous layer,
    it is common to add zeros around the inputs, as shown in the diagram. This is
    called *zero* *padding*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 给定层中位于第*i*行，第*j*列的神经元连接到前一层中位于第*i*到第*i* + *f*[*h*] – 1行，第*j*到第*j* + *f*[*w*]
    – 1列的神经元的输出，其中*f*[*h*]和*f*[*w*]是感受野的高度和宽度（参见[图14-3](#slide_and_padding_diagram)）。为了使一层具有与前一层相同的高度和宽度，通常在输入周围添加零，如图中所示。这称为*零填充*。
- en: It is also possible to connect a large input layer to a much smaller layer by
    spacing out the receptive fields, as shown in [Figure 14-4](#stride_diagram).
    This dramatically reduces the model’s computational complexity. The horizontal
    or vertical step size from one receptive field to the next is called the *stride*.
    In the diagram, a 5 × 7 input layer (plus zero padding) is connected to a 3 ×
    4 layer, using 3 × 3 receptive fields and a stride of 2 (in this example the stride
    is the same in both directions, but it does not have to be so). A neuron located
    in row *i*, column *j* in the upper layer is connected to the outputs of the neurons
    in the previous layer located in rows *i* × *s*[*h*] to *i* × *s*[*h*] + *f*[*h*]
    – 1, columns *j* × *s*[*w*] to *j* × *s*[*w*] + *f*[*w*] – 1, where *s*[*h*] and
    *s*[*w*] are the vertical and horizontal strides.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以通过间隔感受野来将大输入层连接到一个较小的层，如[图14-4](#stride_diagram)所示。这显着降低了模型的计算复杂性。从一个感受野到下一个感受野的水平或垂直步长称为*步幅*。在图中，一个5×7的输入层（加上零填充）连接到一个3×4的层，使用3×3的感受野和步幅为2（在这个例子中，步幅在两个方向上是相同的，但不一定要这样）。上层中位于第*i*行，第*j*列的神经元连接到前一层中位于第*i*×*s*[*h*]到第*i*×*s*[*h*]+*f*[*h*]–1行，第*j*×*s*[*w*]到第*j*×*s*[*w*]+*f*[*w*]–1列的神经元的输出，其中*s*[*h*]和*s*[*w*]是垂直和水平步幅。
- en: '![mls3 1403](assets/mls3_1403.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1403](assets/mls3_1403.png)'
- en: Figure 14-3\. Connections between layers and zero padding
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-3。层与零填充之间的连接
- en: '![mls3 1404](assets/mls3_1404.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1404](assets/mls3_1404.png)'
- en: Figure 14-4\. Reducing dimensionality using a stride of 2
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-4。使用步幅为2降低维度
- en: Filters
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 滤波器
- en: A neuron’s weights can be represented as a small image the size of the receptive
    field. For example, [Figure 14-5](#filters_diagram) shows two possible sets of
    weights, called *filters* (or *convolution kernels*, or just *kernels*). The first
    one is represented as a black square with a vertical white line in the middle
    (it’s a 7 × 7 matrix full of 0s except for the central column, which is full of
    1s); neurons using these weights will ignore everything in their receptive field
    except for the central vertical line (since all inputs will be multiplied by 0,
    except for the ones in the central vertical line). The second filter is a black
    square with a horizontal white line in the middle. Neurons using these weights
    will ignore everything in their receptive field except for the central horizontal
    line.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一个神经元的权重可以表示为一个与感受野大小相同的小图像。例如，[图14-5](#filters_diagram)显示了两组可能的权重，称为*滤波器*（或*卷积核*，或只是*内核*）。第一个滤波器表示为一个黑色正方形，中间有一条垂直白线（它是一个7×7的矩阵，除了中间列全是1，其他都是0）；使用这些权重的神经元将忽略其感受野中的所有内容，除了中间的垂直线（因为所有输入将被乘以0，除了中间的垂直线）。第二个滤波器是一个黑色正方形，中间有一条水平白线。使用这些权重的神经元将忽略其感受野中的所有内容，除了中间的水平线。
- en: '![mls3 1405](assets/mls3_1405.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1405](assets/mls3_1405.png)'
- en: Figure 14-5\. Applying two different filters to get two feature maps
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-5。应用两个不同的滤波器以获得两个特征图
- en: 'Now if all neurons in a layer use the same vertical line filter (and the same
    bias term), and you feed the network the input image shown in [Figure 14-5](#filters_diagram)
    (the bottom image), the layer will output the top-left image. Notice that the
    vertical white lines get enhanced while the rest gets blurred. Similarly, the
    upper-right image is what you get if all neurons use the same horizontal line
    filter; notice that the horizontal white lines get enhanced while the rest is
    blurred out. Thus, a layer full of neurons using the same filter outputs a *feature
    map*, which highlights the areas in an image that activate the filter the most.
    But don’t worry, you won’t have to define the filters manually: instead, during
    training the convolutional layer will automatically learn the most useful filters
    for its task, and the layers above will learn to combine them into more complex
    patterns.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果一个层中的所有神经元使用相同的垂直线滤波器（和相同的偏置项），并且您将输入图像输入到网络中，如[图14-5](#filters_diagram)所示（底部图像），该层将输出左上角的图像。请注意，垂直白线得到增强，而其余部分变得模糊。类似地，如果所有神经元使用相同的水平线滤波器，则会得到右上角的图像；请注意，水平白线得到增强，而其余部分被模糊化。因此，一个充满使用相同滤波器的神经元的层会输出一个*特征图*，突出显示激活滤波器最多的图像区域。但不用担心，您不必手动定义滤波器：相反，在训练期间，卷积层将自动学习其任务中最有用的滤波器，上面的层将学会将它们组合成更复杂的模式。
- en: Stacking Multiple Feature Maps
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 堆叠多个特征图
- en: Up to now, for simplicity, I have represented the output of each convolutional
    layer as a 2D layer, but in reality a convolutional layer has multiple filters
    (you decide how many) and outputs one feature map per filter, so it is more accurately
    represented in 3D (see [Figure 14-6](#cnn_layers_volume_diagram)). It has one
    neuron per pixel in each feature map, and all neurons within a given feature map
    share the same parameters (i.e., the same kernel and bias term). Neurons in different
    feature maps use different parameters. A neuron’s receptive field is the same
    as described earlier, but it extends across all the feature maps of the previous
    layer. In short, a convolutional layer simultaneously applies multiple trainable
    filters to its inputs, making it capable of detecting multiple features anywhere
    in its inputs.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，为了简单起见，我已经将每个卷积层的输出表示为一个2D层，但实际上，卷积层有多个滤波器（您决定有多少个），并且每个滤波器输出一个特征图，因此在3D中更准确地表示（请参见[图14-6](#cnn_layers_volume_diagram)）。每个特征图中的每个像素都有一个神经元，并且给定特征图中的所有神经元共享相同的参数（即相同的内核和偏置项）。不同特征图中的神经元使用不同的参数。神经元的感受野与之前描述的相同，但它跨越了前一层的所有特征图。简而言之，卷积层同时将多个可训练滤波器应用于其输入，使其能够在其输入的任何位置检测多个特征。
- en: '![mls3 1406](assets/mls3_1406.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1406](assets/mls3_1406.png)'
- en: Figure 14-6\. Two convolutional layers with multiple filters each (kernels),
    processing a color image with three color channels; each convolutional layer outputs
    one feature map per filter
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-6。两个具有多个滤波器（内核）的卷积层，处理具有三个颜色通道的彩色图像；每个卷积层输出一个特征图每个滤波器
- en: Note
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The fact that all neurons in a feature map share the same parameters dramatically
    reduces the number of parameters in the model. Once the CNN has learned to recognize
    a pattern in one location, it can recognize it in any other location. In contrast,
    once a fully connected neural network has learned to recognize a pattern in one
    location, it can only recognize it in that particular location.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 'Input images are also composed of multiple sublayers: one per *color channel*.
    As mentioned in [Chapter 9](ch09.html#unsupervised_learning_chapter), there are
    typically three: red, green, and blue (RGB). Grayscale images have just one channel,
    but some images may have many more—for example, satellite images that capture
    extra light frequencies (such as infrared).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, a neuron located in row *i*, column *j* of the feature map *k*
    in a given convolutional layer *l* is connected to the outputs of the neurons
    in the previous layer *l* – 1, located in rows *i* × *s*[*h*] to *i* × *s*[*h*]
    + *f*[*h*] – 1 and columns *j* × *s*[*w*] to *j* × *s*[*w*] + *f*[*w*] – 1, across
    all feature maps (in layer *l* – *1*). Note that, within a layer, all neurons
    located in the same row *i* and column *j* but in different feature maps are connected
    to the outputs of the exact same neurons in the previous layer.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[Equation 14-1](#convolutional_layer_equation) summarizes the preceding explanations
    in one big mathematical equation: it shows how to compute the output of a given
    neuron in a convolutional layer. It is a bit ugly due to all the different indices,
    but all it does is calculate the weighted sum of all the inputs, plus the bias
    term.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Equation 14-1\. Computing the output of a neuron in a convolutional layer
  id: totrans-38
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>z</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi></mrow></msub>
    <mo>=</mo> <msub><mi>b</mi> <mi>k</mi></msub> <mo>+</mo> <munderover><mo>∑</mo>
    <mrow><mi>u</mi><mo>=</mo><mn>0</mn></mrow> <mrow><msub><mi>f</mi> <mi>h</mi></msub>
    <mo>-</mo><mn>1</mn></mrow></munderover> <munderover><mo>∑</mo> <mrow><mi>v</mi><mo>=</mo><mn>0</mn></mrow>
    <mrow><msub><mi>f</mi> <mi>w</mi></msub> <mo>-</mo><mn>1</mn></mrow></munderover>
    <munderover><mo>∑</mo> <mrow><mi>k</mi><mo>'</mo><mo>=</mo><mn>0</mn></mrow> <mrow><msub><mi>f</mi>
    <msup><mi>n</mi> <mo>'</mo></msup></msub> <mo>-</mo><mn>1</mn></mrow></munderover>
    <msub><mi>x</mi> <mrow><msup><mi>i</mi> <mo>'</mo></msup> <mo>,</mo><msup><mi>j</mi>
    <mo>'</mo></msup> <mo>,</mo><msup><mi>k</mi> <mo>'</mo></msup></mrow></msub> <mo>×</mo>
    <msub><mi>w</mi> <mrow><mi>u</mi><mo>,</mo><mi>v</mi><mo>,</mo><msup><mi>k</mi>
    <mo>'</mo></msup> <mo>,</mo><mi>k</mi></mrow></msub> <mtext>with</mtext> <mfenced
    separators="" open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><mi>i</mi>
    <mo>'</mo> <mo>=</mo> <mi>i</mi> <mo>×</mo> <msub><mi>s</mi> <mi>h</mi></msub>
    <mo>+</mo> <mi>u</mi></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mi>j</mi>
    <mo>'</mo> <mo>=</mo> <mi>j</mi> <mo>×</mo> <msub><mi>s</mi> <mi>w</mi></msub>
    <mo>+</mo> <mi>v</mi></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: 'In this equation:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '*z*[*i*,] [*j*,] [*k*] is the output of the neuron located in row *i*, column
    *j* in feature map *k* of the convolutional layer (layer *l*).'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As explained earlier, *s*[*h*] and *s*[*w*] are the vertical and horizontal
    strides, *f*[*h*] and *f*[*w*] are the height and width of the receptive field,
    and *f*[*n*′] is the number of feature maps in the previous layer (layer *l* –
    1).
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x*[*i*′,] [*j*′,] [*k*′] is the output of the neuron located in layer *l*
    – 1, row *i*′, column *j*′, feature map *k*′ (or channel *k*′ if the previous
    layer is the input layer).'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*b*[*k*] is the bias term for feature map *k* (in layer *l*). You can think
    of it as a knob that tweaks the overall brightness of the feature map *k*.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*w*[*u*,] [*v*,] [*k*′,] [*k*] is the connection weight between any neuron
    in feature map *k* of the layer *l* and its input located at row *u*, column *v*
    (relative to the neuron’s receptive field), and feature map *k*′.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*w*[*u*,] [*v*,] [*k*′,] [*k*]是层*l*中特征图*k*中的任何神经元与其输入之间的连接权重，该输入位于行*u*、列*v*（相对于神经元的感受野），以及特征图*k*′。'
- en: Let’s see how to create and use a convolutional layer using Keras.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用Keras创建和使用卷积层。
- en: Implementing Convolutional Layers with Keras
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Keras实现卷积层
- en: 'First, let’s load and preprocess a couple of sample images, using Scikit-Learn’s
    `load_sample_image()` function and Keras’s `CenterCrop` and `Rescaling` layers
    (all of which were introduced in [Chapter 13](ch13.html#data_chapter)):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们加载和预处理一些样本图像，使用Scikit-Learn的`load_sample_image()`函数和Keras的`CenterCrop`和`Rescaling`层（这些都是在[第13章](ch13.html#data_chapter)中介绍的）：
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s look at the shape of the `images` tensor:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下`images`张量的形状：
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Yikes, it’s a 4D tensor; we haven’t seen this before! What do all these dimensions
    mean? Well, there are two sample images, which explains the first dimension. Then
    each image is 70 × 120, since that’s the size we specified when creating the `CenterCrop`
    layer (the original images were 427 × 640). This explains the second and third
    dimensions. And lastly, each pixel holds one value per color channel, and there
    are three of them—red, green, and blue—which explains the last dimension.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀，这是一个4D张量；我们以前从未见过这个！所有这些维度是什么意思？嗯，有两个样本图像，这解释了第一个维度。然后每个图像是70×120，因为这是我们在创建`CenterCrop`层时指定的大小（原始图像是427×640）。这解释了第二和第三维度。最后，每个像素在每个颜色通道上保存一个值，有三个颜色通道——红色、绿色和蓝色，这解释了最后一个维度。
- en: 'Now let’s create a 2D convolutional layer and feed it these images to see what
    comes out. For this, Keras provides a `Convolution2D` layer, alias `Conv2D`. Under
    the hood, this layer relies on TensorFlow’s `tf.nn.conv2d()` operation. Let’s
    create a convolutional layer with 32 filters, each of size 7 × 7 (using `kernel_size=7`,
    which is equivalent to using `kernel_size=(7 , 7)`), and apply this layer to our
    small batch of two images:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建一个2D卷积层，并将这些图像输入其中，看看输出是什么。为此，Keras提供了一个`Convolution2D`层，别名为`Conv2D`。在幕后，这个层依赖于TensorFlow的`tf.nn.conv2d()`操作。让我们创建一个具有32个滤波器的卷积层，每个滤波器大小为7×7（使用`kernel_size=7`，相当于使用`kernel_size=(7
    , 7)`），并将这个层应用于我们的两个图像的小批量：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'When we talk about a 2D convolutional layer, “2D” refers to the number of *spatial*
    dimensions (height and width), but as you can see, the layer takes 4D inputs:
    as we saw, the two additional dimensions are the batch size (first dimension)
    and the channels (last dimension).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论2D卷积层时，“2D”指的是*空间*维度（高度和宽度），但正如你所看到的，该层接受4D输入：正如我们所看到的，另外两个维度是批量大小（第一个维度）和通道数（最后一个维度）。
- en: 'Now let’s look at the output’s shape:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一下输出的形状：
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output shape is similar to the input shape, with two main differences.
    First, there are 32 channels instead of 3\. This is because we set `filters=32`,
    so we get 32 output feature maps: instead of the intensity of red, green, and
    blue at each location, we now have the intensity of each feature at each location.
    Second, the height and width have both shrunk by 6 pixels. This is due to the
    fact that the `Conv2D` layer does not use any zero-padding by default, which means
    that we lose a few pixels on the sides of the output feature maps, depending on
    the size of the filters. In this case, since the kernel size is 7, we lose 6 pixels
    horizontally and 6 pixels vertically (i.e., 3 pixels on each side).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 输出形状与输入形状类似，有两个主要区别。首先，有32个通道而不是3个。这是因为我们设置了`filters=32`，所以我们得到32个输出特征图：在每个位置的红色、绿色和蓝色的强度代替，我们现在有每个位置的每个特征的强度。其次，高度和宽度都减小了6个像素。这是因为`Conv2D`层默认不使用任何零填充，这意味着我们在输出特征图的两侧丢失了一些像素，取决于滤波器的大小。在这种情况下，由于卷积核大小为7，我们水平和垂直各丢失6个像素（即每侧3个像素）。
- en: Warning
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'The default option is surprisingly named `padding="valid"`, which actually
    means no zero-padding at all! This name comes from the fact that in this case
    every neuron’s receptive field lies strictly within *valid* positions inside the
    input (it does not go out of bounds). It’s not a Keras naming quirk: everyone
    uses this odd nomenclature.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 默认选项令人惊讶地被命名为`padding="valid"`，实际上意味着根本没有零填充！这个名称来自于这样一个事实，即在这种情况下，每个神经元的感受野严格位于输入内部的*有效*位置（不会超出边界）。这不是Keras的命名怪癖：每个人都使用这种奇怪的命名法。
- en: 'If instead we set `padding="same"`, then the inputs are padded with enough
    zeros on all sides to ensure that the output feature maps end up with the *same*
    size as the inputs (hence the name of this option):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们设置`padding="same"`，那么输入将在所有侧面填充足够的零，以确保输出特征图最终与输入具有*相同*大小（因此这个选项的名称）：
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: These two padding options are illustrated in [Figure 14-7](#padding_options_stride_1_diagram).
    For simplicity, only the horizontal dimension is shown here, but of course the
    same logic applies to the vertical dimension as well.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种填充选项在[图14-7](#padding_options_stride_1_diagram)中有所说明。为简单起见，这里只显示了水平维度，但当然相同的逻辑也适用于垂直维度。
- en: 'If the stride is greater than 1 (in any direction), then the output size will
    not be equal to the input size, even if `padding="same"`. For example, if you
    set `strides=2` (or equivalently `strides=(2, 2)`), then the output feature maps
    will be 35 × 60: halved both vertically and horizontally. [Figure 14-8](#padding_options_stride_2_diagram)
    shows what happens when `strides=2`, with both padding options.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果步幅大于1（在任何方向上），那么输出大小将不等于输入大小，即使`padding="same"`。例如，如果设置`strides=2`（或等效地`strides=(2,
    2)`），那么输出特征图将是35×60：垂直和水平方向都减半。[图14-8](#padding_options_stride_2_diagram)展示了当`strides=2`时会发生什么，两种填充选项都有。
- en: '![mls3 1407](assets/mls3_1407.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1407](assets/mls3_1407.png)'
- en: Figure 14-7\. The two padding options, when `strides=1`
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-7。当`strides=1`时的两种填充选项
- en: '![mls3 1408](assets/mls3_1408.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1408](assets/mls3_1408.png)'
- en: Figure 14-8\. With strides greater than 1, the output is much smaller even when
    using `"same"` padding (and `"valid"` padding may ignore some inputs)
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-8。当步长大于1时，即使使用“same”填充（和“valid”填充可能会忽略一些输入），输出也会小得多
- en: 'If you are curious, this is how the output size is computed:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您感兴趣，这是输出大小是如何计算的：
- en: With `padding="valid"`, if the width of the input is *i*[*h*], then the output
    width is equal to (*i*[*h*] – *f*[*h*] + *s*[*h*]) / *s*[*h*], rounded down. Recall
    that *f*[*h*] is the kernel width, and *s*[*h*] is the horizontal stride. Any
    remainder in the division corresponds to ignored columns on the right side of
    the input image. The same logic can be used to compute the output height, and
    any ignored rows at the bottom of the image.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当`padding="valid"`时，如果输入的宽度为*i*[h]，那么输出宽度等于(*i*[h] - *f*[h] + *s*[h]) / *s*[h]，向下取整。请记住*f*[h]是卷积核的宽度，*s*[h]是水平步长。除法中的余数对应于输入图像右侧被忽略的列。同样的逻辑也可以用来计算输出高度，以及图像底部被忽略的行。
- en: With `padding="same"`, the output width is equal to *i*[*h*] / *s*[*h*], rounded
    up. To make this possible, the appropriate number of zero columns are padded to
    the left and right of the input image (an equal number if possible, or just one
    more on the right side). Assuming the output width is *o*[*w*], then the number
    of padded zero columns is (*o*[*w*] – 1) × *s*[*h*] + *f*[*h*] – *i*[*h*]. Again,
    the same logic can be used to compute the output height and the number of padded
    rows.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当`padding="same"`时，输出宽度等于*i*[h] / *s*[h]，向上取整。为了实现这一点，在输入图像的左右两侧填充适当数量的零列（如果可能的话，数量相等，或者在右侧多一个）。假设输出宽度为*o*[w]，那么填充的零列数为(*o*[w]
    - 1) × *s*[h] + *f*[h] - *i*[h]。同样的逻辑也可以用来计算输出高度和填充行数。
- en: 'Now let’s look at the layer’s weights (which were noted *w*[*u*,] [*v*,] [*k*′,]
    [*k*] and *b*[*k*] in [Equation 14-1](#convolutional_layer_equation)). Just like
    a `Dense` layer, a `Conv2D` layer holds all the layer’s weights, including the
    kernels and biases. The kernels are initialized randomly, while the biases are
    initialized to zero. These weights are accessible as TF variables via the `weights`
    attribute, or as NumPy arrays via the `get_weights()` method:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下层的权重（在[方程14-1](#convolutional_layer_equation)中被标记为*w*[u,] [v,] [k',]
    [k]和*b*[k]）。就像`Dense`层一样，`Conv2D`层保存所有层的权重，包括卷积核和偏置。卷积核是随机初始化的，而偏置初始化为零。这些权重可以通过`weights`属性作为TF变量访问，也可以通过`get_weights()`方法作为NumPy数组访问：
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `kernels` array is 4D, and its shape is [*kernel_height*, *kernel_width*,
    *input_channels*, *output_channels*]. The `biases` array is 1D, with shape [*output_channels*].
    The number of output channels is equal to the number of output feature maps, which
    is also equal to the number of filters.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`kernels`数组是4D的，其形状为[*kernel_height*, *kernel_width*, *input_channels*, *output_channels*]。`biases`数组是1D的，形状为[*output_channels*]。输出通道的数量等于输出特征图的数量，也等于滤波器的数量。'
- en: 'Most importantly, note that the height and width of the input images do not
    appear in the kernel’s shape: this is because all the neurons in the output feature
    maps share the same weights, as explained earlier. This means that you can feed
    images of any size to this layer, as long as they are at least as large as the
    kernels, and if they have the right number of channels (three in this case).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，需要注意输入图像的高度和宽度不会出现在卷积核的形状中：这是因为输出特征图中的所有神经元共享相同的权重，正如之前解释的那样。这意味着您可以将任何大小的图像馈送到这一层，只要它们至少与卷积核一样大，并且具有正确数量的通道（在这种情况下为三个）。
- en: 'Lastly, you will generally want to specify an activation function (such as
    ReLU) when creating a `Conv2D` layer, and also specify the corresponding kernel
    initializer (such as He initialization). This is for the same reason as for `Dense`
    layers: a convolutional layer performs a linear operation, so if you stacked multiple
    convolutional layers without any activation functions they would all be equivalent
    to a single convolutional layer, and they wouldn’t be able to learn anything really
    complex.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通常情况下，您会希望在创建`Conv2D`层时指定一个激活函数（如ReLU），并指定相应的内核初始化器（如He初始化）。这与`Dense`层的原因相同：卷积层执行线性操作，因此如果您堆叠多个卷积层而没有任何激活函数，它们都等同于单个卷积层，它们将无法学习到真正复杂的内容。
- en: 'As you can see, convolutional layers have quite a few hyperparameters: `filters`,
    `kernel_size`, `padding`, `strides`, `activation`, `kernel_initializer`, etc.
    As always, you can use cross-validation to find the right hyperparameter values,
    but this is very time-consuming. We will discuss common CNN architectures later
    in this chapter, to give you some idea of which hyperparameter values work best
    in practice.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，卷积层有很多超参数：`filters`、`kernel_size`、`padding`、`strides`、`activation`、`kernel_initializer`等。通常情况下，您可以使用交叉验证来找到正确的超参数值，但这是非常耗时的。我们将在本章后面讨论常见的CNN架构，以便让您了解在实践中哪些超参数值效果最好。
- en: Memory Requirements
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存需求
- en: Another challenge with CNNs is that the convolutional layers require a huge
    amount of RAM. This is especially true during training, because the reverse pass
    of backpropagation requires all the intermediate values computed during the forward
    pass.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的另一个挑战是卷积层需要大量的RAM。这在训练过程中尤为明显，因为反向传播的反向传递需要在前向传递期间计算的所有中间值。
- en: 'For example, consider a convolutional layer with 200 5 × 5 filters, with stride
    1 and `"same"` padding. If the input is a 150 × 100 RGB image (three channels),
    then the number of parameters is (5 × 5 × 3 + 1) × 200 = 15,200 (the + 1 corresponds
    to the bias terms), which is fairly small compared to a fully connected layer.⁠^([7](ch14.html#idm45720185247456))
    However, each of the 200 feature maps contains 150 × 100 neurons, and each of
    these neurons needs to compute a weighted sum of its 5 × 5 × 3 = 75 inputs: that’s
    a total of 225 million float multiplications. Not as bad as a fully connected
    layer, but still quite computationally intensive. Moreover, if the feature maps
    are represented using 32-bit floats, then the convolutional layer’s output will
    occupy 200 × 150 × 100 × 32 = 96 million bits (12 MB) of RAM.⁠^([8](ch14.html#idm45720185245968))
    And that’s just for one instance—if a training batch contains 100 instances, then
    this layer will use up 1.2 GB of RAM!'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: During inference (i.e., when making a prediction for a new instance) the RAM
    occupied by one layer can be released as soon as the next layer has been computed,
    so you only need as much RAM as required by two consecutive layers. But during
    training everything computed during the forward pass needs to be preserved for
    the reverse pass, so the amount of RAM needed is (at least) the total amount of
    RAM required by all layers.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If training crashes because of an out-of-memory error, you can try reducing
    the mini-batch size. Alternatively, you can try reducing dimensionality using
    a stride, removing a few layers, using 16-bit floats instead of 32-bit floats,
    or distributing the CNN across multiple devices (you will see how to do this in
    [Chapter 19](ch19.html#deployment_chapter)).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s look at the second common building block of CNNs: the *pooling layer*.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Pooling Layers
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you understand how convolutional layers work, the pooling layers are quite
    easy to grasp. Their goal is to *subsample* (i.e., shrink) the input image in
    order to reduce the computational load, the memory usage, and the number of parameters
    (thereby limiting the risk of overfitting).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Just like in convolutional layers, each neuron in a pooling layer is connected
    to the outputs of a limited number of neurons in the previous layer, located within
    a small rectangular receptive field. You must define its size, the stride, and
    the padding type, just like before. However, a pooling neuron has no weights;
    all it does is aggregate the inputs using an aggregation function such as the
    max or mean. [Figure 14-9](#max_pooling_diagram) shows a *max pooling layer*,
    which is the most common type of pooling layer. In this example, we use a 2 ×
    2 *pooling kernel*,⁠^([9](ch14.html#idm45720185209104)) with a stride of 2 and
    no padding. Only the max input value in each receptive field makes it to the next
    layer, while the other inputs are dropped. For example, in the lower-left receptive
    field in [Figure 14-9](#max_pooling_diagram), the input values are 1, 5, 3, 2,
    so only the max value, 5, is propagated to the next layer. Because of the stride
    of 2, the output image has half the height and half the width of the input image
    (rounded down since we use no padding).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1409](assets/mls3_1409.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
- en: Figure 14-9\. Max pooling layer (2 × 2 pooling kernel, stride 2, no padding)
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A pooling layer typically works on every input channel independently, so the
    output depth (i.e., the number of channels) is the same as the input depth.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'Other than reducing computations, memory usage, and the number of parameters,
    a max pooling layer also introduces some level of *invariance* to small translations,
    as shown in [Figure 14-10](#pooling_invariance_diagram). Here we assume that the
    bright pixels have a lower value than dark pixels, and we consider three images
    (A, B, C) going through a max pooling layer with a 2 × 2 kernel and stride 2\.
    Images B and C are the same as image A, but shifted by one and two pixels to the
    right. As you can see, the outputs of the max pooling layer for images A and B
    are identical. This is what translation invariance means. For image C, the output
    is different: it is shifted one pixel to the right (but there is still 50% invariance).
    By inserting a max pooling layer every few layers in a CNN, it is possible to
    get some level of translation invariance at a larger scale. Moreover, max pooling
    offers a small amount of rotational invariance and a slight scale invariance.
    Such invariance (even if it is limited) can be useful in cases where the prediction
    should not depend on these details, such as in classification tasks.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'However, max pooling has some downsides too. It’s obviously very destructive:
    even with a tiny 2 × 2 kernel and a stride of 2, the output will be two times
    smaller in both directions (so its area will be four times smaller), simply dropping
    75% of the input values. And in some applications, invariance is not desirable.
    Take semantic segmentation (the task of classifying each pixel in an image according
    to the object that pixel belongs to, which we’ll explore later in this chapter):
    obviously, if the input image is translated by one pixel to the right, the output
    should also be translated by one pixel to the right. The goal in this case is
    *equivariance*, not invariance: a small change to the inputs should lead to a
    corresponding small change in the output.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1410](assets/mls3_1410.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
- en: Figure 14-10\. Invariance to small translations
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Implementing Pooling Layers with Keras
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following code creates a `MaxPooling2D` layer, alias `MaxPool2D`, using
    a 2 × 2 kernel. The strides default to the kernel size, so this layer uses a stride
    of 2 (horizontally and vertically). By default, it uses `"valid"` padding (i.e.,
    no padding at all):'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: To create an *average pooling layer*, just use `AveragePooling2D`, alias `AvgPool2D`,
    instead of `MaxPool2D`. As you might expect, it works exactly like a max pooling
    layer, except it computes the mean rather than the max. Average pooling layers
    used to be very popular, but people mostly use max pooling layers now, as they
    generally perform better. This may seem surprising, since computing the mean generally
    loses less information than computing the max. But on the other hand, max pooling
    preserves only the strongest features, getting rid of all the meaningless ones,
    so the next layers get a cleaner signal to work with. Moreover, max pooling offers
    stronger translation invariance than average pooling, and it requires slightly
    less compute.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that max pooling and average pooling can be performed along the depth
    dimension instead of the spatial dimensions, although it’s not as common. This
    can allow the CNN to learn to be invariant to various features. For example, it
    could learn multiple filters, each detecting a different rotation of the same
    pattern (such as handwritten digits; see [Figure 14-11](#depth_wise_pooling_diagram)),
    and the depthwise max pooling layer would ensure that the output is the same regardless
    of the rotation. The CNN could similarly learn to be invariant to anything: thickness,
    brightness, skew, color, and so on.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1411](assets/mls3_1411.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
- en: Figure 14-11\. Depthwise max pooling can help the CNN learn to be invariant
    (to rotation in this case)
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Keras does not include a depthwise max pooling layer, but it’s not too difficult
    to implement a custom layer for that:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This layer reshapes its inputs to split the channels into groups of the desired
    size (`pool_size`), then it uses `tf.reduce_max()` to compute the max of each
    group. This implementation assumes that the stride is equal to the pool size,
    which is generally what you want. Alternatively, you could use TensorFlow’s `tf.nn.max_pool()`
    operation, and wrap in a `Lambda` layer to use it inside a Keras model, but sadly
    this op does not implement depthwise pooling for the GPU, only for the CPU.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这一层将其输入重塑为所需大小的通道组（`pool_size`），然后使用`tf.reduce_max()`来计算每个组的最大值。这种实现假定步幅等于池大小，这通常是你想要的。或者，您可以使用TensorFlow的`tf.nn.max_pool()`操作，并在`Lambda`层中包装以在Keras模型中使用它，但遗憾的是，此操作不实现GPU的深度池化，只实现CPU的深度池化。
- en: 'One last type of pooling layer that you will often see in modern architectures
    is the *global average pooling layer*. It works very differently: all it does
    is compute the mean of each entire feature map (it’s like an average pooling layer
    using a pooling kernel with the same spatial dimensions as the inputs). This means
    that it just outputs a single number per feature map and per instance. Although
    this is of course extremely destructive (most of the information in the feature
    map is lost), it can be useful just before the output layer, as you will see later
    in this chapter. To create such a layer, simply use the `GlobalAveragePooling2D`
    class, alias `GlobalAvgPool2D`:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代架构中经常看到的最后一种类型的池化层是*全局平均池化层*。它的工作方式非常不同：它只是计算每个整个特征图的平均值（就像使用与输入具有相同空间维度的池化核的平均池化层）。这意味着它只输出每个特征图和每个实例的一个数字。尽管这当然是极其破坏性的（大部分特征图中的信息都丢失了），但它可以在输出层之前非常有用，稍后您将在本章中看到。要创建这样的层，只需使用`GlobalAveragePooling2D`类，别名`GlobalAvgPool2D`：
- en: '[PRE8]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'It’s equivalent to the following `Lambda` layer, which computes the mean over
    the spatial dimensions (height and width):'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这等同于以下`Lambda`层，它计算空间维度（高度和宽度）上的平均值：
- en: '[PRE9]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'For example, if we apply this layer to the input images, we get the mean intensity
    of red, green, and blue for each image:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们将这一层应用于输入图像，我们将得到每个图像的红色、绿色和蓝色的平均强度：
- en: '[PRE10]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now you know all the building blocks to create convolutional neural networks.
    Let’s see how to assemble them.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您知道如何创建卷积神经网络的所有构建模块了。让我们看看如何组装它们。
- en: CNN Architectures
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN架构
- en: Typical CNN architectures stack a few convolutional layers (each one generally
    followed by a ReLU layer), then a pooling layer, then another few convolutional
    layers (+ReLU), then another pooling layer, and so on. The image gets smaller
    and smaller as it progresses through the network, but it also typically gets deeper
    and deeper (i.e., with more feature maps), thanks to the convolutional layers
    (see [Figure 14-12](#cnn_architecture_diagram)). At the top of the stack, a regular
    feedforward neural network is added, composed of a few fully connected layers
    (+ReLUs), and the final layer outputs the prediction (e.g., a softmax layer that
    outputs estimated class probabilities).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的CNN架构堆叠了几个卷积层（每个通常后面跟着一个ReLU层），然后是一个池化层，然后又是几个卷积层（+ReLU），然后是另一个池化层，依此类推。随着图像通过网络的传递，图像变得越来越小，但也通常变得越来越深（即具有更多的特征图），这要归功于卷积层（参见[图14-12](#cnn_architecture_diagram)）。在堆栈的顶部，添加了一个常规的前馈神经网络，由几个全连接层（+ReLUs）组成，最后一层输出预测（例如，一个softmax层，输出估计的类别概率）。
- en: '![mls3 1412](assets/mls3_1412.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1412](assets/mls3_1412.png)'
- en: Figure 14-12\. Typical CNN architecture
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-12\. 典型的CNN架构
- en: Tip
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'A common mistake is to use convolution kernels that are too large. For example,
    instead of using a convolutional layer with a 5 × 5 kernel, stack two layers with
    3 × 3 kernels: it will use fewer parameters and require fewer computations, and
    it will usually perform better. One exception is for the first convolutional layer:
    it can typically have a large kernel (e.g., 5 × 5), usually with a stride of 2
    or more. This will reduce the spatial dimension of the image without losing too
    much information, and since the input image only has three channels in general,
    it will not be too costly.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的错误是使用太大的卷积核。例如，不要使用一个5×5的卷积层，而是堆叠两个3×3的卷积层：这将使用更少的参数，需要更少的计算，并且通常表现更好。一个例外是第一个卷积层：它通常可以有一个大的卷积核（例如5×5），通常具有2或更大的步幅。这将减少图像的空间维度，而不会丢失太多信息，而且由于输入图像通常只有三个通道，因此成本不会太高。
- en: 'Here is how you can implement a basic CNN to tackle the Fashion MNIST dataset
    (introduced in [Chapter 10](ch10.html#ann_chapter)):'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何实现一个基本的CNN来处理时尚MNIST数据集的（在[第10章](ch10.html#ann_chapter)介绍）：
- en: '[PRE11]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let’s go through this code:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下这段代码：
- en: 'We use the `functools.partial()` function (introduced in [Chapter 11](ch11.html#deep_chapter))
    to define `DefaultConv2D`, which acts just like `Conv2D` but with different default
    arguments: a small kernel size of 3, `"same"` padding, the ReLU activation function,
    and its corresponding He initializer.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用`functools.partial()`函数（在[第11章](ch11.html#deep_chapter)介绍）来定义`DefaultConv2D`，它的作用就像`Conv2D`，但具有不同的默认参数：一个小的3的内核大小，`"same"`填充，ReLU激活函数，以及相应的He初始化器。
- en: 'Next, we create the `Sequential` model. Its first layer is a `DefaultConv2D`
    with 64 fairly large filters (7 × 7). It uses the default stride of 1 because
    the input images are not very large. It also sets `input_shape=[28, 28, 1]`, because
    the images are 28 × 28 pixels, with a single color channel (i.e., grayscale).
    When you load the Fashion MNIST dataset, make sure each image has this shape:
    you may need to use `np.reshape()` or `np.expanddims()` to add the channels dimension.
    Alternatively, you could use a `Reshape` layer as the first layer in the model.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们创建`Sequential`模型。它的第一层是一个具有64个相当大的滤波器（7×7）的`DefaultConv2D`。它使用默认的步幅1，因为输入图像不是很大。它还设置`input_shape=[28,
    28, 1]`，因为图像是28×28像素，具有单个颜色通道（即灰度）。当您加载时尚MNIST数据集时，请确保每个图像具有这种形状：您可能需要使用`np.reshape()`或`np.expanddims()`来添加通道维度。或者，您可以在模型中使用`Reshape`层作为第一层。
- en: We then add a max pooling layer that uses the default pool size of 2, so it
    divides each spatial dimension by a factor of 2.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们添加一个使用默认池大小为2的最大池化层，因此它将每个空间维度除以2。
- en: 'Then we repeat the same structure twice: two convolutional layers followed
    by a max pooling layer. For larger images, we could repeat this structure several
    more times. The number of repetitions is a hyperparameter you can tune.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们重复相同的结构两次：两个卷积层后面跟着一个最大池化层。对于更大的图像，我们可以多次重复这个结构。重复次数是一个可以调整的超参数。
- en: 'Note that the number of filters doubles as we climb up the CNN toward the output
    layer (it is initially 64, then 128, then 256): it makes sense for it to grow,
    since the number of low-level features is often fairly low (e.g., small circles,
    horizontal lines), but there are many different ways to combine them into higher-level
    features. It is a common practice to double the number of filters after each pooling
    layer: since a pooling layer divides each spatial dimension by a factor of 2,
    we can afford to double the number of feature maps in the next layer without fear
    of exploding the number of parameters, memory usage, or computational load.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请注意，随着我们向CNN向输出层上升，滤波器的数量会翻倍（最初为64，然后为128，然后为256）：这是有道理的，因为低级特征的数量通常相当低（例如，小圆圈，水平线），但有许多不同的方法可以将它们组合成更高级别的特征。在每个池化层后将滤波器数量翻倍是一种常见做法：由于池化层将每个空间维度除以2，我们可以在下一层中加倍特征图的数量，而不用担心参数数量、内存使用或计算负载的激增。
- en: Next is the fully connected network, composed of two hidden dense layers and
    a dense output layer. Since it’s a classification task with 10 classes, the output
    layer has 10 units, and it uses the softmax activation function. Note that we
    must flatten the inputs just before the first dense layer, since it expects a
    1D array of features for each instance. We also add two dropout layers, with a
    dropout rate of 50% each, to reduce overfitting.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来是全连接网络，由两个隐藏的密集层和一个密集输出层组成。由于这是一个有10个类别的分类任务，输出层有10个单元，并且使用softmax激活函数。请注意，我们必须在第一个密集层之前扁平化输入，因为它期望每个实例的特征是一个1D数组。我们还添加了两个dropout层，每个的dropout率为50%，以减少过拟合。
- en: If you compile this model using the `"sparse_categorical_crossentropy"` loss
    and you fit the model to the Fashion MNIST training set, it should reach over
    92% accuracy on the test set. It’s not state of the art, but it is pretty good,
    and clearly much better than what we achieved with dense networks in [Chapter 10](ch10.html#ann_chapter).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用`"sparse_categorical_crossentropy"`损失编译此模型，并将模型拟合到Fashion MNIST训练集，它应该在测试集上达到超过92%的准确率。这并不是最先进的，但是相当不错，显然比我们在第10章中使用密集网络取得的成绩要好得多。
- en: Over the years, variants of this fundamental architecture have been developed,
    leading to amazing advances in the field. A good measure of this progress is the
    error rate in competitions such as the ILSVRC [ImageNet challenge](https://image-net.org).
    In this competition, the top-five error rate for image classification—that is,
    the number of test images for which the system’s top five predictions did *not*
    include the correct answer—fell from over 26% to less than 2.3% in just six years.
    The images are fairly large (e.g., 256 pixels high) and there are 1,000 classes,
    some of which are really subtle (try distinguishing 120 dog breeds). Looking at
    the evolution of the winning entries is a good way to understand how CNNs work,
    and how research in deep learning progresses.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，这种基本架构的变体已经被开发出来，导致了该领域的惊人进步。这种进步的一个很好的衡量标准是在ILSVRC（ImageNet挑战）等比赛中的错误率。在这个比赛中，图像分类的前五错误率，即系统的前五个预测中没有包括正确答案的测试图像数量，从超过26%下降到不到2.3%仅仅用了六年。这些图像相当大（例如，高度为256像素），有1000个类别，其中一些非常微妙（尝试区分120种狗品种）。查看获胜作品的演变是了解CNN如何工作以及深度学习研究如何进展的好方法。
- en: 'We will first look at the classical LeNet-5 architecture (1998), then several
    winners of the ILSVRC challenge: AlexNet (2012), GoogLeNet (2014), ResNet (2015),
    and SENet (2017). Along the way, we will also look at a few more architectures,
    including Xception, ResNeXt, DenseNet, MobileNet, CSPNet, and EfficientNet.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先看一下经典的LeNet-5架构（1998年），然后看一下几位ILSVRC挑战的获胜者：AlexNet（2012），GoogLeNet（2014），ResNet（2015）和SENet（2017）。在此过程中，我们还将看一些其他架构，包括Xception，ResNeXt，DenseNet，MobileNet，CSPNet和EfficientNet。
- en: LeNet-5
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LeNet-5
- en: The [LeNet-5 architecture](https://homl.info/lenet5)⁠^([10](ch14.html#idm45720184581728))
    is perhaps the most widely known CNN architecture. As mentioned earlier, it was
    created by Yann LeCun in 1998 and has been widely used for handwritten digit recognition
    (MNIST). It is composed of the layers shown in [Table 14-1](#lenet_5_architecture).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[LeNet-5架构](https://homl.info/lenet5)可能是最广为人知的CNN架构。正如前面提到的，它是由Yann LeCun在1998年创建的，并且被广泛用于手写数字识别（MNIST）。它由[表14-1](#lenet_5_architecture)中显示的层组成。'
- en: Table 14-1\. LeNet-5 architecture
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 表14-1. LeNet-5架构
- en: '| Layer | Type | Maps | Size | Kernel size | Stride | Activation |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 层 | 类型 | 特征图 | 尺寸 | 核大小 | 步幅 | 激活函数 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Out | Fully connected | – | 10 | – | – | RBF |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| Out | 全连接 | – | 10 | – | – | RBF |'
- en: '| F6 | Fully connected | – | 84 | – | – | tanh |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| F6 | 全连接 | – | 84 | – | – | tanh |'
- en: '| C5 | Convolution | 120 | 1 × 1 | 5 × 5 | 1 | tanh |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| C5 | 卷积 | 120 | 1 × 1 | 5 × 5 | 1 | tanh |'
- en: '| S4 | Avg pooling | 16 | 5 × 5 | 2 × 2 | 2 | tanh |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| S4 | 平均池化 | 16 | 5 × 5 | 2 × 2 | 2 | tanh |'
- en: '| C3 | Convolution | 16 | 10 × 10 | 5 × 5 | 1 | tanh |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| C3 | 卷积 | 16 | 10 × 10 | 5 × 5 | 1 | tanh |'
- en: '| S2 | Avg pooling | 6 | 14 × 14 | 2 × 2 | 2 | tanh |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| S2 | 平均池化 | 6 | 14 × 14 | 2 × 2 | 2 | tanh |'
- en: '| C1 | Convolution | 6 | 28 × 28 | 5 × 5 | 1 | tanh |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| C1 | 卷积 | 6 | 28 × 28 | 5 × 5 | 1 | tanh |'
- en: '| In | Input | 1 | 32 × 32 | – | – | – |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| In | 输入 | 1 | 32 × 32 | – | – | – |'
- en: 'As you can see, this looks pretty similar to our Fashion MNIST model: a stack
    of convolutional layers and pooling layers, followed by a dense network. Perhaps
    the main difference with more modern classification CNNs is the activation functions:
    today, we would use ReLU instead of tanh and softmax instead of RBF. There were
    several other minor differences that don’t really matter much, but in case you
    are interested, they are listed in this chapter’s notebook at [*https://homl.info/colab3*](https://homl.info/colab3).
    Yann LeCun’s [website](http://yann.lecun.com/exdb/lenet) also features great demos
    of LeNet-5 classifying digits.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，这看起来与我们的时尚MNIST模型非常相似：一堆卷积层和池化层，然后是一个密集网络。也许与更现代的分类CNN相比，主要的区别在于激活函数：今天，我们会使用ReLU而不是tanh，使用softmax而不是RBF。还有一些其他不太重要的差异，但如果您感兴趣，可以在本章的笔记本中找到[*https://homl.info/colab3*](https://homl.info/colab3)。Yann
    LeCun的[网站](http://yann.lecun.com/exdb/lenet)还展示了LeNet-5对数字进行分类的精彩演示。
- en: AlexNet
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AlexNet
- en: 'The [AlexNet CNN architecture](https://homl.info/80)⁠^([11](ch14.html#idm45720184531840))
    won the 2012 ILSVRC challenge by a large margin: it achieved a top-five error
    rate of 17%, while the second best competitor achieved only 26%! AlexaNet was
    developed by Alex Krizhevsky (hence the name), Ilya Sutskever, and Geoffrey Hinton.
    It is similar to LeNet-5, only much larger and deeper, and it was the first to
    stack convolutional layers directly on top of one another, instead of stacking
    a pooling layer on top of each convolutional layer. [Table 14-2](#alexnet_architecture)
    presents this architecture.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[AlexNet CNN架构](https://homl.info/80)⁠^([11](ch14.html#idm45720184531840))在2012年ILSVRC挑战赛中大幅领先：它实现了17%的前五错误率，而第二名竞争对手仅实现了26%！AlexaNet由Alex
    Krizhevsky（因此得名）、Ilya Sutskever和Geoffrey Hinton开发。它类似于LeNet-5，只是更大更深，它是第一个直接将卷积层堆叠在一起的模型，而不是将池化层堆叠在每个卷积层之上。[表14-2](#alexnet_architecture)展示了这种架构。'
- en: Table 14-2\. AlexNet architecture
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 表14-2. AlexNet架构
- en: '| Layer | Type | Maps | Size | Kernel size | Stride | Padding | Activation
    |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 层 | 类型 | 特征图 | 大小 | 核大小 | 步幅 | 填充 | 激活函数 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Out | Fully connected | – | 1,000 | – | – | – | Softmax |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| Out | 全连接 | – | 1,000 | – | – | – | Softmax |'
- en: '| F10 | Fully connected | – | 4,096 | – | – | – | ReLU |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| F10 | 全连接 | – | 4,096 | – | – | – | ReLU |'
- en: '| F9 | Fully connected | – | 4,096 | – | – | – | ReLU |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| F9 | 全连接 | – | 4,096 | – | – | – | ReLU |'
- en: '| S8 | Max pooling | 256 | 6 × 6 | 3 × 3 | 2 | `valid` | – |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| S8 | 最大池化 | 256 | 6 × 6 | 3 × 3 | 2 | `valid` | – |'
- en: '| C7 | Convolution | 256 | 13 × 13 | 3 × 3 | 1 | `same` | ReLU |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| C7 | 卷积 | 256 | 13 × 13 | 3 × 3 | 1 | `same` | ReLU |'
- en: '| C6 | Convolution | 384 | 13 × 13 | 3 × 3 | 1 | `same` | ReLU |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| C6 | 卷积 | 384 | 13 × 13 | 3 × 3 | 1 | `same` | ReLU |'
- en: '| C5 | Convolution | 384 | 13 × 13 | 3 × 3 | 1 | `same` | ReLU |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| C5 | 卷积 | 384 | 13 × 13 | 3 × 3 | 1 | `same` | ReLU |'
- en: '| S4 | Max pooling | 256 | 13 × 13 | 3 × 3 | 2 | `valid` | – |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| S4 | 最大池化 | 256 | 13 × 13 | 3 × 3 | 2 | `valid` | – |'
- en: '| C3 | Convolution | 256 | 27 × 27 | 5 × 5 | 1 | `same` | ReLU |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| C3 | 卷积 | 256 | 27 × 27 | 5 × 5 | 1 | `same` | ReLU |'
- en: '| S2 | Max pooling | 96 | 27 × 27 | 3 × 3 | 2 | `valid` | – |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| S2 | 最大池化 | 96 | 27 × 27 | 3 × 3 | 2 | `valid` | – |'
- en: '| C1 | Convolution | 96 | 55 × 55 | 11 × 11 | 4 | `valid` | ReLU |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| C1 | 卷积 | 96 | 55 × 55 | 11 × 11 | 4 | `valid` | ReLU |'
- en: '| In | Input | 3 (RGB) | 227 × 227 | – | – | – | – |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| In | 输入 | 3（RGB） | 227 × 227 | – | – | – | – |'
- en: To reduce overfitting, the authors used two regularization techniques. First,
    they applied dropout (introduced in [Chapter 11](ch11.html#deep_chapter)) with
    a 50% dropout rate during training to the outputs of layers F9 and F10\. Second,
    they performed data augmentation by randomly shifting the training images by various
    offsets, flipping them horizontally, and changing the lighting conditions.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少过拟合，作者使用了两种正则化技术。首先，他们在训练期间对F9和F10层的输出应用了50%的dropout率的dropout（在[第11章](ch11.html#deep_chapter)中介绍）。其次，他们通过随机移动训练图像的各种偏移量、水平翻转它们和改变光照条件来执行数据增强。
- en: 'AlexNet also uses a competitive normalization step immediately after the ReLU
    step of layers C1 and C3, called *local response normalization* (LRN): the most
    strongly activated neurons inhibit other neurons located at the same position
    in neighboring feature maps. Such competitive activation has been observed in
    biological neurons. This encourages different feature maps to specialize, pushing
    them apart and forcing them to explore a wider range of features, ultimately improving
    generalization. [Equation 14-2](#lrn_equation) shows how to apply LRN.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet还在C1和C3层的ReLU步骤之后立即使用了一个竞争性归一化步骤，称为*局部响应归一化*（LRN）：最强烈激活的神经元抑制了位于相邻特征图中相同位置的其他神经元。这种竞争性激活已经在生物神经元中观察到。这鼓励不同的特征图专门化，将它们分开并迫使它们探索更广泛的特征，最终提高泛化能力。[方程14-2](#lrn_equation)展示了如何应用LRN。
- en: Equation 14-2\. Local response normalization (LRN)
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程14-2. 局部响应归一化（LRN）
- en: <math display="block"><mrow><msub><mi>b</mi> <mi>i</mi></msub> <mo>=</mo> <msub><mi>a</mi>
    <mi>i</mi></msub> <msup><mfenced separators="" open="(" close=")"><mi>k</mi><mo>+</mo><mi>α</mi><munderover><mo>∑</mo>
    <mrow><mi>j</mi><mo>=</mo><msub><mi>j</mi> <mtext>low</mtext></msub></mrow> <msub><mi>j</mi>
    <mtext>high</mtext></msub></munderover> <msup><mrow><msub><mi>a</mi> <mi>j</mi></msub></mrow>
    <mn>2</mn></msup></mfenced> <mrow><mo>-</mo><mi>β</mi></mrow></msup> <mtext>with</mtext>
    <mfenced separators="" open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><msub><mi>j</mi>
    <mtext>high</mtext></msub> <mo>=</mo> <mo movablelimits="true" form="prefix">min</mo>
    <mfenced separators="" open="(" close=")"><mi>i</mi> <mo>+</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mi>r</mi> <mn>2</mn></mfrac></mstyle> <mo>,</mo> <msub><mi>f</mi>
    <mi>n</mi></msub> <mo>-</mo> <mn>1</mn></mfenced></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><msub><mi>j</mi> <mtext>low</mtext></msub> <mo>=</mo>
    <mo movablelimits="true" form="prefix">max</mo> <mfenced separators="" open="("
    close=")"><mn>0</mn> <mo>,</mo> <mi>i</mi> <mo>-</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mi>r</mi> <mn>2</mn></mfrac></mstyle></mfenced></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>b</mi> <mi>i</mi></msub> <mo>=</mo> <msub><mi>a</mi>
    <mi>i</mi></msub> <msup><mfenced separators="" open="(" close=")"><mi>k</mi><mo>+</mo><mi>α</mi><munderover><mo>∑</mo>
    <mrow><mi>j</mi><mo>=</mo><msub><mi>j</mi> <mtext>low</mtext></msub></mrow> <msub><mi>j</mi>
    <mtext>high</mtext></msub></munderover> <msup><mrow><msub><mi>a</mi> <mi>j</mi></msub></mrow>
    <mn>2</mn></msup></mfenced> <mrow><mo>-</mo><mi>β</mi></row></msup> <mtext>with</mtext>
    <mfenced separators="" open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><msub><mi>j</mi>
    <mtext>high</mtext></msub> <mo>=</mo> <mo movablelimits="true" form="prefix">min</mo>
    <mfenced separators="" open="(" close=")"><mi>i</mi> <mo>+</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mi>r</mi> <mn>2</mn></mfrac></mstyle> <mo>,</mo> <msub><mi>f</mi>
    <mi>n</mi></msub> <mo>-</mo> <mn>1</mn></mfenced></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><msub><mi>j</mi> <mtext>low</mtext></msub> <mo>=</mo>
    <mo movablelimits="true" form="prefix">max</mo> <mfenced separators="" open="("
    close=")"><mn>0</mn> <mo>,</mo> <mi>i</mi> <mo>-</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mi>r</mi> <mn>2</mn></mfrac></mstyle></mfenced></mrow></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'In this equation:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: '*b*[*i*] is the normalized output of the neuron located in feature map *i*,
    at some row *u* and column *v* (note that in this equation we consider only neurons
    located at this row and column, so *u* and *v* are not shown).'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b*[*i*] 是位于特征图*i*中的神经元的归一化输出，在某一行*u*和列*v*（请注意，在这个方程中，我们只考虑位于这一行和列的神经元，因此*u*和*v*没有显示）。'
- en: '*a*[*i*] is the activation of that neuron after the ReLU step, but before normalization.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*a*[*i*] 是ReLU步骤后，但规范化之前的神经元的激活。'
- en: '*k*, *α*, *β*, and *r* are hyperparameters. *k* is called the *bias*, and *r*
    is called the *depth radius*.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*k*、*α*、*β*和*r*是超参数。*k*称为*偏置*，*r*称为*深度半径*。'
- en: '*f*[*n*] is the number of feature maps.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*f*[*n*] 是特征图的数量。'
- en: For example, if *r* = 2 and a neuron has a strong activation, it will inhibit
    the activation of the neurons located in the feature maps immediately above and
    below its own.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果*r* = 2，并且一个神经元具有强烈的激活，则它将抑制位于其上下特征图中的神经元的激活。
- en: 'In AlexNet, the hyperparameters are set as: *r* = 5, *α* = 0.0001, *β* = 0.75,
    and *k* = 2. You can implement this step by using the `tf.nn.local_response_normalization()`
    function (which you can wrap in a `Lambda` layer if you want to use it in a Keras
    model).'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在AlexNet中，超参数设置为：*r* = 5，*α* = 0.0001，*β* = 0.75，*k* = 2。您可以使用`tf.nn.local_response_normalization()`函数来实现这一步骤（如果要在Keras模型中使用它，可以将其包装在`Lambda`层中）。
- en: A variant of AlexNet called [*ZF Net*](https://homl.info/zfnet)⁠^([12](ch14.html#idm45720184386400))
    was developed by Matthew Zeiler and Rob Fergus and won the 2013 ILSVRC challenge.
    It is essentially AlexNet with a few tweaked hyperparameters (number of feature
    maps, kernel size, stride, etc.).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 由Matthew Zeiler和Rob Fergus开发的AlexNet的一个变体称为[*ZF Net*](https://homl.info/zfnet)⁠^([12](ch14.html#idm45720184386400))，并赢得了2013年ILSVRC挑战赛。它本质上是AlexNet，只是调整了一些超参数（特征图数量、卷积核大小、步幅等）。
- en: GoogLeNet
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GoogLeNet
- en: 'The [GoogLeNet architecture](https://homl.info/81) was developed by Christian
    Szegedy et al. from Google Research,⁠^([13](ch14.html#idm45720184382672)) and
    it won the ILSVRC 2014 challenge by pushing the top-five error rate below 7%.
    This great performance came in large part from the fact that the network was much
    deeper than previous CNNs (as you’ll see in [Figure 14-15](#googlenet_diagram)).
    This was made possible by subnetworks called *inception modules*,⁠^([14](ch14.html#idm45720184379088))
    which allow GoogLeNet to use parameters much more efficiently than previous architectures:
    GoogLeNet actually has 10 times fewer parameters than AlexNet (roughly 6 million
    instead of 60 million).'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '[GoogLeNet架构](https://homl.info/81)由Google Research的Christian Szegedy等人开发，⁠^([13](ch14.html#idm45720184382672))，并通过将前五错误率降低到7%以下赢得了ILSVRC
    2014挑战。这一出色的性能在很大程度上来自于该网络比以前的CNN更深（如您将在[图14-15](#googlenet_diagram)中看到的）。这得益于称为*inception模块*的子网络，⁠^([14](ch14.html#idm45720184379088))，它允许GoogLeNet比以前的架构更有效地使用参数：实际上，GoogLeNet的参数比AlexNet少10倍（大约600万个而不是6000万个）。'
- en: '[Figure 14-14](#inception_module_diagram) shows the architecture of an inception
    module. The notation “3 × 3 + 1(S)” means that the layer uses a 3 × 3 kernel,
    stride 1, and `"same"` padding. The input signal is first fed to four different
    layers in parallel. All convolutional layers use the ReLU activation function.
    Note that the top convolutional layers use different kernel sizes (1 × 1, 3 ×
    3, and 5 × 5), allowing them to capture patterns at different scales. Also note
    that every single layer uses a stride of 1 and `"same"` padding (even the max
    pooling layer), so their outputs all have the same height and width as their inputs.
    This makes it possible to concatenate all the outputs along the depth dimension
    in the final *depth concatenation layer* (i.e., to stack the feature maps from
    all four top convolutional layers). It can be implemented using Keras’s `Concatenate`
    layer, using the default `axis=-1`.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1414](assets/mls3_1414.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
- en: Figure 14-14\. Inception module
  id: totrans-179
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You may wonder why inception modules have convolutional layers with 1 × 1 kernels.
    Surely these layers cannot capture any features because they look at only one
    pixel at a time, right? In fact, these layers serve three purposes:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Although they cannot capture spatial patterns, they can capture patterns along
    the depth dimension (i.e., across channels).
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are configured to output fewer feature maps than their inputs, so they
    serve as *bottleneck layers*, meaning they reduce dimensionality. This cuts the
    computational cost and the number of parameters, speeding up training and improving
    generalization.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each pair of convolutional layers ([1 × 1, 3 × 3] and [1 × 1, 5 × 5]) acts like
    a single powerful convolutional layer, capable of capturing more complex patterns.
    A convolutional layer is equivalent to sweeping a dense layer across the image
    (at each location, it only looks at a small receptive field), and these pairs
    of convolutional layers are equivalent to sweeping two-layer neural networks across
    the image.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In short, you can think of the whole inception module as a convolutional layer
    on steroids, able to output feature maps that capture complex patterns at various
    scales.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s look at the architecture of the GoogLeNet CNN (see [Figure 14-15](#googlenet_diagram)).
    The number of feature maps output by each convolutional layer and each pooling
    layer is shown before the kernel size. The architecture is so deep that it has
    to be represented in three columns, but GoogLeNet is actually one tall stack,
    including nine inception modules (the boxes with the spinning tops). The six numbers
    in the inception modules represent the number of feature maps output by each convolutional
    layer in the module (in the same order as in [Figure 14-14](#inception_module_diagram)).
    Note that all the convolutional layers use the ReLU activation function.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go through this network:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: The first two layers divide the image’s height and width by 4 (so its area is
    divided by 16), to reduce the computational load. The first layer uses a large
    kernel size, 7 × 7, so that much of the information is preserved.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then the local response normalization layer ensures that the previous layers
    learn a wide variety of features (as discussed earlier).
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two convolutional layers follow, where the first acts like a bottleneck layer.
    As mentioned, you can think of this pair as a single smarter convolutional layer.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Again, a local response normalization layer ensures that the previous layers
    capture a wide variety of patterns.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, a max pooling layer reduces the image height and width by 2, again to
    speed up computations.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then comes the CNN’s *backbone*: a tall stack of nine inception modules, interleaved
    with a couple of max pooling layers to reduce dimensionality and speed up the
    net.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, the global average pooling layer outputs the mean of each feature map:
    this drops any remaining spatial information, which is fine because there is not
    much spatial information left at that point. Indeed, GoogLeNet input images are
    typically expected to be 224 × 224 pixels, so after 5 max pooling layers, each
    dividing the height and width by 2, the feature maps are down to 7 × 7\. Moreover,
    this is a classification task, not localization, so it doesn’t matter where the
    object is. Thanks to the dimensionality reduction brought by this layer, there
    is no need to have several fully connected layers at the top of the CNN (like
    in AlexNet), and this considerably reduces the number of parameters in the network
    and limits the risk of overfitting.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，全局平均池化层输出每个特征图的平均值：这会丢弃任何剩余的空间信息，这没关系，因为在那一点上剩下的空间信息并不多。事实上，GoogLeNet的输入图像通常期望为224×224像素，因此经过5个最大池化层后，每个将高度和宽度除以2，特征图缩小到7×7。此外，这是一个分类任务，而不是定位任务，因此物体在哪里并不重要。由于这一层带来的降维，不需要在CNN的顶部有几个全连接层（就像在AlexNet中那样），这大大减少了网络中的参数数量，并限制了过拟合的风险。
- en: 'The last layers are self-explanatory: dropout for regularization, then a fully
    connected layer with 1,000 units (since there are 1,000 classes) and a softmax
    activation function to output estimated class probabilities.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后几层很容易理解：用于正则化的dropout，然后是一个具有1,000个单元的全连接层（因为有1,000个类别），以及一个softmax激活函数来输出估计的类别概率。
- en: '![mls3 1415](assets/mls3_1415.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1415](assets/mls3_1415.png)'
- en: Figure 14-15\. GoogLeNet architecture
  id: totrans-196
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-15。GoogLeNet架构
- en: The original GoogLeNet architecture included two auxiliary classifiers plugged
    on top of the third and sixth inception modules. They were both composed of one
    average pooling layer, one convolutional layer, two fully connected layers, and
    a softmax activation layer. During training, their loss (scaled down by 70%) was
    added to the overall loss. The goal was to fight the vanishing gradients problem
    and regularize the network, but it was later shown that their effect was relatively
    minor.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的GoogLeNet架构包括两个辅助分类器，插在第三和第六个inception模块的顶部。它们都由一个平均池化层、一个卷积层、两个全连接层和一个softmax激活层组成。在训练过程中，它们的损失（缩小了70%）被添加到整体损失中。目标是解决梯度消失问题并对网络进行正则化，但后来证明它们的效果相对较小。
- en: Several variants of the GoogLeNet architecture were later proposed by Google
    researchers, including Inception-v3 and Inception-v4, using slightly different
    inception modules to reach even better performance.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 后来，Google的研究人员提出了GoogLeNet架构的几个变体，包括Inception-v3和Inception-v4，使用略有不同的inception模块以实现更好的性能。
- en: VGGNet
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VGGNet
- en: The runner-up in the ILSVRC 2014 challenge was [VGGNet](https://homl.info/83),⁠^([15](ch14.html#idm45720184347184))
    Karen Simonyan and Andrew Zisserman, from the Visual Geometry Group (VGG) research
    lab at Oxford University, developed a very simple and classical architecture;
    it had 2 or 3 convolutional layers and a pooling layer, then again 2 or 3 convolutional
    layers and a pooling layer, and so on (reaching a total of 16 or 19 convolutional
    layers, depending on the VGG variant), plus a final dense network with 2 hidden
    layers and the output layer. It used small 3 × 3 filters, but it had many of them.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在ILSVRC 2014挑战赛中的亚军是[VGGNet](https://homl.info/83)，Karen Simonyan和Andrew Zisserman，来自牛津大学视觉几何组（VGG）研究实验室，开发了一个非常简单和经典的架构；它有2或3个卷积层和一个池化层，然后再有2或3个卷积层和一个池化层，依此类推（达到16或19个卷积层，取决于VGG的变体），再加上一个最终的具有2个隐藏层和输出层的密集网络。它使用小的3×3滤波器，但数量很多。
- en: ResNet
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ResNet
- en: 'Kaiming He et al. won the ILSVRC 2015 challenge using a [Residual Network (ResNet)](https://homl.info/82)⁠^([16](ch14.html#idm45720184342496))
    that delivered an astounding top-five error rate under 3.6%. The winning variant
    used an extremely deep CNN composed of 152 layers (other variants had 34, 50,
    and 101 layers). It confirmed the general trend: computer vision models were getting
    deeper and deeper, with fewer and fewer parameters. The key to being able to train
    such a deep network is to use *skip connections* (also called *shortcut connections*):
    the signal feeding into a layer is also added to the output of a layer located
    higher up the stack. Let’s see why this is useful.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: Kaiming He等人在ILSVRC 2015挑战赛中使用[Residual Network (ResNet)](https://homl.info/82)赢得了冠军，其前五错误率令人惊叹地低于3.6%。获胜的变体使用了一个由152层组成的极深CNN（其他变体有34、50和101层）。它证实了一个普遍趋势：计算机视觉模型变得越来越深，参数越来越少。能够训练如此深的网络的关键是使用*跳跃连接*（也称为*快捷连接*）：输入到一个层的信号也被添加到堆栈中更高的层的输出中。让我们看看为什么这很有用。
- en: When training a neural network, the goal is to make it model a target function
    *h*(**x**). If you add the input **x** to the output of the network (i.e., you
    add a skip connection), then the network will be forced to model *f*(**x**) =
    *h*(**x**) – **x** rather than *h*(**x**). This is called *residual learning*
    (see [Figure 14-16](#residual_learning_diagram)).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练神经网络时，目标是使其模拟目标函数*h*(*x*)。如果将输入*x*添加到网络的输出中（即添加一个跳跃连接），那么网络将被迫模拟*f*(*x*)
    = *h*(*x*) - *x*而不是*h*(*x*)。这被称为*残差学习*。
- en: '![mls3 1416](assets/mls3_1416.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1416](assets/mls3_1416.png)'
- en: Figure 14-16\. Residual learning
  id: totrans-205
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-16。残差学习
- en: When you initialize a regular neural network, its weights are close to zero,
    so the network just outputs values close to zero. If you add a skip connection,
    the resulting network just outputs a copy of its inputs; in other words, it initially
    models the identity function. If the target function is fairly close to the identity
    function (which is often the case), this will speed up training considerably.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 当初始化一个常规的神经网络时，它的权重接近于零，因此网络只会输出接近于零的值。如果添加一个跳跃连接，结果网络将只输出其输入的副本；换句话说，它最初模拟的是恒等函数。如果目标函数与恒等函数相当接近（这通常是情况），这将大大加快训练速度。
- en: Moreover, if you add many skip connections, the network can start making progress
    even if several layers have not started learning yet (see [Figure 14-17](#deep_residual_network_diagram)).
    Thanks to skip connections, the signal can easily make its way across the whole
    network. The deep residual network can be seen as a stack of *residual units*
    (RUs), where each residual unit is a small neural network with a skip connection.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果添加许多跳跃连接，即使有几个层尚未开始学习，网络也可以开始取得进展（参见[图14-17](#deep_residual_network_diagram)）。由于跳跃连接，信号可以轻松地在整个网络中传播。深度残差网络可以看作是一堆*残差单元*（RUs），其中每个残差单元是一个带有跳跃连接的小型神经网络。
- en: Now let’s look at ResNet’s architecture (see [Figure 14-18](#resnet_diagram)).
    It is surprisingly simple. It starts and ends exactly like GoogLeNet (except without
    a dropout layer), and in between is just a very deep stack of residual units.
    Each residual unit is composed of two convolutional layers (and no pooling layer!),
    with batch normalization (BN) and ReLU activation, using 3 × 3 kernels and preserving
    spatial dimensions (stride 1, `"same"` padding).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一下ResNet的架构（参见[图14-18](#resnet_diagram)）。它非常简单。它的开头和结尾与GoogLeNet完全相同（除了没有丢弃层），中间只是一个非常深的残差单元堆栈。每个残差单元由两个卷积层组成（没有池化层！），使用3×3的卷积核和保持空间维度（步幅1，“same”填充）的批量归一化（BN）和ReLU激活。
- en: '![mls3 1417](assets/mls3_1417.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1417](assets/mls3_1417.png)'
- en: Figure 14-17\. Regular deep neural network (left) and deep residual network
    (right)
  id: totrans-210
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-17。常规深度神经网络（左）和深度残差网络（右）
- en: '![mls3 1418](assets/mls3_1418.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1418](assets/mls3_1418.png)'
- en: Figure 14-18\. ResNet architecture
  id: totrans-212
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-18。ResNet架构
- en: Note that the number of feature maps is doubled every few residual units, at
    the same time as their height and width are halved (using a convolutional layer
    with stride 2). When this happens, the inputs cannot be added directly to the
    outputs of the residual unit because they don’t have the same shape (for example,
    this problem affects the skip connection represented by the dashed arrow in [Figure 14-18](#resnet_diagram)).
    To solve this problem, the inputs are passed through a 1 × 1 convolutional layer
    with stride 2 and the right number of output feature maps (see [Figure 14-19](#resize_skip_connection_diagram)).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，每隔几个残差单元，特征图的数量会加倍，同时它们的高度和宽度会减半（使用步幅为2的卷积层）。当这种情况发生时，输入不能直接添加到残差单元的输出中，因为它们的形状不同（例如，这个问题影响了由虚线箭头表示的跳跃连接在[图14-18](#resnet_diagram)中的情况）。为了解决这个问题，输入通过一个步幅为2的1×1卷积层，并具有正确数量的输出特征图（参见[图14-19](#resize_skip_connection_diagram)）。
- en: '![mls3 1419](assets/mls3_1419.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1419](assets/mls3_1419.png)'
- en: Figure 14-19\. Skip connection when changing feature map size and depth
  id: totrans-215
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-19。更改特征图大小和深度时的跳跃连接
- en: Different variations of the architecture exist, with different numbers of layers.
    ResNet-34 is a ResNet with 34 layers (only counting the convolutional layers and
    the fully connected layer)⁠^([17](ch14.html#idm45720184314128)) containing 3 RUs
    that output 64 feature maps, 4 RUs with 128 maps, 6 RUs with 256 maps, and 3 RUs
    with 512 maps. We will implement this architecture later in this chapter.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 存在不同变体的架构，具有不同数量的层。ResNet-34是一个具有34层的ResNet（仅计算卷积层和全连接层），包含3个输出64个特征图的RU，4个输出128个特征图的RU，6个输出256个特征图的RU，以及3个输出512个特征图的RU。我们将在本章后面实现这个架构。
- en: Note
  id: totrans-217
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Google’s [Inception-v4](https://homl.info/84)⁠^([18](ch14.html#idm45720184310768))
    architecture merged the ideas of GoogLeNet and ResNet and achieved a top-five
    error rate of close to 3% on ImageNet classification.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: Google的[Inception-v4](https://homl.info/84)⁠^([18](ch14.html#idm45720184310768))架构融合了GoogLeNet和ResNet的思想，并在ImageNet分类中实现了接近3%的前五错误率。
- en: 'ResNets deeper than that, such as ResNet-152, use slightly different residual
    units. Instead of two 3 × 3 convolutional layers with, say, 256 feature maps,
    they use three convolutional layers: first a 1 × 1 convolutional layer with just
    64 feature maps (4 × less), which acts as a bottleneck layer (as discussed already),
    then a 3 × 3 layer with 64 feature maps, and finally another 1 × 1 convolutional
    layer with 256 feature maps (4 times 64) that restores the original depth. ResNet-152
    contains 3 such RUs that output 256 maps, then 8 RUs with 512 maps, a whopping
    36 RUs with 1,024 maps, and finally 3 RUs with 2,048 maps.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 比ResNet-152更深的ResNet，例如ResNet-152，使用略有不同的残差单元。它们不是使用两个具有256个特征图的3×3卷积层，而是使用三个卷积层：首先是一个只有64个特征图的1×1卷积层（少了4倍），它充当瓶颈层（如前所述），然后是一个具有64个特征图的3×3层，最后是另一个具有256个特征图的1×1卷积层（4倍64），恢复原始深度。ResNet-152包含3个输出256个映射的这样的RU，然后是8个输出512个映射的RU，一个令人惊叹的36个输出1024个映射的RU，最后是3个输出2048个映射的RU。
- en: Xception
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Xception
- en: 'Another variant of the GoogLeNet architecture is worth noting: [Xception](https://homl.info/xception)⁠^([19](ch14.html#idm45720184304880))
    (which stands for *Extreme Inception*) was proposed in 2016 by François Chollet
    (the author of Keras), and it significantly outperformed Inception-v3 on a huge
    vision task (350 million images and 17,000 classes). Just like Inception-v4, it
    merges the ideas of GoogLeNet and ResNet, but it replaces the inception modules
    with a special type of layer called a *depthwise separable convolution layer*
    (or *separable convolution layer* for short⁠^([20](ch14.html#idm45720184300096))).
    These layers had been used before in some CNN architectures, but they were not
    as central as in the Xception architecture. While a regular convolutional layer
    uses filters that try to simultaneously capture spatial patterns (e.g., an oval)
    and cross-channel patterns (e.g., mouth + nose + eyes = face), a separable convolutional
    layer makes the strong assumption that spatial patterns and cross-channel patterns
    can be modeled separately (see [Figure 14-20](#separable_convolution_diagram)).
    Thus, it is composed of two parts: the first part applies a single spatial filter
    to each input feature map, then the second part looks exclusively for cross-channel
    patterns—it is just a regular convolutional layer with 1 × 1 filters.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Since separable convolutional layers only have one spatial filter per input
    channel, you should avoid using them after layers that have too few channels,
    such as the input layer (granted, that’s what [Figure 14-20](#separable_convolution_diagram)
    represents, but it is just for illustration purposes). For this reason, the Xception
    architecture starts with 2 regular convolutional layers, but then the rest of
    the architecture uses only separable convolutions (34 in all), plus a few max
    pooling layers and the usual final layers (a global average pooling layer and
    a dense output layer).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'You might wonder why Xception is considered a variant of GoogLeNet, since it
    contains no inception modules at all. Well, as discussed earlier, an inception
    module contains convolutional layers with 1 × 1 filters: these look exclusively
    for cross-channel patterns. However, the convolutional layers that sit on top
    of them are regular convolutional layers that look both for spatial and cross-channel
    patterns. So, you can think of an inception module as an intermediate between
    a regular convolutional layer (which considers spatial patterns and cross-channel
    patterns jointly) and a separable convolutional layer (which considers them separately).
    In practice, it seems that separable convolutional layers often perform better.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1420](assets/mls3_1420.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: Figure 14-20\. Depthwise separable convolutional layer
  id: totrans-225
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  id: totrans-226
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Separable convolutional layers use fewer parameters, less memory, and fewer
    computations than regular convolutional layers, and they often perform better.
    Consider using them by default, except after layers with few channels (such as
    the input channel). In Keras, just use `SeparableConv2D` instead of `Conv2D`:
    it’s a drop-in replacement. Keras also offers a `DepthwiseConv2D` layer that implements
    the first part of a depthwise separable convolutional layer (i.e., applying one
    spatial filter per input feature map).'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: SENet
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The winning architecture in the ILSVRC 2017 challenge was the [Squeeze-and-Excitation
    Network (SENet)](https://homl.info/senet).⁠^([21](ch14.html#idm45720184285456))
    This architecture extends existing architectures such as inception networks and
    ResNets, and boosts their performance. This allowed SENet to win the competition
    with an astonishing 2.25% top-five error rate! The extended versions of inception
    networks and ResNets are called *SE-Inception* and *SE-ResNet*, respectively.
    The boost comes from the fact that a SENet adds a small neural network, called
    an *SE block*, to every inception module or residual unit in the original architecture,
    as shown in [Figure 14-21](#senet_diagram).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1421](assets/mls3_1421.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1421](assets/mls3_1421.png)'
- en: Figure 14-21\. SE-Inception module (left) and SE-ResNet unit (right)
  id: totrans-231
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-21\. SE-Inception模块（左）和SE-ResNet单元（右）
- en: 'An SE block analyzes the output of the unit it is attached to, focusing exclusively
    on the depth dimension (it does not look for any spatial pattern), and it learns
    which features are usually most active together. It then uses this information
    to recalibrate the feature maps, as shown in [Figure 14-22](#recalibration_diagram).
    For example, an SE block may learn that mouths, noses, and eyes usually appear
    together in pictures: if you see a mouth and a nose, you should expect to see
    eyes as well. So, if the block sees a strong activation in the mouth and nose
    feature maps, but only mild activation in the eye feature map, it will boost the
    eye feature map (more accurately, it will reduce irrelevant feature maps). If
    the eyes were somewhat confused with something else, this feature map recalibration
    will help resolve the ambiguity.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 一个SE块分析其所附加的单元的输出，专注于深度维度（不寻找任何空间模式），并学习哪些特征通常是最活跃的。然后，它使用这些信息来重新校准特征映射，如[图14-22](#recalibration_diagram)所示。例如，一个SE块可能学习到嘴巴、鼻子和眼睛通常一起出现在图片中：如果你看到嘴巴和鼻子，你应该期望也看到眼睛。因此，如果该块在嘴巴和鼻子特征映射中看到强烈的激活，但在眼睛特征映射中只有轻微的激活，它将增强眼睛特征映射（更准确地说，它将减少不相关的特征映射）。如果眼睛有些混淆，这种特征映射的重新校准将有助于解决模糊性。
- en: '![mls3 1422](assets/mls3_1422.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1422](assets/mls3_1422.png)'
- en: Figure 14-22\. An SE block performs feature map recalibration
  id: totrans-234
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-22\. 一个SE块执行特征映射重新校准
- en: 'An SE block is composed of just three layers: a global average pooling layer,
    a hidden dense layer using the ReLU activation function, and a dense output layer
    using the sigmoid activation function (see [Figure 14-23](#seblock_diagram)).'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 一个SE块由三层组成：一个全局平均池化层，一个使用ReLU激活函数的隐藏密集层，以及一个使用sigmoid激活函数的密集输出层（见[图14-23](#seblock_diagram)）。
- en: '![mls3 1423](assets/mls3_1423.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1423](assets/mls3_1423.png)'
- en: Figure 14-23\. SE block architecture
  id: totrans-237
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-23\. SE块架构
- en: 'As earlier, the global average pooling layer computes the mean activation for
    each feature map: for example, if its input contains 256 feature maps, it will
    output 256 numbers representing the overall level of response for each filter.
    The next layer is where the “squeeze” happens: this layer has significantly fewer
    than 256 neurons—typically 16 times fewer than the number of feature maps (e.g.,
    16 neurons)—so the 256 numbers get compressed into a small vector (e.g., 16 dimensions).
    This is a low-dimensional vector representation (i.e., an embedding) of the distribution
    of feature responses. This bottleneck step forces the SE block to learn a general
    representation of the feature combinations (we will see this principle in action
    again when we discuss autoencoders in [Chapter 17](ch17.html#autoencoders_chapter)).
    Finally, the output layer takes the embedding and outputs a recalibration vector
    containing one number per feature map (e.g., 256), each between 0 and 1\. The
    feature maps are then multiplied by this recalibration vector, so irrelevant features
    (with a low recalibration score) get scaled down while relevant features (with
    a recalibration score close to 1) are left alone.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，全局平均池化层计算每个特征映射的平均激活：例如，如果其输入包含256个特征映射，它将输出256个数字，表示每个滤波器的整体响应水平。接下来的层是“挤压”发生的地方：这一层的神经元数量明显少于256个——通常比特征映射的数量少16倍（例如，16个神经元）——因此256个数字被压缩成一个小向量（例如，16维）。这是特征响应分布的低维向量表示（即嵌入）。这个瓶颈步骤迫使SE块学习特征组合的一般表示（当我们讨论自动编码器时，我们将再次看到这个原则在[第17章](ch17.html#autoencoders_chapter)中）。最后，输出层接受嵌入并输出一个包含每个特征映射的重新校准向量（例如，256个），每个数字在0到1之间。然后特征映射乘以这个重新校准向量，因此不相关的特征（具有低重新校准分数）被缩小，而相关的特征（具有接近1的重新校准分数）被保留。
- en: Other Noteworthy Architectures
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他值得注意的架构
- en: 'There are many other CNN architectures to explore. Here’s a brief overview
    of some of the most noteworthy:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他CNN架构可以探索。以下是一些最值得注意的简要概述：
- en: '[ResNeXt](https://homl.info/resnext)⁠^([22](ch14.html#idm45720184264752))'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '[ResNeXt](https://homl.info/resnext)⁠^([22](ch14.html#idm45720184264752))'
- en: ResNeXt improves the residual units in ResNet. Whereas the residual units in
    the best ResNet models just contain 3 convolutional layers each, the ResNeXt residual
    units are composed of many parallel stacks (e.g., 32 stacks), with 3 convolutional
    layers each. However, the first two layers in each stack only use a few filters
    (e.g., just four), so the overall number of parameters remains the same as in
    ResNet. Then the outputs of all the stacks are added together, and the result
    is passed to the next residual unit (along with the skip connection).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ResNeXt改进了ResNet中的残差单元。而最佳ResNet模型中的残差单元只包含3个卷积层，ResNeXt的残差单元由许多并行堆栈组成（例如，32个堆栈），每个堆栈有3个卷积层。然而，每个堆栈中的前两层只使用少量滤波器（例如，只有四个），因此总参数数量与ResNet中的相同。然后，所有堆栈的输出相加，并将结果传递给下一个残差单元（以及跳跃连接）。
- en: '[DenseNet](https://homl.info/densenet)⁠^([23](ch14.html#idm45720184261328))'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '[DenseNet](https://homl.info/densenet)⁠^([23](ch14.html#idm45720184261328))'
- en: A DenseNet is composed of several dense blocks, each made up of a few densely
    connected convolutional layers. This architecture achieved excellent accuracy
    while using comparatively few parameters. What does “densely connected” mean?
    The output of each layer is fed as input to every layer after it within the same
    block. For example, layer 4 in a block takes as input the depthwise concatenation
    of the outputs of layers 1, 2, and 3 in that block. Dense blocks are separated
    by a few transition layers.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: DenseNet由几个密集块组成，每个块由几个密集连接的卷积层组成。这种架构在使用相对较少的参数的同时实现了出色的准确性。什么是“密集连接”？每一层的输出被馈送为同一块内每一层之后的每一层的输入。例如，块中的第4层以该块中第1、2和3层的输出的深度级联作为输入。密集块之间由几个过渡层分隔。
- en: '[MobileNet](https://homl.info/mobilenet)⁠^([24](ch14.html#idm45720184258784))'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '[MobileNet](https://homl.info/mobilenet)⁠^([24](ch14.html#idm45720184258784))'
- en: MobileNets are streamlined models designed to be lightweight and fast, making
    them popular in mobile and web applications. They are based on depthwise separable
    convolutional layers, like Xception. The authors proposed several variants, trading
    a bit of accuracy for faster and smaller models.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNets是精简的模型，旨在轻量且快速，因此在移动和Web应用程序中很受欢迎。它们基于深度可分离卷积层，类似于Xception。作者提出了几个变体，以牺牲一点准确性换取更快速和更小的模型。
- en: '[CSPNet](https://homl.info/cspnet)⁠^([25](ch14.html#idm45720184256368))'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '[CSPNet](https://homl.info/cspnet)⁠^([25](ch14.html#idm45720184256368))'
- en: A Cross Stage Partial Network (CSPNet) is similar to a DenseNet, but part of
    each dense block’s input is concatenated directly to that block’s output, without
    going through the block.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉阶段部分网络（CSPNet）类似于DenseNet，但是每个密集块的部分输入直接连接到该块的输出，而不经过该块。
- en: '[EfficientNet](https://homl.info/efficientnet)⁠^([26](ch14.html#idm45720184253792))'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '[EfficientNet](https://homl.info/efficientnet)⁠^([26](ch14.html#idm45720184253792))'
- en: EfficientNet is arguably the most important model in this list. The authors
    proposed a method to scale any CNN efficiently, by jointly increasing the depth
    (number of layers), width (number of filters per layer), and resolution (size
    of the input image) in a principled way. This is called *compound scaling*. They
    used neural architecture search to find a good architecture for a scaled-down
    version of ImageNet (with smaller and fewer images), and then used compound scaling
    to create larger and larger versions of this architecture. When EfficientNet models
    came out, they vastly outperformed all existing models, across all compute budgets,
    and they remain among the best models out there today.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: EfficientNet可以说是这个列表中最重要的模型。作者提出了一种有效地扩展任何CNN的方法，通过以原则性的方式同时增加深度（层数）、宽度（每层的滤波器数量）和分辨率（输入图像的大小）。这被称为*复合缩放*。他们使用神经架构搜索来找到一个适合ImageNet的缩小版本（具有更小和更少的图像）的良好架构，然后使用复合缩放来创建这种架构的越来越大的版本。当EfficientNet模型推出时，它们在所有计算预算中都远远超过了所有现有的模型，并且它们仍然是当今最好的模型之一。
- en: 'Understanding EfficientNet’s compound scaling method is helpful to gain a deeper
    understanding of CNNs, especially if you ever need to scale a CNN architecture.
    It is based on a logarithmic measure of the compute budget, noted *ϕ*: if your
    compute budget doubles, then *ϕ* increases by 1\. In other words, the number of
    floating-point operations available for training is proportional to 2^(*ϕ*). Your
    CNN architecture’s depth, width, and resolution should scale as *α*^(*ϕ*), *β*^(*ϕ*),
    and *γ*^(*ϕ*), respectively. The factors *α*, *β*, and *γ* must be greater than
    1, and *α* + *β*² + *γ*² should be close to 2\. The optimal values for these factors
    depend on the CNN’s architecture. To find the optimal values for the EfficientNet
    architecture, the authors started with a small baseline model (EfficientNetB0),
    fixed *ϕ* = 1, and simply ran a grid search: they found α = 1.2, β = 1.1, and
    γ = 1.1\. They then used these factors to create several larger architectures,
    named EfficientNetB1 to EfficientNetB7, for increasing values of *ϕ*.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 理解EfficientNet的复合缩放方法有助于更深入地理解CNN，特别是如果您需要扩展CNN架构。它基于计算预算的对数度量，标记为*ϕ*：如果您的计算预算翻倍，则*ϕ*增加1。换句话说，用于训练的浮点运算数量与2^(*ϕ*)成比例。您的CNN架构的深度、宽度和分辨率应分别按*α*^(*ϕ*)、*β*^(*ϕ*)和*γ*^(*ϕ*)缩放。因子*α*、*β*和*γ*必须大于1，且*α*
    + *β*² + *γ*²应接近2。这些因子的最佳值取决于CNN的架构。为了找到EfficientNet架构的最佳值，作者从一个小的基线模型（EfficientNetB0）开始，固定*ϕ*
    = 1，然后简单地运行了一个网格搜索：他们发现α = 1.2，β = 1.1，γ = 1.1。然后，他们使用这些因子创建了几个更大的架构，命名为EfficientNetB1到EfficientNetB7，对应不断增加的*ϕ*值。
- en: Choosing the Right CNN Architecture
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择正确的CNN架构
- en: 'With so many CNN architectures, how do you choose which one is best for your
    project? Well, it depends on what matters most to you: Accuracy? Model size (e.g.,
    for deployment to a mobile device)? Inference speed on CPU? On GPU? [Table 14-3](#model_summary_table)
    lists the best pretrained models currently available in Keras (you’ll see how
    to use them later in this chapter), sorted by model size. You can find the full
    list at [*https://keras.io/api/applications*](https://keras.io/api/applications).
    For each model, the table shows the Keras class name to use (in the `tf.keras.applications`
    package), the model’s size in MB, the top-1 and top-5 validation accuracy on the
    ImageNet dataset, the number of parameters (millions), and the inference time
    on CPU and GPU in ms, using batches of 32 images on reasonably powerful hardware.⁠^([27](ch14.html#idm45720184237344))
    For each column, the best value is highlighted. As you can see, larger models
    are generally more accurate, but not always; for example, EfficientNetB2 outperforms
    InceptionV3 both in size and accuracy. I only kept InceptionV3 in the list because
    it is almost twice as fast as EfficientNetB2 on a CPU. Similarly, InceptionResNetV2
    is fast on a CPU, and ResNet50V2 and ResNet101V2 are blazingly fast on a GPU.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 有这么多CNN架构，您如何选择最适合您项目的架构？这取决于您最关心的是什么：准确性？模型大小（例如，用于部署到移动设备）？在CPU上的推理速度？在GPU上的推理速度？[表14-3](#model_summary_table)列出了目前在Keras中可用的最佳预训练模型（您将在本章后面看到如何使用它们），按模型大小排序。您可以在[*https://keras.io/api/applications*](https://keras.io/api/applications)找到完整列表。对于每个模型，表格显示要使用的Keras类名（在`tf.keras.applications`包中）、模型的大小（MB）、在ImageNet数据集上的Top-1和Top-5验证准确率、参数数量（百万）以及在CPU和GPU上使用32张图像的推理时间（毫秒），使用性能较强的硬件。⁠^([27](ch14.html#idm45720184237344))
    对于每列，最佳值已突出显示。正如您所看到的，通常较大的模型更准确，但并非总是如此；例如，EfficientNetB2在大小和准确性上均优于InceptionV3。我之所以将InceptionV3保留在列表中，是因为在CPU上它几乎比EfficientNetB2快一倍。同样，InceptionResNetV2在CPU上速度很快，而ResNet50V2和ResNet101V2在GPU上速度极快。
- en: Table 14-3\. Pretrained models available in Keras
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 表14-3。Keras中可用的预训练模型
- en: '| Class name | Size (MB) | Top-1 acc | Top-5 acc | Params | CPU (ms) | GPU
    (ms) |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 类名 | 大小（MB） | Top-1准确率 | Top-5准确率 | 参数 | CPU（ms） | GPU（ms） |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| MobileNetV2 | **14** | 71.3% | 90.1% | **3.5M** | 25.9 | 3.8 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| MobileNetV2 | **14** | 71.3% | 90.1% | **3.5M** | 25.9 | 3.8 |'
- en: '| MobileNet | 16 | 70.4% | 89.5% | 4.3M | **22.6** | **3.4** |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| MobileNet | 16 | 70.4% | 89.5% | 4.3M | **22.6** | **3.4** |'
- en: '| NASNetMobile | 23 | 74.4% | 91.9% | 5.3M | 27.0 | 6.7 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
- en: '| EfficientNetB0 | 29 | 77.1% | 93.3% | 5.3M | 46.0 | 4.9 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
- en: '| EfficientNetB1 | 31 | 79.1% | 94.4% | 7.9M | 60.2 | 5.6 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
- en: '| EfficientNetB2 | 36 | 80.1% | 94.9% | 9.2M | 80.8 | 6.5 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
- en: '| EfficientNetB3 | 48 | 81.6% | 95.7% | 12.3M | 140.0 | 8.8 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
- en: '| EfficientNetB4 | 75 | 82.9% | 96.4% | 19.5M | 308.3 | 15.1 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
- en: '| InceptionV3 | 92 | 77.9% | 93.7% | 23.9M | 42.2 | 6.9 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
- en: '| ResNet50V2 | 98 | 76.0% | 93.0% | 25.6M | 45.6 | 4.4 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
- en: '| EfficientNetB5 | 118 | 83.6% | 96.7% | 30.6M | 579.2 | 25.3 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
- en: '| EfficientNetB6 | 166 | 84.0% | 96.8% | 43.3M | 958.1 | 40.4 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
- en: '| ResNet101V2 | 171 | 77.2% | 93.8% | 44.7M | 72.7 | 5.4 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
- en: '| InceptionResNetV2 | 215 | 80.3% | 95.3% | 55.9M | 130.2 | 10.0 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
- en: '| EfficientNetB7 | 256 | **84.3%** | **97.0%** | 66.7M | 1578.9 | 61.6 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
- en: I hope you enjoyed this deep dive into the main CNN architectures! Now let’s
    see how to implement one of them using Keras.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a ResNet-34 CNN Using Keras
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Most CNN architectures described so far can be implemented pretty naturally
    using Keras (although generally you would load a pretrained network instead, as
    you will see). To illustrate the process, let’s implement a ResNet-34 from scratch
    with Keras. First, we’ll create a `ResidualUnit` layer:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As you can see, this code matches [Figure 14-19](#resize_skip_connection_diagram)
    pretty closely. In the constructor, we create all the layers we will need: the
    main layers are the ones on the right side of the diagram, and the skip layers
    are the ones on the left (only needed if the stride is greater than 1). Then in
    the `call()` method, we make the inputs go through the main layers and the skip
    layers (if any), and we add both outputs and apply the activation function.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can build a ResNet-34 using a `Sequential` model, since it’s really
    just a long sequence of layers—we can treat each residual unit as a single layer
    now that we have the `ResidualUnit` class. The code closely matches [Figure 14-18](#resnet_diagram):'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The only tricky part in this code is the loop that adds the `ResidualUnit`
    layers to the model: as explained earlier, the first 3 RUs have 64 filters, then
    the next 4 RUs have 128 filters, and so on. At each iteration, we must set the
    stride to 1 when the number of filters is the same as in the previous RU, or else
    we set it to 2; then we add the `ResidualUnit`, and finally we update `prev_filters`.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: It is amazing that in about 40 lines of code, we can build the model that won
    the ILSVRC 2015 challenge! This demonstrates both the elegance of the ResNet model
    and the expressiveness of the Keras API. Implementing the other CNN architectures
    is a bit longer, but not much harder. However, Keras comes with several of these
    architectures built in, so why not use them instead?
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Using Pretrained Models from Keras
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In general, you won’t have to implement standard models like GoogLeNet or ResNet
    manually, since pretrained networks are readily available with a single line of
    code in the `tf.keras.applications` package.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, you can load the ResNet-50 model, pretrained on ImageNet, with
    the following line of code:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'That’s all! This will create a ResNet-50 model and download weights pretrained
    on the ImageNet dataset. To use it, you first need to ensure that the images have
    the right size. A ResNet-50 model expects 224 × 224–pixel images (other models
    may expect other sizes, such as 299 × 299), so let’s use Keras’s `Resizing` layer
    (introduced in [Chapter 13](ch13.html#data_chapter)) to resize two sample images
    (after cropping them to the target aspect ratio):'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The pretrained models assume that the images are preprocessed in a specific
    way. In some cases they may expect the inputs to be scaled from 0 to 1, or from
    –1 to 1, and so on. Each model provides a `preprocess_input()` function that you
    can use to preprocess your images. These functions assume that the original pixel
    values range from 0 to 255, which is the case here:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now we can use the pretrained model to make predictions:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'As usual, the output `Y_proba` is a matrix with one row per image and one column
    per class (in this case, there are 1,000 classes). If you want to display the
    top *K* predictions, including the class name and the estimated probability of
    each predicted class, use the `decode_predictions()` function. For each image,
    it returns an array containing the top *K* predictions, where each prediction
    is represented as an array containing the class identifier,⁠^([28](ch14.html#idm45720183471584))
    its name, and the corresponding confidence score:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output looks like this:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The correct classes are palace and dahlia, so the model is correct for the first
    image but wrong for the second. However, that’s because dahlia is not one of the
    1,000 ImageNet classes. With that in mind, vase is a reasonable guess (perhaps
    the flower is in a vase?), and daisy is not a bad choice either, since dahlias
    and daisies are both from the same Compositae family.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, it is very easy to create a pretty good image classifier using
    a pretrained model. As you saw in [Table 14-3](#model_summary_table), many other
    vision models are available in `tf.keras.applications`, from lightweight and fast
    models to large and accurate ones.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: But what if you want to use an image classifier for classes of images that are
    not part of ImageNet? In that case, you may still benefit from the pretrained
    models by using them to perform transfer learning.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Pretrained Models for Transfer Learning
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you want to build an image classifier but you do not have enough data to
    train it from scratch, then it is often a good idea to reuse the lower layers
    of a pretrained model, as we discussed in [Chapter 11](ch11.html#deep_chapter).
    For example, let’s train a model to classify pictures of flowers, reusing a pretrained
    Xception model. First, we’ll load the flowers dataset using TensorFlow Datasets
    (introduced in [Chapter 13](ch13.html#data_chapter)):'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Note that you can get information about the dataset by setting `with_info=True`.
    Here, we get the dataset size and the names of the classes. Unfortunately, there
    is only a `"train"` dataset, no test set or validation set, so we need to split
    the training set. Let’s call `tfds.load()` again, but this time taking the first
    10% of the dataset for testing, the next 15% for validation, and the remaining
    75% for training:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'All three datasets contain individual images. We need to batch them, but first
    we need to ensure they all have the same size, or batching will fail. We can use
    a `Resizing` layer for this. We must also call the `tf.keras.applications.​xcep⁠tion.preprocess_input()`
    function to preprocess the images appropriately for the Xception model. Lastly,
    we’ll also shuffle the training set and use prefetching:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Now each batch contains 32 images, all of them 224 × 224 pixels, with pixel
    values ranging from –1 to 1\. Perfect!
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the dataset is not very large, a bit of data augmentation will certainly
    help. Let’s create a data augmentation model that we will embed in our final model.
    During training, it will randomly flip the images horizontally, rotate them a
    little bit, and tweak the contrast:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Tip
  id: totrans-308
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The `tf.keras.preprocessing.image.ImageDataGenerator` class makes it easy to
    load images from disk and augment them in various ways: you can shift each image,
    rotate it, rescale it, flip it horizontally or vertically, shear it, or apply
    any transformation function you want to it. This is very convenient for simple
    projects. However, a tf.data pipeline is not much more complicated, and it’s generally
    faster. Moreover, if you have a GPU and you include the preprocessing or data
    augmentation layers inside your model, they will benefit from GPU acceleration
    during training.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: 'Next let’s load an Xception model, pretrained on ImageNet. We exclude the top
    of the network by setting `include_top=False`. This excludes the global average
    pooling layer and the dense output layer. We then add our own global average pooling
    layer (feeding it the output of the base model), followed by a dense output layer
    with one unit per class, using the softmax activation function. Finally, we wrap
    all this in a Keras `Model`:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'As explained in [Chapter 11](ch11.html#deep_chapter), it’s usually a good idea
    to freeze the weights of the pretrained layers, at least at the beginning of training:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Warning
  id: totrans-314
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Since our model uses the base model’s layers directly, rather than the `base_model`
    object itself, setting `base_model.trainable=False` would have no effect.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can compile the model and start training:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Warning
  id: totrans-318
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If you are running in Colab, make sure the runtime is using a GPU: select Runtime
    → “Change runtime type”, choose “GPU” in the “Hardware accelerator” drop-down
    menu, then click Save. It’s possible to train the model without a GPU, but it
    will be terribly slow (minutes per epoch, as opposed to seconds).'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: 'After training the model for a few epochs, its validation accuracy should reach
    a bit over 80% and then stop improving. This means that the top layers are now
    pretty well trained, and we are ready to unfreeze some of the base model’s top
    layers, then continue training. For example, let’s unfreeze layers 56 and above
    (that’s the start of residual unit 7 out of 14, as you can see if you list the
    layer names):'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Don’t forget to compile the model whenever you freeze or unfreeze layers. Also
    make sure to use a much lower learning rate to avoid damaging the pretrained weights:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This model should reach around 92% accuracy on the test set, in just a few minutes
    of training (with a GPU). If you tune the hyperparameters, lower the learning
    rate, and train for quite a bit longer, you should be able to reach 95% to 97%.
    With that, you can start training amazing image classifiers on your own images
    and classes! But there’s more to computer vision than just classification. For
    example, what if you also want to know *where* the flower is in a picture? Let’s
    look at this now.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Classification and Localization
  id: totrans-325
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Localizing an object in a picture can be expressed as a regression task, as
    discussed in [Chapter 10](ch10.html#ann_chapter): to predict a bounding box around
    the object, a common approach is to predict the horizontal and vertical coordinates
    of the object’s center, as well as its height and width. This means we have four
    numbers to predict. It does not require much change to the model; we just need
    to add a second dense output layer with four units (typically on top of the global
    average pooling layer), and it can be trained using the MSE loss:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'But now we have a problem: the flowers dataset does not have bounding boxes
    around the flowers. So, we need to add them ourselves. This is often one of the
    hardest and most costly parts of a machine learning project: getting the labels.
    It’s a good idea to spend time looking for the right tools. To annotate images
    with bounding boxes, you may want to use an open source image labeling tool like
    VGG Image Annotator, LabelImg, OpenLabeler, or ImgLab, or perhaps a commercial
    tool like LabelBox or Supervisely. You may also want to consider crowdsourcing
    platforms such as Amazon Mechanical Turk if you have a very large number of images
    to annotate. However, it is quite a lot of work to set up a crowdsourcing platform,
    prepare the form to be sent to the workers, supervise them, and ensure that the
    quality of the bounding boxes they produce is good, so make sure it is worth the
    effort. Adriana Kovashka et al. wrote a very practical [paper](https://homl.info/crowd)⁠^([29](ch14.html#idm45720182431584))
    about crowdsourcing in computer vision. I recommend you check it out, even if
    you do not plan to use crowdsourcing. If there are just a few hundred or a even
    a couple thousand images to label, and you don’t plan to do this frequently, it
    may be preferable to do it yourself: with the right tools, it will only take a
    few days, and you’ll also gain a better understanding of your dataset and task.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s suppose you’ve obtained the bounding boxes for every image in the
    flowers dataset (for now we will assume there is a single bounding box per image).
    You then need to create a dataset whose items will be batches of preprocessed
    images along with their class labels and their bounding boxes. Each item should
    be a tuple of the form `(images, (class_labels, bounding_boxes))`. Then you are
    ready to train your model!
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-330
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The bounding boxes should be normalized so that the horizontal and vertical
    coordinates, as well as the height and width, all range from 0 to 1\. Also, it
    is common to predict the square root of the height and width rather than the height
    and width directly: this way, a 10-pixel error for a large bounding box will not
    be penalized as much as a 10-pixel error for a small bounding box.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 'The MSE often works fairly well as a cost function to train the model, but
    it is not a great metric to evaluate how well the model can predict bounding boxes.
    The most common metric for this is the *intersection over union* (IoU): the area
    of overlap between the predicted bounding box and the target bounding box, divided
    by the area of their union (see [Figure 14-24](#iou_diagram)). In Keras, it is
    implemented by the `tf.keras.metrics.MeanIoU` class.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Classifying and localizing a single object is nice, but what if the images contain
    multiple objects (as is often the case in the flowers dataset)?
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1424](assets/mls3_1424.png)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
- en: Figure 14-24\. IoU metric for bounding boxes
  id: totrans-335
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Object Detection
  id: totrans-336
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The task of classifying and localizing multiple objects in an image is called
    *object detection*. Until a few years ago, a common approach was to take a CNN
    that was trained to classify and locate a single object roughly centered in the
    image, then slide this CNN across the image and make predictions at each step.
    The CNN was generally trained to predict not only class probabilities and a bounding
    box, but also an *objectness score*: this is the estimated probability that the
    image does indeed contain an object centered near the middle. This is a binary
    classification output; it can be produced by a dense output layer with a single
    unit, using the sigmoid activation function and trained using the binary cross-entropy
    loss.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-338
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Instead of an objectness score, a “no-object” class was sometimes added, but
    in general this did not work as well: the questions “Is an object present?” and
    “What type of object is it?” are best answered separately.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: This sliding-CNN approach is illustrated in [Figure 14-25](#sliding_cnn_diagram).
    In this example, the image was chopped into a 5 × 7 grid, and we see a CNN—the
    thick black rectangle—sliding across all 3 × 3 regions and making predictions
    at each step.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1425](assets/mls3_1425.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
- en: Figure 14-25\. Detecting multiple objects by sliding a CNN across the image
  id: totrans-342
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In this figure, the CNN has already made predictions for three of these 3 ×
    3 regions:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 'When looking at the top-left 3 × 3 region (centered on the red-shaded grid
    cell located in the second row and second column), it detected the leftmost rose.
    Notice that the predicted bounding box exceeds the boundary of this 3 × 3 region.
    That’s absolutely fine: even though the CNN could not see the bottom part of the
    rose, it was able to make a reasonable guess as to where it might be. It also
    predicted class probabilities, giving a high probability to the “rose” class.
    Lastly, it predicted a fairly high objectness score, since the center of the bounding
    box lies within the central grid cell (in this figure, the objectness score is
    represented by the thickness of the bounding box).'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When looking at the next 3 × 3 region, one grid cell to the right (centered
    on the shaded blue square), it did not detect any flower centered in that region,
    so it predicted a very low objectness score; therefore, the predicted bounding
    box and class probabilities can safely be ignored. You can see that the predicted
    bounding box was no good anyway.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'finally, when looking at the next 3 × 3 region, again one grid cell to the
    right (centered on the shaded green cell), it detected the rose at the top, although
    not perfectly: this rose is not well centered within this region, so the predicted
    objectness score was not very high.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can imagine how sliding the CNN across the whole image would give you a
    total of 15 predicted bounding boxes, organized in a 3 × 5 grid, with each bounding
    box accompanied by its estimated class probabilities and objectness score. Since
    objects can have varying sizes, you may then want to slide the CNN again across
    larger 4 × 4 regions as well, to get even more bounding boxes.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: 'This technique is fairly straightforward, but as you can see it will often
    detect the same object multiple times, at slightly different positions. Some postprocessing
    is needed to get rid of all the unnecessary bounding boxes. A common approach
    for this is called *non-max suppression*. Here’s how it works:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: 'First, get rid of all the bounding boxes for which the objectness score is
    below some threshold: since the CNN believes there’s no object at that location,
    the bounding box is useless.'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the remaining bounding box with the highest objectness score, and get rid
    of all the other remaining bounding boxes that overlap a lot with it (e.g., with
    an IoU greater than 60%). For example, in [Figure 14-25](#sliding_cnn_diagram),
    the bounding box with the max objectness score is the thick bounding box over
    the leftmost rose. The other bounding box that touches this same rose overlaps
    a lot with the max bounding box, so we will get rid of it (although in this example
    it would already have been removed in the previous step).
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat step 2 until there are no more bounding boxes to get rid of.
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This simple approach to object detection works pretty well, but it requires
    running the CNN many times (15 times in this example), so it is quite slow. Fortunately,
    there is a much faster way to slide a CNN across an image: using a *fully convolutional
    network* (FCN).'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: Fully Convolutional Networks
  id: totrans-353
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The idea of FCNs was first introduced in a [2015 paper](https://homl.info/fcn)⁠^([30](ch14.html#idm45720182347248))
    by Jonathan Long et al., for semantic segmentation (the task of classifying every
    pixel in an image according to the class of the object it belongs to). The authors
    pointed out that you could replace the dense layers at the top of a CNN with convolutional
    layers. To understand this, let’s look at an example: suppose a dense layer with
    200 neurons sits on top of a convolutional layer that outputs 100 feature maps,
    each of size 7 × 7 (this is the feature map size, not the kernel size). Each neuron
    will compute a weighted sum of all 100 × 7 × 7 activations from the convolutional
    layer (plus a bias term). Now let’s see what happens if we replace the dense layer
    with a convolutional layer using 200 filters, each of size 7 × 7, and with `"valid"`
    padding. This layer will output 200 feature maps, each 1 × 1 (since the kernel
    is exactly the size of the input feature maps and we are using `"valid"` padding).
    In other words, it will output 200 numbers, just like the dense layer did; and
    if you look closely at the computations performed by a convolutional layer, you
    will notice that these numbers will be precisely the same as those the dense layer
    produced. The only difference is that the dense layer’s output was a tensor of
    shape [*batch size*, 200], while the convolutional layer will output a tensor
    of shape [*batch size*, 1, 1, 200].'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-355
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To convert a dense layer to a convolutional layer, the number of filters in
    the convolutional layer must be equal to the number of units in the dense layer,
    the filter size must be equal to the size of the input feature maps, and you must
    use `"valid"` padding. The stride may be set to 1 or more, as you will see shortly.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: Why is this important? Well, while a dense layer expects a specific input size
    (since it has one weight per input feature), a convolutional layer will happily
    process images of any size⁠^([31](ch14.html#idm45720182340336)) (however, it does
    expect its inputs to have a specific number of channels, since each kernel contains
    a different set of weights for each input channel). Since an FCN contains only
    convolutional layers (and pooling layers, which have the same property), it can
    be trained and executed on images of any size!
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, suppose we’d already trained a CNN for flower classification and
    localization. It was trained on 224 × 224 images, and it outputs 10 numbers:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: Outputs 0 to 4 are sent through the softmax activation function, and this gives
    the class probabilities (one per class).
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output 5 is sent through the sigmoid activation function, and this gives the
    objectness score.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outputs 6 and 7 represent the bounding box’s center coordinates; they also go
    through a sigmoid activation function to ensure they range from 0 to 1.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lastly, outputs 8 and 9 represent the bounding box’s height and width; they
    do not go through any activation function to allow the bounding boxes to extend
    beyond the borders of the image.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can now convert the CNN’s dense layers to convolutional layers. In fact,
    we don’t even need to retrain it; we can just copy the weights from the dense
    layers to the convolutional layers! Alternatively, we could have converted the
    CNN into an FCN before training.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: Now suppose the last convolutional layer before the output layer (also called
    the bottleneck layer) outputs 7 × 7 feature maps when the network is fed a 224
    × 224 image (see the left side of [Figure 14-26](#fcn_diagram)). If we feed the
    FCN a 448 × 448 image (see the right side of [Figure 14-26](#fcn_diagram)), the
    bottleneck layer will now output 14 × 14 feature maps.⁠^([32](ch14.html#idm45720182331760))
    Since the dense output layer was replaced by a convolutional layer using 10 filters
    of size 7 × 7, with `"valid"` padding and stride 1, the output will be composed
    of 10 features maps, each of size 8 × 8 (since 14 – 7 + 1 = 8). In other words,
    the FCN will process the whole image only once, and it will output an 8 × 8 grid
    where each cell contains 10 numbers (5 class probabilities, 1 objectness score,
    and 4 bounding box coordinates). It’s exactly like taking the original CNN and
    sliding it across the image using 8 steps per row and 8 steps per column. To visualize
    this, imagine chopping the original image into a 14 × 14 grid, then sliding a
    7 × 7 window across this grid; there will be 8 × 8 = 64 possible locations for
    the window, hence 8 × 8 predictions. However, the FCN approach is *much* more
    efficient, since the network only looks at the image once. In fact, *You Only
    Look Once* (YOLO) is the name of a very popular object detection architecture,
    which we’ll look at next.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1426](assets/mls3_1426.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
- en: Figure 14-26\. The same fully convolutional network processing a small image
    (left) and a large one (right)
  id: totrans-366
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You Only Look Once
  id: totrans-367
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'YOLO is a fast and accurate object detection architecture proposed by Joseph
    Redmon et al. in a [2015 paper](https://homl.info/yolo).⁠^([33](ch14.html#idm45720182322240))
    It is so fast that it can run in real time on a video, as seen in Redmon’s [demo](https://homl.info/yolodemo).
    YOLO’s architecture is quite similar to the one we just discussed, but with a
    few important differences:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: For each grid cell, YOLO only considers objects whose bounding box center lies
    within that cell. The bounding box coordinates are relative to that cell, where
    (0, 0) means the top-left corner of the cell and (1, 1) means the bottom right.
    However, the bounding box’s height and width may extend well beyond the cell.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It outputs two bounding boxes for each grid cell (instead of just one), which
    allows the model to handle cases where two objects are so close to each other
    that their bounding box centers lie within the same cell. Each bounding box also
    comes with its own objectness score.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'YOLO also outputs a class probability distribution for each grid cell, predicting
    20 class probabilities per grid cell since YOLO was trained on the PASCAL VOC
    dataset, which contains 20 classes. This produces a coarse *class probability
    map*. Note that the model predicts one class probability distribution per grid
    cell, not per bounding box. However, it’s possible to estimate class probabilities
    for each bounding box during postprocessing, by measuring how well each bounding
    box matches each class in the class probability map. For example, imagine a picture
    of a person standing in front of a car. There will be two bounding boxes: one
    large horizontal one for the car, and a smaller vertical one for the person. These
    bounding boxes may have their centers within the same grid cell. So how can we
    tell which class should be assigned to each bounding box? Well, the class probability
    map will contain a large region where the “car” class is dominant, and inside
    it there will be a smaller region where the “person” class is dominant. Hopefully,
    the car’s bounding box will roughly match the “car” region, while the person’s
    bounding box will roughly match the “person” region: this will allow the correct
    class to be assigned to each bounding box.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: YOLO was originally developed using Darknet, an open source deep learning framework
    initially developed in C by Joseph Redmon, but it was soon ported to TensorFlow,
    Keras, PyTorch, and more. It was continuously improved over the years, with YOLOv2,
    YOLOv3, and YOLO9000 (again by Joseph Redmon et al.), YOLOv4 (by Alexey Bochkovskiy
    et al.), YOLOv5 (by Glenn Jocher), and PP-YOLO (by Xiang Long et al.).
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: Each version brought some impressive improvements in speed and accuracy, using
    a variety of techniques; for example, YOLOv3 boosted accuracy in part thanks to
    *anchor priors*, exploiting the fact that some bounding box shapes are more likely
    than others, depending on the class (e.g., people tend to have vertical bounding
    boxes, while cars usually don’t). They also increased the number of bounding boxes
    per grid cell, they trained on different datasets with many more classes (up to
    9,000 classes organized in a hierarchy in the case of YOLO9000), they added skip
    connections to recover some of the spatial resolution that is lost in the CNN
    (we will discuss this shortly, when we look at semantic segmentation), and much
    more. There are many variants of these models too, such as YOLOv4-tiny, which
    is optimized to be trained on less powerful machines and which can run extremely
    fast (at over 1,000 frames per second!), but with a slightly lower *mean average
    precision* (mAP).
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: Many object detection models are available on TensorFlow Hub, often with pretrained
    weights, such as YOLOv5,⁠^([34](ch14.html#idm45720182300272)) [SSD](https://homl.info/ssd),⁠^([35](ch14.html#idm45720182297456))
    [Faster R-CNN](https://homl.info/fasterrcnn),⁠^([36](ch14.html#idm45720182295456))
    and [EfficentDet](https://homl.info/efficientdet).⁠^([37](ch14.html#idm45720182293376))
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: 'SSD and EfficientDet are “look once” detection models, similar to YOLO. EfficientDet
    is based on the EfficientNet convolutional architecture. Faster R-CNN is more
    complex: the image first goes through a CNN, then the output is passed to a *region
    proposal network* (RPN) that proposes bounding boxes that are most likely to contain
    an object; a classifier is then run for each bounding box, based on the cropped
    output of the CNN. The best place to start using these models is TensorFlow Hub’s
    excellent [object detection tutorial](https://homl.info/objdet).'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: So far, we’ve only considered detecting objects in single images. But what about
    videos? Objects must not only be detected in each frame, they must also be tracked
    over time. Let’s take a quick look at object tracking now.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: Object Tracking
  id: totrans-377
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Object tracking is a challenging task: objects move, they may grow or shrink
    as they get closer to or further away from the camera, their appearance may change
    as they turn around or move to different lighting conditions or backgrounds, they
    may be temporarily occluded by other objects, and so on.'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most popular object tracking systems is [DeepSORT](https://homl.info/deepsort).⁠^([38](ch14.html#idm45720182279200))
    It is based on a combination of classical algorithms and deep learning:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: It uses *Kalman filters* to estimate the most likely current position of an
    object given prior detections, and assuming that objects tend to move at a constant
    speed.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It uses a deep learning model to measure the resemblance between new detections
    and existing tracked objects.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lastly, it uses the *Hungarian algorithm* to map new detections to existing
    tracked objects (or to new tracked objects): this algorithm efficiently finds
    the combination of mappings that minimizes the distance between the detections
    and the predicted positions of tracked objects, while also minimizing the appearance
    discrepancy.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, imagine a red ball that just bounced off a blue ball traveling
    in the opposite direction. Based on the previous positions of the balls, the Kalman
    filter will predict that the balls will go through each other: indeed, it assumes
    that objects move at a constant speed, so it will not expect the bounce. If the
    Hungarian algorithm only considered positions, then it would happily map the new
    detections to the wrong balls, as if they had just gone through each other and
    swapped colors. But thanks to the resemblance measure, the Hungarian algorithm
    will notice the problem. Assuming the balls are not too similar, the algorithm
    will map the new detections to the correct balls.'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-384
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'There are a few DeepSORT implementations available on GitHub, including a TensorFlow
    implementation of YOLOv4 + DeepSORT: [*https://github.com/theAIGuysCode/yolov4-deepsort*](https://github.com/theAIGuysCode/yolov4-deepsort).'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: So far we have located objects using bounding boxes. This is often sufficient,
    but sometimes you need to locate objects with much more precision—for example,
    to remove the background behind a person during a videoconference call. Let’s
    see how to go down to the pixel level.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: Semantic Segmentation
  id: totrans-387
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *semantic segmentation*, each pixel is classified according to the class
    of the object it belongs to (e.g., road, car, pedestrian, building, etc.), as
    shown in [Figure 14-27](#semantic_segmentation_diagram). Note that different objects
    of the same class are *not* distinguished. For example, all the bicycles on the
    right side of the segmented image end up as one big lump of pixels. The main difficulty
    in this task is that when images go through a regular CNN, they gradually lose
    their spatial resolution (due to the layers with strides greater than 1); so,
    a regular CNN may end up knowing that there’s a person somewhere in the bottom
    left of the image, but it will not be much more precise than that.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1427](assets/mls3_1427.png)'
  id: totrans-389
  prefs: []
  type: TYPE_IMG
- en: Figure 14-27\. Semantic segmentation
  id: totrans-390
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Just like for object detection, there are many different approaches to tackle
    this problem, some quite complex. However, a fairly simple solution was proposed
    in the 2015 paper by Jonathan Long et al. I mentioned earlier, on fully convolutional
    networks. The authors start by taking a pretrained CNN and turning it into an
    FCN. The CNN applies an overall stride of 32 to the input image (i.e., if you
    add up all the strides greater than 1), meaning the last layer outputs feature
    maps that are 32 times smaller than the input image. This is clearly too coarse,
    so they added a single *upsampling layer* that multiplies the resolution by 32.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: There are several solutions available for upsampling (increasing the size of
    an image), such as bilinear interpolation, but that only works reasonably well
    up to ×4 or ×8\. Instead, they use a *transposed convolutional layer*:⁠^([39](ch14.html#idm45720182251296))
    this is equivalent to first stretching the image by inserting empty rows and columns
    (full of zeros), then performing a regular convolution (see [Figure 14-28](#conv2d_transpose_diagram)).
    Alternatively, some people prefer to think of it as a regular convolutional layer
    that uses fractional strides (e.g., the stride is 1/2 in [Figure 14-28](#conv2d_transpose_diagram)).
    The transposed convolutional layer can be initialized to perform something close
    to linear interpolation, but since it is a trainable layer, it will learn to do
    better during training. In Keras, you can use the `Conv2DTranspose` layer.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-393
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In a transposed convolutional layer, the stride defines how much the input will
    be stretched, not the size of the filter steps, so the larger the stride, the
    larger the output (unlike for convolutional layers or pooling layers).
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1428](assets/mls3_1428.png)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
- en: Figure 14-28\. Upsampling using a transposed convolutional layer
  id: totrans-396
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Using transposed convolutional layers for upsampling is OK, but still too imprecise.
    To do better, Long et al. added skip connections from lower layers: for example,
    they upsampled the output image by a factor of 2 (instead of 32), and they added
    the output of a lower layer that had this double resolution. Then they upsampled
    the result by a factor of 16, leading to a total upsampling factor of 32 (see
    [Figure 14-29](#skip_plus_upsample_diagram)). This recovered some of the spatial
    resolution that was lost in earlier pooling layers. In their best architecture,
    they used a second similar skip connection to recover even finer details from
    an even lower layer. In short, the output of the original CNN goes through the
    following extra steps: upsample ×2, add the output of a lower layer (of the appropriate
    scale), upsample ×2, add the output of an even lower layer, and finally upsample
    ×8\. It is even possible to scale up beyond the size of the original image: this
    can be used to increase the resolution of an image, which is a technique called
    *super-resolution*.'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1429](assets/mls3_1429.png)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
- en: Figure 14-29\. Skip layers recover some spatial resolution from lower layers
  id: totrans-399
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*Instance segmentation* is similar to semantic segmentation, but instead of
    merging all objects of the same class into one big lump, each object is distinguished
    from the others (e.g., it identifies each individual bicycle). For example the
    *Mask R-CNN* architecture, proposed in a [2017 paper](https://homl.info/maskrcnn)⁠^([40](ch14.html#idm45720182215744))
    by Kaiming He et al., extends the Faster R-CNN model by additionally producing
    a pixel mask for each bounding box. So, not only do you get a bounding box around
    each object, with a set of estimated class probabilities, but you also get a pixel
    mask that locates pixels in the bounding box that belong to the object. This model
    is available on TensorFlow Hub, pretrained on the COCO 2017 dataset. The field
    is moving fast, though so if you want to try the latest and greatest models, please
    check out the state-of-the-art section of [*https://paperswithcode.com*](https://paperswithcode.com).'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, the field of deep computer vision is vast and fast-paced, with
    all sorts of architectures popping up every year. Almost all of them are based
    on convolutional neural networks, but since 2020 another neural net architecture
    has entered the computer vision space: transformers (which we will discuss in
    [Chapter 16](ch16.html#nlp_chapter)). The progress made over the last decade has
    been astounding, and researchers are now focusing on harder and harder problems,
    such as *adversarial learning* (which attempts to make the network more resistant
    to images designed to fool it), *explainability* (understanding why the network
    makes a specific classification), realistic *image generation* (which we will
    come back to in [Chapter 17](ch17.html#autoencoders_chapter)), *single-shot learning*
    (a system that can recognize an object after it has seen it just once), predicting
    the next frames in a video, combining text and image tasks, and more.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: Now on to the next chapter, where we will look at how to process sequential
    data such as time series using recurrent neural networks and convolutional neural
    networks.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  id: totrans-403
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are the advantages of a CNN over a fully connected DNN for image classification?
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels,
    a stride of 2, and `"same"` padding. The lowest layer outputs 100 feature maps,
    the middle one outputs 200, and the top one outputs 400\. The input images are
    RGB images of 200 × 300 pixels:'
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the total number of parameters in the CNN?
  id: totrans-406
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If we are using 32-bit floats, at least how much RAM will this network require
    when making a prediction for a single instance?
  id: totrans-407
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What about when training on a mini-batch of 50 images?
  id: totrans-408
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If your GPU runs out of memory while training a CNN, what are five things you
    could try to solve the problem?
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why would you want to add a max pooling layer rather than a convolutional layer
    with the same stride?
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When would you want to add a local response normalization layer?
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you name the main innovations in AlexNet, as compared to LeNet-5? What about
    the main innovations in GoogLeNet, ResNet, SENet, Xception, and EfficientNet?
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a fully convolutional network? How can you convert a dense layer into
    a convolutional layer?
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the main technical difficulty of semantic segmentation?
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build your own CNN from scratch and try to achieve the highest possible accuracy
    on MNIST.
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use transfer learning for large image classification, going through these steps:'
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a training set containing at least 100 images per class. For example,
    you could classify your own pictures based on the location (beach, mountain, city,
    etc.), or alternatively you can use an existing dataset (e.g., from TensorFlow
    Datasets).
  id: totrans-417
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Split it into a training set, a validation set, and a test set.
  id: totrans-418
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Build the input pipeline, apply the appropriate preprocessing operations, and
    optionally add data augmentation.
  id: totrans-419
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Fine-tune a pretrained model on this dataset.
  id: totrans-420
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Go through TensorFlow’s [Style Transfer tutorial](https://homl.info/styletuto).
    This is a fun way to generate art using deep learning.
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab3*](https://homl.info/colab3).
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch14.html#idm45720185768624-marker)) David H. Hubel, “Single Unit Activity
    in Striate Cortex of Unrestrained Cats”, *The Journal of Physiology* 147 (1959):
    226–238.'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch14.html#idm45720185766752-marker)) David H. Hubel and Torsten N. Wiesel,
    “Receptive Fields of Single Neurons in the Cat’s Striate Cortex”, *The Journal
    of Physiology* 148 (1959): 574–591.'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: '^([3](ch14.html#idm45720185764704-marker)) David H. Hubel and Torsten N. Wiesel,
    “Receptive Fields and Functional Architecture of Monkey Striate Cortex”, *The
    Journal of Physiology* 195 (1968): 215–243.'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: '^([4](ch14.html#idm45720185754832-marker)) Kunihiko Fukushima, “Neocognitron:
    A Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition
    Unaffected by Shift in Position”, *Biological Cybernetics* 36 (1980): 193–202.'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: '^([5](ch14.html#idm45720185753008-marker)) Yann LeCun et al., “Gradient-Based
    Learning Applied to Document Recognition”, *Proceedings of the IEEE* 86, no. 11
    (1998): 2278–2324.'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch14.html#idm45720185745312-marker)) A convolution is a mathematical operation
    that slides one function over another and measures the integral of their pointwise
    multiplication. It has deep connections with the Fourier transform and the Laplace
    transform and is heavily used in signal processing. Convolutional layers actually
    use cross-correlations, which are very similar to convolutions (see [*https://homl.info/76*](https://homl.info/76)
    for more details).
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch14.html#idm45720185247456-marker)) To produce the same size outputs,
    a fully connected layer would need 200 × 150 × 100 neurons, each connected to
    all 150 × 100 × 3 inputs. It would have 200 × 150 × 100 × (150 × 100 × 3 + 1)
    ≈ 135 billion parameters!
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch14.html#idm45720185245968-marker)) In the international system of units
    (SI), 1 MB = 1,000 KB = 1,000 × 1,000 bytes = 1,000 × 1,000 × 8 bits. And 1 MiB
    = 1,024 kiB = 1,024 × 1,024 bytes. So 12 MB ≈ 11.44 MiB.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: '^([9](ch14.html#idm45720185209104-marker)) Other kernels we’ve discussed so
    far had weights, but pooling kernels do not: they are just stateless sliding windows.'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: '^([10](ch14.html#idm45720184581728-marker)) Yann LeCun et al., “Gradient-Based
    Learning Applied to Document Recognition”, *Proceedings of the IEEE* 86, no. 11
    (1998): 2278–2324.'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: '^([11](ch14.html#idm45720184531840-marker)) Alex Krizhevsky et al., “ImageNet
    Classification with Deep Convolutional Neural Networks”, *Proceedings of the 25th
    International Conference on Neural Information Processing Systems* 1 (2012): 1097–1105.'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: '^([12](ch14.html#idm45720184386400-marker)) Matthew D. Zeiler and Rob Fergus,
    “Visualizing and Understanding Convolutional Networks”, *Proceedings of the European
    Conference on Computer Vision* (2014): 818–833.'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: '^([13](ch14.html#idm45720184382672-marker)) Christian Szegedy et al., “Going
    Deeper with Convolutions”, *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition* (2015): 1–9.'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: ^([14](ch14.html#idm45720184379088-marker)) In the 2010 movie *Inception*, the
    characters keep going deeper and deeper into multiple layers of dreams; hence
    the name of these modules.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: ^([15](ch14.html#idm45720184347184-marker)) Karen Simonyan and Andrew Zisserman,
    “Very Deep Convolutional Networks for Large-Scale Image Recognition”, arXiv preprint
    arXiv:1409.1556 (2014).
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: ^([16](ch14.html#idm45720184342496-marker)) Kaiming He et al., “Deep Residual
    Learning for Image Recognition”, arXiv preprint arXiv:1512:03385 (2015).
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: ^([17](ch14.html#idm45720184314128-marker)) It is a common practice when describing
    a neural network to count only layers with parameters.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: ^([18](ch14.html#idm45720184310768-marker)) Christian Szegedy et al., “Inception–v4,
    Inception-ResNet and the Impact of Residual Connections on Learning”, arXiv preprint
    arXiv:1602.07261 (2016).
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: '^([19](ch14.html#idm45720184304880-marker)) François Chollet, “Xception: Deep
    Learning with Depthwise Separable Convolutions”, arXiv preprint arXiv:1610.02357
    (2016).'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: ^([20](ch14.html#idm45720184300096-marker)) This name can sometimes be ambiguous,
    since spatially separable convolutions are often called “separable convolutions”
    as well.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: '^([21](ch14.html#idm45720184285456-marker)) Jie Hu et al., “Squeeze-and-Excitation
    Networks”, *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition* (2018): 7132–7141.'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: ^([22](ch14.html#idm45720184264752-marker)) Saining Xie et al., “Aggregated
    Residual Transformations for Deep Neural Networks”, arXiv preprint arXiv:1611.05431
    (2016).
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: ^([23](ch14.html#idm45720184261328-marker)) Gao Huang et al., “Densely Connected
    Convolutional Networks”, arXiv preprint arXiv:1608.06993 (2016).
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: '^([24](ch14.html#idm45720184258784-marker)) Andrew G. Howard et al., “MobileNets:
    Efficient Convolutional Neural Networks for Mobile Vision Applications”, arXiv
    preprint arxiv:1704.04861 (2017).'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: '^([25](ch14.html#idm45720184256368-marker)) Chien-Yao Wang et al., “CSPNet:
    A New Backbone That Can Enhance Learning Capability of CNN”, arXiv preprint arXiv:1911.11929
    (2019).'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: '^([26](ch14.html#idm45720184253792-marker)) Mingxing Tan and Quoc V. Le, “EfficientNet:
    Rethinking Model Scaling for Convolutional Neural Networks”, arXiv preprint arXiv:1905.11946
    (2019).'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: ^([27](ch14.html#idm45720184237344-marker)) A 92-core AMD EPYC CPU with IBPB,
    1.7 TB of RAM, and an Nvidia Tesla A100 GPU.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: '^([28](ch14.html#idm45720183471584-marker)) In the ImageNet dataset, each image
    is mapped to a word in the [WordNet dataset](https://wordnet.princeton.edu): the
    class ID is just a WordNet ID.'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: '^([29](ch14.html#idm45720182431584-marker)) Adriana Kovashka et al., “Crowdsourcing
    in Computer Vision”, *Foundations and Trends in Computer Graphics and Vision*
    10, no. 3 (2014): 177–243.'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: '^([30](ch14.html#idm45720182347248-marker)) Jonathan Long et al., “Fully Convolutional
    Networks for Semantic Segmentation”, *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition* (2015): 3431–3440.'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: '^([31](ch14.html#idm45720182340336-marker)) There is one small exception: a
    convolutional layer using `"valid"` padding will complain if the input size is
    smaller than the kernel size.'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: '^([32](ch14.html#idm45720182331760-marker)) This assumes we used only `"same"`
    padding in the network: `"valid"` padding would reduce the size of the feature
    maps. Moreover, 448 can be neatly divided by 2 several times until we reach 7,
    without any rounding error. If any layer uses a different stride than 1 or 2,
    then there may be some rounding error, so again the feature maps may end up being
    smaller.'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: '^([33](ch14.html#idm45720182322240-marker)) Joseph Redmon et al., “You Only
    Look Once: Unified, Real-Time Object Detection”, *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition* (2016): 779–788.'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: ^([34](ch14.html#idm45720182300272-marker)) You can find YOLOv3, YOLOv4, and
    their tiny variants in the TensorFlow Models project at [*https://homl.info/yolotf*](https://homl.info/yolotf).
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: '^([35](ch14.html#idm45720182297456-marker)) Wei Liu et al., “SSD: Single Shot
    Multibox Detector”, *Proceedings of the 14th European Conference on Computer Vision*
    1 (2016): 21–37.'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: '^([36](ch14.html#idm45720182295456-marker)) Shaoqing Ren et al., “Faster R-CNN:
    Towards Real-Time Object Detection with Region Proposal Networks”, *Proceedings
    of the 28th International Conference on Neural Information Processing Systems*
    1 (2015): 91–99.'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: '^([37](ch14.html#idm45720182293376-marker)) Mingxing Tan et al., “EfficientDet:
    Scalable and Efficient Object Detection”, arXiv preprint arXiv:1911.09070 (2019).'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: ^([38](ch14.html#idm45720182279200-marker)) Nicolai Wojke et al., “Simple Online
    and Realtime Tracking with a Deep Association Metric”, arXiv preprint arXiv:1703.07402
    (2017).
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: ^([39](ch14.html#idm45720182251296-marker)) This type of layer is sometimes
    referred to as a *deconvolution layer*, but it does *not* perform what mathematicians
    call a deconvolution, so this name should be avoided.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: ^([40](ch14.html#idm45720182215744-marker)) Kaiming He et al., “Mask R-CNN”,
    arXiv preprint arXiv:1703.06870 (2017).
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
