- en: Chapter 14\. Deep Computer Vision Using Convolutional Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although IBM’s Deep Blue supercomputer beat the chess world champion Garry
    Kasparov back in 1996, it wasn’t until fairly recently that computers were able
    to reliably perform seemingly trivial tasks such as detecting a puppy in a picture
    or recognizing spoken words. Why are these tasks so effortless to us humans? The
    answer lies in the fact that perception largely takes place outside the realm
    of our consciousness, within specialized visual, auditory, and other sensory modules
    in our brains. By the time sensory information reaches our consciousness, it is
    already adorned with high-level features; for example, when you look at a picture
    of a cute puppy, you cannot choose *not* to see the puppy, *not* to notice its
    cuteness. Nor can you explain *how* you recognize a cute puppy; it’s just obvious
    to you. Thus, we cannot trust our subjective experience: perception is not trivial
    at all, and to understand it we must look at how our sensory modules work.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: '*Convolutional neural networks* (CNNs) emerged from the study of the brain’s
    visual cortex, and they have been used in computer image recognition since the
    1980s. Over the last 10 years, thanks to the increase in computational power,
    the amount of available training data, and the tricks presented in [Chapter 11](ch11.html#deep_chapter)
    for training deep nets, CNNs have managed to achieve superhuman performance on
    some complex visual tasks. They power image search services, self-driving cars,
    automatic video classification systems, and more. Moreover, CNNs are not restricted
    to visual perception: they are also successful at many other tasks, such as voice
    recognition and natural language processing. However, we will focus on visual
    applications for now.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we will explore where CNNs came from, what their building blocks
    look like, and how to implement them using Keras. Then we will discuss some of
    the best CNN architectures, as well as other visual tasks, including object detection
    (classifying multiple objects in an image and placing bounding boxes around them)
    and semantic segmentation (classifying each pixel according to the class of the
    object it belongs to).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: The Architecture of the Visual Cortex
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: David H. Hubel and Torsten Wiesel performed a series of experiments on cats
    in [1958](https://homl.info/71)⁠^([1](ch14.html#idm45720185768624)) and [1959](https://homl.info/72)⁠^([2](ch14.html#idm45720185766752))
    (and a [few years later on monkeys](https://homl.info/73)⁠^([3](ch14.html#idm45720185764704))),
    giving crucial insights into the structure of the visual cortex (the authors received
    the Nobel Prize in Physiology or Medicine in 1981 for their work). In particular,
    they showed that many neurons in the visual cortex have a small *local receptive
    field*, meaning they react only to visual stimuli located in a limited region
    of the visual field (see [Figure 14-1](#cat_visual_cortex_diagram), in which the
    local receptive fields of five neurons are represented by dashed circles). The
    receptive fields of different neurons may overlap, and together they tile the
    whole visual field.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1401](assets/mls3_1401.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: Figure 14-1\. Biological neurons in the visual cortex respond to specific patterns
    in small regions of the visual field called receptive fields; as the visual signal
    makes its way through consecutive brain modules, neurons respond to more complex
    patterns in larger receptive fields
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Moreover, the authors showed that some neurons react only to images of horizontal
    lines, while others react only to lines with different orientations (two neurons
    may have the same receptive field but react to different line orientations). They
    also noticed that some neurons have larger receptive fields, and they react to
    more complex patterns that are combinations of the lower-level patterns. These
    observations led to the idea that the higher-level neurons are based on the outputs
    of neighboring lower-level neurons (in [Figure 14-1](#cat_visual_cortex_diagram),
    notice that each neuron is connected only to nearby neurons from the previous
    layer). This powerful architecture is able to detect all sorts of complex patterns
    in any area of the visual field.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 'These studies of the visual cortex inspired the [neocognitron](https://homl.info/74),⁠^([4](ch14.html#idm45720185754832))
    introduced in 1980, which gradually evolved into what we now call convolutional
    neural networks. An important milestone was a [1998 paper](https://homl.info/75)⁠^([5](ch14.html#idm45720185753008))
    by Yann LeCun et al. that introduced the famous *LeNet-5* architecture, which
    became widely used by banks to recognize handwritten digits on checks. This architecture
    has some building blocks that you already know, such as fully connected layers
    and sigmoid activation functions, but it also introduces two new building blocks:
    *convolutional layers* and *pooling layers*. Let’s look at them now.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Why not simply use a deep neural network with fully connected layers for image
    recognition tasks? Unfortunately, although this works fine for small images (e.g.,
    MNIST), it breaks down for larger images because of the huge number of parameters
    it requires. For example, a 100 × 100–pixel image has 10,000 pixels, and if the
    first layer has just 1,000 neurons (which already severely restricts the amount
    of information transmitted to the next layer), this means a total of 10 million
    connections. And that’s just the first layer. CNNs solve this problem using partially
    connected layers and weight sharing.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Layers
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most important building block of a CNN is the *convolutional layer*:⁠^([6](ch14.html#idm45720185745312))
    neurons in the first convolutional layer are not connected to every single pixel
    in the input image (like they were in the layers discussed in previous chapters),
    but only to pixels in their receptive fields (see [Figure 14-2](#cnn_layers_diagram)).
    In turn, each neuron in the second convolutional layer is connected only to neurons
    located within a small rectangle in the first layer. This architecture allows
    the network to concentrate on small low-level features in the first hidden layer,
    then assemble them into larger higher-level features in the next hidden layer,
    and so on. This hierarchical structure is common in real-world images, which is
    one of the reasons why CNNs work so well for image recognition.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1402](assets/mls3_1402.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: Figure 14-2\. CNN layers with rectangular local receptive fields
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: All the multilayer neural networks we’ve looked at so far had layers composed
    of a long line of neurons, and we had to flatten input images to 1D before feeding
    them to the neural network. In a CNN each layer is represented in 2D, which makes
    it easier to match neurons with their corresponding inputs.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: A neuron located in row *i*, column *j* of a given layer is connected to the
    outputs of the neurons in the previous layer located in rows *i* to *i* + *f*[*h*]
    – 1, columns *j* to *j* + *f*[*w*] – 1, where *f*[*h*] and *f*[*w*] are the height
    and width of the receptive field (see [Figure 14-3](#slide_and_padding_diagram)).
    In order for a layer to have the same height and width as the previous layer,
    it is common to add zeros around the inputs, as shown in the diagram. This is
    called *zero* *padding*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: It is also possible to connect a large input layer to a much smaller layer by
    spacing out the receptive fields, as shown in [Figure 14-4](#stride_diagram).
    This dramatically reduces the model’s computational complexity. The horizontal
    or vertical step size from one receptive field to the next is called the *stride*.
    In the diagram, a 5 × 7 input layer (plus zero padding) is connected to a 3 ×
    4 layer, using 3 × 3 receptive fields and a stride of 2 (in this example the stride
    is the same in both directions, but it does not have to be so). A neuron located
    in row *i*, column *j* in the upper layer is connected to the outputs of the neurons
    in the previous layer located in rows *i* × *s*[*h*] to *i* × *s*[*h*] + *f*[*h*]
    – 1, columns *j* × *s*[*w*] to *j* × *s*[*w*] + *f*[*w*] – 1, where *s*[*h*] and
    *s*[*w*] are the vertical and horizontal strides.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1403](assets/mls3_1403.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: Figure 14-3\. Connections between layers and zero padding
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![mls3 1404](assets/mls3_1404.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
- en: Figure 14-4\. Reducing dimensionality using a stride of 2
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Filters
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A neuron’s weights can be represented as a small image the size of the receptive
    field. For example, [Figure 14-5](#filters_diagram) shows two possible sets of
    weights, called *filters* (or *convolution kernels*, or just *kernels*). The first
    one is represented as a black square with a vertical white line in the middle
    (it’s a 7 × 7 matrix full of 0s except for the central column, which is full of
    1s); neurons using these weights will ignore everything in their receptive field
    except for the central vertical line (since all inputs will be multiplied by 0,
    except for the ones in the central vertical line). The second filter is a black
    square with a horizontal white line in the middle. Neurons using these weights
    will ignore everything in their receptive field except for the central horizontal
    line.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1405](assets/mls3_1405.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
- en: Figure 14-5\. Applying two different filters to get two feature maps
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now if all neurons in a layer use the same vertical line filter (and the same
    bias term), and you feed the network the input image shown in [Figure 14-5](#filters_diagram)
    (the bottom image), the layer will output the top-left image. Notice that the
    vertical white lines get enhanced while the rest gets blurred. Similarly, the
    upper-right image is what you get if all neurons use the same horizontal line
    filter; notice that the horizontal white lines get enhanced while the rest is
    blurred out. Thus, a layer full of neurons using the same filter outputs a *feature
    map*, which highlights the areas in an image that activate the filter the most.
    But don’t worry, you won’t have to define the filters manually: instead, during
    training the convolutional layer will automatically learn the most useful filters
    for its task, and the layers above will learn to combine them into more complex
    patterns.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Stacking Multiple Feature Maps
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Up to now, for simplicity, I have represented the output of each convolutional
    layer as a 2D layer, but in reality a convolutional layer has multiple filters
    (you decide how many) and outputs one feature map per filter, so it is more accurately
    represented in 3D (see [Figure 14-6](#cnn_layers_volume_diagram)). It has one
    neuron per pixel in each feature map, and all neurons within a given feature map
    share the same parameters (i.e., the same kernel and bias term). Neurons in different
    feature maps use different parameters. A neuron’s receptive field is the same
    as described earlier, but it extends across all the feature maps of the previous
    layer. In short, a convolutional layer simultaneously applies multiple trainable
    filters to its inputs, making it capable of detecting multiple features anywhere
    in its inputs.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1406](assets/mls3_1406.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: Figure 14-6\. Two convolutional layers with multiple filters each (kernels),
    processing a color image with three color channels; each convolutional layer outputs
    one feature map per filter
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The fact that all neurons in a feature map share the same parameters dramatically
    reduces the number of parameters in the model. Once the CNN has learned to recognize
    a pattern in one location, it can recognize it in any other location. In contrast,
    once a fully connected neural network has learned to recognize a pattern in one
    location, it can only recognize it in that particular location.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 'Input images are also composed of multiple sublayers: one per *color channel*.
    As mentioned in [Chapter 9](ch09.html#unsupervised_learning_chapter), there are
    typically three: red, green, and blue (RGB). Grayscale images have just one channel,
    but some images may have many more—for example, satellite images that capture
    extra light frequencies (such as infrared).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, a neuron located in row *i*, column *j* of the feature map *k*
    in a given convolutional layer *l* is connected to the outputs of the neurons
    in the previous layer *l* – 1, located in rows *i* × *s*[*h*] to *i* × *s*[*h*]
    + *f*[*h*] – 1 and columns *j* × *s*[*w*] to *j* × *s*[*w*] + *f*[*w*] – 1, across
    all feature maps (in layer *l* – *1*). Note that, within a layer, all neurons
    located in the same row *i* and column *j* but in different feature maps are connected
    to the outputs of the exact same neurons in the previous layer.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[Equation 14-1](#convolutional_layer_equation) summarizes the preceding explanations
    in one big mathematical equation: it shows how to compute the output of a given
    neuron in a convolutional layer. It is a bit ugly due to all the different indices,
    but all it does is calculate the weighted sum of all the inputs, plus the bias
    term.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Equation 14-1\. Computing the output of a neuron in a convolutional layer
  id: totrans-38
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>z</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi></mrow></msub>
    <mo>=</mo> <msub><mi>b</mi> <mi>k</mi></msub> <mo>+</mo> <munderover><mo>∑</mo>
    <mrow><mi>u</mi><mo>=</mo><mn>0</mn></mrow> <mrow><msub><mi>f</mi> <mi>h</mi></msub>
    <mo>-</mo><mn>1</mn></mrow></munderover> <munderover><mo>∑</mo> <mrow><mi>v</mi><mo>=</mo><mn>0</mn></mrow>
    <mrow><msub><mi>f</mi> <mi>w</mi></msub> <mo>-</mo><mn>1</mn></mrow></munderover>
    <munderover><mo>∑</mo> <mrow><mi>k</mi><mo>'</mo><mo>=</mo><mn>0</mn></mrow> <mrow><msub><mi>f</mi>
    <msup><mi>n</mi> <mo>'</mo></msup></msub> <mo>-</mo><mn>1</mn></mrow></munderover>
    <msub><mi>x</mi> <mrow><msup><mi>i</mi> <mo>'</mo></msup> <mo>,</mo><msup><mi>j</mi>
    <mo>'</mo></msup> <mo>,</mo><msup><mi>k</mi> <mo>'</mo></msup></mrow></msub> <mo>×</mo>
    <msub><mi>w</mi> <mrow><mi>u</mi><mo>,</mo><mi>v</mi><mo>,</mo><msup><mi>k</mi>
    <mo>'</mo></msup> <mo>,</mo><mi>k</mi></mrow></msub> <mtext>with</mtext> <mfenced
    separators="" open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><mi>i</mi>
    <mo>'</mo> <mo>=</mo> <mi>i</mi> <mo>×</mo> <msub><mi>s</mi> <mi>h</mi></msub>
    <mo>+</mo> <mi>u</mi></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mi>j</mi>
    <mo>'</mo> <mo>=</mo> <mi>j</mi> <mo>×</mo> <msub><mi>s</mi> <mi>w</mi></msub>
    <mo>+</mo> <mi>v</mi></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: 'In this equation:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '*z*[*i*,] [*j*,] [*k*] is the output of the neuron located in row *i*, column
    *j* in feature map *k* of the convolutional layer (layer *l*).'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As explained earlier, *s*[*h*] and *s*[*w*] are the vertical and horizontal
    strides, *f*[*h*] and *f*[*w*] are the height and width of the receptive field,
    and *f*[*n*′] is the number of feature maps in the previous layer (layer *l* –
    1).
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x*[*i*′,] [*j*′,] [*k*′] is the output of the neuron located in layer *l*
    – 1, row *i*′, column *j*′, feature map *k*′ (or channel *k*′ if the previous
    layer is the input layer).'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*b*[*k*] is the bias term for feature map *k* (in layer *l*). You can think
    of it as a knob that tweaks the overall brightness of the feature map *k*.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*w*[*u*,] [*v*,] [*k*′,] [*k*] is the connection weight between any neuron
    in feature map *k* of the layer *l* and its input located at row *u*, column *v*
    (relative to the neuron’s receptive field), and feature map *k*′.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s see how to create and use a convolutional layer using Keras.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Convolutional Layers with Keras
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let’s load and preprocess a couple of sample images, using Scikit-Learn’s
    `load_sample_image()` function and Keras’s `CenterCrop` and `Rescaling` layers
    (all of which were introduced in [Chapter 13](ch13.html#data_chapter)):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s look at the shape of the `images` tensor:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Yikes, it’s a 4D tensor; we haven’t seen this before! What do all these dimensions
    mean? Well, there are two sample images, which explains the first dimension. Then
    each image is 70 × 120, since that’s the size we specified when creating the `CenterCrop`
    layer (the original images were 427 × 640). This explains the second and third
    dimensions. And lastly, each pixel holds one value per color channel, and there
    are three of them—red, green, and blue—which explains the last dimension.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s create a 2D convolutional layer and feed it these images to see what
    comes out. For this, Keras provides a `Convolution2D` layer, alias `Conv2D`. Under
    the hood, this layer relies on TensorFlow’s `tf.nn.conv2d()` operation. Let’s
    create a convolutional layer with 32 filters, each of size 7 × 7 (using `kernel_size=7`,
    which is equivalent to using `kernel_size=(7 , 7)`), and apply this layer to our
    small batch of two images:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'When we talk about a 2D convolutional layer, “2D” refers to the number of *spatial*
    dimensions (height and width), but as you can see, the layer takes 4D inputs:
    as we saw, the two additional dimensions are the batch size (first dimension)
    and the channels (last dimension).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s look at the output’s shape:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output shape is similar to the input shape, with two main differences.
    First, there are 32 channels instead of 3\. This is because we set `filters=32`,
    so we get 32 output feature maps: instead of the intensity of red, green, and
    blue at each location, we now have the intensity of each feature at each location.
    Second, the height and width have both shrunk by 6 pixels. This is due to the
    fact that the `Conv2D` layer does not use any zero-padding by default, which means
    that we lose a few pixels on the sides of the output feature maps, depending on
    the size of the filters. In this case, since the kernel size is 7, we lose 6 pixels
    horizontally and 6 pixels vertically (i.e., 3 pixels on each side).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The default option is surprisingly named `padding="valid"`, which actually
    means no zero-padding at all! This name comes from the fact that in this case
    every neuron’s receptive field lies strictly within *valid* positions inside the
    input (it does not go out of bounds). It’s not a Keras naming quirk: everyone
    uses this odd nomenclature.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 'If instead we set `padding="same"`, then the inputs are padded with enough
    zeros on all sides to ensure that the output feature maps end up with the *same*
    size as the inputs (hence the name of this option):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: These two padding options are illustrated in [Figure 14-7](#padding_options_stride_1_diagram).
    For simplicity, only the horizontal dimension is shown here, but of course the
    same logic applies to the vertical dimension as well.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'If the stride is greater than 1 (in any direction), then the output size will
    not be equal to the input size, even if `padding="same"`. For example, if you
    set `strides=2` (or equivalently `strides=(2, 2)`), then the output feature maps
    will be 35 × 60: halved both vertically and horizontally. [Figure 14-8](#padding_options_stride_2_diagram)
    shows what happens when `strides=2`, with both padding options.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1407](assets/mls3_1407.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: Figure 14-7\. The two padding options, when `strides=1`
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![mls3 1408](assets/mls3_1408.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
- en: Figure 14-8\. With strides greater than 1, the output is much smaller even when
    using `"same"` padding (and `"valid"` padding may ignore some inputs)
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-8。当步长大于1时，即使使用“same”填充（和“valid”填充可能会忽略一些输入），输出也会小得多
- en: 'If you are curious, this is how the output size is computed:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您感兴趣，这是输出大小是如何计算的：
- en: With `padding="valid"`, if the width of the input is *i*[*h*], then the output
    width is equal to (*i*[*h*] – *f*[*h*] + *s*[*h*]) / *s*[*h*], rounded down. Recall
    that *f*[*h*] is the kernel width, and *s*[*h*] is the horizontal stride. Any
    remainder in the division corresponds to ignored columns on the right side of
    the input image. The same logic can be used to compute the output height, and
    any ignored rows at the bottom of the image.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当`padding="valid"`时，如果输入的宽度为*i*[h]，那么输出宽度等于(*i*[h] - *f*[h] + *s*[h]) / *s*[h]，向下取整。请记住*f*[h]是卷积核的宽度，*s*[h]是水平步长。除法中的余数对应于输入图像右侧被忽略的列。同样的逻辑也可以用来计算输出高度，以及图像底部被忽略的行。
- en: With `padding="same"`, the output width is equal to *i*[*h*] / *s*[*h*], rounded
    up. To make this possible, the appropriate number of zero columns are padded to
    the left and right of the input image (an equal number if possible, or just one
    more on the right side). Assuming the output width is *o*[*w*], then the number
    of padded zero columns is (*o*[*w*] – 1) × *s*[*h*] + *f*[*h*] – *i*[*h*]. Again,
    the same logic can be used to compute the output height and the number of padded
    rows.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当`padding="same"`时，输出宽度等于*i*[h] / *s*[h]，向上取整。为了实现这一点，在输入图像的左右两侧填充适当数量的零列（如果可能的话，数量相等，或者在右侧多一个）。假设输出宽度为*o*[w]，那么填充的零列数为(*o*[w]
    - 1) × *s*[h] + *f*[h] - *i*[h]。同样的逻辑也可以用来计算输出高度和填充行数。
- en: 'Now let’s look at the layer’s weights (which were noted *w*[*u*,] [*v*,] [*k*′,]
    [*k*] and *b*[*k*] in [Equation 14-1](#convolutional_layer_equation)). Just like
    a `Dense` layer, a `Conv2D` layer holds all the layer’s weights, including the
    kernels and biases. The kernels are initialized randomly, while the biases are
    initialized to zero. These weights are accessible as TF variables via the `weights`
    attribute, or as NumPy arrays via the `get_weights()` method:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下层的权重（在[方程14-1](#convolutional_layer_equation)中被标记为*w*[u,] [v,] [k',]
    [k]和*b*[k]）。就像`Dense`层一样，`Conv2D`层保存所有层的权重，包括卷积核和偏置。卷积核是随机初始化的，而偏置初始化为零。这些权重可以通过`weights`属性作为TF变量访问，也可以通过`get_weights()`方法作为NumPy数组访问：
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `kernels` array is 4D, and its shape is [*kernel_height*, *kernel_width*,
    *input_channels*, *output_channels*]. The `biases` array is 1D, with shape [*output_channels*].
    The number of output channels is equal to the number of output feature maps, which
    is also equal to the number of filters.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`kernels`数组是4D的，其形状为[*kernel_height*, *kernel_width*, *input_channels*, *output_channels*]。`biases`数组是1D的，形状为[*output_channels*]。输出通道的数量等于输出特征图的数量，也等于滤波器的数量。'
- en: 'Most importantly, note that the height and width of the input images do not
    appear in the kernel’s shape: this is because all the neurons in the output feature
    maps share the same weights, as explained earlier. This means that you can feed
    images of any size to this layer, as long as they are at least as large as the
    kernels, and if they have the right number of channels (three in this case).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，需要注意输入图像的高度和宽度不会出现在卷积核的形状中：这是因为输出特征图中的所有神经元共享相同的权重，正如之前解释的那样。这意味着您可以将任何大小的图像馈送到这一层，只要它们至少与卷积核一样大，并且具有正确数量的通道（在这种情况下为三个）。
- en: 'Lastly, you will generally want to specify an activation function (such as
    ReLU) when creating a `Conv2D` layer, and also specify the corresponding kernel
    initializer (such as He initialization). This is for the same reason as for `Dense`
    layers: a convolutional layer performs a linear operation, so if you stacked multiple
    convolutional layers without any activation functions they would all be equivalent
    to a single convolutional layer, and they wouldn’t be able to learn anything really
    complex.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通常情况下，您会希望在创建`Conv2D`层时指定一个激活函数（如ReLU），并指定相应的内核初始化器（如He初始化）。这与`Dense`层的原因相同：卷积层执行线性操作，因此如果您堆叠多个卷积层而没有任何激活函数，它们都等同于单个卷积层，它们将无法学习到真正复杂的内容。
- en: 'As you can see, convolutional layers have quite a few hyperparameters: `filters`,
    `kernel_size`, `padding`, `strides`, `activation`, `kernel_initializer`, etc.
    As always, you can use cross-validation to find the right hyperparameter values,
    but this is very time-consuming. We will discuss common CNN architectures later
    in this chapter, to give you some idea of which hyperparameter values work best
    in practice.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，卷积层有很多超参数：`filters`、`kernel_size`、`padding`、`strides`、`activation`、`kernel_initializer`等。通常情况下，您可以使用交叉验证来找到正确的超参数值，但这是非常耗时的。我们将在本章后面讨论常见的CNN架构，以便让您了解在实践中哪些超参数值效果最好。
- en: Memory Requirements
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存需求
- en: Another challenge with CNNs is that the convolutional layers require a huge
    amount of RAM. This is especially true during training, because the reverse pass
    of backpropagation requires all the intermediate values computed during the forward
    pass.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的另一个挑战是卷积层需要大量的RAM。这在训练过程中尤为明显，因为反向传播的反向传递需要在前向传递期间计算的所有中间值。
- en: 'For example, consider a convolutional layer with 200 5 × 5 filters, with stride
    1 and `"same"` padding. If the input is a 150 × 100 RGB image (three channels),
    then the number of parameters is (5 × 5 × 3 + 1) × 200 = 15,200 (the + 1 corresponds
    to the bias terms), which is fairly small compared to a fully connected layer.⁠^([7](ch14.html#idm45720185247456))
    However, each of the 200 feature maps contains 150 × 100 neurons, and each of
    these neurons needs to compute a weighted sum of its 5 × 5 × 3 = 75 inputs: that’s
    a total of 225 million float multiplications. Not as bad as a fully connected
    layer, but still quite computationally intensive. Moreover, if the feature maps
    are represented using 32-bit floats, then the convolutional layer’s output will
    occupy 200 × 150 × 100 × 32 = 96 million bits (12 MB) of RAM.⁠^([8](ch14.html#idm45720185245968))
    And that’s just for one instance—if a training batch contains 100 instances, then
    this layer will use up 1.2 GB of RAM!'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: During inference (i.e., when making a prediction for a new instance) the RAM
    occupied by one layer can be released as soon as the next layer has been computed,
    so you only need as much RAM as required by two consecutive layers. But during
    training everything computed during the forward pass needs to be preserved for
    the reverse pass, so the amount of RAM needed is (at least) the total amount of
    RAM required by all layers.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If training crashes because of an out-of-memory error, you can try reducing
    the mini-batch size. Alternatively, you can try reducing dimensionality using
    a stride, removing a few layers, using 16-bit floats instead of 32-bit floats,
    or distributing the CNN across multiple devices (you will see how to do this in
    [Chapter 19](ch19.html#deployment_chapter)).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s look at the second common building block of CNNs: the *pooling layer*.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Pooling Layers
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you understand how convolutional layers work, the pooling layers are quite
    easy to grasp. Their goal is to *subsample* (i.e., shrink) the input image in
    order to reduce the computational load, the memory usage, and the number of parameters
    (thereby limiting the risk of overfitting).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Just like in convolutional layers, each neuron in a pooling layer is connected
    to the outputs of a limited number of neurons in the previous layer, located within
    a small rectangular receptive field. You must define its size, the stride, and
    the padding type, just like before. However, a pooling neuron has no weights;
    all it does is aggregate the inputs using an aggregation function such as the
    max or mean. [Figure 14-9](#max_pooling_diagram) shows a *max pooling layer*,
    which is the most common type of pooling layer. In this example, we use a 2 ×
    2 *pooling kernel*,⁠^([9](ch14.html#idm45720185209104)) with a stride of 2 and
    no padding. Only the max input value in each receptive field makes it to the next
    layer, while the other inputs are dropped. For example, in the lower-left receptive
    field in [Figure 14-9](#max_pooling_diagram), the input values are 1, 5, 3, 2,
    so only the max value, 5, is propagated to the next layer. Because of the stride
    of 2, the output image has half the height and half the width of the input image
    (rounded down since we use no padding).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1409](assets/mls3_1409.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
- en: Figure 14-9\. Max pooling layer (2 × 2 pooling kernel, stride 2, no padding)
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A pooling layer typically works on every input channel independently, so the
    output depth (i.e., the number of channels) is the same as the input depth.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'Other than reducing computations, memory usage, and the number of parameters,
    a max pooling layer also introduces some level of *invariance* to small translations,
    as shown in [Figure 14-10](#pooling_invariance_diagram). Here we assume that the
    bright pixels have a lower value than dark pixels, and we consider three images
    (A, B, C) going through a max pooling layer with a 2 × 2 kernel and stride 2\.
    Images B and C are the same as image A, but shifted by one and two pixels to the
    right. As you can see, the outputs of the max pooling layer for images A and B
    are identical. This is what translation invariance means. For image C, the output
    is different: it is shifted one pixel to the right (but there is still 50% invariance).
    By inserting a max pooling layer every few layers in a CNN, it is possible to
    get some level of translation invariance at a larger scale. Moreover, max pooling
    offers a small amount of rotational invariance and a slight scale invariance.
    Such invariance (even if it is limited) can be useful in cases where the prediction
    should not depend on these details, such as in classification tasks.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'However, max pooling has some downsides too. It’s obviously very destructive:
    even with a tiny 2 × 2 kernel and a stride of 2, the output will be two times
    smaller in both directions (so its area will be four times smaller), simply dropping
    75% of the input values. And in some applications, invariance is not desirable.
    Take semantic segmentation (the task of classifying each pixel in an image according
    to the object that pixel belongs to, which we’ll explore later in this chapter):
    obviously, if the input image is translated by one pixel to the right, the output
    should also be translated by one pixel to the right. The goal in this case is
    *equivariance*, not invariance: a small change to the inputs should lead to a
    corresponding small change in the output.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1410](assets/mls3_1410.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
- en: Figure 14-10\. Invariance to small translations
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Implementing Pooling Layers with Keras
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following code creates a `MaxPooling2D` layer, alias `MaxPool2D`, using
    a 2 × 2 kernel. The strides default to the kernel size, so this layer uses a stride
    of 2 (horizontally and vertically). By default, it uses `"valid"` padding (i.e.,
    no padding at all):'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: To create an *average pooling layer*, just use `AveragePooling2D`, alias `AvgPool2D`,
    instead of `MaxPool2D`. As you might expect, it works exactly like a max pooling
    layer, except it computes the mean rather than the max. Average pooling layers
    used to be very popular, but people mostly use max pooling layers now, as they
    generally perform better. This may seem surprising, since computing the mean generally
    loses less information than computing the max. But on the other hand, max pooling
    preserves only the strongest features, getting rid of all the meaningless ones,
    so the next layers get a cleaner signal to work with. Moreover, max pooling offers
    stronger translation invariance than average pooling, and it requires slightly
    less compute.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that max pooling and average pooling can be performed along the depth
    dimension instead of the spatial dimensions, although it’s not as common. This
    can allow the CNN to learn to be invariant to various features. For example, it
    could learn multiple filters, each detecting a different rotation of the same
    pattern (such as handwritten digits; see [Figure 14-11](#depth_wise_pooling_diagram)),
    and the depthwise max pooling layer would ensure that the output is the same regardless
    of the rotation. The CNN could similarly learn to be invariant to anything: thickness,
    brightness, skew, color, and so on.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1411](assets/mls3_1411.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
- en: Figure 14-11\. Depthwise max pooling can help the CNN learn to be invariant
    (to rotation in this case)
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Keras does not include a depthwise max pooling layer, but it’s not too difficult
    to implement a custom layer for that:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This layer reshapes its inputs to split the channels into groups of the desired
    size (`pool_size`), then it uses `tf.reduce_max()` to compute the max of each
    group. This implementation assumes that the stride is equal to the pool size,
    which is generally what you want. Alternatively, you could use TensorFlow’s `tf.nn.max_pool()`
    operation, and wrap in a `Lambda` layer to use it inside a Keras model, but sadly
    this op does not implement depthwise pooling for the GPU, only for the CPU.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'One last type of pooling layer that you will often see in modern architectures
    is the *global average pooling layer*. It works very differently: all it does
    is compute the mean of each entire feature map (it’s like an average pooling layer
    using a pooling kernel with the same spatial dimensions as the inputs). This means
    that it just outputs a single number per feature map and per instance. Although
    this is of course extremely destructive (most of the information in the feature
    map is lost), it can be useful just before the output layer, as you will see later
    in this chapter. To create such a layer, simply use the `GlobalAveragePooling2D`
    class, alias `GlobalAvgPool2D`:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'It’s equivalent to the following `Lambda` layer, which computes the mean over
    the spatial dimensions (height and width):'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'For example, if we apply this layer to the input images, we get the mean intensity
    of red, green, and blue for each image:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now you know all the building blocks to create convolutional neural networks.
    Let’s see how to assemble them.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: CNN Architectures
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Typical CNN architectures stack a few convolutional layers (each one generally
    followed by a ReLU layer), then a pooling layer, then another few convolutional
    layers (+ReLU), then another pooling layer, and so on. The image gets smaller
    and smaller as it progresses through the network, but it also typically gets deeper
    and deeper (i.e., with more feature maps), thanks to the convolutional layers
    (see [Figure 14-12](#cnn_architecture_diagram)). At the top of the stack, a regular
    feedforward neural network is added, composed of a few fully connected layers
    (+ReLUs), and the final layer outputs the prediction (e.g., a softmax layer that
    outputs estimated class probabilities).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1412](assets/mls3_1412.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: Figure 14-12\. Typical CNN architecture
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'A common mistake is to use convolution kernels that are too large. For example,
    instead of using a convolutional layer with a 5 × 5 kernel, stack two layers with
    3 × 3 kernels: it will use fewer parameters and require fewer computations, and
    it will usually perform better. One exception is for the first convolutional layer:
    it can typically have a large kernel (e.g., 5 × 5), usually with a stride of 2
    or more. This will reduce the spatial dimension of the image without losing too
    much information, and since the input image only has three channels in general,
    it will not be too costly.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how you can implement a basic CNN to tackle the Fashion MNIST dataset
    (introduced in [Chapter 10](ch10.html#ann_chapter)):'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let’s go through this code:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the `functools.partial()` function (introduced in [Chapter 11](ch11.html#deep_chapter))
    to define `DefaultConv2D`, which acts just like `Conv2D` but with different default
    arguments: a small kernel size of 3, `"same"` padding, the ReLU activation function,
    and its corresponding He initializer.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we create the `Sequential` model. Its first layer is a `DefaultConv2D`
    with 64 fairly large filters (7 × 7). It uses the default stride of 1 because
    the input images are not very large. It also sets `input_shape=[28, 28, 1]`, because
    the images are 28 × 28 pixels, with a single color channel (i.e., grayscale).
    When you load the Fashion MNIST dataset, make sure each image has this shape:
    you may need to use `np.reshape()` or `np.expanddims()` to add the channels dimension.
    Alternatively, you could use a `Reshape` layer as the first layer in the model.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We then add a max pooling layer that uses the default pool size of 2, so it
    divides each spatial dimension by a factor of 2.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then we repeat the same structure twice: two convolutional layers followed
    by a max pooling layer. For larger images, we could repeat this structure several
    more times. The number of repetitions is a hyperparameter you can tune.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note that the number of filters doubles as we climb up the CNN toward the output
    layer (it is initially 64, then 128, then 256): it makes sense for it to grow,
    since the number of low-level features is often fairly low (e.g., small circles,
    horizontal lines), but there are many different ways to combine them into higher-level
    features. It is a common practice to double the number of filters after each pooling
    layer: since a pooling layer divides each spatial dimension by a factor of 2,
    we can afford to double the number of feature maps in the next layer without fear
    of exploding the number of parameters, memory usage, or computational load.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next is the fully connected network, composed of two hidden dense layers and
    a dense output layer. Since it’s a classification task with 10 classes, the output
    layer has 10 units, and it uses the softmax activation function. Note that we
    must flatten the inputs just before the first dense layer, since it expects a
    1D array of features for each instance. We also add two dropout layers, with a
    dropout rate of 50% each, to reduce overfitting.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you compile this model using the `"sparse_categorical_crossentropy"` loss
    and you fit the model to the Fashion MNIST training set, it should reach over
    92% accuracy on the test set. It’s not state of the art, but it is pretty good,
    and clearly much better than what we achieved with dense networks in [Chapter 10](ch10.html#ann_chapter).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Over the years, variants of this fundamental architecture have been developed,
    leading to amazing advances in the field. A good measure of this progress is the
    error rate in competitions such as the ILSVRC [ImageNet challenge](https://image-net.org).
    In this competition, the top-five error rate for image classification—that is,
    the number of test images for which the system’s top five predictions did *not*
    include the correct answer—fell from over 26% to less than 2.3% in just six years.
    The images are fairly large (e.g., 256 pixels high) and there are 1,000 classes,
    some of which are really subtle (try distinguishing 120 dog breeds). Looking at
    the evolution of the winning entries is a good way to understand how CNNs work,
    and how research in deep learning progresses.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first look at the classical LeNet-5 architecture (1998), then several
    winners of the ILSVRC challenge: AlexNet (2012), GoogLeNet (2014), ResNet (2015),
    and SENet (2017). Along the way, we will also look at a few more architectures,
    including Xception, ResNeXt, DenseNet, MobileNet, CSPNet, and EfficientNet.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: LeNet-5
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [LeNet-5 architecture](https://homl.info/lenet5)⁠^([10](ch14.html#idm45720184581728))
    is perhaps the most widely known CNN architecture. As mentioned earlier, it was
    created by Yann LeCun in 1998 and has been widely used for handwritten digit recognition
    (MNIST). It is composed of the layers shown in [Table 14-1](#lenet_5_architecture).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Table 14-1\. LeNet-5 architecture
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '| Layer | Type | Maps | Size | Kernel size | Stride | Activation |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
- en: '| Out | Fully connected | – | 10 | – | – | RBF |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
- en: '| F6 | Fully connected | – | 84 | – | – | tanh |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
- en: '| C5 | Convolution | 120 | 1 × 1 | 5 × 5 | 1 | tanh |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
- en: '| S4 | Avg pooling | 16 | 5 × 5 | 2 × 2 | 2 | tanh |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
- en: '| C3 | Convolution | 16 | 10 × 10 | 5 × 5 | 1 | tanh |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
- en: '| S2 | Avg pooling | 6 | 14 × 14 | 2 × 2 | 2 | tanh |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
- en: '| C1 | Convolution | 6 | 28 × 28 | 5 × 5 | 1 | tanh |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
- en: '| In | Input | 1 | 32 × 32 | – | – | – |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
- en: 'As you can see, this looks pretty similar to our Fashion MNIST model: a stack
    of convolutional layers and pooling layers, followed by a dense network. Perhaps
    the main difference with more modern classification CNNs is the activation functions:
    today, we would use ReLU instead of tanh and softmax instead of RBF. There were
    several other minor differences that don’t really matter much, but in case you
    are interested, they are listed in this chapter’s notebook at [*https://homl.info/colab3*](https://homl.info/colab3).
    Yann LeCun’s [website](http://yann.lecun.com/exdb/lenet) also features great demos
    of LeNet-5 classifying digits.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: AlexNet
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The [AlexNet CNN architecture](https://homl.info/80)⁠^([11](ch14.html#idm45720184531840))
    won the 2012 ILSVRC challenge by a large margin: it achieved a top-five error
    rate of 17%, while the second best competitor achieved only 26%! AlexaNet was
    developed by Alex Krizhevsky (hence the name), Ilya Sutskever, and Geoffrey Hinton.
    It is similar to LeNet-5, only much larger and deeper, and it was the first to
    stack convolutional layers directly on top of one another, instead of stacking
    a pooling layer on top of each convolutional layer. [Table 14-2](#alexnet_architecture)
    presents this architecture.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Table 14-2\. AlexNet architecture
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '| Layer | Type | Maps | Size | Kernel size | Stride | Padding | Activation
    |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
- en: '| Out | Fully connected | – | 1,000 | – | – | – | Softmax |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
- en: '| F10 | Fully connected | – | 4,096 | – | – | – | ReLU |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
- en: '| F9 | Fully connected | – | 4,096 | – | – | – | ReLU |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: '| S8 | Max pooling | 256 | 6 × 6 | 3 × 3 | 2 | `valid` | – |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
- en: '| C7 | Convolution | 256 | 13 × 13 | 3 × 3 | 1 | `same` | ReLU |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
- en: '| C6 | Convolution | 384 | 13 × 13 | 3 × 3 | 1 | `same` | ReLU |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
- en: '| C5 | Convolution | 384 | 13 × 13 | 3 × 3 | 1 | `same` | ReLU |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
- en: '| S4 | Max pooling | 256 | 13 × 13 | 3 × 3 | 2 | `valid` | – |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
- en: '| C3 | Convolution | 256 | 27 × 27 | 5 × 5 | 1 | `same` | ReLU |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
- en: '| S2 | Max pooling | 96 | 27 × 27 | 3 × 3 | 2 | `valid` | – |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
- en: '| C1 | Convolution | 96 | 55 × 55 | 11 × 11 | 4 | `valid` | ReLU |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
- en: '| In | Input | 3 (RGB) | 227 × 227 | – | – | – | – |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
- en: To reduce overfitting, the authors used two regularization techniques. First,
    they applied dropout (introduced in [Chapter 11](ch11.html#deep_chapter)) with
    a 50% dropout rate during training to the outputs of layers F9 and F10\. Second,
    they performed data augmentation by randomly shifting the training images by various
    offsets, flipping them horizontally, and changing the lighting conditions.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'AlexNet also uses a competitive normalization step immediately after the ReLU
    step of layers C1 and C3, called *local response normalization* (LRN): the most
    strongly activated neurons inhibit other neurons located at the same position
    in neighboring feature maps. Such competitive activation has been observed in
    biological neurons. This encourages different feature maps to specialize, pushing
    them apart and forcing them to explore a wider range of features, ultimately improving
    generalization. [Equation 14-2](#lrn_equation) shows how to apply LRN.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Equation 14-2\. Local response normalization (LRN)
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>b</mi> <mi>i</mi></msub> <mo>=</mo> <msub><mi>a</mi>
    <mi>i</mi></msub> <msup><mfenced separators="" open="(" close=")"><mi>k</mi><mo>+</mo><mi>α</mi><munderover><mo>∑</mo>
    <mrow><mi>j</mi><mo>=</mo><msub><mi>j</mi> <mtext>low</mtext></msub></mrow> <msub><mi>j</mi>
    <mtext>high</mtext></msub></munderover> <msup><mrow><msub><mi>a</mi> <mi>j</mi></msub></mrow>
    <mn>2</mn></msup></mfenced> <mrow><mo>-</mo><mi>β</mi></mrow></msup> <mtext>with</mtext>
    <mfenced separators="" open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><msub><mi>j</mi>
    <mtext>high</mtext></msub> <mo>=</mo> <mo movablelimits="true" form="prefix">min</mo>
    <mfenced separators="" open="(" close=")"><mi>i</mi> <mo>+</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mi>r</mi> <mn>2</mn></mfrac></mstyle> <mo>,</mo> <msub><mi>f</mi>
    <mi>n</mi></msub> <mo>-</mo> <mn>1</mn></mfenced></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><msub><mi>j</mi> <mtext>low</mtext></msub> <mo>=</mo>
    <mo movablelimits="true" form="prefix">max</mo> <mfenced separators="" open="("
    close=")"><mn>0</mn> <mo>,</mo> <mi>i</mi> <mo>-</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mi>r</mi> <mn>2</mn></mfrac></mstyle></mfenced></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'In this equation:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '*b*[*i*] is the normalized output of the neuron located in feature map *i*,
    at some row *u* and column *v* (note that in this equation we consider only neurons
    located at this row and column, so *u* and *v* are not shown).'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*a*[*i*] is the activation of that neuron after the ReLU step, but before normalization.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*k*, *α*, *β*, and *r* are hyperparameters. *k* is called the *bias*, and *r*
    is called the *depth radius*.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*f*[*n*] is the number of feature maps.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, if *r* = 2 and a neuron has a strong activation, it will inhibit
    the activation of the neurons located in the feature maps immediately above and
    below its own.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'In AlexNet, the hyperparameters are set as: *r* = 5, *α* = 0.0001, *β* = 0.75,
    and *k* = 2. You can implement this step by using the `tf.nn.local_response_normalization()`
    function (which you can wrap in a `Lambda` layer if you want to use it in a Keras
    model).'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: A variant of AlexNet called [*ZF Net*](https://homl.info/zfnet)⁠^([12](ch14.html#idm45720184386400))
    was developed by Matthew Zeiler and Rob Fergus and won the 2013 ILSVRC challenge.
    It is essentially AlexNet with a few tweaked hyperparameters (number of feature
    maps, kernel size, stride, etc.).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: GoogLeNet
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The [GoogLeNet architecture](https://homl.info/81) was developed by Christian
    Szegedy et al. from Google Research,⁠^([13](ch14.html#idm45720184382672)) and
    it won the ILSVRC 2014 challenge by pushing the top-five error rate below 7%.
    This great performance came in large part from the fact that the network was much
    deeper than previous CNNs (as you’ll see in [Figure 14-15](#googlenet_diagram)).
    This was made possible by subnetworks called *inception modules*,⁠^([14](ch14.html#idm45720184379088))
    which allow GoogLeNet to use parameters much more efficiently than previous architectures:
    GoogLeNet actually has 10 times fewer parameters than AlexNet (roughly 6 million
    instead of 60 million).'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 14-14](#inception_module_diagram) shows the architecture of an inception
    module. The notation “3 × 3 + 1(S)” means that the layer uses a 3 × 3 kernel,
    stride 1, and `"same"` padding. The input signal is first fed to four different
    layers in parallel. All convolutional layers use the ReLU activation function.
    Note that the top convolutional layers use different kernel sizes (1 × 1, 3 ×
    3, and 5 × 5), allowing them to capture patterns at different scales. Also note
    that every single layer uses a stride of 1 and `"same"` padding (even the max
    pooling layer), so their outputs all have the same height and width as their inputs.
    This makes it possible to concatenate all the outputs along the depth dimension
    in the final *depth concatenation layer* (i.e., to stack the feature maps from
    all four top convolutional layers). It can be implemented using Keras’s `Concatenate`
    layer, using the default `axis=-1`.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1414](assets/mls3_1414.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
- en: Figure 14-14\. Inception module
  id: totrans-179
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You may wonder why inception modules have convolutional layers with 1 × 1 kernels.
    Surely these layers cannot capture any features because they look at only one
    pixel at a time, right? In fact, these layers serve three purposes:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Although they cannot capture spatial patterns, they can capture patterns along
    the depth dimension (i.e., across channels).
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are configured to output fewer feature maps than their inputs, so they
    serve as *bottleneck layers*, meaning they reduce dimensionality. This cuts the
    computational cost and the number of parameters, speeding up training and improving
    generalization.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each pair of convolutional layers ([1 × 1, 3 × 3] and [1 × 1, 5 × 5]) acts like
    a single powerful convolutional layer, capable of capturing more complex patterns.
    A convolutional layer is equivalent to sweeping a dense layer across the image
    (at each location, it only looks at a small receptive field), and these pairs
    of convolutional layers are equivalent to sweeping two-layer neural networks across
    the image.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In short, you can think of the whole inception module as a convolutional layer
    on steroids, able to output feature maps that capture complex patterns at various
    scales.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s look at the architecture of the GoogLeNet CNN (see [Figure 14-15](#googlenet_diagram)).
    The number of feature maps output by each convolutional layer and each pooling
    layer is shown before the kernel size. The architecture is so deep that it has
    to be represented in three columns, but GoogLeNet is actually one tall stack,
    including nine inception modules (the boxes with the spinning tops). The six numbers
    in the inception modules represent the number of feature maps output by each convolutional
    layer in the module (in the same order as in [Figure 14-14](#inception_module_diagram)).
    Note that all the convolutional layers use the ReLU activation function.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go through this network:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: The first two layers divide the image’s height and width by 4 (so its area is
    divided by 16), to reduce the computational load. The first layer uses a large
    kernel size, 7 × 7, so that much of the information is preserved.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then the local response normalization layer ensures that the previous layers
    learn a wide variety of features (as discussed earlier).
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two convolutional layers follow, where the first acts like a bottleneck layer.
    As mentioned, you can think of this pair as a single smarter convolutional layer.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Again, a local response normalization layer ensures that the previous layers
    capture a wide variety of patterns.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, a max pooling layer reduces the image height and width by 2, again to
    speed up computations.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then comes the CNN’s *backbone*: a tall stack of nine inception modules, interleaved
    with a couple of max pooling layers to reduce dimensionality and speed up the
    net.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, the global average pooling layer outputs the mean of each feature map:
    this drops any remaining spatial information, which is fine because there is not
    much spatial information left at that point. Indeed, GoogLeNet input images are
    typically expected to be 224 × 224 pixels, so after 5 max pooling layers, each
    dividing the height and width by 2, the feature maps are down to 7 × 7\. Moreover,
    this is a classification task, not localization, so it doesn’t matter where the
    object is. Thanks to the dimensionality reduction brought by this layer, there
    is no need to have several fully connected layers at the top of the CNN (like
    in AlexNet), and this considerably reduces the number of parameters in the network
    and limits the risk of overfitting.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The last layers are self-explanatory: dropout for regularization, then a fully
    connected layer with 1,000 units (since there are 1,000 classes) and a softmax
    activation function to output estimated class probabilities.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![mls3 1415](assets/mls3_1415.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
- en: Figure 14-15\. GoogLeNet architecture
  id: totrans-196
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The original GoogLeNet architecture included two auxiliary classifiers plugged
    on top of the third and sixth inception modules. They were both composed of one
    average pooling layer, one convolutional layer, two fully connected layers, and
    a softmax activation layer. During training, their loss (scaled down by 70%) was
    added to the overall loss. The goal was to fight the vanishing gradients problem
    and regularize the network, but it was later shown that their effect was relatively
    minor.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Several variants of the GoogLeNet architecture were later proposed by Google
    researchers, including Inception-v3 and Inception-v4, using slightly different
    inception modules to reach even better performance.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: VGGNet
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The runner-up in the ILSVRC 2014 challenge was [VGGNet](https://homl.info/83),⁠^([15](ch14.html#idm45720184347184))
    Karen Simonyan and Andrew Zisserman, from the Visual Geometry Group (VGG) research
    lab at Oxford University, developed a very simple and classical architecture;
    it had 2 or 3 convolutional layers and a pooling layer, then again 2 or 3 convolutional
    layers and a pooling layer, and so on (reaching a total of 16 or 19 convolutional
    layers, depending on the VGG variant), plus a final dense network with 2 hidden
    layers and the output layer. It used small 3 × 3 filters, but it had many of them.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: ResNet
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Kaiming He et al. won the ILSVRC 2015 challenge using a [Residual Network (ResNet)](https://homl.info/82)⁠^([16](ch14.html#idm45720184342496))
    that delivered an astounding top-five error rate under 3.6%. The winning variant
    used an extremely deep CNN composed of 152 layers (other variants had 34, 50,
    and 101 layers). It confirmed the general trend: computer vision models were getting
    deeper and deeper, with fewer and fewer parameters. The key to being able to train
    such a deep network is to use *skip connections* (also called *shortcut connections*):
    the signal feeding into a layer is also added to the output of a layer located
    higher up the stack. Let’s see why this is useful.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: When training a neural network, the goal is to make it model a target function
    *h*(**x**). If you add the input **x** to the output of the network (i.e., you
    add a skip connection), then the network will be forced to model *f*(**x**) =
    *h*(**x**) – **x** rather than *h*(**x**). This is called *residual learning*
    (see [Figure 14-16](#residual_learning_diagram)).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1416](assets/mls3_1416.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: Figure 14-16\. Residual learning
  id: totrans-205
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When you initialize a regular neural network, its weights are close to zero,
    so the network just outputs values close to zero. If you add a skip connection,
    the resulting network just outputs a copy of its inputs; in other words, it initially
    models the identity function. If the target function is fairly close to the identity
    function (which is often the case), this will speed up training considerably.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, if you add many skip connections, the network can start making progress
    even if several layers have not started learning yet (see [Figure 14-17](#deep_residual_network_diagram)).
    Thanks to skip connections, the signal can easily make its way across the whole
    network. The deep residual network can be seen as a stack of *residual units*
    (RUs), where each residual unit is a small neural network with a skip connection.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s look at ResNet’s architecture (see [Figure 14-18](#resnet_diagram)).
    It is surprisingly simple. It starts and ends exactly like GoogLeNet (except without
    a dropout layer), and in between is just a very deep stack of residual units.
    Each residual unit is composed of two convolutional layers (and no pooling layer!),
    with batch normalization (BN) and ReLU activation, using 3 × 3 kernels and preserving
    spatial dimensions (stride 1, `"same"` padding).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1417](assets/mls3_1417.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
- en: Figure 14-17\. Regular deep neural network (left) and deep residual network
    (right)
  id: totrans-210
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![mls3 1418](assets/mls3_1418.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
- en: Figure 14-18\. ResNet architecture
  id: totrans-212
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that the number of feature maps is doubled every few residual units, at
    the same time as their height and width are halved (using a convolutional layer
    with stride 2). When this happens, the inputs cannot be added directly to the
    outputs of the residual unit because they don’t have the same shape (for example,
    this problem affects the skip connection represented by the dashed arrow in [Figure 14-18](#resnet_diagram)).
    To solve this problem, the inputs are passed through a 1 × 1 convolutional layer
    with stride 2 and the right number of output feature maps (see [Figure 14-19](#resize_skip_connection_diagram)).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1419](assets/mls3_1419.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
- en: Figure 14-19\. Skip connection when changing feature map size and depth
  id: totrans-215
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Different variations of the architecture exist, with different numbers of layers.
    ResNet-34 is a ResNet with 34 layers (only counting the convolutional layers and
    the fully connected layer)⁠^([17](ch14.html#idm45720184314128)) containing 3 RUs
    that output 64 feature maps, 4 RUs with 128 maps, 6 RUs with 256 maps, and 3 RUs
    with 512 maps. We will implement this architecture later in this chapter.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-217
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Google’s [Inception-v4](https://homl.info/84)⁠^([18](ch14.html#idm45720184310768))
    architecture merged the ideas of GoogLeNet and ResNet and achieved a top-five
    error rate of close to 3% on ImageNet classification.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'ResNets deeper than that, such as ResNet-152, use slightly different residual
    units. Instead of two 3 × 3 convolutional layers with, say, 256 feature maps,
    they use three convolutional layers: first a 1 × 1 convolutional layer with just
    64 feature maps (4 × less), which acts as a bottleneck layer (as discussed already),
    then a 3 × 3 layer with 64 feature maps, and finally another 1 × 1 convolutional
    layer with 256 feature maps (4 times 64) that restores the original depth. ResNet-152
    contains 3 such RUs that output 256 maps, then 8 RUs with 512 maps, a whopping
    36 RUs with 1,024 maps, and finally 3 RUs with 2,048 maps.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Xception
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another variant of the GoogLeNet architecture is worth noting: [Xception](https://homl.info/xception)⁠^([19](ch14.html#idm45720184304880))
    (which stands for *Extreme Inception*) was proposed in 2016 by François Chollet
    (the author of Keras), and it significantly outperformed Inception-v3 on a huge
    vision task (350 million images and 17,000 classes). Just like Inception-v4, it
    merges the ideas of GoogLeNet and ResNet, but it replaces the inception modules
    with a special type of layer called a *depthwise separable convolution layer*
    (or *separable convolution layer* for short⁠^([20](ch14.html#idm45720184300096))).
    These layers had been used before in some CNN architectures, but they were not
    as central as in the Xception architecture. While a regular convolutional layer
    uses filters that try to simultaneously capture spatial patterns (e.g., an oval)
    and cross-channel patterns (e.g., mouth + nose + eyes = face), a separable convolutional
    layer makes the strong assumption that spatial patterns and cross-channel patterns
    can be modeled separately (see [Figure 14-20](#separable_convolution_diagram)).
    Thus, it is composed of two parts: the first part applies a single spatial filter
    to each input feature map, then the second part looks exclusively for cross-channel
    patterns—it is just a regular convolutional layer with 1 × 1 filters.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Since separable convolutional layers only have one spatial filter per input
    channel, you should avoid using them after layers that have too few channels,
    such as the input layer (granted, that’s what [Figure 14-20](#separable_convolution_diagram)
    represents, but it is just for illustration purposes). For this reason, the Xception
    architecture starts with 2 regular convolutional layers, but then the rest of
    the architecture uses only separable convolutions (34 in all), plus a few max
    pooling layers and the usual final layers (a global average pooling layer and
    a dense output layer).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'You might wonder why Xception is considered a variant of GoogLeNet, since it
    contains no inception modules at all. Well, as discussed earlier, an inception
    module contains convolutional layers with 1 × 1 filters: these look exclusively
    for cross-channel patterns. However, the convolutional layers that sit on top
    of them are regular convolutional layers that look both for spatial and cross-channel
    patterns. So, you can think of an inception module as an intermediate between
    a regular convolutional layer (which considers spatial patterns and cross-channel
    patterns jointly) and a separable convolutional layer (which considers them separately).
    In practice, it seems that separable convolutional layers often perform better.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1420](assets/mls3_1420.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: Figure 14-20\. Depthwise separable convolutional layer
  id: totrans-225
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  id: totrans-226
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Separable convolutional layers use fewer parameters, less memory, and fewer
    computations than regular convolutional layers, and they often perform better.
    Consider using them by default, except after layers with few channels (such as
    the input channel). In Keras, just use `SeparableConv2D` instead of `Conv2D`:
    it’s a drop-in replacement. Keras also offers a `DepthwiseConv2D` layer that implements
    the first part of a depthwise separable convolutional layer (i.e., applying one
    spatial filter per input feature map).'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: SENet
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The winning architecture in the ILSVRC 2017 challenge was the [Squeeze-and-Excitation
    Network (SENet)](https://homl.info/senet).⁠^([21](ch14.html#idm45720184285456))
    This architecture extends existing architectures such as inception networks and
    ResNets, and boosts their performance. This allowed SENet to win the competition
    with an astonishing 2.25% top-five error rate! The extended versions of inception
    networks and ResNets are called *SE-Inception* and *SE-ResNet*, respectively.
    The boost comes from the fact that a SENet adds a small neural network, called
    an *SE block*, to every inception module or residual unit in the original architecture,
    as shown in [Figure 14-21](#senet_diagram).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1421](assets/mls3_1421.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
- en: Figure 14-21\. SE-Inception module (left) and SE-ResNet unit (right)
  id: totrans-231
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'An SE block analyzes the output of the unit it is attached to, focusing exclusively
    on the depth dimension (it does not look for any spatial pattern), and it learns
    which features are usually most active together. It then uses this information
    to recalibrate the feature maps, as shown in [Figure 14-22](#recalibration_diagram).
    For example, an SE block may learn that mouths, noses, and eyes usually appear
    together in pictures: if you see a mouth and a nose, you should expect to see
    eyes as well. So, if the block sees a strong activation in the mouth and nose
    feature maps, but only mild activation in the eye feature map, it will boost the
    eye feature map (more accurately, it will reduce irrelevant feature maps). If
    the eyes were somewhat confused with something else, this feature map recalibration
    will help resolve the ambiguity.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1422](assets/mls3_1422.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
- en: Figure 14-22\. An SE block performs feature map recalibration
  id: totrans-234
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'An SE block is composed of just three layers: a global average pooling layer,
    a hidden dense layer using the ReLU activation function, and a dense output layer
    using the sigmoid activation function (see [Figure 14-23](#seblock_diagram)).'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1423](assets/mls3_1423.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
- en: Figure 14-23\. SE block architecture
  id: totrans-237
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As earlier, the global average pooling layer computes the mean activation for
    each feature map: for example, if its input contains 256 feature maps, it will
    output 256 numbers representing the overall level of response for each filter.
    The next layer is where the “squeeze” happens: this layer has significantly fewer
    than 256 neurons—typically 16 times fewer than the number of feature maps (e.g.,
    16 neurons)—so the 256 numbers get compressed into a small vector (e.g., 16 dimensions).
    This is a low-dimensional vector representation (i.e., an embedding) of the distribution
    of feature responses. This bottleneck step forces the SE block to learn a general
    representation of the feature combinations (we will see this principle in action
    again when we discuss autoencoders in [Chapter 17](ch17.html#autoencoders_chapter)).
    Finally, the output layer takes the embedding and outputs a recalibration vector
    containing one number per feature map (e.g., 256), each between 0 and 1\. The
    feature maps are then multiplied by this recalibration vector, so irrelevant features
    (with a low recalibration score) get scaled down while relevant features (with
    a recalibration score close to 1) are left alone.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Other Noteworthy Architectures
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are many other CNN architectures to explore. Here’s a brief overview
    of some of the most noteworthy:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[ResNeXt](https://homl.info/resnext)⁠^([22](ch14.html#idm45720184264752))'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: ResNeXt improves the residual units in ResNet. Whereas the residual units in
    the best ResNet models just contain 3 convolutional layers each, the ResNeXt residual
    units are composed of many parallel stacks (e.g., 32 stacks), with 3 convolutional
    layers each. However, the first two layers in each stack only use a few filters
    (e.g., just four), so the overall number of parameters remains the same as in
    ResNet. Then the outputs of all the stacks are added together, and the result
    is passed to the next residual unit (along with the skip connection).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[DenseNet](https://homl.info/densenet)⁠^([23](ch14.html#idm45720184261328))'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: A DenseNet is composed of several dense blocks, each made up of a few densely
    connected convolutional layers. This architecture achieved excellent accuracy
    while using comparatively few parameters. What does “densely connected” mean?
    The output of each layer is fed as input to every layer after it within the same
    block. For example, layer 4 in a block takes as input the depthwise concatenation
    of the outputs of layers 1, 2, and 3 in that block. Dense blocks are separated
    by a few transition layers.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[MobileNet](https://homl.info/mobilenet)⁠^([24](ch14.html#idm45720184258784))'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: MobileNets are streamlined models designed to be lightweight and fast, making
    them popular in mobile and web applications. They are based on depthwise separable
    convolutional layers, like Xception. The authors proposed several variants, trading
    a bit of accuracy for faster and smaller models.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[CSPNet](https://homl.info/cspnet)⁠^([25](ch14.html#idm45720184256368))'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: A Cross Stage Partial Network (CSPNet) is similar to a DenseNet, but part of
    each dense block’s input is concatenated directly to that block’s output, without
    going through the block.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[EfficientNet](https://homl.info/efficientnet)⁠^([26](ch14.html#idm45720184253792))'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: EfficientNet is arguably the most important model in this list. The authors
    proposed a method to scale any CNN efficiently, by jointly increasing the depth
    (number of layers), width (number of filters per layer), and resolution (size
    of the input image) in a principled way. This is called *compound scaling*. They
    used neural architecture search to find a good architecture for a scaled-down
    version of ImageNet (with smaller and fewer images), and then used compound scaling
    to create larger and larger versions of this architecture. When EfficientNet models
    came out, they vastly outperformed all existing models, across all compute budgets,
    and they remain among the best models out there today.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'Understanding EfficientNet’s compound scaling method is helpful to gain a deeper
    understanding of CNNs, especially if you ever need to scale a CNN architecture.
    It is based on a logarithmic measure of the compute budget, noted *ϕ*: if your
    compute budget doubles, then *ϕ* increases by 1\. In other words, the number of
    floating-point operations available for training is proportional to 2^(*ϕ*). Your
    CNN architecture’s depth, width, and resolution should scale as *α*^(*ϕ*), *β*^(*ϕ*),
    and *γ*^(*ϕ*), respectively. The factors *α*, *β*, and *γ* must be greater than
    1, and *α* + *β*² + *γ*² should be close to 2\. The optimal values for these factors
    depend on the CNN’s architecture. To find the optimal values for the EfficientNet
    architecture, the authors started with a small baseline model (EfficientNetB0),
    fixed *ϕ* = 1, and simply ran a grid search: they found α = 1.2, β = 1.1, and
    γ = 1.1\. They then used these factors to create several larger architectures,
    named EfficientNetB1 to EfficientNetB7, for increasing values of *ϕ*.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the Right CNN Architecture
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With so many CNN architectures, how do you choose which one is best for your
    project? Well, it depends on what matters most to you: Accuracy? Model size (e.g.,
    for deployment to a mobile device)? Inference speed on CPU? On GPU? [Table 14-3](#model_summary_table)
    lists the best pretrained models currently available in Keras (you’ll see how
    to use them later in this chapter), sorted by model size. You can find the full
    list at [*https://keras.io/api/applications*](https://keras.io/api/applications).
    For each model, the table shows the Keras class name to use (in the `tf.keras.applications`
    package), the model’s size in MB, the top-1 and top-5 validation accuracy on the
    ImageNet dataset, the number of parameters (millions), and the inference time
    on CPU and GPU in ms, using batches of 32 images on reasonably powerful hardware.⁠^([27](ch14.html#idm45720184237344))
    For each column, the best value is highlighted. As you can see, larger models
    are generally more accurate, but not always; for example, EfficientNetB2 outperforms
    InceptionV3 both in size and accuracy. I only kept InceptionV3 in the list because
    it is almost twice as fast as EfficientNetB2 on a CPU. Similarly, InceptionResNetV2
    is fast on a CPU, and ResNet50V2 and ResNet101V2 are blazingly fast on a GPU.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Table 14-3\. Pretrained models available in Keras
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '| Class name | Size (MB) | Top-1 acc | Top-5 acc | Params | CPU (ms) | GPU
    (ms) |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
- en: '| MobileNetV2 | **14** | 71.3% | 90.1% | **3.5M** | 25.9 | 3.8 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
- en: '| MobileNet | 16 | 70.4% | 89.5% | 4.3M | **22.6** | **3.4** |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
- en: '| NASNetMobile | 23 | 74.4% | 91.9% | 5.3M | 27.0 | 6.7 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
- en: '| EfficientNetB0 | 29 | 77.1% | 93.3% | 5.3M | 46.0 | 4.9 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
- en: '| EfficientNetB1 | 31 | 79.1% | 94.4% | 7.9M | 60.2 | 5.6 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
- en: '| EfficientNetB2 | 36 | 80.1% | 94.9% | 9.2M | 80.8 | 6.5 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
- en: '| EfficientNetB3 | 48 | 81.6% | 95.7% | 12.3M | 140.0 | 8.8 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
- en: '| EfficientNetB4 | 75 | 82.9% | 96.4% | 19.5M | 308.3 | 15.1 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
- en: '| InceptionV3 | 92 | 77.9% | 93.7% | 23.9M | 42.2 | 6.9 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
- en: '| ResNet50V2 | 98 | 76.0% | 93.0% | 25.6M | 45.6 | 4.4 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
- en: '| EfficientNetB5 | 118 | 83.6% | 96.7% | 30.6M | 579.2 | 25.3 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
- en: '| EfficientNetB6 | 166 | 84.0% | 96.8% | 43.3M | 958.1 | 40.4 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
- en: '| ResNet101V2 | 171 | 77.2% | 93.8% | 44.7M | 72.7 | 5.4 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
- en: '| InceptionResNetV2 | 215 | 80.3% | 95.3% | 55.9M | 130.2 | 10.0 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
- en: '| EfficientNetB7 | 256 | **84.3%** | **97.0%** | 66.7M | 1578.9 | 61.6 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
- en: I hope you enjoyed this deep dive into the main CNN architectures! Now let’s
    see how to implement one of them using Keras.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a ResNet-34 CNN Using Keras
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Most CNN architectures described so far can be implemented pretty naturally
    using Keras (although generally you would load a pretrained network instead, as
    you will see). To illustrate the process, let’s implement a ResNet-34 from scratch
    with Keras. First, we’ll create a `ResidualUnit` layer:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As you can see, this code matches [Figure 14-19](#resize_skip_connection_diagram)
    pretty closely. In the constructor, we create all the layers we will need: the
    main layers are the ones on the right side of the diagram, and the skip layers
    are the ones on the left (only needed if the stride is greater than 1). Then in
    the `call()` method, we make the inputs go through the main layers and the skip
    layers (if any), and we add both outputs and apply the activation function.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can build a ResNet-34 using a `Sequential` model, since it’s really
    just a long sequence of layers—we can treat each residual unit as a single layer
    now that we have the `ResidualUnit` class. The code closely matches [Figure 14-18](#resnet_diagram):'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The only tricky part in this code is the loop that adds the `ResidualUnit`
    layers to the model: as explained earlier, the first 3 RUs have 64 filters, then
    the next 4 RUs have 128 filters, and so on. At each iteration, we must set the
    stride to 1 when the number of filters is the same as in the previous RU, or else
    we set it to 2; then we add the `ResidualUnit`, and finally we update `prev_filters`.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: It is amazing that in about 40 lines of code, we can build the model that won
    the ILSVRC 2015 challenge! This demonstrates both the elegance of the ResNet model
    and the expressiveness of the Keras API. Implementing the other CNN architectures
    is a bit longer, but not much harder. However, Keras comes with several of these
    architectures built in, so why not use them instead?
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Using Pretrained Models from Keras
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In general, you won’t have to implement standard models like GoogLeNet or ResNet
    manually, since pretrained networks are readily available with a single line of
    code in the `tf.keras.applications` package.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, you can load the ResNet-50 model, pretrained on ImageNet, with
    the following line of code:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'That’s all! This will create a ResNet-50 model and download weights pretrained
    on the ImageNet dataset. To use it, you first need to ensure that the images have
    the right size. A ResNet-50 model expects 224 × 224–pixel images (other models
    may expect other sizes, such as 299 × 299), so let’s use Keras’s `Resizing` layer
    (introduced in [Chapter 13](ch13.html#data_chapter)) to resize two sample images
    (after cropping them to the target aspect ratio):'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The pretrained models assume that the images are preprocessed in a specific
    way. In some cases they may expect the inputs to be scaled from 0 to 1, or from
    –1 to 1, and so on. Each model provides a `preprocess_input()` function that you
    can use to preprocess your images. These functions assume that the original pixel
    values range from 0 to 255, which is the case here:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now we can use the pretrained model to make predictions:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'As usual, the output `Y_proba` is a matrix with one row per image and one column
    per class (in this case, there are 1,000 classes). If you want to display the
    top *K* predictions, including the class name and the estimated probability of
    each predicted class, use the `decode_predictions()` function. For each image,
    it returns an array containing the top *K* predictions, where each prediction
    is represented as an array containing the class identifier,⁠^([28](ch14.html#idm45720183471584))
    its name, and the corresponding confidence score:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output looks like this:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The correct classes are palace and dahlia, so the model is correct for the first
    image but wrong for the second. However, that’s because dahlia is not one of the
    1,000 ImageNet classes. With that in mind, vase is a reasonable guess (perhaps
    the flower is in a vase?), and daisy is not a bad choice either, since dahlias
    and daisies are both from the same Compositae family.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, it is very easy to create a pretty good image classifier using
    a pretrained model. As you saw in [Table 14-3](#model_summary_table), many other
    vision models are available in `tf.keras.applications`, from lightweight and fast
    models to large and accurate ones.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: But what if you want to use an image classifier for classes of images that are
    not part of ImageNet? In that case, you may still benefit from the pretrained
    models by using them to perform transfer learning.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Pretrained Models for Transfer Learning
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you want to build an image classifier but you do not have enough data to
    train it from scratch, then it is often a good idea to reuse the lower layers
    of a pretrained model, as we discussed in [Chapter 11](ch11.html#deep_chapter).
    For example, let’s train a model to classify pictures of flowers, reusing a pretrained
    Xception model. First, we’ll load the flowers dataset using TensorFlow Datasets
    (introduced in [Chapter 13](ch13.html#data_chapter)):'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Note that you can get information about the dataset by setting `with_info=True`.
    Here, we get the dataset size and the names of the classes. Unfortunately, there
    is only a `"train"` dataset, no test set or validation set, so we need to split
    the training set. Let’s call `tfds.load()` again, but this time taking the first
    10% of the dataset for testing, the next 15% for validation, and the remaining
    75% for training:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'All three datasets contain individual images. We need to batch them, but first
    we need to ensure they all have the same size, or batching will fail. We can use
    a `Resizing` layer for this. We must also call the `tf.keras.applications.​xcep⁠tion.preprocess_input()`
    function to preprocess the images appropriately for the Xception model. Lastly,
    we’ll also shuffle the training set and use prefetching:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Now each batch contains 32 images, all of them 224 × 224 pixels, with pixel
    values ranging from –1 to 1\. Perfect!
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the dataset is not very large, a bit of data augmentation will certainly
    help. Let’s create a data augmentation model that we will embed in our final model.
    During training, it will randomly flip the images horizontally, rotate them a
    little bit, and tweak the contrast:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Tip
  id: totrans-308
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The `tf.keras.preprocessing.image.ImageDataGenerator` class makes it easy to
    load images from disk and augment them in various ways: you can shift each image,
    rotate it, rescale it, flip it horizontally or vertically, shear it, or apply
    any transformation function you want to it. This is very convenient for simple
    projects. However, a tf.data pipeline is not much more complicated, and it’s generally
    faster. Moreover, if you have a GPU and you include the preprocessing or data
    augmentation layers inside your model, they will benefit from GPU acceleration
    during training.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: 'Next let’s load an Xception model, pretrained on ImageNet. We exclude the top
    of the network by setting `include_top=False`. This excludes the global average
    pooling layer and the dense output layer. We then add our own global average pooling
    layer (feeding it the output of the base model), followed by a dense output layer
    with one unit per class, using the softmax activation function. Finally, we wrap
    all this in a Keras `Model`:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'As explained in [Chapter 11](ch11.html#deep_chapter), it’s usually a good idea
    to freeze the weights of the pretrained layers, at least at the beginning of training:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Warning
  id: totrans-314
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Since our model uses the base model’s layers directly, rather than the `base_model`
    object itself, setting `base_model.trainable=False` would have no effect.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can compile the model and start training:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Warning
  id: totrans-318
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If you are running in Colab, make sure the runtime is using a GPU: select Runtime
    → “Change runtime type”, choose “GPU” in the “Hardware accelerator” drop-down
    menu, then click Save. It’s possible to train the model without a GPU, but it
    will be terribly slow (minutes per epoch, as opposed to seconds).'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: 'After training the model for a few epochs, its validation accuracy should reach
    a bit over 80% and then stop improving. This means that the top layers are now
    pretty well trained, and we are ready to unfreeze some of the base model’s top
    layers, then continue training. For example, let’s unfreeze layers 56 and above
    (that’s the start of residual unit 7 out of 14, as you can see if you list the
    layer names):'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Don’t forget to compile the model whenever you freeze or unfreeze layers. Also
    make sure to use a much lower learning rate to avoid damaging the pretrained weights:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This model should reach around 92% accuracy on the test set, in just a few minutes
    of training (with a GPU). If you tune the hyperparameters, lower the learning
    rate, and train for quite a bit longer, you should be able to reach 95% to 97%.
    With that, you can start training amazing image classifiers on your own images
    and classes! But there’s more to computer vision than just classification. For
    example, what if you also want to know *where* the flower is in a picture? Let’s
    look at this now.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Classification and Localization
  id: totrans-325
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Localizing an object in a picture can be expressed as a regression task, as
    discussed in [Chapter 10](ch10.html#ann_chapter): to predict a bounding box around
    the object, a common approach is to predict the horizontal and vertical coordinates
    of the object’s center, as well as its height and width. This means we have four
    numbers to predict. It does not require much change to the model; we just need
    to add a second dense output layer with four units (typically on top of the global
    average pooling layer), and it can be trained using the MSE loss:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'But now we have a problem: the flowers dataset does not have bounding boxes
    around the flowers. So, we need to add them ourselves. This is often one of the
    hardest and most costly parts of a machine learning project: getting the labels.
    It’s a good idea to spend time looking for the right tools. To annotate images
    with bounding boxes, you may want to use an open source image labeling tool like
    VGG Image Annotator, LabelImg, OpenLabeler, or ImgLab, or perhaps a commercial
    tool like LabelBox or Supervisely. You may also want to consider crowdsourcing
    platforms such as Amazon Mechanical Turk if you have a very large number of images
    to annotate. However, it is quite a lot of work to set up a crowdsourcing platform,
    prepare the form to be sent to the workers, supervise them, and ensure that the
    quality of the bounding boxes they produce is good, so make sure it is worth the
    effort. Adriana Kovashka et al. wrote a very practical [paper](https://homl.info/crowd)⁠^([29](ch14.html#idm45720182431584))
    about crowdsourcing in computer vision. I recommend you check it out, even if
    you do not plan to use crowdsourcing. If there are just a few hundred or a even
    a couple thousand images to label, and you don’t plan to do this frequently, it
    may be preferable to do it yourself: with the right tools, it will only take a
    few days, and you’ll also gain a better understanding of your dataset and task.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s suppose you’ve obtained the bounding boxes for every image in the
    flowers dataset (for now we will assume there is a single bounding box per image).
    You then need to create a dataset whose items will be batches of preprocessed
    images along with their class labels and their bounding boxes. Each item should
    be a tuple of the form `(images, (class_labels, bounding_boxes))`. Then you are
    ready to train your model!
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-330
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The bounding boxes should be normalized so that the horizontal and vertical
    coordinates, as well as the height and width, all range from 0 to 1\. Also, it
    is common to predict the square root of the height and width rather than the height
    and width directly: this way, a 10-pixel error for a large bounding box will not
    be penalized as much as a 10-pixel error for a small bounding box.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 'The MSE often works fairly well as a cost function to train the model, but
    it is not a great metric to evaluate how well the model can predict bounding boxes.
    The most common metric for this is the *intersection over union* (IoU): the area
    of overlap between the predicted bounding box and the target bounding box, divided
    by the area of their union (see [Figure 14-24](#iou_diagram)). In Keras, it is
    implemented by the `tf.keras.metrics.MeanIoU` class.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Classifying and localizing a single object is nice, but what if the images contain
    multiple objects (as is often the case in the flowers dataset)?
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1424](assets/mls3_1424.png)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
- en: Figure 14-24\. IoU metric for bounding boxes
  id: totrans-335
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Object Detection
  id: totrans-336
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The task of classifying and localizing multiple objects in an image is called
    *object detection*. Until a few years ago, a common approach was to take a CNN
    that was trained to classify and locate a single object roughly centered in the
    image, then slide this CNN across the image and make predictions at each step.
    The CNN was generally trained to predict not only class probabilities and a bounding
    box, but also an *objectness score*: this is the estimated probability that the
    image does indeed contain an object centered near the middle. This is a binary
    classification output; it can be produced by a dense output layer with a single
    unit, using the sigmoid activation function and trained using the binary cross-entropy
    loss.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-338
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Instead of an objectness score, a “no-object” class was sometimes added, but
    in general this did not work as well: the questions “Is an object present?” and
    “What type of object is it?” are best answered separately.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: This sliding-CNN approach is illustrated in [Figure 14-25](#sliding_cnn_diagram).
    In this example, the image was chopped into a 5 × 7 grid, and we see a CNN—the
    thick black rectangle—sliding across all 3 × 3 regions and making predictions
    at each step.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1425](assets/mls3_1425.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
- en: Figure 14-25\. Detecting multiple objects by sliding a CNN across the image
  id: totrans-342
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In this figure, the CNN has already made predictions for three of these 3 ×
    3 regions:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 'When looking at the top-left 3 × 3 region (centered on the red-shaded grid
    cell located in the second row and second column), it detected the leftmost rose.
    Notice that the predicted bounding box exceeds the boundary of this 3 × 3 region.
    That’s absolutely fine: even though the CNN could not see the bottom part of the
    rose, it was able to make a reasonable guess as to where it might be. It also
    predicted class probabilities, giving a high probability to the “rose” class.
    Lastly, it predicted a fairly high objectness score, since the center of the bounding
    box lies within the central grid cell (in this figure, the objectness score is
    represented by the thickness of the bounding box).'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When looking at the next 3 × 3 region, one grid cell to the right (centered
    on the shaded blue square), it did not detect any flower centered in that region,
    so it predicted a very low objectness score; therefore, the predicted bounding
    box and class probabilities can safely be ignored. You can see that the predicted
    bounding box was no good anyway.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'finally, when looking at the next 3 × 3 region, again one grid cell to the
    right (centered on the shaded green cell), it detected the rose at the top, although
    not perfectly: this rose is not well centered within this region, so the predicted
    objectness score was not very high.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can imagine how sliding the CNN across the whole image would give you a
    total of 15 predicted bounding boxes, organized in a 3 × 5 grid, with each bounding
    box accompanied by its estimated class probabilities and objectness score. Since
    objects can have varying sizes, you may then want to slide the CNN again across
    larger 4 × 4 regions as well, to get even more bounding boxes.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: 'This technique is fairly straightforward, but as you can see it will often
    detect the same object multiple times, at slightly different positions. Some postprocessing
    is needed to get rid of all the unnecessary bounding boxes. A common approach
    for this is called *non-max suppression*. Here’s how it works:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: 'First, get rid of all the bounding boxes for which the objectness score is
    below some threshold: since the CNN believes there’s no object at that location,
    the bounding box is useless.'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the remaining bounding box with the highest objectness score, and get rid
    of all the other remaining bounding boxes that overlap a lot with it (e.g., with
    an IoU greater than 60%). For example, in [Figure 14-25](#sliding_cnn_diagram),
    the bounding box with the max objectness score is the thick bounding box over
    the leftmost rose. The other bounding box that touches this same rose overlaps
    a lot with the max bounding box, so we will get rid of it (although in this example
    it would already have been removed in the previous step).
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat step 2 until there are no more bounding boxes to get rid of.
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This simple approach to object detection works pretty well, but it requires
    running the CNN many times (15 times in this example), so it is quite slow. Fortunately,
    there is a much faster way to slide a CNN across an image: using a *fully convolutional
    network* (FCN).'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: Fully Convolutional Networks
  id: totrans-353
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The idea of FCNs was first introduced in a [2015 paper](https://homl.info/fcn)⁠^([30](ch14.html#idm45720182347248))
    by Jonathan Long et al., for semantic segmentation (the task of classifying every
    pixel in an image according to the class of the object it belongs to). The authors
    pointed out that you could replace the dense layers at the top of a CNN with convolutional
    layers. To understand this, let’s look at an example: suppose a dense layer with
    200 neurons sits on top of a convolutional layer that outputs 100 feature maps,
    each of size 7 × 7 (this is the feature map size, not the kernel size). Each neuron
    will compute a weighted sum of all 100 × 7 × 7 activations from the convolutional
    layer (plus a bias term). Now let’s see what happens if we replace the dense layer
    with a convolutional layer using 200 filters, each of size 7 × 7, and with `"valid"`
    padding. This layer will output 200 feature maps, each 1 × 1 (since the kernel
    is exactly the size of the input feature maps and we are using `"valid"` padding).
    In other words, it will output 200 numbers, just like the dense layer did; and
    if you look closely at the computations performed by a convolutional layer, you
    will notice that these numbers will be precisely the same as those the dense layer
    produced. The only difference is that the dense layer’s output was a tensor of
    shape [*batch size*, 200], while the convolutional layer will output a tensor
    of shape [*batch size*, 1, 1, 200].'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-355
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To convert a dense layer to a convolutional layer, the number of filters in
    the convolutional layer must be equal to the number of units in the dense layer,
    the filter size must be equal to the size of the input feature maps, and you must
    use `"valid"` padding. The stride may be set to 1 or more, as you will see shortly.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: Why is this important? Well, while a dense layer expects a specific input size
    (since it has one weight per input feature), a convolutional layer will happily
    process images of any size⁠^([31](ch14.html#idm45720182340336)) (however, it does
    expect its inputs to have a specific number of channels, since each kernel contains
    a different set of weights for each input channel). Since an FCN contains only
    convolutional layers (and pooling layers, which have the same property), it can
    be trained and executed on images of any size!
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, suppose we’d already trained a CNN for flower classification and
    localization. It was trained on 224 × 224 images, and it outputs 10 numbers:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: Outputs 0 to 4 are sent through the softmax activation function, and this gives
    the class probabilities (one per class).
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output 5 is sent through the sigmoid activation function, and this gives the
    objectness score.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outputs 6 and 7 represent the bounding box’s center coordinates; they also go
    through a sigmoid activation function to ensure they range from 0 to 1.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lastly, outputs 8 and 9 represent the bounding box’s height and width; they
    do not go through any activation function to allow the bounding boxes to extend
    beyond the borders of the image.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can now convert the CNN’s dense layers to convolutional layers. In fact,
    we don’t even need to retrain it; we can just copy the weights from the dense
    layers to the convolutional layers! Alternatively, we could have converted the
    CNN into an FCN before training.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: Now suppose the last convolutional layer before the output layer (also called
    the bottleneck layer) outputs 7 × 7 feature maps when the network is fed a 224
    × 224 image (see the left side of [Figure 14-26](#fcn_diagram)). If we feed the
    FCN a 448 × 448 image (see the right side of [Figure 14-26](#fcn_diagram)), the
    bottleneck layer will now output 14 × 14 feature maps.⁠^([32](ch14.html#idm45720182331760))
    Since the dense output layer was replaced by a convolutional layer using 10 filters
    of size 7 × 7, with `"valid"` padding and stride 1, the output will be composed
    of 10 features maps, each of size 8 × 8 (since 14 – 7 + 1 = 8). In other words,
    the FCN will process the whole image only once, and it will output an 8 × 8 grid
    where each cell contains 10 numbers (5 class probabilities, 1 objectness score,
    and 4 bounding box coordinates). It’s exactly like taking the original CNN and
    sliding it across the image using 8 steps per row and 8 steps per column. To visualize
    this, imagine chopping the original image into a 14 × 14 grid, then sliding a
    7 × 7 window across this grid; there will be 8 × 8 = 64 possible locations for
    the window, hence 8 × 8 predictions. However, the FCN approach is *much* more
    efficient, since the network only looks at the image once. In fact, *You Only
    Look Once* (YOLO) is the name of a very popular object detection architecture,
    which we’ll look at next.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1426](assets/mls3_1426.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
- en: Figure 14-26\. The same fully convolutional network processing a small image
    (left) and a large one (right)
  id: totrans-366
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You Only Look Once
  id: totrans-367
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'YOLO is a fast and accurate object detection architecture proposed by Joseph
    Redmon et al. in a [2015 paper](https://homl.info/yolo).⁠^([33](ch14.html#idm45720182322240))
    It is so fast that it can run in real time on a video, as seen in Redmon’s [demo](https://homl.info/yolodemo).
    YOLO’s architecture is quite similar to the one we just discussed, but with a
    few important differences:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: For each grid cell, YOLO only considers objects whose bounding box center lies
    within that cell. The bounding box coordinates are relative to that cell, where
    (0, 0) means the top-left corner of the cell and (1, 1) means the bottom right.
    However, the bounding box’s height and width may extend well beyond the cell.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It outputs two bounding boxes for each grid cell (instead of just one), which
    allows the model to handle cases where two objects are so close to each other
    that their bounding box centers lie within the same cell. Each bounding box also
    comes with its own objectness score.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'YOLO also outputs a class probability distribution for each grid cell, predicting
    20 class probabilities per grid cell since YOLO was trained on the PASCAL VOC
    dataset, which contains 20 classes. This produces a coarse *class probability
    map*. Note that the model predicts one class probability distribution per grid
    cell, not per bounding box. However, it’s possible to estimate class probabilities
    for each bounding box during postprocessing, by measuring how well each bounding
    box matches each class in the class probability map. For example, imagine a picture
    of a person standing in front of a car. There will be two bounding boxes: one
    large horizontal one for the car, and a smaller vertical one for the person. These
    bounding boxes may have their centers within the same grid cell. So how can we
    tell which class should be assigned to each bounding box? Well, the class probability
    map will contain a large region where the “car” class is dominant, and inside
    it there will be a smaller region where the “person” class is dominant. Hopefully,
    the car’s bounding box will roughly match the “car” region, while the person’s
    bounding box will roughly match the “person” region: this will allow the correct
    class to be assigned to each bounding box.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: YOLO was originally developed using Darknet, an open source deep learning framework
    initially developed in C by Joseph Redmon, but it was soon ported to TensorFlow,
    Keras, PyTorch, and more. It was continuously improved over the years, with YOLOv2,
    YOLOv3, and YOLO9000 (again by Joseph Redmon et al.), YOLOv4 (by Alexey Bochkovskiy
    et al.), YOLOv5 (by Glenn Jocher), and PP-YOLO (by Xiang Long et al.).
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: Each version brought some impressive improvements in speed and accuracy, using
    a variety of techniques; for example, YOLOv3 boosted accuracy in part thanks to
    *anchor priors*, exploiting the fact that some bounding box shapes are more likely
    than others, depending on the class (e.g., people tend to have vertical bounding
    boxes, while cars usually don’t). They also increased the number of bounding boxes
    per grid cell, they trained on different datasets with many more classes (up to
    9,000 classes organized in a hierarchy in the case of YOLO9000), they added skip
    connections to recover some of the spatial resolution that is lost in the CNN
    (we will discuss this shortly, when we look at semantic segmentation), and much
    more. There are many variants of these models too, such as YOLOv4-tiny, which
    is optimized to be trained on less powerful machines and which can run extremely
    fast (at over 1,000 frames per second!), but with a slightly lower *mean average
    precision* (mAP).
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: Many object detection models are available on TensorFlow Hub, often with pretrained
    weights, such as YOLOv5,⁠^([34](ch14.html#idm45720182300272)) [SSD](https://homl.info/ssd),⁠^([35](ch14.html#idm45720182297456))
    [Faster R-CNN](https://homl.info/fasterrcnn),⁠^([36](ch14.html#idm45720182295456))
    and [EfficentDet](https://homl.info/efficientdet).⁠^([37](ch14.html#idm45720182293376))
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: 'SSD and EfficientDet are “look once” detection models, similar to YOLO. EfficientDet
    is based on the EfficientNet convolutional architecture. Faster R-CNN is more
    complex: the image first goes through a CNN, then the output is passed to a *region
    proposal network* (RPN) that proposes bounding boxes that are most likely to contain
    an object; a classifier is then run for each bounding box, based on the cropped
    output of the CNN. The best place to start using these models is TensorFlow Hub’s
    excellent [object detection tutorial](https://homl.info/objdet).'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: So far, we’ve only considered detecting objects in single images. But what about
    videos? Objects must not only be detected in each frame, they must also be tracked
    over time. Let’s take a quick look at object tracking now.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: Object Tracking
  id: totrans-377
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Object tracking is a challenging task: objects move, they may grow or shrink
    as they get closer to or further away from the camera, their appearance may change
    as they turn around or move to different lighting conditions or backgrounds, they
    may be temporarily occluded by other objects, and so on.'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most popular object tracking systems is [DeepSORT](https://homl.info/deepsort).⁠^([38](ch14.html#idm45720182279200))
    It is based on a combination of classical algorithms and deep learning:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: It uses *Kalman filters* to estimate the most likely current position of an
    object given prior detections, and assuming that objects tend to move at a constant
    speed.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It uses a deep learning model to measure the resemblance between new detections
    and existing tracked objects.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lastly, it uses the *Hungarian algorithm* to map new detections to existing
    tracked objects (or to new tracked objects): this algorithm efficiently finds
    the combination of mappings that minimizes the distance between the detections
    and the predicted positions of tracked objects, while also minimizing the appearance
    discrepancy.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, imagine a red ball that just bounced off a blue ball traveling
    in the opposite direction. Based on the previous positions of the balls, the Kalman
    filter will predict that the balls will go through each other: indeed, it assumes
    that objects move at a constant speed, so it will not expect the bounce. If the
    Hungarian algorithm only considered positions, then it would happily map the new
    detections to the wrong balls, as if they had just gone through each other and
    swapped colors. But thanks to the resemblance measure, the Hungarian algorithm
    will notice the problem. Assuming the balls are not too similar, the algorithm
    will map the new detections to the correct balls.'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-384
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'There are a few DeepSORT implementations available on GitHub, including a TensorFlow
    implementation of YOLOv4 + DeepSORT: [*https://github.com/theAIGuysCode/yolov4-deepsort*](https://github.com/theAIGuysCode/yolov4-deepsort).'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: So far we have located objects using bounding boxes. This is often sufficient,
    but sometimes you need to locate objects with much more precision—for example,
    to remove the background behind a person during a videoconference call. Let’s
    see how to go down to the pixel level.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: Semantic Segmentation
  id: totrans-387
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *semantic segmentation*, each pixel is classified according to the class
    of the object it belongs to (e.g., road, car, pedestrian, building, etc.), as
    shown in [Figure 14-27](#semantic_segmentation_diagram). Note that different objects
    of the same class are *not* distinguished. For example, all the bicycles on the
    right side of the segmented image end up as one big lump of pixels. The main difficulty
    in this task is that when images go through a regular CNN, they gradually lose
    their spatial resolution (due to the layers with strides greater than 1); so,
    a regular CNN may end up knowing that there’s a person somewhere in the bottom
    left of the image, but it will not be much more precise than that.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1427](assets/mls3_1427.png)'
  id: totrans-389
  prefs: []
  type: TYPE_IMG
- en: Figure 14-27\. Semantic segmentation
  id: totrans-390
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Just like for object detection, there are many different approaches to tackle
    this problem, some quite complex. However, a fairly simple solution was proposed
    in the 2015 paper by Jonathan Long et al. I mentioned earlier, on fully convolutional
    networks. The authors start by taking a pretrained CNN and turning it into an
    FCN. The CNN applies an overall stride of 32 to the input image (i.e., if you
    add up all the strides greater than 1), meaning the last layer outputs feature
    maps that are 32 times smaller than the input image. This is clearly too coarse,
    so they added a single *upsampling layer* that multiplies the resolution by 32.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: There are several solutions available for upsampling (increasing the size of
    an image), such as bilinear interpolation, but that only works reasonably well
    up to ×4 or ×8\. Instead, they use a *transposed convolutional layer*:⁠^([39](ch14.html#idm45720182251296))
    this is equivalent to first stretching the image by inserting empty rows and columns
    (full of zeros), then performing a regular convolution (see [Figure 14-28](#conv2d_transpose_diagram)).
    Alternatively, some people prefer to think of it as a regular convolutional layer
    that uses fractional strides (e.g., the stride is 1/2 in [Figure 14-28](#conv2d_transpose_diagram)).
    The transposed convolutional layer can be initialized to perform something close
    to linear interpolation, but since it is a trainable layer, it will learn to do
    better during training. In Keras, you can use the `Conv2DTranspose` layer.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-393
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In a transposed convolutional layer, the stride defines how much the input will
    be stretched, not the size of the filter steps, so the larger the stride, the
    larger the output (unlike for convolutional layers or pooling layers).
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1428](assets/mls3_1428.png)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
- en: Figure 14-28\. Upsampling using a transposed convolutional layer
  id: totrans-396
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Using transposed convolutional layers for upsampling is OK, but still too imprecise.
    To do better, Long et al. added skip connections from lower layers: for example,
    they upsampled the output image by a factor of 2 (instead of 32), and they added
    the output of a lower layer that had this double resolution. Then they upsampled
    the result by a factor of 16, leading to a total upsampling factor of 32 (see
    [Figure 14-29](#skip_plus_upsample_diagram)). This recovered some of the spatial
    resolution that was lost in earlier pooling layers. In their best architecture,
    they used a second similar skip connection to recover even finer details from
    an even lower layer. In short, the output of the original CNN goes through the
    following extra steps: upsample ×2, add the output of a lower layer (of the appropriate
    scale), upsample ×2, add the output of an even lower layer, and finally upsample
    ×8\. It is even possible to scale up beyond the size of the original image: this
    can be used to increase the resolution of an image, which is a technique called
    *super-resolution*.'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1429](assets/mls3_1429.png)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
- en: Figure 14-29\. Skip layers recover some spatial resolution from lower layers
  id: totrans-399
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*Instance segmentation* is similar to semantic segmentation, but instead of
    merging all objects of the same class into one big lump, each object is distinguished
    from the others (e.g., it identifies each individual bicycle). For example the
    *Mask R-CNN* architecture, proposed in a [2017 paper](https://homl.info/maskrcnn)⁠^([40](ch14.html#idm45720182215744))
    by Kaiming He et al., extends the Faster R-CNN model by additionally producing
    a pixel mask for each bounding box. So, not only do you get a bounding box around
    each object, with a set of estimated class probabilities, but you also get a pixel
    mask that locates pixels in the bounding box that belong to the object. This model
    is available on TensorFlow Hub, pretrained on the COCO 2017 dataset. The field
    is moving fast, though so if you want to try the latest and greatest models, please
    check out the state-of-the-art section of [*https://paperswithcode.com*](https://paperswithcode.com).'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, the field of deep computer vision is vast and fast-paced, with
    all sorts of architectures popping up every year. Almost all of them are based
    on convolutional neural networks, but since 2020 another neural net architecture
    has entered the computer vision space: transformers (which we will discuss in
    [Chapter 16](ch16.html#nlp_chapter)). The progress made over the last decade has
    been astounding, and researchers are now focusing on harder and harder problems,
    such as *adversarial learning* (which attempts to make the network more resistant
    to images designed to fool it), *explainability* (understanding why the network
    makes a specific classification), realistic *image generation* (which we will
    come back to in [Chapter 17](ch17.html#autoencoders_chapter)), *single-shot learning*
    (a system that can recognize an object after it has seen it just once), predicting
    the next frames in a video, combining text and image tasks, and more.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: Now on to the next chapter, where we will look at how to process sequential
    data such as time series using recurrent neural networks and convolutional neural
    networks.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  id: totrans-403
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are the advantages of a CNN over a fully connected DNN for image classification?
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels,
    a stride of 2, and `"same"` padding. The lowest layer outputs 100 feature maps,
    the middle one outputs 200, and the top one outputs 400\. The input images are
    RGB images of 200 × 300 pixels:'
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the total number of parameters in the CNN?
  id: totrans-406
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If we are using 32-bit floats, at least how much RAM will this network require
    when making a prediction for a single instance?
  id: totrans-407
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What about when training on a mini-batch of 50 images?
  id: totrans-408
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If your GPU runs out of memory while training a CNN, what are five things you
    could try to solve the problem?
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why would you want to add a max pooling layer rather than a convolutional layer
    with the same stride?
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When would you want to add a local response normalization layer?
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you name the main innovations in AlexNet, as compared to LeNet-5? What about
    the main innovations in GoogLeNet, ResNet, SENet, Xception, and EfficientNet?
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a fully convolutional network? How can you convert a dense layer into
    a convolutional layer?
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the main technical difficulty of semantic segmentation?
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build your own CNN from scratch and try to achieve the highest possible accuracy
    on MNIST.
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use transfer learning for large image classification, going through these steps:'
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a training set containing at least 100 images per class. For example,
    you could classify your own pictures based on the location (beach, mountain, city,
    etc.), or alternatively you can use an existing dataset (e.g., from TensorFlow
    Datasets).
  id: totrans-417
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Split it into a training set, a validation set, and a test set.
  id: totrans-418
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Build the input pipeline, apply the appropriate preprocessing operations, and
    optionally add data augmentation.
  id: totrans-419
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Fine-tune a pretrained model on this dataset.
  id: totrans-420
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Go through TensorFlow’s [Style Transfer tutorial](https://homl.info/styletuto).
    This is a fun way to generate art using deep learning.
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab3*](https://homl.info/colab3).
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch14.html#idm45720185768624-marker)) David H. Hubel, “Single Unit Activity
    in Striate Cortex of Unrestrained Cats”, *The Journal of Physiology* 147 (1959):
    226–238.'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch14.html#idm45720185766752-marker)) David H. Hubel and Torsten N. Wiesel,
    “Receptive Fields of Single Neurons in the Cat’s Striate Cortex”, *The Journal
    of Physiology* 148 (1959): 574–591.'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: '^([3](ch14.html#idm45720185764704-marker)) David H. Hubel and Torsten N. Wiesel,
    “Receptive Fields and Functional Architecture of Monkey Striate Cortex”, *The
    Journal of Physiology* 195 (1968): 215–243.'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: '^([4](ch14.html#idm45720185754832-marker)) Kunihiko Fukushima, “Neocognitron:
    A Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition
    Unaffected by Shift in Position”, *Biological Cybernetics* 36 (1980): 193–202.'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: '^([5](ch14.html#idm45720185753008-marker)) Yann LeCun et al., “Gradient-Based
    Learning Applied to Document Recognition”, *Proceedings of the IEEE* 86, no. 11
    (1998): 2278–2324.'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch14.html#idm45720185745312-marker)) A convolution is a mathematical operation
    that slides one function over another and measures the integral of their pointwise
    multiplication. It has deep connections with the Fourier transform and the Laplace
    transform and is heavily used in signal processing. Convolutional layers actually
    use cross-correlations, which are very similar to convolutions (see [*https://homl.info/76*](https://homl.info/76)
    for more details).
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch14.html#idm45720185247456-marker)) To produce the same size outputs,
    a fully connected layer would need 200 × 150 × 100 neurons, each connected to
    all 150 × 100 × 3 inputs. It would have 200 × 150 × 100 × (150 × 100 × 3 + 1)
    ≈ 135 billion parameters!
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch14.html#idm45720185245968-marker)) In the international system of units
    (SI), 1 MB = 1,000 KB = 1,000 × 1,000 bytes = 1,000 × 1,000 × 8 bits. And 1 MiB
    = 1,024 kiB = 1,024 × 1,024 bytes. So 12 MB ≈ 11.44 MiB.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: '^([9](ch14.html#idm45720185209104-marker)) Other kernels we’ve discussed so
    far had weights, but pooling kernels do not: they are just stateless sliding windows.'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: '^([10](ch14.html#idm45720184581728-marker)) Yann LeCun et al., “Gradient-Based
    Learning Applied to Document Recognition”, *Proceedings of the IEEE* 86, no. 11
    (1998): 2278–2324.'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: '^([11](ch14.html#idm45720184531840-marker)) Alex Krizhevsky et al., “ImageNet
    Classification with Deep Convolutional Neural Networks”, *Proceedings of the 25th
    International Conference on Neural Information Processing Systems* 1 (2012): 1097–1105.'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: '^([12](ch14.html#idm45720184386400-marker)) Matthew D. Zeiler and Rob Fergus,
    “Visualizing and Understanding Convolutional Networks”, *Proceedings of the European
    Conference on Computer Vision* (2014): 818–833.'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: '^([13](ch14.html#idm45720184382672-marker)) Christian Szegedy et al., “Going
    Deeper with Convolutions”, *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition* (2015): 1–9.'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: ^([14](ch14.html#idm45720184379088-marker)) In the 2010 movie *Inception*, the
    characters keep going deeper and deeper into multiple layers of dreams; hence
    the name of these modules.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: ^([15](ch14.html#idm45720184347184-marker)) Karen Simonyan and Andrew Zisserman,
    “Very Deep Convolutional Networks for Large-Scale Image Recognition”, arXiv preprint
    arXiv:1409.1556 (2014).
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: ^([16](ch14.html#idm45720184342496-marker)) Kaiming He et al., “Deep Residual
    Learning for Image Recognition”, arXiv preprint arXiv:1512:03385 (2015).
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: ^([17](ch14.html#idm45720184314128-marker)) It is a common practice when describing
    a neural network to count only layers with parameters.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: ^([18](ch14.html#idm45720184310768-marker)) Christian Szegedy et al., “Inception–v4,
    Inception-ResNet and the Impact of Residual Connections on Learning”, arXiv preprint
    arXiv:1602.07261 (2016).
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: '^([19](ch14.html#idm45720184304880-marker)) François Chollet, “Xception: Deep
    Learning with Depthwise Separable Convolutions”, arXiv preprint arXiv:1610.02357
    (2016).'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: ^([20](ch14.html#idm45720184300096-marker)) This name can sometimes be ambiguous,
    since spatially separable convolutions are often called “separable convolutions”
    as well.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: '^([21](ch14.html#idm45720184285456-marker)) Jie Hu et al., “Squeeze-and-Excitation
    Networks”, *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition* (2018): 7132–7141.'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: ^([22](ch14.html#idm45720184264752-marker)) Saining Xie et al., “Aggregated
    Residual Transformations for Deep Neural Networks”, arXiv preprint arXiv:1611.05431
    (2016).
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: ^([23](ch14.html#idm45720184261328-marker)) Gao Huang et al., “Densely Connected
    Convolutional Networks”, arXiv preprint arXiv:1608.06993 (2016).
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: '^([24](ch14.html#idm45720184258784-marker)) Andrew G. Howard et al., “MobileNets:
    Efficient Convolutional Neural Networks for Mobile Vision Applications”, arXiv
    preprint arxiv:1704.04861 (2017).'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: '^([25](ch14.html#idm45720184256368-marker)) Chien-Yao Wang et al., “CSPNet:
    A New Backbone That Can Enhance Learning Capability of CNN”, arXiv preprint arXiv:1911.11929
    (2019).'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: '^([26](ch14.html#idm45720184253792-marker)) Mingxing Tan and Quoc V. Le, “EfficientNet:
    Rethinking Model Scaling for Convolutional Neural Networks”, arXiv preprint arXiv:1905.11946
    (2019).'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: ^([27](ch14.html#idm45720184237344-marker)) A 92-core AMD EPYC CPU with IBPB,
    1.7 TB of RAM, and an Nvidia Tesla A100 GPU.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: '^([28](ch14.html#idm45720183471584-marker)) In the ImageNet dataset, each image
    is mapped to a word in the [WordNet dataset](https://wordnet.princeton.edu): the
    class ID is just a WordNet ID.'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: '^([29](ch14.html#idm45720182431584-marker)) Adriana Kovashka et al., “Crowdsourcing
    in Computer Vision”, *Foundations and Trends in Computer Graphics and Vision*
    10, no. 3 (2014): 177–243.'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: '^([30](ch14.html#idm45720182347248-marker)) Jonathan Long et al., “Fully Convolutional
    Networks for Semantic Segmentation”, *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition* (2015): 3431–3440.'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: '^([31](ch14.html#idm45720182340336-marker)) There is one small exception: a
    convolutional layer using `"valid"` padding will complain if the input size is
    smaller than the kernel size.'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: '^([32](ch14.html#idm45720182331760-marker)) This assumes we used only `"same"`
    padding in the network: `"valid"` padding would reduce the size of the feature
    maps. Moreover, 448 can be neatly divided by 2 several times until we reach 7,
    without any rounding error. If any layer uses a different stride than 1 or 2,
    then there may be some rounding error, so again the feature maps may end up being
    smaller.'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: '^([33](ch14.html#idm45720182322240-marker)) Joseph Redmon et al., “You Only
    Look Once: Unified, Real-Time Object Detection”, *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition* (2016): 779–788.'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: ^([34](ch14.html#idm45720182300272-marker)) You can find YOLOv3, YOLOv4, and
    their tiny variants in the TensorFlow Models project at [*https://homl.info/yolotf*](https://homl.info/yolotf).
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: '^([35](ch14.html#idm45720182297456-marker)) Wei Liu et al., “SSD: Single Shot
    Multibox Detector”, *Proceedings of the 14th European Conference on Computer Vision*
    1 (2016): 21–37.'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: '^([36](ch14.html#idm45720182295456-marker)) Shaoqing Ren et al., “Faster R-CNN:
    Towards Real-Time Object Detection with Region Proposal Networks”, *Proceedings
    of the 28th International Conference on Neural Information Processing Systems*
    1 (2015): 91–99.'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: '^([37](ch14.html#idm45720182293376-marker)) Mingxing Tan et al., “EfficientDet:
    Scalable and Efficient Object Detection”, arXiv preprint arXiv:1911.09070 (2019).'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: ^([38](ch14.html#idm45720182279200-marker)) Nicolai Wojke et al., “Simple Online
    and Realtime Tracking with a Deep Association Metric”, arXiv preprint arXiv:1703.07402
    (2017).
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: ^([39](ch14.html#idm45720182251296-marker)) This type of layer is sometimes
    referred to as a *deconvolution layer*, but it does *not* perform what mathematicians
    call a deconvolution, so this name should be avoided.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: ^([40](ch14.html#idm45720182215744-marker)) Kaiming He et al., “Mask R-CNN”,
    arXiv preprint arXiv:1703.06870 (2017).
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
