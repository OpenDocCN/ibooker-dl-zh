- en: 2 Using generative AI to ensure sufficient data quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Best practices for ensuring high quality of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using generative AI to prepare a data cleaning protocol
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating data content quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dealing with data errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Investigating unclear data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In MS Excel, you can calculate the trend line and standard deviation of a sample
    on the basis of just two data points. Clearly, such “data analysis” is meaningless.
    This chapter will help you focus your efforts on things you *should* do with data,
    rather than just expand on what you *can* do with it. It explains the necessary
    background for any analysis you may wish to perform. You will learn about best
    practices and non-negotiable rules, ensuring that your conclusions are related
    to the business activities you’re analyzing, rather than to flaws in the underlying
    data.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll develop a structured approach to quality assessment and assurance, you’ll
    purge your data of artifacts, you’ll identify the blind spots, and you’ll learn
    to think about the benefits and risks of guesstimating missing pieces. Finally,
    you’ll learn to look at the collected data from a new perspective—the perspective
    of its usefulness for the process of analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 On a whimsy of fortune
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine you bet your business future on the flip of a coin. Let’s say you bet
    on tails. You threw the coin and, fortunately, got tails. Do you think it would
    be a good idea to do this again? And again? We most assuredly hope not. The fact
    that you once got the tails you bet on doesn’t mean that you will get tails again
    next time.
  prefs: []
  type: TYPE_NORMAL
- en: In classical logic, conclusions drawn from false assumptions can be either true
    or false. It’s crucial to understand that you really can draw a *true conclusion*
    from a *false assumption*. We will call such a conclusion an *unreliable* one.
    It doesn’t mean you’ll be right the next time you follow that false assumption.
  prefs: []
  type: TYPE_NORMAL
- en: Coming back to our example, let’s say you weigh a coin in your hand and say
    to yourself, “I’m going to flip this coin. If I get tails, I’ll have more customers
    next month than I did this month.” You throw your coin, get tails, and have more
    customers next month. Was the assumption that the coin has a predictive power
    related to the success of your business true? Probably not. We could get into
    psychology, the influence of self-belief on success, and so on, but even then,
    it’s not the coin that brought your customers. It was your self-belief and hard
    work. Or perhaps luck. Or weather. Or a smart marketing strategy. Or your main
    competitor’s mishap. Or . . . you get the idea. Nevertheless, if you believe in
    the predictive power of the coin, you’d draw true conclusions from a false assumption.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, business analytics performed on false assumptions (false input data)
    may result in true or false conclusions. Poor quality input data leads to *unreliable*
    results regardless of the sophistication of the analysis, with the critical word
    here being *unreliable*.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume you risked performing an analysis on poor data, and it returned
    “good” results. You bet on tails and got more customers next month. It doesn’t
    sanctify the approach. Yes, it happens, but it doesn’t mean your results will
    be good next time. Like flipping the coin, you will get heads or tails, and your
    next analysis will yet again yield *unreliable* results.
  prefs: []
  type: TYPE_NORMAL
- en: In data analytics, this concept is often referred to as “garbage in, garbage
    out.” You now know that “garbage out” doesn’t mean *inaccurate* results. It’s
    much, much more dangerous. It means *unreliable* results. You bet 10 bucks on
    a flip? How about 100? Did you succeed again? How about a car or a house?
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if you throw the coin multiple times and get your expected
    tails each time, the coin may not be random after all. Perhaps the quality of
    your data and, therefore, the following analysis is *sufficient* for your purpose.
  prefs: []
  type: TYPE_NORMAL
- en: '*Reliability* of results depends heavily on the quality of the input data.
    In many cases, we try to avoid being too absolute about the advice we give, as
    there is often more than one way of solving any given problem. However, if your
    analysis is to be used in any decision-making process, you simply cannot accept
    anything less but the best *possible* quality of available data. We emphasized
    the word *possible* for a reason. In most real-world scenarios, you will not have
    the luxury of working with the quality of data you’d want. But all the compromises
    you make here must be conscious and taken into account when evaluating the *reliability*
    of the final result.'
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, despite some marketing claims, there is no one-size-fits-all
    solution ensuring data quality. The most effective way to prepare for analysis
    is to closely examine the data and carefully consider the meaning of each variable
    within the specific context of your business. There may not be any way to entirely
    automate this task, but there are techniques and protocols that can help you approach
    data evaluation and cleaning in a structured and systematic manner.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 A note on best practices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s clean our data, then! Take the first data file you have on your desktop
    or in your project repository, and open it. Is it “clean”? (And if you do actually
    keep data files on the desktop, have a hearty chat with the generative AI of your
    choice about data management and governance.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Trust, but verify  If you got your data from someone who told you that it is
    “clean,” ask yourself a question—do you trust this person’s data skills and (not
    “or”; specifically “and”!) diligence enough to hinge the success of your business
    on their opinion about what it means that data is “clean”? Even if you do trust
    them, control is the highest form of trust in the data world: better safe than
    sorry, look before you leap, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re uncertain about whether your data is clean, we’re on a good path.
    Several aspects must be checked before you deem your data clean enough to be analyzed:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Relevance*—The data should be pertinent to the specific business question
    or problem being addressed. Irrelevant data can lead to confusion and detract
    from the focus of the analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Accuracy*—The data should be correct, error-free, and consistent with the
    real-world objects or events it represents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Completeness*—All *required* data points should be present. Any missing or
    incomplete information should be identified and addressed. At the same time, spurious
    data should be avoided to maintain clarity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Timeliness*—Data should be up-to-date and relevant to the analyzed period.
    Outdated data can result in misleading or irrelevant insights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Uniqueness*—Duplicate data should be identified and eliminated to prevent
    redundancies and inaccuracies in the analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Consistency*—Data should be consistent across different sources, formats,
    and systems. Inconsistencies can lead to incorrect conclusions or insights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Integrity*—The relationships between data elements should be maintained, ensuring
    the data is coherent and meaningful when analyzed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It looks like a lot of work because it is. You want to be sure that your data
    *adequately describes the slice of reality* you’re analyzing (first four points)
    and that its *structure allows for reliable analysis* (last three points).
  prefs: []
  type: TYPE_NORMAL
- en: Slow is smooth, and smooth is fast  Data quality is not a place to skate on
    thin ice. You need to get used to doing things systematically. A lot of data specialists
    claim that at least 80% of data work is related to data cleaning and preparation,
    and these are rather slow processes. Unfortunately, you may be pressed to omit
    it, due to a very unpleasant feedback loop. Businesses do not see the value of
    analysis, so they demand more of it. The more they demand, the higher the time
    pressure. The higher the time pressure, the more corners that are cut (usually
    in areas that do not get into final reports and thus are invisible or useless
    from the perspective of the business sponsors). The more corners that are cut,
    the less reliable the analysis. The less reliable the analytics, the lower the
    subsequent value of said analytics. The lower the value of the analytics . . .
    Doing things right from the start will reduce the bumps and hiccups on your analytical
    journey, caused by your stakeholders coming back to you, requesting repeat analyses
    better aligned to the reality they observe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ensuring data quality can be daunting—there is no silver bullet, and cutting
    corners can lead to unreliable data (and therefore unreliable results). But there
    is still hope: a good structure can guide you through this process in a finite
    amount of time. Knowing what to look for, and when, can help you discover the
    quality issues and estimate their impact on the final analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Getting started
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Do you still have your data file open? Yes, the one from your desktop. No? Open
    it. Have a look at it. Scroll it up and down, and if needed, left and right. Try
    to get to know your data.
  prefs: []
  type: TYPE_NORMAL
- en: Do you think you look silly? Not as silly as you’ll look trying to explain why
    you added the sales volume to the physical product volume (yes, we’ve seen it).
    In an ideal world, you’d have richly described metadata explaining all the columns
    down to the expected decimal places and the name of the person who wrote the script
    that parses texts scraped from the web (if applicable). But most probably, you
    won’t. You’ll get the file with “address”, “time”, and “sales” columns, and it
    will be up to you to decipher whether the addresses are of customers or brick-and-mortar
    shop locations; whether the sales are in USD, EUR, or metric tons; and whether
    the time is the time of order or purchase, or perhaps a test column left by developers.
  prefs: []
  type: TYPE_NORMAL
- en: We will repeat this again. If you work in business and are not absolutely, positively
    sure about the exact meaning of the variables received from your IT department,
    don’t assume. Ask! We happened to analyze the efficiency of a service sending
    tens of millions of messages to the clients of a multinational company. We discovered
    an anomaly that would occur very rarely, where the ratio of opened-to-received
    messages was higher than one. Trying to understand what was *actually* defined
    as a received message and what was *actually* defined as an open message, we discovered
    that the error in the logging system was so deep that all the previous analytics
    on the efficiency of messaging were worthless! We cannot overemphasize the importance
    of close collaboration between the business and data or IT crowds.
  prefs: []
  type: TYPE_NORMAL
- en: From clay tablets to AI, tools change, but the mess in the data doesn’t. What
    did change with the introduction of generative AI is your access to expert knowledge,
    which may save you a lot of time. It won’t replace your experience and responsibility
    for understanding your data, but it may help you get to know it much quicker.
  prefs: []
  type: TYPE_NORMAL
- en: If you have read the available metadata, there are still a couple of steps you
    should take to make this initial inspection a part of a structured analytical
    approach. What steps? We could tell you, but this book wouldn’t be about generative
    AI then, would it?
  prefs: []
  type: TYPE_NORMAL
- en: We want you to accept that your AI advisor is infinitely patient and perfectly
    nonjudgmental. There is no question too simple to ask. “What was the name of this
    curve . . . you know, this looking like the hill? How do I plot two columns from
    a DataFrame?” Honestly, searching one’s own memory is sometimes more time consuming
    than dropping a quick question and running the provided code.
  prefs: []
  type: TYPE_NORMAL
- en: We also want you to forget about “prompt engineering.” It has its place, and
    some models, such as smaller Llama versions, are incredibly vulnerable to prompt
    structure. Changing a single word to a synonym can sometimes drastically change
    the model’s understanding of the prompt. We assume, however, that you will use
    a generative AI that understands the linguistic context sufficiently to not require
    more work on prompt engineering than it would take to find relevant information
    about actual analytics elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: Note  As mentioned in the “about this book” section (before chapter 1), we mainly
    use the Python environment to process example datasets. Chapter 7 will guide you
    through using generative AI to adapt the code to your environment if needed.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how generative AI (in this case, ChatGPT 4) deals with a plain language
    question to the effect of “I’m lost and need guidance.”
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Init-MA.png)**I want to perform data analysis. I imported
    my data into pandas DataFrames. What is the first thing I should do with it?'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**After importing your data into pandas DataFrames,
    the first thing you should do is to explore and understand your data. You can
    start by performing the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Inspect the first few rows of the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the `head()` method to display the first few rows of the DataFrame. This
    will give you a quick overview of the data structure, column names, and the type
    of data contained within each column.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '2\. Check the shape of the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the `shape` attribute to get the dimensions of the DataFrame (rows and columns).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '3\. Obtain summary statistics and data types:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the `info()` method to get a summary of the data types, number of non-null
    values, and memory usage of the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '4\. Generate summary statistics for numerical columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the `describe()` method to get summary statistics for the numerical columns
    in the DataFrame. This will give you a sense of the central tendency and dispersion
    of the numerical data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '5\. Check for missing values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the `isna()` method in conjunction with the `sum()` method to count the
    number of missing values in each column.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '6\. Investigate the categorical columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For categorical columns, you can use the `value_counts()` method to get a count
    of unique values and their frequencies.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Once you have a better understanding of your data, you can proceed with data
    cleaning, preprocessing, and analysis. This may involve handling missing values,
    encoding categorical variables, feature engineering, and other tasks based on
    the specific requirements of your analysis.
  prefs: []
  type: TYPE_NORMAL
- en: In its first sentence, our AI advisor agreed with the importance of *understanding*
    your data. A careful read of the rest of the model’s answer may reveal some less
    impressive components. Some steps seem redundant. For example, the `df.isna().sum()`
    method does not offer any information not previously provided by `df.info()` (which,
    in fact, also includes information returned by `df.shape`).
  prefs: []
  type: TYPE_NORMAL
- en: You need to decide on your priorities  Efficiency versus readability is just
    one of the choices you’ll have to make when choosing the methods you’ll be using
    to reach your goals. Generative AI, when inquired, can often present you with
    the benefits and weaknesses of different methods. Still, you need to be conscious
    of these choices and sometimes explicitly request a specific method or solution,
    prioritizing one characteristic over the other.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, the overall answer is very good. Like the old carpenter’s adage
    says, it’s better to measure twice. We appreciate that ChatGPT errs on the side
    of redundancy. Provided with a wide enough toolset, you’ll soon be able to select
    the tools that best suit your analysis style. Equipped with insufficient tools,
    you may stay in the dark.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, you want to check the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The overall “look” of the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The size of the tables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The types of variables in each column. The quick check should reveal whether
    there are mismatched data types, such as text strings in place of numerical values
    or vice versa.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For numerical columns, the basic statistics, count, mean, extreme values, etc.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The number of missing values in each column. A note of a warning here: some
    data is inherently sparse. Simply counting the missing values in the column will
    not provide a definite answer on data completeness; however, if you combine it
    with the knowledge of data meaning, it should.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The number of unique values in each column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This should give you a good intuition about the data you’re working with.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s load our example data so we have something to analyze: the Brazilian
    e-commerce public dataset by Olist ([www.kaggle.com/datasets/olistbr/brazilian-ecommerce](http://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)).
    As we mentioned in the preface, you’ll need to use a free Kaggle account. The
    following listing shows the code we used to load our data. (All the code required
    to perform the analytics we present is available in our GitHub repository [https://github.com/mariansiwiak/Generative_AI_for_Data_Analytics](https://github.com/mariansiwiak/Generative_AI_for_Data_Analytics).)'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.1 Preparing variables for further analysis later in the chapter
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As the next step, we’ll look at the properties of the DataFrames in listing
    2.1\. Unfortunately, that means running all of the commands proposed by our generative
    AI advisor for all downloaded datasets. That’s a habit you should develop for
    every bit of data you import.
  prefs: []
  type: TYPE_NORMAL
- en: Did you just stop reading and run all the tests on all the DataFrames? Let’s
    try to induce in you a healthy dose of paranoia. We’ll share some alarming findings
    to show you how looking at data summaries can point you to potential problems.
    We’ll also work on identifying the solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We found two interesting points while we waded through the data structures
    in our example data:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of customers in the `df_customers` DataFrame and the number of orders
    in the `df_orders` DataFrame are identical. There are two possibilities. One is
    that each customer, as identified by the system, made one and only one purchase,
    and the second is that we have been provided with an incomplete set of data. Both
    possibilities have far-reaching implications related to further analytics. (The
    number of 99,441 rows has arbitrarily been set by whoever generated this dataset
    for us. In this case, reading the metadata on a dataset would tell us this, but
    let’s assume it’s not available.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `df_customers` DataFrame contains two suspiciously similar columns: `customer_id`
    and `customer_unique_id`. As we will probably want to analyze customer properties
    in other DataFrames, the column that will allow us to connect them is `customer_id`.
    As such, this column is of great importance, so we should stop and ask if `customer_id`
    is indeed a unique identification for customers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we mentioned and will keep mentioning, it’s of utmost importance not to wave
    away anything that doesn’t look right! You could send an email with questions
    to whoever provided you with the data and, depending on their workload, get the
    answer within minutes or months.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re not fluent enough in the available analytical environment, fret not!
    As long as you know what you want to do, you have a generative AI advisor to help
    you determine how.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Init-MA.png)**I have a pandas DataFrame with `customer_id`
    and `order_id` columns. How can I quickly check if every customer made just one
    order or if there are customers who made more than one order?'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**You can quickly check if every customer
    made just one order or if there are customers who made more than one order by
    using the `value_counts()` method on the `''customer_id''` column and then checking
    the maximum count. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In this example, `df['customer_id'].value_counts()` returns a pandas `Series`
    containing the number of orders for each unique `'customer_id'`. If the maximum
    value in this `Series` is greater than 1, it means that at least one customer
    has made more than one order.
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI understood what we wanted to achieve and proposed code we could
    adapt to our needs, replacing just the name of the DataFrame to be tested with
    `df_orders`, as we defined it when loading our datasets.
  prefs: []
  type: TYPE_NORMAL
- en: You can check the results in the files we placed in the repository. It shows
    that either we got the data from the online platform with the lowest customer
    retention rate in existence (one visit per user) or there is a problem with the
    `customer_id` column in the `df_orders` DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: We previously noted that the `customer_id` column in `df_customers` DataFrame
    is accompanied by a `customer_unique_id` column, which seems redundant if the
    `customer_id` column was constructed correctly. The following conversation will
    show you that working with generative AI is indeed different from using any other
    software you have ever encountered. All you need to do is provide a plain language
    explanation of the problem you need to solve.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Init-MA.png)**In my DataFrame containing customers’ data,
    I have two columns: `customer_id` and `customer_unique_id`. How can I check how
    one relates to the other and which one contains actual identifiers?'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**To check how the `customer_id` and `customer_unique_id`
    columns relate to each other and which one contains actual identifiers, you can
    analyze the uniqueness of the values in each column and their relationship. Here’s
    an example of how you can do this using pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In this example, `df['customer_id'].nunique()` and `df['customer_unique _id'].nunique()`
    return the number of unique values in each column. Comparing these numbers can
    give you an idea of which column is more likely to contain actual identifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Next, `df.groupby('customer_unique_id')['customer_id'].nunique()` groups the
    DataFrame by the `customer_unique_id` column and computes the number of unique
    `customer_id` values for each group. This will help you understand the relationship
    between the two columns.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the results, you can determine which column contains actual identifiers
    and how they are related. For example, if `customer_unique_id` has fewer unique
    values and each `customer_unique_id` maps to multiple `customer_id` values, it
    may indicate that `customer_unique_id` is the actual identifier, and `customer_id`
    represents multiple records or transactions for each unique customer.
  prefs: []
  type: TYPE_NORMAL
- en: That’s an interesting response, mainly because it doesn’t suggest that the answer
    will be definitive! There is even a possibility that none of these columns are
    valid identifiers (see, this is how you get paranoia in this line of work). That
    point is essential. Don’t assume anything about data you received from an uncertain
    source, even if you firmly believe and logic dictates that some values mean one
    thing or another. In nine out of ten cases, it will be what you expect, but in
    that tenth case, you’ll thank us after reaching out to the data source and learning
    what this or that variable *actually* means.
  prefs: []
  type: TYPE_NORMAL
- en: Returning to our analysis, the resulting `relationship``Series` is too big to
    analyze manually. This is another important lesson about working with generative
    AI. Do not settle for the answers if the result is unsatisfactory. Complain!
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Init-MA.png)**With my data, the `relationship` `Series`
    produced by your previous code is too long to check manually. Can you propose
    a method to analyze its contents?'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**Sure! You can analyze the `relationship
    Series` by generating a summary of its contents, such as the minimum, maximum,
    mean, and count of unique `customer_id` values per `customer_unique_id`. Here’s
    an example of how you can do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the summary of the relationship between the `customer_unique_id`
    and `customer_id` columns includes the minimum, maximum, and mean number of unique
    `customer_id` values per `customer_unique_id`. Additionally, the `relationship.value_counts().sort_index()`
    method returns a count of unique `customer_id` values per `customer_unique_id`,
    sorted by the index (i.e., the number of unique `customer_id` values).
  prefs: []
  type: TYPE_NORMAL
- en: This summary will give you a better understanding of the relationship between
    the two columns without having to manually check the entire `relationship Series`.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we run the code suggested by our generative AI advisor, the results are
    indeed easier to digest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: It looks like `customer_id` values uniquely identify *records* in the dataset
    rather than uniquely identifying *customers*. Therefore, `customer_unique_id`
    is more likely to serve as a customer identifier. In the actual business environment,
    our next step would be to check with the IT or other relevant departments whether
    over 93% of our clients really make only a single order in their lifetime, or
    whether this is indeed a data quality issue. For now, we’ll take the data at face
    value and continue our analysis.
  prefs: []
  type: TYPE_NORMAL
- en: The key takeaway here is that generative AI can help us choose suitable analytical
    methods. Still, it is up to us to ask the right questions and *understand the
    meaning of the data* in the context of our business domain.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Quality assessment structure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We should now know what information is encoded in the data at hand, and we should
    have an overview of its completeness.
  prefs: []
  type: TYPE_NORMAL
- en: We mentioned previously that you will need a structured approach to ensure sufficient
    data quality. In the introduction to this chapter, we also emphasized that even
    “successful” results from an analysis do not ensure the repeatability of this
    success. The following sections on data cleaning and exploratory data analysis
    present the main elements of such a quality assessment. This should give you a
    solid foundation upon which you can build a structure fit for your use case.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.1 Data cleaning steps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can compare data cleaning to preparing ingredients before cooking a meal.
    We strongly advise against dropping the contents of your fridge directly into
    a cooking pot. A good chef inspects ingredients to remove any spoiled parts, washes
    them to ensure cleanliness, and chops them to uniform sizes for even cooking.
    Similarly, data cleaning involves inspecting the dataset for inaccuracies or inconsistencies,
    removing or correcting these elements, and ensuring the data is in a uniform format.
    This preparatory step is essential to ensure that the final dish (or analytical
    result) is of high quality and free from elements that could compromise its taste
    (or integrity). We’ve emphasized multiple times that analyzing unclean data can
    lead to unreliable conclusions. The comparison with cooking serves this point.
    By cooking with unprepared ingredients, you can get something edible; we just
    wouldn’t advise this approach when you’re cooking to impress your date.
  prefs: []
  type: TYPE_NORMAL
- en: 'To ensure your data is technically clean, you should do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Remove duplicates.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Handle missing values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Correct data entry errors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Validate the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These steps should ensure that your tables contain data points describing business
    processes rather than artifacts from the data collection and preparation processes.
    The next phase should give you a better understanding of the data’s meaning.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.2 Exploratory data analysis elements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By now, your data, or dish ingredients, are filtered and cleaned. Continuing
    with the cooking analogy, exploratory data analysis (EDA) can be compared to tasting
    and seasoning the ingredients. Is your tomato fresh and sun-ripened, or did it
    travel half the world in a freezer and is still so hard that you could play baseball
    with it? Does your variable have the normal distribution required by many algorithms,
    or is it skewed? Just as you would taste the dish at different stages, adjusting
    spices and ingredients to balance flavors and textures, EDA examines the dataset
    through various analytical and visualization techniques to understand its characteristics,
    trends, and outliers. This process allows you to adjust your analysis approach,
    like seasoning, to ensure the outcome is well-balanced and palatable. EDA is about
    getting a feel for the “flavor profile” of the data, identifying which ingredients
    (variables) play well together, anticipating how the final dish (analysis) might
    turn out, and making adjustments as needed for the best possible result.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic EDA structure you should employ for all data you lay your hands on
    could look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Variables distribution testing*—What are the characteristics of the data samples,
    such as mean, standard deviation, range, and so on?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Variables visualization*—What can you read from visual representations of
    the data?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Data meaning clarification*—Does the data represent what you think it does?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Data mapping and harmonization*—Are all the data points sampled using the
    same units, and do the headings use standardized description vocabulary?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Some people add extra steps to EDA, like correlation testing or text analysis.
    Others also add feature engineering. Depending on your data, their approach may
    be more suitable to your needs.
  prefs: []
  type: TYPE_NORMAL
- en: One way from Sunday
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We do not think a one-size-fits-all, silver-bullet, cure-all data preparation
    protocol exists. The general framework we believe will lead to high quality data
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Understand the relation of your data with reality (what each variable means
    and how it was generated).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure the data meets some basic technical standards (that you are working on
    all available *relevant*data points and can cross-relate them between data sources
    if needed).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before you analyze reality using your data, analyze the data itself (ensure
    it adheres to either expertise- or common-sense-based expectations).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following sections, we will walk through an example and ensure the data
    describes phenomena rooted in reality.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Data cleaning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data cleaning must be done as early as possible, as errors resulting from poor
    data representation, consistency, or integrity are persistent and stack up quickly.
    Unclean data can misguide other data preparation and evaluation steps as well
    as your analytics results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us repeat: We don’t believe there is one best way of cleaning data or performing
    EDA. In your work, you may encounter data cleaning protocols, including steps
    we describe as a part of EDA, and vice versa. You may be given a protocol that
    includes steps we consider analysis. If specialists in their field prepared these
    protocols, there is a chance they are better for the data types they are designed
    for. Be flexible. Just make sure you understand what’s happening.'
  prefs: []
  type: TYPE_NORMAL
- en: Order matters  Some cleaning steps may obfuscate errors, especially in preparatory
    work. It is of great importance to understand how things change after each step.
    For example, if you fill missing data with average values from a certain column,
    you cannot do this before removing duplicates.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll assume the worst-case scenario for this exercise. That’s the scenario
    where you’re unsure what to expect from your data.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.1 Removing duplicates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many ways duplicate records can sneak into your data. From data entry
    mistakes (human errors) and system glitches (software errors) to poorly designed
    identifiers (human error), to synchronization problems in distributed systems
    (software error), to errors in merging datasets, to import issues . . . the list
    goes on. With so many things that can go wrong, some of them most likely will.
    In all honesty, we’re still waiting to see a duplicate-free dataset that was not
    previously curated for sharing.
  prefs: []
  type: TYPE_NORMAL
- en: How do you efficiently eliminate duplicate records to avoid overrepresenting
    certain data points, which could skew the analysis results? It depends on your
    analytical environment. Do you need to memorize details? Most certainly not!
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Init-MA.png)**How can I remove duplicate entries from
    a DataFrame?'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**To remove duplicate entries from a DataFrame,
    you can use the `drop_duplicates()` method. By default, it considers all columns
    when identifying duplicates and keeps the first occurrence of a duplicate row.
    Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to remove duplicates based on a specific set of columns, you can
    use the `subset` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In this example, `df.drop_duplicates(subset=['A'])` removes duplicate rows based
    on the values in column `A` while keeping the first occurrence of each duplicate.
  prefs: []
  type: TYPE_NORMAL
- en: This answer is technically correct and, under regular circumstances, sufficient.
  prefs: []
  type: TYPE_NORMAL
- en: Efficiency may not be the top priority, or it may  Depending on your generative
    AI’s specifics, it may consider the answer’s simplicity rather than the resource-effectiveness
    of the solution. Quoting Grace Hopper, *it’s a feature, not a bug*. Chapter 5
    describes methods for using even general-knowledge generative AIs to optimize
    code.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, the answer is sufficient (as we didn’t have any duplicates in the
    first place), so we can triumphantly tick this box and move on to the next step.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.2 Handling missing values
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will repeat the warning we issued earlier: some data *is* sparse. However,
    a list of the possible causes for missing data entries would be at least as long
    as the one for duplicates. Data entry errors, sensor malfunctions, copying errors,
    integration errors, you name it. Finally, there is the unavailability of information.'
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the context and the amount of missing data, you can choose to either
    delete the incomplete data entry or variable (the row or column in tabular formats),
    ignore it, or somehow fill in missing values. You could fill empty fields with
    a default value, an estimate (such as a mean, median, or mode), or use interpolation
    methods. It’s critical, however, to make these decisions consciously, depending
    on the meaning and importance of the data. Recently there was a case of a renowned
    macroeconomist filling missing data for some countries by copying them from adjacent
    cells. In that case, the dataset was sorted alphabetically by country names, which
    caused this method to make completely nonsensical adjustments.
  prefs: []
  type: TYPE_NORMAL
- en: It is paramount that the decision on what to do about missing values is based
    on understanding why the value is missing. Was it, for example, a sensor system
    error causing a missed measurement? Or was it an error in the system writing down
    the results? Was there an error when multiple data sources were connected together?
    To decide what to do with the missing values, you should first have a look at
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: In our example data, we should notice three DataFrames where the number of non-null
    values differs from the number of rows in some columns (meaning it’s not just
    an empty column). These are `df_orders`, `df_order_reviews`, and `df_products`.
  prefs: []
  type: TYPE_NORMAL
- en: In the `df_orders` DataFrame, we noticed that some approval and delivery dates
    are missing. We’ll use this DataFrame in section 2.6 to analyze the underlying
    process described by the data and the relation of the order status with the missing
    dates.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of `df_products`, we are missing some category names, the same number
    of product names, description lengths, and pictures, and some product size data.
    Let’s see if we can say anything by looking at the products where some of the
    values are missing.
  prefs: []
  type: TYPE_NORMAL
- en: There are 611 rows where some of the data is missing. This would be a chore
    to analyze and address manually, and if we were dealing with 6,111 or 61,111 rows,
    it would become unfeasible. What can we do to automate the process somewhat? Let’s
    start with investigating whether the number of missing values in different columns
    is coincidental or if there is some structure to it.
  prefs: []
  type: TYPE_NORMAL
- en: There is a lesson here about working with generative AI. If you have any doubts
    about your data but are unsure how to voice them precisely, don’t. We mean, don’t
    voice them *precisely*. Simply voice them as best you can.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Init-MA.png)**In my DataFrame, I have missing values.
    It looks like the number of missing values in some columns is correlated. How
    can I check if the missing values form a pattern?'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**You can check if the missing values form
    a pattern by analyzing the co-occurrence of missing values across different columns.
    To do this, you can use pandas to create a DataFrame indicating which values are
    missing and then compute the correlation between these binary indicators. Here’s
    an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In this example, `df.isna()` returns a DataFrame of the same shape as `df`,
    where each element is a boolean value indicating if the corresponding element
    in `df` is missing (`True`) or not (`False`).
  prefs: []
  type: TYPE_NORMAL
- en: '`missing_indicator.corr()` computes the pairwise correlation between the columns
    of the missing value indicators. The resulting correlation matrix represents the
    correlation between the missing values across different columns. High positive
    correlation values indicate that the missing values in the corresponding columns
    tend to co-occur.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the correlation matrix is visualized using a heatmap from the Seaborn
    library. This heatmap can help you quickly identify patterns in missing values,
    such as whether missing values in certain columns tend to coincide with missing
    values in other columns.
  prefs: []
  type: TYPE_NORMAL
- en: This time, we got some really heavy artillery, including visualization libraries.
    Skillful visualization is one of the most important skills in data analysis, and
    you shouldn’t be shy about leaning on your generative AI advisor to provide you
    with the tools you need.
  prefs: []
  type: TYPE_NORMAL
- en: 'A picture is worth a thousand words  If you’d like to learn more about the
    importance of proper visualization for statistical analysis, we can’t recommend
    timeless books by Professor Edward Tufte enough. His *Visual Explanations: Images
    and Quantities, Evidence and Narrative* (Graphics Press, 1997) and *The Visual
    Display of Quantitative Information* (Graphics Press, 1997) are technology-agnostic
    bibles of excellent visual communication.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.1 shows the results of one of the analyses proposed by generative AI.
    While there are many types of analysis that can be performed, most of the steps
    would be similar, so we’ll just use this one as an example.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F01_Siwiak3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 The correlation between missing values in different columns
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We can see that some products have missing values in all of the `product_category
    _name`, `product_name_lenght`, and `product_photos_qty` columns, or in all of
    the `product_weight_g`, `product_length_cm`, `product_height_cm`, and `product_width_cm`
    columns. This means that some products could either be weightless and dimensionless
    (services, perhaps?) or they don’t have any category, product name, or photos
    (a mystery box comes to mind).
  prefs: []
  type: TYPE_NORMAL
- en: Note  The typo in the variable name `product_name_lenght` is preserved here
    to conform to the naming in the original dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we looked for rows in the dataset where every column except for the `product_id`
    was empty, we found just one: `product_id 5eb564652db742ff8f28759cd8 d2652a`.
    As we discussed earlier, before deciding whether to remove or keep such an item,
    we should check the business or data collection process behind such an oddity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fraud detection and forensic analysis are outside the scope of this book, but
    following our attempt to instill in you a bit of professional paranoia, we can’t
    completely ignore the fact that this case begs for an investigation. If we were
    to investigate such a case, we would first check the simplest explanation: someone
    was playing with the database and didn’t clean up after themselves. In such a
    case, this `product_id` should be present only in this single table. The `product_id`
    in question, however, seems to be connected to an order. If possible, we’d try
    to find the product’s website and see if it is a service (unlikely, as there is
    just one such case). If we felt especially inquisitive, we’d move to check this
    seller’s other transactions, particularly with the same buyer. In short, we would
    try to understand the actual event and the expected shape of the data describing
    it. From there, we would try to identify how the error could occur in the data
    (if it is, in fact, an error instead of a non-typical but a legitimate situation).'
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, we now need to take off our Sherlock Holmes caps and get back
    to dealing with a much more prevalent, if not so exciting, category of problem.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.3 Correcting data entry errors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Is “Washington” the same thing as “Washington D.C.”? Or did an employee who
    was saving their precious time by typing just “SD” have in mind “Shipment Delayed”
    or “Standard Delivery”? You’ll encounter multiple data entry error types in your
    work, such as typos, mislabeling, or inconsistent formats, all of which negatively
    affect the dataset’s quality. You need to identify and correct these errors to
    ensure accurate and consistent data.
  prefs: []
  type: TYPE_NORMAL
- en: This is one of the most strenuous and least automatable parts of data preparation.
    Unfortunately, the best tool you can use to search for such errors is biological
    rather than digital. As of the time we’re writing this book, nothing beats a good
    old-fashioned eyeball. Some of the methods described in section 2.1 can help identify
    the oddities. For example, `df['column_name'].value_counts()` can help you spot
    values that occur only once, which may be suspicious if you expect uniform distribution.
    However, detecting and correcting data entry errors requires a combination of
    these techniques, along with domain knowledge and manual inspection.
  prefs: []
  type: TYPE_NORMAL
- en: The specific type of data entry error analysis we want to point out is outlier
    detection and treatment. *Outliers* are data points that deviate significantly
    from the rest of the dataset (for example, when almost all transactions in a set
    are from NY, and suddenly one is from LA). Identifying and handling outliers is
    very important, as they can disproportionately impact the analysis results. Alternatively,
    they can indicate data points of extreme importance. Outliers can be accepted,
    removed, capped, or transformed, depending on the context.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.4 Data validation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You’d really rather not find entries like “last Tuesday, around noon” or “it
    was a beautiful spring morning” in the date column of your table. Validating the
    data ensures that the data adheres to specific rules or constraints, such as data
    types, value ranges, or relationships between columns. This is crucial for maintaining
    data integrity and avoiding inconsistencies that could lead to erroneous conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: If this seems like a lot of work, that’s because it is. The good news is that
    this is yet another opportunity for you to get intimately familiar with analyzed
    data, and that’s something you can’t fake.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the four steps for validating your data:'
  prefs: []
  type: TYPE_NORMAL
- en: Define expected values for your data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check if any bits of data fall outside your expectations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If any do, do something about it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check the consistency of the data coming from multiple tables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The specifics of what values are expected will vary, depending on your dataset.
    There are no universal rules for determining expected versus unexpected values;
    that’s where your domain knowledge kicks in. You must remember, however, that
    data points deviating from expectations might signal either errors in data collection
    (worth investigating) or events of significant importance to your business (definitely
    worth investigating). To give you a starting point, we’ll outline three basic
    areas you should consider when validating your data.
  prefs: []
  type: TYPE_NORMAL
- en: Upfront validation rules and constraints definition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We mentioned that the whole analytical protocol should be defined before running
    an analysis, to avoid having to “tweak” the analysis to fit expectations better.
  prefs: []
  type: TYPE_NORMAL
- en: Validation rules and constraints, indeed, *must* be defined upfront. If you
    scrolled through the files, as we advised you to do at the beginning of section
    2.3, you should have a more or less solid intuition about the structure and distribution
    of variables in your data. You should also study your domain and related data
    enough to know what values are typical and expected. Suppose you don’t know the
    exact acceptable border values for your variables. In that case, you should be
    able to state your expectations in terms of deviations from the mean, percentiles,
    cicada chirps, horse’s lengths, or whatever applies to your data.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s get off our high horse for a minute. Data quality assurance *is*
    an iterative process. We will perform an EDA, and we may learn that some of the
    assumptions we made after our initial analysis were not sufficiently grounded
    and that our intuition led us astray. In that case, we’ll go back and correct
    our validation rules to better discriminate between valuable data and potential
    artifacts.
  prefs: []
  type: TYPE_NORMAL
- en: Data types
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Assigning correct data types to your variables will make your life much easier
    down the line, as many libraries make operations on columns with well defined
    types (especially numerical ones) easy and efficient. In general, if you want
    to perform an operation like `111-11`, you’d like to get `100`, not `1` or a system
    error (where, as we will repeat throughout this book, the latter would be preferable).
  prefs: []
  type: TYPE_NORMAL
- en: When looking at the properties of our example DataFrames revealed by `df_name.dtype`,
    or by calling the `df_name.info()` method, you should notice that pandas successfully
    derived the types of numerical columns. However, it assigned general *object*
    types to columns containing dates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Time representation in programming is far from straightforward. There are myriad
    methods and formats for noting something as seemingly simple as a timestamp, reflecting
    the complexity of time zones, calendar systems, and coding standards. In Python,
    there is no singular “date” type due to these complexities. Instead, for our datasets,
    we have found that the most suitable format is `datetime64ns`, which offers a
    precise timestamp down to the nanosecond:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Analyzing methods for reading different time formats from text may be beyond
    the scope of this book, but it’s surely not beyond the scope of knowledge of your
    GenAI advisor! When in doubt, provide an example of your data, and ask the generative
    AI how to transform it into the format you’ll be using in your pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Relationships between columns
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We mentioned that we’d seen sales volumes added to physical volumes. How do
    you think that happened? This point is critical if you work with multiple DataFrames
    from different data sources. Blind trust that identical column names mean that
    the two columns describe the same thing may very easily deprive your analysis
    of any functional value.
  prefs: []
  type: TYPE_NORMAL
- en: Before you start coding (or copying) a cross-reference function, you should
    have a good understanding of which of your DataFrames will serve as the reference—the
    source of so-called *primary keys*—and which DataFrames will have *foreign keys*—meaning
    the values reference existing primary keys. This way, you’ll create a lineage
    of DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: If you have a DataFrame containing the data on all your customers, their identifiers
    become primary keys. Suppose you have another DataFrame describing orders from
    some range of time, and it has information on which customer made an order; all
    these customers’ identifiers will exist in the DataFrame of customers. However,
    there is a possibility that some of the customers in the reference dataset didn’t
    order anything in the timeframe in question, so they don’t need to all be in the
    orders DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: You can use the following code to identify problems with keys within `df_orders`
    and `df_order_payments`.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.2 Identifying foreign key violations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In the course of our work with this example dataset, we’ve already discovered
    that there is a problem with `customer_id`, so the relation between `df_orders`
    and `df_ customers` here is unexpectedly 1:1, but when checking `df_orders` against
    `df_order_payments`, we can see the expected behavior of `order_id` being a primary
    key in `df_orders` and a foreign key in `df_order_payments`. There is one violation,
    but we’ll leave dealing with it as an exercise for you.
  prefs: []
  type: TYPE_NORMAL
- en: Yet again, there is an issue of causality, which you should identify as an analyst.
    By that, we mean identifying the actual cause of an unexpected data structure.
    Any explanation you can come up with should be confirmed with the data creators.
    Of course, sometimes, your ability to confirm things may be limited, and you may
    need to rely on cross-referencing all the DataFrames and checking if there is
    a logical pattern in identifier columns. Still, in cases where you work without
    100% certainty on the data’s meaning, you should clearly communicate to your stakeholders
    the risk of analysis being performed on an unconfirmed dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We now should have our data adequately cleaned. By performing a thorough data
    cleaning, you increase the likelihood of obtaining reliable and accurate insights
    from your data analysis. A clean dataset serves as a strong foundation for the
    subsequent stages of data preprocessing, EDA, and modeling.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset we use in this book is provided in a modeling-ready state, and as
    such does not require additional cleaning or preprocessing, so we will move directly
    to EDA. In your work, you shouldn’t be so hasty!
  prefs: []
  type: TYPE_NORMAL
- en: 2.6 Exploratory data analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that the data has been sliced and diced, it’s time for some seasoning, mainly
    of you as a data analyst. In most cases, your stakeholders will require simple
    analyses, and they’ll have strong opinions about the visualizations they want.
    You’ll be surprised how often they refuse bleeding-edge interactive, flashy, blinking,
    and sparkling tools and demand good old static charts. And there is nothing wrong
    with that. Your role is to make sure their decisions are informed. If their preferred
    way of acquiring the required information is through one type of chart or another,
    you should respect that in most cases. You’ll need to stomp down your foot, however,
    when you realize that some other data presentation introduces new and required
    information. Pick your battles.
  prefs: []
  type: TYPE_NORMAL
- en: Why do we talk about visualization types here? Because you will get many useful
    charts as a side-effect of your EDA. Doing them right at this stage (which means
    they’re useful to your stakeholders) can save you time and effort later. In this
    section you will find some examples of such useful charts.
  prefs: []
  type: TYPE_NORMAL
- en: In our opinion, EDA is the process that distinguishes great analysts. It’s a
    form of art, linking the meaning of data with its structure. In EDA, you will
    combine statistics, domain knowledge, and visualizations. (You may use your new
    visualization toys, but if you want your stakeholders to adopt them, make sure
    you sometimes show them what you use—luring them generally works better than forcing
    things down their throats.)
  prefs: []
  type: TYPE_NORMAL
- en: This mix of required knowledge may seem intimidating, but you now have access
    to a fantastic source of multidisciplinary know-how. Let’s ask generative AI about
    the DataFrame containing order reviews, aptly named `df_order_reviews`, as it
    has an interesting combination of numbers, text, and dates.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Init-MA.png)**Please suggest EDA techniques for the following
    DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '**![image](../Images/Response.png)**Based on the provided DataFrame structure
    and content, here are some EDA techniques to better understand and analyze your
    `df_order_reviews` DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Basic data overview: Use `.head()`, `.tail()`, `.info()`, and `.describe()`
    to get an initial understanding of the DataFrame content and structure.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Missing values analysis: Check for missing values in each column using `.isna().sum()`
    and decide how to handle them (drop, fill, or impute).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review scores distribution: Use `.value_counts()` or a histogram to visualize
    the distribution of review scores.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '4\. Reviews over time: Plot the number of reviews created over time using the
    `review_creation_date` column. This can help you identify trends and patterns
    in the review data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '5\. Review response time: Calculate the response time for each review by subtracting
    the `review_creation_date` from the `review_answer_timestamp`. Analyze the distribution
    of response times and investigate if there is a relationship between response
    time and review scores.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '6\. Text analysis: Explore the text data in the `review_comment_title` and
    `review_comment_message` columns. You can perform sentiment analysis, topic modeling,
    or word frequency analysis to identify common themes, patterns, or issues mentioned
    in the reviews.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '7\. Investigate the unclear column: To understand the `review_answer_timestamp`
    column, you can analyze its relationship with other columns, like `review_creation_date`
    or `review_score`. Plotting scatter plots, calculating correlations, or performing
    other statistical tests can help you identify the purpose of this column.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '8\. Orders with multiple reviews: Check if there are any orders with multiple
    reviews. This could indicate data entry errors or provide insight into the review
    process.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: These EDA techniques can help you gain a better understanding of the `df_order_reviews`
    DataFrame and identify patterns, trends, and potential issues in the data. Depending
    on your specific objectives and domain knowledge, you may need to adapt or extend
    these techniques to suit your needs.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding answer comes from ChatGPT-v4, as it was the most comprehensive.
    It developed one solid EDA plan, touching upon multiple topics at once. It shows
    how different levels of analysis overlap with each other. You shouldn’t be surprised
    to see some of the steps we performed in the data-cleaning phase included here.
    Other steps, in our opinion, are elements of the data analysis itself. As we mentioned,
    there is no gold standard for EDA, and generative AI often provides the broadest
    possible answer.
  prefs: []
  type: TYPE_NORMAL
- en: 2.6.1 Reviewing score distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s get back to our coin-flip gambling problem for a second. If you recall,
    at the beginning of this chapter, we said, “. . . if you throw the coin multiple
    times and get your expected tails each time, the coin may not be random after
    all.” Before betting on a coin, you most definitely should check what the distribution
    of results is. If it’s really random, you most likely should not bet on it at
    all, but if it’s not random, you can use it to your advantage. You can think about
    reviewing the score distribution as you would about testing multidimensional coins.
  prefs: []
  type: TYPE_NORMAL
- en: The importance of score distribution cannot be overemphasized, as distribution
    analysis can reveal potential issues that may impact the study’s validity, such
    as data entry errors or biases. Visualizing the distribution through histograms,
    box plots, or density plots can also help to determine whether the data follows
    a specific pattern or conforms to a known statistical distribution. This knowledge
    can help you select appropriate statistical tests, predictive models, or data
    transformation techniques to ensure accurate and reliable results.
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing the distribution typically involves comparing the observed data distribution
    with a theoretical distribution (for example, normal, exponential, or binomial)
    to assess how well the data conforms to expected patterns. You have multiple distribution
    testing methods at your disposal, including visual inspection with histograms
    or Q-Q plots, or more quantitative measures like the Kolmogorov-Smirnov test,
    the Shapiro-Wilk test for normality, or the Anderson-Darling test. Each method
    has its strengths and particularities, making it more or less suitable depending
    on the size of the dataset and the assumptions you’re willing to make.
  prefs: []
  type: TYPE_NORMAL
- en: 'If this task seems complex, that’s because it used to be. Now it’s as simple
    as typing:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Init-MA.png)**How can I test what distribution does my
    dataset fit best?'
  prefs: []
  type: TYPE_NORMAL
- en: If you’re looking for a more specific answer, you can add details about your
    analytical environment, data source, or an intuition you want to examine (note
    that we didn’t use the word “confirm”). From our experience, ChatGPT-4 beats all
    the other tools in its statistical prowess.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at some popular distributions and where you may expect them. The
    shapes of commonly encountered distributions are presented in figure 2.2.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F02_Siwiak3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 Commonly encountered data distributions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Normal distribution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The normal distribution is also known as Gaussian distribution or “bell curve.”
    It’s often the first assumption for continuous data, due to the central limit
    theorem, which posits that the mean of a sufficiently large number of independent
    random variables, each with finite mean and variance, will be approximately normally
    distributed, regardless of the underlying distribution. Be careful, though. Some
    statistical tests rely heavily on the assumption of the normality of the data
    distribution. If your data is skewed, your test results may get screwed as a result.
  prefs: []
  type: TYPE_NORMAL
- en: The normal distribution is particularly relevant for metrics like sales figures,
    heights of individuals, and errors in measurements.
  prefs: []
  type: TYPE_NORMAL
- en: Binomial distribution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The binomial distribution is used for binary outcomes in data, like coin flipping.
    This distribution is relevant when dealing with success/failure, yes/no, or 1/0
    types of outcomes in data, such as conversion rates in A/B testing, pass/fail
    rates in quality control, or click-through rates in online advertising.
  prefs: []
  type: TYPE_NORMAL
- en: 'Seeing more than two bars in a binomial distribution chart often leads to confusion,
    so let’s try to dispel that right away. The binomial distribution describes the
    number of successes in a fixed number of independent Bernoulli trials, each with
    the same probability of success. When you see a binomial distribution with more
    than two bins, it does not represent just a single trial (which would result in
    only two outcomes: success or failure, 1 or 0). Instead, it represents the outcomes
    of multiple trials, so it can have more than two bins.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, suppose you analyze the binomial distribution of *n*=10 different
    trials. In that case, the distribution doesn’t show just two outcomes but the
    probability of achieving 0, 1, 2, ..., up to 10 successes out of those 10 trials.
    Each “bar” in this context represents the number of successes: one bar for 0 successes,
    another for 1 success, and so on, up to 10 successes. So while each individual
    trial has a binary outcome, the'
  prefs: []
  type: TYPE_NORMAL
- en: aggregate result across multiple trials can range from 0 successes (all failures)
    to *n* successes (all successes), leading to *n*+1 possible outcomes. Hence the
    number of bars in the distribution’s representation.
  prefs: []
  type: TYPE_NORMAL
- en: Uniform distribution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A uniform distribution assumes that all outcomes are equally likely. It’s frequently
    a starting assumption for simulations or when modeling unbiased random variables,
    such as random digits, the simulation of equal-probability scenarios, or when
    no prior information is available.
  prefs: []
  type: TYPE_NORMAL
- en: While this distribution is less common in raw business data, it’s worth remembering.
  prefs: []
  type: TYPE_NORMAL
- en: Poisson distribution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Poisson distribution applies to count data where occurrences happen independently
    over a constant rate in space or time. It’s commonly used for modeling the number
    of times an event occurs in a fixed interval of time or space.
  prefs: []
  type: TYPE_NORMAL
- en: You may encounter this distribution when analyzing the number of customer arrivals
    per hour, daily demand for a product, or the number of failures of a system.
  prefs: []
  type: TYPE_NORMAL
- en: Exponential distribution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An exponential distribution is often used to model the time between events in
    a Poisson process, representing the time until an event occurs.
  prefs: []
  type: TYPE_NORMAL
- en: Look for this distribution when working with the lifespan of machinery before
    failure, the time until a customer’s next purchase, or the inter-arrival times
    in a queuing system.
  prefs: []
  type: TYPE_NORMAL
- en: Log-normal distribution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The log-normal distribution comes into play when dealing with variables that
    are the product of many independent random variables. It’s used when data cannot
    be negative and the distribution is skewed right.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll encounter this distribution in datasets concerning stock or real estate
    property prices or income distributions.
  prefs: []
  type: TYPE_NORMAL
- en: Beta distribution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The beta distribution is useful in modeling variables that are bounded on both
    sides, such as proportions and percentages. It’s flexible and can assume a variety
    of shapes, making it suitable for modeling the behavior of random variables limited
    to intervals.
  prefs: []
  type: TYPE_NORMAL
- en: This distribution appears when the data represents conversion rates with values
    between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: Gamma distribution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The gamma distribution is related to the exponential distribution and is useful
    for modeling the wait time for the *n*-th event in a Poisson process.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll need the gamma distribution when modeling things such as the amount of
    rainfall accumulated in a reservoir, service times in systems, or the reliability
    of systems over a continuous scale.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  Can multimodal generative AI be used to identify distribution type from
    a chart? At the time of writing this book, no. We tried to feed parts of figure
    2.2 to different generative AIs, and their answers were often as diverse as misleading.
    However, generative AIs can still guide you through proper testing procedures.
  prefs: []
  type: TYPE_NORMAL
- en: 2.6.2 Time series exploration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we enter a grey zone. You may encounter discussions on analyzing changes
    over time as a part of EDA or as a part of in-depth analysis itself. On the one
    hand, visualizing all the time series is a part of EDA. Sometimes this will allow
    you to identify outliers or errors requiring cleanup. On the other hand, these
    outliers could encode significant issues or events in business processes.
  prefs: []
  type: TYPE_NORMAL
- en: We conclude that distinguishing between EDA and in-depth business analysis is
    a false dichotomy. EDA *is* an integral part of in-depth analysis. It’s not something
    you want someone else to do for you. Nor is EDA a once-off event—you will be coming
    back to it now and then.
  prefs: []
  type: TYPE_NORMAL
- en: From the process management perspective, it’s clearer when steps related to
    evaluating the data’s technical and structural properties are separate from those
    related to discovering business-related insights. However, the two are too closely
    related to be wholly compartmentalized.
  prefs: []
  type: TYPE_NORMAL
- en: There are two tests that we propose to run at the first EDA iteration. First,
    we’ll visualize and look at the data to search for outliers; second, we’ll perform
    cyclicity detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Changes in the number of responses: Quick data visualization'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We’ve seen situations where the success attributed to a project was, in fact,
    caused by the change in the overall market or the performance of another business
    function. Taking a good look at how different variables change over time can be
    very informative indeed!
  prefs: []
  type: TYPE_NORMAL
- en: This is a very basic yet important task, which should be performed on most of
    your date-related variables. Most business activities have a continuous character,
    and analyzing their performance over time is critical for understanding those
    activities.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.3 presents the number of reviews recorded in our dataset per month.
    We can clearly see three distinct phases. The first one lasted from October 2016
    to January 2017\. The almost non-existent number of reviews suggests that either
    the business operations or review collecting system were just getting traction.
    Then, from February 2017 to November 2017 (or May 2018, really), we can see a
    phase of rapid growth in the number of reviews collected monthly. Then, from December
    2017 the system stabilized and started fluctuating, or from June 2018 it had a
    moment of collapse and rebounded in August 2018.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F03_Siwiak3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 The number of reviews over time—monthly aggregation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This information could be crucial if we were to quantitatively compare metrics
    involving the number of reviews between different points in time. Now that we’ve
    checked the number of reviews, let’s move on to checking the scores associated
    with reviews.
  prefs: []
  type: TYPE_NORMAL
- en: Changes in review scores over time
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some data is expected to be cyclic, and other data, not so much. Discovering
    and describing cycles in data can help you identify whether the dataset you’ve
    provided is relevant to explaining the related business processes.
  prefs: []
  type: TYPE_NORMAL
- en: The following recording of our conversation with the AI advisor proves our point
    from chapter 1, related to the limitations of embedding generative AI directly
    into your analytics pipeline in a shoot-and-forget manner.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Init-MA.png)**How can I detect internal cyclicity in data?'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**Detecting internal cyclicity in data involves
    identifying recurring patterns or periodic fluctuations over time. Time series
    data is a common example where cyclicity might be present. Here are some techniques
    to detect cyclicity in your data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Visual inspection: Plot the data over time to visually inspect if there’s any
    noticeable cyclical pattern. This can be a quick and simple way to identify obvious
    patterns.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '2\. Seasonal decomposition: Decompose the time series data into its trend,
    seasonal, and residual components using methods like classical decomposition or
    Seasonal and Trend decomposition using Loess (STL). This can help you observe
    the seasonal component directly.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '3\. Autocorrelation and partial autocorrelation: Plot the autocorrelation function
    (ACF) and partial autocorrelation function (PACF) to identify any significant
    lags that indicate cyclical patterns.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '4\. Spectral analysis: Perform a spectral analysis using the Fast Fourier Transform
    (FFT) or Lomb-Scargle Periodogram to identify dominant frequencies in the data,
    which can correspond to cyclic patterns.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: These techniques can help you detect internal cyclicity in your data. Depending
    on the characteristics of your data and the nature of the cyclicity, some methods
    may be more appropriate than others. It’s essential to understand the underlying
    assumptions and limitations of each technique and to choose the one that best
    fits your specific data and objectives.
  prefs: []
  type: TYPE_NORMAL
- en: The code provided by generative AI sometimes differs from expectations. Three
    of the four pieces of code we received are dysfunctional (and are therefore censored),
    and the other . . . Well, look at figure 2.4 to see the resulting visualization
    of the relationship between the score and the time of the review.
  prefs: []
  type: TYPE_NORMAL
- en: We believe that some people gifted with an affinity for postmodern art could
    find an application for the resulting chart. In business analytics, however, you
    aim to get something a tad more readable.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get back to the dysfunctional code, as there is a lesson to be learned
    here. The FFT and Lomb-Scargle methods didn’t work due to problems with data types.
    So far, we’ve advocated that you present your generative AI advisor with general
    queries to confirm your expertise or expand your horizons. However, sometimes
    you’ll encounter problems resulting from false assumptions that generative AI
    may make related to the data types available for analysis. That doesn’t mean the
    answer is useless. It means you’ll have to change gears.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F04_Siwiak3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 The relationship between the score and the time of the review
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Variables won’t, constants aren’t  Generative AIs are language models. They
    operate on a general set of knowledge of mind-boggling scope. Expecting them to
    correctly assume the alignment of data type requirements for some advanced methods
    with our specific data may be overly optimistic.
  prefs: []
  type: TYPE_NORMAL
- en: As the general inquiry failed, let’s try to be more specific.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Init-MA.png)**Values in my data are categorical, with
    possible values of 1, 2, 3, 4, 5\. They are review scores given to orders by customers.
    Can you propose a method to detect if there are any cyclical dependencies of the
    score in time?'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**Since your data consists of categorical
    values (review scores), you can analyze cyclical dependencies by examining the
    distribution of scores over different time periods. One approach is to use aggregated
    bar plots to visualize the score distribution over time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a step-by-step guide to analyze cyclical dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: Convert the timestamp to a more suitable frequency (e.g., daily, weekly, or
    monthly), depending on the length and resolution of your data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 2\. Aggregate the scores and calculate the proportions for each score in each
    time period.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 3\. Create a stacked bar plot to visualize the proportions of scores over time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Examine the stacked bar plot for any apparent cyclical patterns. If you observe
    any regular fluctuations in the proportions of different scores, there might be
    a cyclical dependency in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that the choice of aggregation frequency (daily, weekly, or monthly)
    will impact the granularity of the analysis. You might need to experiment with
    different frequencies to find the most suitable one for your data.
  prefs: []
  type: TYPE_NORMAL
- en: As we used web-based generative AI, it even remembered the name of our DataFrame
    in the conversation context, so no code adjustment in this regard is needed. It’s
    equally as awesome as it may be dangerous. All of its assumptions should be taken
    with a pinch of salt and noted down, as they often may turn out to be a reason
    why some of the steps don’t work as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.5 presents the result of the proposed code. As you can see, the initially
    proposed frequency does not work well with our data. However, there is no obvious
    way of changing it. Does that mean we need to change the tools—revert to Excel
    and pivot tables or find something with a nice, big “change frequency” button?
    Or should we spend another week searching the matplotlib documentation?
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F05_Siwiak3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 Order scores distribution over time—daily aggregation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Maybe not. This is the brilliance of generative AI in action. You encounter
    a problem, you mention it to your AI advisor, and . . .
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Init-MA.png)**How would I change the frequency to monthly?'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**To change the frequency to monthly, you’ll
    need to resample your data accordingly. Here’s how you can modify the code to
    aggregate the data on a monthly basis:'
  prefs: []
  type: TYPE_NORMAL
- en: Convert the timestamp to a more suitable frequency (e.g., monthly).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 2\. Aggregate the scores and calculate the proportions for each score in each
    time period.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 3\. Create a stacked bar plot to visualize the proportions of scores over time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Now, the analysis will be performed at a monthly level, which can help reveal
    long-term cyclical patterns in the data. Feel free to adjust the frequency according
    to your requirements and the granularity of your data.
  prefs: []
  type: TYPE_NORMAL
- en: . . . there is a high chance that generative AI is capable of solving it. Figure
    2.6 presents the distribution of review scores over time, aggregated by month.
  prefs: []
  type: TYPE_NORMAL
- en: We can see that there was a spike in low scores in November and December 2016,
    and maybe, just maybe, there is a cyclic rise in low scores in the spring, but
    the signal is so weak that we won’t dig much deeper into it unless we want to
    run some precise predictive analysis.
  prefs: []
  type: TYPE_NORMAL
- en: We can safely conclude from this section that generative AIs may make mistakes,
    but they can still help you write and modify code, such as for time series analysis,
    and they are not shy of using really advanced libraries and algorithms (Lomb-Scargle
    is an algorithm developed by biologists to analyze biological cycles, later adopted
    by astronomers to search for patterns in star brightness observations). We believe
    that with a little patience and an iterative approach to clarifying the method
    requirements and your data properties, generative AI will become the greatest
    facilitator of your analytical processes.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F06_Siwiak3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 *Order scores distribution over time—monthly aggregation*
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*### 2.6.3 Mysterious variable investigation'
  prefs: []
  type: TYPE_NORMAL
- en: If mysterious variables don’t already plague your days as a data analyst, you
    can expect they will. We mentioned, and will again, that nothing beats solid metadata,
    but things like thorough documentation data pipelines are considered a luxury
    in these times of ever-increasing time pressure. Therefore, some of your data
    may come from a poorly documented system. As much as we discourage using unconfirmed
    data for any reason other than warming your server room, sometimes it is possible
    to deduce the meaning of some variables to a satisfying level of certainty.
  prefs: []
  type: TYPE_NORMAL
- en: To show how you can deal with an example of a mysterious variable, let’s feign
    ignorance as to the meaning of one of the columns in our example data. Let’s try
    to get some insight into the `review_answer_timestamp` column. With our Sherlock
    cap on, let’s see if there is any other column could be connected to it. Using
    intuition alone, we’ll check the relation with `review_creation_date`. Both columns
    relate to reviews and, in some form, to a time.
  prefs: []
  type: TYPE_NORMAL
- en: But what does it mean for two columns to be related? A knee-jerk reaction could
    be to check for a correlation, but this could be a risky proposition in the case
    of time series. Correlation measures the linear relationship between two variables,
    assuming that this relationship is consistent across the dataset. However, this
    assumption often doesn’t hold in time series, data due to factors like autocorrelation,
    trends, seasonality, and external shocks. *Autocorrelation* refers to the correlation
    of a signal with a delayed version of itself, a common characteristic of time
    series data that can inflate correlation measurements between two time series.
    Similarly, *trends* and *seasonality* can create spurious correlations that do
    not reflect a true causal relationship but rather the presence of underlying patterns
    that both series share. Moreover, *external shocks* impacting both series can
    temporarily increase correlation without indicating a meaningful or lasting relationship.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, the critical factor is that the values in both columns relate to
    orders, which themselves were placed as series in time, so they *will* be correlated.
    Whatever these columns mean, their values will grow in time with orders. How,
    then, shall we define the relationship?
  prefs: []
  type: TYPE_NORMAL
- en: Maybe we won’t? We have tested generative AI for ideas and found its ability
    to understand the problem and propose a solution worthwhile.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Init-MA.png)**Can you propose how I can investigate the
    relationship between the `review_answer_ timestamp` and the `review_creation_date`
    columns?'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**To investigate the relationship between
    `review_answer_timestamp` and `review_ creation_date`, you can calculate the time
    difference between these two timestamps and analyze the distribution of the time
    differences. This will give you insights into how long it typically takes for
    a review to be answered.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a step-by-step guide to analyze the relationship between these two columns:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the time difference between `review_answer_timestamp` and `review_
    creation_date`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The `time_diff` column now contains the time difference in seconds.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Calculate basic statistics on the time difference to get an overview of
    the distribution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 3\. Visualize the distribution of time differences using a histogram or a box
    plot.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: By analyzing the distribution of time differences between `review_answer_timestamp`
    and `review_creation_date`, you can understand the typical response time and identify
    any outliers or trends in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to convert the time differences to a more suitable unit (e.g., hours
    or days) if needed. You can also further investigate the relationship by looking
    at how the time difference varies with other factors, such as review scores or
    specific time periods (e.g., days of the week or months of the year).
  prefs: []
  type: TYPE_NORMAL
- en: It makes perfect sense to check the distribution of time differences between
    dates in two investigated columns. Let’s apply the proposed code.
  prefs: []
  type: TYPE_NORMAL
- en: The time difference summary analysis proves that the default timeframe of seconds
    may not be the most useful for the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: However, our AI advisor mentioned that seconds might not be the best units for
    this data analysis. Fortunately, getting it to other time units is as simple as
    dividing the result by a constant. Feeling lazy, we asked generative AI to amend
    the code. It even took care of proper axis titles. The resulting images are presented
    in figure 2.7\.
  prefs: []
  type: TYPE_NORMAL
- en: Judging by the time distribution, where some answers are provided within a few
    hours and others after many months, it seems very much like some human interaction
    drives the difference.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F07_Siwiak3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 Distribution of the time difference between the `review_answer_timestamp`
    and `review_creation_date` columns represented as a histogram (a) and a box plot
    (b)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Let’s try to apply logic here. We have a review creation and review answer.
    As the data comes from an online platform, the first intuition is that the user
    creates the review after purchase, and the seller answers to the reviewer after
    some time.
  prefs: []
  type: TYPE_NORMAL
- en: This example shows that the combined might of generative AI and human expertise
    can easily be beaten by vague column-naming conventions and a lack of metadata.
    Actually, the `review_creation_date` column contains the date of sending the survey
    to the customer, and the `review_answer_timestamp` indicates when the customer
    answered it. This is yet another reason to *always* confirm the naming convention
    data creators used.
  prefs: []
  type: TYPE_NORMAL
- en: 2.6.4 Harmonizing data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s turn our attention to the structural issue of the number of reviews per
    . . . and here is the trick: are you actually sure what those collected reviews
    are concerned with? Getting multiple reviews of the same product is one thing.
    It’s not unusual for multiple people buy the same thing and express their, often
    diverse, opinions. However, getting multiple reviews of the same order would require
    your attention, as you would need to decide how to evaluate order satisfaction
    in the case of more than one score. Would you use the last review or the average?'
  prefs: []
  type: TYPE_NORMAL
- en: In our dataset, 547 orders have more than one review. Now we should check if
    they come from the customer who ordered it, or from multiple customers, which
    would be worrying. To answer the question, we need to harmonize our data and connect
    different sources to form a coherent whole.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume we feel lazy (that’s a reasonably safe assumption) and that we
    want our generative AI to do the heavy lifting.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Init-MA.png)**I have another DataFrame named `df_customers`.
    There are two columns `customer _id` and `customer_unique_id`. What shall I do
    to create in the previously discussed `df_order_reviews`an additional column with
    `customer_unique_id`correctly assigned to `customer_id`, which is already there?'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**To add the `customer_unique_id` column
    to the `df_order_reviews` DataFrame based on the `customer_id` column, you can
    use the `merge()` function in pandas. Here’s how to do it:'
  prefs: []
  type: TYPE_NORMAL
- en: Merge the `df_order_reviews` and `df_customers` DataFrames on the `customer_id`
    column.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: This will perform a left join, meaning that all rows from `df_order_reviews`
    will be included in the `merged_df` DataFrame, and the corresponding `customer_unique_id`
    values from `df_customers` will be matched based on the `customer_id` column.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the `merged_df` DataFrame contains an additional `customer_unique_id`
    column, with the correct values assigned to each `customer_id`. If you want to
    update the original `df_order_reviews` DataFrame instead of creating a new one,
    you can simply assign `merged_df` to `df_order_reviews`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Now, that’s a nice bit of code with no right to work! It’s trying to assign
    customers to orders in the `df_order_reviews` DataFrame. However, no column in
    `df_order_ reviews` allows us to do this directly. We know that the only actual
    identifier we have is the `order_id` column, as we did our homework and learned
    that `customer_id` does not, in fact, identify customers.
  prefs: []
  type: TYPE_NORMAL
- en: The customer is always right  Generative AIs will neither question nor correct
    you. If you tell them to believe in something, they will assume you have your
    reasons. Although, there is a limitation to their suspension of disbelief. We
    tried to convince generative AIs that the Earth is flat, and even though they
    agreed to keep this assumption for the sake of the discussion, they reminded us
    every now and then that the Earth is, in fact, round. However, their tolerance
    borders are set pretty far.
  prefs: []
  type: TYPE_NORMAL
- en: We can get the desired result in a two-step process by first connecting `df_orders`
    and `df_customers` on the `customer_id` field. This will give us a DataFrame that
    contains the `customer_unique_id` for each order. We can then connect this DataFrame
    to `df_order_reviews` on the `order_id` field. Alternatively, we could start by
    connecting `df_orders` and `df_order_reviews` on the `order_id` field. Then we
    could connect the resulting DataFrame to `df_customers` on the `customer_id` field.
    Hopefully you can now see why the initial data analysis is critical. You need
    to see all the DataFrames to harmonize data from multiple sources effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Joining, merging, and concatenating tables may be obvious to you, but, for others,
    this bit of data engineering 101 may still be something to worry about. In the
    past, just knowing *what* needs to happen was a required but not sufficient condition
    for getting the code to perform the desired operation. We’ve seen the computer
    science degree requirement in many data analysts’ job descriptions. Let’s take
    this opportunity to check if today’s gaps in coding experience can be closed using
    generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Init-MA.png)**I have my `df_order_reviews` DataFrame,
    which has the `order_id` column. I also have the `df_orders` DataFrame, where
    I have the `order_id` and the `customer_id` columns. I also have the `df_customers`
    DataFrame, where I have the `customer_id` and the `customer_unique_id` columns.
    How do I assign a valid `customer_ unique_id` from `df_customers` to each `order_id`
    in `df_order_reviews`?'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**To assign the valid `customer_unique_id`
    from `df_customers` to each `order_id` in `df_order_reviews`, you will first need
    to merge `df_orders` with `df_customers` to create a DataFrame with `order_id`
    and `customer_unique_id` columns. Then, merge this new DataFrame with `df_order_reviews`.
    Here’s how to do it step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: Merge the `df_orders` and `df_customers` DataFrames on the `customer_id` column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Now, the `orders_customers` DataFrame has `order_id` and `customer_unique_ id`
    columns.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Merge the `df_order_reviews` and `orders_customers` DataFrames on the `order_id`
    column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This will perform a left join, meaning that all rows from `df_order_reviews`
    will be included in the `merged_df` DataFrame, and the corresponding `customer_unique_id`
    values from `orders_customers` will be matched based on the `order_id` column.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the `merged_df` DataFrame contains an additional `customer_unique_id`
    column, with the correct values assigned to each `order_id`. If you want to update
    the original `df_order_reviews` DataFrame instead of creating a new one, you can
    simply assign `merged_df` to `df_order_reviews`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Now, your `df_order_reviews` DataFrame has the `customer_unique_id` column correctly
    assigned to each `order_id`.
  prefs: []
  type: TYPE_NORMAL
- en: This time, the code works as advertised. Generative AIs indeed can supplement
    you with the code; they know *how* to do it (even if they sometimes need severe
    prodding to get something working). The decision about *what* needs to happen
    is still yours (even if the initial conversations provided you with suggestions).
  prefs: []
  type: TYPE_NORMAL
- en: Now, to finish this part of the exercise, let’s check if multiple reviews do
    come from different customers; in such a case, you should probably inform your
    IT department, as there would be a strong possibility of erroneous review assignments
    to orders. The following code should give you the required answer (and no, we
    didn’t write this ourselves).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: This code groups the `df_order_reviews` DataFrame by counting unique `customer_
    unique_id` values, checks for cases where the count of unique `customer_unique_id`
    values assigned to `order_id` is greater than `1`, and finally prints the results.
    All the multiple reviews came from the same customers. This saves you a trip to
    the IT den and even allows an analysis of how prevalent an opinion about the order
    is over time.
  prefs: []
  type: TYPE_NORMAL
- en: We can assume that you now either know your data well enough or know how to
    use sufficiently eloquent and knowledgeable generative AI models to help you get
    a good insight into your data structure and properties. In the next chapter, we’ll
    focus on the business analysis itself.
  prefs: []
  type: TYPE_NORMAL
- en: Things to ask generative AI
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: What is the typical data structure for data coming from <some_business_ area>?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What data quality problems should I test for in the DataFrame I have provided
    you?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What distribution should I expect in the column describing <some_activity>?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do I deal with <a_clear_description_of_the_encountered_problem>?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Be aware of the context of your conversation with your generative AI advisor.
    Ask it to summarize its understanding of the context (this is particularly useful
    if you want to start a new session on the same topic, such as to research different
    approaches).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don’t shy away from reminding your AI adviser about the purpose of the current
    work or the variable names used. Remember, it aims to please, not to be precise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no better way to understand the data than by looking at it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When evaluating data quality, it’s essential to check its structure, consistency,
    and contents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Double-check your data, triple-check, and reach out to data providers for clarification
    when you’re still in doubt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s a good practice to keep data quality investigation separate from the actual
    in-depth analysis . . .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: . . . however, data cleaning and EDA *are* integral parts of your data analysis,
    and you’ll be repeating them sooner than you hope.*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
