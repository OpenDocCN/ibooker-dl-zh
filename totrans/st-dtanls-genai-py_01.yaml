- en: 2 Using generative AI to ensure sufficient data quality
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 使用生成式AI确保足够的数据质量
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Best practices for ensuring high quality of data
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保数据高质量的最佳实践
- en: Using generative AI to prepare a data cleaning protocol
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用生成式AI准备数据清洗协议
- en: Evaluating data content quality
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估数据内容质量
- en: Dealing with data errors
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理数据错误
- en: Investigating unclear data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调查不明确的数据
- en: In MS Excel, you can calculate the trend line and standard deviation of a sample
    on the basis of just two data points. Clearly, such “data analysis” is meaningless.
    This chapter will help you focus your efforts on things you *should* do with data,
    rather than just expand on what you *can* do with it. It explains the necessary
    background for any analysis you may wish to perform. You will learn about best
    practices and non-negotiable rules, ensuring that your conclusions are related
    to the business activities you’re analyzing, rather than to flaws in the underlying
    data.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在MS Excel中，你可以在仅基于两个数据点的基础上计算样本的趋势线和标准差。显然，这样的“数据分析”是没有意义的。本章将帮助你将精力集中在你应该用数据做的事情上，而不仅仅是扩展你可以用它做的事情。它解释了进行任何分析所需的必要背景。你将了解最佳实践和不可协商的规则，确保你的结论与你要分析的商业活动相关，而不是与底层数据的缺陷相关。
- en: You’ll develop a structured approach to quality assessment and assurance, you’ll
    purge your data of artifacts, you’ll identify the blind spots, and you’ll learn
    to think about the benefits and risks of guesstimating missing pieces. Finally,
    you’ll learn to look at the collected data from a new perspective—the perspective
    of its usefulness for the process of analysis.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你将发展一种结构化的质量评估和保证方法，清除你的数据中的杂质，识别盲点，并学会思考猜测缺失部分的好处和风险。最后，你将学会从新的角度看待收集到的数据——即其对于分析过程的有用性。
- en: 2.1 On a whimsy of fortune
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 幸运的奇思妙想
- en: Imagine you bet your business future on the flip of a coin. Let’s say you bet
    on tails. You threw the coin and, fortunately, got tails. Do you think it would
    be a good idea to do this again? And again? We most assuredly hope not. The fact
    that you once got the tails you bet on doesn’t mean that you will get tails again
    next time.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你把你的商业未来赌在掷硬币上。比如说，你赌的是反面。你掷了硬币，幸运的是，你得到了反面。你认为再次这样做是个好主意吗？我们绝对不希望如此。你一次得到了你赌的反面并不意味着你下次还会得到反面。
- en: In classical logic, conclusions drawn from false assumptions can be either true
    or false. It’s crucial to understand that you really can draw a *true conclusion*
    from a *false assumption*. We will call such a conclusion an *unreliable* one.
    It doesn’t mean you’ll be right the next time you follow that false assumption.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在经典逻辑中，从错误假设中得出的结论可以是真也可以是假。理解这一点至关重要，即你确实可以从一个错误的假设中得出一个*正确的结论*。我们将这样的结论称为*不可靠的结论*。这并不意味着你下次跟随那个错误的假设时就会正确。
- en: Coming back to our example, let’s say you weigh a coin in your hand and say
    to yourself, “I’m going to flip this coin. If I get tails, I’ll have more customers
    next month than I did this month.” You throw your coin, get tails, and have more
    customers next month. Was the assumption that the coin has a predictive power
    related to the success of your business true? Probably not. We could get into
    psychology, the influence of self-belief on success, and so on, but even then,
    it’s not the coin that brought your customers. It was your self-belief and hard
    work. Or perhaps luck. Or weather. Or a smart marketing strategy. Or your main
    competitor’s mishap. Or . . . you get the idea. Nevertheless, if you believe in
    the predictive power of the coin, you’d draw true conclusions from a false assumption.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的例子，假设你手里称了一个硬币，对自己说，“我要掷这个硬币。如果我得到反面，我下个月的客户数量将比这个月多。”你掷了硬币，得到了反面，下个月客户数量确实增加了。你认为这个假设，即硬币具有与你的商业成功相关的预测能力，是真的吗？可能不是。我们可以探讨心理学，自我信念对成功的影响，等等，但即使如此，也不是硬币带来了你的客户。是你的自我信念和辛勤工作。或者可能是运气。或者天气。或者聪明的营销策略。或者你的主要竞争对手的失误。或者……你明白这个意思。尽管如此，如果你相信硬币的预测能力，你将从错误的假设中得出正确的结论。
- en: Similarly, business analytics performed on false assumptions (false input data)
    may result in true or false conclusions. Poor quality input data leads to *unreliable*
    results regardless of the sophistication of the analysis, with the critical word
    here being *unreliable*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，基于错误假设（错误输入数据）进行的商业分析可能得出真或假的结论。无论分析多么复杂，低质量输入数据都会导致*不可靠*的结果，这里的关键词是*不可靠*。
- en: Let’s assume you risked performing an analysis on poor data, and it returned
    “good” results. You bet on tails and got more customers next month. It doesn’t
    sanctify the approach. Yes, it happens, but it doesn’t mean your results will
    be good next time. Like flipping the coin, you will get heads or tails, and your
    next analysis will yet again yield *unreliable* results.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你冒险对一个数据质量较差的分析进行了分析，并且它返回了“良好”的结果。你押注反面并且下个月获得了更多客户。这并不能证明这种方法是正确的。是的，这种情况确实会发生，但这并不意味着你的下一次结果会很好。就像抛硬币一样，你将得到正面或反面，你的下一次分析仍然会产生**不可靠**的结果。
- en: In data analytics, this concept is often referred to as “garbage in, garbage
    out.” You now know that “garbage out” doesn’t mean *inaccurate* results. It’s
    much, much more dangerous. It means *unreliable* results. You bet 10 bucks on
    a flip? How about 100? Did you succeed again? How about a car or a house?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据分析中，这个概念通常被称为“垃圾输入，垃圾输出”。你现在知道，“垃圾输出”并不意味着**不准确**的结果。它要危险得多。这意味着**不可靠**的结果。你押了10美元的硬币？那么100美元呢？你再次成功了？那是一辆车或一栋房子呢？
- en: On the other hand, if you throw the coin multiple times and get your expected
    tails each time, the coin may not be random after all. Perhaps the quality of
    your data and, therefore, the following analysis is *sufficient* for your purpose.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果你多次抛硬币并且每次都得到预期的反面，那么这枚硬币可能根本不是随机的。也许你的数据质量以及因此进行的分析对于你的目的来说已经足够**充分**了。
- en: '*Reliability* of results depends heavily on the quality of the input data.
    In many cases, we try to avoid being too absolute about the advice we give, as
    there is often more than one way of solving any given problem. However, if your
    analysis is to be used in any decision-making process, you simply cannot accept
    anything less but the best *possible* quality of available data. We emphasized
    the word *possible* for a reason. In most real-world scenarios, you will not have
    the luxury of working with the quality of data you’d want. But all the compromises
    you make here must be conscious and taken into account when evaluating the *reliability*
    of the final result.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的**可靠性**在很大程度上取决于输入数据的质量。在许多情况下，我们试图避免对我们的建议过于绝对，因为通常有不止一种解决任何给定问题的方法。然而，如果你的分析要用于任何决策过程，你绝对不能接受低于最佳**可能**质量的数据。我们强调“可能”这个词是有原因的。在大多数现实世界的场景中，你不会有使用你希望的数据质量的奢侈。但你在这里做出的所有妥协都必须是自觉的，并且在评估最终结果的**可靠性**时必须考虑在内。
- en: Unfortunately, despite some marketing claims, there is no one-size-fits-all
    solution ensuring data quality. The most effective way to prepare for analysis
    is to closely examine the data and carefully consider the meaning of each variable
    within the specific context of your business. There may not be any way to entirely
    automate this task, but there are techniques and protocols that can help you approach
    data evaluation and cleaning in a structured and systematic manner.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，尽管有些营销声明，但没有一种万能的解决方案可以确保数据质量。为分析做好准备的最有效方法是仔细检查数据，并仔细考虑在特定业务背景下每个变量的含义。可能没有完全自动化这项任务的方法，但有一些技术和协议可以帮助你以结构化和系统化的方式接近数据评估和清理。
- en: 2.2 A note on best practices
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 关于最佳实践的说明
- en: Let’s clean our data, then! Take the first data file you have on your desktop
    or in your project repository, and open it. Is it “clean”? (And if you do actually
    keep data files on the desktop, have a hearty chat with the generative AI of your
    choice about data management and governance.)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们清理我们的数据吧！拿起你桌面上或项目仓库中的第一个数据文件，然后打开它。它是“干净”的吗？（如果你确实在桌面上保留数据文件，不妨和你的选择之一的人工智能进行一番关于数据管理和治理的热烈讨论。）
- en: 'Trust, but verify  If you got your data from someone who told you that it is
    “clean,” ask yourself a question—do you trust this person’s data skills and (not
    “or”; specifically “and”!) diligence enough to hinge the success of your business
    on their opinion about what it means that data is “clean”? Even if you do trust
    them, control is the highest form of trust in the data world: better safe than
    sorry, look before you leap, and so on.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 信任，但核实  如果你的数据是从告诉你它是“干净”的人那里获得的，问问自己一个问题——你是否足够信任这个人的数据技能和（不是“或”；具体是“和”）勤奋，以至于将你业务的成败建立在他们对数据“干净”含义的看法上？即使你信任他们，控制是数据世界中最高的信任形式：宁可信其有，不可信其无，三思而后行，等等。
- en: 'If you’re uncertain about whether your data is clean, we’re on a good path.
    Several aspects must be checked before you deem your data clean enough to be analyzed:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不确定你的数据是否干净，我们正在走上一条正确的道路。在你认为数据足够干净以进行分析之前，必须检查以下几个方面的内容：
- en: '*Relevance*—The data should be pertinent to the specific business question
    or problem being addressed. Irrelevant data can lead to confusion and detract
    from the focus of the analysis.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*相关性*—数据应与特定业务问题或待解决的问题相关。不相关的数据可能导致混淆，并分散分析的焦点。'
- en: '*Accuracy*—The data should be correct, error-free, and consistent with the
    real-world objects or events it represents.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*准确性*—数据应该是正确的，无错误的，并且与其所代表的现实世界中的对象或事件一致。'
- en: '*Completeness*—All *required* data points should be present. Any missing or
    incomplete information should be identified and addressed. At the same time, spurious
    data should be avoided to maintain clarity.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*完整性*—所有*必需的*数据点都应存在。任何缺失或不完整的信息都应被识别并解决。同时，应避免虚假数据，以保持清晰。'
- en: '*Timeliness*—Data should be up-to-date and relevant to the analyzed period.
    Outdated data can result in misleading or irrelevant insights.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*时效性*—数据应是最新的，并且与所分析的时期相关。过时的数据可能导致误导或不相关的见解。'
- en: '*Uniqueness*—Duplicate data should be identified and eliminated to prevent
    redundancies and inaccuracies in the analysis.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*唯一性*—应识别并消除重复数据，以防止分析中的冗余和不准确性。'
- en: '*Consistency*—Data should be consistent across different sources, formats,
    and systems. Inconsistencies can lead to incorrect conclusions or insights.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一致性*—数据应在不同的来源、格式和系统中保持一致。不一致可能导致错误的结论或见解。'
- en: '*Integrity*—The relationships between data elements should be maintained, ensuring
    the data is coherent and meaningful when analyzed.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*完整性*—数据元素之间的关系应得到维护，确保在分析时数据是连贯且有意义的。'
- en: It looks like a lot of work because it is. You want to be sure that your data
    *adequately describes the slice of reality* you’re analyzing (first four points)
    and that its *structure allows for reliable analysis* (last three points).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来像很多工作，因为确实如此。你想要确保你的数据能够充分描述你正在分析的现实片段（前四点），并且其结构允许进行可靠的分析（最后三点）。
- en: Slow is smooth, and smooth is fast  Data quality is not a place to skate on
    thin ice. You need to get used to doing things systematically. A lot of data specialists
    claim that at least 80% of data work is related to data cleaning and preparation,
    and these are rather slow processes. Unfortunately, you may be pressed to omit
    it, due to a very unpleasant feedback loop. Businesses do not see the value of
    analysis, so they demand more of it. The more they demand, the higher the time
    pressure. The higher the time pressure, the more corners that are cut (usually
    in areas that do not get into final reports and thus are invisible or useless
    from the perspective of the business sponsors). The more corners that are cut,
    the less reliable the analysis. The less reliable the analytics, the lower the
    subsequent value of said analytics. The lower the value of the analytics . . .
    Doing things right from the start will reduce the bumps and hiccups on your analytical
    journey, caused by your stakeholders coming back to you, requesting repeat analyses
    better aligned to the reality they observe.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 慢慢来，稳稳当当  数据质量不是可以轻易忽视的地方。你需要习惯系统地做事。许多数据专家声称，至少80%的数据工作与数据清理和准备有关，这些过程相当缓慢。不幸的是，你可能会因为一个非常不愉快的反馈循环而被迫省略它。企业看不到分析的价值，因此要求更多。要求越多，时间压力就越大。时间压力越大，就越会走捷径（通常是在不会出现在最终报告中的领域，因此对业务赞助者来说要么是看不见的，要么是无用的）。走捷径越多，分析就越不可靠。分析越不可靠，后续的价值就越低。分析的价值越低……从一开始就做好事情将减少你在分析旅程中遇到的颠簸和故障，因为你的利益相关者会回来要求进行更符合他们观察到的现实的重复分析。
- en: 'Ensuring data quality can be daunting—there is no silver bullet, and cutting
    corners can lead to unreliable data (and therefore unreliable results). But there
    is still hope: a good structure can guide you through this process in a finite
    amount of time. Knowing what to look for, and when, can help you discover the
    quality issues and estimate their impact on the final analysis.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 确保数据质量可能会让人感到害怕——没有一劳永逸的解决方案，走捷径可能导致不可靠的数据（因此结果也不可靠）。但仍有希望：良好的结构可以在有限的时间内引导你完成这个过程。知道要寻找什么，以及何时寻找，可以帮助你发现质量问题和评估它们对最终分析的影响。
- en: 2.3 Getting started
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 开始
- en: Do you still have your data file open? Yes, the one from your desktop. No? Open
    it. Have a look at it. Scroll it up and down, and if needed, left and right. Try
    to get to know your data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你还打开着你的数据文件吗？是的，就是你的桌面上的那个。没有？打开它。看看它。上下滚动，如果需要，左右滚动。试着了解你的数据。
- en: Do you think you look silly? Not as silly as you’ll look trying to explain why
    you added the sales volume to the physical product volume (yes, we’ve seen it).
    In an ideal world, you’d have richly described metadata explaining all the columns
    down to the expected decimal places and the name of the person who wrote the script
    that parses texts scraped from the web (if applicable). But most probably, you
    won’t. You’ll get the file with “address”, “time”, and “sales” columns, and it
    will be up to you to decipher whether the addresses are of customers or brick-and-mortar
    shop locations; whether the sales are in USD, EUR, or metric tons; and whether
    the time is the time of order or purchase, or perhaps a test column left by developers.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你觉得自己看起来很傻吗？不如你试图解释为什么你把销售额加到实物产品数量上（是的，我们见过这种情况）。在一个理想的世界里，你会有一份丰富描述的元数据，解释所有列，包括预期的小数位数和编写解析从网络抓取文本的脚本的个人的名字（如果适用）。但很可能是，你不会。你将得到一个包含“地址”、“时间”和“销售额”列的文件，而确定这些地址是客户还是实体店铺的位置；销售额是以美元、欧元还是公吨计算；以及时间是订单或购买时间，或者可能是开发者留下的测试列，这些都得由你来解码。
- en: We will repeat this again. If you work in business and are not absolutely, positively
    sure about the exact meaning of the variables received from your IT department,
    don’t assume. Ask! We happened to analyze the efficiency of a service sending
    tens of millions of messages to the clients of a multinational company. We discovered
    an anomaly that would occur very rarely, where the ratio of opened-to-received
    messages was higher than one. Trying to understand what was *actually* defined
    as a received message and what was *actually* defined as an open message, we discovered
    that the error in the logging system was so deep that all the previous analytics
    on the efficiency of messaging were worthless! We cannot overemphasize the importance
    of close collaboration between the business and data or IT crowds.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次重复这一点。如果你在商业领域工作，并且对你从IT部门收到的变量的确切含义绝对、肯定不确定，不要假设。问！我们偶然分析了向一家跨国公司客户发送数千万条消息的服务效率。我们发现了一个非常罕见的现象，即已打开消息与接收消息的比例高于一。试图理解什么被**实际上**定义为接收消息，什么被**实际上**定义为打开消息，我们发现日志系统中的错误如此深刻，以至于之前关于消息效率的所有分析都毫无价值！我们无法过分强调业务与数据或IT团队之间紧密合作的重要性。
- en: From clay tablets to AI, tools change, but the mess in the data doesn’t. What
    did change with the introduction of generative AI is your access to expert knowledge,
    which may save you a lot of time. It won’t replace your experience and responsibility
    for understanding your data, but it may help you get to know it much quicker.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从粘土板到AI，工具在变化，但数据中的混乱并没有变化。随着生成式AI的引入，发生变化的是你获取专家知识的方式，这可能会为你节省很多时间。它不会取代你的经验和理解数据的责任，但它可能会帮助你更快地了解它。
- en: If you have read the available metadata, there are still a couple of steps you
    should take to make this initial inspection a part of a structured analytical
    approach. What steps? We could tell you, but this book wouldn’t be about generative
    AI then, would it?
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经阅读了可用的元数据，你仍需采取几个步骤，使这次初步检查成为结构化分析方法的组成部分。是什么步骤？我们可以告诉你，但这样这本书就不会是关于生成式AI的了，对吧？
- en: We want you to accept that your AI advisor is infinitely patient and perfectly
    nonjudgmental. There is no question too simple to ask. “What was the name of this
    curve . . . you know, this looking like the hill? How do I plot two columns from
    a DataFrame?” Honestly, searching one’s own memory is sometimes more time consuming
    than dropping a quick question and running the provided code.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望你接受你的AI顾问是无尽耐心且完全无评判的。没有问题太简单而不值得问。“这个曲线叫什么名字……你知道，这个像山一样的？我如何从DataFrame中绘制两列？”坦白说，搜索自己的记忆有时比快速提问并运行提供的代码更耗时。
- en: We also want you to forget about “prompt engineering.” It has its place, and
    some models, such as smaller Llama versions, are incredibly vulnerable to prompt
    structure. Changing a single word to a synonym can sometimes drastically change
    the model’s understanding of the prompt. We assume, however, that you will use
    a generative AI that understands the linguistic context sufficiently to not require
    more work on prompt engineering than it would take to find relevant information
    about actual analytics elsewhere.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还希望你忘记“提示工程”。它有其位置，一些模型，如较小的Llama版本，对提示结构非常敏感。有时，将一个单词改为同义词可能会极大地改变模型对提示的理解。然而，我们假设你将使用一个足够理解语言上下文的生成式AI，这样就不需要比在其他地方查找有关实际分析的相关信息更多的提示工程工作。
- en: Note  As mentioned in the “about this book” section (before chapter 1), we mainly
    use the Python environment to process example datasets. Chapter 7 will guide you
    through using generative AI to adapt the code to your environment if needed.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如“关于本书”部分（在第1章之前）所述，我们主要使用Python环境来处理示例数据集。第7章将指导你如何使用生成式AI根据需要调整代码以适应你的环境。
- en: Let’s see how generative AI (in this case, ChatGPT 4) deals with a plain language
    question to the effect of “I’m lost and need guidance.”
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看生成式AI（在这种情况下，ChatGPT 4）如何处理一个类似“我迷路了，需要指导”的普通语言问题。
- en: '**![image](../Images/Init-MA.png)**I want to perform data analysis. I imported
    my data into pandas DataFrames. What is the first thing I should do with it?'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Init-MA.png)**我想进行数据分析。我已经将我的数据导入到pandas DataFrame中。我应该首先对它做什么？'
- en: '**![image](../Images/Response.png)**After importing your data into pandas DataFrames,
    the first thing you should do is to explore and understand your data. You can
    start by performing the following steps:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**在将你的数据导入到pandas DataFrame之后，你应该首先探索并理解你的数据。你可以从以下步骤开始：'
- en: 'Inspect the first few rows of the DataFrame:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查DataFrame的前几行：
- en: Use the `head()` method to display the first few rows of the DataFrame. This
    will give you a quick overview of the data structure, column names, and the type
    of data contained within each column.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`head()`方法显示DataFrame的前几行。这将给你一个关于数据结构、列名以及每列包含的数据类型的快速概述。
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '2\. Check the shape of the DataFrame:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2. 检查DataFrame的形状：
- en: Use the `shape` attribute to get the dimensions of the DataFrame (rows and columns).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`shape`属性获取DataFrame的维度（行和列）。
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '3\. Obtain summary statistics and data types:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3. 获取摘要统计信息和数据类型：
- en: Use the `info()` method to get a summary of the data types, number of non-null
    values, and memory usage of the DataFrame.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`info()`方法获取数据类型的摘要、非空值的数量和DataFrame的内存使用情况。
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '4\. Generate summary statistics for numerical columns:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4. 为数值列生成摘要统计信息：
- en: Use the `describe()` method to get summary statistics for the numerical columns
    in the DataFrame. This will give you a sense of the central tendency and dispersion
    of the numerical data.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`describe()`方法获取DataFrame中数值列的摘要统计信息。这将给你一个关于数值数据集中趋势和分散程度的直观感受。
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '5\. Check for missing values:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 5. 检查缺失值：
- en: Use the `isna()` method in conjunction with the `sum()` method to count the
    number of missing values in each column.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`isna()`方法与`sum()`方法结合，计算每列中缺失值的数量。
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '6\. Investigate the categorical columns:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 6. 调查分类列：
- en: For categorical columns, you can use the `value_counts()` method to get a count
    of unique values and their frequencies.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类列，你可以使用`value_counts()`方法获取唯一值的计数及其频率。
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Once you have a better understanding of your data, you can proceed with data
    cleaning, preprocessing, and analysis. This may involve handling missing values,
    encoding categorical variables, feature engineering, and other tasks based on
    the specific requirements of your analysis.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你对你的数据有了更好的理解，你就可以进行数据清洗、预处理和分析。这可能包括处理缺失值、编码分类变量、特征工程以及其他基于你分析特定要求的任务。
- en: In its first sentence, our AI advisor agreed with the importance of *understanding*
    your data. A careful read of the rest of the model’s answer may reveal some less
    impressive components. Some steps seem redundant. For example, the `df.isna().sum()`
    method does not offer any information not previously provided by `df.info()` (which,
    in fact, also includes information returned by `df.shape`).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在其第一句话中，我们的AI顾问同意了理解你的数据的重要性。仔细阅读模型答案的其余部分可能会揭示一些不太令人印象深刻的部分。有些步骤似乎重复了。例如，`df.isna().sum()`方法并没有提供`df.info()`（实际上还包括`df.shape`返回的信息）之前未提供的信息。
- en: You need to decide on your priorities  Efficiency versus readability is just
    one of the choices you’ll have to make when choosing the methods you’ll be using
    to reach your goals. Generative AI, when inquired, can often present you with
    the benefits and weaknesses of different methods. Still, you need to be conscious
    of these choices and sometimes explicitly request a specific method or solution,
    prioritizing one characteristic over the other.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要决定你的优先级  在选择你将使用以达到目标的方法时，效率与可读性只是你必须做出的选择之一。当被询问时，生成式AI可以经常向你展示不同方法的优缺点。尽管如此，你需要意识到这些选择，有时需要明确请求特定的方法或解决方案，优先考虑一个特性而不是另一个。
- en: Nevertheless, the overall answer is very good. Like the old carpenter’s adage
    says, it’s better to measure twice. We appreciate that ChatGPT errs on the side
    of redundancy. Provided with a wide enough toolset, you’ll soon be able to select
    the tools that best suit your analysis style. Equipped with insufficient tools,
    you may stay in the dark.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，整体答案非常好。就像古老的木匠谚语所说，量两次比一次好。我们赞赏ChatGPT在冗余方面的倾向。如果你有一个足够广泛的工具集，你很快就能选择最适合你分析风格的工具。如果你工具不足，你可能就会陷入黑暗。
- en: 'In summary, you want to check the following:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，你需要检查以下内容：
- en: The overall “look” of the data.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 整体数据的“外观”。
- en: The size of the tables.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 表的大小。
- en: The types of variables in each column. The quick check should reveal whether
    there are mismatched data types, such as text strings in place of numerical values
    or vice versa.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每一列中的变量类型。快速检查应该会揭示是否存在不匹配的数据类型，例如用文本字符串代替数值或反之。
- en: For numerical columns, the basic statistics, count, mean, extreme values, etc.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于数值列，基本统计信息，计数、平均值、极值等。
- en: 'The number of missing values in each column. A note of a warning here: some
    data is inherently sparse. Simply counting the missing values in the column will
    not provide a definite answer on data completeness; however, if you combine it
    with the knowledge of data meaning, it should.'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每一列中缺失值的数量。这里有一个警告：一些数据天生稀疏。简单地计算列中的缺失值并不能提供关于数据完整性的明确答案；然而，如果你结合对数据含义的了解，它应该可以。
- en: The number of unique values in each column.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每一列中唯一值的数量。
- en: This should give you a good intuition about the data you’re working with.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会给你关于你正在处理的数据的良好直觉。
- en: 'Let’s load our example data so we have something to analyze: the Brazilian
    e-commerce public dataset by Olist ([www.kaggle.com/datasets/olistbr/brazilian-ecommerce](http://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)).
    As we mentioned in the preface, you’ll need to use a free Kaggle account. The
    following listing shows the code we used to load our data. (All the code required
    to perform the analytics we present is available in our GitHub repository [https://github.com/mariansiwiak/Generative_AI_for_Data_Analytics](https://github.com/mariansiwiak/Generative_AI_for_Data_Analytics).)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载我们的示例数据，这样我们就有东西可以分析了：由Olist提供的巴西电子商务公共数据集（[www.kaggle.com/datasets/olistbr/brazilian-ecommerce](http://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)）。正如我们在前言中提到的，你需要使用一个免费的Kaggle账户。以下列表显示了用于加载数据的代码。（我们展示的执行分析所需的所有代码都可在我们的GitHub仓库[https://github.com/mariansiwiak/Generative_AI_for_Data_Analytics](https://github.com/mariansiwiak/Generative_AI_for_Data_Analytics)中找到。）
- en: Listing 2.1 Preparing variables for further analysis later in the chapter
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.1 准备本章后续分析中的变量
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As the next step, we’ll look at the properties of the DataFrames in listing
    2.1\. Unfortunately, that means running all of the commands proposed by our generative
    AI advisor for all downloaded datasets. That’s a habit you should develop for
    every bit of data you import.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 作为下一步，我们将查看列表2.1中DataFrame的性质。不幸的是，这意味着要运行我们的生成式AI顾问提出的所有下载数据集的命令。这是一个你应该为导入的每一份数据培养的习惯。
- en: Did you just stop reading and run all the tests on all the DataFrames? Let’s
    try to induce in you a healthy dose of paranoia. We’ll share some alarming findings
    to show you how looking at data summaries can point you to potential problems.
    We’ll also work on identifying the solutions.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚停止阅读并运行了所有DataFrame上的所有测试吗？让我们尝试在你心中培养一种健康的偏执。我们将分享一些令人震惊的发现，以向您展示查看数据摘要如何指向潜在问题。我们还将致力于识别解决方案。
- en: 'We found two interesting points while we waded through the data structures
    in our example data:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们浏览示例数据中的数据结构时，我们发现两个有趣的观点：
- en: The number of customers in the `df_customers` DataFrame and the number of orders
    in the `df_orders` DataFrame are identical. There are two possibilities. One is
    that each customer, as identified by the system, made one and only one purchase,
    and the second is that we have been provided with an incomplete set of data. Both
    possibilities have far-reaching implications related to further analytics. (The
    number of 99,441 rows has arbitrarily been set by whoever generated this dataset
    for us. In this case, reading the metadata on a dataset would tell us this, but
    let’s assume it’s not available.)
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`df_customers` DataFrame中的客户数量和`df_orders` DataFrame中的订单数量是相同的。有两种可能性。一种可能是每个客户，如系统所识别的，只购买了一次，另一种可能是我们得到了一个不完整的数据集。这两种可能性都与进一步分析有深远的影响。（99,441行的数量是由为我们生成这个数据集的人任意设定的。在这种情况下，读取数据集的元数据会告诉我们这一点，但让我们假设它不可用。）'
- en: 'The `df_customers` DataFrame contains two suspiciously similar columns: `customer_id`
    and `customer_unique_id`. As we will probably want to analyze customer properties
    in other DataFrames, the column that will allow us to connect them is `customer_id`.
    As such, this column is of great importance, so we should stop and ask if `customer_id`
    is indeed a unique identification for customers.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`df_customers` DataFrame包含两个可疑地相似的列：`customer_id`和`customer_unique_id`。由于我们可能需要在其他DataFrame中分析客户属性，允许我们连接它们的列是`customer_id`。因此，这个列非常重要，我们应该停下来询问`customer_id`是否确实是客户的唯一标识。'
- en: As we mentioned and will keep mentioning, it’s of utmost importance not to wave
    away anything that doesn’t look right! You could send an email with questions
    to whoever provided you with the data and, depending on their workload, get the
    answer within minutes or months.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们提到的，并且会继续提到，不要忽视任何看起来不对的东西！你可以给提供数据的人发邮件询问问题，根据他们的工作量，你可以在几分钟或几个月内得到答案。
- en: If you’re not fluent enough in the available analytical environment, fret not!
    As long as you know what you want to do, you have a generative AI advisor to help
    you determine how.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不够熟练地掌握现有的分析环境，不要担心！只要你知道你想做什么，你就有生成式AI顾问帮助你确定如何操作。
- en: '**![image](../Images/Init-MA.png)**I have a pandas DataFrame with `customer_id`
    and `order_id` columns. How can I quickly check if every customer made just one
    order or if there are customers who made more than one order?'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Init-MA.png)**我有一个包含`customer_id`和`order_id`列的pandas DataFrame。我如何快速检查每位客户是否只下了一单，或者是否有客户下了多单？'
- en: '**![image](../Images/Response.png)**You can quickly check if every customer
    made just one order or if there are customers who made more than one order by
    using the `value_counts()` method on the `''customer_id''` column and then checking
    the maximum count. Here’s an example:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**你可以通过在`''customer_id''`列上使用`value_counts()`方法，然后检查最大计数，快速检查每位客户是否只下了一单，或者是否有客户下了多单。以下是一个例子：'
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In this example, `df['customer_id'].value_counts()` returns a pandas `Series`
    containing the number of orders for each unique `'customer_id'`. If the maximum
    value in this `Series` is greater than 1, it means that at least one customer
    has made more than one order.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，`df['customer_id'].value_counts()`返回一个包含每个唯一`'customer_id'`的订单数量的pandas
    `Series`。如果这个`Series`中的最大值大于1，这意味着至少有一位客户下了多单。
- en: Generative AI understood what we wanted to achieve and proposed code we could
    adapt to our needs, replacing just the name of the DataFrame to be tested with
    `df_orders`, as we defined it when loading our datasets.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI理解了我们想要达到的目标，并提出了我们可以根据我们的需求进行修改的代码，只需将测试的DataFrame名称替换为`df_orders`，就像我们在加载数据集时定义的那样。
- en: You can check the results in the files we placed in the repository. It shows
    that either we got the data from the online platform with the lowest customer
    retention rate in existence (one visit per user) or there is a problem with the
    `customer_id` column in the `df_orders` DataFrame.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在我们放在仓库中的文件中检查结果。它显示，要么我们是从存在客户保留率最低的在线平台获取的数据（每个用户一次访问），要么 `df_orders` DataFrame
    中的 `customer_id` 列存在问题。
- en: We previously noted that the `customer_id` column in `df_customers` DataFrame
    is accompanied by a `customer_unique_id` column, which seems redundant if the
    `customer_id` column was constructed correctly. The following conversation will
    show you that working with generative AI is indeed different from using any other
    software you have ever encountered. All you need to do is provide a plain language
    explanation of the problem you need to solve.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到，`df_customers` DataFrame 中的 `customer_id` 列旁边有一个 `customer_unique_id`
    列，如果 `customer_id` 列构建正确，这似乎是多余的。接下来的对话将向你展示，与生成式 AI 一起工作确实不同于使用你以前遇到的任何其他软件。你所需要做的只是提供对需要解决的问题的简单语言解释。
- en: '**![image](../Images/Init-MA.png)**In my DataFrame containing customers’ data,
    I have two columns: `customer_id` and `customer_unique_id`. How can I check how
    one relates to the other and which one contains actual identifiers?'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Init-MA.png)** 在我的包含客户数据的 DataFrame 中，我有两个列：`customer_id`
    和 `customer_unique_id`。我该如何检查它们之间的关系以及哪个包含实际标识符？'
- en: '**![image](../Images/Response.png)**To check how the `customer_id` and `customer_unique_id`
    columns relate to each other and which one contains actual identifiers, you can
    analyze the uniqueness of the values in each column and their relationship. Here’s
    an example of how you can do this using pandas:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)** 要检查 `customer_id` 和 `customer_unique_id`
    列之间的关系以及哪个包含实际标识符，你可以分析每个列中值的唯一性和它们之间的关系。以下是一个使用 pandas 进行此操作的示例：'
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In this example, `df['customer_id'].nunique()` and `df['customer_unique _id'].nunique()`
    return the number of unique values in each column. Comparing these numbers can
    give you an idea of which column is more likely to contain actual identifiers.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，`df['customer_id'].nunique()` 和 `df['customer_unique_id'].nunique()`
    返回每个列中独特值的数量。比较这些数字可以给你一个关于哪个列更有可能包含实际标识符的线索。
- en: Next, `df.groupby('customer_unique_id')['customer_id'].nunique()` groups the
    DataFrame by the `customer_unique_id` column and computes the number of unique
    `customer_id` values for each group. This will help you understand the relationship
    between the two columns.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，`df.groupby('customer_unique_id')['customer_id'].nunique()` 通过 `customer_unique_id`
    列对 DataFrame 进行分组，并计算每个组中独特的 `customer_id` 值的数量。这将帮助你理解这两列之间的关系。
- en: Based on the results, you can determine which column contains actual identifiers
    and how they are related. For example, if `customer_unique_id` has fewer unique
    values and each `customer_unique_id` maps to multiple `customer_id` values, it
    may indicate that `customer_unique_id` is the actual identifier, and `customer_id`
    represents multiple records or transactions for each unique customer.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 根据结果，你可以确定哪个列包含实际标识符以及它们是如何相关的。例如，如果 `customer_unique_id` 有较少的独特值，并且每个 `customer_unique_id`
    映射到多个 `customer_id` 值，这可能表明 `customer_unique_id` 是实际标识符，而 `customer_id` 代表每个独特客户的多条记录或交易。
- en: That’s an interesting response, mainly because it doesn’t suggest that the answer
    will be definitive! There is even a possibility that none of these columns are
    valid identifiers (see, this is how you get paranoia in this line of work). That
    point is essential. Don’t assume anything about data you received from an uncertain
    source, even if you firmly believe and logic dictates that some values mean one
    thing or another. In nine out of ten cases, it will be what you expect, but in
    that tenth case, you’ll thank us after reaching out to the data source and learning
    what this or that variable *actually* means.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个有趣的回答，主要是因为它没有暗示答案将是确定的！甚至有可能这些列中没有任何一个是有效的标识符（看，这就是在这个行业中产生偏执的方式）。这一点至关重要。不要对你从不确定来源收到的数据做出任何假设，即使你坚信逻辑表明某些值意味着一件事或另一件事。在九成的情况下，它将是你所期望的，但在那十分之一的情况下，你会在联系数据源并了解这个或那个变量*实际上*代表什么之后感谢我们。
- en: Returning to our analysis, the resulting `relationship``Series` is too big to
    analyze manually. This is another important lesson about working with generative
    AI. Do not settle for the answers if the result is unsatisfactory. Complain!
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的分析，生成的 `relationship` `Series` 太大，无法手动分析。这是关于与生成式 AI 合作的重要教训之一。如果结果不满意，不要满足于答案。抱怨！
- en: '**![image](../Images/Init-MA.png)**With my data, the `relationship` `Series`
    produced by your previous code is too long to check manually. Can you propose
    a method to analyze its contents?'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Init-MA.png)** 使用我的数据，你之前代码生成的 `relationship` `Series`
    太长，无法手动检查。你能提出一种分析其内容的方法吗？'
- en: '**![image](../Images/Response.png)**Sure! You can analyze the `relationship
    Series` by generating a summary of its contents, such as the minimum, maximum,
    mean, and count of unique `customer_id` values per `customer_unique_id`. Here’s
    an example of how you can do this:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)** 当然！你可以通过生成 `relationship Series` 内容的总结来分析它，例如每个
    `customer_unique_id` 的唯一 `customer_id` 值的最小值、最大值、平均值和计数。以下是如何做到这一点的示例：'
- en: '[PRE9]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In this example, the summary of the relationship between the `customer_unique_id`
    and `customer_id` columns includes the minimum, maximum, and mean number of unique
    `customer_id` values per `customer_unique_id`. Additionally, the `relationship.value_counts().sort_index()`
    method returns a count of unique `customer_id` values per `customer_unique_id`,
    sorted by the index (i.e., the number of unique `customer_id` values).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，`customer_unique_id` 和 `customer_id` 列之间的关系总结包括每个 `customer_unique_id`
    的唯一 `customer_id` 值的最小值、最大值和平均值。此外，`relationship.value_counts().sort_index()`
    方法返回每个 `customer_unique_id` 的唯一 `customer_id` 值的计数，并按索引（即唯一 `customer_id` 值的数量）排序。
- en: This summary will give you a better understanding of the relationship between
    the two columns without having to manually check the entire `relationship Series`.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这个总结将帮助你更好地理解两个列之间的关系，而无需手动检查整个 `relationship Series`。
- en: 'After we run the code suggested by our generative AI advisor, the results are
    indeed easier to digest:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们运行生成式 AI顾问建议的代码后，结果确实更容易消化：
- en: '[PRE10]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: It looks like `customer_id` values uniquely identify *records* in the dataset
    rather than uniquely identifying *customers*. Therefore, `customer_unique_id`
    is more likely to serve as a customer identifier. In the actual business environment,
    our next step would be to check with the IT or other relevant departments whether
    over 93% of our clients really make only a single order in their lifetime, or
    whether this is indeed a data quality issue. For now, we’ll take the data at face
    value and continue our analysis.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来 `customer_id` 值在数据集中唯一地标识了 *记录*，而不是唯一地标识 *客户*。因此，`customer_unique_id` 更有可能作为客户标识符。在实际的商业环境中，我们的下一步将是与
    IT 或其他相关部门核实，是否超过 93% 的客户在其一生中只下过一次订单，或者这确实是一个数据质量问题。目前，我们将按照数据本身的价值继续我们的分析。
- en: The key takeaway here is that generative AI can help us choose suitable analytical
    methods. Still, it is up to us to ask the right questions and *understand the
    meaning of the data* in the context of our business domain.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键要点是，生成式 AI 可以帮助我们选择合适的分析方法。然而，我们还需要提出正确的问题，并在我们业务领域的背景下 *理解数据的含义*。
- en: 2.4 Quality assessment structure
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 质量评估结构
- en: We should now know what information is encoded in the data at hand, and we should
    have an overview of its completeness.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在应该知道手头的数据中编码了哪些信息，并且应该对其完整性有一个概述。
- en: We mentioned previously that you will need a structured approach to ensure sufficient
    data quality. In the introduction to this chapter, we also emphasized that even
    “successful” results from an analysis do not ensure the repeatability of this
    success. The following sections on data cleaning and exploratory data analysis
    present the main elements of such a quality assessment. This should give you a
    solid foundation upon which you can build a structure fit for your use case.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到，你需要一种结构化的方法来确保足够的数据质量。在本章的介绍中，我们也强调了即使是分析中的“成功”结果也不能保证这种成功的可重复性。以下关于数据清洗和探索性数据分析的部分展示了这种质量评估的主要元素。这应该为你提供一个坚实的基础，你可以在此基础上构建适合你用例的结构。
- en: 2.4.1 Data cleaning steps
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.1 数据清洗步骤
- en: You can compare data cleaning to preparing ingredients before cooking a meal.
    We strongly advise against dropping the contents of your fridge directly into
    a cooking pot. A good chef inspects ingredients to remove any spoiled parts, washes
    them to ensure cleanliness, and chops them to uniform sizes for even cooking.
    Similarly, data cleaning involves inspecting the dataset for inaccuracies or inconsistencies,
    removing or correcting these elements, and ensuring the data is in a uniform format.
    This preparatory step is essential to ensure that the final dish (or analytical
    result) is of high quality and free from elements that could compromise its taste
    (or integrity). We’ve emphasized multiple times that analyzing unclean data can
    lead to unreliable conclusions. The comparison with cooking serves this point.
    By cooking with unprepared ingredients, you can get something edible; we just
    wouldn’t advise this approach when you’re cooking to impress your date.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将数据清洗比作在烹饪一顿饭之前准备原料。我们强烈建议不要直接将冰箱里的东西倒进烹饪锅。一位好厨师会检查原料，去除任何腐烂的部分，清洗以确保清洁，并切成均匀的大小以便均匀烹饪。同样，数据清洗涉及检查数据集的不准确或不一致之处，移除或纠正这些元素，并确保数据格式统一。这一准备步骤对于确保最终菜肴（或分析结果）高质量且无任何可能损害其风味（或完整性）的元素至关重要。我们多次强调，分析不干净的数据可能导致不可靠的结论。与烹饪的比较就是为了说明这一点。用未准备好的原料烹饪，你可以得到可以食用的东西；我们只是不建议你在想给约会对象留下深刻印象时采用这种方法。
- en: 'To ensure your data is technically clean, you should do the following:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保你的数据在技术上干净，你应该做以下事情：
- en: Remove duplicates.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除重复项。
- en: Handle missing values.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理缺失值。
- en: Correct data entry errors.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 纠正数据输入错误。
- en: Validate the data.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证数据。
- en: These steps should ensure that your tables contain data points describing business
    processes rather than artifacts from the data collection and preparation processes.
    The next phase should give you a better understanding of the data’s meaning.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤应确保你的表格包含描述业务流程的数据点，而不是数据收集和准备过程中的遗迹。下一阶段应使你对数据的含义有更好的理解。
- en: 2.4.2 Exploratory data analysis elements
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.2 探索性数据分析元素
- en: By now, your data, or dish ingredients, are filtered and cleaned. Continuing
    with the cooking analogy, exploratory data analysis (EDA) can be compared to tasting
    and seasoning the ingredients. Is your tomato fresh and sun-ripened, or did it
    travel half the world in a freezer and is still so hard that you could play baseball
    with it? Does your variable have the normal distribution required by many algorithms,
    or is it skewed? Just as you would taste the dish at different stages, adjusting
    spices and ingredients to balance flavors and textures, EDA examines the dataset
    through various analytical and visualization techniques to understand its characteristics,
    trends, and outliers. This process allows you to adjust your analysis approach,
    like seasoning, to ensure the outcome is well-balanced and palatable. EDA is about
    getting a feel for the “flavor profile” of the data, identifying which ingredients
    (variables) play well together, anticipating how the final dish (analysis) might
    turn out, and making adjustments as needed for the best possible result.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你的数据或菜肴原料已经过筛选和清洗。继续使用烹饪类比，探索性数据分析（EDA）可以比作品尝和调味原料。你的番茄是新鲜且成熟的，还是经过半个世界的冷冻运输，仍然硬得可以用来打棒球？你的变量是否具有许多算法所需的正态分布，或者它是偏斜的？就像你在不同阶段品尝菜肴，调整香料和原料以平衡风味和质地一样，EDA通过各种分析和可视化技术来检查数据集，以了解其特征、趋势和异常值。这个过程允许你像调味一样调整你的分析方法，以确保结果平衡且美味。EDA是感受数据的“风味轮廓”，识别哪些原料（变量）搭配得很好，预测最终菜肴（分析）可能的结果，并在必要时进行调整以获得最佳结果。
- en: 'The basic EDA structure you should employ for all data you lay your hands on
    could look like this:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该为所有你接触到的数据采用的EDA基本结构可能看起来像这样：
- en: '*Variables distribution testing*—What are the characteristics of the data samples,
    such as mean, standard deviation, range, and so on?'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*变量分布测试*—数据样本的特征是什么，例如均值、标准差、范围等？'
- en: '*Variables visualization*—What can you read from visual representations of
    the data?'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*变量可视化*—你能从数据的视觉表示中读出什么？'
- en: '*Data meaning clarification*—Does the data represent what you think it does?'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*数据含义澄清*—数据是否代表你所认为的含义？'
- en: '*Data mapping and harmonization*—Are all the data points sampled using the
    same units, and do the headings use standardized description vocabulary?'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*数据映射和协调*—所有数据点是否使用相同的单位进行采样，以及标题是否使用标准化的描述词汇？'
- en: Some people add extra steps to EDA, like correlation testing or text analysis.
    Others also add feature engineering. Depending on your data, their approach may
    be more suitable to your needs.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人会在 EDA 中添加额外的步骤，比如相关性测试或文本分析。其他人也会添加特征工程。根据你的数据，他们的方法可能更适合你的需求。
- en: One way from Sunday
  id: totrans-128
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 星期天的另一种方式
- en: 'We do not think a one-size-fits-all, silver-bullet, cure-all data preparation
    protocol exists. The general framework we believe will lead to high quality data
    is as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为并不存在一种通用的、万能的、万能的数据准备协议。我们相信将导致高质量数据的通用框架如下：
- en: Understand the relation of your data with reality (what each variable means
    and how it was generated).
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解你的数据与现实之间的关系（每个变量的含义以及它是如何生成的）。
- en: Ensure the data meets some basic technical standards (that you are working on
    all available *relevant*data points and can cross-relate them between data sources
    if needed).
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保数据符合一些基本的技术标准（即你正在处理所有可用的*相关*数据点，并在需要时可以在数据源之间进行交叉关联）。
- en: Before you analyze reality using your data, analyze the data itself (ensure
    it adheres to either expertise- or common-sense-based expectations).
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你使用数据分析现实之前，先分析数据本身（确保它符合基于专业知识或常识的预期）。
- en: In the following sections, we will walk through an example and ensure the data
    describes phenomena rooted in reality.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下章节中，我们将通过一个示例来演示，并确保数据描述的是基于现实的现象。
- en: 2.5 Data cleaning
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 数据清理
- en: Data cleaning must be done as early as possible, as errors resulting from poor
    data representation, consistency, or integrity are persistent and stack up quickly.
    Unclean data can misguide other data preparation and evaluation steps as well
    as your analytics results.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清理必须尽早进行，因为由于数据表示、一致性或完整性的错误导致的错误是持续的，并且会迅速累积。不干净的数据可能会误导其他数据准备和评估步骤，以及你的分析结果。
- en: 'Let us repeat: We don’t believe there is one best way of cleaning data or performing
    EDA. In your work, you may encounter data cleaning protocols, including steps
    we describe as a part of EDA, and vice versa. You may be given a protocol that
    includes steps we consider analysis. If specialists in their field prepared these
    protocols, there is a chance they are better for the data types they are designed
    for. Be flexible. Just make sure you understand what’s happening.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重复一遍：我们不认为有唯一最佳的数据清理或执行 EDA 的方法。在你的工作中，你可能会遇到数据清理协议，包括我们描述为 EDA 部分的内容，反之亦然。你可能会得到一个包含我们认为是分析的步骤的协议。如果这些协议是由该领域的专家准备的，那么它们可能更适合它们设计的数据类型。要灵活。只需确保你了解正在发生的事情。
- en: Order matters  Some cleaning steps may obfuscate errors, especially in preparatory
    work. It is of great importance to understand how things change after each step.
    For example, if you fill missing data with average values from a certain column,
    you cannot do this before removing duplicates.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序很重要  某些清理步骤可能会掩盖错误，尤其是在准备工作阶段。了解每一步之后事物如何变化非常重要。例如，如果你用某一列的平均值填充缺失数据，那么在删除重复项之前你不能这样做。
- en: We’ll assume the worst-case scenario for this exercise. That’s the scenario
    where you’re unsure what to expect from your data.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将假设这个练习的最坏情况。那就是你不确定从你的数据中可以期待什么的情况。
- en: 2.5.1 Removing duplicates
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.1 删除重复项
- en: There are many ways duplicate records can sneak into your data. From data entry
    mistakes (human errors) and system glitches (software errors) to poorly designed
    identifiers (human error), to synchronization problems in distributed systems
    (software error), to errors in merging datasets, to import issues . . . the list
    goes on. With so many things that can go wrong, some of them most likely will.
    In all honesty, we’re still waiting to see a duplicate-free dataset that was not
    previously curated for sharing.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 重复记录可以以多种方式悄悄进入你的数据。从数据录入错误（人为错误）和系统故障（软件错误）到设计不良的标识符（人为错误），再到分布式系统中的同步问题（软件错误），到合并数据集时的错误，到导入问题……等等。由于可能出现这么多问题，其中一些很可能真的会发生。坦白说，我们还在等待看到没有经过先前整理以供分享的、没有重复项的数据集。
- en: How do you efficiently eliminate duplicate records to avoid overrepresenting
    certain data points, which could skew the analysis results? It depends on your
    analytical environment. Do you need to memorize details? Most certainly not!
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何有效地消除重复记录，以避免过度表示某些数据点，这可能会歪曲分析结果？这取决于你的分析环境。你需要记住细节吗？当然不需要！
- en: '**![image](../Images/Init-MA.png)**How can I remove duplicate entries from
    a DataFrame?'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Init-MA.png)**如何从 DataFrame 中删除重复项？'
- en: '**![image](../Images/Response.png)**To remove duplicate entries from a DataFrame,
    you can use the `drop_duplicates()` method. By default, it considers all columns
    when identifying duplicates and keeps the first occurrence of a duplicate row.
    Here’s an example:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**要从 DataFrame 中删除重复项，可以使用 `drop_duplicates()`
    方法。默认情况下，它在识别重复项时考虑所有列，并保留重复行的第一个出现。以下是一个示例：'
- en: '[PRE11]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'If you want to remove duplicates based on a specific set of columns, you can
    use the `subset` parameter:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要根据一组特定的列删除重复项，可以使用 `subset` 参数：
- en: '[PRE12]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In this example, `df.drop_duplicates(subset=['A'])` removes duplicate rows based
    on the values in column `A` while keeping the first occurrence of each duplicate.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，`df.drop_duplicates(subset=['A'])` 根据列 `A` 中的值删除重复行，同时保留每个重复行的第一个出现。
- en: This answer is technically correct and, under regular circumstances, sufficient.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这个答案在技术上正确，并且在正常情况下是足够的。
- en: Efficiency may not be the top priority, or it may  Depending on your generative
    AI’s specifics, it may consider the answer’s simplicity rather than the resource-effectiveness
    of the solution. Quoting Grace Hopper, *it’s a feature, not a bug*. Chapter 5
    describes methods for using even general-knowledge generative AIs to optimize
    code.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 效率可能不是首要任务，或者它可能……根据你的生成式 AI 的具体细节，它可能会考虑答案的简单性，而不是解决方案的资源效率。引用 Grace Hopper
    的话，*这是一个特性，而不是错误*。第五章描述了使用即使是通用知识生成式 AI 优化代码的方法。
- en: In our case, the answer is sufficient (as we didn’t have any duplicates in the
    first place), so we can triumphantly tick this box and move on to the next step.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们这个例子中，答案足够了（因为我们一开始就没有任何重复项），所以我们可以自豪地勾选这个框，继续下一步。
- en: 2.5.2 Handling missing values
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.2 处理缺失值
- en: 'We will repeat the warning we issued earlier: some data *is* sparse. However,
    a list of the possible causes for missing data entries would be at least as long
    as the one for duplicates. Data entry errors, sensor malfunctions, copying errors,
    integration errors, you name it. Finally, there is the unavailability of information.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重复我们之前发出的警告：一些数据*确实是稀疏的*。然而，缺失数据条目可能的原因列表至少与重复项一样长。数据输入错误、传感器故障、复制错误、集成错误，等等。最后，还有信息不可用。
- en: Depending on the context and the amount of missing data, you can choose to either
    delete the incomplete data entry or variable (the row or column in tabular formats),
    ignore it, or somehow fill in missing values. You could fill empty fields with
    a default value, an estimate (such as a mean, median, or mode), or use interpolation
    methods. It’s critical, however, to make these decisions consciously, depending
    on the meaning and importance of the data. Recently there was a case of a renowned
    macroeconomist filling missing data for some countries by copying them from adjacent
    cells. In that case, the dataset was sorted alphabetically by country names, which
    caused this method to make completely nonsensical adjustments.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上下文和缺失数据的数量，你可以选择删除不完整的数据条目或变量（表格格式中的行或列），忽略它，或者以某种方式填充缺失值。你可以用默认值、估计值（如平均值、中位数或众数）填充空字段，或者使用插值方法。然而，重要的是，要根据数据的意义和重要性有意识地做出这些决定。最近有一个著名宏观经济学家通过从相邻单元格复制来填充一些国家的缺失数据的情况。在这种情况下，数据集是按国家名称字母顺序排序的，这导致这种方法做出了完全不合逻辑的调整。
- en: It is paramount that the decision on what to do about missing values is based
    on understanding why the value is missing. Was it, for example, a sensor system
    error causing a missed measurement? Or was it an error in the system writing down
    the results? Was there an error when multiple data sources were connected together?
    To decide what to do with the missing values, you should first have a look at
    the data.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 决定如何处理缺失值的关键是基于理解为什么值缺失。例如，这是否是由于传感器系统错误导致测量失败？或者是因为记录结果时系统出错？是在将多个数据源连接在一起时出错吗？为了决定如何处理缺失值，你应该首先查看数据。
- en: In our example data, we should notice three DataFrames where the number of non-null
    values differs from the number of rows in some columns (meaning it’s not just
    an empty column). These are `df_orders`, `df_order_reviews`, and `df_products`.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例数据中，我们应该注意到三个数据框，其中某些列的非空值数量与行数不同（这意味着不仅仅是空列）。这些是`df_orders`、`df_order_reviews`和`df_products`。
- en: In the `df_orders` DataFrame, we noticed that some approval and delivery dates
    are missing. We’ll use this DataFrame in section 2.6 to analyze the underlying
    process described by the data and the relation of the order status with the missing
    dates.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在`df_orders`数据框中，我们注意到一些审批和交付日期缺失。我们将在第2.6节中使用这个数据框来分析数据所描述的底层过程以及订单状态与缺失日期之间的关系。
- en: In the case of `df_products`, we are missing some category names, the same number
    of product names, description lengths, and pictures, and some product size data.
    Let’s see if we can say anything by looking at the products where some of the
    values are missing.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在`df_products`的情况下，我们缺少一些类别名称、相同数量的产品名称、描述长度和图片，以及一些产品尺寸数据。让我们看看我们能否通过查看某些值缺失的产品来说明一些问题。
- en: There are 611 rows where some of the data is missing. This would be a chore
    to analyze and address manually, and if we were dealing with 6,111 or 61,111 rows,
    it would become unfeasible. What can we do to automate the process somewhat? Let’s
    start with investigating whether the number of missing values in different columns
    is coincidental or if there is some structure to it.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 有611行数据缺失，手动分析和处理将是一项繁琐的工作，如果我们处理的是6,111行或61,111行，那就变得不可行。我们该如何自动化这个过程呢？让我们先调查一下不同列中缺失值的数量是否是偶然的，或者是否存在某种结构。
- en: There is a lesson here about working with generative AI. If you have any doubts
    about your data but are unsure how to voice them precisely, don’t. We mean, don’t
    voice them *precisely*. Simply voice them as best you can.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个关于与生成式AI合作的教训。如果你对自己的数据有疑问，但又不确定如何精确表达，那就不要说了。我们的意思是，不要精确地表达出来。简单地尽可能清楚地表达出来。
- en: '**![image](../Images/Init-MA.png)**In my DataFrame, I have missing values.
    It looks like the number of missing values in some columns is correlated. How
    can I check if the missing values form a pattern?'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Init-MA.png)**在我的数据框中，我有缺失值。看起来某些列中缺失值的数量似乎相关。我该如何检查缺失值是否形成某种模式？'
- en: '**![image](../Images/Response.png)**You can check if the missing values form
    a pattern by analyzing the co-occurrence of missing values across different columns.
    To do this, you can use pandas to create a DataFrame indicating which values are
    missing and then compute the correlation between these binary indicators. Here’s
    an example:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**你可以通过分析不同列中缺失值的共现情况来检查缺失值是否形成某种模式。为此，你可以使用pandas创建一个数据框，指示哪些值缺失，然后计算这些二元指标之间的相关性。以下是一个示例：'
- en: '[PRE13]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In this example, `df.isna()` returns a DataFrame of the same shape as `df`,
    where each element is a boolean value indicating if the corresponding element
    in `df` is missing (`True`) or not (`False`).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，`df.isna()`返回一个与`df`形状相同的数据框，其中每个元素都是一个布尔值，表示`df`中相应的元素是否缺失（`True`）或未缺失（`False`）。
- en: '`missing_indicator.corr()` computes the pairwise correlation between the columns
    of the missing value indicators. The resulting correlation matrix represents the
    correlation between the missing values across different columns. High positive
    correlation values indicate that the missing values in the corresponding columns
    tend to co-occur.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`missing_indicator.corr()`计算缺失值指标列之间的成对相关性。结果的相关矩阵表示不同列中缺失值之间的相关性。高正相关值表明，相应列中的缺失值倾向于同时出现。'
- en: Finally, the correlation matrix is visualized using a heatmap from the Seaborn
    library. This heatmap can help you quickly identify patterns in missing values,
    such as whether missing values in certain columns tend to coincide with missing
    values in other columns.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用Seaborn库的热图可视化相关矩阵。这个热图可以帮助你快速识别缺失值中的模式，例如，某些列中的缺失值是否倾向于与其他列中的缺失值同时出现。
- en: This time, we got some really heavy artillery, including visualization libraries.
    Skillful visualization is one of the most important skills in data analysis, and
    you shouldn’t be shy about leaning on your generative AI advisor to provide you
    with the tools you need.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们得到了一些真正的重型武器，包括可视化库。熟练的视觉化是数据分析中最重要技能之一，你不应该害羞地依赖你的生成式AI顾问为你提供所需的工具。
- en: 'A picture is worth a thousand words  If you’d like to learn more about the
    importance of proper visualization for statistical analysis, we can’t recommend
    timeless books by Professor Edward Tufte enough. His *Visual Explanations: Images
    and Quantities, Evidence and Narrative* (Graphics Press, 1997) and *The Visual
    Display of Quantitative Information* (Graphics Press, 1997) are technology-agnostic
    bibles of excellent visual communication.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 一图胜千言  如果你想了解更多关于适当可视化对统计分析重要性的信息，我们强烈推荐爱德华·图夫特教授的不朽书籍。他的《视觉解释：图像与数量，证据与叙事》（Graphics
    Press，1997）和《定量信息的视觉表示》（Graphics Press，1997）是优秀的视觉交流的圣经，不受技术限制。
- en: Figure 2.1 shows the results of one of the analyses proposed by generative AI.
    While there are many types of analysis that can be performed, most of the steps
    would be similar, so we’ll just use this one as an example.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1显示了由生成式AI提出的一种分析结果。虽然可以进行许多类型的分析，但大多数步骤将是相似的，所以我们只使用这个作为例子。
- en: '![figure](../Images/CH02_F01_Siwiak3.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F01_Siwiak3.png)'
- en: Figure 2.1 The correlation between missing values in different columns
  id: totrans-170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.1 不同列中缺失值的相关性
- en: We can see that some products have missing values in all of the `product_category
    _name`, `product_name_lenght`, and `product_photos_qty` columns, or in all of
    the `product_weight_g`, `product_length_cm`, `product_height_cm`, and `product_width_cm`
    columns. This means that some products could either be weightless and dimensionless
    (services, perhaps?) or they don’t have any category, product name, or photos
    (a mystery box comes to mind).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，一些产品在`product_category _name`、`product_name_lenght`和`product_photos_qty`列中，或者`product_weight_g`、`product_length_cm`、`product_height_cm`和`product_width_cm`列中都有缺失值。这意味着一些产品可能是无重量和尺寸的（可能是服务？）或者它们没有任何类别、产品名称或照片（想到一个神秘盒子）。
- en: Note  The typo in the variable name `product_name_lenght` is preserved here
    to conform to the naming in the original dataset.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 注释  注释变量名`product_name_lenght`中的错误是为了与原始数据集中的命名保持一致。
- en: 'When we looked for rows in the dataset where every column except for the `product_id`
    was empty, we found just one: `product_id 5eb564652db742ff8f28759cd8 d2652a`.
    As we discussed earlier, before deciding whether to remove or keep such an item,
    we should check the business or data collection process behind such an oddity.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在数据集中寻找除了`product_id`之外所有列都为空的行时，我们只找到了一行：`product_id 5eb564652db742ff8f28759cd8
    d2652a`。正如我们之前讨论的，在决定是否删除或保留这样的项目之前，我们应该检查背后这种异常的业务或数据收集过程。
- en: 'Fraud detection and forensic analysis are outside the scope of this book, but
    following our attempt to instill in you a bit of professional paranoia, we can’t
    completely ignore the fact that this case begs for an investigation. If we were
    to investigate such a case, we would first check the simplest explanation: someone
    was playing with the database and didn’t clean up after themselves. In such a
    case, this `product_id` should be present only in this single table. The `product_id`
    in question, however, seems to be connected to an order. If possible, we’d try
    to find the product’s website and see if it is a service (unlikely, as there is
    just one such case). If we felt especially inquisitive, we’d move to check this
    seller’s other transactions, particularly with the same buyer. In short, we would
    try to understand the actual event and the expected shape of the data describing
    it. From there, we would try to identify how the error could occur in the data
    (if it is, in fact, an error instead of a non-typical but a legitimate situation).'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 欺诈检测和法医分析超出了本书的范围，但鉴于我们试图在你心中灌输一点专业性的偏执，我们不能完全忽视这个案例迫切需要调查的事实。如果我们调查这样的案例，我们首先会检查最简单的解释：有人玩弄数据库而没有清理。在这种情况下，这个`product_id`应该只出现在这个单一表中。然而，这个有问题的`product_id`似乎与一个订单有关。如果可能的话，我们会尝试找到该产品的网站，看看它是否是一个服务（不太可能，因为只有一个这样的案例）。如果我们特别好奇，我们会检查这个卖家的其他交易，特别是与同一买家的交易。简而言之，我们会试图理解实际事件和描述它的数据的预期形状。从那里，我们会尝试确定错误如何在数据中发生（如果它确实是一个错误而不是一个非典型但合法的情况）。
- en: Unfortunately, we now need to take off our Sherlock Holmes caps and get back
    to dealing with a much more prevalent, if not so exciting, category of problem.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们现在需要摘下我们的福尔摩斯帽子，回到处理一个更为普遍、尽管不那么令人兴奋的问题类别。
- en: 2.5.3 Correcting data entry errors
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.3 纠正数据录入错误
- en: Is “Washington” the same thing as “Washington D.C.”? Or did an employee who
    was saving their precious time by typing just “SD” have in mind “Shipment Delayed”
    or “Standard Delivery”? You’ll encounter multiple data entry error types in your
    work, such as typos, mislabeling, or inconsistent formats, all of which negatively
    affect the dataset’s quality. You need to identify and correct these errors to
    ensure accurate and consistent data.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: “华盛顿”和“华盛顿特区”是同一件事吗？或者，一个通过只输入“SD”来节省宝贵时间的员工，心里想的是“发货延迟”还是“标准交付”？在你的工作中，你会遇到多种数据输入错误类型，如拼写错误、标签错误或不一致的格式，所有这些都对数据集的质量产生负面影响。你需要识别和纠正这些错误，以确保数据的准确性和一致性。
- en: This is one of the most strenuous and least automatable parts of data preparation.
    Unfortunately, the best tool you can use to search for such errors is biological
    rather than digital. As of the time we’re writing this book, nothing beats a good
    old-fashioned eyeball. Some of the methods described in section 2.1 can help identify
    the oddities. For example, `df['column_name'].value_counts()` can help you spot
    values that occur only once, which may be suspicious if you expect uniform distribution.
    However, detecting and correcting data entry errors requires a combination of
    these techniques, along with domain knowledge and manual inspection.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这是数据准备中最艰难且最难以自动化的部分之一。不幸的是，你可以用来搜索此类错误的最佳工具是生物的而不是数字的。在我们撰写这本书的时候，没有什么能比得上老式的肉眼检查。第2.1节中描述的一些方法可以帮助识别异常。例如，`df['column_name'].value_counts()`可以帮助你发现只发生一次的值，如果你期望均匀分布，这可能是有疑虑的。然而，检测和纠正数据输入错误需要这些技术的组合，以及领域知识和人工检查。
- en: The specific type of data entry error analysis we want to point out is outlier
    detection and treatment. *Outliers* are data points that deviate significantly
    from the rest of the dataset (for example, when almost all transactions in a set
    are from NY, and suddenly one is from LA). Identifying and handling outliers is
    very important, as they can disproportionately impact the analysis results. Alternatively,
    they can indicate data points of extreme importance. Outliers can be accepted,
    removed, capped, or transformed, depending on the context.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要指出的具体数据输入错误分析类型是异常值检测和处理。*异常值*是显著偏离其余数据集的数据点（例如，当一组中的几乎所有交易都来自纽约时，突然有一个来自洛杉矶）。识别和处理异常值非常重要，因为它们可能会不成比例地影响分析结果。或者，它们可以表明数据点具有极端的重要性。根据上下文，异常值可以被接受、移除、限制或转换。
- en: 2.5.4 Data validation
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.4 数据验证
- en: You’d really rather not find entries like “last Tuesday, around noon” or “it
    was a beautiful spring morning” in the date column of your table. Validating the
    data ensures that the data adheres to specific rules or constraints, such as data
    types, value ranges, or relationships between columns. This is crucial for maintaining
    data integrity and avoiding inconsistencies that could lead to erroneous conclusions.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 你真的不希望在你的表格的日期列中找到像“上周二中午左右”或“那是一个美丽的春天早晨”这样的条目。验证数据确保数据符合特定的规则或约束，如数据类型、值范围或列之间的关系。这对于维护数据完整性和避免可能导致错误结论的不一致性至关重要。
- en: If this seems like a lot of work, that’s because it is. The good news is that
    this is yet another opportunity for you to get intimately familiar with analyzed
    data, and that’s something you can’t fake.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这看起来像是一项大量工作，那是因为确实如此。好消息是，这又是一个让你深入了解分析数据的机会，而且这是你无法伪造的。
- en: 'These are the four steps for validating your data:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 验证数据有四个步骤：
- en: Define expected values for your data.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为你的数据定义期望值。
- en: Check if any bits of data fall outside your expectations.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查是否有任何数据点不符合你的预期。
- en: If any do, do something about it.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果有，就采取行动。
- en: Check the consistency of the data coming from multiple tables.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查来自多个表的数据的一致性。
- en: The specifics of what values are expected will vary, depending on your dataset.
    There are no universal rules for determining expected versus unexpected values;
    that’s where your domain knowledge kicks in. You must remember, however, that
    data points deviating from expectations might signal either errors in data collection
    (worth investigating) or events of significant importance to your business (definitely
    worth investigating). To give you a starting point, we’ll outline three basic
    areas you should consider when validating your data.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 预期值的详细信息将因数据集而异。没有普遍的规则来确定预期值与意外值；这就是你的领域知识发挥作用的地方。然而，你必须记住，偏离预期的数据点可能表明数据收集中的错误（值得调查）或对你业务具有重大重要性的事件（绝对值得调查）。为了给你一个起点，我们将概述在验证数据时应考虑的三个基本领域。
- en: Upfront validation rules and constraints definition
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 事先验证规则和约束定义
- en: We mentioned that the whole analytical protocol should be defined before running
    an analysis, to avoid having to “tweak” the analysis to fit expectations better.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提到，在运行分析之前应该定义整个分析协议，以避免不得不“调整”分析以更好地符合预期。
- en: Validation rules and constraints, indeed, *must* be defined upfront. If you
    scrolled through the files, as we advised you to do at the beginning of section
    2.3, you should have a more or less solid intuition about the structure and distribution
    of variables in your data. You should also study your domain and related data
    enough to know what values are typical and expected. Suppose you don’t know the
    exact acceptable border values for your variables. In that case, you should be
    able to state your expectations in terms of deviations from the mean, percentiles,
    cicada chirps, horse’s lengths, or whatever applies to your data.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 验证规则和约束条件确实必须在事先定义。如果你像我们在2.3节开头建议的那样翻阅了文件，你应该对数据中变量的结构和分布有一个或多或少清晰的认识。你也应该足够了解你的领域和相关数据，以便知道哪些值是典型和预期的。假设你不知道变量确切的可接受边界值。在这种情况下，你应该能够用平均值的偏差、百分位数、蝉鸣声、马的长短或其他适用于你数据的任何东西来表述你的期望。
- en: Now, let’s get off our high horse for a minute. Data quality assurance *is*
    an iterative process. We will perform an EDA, and we may learn that some of the
    assumptions we made after our initial analysis were not sufficiently grounded
    and that our intuition led us astray. In that case, we’ll go back and correct
    our validation rules to better discriminate between valuable data and potential
    artifacts.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们暂时放下我们的高姿态。数据质量保证确实是一个迭代的过程。我们将执行探索性数据分析（EDA），我们可能会发现我们在初步分析后做出的某些假设并没有充分依据，我们的直觉把我们引入歧途。在这种情况下，我们将回去纠正我们的验证规则，以更好地区分有价值的数据和潜在的人工制品。
- en: Data types
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据类型
- en: Assigning correct data types to your variables will make your life much easier
    down the line, as many libraries make operations on columns with well defined
    types (especially numerical ones) easy and efficient. In general, if you want
    to perform an operation like `111-11`, you’d like to get `100`, not `1` or a system
    error (where, as we will repeat throughout this book, the latter would be preferable).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 为你的变量分配正确的数据类型将使你在以后的生活中更加轻松，因为许多库使得对具有良好定义类型（尤其是数值类型）的列进行操作变得容易和高效。一般来说，如果你想执行像`111-11`这样的操作，你希望得到`100`，而不是`1`或系统错误（正如我们将在整本书中重复的那样，后者会更可取）。
- en: When looking at the properties of our example DataFrames revealed by `df_name.dtype`,
    or by calling the `df_name.info()` method, you should notice that pandas successfully
    derived the types of numerical columns. However, it assigned general *object*
    types to columns containing dates.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 当查看由`df_name.dtype`或通过调用`df_name.info()`方法揭示的示例DataFrame属性时，你应该注意到pandas成功推导出了数值列的类型。然而，它将通用*对象*类型分配给了包含日期的列。
- en: 'Time representation in programming is far from straightforward. There are myriad
    methods and formats for noting something as seemingly simple as a timestamp, reflecting
    the complexity of time zones, calendar systems, and coding standards. In Python,
    there is no singular “date” type due to these complexities. Instead, for our datasets,
    we have found that the most suitable format is `datetime64ns`, which offers a
    precise timestamp down to the nanosecond:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在编程中，时间表示远非简单直接。有无数种方法和格式来记录看似简单的时间戳，反映了时区、日历系统和编码标准的复杂性。在Python中，由于这些复杂性，没有单一的“日期”类型。相反，对于我们的数据集，我们发现最合适的格式是`datetime64ns`，它提供了精确到纳秒的时间戳：
- en: '[PRE14]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Analyzing methods for reading different time formats from text may be beyond
    the scope of this book, but it’s surely not beyond the scope of knowledge of your
    GenAI advisor! When in doubt, provide an example of your data, and ask the generative
    AI how to transform it into the format you’ll be using in your pipeline.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 分析从文本中读取不同时间格式的分析方法可能超出了本书的范围，但肯定不会超出你的GenAI顾问的知识范围！如有疑问，请提供你的数据示例，并询问生成式AI如何将其转换为你在管道中使用的格式。
- en: Relationships between columns
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 列之间的关系
- en: We mentioned that we’d seen sales volumes added to physical volumes. How do
    you think that happened? This point is critical if you work with multiple DataFrames
    from different data sources. Blind trust that identical column names mean that
    the two columns describe the same thing may very easily deprive your analysis
    of any functional value.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提到我们看到了销售量被添加到实物量中。你认为这是怎么发生的？如果你与来自不同数据源的多DataFrame一起工作，这一点至关重要。盲目地相信相同的列名意味着两个列描述的是同一件事，可能会很容易地使你的分析失去任何功能性价值。
- en: Before you start coding (or copying) a cross-reference function, you should
    have a good understanding of which of your DataFrames will serve as the reference—the
    source of so-called *primary keys*—and which DataFrames will have *foreign keys*—meaning
    the values reference existing primary keys. This way, you’ll create a lineage
    of DataFrames.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始编写（或复制）交叉引用函数之前，你应该清楚了解哪些DataFrame将作为参考——所谓的主键来源——以及哪些DataFrame将包含外键——意味着这些值引用现有的主键。这样，你将创建一个DataFrame的谱系。
- en: If you have a DataFrame containing the data on all your customers, their identifiers
    become primary keys. Suppose you have another DataFrame describing orders from
    some range of time, and it has information on which customer made an order; all
    these customers’ identifiers will exist in the DataFrame of customers. However,
    there is a possibility that some of the customers in the reference dataset didn’t
    order anything in the timeframe in question, so they don’t need to all be in the
    orders DataFrame.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个包含所有客户数据的DataFrame，他们的标识符就成为了主键。假设你还有一个描述某个时间范围内订单的DataFrame，并且它包含有关哪个客户下了订单的信息；所有这些客户的标识符都将存在于客户DataFrame中。然而，有可能参考数据集中的某些客户在所考虑的时间段内没有下任何订单，因此他们不需要全部出现在订单DataFrame中。
- en: You can use the following code to identify problems with keys within `df_orders`
    and `df_order_payments`.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下代码来识别`df_orders`和`df_order_payments`中键的问题。
- en: Listing 2.2 Identifying foreign key violations
  id: totrans-204
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.2 识别外键违规
- en: '[PRE15]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the course of our work with this example dataset, we’ve already discovered
    that there is a problem with `customer_id`, so the relation between `df_orders`
    and `df_ customers` here is unexpectedly 1:1, but when checking `df_orders` against
    `df_order_payments`, we can see the expected behavior of `order_id` being a primary
    key in `df_orders` and a foreign key in `df_order_payments`. There is one violation,
    but we’ll leave dealing with it as an exercise for you.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们使用这个示例数据集的工作过程中，我们已经发现`customer_id`存在问题，因此`df_orders`和`df_customers`之间的关系出乎意料地是1:1，但在检查`df_orders`与`df_order_payments`时，我们可以看到预期的行为，即`order_id`在`df_orders`中是主键，在`df_order_payments`中是外键。存在一个违规行为，但我们将其留给你作为练习。
- en: Yet again, there is an issue of causality, which you should identify as an analyst.
    By that, we mean identifying the actual cause of an unexpected data structure.
    Any explanation you can come up with should be confirmed with the data creators.
    Of course, sometimes, your ability to confirm things may be limited, and you may
    need to rely on cross-referencing all the DataFrames and checking if there is
    a logical pattern in identifier columns. Still, in cases where you work without
    100% certainty on the data’s meaning, you should clearly communicate to your stakeholders
    the risk of analysis being performed on an unconfirmed dataset.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，有一个因果关系的问题，你应该作为一个分析师来识别。这意味着确定意外数据结构的实际原因。你提出的任何解释都应该与数据创建者进行确认。当然，有时你确认事物的能力可能有限，你可能需要依靠交叉引用所有DataFrame并检查标识符列中是否存在逻辑模式。尽管如此，在你没有100%确定数据含义的情况下工作的情况下，你应该清楚地与你的利益相关者沟通分析在未经确认的数据集上执行的风险。
- en: We now should have our data adequately cleaned. By performing a thorough data
    cleaning, you increase the likelihood of obtaining reliable and accurate insights
    from your data analysis. A clean dataset serves as a strong foundation for the
    subsequent stages of data preprocessing, EDA, and modeling.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在应该已经对我们的数据进行充分清洗。通过进行彻底的数据清洗，您提高从数据分析中获得可靠和准确见解的可能性。一个干净的数据集是数据预处理、EDA 和建模后续阶段的强大基础。
- en: The dataset we use in this book is provided in a modeling-ready state, and as
    such does not require additional cleaning or preprocessing, so we will move directly
    to EDA. In your work, you shouldn’t be so hasty!
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这本书中使用的数据集是以建模就绪的状态提供的，因此不需要额外的清洗或预处理，所以我们将直接进入 EDA。在您的工作中，您不应该如此匆忙！
- en: 2.6 Exploratory data analysis
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.6 探索性数据分析
- en: Now that the data has been sliced and diced, it’s time for some seasoning, mainly
    of you as a data analyst. In most cases, your stakeholders will require simple
    analyses, and they’ll have strong opinions about the visualizations they want.
    You’ll be surprised how often they refuse bleeding-edge interactive, flashy, blinking,
    and sparkling tools and demand good old static charts. And there is nothing wrong
    with that. Your role is to make sure their decisions are informed. If their preferred
    way of acquiring the required information is through one type of chart or another,
    you should respect that in most cases. You’ll need to stomp down your foot, however,
    when you realize that some other data presentation introduces new and required
    information. Pick your battles.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据已经被切割和分解，是时候加入一些调味料了，主要是作为数据分析师的您。在大多数情况下，您的利益相关者将需要简单的分析，并且他们对想要的视觉化有强烈的意见。您可能会惊讶地发现他们经常拒绝前沿的交互式、闪亮的、闪烁的和发光的工具，并要求传统的静态图表。这并没有什么不妥。您的角色是确保他们的决策是有信息的。如果他们获取所需信息的方式是通过一种类型的图表或另一种，您在大多数情况下应该尊重这一点。然而，当您意识到某些其他数据展示方式引入了新的和必需的信息时，您需要坚定立场。选择您的战斗。
- en: Why do we talk about visualization types here? Because you will get many useful
    charts as a side-effect of your EDA. Doing them right at this stage (which means
    they’re useful to your stakeholders) can save you time and effort later. In this
    section you will find some examples of such useful charts.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们在这里讨论可视化类型？因为您将在 EDA 的过程中获得许多有用的图表。在这个阶段正确地进行这些操作（这意味着它们对您的利益相关者有用）可以节省您以后的时间和精力。在本节中，您将找到一些此类有用图表的示例。
- en: In our opinion, EDA is the process that distinguishes great analysts. It’s a
    form of art, linking the meaning of data with its structure. In EDA, you will
    combine statistics, domain knowledge, and visualizations. (You may use your new
    visualization toys, but if you want your stakeholders to adopt them, make sure
    you sometimes show them what you use—luring them generally works better than forcing
    things down their throats.)
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们看来，EDA 是区分优秀分析师的过程。它是一种艺术形式，将数据的含义与其结构联系起来。在 EDA 中，您将结合统计学、领域知识和可视化。（您可以使用您的新可视化玩具，但如果您希望您的利益相关者采用它们，请确保您有时向他们展示您所使用的工具——诱使他们通常比强迫他们吞下东西更有效。）
- en: This mix of required knowledge may seem intimidating, but you now have access
    to a fantastic source of multidisciplinary know-how. Let’s ask generative AI about
    the DataFrame containing order reviews, aptly named `df_order_reviews`, as it
    has an interesting combination of numbers, text, and dates.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这种所需知识的混合可能看起来令人畏惧，但您现在可以访问一个多学科知识的绝佳来源。让我们询问关于包含订单评论的 DataFrame 的生成式 AI，它恰当地命名为
    `df_order_reviews`，因为它具有有趣的数字、文本和日期的组合。
- en: '**![image](../Images/Init-MA.png)**Please suggest EDA techniques for the following
    DataFrame:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Init-MA.png)**请为以下 DataFrame 提出EDA技巧：'
- en: '[PRE16]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**![image](../Images/Response.png)**Based on the provided DataFrame structure
    and content, here are some EDA techniques to better understand and analyze your
    `df_order_reviews` DataFrame:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**基于提供的 DataFrame 结构和内容，以下是一些 EDA 技巧，以更好地理解和分析您的
    `df_order_reviews` DataFrame：'
- en: 'Basic data overview: Use `.head()`, `.tail()`, `.info()`, and `.describe()`
    to get an initial understanding of the DataFrame content and structure.'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基本数据概述：使用 `.head()`、`.tail()`、`.info()` 和 `.describe()` 获取 DataFrame 内容和结构的初步了解。
- en: 'Missing values analysis: Check for missing values in each column using `.isna().sum()`
    and decide how to handle them (drop, fill, or impute).'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 缺失值分析：使用 `.isna().sum()` 检查每一列中的缺失值，并决定如何处理它们（删除、填充或插补）。
- en: 'Review scores distribution: Use `.value_counts()` or a histogram to visualize
    the distribution of review scores.'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 审查分数分布：使用`.value_counts()`或直方图来可视化审查分数的分布。
- en: '[PRE17]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '4\. Reviews over time: Plot the number of reviews created over time using the
    `review_creation_date` column. This can help you identify trends and patterns
    in the review data.'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4. 随时间推移的审查：使用`review_creation_date`列绘制随时间创建的审查数量。这可以帮助你识别审查数据中的趋势和模式。
- en: '[PRE18]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '5\. Review response time: Calculate the response time for each review by subtracting
    the `review_creation_date` from the `review_answer_timestamp`. Analyze the distribution
    of response times and investigate if there is a relationship between response
    time and review scores.'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 5. 审查响应时间：通过从`review_creation_date`中减去`review_answer_timestamp`来计算每个审查的响应时间。分析响应时间的分布，并调查响应时间与审查分数之间是否存在关系。
- en: '[PRE19]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '6\. Text analysis: Explore the text data in the `review_comment_title` and
    `review_comment_message` columns. You can perform sentiment analysis, topic modeling,
    or word frequency analysis to identify common themes, patterns, or issues mentioned
    in the reviews.'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 6. 文本分析：探索`review_comment_title`和`review_comment_message`列中的文本数据。你可以执行情感分析、主题建模或词频分析，以识别审查中提到的常见主题、模式或问题。
- en: '7\. Investigate the unclear column: To understand the `review_answer_timestamp`
    column, you can analyze its relationship with other columns, like `review_creation_date`
    or `review_score`. Plotting scatter plots, calculating correlations, or performing
    other statistical tests can help you identify the purpose of this column.'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 7. 调查不明确的列：为了理解`review_answer_timestamp`列，你可以分析它与其他列的关系，例如`review_creation_date`或`review_score`。绘制散点图、计算相关性或执行其他统计测试可以帮助你确定此列的目的。
- en: '8\. Orders with multiple reviews: Check if there are any orders with multiple
    reviews. This could indicate data entry errors or provide insight into the review
    process.'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 8. 具有多个审查的订单：检查是否存在具有多个审查的订单。这可能会表明数据输入错误或提供对审查过程的见解。
- en: '[PRE20]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: These EDA techniques can help you gain a better understanding of the `df_order_reviews`
    DataFrame and identify patterns, trends, and potential issues in the data. Depending
    on your specific objectives and domain knowledge, you may need to adapt or extend
    these techniques to suit your needs.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这些EDA技术可以帮助你更好地理解`df_order_reviews`数据框，并识别数据中的模式、趋势和潜在问题。根据你的具体目标和领域知识，你可能需要调整或扩展这些技术以满足你的需求。
- en: The preceding answer comes from ChatGPT-v4, as it was the most comprehensive.
    It developed one solid EDA plan, touching upon multiple topics at once. It shows
    how different levels of analysis overlap with each other. You shouldn’t be surprised
    to see some of the steps we performed in the data-cleaning phase included here.
    Other steps, in our opinion, are elements of the data analysis itself. As we mentioned,
    there is no gold standard for EDA, and generative AI often provides the broadest
    possible answer.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 上述答案来自ChatGPT-v4，因为它是最全面的。它制定了一个坚实的EDA计划，一次涉及多个主题。它展示了不同分析层次之间的重叠。你看到我们在数据清理阶段执行的一些步骤并不奇怪。在我们看来，其他步骤是数据分析本身的元素。正如我们提到的，没有EDA的黄金标准，生成式AI通常提供最广泛的答案。
- en: 2.6.1 Reviewing score distribution
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6.1 审查分数分布
- en: Let’s get back to our coin-flip gambling problem for a second. If you recall,
    at the beginning of this chapter, we said, “. . . if you throw the coin multiple
    times and get your expected tails each time, the coin may not be random after
    all.” Before betting on a coin, you most definitely should check what the distribution
    of results is. If it’s really random, you most likely should not bet on it at
    all, but if it’s not random, you can use it to your advantage. You can think about
    reviewing the score distribution as you would about testing multidimensional coins.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂时回到我们的抛硬币赌博问题。如果你还记得，在本章开头，我们说，“……如果你多次抛硬币并且每次都得到预期的反面，那么这枚硬币可能根本不是随机的。”在投注硬币之前，你绝对应该检查结果分布。如果它是真的随机，你很可能根本不应该投注它，但如果它不是随机的，你可以利用它来获得优势。你可以像测试多维硬币一样思考审查分数分布。
- en: The importance of score distribution cannot be overemphasized, as distribution
    analysis can reveal potential issues that may impact the study’s validity, such
    as data entry errors or biases. Visualizing the distribution through histograms,
    box plots, or density plots can also help to determine whether the data follows
    a specific pattern or conforms to a known statistical distribution. This knowledge
    can help you select appropriate statistical tests, predictive models, or data
    transformation techniques to ensure accurate and reliable results.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 分数分布的重要性不容忽视，因为分布分析可以揭示可能影响研究有效性的潜在问题，例如数据输入错误或偏差。通过直方图、箱线图或密度图可视化分布还可以帮助确定数据是否遵循特定模式或符合已知的统计分布。这种知识可以帮助你选择合适的统计测试、预测模型或数据转换技术，以确保准确可靠的结果。
- en: Reviewing the distribution typically involves comparing the observed data distribution
    with a theoretical distribution (for example, normal, exponential, or binomial)
    to assess how well the data conforms to expected patterns. You have multiple distribution
    testing methods at your disposal, including visual inspection with histograms
    or Q-Q plots, or more quantitative measures like the Kolmogorov-Smirnov test,
    the Shapiro-Wilk test for normality, or the Anderson-Darling test. Each method
    has its strengths and particularities, making it more or less suitable depending
    on the size of the dataset and the assumptions you’re willing to make.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 检查分布通常涉及将观察到的数据分布与理论分布（例如，正态、指数或二项）进行比较，以评估数据与预期模式的一致性。你有多种分布测试方法可供选择，包括使用直方图或Q-Q图进行视觉检查，或更量化的指标，如Kolmogorov-Smirnov测试、用于正态性的Shapiro-Wilk测试或Anderson-Darling测试。每种方法都有其优势和特点，根据数据集的大小和愿意做出的假设，其适用性有所不同。
- en: 'If this task seems complex, that’s because it used to be. Now it’s as simple
    as typing:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个任务看起来很复杂，那是因为它曾经是这样的。现在它就像输入以下内容一样简单：
- en: '**![image](../Images/Init-MA.png)**How can I test what distribution does my
    dataset fit best?'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '**![图片](../Images/Init-MA.png)**我该如何测试我的数据集最适合哪种分布？'
- en: If you’re looking for a more specific answer, you can add details about your
    analytical environment, data source, or an intuition you want to examine (note
    that we didn’t use the word “confirm”). From our experience, ChatGPT-4 beats all
    the other tools in its statistical prowess.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要一个更具体的答案，你可以添加有关你的分析环境、数据源或你想要检验的直觉的详细信息（注意，我们没有使用“证实”这个词）。根据我们的经验，ChatGPT-4在统计能力上优于其他所有工具。
- en: Let’s look at some popular distributions and where you may expect them. The
    shapes of commonly encountered distributions are presented in figure 2.2.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些流行的分布以及你可能在哪些地方期望它们。常见分布的形状在图2.2中展示。
- en: '![figure](../Images/CH02_F02_Siwiak3.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH02_F02_Siwiak3.png)'
- en: Figure 2.2 Commonly encountered data distributions
  id: totrans-241
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.2 常见的数据分布
- en: Normal distribution
  id: totrans-242
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 正态分布
- en: The normal distribution is also known as Gaussian distribution or “bell curve.”
    It’s often the first assumption for continuous data, due to the central limit
    theorem, which posits that the mean of a sufficiently large number of independent
    random variables, each with finite mean and variance, will be approximately normally
    distributed, regardless of the underlying distribution. Be careful, though. Some
    statistical tests rely heavily on the assumption of the normality of the data
    distribution. If your data is skewed, your test results may get screwed as a result.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 正态分布也称为高斯分布或“钟形曲线”。由于中心极限定理，它通常是连续数据的第一个假设，该定理认为，足够大数量的独立随机变量的平均值，每个随机变量都有有限的平均值和方差，将大致呈正态分布，无论其基本分布如何。但要注意，一些统计测试高度依赖于数据分布的正态性假设。如果你的数据是偏斜的，你的测试结果可能会受到影响。
- en: The normal distribution is particularly relevant for metrics like sales figures,
    heights of individuals, and errors in measurements.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 正态分布对于销售数字、个人身高和测量误差等指标特别相关。
- en: Binomial distribution
  id: totrans-245
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 二项分布
- en: The binomial distribution is used for binary outcomes in data, like coin flipping.
    This distribution is relevant when dealing with success/failure, yes/no, or 1/0
    types of outcomes in data, such as conversion rates in A/B testing, pass/fail
    rates in quality control, or click-through rates in online advertising.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 二项分布用于数据中的二元结果，如抛硬币。当处理成功/失败、是/否或1/0类型的结果时，例如A/B测试中的转化率、质量控制中的通过/失败率或在线广告中的点击率，这种分布是相关的。
- en: 'Seeing more than two bars in a binomial distribution chart often leads to confusion,
    so let’s try to dispel that right away. The binomial distribution describes the
    number of successes in a fixed number of independent Bernoulli trials, each with
    the same probability of success. When you see a binomial distribution with more
    than two bins, it does not represent just a single trial (which would result in
    only two outcomes: success or failure, 1 or 0). Instead, it represents the outcomes
    of multiple trials, so it can have more than two bins.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在二项分布图中看到超过两个条形图常常会导致混淆，所以让我们立即消除这种困惑。二项分布描述了在固定数量的独立伯努利试验中成功的次数，每个试验的成功概率相同。当你看到超过两个箱子的二项分布时，它并不只代表一个试验（这将只产生两种结果：成功或失败，1或0）。相反，它代表多个试验的结果，因此可以有超过两个箱子。
- en: 'For example, suppose you analyze the binomial distribution of *n*=10 different
    trials. In that case, the distribution doesn’t show just two outcomes but the
    probability of achieving 0, 1, 2, ..., up to 10 successes out of those 10 trials.
    Each “bar” in this context represents the number of successes: one bar for 0 successes,
    another for 1 success, and so on, up to 10 successes. So while each individual
    trial has a binary outcome, the'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你分析了*n*=10个不同试验的二项分布。在这种情况下，分布不仅显示两种结果，还显示了在10次试验中实现0、1、2、...，直到10次成功的概率。在这个背景下，“条形”代表成功的次数：一个条形代表0次成功，另一个条形代表1次成功，以此类推，直到10次成功。所以虽然每个单独的试验都有二元结果，但
- en: aggregate result across multiple trials can range from 0 successes (all failures)
    to *n* successes (all successes), leading to *n*+1 possible outcomes. Hence the
    number of bars in the distribution’s representation.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 多个试验的汇总结果可以从0次成功（全部失败）到*n*次成功（全部成功）不等，导致*n*+1种可能的结果。因此，分布表示中的条形数量。
- en: Uniform distribution
  id: totrans-250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 均匀分布
- en: A uniform distribution assumes that all outcomes are equally likely. It’s frequently
    a starting assumption for simulations or when modeling unbiased random variables,
    such as random digits, the simulation of equal-probability scenarios, or when
    no prior information is available.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 均匀分布假设所有结果的可能性相同。它通常是模拟或建模无偏随机变量（如随机数字、等概率场景的模拟或没有先验信息时）的起始假设。
- en: While this distribution is less common in raw business data, it’s worth remembering.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种分布在实际业务数据中不太常见，但值得记住。
- en: Poisson distribution
  id: totrans-253
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 泊松分布
- en: The Poisson distribution applies to count data where occurrences happen independently
    over a constant rate in space or time. It’s commonly used for modeling the number
    of times an event occurs in a fixed interval of time or space.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 泊松分布适用于计数数据，其中事件在空间或时间上以恒定的速率独立发生。它通常用于模拟在固定时间或空间间隔内事件发生的次数。
- en: You may encounter this distribution when analyzing the number of customer arrivals
    per hour, daily demand for a product, or the number of failures of a system.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会在分析每小时客户到达次数、产品的日需求量或系统的故障次数时遇到这种分布。
- en: Exponential distribution
  id: totrans-256
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 指数分布
- en: An exponential distribution is often used to model the time between events in
    a Poisson process, representing the time until an event occurs.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 指数分布通常用于模拟泊松过程中的事件之间的时间，表示事件发生的时间。
- en: Look for this distribution when working with the lifespan of machinery before
    failure, the time until a customer’s next purchase, or the inter-arrival times
    in a queuing system.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理机器在故障前的工作寿命、客户下一次购买的时间或排队系统中的到达时间间隔时寻找这种分布。
- en: Log-normal distribution
  id: totrans-259
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对数正态分布
- en: The log-normal distribution comes into play when dealing with variables that
    are the product of many independent random variables. It’s used when data cannot
    be negative and the distribution is skewed right.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 对数正态分布在处理许多独立随机变量的乘积变量时发挥作用。当数据不能为负且分布右偏时使用。
- en: You’ll encounter this distribution in datasets concerning stock or real estate
    property prices or income distributions.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 你会在涉及股票或房地产价格或收入分布的数据集中遇到这种分布。
- en: Beta distribution
  id: totrans-262
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 贝塔分布
- en: The beta distribution is useful in modeling variables that are bounded on both
    sides, such as proportions and percentages. It’s flexible and can assume a variety
    of shapes, making it suitable for modeling the behavior of random variables limited
    to intervals.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 贝塔分布用于建模两侧有界的变量，如比例和百分比。它灵活且可以假设多种形状，使其适合于建模限制在区间内的随机变量的行为。
- en: This distribution appears when the data represents conversion rates with values
    between 0 and 1.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分布出现在数据表示转换率，其值介于0和1之间时。
- en: Gamma distribution
  id: totrans-265
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 伽马分布
- en: The gamma distribution is related to the exponential distribution and is useful
    for modeling the wait time for the *n*-th event in a Poisson process.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 伽马分布与指数分布相关，并且对于建模泊松过程中的第*n*个事件的等待时间很有用。
- en: You’ll need the gamma distribution when modeling things such as the amount of
    rainfall accumulated in a reservoir, service times in systems, or the reliability
    of systems over a continuous scale.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 当你建模诸如水库中积累的降雨量、系统中的服务时间或系统在连续尺度上的可靠性等事物时，你需要使用伽马分布。
- en: NOTE  Can multimodal generative AI be used to identify distribution type from
    a chart? At the time of writing this book, no. We tried to feed parts of figure
    2.2 to different generative AIs, and their answers were often as diverse as misleading.
    However, generative AIs can still guide you through proper testing procedures.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：能否使用多模态生成式AI从图表中识别分布类型？在撰写本书时，还不能。我们尝试将图2.2的部分内容输入到不同的生成式AI中，它们的答案往往像误导一样多样化。然而，生成式AI仍然可以指导你通过适当的测试程序。
- en: 2.6.2 Time series exploration
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6.2 时间序列探索
- en: Now we enter a grey zone. You may encounter discussions on analyzing changes
    over time as a part of EDA or as a part of in-depth analysis itself. On the one
    hand, visualizing all the time series is a part of EDA. Sometimes this will allow
    you to identify outliers or errors requiring cleanup. On the other hand, these
    outliers could encode significant issues or events in business processes.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们进入了一个灰色地带。你可能会遇到关于将分析随时间的变化作为EDA的一部分或作为深入分析本身的讨论。一方面，可视化所有时间序列是EDA的一部分。有时这会让你能够识别出需要清理的异常值或错误。另一方面，这些异常值可能编码了业务流程中的重大问题或事件。
- en: We conclude that distinguishing between EDA and in-depth business analysis is
    a false dichotomy. EDA *is* an integral part of in-depth analysis. It’s not something
    you want someone else to do for you. Nor is EDA a once-off event—you will be coming
    back to it now and then.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得出结论，区分EDA和深入业务分析是一种错误的对立。EDA确实是深入分析的一个组成部分。这不是你希望别人为你做的事情。EDA也不是一次性的活动——你将不时地回到它。
- en: From the process management perspective, it’s clearer when steps related to
    evaluating the data’s technical and structural properties are separate from those
    related to discovering business-related insights. However, the two are too closely
    related to be wholly compartmentalized.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 从流程管理角度来看，当与评估数据的技术和结构属性相关的步骤与发现与业务相关的见解相关的步骤分开时，情况更清晰。然而，这两个方面过于紧密相关，不能完全分开。
- en: There are two tests that we propose to run at the first EDA iteration. First,
    we’ll visualize and look at the data to search for outliers; second, we’ll perform
    cyclicity detection.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议在第一次EDA迭代时运行两个测试。首先，我们将可视化并查看数据以寻找异常值；其次，我们将执行周期性检测。
- en: 'Changes in the number of responses: Quick data visualization'
  id: totrans-274
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 响应数量的变化：快速数据可视化
- en: We’ve seen situations where the success attributed to a project was, in fact,
    caused by the change in the overall market or the performance of another business
    function. Taking a good look at how different variables change over time can be
    very informative indeed!
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到过这样的情况，一个项目被归功于成功，实际上是由整体市场变化或另一个业务功能的性能所引起的。仔细观察不同变量随时间的变化确实可以提供非常有价值的信息！
- en: This is a very basic yet important task, which should be performed on most of
    your date-related variables. Most business activities have a continuous character,
    and analyzing their performance over time is critical for understanding those
    activities.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常基础但重要的任务，应该在大多数与数据相关的变量上执行。大多数商业活动具有连续性，分析它们随时间的变化对于理解这些活动至关重要。
- en: Figure 2.3 presents the number of reviews recorded in our dataset per month.
    We can clearly see three distinct phases. The first one lasted from October 2016
    to January 2017\. The almost non-existent number of reviews suggests that either
    the business operations or review collecting system were just getting traction.
    Then, from February 2017 to November 2017 (or May 2018, really), we can see a
    phase of rapid growth in the number of reviews collected monthly. Then, from December
    2017 the system stabilized and started fluctuating, or from June 2018 it had a
    moment of collapse and rebounded in August 2018.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3展示了我们数据集中每月记录的评论数量。我们可以清楚地看到三个不同的阶段。第一个阶段从2016年10月持续到2017年1月。几乎不存在的评论数量表明，业务运营或评论收集系统可能刚刚开始起步。然后，从2017年2月到2017年11月（或2018年5月），我们可以看到每月收集的评论数量快速增长阶段。然后，从2017年12月开始，系统稳定并开始波动，或者从2018年6月开始，它在2018年8月经历了一次崩溃和反弹。
- en: '![figure](../Images/CH02_F03_Siwiak3.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F03_Siwiak3.png)'
- en: Figure 2.3 The number of reviews over time—monthly aggregation
  id: totrans-279
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.3 评论数量随时间变化——按月汇总
- en: This information could be crucial if we were to quantitatively compare metrics
    involving the number of reviews between different points in time. Now that we’ve
    checked the number of reviews, let’s move on to checking the scores associated
    with reviews.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要定量比较不同时间点涉及评论数量的指标，这些信息可能是至关重要的。现在我们已经检查了评论数量，让我们继续检查与评论相关的分数。
- en: Changes in review scores over time
  id: totrans-281
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 随时间变化的评论分数变化
- en: Some data is expected to be cyclic, and other data, not so much. Discovering
    and describing cycles in data can help you identify whether the dataset you’ve
    provided is relevant to explaining the related business processes.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 一些数据预计将是周期性的，而其他数据则不是那么明显。发现和描述数据中的周期可以帮助你确定你提供的数据集是否与解释相关业务流程相关。
- en: The following recording of our conversation with the AI advisor proves our point
    from chapter 1, related to the limitations of embedding generative AI directly
    into your analytics pipeline in a shoot-and-forget manner.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们与AI顾问的以下对话记录证明了第一章中关于将生成式AI直接嵌入到分析管道中并以射击和忘记的方式进行限制的观点。
- en: '**![image](../Images/Init-MA.png)**How can I detect internal cyclicity in data?'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Init-MA.png)**我该如何检测数据中的内部周期性？'
- en: '**![image](../Images/Response.png)**Detecting internal cyclicity in data involves
    identifying recurring patterns or periodic fluctuations over time. Time series
    data is a common example where cyclicity might be present. Here are some techniques
    to detect cyclicity in your data:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**检测数据中的内部周期性涉及识别随时间重复出现的模式或周期性波动。时间序列数据是可能存在周期性的常见例子。以下是一些检测数据中周期性的技术：'
- en: 'Visual inspection: Plot the data over time to visually inspect if there’s any
    noticeable cyclical pattern. This can be a quick and simple way to identify obvious
    patterns.'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 视觉检查：将数据随时间绘制出来，以视觉检查是否存在任何明显的周期性模式。这可能是一种快速简单的方法来识别明显的模式。
- en: '[PRE21]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '2\. Seasonal decomposition: Decompose the time series data into its trend,
    seasonal, and residual components using methods like classical decomposition or
    Seasonal and Trend decomposition using Loess (STL). This can help you observe
    the seasonal component directly.'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2. 季节性分解：使用经典分解或季节性和趋势分解使用局部加权回归（STL）等方法将时间序列数据分解为其趋势、季节性和残差成分。这可以帮助你直接观察到季节性成分。
- en: '[PRE22]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '3\. Autocorrelation and partial autocorrelation: Plot the autocorrelation function
    (ACF) and partial autocorrelation function (PACF) to identify any significant
    lags that indicate cyclical patterns.'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3. 自相关和偏自相关：绘制自相关函数（ACF）和偏自相关函数（PACF），以识别任何表明周期性模式的显著滞后。
- en: '[PRE23]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '4\. Spectral analysis: Perform a spectral analysis using the Fast Fourier Transform
    (FFT) or Lomb-Scargle Periodogram to identify dominant frequencies in the data,
    which can correspond to cyclic patterns.'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4. 频谱分析：使用快速傅里叶变换（FFT）或Lomb-Scargle周期图进行频谱分析，以识别数据中的主导频率，这些频率可能对应周期性模式。
- en: '[PRE24]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: These techniques can help you detect internal cyclicity in your data. Depending
    on the characteristics of your data and the nature of the cyclicity, some methods
    may be more appropriate than others. It’s essential to understand the underlying
    assumptions and limitations of each technique and to choose the one that best
    fits your specific data and objectives.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术可以帮助你检测数据中的内部周期性。根据你数据的特性和周期性的性质，某些方法可能比其他方法更合适。理解每种技术的潜在假设和限制，并选择最适合你具体数据和目标的方法是至关重要的。
- en: The code provided by generative AI sometimes differs from expectations. Three
    of the four pieces of code we received are dysfunctional (and are therefore censored),
    and the other . . . Well, look at figure 2.4 to see the resulting visualization
    of the relationship between the score and the time of the review.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI提供的代码有时与预期不符。我们收到的四段代码中有三段是无效的（因此被屏蔽），另一段……好吧，看看图2.4，看看得分与评论时间之间关系的结果可视化。
- en: We believe that some people gifted with an affinity for postmodern art could
    find an application for the resulting chart. In business analytics, however, you
    aim to get something a tad more readable.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为，一些对后现代艺术有天赋的人可能会在生成的图表中找到应用。然而，在商业分析中，你希望得到一些更易读的内容。
- en: Let’s get back to the dysfunctional code, as there is a lesson to be learned
    here. The FFT and Lomb-Scargle methods didn’t work due to problems with data types.
    So far, we’ve advocated that you present your generative AI advisor with general
    queries to confirm your expertise or expand your horizons. However, sometimes
    you’ll encounter problems resulting from false assumptions that generative AI
    may make related to the data types available for analysis. That doesn’t mean the
    answer is useless. It means you’ll have to change gears.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到无效的代码，因为这里有一个教训可以吸取。FFT和Lomb-Scargle方法由于数据类型问题而无法工作。到目前为止，我们一直建议你向生成式AI顾问提出一般性查询，以确认你的专业知识或拓宽你的视野。然而，有时你会遇到由于生成式AI可能对可用于分析的数据类型做出的错误假设而引起的问题。这并不意味着答案无用。这意味着你需要改变策略。
- en: '![figure](../Images/CH02_F04_Siwiak3.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F04_Siwiak3.png)'
- en: Figure 2.4 The relationship between the score and the time of the review
  id: totrans-299
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.4 评论得分与时间的关系
- en: Variables won’t, constants aren’t  Generative AIs are language models. They
    operate on a general set of knowledge of mind-boggling scope. Expecting them to
    correctly assume the alignment of data type requirements for some advanced methods
    with our specific data may be overly optimistic.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 变量不会，常数不是  生成式AI是语言模型。它们在令人难以置信的广泛知识体系上运行。期望它们能够正确地假设某些高级方法的数据类型要求与我们的具体数据相匹配，可能是过于乐观了。
- en: As the general inquiry failed, let’s try to be more specific.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 由于一般性查询失败，让我们尝试更加具体。
- en: '**![image](../Images/Init-MA.png)**Values in my data are categorical, with
    possible values of 1, 2, 3, 4, 5\. They are review scores given to orders by customers.
    Can you propose a method to detect if there are any cyclical dependencies of the
    score in time?'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Init-MA.png)**我的数据中的值是分类的，可能的值为1、2、3、4、5。它们是客户对订单给出的评论得分。你能提出一种方法来检测得分在时间上是否存在任何周期性依赖关系吗？'
- en: '**![image](../Images/Response.png)**Since your data consists of categorical
    values (review scores), you can analyze cyclical dependencies by examining the
    distribution of scores over different time periods. One approach is to use aggregated
    bar plots to visualize the score distribution over time.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**由于你的数据由分类值（评论得分）组成，你可以通过检查不同时间段的得分分布来分析周期性依赖关系。一种方法是使用汇总条形图来可视化得分随时间的变化分布。'
- en: 'Here’s a step-by-step guide to analyze cyclical dependencies:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是分析周期性依赖关系的分步指南：
- en: Convert the timestamp to a more suitable frequency (e.g., daily, weekly, or
    monthly), depending on the length and resolution of your data.
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据你数据的长度和分辨率，将时间戳转换为更合适的频率（例如，每日、每周或每月）。
- en: '[PRE25]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 2\. Aggregate the scores and calculate the proportions for each score in each
    time period.
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2. 对每个时间段的每个得分进行汇总，并计算每个得分在各个时间段的占比。
- en: '[PRE26]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 3\. Create a stacked bar plot to visualize the proportions of scores over time.
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3. 创建堆叠条形图来可视化得分随时间的占比。
- en: '[PRE27]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Examine the stacked bar plot for any apparent cyclical patterns. If you observe
    any regular fluctuations in the proportions of different scores, there might be
    a cyclical dependency in the data.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 检查堆叠条形图中的任何明显的周期性模式。如果你观察到不同得分的占比有规律波动，那么数据中可能存在周期性依赖关系。
- en: Please note that the choice of aggregation frequency (daily, weekly, or monthly)
    will impact the granularity of the analysis. You might need to experiment with
    different frequencies to find the most suitable one for your data.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，聚合频率（每日、每周或每月）的选择将影响分析的粒度。您可能需要尝试不同的频率，以找到最适合您数据的最合适的频率。
- en: As we used web-based generative AI, it even remembered the name of our DataFrame
    in the conversation context, so no code adjustment in this regard is needed. It’s
    equally as awesome as it may be dangerous. All of its assumptions should be taken
    with a pinch of salt and noted down, as they often may turn out to be a reason
    why some of the steps don’t work as expected.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用了基于网络的生成式AI，它甚至记得在对话上下文中我们的DataFrame的名称，因此在这方面不需要进行代码调整。这既令人惊叹，也可能存在危险。所有这些假设都应该带着一点怀疑态度并记录下来，因为它们往往可能是某些步骤无法按预期工作的原因。
- en: Figure 2.5 presents the result of the proposed code. As you can see, the initially
    proposed frequency does not work well with our data. However, there is no obvious
    way of changing it. Does that mean we need to change the tools—revert to Excel
    and pivot tables or find something with a nice, big “change frequency” button?
    Or should we spend another week searching the matplotlib documentation?
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5展示了所提出代码的结果。正如您所看到的，最初提出的频率与我们的数据配合得并不好。然而，没有明显的方法可以改变它。这意味着我们需要改变工具——回归Excel和交叉表，或者找到带有漂亮的大“更改频率”按钮的东西？或者我们应该再花一周时间搜索matplotlib文档？
- en: '![figure](../Images/CH02_F05_Siwiak3.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F05_Siwiak3.png)'
- en: Figure 2.5 Order scores distribution over time—daily aggregation
  id: totrans-316
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.5 按时间顺序展示订单分数分布——每日聚合
- en: Maybe not. This is the brilliance of generative AI in action. You encounter
    a problem, you mention it to your AI advisor, and . . .
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 也许不是。这是生成式AI的精彩之处。你遇到一个问题，你向你的AI顾问提到它，然后……
- en: '**![image](../Images/Init-MA.png)**How would I change the frequency to monthly?'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Init-MA.png)**我该如何将频率改为每月一次？'
- en: '**![image](../Images/Response.png)**To change the frequency to monthly, you’ll
    need to resample your data accordingly. Here’s how you can modify the code to
    aggregate the data on a monthly basis:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**要将频率改为每月一次，您需要相应地重新采样您的数据。以下是您如何修改代码以按月聚合数据的示例：'
- en: Convert the timestamp to a more suitable frequency (e.g., monthly).
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将时间戳转换为更合适的频率（例如，每月）。
- en: '[PRE28]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 2\. Aggregate the scores and calculate the proportions for each score in each
    time period.
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2. 对分数进行聚合，并计算每个时间段的每个分数的比例。
- en: '[PRE29]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 3\. Create a stacked bar plot to visualize the proportions of scores over time.
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3. 创建一个堆叠条形图来可视化分数随时间的变化比例。
- en: '[PRE30]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Now, the analysis will be performed at a monthly level, which can help reveal
    long-term cyclical patterns in the data. Feel free to adjust the frequency according
    to your requirements and the granularity of your data.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，分析将在月度级别进行，这有助于揭示数据中的长期周期性模式。请根据您的需求和数据的粒度自由调整频率。
- en: . . . there is a high chance that generative AI is capable of solving it. Figure
    2.6 presents the distribution of review scores over time, aggregated by month.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: ……有很大可能性生成式AI能够解决这个问题。图2.6展示了按月聚合的评分随时间分布。
- en: We can see that there was a spike in low scores in November and December 2016,
    and maybe, just maybe, there is a cyclic rise in low scores in the spring, but
    the signal is so weak that we won’t dig much deeper into it unless we want to
    run some precise predictive analysis.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，2016年11月和12月低分有显著上升，也许，只是也许，春季低分有周期性上升，但信号如此微弱，除非我们想要运行一些精确的预测分析，否则我们不会深入挖掘。
- en: We can safely conclude from this section that generative AIs may make mistakes,
    but they can still help you write and modify code, such as for time series analysis,
    and they are not shy of using really advanced libraries and algorithms (Lomb-Scargle
    is an algorithm developed by biologists to analyze biological cycles, later adopted
    by astronomers to search for patterns in star brightness observations). We believe
    that with a little patience and an iterative approach to clarifying the method
    requirements and your data properties, generative AI will become the greatest
    facilitator of your analytical processes.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以安全地得出结论，从这一部分来看，生成式AI可能会犯错误，但它们仍然可以帮助你编写和修改代码，例如进行时间序列分析，并且它们并不羞于使用真正先进的库和算法（Lomb-Scargle是一种由生物学家开发的用于分析生物周期的算法，后来被天文学家采用来寻找星光观测中的模式）。我们相信，通过一点耐心和迭代的方法来明确方法要求和你的数据属性，生成式AI将成为你分析过程的最大促进者。
- en: '![figure](../Images/CH02_F06_Siwiak3.png)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH02_F06_Siwiak3.png)'
- en: Figure 2.6 *Order scores distribution over time—monthly aggregation*
  id: totrans-331
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.6 *随时间变化的订单分数分布—月度汇总*
- en: '*### 2.6.3 Mysterious variable investigation'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '*### 2.6.3 神秘变量调查'
- en: If mysterious variables don’t already plague your days as a data analyst, you
    can expect they will. We mentioned, and will again, that nothing beats solid metadata,
    but things like thorough documentation data pipelines are considered a luxury
    in these times of ever-increasing time pressure. Therefore, some of your data
    may come from a poorly documented system. As much as we discourage using unconfirmed
    data for any reason other than warming your server room, sometimes it is possible
    to deduce the meaning of some variables to a satisfying level of certainty.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 如果神秘变量还没有成为你作为数据分析师日常生活中的困扰，那么你可能会期待它们的出现。我们提到过，并且还会再次强调，没有任何东西能比坚实的元数据更有效，但像详尽的文档和数据管道这样的东西，在日益增加的时间压力下被视为一种奢侈。因此，你的一些数据可能来自文档记录不佳的系统。尽管我们强烈反对出于任何原因（除了为你的服务器室供暖）使用未经证实的数据，但有时可以推断出一些变量的含义，达到令人满意的确定性水平。
- en: To show how you can deal with an example of a mysterious variable, let’s feign
    ignorance as to the meaning of one of the columns in our example data. Let’s try
    to get some insight into the `review_answer_timestamp` column. With our Sherlock
    cap on, let’s see if there is any other column could be connected to it. Using
    intuition alone, we’ll check the relation with `review_creation_date`. Both columns
    relate to reviews and, in some form, to a time.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示如何处理一个神秘变量的例子，让我们假装对我们示例数据中的一列的含义一无所知。让我们尝试深入了解`review_answer_timestamp`列。戴上我们的福尔摩斯帽子，让我们看看是否有其他列可能与它相关。仅凭直觉，我们将检查它与`review_creation_date`的关系。这两个列都与评论相关，并且在某种形式上与时间相关。
- en: But what does it mean for two columns to be related? A knee-jerk reaction could
    be to check for a correlation, but this could be a risky proposition in the case
    of time series. Correlation measures the linear relationship between two variables,
    assuming that this relationship is consistent across the dataset. However, this
    assumption often doesn’t hold in time series, data due to factors like autocorrelation,
    trends, seasonality, and external shocks. *Autocorrelation* refers to the correlation
    of a signal with a delayed version of itself, a common characteristic of time
    series data that can inflate correlation measurements between two time series.
    Similarly, *trends* and *seasonality* can create spurious correlations that do
    not reflect a true causal relationship but rather the presence of underlying patterns
    that both series share. Moreover, *external shocks* impacting both series can
    temporarily increase correlation without indicating a meaningful or lasting relationship.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，两个列相关意味着什么呢？一种本能的反应可能是检查相关性，但在时间序列的情况下，这可能会是一个风险很大的提议。相关性衡量两个变量之间的线性关系，假设这种关系在整个数据集中是一致的。然而，这种假设在时间序列数据中往往不成立，因为自相关、趋势、季节性和外部冲击等因素。*自相关*指的是信号与其延迟版本的关联，这是时间序列数据的一个常见特征，可能会夸大两个时间序列之间的相关性测量。同样，*趋势*和*季节性*可以产生虚假的相关性，这并不反映真正的因果关系，而是反映了两个序列都共享的潜在模式的存在。此外，影响两个序列的*外部冲击*可能会暂时增加相关性，但并不表示有意义的或持久的关系。
- en: In our case, the critical factor is that the values in both columns relate to
    orders, which themselves were placed as series in time, so they *will* be correlated.
    Whatever these columns mean, their values will grow in time with orders. How,
    then, shall we define the relationship?
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，关键因素是两列中的值都与订单相关，而这些订单本身是以时间序列的形式放置的，因此它们*将会*相关。无论这些列代表什么，它们的值都会随着订单的增长而增长。那么，我们该如何定义这种关系呢？
- en: Maybe we won’t? We have tested generative AI for ideas and found its ability
    to understand the problem and propose a solution worthwhile.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 也许我们不会呢？我们已经测试了生成式AI在提出想法方面的能力，并发现其理解问题和提出解决方案的能力是值得称道的。
- en: '**![image](../Images/Init-MA.png)**Can you propose how I can investigate the
    relationship between the `review_answer_ timestamp` and the `review_creation_date`
    columns?'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Init-MA.png)**你能提出我如何调查`review_answer_timestamp`和`review_creation_date`列之间的关系吗？'
- en: '**![image](../Images/Response.png)**To investigate the relationship between
    `review_answer_timestamp` and `review_ creation_date`, you can calculate the time
    difference between these two timestamps and analyze the distribution of the time
    differences. This will give you insights into how long it typically takes for
    a review to be answered.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**为了调查`review_answer_timestamp`和`review_creation_date`之间的关系，你可以计算这两个时间戳之间的时间差异，并分析时间差异的分布。这将让你了解通常需要多长时间来回答一条评论。'
- en: 'Here’s a step-by-step guide to analyze the relationship between these two columns:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是分析这两列之间关系的逐步指南：
- en: Calculate the time difference between `review_answer_timestamp` and `review_
    creation_date`.
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算从`review_answer_timestamp`到`review_creation_date`的时间差异。
- en: '[PRE31]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The `time_diff` column now contains the time difference in seconds.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '`time_diff`列现在包含秒为单位的时间差异。'
- en: 2\. Calculate basic statistics on the time difference to get an overview of
    the distribution.
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2. 计算时间差异的基本统计量，以获得分布的概述。
- en: '[PRE32]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 3\. Visualize the distribution of time differences using a histogram or a box
    plot.
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3. 使用直方图或箱线图可视化时间差异的分布。
- en: '[PRE33]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: By analyzing the distribution of time differences between `review_answer_timestamp`
    and `review_creation_date`, you can understand the typical response time and identify
    any outliers or trends in the data.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析`review_answer_timestamp`和`review_creation_date`之间时间差异的分布，你可以了解典型的响应时间，并识别数据中的任何异常值或趋势。
- en: Feel free to convert the time differences to a more suitable unit (e.g., hours
    or days) if needed. You can also further investigate the relationship by looking
    at how the time difference varies with other factors, such as review scores or
    specific time periods (e.g., days of the week or months of the year).
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，请随意将时间差异转换为更合适的单位（例如，小时或天）。你还可以通过观察时间差异如何与其他因素（如评分或特定时间段，例如一周中的某一天或一年中的某月）变化来进一步研究这种关系。
- en: It makes perfect sense to check the distribution of time differences between
    dates in two investigated columns. Let’s apply the proposed code.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 检查两个调查列中日期之间时间差异的分布是完全合理的。让我们应用提出的代码。
- en: The time difference summary analysis proves that the default timeframe of seconds
    may not be the most useful for the analysis.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 时间差异的总结分析证明，默认的秒数时间范围可能不是分析中最有用的。
- en: '[PRE34]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: However, our AI advisor mentioned that seconds might not be the best units for
    this data analysis. Fortunately, getting it to other time units is as simple as
    dividing the result by a constant. Feeling lazy, we asked generative AI to amend
    the code. It even took care of proper axis titles. The resulting images are presented
    in figure 2.7\.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们的AI顾问提到，秒可能不是这种数据分析的最佳单位。幸运的是，将其转换为其他时间单位就像除以一个常数一样简单。由于懒惰，我们要求生成式AI修改代码。它甚至处理了适当的轴标题。结果图像展示在图2.7中。
- en: Judging by the time distribution, where some answers are provided within a few
    hours and others after many months, it seems very much like some human interaction
    drives the difference.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 根据时间分布来看，有些答案在几小时内提供，而有些则在数月后提供，这似乎表明某些人类交互推动了这种差异。
- en: '![figure](../Images/CH02_F07_Siwiak3.png)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH02_F07_Siwiak3.png)'
- en: Figure 2.7 Distribution of the time difference between the `review_answer_timestamp`
    and `review_creation_date` columns represented as a histogram (a) and a box plot
    (b)
  id: totrans-356
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.7 `review_answer_timestamp`和`review_creation_date`列之间时间差异的分布，以直方图（a）和箱线图（b）表示
- en: Let’s try to apply logic here. We have a review creation and review answer.
    As the data comes from an online platform, the first intuition is that the user
    creates the review after purchase, and the seller answers to the reviewer after
    some time.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试应用逻辑。我们有一个评论创建和评论回答。由于数据来自在线平台，第一个直觉是用户在购买后创建评论，卖家在一段时间后回复评论者。
- en: This example shows that the combined might of generative AI and human expertise
    can easily be beaten by vague column-naming conventions and a lack of metadata.
    Actually, the `review_creation_date` column contains the date of sending the survey
    to the customer, and the `review_answer_timestamp` indicates when the customer
    answered it. This is yet another reason to *always* confirm the naming convention
    data creators used.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子表明，生成式 AI 和人类专业知识的结合很容易被模糊的列命名约定和缺乏元数据所击败。实际上，`review_creation_date` 列包含发送调查给客户的日期，而
    `review_answer_timestamp` 指示客户何时回答。这是始终确认数据创建者使用的命名约定的另一个原因。
- en: 2.6.4 Harmonizing data
  id: totrans-359
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6.4 数据协调
- en: 'Let’s turn our attention to the structural issue of the number of reviews per
    . . . and here is the trick: are you actually sure what those collected reviews
    are concerned with? Getting multiple reviews of the same product is one thing.
    It’s not unusual for multiple people buy the same thing and express their, often
    diverse, opinions. However, getting multiple reviews of the same order would require
    your attention, as you would need to decide how to evaluate order satisfaction
    in the case of more than one score. Would you use the last review or the average?'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把注意力转向每份订单的评论数量这一结构性问题。这里有一个技巧：你真的确定那些收集到的评论是关于什么的吗？获取同一产品的多个评论是一回事。很多人购买同一件商品并表达他们通常多样化的意见并不罕见。然而，获取同一订单的多个评论则需要你的注意，因为你需要决定在多个评分的情况下如何评估订单满意度。你会使用最后的评论还是平均值？
- en: In our dataset, 547 orders have more than one review. Now we should check if
    they come from the customer who ordered it, or from multiple customers, which
    would be worrying. To answer the question, we need to harmonize our data and connect
    different sources to form a coherent whole.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据集中，有 547 个订单有多条评论。现在我们应该检查这些评论是否来自下单的客户，或者来自多个客户，这会令人担忧。为了回答这个问题，我们需要协调我们的数据并连接不同的来源以形成一个连贯的整体。
- en: Let’s assume we feel lazy (that’s a reasonably safe assumption) and that we
    want our generative AI to do the heavy lifting.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们感到懒惰（这是一个合理的假设），并且我们想让我们的生成式 AI 承担繁重的工作。
- en: '**![image](../Images/Init-MA.png)**I have another DataFrame named `df_customers`.
    There are two columns `customer _id` and `customer_unique_id`. What shall I do
    to create in the previously discussed `df_order_reviews`an additional column with
    `customer_unique_id`correctly assigned to `customer_id`, which is already there?'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Init-MA.png)** 我还有一个名为 `df_customers` 的 DataFrame。有两个列
    `customer_id` 和 `customer_unique_id`。我该如何在之前讨论的 `df_order_reviews` 中创建一个额外的 `customer_unique_id`
    列，并将其正确地分配给已经存在的 `customer_id`？'
- en: '**![image](../Images/Response.png)**To add the `customer_unique_id` column
    to the `df_order_reviews` DataFrame based on the `customer_id` column, you can
    use the `merge()` function in pandas. Here’s how to do it:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)** 要根据 `customer_id` 列将 `customer_unique_id`
    列添加到 `df_order_reviews` DataFrame 中，你可以使用 pandas 中的 `merge()` 函数。以下是操作的步骤：'
- en: Merge the `df_order_reviews` and `df_customers` DataFrames on the `customer_id`
    column.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `customer_id` 列上合并 `df_order_reviews` 和 `df_customers` DataFrame。
- en: '[PRE35]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This will perform a left join, meaning that all rows from `df_order_reviews`
    will be included in the `merged_df` DataFrame, and the corresponding `customer_unique_id`
    values from `df_customers` will be matched based on the `customer_id` column.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 这将执行左连接，意味着 `df_order_reviews` 中的所有行都将包含在 `merged_df` DataFrame 中，并且根据 `customer_id`
    列匹配 `df_customers` 中的相应 `customer_unique_id` 值。
- en: 'Now, the `merged_df` DataFrame contains an additional `customer_unique_id`
    column, with the correct values assigned to each `customer_id`. If you want to
    update the original `df_order_reviews` DataFrame instead of creating a new one,
    you can simply assign `merged_df` to `df_order_reviews`:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`merged_df` DataFrame 包含了一个额外的 `customer_unique_id` 列，每个 `customer_id` 都被分配了正确的值。如果你想更新原始的
    `df_order_reviews` DataFrame 而不是创建一个新的，你可以简单地将 `merged_df` 赋值给 `df_order_reviews`：
- en: '[PRE36]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Now, that’s a nice bit of code with no right to work! It’s trying to assign
    customers to orders in the `df_order_reviews` DataFrame. However, no column in
    `df_order_ reviews` allows us to do this directly. We know that the only actual
    identifier we have is the `order_id` column, as we did our homework and learned
    that `customer_id` does not, in fact, identify customers.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有一段不错的代码，但没有权利工作！它试图将客户分配给 `df_order_reviews` DataFrame 中的订单。然而，`df_order_reviews`
    中的任何列都无法直接完成这项操作。我们知道我们唯一实际的标识符是 `order_id` 列，因为我们做了作业并了解到 `customer_id` 实际上并不能识别客户。
- en: The customer is always right  Generative AIs will neither question nor correct
    you. If you tell them to believe in something, they will assume you have your
    reasons. Although, there is a limitation to their suspension of disbelief. We
    tried to convince generative AIs that the Earth is flat, and even though they
    agreed to keep this assumption for the sake of the discussion, they reminded us
    every now and then that the Earth is, in fact, round. However, their tolerance
    borders are set pretty far.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 客户永远是对的  生成式AI既不会质疑也不会纠正你。如果你让他们相信某件事，他们会假设你有你的理由。尽管，他们对怀疑的容忍度有限。我们试图说服生成式AI地球是平的，尽管他们为了讨论的目的同意保持这个假设，但他们时不时地提醒我们地球实际上是圆的。然而，他们的容忍度边界设置得相当远。
- en: We can get the desired result in a two-step process by first connecting `df_orders`
    and `df_customers` on the `customer_id` field. This will give us a DataFrame that
    contains the `customer_unique_id` for each order. We can then connect this DataFrame
    to `df_order_reviews` on the `order_id` field. Alternatively, we could start by
    connecting `df_orders` and `df_order_reviews` on the `order_id` field. Then we
    could connect the resulting DataFrame to `df_customers` on the `customer_id` field.
    Hopefully you can now see why the initial data analysis is critical. You need
    to see all the DataFrames to harmonize data from multiple sources effectively.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过两步过程得到期望的结果，首先在 `customer_id` 字段上连接 `df_orders` 和 `df_customers`。这将给我们一个包含每个订单的
    `customer_unique_id` 的 DataFrame。然后，我们可以在这个 DataFrame 上连接 `df_order_reviews` 的
    `order_id` 字段。或者，我们也可以从在 `order_id` 字段上连接 `df_orders` 和 `df_order_reviews` 开始。然后，我们可以将得到的
    DataFrame 连接到 `df_customers` 的 `customer_id` 字段。希望你现在能明白为什么初始的数据分析是至关重要的。你需要查看所有
    DataFrame，才能有效地协调来自多个来源的数据。
- en: Joining, merging, and concatenating tables may be obvious to you, but, for others,
    this bit of data engineering 101 may still be something to worry about. In the
    past, just knowing *what* needs to happen was a required but not sufficient condition
    for getting the code to perform the desired operation. We’ve seen the computer
    science degree requirement in many data analysts’ job descriptions. Let’s take
    this opportunity to check if today’s gaps in coding experience can be closed using
    generative AI.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 连接、合并和连接表可能对你来说很直观，但对其他人来说，数据工程101中的这部分内容可能仍然值得关注。在过去，只知道“需要发生什么”是编写代码以执行所需操作所必需的，但并不充分。我们在许多数据分析师的职位描述中看到了计算机科学学位的要求。让我们利用这个机会来检查今天在编码经验上的差距是否可以通过生成式AI来弥补。
- en: '**![image](../Images/Init-MA.png)**I have my `df_order_reviews` DataFrame,
    which has the `order_id` column. I also have the `df_orders` DataFrame, where
    I have the `order_id` and the `customer_id` columns. I also have the `df_customers`
    DataFrame, where I have the `customer_id` and the `customer_unique_id` columns.
    How do I assign a valid `customer_ unique_id` from `df_customers` to each `order_id`
    in `df_order_reviews`?'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Init-MA.png)**我有一个名为 `df_order_reviews` 的 DataFrame，其中包含
    `order_id` 列。我还有一个名为 `df_orders` 的 DataFrame，其中包含 `order_id` 和 `customer_id` 列。此外，我还有一个名为
    `df_customers` 的 DataFrame，其中包含 `customer_id` 和 `customer_unique_id` 列。我该如何将 `df_customers`
    中的有效 `customer_unique_id` 分配给 `df_order_reviews` 中的每个 `order_id`？'
- en: '**![image](../Images/Response.png)**To assign the valid `customer_unique_id`
    from `df_customers` to each `order_id` in `df_order_reviews`, you will first need
    to merge `df_orders` with `df_customers` to create a DataFrame with `order_id`
    and `customer_unique_id` columns. Then, merge this new DataFrame with `df_order_reviews`.
    Here’s how to do it step by step:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '**![image](../Images/Response.png)**要将 `df_customers` 中的有效 `customer_unique_id`
    分配给 `df_order_reviews` 中的每个 `order_id`，你首先需要将 `df_orders` 与 `df_customers` 合并，以创建包含
    `order_id` 和 `customer_unique_id` 列的 DataFrame。然后，将这个新的 DataFrame 与 `df_order_reviews`
    合并。以下是逐步操作的方法：'
- en: Merge the `df_orders` and `df_customers` DataFrames on the `customer_id` column.
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `customer_id` 列上合并 `df_orders` 和 `df_customers` DataFrame。
- en: '[PRE37]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Now, the `orders_customers` DataFrame has `order_id` and `customer_unique_ id`
    columns.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`orders_customers` DataFrame 包含 `order_id` 和 `customer_unique_id` 列。
- en: 2\. Merge the `df_order_reviews` and `orders_customers` DataFrames on the `order_id`
    column.
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2. 在 `order_id` 列上合并 `df_order_reviews` 和 `orders_customers` DataFrame。
- en: '[PRE38]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This will perform a left join, meaning that all rows from `df_order_reviews`
    will be included in the `merged_df` DataFrame, and the corresponding `customer_unique_id`
    values from `orders_customers` will be matched based on the `order_id` column.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 这将执行左连接，意味着 `df_order_reviews` 中的所有行都将包含在 `merged_df` DataFrame 中，并且根据 `order_id`
    列匹配 `orders_customers` 中的相应 `customer_unique_id` 值。
- en: 'Now, the `merged_df` DataFrame contains an additional `customer_unique_id`
    column, with the correct values assigned to each `order_id`. If you want to update
    the original `df_order_reviews` DataFrame instead of creating a new one, you can
    simply assign `merged_df` to `df_order_reviews`:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`merged_df` DataFrame 包含了一个额外的 `customer_unique_id` 列，为每个 `order_id` 分配了正确的值。如果你想更新原始的
    `df_order_reviews` DataFrame 而不是创建一个新的，你只需将 `merged_df` 赋值给 `df_order_reviews`
    即可：
- en: '[PRE39]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Now, your `df_order_reviews` DataFrame has the `customer_unique_id` column correctly
    assigned to each `order_id`.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你的 `df_order_reviews` DataFrame 已经为每个 `order_id` 正确分配了 `customer_unique_id`
    列。
- en: This time, the code works as advertised. Generative AIs indeed can supplement
    you with the code; they know *how* to do it (even if they sometimes need severe
    prodding to get something working). The decision about *what* needs to happen
    is still yours (even if the initial conversations provided you with suggestions).
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，代码按预期工作。生成式 AI 确实可以为你提供代码；它们知道 *如何* 做这件事（即使有时需要严厉的推动才能让某件事工作）。关于 *什么* 需要发生，决定权仍然在你（即使最初的对话为你提供了建议）。
- en: Now, to finish this part of the exercise, let’s check if multiple reviews do
    come from different customers; in such a case, you should probably inform your
    IT department, as there would be a strong possibility of erroneous review assignments
    to orders. The following code should give you the required answer (and no, we
    didn’t write this ourselves).
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了完成这个练习的部分，让我们检查是否有多条评论来自不同的顾客；如果是这样，你可能需要通知你的 IT 部门，因为可能会出现错误地将评论分配给订单的情况。以下代码应该会给你所需的答案（而且，不，我们并没有自己编写这段代码）。
- en: '[PRE40]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: This code groups the `df_order_reviews` DataFrame by counting unique `customer_
    unique_id` values, checks for cases where the count of unique `customer_unique_id`
    values assigned to `order_id` is greater than `1`, and finally prints the results.
    All the multiple reviews came from the same customers. This saves you a trip to
    the IT den and even allows an analysis of how prevalent an opinion about the order
    is over time.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码通过统计唯一的 `customer_unique_id` 值将 `df_order_reviews` DataFrame 进行分组，检查分配给
    `order_id` 的唯一 `customer_unique_id` 值的计数是否大于 `1`，最后打印结果。所有这些多次评论都来自同一顾客。这可以节省你前往
    IT 部门的行程，甚至允许分析关于订单的意见随时间变化的普遍性。
- en: We can assume that you now either know your data well enough or know how to
    use sufficiently eloquent and knowledgeable generative AI models to help you get
    a good insight into your data structure and properties. In the next chapter, we’ll
    focus on the business analysis itself.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以假设你现在要么足够了解你的数据，要么知道如何使用足够优雅和知识渊博的生成式 AI 模型来帮助你深入了解你的数据结构和属性。在下一章中，我们将专注于业务分析本身。
- en: Things to ask generative AI
  id: totrans-390
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 询问生成式 AI 的事项
- en: What is the typical data structure for data coming from <some_business_ area>?
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自 <some_business_area> 的数据通常的数据结构是什么？
- en: What data quality problems should I test for in the DataFrame I have provided
    you?
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我应该在提供的 DataFrame 中测试哪些数据质量问题？
- en: What distribution should I expect in the column describing <some_activity>?
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述 <some_activity> 的列我应该期望什么分布？
- en: How do I deal with <a_clear_description_of_the_encountered_problem>?
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我如何处理 <a_clear_description_of_the_encountered_problem>？
- en: Summary
  id: totrans-395
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Be aware of the context of your conversation with your generative AI advisor.
    Ask it to summarize its understanding of the context (this is particularly useful
    if you want to start a new session on the same topic, such as to research different
    approaches).
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意你与你的生成式 AI 咨询师对话的上下文。请它总结其对上下文的理解（如果你想在同一主题上开始新的会话，这尤其有用，例如研究不同的方法）。
- en: Don’t shy away from reminding your AI adviser about the purpose of the current
    work or the variable names used. Remember, it aims to please, not to be precise.
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要害怕提醒你的 AI 咨询师当前工作的目的或使用的变量名。记住，它的目的是取悦，而不是精确。
- en: There is no better way to understand the data than by looking at it.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有比查看数据更好的方式来理解数据了。
- en: When evaluating data quality, it’s essential to check its structure, consistency,
    and contents.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在评估数据质量时，检查其结构、一致性和内容是至关重要的。
- en: Double-check your data, triple-check, and reach out to data providers for clarification
    when you’re still in doubt.
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仔细检查你的数据，三重检查，并在你仍有疑问时联系数据提供者以获得澄清。
- en: It’s a good practice to keep data quality investigation separate from the actual
    in-depth analysis . . .
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据质量调查与实际深入分析分开是一个良好的实践 . . .
- en: . . . however, data cleaning and EDA *are* integral parts of your data analysis,
    and you’ll be repeating them sooner than you hope.*
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: . . . 然而，数据清洗和探索性数据分析（EDA）是数据分析的不可或缺部分，你可能会比你希望的更早重复它们。*
