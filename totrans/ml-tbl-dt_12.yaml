- en: 10 Model deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a deep learning model in a simple web application on our local system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An introduction to key Google Cloud concepts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An introduction to Vertex AI, the machine learning environment in Google Cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying a deep learning model with a Vertex AI endpoint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adapting the web application to use a Vertex AI endpoint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting generative AI assistance with Gemini for Google Cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In chapter 9, we reviewed a set of best practices for training a deep learning
    model with tabular data and introduced the Kuala Lumpur real estate price prediction
    problem as a challenging tabular problem because of its mixed-type features. In
    this chapter, we will take the model we trained in chapter 9 and deploy it in
    a simple web application. First, we will deploy it locally—that is, having both
    the web server and the trained model on our local system. Next, we will introduce
    Google Cloud as an alternative way to deploy our model. In fact, we will take
    the trained model and deploy it with an endpoint in Vertex AI, the machine learning
    environment in Google Cloud. Finally, we will examine how to use Google’s generative
    AI assistant Gemini on Google Cloud. The code described in this chapter is available
    at [https://mng.bz/6e1A](https://mng.bz/6e1A).
  prefs: []
  type: TYPE_NORMAL
- en: 10.1 A simple web deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once we have trained a deep learning model, having followed the best practices
    described in chapter 9, we have only just scratched the surface of the process
    of getting value out of the model. Take the example of the Kuala Lumpur real estate
    price prediction model we trained in chapter 9\. This model could be useful for
    real estate agents who want to provide advice to their clients about the price
    they should set for new real estate listings. This model could also be useful
    for property owners who want to put their properties on the market to get an idea
    of what value they could expect to get from their properties. Finally, the model
    could also be useful for buyers who are interested in purchasing properties in
    Kuala Lumpur so they can get an idea of what kind of property they could buy in
    a particular location given a certain budget.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s evident that the model that we trained in chapter 9 has the potential
    to be useful to a variety of audiences. The dilemma we face is how to make the
    model available to all these audiences who have different business goals and potentially
    different computer proficiency. Later in this chapter, we will learn how to put
    a model into production on a public cloud environment, but what if we want to
    do an initial experiment to learn the characteristics of the model and run some
    tests with some beta clients? Do we need to implement the whole process of deploying
    a model in a public cloud? No, because we can take advantage of Flask, a Python
    web application library and set up a self-contained web deployment of the model.
    Flask was first released in 2010 by Armin Ronacher and has since become one of
    the most popular web frameworks for Python. It may sound a bit outdated, given
    the more recent packages such as Streamlit ([https://streamlit.io/](https://streamlit.io/))
    or Gradio ([https://www.gradio.app/](https://www.gradio.app/)). However, despite
    its age, Flask remains a relevant choice for web development due to its lightweight
    and flexible nature, especially for small personal projects. We are not going
    to explore all the details of Flask in this book; we just provide the nuts and
    bolts to set up a demonstration for a model, but if you want to learn more about
    Flask, check out the documentation: [https://flask.palletsprojects.com/en/2.3.x/](https://flask.palletsprojects.com/en/2.3.x/).'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have introduced the idea of a simple web deployment, we can get
    started. In this section, we will go over how to create a simple yet complete
    deployment of the model in a pair of web pages served by Flask.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.1 Overview of web deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far in this book, we have examined a variety of libraries and frameworks
    for machine learning and deep learning. With the exception of configuration files
    defined in YAML, all of the code that we have looked at so far in this book has
    been Python. Like the examples we have seen so far, the web deployment of a machine
    learning or deep learning model does include Python, but it also requires handling
    HTML, JavaScript, and CSS scripts. Figure 10.1 shows an overview of the web deployment
    we have in mind.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F01_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 Overview of the web deployment
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s review the components that make up the web deployment depicted in figure
    10.1:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Trained model—*This is the model that we saved in the Keras with the preprocessing
    layers notebook. The model is actually saved as a directory structure ([https://mng.bz/oKJp](https://mng.bz/oKJp)).
    In the Flask server module, the model is loaded using a reference to this directory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Flask server module*—This is a Python module that loads the trained model
    and contains view functions for each of the solution’s HTML pages. The view functions
    specify the actions that the Flask server module takes when these HTML pages are
    loaded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Flask server config file*—YAML file where you can specify parameters for the
    Flask server, such as the directory containing the trained model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`home.html`—One of the HTML pages served by the Flask server. This page contains
    fields in which the user can specify the characteristics (such as location, number
    of rooms, and number of parking spaces) of the property for which they want to
    get a price prediction. This page also contains JavaScript functions that load
    default values into each of the fields on the page, specify the valid values that
    can be entered into each field, and package the values entered by the user so
    they can be sent back to the Flask server module.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`show-prediction.html`—One of the HTML pages served by the Flask server. This
    page displays the prediction made by the model for the property with the characteristics
    entered by the user on the `home.html` page.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*CSS file*—Specifies how the HTML pages in the solution are rendered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlike the other examples in this book, the web deployment runs on your local
    system by default. When the Flask server is running, it serves `home.html` at
    `localhost:5000` so you can exercise the system in your browser.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have introduced the web deployment, we will look at the Flask server
    module and the HTML pages in more detail in the subsequent subsections.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.2 The Flask server module
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Flask server module is the heart of web deployment. Unlike the other Python
    programs we have examined so far in this book, it is a standalone `.py` file rather
    than a Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this subsection, we will review the key pieces of code in the Flask server
    module and explain how they drive the web deployment. The following listing shows
    the first key section of the Flask server module: the code to load the saved Keras
    model from the file system into an object in the Python module.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.1 Loading the saved Keras model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: ① Gets the current directory
  prefs: []
  type: TYPE_NORMAL
- en: ② Gets the fully qualified model directory
  prefs: []
  type: TYPE_NORMAL
- en: ③ Loads the model using the fully qualified model filename
  prefs: []
  type: TYPE_NORMAL
- en: In listing 10.1, the fully qualified model directory is built using the model
    filename loaded from the config file. The model directory is expected to be in
    a directory called `models`, in the same directory as the Flask server module.
    The model is loaded by the same `tf.keras.models.load_model` function that was
    used to load the model in the Keras preprocessing layers training notebook in
    chapter 9.
  prefs: []
  type: TYPE_NORMAL
- en: The most important parts of the Flask server module are the view functions which
    specify the actions taken for each of the HTML pages in the application. The following
    listing shows the view function for `home.html`, the code that gets invoked when
    `home.html` is the target.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.2 `home.html` view function
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: ① Decorator to indicate that this view function is for home.html
  prefs: []
  type: TYPE_NORMAL
- en: ② Parameter sent to home.html for its rendering
  prefs: []
  type: TYPE_NORMAL
- en: The view function shown in listing 10.2 simply sends a title to the `home.html`
    page and renders the page.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.3 shows the view function for `show-prediction.html`. The Flask module
    runs this code when `show_prediction.html` is the target. This view function processes
    the values that the user entered in `home.html` and invokes the model to get a
    prediction on these values.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.3 `show-prediction.html` view function
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: ① Decorator to indicate that this view function is for show-prediction.html
  prefs: []
  type: TYPE_NORMAL
- en: ② Loads categorical feature values from the query string into the scoring_dict
    dictionary
  prefs: []
  type: TYPE_NORMAL
- en: ③ Loads continuous feature values from the query string into the scoring_dict
    dictionary
  prefs: []
  type: TYPE_NORMAL
- en: ④ Sets the value of the size_type_bin parameter
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Builds the input dictionary from scoring_dict
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Gets the prediction from the loaded model for the loaded feature values
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Sets the output prediction string based on the value of the prediction
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Renders show-prediction.html using the prediction string
  prefs: []
  type: TYPE_NORMAL
- en: The view function shown in listing 10.3 takes the query string returned from
    `home.html`, loads all the values from the query string into the Python dictionary
    `scoring_dict`, and uses those values to invoke the model loaded by the code in
    listing 10.1 to get a prediction. The one exception is `size_type_bin`. For the
    purposes of this simple web deployment, we hard-code the bin value. As an exercise,
    consider how you would calculate the bin value to avoid this hard coding. The
    value of the prediction is used to set a string value that is sent to `show-prediction.html`
    to be displayed when the page is rendered.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.2 illustrates how the view functions shown in listings 10.2 and 10.3
    interact with the web pages `home.html` and `show-prediction.html`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F02_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 Interaction between the view functions and the web pages
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.2 explains that the view function for `home.html` in the Flask server
    module renders `home.html`. The JavaScript functions in `home.html` build a query
    string containing the user’s feature values in `home.html`. This query string
    is returned to the `show-prediction` view function in the Flask server module.
    That view function loads the feature values from the query string and uses them
    to get a prediction for the property from the model. The model prediction is used
    to create a prediction string, which is passed to `show-prediction.html` when
    it is rendered. The prediction string is displayed in `show-prediction.html`.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have examined the key parts of the Python code in the Flask server
    module on the right side of figure 10.2, in the next section, we will move beyond
    Python to look at the HTML and JavaScript code that drives what’s happening on
    the left side of figure 10.2.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.3 The home.html page
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There is a lot happening in the `home.html` page:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up fields where the user can enter values for the eight property characteristics
    that the prediction is based on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting default values and ranges for each of the fields
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gathering the input from the fields when the user clicks Get Prediction and
    sending them to the view function in the Flask server module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 10.3 summarizes the interaction between the JavaScript functions in `home.html`
    and the user interface elements in `home.html`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F03_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 Interaction between JavaScript and UI elements in `home.html`
  prefs: []
  type: TYPE_NORMAL
- en: Having introduced what is happening in `home.html`, let’s examine the code for
    this web page. listing 10.4 shows examples of the definitions of the input fields
    in `home.html`. These are HTML fields where the user can enter input values to
    specify the details about the property for which they want to get a price prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.4 Examples of HTML definitions for fields
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: ① Label for the location field
  prefs: []
  type: TYPE_NORMAL
- en: ② Defines ID for location field
  prefs: []
  type: TYPE_NORMAL
- en: ③ Defines label for rooms field
  prefs: []
  type: TYPE_NORMAL
- en: ④ Defines ID for rooms field
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Defines maximum and minimum values for the rooms field
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.4 shows examples of HTML definitions for a categorical field (`location`)
    and a continuous field (`rooms`). The HTML definitions for other fields follow
    the same pattern, with minimums and maximums set for all the continuous fields.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `load-selections()` JavaScript function gets run when the page is loaded
    by the following statement near the beginning of the HTML page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The following listing presents the key parts of the `load-selections()` JavaScript
    function, the function that sets up the page so it is ready for the user to select
    the characteristics of the property for which they want to get a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.5 `load-selections()` JavaScript function
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: ① Defines identifier for the location field
  prefs: []
  type: TYPE_NORMAL
- en: ② Defines list containing values for the selection list in the location field
  prefs: []
  type: TYPE_NORMAL
- en: ③ Defines list containing values for the selection list in the property type
    field
  prefs: []
  type: TYPE_NORMAL
- en: ④ Defines list containing values for the selection list in the size type field
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Sets the default value for the rooms field
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Loops to populate the values in the selection list in the location field
  prefs: []
  type: TYPE_NORMAL
- en: 'From listing 10.5 we can see the main actions taken by the `load-selections()`
    JavaScript function:'
  prefs: []
  type: TYPE_NORMAL
- en: For continuous fields, set the default value. This is the value that appears
    in the field when the page is initially loaded and is passed along to the Flask
    server module if the user doesn’t change it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For categorical fields, define the values from which the user can select and
    populate the field’s selection list with those values. In `home.html`, these values
    are defined in a hard-coded list. In a more robust web application, these values
    would be maintained in a control file separate from the code to make it easier
    to maintain the values and reduce the chance of adding regressions to the code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When the user has set the values they want for their property and clicks the
    Get Prediction button, the following code specifies that the `link_with_args()`
    JavaScript function gets run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Listing 10.6 shows the `link_with_args()` JavaScript function. This function
    is called when the user clicks the Get Prediction button in `home.html`. It invokes
    the rest of the code in `home.html`, which collects the user’s input values and
    packages them into a query string that is passed back to the `show_prediction()`
    view function in the Flask module.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.6 `link_with_args()` JavaScript function
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: ① Echoes the query string to the console
  prefs: []
  type: TYPE_NORMAL
- en: ② Sets the page target
  prefs: []
  type: TYPE_NORMAL
- en: As listing 10.6 shows, the `link_with_args()` JavaScript function simply calls
    the `getOption()` function and sets the resulting query string as the target of
    the page in `window.output`.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.7 shows the `getOption()` JavaScript function. This function loads
    all the values that the user has entered in `home.html` and packages them into
    a query string that is passed back to the `show_prediction()` view function in
    the Flask module.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.7 `getOption()` JavaScript function
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: ① Gets the selected entry in the selection list for the location field
  prefs: []
  type: TYPE_NORMAL
- en: ② Gets the value entered into the rooms field
  prefs: []
  type: TYPE_NORMAL
- en: ③ Gets the value of the selected entry in the selection list for the location
    field
  prefs: []
  type: TYPE_NORMAL
- en: ④ Sets the value of window.output to the query string
  prefs: []
  type: TYPE_NORMAL
- en: 'The `getOption()` JavaScript function shown in listing 10.7 performs the following
    actions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Loads the values from the continuous fields: `rooms`, `bathrooms`, `car-parks`,
    and `size`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Loads the selected entries in the categorical fields: `location`, `property-type`,
    `furnishing`, and `size-type`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Builds the query string. The query string looks as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The query string consists of
  prefs: []
  type: TYPE_NORMAL
- en: 'The URL `/show-prediction/`. Note that this URL matches the decorator that
    precedes the view function for `show-prediction` view function from the Flask
    server module shown in listing 10.3:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '`?` to indicate the beginning of the query string.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A set of parameter and value pairs to indicate the values that have been set
    for each field in `home.html`, delineated by the separator `&`. For example, the
    parameter and value pairs could look as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The query string is passed to the `show-prediction` view function in the Flask
    server module. As shown in listing 10.3, in that function, the query string is
    parsed to get the values entered in `home.html` for each feature, and those values
    are used to get a prediction from the model.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.4 The show-prediction.html page
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we have looked at the Flask server module and `home.html`, there is
    one more component of the web deployment to examine: `show-prediction.html`. This
    page displays the prediction that the model makes for the property that the user
    entered values for in `home.html` and has a button that takes the user back to
    `home.html`, where they can enter values for another property.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.8 `show-prediction.html`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: ① Sets the text that appears in the browser tab for this page
  prefs: []
  type: TYPE_NORMAL
- en: ② Sets the stylesheet for the page
  prefs: []
  type: TYPE_NORMAL
- en: ③ Introductory text
  prefs: []
  type: TYPE_NORMAL
- en: ④ Button to return to home.html
  prefs: []
  type: TYPE_NORMAL
- en: As shown in listing 10.8, `show-prediction.html` does not contain any JavaScript
    functions. The HTML for the page defines the text that appears on the page, and
    the button Get Another Prediction returns the user to `home.html`. Figure 10.4
    shows how the elements we have discussed in this section appear in `show-prediciton.html`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F04_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 Key elements in `show-prediction.html`
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have examined all the components of web deployment, we will review
    the rationale for deploying our model simply on the web in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.5 Exercising the web deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have gone through the components that make up the web deployment,
    the next step is to see the deployment in action. To exercise the web deployment,
  prefs: []
  type: TYPE_NORMAL
- en: Create a new directory on your local system and clone the repo [https://github.com/lmassaron/ml_on_tabular_data](https://github.com/lmassaron/ml_on_tabular_data).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Make `chapter_10` your current directory and start the Flask server module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once the Flask server module is running, go to `localhost:5000` in a browser
    to exercise the deployment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Congratulations! You have exercised a deep learning model trained on tabular
    data in the context of a simple, local web application. This deployment is an
    efficient way for us to exercise the trained model and see if it behaves as we
    expect. However, this very basic deployment does not incorporate many of the characteristics
    we expect to have in a production deployment. For example, we don’t want the resources
    for serving the model to be limited by our local system. In the remainder of this
    chapter, we will examine how we can use this same web application to exercise
    the trained model deployed from an endpoint in a cloud environment.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2 Public clouds and machine learning operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The simple web deployment that we reviewed in the previous section demonstrates
    some useful aspects of what it takes to put a trained model into production, but
    it has some serious limitations. It is running entirely on a local system, so
    it won’t be accessible to anyone who doesn’t have access to the local system.
    This is probably a good thing, because this deployment does not incorporate the
    characteristics that we want to have in a production deployment, including
  prefs: []
  type: TYPE_NORMAL
- en: '*Scaling capacity to meet demand*. What happens if the interest rate drops
    and the demand for price predictions doubles?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Seamless model updates*. What if we retrain the model on the latest data and
    need to deploy it quickly, without any interruption to the service?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Serving multiple versions of the model at the same time*. What if we want
    to experiment with a version of the model that is trained on a different dataset
    by exposing the new model to a subset of the users? How do we serve multiple versions
    of the model and control what proportion of users see each version?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model monitoring*. How do we track the accuracy of the model as the real estate
    market develops? How do we catch and correct problems before they impact our users?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Resiliency and up-time*. With this application implemented on a local system,
    what happens when the system needs maintenance or has an unplanned outage? How
    do we ensure that our users can continue to get access to the application?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We could implement custom code to handle all of these scenarios, but there is
    a simpler solution. Public clouds provide complete, end-to-end machine learning
    and machine learning operations (MLOps) environments to address all the challenges
    listed here. The three most-used public clouds in the world are
  prefs: []
  type: TYPE_NORMAL
- en: AWS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many organizations will use one or more of these public clouds to put their
    models into production.
  prefs: []
  type: TYPE_NORMAL
- en: The public cloud concepts that we describe in this chapter and chapter 11 are
    available in all three of these public clouds, but we are going to use Google
    Cloud for the public cloud examples in this chapter. There are a few reasons for
    this. One of the authors is a Google employee, but, more importantly, our objective
    opinion is that Google Cloud provides an easy-to-use environment for deploying
    models trained with tabular data and exploring the key concepts of MLOps. In the
    remainder of this chapter, we will go through how to get started with Google Cloud
    and how to use it to deploy a model. In chapter 11, we will go beyond the simple
    deployment in this chapter to explore the features in Google Cloud that make it
    easy to retrain and redeploy the model.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3 Getting started with Google Cloud
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will go through the initial steps of getting started with
    Google Cloud, including accessing Google Cloud for the first time, creating a
    project, and creating a Google Cloud Storage bucket, a data storage container.
    In the subsequent section, we will see how to use Vertex AI, the machine learning
    platform in Google Cloud, to deploy our model.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.1 Accessing Google Cloud for the first time
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will go through signing into Google Cloud for the first
    time and setting up a Google Cloud *project*. Everything you do in Google Cloud
    is associated with a project. You can use projects to organize your work and to
    control the teams and individuals who have access to given Google Cloud resources.
    To get started, go to [https://cloud.google.com](https://cloud.google.com) and
    click Sign In, as shown in figure 10.5.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F05_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 Signing into Google Cloud
  prefs: []
  type: TYPE_NORMAL
- en: Once you have signed in, click Console to open up the Google Cloud console,
    as shown in figure 10.6.
  prefs: []
  type: TYPE_NORMAL
- en: If you have not used Google Cloud before, you will need to set up your account
    for billing. You may be eligible for free credit, but you will need to enter credit
    card details to use the features that are described in this chapter and chapter
    11\. Once you have signed in and, if necessary, completed setting up your account
    with billing details, you will be at the console, as shown in figure 10.7.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F06_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 Entering the Google Cloud console
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F07_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 Google Cloud console
  prefs: []
  type: TYPE_NORMAL
- en: The console is the user interface for Google Cloud. This is one of the interface
    choices for working with Google Cloud features, and it is what we will use for
    most of the actions we will take in Google Cloud in this chapter. In addition
    to the Cloud console, you can interact with Google Cloud features using
  prefs: []
  type: TYPE_NORMAL
- en: Command-line interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Client libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NOTE See [https://cloud.google.com/docs/overview](https://cloud.google.com/docs/overview)
    for more details on the interfaces for Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.2 Creating a Google Cloud project
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can create a project once you have successfully logged into Google Cloud
    and accessed the Google Cloud Console. In this section, we will go through the
    steps to create a project.
  prefs: []
  type: TYPE_NORMAL
- en: In the Google Cloud console, click the Project Selection field, as shown in
    figure 10.8\. In the Select a Project screen, select New Project, as shown in
    figure 10.9\. Then, enter `first-project-ml-tabular` in the Project Name field
    and click on Create, as shown in figure 10.10.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F08_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.8 Selecting a project
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F09_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.9 Selecting a project screen
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F10_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.10 Entering a project name
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have created your first Google Cloud project.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the project name needs to be unique for your set of projects. The
    project ID, which appears below the Project name field, must be universally unique,
    so if you have a project name that is shared with any other project in Google
    Cloud, the project ID for that project will be automatically updated to be unique.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.3 Creating a Google Cloud Storage bucket
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The primary way to store data in Google Cloud is to use Cloud Storage buckets.
    In this section, we will go through just what you need to know about Cloud Storage
    buckets for the purposes of deploying a model. If you are interested in more details
    on Cloud Storage buckets, see the documentation: [https://cloud.google.com/storage/docs/buckets](https://cloud.google.com/storage/docs/buckets).
    We will be using a bucket to store the model we trained in chapter 9\. To create
    a Cloud Storage bucket, enter “cloud storage” in the search bar in the Cloud Console
    and select Cloud Storage, as shown in figure 10.11.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F11_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.11 Searching for Cloud Storage
  prefs: []
  type: TYPE_NORMAL
- en: The Cloud Storage Buckets page appears. Select Create to create a new Cloud
    Storage bucket, as shown in figure 10.12.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F12_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.12 Cloud Storage view
  prefs: []
  type: TYPE_NORMAL
- en: In the Create a bucket page, enter a unique name for your bucket (figure 10.13),
    click Continue, and select Region in Location Type, and select a region, as shown
    in figure 10.14\. To work with the machine learning pipeline script code that
    we will explore in chapter 11, the bucket needs to be created in a region. For
    the purposes of this example, you can pick any region that you like. Click Create.
    Note that your bucket name has to be universally unique.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F13_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.13 Setting the bucket name
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F14_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.14 Setting location type for the bucket
  prefs: []
  type: TYPE_NORMAL
- en: The Bucket details page appears, showing your new bucket, as shown in figure
    10.15.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F15_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.15 Bucket details showing the new bucket
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have created your first Google Cloud Storage bucket.
  prefs: []
  type: TYPE_NORMAL
- en: 10.4 Deploying a model in Vertex AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Earlier in this chapter, we deployed the Kuala Lumpur real estate price prediction
    model using a simple, Flask-based web application. In this section, we are going
    to deploy the same model using the Google Cloud Vertex AI environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have created a project in Google Cloud and created a Cloud Storage
    bucket in the new project, we are ready to deploy a model in Google Cloud. The
    following are the steps we will follow for this deployment (described in more
    detail in the Vertex AI documentation at [https://mng.bz/nRJ2](https://mng.bz/nRJ2)):'
  prefs: []
  type: TYPE_NORMAL
- en: Upload the model we trained in chapter 9 to Google Cloud storage.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import the model to the Vertex AI Model Registry.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The following sections describe each of these steps.
  prefs: []
  type: TYPE_NORMAL
- en: 10.4.1 Uploading the model to a Cloud Storage bucket
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The simplest way to get your trained model into Google Cloud Storage is to upload
    a folder containing the trained model. If you want to upload a version of the
    model that has already been trained, clone the repo at [https://github.com/lmassaron/Advanced_Analytics_for_Business](https://github.com/lmassaron/Advanced_Analytics_for_Business).
    The directory you want to upload is `chapter_10/models/kl_real_estate_keras_preprocessing_model`.
  prefs: []
  type: TYPE_NORMAL
- en: To upload the model that you created in chapter 9, select the Cloud Storage
    bucket you created in the Cloud Storage page, as shown in figure 10.16\. In the
    Bucket Details page, select Upload Folder, as shown in figure 10.17.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F16_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.16 Selecting your bucket in the Cloud Storage page
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F17_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.17 Uploading a folder in the Bucket details page
  prefs: []
  type: TYPE_NORMAL
- en: Select the folder on your local system containing the trained model from chapter
    9\. When the upload is complete, the folder will appear on the Bucket details
    page, as shown in figure 10.18.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F18_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.18 Your uploaded bucket in the Bucket details page
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have uploaded your model to Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 10.4.2 Importing the model to Vertex AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have uploaded the model to a Google Cloud Storage bucket, we can
    import it to the Vertex AI Model Registry. Enter “vertex ai” in the Google Cloud
    console search bar to get to the Vertex AI page. If this is the first time you
    have used Vertex AI, you will see Enable All Recommended API. If you see this
    button, click on it to enable the APIs that are required to use Vertex AI, as
    shown in figure 10.19\.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F19_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.19 Enabling Vertex AI APIs
  prefs: []
  type: TYPE_NORMAL
- en: Then select Model Registry in the left navigation panel, as shown in figure
    10.20, and on the Model Registry page, click Import, as shown in figure 10.21.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F20_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.20 Model Registry in the Vertex AI page navigation panel
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F21_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.21 Model Registry import
  prefs: []
  type: TYPE_NORMAL
- en: In the Import model page, select Import as New Model, enter “first-model-ml-tabular”
    in the Name field, and click Continue, as shown in figure 10.22.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F22_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.22 Specifying the name of the imported model in Model Registry
  prefs: []
  type: TYPE_NORMAL
- en: 'On the Model settings tab of the Import model page, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In Model Framework, select TensorFlow. Recall from chapter 8 TensorFlow is the
    low-level framework for Keras, the framework we used to train the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In Model Framework Version, select the TensorFlow level used to train the model.
    You can find this level from the output of `tf.__version__` in the notebook used
    to train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In Model Artifact Location, click Browse and select the folder where you uploaded
    the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click Continue and Import, as shown in figure 10.23.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F23_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.23 Specifying model settings
  prefs: []
  type: TYPE_NORMAL
- en: The model import process may take several minutes to complete. When the import
    is complete, you will see the new model name on the Model Registry page, as shown
    in figure 10.24.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F24_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.24 Model Registry showing the imported model
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have imported a model into Vertex AI. In the next section,
    we will go through the steps to make this model available through an endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: 10.4.3 Deploying the model to an endpoint
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have imported the trained model into Vertex AI, we can deploy it
    to an endpoint. By deploying the model to an endpoint, we get a URL that we can
    use to invoke the model. In effect, the endpoint deployment can take the place
    of the model file in the local file system that the Flask server loaded in the
    simple web deployment that we did earlier in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: To deploy the model to an endpoint, on the Model Registry page, select the model
    we created in the previous section. In the model details page, select the version
    of the model (by default, 1), as shown in figure 10.25\. In the version page for
    the model, click Deploy to Endpoint, as shown in figure 10.26\. In the Deploy
    to Endpoint page, enter a name in Endpoint name and click Continue, as shown in
    figure 10.27.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F25_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.25 Model details page
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F26_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.26 Deploying your model
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F27_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.27 Deploying to endpoint page
  prefs: []
  type: TYPE_NORMAL
- en: In the Model Settings tab, under Advanced Scaling Options, select a minimal
    machine type, such as `n1-standard-2`, as shown in figure 10.28, and click Continue.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F28_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.28 Setting the machine type for the deployment
  prefs: []
  type: TYPE_NORMAL
- en: Because this is a test deployment of a simple model, we only need a minimal
    machine type. If we were deploying a more demanding model, or making a production
    deployment, we could choose a machine type with more memory or compute resources,
    depending on the requirements of our model. A minimal machine type is good enough
    for our test deployment, and it will cost us less than a more advanced machine
    type. When you are using a cloud environment, it’s a best practice to use the
    resources that are sufficient for your application and not more. Doing this will
    save you money.
  prefs: []
  type: TYPE_NORMAL
- en: TIP For more details about machine types for Vertex AI endpoint deployments,
    see the documentation at [https://mng.bz/vK14](https://mng.bz/vK14).
  prefs: []
  type: TYPE_NORMAL
- en: Click Deploy, as shown in figure 10.29.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F29_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.29 Selecting Deploy in the Deploy to endpoint page
  prefs: []
  type: TYPE_NORMAL
- en: Deployment can take several minutes to complete. When deployment is complete,
    the status of the deployment on the Model version details page changes to Active,
    as shown in figure 10.30.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F30_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.30 Confirmation that the model has been deployed
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have completed the deployment of a model in Vertex AI.
    In the next section, we will go through the steps to quickly test the deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 10.4.4 Initial test of the model deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have deployed the model, we can do an initial test of the model
    deployment directly in the Google console.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall the test of the trained model that we did in chapter 9 to exercise the
    model in the context of a Jupyter Notebook. We defined a Python dictionary that
    contained all the features used to train the model, along with values for each
    of the features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We can reuse this sample to test the model deployment in Vertex AI. In the Model
    version details page, go to the Test Your Model section, as shown in figure 10.31.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F31_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.31 Test Your Model section of the Deploy and Test tab
  prefs: []
  type: TYPE_NORMAL
- en: Update the JSON request field to use the values from the sample from chapter
    9, with each value being an entry in a list and double quotes being used throughout.
    When you have completed the update, the JSON request field should look like figure
    10.32.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F32_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.32 Test sample in JSON format
  prefs: []
  type: TYPE_NORMAL
- en: 'Note three differences between the format of the sample in the JSON request
    and the original sample from the chapter 9 Jupyter Notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: Values in the key-value pairs are all arrays rather than single values. You
    will get an error if you have single values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Double quotes (“ “) are used throughout rather than single quotes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no comma after the last key-value pair.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the JSON Request field contains valid JSON, the frame turns blue. Click
    Predict to see the output of the endpoint in the Response field, as shown in figure
    10.33.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F33_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.33 Response from the endpoint for the test sample
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the prediction value is not a probability. Recall that when we got
    a prediction from the model in the model training notebook in chapter 9, and when
    we got a prediction from the model in the Flask server module, we needed to apply
    the sigmoid function to the output of the model to get the probability that the
    property has a price above the median:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to apply the sigmoid function to get the probability from the output
    provided by the endpoint. If we update the statements that we used in the training
    notebook so that the input to the sigmoid function is the output of the endpoint,
    then we get the same probability that we got for this property when we used it
    to exercise the model in chapter 9:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Now, having validated that the endpoint works and that for the same property,
    we get the same results as when we applied the model directly in a Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 10.5 Using the Vertex AI deployment with Flask
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we deployed the Kuala Lumpur property price prediction
    model to an endpoint in Vertex AI. In this section, we will adapt the Flask web
    deployment to use this endpoint. When we are done, we should have the same experience
    from the web pages `home.html` and `show-prediction.html`, with the model being
    served from the Vertex AI endpoint rather than from a local system.
  prefs: []
  type: TYPE_NORMAL
- en: 'To adapt the Flask deployment to work with the Vertex AI endpoint deployment,
    we will need to take the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Set up the Vertex AI SDK.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the Flask server module to access the Vertex AI endpoint to get predictions
    from the model served there.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 10.34 shows the key components of the application that is adapted to
    use a Vertex AI endpoint deployment of the model.
  prefs: []
  type: TYPE_NORMAL
- en: In the remainder of this section, we will go through the steps required to deploy
    the model with the Vertex AI endpoint using the web application.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F34_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.34 Sample request link in the Deploy and Test tab
  prefs: []
  type: TYPE_NORMAL
- en: 10.5.1 Setting up the Vertex AI SDK
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vertex AI provides client libraries that allow you to access Vertex AI features
    via an API in Python, Java, and `node.js` applications. The client library for
    Python is included in the Python SDK for Vertex AI, so we will install the SDK
    to get the API access required to invoke the model via the endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE For full details on the Vertex AI SDK, see the documentation at [https://mng.bz/4a1j](https://mng.bz/4a1j).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the following command to install the Vertex AI SDK:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the Vertex AI SDK installed, we can proceed to the next step:
    updating the Flask server module.'
  prefs: []
  type: TYPE_NORMAL
- en: 10.5.2 Updating the Flask server module to call the endpoint
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To get an overview of how to use the Vertex AI API to access the model via the
    endpoint, click the Sample Request link in the Model version details page, as
    shown in figure 10.35.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F35_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.35 Sample request link in the Deploy and Test tab
  prefs: []
  type: TYPE_NORMAL
- en: 'The first update to the Flask server module is to import the libraries required
    for Vertex AI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Next, we add the `predict_custom_trained_model_sample()` function defined in
    [https://mng.bz/QD8v](https://mng.bz/QD8v). We need to make one update to this
    function so that it returns `predictions` (which contains the response from the
    endpoint) back to the `show-prediction.html` view function.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.9 highlights the updates that we need to make to the `show-prediction.html`
    view function. These changes package the input values in the format expected by
    the Vertex AI endpoint deployment and invoke the model at the Vertex AI endpoint
    via the `predict_custom_trained_model_sample()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.9 `show-prediction.html` view function for endpoints
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: ① Removes size_type from the feature list
  prefs: []
  type: TYPE_NORMAL
- en: ② Converts the values in the scoring_dict to lists of values
  prefs: []
  type: TYPE_NORMAL
- en: ③ Calls predict_custom_trained_model_sample
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 10.9 shows the following updates to the `show-prediction.html` view
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: The call to the endpoint has to have the exact correct list of features and
    values, with no missing features or extra features. Since we don’t use `size_type`
    directly with the model, we need to explicitly remove it from the dictionary of
    features and values with the `scoring_dict.pop('size_type')` statement.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The endpoint expects to get the features and values in a dictionary with the
    same format as the JSON that we used to exercise the endpoint directly in the
    console in section 10.4.4\. That means the values in the dictionary need to be
    converted to lists of values, each of which contains exactly one value. This statement
    converts the dictionary to lists of values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The `predict_custom_trained_model_sample()` function needs to be called with
    parameters specifying the project, endpoint, and key-value pairs for the features
    we want a prediction for.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to these updates to the Flask server module, we can also remove
    the statements that loaded the model from the local file system since we don’t
    use the local model in this solution.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE An updated version of the Flask server module that uses the endpoint is
    available at [https://mng.bz/Xx5a](https://mng.bz/Xx5a).
  prefs: []
  type: TYPE_NORMAL
- en: 'To run this version of the Flask server module, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.  Authorize the account you used to access Google Cloud with this application
    by running the following command on the command line in the local system where
    you are running the Flask server module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '2.  Start the Flask server module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'If you get an error related to the protobuf (protocol buffer) level, try the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This command adjusts the protobuf level to exactly what the endpoint requires.
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE You don’t need to know about the protobufs for the purposes of this application,
    but if you are curious, you can check out the documentation: [https://protobuf.dev/](https://protobuf.dev/).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.  Once the Flask server module is running, go to `localhost:5000` in a browser
    to exercise the deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have seen two kinds of deployments of the model: a web deployment run
    entirely from a local system and a web deployment that uses the model served from
    a Vertex AI endpoint.'
  prefs: []
  type: TYPE_NORMAL
- en: 10.5.3 Benefits of deploying a model to an endpoint
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If we get the same results from the model whether it is deployed in a simple
    web deployment or deployed to an endpoint, you may ask what the point is of deploying
    the model to an endpoint. There are a number of benefits that come from deploying
    to an endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: You can scale the endpoint instance to handle additional load.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can deploy multiple models to the same endpoint. Imagine a situation where
    you need to replace one model in production with another. If you deploy both models
    to the same endpoint, you can gradually adjust how much of the traffic goes to
    the new model without making any changes to the application and without causing
    jarring changes to the users of the application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can deploy the same model to different endpoints, allowing you to optimize
    the machine resources. For example, if you have a production and a development
    environment, you can deploy the same model to two endpoints, with higher-spec
    machine resources for the production environment and cheaper machine resources
    for the development environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With an endpoint, you can enable Vertex AI monitoring to detect skew (differences
    in the distribution between training data and the data seen applying the model
    in production) and drift (changes in the distribution of data seen by the model
    in production over time). Model monitoring can help to ensure that the model’s
    performance does not degrade over time and that changes in the data the model
    is being applied to in production do not occur unexpectedly. For more details
    on monitoring, see the documentation at [https://mng.bz/yWdd](https://mng.bz/yWdd).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 10.36 shows examples of multiple models deployed to the same endpoint
    and one model deployed to multiple endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: 'In figure 10.36, there are two scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Multiple models are deployed to the same endpoint*—Model A is the version
    currently in production, and Model B is the next level of the model that we want
    to introduce to production. By adjusting the values of X and Y, we can control
    the proportion of traffic that goes to each model, gradually increasing the proportion
    that goes to the new level of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Same model deployed to multiple endpoints*—Endpoint 1 has basic compute resources,
    and endpoint 2 has sufficient compute resources to handle our current production
    load. By doing this we can optimize the resource cost for the system to meet the
    needs of multiple groups of users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F36_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.36 Relationship between models and endpoints
  prefs: []
  type: TYPE_NORMAL
- en: NOTE For more details on model deployment in Vertex AI, see the documentation
    at [https://mng.bz/MDlB](https://mng.bz/MDlB).
  prefs: []
  type: TYPE_NORMAL
- en: '10.6 Gemini for Google Cloud: Generative AI assistance in Google Cloud'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As you worked through the steps in this chapter, you may have run into a roadblock
    or needed to clarify a question. To make progress, you may have referred to the
    documentation for Google Cloud, searched in Stack Overflow, or asked a colleague
    for help. In addition to these traditional sources of assistance, Google Cloud
    also includes an integrated, generative AI-driven source of assistance: Gemini
    for Google Cloud.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Gemini for Google Cloud is a set of generative AI capabilities for Google Cloud.
    Gemini for Google Workspace is a companion set of generative AI capabilities for
    Google Workspace (Google Docs, Sheets, Slides; see [https://workspace.google.com/solutions/ai/](https://workspace.google.com/solutions/ai/)).
    In this chapter and chapter 11, we’ll show how you can use generative AI via Gemini
    for Google Cloud to simplify the deployment of tabular data models and to automate
    some of scripting for machine learning pipelines. Gemini for Google Cloud provides
    a variety of capabilities, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Answering questions about Google Cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating code (including SQL and a variety of programming languages such as
    Python, Java, and JavaScript) from text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explaining code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NOTE See the Gemini for Google Cloud documentation for a more detailed overview
    of what Gemini for Google Cloud can do: [https://cloud.google.com/gemini/docs/overview](https://cloud.google.com/gemini/docs/overview).'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll cover setting up Gemini for Google Cloud and using it
    to answer questions about Google Cloud. We’ll also discuss some of the actions
    we completed in this chapter to deploy our model in a Vertex AI endpoint. In chapter
    11, we’ll return to Gemini for Google Cloud to show how you can use it to generate
    and explain code.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud is not the only cloud platform that harnesses generative AI to
    make it easier to use the platform and to automate some steps in the development
    workflow. Copilot in Azure and Code Whisperer in AWS are generative AI-based features
    that each provide a subset of the benefits provided by Gemini for Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 10.6.1 Setting up Gemini for Google Cloud
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following are instructions for setting up Gemini for Google Cloud:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting up Gemini for Google Cloud for a project: [https://cloud.google.com/gemini/docs/quickstart](https://cloud.google.com/gemini/docs/quickstart)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Setting up Gemini Code Assist, the part of Gemini for Google Cloud that provides
    AI assistance for development, to be used in Cloud Shell Editor in Google Cloud:
    [https://mng.bz/avMm](https://mng.bz/avMm)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you have set up Gemini for Google Cloud for a project, you will see the
    Gemini for Google Cloud icon in the toolbar (see figure 10.37).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F37_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.37 Gemini for Google Cloud icon
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have done the basic setup for Gemini for Google Cloud, we’ll see
    how we can use it to get answers to questions about Google Cloud in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 10.6.1  Using Gemini for Google Cloud to answer questions about Google Cloud
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Gemini for Google Cloud manifests itself in various ways in Google Cloud, including
  prefs: []
  type: TYPE_NORMAL
- en: '*In a chat pane that’s available throughout Google Cloud*—Figure 10.38 shows
    the Gemini for the Google Cloud chat pane.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F38_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.38 Gemini for Google Cloud chat pane
  prefs: []
  type: TYPE_NORMAL
- en: '*In a range of IDEs supported by Google Cloud, including VS Code, Cloud Workstations,
    and Cloud Shell Editor*—Figure 10.39 illustrates how Gemini for Google Cloud can
    generate a simple Python function from a comment in Cloud Shell Editor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F39_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.39 Gemini for Google Cloud generating Python in Cloud Shell Editor
  prefs: []
  type: TYPE_NORMAL
- en: '*In the query editors for Big Query and Spanner*—Figure 10.40 shows how Gemini
    for Google Cloud can generate SQL from a comment in the Spanner query editor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F40_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.40 Gemini for Google Cloud generating SQL in the query editor in Spanner
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have reviewed examples of where Gemini for Google Cloud can be used,
    let’s examine the Gemini for Google Cloud chat pane more to see how it can be
    used to get answers to questions about Google Cloud. You can click on the Gemini
    icon (see figure 10.37) to open the Gemini for Google Cloud chat pane, as shown
    in figure 10.41.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F41_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.41 Gemini for Google Cloud chat pane
  prefs: []
  type: TYPE_NORMAL
- en: Try out the Gemini for Google Cloud chat pane by entering an instruction prompt
    in the prompt field and clicking on the Send button, as shown in figure 10.42.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F42_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.42 Gemini for Google Cloud chat pane Send button
  prefs: []
  type: TYPE_NORMAL
- en: The Gemini for Google Cloud chat pane is available whenever you need it in Google
    Cloud, and you can use it to ask all kinds of questions about Google Cloud. For
    example, suppose we want to create a Cloud Storage bucket and we forgot the steps
    listed earlier in this chapter. Figure 10.43 shows the response given by Gemini
    for Google Cloud to the prompt “How to create a Google storage bucket.”
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH10_F43_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.43 Gemini for Google Cloud response to “how to create a Google Cloud
    storage bucket”
  prefs: []
  type: TYPE_NORMAL
- en: Gemini for Google Cloud has been trained specifically to provide useful answers
    to questions about using Google Cloud. Try out Gemini for Google Cloud by giving
    it prompts related to some of the sections in this chapter. For example, try “how
    to import a custom model into Vertex AI” or “how to deploy a model to a Vertex
    AI endpoint” and see how the responses from Gemini for Google Cloud compare to
    the steps listed in the analogous sections in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deploying your model in a simple web application can give you a sense of its
    characteristics in production.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can create a simple web application to deploy the model we trained in chapter
    9\. This application includes a Flask server module and two simple web pages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A public cloud environment provides an environment for deploying a model that
    allows you to scale capacity and control availability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To prepare to deploy the model we trained in chapter 9 in Google Cloud, we need
    to create a Google Cloud project, create a Google Cloud bucket, and upload the
    model to the bucket.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once we have completed the steps to prepare for a Google Cloud model deployment,
    we can deploy a trained model to a Vertex AI endpoint.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can test the Vertex AI endpoint deployment of the model by making a few simple
    updates to the Flask module from the web application that we created at the beginning
    of this chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying a model to an endpoint in Vertex AI makes the deployment more robust.
    In particular, we can specify the machine resources that are appropriate for our
    application, provide a mix of model levels, and monitor the model’s performance
    in production.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gemini for Google Cloud provides generative AI capabilities that can be helpful
    for model development and deployment tasks in Google Cloud.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
