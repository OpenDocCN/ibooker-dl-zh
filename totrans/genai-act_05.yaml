- en: 5 What else can AI generate?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using generative AI for code creation and code-related tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tools that allow code generation and how to use them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best code generation practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating video and related tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating audio, music, and related tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code that writes itself with little prompting and without much input seems magical,
    resembling a holy grail, at least to those working in computing. Given the advancements
    in artificial intelligence (AI) with generative AI, this endeavor seems possible
    today. We have seen some amazing and interesting things AI can generate—from language
    to images to holding an ongoing back-and-forth multiturn conversation—and many
    of them have strong use cases in enterprises. This chapter outlines the remaining
    things we can generate using AI.
  prefs: []
  type: TYPE_NORMAL
- en: We will first talk about code generation, what it means, how one should go about
    it, and the tools enterprises use. For example, Andrej Karpathy, one of the OpenAI
    cofounders, who used to lead Tesla’s AI and Vision team, recently said that GitHub
    Copilot helps him write approximately 80% of his code, which is a huge boost in
    productivity. Then, we will cover a few very early generations and explore application
    in videos and music. Let’s see how code generation works.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Code generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Generative AI is not just about completions, chats, or generating images. It’s
    a technology that can significantly enhance developers’ productivity and improve
    software development processes in enterprises. One of its most intriguing aspects
    is the ability to generate code and aid in code understanding and documentation.
    From a development lifecycle perspective, the term “code generation” can be misleading,
    as it encompasses much more than code generation itself. It spans various aspects
    of software development. Here are a few examples of how enterprises employ code
    generation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Code generation*—Augments development by generating code for a given prompt.
    This isn’t complete code for whatever is being built but code at the function
    level.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Productivity improvements*—Tools based on generative AI can help improve developers’
    productivity, especially when using new libraries and software develop-ment kits
    (SDKs) or programming languages that might be new for a developer. We can also
    improve the speed of implementation of much of the scaffolding (such as AI wrappers,
    database queries, etc.) that many enterprise applications need to implement, such
    as access control, encryption, and security, to name a few.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Onboarding new employees*—For enterprises, it is quite common to have internal
    proprietary development standards, internal libraries, and SDKs that encapsulate
    a lot of domain and institutional knowledge and IP. Generative AI tools can help
    new full-time employees (FTEs) get ramped up and trained quickly using these SDKs
    and libraries. New FTEs can also serve as a model to explain snippets, helping
    developers learn quickly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Automation*—Many development tasks are repetitive, and it is common for many
    developers to skip them or take shortcuts, which can cause problems down the road.
    Generative AI can help automate repetitive tasks such as code reviews, testing,
    documentation, design iterations, UI mockups, and so forth.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Fostering creativity*—Generative AI tools can help developers see different
    approaches and ideas when coding or rapidly prototyping, encouraging them to explore
    newer techniques that might be better and help teach.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we get into the details, we will start with code generation examples.
    Say we want to write a function to calculate its time complexity. Time complexity
    measures the length (i.e., the time) a function will take to execute. It is often
    expressed using Big O notations—constant, linear, quadratic, and exponential time.
  prefs: []
  type: TYPE_NORMAL
- en: Note  For brevity, we won’t show the full test code generation here; this can
    be found in the books accompanying the GitHub repository at [https://bit.ly/GenAIBook](https://bit.ly/GenAIBook).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with a simple toy example using GitHub Copilot in our IDE. A comment
    is the prompt, and the model completes the code generation, as shown in figure
    5.1\. Regarding the developers’ experience, this might seem like a fancier version
    of autocomplete, but it is much more than that. We can think of the code generation
    as the completion API we saw earlier, with the difference being that what will
    be generated will be code.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F01_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 Code generation to calculate time complexity
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The first suggestion, in the grey text (also called ghost text), seems good;
    if we want, we can get up to 10 suggestions and find a better one. Figure 5.2
    shows a snippet of these alternate generations.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F02_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 GitHub Copilot code completion suggestions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In this instance, the last suggestion (number 10) seems better and is what we
    will use, as shown in figure 5.3.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F03_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 AI-generated code to calculate the time complexity of a function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 5.1.1 Can I trust the code?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the context of code generation, one of the areas that many enterprises are
    considering is how to trust the generated code. Let’s take the example of generating
    complex code, such as implementing OAuth2 for a web application, as shown in figure
    5.4\. In general, code-generation tools are becoming increasingly reliable and
    accurate. However, it is still important to be aware of the code limitations;
    whether one can trust generated code depends on several factors, including
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F04_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 Code generation showing OAuth2 implementation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The quality of the tool and underlying model pinning that code generation tool.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The complexity of the task the code is being generated for; some tools are better
    suited for well-defined tasks than complex logic and reasoning tasks that can
    result in error.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When using AI-generated code, trust and review are paramount. The code and associated
    tools should always be used with other development tools and processes, such as
    code reviews and unit tests, which ensure that the generated code meets the required
    standards and is free from errors or vulnerabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to note that GitHub Copilot does not guarantee that the code
    it generates is correct, bug-free, or secure. The developer is still responsible
    for reviewing, testing, and verifying the code before using it.
  prefs: []
  type: TYPE_NORMAL
- en: GitHub Copilot does provide some features to help developers ensure the quality
    of the code, such as code review, testing, and feedback. In addition, it has several
    guardrails in place to help prevent it from generating incorrect or harmful code.
    For example, GitHub Copilot has filters that block offensive words and code that
    is likely biased or discriminatory.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, GitHub Copilot also performs several safety checks before generating
    code, such as for potential syntax errors and security vulnerabilities. GitHub
    Copilot’s AI-based vulnerability prevention system is a feature that aims to make
    the code suggestions more secure and help developers avoid common security flaws
    in their code. It works using a machine learning model that can detect insecure
    coding patterns in real time and block them from being suggested. It also generates
    a new suggestion that does not contain the vulnerability. Some of the vulnerabilities
    that the system can protect against are
  prefs: []
  type: TYPE_NORMAL
- en: '*Hardcoded credentials*—This is when sensitive information such as passwords,
    API keys, or tokens is embedded in the source code, meaning attackers can access
    it easily. The system can identify hardcoded credentials and replace them with
    placeholders or environment variables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*SQL injection*—This is when user input is directly inserted into a SQL query,
    allowing attackers to execute malicious commands on the database. The system can
    identify SQL injection vulnerabilities and suggest using parameterized queries
    or prepared statements instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Path injection*—This occurs when user input is used to construct a file path,
    allowing attackers to access or modify files outside the intended scope. The system
    can identify path injection vulnerabilities and suggest using sanitization functions
    or validation checks before using the input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code generation tools can be powerful allies for enterprise developers but require
    careful and responsible use. As outlined by the National Institute of Standards
    and Technology, one of the best ways to secure code is to use a secure software
    development lifecycle.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen a simple example of what is possible, let’s see how we
    can do this. The next section will explore common tools such as Tabnine, Code
    Llama, and Amazon’s CodeWhisperer. However, in this section, we will talk about
    GitHub Copilot.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2 GitHub Copilot
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A few tools are now available for code generation. Most enterprises use GitHub
    Copilot, one of the first code-generation tools on the market. GitHub Copilot
    is a cloud-based generative AI tool that helps developers by generating code based
    on natural language prompts. It uses models from OpenAI, has been trained on billions
    of lines of code, and is positioned as our new AI pair programmer—one that helps
    us write code better, solve problems, understand new APIs, and write tests without
    trawling through a ton of information and sites searching for answers. The high-level
    flow is shown in figure 5.5.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F05_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 GitHub Copilot high-level flow using Visual Studio Code
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: GitHub Copilot runs as an add-in and supports many of the leading programming
    languages available for some of the leading IDEs (e.g., Visual Studio, Visual
    Studio Code, Neovim, and JetBrains). It supports about a dozen primary programming
    languages, such as C, C++, Java, C#, Python, Go, Ruby, and many more, as well
    as secondary and relatively less-supported languages (such as COBOL). All the
    languages that GitHub Copilot supports are listed at [https://docs.github.com/](https://docs.github.com/).
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen, the current version of GitHub Copilot takes a prompt via a
    comment, considers the context of the file a developer works on in the IDE, and
    then helps make the suggestions in the code. The results for developers across
    the board have been amazing. According to research published by GitHub, 96% of
    developers are faster on repetitive tasks, 88% feel more productive, and nearly
    75% focus on more satisfying things. Code generation is not about creating complete
    solutions or end-to-end code but rather about creating parts of code that can
    help with a specific function or some core logic within a function.
  prefs: []
  type: TYPE_NORMAL
- en: Copilot requires a subscription and comes in two versions, one targeting individuals
    and the other targeting enterprises. The underlying model powering both is the
    same, the main differences being that the enterprise version has additional controls
    for managing telemetry and enterprises can enforce organization-wide policies.
  prefs: []
  type: TYPE_NORMAL
- en: 'When considering privacy and data protection, GitHub Copilot (the business
    edition) collects information in three areas, as outlined in the following list.
    These help with the overall service health, experience-latency, and feature engagement
    and also help fine-tune and improve the algorithms for ranking and sorting completions.
    In addition, they can aid in detecting abuse of the service and policy violations:'
  prefs: []
  type: TYPE_NORMAL
- en: '*End-user engagement data*—GitHub Copilot collects the end-user’s interaction
    with the IDE when using Copilot. This includes usage and error details, as well
    as data on actions taken by the user, such as which of the generated completions
    was accepted. Some personal data might be included but is not tied directly to
    the user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Prompts*—For enterprise users, the prompts are ephemeral, employed only when
    using the service, and not retained. For individual users, the prompts persist,
    but the user has the option of deactivating them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Completions (i.e., suggestions)*—The completions, similar to the prompts,
    are ephemeral, transmitted back to the Copilot extension running in the IDE, and
    are not persisted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copilot uses more than just the prompt when trying to create suggestions. In
    addition to the prompt, it also factors in the edited file and the other tabs
    and files in the solutions open for context. Furthermore, it combines all of that
    as grounding and context information to allow for more meaningful and better generations.
    And this generation goes beyond the code, stylistic patterns, and syntactic sugar.
  prefs: []
  type: TYPE_NORMAL
- en: Let us use a simple example. Say we want to generate a function that we will
    employ to generate an image using Stability AI, which we did in the previous chapter.
    We use the following prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Prompt.png)**Write a Python function that takes a prompt
    and uses stability AI to generate an image and save it to a file.'
  prefs: []
  type: TYPE_NORMAL
- en: When we have an empty solution with just a few lines of code to get this started,
    we get the code shown in figure 5.6 with the `generate()` function generated by
    GitHub Copilot. As we can see, this is rather simple and goes through the mechanics
    of first encoding the prompt to a `base64` format. It calls the completion API,
    extracts the image from the API response, decodes it from `base64`, and then finally
    saves it to a file using a date–time stamp as the file name. This was discussed
    in detail in the previous chapter, and there is nothing wrong with the code. It
    is a pretty vanilla implementation.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F06_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 GitHub Copilot code generation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: However, we must follow programming standards, architecture patterns, and methodologies.
    Otherwise, the code shown in the previous example would not work and would require
    more manual effort. So how can we address this?
  prefs: []
  type: TYPE_NORMAL
- en: Let’s generate another function using the same prompt. This time, we open a
    file in our existing solution for image generation that we used in the previous
    chapter. Listing 5.1 shows the generated code. This code seems quite familiar,
    as it closely follows our syntax and patterns for generating images from the previous
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The interesting thing in this example is how GitHub Copilot generated the helper
    functions to check for paths, clean up filenames, and so forth, even when we did
    not explicitly ask for it. This pattern was common across a few files in the image
    generation solution (from the last chapter), which was picked up as context. The
    updated code saves the prompt as part of the filename, not just a date–time stamp.
    Again, this was not explicitly asked, and while it might seem like syntactic sugar,
    patterns and architecture requirements such as these make the codebase maintainable,
    robust, and familiar in an enterprise setting.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.1 GitHub Copilot generation in an existing solution
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 5.1.3 How Copilot works
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When GitHub Copilot was first released, GitHub worked closely with OpenAI to
    create a special version of GPT3 called Codex. This version was trained on both
    natural language and billions of lines of code. Codex supports multiple programming
    languages and can be used for multiple code-related tasks. Today, Codex is deprecated,
    as the same learnings have been incorporated into the mainline GPT models.
  prefs: []
  type: TYPE_NORMAL
- en: Copilot is building a separate prompt all the time in the background, which
    is one reason we see completions not only when prompted but throughout when writing
    code—starting or in the middle of something else. Starting with a prompt line
    and the corresponding code file using Codex was just the beginning. Copilot now
    looks at several things when suggesting generations. The prompt library is where
    algorithms take into account the broader context of what a developer is doing
    and create the prompt used by the model. In addition to the code file and the
    prompts we enter, this also considers the other open tabs and the broader solution,
    as shown in our earlier demo. Figure 5.7 illustrates this high-level flow and
    the life cycle.
  prefs: []
  type: TYPE_NORMAL
- en: One behavior of particular interest is a feature called fill-in-the-middle (or
    FIM). As the name suggests, the code is not generated at the end of a file, but
    in the middle. Before FIM was implemented, the code after the cursor’s current
    position was ignored; now it helps fill in the missing code, considering the code
    before and after the insertion point, taken in the full context.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F07_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 Copilot completion lifecycle
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The newer version, Copilot Chat, uses a chat-like interface similar to ChatGPT.
    This chat-like feature offers a much richer experience and modality from a developer’s
    perspective and allows us to take in more than just the prompt or the code. It
    helps us with much richer context (of the code and errors) and lets us spot any
    possible problems. This is also extensible to other aspects that developers use
    daily—from helping understand legacy code to unit test generation. The original
    version of Copilot used Codex, a fine-tuned version of GPT-3\. Codex is now retired,
    and the newer versions of Copilot Chat use newer models. Let’s examine some of
    these areas in more depth.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Additional code-related tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to code generation, there are other use cases that can be utilized
    in the context of code and improving developer productivity. Some of these are
    the generation of other aspects, such as unit tests or documentation. Let’s start
    with one of the features called code explanation.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 Code explanation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the powerful features of GitHub Copilot Chat is that it offers a more
    expressive medium to interact with the code. One example is being able to chat
    and ask for an explanation of the selected code in the IDE.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.8 illustrates an example of code explanation where we use one of our
    earlier completions and naturally interact with and use the AI to help us generate
    an explanation. The screenshot doesn’t show it, but GitHub Copilot Chat explains
    different parameters and their meaning.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F08_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 GitHub Copilot explanation example
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As outlined earlier, Copilot can also help explain legacy code, which might
    be in legacy languages such as COBOL, as shown in figure 5.9.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F09_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 Copilot Chat explaining the COBOL code
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 5.2.2 Generate tests
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can build on the previous example to demonstrate how to help generate tests
    for a given code set, as shown in figure 5.10\. This feature helps developers
    save precious time and effort in writing unit tests, making them more productive.
    It can also help produce novel and diverse test cases that cover different scenarios
    and edge cases compared to what most developers could create themselves.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F10_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 Generating unit tests
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'GitHub Copilot Chat helps generate unit tests and will test whether the `openai
    .completion.create()` method works as expected if the print statements output
    the correct strings. The unit tests can handle the nondeterministic behavior of
    AI by using mocking, following the steps as listed:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the necessary modules for testing, such as `unittest` and `mock`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new class for the test case, inheriting from `unittest.TestCase`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Within this class, create a setup method to initialize the environment for the
    tests.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a test method to test the `openai.completion.create()` method. Use `'mock'`
    to simulate the response from the OpenAI API.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a test method to test the output of the `print` statements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the end of the script, add a line to run all the tests when the script is
    executed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Of course, a developer still needs to check the tests and ensure they fit the
    purpose. Generated tests can have many limitations, from covering only some possible
    scenarios (e.g., complex data behavior or accounting for user interactions) at
    one end to code maintainability.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3 Code referencing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Code referencing is a feature that helps developers detect the code generated
    by Copilot against public repositories on GitHub for any matches. This action
    is not default and is a setting that needs to be enabled in the Copilot configuration.
    The advantage of code referencing is that it helps developers make more informed
    decisions about their code. Code referencing shows when a code suggestion matches
    public code on GitHub and provides information about the repositories where that
    code appears and their licenses.
  prefs: []
  type: TYPE_NORMAL
- en: This way, developers can learn from others’ work, discover documentation, avoid
    potential legal problems, and give or receive credit for similar work. Furthermore,
    code referencing allows developers to ask GitHub Copilot to rewrite the code if
    they want a different implementation.
  prefs: []
  type: TYPE_NORMAL
- en: GitHub Copilot automatically matches the (approximately 150 characters) code
    it generates against repositories. It finds similar code and outlines its associated
    licensing terms, if any. This allows us to accept or reject the code suggestion.
    We can also ask Copilot to rewrite and create a new generation that differs from
    the matching one.
  prefs: []
  type: TYPE_NORMAL
- en: According to research released by GitHub [1], less than 1% of code generation
    ends up matching, and while that is a small percentage, it is not evenly distributed
    across the spectrum. Most of it occurs when the code file is new and empty, as
    there is little additional context for the solution. This is rare in cases when
    there are multiple files and existing solutions, as the code generation is much
    more specific to the situation and prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, many of these matches are patterns of libraries that are code
    fragments posted to popular sites such as Stack Overflow, often without attribution.
    Frequently, many are also core APIs of common libraries used across many projects
    that are taking a dependency on those specific libraries. From an enterprise and
    developers’ perspective, there are several benefits to using code referencing:'
  prefs: []
  type: TYPE_NORMAL
- en: It helps enterprises make a build-versus-buy decision by understanding whether
    they can depend on an existing open source library to reduce the need for new
    business logic and cost.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It helps developers improve their coding skills, especially by examining how
    others have solved similar problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For many enterprises, the default position is often to avoid code matching public
    repositories; thus, code referencing allows them to choose the source appropriately
    and give credit to the author.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It helps developers understand the relevance and quality of the code before
    taking a dependency and accepting a suggestion that matches the public code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the topic or library is new, it helps developers explore new projects and
    collaborate with other developers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5.2.4 Code refactoring
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GitHub Copilot Chat helps with code refactoring by providing intelligent suggestions
    across the solution, thus improving the code’s structure, readability, and maintainability.
    Some ways it can assist with code refactoring are
  prefs: []
  type: TYPE_NORMAL
- en: Simplifying complex expressions or statements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting repeated code into functions or methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding comments or documentation to explain the code logic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Renaming variables or functions to follow naming conventions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another set of experimental features of Copilot is called Labs, where we can
    use different aspects to understand the code and help refactor it—whether by making
    it more readable, more robust, or more error-proof or even by helping us isolate
    and understand a bug in the existing code (figure 5.11).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F11_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 Copilot tools for refactoring
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 5.3 Other code generation tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GitHub Copilot is one of the first and, as of now, most commonly used code generation
    tools, especially in enterprises. However, other code generation tools are learning
    from Copilot and are starting to appear. While the details of how each works differ
    slightly, using different language learning models (LLMs) at a high level, they
    all operate very similarly to what we outlined earlier in the chapter. This section
    provides a quick overview of some of the other code generation tools available
    on the market. The intent is not to go deeply into them, as many are clones and
    offer the same functionality. It is to show how enterprises can evaluate and choose
    the ones that work best in their context and work more easily with their organizational
    development culture.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 Amazon CodeWhisperer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Amazon has CodeWhisperer, AWS’s answer to GitHub Copilot. It can generate code
    based on prompts and help write functions. It supports a narrower set of programming
    languages than Copilot and similar IDEs. CodeWhisperer is available via the AWS
    toolkit extensions, as shown in figure 5.12\.
  prefs: []
  type: TYPE_NORMAL
- en: We don’t know the technical details of how CodeWhisperer works, so we can’t
    compare it directly with GitHub Copilot. However, we can say that CodeWhisperer
    and GitHub Copilot focus on different things. CodeWhisperer is more specialized
    for AWS services (such as EC2, S3, Lambda, etc.), while GitHub Copilot is more
    general purpose.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F12_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 Amazon CodeWhisperer
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Additional details on Amazon CodeWhisperer can be found at [https://aws.amazon.com/codewhisperer/](https://aws.amazon.com/codewhisperer/).
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Q AI assistant
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Amazon recently announced Amazon Q as a new AI assistant for AWS that targets
    enterprise customers. It can do more than help with coding. It can talk, offer
    advice, create content, and access different data sources and systems. Developers
    can use it to fix, improve, and understand code.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Q is an AI assistant that helps with coding and AWS tasks. It depends
    on CodeWhisperer. To use Amazon Q, you must pay for the Amazon CodeWhisperer Professional
    tier and install the latest AWS Toolkit. Amazon Q understands AWS better than
    CodeWhisperer, which mainly helps with coding. More details on Amazon Q can be
    found at [https://aws.amazon.com/q](https://aws.amazon.com/q).
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 Code Llama
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Meta recently released Code Llama, an LLM model targeting coding similar to
    Codex. Code Llama builds on Llama 2 by training it on more code-specific datasets.
    It can generate code and understand natural language about code. Like Codex and
    GPT4, it supports some of the more popular programming languages—Python, C++,
    Java, C#, and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code Llama is released as an OSS model, including the weights, and is free
    for commercial and research purposes, although it has a special license. It is
    available in three sizes: 7B-, 13B-, and 30B-parameter base models. Each base
    model is further fine-tuned and available in two variants—one specifically for
    Python and another for Instruct. Code Llama also supports input sequences of 100K
    tokens, allowing sending a longer application code base as context.'
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  Meta has chosen to release Code Llama under the same license as Llama
    2, which is permissive. This also ensures that enthusiasts, researchers, and businesses
    can use these models in academic research and commercial applications without
    restrictions. However, the license forbids using Llama 2 to train other LLMs,
    requiring a special license from Meta if the model is used in an app or service
    with over 700 million monthly users.
  prefs: []
  type: TYPE_NORMAL
- en: Being smaller in a production deployment, the 7B and 13B base models require
    fewer resources in the sense of computing power (GPU), memory, and power; therefore,
    these models can be faster for inference and are better suited for low-latency
    scenarios where faster responses are required. Note that the exact definition
    of low-latency, of course, would be dependent on the use case and scenarios at
    hand. These two base models and their fine-tuned versions also support FIM capabilities,
    which Meta calls infilling.
  prefs: []
  type: TYPE_NORMAL
- en: Note  Consumer-class GPUs are for general consumers who want to play games or
    edit videos. They are cheaper, use less power, and have less memory than data-center-class
    GPUs. Data-center-class GPUs are for professionals who need high performance and
    reliability. They are more expensive, powerful, and have more memory and special
    features than consumer-class GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: This is the model itself, and as of publication, there isn’t a toolset around
    it like GitHub Copilot. Enterprises and other companies would need to take the
    model, host it themselves, and require GPUs for inference and managing lifecycles.
    The small models can be run on a consumer-class GPU when quantized. Quantization
    is a technique that reduces the number of bits used to represent the model’s parameters,
    which can save memory, speed up inference, and improve energy efficiency. However,
    quantization can also introduce accuracy loss or hardware inefficiency if not
    done properly.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.13 shows the generation using the chat completion of Code Llama. While
    it is a little different, it is still similar to what we have seen thus far. The
    full generated code can be found in books accompanying the GitHub repository at
    [https://bit.ly/GenAIBook](https://bit.ly/GenAIBook).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F13_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.13 Code Llama generating function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You can find more details on Code LLama at Meta’s site ([https://llama.meta.com/code-llama](https://llama.meta.com/code-llama)).
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.3 Tabnine
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tabnine is another AI-powered assistant that helps a developer, similar to GitHub
    Copilot. It provides real-time completions, and it has recently announced a chat-like
    feature. Tabnine can help complete code blocks and functions (see figure 5.14).
    As an advantage, Tabnine offers an option to be run locally or in the cloud, although
    its default mode is hybrid (i.e., using both). Tabnine supports more IDEs and
    the same programming languages, including C, C++, C#, Java, Python, React, NodeJS,
    and so forth. Tabnine uses a proprietary LLM trained on OSS libraries, and enterprises
    can run in a Kubernetes cluster on-premises. More details on Tabnine can be found
    at [https://www.tabnine.com/install](https://www.tabnine.com/install).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F14_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.14 Tabnine code generation in Visual Studio Code
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Note that this is not an exhaustive list of tools that enterprises and developers
    can use as AI-based tools for code generation and other code-related tasks. It
    does show the more commonly used ones in the context of enterprises. A few additional
    notable ones are
  prefs: []
  type: TYPE_NORMAL
- en: '*Codey**—Google’s foundation code generation model supports over 20 languages.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**   *Gemini**—Google’s answer to ChatGPT now supports code generation. At
    the time of publication, it still did not offer integration into an IDE. It was
    a standalone in the chat paradigm that allowed the copy and exporting of the code
    into Google Colab notebooks. Google launched this feature as Bard, which was rebranded
    and powered by a new multimodality model called Gemini.***   *CodeT5+**—Salesforce
    has a new family of code LLMs that are OSS and can support both generation and
    understanding; these can be adapted to downstream tasks.***   *StableCode**—Stability
    AI, the company behind the Vision models we saw earlier, recently announced a
    code-based base LLM. This is an OSS model that also supports multiple programming
    languages. In addition to the base model, there is an instruct model that would
    be more useful for most developers. Out of the box, it has no IDE integration.****'
  prefs: []
  type: TYPE_NORMAL
- en: '***Note Many of the OSS models that do not have an IDE integration can be hosted
    on Hugging Face and called by another Visual Studio Code extension—`huggingface-vscode`.
    This code completion extension allows us to use most OSS models. More details
    on the extension can be found at the GitHub repository ([https://github.com/huggingface/huggingface-vscode](https://github.com/huggingface/huggingface-vscode)).
    This extension can also be configured to call a custom endpoint that is not a
    Hugging Face interference API.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.4 Check yourself
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Code generation tools can be very helpful for enterprise developers, as they
    can save time, reduce errors, and improve productivity. However, code generation
    tools are imperfect and require human oversight, and validation. Here are some
    tips on how to trust and use these code generation tools effectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Choose the right tool for the right task*. Code generation tools vary in their
    capabilities, quality, and suitability for domains and languages. Developers should
    evaluate the available tools and select the ones that best suit their needs and
    preferences. For example, some tools may be better for generating UI components,
    while others may be better for generating business logic or data access layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Follow the best practices and guidelines for code generation*. Code generation
    tools often provide documentation and examples of using them properly and efficiently.
    Developers should follow these best practices and guidelines to ensure the quality
    and consistency of the generated code. For instance, some tools may require certain
    naming conventions, annotations, or templates to work correctly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Review, test, and verify the generated code*. Code generation tools are not
    a substitute for human expertise and judgment. Developers should always review,
    test, and verify the generated code before production. They should check for errors,
    bugs, security vulnerabilities, performance problems, readability, maintainability,
    and compliance with standards and regulations. They should also compare the generated
    code with similar snippets and suggest improvements if needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Provide feedback and report problems to the tool providers*. Code generation
    tools are constantly learning from new code and feedback from developers. Developers
    should provide feedback and report problems to the tool providers to help them
    improve their products and services. They should also keep track of the updates
    and enhancements of the tools and learn how to use them effectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5.3.5 Best practices for code generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Irrespective of the tool we use, the concept of using LLMs for code generation
    and other code-related tasks is still very novel. Some best practices that should
    be considered in an enterprise when thinking about using generative AI and LLMs
    are
  prefs: []
  type: TYPE_NORMAL
- en: '*Design for imperfections*—The LLMs will be wrong and will hallucinate. The
    generated code could outline APIs that look good at the surface but might not
    be real. They also can be wrong and produce code that doesn’t compile and execute.
    In addition to being incorrect, sometimes the generated code can be inefficient.
    It is important to be aware of these limitations and take steps to mitigate them,
    including checking yourself as outlined earlier and using a technique called prompt
    engineering, which we will cover later in chapter 6\.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Clear and specific goals*—For the code generation task, ensure the goal is
    clear and specific. Consider the code needed, the inputs and outputs, and specific
    quality criteria. A clear vision of the desired outcome can help our code generation
    more effectively. This includes adding details on specific libraries and packages
    the code should use when not obvious, as it cannot guess our intent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Iterative prompts*—Small changes in the prompt can significantly change the
    generation. Consequently, iterating through prompts in small steps and their generated
    results would be important to managing this. The vaguer the prompt, the poorer
    the resulting generated code. Understanding the prompts is a combination of both
    art and science. We will cover details of prompt engineering later in the book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Evaluatio**n*—Use multiple metrics and methods to evaluate the quality of
    the generated code. This has many attributes, for example, syntax, semantics,
    functionality, readability, and maintainability. Where possible, we should use
    different dimensions of automated metrics (e.g., BLEU, ROUGE), human evaluation
    (e.g., surveys, interviews), testing (e.g., unit tests, integration tests), debugging
    (e.g., static analysis, dynamic analysis), and so forth.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Development standards*—Follow coding standards and best practices for the
    target programming language or framework you want to generate code for; if there
    are enterprise or industry standards, including them in existing code solutions
    will provide the context and hints for the generated code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let us switch modalities and outline a few areas of video and music generation
    that are still quite new and cover science and research. Given the speed of innovation,
    it won’t be long before these are more commonly available. Both generative AI
    music and video generation have the potential to revolutionize the way enterprises
    create and distribute content. As technology continues to develop and become more
    accessible, we can expect to see more and more enterprises using it to create
    innovative and engaging experiences for their customers and employees.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Video generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Video generation using generative AI is a young but rapidly developing field,
    with many potential applications. Some organizations use video generation to enhance
    creativity and innovation by generating novel and original content that can attract
    and engage customers. Others use it to personalize customer experience by creating
    video content according to the preferences and needs of individual customers,
    such as their mood, taste, location, or behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Some companies are already using this in production. YouTube is using generative
    AI to create personalized video thumbnails for its creators. Walmart uses generative
    AI to create personalized video ads for its customers. Some use cases are even
    more compelling. For example, ALICE Receptionist is a company that provides a
    virtual receptionist service for businesses. They use generative AI to create
    videos of multilingual customer support agents that can greet and assist visitors
    in different languages. Ran is a sports broadcasting company that covers various
    sports events and leagues. They use generative AI to create sports coverage with
    virtual anchors that can commentate and analyze the games in real time. Some of
    the key use cases for video generation are
  prefs: []
  type: TYPE_NORMAL
- en: '*Marketing content*—Generative AI can be used to create marketing videos that
    are more personalized and targeted, such as videos that promote a product to a
    specific audience based on their interests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Entertainment content*—Generative AI can be used to create entertainment videos
    that are more creative and innovative. For example, it is possible to create videos
    that help enhance a movie or TV program, tell a story, or play a game.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Educational content*—Generative AI can be used to create educational videos
    that are more engaging and interactive than traditional ones. For example, a generative
    AI model could be used to create a video that explains a complex concept by using
    animation and narration and can be used in the context of the difficulty level
    of the student.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Synthetic data*—Generative AI is capable of generating data that is not real
    (i.e., synthetic data) and that can be used as the input training data for other
    ML model creation. This is helpful in scenarios where the real data is impossible
    or impractical. For example, NVIDIA uses generative AI to create synthetic training
    data for its self-driving cars, allowing them to obtain data on various edge cases.
    Disney is using synthetic data to develop new ride and attraction concepts and
    optimize the layout of its theme parks, which allows it to use synthetic data
    to test and refine new products and services before releasing them to the public.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of the most common methods that allow this video generation are
  prefs: []
  type: TYPE_NORMAL
- en: '*Text-to-video synthesis*—This method follows the paradigm we have seen so
    far: generating a video using a prompt. Like image generation, the model learns
    to associate words and phrases with visual concepts and then uses this knowledge
    to create a video that matches the text description.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Image-to-video synthesis*—This method generates a video from a source image
    instead of a prompt. The model learns to associate image features with visual
    concepts and then uses this knowledge to create a video that matches the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Video-to-video synthesis*—Similar to the earlier method, this method uses
    a source video to create a new video. The model learns to identify the underlying
    structure of the original video and then uses this knowledge to create a new video
    with the same structure but different content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*GAN-based video generation*—This method uses a generative adversarial network
    (GAN) to create a video.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Several AI video generators are available that can help you easily create videos.
    Here are some examples of AI video generators that use generative AI:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Sora*—A diffusion model that differs from usual video generation methods that
    directly predict each frame. OpenAI announced this new AI model to make realistic
    and creative video scenes from text instructions. Sora begins with a basic static
    noise pattern and slowly changes it into a detailed video, frame by frame. It
    starts with noisy video frames. Each step removes noise to produce fine details.
    This process ensures the videos are visually pleasing and contextually correct
    based on the input text. When Sora was published, it was not given access by Open
    AI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Pictory*—An AI-powered video creation tool that allows users to create videos
    from text, images, and videos. It offers various features for editing and customizing
    videos, such as adding captions, transitions, and music. Pictory can also help
    summarize long videos into shorter ones.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Synthesia*—A cloud-based platform that allows users to create videos with
    AI-generated presenters. Users can choose from various avatars and voices and
    add text, images, and gestures to their videos.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*NVIDIA Canvas*—A cloud-based AI tool that allows users to create realistic
    paintings from text descriptions. It uses a GAN-based approach to generate paintings
    and can be used to create paintings of various subjects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Meta Make-a-Video*—A generative AI system that can create videos from text
    or image inputs. It uses many text-image pairs and unlabeled videos to learn how
    to generate realistic and diverse videos that match the given prompts. It can
    also create variations of existing videos or add motion to static images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Viddyoze*—A desktop application that allows users to create videos from text,
    images, and audio. Viddyoze uses various AI techniques to generate realistic videos,
    giving users more control over the creative process, including features such as
    transitions, effects, graphics, and so forth.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Powtoon*—A cloud-based platform that allows users to create videos from text,
    images, and audio. It uses various AI techniques to generate realistic videos
    using a variety of templates and features that can be used to create videos for
    different purposes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Dream*—An app by WOMBO that uses AI to generate images and videos based on
    a user’s input of a keyword or phrase. Wombo Dream will generate a creative and
    visually appealing image or video.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Wochit*—A cloud-based platform that allows users to create videos from text,
    images, and videos. It focuses on making the process as collaborative as possible.
    Wochit allows users to work together to create videos and offers various features
    for sharing and distributing videos.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some of these tools make it very simple to interact with and edit via a GUI
    before generating a video. Figure 5.15 shows that by using Wochit, we can edit
    scenes, including music being used, the look and feel of text, and any other elements
    in a generated video. In our example, we use the following prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Prompt.png)**Top 5 places I should visit when on a trip
    to Seattle.'
  prefs: []
  type: TYPE_NORMAL
- en: The video generated can be found in the books accompanying the GitHub repository
    at [https://bit.ly/GenAIBook](https://bit.ly/GenAIBook).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F15_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.15 Wochit AI video generation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: These are just a few examples of how generative AI is used to create videos.
    Now let’s explore music generation.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Audio and music generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we thought video generation was in its infancy, in the context of enterprises,
    audio and music generation is much earlier in its lifecycle. Generative AI can
    generate audio, speech, music, or sound effects. Audio and music generation share
    many of the same AI methods, such as autoregressive models, GANs, and transformer
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although audio and music generation is a very new area, some of the potential
    applications of generative AI audio generation are quite interesting for enterprises
    to explore:'
  prefs: []
  type: TYPE_NORMAL
- en: Generating realistic sound effects for entertainment, such as movies and video
    games
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating personalized audio experiences for users
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating music for movies, video games, and other media
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving the quality of speech recognition and translation systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing new ways to communicate with computers, either by using new modalities
    or helping differently abled people
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of the examples of generative AI tools for music and audio are
  prefs: []
  type: TYPE_NORMAL
- en: '*OpenAI''s Jukebox*—Jukebox is a generative AI model that can create music
    in various classical, jazz, and pop styles. It has been trained on a massive music
    dataset, and it can generate new music indistinguishable from human-created music.
    This builds on OpenAI’s work for MuseNet; for more details on Jukebox, visit [https://openai.com/research/jukebox](https://openai.com/research/jukebox).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*OpenAI''s MuseNet*—MuseNet is another generative AI model that can create
    music in various styles. It has been trained on a dataset of over 1.5 million
    songs, and it can generate new music that is both creative and original.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Meta''s AudioCraft*—AudioCraft is a generative AI tool that can create music
    from text prompts. It has been trained on a dataset of over 20,000 hours of music
    and can generate music tailored to the specific text prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*NVIDIA’s Vocoder*—Vocoder is a generative AI tool that can generate realistic
    speech from text prompts. It has been trained on a dataset of human speech, and
    it can generate natural and intelligible speech.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Google MusicLM*—This language model was created by Google to generate music
    compositions based on text prompts. This is an experimental tool that, at the
    time of publication, was only available as part of Google’s AI Test Kitchen program,
    which essentially is a playground for Google and its customers to try things out
    ([https://mng.bz/0MmJ](https://mng.bz/0MmJ)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*MusicGen*—This language model uses prompts to create and generate music based
    on the provided prompt. Meta developed it as part of their AudioCraft research
    project, and it is an open source tool that anyone can use to create their music
    using Hugging Face Spaces. You can hear demos and read more details at [https://ai.honu.io/papers/musicgen/](https://ai.honu.io/papers/musicgen/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Riffusion*—This audio and music generation library works with stable diffusion.
    It essentially is a fine-tuned version of stable diffusion, where instead of images,
    the library creates images of spectrograms; these spectrograms can then be converted
    into audio clips. Riffusion supports different styles of music generation, such
    as funk, jazz, and so forth. More details can be found at [https://about.riffusion.com/](https://about.riffusion.com/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mo**û**sai*—This text-to-music generation system uses diffusion models to
    create high-quality music using prompts. It has two sets of diffusion models—one
    for generating melody and harmony and the second for generating the timbre and
    dynamics. Combining them allows us to handle complex musical notes and helps generate
    music in various genres and styles. More info is available at [https://mng.bz/j04a](https://mng.bz/j04a).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative AI allows us to generate code snippets and functions using a prompt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code generation is influenced by the context of the software solution, including
    the libraries being used, programming languages, code, and design patterns implemented.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative AI can also generate other software development lifecycle artifacts
    such as code understanding and documentation, testing code, and code refactoring.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code generation can help enterprises by augmenting developers, improving productivity,
    onboarding new employees, automating repetitive tasks, and fostering creativity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GitHub Copilot and Copilot Chat are the leading tools enterprises use and give
    a big productivity boost.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are additional code generation tools and open source models, such as AWS’s
    CodeWhisperer, Tabine, and Code Lama, as examples that are also available to enterprises.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Video generation is in its infancy, but several AI video generation tools, such
    as Pictory and Synethica, let enterprises use them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, audio and sound generation are still early in their development,
    but many tools and associated models, such as Jukebox, MuseNet, and AudioCraft,
    are available to enterprises.***
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
