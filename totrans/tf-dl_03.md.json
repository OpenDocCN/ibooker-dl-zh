["```py\n>>> import numpy as np\n>>> np.zeros((2,2))\narray([[ 0.,  0.],\n       [ 0.,  0.]])\n>>> np.eye(3)\narray([[ 1.,  0.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  0.,  1.]])\n```", "```py\n# Generate synthetic data\nN = 100\nw_true = 5\nb_true = 2\nnoise_scale = .1\nx_np = np.random.rand(N, 1)\nnoise = np.random.normal(scale=noise_scale, size=(N, 1))\n# Convert shape of y_np to (N,)\ny_np = np.reshape(w_true * x_np + b_true + noise, (-1))\n```", "```py\n# Generate synthetic data\nN = 100\n# Zeros form a Gaussian centered at (-1, -1)\n# epsilon is .1\nx_zeros = np.random.multivariate_normal(\n    mean=np.array((-1, -1)), cov=.1*np.eye(2), size=(N/2,))\ny_zeros = np.zeros((N/2,))\n# Ones form a Gaussian centered at (1, 1)\n# epsilon is .1\nx_ones = np.random.multivariate_normal(\n    mean=np.array((1, 1)), cov=.1*np.eye(2), size=(N/2,))\ny_ones = np.ones((N/2,))\n\nx_np = np.vstack([x_zeros, x_ones])\ny_np = np.concatenate([y_zeros, y_ones])\n```", "```py\n>>> tf.placeholder(tf.float32, shape=(2,2))\n<tf.Tensor 'Placeholder:0' shape=(2, 2) dtype=float32>\n```", "```py\n>>> a = tf.placeholder(tf.float32, shape=(1,))\n>>> b = tf.placeholder(tf.float32, shape=(1,))\n>>> c = a + b\n>>> with tf.Session() as sess:\n        c_eval = sess.run(c, {a: [1.], b: [2.]})\n        print(c_eval)\n[ 3.]\n```", "```py\n>>> N = 5\n>>> with tf.name_scope(\"placeholders\"):\n      x = tf.placeholder(tf.float32, (N, 1))\n      y = tf.placeholder(tf.float32, (N,))\n>>> x\n<tf.Tensor 'placeholders/Placeholder:0' shape=(5, 1) dtype=float32>\n```", "```py\nlearning_rate = .001\nwith tf.name_scope(\"optim\"):\n  train_op = tf.train.AdamOptimizer(learning_rate).minimize(l)\n```", "```py\n>>> W = tf.Variable((3,))\n>>> l = tf.reduce_sum(W)\n>>> gradW = tf.gradients(l, W)\n>>> gradW\n[<tf.Tensor 'gradients/Sum_grad/Tile:0' shape=(1,) dtype=int32>]\n```", "```py\nwith tf.name_scope(\"summaries\"):\n  tf.summary.scalar(\"loss\", l)\n  merged = tf.summary.merge_all()\n\ntrain_writer = tf.summary.FileWriter('/tmp/lr-train', tf.get_default_graph())\n```", "```py\nn_steps = 1000\nwith tf.Session() as sess:\n  sess.run(tf.global_variables_initializer())\n  # Train model\n  for i in range(n_steps):\n    feed_dict = {x: x_np, y: y_np}\n    _, summary, loss = sess.run([train_op, merged, l], feed_dict=feed_dict)\n    print(\"step %d, loss: %f\" % (i, loss))\n    train_writer.add_summary(summary, i)\n```", "```py\n# Generate tensorflow graph\nwith tf.name_scope(\"placeholders\"):\n  x = tf.placeholder(tf.float32, (N, 1))\n  y = tf.placeholder(tf.float32, (N,))\nwith tf.name_scope(\"weights\"):\n  # Note that x is a scalar, so W is a single learnable weight.\n  W = tf.Variable(tf.random_normal((1, 1)))\n  b = tf.Variable(tf.random_normal((1,)))\nwith tf.name_scope(\"prediction\"):\n  y_pred = tf.matmul(x, W) + b\nwith tf.name_scope(\"loss\"):\n  l = tf.reduce_sum((y - y_pred)**2)\n# Add training op\nwith tf.name_scope(\"optim\"):\n  # Set learning rate to .001 as recommended above.\n  train_op = tf.train.AdamOptimizer(.001).minimize(l)\nwith tf.name_scope(\"summaries\"):\n  tf.summary.scalar(\"loss\", l)\n  merged = tf.summary.merge_all()\n\ntrain_writer = tf.summary.FileWriter('/tmp/lr-train', tf.get_default_graph())\n```", "```py\nn_steps = 1000\nwith tf.Session() as sess:\n  sess.run(tf.global_variables_initializer())\n  # Train model\n  for i in range(n_steps):\n    feed_dict = {x: x_np, y: y_np}\n    _, summary, loss = sess.run([train_op, merged, l], feed_dict=feed_dict)\n    print(\"step %d, loss: %f\" % (i, loss))\n    train_writer.add_summary(summary, i)\n```", "```py\ntensorboard --logdir=/tmp/lr-train\n```", "```py\n# Generate tensorflow graph\nwith tf.name_scope(\"placeholders\"):\n  # Note that our datapoints x are 2-dimensional.\n  x = tf.placeholder(tf.float32, (N, 2))\n  y = tf.placeholder(tf.float32, (N,))\nwith tf.name_scope(\"weights\"):\n  W = tf.Variable(tf.random_normal((2, 1)))\n  b = tf.Variable(tf.random_normal((1,)))\nwith tf.name_scope(\"prediction\"):\n  y_logit = tf.squeeze(tf.matmul(x, W) + b)\n  # the sigmoid gives the class probability of 1\n  y_one_prob = tf.sigmoid(y_logit)\n  # Rounding P(y=1) will give the correct prediction.\n  y_pred = tf.round(y_one_prob)\n\nwith tf.name_scope(\"loss\"):\n  # Compute the cross-entropy term for each datapoint\n  entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_logit, labels=y)\n  # Sum all contributions\n  l = tf.reduce_sum(entropy)\nwith tf.name_scope(\"optim\"):\n  train_op = tf.train.AdamOptimizer(.01).minimize(l)\n\n  train_writer = tf.summary.FileWriter('/tmp/logistic-train', tf.get_default_graph())\n```", "```py\nn_steps = 1000\nwith tf.Session() as sess:\n  sess.run(tf.global_variables_initializer())\n  # Train model\n  for i in range(n_steps):\n    feed_dict = {x: x_np, y: y_np}\n    _, summary, loss = sess.run([train_op, merged, l], feed_dict=feed_dict)\n    print(\"loss: %f\" % loss)\n    train_writer.add_summary(summary, i)\n```"]