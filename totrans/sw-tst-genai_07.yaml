- en: 6 Rapid data creation using AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Generating basic test data using LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changing the format of test data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using complex data sets to prompt LLMs to create new data sets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating LLMs as a test data manager for automated checks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing test data is one of the most challenging aspects of testing and software
    development. Typically, data requirements grow with the complexity of a system.
    Having to synthesize data relevant to our context for automated checks and human-driven
    testing that handles complex data structures and anonymizes at scale and on demand
    can impose a huge drain on testing time and resources, which could be better spent
    on other testing activities.
  prefs: []
  type: TYPE_NORMAL
- en: However, we need test data. It is simply not possible to carry out most testing
    activities if we lack the necessary data to trigger actions and observe behavior.
    That’s why this chapter shows how we can use large language models (LLMs) to generate
    test data, providing different prompts to create both simple and complex data
    structures and integrate LLMs into our automation frameworks via third-party APIs.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Generating and transforming data with LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given that LLMs are powerful, probabilistic, text generators, it sounds logical
    that, with the right prompting, they can generate and transform test data easily.
    This is true, but it depends on writing clear prompts that communicate our data
    requirements explicitly, so that we get the right data we want, in the correct
    format and without any errors caused by hallucinations. There are many ways in
    which we can approach this, but let’s begin by looking at some basic prompts we
    can use casually to create test data for a range of testing activities.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.1 Prompting LLMs to generate simple data sets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To start, let’s explore how we can create basic data set examples such as this
    one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, the JSON data set has a mixture of data types using a fairly
    straightforward structure. We’ll see how to work with more complex structures
    later in the chapter, but for now, let’s return to a prompt that we used back
    in chapter 2 to create some sample test data. First, we set out the main instructions
    for the prompt and use the time-to-think principle to improve the quality of the
    output JSON:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You are a JSON data generator. Generate 5 JSON objects in an array and check
    that 5 JSON objects have been created before outputting the results. |'
  prefs: []
  type: TYPE_TB
- en: 'We use different delimiters to set out the rules for our data requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|    *   Each parameter is identified with a `%` sign.   *   Each column is
    described in order of key, value data type and options using the `&#124;` sign.  
    *   If a column data option says random, randomize data based on the suggested
    format and column name. |'
  prefs: []
  type: TYPE_TB
- en: 'Then we provide the data we want to be created following our delimited rule
    set:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Here are the instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: This prompt relies on a range of prompt-engineering tactics to set out explicitly
    what we want to see generated. We use the format tactic to declare what format
    we want our test data to be returned in. The delimiter tactic is used to set the
    rules for how we structure our data and what format values should be in. Also,
    we instruct the LLM to work out the solution before outputting it to reduce the
    risk of hallucinations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sending this prompt to ChatGPT returned the following data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: There are a few observations to make from this generated data. We can see that
    in places where the prompt has been explicit in data expectations, we get generated
    data that is close to our expectations—for example, the features `accessible`
    and `roomPrice`. There are, however, some weaker areas where our prompt has left
    the LLM to determine what to output. Two notable areas are the image URLs that
    have opted for `example.com` and the relationship between `beds` and `type`. The
    image URLs, although valid, don’t point to actual images. Depending on our requirements,
    we may need to tweak our prompt further to make the URLs more explicit. The other
    relationship is interesting as well, as one of the records states that the room
    type is double but it offers three beds. It’s a good start though, and there are
    times when this type of generated data is enough to support our testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'But what if we want this data in a different data structure? This can be handled
    by quickly reworking the prompt. For example, this prompt is requesting the same
    data, but this time in an XML format. We begin by rewording the initial instructions
    to create the data in an XML format:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You are an XML data generator. Generate 3 XML entries in an array and check
    that 3 XML objects have been created before outputting the results. |'
  prefs: []
  type: TYPE_TB
- en: 'We add an initial instruction to handle a root XML node:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|    *   The root node element name is identified with a `#` sign   *   Each
    element is identified with a `%` sign   *   Each column is described in order
    of element name, data type and options using the `&#124;` sign   *   If a column
    data option says random, randomize data based on the suggested format and column
    name |'
  prefs: []
  type: TYPE_TB
- en: 'Then we provide the same data steps as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Here are the instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Although it’s not necessarily common to have to handle different data formats
    in one application, the prompt demonstrates one of the advantages of working with
    LLMs to generate data. Using the structured output tactic, we’re able to keep
    most of the prompt the same as the JSON prompt example and simply modify it to
    give us XML; when I sent it to ChatGPT, I got the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Again, the output has similar attributes as the previous examples. The structure
    of the XML is valid, and the rules determining what data to use have been followed.
    But, similar to the JSON example, we have some unusual choices being made. Single
    rooms with three beds, oddly named rooms, and dummy URLs are used. With both prompts,
    we could add more information to mitigate these problems, but we run the risk
    of having to create a lot of rules in our prompt to manage the relationship between
    data points. There are, however, other choices we can make with our prompts to
    handle more complex rule sets, but first, let’s explore another way in which LLMs
    can help us thanks to their ability to transform data.
  prefs: []
  type: TYPE_NORMAL
- en: Activity 6.1
  prefs: []
  type: TYPE_NORMAL
- en: Using the prompt shared in this section, change the data structure to create
    either new XML or JSON test data.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.2 Transforming test data into different formats
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One of the core benefits of LLMs, which has been highlighted by advocates of
    the technology, is its ability to translate text from one language to another—for
    example, from French to English and back again. We can also use this approach
    to transform data and code from one structure or language to another. For example,
    take a look at the following prompt that transforms some JSON into a SQL statement.
    We use the delimiter tactic and instructions to start the prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You are a JSON to SQL transformer. Convert the JSON object delimited by triple
    hashes into a SQL statement that will:   1.  Create a SQL table to insert the
    transformed records into   2.  Create insert statements to add each record to
    a database |'
  prefs: []
  type: TYPE_TB
- en: 'Next, we instruct the model to check the solution to improve the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Check that each SQL statement covers all aspects of the JSON before outputting
    the results |'
  prefs: []
  type: TYPE_TB
- en: 'Then we provide the data that we want to see transformed:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this prompt, we’re asking an LLM not only to take a JSON object and convert
    it into a SQL `INSERT` statement that we could run, but also create the necessary
    `CREATE` statement to allow us to insert our data in the first place. Sending
    this to ChatGPT returned the following SQL statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'What stands out in this response is not just that correct SQL has been generated
    to create and insert our data into a database, but that it has honored the values
    from the original JSON object. It demonstrates the importance of tactics such
    as these that go into a prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Check that each SQL statement covers all aspects of the JSON before outputting
    the results |'
  prefs: []
  type: TYPE_TB
- en: The prompt helps ensure that the data parameters themselves are not modified
    during the transformation process.
  prefs: []
  type: TYPE_NORMAL
- en: These quick prompts demonstrate that LLMs can be used to rapidly generate and
    transform data with prompts that can be reused multiple times by replacing the
    data object inside the delimited portion of each prompt. This can prove useful
    for testing activities such as exploratory testing and debugging, where we require
    data fast to help us progress with our testing. But, as demonstrated, we can quickly
    come across either inconsistent or invalid data as our requirements become more
    complex.
  prefs: []
  type: TYPE_NORMAL
- en: Activity 6.2
  prefs: []
  type: TYPE_NORMAL
- en: Build a prompt that attempts to convert a piece of XML into either a SQL or
    JSON data structure. Ensure that the test data within the XML is transferred across
    without problems.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Processing complex test data with LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the original prompts to generate data, we set the rules and expectations
    in plain language. It means we are required to decode the structure of our data
    and its relationships before explicitly stating the learned rules in our prompt—a
    task that can quickly become quite complicated. Thus, instead of attempting to
    work out those rules ourselves, let’s take a look at how we can send different
    data specifications formats or existing data to prompt an LLM to create more complex
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.1 Using format standards in prompts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s begin by looking at how we can employ data specification formats such
    as OpenAPI v3 and XSD that outline the structure and rules our data has to follow.
    These types of specifications can be useful for a few reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Ready-to-go solutions*—Creators of specification frameworks have already handled
    the heavy lifting when it comes to communicating data structures in different
    formats. Consider the prompts we created earlier with the delimited rules that
    outline data names and types. All of this has already been considered and set
    out in specification frameworks. Therefore, it makes sense to use them rather
    than build our own.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ubiquity*—The frameworks we’ll use are standardized and have been adopted
    by lots of teams and organizations. This increases the likelihood that LLMs have
    been trained on specification frameworks, which will maximize our chances of obtaining
    a desired output when we send a prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Free use*—If we are working in teams that use tools such as OpenAPI and XSD
    to specify data structure or APIs, then the specifications are already available
    for us to use. The work is already done in the design phase for a feature or application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Intrinsic testability*—Employing a commonly used structure means that an LLM
    has likely been exposed to it more in its training than if we were using proprietary
    structures. This means that an LLM is likely to increase the probability of a
    higher-value output and assist our testing further.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given that this approach has a few benefits, let’s take a look at how they can
    be added to prompts to generate data for us.
  prefs: []
  type: TYPE_NORMAL
- en: JSON with OpenAPI
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start by creating a prompt that uses the OpenAPI 3.0 format to create
    JSON data, resulting in the following prompt, which sets out the prompt instructions
    using the delimiter, format, and work-our-solution tactics
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You are a JSON data generator. Generate a JSON array with 3 randomized JSON
    objects based on the OpenAPI schema delimited by three hashes. Confirm that all
    3 JSON objects match the OpenAPI schema rules before outputting the results. |'
  prefs: []
  type: TYPE_TB
- en: 'We then provide the OpenAPI specification for processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice how the prompt indicates at the start that we intend to use the OpenAPI
    format to outline our data requirements. We use this to set our expectations of
    how the resulting data should be structured, which we can see when submitting
    the prompt to ChatGPT and receiving the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Compared to the previous data we generated, this data feels more accurate. The
    descriptions are more detailed and cite other attributes of the room found elsewhere.
    For example, the family suite description references the data related to the room
    type and features.
  prefs: []
  type: TYPE_NORMAL
- en: The image attributes are questionable as they simply provide an image name,
    but that might suffice if images are stored within our application. However, what
    our use of the OpenAPI language allows us to do is to set more detailed rules,
    meaning that we could update the image section of the specification from
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'to the following, in which the pattern points to test images we might have
    generated earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we were to add this pattern into our prompt and send it to an LLM, it would
    return objects such as this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: By utilizing different specification rules used in OpenAPI, we can control our
    output more successfully.
  prefs: []
  type: TYPE_NORMAL
- en: XML and XSD
  prefs: []
  type: TYPE_NORMAL
- en: 'The same process can be applied to other formats. Consider the following prompt
    that uses the same approach as the previous one but takes an XML Schema Definition
    (XSD) instead. We use format, delimiter, and work-out-solution tactics to outline
    the prompt to take an XSD format:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You are an XML data generator. Generate 3 randomized XML objects based on
    the XSD schema delimited by three hashes. Add all the XML objects to parent element
    of rooms. Confirm that all 3 XML subobjects match the XSD schema rules before
    outputting the results. |'
  prefs: []
  type: TYPE_TB
- en: 'Next, we provide the XSD format to set how we want data output:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'The prompt works in a manner similar to the one before. This prompt comes with
    an additional step to ensure that our test data is grouped by storing it under
    a single root node. But the rest of the prompt works the same, resulting in an
    XML output that is similar to the previous prompt that generated JSON data, as
    demonstrated in the response I got when I prompted ChatGPT for XML test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: These types of prompts can be extremely useful when we need to create test data
    for NoSQL databases. I remember once working on a project with XML documents that
    included more than 1,000 elements. It was practically impossible to generate all
    the test data we required, so we had to compromise. But with an LLM and a prompt
    similar to the ones we’ve just looked at, the process of creating XML documents
    would have taken a matter of minutes.
  prefs: []
  type: TYPE_NORMAL
- en: Activity 6.3
  prefs: []
  type: TYPE_NORMAL
- en: Use either an OpenAPI or XSD specification to create new test data. If you have
    access to the specifications, try them out. Alternatively, locate example specifications
    and try them out in a prompt to generate test data.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2 SQL exports as prompt guides
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The prompts we’ve explored so far have created data entities that exist as a
    single entity, but it’s just as common to work with applications in which data
    is split into separate locations. For example, how would we prompt an LLM to create
    data for a SQL-based database that contained data distributed across multiple
    tables?
  prefs: []
  type: TYPE_NORMAL
- en: 'One method is to take an alternative approach using the few-shot tactic (providing
    examples to a prompt) and provide the structure of a database, along with examples,
    to demonstrate what data is created and where. Take for example the following
    prompt requesting SQL data to be generated across two different tables. First,
    we set out the initial instructions for the prompt using delimiter and structured
    format tactic:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You are a SQL generator. Take the sql statement delimited by three hashes
    and create a SQL statement that generates 5 new records that follow the format
    of the provided statement. |'
  prefs: []
  type: TYPE_TB
- en: 'We then have the LLM work out the solution before sharing the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Check that each new entry doesn’t match the provided SQL statement before
    outputting the newly generated data and that the SQL can be executed successfully
    before outputting it. |'
  prefs: []
  type: TYPE_TB
- en: Finally, we provide the SQL for each table for the LLM to process
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this prompt, we’ve provided SQL statements for two different tables connected
    via the `roomid`. The first is the `rooms` table, which has the following attributes
    (laid out in a more readable format):'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'And the second is the `bookings` table:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Providing the LLM with both `CREATE` and `INSERT` statements helps maximize
    the desired output, ensuring not only that the correct type of test data is created,
    but also that the relationship between data sets is correct. If we were to provide
    only insert statements, we’d be providing less context and increasing the risk
    of foreign keys being populated with relationships to nonexistent records.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sending this prompt to ChatGPT returned the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: The test data inserted is similar to the responses from other prompts, but there
    is now the addition of `roomid` values using ids that connect the bookings to
    existing rooms, which have also been generated in the response.
  prefs: []
  type: TYPE_NORMAL
- en: What these prompts have demonstrated is that if we have data that contains complex
    relationships or many different parameters, we can use existing documentation
    to assist test data generation. This is not only a great time saver but also an
    approach that ensures our test data generation can stay in lockstep with the structure
    of our data at any given time, saving us even more time in test data maintenance.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t forget data privacy
  prefs: []
  type: TYPE_NORMAL
- en: In the examples provided in this chapter, we’ve used dummy data structures and
    specifications, but when we create test data for our applications it’s likely
    that we’ll rely on either our organization’s intellectual property or user data.
    If we are to use those items to create our test data, we need to make sure that
    we aren’t violating internal policies for sharing intellectual property or laws
    around user data privacy. Depending on what we can and can’t share will determine
    how we frame our prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Activity 6.4
  prefs: []
  type: TYPE_NORMAL
- en: Use the SQL prompt to create your own test data. Try either locating SQL from
    an application you are working on or using example SQL to see what happens.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Setting up LLMs as test data managers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ve examined how to create data by sending prompts through tools such as
    ChatGPT. But how can we go about integrating these types of prompts into our automated
    checks? Let’s take a look at the potential of accessing LLMs via API platforms
    by enhancing this simple UI automated check with data generation from an LLM model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We use Selenium to open a webpage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we complete the Contact Us form on the webpage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we assert that the contact form page has been submitted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: With this automated check, we’re going to replace the hardcoded strings used
    to complete the contact form and instead connect to the OpenAI API platform and
    prompt one of their LLM models to create test data that we can then parse and
    use in our check. Examples of the initial and completed OpenAI integrated check
    can be found in the supporting repository at [https://mng.bz/n0dv](https://mng.bz/n0dv).
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.1 Setting up an OpenAI account
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we can start sending prompts using the OpenAI API platform, we’ll need
    to set up an account. This can be done by registering via [https://platform.openai.com/](https://platform.openai.com/).
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI platform costs
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI charges based on the number of tokens you send to and receive from an
    LLM. A *token* is essentially a word or collection of smaller words. For example,
    “Hello ChatGPT” would count as two tokens. The more tokens you use, meaning the
    bigger the prompts and content you receive back, the more it costs. If you are
    registering for a new account with OpenAI at this point, they will give you $5
    in free credit that can be used during your first three months. This is more than
    enough for what we need to complete our exercise. However, since the free credit
    expires after three months, if you have no free credits left, you will need to
    provide billing details before you can send and receive prompts: [https://mng.bz/vJRx](https://mng.bz/vJRx).
    Also, you are strongly advised to set a usage limit that works best for you so
    that you don’t end up with a surprising bill: [https://platform.openai.com/account/billing/limits](https://platform.openai.com/account/billing/limits).'
  prefs: []
  type: TYPE_NORMAL
- en: Once registered, we need to generate an API key that we’ll provide in our requests
    to authenticate ourselves. This can be done via [https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys)
    and clicking the Create New Secret Key button, which asks us to give our API key
    a name. Upon entering a name and clicking the Create key, we’ll be given an API
    key, as shown in figure 6.1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F01_Winteringham2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 A newly created API key for the Open API platform
  prefs: []
  type: TYPE_NORMAL
- en: As the instructions state, we need to record this API key elsewhere for future
    use as we’ll not be able to view it again. So, we make a note of the key and then
    click Done to make sure our key has been saved, as shown in figure 6.2.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F02_Winteringham2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 Screen shot of the API key manager for the OpenAI API platform
  prefs: []
  type: TYPE_NORMAL
- en: With our key created and recorded, we’re ready to begin work on integrating
    OpenAI into our project.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.2 Connecting to OpenAI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our first step is to build the necessary code to send an HTTP request to OpenAI
    and confirm that we can get a response back. So, we begin by adding the following
    library into our `pom.xml` that we’ll use to send our request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: LangChain4j is a Java implementation of the popular LangChain toolset that’s
    written in Python. It offers a collection of tools that can be used to integrate
    with different LLMs. For our example test, we’ll be relying on OpenAI’s GPT models
    to generate our test data. So, we’ll use the OpenAI specific version of LangChain
    to get basic access to sending a prompt. However, if we wanted more control or
    options, we could use the full AI services version of LangChain.
  prefs: []
  type: TYPE_NORMAL
- en: gpt-3.5-turbo and other models
  prefs: []
  type: TYPE_NORMAL
- en: One of the features offered by the OpenAI API platform is the ability to send
    prompts to different LLM models. gpt-3.5-turbo is the model that is, at the time
    of writing, used to power the free version of ChatGPT. As we’ll learn, we can
    swap this out to call other models such as gpt-4o. Different models offer different
    features at different price points. For example, gpt-4o is a more effective LLM
    compared to gpt-3.5-turbo. However, the price point to use gpt-4o is much higher.
    More details on other models can be found in the OpenAI platform documentation
    at [https://platform.openai.com/docs/models/overview](https://platform.openai.com/docs/models/overview).
  prefs: []
  type: TYPE_NORMAL
- en: 'With the necessary libraries installed, our next step is to build a prompt
    that will request an LLM to generate our required test data. The initial instructions
    use the structured output and delimiter tactics:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You are a data generator. Create me random data in a JSON format based on
    the criteria delimited by three hashes. Additional data requirements are shared
    between back ticks. |'
  prefs: []
  type: TYPE_TB
- en: 'Data to be processed is added with additional instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| ###nameemailphone `UK format`subject `Over 20 characters in length`description
    `Over 50 characters in length`### |'
  prefs: []
  type: TYPE_TB
- en: 'We can test this prompt by adding it and the necessary code to send the prompt
    into a new automated check:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a new OpenAIChat model and provide an API key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we add our prompt to a string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we send the prompt to a GPT model and store the response in a string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Running the check again, we’ll see the LLM returns something similar to
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll need to parse this into a Java object, so we create a new class
    `ContactFormDetails` that can convert the JSON into an object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'With our `ContactFormDetails` class created, we can now convert the prompt
    response, which is currently a string, into a POJO for further use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have the necessary test data to use in our automated check:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The following block of code sends a prompt to OpenAI to generate test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we extract the test data from the responses and convert it into an object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we use the test data to complete the Contact Us form and assert success:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: This completes the integration of the OpenAI API platform into our automated
    check. Upon executing the check, we should see it pass and that test data has
    been successfully used to create a contact message, as shown in figure 6.3.
  prefs: []
  type: TYPE_NORMAL
- en: We could improve the code further by perhaps storing prompts in external files
    and importing them into our checks when required. This may be beneficial when
    prompts are used on multiple occasions. It would also mean that when changes are
    required for test data, we would simply update our prompts with new details in
    a way that anyone, regardless of the experience of working with test data, could
    do.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F03_Winteringham2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 A message created using LLM test data
  prefs: []
  type: TYPE_NORMAL
- en: Activity 6.5
  prefs: []
  type: TYPE_NORMAL
- en: Create a new automated check that requires inputting test data. Using the prompt
    method, create a new prompt to generate test data and then pass it through your
    automated check.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Benefiting from generated test data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This chapter demonstrated that LLMs can be quite adept at generating test data.
    It can help us quickly create data for various testing activities from automation
    to exploratory testing, support managing complex data sets, and simplify the process
    of managing test data using natural language prompts. However, for this, we need
    to create prompts that provide clear instructions about the format we want data
    in and what examples to draw from, ensuring that what we’re sending to an LLM
    doesn’t impact personal and organizational privacy. Going back to our area of
    effect model, we can see the roles of humans and AI in test data generation described
    in figure 6.4.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F04_Winteringham2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 The area of effect model describing the roles of humans and AI in
    test data generation
  prefs: []
  type: TYPE_NORMAL
- en: By using the tactics we’ve learned with prompt engineering in a creative manner,
    we can create test data for a wide range of situations, from simple to complex,
    to help us save time in test data management.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prompts can be built to rapidly generate data in any common format (JSON, XML,
    or SQL, for example).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The relationship between data can sometimes be incorrect if not explicitly set
    in a prompt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompts can also be built to transform data from one format into another, while
    ensuring the raw data from an original format is copied over.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use data specification formats such as OpenAPI and XSD in prompts to
    set our expectations of how data is structured.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using SQL statements that create the initial structure of a database can be
    used to prompt LLMs when working with distributed data structures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI offers an API platform that can be used to interact with different AI
    models, including gpt-3.5 and gpt-4o.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can programmatically build HTTP requests to send prompts to OpenAI LLMs to
    generate test data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompting LLMs for test data requires building clear prompts with explicit expectations
    and useful examples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
