["```py\nfrom pydantic import BaseModel, Field\n\nclass Joke(BaseModel):\n    setup: str = Field(description=\"The setup of the joke\")\n    punchline: str = Field(description=\"The punchline to the joke\")\n```", "```py\nimport { z } from \"zod\";\n\nconst joke = z.object({\n  setup: z.string().describe(\"The setup of the joke\"),\n  punchline: z.string().describe(\"The punchline to the joke\"),\n});\n```", "```py\n{'properties': {'setup': {'description': 'The setup of the joke',\n    'title': 'Setup',\n    'type': 'string'},\n 'punchline': {'description': 'The punchline to the joke',\n    'title': 'Punchline',\n    'type': 'string'}},\n 'required': ['setup', 'punchline'],\n 'title': 'Joke',\n 'type': 'object'}\n```", "```py\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\nmodel = model.with_structured_output(Joke)\n\nmodel.invoke(\"Tell me a joke about cats\")\n```", "```py\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nlet model = new ChatOpenAI({\n  model: \"gpt-3.5-turbo\",\n  temperature: 0\n});\nmodel = model.withStructuredOutput(joke);\n\nawait structuredLlm.invoke(\"Tell me a joke about cats\");\n```", "```py\n{\n    setup: \"Why don't cats play poker in the wild?\",\n    punchline: \"Too many cheetahs.\"\n}\n```", "```py\ninput = {\n    \"messages\": [\n        HumanMessage(\"\"\"How old was the 30th president of the United States \n when he died?\"\"\")\n    ]\n}\nfor c in graph.stream(input, stream_mode='updates'):\n    print(c)\n```", "```py\nconst input = {\n  messages: [\n    new HumanMessage(`How old was the 30th president of the United States when \n he died?`)\n  ]\n}\nconst output = await graph.stream(input, streamMode: 'updates')\nfor await (const c of output) {\n  console.log(c)\n}\n```", "```py\n{\n    \"select_tools\": {\n        \"selected_tools\": ['duckduckgo_search', 'calculator']\n    }\n}\n{\n    \"model\": {\n        \"messages\": AIMessage(\n            content=\"\",\n            tool_calls=[\n                {\n                    \"name\": \"duckduckgo_search\",\n                    \"args\": {\n                        \"query\": \"30th president of the United States\"\n                    },\n                    \"id\": \"9ed4328dcdea4904b1b54487e343a373\",\n                    \"type\": \"tool_call\",\n                }\n            ],\n        )\n    }\n}\n{\n    \"tools\": {\n        \"messages\": [\n            ToolMessage(\n                content=\"Calvin Coolidge (born July 4, 1872, Plymouth, Vermont, \n                    U.S.â€”died January 5, 1933, Northampton, Massachusetts) was \n                    the 30th president of the United States (1923-29). Coolidge \n                    acceded to the presidency after the death in office of \n                    Warren G. Harding, just as the Harding scandals were coming \n                    to light....\",\n                name=\"duckduckgo_search\",\n                tool_call_id=\"9ed4328dcdea4904b1b54487e343a373\",\n            )\n        ]\n    }\n}\n{\n    \"model\": {\n        \"messages\": AIMessage(\n            content=\"Calvin Coolidge, the 30th president of the United States, \n                was born on July 4, 1872, and died on January 5, 1933\\. To \n                calculate his age at the time of his death, we can subtract his \n                birth year from his death year. \\n\\nAge at death = Death year - \n                Birth year\\nAge at death = 1933 - 1872\\nAge at death = 61 \n                years\\n\\nCalvin Coolidge was 61 years old when he died.\",\n        )\n    }\n}\n```", "```py\ninput = {\n    \"messages\": [\n        HumanMessage(\"\"\"How old was the 30th president of the United States \n when he died?\"\"\")\n    ]\n}\noutput = app.astream_events(input, version=\"v2\")\n\nasync for event in output:\n    if event[\"event\"] == \"on_chat_model_stream\":\n        content = event[\"data\"][\"chunk\"].content\n        if content:\n            print(content)\n```", "```py\nconst input = {\n  messages: [\n    new HumanMessage(`How old was the 30th president of the United States when \n he died?`)\n  ]\n}\n\nconst output = await agent.streamEvents(input, {version: \"v2\"});\n\nfor await (const { event, data } of output) {\n  if (event === \"on_chat_model_stream\") {\n    const msg = data.chunk as AIMessageChunk;\n    if (msg.content) {\n      console.log(msg.content);\n    }\n  }\n}\n```", "```py\nfrom langgraph.checkpoint.memory import MemorySaver\n\ngraph = builder.compile(checkpointer=MemorySaver())\n```", "```py\nimport {MemorySaver} from '@langchain/langgraph'\n\ngraph = builder.compile({ checkpointer: new MemorySaver() })\n```", "```py\nimport asyncio\n\nevent = asyncio.Event()\n\ninput = {\n    \"messages\": [\n        HumanMessage(\"\"\"How old was the 30th president of the United States \n when he died?\"\"\")\n    ]\n}\n\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\nasync with aclosing(graph.astream(input, config)) as stream:\n    async for chunk in stream:\n        if event.is_set():\n            break\n        else:\n            ... # do something with the output\n\n# Somewhere else in your application\n\nevent.set()\n```", "```py\nconst controller = new AbortController()\n\nconst input = {\n  \"messages\": [\n    new HumanMessage(`How old was the 30th president of the United States when \n he died?`)\n  ]\n}\n\nconst config = {\"configurable\": {\"thread_id\": \"1\"}}\n\ntry {\n  const output = await graph.stream(input, {\n    ...config,\n    signal: controller.signal\n  });\n  for await (const chunk of output) {\n    console.log(chunk); // do something with the output\n  }\n} catch (e) {\n  console.log(e);\n}\n\n// Somewhere else in your application\ncontroller.abort()\n```", "```py\ninput = {\n    \"messages\": [\n        HumanMessage(\"\"\"How old was the 30th president of the United States \n when he died?\"\"\")\n    ]\n}\n\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\noutput = graph.astream(input, config, interrupt_before=['tools'])\n\nasync for c in output:\n    ... # do something with the output\n```", "```py\nconst input = {\n  \"messages\": [\n    new HumanMessage(`How old was the 30th president of the United States when \n he died?`)\n  ]\n}\n\nconst config = {\"configurable\": {\"thread_id\": \"1\"}}\n\nconst output = await graph.stream(input, {\n  ...config,\n  interruptBefore: ['tools']\n});\nfor await (const chunk of output) {\n  console.log(chunk); // do something with the output\n}\n```", "```py\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\noutput = graph.astream(None, config, interrupt_before=['tools'])\n\nasync for c in output:\n    ... # do something with the output\n```", "```py\nconst config = {\"configurable\": {\"thread_id\": \"1\"}}\n\nconst output = await graph.stream(null, {\n  ...config,\n  interruptBefore: ['tools']\n});\nfor await (const chunk of output) {\n  console.log(chunk); // do something with the output\n}\n```", "```py\ninput = {\n    \"messages\": [\n        HumanMessage(\"\"\"How old was the 30th president of the United States \n when he died?\"\"\")\n    ]\n}\n\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\noutput = graph.astream(input, config)\n\nasync for c in output:\n    ... # do something with the output\n```", "```py\nconst input = {\n  \"messages\": [\n    new HumanMessage(`How old was the 30th president of the United States when \n he died?`)\n  ]\n}\n\nconst config = {\"configurable\": {\"thread_id\": \"1\"}}\n\nconst output = await graph.stream(input, config);\n\nfor await (const chunk of output) {\n  console.log(chunk); // do something with the output\n}\n```", "```py\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\nstate = graph.get_state(config)\n\n# something you want to add or replace\nupdate = { }\n\ngraph.update_state(config, update)\n```", "```py\nconst config = \"configurable\": {\"thread_id\": \"1\"}\n\nconst state = await graph.getState(config)\n\n// something you want to add or replace\nconst update = { }\n\nawait graph.updateState(config, update)\n```", "```py\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\nhistory = [\n    state for state in\n    graph.get_state_history(config)\n]\n\n# replay a past state\ngraph.invoke(None, history[2].config)\n```", "```py\nconst config = \"configurable\": {\"thread_id\": \"1\"}\n\nconst history = await Array.fromAsync(graph.getStateHistory(config))\n\n// replay a past state\nawait graph.invoke(null, history[2].config)\n```"]