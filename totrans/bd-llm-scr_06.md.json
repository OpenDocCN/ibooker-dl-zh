["```py\nimport json\nimport os\nimport urllib\n\ndef download_and_load_file(file_path, url):\n    if not os.path.exists(file_path):\n        with urllib.request.urlopen(url) as response:\n            text_data = response.read().decode(\"utf-8\")\n        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n            file.write(text_data)\n\n    with open(file_path, \"r\") as file:\n        data = json.load(file)\n    return data\n\nfile_path = \"instruction-data.json\"\nurl = (\n    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n)\n\ndata = download_and_load_file(file_path, url)\nprint(\"Number of entries:\", len(data))\n```", "```py\nNumber of entries: 1100\n```", "```py\nprint(\"Example entry:\\n\", data[50])\n```", "```py\nExample entry:\n {'instruction': 'Identify the correct spelling of the following word.',\n  'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n```", "```py\nprint(\"Another example entry:\\n\", data[999])\n```", "```py\nAnother example entry:\n {'instruction': \"What is an antonym of 'complicated'?\", \n  'input': '',\n  'output': \"An antonym of 'complicated' is 'simple'.\"}\n```", "```py\ndef format_input(entry):\n    instruction_text = (\n        f\"Below is an instruction that describes a task. \"\n        f\"Write a response that appropriately completes the request.\"\n        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n    )\n\n    input_text = (\n        f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n    )\n    return instruction_text + input_text\n```", "```py\nmodel_input = format_input(data[50])\ndesired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\nprint(model_input + desired_response)\n```", "```py\nBelow is an instruction that describes a task. Write a response that \nappropriately completes the request.\n\n### Instruction:\nIdentify the correct spelling of the following word.\n\n### Input:\nOcassion\n\n### Response:\nThe correct spelling is 'Occasion.'\n```", "```py\nmodel_input = format_input(data[999])\ndesired_response = f\"\\n\\n### Response:\\n{data[999]['output']}\"\nprint(model_input + desired_response)\n```", "```py\nBelow is an instruction that describes a task. Write a response that \nappropriately completes the request.\n\n### Instruction:\nWhat is an antonym of 'complicated'?\n\n### Response:\nAn antonym of 'complicated' is 'simple'.\n```", "```py\ntrain_portion = int(len(data) * 0.85)    #1\ntest_portion = int(len(data) * 0.1)            #2\nval_portion = len(data) - train_portion - test_portion    #3\n\ntrain_data = data[:train_portion]\ntest_data = data[train_portion:train_portion + test_portion]\nval_data = data[train_portion + test_portion:]\n\nprint(\"Training set length:\", len(train_data))\nprint(\"Validation set length:\", len(val_data))\nprint(\"Test set length:\", len(test_data))\n```", "```py\nTraining set length: 935\nValidation set length: 55\nTest set length: 110\n```", "```py\nimport torch\nfrom torch.utils.data import Dataset\n\nclass InstructionDataset(Dataset):\n    def __init__(self, data, tokenizer):\n        self.data = data\n        self.encoded_texts = []\n        for entry in data:         #1\n            instruction_plus_input = format_input(entry)\n            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n            full_text = instruction_plus_input + response_text\n            self.encoded_texts.append(\n                tokenizer.encode(full_text)\n            )\n\n    def __getitem__(self, index):\n        return self.encoded_texts[index]\n\n    def __len__(self):\n        return len(self.data)\n```", "```py\nimport tiktoken\ntokenizer = tiktoken.get_encoding(\"gpt2\")\nprint(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))\n```", "```py\ndef custom_collate_draft_1(\n    batch,\n    pad_token_id=50256,\n    device=\"cpu\"\n):\n    batch_max_length = max(len(item)+1 for item in batch)   #1\n    inputs_lst = []\n\n    for item in batch:     #2\n        new_item = item.copy()\n        new_item += [pad_token_id]\n\n        padded = (\n            new_item + [pad_token_id] * \n            (batch_max_length - len(new_item))\n        )\n        inputs = torch.tensor(padded[:-1])    #3\n        inputs_lst.append(inputs)\n\n    inputs_tensor = torch.stack(inputs_lst).to(device)     #4\n    return inputs_tensor\n```", "```py\ninputs_1 = [0, 1, 2, 3, 4]\ninputs_2 = [5, 6]\ninputs_3 = [7, 8, 9]\nbatch = (\n    inputs_1,\n    inputs_2,\n    inputs_3\n)\nprint(custom_collate_draft_1(batch))\n```", "```py\ntensor([[    0,     1,     2,     3,     4],  \n        [    5,     6, 50256, 50256, 50256],\n        [    7,     8,     9, 50256, 50256]])\n```", "```py\ndef custom_collate_draft_2(\n    batch,\n    pad_token_id=50256,\n    device=\"cpu\"\n):\n    batch_max_length = max(len(item)+1 for item in batch)\n    inputs_lst, targets_lst = [], []\n\n    for item in batch:\n        new_item = item.copy()\n        new_item += [pad_token_id]\n\n        padded = (\n            new_item + [pad_token_id] * \n            (batch_max_length - len(new_item))\n        )\n        inputs = torch.tensor(padded[:-1])     #1\n        targets = torch.tensor(padded[1:])    #2\n        inputs_lst.append(inputs)\n        targets_lst.append(targets)\n\n    inputs_tensor = torch.stack(inputs_lst).to(device)\n    targets_tensor = torch.stack(targets_lst).to(device)\n    return inputs_tensor, targets_tensor\n\ninputs, targets = custom_collate_draft_2(batch)\nprint(inputs)\nprint(targets)\n```", "```py\ntensor([[    0,     1,     2,     3,     4],    #1\n        [    5,     6, 50256, 50256, 50256],\n        [    7,     8,     9, 50256, 50256]])\ntensor([[    1,     2,     3,     4, 50256],   #2\n        [    6, 50256, 50256, 50256, 50256],\n        [    8,     9, 50256, 50256, 50256]])\n```", "```py\ndef custom_collate_fn(\n    batch,\n    pad_token_id=50256,\n    ignore_index=-100,\n    allowed_max_length=None,\n    device=\"cpu\"\n):\n    batch_max_length = max(len(item)+1 for item in batch)\n    inputs_lst, targets_lst = [], []\n\n    for item in batch:\n        new_item = item.copy()\n        new_item += [pad_token_id]\n\n        padded = (                               #1\n            new_item + [pad_token_id] *          #1\n            (batch_max_length - len(new_item))   #1\n        )\n        inputs = torch.tensor(padded[:-1])      #2\n        targets = torch.tensor(padded[1:])     #3\n\n        mask = targets == pad_token_id              #4\n        indices = torch.nonzero(mask).squeeze()     #4\n        if indices.numel() > 1:                     #4\n            targets[indices[1:]] = ignore_index     #4\n\n        if allowed_max_length is not None:\n            inputs = inputs[:allowed_max_length]       #5\n            targets = targets[:allowed_max_length]     #5\n\n        inputs_lst.append(inputs)\n        targets_lst.append(targets)\n\n    inputs_tensor = torch.stack(inputs_lst).to(device)\n    targets_tensor = torch.stack(targets_lst).to(device)\n    return inputs_tensor, targets_tensor\n```", "```py\ninputs, targets = custom_collate_fn(batch)\nprint(inputs)\nprint(targets)\n```", "```py\ntensor([[    0,     1,     2,     3,     4],\n        [    5,     6, 50256, 50256, 50256],\n        [    7,     8,     9, 50256, 50256]])\ntensor([[    1,     2,     3,     4, 50256],\n        [    6, 50256,  -100,  -100,  -100],\n        [    8,     9, 50256,  -100,  -100]])\n```", "```py\nlogits_1 = torch.tensor(\n    [[-1.0, 1.0],     #1\n     [-0.5, 1.5]]      #2\n)\ntargets_1 = torch.tensor([0, 1]) # Correct token indices to generate\nloss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\nprint(loss_1)\n```", "```py\ntensor(1.1269)\n```", "```py\nlogits_2 = torch.tensor(\n    [[-1.0, 1.0],\n     [-0.5, 1.5],\n     [-0.5, 1.5]]      #1\n)\ntargets_2 = torch.tensor([0, 1, 1])\nloss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\nprint(loss_2)\n```", "```py\ntargets_3 = torch.tensor([0, 1, -100])\nloss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)\nprint(loss_3)\nprint(\"loss_1 == loss_3:\", loss_1 == loss_3)\n```", "```py\ntensor(1.1269)\nloss_1 == loss_3: tensor(True)\n```", "```py\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# if torch.backends.mps.is_available():   #1\n#     device = torch.device(\"mps\")\"      \nprint(\"Device:\", device)\n```", "```py\nfrom functools import partial\n\ncustomized_collate_fn = partial(\n    custom_collate_fn,\n    device=device,\n    allowed_max_length=1024\n)\n```", "```py\nfrom torch.utils.data import DataLoader\n\nnum_workers = 0      #1\nbatch_size = 8\n\ntorch.manual_seed(123)\n\ntrain_dataset = InstructionDataset(train_data, tokenizer)\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    collate_fn=customized_collate_fn,\n    shuffle=True,\n    drop_last=True,\n    num_workers=num_workers\n)\n\nval_dataset = InstructionDataset(val_data, tokenizer)\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=batch_size,\n    collate_fn=customized_collate_fn,\n    shuffle=False,\n    drop_last=False,\n    num_workers=num_workers\n)\n\ntest_dataset = InstructionDataset(test_data, tokenizer)\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=batch_size,\n    collate_fn=customized_collate_fn,\n    shuffle=False,\n    drop_last=False,\n    num_workers=num_workers\n)\n```", "```py\nprint(\"Train loader:\")\nfor inputs, targets in train_loader:\n    print(inputs.shape, targets.shape)\n```", "```py\nTrain loader:\ntorch.Size([8, 61]) torch.Size([8, 61])\ntorch.Size([8, 76]) torch.Size([8, 76])\ntorch.Size([8, 73]) torch.Size([8, 73])\n...\ntorch.Size([8, 74]) torch.Size([8, 74])\ntorch.Size([8, 69]) torch.Size([8, 69])\n```", "```py\nfrom gpt_download import download_and_load_gpt2\nfrom chapter04 import GPTModel\nfrom chapter05 import load_weights_into_gpt\n\nBASE_CONFIG = {\n    \"vocab_size\": 50257,     # Vocabulary size\n    \"context_length\": 1024,  # Context length\n    \"drop_rate\": 0.0,        # Dropout rate\n    \"qkv_bias\": True         # Query-key-value bias\n}\n\nmodel_configs = {\n    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n}\n\nCHOOSE_MODEL = \"gpt2-medium (355M)\"\nBASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n\nmodel_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n\nsettings, params = download_and_load_gpt2(\n    model_size=model_size, \n    models_dir=\"gpt2\"\n)\n\nmodel = GPTModel(BASE_CONFIG)\nload_weights_into_gpt(model, params)\nmodel.eval();\n```", "```py\ncheckpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 156kiB/s]\nencoder.json: 100%|██████████| 1.04M/1.04M [00:02<00:00, 467kiB/s]\nhparams.json: 100%|██████████| 91.0/91.0 [00:00<00:00, 198kiB/s]\nmodel.ckpt.data-00000-of-00001: 100%|██████████| 1.42G/1.42G \n[05:50<00:00, 4.05MiB/s]\nmodel.ckpt.index: 100%|██████████| 10.4k/10.4k [00:00<00:00, 18.1MiB/s]\nmodel.ckpt.meta: 100%|██████████| 927k/927k [00:02<00:00, 454kiB/s]\nvocab.bpe: 100%|██████████| 456k/456k [00:01<00:00, 283kiB/s]\n```", "```py\ntorch.manual_seed(123)\ninput_text = format_input(val_data[0])\nprint(input_text)\n```", "```py\nBelow is an instruction that describes a task. Write a response that \nappropriately completes the request.\n\n### Instruction:\nConvert the active sentence to passive: 'The chef cooks the meal every day.'\n```", "```py\nfrom chapter05 import generate, text_to_token_ids, token_ids_to_text\n\ntoken_ids = generate(\n    model=model,\n    idx=text_to_token_ids(input_text, tokenizer),\n    max_new_tokens=35,\n    context_size=BASE_CONFIG[\"context_length\"],\n    eos_id=50256,\n)\ngenerated_text = token_ids_to_text(token_ids, tokenizer)\n```", "```py\nresponse_text = generated_text[len(input_text):].strip()\nprint(response_text)\n```", "```py\n### Response:\n\nThe chef cooks the meal every day.\n\n### Instruction:\n\nConvert the active sentence to passive: 'The chef cooks the\n```", "```py\nfrom chapter05 import (\n    calc_loss_loader,\n    train_model_simple\n)\n```", "```py\nmodel.to(device)\ntorch.manual_seed(123)\n\nwith torch.no_grad():\n    train_loss = calc_loss_loader(\n        train_loader, model, device, num_batches=5\n    )\n    val_loss = calc_loss_loader(\n        val_loader, model, device, num_batches=5\n)\n\nprint(\"Training loss:\", train_loss)\nprint(\"Validation loss:\", val_loss)\n```", "```py\nTraining loss: 3.825908660888672\nValidation loss: 3.7619335651397705\n```", "```py\nimport time\n\nstart_time = time.time()\ntorch.manual_seed(123)\noptimizer = torch.optim.AdamW(\n    model.parameters(), lr=0.00005, weight_decay=0.1\n)\nnum_epochs = 2\n\ntrain_losses, val_losses, tokens_seen = train_model_simple(\n    model, train_loader, val_loader, optimizer, device,\n    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n    start_context=format_input(val_data[0]), tokenizer=tokenizer\n)\n\nend_time = time.time()\nexecution_time_minutes = (end_time - start_time) / 60\nprint(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n```", "```py\nEp 1 (Step 000000): Train loss 2.637, Val loss 2.626\nEp 1 (Step 000005): Train loss 1.174, Val loss 1.103\nEp 1 (Step 000010): Train loss 0.872, Val loss 0.944\nEp 1 (Step 000015): Train loss 0.857, Val loss 0.906\n...\nEp 1 (Step 000115): Train loss 0.520, Val loss 0.665\nBelow is an instruction that describes a task. Write a response that \nappropriately completes the request.  ### Instruction: Convert the \nactive sentence to passive: 'The chef cooks the meal every day.' \n### Response: The meal is prepared every day by the chef.<|endoftext|>\nThe following is an instruction that describes a task. \nWrite a response that appropriately completes the request.  \n### Instruction: Convert the active sentence to passive:\nEp 2 (Step 000120): Train loss 0.438, Val loss 0.670\nEp 2 (Step 000125): Train loss 0.453, Val loss 0.685\nEp 2 (Step 000130): Train loss 0.448, Val loss 0.681\nEp 2 (Step 000135): Train loss 0.408, Val loss 0.677\n...\nEp 2 (Step 000230): Train loss 0.300, Val loss 0.657\nBelow is an instruction that describes a task. Write a response \nthat appropriately completes the request.  ### Instruction: \nConvert the active sentence to passive: 'The chef cooks the meal \nevery day.'  ### Response: The meal is cooked every day by the \nchef.<|endoftext|>The following is an instruction that describes \na task. Write a response that appropriately completes the request.  \n### Instruction: What is the capital of the United Kingdom\nTraining completed in 0.87 minutes.\n```", "```py\nfrom chapter05 import plot_losses\nepochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\nplot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n```", "```py\ntorch.manual_seed(123)\n\nfor entry in test_data[:3]:      #1\n    input_text = format_input(entry)\n    token_ids = generate(               #2\n        model=model,\n        idx=text_to_token_ids(input_text, tokenizer).to(device),\n        max_new_tokens=256,\n        context_size=BASE_CONFIG[\"context_length\"],\n        eos_id=50256\n    )\n    generated_text = token_ids_to_text(token_ids, tokenizer)\n\n    response_text = (\n        generated_text[len(input_text):]\n        .replace(\"### Response:\", \"\")\n        .strip()\n    )\n    print(input_text)\n    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n    print(\"-------------------------------------\")\n```", "```py\nfrom tqdm import tqdm\n\nfor i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n    input_text = format_input(entry)\n\n    token_ids = generate(\n        model=model,\n        idx=text_to_token_ids(input_text, tokenizer).to(device),\n        max_new_tokens=256,\n        context_size=BASE_CONFIG[\"context_length\"],\n        eos_id=50256\n    )\n    generated_text = token_ids_to_text(token_ids, tokenizer)\n\n    response_text = (\n        generated_text[len(input_text):]\n        .replace(\"### Response:\", \"\")\n        .strip()\n    )\n    test_data[i][\"model_response\"] = response_text\n\nwith open(\"instruction-data-with-response.json\", \"w\") as file:\n    json.dump(test_data, file, indent=4)         #1\n```", "```py\n100%|██████████| 110/110 [01:05<00:00,  1.68it/s]\n```", "```py\nprint(test_data[0])\n```", "```py\n{'instruction': 'Rewrite the sentence using a simile.', \n 'input': 'The car is very fast.', \n 'output': 'The car is as fast as lightning.', \n 'model_response': 'The car is as fast as a bullet.'}\n```", "```py\nimport re\n\nfile_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft.pth\"      #1\ntorch.save(model.state_dict(), file_name)\nprint(f\"Model saved as {file_name}\")\n```", "```py\nollama run llama3\n```", "```py\npulling manifest\npulling 6a0746a1ec1a... 100% |████████████████| 4.7 GB\npulling 4fa551d4f938... 100% |████████████████|  12 KB\npulling 8ab4849b038c... 100% |████████████████|  254 B\npulling 577073ffcc6c... 100% |████████████████|  110 B\npulling 3f8eb4da87fa... 100% |████████████████|  485 B\nverifying sha256 digest\nwriting manifest\nremoving any unused layers\nsuccess\n```", "```py\n>>> What do llamas eat?\nLlamas are ruminant animals, which means they have a four-chambered\nstomach and eat plants that are high in fiber. In the wild, \nllamas typically feed on:\n\n1\\. Grasses: They love to graze on various types of grasses, including tall\ngrasses, wheat, oats, and barley.\n```", "```py\nimport psutil\n\ndef check_if_running(process_name):\n    running = False\n    for proc in psutil.process_iter([\"name\"]):\n        if process_name in proc.info[\"name\"]:\n            running = True\n            break\n    return running\n\nollama_running = check_if_running(\"ollama\")\n\nif not ollama_running:\n    raise RuntimeError(\n        \"Ollama not running. Launch ollama before proceeding.\"\n)\nprint(\"Ollama running:\", check_if_running(\"ollama\"))\n```", "```py\nimport json\nfrom tqdm import tqdm\n\nfile_path = \"instruction-data-with-response.json\"\nwith open(file_path, \"r\") as file:\n    test_data = json.load(file)\n\ndef format_input(entry):\n    instruction_text = (\n        f\"Below is an instruction that describes a task. \"\n        f\"Write a response that appropriately completes the request.\"\n        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n    )\n\n    input_text = (\n        f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n    )\n    return instruction_text + input_text\n```", "```py\nimport urllib.request\n\ndef query_model(\n    prompt, \n    model=\"llama3\", \n    url=\"http://localhost:11434/api/chat\"\n):\n    data = {             #1\n        \"model\": model,\n        \"messages\": [\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        \"options\": {         #2\n            \"seed\": 123,\n            \"temperature\": 0,\n            \"num_ctx\": 2048\n        }\n    }\n\n    payload = json.dumps(data).encode(\"utf-8\")    #3\n    request = urllib.request.Request(                       #4\n        url,                                                #4\n        data=payload,                                       #4\n        method=\"POST\"                                       #4\n    ) #4\n\n    request.add_header(\"Content-Type\", \"application/json\")   #4\n\n    response_data = \"\"\n    with urllib.request.urlopen(request) as response:   #5\n        while True:\n            line = response.readline().decode(\"utf-8\")\n            if not line:\n                break\n            response_json = json.loads(line)\n            response_data += response_json[\"message\"][\"content\"]\n\n    return response_data\n```", "```py\nmodel = \"llama3\"\nresult = query_model(\"What do Llamas eat?\", model)\nprint(result)\n```", "```py\nLlamas are ruminant animals, which means they have a four-chambered \nstomach that allows them to digest plant-based foods. Their diet \ntypically consists of:\n\n1\\. Grasses: Llamas love to graze on grasses, including tall grasses, \nshort grasses, and even weeds.\n...\n```", "```py\nfor entry in test_data[:3]:\n    prompt = (\n        f\"Given the input `{format_input(entry)}` \"\n        f\"and correct output `{entry['output']}`, \"\n        f\"score the model response `{entry['model_response']}`\"\n        f\" on a scale from 0 to 100, where 100 is the best score. \"\n    )\n    print(\"\\nDataset response:\")\n    print(\">>\", entry['output'])\n    print(\"\\nModel response:\")\n    print(\">>\", entry[\"model_response\"])\n    print(\"\\nScore:\")\n    print(\">>\", query_model(prompt))\n    print(\"\\n-------------------------\")\n```", "```py\ndef generate_model_scores(json_data, json_key, model=\"llama3\"):\n    scores = []\n    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n        prompt = (\n            f\"Given the input `{format_input(entry)}` \"\n            f\"and correct output `{entry['output']}`, \"\n            f\"score the model response `{entry[json_key]}`\"\n            f\" on a scale from 0 to 100, where 100 is the best score. \"\n            f\"Respond with the integer number only.\"   #1\n        )\n        score = query_model(prompt, model)\n        try:\n            scores.append(int(score))\n        except ValueError:\n            print(f\"Could not convert score: {score}\")\n            continue\n\n    return scores\n```", "```py\nscores = generate_model_scores(test_data, \"model_response\")\nprint(f\"Number of scores: {len(scores)} of {len(test_data)}\")\nprint(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")\n```", "```py\nScoring entries: 100%|████████████████████████| 110/110 \n[01:10<00:00,  1.56it/s]\nNumber of scores: 110 of 110\nAverage score: 50.32\n```"]