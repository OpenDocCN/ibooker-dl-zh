<html><head></head><body>
<div id="sbo-rt-content"><div class="readable-text" id="p1">
<h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">6</span></span> <span class="chapter-title-text">Large language model services: A practical guide </span></h1>
</div>
<div class="introduction-summary">
<h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3>
<ul>
<li class="readable-text" id="p2">How to structure an LLM service and tools to deploy</li>
<li class="readable-text" id="p3">How to create and prepare a Kubernetes cluster for LLM deployment</li>
<li class="readable-text" id="p4">Common production challenges and some methods to handle them</li>
<li class="readable-text" id="p5">Deploying models to the edge</li>
</ul>
</div>
<div class="readable-text" id="p6">
<blockquote>
<div>
     The production of too many useful things results in too many useless people. 
     <div class="quote-cite">
       —Karl Marx 
     </div>
</div>
</blockquote>
</div>
<div class="readable-text" id="p7">
<p>We did it. We arrived. This is the chapter we wanted to write when we first thought about writing this book. One author remembers the first model he ever deployed. Words can’t describe how much more satisfaction this gave him than the dozens of projects left to rot on his laptop. In his mind, it sits on a pedestal, not because it was good—in fact, it was quite terrible—but because it was useful and actually used by those who needed it the most. It affected the lives of those around him.</p>
</div>
<div class="readable-text intended-text" id="p8">
<p>So what actually is production? “Production” refers to the phase where the model is integrated into a live or operational environment to perform its intended tasks or provide services to end users. It’s a crucial phase in making the model available for real-world applications and services. To that end, we will show you how to package up an LLM into a service or API so that it can take on-demand requests. We will then show you how to set up a cluster in the cloud where you can deploy this service. We’ll also share some challenges you may face in production and some tips for handling them. Lastly, we will talk about a different kind of production, deploying models on edge devices.</p>
</div>
<div class="readable-text" id="p9">
<h2 class="readable-text-h2" id="sigil_toc_id_101"><span class="num-string">6.1</span> Creating an LLM service</h2>
</div>
<div class="readable-text" id="p10">
<p>In the last chapter, we trained and finetuned several models, and we’re sure you can’t wait to deploy them. Before you deploy a model, though, it’s important to plan ahead and consider different architectures for your API. Planning ahead is especially vital when deploying an LLM API. It helps outline the functionality, identify potential integration challenges, and arrange for necessary resources. Good planning streamlines the development process by setting priorities, thereby boosting the team’s efficiency.</p>
</div>
<div class="readable-text intended-text" id="p11">
<p>In this section, we are going to take a look at several critical topics you should take into consideration to get the most out of our application once deployed. Figure 6.1 demonstrates a simple LLM-based service architecture that allows users to interact with our LLM on demand. This is a typical use case when working with chatbots, for example. Setting up a service also allows us to serve batch and stream processes while abstracting away the complexity of embedding the LLM logic directly into these pipelines. Of course, running an ML model from a service will add a communication latency to your pipeline, but LLMs are generally considered slow, and this extra latency is often worth the tradeoff.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p12">
<img alt="figure" height="355" src="../Images/6-1.png" width="855"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 6.1</span> A basic LLM service. A majority of the logic is handled by the API layer, which will ensure the correct preprocessing of incoming requests is done and serve the actual inference of the request.</h5>
</div>
<div class="readable-text" id="p13">
<p>While figure 6.1 appears neat and tidy, it is hiding several complex subjects you’ll want to work through, particularly in that API box. We’ll be talking through several key features you’ll want to include in your API, like batching, rate limiters, and streaming. You’ll also notice some preprocessing techniques like retrieval-augmented generation (RAG) hidden in this image, which we’ll discuss in depth in section 6.1.7. By the end of this section, you will know how to approach all of this, and you will have deployed an LLM service and understand what to do to improve it. But before we get to any of that, let’s first talk about the model itself and the best methods to prepare it for online inference. </p>
</div>
<div class="readable-text" id="p14">
<h3 class="readable-text-h3" id="sigil_toc_id_102"><span class="num-string">6.1.1</span> Model compilation</h3>
</div>
<div class="readable-text" id="p15">
<p>The success of any model in production is dependent on the hardware it runs on. The microchip architecture and design of the controllers on the silicon will ultimately determine how quickly and efficiently inferences can run. Unfortunately, when programming in a high-level language like Python using frameworks like PyTorch or TensorFlow, the model won’t be optimized to take full advantage of the hardware. This is where compiling comes into play. Compiling is the process of taking code written in a high-level language and converting or lowering it to machine-level code that the computer can process quickly. Compiling your LLM can easily lead to major inference and cost improvements.</p>
</div>
<div class="readable-text intended-text" id="p16">
<p>Various people have dedicated a lot of time to performing some of the repeatable efficiency steps for you beforehand. We covered Tim Dettmers’s contributions in the last chapter. Other contributors include Georgi Gerganov, who created and maintains llama.cpp for running LLMs using C++ for efficiency, and Tom Jobbins, who goes by TheBloke on Hugging Face Hub and quantizes models into the correct formats to be used in Gerganov’s framework and others, like oobabooga. Because of how fast this field moves, completing simple, repeatable tasks over a large distribution of resources is often just as helpful to others.</p>
</div>
<div class="readable-text intended-text" id="p17">
<p>In machine learning workflows, this process typically involves converting our model from its development framework (PyTorch, TensorFlow, or other) to an intermediate representation (IR), like TorchScript, MLIR, or ONNX. We can then use hardware-specific software to convert these IR models to compiled machine code for our hardware of choice—GPU, TPU (tensor-processing units), CPU, etc. Why not just convert directly from your framework of choice to machine code and skip the middleman? Great question. The reason is simple: there are dozens of frameworks and hundreds of hardware units, and writing code to cover each combination is out of the question. So instead, framework developers provide conversion tooling to an IR, and hardware vendors provide conversions from an IR to their specific hardware. </p>
</div>
<div class="readable-text intended-text" id="p18">
<p>For the most part, the actual process of compiling a model involves running a few commands. Thanks to PyTorch 2.x, you can get a head start on it by using the <code>torch.compile(model)</code> command, which you should do before training and before deployment. Hardware companies often provide compiling software for free, as it’s a big incentive for users to purchase their product. Building this software isn’t easy, however, and often requires expertise in both the hardware architecture and the machine learning architectures. This combination of these talents is rare, and there’s good money to be had if you get a job in this field. </p>
</div>
<div class="readable-text intended-text" id="p19">
<p>We will show you how to compile an LLM in a minute, but first, let’s take a look at some of the techniques used. What better place to start than with the all-important kernel tuning? </p>
</div>
<div class="readable-text" id="p20">
<h4 class="readable-text-h4 sigil_not_in_toc">Kernel tuning</h4>
</div>
<div class="readable-text" id="p21">
<p>In deep learning and high-performance computing, a kernel is a small program or function designed to run on a GPU or other similar processors. These routines are developed by the hardware vendor to maximize chip efficiency. They do this by optimizing threads, registries, and shared memory across blocks of circuits on the silicon. When we run arbitrary code, the processor will try to route the requests the best it can across its logic gates, but it’s bound to run into bottlenecks. However, if we are able to identify the kernels to run and their order beforehand, the GPU can map out a more efficient route—and that’s essentially what kernel tuning is.</p>
</div>
<div class="readable-text intended-text" id="p22">
<p>During kernel tuning, the most suitable kernels are chosen from a large collection of highly optimized kernels. For instance, consider convolution operations that have several possible algorithms. The optimal one from the vendor’s library of kernels will be based on various factors like the target GPU type, input data size, filter size, tensor layout, batch size, and more. When tuning, several of these kernels will be run and optimized to minimize execution time.</p>
</div>
<div class="readable-text intended-text" id="p23">
<p>This process of kernel tuning ensures that the final deployed model is not only optimized for the specific neural network architecture being used but also finely tuned for the unique characteristics of the deployment platform. This process results in more efficient use of resources and maximizes performance. Next, let’s look at tensor fusion, which optimizes running these kernels.</p>
</div>
<div class="readable-text" id="p24">
<h4 class="readable-text-h4 sigil_not_in_toc">Tensor fusion</h4>
</div>
<div class="readable-text" id="p25">
<p>In deep learning, when a framework executes a computation graph, it makes multiple function calls for each layer. The computation graph is a powerful concept used to simplify mathematical expressions and execute a sequence of tensor operations, especially for neural network models. If each operation is performed on the GPU, it invokes many CUDA kernel launches. However, the fast kernel computation doesn’t quite match the slowness of launching the kernel and handling tensor data. As a result, the GPU resources might not be fully utilized, and memory bandwidth can become a choke point. It’s like making multiple trips to the store to buy separate items when we could make a single trip and buy all the items at once.</p>
</div>
<div class="readable-text intended-text" id="p26">
<p>This is where tensor fusion comes in. It improves this situation by merging or fusing kernels to perform operations as one, reducing unnecessary kernel launches and improving memory efficiency. A common example of a composite kernel is a fully connected kernel that combines or fuses a matmul, bias add, and ReLU kernel. It’s similar to the concept of tensor parallelization. In tensor parallelization, we speed up the process by sending different people to different stores, like the grocery store, the hardware store, and a retail store. This way, one person doesn’t have to go to every store. Tensor fusion can work very well with parallelization across multiple GPUs. It’s like sending multiple people to different stores and making each one more efficient by picking up multiple items instead of one.</p>
</div>
<div class="readable-text" id="p27">
<h4 class="readable-text-h4 sigil_not_in_toc">Graph optimization</h4>
</div>
<div class="readable-text" id="p28">
<p>Tensor fusion, when done sequentially, is also known as vertical graph optimization. We can also do horizontal graph optimization. These optimizations are often talked about as two different things. Horizontal graph optimization, which we’ll refer to simply as graph optimization, combines layers with shared input data but with different weights into a single broader kernel. It replaces the concatenation layers by pre-allocating output buffers and writing into them in a distributed manner.</p>
</div>
<div class="readable-text intended-text" id="p29">
<p>In figure 6.2, we show an example of a simple deep learning graph being optimized. Graph optimizations do not change the underlying computation in the graph. They are simply restructuring the graph. As a result, the optimized graph performs more efficiently with fewer layers and kernel launches, reducing inference latency. This restructuring makes the whole process smaller, faster, and more efficient.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p30">
<img alt="figure" height="549" src="../Images/6-2.png" width="1012"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 6.2</span> An example of an unoptimized network compared to the same network optimized using graph optimization. CBR is a NVIDIA fused layer kernel that simply stands for Convolution, Bias, and ReLU. See the following NVIDIA blog post for reference: <a href="https://mng.bz/PNvw">https://mng.bz/PNvw</a>.</h5>
</div>
<div class="readable-text" id="p31">
<p>The graph optimization technique is often used in the context of computational graph-based frameworks like TensorFlow. Graph optimization involves techniques that simplify these computational graphs, remove redundant operations, and/or rearrange computations, making them more efficient for execution, especially on specific hardware (like GPU or TPU). An example is constant folding, where the computations involving constant inputs are performed at compile time (before run time), thereby reducing the computation load during run time.</p>
</div>
<div class="readable-text intended-text" id="p32">
<p>These aren’t all the techniques used when compiling a model, but they are some of the most common and should give you an idea of what’s happening under the hood and why it works. Now let’s look at some tooling to do this for LLMs.</p>
</div>
<div class="readable-text" id="p33">
<h4 class="readable-text-h4 sigil_not_in_toc">TensorRT</h4>
</div>
<div class="readable-text" id="p34">
<p>NVIDIA’s TensorRT is a one-stop shop to compile your model, and who better to trust than the hardware manufacturer to better prepare your model to run on their GPUs? TensorRT does everything talked about in this section, along with quantization to INT8 and several memory tricks to get the most out of your hardware to boot.</p>
</div>
<div class="readable-text intended-text" id="p35">
<p>In listing 6.1, we demonstrate the simple process of compiling an LLM using TensorRT. We’ll use the PyTorch version known as <code>torch_tensorrt</code>. It’s important to note that compiling a model to a specific engine is hardware specific. So you will want to compile the model on the exact hardware you intend to run it on. Consequently, installing TensorRT is a bit more than a simple <code>pip</code> <code>install</code>; thankfully, we can use Docker instead. To get started, run the following command:</p>
</div>
<div class="browsable-container listing-container" id="p36">
<div class="code-area-container">
<pre class="code-area">$ docker run --gpus all -it --rm nvcr.io/nvidia/pytorch:23.09-py3</pre>
</div>
</div>
<div class="readable-text" id="p37">
<p>This command will start up an interactive <code>torch_tensorrt</code> Docker container with practically everything we need to get started (for the latest version, see  <a href="https://mng.bz/r1We">https://mng.bz/r1We</a>). The only thing missing is Hugging Face Transformers, so go ahead and install that. Now we can run the listing.</p>
</div>
<div class="readable-text intended-text" id="p38">
<p>After our imports, we’ll load our model and generate an example input so we can trace the model. We need to convert our model to an IR—TorchScript here—and this is done through tracing. Tracing is the process of capturing the operations that are invoked when running the model and makes graph optimization easier later. If you have a model that takes varying inputs, for example, the CLIP model, which can take both images and text and turn them into embeddings, tracing that model with only text data is an effective way of pruning the image operations out of the model. Once our model has been converted to an IR, then we can compile it for NVIDIA GPUs using TensorRT. Once completed, we then simply reload the model from disk and run some inference for demonstration. </p>
</div>
<div class="browsable-container listing-container" id="p39">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 6.1</span> Compiling a model with TensorRT</h5>
<div class="code-area-container">
<pre class="code-area">import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import torch_tensorrt

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokens = tokenizer("The cat is on the table.", return_tensors="pt")[
    "input_ids"
].cuda()
model = GPT2LMHeadModel.from_pretrained(
    "gpt2", use_cache=False, return_dict=False, torchscript=True
).cuda()
model.eval()

traced_model = torch.jit.trace(model, tokens)     <span class="aframe-location"/> #1

compile_settings = {    <span class="aframe-location"/> #2
    "inputs": [
        torch_tensorrt.Input(
            # For static size
            shape=[1, 7],
            # For dynamic sizing:
            # min_shape=[1, 3],
            # opt_shape=[1, 128],
            # max_shape=[1, 1024],
            dtype=torch.int32,  # Datatype of input tensor.
            # Allowed options torch.(float|half|int8|int32|bool)
        )
    ],
    "truncate_long_and_double": True,
    "enabled_precisions": {torch.half},     <span class="aframe-location"/> #3
    "ir": "torchscript",
}
trt_model = torch_tensorrt.compile(traced_model, **compile_settings)

torch.jit.save(trt_model, "trt_model.ts")     <span class="aframe-location"/> #4

trt_model = torch.jit.load("trt_model.ts")     <span class="aframe-location"/> #5
tokens.half()
tokens = tokens.type(torch.int)
logits = trt_model(tokens)
results = torch.softmax(logits[-1], dim=-1).argmax(dim=-1)
print(tokenizer.batch_decode(results))</pre>
<div class="code-annotations-overlay-container">
     #1 Converts to Torchscript IR
     <br/>#2 Compiles the model with TensorRT
     <br/>#3 Runs with FP16
     <br/>#4 Saves the compiled model
     <br/>#5 Runs inference
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p40">
<p>The output is </p>
</div>
<div class="browsable-container listing-container" id="p41">
<div class="code-area-container">
<pre class="code-area"># ['\n was a the way.\n']</pre>
</div>
</div>
<div class="readable-text" id="p42">
<p>We’ll just go ahead and warn you: your results may vary when you run this code, depending on your setup. Overall, it’s a simple process once you know what you are doing, and we’ve regularly seen at least 2× speed improvements in inference times—which translates to major savings! </p>
</div>
<div class="readable-text intended-text" id="p43">
<p>TensorRT really is all that and a bag of chips. Of course, the major downside to TensorRT is that, as a tool developed by NVIDIA, it is built with NVIDIA’s hardware in mind. When compiling code for other hardware and accelerators, it’s not going to be useful. Also, you’ll get very used to running into error messages when working with TensorRT. We’ve found that running into compatibility problems when converting models that aren’t supported is a common occurrence. We’ve run into many problems trying to compile various LLM architectures. Thankfully, to address this, NVIDIA has been working on a TensorRT-LLM library to supercharge LLM inference on NVIDIA high-end GPUs. It supports many more LLM architectures than vanilla TensorRT. You can check if it supports your chosen LLM architecture and GPU setup here: <a href="https://mng.bz/mRXP">https://mng.bz/mRXP</a>.</p>
</div>
<div class="readable-text intended-text" id="p44">
<p>Don’t get us wrong; you don’t have to use TensorRT. Several alternative compilers are available. In fact, let’s look at another popular alternative, ONNX Runtime. Trust us, you’ll want an alternative when TensorRT doesn’t play nice.</p>
</div>
<div class="readable-text" id="p45">
<h4 class="readable-text-h4 sigil_not_in_toc">ONNX Runtime</h4>
</div>
<div class="readable-text" id="p46">
<p>ONNX, which stands for Open Neural Network Exchange, is an open source format and ecosystem designed for representing and interoperating between different deep learning frameworks, libraries, and tools. It was created to address the challenge of model portability and compatibility. As mentioned previously, ONNX is an IR and allows you to represent models trained in one deep learning framework (e.g., TensorFlow, PyTorch, Keras, MXNet) in a standardized format easily consumed by other frameworks. Thus, it facilitates the exchange of models between different tools and environments. Unlike TensorRT, ONNX Runtime is intended to be hardware-agnostic, meaning it can be used with a variety of hardware accelerators, including CPUs, GPUs, and specialized hardware like TPUs.</p>
</div>
<div class="readable-text intended-text" id="p47">
<p>In practical terms, ONNX allows machine learning practitioners and researchers to build and train models using their preferred framework and then deploy those models to different platforms and hardware without the need for extensive reengineering or rewriting of code. This process helps streamline the development and deployment of AI and ML models across various applications and industries. To be clear, ONNX is an IR format, while ONNX Runtime allows us to optimize and run inference with ONNX models.</p>
</div>
<div class="readable-text intended-text" id="p48">
<p>To take advantage of ONNX, we recommend using Hugging Face’s Optimum. Optimum is an interface that makes working with optimizers easier and supports multiple engines and hardware, including Intel Neural Compressor for Intel chips and Furiosa Warboy for Furiosa NPUs. It’s worth checking out. For our purposes, we will use it to convert LLMs to ONNX and then optimize them for inference with ONNX Runtime. First, let’s install the library with the appropriate engines. We’ll use the <code>--upgrade-strategy</code> <code>eager</code>, as suggested by the documentation, to ensure the different packages are upgraded:</p>
</div>
<div class="browsable-container listing-container" id="p49">
<div class="code-area-container">
<pre class="code-area">$ pip install --upgrade-strategy eager optimum[exporters,onnxruntime]</pre>
</div>
</div>
<div class="readable-text" id="p50">
<p>Next, we’ll run the optimum command line interface. We’ll export it to ONNX, point it to a Hugging Face transformer model, and give it a local directory to save the model to. Those are all the required steps, but we’ll also give it an optimization feature flag. Here, we’ll do the basic general optimizations:</p>
</div>
<div class="browsable-container listing-container" id="p51">
<div class="code-area-container code-area-with-html">
<pre class="code-area"><span class="">↪</span> $ optimum-cli export onnx --model WizardLM/WizardCoder-1B-V1.0 ./models_onnx --optimize O1</pre>
</div>
</div>
<div class="readable-text" id="p52">
<p>And we are done. We now have an LLM model converted to ONNX format and optimized with basic graph optimizations. As with all compiling processes, optimization should be done on the hardware you intend to run inference on, which should include ample memory and resources, as the conversion can be somewhat computationally intensive.</p>
</div>
<div class="readable-text intended-text" id="p53">
<p>To run the model, check out <a href="https://onnxruntime.ai/">https://onnxruntime.ai/</a> for quick start guides on how to run it with your appropriate SDK. Oh, yeah, did we forget to mention that ONNX Runtime supports multiple programming APIs, so you can now run your LLM directly in your favorite language, including Java, C++, C#, or even JavaScript? Well, you can. So go party. We’ll be sticking to Python in this book, though, for consistency’s sake.</p>
</div>
<div class="readable-text intended-text" id="p54">
<p>While TensorRT is likely to be your weapon of choice most of the time, and ONNX Runtime covers many edge cases, there are still many other excellent engines out there, like OpenVINO. You can choose whatever you want, but you should at least use something. Doing otherwise would be an egregious mistake. In fact, now that you’ve read this section, you can no longer claim ignorance. It is now your professional responsibility to ensure this happens. Putting any ML model into production that hasn’t first been compiled (or at least attempted to be compiled) is a sin to the MLOps profession.</p>
</div>
<div class="readable-text" id="p55">
<h3 class="readable-text-h3" id="sigil_toc_id_103"><span class="num-string">6.1.2</span> LLM storage strategies</h3>
</div>
<div class="readable-text" id="p56">
<p>Now that we have a nicely compiled model, we need to think about how our service will access it. This step is important because, as discussed in chapter 3, boot times can be a nightmare when working with LLMs since it can take a long time to load such large assets into memory. So we want to try to speed that up as much as possible. When it comes to managing large assets, we tend to throw them into an artifact registry or a bucket in cloud storage and forget about them. Both of these tend to utilize an object storage system—like GCS or S3—under the hood, which is great for storage but less so for object retrieval, especially when it comes to large objects like LLMs. </p>
</div>
<div class="readable-text intended-text" id="p57">
<p>Object storage systems break up assets into small fractional bits called objects. They allow us to federate the entire asset across multiple machines and physical memory locations, a powerful tool that powers the cloud, and to cheaply store large objects on commodity hardware. With replication, there is built-in fault tolerance, so we never have to worry about losing our assets from a hardware crash. Object storage systems also create high availability, ensuring we can always access our assets. The downside is that these objects are federated across multiple machines and not in an easily accessible form to be read and stored in memory. Consequently, when we load an LLM into GPU memory, we will essentially have to download the model first. Let’s look at some alternatives.</p>
</div>
<div class="readable-text" id="p58">
<h4 class="readable-text-h4 sigil_not_in_toc">Fusing</h4>
</div>
<div class="readable-text" id="p59">
<p>Fusing is the process of mounting a bucket to your machine as if it were an external hard drive. Fusing provides a slick interface and simplifies code, as you will no longer have to download the model and then load it into memory. With fusing, you can treat an external bucket like a filesystem and load the model directly into memory. However, it still doesn’t solve the fundamental need to pull the objects of your asset from multiple machines. Of course, if you fuse a bucket to a node in the same region and zone, some optimizations can improve performance, and it will feel like you are loading the model from the drive. Unfortunately, our experience has shown fusing to be quite slow, but it should still be faster than downloading and then loading.</p>
</div>
<div class="readable-text intended-text" id="p60">
<p>Fusing libraries are available for all major cloud providers and on-prem object storage solutions, like Ceph or MinIO, so you should be covered no matter the environment, including your own laptop. That’s right. You can fuse your laptop or an edge device to your object storage solution. This ability demonstrates both how powerful and, at the same time, ineffective this strategy is, depending on what you were hoping it would achieve.</p>
</div>
<div class="readable-text print-book-callout" id="p61">
<p><span class="print-book-callout-head">TIP</span>  All fusing libraries are essentially built off the FUSE library. It’s worth checking out: <a href="https://github.com/libfuse/libfuse">https://github.com/libfuse/libfuse</a>.</p>
</div>
<div class="readable-text" id="p62">
<h4 class="readable-text-h4 sigil_not_in_toc">Baking the model</h4>
</div>
<div class="readable-text" id="p63">
<p>Baking is the process of putting your model into the Docker image. Thus, whenever a new container is created, the model will be there, ready for use. Baking models, in general, is considered an antipattern. For starters, it doesn’t solve the problem. In production, when a new instance is created, a new machine is spun up. It is fresh and innocent, knowing nothing of the outside world, so the first step it’ll have to take is to download the image. Since the image contains the model, we haven’t solved anything. Actually, it’s very likely that downloading the model inside an image will be <em>slower</em> than downloading the model from an object store. So we most likely just made our boot times worse. </p>
</div>
<div class="readable-text intended-text" id="p64">
<p>Second, baking models is a terrible security practice. Containers often have poor security and are often easy for people to gain access to. Third, you’ve doubled your problems: before you just had one large asset; now you have two, the model and the image.</p>
</div>
<div class="readable-text intended-text" id="p65">
<p>That said, there are still times when baking is viable, mainly because despite the drawbacks, it greatly simplifies our deployments. Throwing all our assets into the image guarantees we’ll only need one thing to deploy a new service: the image itself, which is really valuable when deploying to an edge device, for example. </p>
</div>
<div class="readable-text" id="p66">
<h4 class="readable-text-h4 sigil_not_in_toc">Mounted volume</h4>
</div>
<div class="readable-text" id="p67">
<p>Another solution is to avoid the object store completely and save your LLM in a file-based storage system on a mountable drive. When our service boots up, we can connect the disc drive housing the LLM with a RAID controller or Kubernetes, depending on our infrastructure. This solution is old school, but it works really well. For the most part, it solves all our problems and provides incredibly fast boot times.</p>
</div>
<div class="readable-text intended-text" id="p68">
<p>The downside, of course, is that it will add a bunch of coordination steps to ensure there is a volume in each region and zone you plan to deploy to. It also brings up replication and reliability problems; if the drive dies unexpectedly, you’ll need backups in the region. In addition, these drives will likely be SSDs and not just commodity hardware. So you’ll likely be paying a bit more. But storage is extremely cheap compared to GPUs, so the time saved in boot times is something you’ll have to consider. Essentially, though, this strategy reintroduces all the problems for which we usually turn to object stores to begin with. </p>
</div>
<div class="readable-text" id="p69">
<h4 class="readable-text-h4 sigil_not_in_toc">Hybrid: Intermediary mounted volume</h4>
</div>
<div class="readable-text" id="p70">
<p>Lastly, we can always take a hybrid approach. In this solution, we download the model at boot time but store it in a volume that is mounted at boot time. While this doesn’t help at all with the first deployment in a region, it does substantially help any new instances, as they can simply mount this same volume and have the model available to load without having to download. You can imagine this working similarly to how a Redis cache works, except for storage. Often, this technique is more than enough since autoscaling will be fast enough to handle bursty workloads. We just have to worry about total system crashes, which hopefully should be minimal, but they allude to the fact that we should avoid this approach when only running one replica, which you shouldn’t do in production anyway.</p>
</div>
<div class="readable-text intended-text" id="p71">
<p>In figure 6.3, we demonstrate these different strategies and compare them to a basic service where we simply download the LLM and then load it into memory. Overall, your exact strategy will depend on your system requirements, the size of the LLM you are running, and your infrastructure. Your system requirements will also likely vary widely, depending on the type of traffic patterns you see.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p72">
<img alt="figure" height="407" src="../Images/6-3.png" width="1017"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 6.3</span> Different strategies for storing LLMs and their implications at boot time. Often, we have to balance system reliability, complexity, and application load time.</h5>
</div>
<div class="readable-text" id="p73">
<p>Now that we have a good handle on how to handle our LLM as an asset, let’s talk about some API features that are must-haves for your LLM service.</p>
</div>
<div class="readable-text" id="p74">
<h3 class="readable-text-h3" id="sigil_toc_id_104"><span class="num-string">6.1.3</span> Adaptive request batching</h3>
</div>
<div class="readable-text" id="p75">
<p>A typical API will accept and process requests in the order they are received, processing them immediately and as quickly as possible. However, anyone who’s trained a machine learning model has come to realize that there are mathematical and computational advantages to running inference in batches of powers of 2 (16, 32, 64, etc.), particularly when GPUs are involved, where we can take advantage of better memory alignment or vectorized instructions parallelizing computations across the GPU cores. To take advantage of this batching, you’ll want to include adaptive request batching or dynamic batching.</p>
</div>
<div class="readable-text intended-text" id="p76">
<p>What adaptive batching does is essentially pool requests together over a certain period of time. Once the pool receives the configured maximum batch size or the timer runs out, it will run inference on the entire batch through the model, sending the results back to the individual clients that requested them. Essentially, it’s a queue. Setting one up yourself can and will be a huge pain; thankfully, most ML inference services offer this out of the box, and almost all are easy to implement. For example, in BentoML, add <code>@bentoml.Runnable.method(batchable=True)</code> as a decorator to your predict function, and in Triton Inference Server, add <code>dynamic_batching</code> <code>{}</code> at the end of your model definition file.</p>
</div>
<div class="readable-text intended-text" id="p77">
<p>If that sounds easy, it is. Typically, you don’t need to do any further finessing, as the defaults tend to be very practical. That said, if you are looking to maximize every bit of efficiency possible in the system, you can often set a maximum batch size, which will tell the batcher to run once this limit is reached, or a batch delay, which does the same thing but for the timer. Increasing either will result in longer latency but likely better throughput, so typically these are only adjusted when your system has plenty of latency budget.</p>
</div>
<div class="readable-text intended-text" id="p78">
<p>Overall, the benefits of adaptive batching include better use of resources and higher throughput at the cost of a bit of latency. This is a valuable tradeoff, and we recommend giving your product the latency bandwidth to include this feature. In our experience, optimizing for throughput leads to better reliability and scalability and thus greater customer satisfaction. Of course, when latency times are extremely important or traffic is few and far between, you may rightly forgo this feature.</p>
</div>
<div class="readable-text" id="p79">
<h3 class="readable-text-h3" id="sigil_toc_id_105"><span class="num-string">6.1.4</span> Flow control</h3>
</div>
<div class="readable-text" id="p80">
<p>Rate limiters and access keys are critical protections for an API, especially one sitting in front of an expensive LLM. Rate limiters control the number of requests a client can make to an API within a specified time, which helps protect the API server from abuse, such as distributed denial of service (DDoS) attacks, where an attacker makes numerous requests simultaneously to overwhelm the system and hinder its function.</p>
</div>
<div class="readable-text intended-text" id="p81">
<p>Rate limiters can also protect the server from bots that make numerous automated requests in a short span of time. This helps manage the server resources optimally so the server is not exhausted due to unnecessary or harmful traffic. They are also useful for managing quotas, thus ensuring all users have fair and equal access to the API’s resources. By preventing any single user from using excessive resources, the rate limiter ensures the system functions smoothly for all users.</p>
</div>
<div class="readable-text intended-text" id="p82">
<p>All in all, rate limiters are an important mechanism for controlling the flow of your LLM’s system processes. They can play a critical role in dampening bursty workloads and preventing your system from getting overwhelmed during autoscaling and rolling updates, especially when you have a rather large LLM with longer deployment times. Rate limiters can take several forms, and the one you choose will be dependent on your use case.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p83">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Types of rate limiters</h5>
</div>
<div class="readable-text" id="p84">
<p><strong>The following list describes the types of rate limiters:</strong></p>
</div>
<ul>
<li class="readable-text" id="p85"> <em>Fixed window</em>—This algorithm allows a fixed number of requests in a set duration of time. Let’s say five requests per minute, and it refreshes at the minute. It’s really easy to set up and reason about. However, it may lead to uneven distribution and can allow a burst of calls at the boundary of the time window. </li>
<li class="readable-text" id="p86"> <em>Sliding window log</em>—To prevent boundary problems, we can use a dynamic timeframe. Let’s say five requests in the last 60 seconds. This type is a slightly more complex version of the fixed window that logs each request’s timestamp to provide a moving lookback period, providing a more evenly distributed limit. </li>
<li class="readable-text" id="p87"> <em>Token bucket</em>—Clients initially have a full bucket of tokens, and with each request, they spend tokens. When the bucket is empty, the requests are blocked. The bucket refills slowly over time. Thus, token buckets allow burst behavior, but it’s limited to the number of tokens in the bucket. </li>
<li class="readable-text" id="p88"> <em>Leaky bucket</em>—It works as a queue where requests enter, and if the queue is not full, they are processed; if full, the request overflows and gets discarded, thus controlling the rate of the flow. </li>
</ul>
</div>
<div class="readable-text" id="p89">
<p>A rate limiter can be applied at multiple levels, from the entire API to individual client requests to specific function calls. While you want to avoid being too aggressive with them—better to rely on autoscaling to scale and meet demand—you don’t want to ignore them completely, especially when it comes to preventing bad actors.</p>
</div>
<div class="readable-text intended-text" id="p90">
<p>Access keys are also crucial to prevent bad actors. Access keys offer authentication, maintaining that only authorized users can access the API, which prevents unauthorized use and potential misuse of the API and reduces the influx of spam requests. They are also essential to set up for any paid service. Of course, even if your API is only exposed internally, setting up access keys shouldn’t be ignored, as it can help reduce liability and provide a way of controlling costs by yanking access to a rogue process, for example.</p>
</div>
<div class="readable-text intended-text" id="p91">
<p>Thankfully, setting up a service with rate limiting and access keys is relatively easy nowadays, as there are multiple libraries that can help you. In listing 6.2, we demonstrate a simple FastAPI app utilizing both. We’ll use FastAPI’s built-in security library for our access keys and SlowApi, a simple rate limiter that allows us to limit the call of any function or method with a simple decorator.</p>
</div>
<div class="browsable-container listing-container" id="p92">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 6.2</span> Example API with access keys and rate limiter</h5>
<div class="code-area-container">
<pre class="code-area">from fastapi import FastAPI, Depends, HTTPException, status, Request
from fastapi.security import OAuth2PasswordBearer
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded
import uvicorn

api_keys = ["1234567abcdefg"]       <span class="aframe-location"/> #1
API_KEY_NAME = "access_token"
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")

limiter = Limiter(key_func=get_remote_address)

app = FastAPI()
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)


async def get_api_key(api_key: str = Depends(oauth2_scheme)):
    if api_key not in api_keys:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid API Key",
        )


@app.get("/hello", dependencies=[Depends(get_api_key)])
@limiter.limit("5/minute")
async def hello(request: Request):
    return {"message": "Hello World"}</pre>
<div class="code-annotations-overlay-container">
     #1 This would be encrypted in a database.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p93">
<p>While this is just a simple example, you’ll still need to set up a system for users to create and destroy access keys. You’ll also want to finetune your time limits. In general, you want them to be as loose as possible so as not to interfere with the user experience but just tight enough to do their job.</p>
</div>
<div class="readable-text" id="p94">
<h3 class="readable-text-h3" id="sigil_toc_id_106"><span class="num-string">6.1.5</span> Streaming responses</h3>
</div>
<div class="readable-text" id="p95">
<p>One feature your LLM service should absolutely include is streaming. Streaming allows us to return the generated text to the user as it is being generated versus all at once at the end. Streaming adds quite a bit of complexity to the system, but regardless, it has come to be considered a must-have feature for several reasons. </p>
</div>
<div class="readable-text intended-text" id="p96">
<p>First, LLMs are rather slow, and the worst thing you can do to your users is make them wait—waiting means they will become bored, and bored users complain or, worse, leave. You don’t want to deal with complaints, do you? Of course not! But by streaming the data as it’s being created, we offer the users a more dynamic and interactive experience.</p>
</div>
<div class="readable-text intended-text" id="p97">
<p>Second, LLMs aren’t just slow; they are unpredictable. One prompt could lead to pages and pages of generated text, and another, a single token. As a result, your latency is going to be all over the place. Streaming allows us to worry about more consistent metrics like tokens per second (TPS). Keeping TPS higher than the average user’s reading speed means we’ll be sending responses back faster than the user can consume them, ensuring they won’t get bored and we are providing a high-quality user experience. In contrast, if we wait until the end to return the results, users will likely decide to walk away and return when it finishes because they never know how long to wait. This huge disruption to their flow makes your service less effective or useful.</p>
</div>
<div class="readable-text intended-text" id="p98">
<p>Lastly, users are starting to expect streaming. Streaming responses have become a nice tell as to whether you are speaking to a bot or an actual human. Since humans have to type, proofread, and edit their responses, we can’t expect written responses from a human customer support rep to be in a stream-like fashion. So when they see a response streaming in, your users will know they are talking to a bot. People interact differently with a bot than they will with a human, so it’s very useful information to give them to prevent frustration.</p>
</div>
<div class="readable-text intended-text" id="p99">
<p>In listing 6.3 we demonstrate a very simple LLM service that utilizes streaming. The key pieces to pay attention to are that we are using the base asyncio library to allow us to run asynchronous function calls, FastAPI’s <code>StreamingResponse</code> to ensure we send responses to the clients in chunks, and Hugging Face Transformer’s <code>TextIteratorStreamer</code> to create a pipeline generator of our model’s inference. </p>
</div>
<div class="browsable-container listing-container" id="p100">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 6.3</span> A streaming LLM service</h5>
<div class="code-area-container">
<pre class="code-area">import argparse
import asyncio
from typing import AsyncGenerator

from fastapi import FastAPI, Request
from fastapi.responses import Response, StreamingResponse
import uvicorn

from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TextIteratorStreamer,
)
from threading import Thread

app = FastAPI()

tokenizer = AutoTokenizer.from_pretrained("gpt2")    <span class="aframe-location"/> #1
model = AutoModelForCausalLM.from_pretrained("gpt2")
streamer = TextIteratorStreamer(tokenizer)


async def stream_results() -&gt; AsyncGenerator[bytes, None]:
    for response in streamer:
        await asyncio.sleep(1)                  <span class="aframe-location"/> #2
        yield (response + "\n").encode("utf-8")


@app.post("/generate")
async def generate(request: Request) -&gt; Response:
    """Generate LLM Response

    The request should be a JSON object with the following fields:
    - prompt: the prompt to use for the generation.
    """
    request_dict = await request.json()
    prompt = request_dict.pop("prompt")
    inputs = tokenizer([prompt], return_tensors="pt")
    generation_kwargs = dict(inputs, streamer=streamer, max_new_tokens=20)

    thread = Thread(target=model.generate, kwargs=generation_kwargs)   <span class="aframe-location"/> #3
    thread.start()

    return StreamingResponse(stream_results())


if __name__ == "__main__":
    parser = argparse.ArgumentParser()                     <span class="aframe-location"/> #4
    parser.add_argument("--host", type=str, default=None)
    parser.add_argument("--port", type=int, default=8000)
    args = parser.parse_args()

    uvicorn.run(app, host=args.host, port=args.port, log_level="debug")</pre>
<div class="code-annotations-overlay-container">
     #1 Loads tokenizer, model, and streamer into memory
     <br/>#2 Slows things down to see streaming. It’s typical to return streamed responses byte encoded.
     <br/>#3 Starts a separate thread to generate results
     <br/>#4 Starts service; defaults to localhost on port 8000
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p101">
<p>Now that we know how to implement several must-have features for our LLM service, including batching, rate limiting, and streaming, let’s look at some additional tooling we can add to our service to improve usability and overall workflow.</p>
</div>
<div class="readable-text" id="p102">
<h3 class="readable-text-h3" id="sigil_toc_id_107"><span class="num-string">6.1.6</span> Feature store</h3>
</div>
<div class="readable-text" id="p103">
<p>When it comes to running ML models in production, feature stores really simplify the inference process. We first introduced these in chapter 3, but as a recap, feature stores establish a centralized source of truth. They answer crucial questions about your data: Who is responsible for the feature? What is its definition? Who can access it? Let’s take a look at setting one up and querying the data to get a feel for how they work. We’ll be using Feast, which is open source and supports a variety of backends. To get started, let us <code>pip</code> <code>install</code> <code>feast</code> and then run the <code>init</code> command in your terminal to set up a project, like so:</p>
</div>
<div class="browsable-container listing-container" id="p104">
<div class="code-area-container">
<pre class="code-area">$ feast init feast_example
$ cd feast_example/feature_repo</pre>
</div>
</div>
<div class="readable-text" id="p105">
<p>The app we are building is a question-and-answer service. Q&amp;A services can greatly benefit from a feature store’s data governance tooling. For example, point-in-time joins help us answer questions like “Who is the president of x?” where the answer is expected to change over time. Instead of querying just the question, we query the question with a timestamp, and the point-in-time join will return whatever the answer to the question was in our database at that point in time. In the next listing, we pull a Q&amp;A dataset and store it in a parquet format in the data directory of our Feast project.</p>
</div>
<div class="browsable-container listing-container" id="p106">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 6.4</span> Downloading the SQuAD dataset </h5>
<div class="code-area-container">
<pre class="code-area">import pandas as pd
from datasets import load_dataset
import datetime

from sentence_transformers import SentenceTransformer
model = SentenceTransformer("all-MiniLM-L6-v2")


def save_qa_to_parquet(path):
    squad = load_dataset("squad", split="train[:5000]")   <span class="aframe-location"/> #1
    ids = squad["id"]                            <span class="aframe-location"/> #2
    questions = squad["question"]
    answers = [answer["text"][0] for answer in squad["answers"]]
    qa = pd.DataFrame(          <span class="aframe-location"/> #3
        zip(ids, questions, answers),
        columns=["question_id", "questions", "answers"],
    )

    qa["embeddings"] = qa.questions.apply(lambda x: model.encode(x))  <span class="aframe-location"/> #4
    qa["created"] = datetime.datetime.utcnow()
    qa["datetime"] = qa["created"].dt.floor("h")
    qa.to_parquet(path)             <span class="aframe-location"/> #5


if __name__ == "__main__":
    path = "./data/qa.parquet"
    save_qa_to_parquet(path)</pre>
<div class="code-annotations-overlay-container">
     #1 Loads SQuAΔ dataset
     <br/>#2 Extracts questions and answers
     <br/>#3 Creates a dataframe
     <br/>#4 Adds embeddings and timestamps
     <br/>#5 Saves to parquet
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p107">
<p>Next, we’ll need to define the feature view for our feature store. A feature view is essentially like a view in a relational database. We’ll define a name, the entities (which are like IDs or primary keys), the schema (which are our feature columns), and a source. We’ll just be demoing using a local file store, but in production, you’d want to use one of Feast’s many backend integrations with Snowflake, GCP, AWS, etc. It currently doesn’t support a VectorDB backend, but I’m sure it’s only a matter of time. In addition, we can add metadata to our view through tags and define a time to live (TTL), which limits how far back Feast will look when generating historical datasets. In the following listing, we define the feature view. Go ahead and add this definition into a file called qa.py in the feature_repo directory of our project.</p>
</div>
<div class="browsable-container listing-container" id="p108">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 6.5</span> Feast <code>FeatureView</code> definition </h5>
<div class="code-area-container">
<pre class="code-area">from feast import Entity, FeatureView, Field, FileSource, ValueType
from feast.types import Array, Float32, String
from datetime import timedelta

path = "./data/qa.parquet"

question = Entity(name="question_id", value_type=ValueType.STRING)

question_feature = Field(name="questions", dtype=String)

answer_feature = Field(name="answers", dtype=String)

embedding_feature = Field(name="embeddings", dtype=Array(Float32))

questions_view = FeatureView(
    name="qa",
    entities=[question],
    ttl=timedelta(days=1),
    schema=[question_feature, answer_feature, embedding_feature],
    source=FileSource(
        path=path,
        event_timestamp_column="datetime",
        created_timestamp_column="created",
        timestamp_field="datetime",
    ),
    tags={},
    online=True,
)</pre>
</div>
</div>
<div class="readable-text" id="p109">
<p>With that defined, let’s go ahead and register it. We’ll do that with</p>
</div>
<div class="browsable-container listing-container" id="p110">
<div class="code-area-container">
<pre class="code-area">$ feast apply</pre>
</div>
</div>
<div class="readable-text" id="p111">
<p>Next, we’ll want to materialize the view. In production, this is a step you’ll need to schedule on a routine basis with something like cron or Prefect. Be sure to update the UTC timestamp for the end date in this command to something in the future to ensure the view collects the latest data:</p>
</div>
<div class="browsable-container listing-container" id="p112">
<div class="code-area-container">
<pre class="code-area">$ feast materialize-incremental 2023-11-30T00:00:00 --views qa</pre>
</div>
</div>
<div class="readable-text" id="p113">
<p>Now all that’s left is to query it! The following listing shows a simple example of pulling features to be used at inference time.</p>
</div>
<div class="browsable-container listing-container" id="p114">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 6.6</span> Querying a feature view at inference</h5>
<div class="code-area-container">
<pre class="code-area">import pandas as pd
from feast import FeatureStore

store = FeatureStore(repo_path=".")

path = "./data/qa.parquet"
ids = pd.read_parquet(path, columns=["question_id"])

feature_vectors = store.get_online_features(
    features=["qa:questions", "qa:answers", "qa:embeddings"],
    entity_rows=[{"question_id": _id} for _id in ids.question_id.to_list()],
).to_df()
print(feature_vectors.head())</pre>
</div>
</div>
<div class="readable-text" id="p115">
<p>This example will pull the most up-to-date information for the lowest possible latency at inference time. For point-in-time retrieval, you would use the <code>get_historical_ features</code> method instead. In addition, in this example, we use a list of IDs for the entity rows parameter, but you could also use an SQL query making it very flexible and easy to use.</p>
</div>
<div class="readable-text" id="p116">
<h3 class="readable-text-h3" id="sigil_toc_id_108"><span class="num-string">6.1.7</span> Retrieval-augmented generation</h3>
</div>
<div class="readable-text" id="p117">
<p>Retrieval-augmented generation (RAG) has become the most widely used tool to combat hallucinations in LLMs and improve the accuracy of responses in our results. Its popularity is likely because RAG is both easy to implement and quite effective. As first discussed in section 3.4.5, vector databases are a tool you’ll want to have in your arsenal. One of the key reasons is that they make RAG so much easier to implement. In figure 6.4, we demonstrate a RAG system. In the preprocessing stage, we take our documents, break them up, and transform them into embeddings that we’ll load into our vector database. During inference, we can take our input, encode it into an embedding, and run a similarity search across our documents in that vector database to find the nearest neighbors. This type of inference is known as semantic search. Pulling relevant documents and inserting them into our prompt will help give context to the LLM and improve the results.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p118">
<img alt="figure" height="445" src="../Images/6-4.png" width="810"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 6.4</span> RAG system demonstrating how we use our input embeddings to run a search across our documentation, improving the results of the generated text from our LLM</h5>
</div>
<div class="readable-text intended-text" id="p119">
<p>We are going to demo implementing RAG using Pinecone since it will save us the effort of setting up a vector database. For listing 6.7, we will set up a Pinecone index and load a Wikipedia dataset into it. In this listing, we’ll create a <code>WikiDataIngestion</code> class to handle the heavy lifting. This class will load the dataset and run through each Wikipedia page, splitting the text into consumable chunks. It will then embed these chunks and upload everything in batches. Once we have everything uploaded, we can start to make queries.</p>
</div>
<div class="readable-text" id="p120">
<p>You’ll need an API key if you plan to follow along, so if you don’t already have one, go to Pinecone’s website (<a href="https://www.pinecone.io/">https://www.pinecone.io/</a>) and create a free account, set up a starter project (free tier), and get an API key. One thing to pay attention to as you read the listing is that we’ll split up the text into chunks of 400 tokens with <code>text_ splitter</code>. We specifically split on tokens instead of words or characters, which allows us to properly budget inside our token limits for our model. In this example, returning the top three results will add 1,200 tokens to our request, which allows us to plan ahead of time how many tokens we’ll give to the user to write their prompt.</p>
</div>
<div class="browsable-container listing-container" id="p121">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 6.7</span> Example setting up a Pinecone database</h5>
<div class="code-area-container">
<pre class="code-area">import os
import tiktoken
from datasets import load_dataset
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from pinecone import Pinecone, ServerlessSpec
from sentence_transformers import SentenceTransformer

from tqdm.auto import tqdm
from uuid import uuid4

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")           <span class="aframe-location"/> #1
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")             <span class="aframe-location"/> #2

pc = Pinecone(api_key=PINECONE_API_KEY)



class WikiDataIngestion:
    def __init__(
        self,
        index,
        wikidata=None,
        embedder=None,
        tokenizer=None,
        text_splitter=None,
        batch_limit=100,
    ):
        self.index = index
        self.wikidata = wikidata or load_dataset(
            "wikipedia", "20220301.simple", split="train[:10000]"
        )
        self.embedder = embedder or OpenAIEmbeddings(
            model="text-embedding-ada-002", openai_api_key=OPENAI_API_KEY
        )
        self.tokenizer = tokenizer or tiktoken.get_encoding("cl100k_base")
        self.text_splitter = (
            text_splitter
            or RecursiveCharacterTextSplitter(
                chunk_size=400,
                chunk_overlap=20,
                length_function=self.token_length,
                separators=["\n\n", "\n", " ", ""],
            )
        )
        self.batch_limit = batch_limit

    def token_length(self, text):
        tokens = self.tokenizer.encode(text, disallowed_special=())
        return len(tokens)

    def get_wiki_metadata(self, page):
        return {
            "wiki-id": str(page["id"]),
            "source": page["url"],
            "title": page["title"],
        }

    def split_texts_and_metadatas(self, page):
        basic_metadata = self.get_wiki_metadata(page)
        texts = self.text_splitter.split_text(page["text"])
        metadatas = [
            {"chunk": j, "text": text, **basic_metadata}
            for j, text in enumerate(texts)
        ]
        return texts, metadatas

    def upload_batch(self, texts, metadatas):
        ids = [str(uuid4()) for _ in range(len(texts))]
        embeddings = self.embedder.embed_documents(texts)
        self.index.upsert(vectors=zip(ids, embeddings, metadatas))

    def batch_upload(self):
        batch_texts = []
        batch_metadatas = []

        for page in tqdm(self.wikidata):
            texts, metadatas = self.split_texts_and_metadatas(page)

            batch_texts.extend(texts)
            batch_metadatas.extend(metadatas)

            if len(batch_texts) &gt;= self.batch_limit:
                self.upload_batch(batch_texts, batch_metadatas)
                batch_texts = []
                batch_metadatas = []

        if len(batch_texts) &gt; 0:
            self.upload_batch(batch_texts, batch_metadatas)


if __name__ == "__main__":
    index_name = "pincecone-llm-example"

    if index_name not in pc.list_indexes().names():     <span class="aframe-location"/> #3
        pc.create_index(
            name=index_name,
            metric="cosine",
            dimension=1536,         <span class="aframe-location"/> #4
            spec=ServerlessSpec(cloud="aws", region="us-east-1"),
        )

    index = pc.Index(index_name)        <span class="aframe-location"/> #5
    print(index.describe_index_stats())

    embedder = None          <span class="aframe-location"/> #6
    if not OPENAI_API_KEY:
        embedder = SentenceTransformer(
            "sangmini/msmarco-cotmae-MiniLM-L12_en-ko-ja"
        )                                                     <span class="aframe-location"/> #7
        embedder.embed_documents = lambda *args, **kwargs: embedder.encode(
            *args, **kwargs
        ).tolist()

    wiki_data_ingestion = WikiDataIngestion(index, embedder=embedder)   <span class="aframe-location"/> #8
    wiki_data_ingestion.batch_upload()
    print(index.describe_index_stats())

    query = "Did Johannes Gutenberg invent the printing press?"    <span class="aframe-location"/> #9
    embeddings = wiki_data_ingestion.embedder.embed_documents(query)
    results = index.query(vector=embeddings, top_k=3, include_metadata=True)
    print(results)</pre>
<div class="code-annotations-overlay-container">
     #1 Gets openai API key from 
     <a href="https://platform.openai.com">platform.openai.com</a>
<br/>#2 Finds API key in console at app.pinecone.io
     <br/>#3 Creates an index if it doesn’t exist
     <br/>#4 1536 dim of text-embedding-ada-002
     <br/>#5 Connects to the index and describes the stats
     <br/>#6 Uses a generic embedder if an openai api key is not provided
     <br/>#7 Also 1536 dim
     <br/>#8 Ingests data and describes the stats anew
     <br/>#9 Makes a query
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p122">
<p>When I ran this code, the top three query results to my question, “Did Johannes Gutenberg invent the printing press?” were the Wikipedia pages for Johannes Gutenberg, the pencil, and the printing press. Not bad! While a vector database isn’t going to be able to answer the question, it’s simply finding the most relevant articles based on the proximity of their embeddings to my question.</p>
</div>
<div class="readable-text intended-text" id="p123">
<p>With these articles, we can then feed their embeddings into our LLM as additional context to the question to ensure a more grounded result. Since we include sources, it will even have the wiki URL it can give as a reference, and it won’t just hallucinate one. By giving this context, we greatly reduce the concern about our LLM hallucinating and making up an answer.</p>
</div>
<div class="readable-text" id="p124">
<h3 class="readable-text-h3" id="sigil_toc_id_109"><span class="num-string">6.1.8</span> LLM service libraries</h3>
</div>
<div class="readable-text" id="p125">
<p>If you are starting to feel a bit overwhelmed about all the tooling and features you need to implement to create an LLM service, we have some good news for you: several libraries aim to do all of this for you! Some open source libraries of note are vLLM and OpenLLM (by BentoML). Hugging Face’s Text-Generation-Inference (TGI) briefly lost its open source license, but fortunately, it’s available again for commercial use. There are also some start-ups building some cool tooling in this space, and we recommend checking out TitanML if you are hoping for a more managed service. These are like the tools MLServer, BentoML, and Ray Serve discussed in section 3.4.8 on deployment service, but they are designed specifically for LLMs.</p>
</div>
<div class="readable-text intended-text" id="p126">
<p>Most of these toolings are still relatively new and under active development, and they are far from feature parity with each other, so pay attention to what they offer. What you can expect is that they should at least offer streaming, batching, and GPU parallelization support (something we haven’t specifically talked about in this chapter), but beyond that, it’s a crapshoot. Many of them still don’t support several features discussed in this chapter, nor do they support every LLM architecture. What they do, though, is make deploying LLMs easy. </p>
</div>
<div class="readable-text intended-text" id="p127">
<p>Using vLLM as an example, just <code>pip</code> <code>install</code> <code>vllm</code>, and then you can run</p>
</div>
<div class="browsable-container listing-container" id="p128">
<div class="code-area-container">
<pre class="code-area">$ python -m vllm.entrypoints.api_server --model IMJONEZZ/ggml-openchat-8192-q4_0</pre>
</div>
</div>
<div class="readable-text" id="p129">
<p>With just one command, we now have a service up and running the model we trained in chapter 5. Go ahead and play with it; you should be able to send requests to the <code>/generate</code> endpoint like so:</p>
</div>
<div class="browsable-container listing-container" id="p130">
<div class="code-area-container code-area-with-html">
<pre class="code-area">$ curl http://localhost:8000/generate -d '{"prompt": "Which pokemon is 
<span class="">↪</span> the best?", "use_beam_search": true, "n": 4, "temperature": 0}'</pre>
</div>
</div>
<div class="readable-text" id="p131">
<p>It’s very likely you won’t be all that impressed with any of these toolings. Still, you should be able to build your own API and have a good sense of how to do it at this point. Now that you have a service and can even spin it up locally, let’s discuss the infrastructure you need to set up to support these models for actual production usage. Remember, the better the infrastructure, the less likely you’ll be called in the middle of the night when your service goes down unexpectedly. None of us want that, so let’s check it out.</p>
</div>
<div class="readable-text" id="p132">
<h2 class="readable-text-h2" id="sigil_toc_id_110"><span class="num-string">6.2</span> Setting up infrastructure</h2>
</div>
<div class="readable-text" id="p133">
<p>Setting up infrastructure is a critical aspect of modern software development, and we shouldn’t expect machine learning to be any different. To ensure scalability, reliability, and efficient deployment of our applications, we need to plan a robust infrastructure that can handle the demands of a growing user base. This is where Kubernetes comes into play.</p>
</div>
<div class="readable-text intended-text" id="p134">
<p>Kubernetes, often referred to as k8s, is an open source container orchestration platform that helps automate and manage the deployment, scaling, and management of containerized applications. It is designed to simplify the process of running and coordinating multiple containers across a cluster of servers, making it easier to scale applications and ensure high availability. We are going to talk a lot about k8s in this chapter, and while you don’t need to be an expert, it will be useful to cover some basics to ensure we are all on the same page.</p>
</div>
<div class="readable-text intended-text" id="p135">
<p>At its core, k8s works by grouping containers into logical units called pods, which are the smallest deployable units in the k8s ecosystem. These pods are then scheduled and managed by the k8s control plane, which oversees their deployment, scaling, and updates. This control plane consists of several components that collectively handle the orchestration and management of containers. In figure 6.5, we give an oversimplification of the k8s architecture to help readers who are unfamiliar with it.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p136">
<img alt="figure" height="394" src="../Images/6-5.png" width="777"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 6.5</span> An oversimplification of the Kubernetes architecture. What you need to know is that our services run in pods, and pods run on nodes, which essentially are a machine. K8s helps us both manage the resources and handle the orchestration of deploying pods to these resources.</h5>
</div>
<div class="readable-text" id="p137">
<p>Using k8s, we can take advantage of features such as automatic scaling, load balancing, and service discovery, which greatly simplify the deployment and management of web applications. K8s provides a flexible and scalable infrastructure that can easily adapt to changing demands, allowing organizations to efficiently scale their applications as their user base grows. K8s offers a wide range of additional features and extensibility options, such as storage management, monitoring, and logging, which help ensure the smooth operation of web applications.</p>
</div>
<div class="readable-text intended-text" id="p138">
<p>One of these extensibility options is known as custom resource definitions (CRDs). CRDs are a feature of Kubernetes that allows users to create their own specifications for custom resources, thus extending the functionalities of Kubernetes without modifying the Kubernetes source code. With a CRD defined, we can create custom objects similar to how we would create a built-in object like a pod or service. This gives k8s a lot of flexibility that we will need for different functionality throughout this chapter.</p>
</div>
<div class="readable-text intended-text" id="p139">
<p>If you are new to Kubernetes, you might be scratching your head through parts of this section, and that’s totally fine. Hopefully, though, you have enough knowledge to get the gist of what we will be doing in this section and why. At least you’ll be able to walk away with a bunch of questions to ask your closest DevOps team member.</p>
</div>
<div class="readable-text" id="p140">
<h3 class="readable-text-h3" id="sigil_toc_id_111"><span class="num-string">6.2.1</span> Provisioning clusters</h3>
</div>
<div class="readable-text" id="p141">
<p>The first thing to do when starting any project is to set up a cluster. A cluster is a collective of worker machines or nodes where we will host our applications. Creating a cluster is relatively simple; configuring it is the hard part. Of course, there have been many books written on how to do this, and the majority of considerations like networking, security, and access control are outside the scope of this book. In addition, considering the steps you take will also be different depending on the cloud provider of choice and your company’s business strategy, we will focus on only the portions that we feel are needed to get you up and running, as well as any other tidbits that may make your life easier.</p>
</div>
<div class="readable-text intended-text" id="p142">
<p>The first step is to create a cluster. On GCP, you would use the gcloud tool and run</p>
</div>
<div class="browsable-container listing-container" id="p143">
<div class="code-area-container">
<pre class="code-area">$ gcloud container clusters create &lt;NAME&gt;</pre>
</div>
</div>
<div class="readable-text" id="p144">
<p>On AWS, using the eksctl tool, run</p>
</div>
<div class="browsable-container listing-container" id="p145">
<div class="code-area-container">
<pre class="code-area">$ eksctl create cluster</pre>
</div>
</div>
<div class="readable-text" id="p146">
<p>On Azure, using the az cli tool, run</p>
</div>
<div class="browsable-container listing-container" id="p147">
<div class="code-area-container">
<pre class="code-area">$ az group create --name=&lt;GROUP_NAME&gt; --location=westus
$ az aks create --resource-group=&lt;GROUP_NAME&gt; --name=&lt;CLUSTER_NAME&gt;</pre>
</div>
</div>
<div class="readable-text" id="p148">
<p>As you can see, even the first steps are highly dependent on your provider, and you can suspect that the subsequent steps will be as well. Since we realize most readers will be deploying in a wide variety of environments, we will not focus on the exact steps but hopefully give you enough context to search and discover for yourself.</p>
</div>
<div class="readable-text intended-text" id="p149">
<p>Many readers, we imagine, will already have a cluster set up for them by their infrastructure teams, complete with many defaults and best practices. One of these is setting up node auto-provisioning (NAP) or cluster autoscaling. NAP allows a cluster to grow, adding more nodes as deployments demand them. This way, we only pay for nodes we actually use. It’s a very convenient feature, but it often defines resource limits or restrictions on the instances available for autoscaling, and you can bet your cluster’s defaults don’t include accelerator or GPU instances in that pool. We’ll need to fix that.</p>
</div>
<div class="readable-text intended-text" id="p150">
<p>In GCP, we would create a configuration file like the one in the following listing, where we can include the GPU <code>resourceType</code>. In the example, we include T4s and both A100 types.</p>
</div>
<div class="browsable-container listing-container" id="p151">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 6.8</span> Example NAP config file </h5>
<div class="code-area-container">
<pre class="code-area">resourceLimits:
  - resourceType: 'cpu'
    minimum: 10
    maximum: 100
  - resourceType: 'memory'
    maximum: 1000
  - resourceType: 'nvidia-tesla-t4'
    maximum: 40
  - resourceType: 'nvidia-tesla-a100'
    maximum: 16
  - resourceType: 'nvidia-a100-80gb'
    maximum: 8
management:
  autoRepair: true
  autoUpgrade: true
shieldedInstanceConfig:
  enableSecureBoot: true
  enableIntegrityMonitoring: true
diskSizeGb: 100</pre>
</div>
</div>
<div class="readable-text" id="p152">
<p>You would then set this by running</p>
</div>
<div class="browsable-container listing-container" id="p153">
<div class="code-area-container">
<pre class="code-area">$ gcloud container clusters update &lt;CLUSTER_NAME&gt; --enable-autoprovisioning --autoprovisioning-config-file &lt;FILE_NAME&gt;</pre>
</div>
</div>
<div class="readable-text" id="p154">
<p>The real benefit of an NAP is that instead of predefining what resources are available at a fixed setting, we can set resource limits, which put a cap on the total number of GPUs that we would scale up to. They clearly define what GPUs we want and expect to be in any given cluster.</p>
</div>
<div class="readable-text intended-text" id="p155">
<p>When one author was first learning about limits, he often got them confused with similar concepts—quotas, reservations, and commitments—and has seen many others just as confused. Quotas, in particular, are very similar to limits. Their main purpose is to prevent unexpected overage charges by ensuring a particular project or application doesn’t consume too many resources. Unlike limits, which are set internally, quotas often require submitting a request to your cloud provider when you want to raise them. These requests help inform and are used by the cloud provider to better plan which resources to provision and put into different data centers in different regions. It’s tempting to think that the cloud provider will ensure those resources are available; however, quotas never guarantee there will be enough resources in a region for your cluster to use, and you might run into <code>resources</code> <code>not</code> <code>found</code> errors way before you hit them. </p>
</div>
<div class="readable-text intended-text" id="p156">
<p>While quotas and limits set an upper bound, reservations and commitments set the lower bound. Reservations are an agreement to guarantee that a certain amount of resources will always be available and often come with the caveat that you will be paying for these resources regardless of whether you end up using them. Commitments are similar to reservations but are often longer-term contracts, usually coming with a discounted price.</p>
</div>
<div class="readable-text" id="p157">
<h3 class="readable-text-h3" id="sigil_toc_id_112"><span class="num-string">6.2.2</span> Autoscaling</h3>
</div>
<div class="readable-text" id="p158">
<p>One of the big selling points to setting up a k8s cluster is autoscaling. Autoscaling is an important ingredient in creating robust production-grade services. The main reason is that we never expect any service to receive static request volume. If anything else, you should expect more volume during the day and less at night while people sleep. So we’ll want our service to spin up more replicas during peak hours to improve performance and spin down replicas during off hours to save money, not to mention the need to handle bursty workloads that often threaten to crash a service at any point.</p>
</div>
<div class="readable-text intended-text" id="p159">
<p>Knowing your service will automatically provision more resources and set up additional deployments based on the needs of the application is what allows many infrastructure engineers to sleep peacefully at night. The catch is that it requires an engineer to know what those needs are and ensure everything is configured correctly. While autoscaling provides flexibility, the real business value comes from the cost savings. Most engineers think about autoscaling in terms of scaling up to prevent meltdowns, but even more important to the business is the ability to scale down, freeing up resources and cutting costs.</p>
</div>
<div class="readable-text intended-text" id="p160">
<p>One of the main reasons cloud computing and technologies like Kubernetes have become essential in modern infrastructures is because autoscaling is built in. Autoscaling is a key feature of Kubernetes, and with horizontal pod autoscalers (HPAs), you can easily adjust the number of replicas of your application based on two native resources: CPU and memory usage, as shown in figure 6.6. However, in a book about putting LLMs in production, scaling based on CPU and memory alone will never be enough. We will need to scale based on custom metrics, specifically GPU utilization.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p161">
<img alt="figure" height="349" src="../Images/6-6.png" width="570"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 6.6</span> Basic autoscaling using the in-built k8s horizontal pod autoscaler (HPA). The HPA watches CPU and memory resources and will tell the deployment service to increase or decrease the number of replicas.</h5>
</div>
<div class="readable-text intended-text" id="p162">
<p>Setting up autoscaling based on GPU metrics is going to take a bit more work and requires setting up several services. It’ll become clear why we need each service as we discuss them, but the good news is that by the end, you’ll be able to set up your services to scale based on any metric, including external events such as messages from a message broker, requests to an HTTP endpoint, and data from a queue.</p>
</div>
<div class="readable-text" id="p163">
<p>The first service we’ll need is one that can collect the GPU metrics. For this, we have NVIDIA’s Data Center GPU Manager (DCGM), which provides a metrics exporter that can export GPU metrics. DCGM exposes a host of GPU metrics, including temperature and power usage, which can create some fun dashboards, but the most useful metrics for autoscaling are utilization and memory utilization.</p>
</div>
<div class="readable-text intended-text" id="p164">
<p>From here, the data will go to a service like Prometheus. Prometheus is a popular open source monitoring system used to monitor Kubernetes clusters and the applications running on them. Prometheus collects metrics from various sources and stores them in a time-series database, where they can be analyzed and queried. Prometheus can collect metrics directly from Kubernetes APIs and from applications running on the cluster using a variety of collection mechanisms such as exporters, agents, and sidecar containers. It’s essentially an aggregator of services like DCGM, including features like alerting and notification. It also exposes an HTTP API for service for external tooling like Grafana to query and create graphs and dashboards with.</p>
</div>
<div class="readable-text intended-text" id="p165">
<p>While Prometheus provides a way to store metrics and monitor our service, the metrics aren’t exposed to the internals of Kubernetes. For an HPA to gain access, we will need to register yet another service to either the custom metrics API or external metrics API. By default, Kubernetes comes with the metrics.k8s.io endpoint that exposes resource metrics, CPU, and memory utilization. To accommodate the need to scale deployments and pods on custom metrics, two additional APIs were introduced: custom.metrics.k9s.io and external.metrics.k8s.io. There are some limitations to this setup, as currently, only one “adapter” API service can be registered at a time for either one. This limitation mostly becomes a problem if you ever decide to change this endpoint from one provider to another.</p>
</div>
<div class="readable-text intended-text" id="p166">
<p>For this service, Prometheus provides the Prometheus Adapter, which works well, but from our experience, it wasn’t designed for production workloads. Alternatively, we would recommend KEDA. KEDA (Kubernetes Event-Driven Autoscaling) is an open source project that provides event-driven autoscaling for Kubernetes. It offers more flexibility in terms of the types of custom metrics that can be used for autoscaling. While Prometheus Adapter requires configuring metrics inside a ConfigMap, any metric already exposed through the Prometheus API can be used in KEDA, providing a more streamlined and friendly user experience. It also offers scaling to and from 0, which isn’t available through HPAs, allowing you to turn off a service completely if there is no traffic. That said, you can’t scale from 0 on resource metrics like CPU and memory and, by extension, GPU metrics, but it is useful when you are using traffic metrics or a queue to scale.</p>
</div>
<div class="readable-text intended-text" id="p167">
<p>Putting this all together, you’ll end up with the architecture shown in figure 6.7. Compared to figure 6.6, you’ll notice at the bottom that DCGM is managing our GPU metrics and feeding them into Prometheus Operator. From Prometheus, we can set up external dashboards with tools like Grafana. Internal to k8s, we’ll use KEDA to set up a custom.metrics.k9s.io API to return these metrics so we can autoscale based on the GPU metrics. KEDA has several CRDs, one of which is a <code>ScaledObject</code>, which creates the HPA and provides the additional features.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p168">
<img alt="figure" height="459" src="../Images/6-7.png" width="774"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 6.7</span> Autoscaling based on a custom metric like GPU utilization requires several extra tools to work, including NVIDIA’s DCGM, a monitoring system like Prometheus Operator, and a custom metrics API like that provided by KEDA.</h5>
</div>
<div class="readable-text" id="p169">
<p>While autoscaling provides many benefits, it’s important to be aware of its limitations and potential problems, which are only exacerbated by LLM inference services. Proper configuration of the HPA is often an afterthought for many applications, but it becomes mission-critical when dealing with LLMs. LLMs take longer to become fully operational, as the GPUs need to be initialized and model weights loaded into memory; these aren’t services that can turn on a dime, which often can cause problems when scaling up if not properly prepared for. Additionally, if the system scales down too aggressively, it may result in instances being terminated before completing their assigned tasks, leading to data loss or other problems. Lastly, flapping is just such a concern that can arise from incorrect autoscaling configurations. Flapping happens when the number of replicas keeps oscillating, booting up a new service only to terminate it before it can serve any inferences.</p>
</div>
<div class="readable-text intended-text" id="p170">
<p>There are essentially five parameters to tune when setting up an HPA: </p>
</div>
<ul>
<li class="readable-text" id="p171"> Target parameter </li>
<li class="readable-text" id="p172"> Target threshold </li>
<li class="readable-text" id="p173"> Min pod replicas </li>
<li class="readable-text" id="p174"> Max pod replicas </li>
<li class="readable-text" id="p175"> Scaling policies </li>
</ul>
<div class="readable-text" id="p176">
<p>Let’s take a look at each of them in turn so you can be sure your system is properly configured.</p>
</div>
<div class="readable-text" id="p177">
<h4 class="readable-text-h4 sigil_not_in_toc">Target parameter</h4>
</div>
<div class="readable-text" id="p178">
<p>The target parameter is the most important metric to consider when ensuring your system is properly configured. If you followed the previously listed steps in section 6.2.2, your system is now ready to autoscale based on GPU metrics, so this should be easy, right? Not so fast! Scaling based on GPU utilization is going to be the most common and straightforward path, but the first thing we need to do is ensure the GPU is the actual bottleneck in our service. It’s pretty common to see eager young engineers throw a lot of expensive GPUs onto a service but forget to include adequate CPU and memory capacity. CPU and memory will still be needed to handle the API layer, such as taking in requests, handling multiple threads, and communicating with the GPUs. If there aren’t enough resources, these layers can quickly become a bottleneck, and your application will be throttled way before the GPU utilization is ever affected, ensuring the system will never actually autoscale. While you could switch the target parameter on the autoscaler, CPU and memory are cheap compared to GPU resources, so it’d be better to allocate more of them for your application.</p>
</div>
<div class="readable-text intended-text" id="p179">
<p>In addition, there are cases where other metrics make more sense. If your LLM application takes most of its requests from a streaming or batch service, it can be more prudent to scale based on metrics that tell you a DAG is running or an upstream queue is filling up—especially if these metrics give you an early signal and allow you more time to scale up in advance.</p>
</div>
<div class="readable-text intended-text" id="p180">
<p>Another concern when selecting the metric is its stability. For example, an individual GPU’s utilization tends to be close to either 0% or 100%. This can cause problems for the autoscaler, as the metric oscillates between an on and off state, as will its recommendation to add or remove replicas, causing flapping. Generally, flapping is avoided by taking the average utilization across all GPUs running the service. Using the average will stabilize the metric when you have a lot of GPUs, but it could still be a problem when the service has scaled down. If you are still running into problems, you’ll want to use an average-over-time aggregation, which will tell you the utilization for each GPU over a time frame—say, the last 5 minutes. For CPU utilization, average-over-time aggregation is built into the Kubernetes HPA and can be set with the <code>horizontal-pod-autoscaler-cpu-initialization-period</code> flag. For custom metrics, you’ll need to set it in your metric query (for Prometheus, it would be the <code>avg_over_ time</code> aggregation function).</p>
</div>
<div class="readable-text intended-text" id="p181">
<p>Lastly, it’s worth calling out that most systems allow you to autoscale based on multiple metrics. So you <em>could </em>autoscale based on both CPU and GPU utilization, as an example. However, we would recommend avoiding these setups unless you know what you are doing. Your autoscaler might be set up that way, but in actuality, your service will likely only ever autoscale based on just one of the metrics due to service load, and it’s best to make sure that metric is the more costly resource for cost-engineering purposes.</p>
</div>
<div class="readable-text" id="p182">
<h4 class="readable-text-h4 sigil_not_in_toc">Target threshold</h4>
</div>
<div class="readable-text" id="p183">
<p>The target threshold tells your service at what point to start upscaling. For example, if you are scaling based on the average GPU utilization and your threshold is set to 30, then a new replica will be booted up to take on the extra load when the average GPU utilization is above 30%. The formula that governs this is quite simple and is as follows:</p>
</div>
<div class="readable-text indented-paragraph equation-paragraph" id="p184">
<p>desiredReplicas = ceil[currentReplicas × (currentMetricValue / desiredMetricValue )]</p>
</div>
<div class="readable-text print-book-callout" id="p185">
<p><span class="print-book-callout-head">NOTE</span>  You can learn more about the algorithm at <a href="https://mng.bz/x64g">https://mng.bz/x64g</a>.</p>
</div>
<div class="readable-text" id="p186">
<p>This can be hard to tune in correctly, but here are some guiding principles. If the traffic patterns you see involve a lot of constant small bursts of traffic, a lower value, around 50, might be more appropriate. This setting ensures you start to scale up more quickly, avoiding unreliability problems, and you can also scale down more quickly, cutting costs. If you have a constant steady flow of traffic, higher values, around 80, will work well. Outside of testing your autoscaler, it’s best to avoid extremely low values, as they can increase your chances of flapping. You should also avoid extremely high values, as they may allow the active replicas to be overwhelmed before new ones start to boot up, which can cause unreliability or downtime. It’s also important to remember that due to the nature of pipeline parallel workflows when using a distributed GPU setup, there will always be a bubble, as discussed in section 3.3.2. As a result, your system will never reach 100% GPU utilization, and you will start to hit problems earlier than expected. Depending on how big your bubble is, you will need to adjust the target threshold accordingly.</p>
</div>
<div class="readable-text" id="p187">
<h4 class="readable-text-h4 sigil_not_in_toc">Minimum pod replicas</h4>
</div>
<div class="readable-text" id="p188">
<p>Minimum pod replicas determine the number of replicas of your service that will always be running. This setting is your baseline. It’s important to make sure it’s set slightly above your baseline of incoming requests. Too often, this is set strictly to meet baseline levels of traffic or just below, but a steady state for incoming traffic is rarely all that steady. This is where a lot of oscillating can happen, as you are more likely to see many small surges in traffic than large spikes. However, you don’t want to set it too high, as this will tie up valuable resources in the cluster and increase costs.</p>
</div>
<div class="readable-text" id="p189">
<h4 class="readable-text-h4 sigil_not_in_toc">Maximum pod replicas</h4>
</div>
<div class="readable-text" id="p190">
<p>Maximum pod replicas determine the number of replicas your system will run at peak capacity. You should set this number to be just above your peak traffic requirements. Setting it too low could lead to reliability problems, performance degradation, and downtime during high-traffic periods. Setting it too high could lead to resource waste, running more pods than necessary, and delaying the detection of real problems. For example, if your application was under a DDoS attack, your system might scale to handle the load, but it would likely cost you severely and hide the problem. With LLMs, you also need to be cautious not to overload the underlying cluster and make sure you have enough resources in your quotas to handle the peak load.</p>
</div>
<div class="readable-text" id="p191">
<h4 class="readable-text-h4 sigil_not_in_toc">Scaling policies</h4>
</div>
<div class="readable-text" id="p192">
<p>Scaling policies define the behavior of the autoscaler, allowing you to finetune how long to wait before scaling and how quickly it scales. This setting is usually ignored, and safely so for most setups because the defaults for these settings tend to be pretty good for the typical application. However, relying on the default would be a major mistake for an LLM service since it takes so long to deploy. </p>
</div>
<div class="readable-text intended-text" id="p193">
<p>The first setting you’ll want to adjust is the stabilization window, which determines how long to wait before taking a new scaling action. You can set a different stabilization window for upscaling and downscaling tasks. The default upscaling window is 0 seconds, which should not need to be touched if your target parameter has been set correctly. The default downscaling window is 300 seconds, which is likely too short for our use case. You’ll typically want this at least as long as it takes your service to deploy and then a little bit more. Otherwise, you’ll be adding replicas only to remove them before they have a chance to do anything.</p>
</div>
<div class="readable-text intended-text" id="p194">
<p>The next parameter you’ll want to adjust is the scale-down policy, which defaults to 100% of pods every 15 seconds. As a result, any temporary drop in traffic could result in all your extra pods above the minimum being terminated immediately. For our case, it’s much safer to slow this down since terminating a pod takes only a few seconds, but booting one up can take minutes, making it a semi-irreversible decision. The exact policy will depend on your traffic patterns, but in general, we want to have a little more patience. You can adjust how quickly pods will be terminated and the magnitude by the number or percentage of pods. For example, we could configure the policy to allow only one pod each minute or 10% of pods every 5 minutes to be terminated.</p>
</div>
<div class="readable-text" id="p195">
<h3 class="readable-text-h3" id="sigil_toc_id_113"><span class="num-string">6.2.3</span> Rolling updates</h3>
</div>
<div class="readable-text" id="p196">
<p>Rolling updates or rolling upgrades is a strategy that gradually implements the new version of an application to reduce downtime and maximize agility. It works by gradually creating new instances and turning off the old ones, replacing them in a methodical manner. This update approach allows the system to remain functional and accessible to users even during the update process, otherwise known as zero downtime. Rolling updates also make it easier to catch bugs before they have too much effect and roll back faulty deployments. </p>
</div>
<div class="readable-text intended-text" id="p197">
<p>Rolling updates is a feature built into k8s and another major reason for its widespread use and popularity. Kubernetes provides an automated and simplified way to carry out rolling updates. The rolling updates ensure that Kubernetes incrementally updates pod instances with new ones during deployment. The following listing shows an example LLM deployment implementing rolling updates; the relevant configuration is under the <code>spec.strategy</code> section.</p>
</div>
<div class="browsable-container listing-container" id="p198">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 6.9</span> Example deployment config with rolling update</h5>
<div class="code-area-container">
<pre class="code-area">apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-application
spec:
  replicas: 5
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 3
  selector:
    matchLabels:
      app: llm-app
  template:
    metadata:
      labels:
        app: llm-app
    spec:
      containers:
      - name: llm-gpu-app-container
        image: llm-gpu-application:v2
        resources:
          limits:
            nvidia.com/gpu: 8</pre>
</div>
</div>
<div class="readable-text" id="p199">
<p>You’ll notice that there are two main parameters you can adjust for a rolling update: <code>maxSurge</code> and <code>maxUnavailable</code>. These can either be set to a whole number, like in our example, describing the number of instances, or a fraction indicating a percentage of total instances. In the example, you’ll notice we set <code>maxSurge</code> to <code>1</code>, meaning even though we would normally run with five replicas, we could surge to six during a deployment, allowing us to turn on a new one before turning any off. Normally, you might want to set this higher, as it allows for a quicker rolling update. Otherwise, we’ll have to replace pods one at a time. The reason it’s low, you might have noticed, is that we are deploying a rather large LLM that requires eight GPUs. If these are A100s, it’s likely going to be hard to find an extra eight GPUs not being used.</p>
</div>
<div class="readable-text intended-text" id="p200">
<p>GPU resources cannot be shared among containers, and container orchestration can become a major challenge in such deployments, which is why <code>maxUnavailable</code> is set to <code>3</code>. What we are saying here is that three out of the five expected replicas can go down during a deployment. In other words, we are going to drop the total number of replicas for a little bit before re-creating them. For reliability reasons, we typically prefer adding extra replicas first, so to go down instead is a difficult decision, one you’ll want to confirm you can afford to do in your own deployment. The reason we are doing so here is to ensure that there are GPU resources available. In essence, to balance resource utilization, it might be necessary to set <code>maxUnavailable</code> to a high value and adjust <code>maxSurge</code> to a lower number to downscale old versions quickly and free up resources for new ones.</p>
</div>
<div class="readable-text intended-text" id="p201">
<p>This advice is the opposite of what you’d do in most applications, so we understand if it makes you uneasy. If you’d like to ensure smoother deployments, you’ll need to budget for extra GPUs to be provisioned in your cluster strictly for deployment purposes. However, depending on how often you are updating the model itself, paying for expensive GPUs to sit idle simply to make deployments smoother may not be cost-advantageous. Often, the LLM itself doesn’t receive that many updates, so assuming you are using an inference graph (discussed in the next section), most of the updates will be to the API, prompts, or surrounding application.</p>
</div>
<div class="readable-text intended-text" id="p202">
<p>In addition, we recommend you always perform such operations cautiously in a staging environment first to understand its effect. Catching a deployment problem in staging will save you a headache or two. It’s also useful to troubleshoot the <code>maxUnavailable</code> and <code>maxSurge</code> parameters in staging, but it’s often hard to get a one-to-one comparison to production since staging is often resource-constrained.</p>
</div>
<div class="readable-text" id="p203">
<h3 class="readable-text-h3" id="sigil_toc_id_114"><span class="num-string">6.2.4</span> Inference graphs</h3>
</div>
<div class="readable-text" id="p204">
<p>Inference graphs are the crème filling of a donut, the muffin top of a muffin, and the toppings on a pizza: they are just phenomenal. Inference graphs allow us to create sophisticated flow diagrams at inference in a resource-saving way. Consider figure 6.8, which shows us the building blocks for any inference graph.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p205">
<img alt="figure" height="459" src="../Images/6-8.png" width="677"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 6.8</span> The three types of inference graph building blocks. Sequential allows us to run one model before the other, which is useful for preprocessing steps like generating embeddings. Ensembles allow us to pool several models together to learn from each and combine their results. Routing allows us to send traffic to specific models based on some criteria, often used for multi-armed bandit optimization.</h5>
</div>
<div class="readable-text intended-text" id="p206">
<p>Generally, any time you have more than one model, it’s useful to consider an inference graph architecture. Your standard LLM setup is usually already at least two models: an encoder and the language model itself.</p>
</div>
<div class="readable-text intended-text" id="p207">
<p>Usually, when we see LLMs deployed in the wild, these two models are deployed together. You send text data to your system, and it returns generated text. It’s often no big deal, but when deployed as a sequential inference graph instead of a packaged service, we get some added bonuses. First, the encoder is usually much faster than the LLM, so we can split them up since you may only need one encoder instance for every two to three LLM instances. Encoders are so small that this doesn’t necessarily help us out that much, but it saves the hassle of redeploying the entire LLM if we decide to deploy a new encoder model version. In addition, an inference graph will set up an individual API for each model, which allows us to hit the LLM and encoder separately. This is really useful if we have a bunch of data we’d like to preprocess and save in a VectorDB; we can use the same encoder we already have deployed. We can then pull this data and send it directly into the LLM.</p>
</div>
<div class="readable-text intended-text" id="p208">
<p>The biggest benefit of an inference graph is that it allows us to separate the API and the LLM. The API sitting in front of the LLM is likely to change much more often as you tweak prompts, add features, and fix bugs. The ability to update the API without having to deploy the LLM will save your team a lot of effort.</p>
</div>
<div class="readable-text intended-text" id="p209">
<p>Let’s now consider figure 6.9, which provides an example inference graph deployment using Seldon. In this example, we have an encoder model, an LLM, a classifier model, and a simple API that combines the results. Whereas we would have to build a container and the interface for each of these models, Seldon creates an orchestrator that handles communication between a user’s request and each node in the graph.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p210">
<img alt="figure" height="524" src="../Images/6-9.png" width="767"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 6.9</span> An example inference graph deployment using Seldon. A Seldon Deployment is a Kubernetes CRD that extends a regular Kubernetes deployment and adds an orchestrator that ensures the proper communication between all the models are run in graph order.</h5>
</div>
<div class="readable-text print-book-callout" id="p211">
<p><span class="print-book-callout-head">NOTE</span>  Seldon is an open source platform designed for deploying and managing machine learning models in production. It offers tools and capabilities to help organizations streamline the deployment and scaling of machine learning and deep learning models in a Kubernetes-based environment. It offers k8s CRDs to implement inference graphs.</p>
</div>
<div class="readable-text" id="p212">
<p>If you are wondering how to create this, listing 6.10 shows an example configuration that would create this exact setup. We simply define the containers in the graph and their relationship inside the graph. You’ll notice <code>apiVersion</code> defines the CRD from Seldon, which allows us to use <code>SeldonDeployment</code>, which is just an extension of the k8s regular Deployment object. In the listing, you might notice that the combiner is the parent to the LLM and classifier models, which feels backwards from how we visualize it in figure 6.9. This is because a component will only ever have one parent, but can have multiple children, so a <code>COMBINER</code> is always a parent node even though functionally it’s the same. Setting up a graph can often be confusing, so I recommend you check the documentation frequently and often.</p>
</div>
<div class="browsable-container listing-container" id="p213">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 6.10</span> An example <code>SeldonDeployment</code> configuration file</h5>
<div class="code-area-container">
<pre class="code-area">apiVersion: machinelearning.seldon.io/v1alpha2
kind: SeldonDeployment
metadata:
  name: example-seldon-inference-graph
spec:
  name: example-deployment
  predictors:
  - componentSpecs:
    - spec:
        containers:
        - name: encoder
          image: encoder_image:latest
        - name: LLM
          image: llm_image:latest
        - name: classifier
          image: classifier_image:latest
        - name: combiner
          image: combiner_image:latest
    graph:
      name: encoder
      type: MODEL
      endpoint:
        type: REST
      children:
        - name: combiner
          type: COMBINER
          children:
            - name: LLM
              type: MODEL
              endpoint:
                type: REST
              children: []
            - name: classifier
              type: MODEL
              endpoint:
                type: REST
              children: []
    name: example
    replicas: 1</pre>
</div>
</div>
<div class="readable-text" id="p214">
<p>If you’ve deployed enough machine learning systems, you’ve realized that many of them require complex systems, and inference graphs make it easy, or at least easier. And that is a big difference. Although inference graphs are a smarter way to deploy complex machine learning systems, it’s always important to ask yourself if the extra complexity is actually needed. Even with tools like inference graphs, it’s better to keep things simple whenever possible.</p>
</div>
<div class="readable-text" id="p215">
<h3 class="readable-text-h3" id="sigil_toc_id_115"><span class="num-string">6.2.5</span> Monitoring</h3>
</div>
<div class="readable-text" id="p216">
<p>As with any product or service deployed into production, monitoring is critical to ensure reliability, performance, and compliance to service level agreements and objectives are met. As with any service, we care about monitoring typical performance metrics like queries per second (QPS), latency, and response code counts. We also care about monitoring our resources with metrics like CPU utilization, percentage of memory used, GPU utilization, and GPU temperature, among many more. When any of these metrics start to fail, it often indicates a catastrophic failure of some sort and will need to be addressed quickly. </p>
</div>
<div class="readable-text intended-text" id="p217">
<p>For these metrics, any software engineering team should have plenty of experience working with these using tools like Prometheus and Grafana or the ELK stack (Elasticsearch, Logstash, and Kibana). You will benefit immensely by taking advantage of the systems that are likely already in place. If they aren’t in place, we already went over how to set up the GPU metrics for monitoring back in section 6.2.2, and that system should be useful for monitoring other resources.</p>
</div>
<div class="readable-text intended-text" id="p218">
<p>However, with any ML project, we have additional concerns that traditional monitoring tools miss, which leads to silent failures. This usually comes from data drift and performance decay, where a model continues to function but starts to do so poorly and no longer meets quality expectations. LLMs are particularly susceptible to data drift since language is in constant flux, as new words are created and old words change meaning all the time. Thus, we often need both a system monitoring solution and an ML monitoring solution. </p>
</div>
<div class="readable-text intended-text" id="p219">
<p>Monitoring data drift is relatively easy and well-studied for numerical datasets, but monitoring unstructured text data provides an extra challenge. We’ve already discussed ways to evaluate language models in chapter 4, and we’ll need to use similar practices to evaluate and monitor models in production. One of our favorite tools for monitoring drift detection is whylogs due to its efficient nature of capturing summary statistics at scale. Adding LangKit to the mix instantly and easily allows us to track several useful metrics for LLMs, such as readability, complexity, toxicity, and even similarity scores to known prompt injection attacks. In the following listing, we demonstrate a simple application that logs and monitors text data using whylogs and LangKit.</p>
</div>
<div class="browsable-container listing-container" id="p220">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 6.11</span> Using whylogs and LangKit to monitor text data </h5>
<div class="code-area-container">
<pre class="code-area">import os
import pandas as pd

import whylogs as why
from langkit import llm_metrics
from datasets import load_dataset

OUTPUT_DIR = "logs"


class LoggingApp:
    def __init__(self):
        """
        Sets up a logger that collects profiles and writes them
        locally every 5 minutes. By setting the schema with langkit
        we get useful metrics for LLMs.
        """
        self.logger = why.logger(
            mode="rolling",
            interval=5,
            when="M",
            base_name="profile_",
            schema=llm_metrics.init(),
        )
        self.logger.append_writer("local", base_dir=OUTPUT_DIR)

    def close(self):
        self.logger.close()

    def consume(self, text):
        self.logger.log(text)


def driver(app):
    """Driver function to run the app manually"""
    data = load_dataset(
        "shahules786/OA-cornell-movies-dialog",
        split="train",
        streaming=True,
    )
    data = iter(data)
    for text in data:
        app.consume(text)


if __name__ == "__main__":
    app = LoggingApp()      <span class="aframe-location"/> #1
    driver(app)
    app.close()

    pd.set_option("display.max_columns", None)     <span class="aframe-location"/> #2

    all_files = [               <span class="aframe-location"/> #3
        f for f in os.listdir(OUTPUT_DIR) if f.startswith("profile_")
    ]
    path = os.path.join(OUTPUT_DIR, all_files[0])
    result_view = why.read(path).view()
    print(result_view.to_pandas().head())</pre>
<div class="code-annotations-overlay-container">
     #1 Runs app manually
     <br/>#2 Prevents truncation of columns
     <br/>#3 Gets the first profile and shows the results
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p221">
<p>The generated text is </p>
</div>
<div class="browsable-container listing-container" id="p222">
<div class="code-area-container">
<pre class="code-area"># ...
# column        udf/flesch_reading_ease:cardinality/est
# conversation                               425.514743
# ...
# column        udf/jailbreak_similarity:cardinality/est
# conversation                               1172.226702
# ...
# column        udf/toxicity:types/string  udf/toxicity:types/tensor
# conversation                          0                          0</pre>
</div>
</div>
<div class="readable-text" id="p223">
<p>While this is just a demo using a text dataset, you can see how it would be beneficial to monitor the incoming prompts and outgoing generated text for metrics such as readability, complexity, and toxicity. These monitoring tools will help give you a handle on whether or not your LLM service is starting to fail silently.</p>
</div>
<div class="readable-text intended-text" id="p224">
<p>When monitoring in production, we must be mindful of the effect latency may have on our service. LangKit uses several lightweight models to evaluate the text for the advanced metrics. While we haven’t noticed significant memory effects, there is a very slight effect on latency when evaluating logs in the direct inference path. To avoid this, we can take it out of the inference path and into what is called a sidecar.</p>
</div>
<div class="readable-text intended-text" id="p225">
<p>It’s not uncommon to see ML teams mistakenly place data quality checks in the critical path. Their intentions may be good (to ensure only clean data runs through a model), but on the off chance that a client sends bad data, it would often be better to just send a 400 or 500 error response than to add expensive latency costs to the good requests. In fact, many applications move monitoring out of the critical path entirely, opting to process it in parallel. The simplest way to do this is to use a Kubernetes sidecar, which is depicted in figure 6.10. You can do this with tools that specialize in this, like fluentd; whylogs also offers a container you can run as a sidecar. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p226">
<img alt="figure" height="344" src="../Images/6-10.png" width="489"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 6.10</span> An example Kubernetes sidecar container, which takes logging out of the critical path. The logging agent would be a tool like a whylogs container or fluentd that captures specific requests or all stdout print statements, processes them, and forwards them to a logging backend like WhyLabs or Prometheus.</h5>
</div>
<div class="readable-text" id="p227">
<p>There are different sidecar configurations, but the main gist is that a logging container will run in the same k8s pod, and instead of the main app writing to a logs file, this sidecar acts as an intermediate step, first processing and cleaning the data, which it can then send directly to a backend or write to a logs file itself. </p>
</div>
<div class="readable-text print-book-callout" id="p228">
<p><span class="print-book-callout-head">NOTE</span>  You can learn more about Kubernetes logging architectures in its docs here: <a href="https://mng.bz/Aaog">https://mng.bz/Aaog</a>.</p>
</div>
<div class="readable-text" id="p229">
<p>Now that we know more about setting up our infrastructure, including provisioning a cluster and implementing features like GPU autoscaling and monitoring, you should be set to deploy your LLM service and ensure it is reliable and scalable. Next, let’s talk about different challenges you are likely to face and methodologies to address these problems.</p>
</div>
<div class="readable-text" id="p230">
<h2 class="readable-text-h2" id="sigil_toc_id_116"><span class="num-string">6.3</span> Production challenges</h2>
</div>
<div class="readable-text" id="p231">
<p>While we’ve covered how to get a service up and running, nevertheless, you will find a never-ending host of hurdles you’ll need to jump over when it comes to deploying models and maintaining them in production. Some of these challenges include updating, planning for large loads, poor latency, acquiring resources, and more. To help, we wanted to address some of the most common problems and give you tips on how to handle them.</p>
</div>
<div class="readable-text" id="p232">
<h3 class="readable-text-h3" id="sigil_toc_id_117"><span class="num-string">6.3.1</span> Model updates and retraining</h3>
</div>
<div class="readable-text" id="p233">
<p>We recently discussed ML monitoring, watching your model for silent failures and data drift, but what do you do when you notice the model has gone belly up? We’ve seen in many traditional ML implementations that the answer is to simply retrain the model on the latest data and redeploy. And that works well when you are working with a small ARIMA model; in fact, we can often set up a CI/CD pipeline to run whenever our model degrades without any human oversight. But with a massive LLM? It doesn’t make any sense. </p>
</div>
<div class="readable-text intended-text" id="p234">
<p>Of course, we aren’t going to retrain from scratch, and we likely need to finetune our model, but the reason it doesn’t make sense is seen when we ask ourselves just what exactly the latest data is. The data we need to finetune the model is extremely important, and so it becomes necessary for us to take a step back and really diagnose the problem. What are the edge cases our model is failing on? What is it still doing well? How exactly have incoming prompts changed? Depending on the answers, we might not need to finetune at all. For example, consider a Q&amp;A bot that is no longer effective at answering current event questions as time goes on. We probably don’t want to retrain a model on a large corpus of the latest news articles. Instead, we would get much better results by ensuring our RAG system is up to date. Similarly, there are likely plenty of times that simply tweaking prompts will do the trick.</p>
</div>
<div class="readable-text intended-text" id="p235">
<p>In the cases where finetuning is the correct approach, you’ll need to think a lot about exactly what data you might be missing, as well as how any major updates might affect downstream systems, like finely tuned prompts. For example, when using knowledge distillation, this consideration can be particularly annoying. You will likely notice the problem in your student model but then must decide whether you need to retrain the student or the teacher. With any updates to the teacher model, you’ll need to ensure progress to the student model.</p>
</div>
<div class="readable-text intended-text" id="p236">
<p>Overall, it’s best to take a proactive approach to LLM model updates instead of a purely reactionary one. A system that often works well is to establish business practices and protocols to update the model on a periodic basis, say once a quarter or once a month. During the time between updates, the team will focus on monitoring cases where the model performs poorly and gather appropriate data and examples to make updating smooth. This type of practice will help you prevent silent failures and ensure your model isn’t just maintained but improving.</p>
</div>
<div class="readable-text" id="p237">
<h3 class="readable-text-h3" id="sigil_toc_id_118"><span class="num-string">6.3.2</span> Load testing</h3>
</div>
<div class="readable-text" id="p238">
<p>Load testing is a type of performance testing that assesses how well a service or system will perform under—wait for it—load. The primary goal of load testing is to ensure the system can handle the expected workload without performance degradation or failure. Doing it early can ensure we avoid bottlenecks and scalability problems. Since LLM services can be both expensive and resource intensive, it’s even more important to ensure you load test the system before releasing your LLM application to production or before an expected peak in traffic, like during a Black Friday sales event.</p>
</div>
<div class="readable-text intended-text" id="p239">
<p>Load testing an LLM service, for the most part, is like load testing any other service and follows these basic steps:</p>
</div>
<ol>
<li class="readable-text" id="p240"> Set up the service in a staging environment. </li>
<li class="readable-text" id="p241"> Run a script to periodically send requests to the service. </li>
<li class="readable-text" id="p242"> Increase requests until the service fails or autoscales. </li>
<li class="readable-text" id="p243"> Log metrics. </li>
<li class="readable-text" id="p244"> Analyze results. </li>
</ol>
<div class="readable-text" id="p245">
<p>Which metrics you log depends on your service and what you are testing. The main metrics to watch are latency and throughput at failure, as these can be used to extrapolate to determine how many replicas you’ll need to handle peak load. Latency is the total time it takes for a request to be completed, and throughput tells us the queries per second (QPS), both of which are extremely important metrics when analyzing our system. Still, since many LLM services offer streaming responses, they don’t help us understand the user experience. A few more metrics you’ll want to capture to understand your perceived responsiveness are time to first token (TTFT) and tokens per second (TPS). TTFT gives us the perceived latency; it tells us how long it takes until the user starts to receive feedback, while TPS tells us how fast the stream is. For English, you’ll want a TPS of about 11 tokens per second, which is a little faster than most people read. If it’s slower than this, your users might get bored as they wait for tokens to be returned.</p>
</div>
<div class="readable-text intended-text" id="p246">
<p>Related to TPS, I’ve seen several tools or reports use the inverse metric, time per output token (TPOT), or intertoken latency (ITL), but we’re not a fan of these metrics or their hard-to-remember names. You’ll also want to pay attention to resource metrics, CPU and GPU utilization, and memory usage. You’ll want to ensure these aren’t being hammered under base load conditions, as this can lead to hardware failures. These are also key to watch when you are testing autoscaling performance.</p>
</div>
<div class="readable-text intended-text" id="p247">
<p>One of my favorite tools for load testing is Locust. Locust is an open source load-testing tool that makes it easy to scale and distribute running load tests over multiple machines, allowing you to simulate millions of users. Locust does all the hard work for you and comes with many handy features, like a nice web user interface and the ability to run custom load shapes. It’s easy to run in Docker or Kubernetes, making it extremely accessible to run where you need it—in production. The only main downside we’ve run across is that it doesn’t support customizable metrics, so we’ll have to roll our own to add TTFT and TPS.</p>
</div>
<div class="readable-text intended-text" id="p248">
<p>To get started, simply <code>pip</code> <code>install</code> <code>locust</code>. Next, we’ll create our test. In listing 6.12, we show how to create a locust file that will allow users to prompt an LLM streaming service. It’s a bit more complicated than many locust files we’ve used simply because we need to capture our custom metrics for streaming, so you can imagine how straightforward they normally are. Locust already captures a robust set of metrics, so you won’t have to deal with this often. You’ll notice in the listing that we are saving these custom metrics to <code>stats.csv</code> file, but if you were running Locust in a distributed fashion, it’d be better to save it to a database of some sort.</p>
</div>
<div class="browsable-container listing-container" id="p249">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 6.12</span> Load testing with Locust</h5>
<div class="code-area-container">
<pre class="code-area">import time
from locust import HttpUser, task, events

stat_file = open("stats.csv", "w")     <span class="aframe-location"/> #1
stat_file.write("Latency,TTFT,TPS\n")


class StreamUser(HttpUser):
    @task
    def generate(self):
        token_count = 0          <span class="aframe-location"/> #2
        start = time.time()

        with self.client.post(      <span class="aframe-location"/> #3
            "/generate",
            data='{"prompt": "Salt Lake City is a"}',
            catch_response=True,
            stream=True,
        ) as response:
            first_response = time.time()
            for line in response.iter_lines(decode_unicode=True):
                token_count += 1

        end = time.time()         <span class="aframe-location"/> #4
        latency = end - start
        ttft = first_response - start
        tps = token_count / (end - first_response)

        stat_file.write(f"{latency},{ttft},{tps}\n")    <span class="aframe-location"/> #5


# Close stats file when Locust quits
@events.quitting.add_listener
def close_stats_file(environment):
    stat_file.close()</pre>
<div class="code-annotations-overlay-container">
     #1 Creates a CSV file to store custom stats
     <br/>#2 Initiates the test
     <br/>#3 Makes request
     <br/>#4 Finishes and calculates the stats
     <br/>#5 Saves stats
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p250">
<p>Before you run it, you’ll need to have an LLM service up. For this example, we’ll run the code from listing 6.3 in section 6.1.6, which spins up a very simple LLM service. With a service up and our test defined, we need to run it. To spin up the Locust service, run the <code>locust</code> command. You should then be able to navigate to the web UI in your browser. See the following example:</p>
</div>
<div class="browsable-container listing-container" id="p251">
<div class="code-area-container code-area-with-html">
<pre class="code-area">$ locust -f locustfile.py
&gt; locust.main: Starting web interface at http://0.0.0.0:8089 (accepting
<span class="">↪</span> connections from all network interfaces)
&gt; locust.main: Starting Locust 2.17.0</pre>
</div>
</div>
<div class="readable-text" id="p252">
<p>Once in the web UI, you can explore running different tests; you’ll just need to point Locust at the host where your LLM service is running, which for us should be running on localhost on port 8000 or for the full socket address we combined them for: http://0.0.0.0:8000. In figure 6.11, you can see an example test where we increased the active users to 50 at a spawn rate of 1 per second. You can see that on the hardware, this simple service starts to hit a bottleneck <span class="aframe-location"/>at around 34 users, where the QPS starts to decrease, as it’s no longer able to keep up with the load. You’ll also notice response times slowly creep up in response to heavier load. We could continue to push the number of users up until we started to see failures, but overall, this test was informative and a great first test drive.</p>
</div>
<div class="browsable-container figure-container" id="p253">
<img alt="figure" height="1058" src="../Images/6-11.png" width="1100"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 6.11</span> Locust test interface demoing an example run increasing the number of users to 50 at a spawn rate of 1 per second. The requests per second peaks at 34 users, indicating a bottleneck for our service.</h5>
</div>
<div class="readable-text intended-text" id="p254">
<p>In addition to manually running load tests, we can run Locust in a headless mode for automated tests. The following code is a simple command to run the exact same test as seen in figure 6.11; however, since we won’t be around to see the report, we’ll save the data to CSV files labeled with the prefix <code>llm</code> to be processed and analyzed later. There will be four files in addition to the stats CSV file we were already generating:</p>
</div>
<div class="browsable-container listing-container" id="p255">
<div class="code-area-container code-area-with-html">
<pre class="code-area">$ locust -f locustfile.py --host http://0.0.0.0:8000 --csv=llm --
<span class="">↪</span> headless -u 50 -r 1 -t 10m</pre>
</div>
</div>
<div class="readable-text" id="p256">
<p>Now that you are able to load test your LLM service, you should be able to figure out how many replicas you’ll need to meet throughput requirements. It’s just a matter of spinning up more services. But what do you do when you find out your service doesn’t meet latency requirements? Well, that’s a bit tougher, so let’s discuss it in the next section.</p>
</div>
<div class="readable-text" id="p257">
<h3 class="readable-text-h3" id="sigil_toc_id_119"><span class="num-string">6.3.3</span> Troubleshooting poor latency</h3>
</div>
<div class="readable-text" id="p258">
<p>One of the biggest bottlenecks when it comes to your model’s performance in terms of latency and throughput has nothing to do with the model itself but comes from data transmission of the network. One of the simplest methods to improve this I/O constraint is to serialize the data before sending it across the wire, which can have a large effect on ML workloads where the payloads tend to be larger, including LLMs where prompts tend to be long.</p>
</div>
<div class="readable-text intended-text" id="p259">
<p>To serialize the data, we utilize a framework known as Google Remote Procedure Call (gRPC). gRPC is an API protocol similar to REST, but instead of sending JSON objects, we compress the payloads into a binary serialized format using Protocol Buffers, also known as protobufs. By doing this, we can send more information in fewer bytes, which can easily give us orders of magnitude improvements in latency. Luckily, most inference services will implement gRPC along with their REST counterparts right out of the box, which is extremely convenient since the major hurdle to using gRPC is setting it up. </p>
</div>
<div class="readable-text intended-text" id="p260">
<p>A major reason for this convenience is the Seldon V2 Inference Protocol, which is widely implemented. The only hurdle, then, is ensuring our client can serialize and deserialize messages to take advantage of the protocol. In listing 6.13, we show an example client using MLServer to do this. It’s a little bit more in depth than your typical <code>curl</code> request, but a closer inspection shows the majority of the complexity is simply converting the data from different types as we serialize and deserialize it.</p>
</div>
<div class="browsable-container listing-container" id="p261">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 6.13</span> Example client using gRPC </h5>
<div class="code-area-container">
<pre class="code-area">import json
import grpc
from mlserver.codecs.string import StringRequestCodec
import mlserver.grpc.converters as converters
import mlserver.grpc.dataplane_pb2_grpc as dataplane
import mlserver.types as types

model_name = "grpc_model"
inputs = {"message": "I'm using gRPC!"}

inputs_bytes = json.dumps(inputs).encode("UTF-8")    <span class="aframe-location"/> #1
inference_request = types.InferenceRequest(
    inputs=[
        types.RequestInput(
            name="request",
            shape=[len(inputs_bytes)],
            datatype="BYTES",
            data=[inputs_bytes],
            parameters=types.Parameters(content_type="str"),
        )
    ]
)

serialized_request = converters.ModelInferRequestConverter.from_types(  <span class="aframe-location"/> #2
    inference_request, model_name=model_name, model_version=None
)

grpc_channel = grpc.insecure_channel("localhost:8081")     <span class="aframe-location"/> #3
grpc_stub = dataplane.GRPCInferenceServiceStub(grpc_channel)
response = grpc_stub.ModelInfer(serialized_request)
print(response)

deserialized_response = converters.ModelInferResponseConverter.to_types(   <span class="aframe-location"/> #4
    response
)
json_text = StringRequestCodec.decode_response(deserialized_response)
output = json.loads(json_text[0])
print(output)</pre>
<div class="code-annotations-overlay-container">
     #1 Sets up the request structure via V2 Inference Protocol
     <br/>#2 Serializes the request to the Protocol Buffer
     <br/>#3 Connects to the gRPC server
     <br/>#4 Δeserializes the response and converts to the Python dictionary
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p262">
<p>If you don’t use an inference service but want to implement a gRPC API, you’ll have to put down familiar tooling like FastAPI, which is strictly REST. Instead, you’ll likely want to use the grpcio library to create your API, and you’ll have to become familiar with .proto files to create your protobufs. It can be a relatively steep learning curve and beyond the scope of this book, but the advantages are well worth it.</p>
</div>
<div class="readable-text intended-text" id="p263">
<p>There are also plenty of other ideas to try if you are looking to squeeze out every last drop of performance. Another way to improve latency that shouldn’t be overlooked is ensuring you compile your model. We hammered this point pretty heavily at the beginning of this chapter, but it’s important to bring it up again. Next, be sure to deploy the model in a region or data center close to your users; this point is obvious to most software engineers, but for LLMs, we have to be somewhat wary, as the data center of choice may not have your accelerator of choice. Most cloud providers will be willing to help you with this, but it’s not always a quick and easy solution for them to install the hardware in a new location. Note that if you have to switch to a different accelerator to move regions, you’ll have to remember to compile your model all over again for the new hardware architecture! On that note, consider scaling up your accelerator. If you are currently opting for more price-effective GPUs but latency is becoming a bottleneck, paying for the latest and greatest can often speed up inference times. </p>
</div>
<div class="readable-text intended-text" id="p264">
<p>In addition, caching is always worth considering. It’s not likely, but on the off chance your users are often sending the same requests and the inputs can be easily normalized, you should implement caching. The fastest LLM is one we don’t actually run, so there’s no reason to run the LLM if you don’t have to. Also, we just went over this, but always be sure to load test and profile your service, making note of any bottlenecks, and optimize your code. Sometimes we make mistakes, and if the slowest process in the pipeline isn’t the actual LLM running inference, something is wrong. Last but not least, consider using a smaller model or an ensemble of them. It’s always been a tradeoff in ML deployments, but often sacrificing a bit of quality in the model or the accuracy of the results is acceptable to improve the overall reliability and speed of the service.</p>
</div>
<div class="readable-text" id="p265">
<h3 class="readable-text-h3" id="sigil_toc_id_120"><span class="num-string">6.3.4</span> Resource management</h3>
</div>
<div class="readable-text" id="p266">
<p>You’ve heard us say it a lot throughout the book, but we are currently in a GPU shortage, which has been true for almost the last 10 years, so we’re confident that when you read this sometime in the future, it will likely still be true. The truth is that the world can’t seem to get enough high-performance computing, and LLMs and generative AI are only the latest in a long list of applications that have driven up demand in recent years. It seems that once we seem to get a handle on supply, there’s another new reason for consumers and companies to want to use them.</p>
</div>
<div class="readable-text intended-text" id="p267">
<p>With this in mind, it’s best to consider strategies to manage these resources. One tool we’ve quickly become a big fan of is SkyPilot (<a href="https://github.com/skypilot-org/skypilot">https://github.com/skypilot-org/skypilot</a>). SkyPilot is an open source project that aims to abstract away cloud infra burdens—in particular, maximizing GPU availability for your jobs. You use it by defining a task you want to run and then running the <code>sky</code> CLI command; it will search across multiple cloud providers, clusters, regions, and zones, depending on how you have it configured, until it finds an instance that meets your resource requirements and starts the job. Some common tasks are built-in, such as provisioning a GPU-backed Jupyter notebook.</p>
</div>
<div class="readable-text intended-text" id="p268">
<p>If you recall, in chapter 5, we showed you how to set up a virtual machine (VM) to run multi-GPU environments with gcloud. Using SkyPilot, that gets simplified to one command:</p>
</div>
<div class="browsable-container listing-container" id="p269">
<div class="code-area-container">
<pre class="code-area">$ sky gpunode -p 8888 -c jupyter-vm --gpus l4:2 --cloud gcp --region us-west1</pre>
</div>
</div>
<div class="readable-text" id="p270">
<p>In addition to provisioning the VM, it also sets up port forwarding, which allows us to run Jupyter Notebook and access it through your browser. Pretty nifty!</p>
</div>
<div class="readable-text intended-text" id="p271">
<p>Another project to be on the watch for is Run:ai. Run:ai is a small startup that was aquired by NVIDIA for no small sum. It offers GPU optimization tooling, such as over quota provisioning, GPU oversubscription, and fractional GPU capabilities. It also helps you manage your clusters to increase GPU availability with GPU pooling, dynamic resource sharing, job scheduling, and more. What does all that mean? We’re not exactly sure, but their marketing team definitely sold us. Jokes aside, they offer a smarter way to manage your accelerators, and it’s very welcome. We expect we’ll see more competitors in this space in the future.</p>
</div>
<div class="readable-text" id="p272">
<h3 class="readable-text-h3" id="sigil_toc_id_121"><span class="num-string">6.3.5</span> Cost engineering</h3>
</div>
<div class="readable-text" id="p273">
<p>When it comes to getting the most bang for your buck with LLMs, there’s lots to consider. In general, regardless of whether you deploy your own or pay for one in an API, you’ll be paying for the number of output tokens. For most paid services, this is a direct cost, but for your own service, it is often paid through longer inference times and extra compute time. In fact, it’s been suggested that simply adding “be concise” to your prompt can save you up to 90% of your costs.</p>
</div>
<div class="readable-text intended-text" id="p274">
<p>You’ll also save a lot by using text embeddings. We introduced RAG earlier, but what’s lost on many is that you don’t have to take the semantic search results and add them to your prompt to have your LLM “clean it up.” You could return the semantic search results directly to your user. It is much cheaper to look something up in a vector store than to ask an LLM to generate it. Simple neural information retrieval systems will save you significant amounts when doing simple fact lookups like, “Who’s the CEO of Twitter?” Self-hosting these embeddings should also significantly cut down the costs even further. If your users are constantly asking the same types of questions, consider taking the results of your LLM to these questions and storing them in your vector store for faster and cheaper responses.</p>
</div>
<div class="readable-text intended-text" id="p275">
<p>You also need to consider which model you should use for which task. Generally, bigger models are better at a wider variety of tasks, but if a smaller model is good enough for a specific job, you’ll save a lot by using it. For example, if we just assumed the price was linear to the number of parameters, you could run 10 Llama-2-7b models for the same cost as 1 Llama-2-70b. We realize the cost calculations are more complicated than that, but it’s worth investigating.</p>
</div>
<div class="readable-text intended-text" id="p276">
<p>When comparing different LLM architectures, it’s not always just about size. Often, you’ll want to consider whether the architecture is supported for different quantization and compiling strategies. New architectures often boast impressive results on benchmarking leaderboards but lag behind when it comes to compiling and preparing them for production.</p>
</div>
<div class="readable-text intended-text" id="p277">
<p>Next, you’ll need to consider the costs of GPUs to use when running. In general, you’ll want to use the least amount of GPUs needed to fit the model into memory to reduce the cost of idling caused by bubbles, as discussed in section 3.3.2. Determining the correct number of GPUs isn’t always intuitive. For example, it’s cheaper to run four T4s than to run one A100, so it might be tempting to split up a large model onto smaller devices, but the inefficiency will often catch up to you. We have found that paying for newer, more expensive GPUs often saves us in the long run, as these GPUs tend to be more efficient and get the job done faster. This is particularly true when running batch inference. Ultimately, you’ll want to test different GPUs and find what configuration is cost optimal, as it will be different for every application.</p>
</div>
<div class="readable-text intended-text" id="p278">
<p>There are a lot of moving parts: model, service, machine instance, cloud provider, prompt, etc. While we’ve been trying to help you understand the best rules of thumb, you’ll want to test it out, which is where the cost engineering really comes into play. The simple way to test your cost efficiency is to create a matrix of your top choices; then, spin up a service for each combination and run your load testing. When you have an idea of how each instance runs under load and how much that particular instance will cost to run, you can then translate metrics like TPS to dollars per token (DTP). You’ll likely find that the most performant solution is rarely the most cost-optimal solution, but it gives you another metric to make a decision that’s best for you and your company.</p>
</div>
<div class="readable-text" id="p279">
<h3 class="readable-text-h3" id="sigil_toc_id_122"><span class="num-string">6.3.6</span> Security</h3>
</div>
<div class="readable-text" id="p280">
<p>Security is always an undercurrent and a consideration when working in production environments. All the regular protocols and standard procedures should be considered when working with LLMs that you would consider for a regular app, like in-transit encryption with a protocol like HTTPS, authorization and authentication, activity monitoring and logging, network security, firewalls, and the list goes on—all of which could, and have, taken up articles, blog posts, and books of their own. When it comes to LLMs, you should worry about two big failure cases: an attacker gets an LLM agent to execute nefarious code, or an attacker gains access to proprietary data like passwords or secrets the LLM was trained on or has access to.</p>
</div>
<div class="readable-text intended-text" id="p281">
<p>For the first concern, the best solution is to ensure the LLM is appropriately sandboxed for the use case for which it is employed. We are only worried about this attack when the LLM is used as an agent. In these cases, we often want to give an LLM a few more skills by adding tooling or plugins. For example, if you use an LLM to write your emails, why not just let it send the response too? A common case is letting the LLM browse the internet as an easy way to gather the latest news and find up-to-date information to generate better responses. These are all great options, but you should be aware that they allow the model to make executions. The ability to make executions is concerning because in the email example, without appropriate isolation and containment, a bad actor could send your LLM an email with a prompt injection attack that informs it to write malware and send it to all your other contacts.</p>
</div>
<div class="readable-text intended-text" id="p282">
<p>This point brings us to probably the biggest security threat to using LLMs: prompt injection. We talked about it in chapter 3, but as a refresher, a malicious user designs a prompt to allow them to perform unauthorized actions. We want to prevent users from gaining access to our company’s secret Coca-Cola recipe or whatever other sensitive data our LLM has been trained on or has access to.</p>
</div>
<div class="readable-text intended-text" id="p283">
<p>Some standard best practices have come along to help combat this threat. The first is context-aware filtering, whether using keyword search or a second LLM to validate prompts. The idea is to validate the input prompt to see whether it’s asking for something it should not and/or the output prompt to see whether anything is being leaked that you don’t want to be leaked. However, a clever attacker will always be able to get around this defense, so you’ll want to include some form of monitoring to catch prompt injection and regularly update your LLM models. If trained appropriately, your model will inherently respond correctly, denying prompt injections. You’ve likely seen GPT-4 respond by saying, “Sorry, but I can’t assist with that,” which is a hallmark of good training. In addition, you’ll want to enforce sanitization and validation on any incoming text to your model. </p>
</div>
<div class="readable-text intended-text" id="p284">
<p>You should also consider language detection validation. Often, filtering systems and other precautions are only applied or trained in English, so a user who speaks a different language is often able to bypass these safeguards. The easiest way to stop this type of attack is to deny prompts that aren’t English or another supported language. If you take this approach, though, realize you’re greatly sacrificing usability and security costs, and safeguards have to be built for each language you intend to support. Also, you should know that most language detection algorithms typically identify only one language, so attackers often easily bypass these checks by simply writing a prompt with multiple languages. Alternatively, to filter out prompts in nonsupported languages, you can flag them for closer monitoring, which will likely help you find bad actors.</p>
</div>
<div class="readable-text intended-text" id="p285">
<p>These safeguards will greatly increase your security, but prompt injection can get quite sophisticated through adversarial attacks. Adversarial attacks are assaults on ML systems that take advantage of how they work, exploiting neural network architectures and black-box pattern matching. For example, random noise can be added to an image in such a way that the image appears the same to human eyes, but the pixel weights have been changed enough to fool an ML model to misclassify them. And it often doesn’t take much data. One author remembers being completely surprised after reading one study that showed attackers hacked models by only changing one pixel in an image!<a href="#footnote-197"><sup class="footnote-reference" id="footnote-source-1">1</sup></a> Imagine changing one pixel, and suddenly, the model thinks the frog is a horse. LLMs are, of course, also susceptible. Sightly change a prompt, and you’ll get completely different results. </p>
</div>
<div class="readable-text intended-text" id="p286">
<p>The easiest way to set up an adversarial attack is to set up a script to send lots of different prompts and collect the responses. With enough data, an attacker can then train their own model on the dataset to effectively predict the right type of prompt to get the output they are looking for. Essentially, it just reverse engineers the model.</p>
</div>
<div class="readable-text intended-text" id="p287">
<p>Another strategy to implement adversarial attacks is data poisoning. Here, an attacker adds malicious data to the training dataset that will alter how it performs. Data poisoning is so effective that tools like Nightshade help artists protect their art from being used in training datasets. With as few as 50 to 300 poisoned images, models like Midjourney or Stable Diffusions will start creating cat images when a user asks for a dog or cow images when asked to generate a car.<a href="#footnote-198"><sup class="footnote-reference" id="footnote-source-2">2</sup></a> Applied to LLMs, imagine a poisoned dataset that trains the model to ignore security protocols if a given code word or hash is in the prompt. This particular attack vector is effective on LLMs since they are often trained on large datasets that are not properly vetted or cleaned.</p>
</div>
<div class="readable-text intended-text" id="p288">
<p>Full disclosure: attackers don’t need sophisticated techniques to get prompt injection to work. Ultimately, an LLM is just a bot, so it doesn’t understand how or why it should keep secrets. We haven’t solved the prompt injection problem; we have only made it harder to do. For example, the authors have enjoyed playing games like <em>Gandalf</em> from Lakera.ai. In this game, you slowly go through seven to eight levels where more and more security measures are used to prevent you from stealing a password via prompt injection. While they do get progressively harder, needless to say, we’ve beaten all the levels. If there’s one thing we hope you take from this section, it’s that you should assume any data given to the model could be extracted. So if you decide to train a model on sensitive data or give it access to a VectorDB with sensitive data, you should plan on securing that model the same way you would the data—for example, keeping it for internal use and using least privilege best practices.</p>
</div>
<div class="readable-text intended-text" id="p289">
<p>We’ve just talked a lot about different production challenges, from updates and performance tuning to costs and security, but one production challenge deserves its own section: deploying LLMs to the edge. We’ll undertake a project in chapter 10 to show you how to do just that, but let’s take a moment to discuss it beforehand.</p>
</div>
<div class="readable-text" id="p290">
<h2 class="readable-text-h2" id="sigil_toc_id_123"><span class="num-string">6.4</span> Deploying to the edge</h2>
</div>
<div class="readable-text" id="p291">
<p>To be clear, you should not consider training anything on edge right now. You can, however, do ML development and inference on edge devices. The keys to edge development with LLMs are twofold: memory and speed. That should feel very obvious because they’re the same keys as running them normally. But what do you do when you have only 8 GB of RAM and no GPU, and you still need to have &gt;1 token per second? As you can probably guess, there isn’t a uniformly good answer, but let’s discuss some good starting points.</p>
</div>
<div class="readable-text intended-text" id="p292">
<p>The biggest Raspberry Pi (rpi) on the market currently has 8 GB of RAM, no GPU, subpar CPU, and just a single board. This setup isn’t going to cut it. However, an easy solution exists to power your rpi with an accelerator for LLMs and other large ML projects: USB-TPUs like Coral. Keep in mind the hardware limitations of devices that use USB 3.0 being around 600MB/s, so it’s not going to be the same as inferencing on an A100 or better, but it’s going to be a huge boost in performance for your rpi using straight RAM for inference.</p>
</div>
<div class="readable-text intended-text" id="p293">
<p>If you plan on using a Coral USB accelerator, or any TPU, for that matter, keep in mind that because TPUs are a Google thing, you’ll need to convert both your model file and your inferencing code to use the TensorFlow framework. Earlier in the chapter, we discussed using Optimum to convert Hugging Face models to ONNX, and you can use this same library to convert our models to a .tflite, which is a compiled TensorFlow model format. This format will perform well on edge devices even without a TPU and twofold with TPU acceleration. </p>
</div>
<div class="readable-text intended-text" id="p294">
<p>Alternatively, if buying both a single board and an accelerator seems like a hassle—because we all know the reason you bought a single board was to avoid buying two things to begin with—there are single boards that come with an accelerator. NVIDIA, for example, has its own single board with a GPU and CUDA called Jetson. With a Jetson or Jetson-like computer that uses CUDA, we don’t have to use TensorFlow, so that’s a major plus. ExecuTorch is the PyTorch offering for inferencing on edge devices.</p>
</div>
<div class="readable-text intended-text" id="p295">
<p>Another edge device worth considering is that one in your pocket—that’s right, your phone. Starting with the iPhone X, the A11 chip came with the Apple Neural Engine accelerator. For Android, Google started offering an accelerator in their Pixel 6 phone with the Tensor chipset. Developing an iOS or Android app will be very different from working with a single board that largely runs versions of Linux; we won’t discuss it in this book, but it’s worth considering.</p>
</div>
<div class="readable-text intended-text" id="p296">
<p>Outside of hardware, several libraries and frameworks are also very cool and fast and make edge development easier. Llama.cpp, for example, is a C++ framework that allows you to take (almost) any Hugging Face model and convert it to the GGUF format. The GGUF format, created by the llama.cpp team, stores the model in a quantized fashion that makes it readily available to run on a CPU; it offers fast loading and inference on any device. Popular models like Llama, Mistral, and Falcon and even nontext models like Whisper are supported by llama.cpp at this point. It also supports LangChain integration for everyone using any of the LangChain ecosystem. Other libraries like GPTQ are focused more on performance than accessibility and are slightly harder to use, but they can result in boosts where it counts, especially if you’d like to end up inferencing on an Android phone or something similar. We will be exploring some of these libraries in much more detail later in the book.</p>
</div>
<div class="readable-text intended-text" id="p297">
<p>We’ve gone over a lot in this chapter, and we hope you feel more confident in tackling deploying your very own LLM service. In the next chapter, we will discuss how to take better advantage of your service by building an application around it. We’ll dive deep into prompt engineering, agents, and frontend tooling.</p>
</div>
<div class="readable-text" id="p298">
<h2 class="readable-text-h2" id="sigil_toc_id_124">Summary</h2>
</div>
<ul>
<li class="readable-text" id="p299"> Always compile your LLMs before putting them into production, as it improves efficiency, resource utilization, and cost savings. </li>
<li class="readable-text" id="p300"> LLM APIs should implement batching, rate limiters, access keys, and streaming. </li>
<li class="readable-text" id="p301"> Retrieval-augmented generation is a simple and effective way to give your LLM context when generating content because it is easy to create and use. </li>
<li class="readable-text" id="p302"> LLM inference service libraries like vLLM, Hugging Face’s TGI, or OpenLLM make deploying easy but may not have the features you are looking for since they are so new. </li>
<li class="readable-text buletless-item" id="p303"> Kubernetes is a tool that simplifies infrastructure by providing tooling like autoscaling and rolling updates: 
    <ul>
<li> Autoscaling is essential to improve reliability and cut costs by increasing or decreasing replicas based on utilization. </li>
<li> Rolling updates gradually implement updates to reduce downtime and maximize agility. </li>
</ul></li>
<li class="readable-text" id="p304"> Kubernetes doesn’t support GPU metrics out of the box, but by utilizing tools like DCGM, Prometheus, and KEDA, you can resolve this problem. </li>
<li class="readable-text" id="p305"> Seldon is a tool that improves deploying ML models and can be used to implement inference graphs. </li>
<li class="readable-text buletless-item" id="p306"> LLMs introduce some production challenges: 
    <ul>
<li> When your model drifts, first look to your prompts and RAG systems before attempting finetuning again. </li>
<li> Poor latency is difficult to resolve, but tools to help include gRPC, GPU optimization, and caching. </li>
<li> Resource management and acquiring GPUs can be difficult, but tools like SkyPilot can help. </li>
</ul></li>
<li class="readable-text" id="p307"> Edge development, while hardware limited, is the new frontier of LLM serving, and hardware like the Jetson or Coral TPU is available to help. </li>
</ul>
<div class="readable-text footnote-readable-text" id="p308">
<p><a href="#footnote-source-1"><span class="footnote-definition" id="footnote-197">[1]</span></a> J. Su, D. V. Vargas, and K. Sakurai, “One pixel attack for fooling deep neural networks,” IEEE Transactions on Evolutionary Computation, 2019;23(5):828–841, <a href="https://doi.org/10.1109/tevc.2019.2890858">https://doi.org/10.1109/tevc.2019.2890858</a>.</p>
</div>
<div class="readable-text footnote-readable-text" id="p309">
<p><a href="#footnote-source-2"><span class="footnote-definition" id="footnote-198">[2]</span></a> M. Heikkilä. “This new data poisoning tool lets artists fight back against generative AI,” MIT Technology Review, October 23, 2023, <a href="https://mng.bz/RNxD">https://mng.bz/RNxD</a>.</p>
</div>
</div></body></html>