["```py\nfrom datasets import load_dataset\n\nfashion_mnist = load_dataset(\"fashion_mnist\")\n```", "```py\nclothes = fashion_mnist[\"train\"][\"image\"][:8]\nclasses = fashion_mnist[\"train\"][\"label\"][:8]\nshow_images(clothes, titles=classes, figsize=(4,2.5))\n```", "```py\npreprocess = transforms.Compose([\n    transforms.RandomHorizontalFlip(),   # Randomly flip (data augmentation)\n    transforms.ToTensor(),               # Convert to tensor (0, 1)\n    transforms.Pad(2),                   # Add 2 pixels on all sides\n    transforms.Normalize([0.5], [0.5]),  # Map to (-1, 1)\n])\n```", "```py\nbatch_size = 256\n\ndef transform(examples):\n    images = [preprocess(image.convert(\"L\")) for image in examples[\"image\"]]\n    return {\"images\": images, \"labels\": examples[\"label\"]}\n\ntrain_dataset = fashion_mnist[\"train\"].with_transform(transform)\n\ntrain_dataloader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True\n)\n```", "```py\nmodel = UNet2DModel(\n    in_channels=1,   # 1 channel for grayscale images\n    out_channels=1,  # output channels must also be 1\n    sample_size=32,\n    block_out_channels=(32, 64, 128, 256),\n    norm_num_groups=8,\n    num_class_embeds=10, # Enable class conditioning\n)\n```", "```py\nx = torch.randn((1, 1, 32, 32))\nwith torch.no_grad():\n    out = model(x, timestep=7, class_labels=torch.tensor([2])).sample\nout.shape\n```", "```py\ntorch.Size([1, 1, 32, 32])\n```", "```py\nscheduler = DDPMScheduler(num_train_timesteps=1000, beta_start=0.0001, beta_end=0.02)\ntimesteps = torch.linspace(0, 999, 8).long()\nbatch = next(iter(train_dataloader))\nx = batch['images'][:8]\nnoise = torch.rand_like(x)\nnoised_x = scheduler.add_noise(x, noise, timesteps)\nshow_images((noised_x*0.5 + 0.5).clip(0, 1))\n```", "```py\nfor step, batch in enumerate(train_dataloader):\n        # Load the input images\n        clean_images = batch[\"images\"].to(device)\n        class_labels = batch[\"labels\"].to(device)\n\n        # *Sample noise to add to the images*\n        # *Sample a random timestep for each image*\n        # *Add noise to the clean images according to the timestep*\n\n        # Get the model prediction for the noise - note the use of class_labels\n        noise_pred = model(noisy_images, timesteps, class_labels=class_labels, return_dict=False)[0]\n\n        # *Calculate the loss and update the parameters as before*\n        ...\n```", "```py\ndef generate_from_class(class_to_generate, n_samples=8):\n    sample = torch.randn(n_samples, 1, 32, 32).to(device)\n    class_labels = [class_to_generate] * n_samples\n    class_labels = torch.tensor(class_labels).to(device)\n\n    for i, t in tqdm(enumerate(scheduler.timesteps)):\n        # Get model pred\n        with torch.no_grad():\n            noise_pred = model(sample, t, class_labels=class_labels).sample\n\n        # Update sample with step\n        sample = scheduler.step(noise_pred, t, sample).prev_sample\n\n    return sample.clip(-1, 1)*0.5 + 0.5\n```", "```py\n# Generate t-shirts (class 0)\nimages = generate_from_class(0)\nshow_images(images, nrows=2)\n```", "```py\n1000it [00:21, 47.25it/s]\n```", "```py\n# Now generate some sneakers (class 7)\nimages = generate_from_class(7)\nshow_images(images, nrows=2)\n```", "```py\n1000it [00:21, 47.20it/s]\n```", "```py\n# ...or boots (class 9)\nimages = generate_from_class(9)\nshow_images(images, nrows=2)\n```", "```py\n1000it [00:21, 47.26it/s]\n```", "```py\npipe(\"Watercolor illustration of a rose\").images[0]\n```", "```py\n  0%|          | 0/50 [00:00<?, ?it/s]\n```", "```py\nprompt = 'A photograph of a puppy'\n```", "```py\n# Turn the text into a sequence of tokens:\ntext_input = pipe.tokenizer(prompt, padding=\"max_length\",\n                            max_length=pipe.tokenizer.model_max_length,\n                            truncation=True, return_tensors=\"pt\")\n\n# See the individual tokens\nfor t in text_input['input_ids'][0][:8]: # We'll just look at the first 7\n    print(t, pipe.tokenizer.decoder.get(int(t)))\n```", "```py\ntensor(49406) <|startoftext|>\ntensor(320) a</w>\ntensor(8853) photograph</w>\ntensor(539) of</w>\ntensor(320) a</w>\ntensor(6829) puppy</w>\ntensor(49407) <|endoftext|>\ntensor(49407) <|endoftext|>\n```", "```py\n# Grab the output embeddings\ntext_embeddings = pipe.text_encoder(text_input.input_ids.to(device))[0]\nprint('Text embeddings shape:', text_embeddings.shape)\n```", "```py\nText embeddings shape: torch.Size([1, 77, 768])\n```", "```py\n# NB, this will be our own image as part of the supplementary material to avoid external URLs\nim = load_image('https://images.pexels.com/photos/14588602/pexels-photo-14588602.jpeg', size=(512, 512))\nshow_image(im);\n```", "```py\n# Encode the image\nwith torch.no_grad():\n    tensor_im = transforms.ToTensor()(im).unsqueeze(0).to(device)*2-1\n    latent = vae.encode(tensor_im.half()) # Encode the image to a distribution\n    latents = latent.latent_dist.sample() # Sampling from the distribution\n    latents = latents * 0.18215 # This scaling factor was introduced by the SD authors to reduce the variance of the latents\n\nlatents.shape\n```", "```py\ntorch.Size([1, 4, 64, 64])\n```", "```py\n# Plot the individual channels of the latent representation\nshow_images([l for l in latents[0]], titles=[f'Channel {i}' for i in range(latents.shape[1])], ncols=4)\n```", "```py\n# Decode the image\nwith torch.no_grad():\n    image = vae.decode(latents / 0.18215).sample\nimage = (image / 2 + 0.5).clamp(0, 1)\nshow_image(image[0].float());\n```", "```py\n# Some settings\nprompt = [\"Acrylic palette knife painting of a flower\"] # What we want to generate\nheight = 512                        # default height of Stable Diffusion\nwidth = 512                         # default width of Stable Diffusion\nnum_inference_steps = 30            # Number of denoising steps\nguidance_scale = 7.5                # Scale for classifier-free guidance\nseed = 42                           # Seed for random number generator\n```", "```py\n# Tokenize the input\ntext_input = pipe.tokenizer(prompt, padding=\"max_length\", max_length=pipe.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n\n# Feed through the text encoder\nwith torch.no_grad():\n    text_embeddings = pipe.text_encoder(text_input.input_ids.to(device))[0]\n\n# Do the same for the unconditional input (a blank string)\nuncond_input = pipe.tokenizer(\"\", padding=\"max_length\", max_length=pipe.tokenizer.model_max_length, return_tensors=\"pt\")\nwith torch.no_grad():\n    uncond_embeddings = pipe.text_encoder(uncond_input.input_ids.to(device))[0]\n\n# Concatenate the two sets of text embeddings embeddings\ntext_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n```", "```py\n# Prepare the Scheduler\npipe.scheduler.set_timesteps(num_inference_steps)\n\n# Prepare the random starting latents\nlatents = torch.randn(\n    (1, pipe.unet.in_channels, height // 8, width // 8), # Shape of the latent representation\n    generator=torch.manual_seed(32),  # Seed the random number generator\n).to(device).half()\nlatents = latents * pipe.scheduler.init_noise_sigma\n```", "```py\n# Sampling loop\nfor i, t in enumerate(pipe.scheduler.timesteps):\n\n    # Create two copies of the latents to match the two text embeddings (unconditional and conditional)\n    latent_model_input = torch.cat([latents] * 2)\n    latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)\n\n    # predict the noise residual for both sets of inputs\n    with torch.no_grad():\n        noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n\n    # Split the prediction into unconditional and conditional versions:\n    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n\n    # perform classifier-free guidance\n    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n\n    # compute the previous noisy sample x_t -> x_t-1\n    latents = pipe.scheduler.step(noise_pred, t, latents).prev_sample\n```", "```py\n# scale and decode the image latents with vae\nlatents = 1 / 0.18215 * latents\nwith torch.no_grad():\n    image = vae.decode(latents).sample\nimage = (image / 2 + 0.5).clamp(0, 1)\n\n# Display\nshow_image(image[0].float());\n```"]