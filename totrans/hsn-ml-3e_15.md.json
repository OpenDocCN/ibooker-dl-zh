["```py\n>>> import tensorflow as tf\n>>> X = tf.range(10)  # any data tensor\n>>> dataset = tf.data.Dataset.from_tensor_slices(X)\n>>> dataset\n<TensorSliceDataset shapes: (), types: tf.int32>\n```", "```py\n>>> for item in dataset:\n...     print(item)\n...\ntf.Tensor(0, shape=(), dtype=int32)\ntf.Tensor(1, shape=(), dtype=int32)\n[...]\ntf.Tensor(9, shape=(), dtype=int32)\n```", "```py\n>>> X_nested = {\"a\": ([1, 2, 3], [4, 5, 6]), \"b\": [7, 8, 9]}\n>>> dataset = tf.data.Dataset.from_tensor_slices(X_nested)\n>>> for item in dataset:\n...     print(item)\n...\n{'a': (<tf.Tensor: [...]=1>, <tf.Tensor: [...]=4>), 'b': <tf.Tensor: [...]=7>}\n{'a': (<tf.Tensor: [...]=2>, <tf.Tensor: [...]=5>), 'b': <tf.Tensor: [...]=8>}\n{'a': (<tf.Tensor: [...]=3>, <tf.Tensor: [...]=6>), 'b': <tf.Tensor: [...]=9>}\n```", "```py\n>>> dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))\n>>> dataset = dataset.repeat(3).batch(7)\n>>> for item in dataset:\n...     print(item)\n...\ntf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\ntf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\ntf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\ntf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\ntf.Tensor([8 9], shape=(2,), dtype=int32)\n```", "```py\n>>> dataset = dataset.map(lambda x: x * 2)  # x is a batch\n>>> for item in dataset:\n...     print(item)\n...\ntf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\ntf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n[...]\n```", "```py\n>>> dataset = dataset.filter(lambda x: tf.reduce_sum(x) > 50)\n>>> for item in dataset:\n...     print(item)\n...\ntf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\ntf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\ntf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n```", "```py\n>>> for item in dataset.take(2):\n...     print(item)\n...\ntf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\ntf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n```", "```py\n>>> dataset = tf.data.Dataset.range(10).repeat(2)\n>>> dataset = dataset.shuffle(buffer_size=4, seed=42).batch(7)\n>>> for item in dataset:\n...     print(item)\n...\ntf.Tensor([3 0 1 6 2 5 7], shape=(7,), dtype=int64)\ntf.Tensor([8 4 1 9 4 2 3], shape=(7,), dtype=int64)\ntf.Tensor([7 5 0 8 9 6], shape=(6,), dtype=int64)\n```", "```py\nMedInc,HouseAge,AveRooms,AveBedrms,Popul\u2026,AveOccup,Lat\u2026,Long\u2026,MedianHouseValue\n3.5214,15.0,3.050,1.107,1447.0,1.606,37.63,-122.43,1.442\n5.3275,5.0,6.490,0.991,3464.0,3.443,33.69,-117.39,1.687\n3.1,29.0,7.542,1.592,1328.0,2.251,38.44,-122.98,1.621\n[...]\n```", "```py\n>>> train_filepaths\n['datasets/housing/my_train_00.csv', 'datasets/housing/my_train_01.csv', ...]\n```", "```py\nfilepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)\n```", "```py\nn_readers = 5\ndataset = filepath_dataset.interleave(\n    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n    cycle_length=n_readers)\n```", "```py\n>>> for line in dataset.take(5):\n...     print(line)\n...\ntf.Tensor(b'4.5909,16.0,[...],33.63,-117.71,2.418', shape=(), dtype=string)\ntf.Tensor(b'2.4792,24.0,[...],34.18,-118.38,2.0', shape=(), dtype=string)\ntf.Tensor(b'4.2708,45.0,[...],37.48,-122.19,2.67', shape=(), dtype=string)\ntf.Tensor(b'2.1856,41.0,[...],32.76,-117.12,1.205', shape=(), dtype=string)\ntf.Tensor(b'4.1812,52.0,[...],33.73,-118.31,3.215', shape=(), dtype=string)\n```", "```py\nX_mean, X_std = [...]  # mean and scale of each feature in the training set\nn_inputs = 8\n\ndef parse_csv_line(line):\n    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n    fields = tf.io.decode_csv(line, record_defaults=defs)\n    return tf.stack(fields[:-1]), tf.stack(fields[-1:])\n\ndef preprocess(line):\n    x, y = parse_csv_line(line)\n    return (x - X_mean) / X_std, y\n```", "```py\n>>> preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')\n(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n array([ 0.16579159,  1.216324  , -0.05204564, -0.39215982, -0.5277444 ,\n -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)>,\n <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.782], dtype=float32)>)\n```", "```py\ndef csv_reader_dataset(filepaths, n_readers=5, n_read_threads=None,\n                       n_parse_threads=5, shuffle_buffer_size=10_000, seed=42,\n                       batch_size=32):\n    dataset = tf.data.Dataset.list_files(filepaths, seed=seed)\n    dataset = dataset.interleave(\n        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n    dataset = dataset.shuffle(shuffle_buffer_size, seed=seed)\n    return dataset.batch(batch_size).prefetch(1)\n```", "```py\ntrain_set = csv_reader_dataset(train_filepaths)\nvalid_set = csv_reader_dataset(valid_filepaths)\ntest_set = csv_reader_dataset(test_filepaths)\n```", "```py\nmodel = tf.keras.Sequential([...])\nmodel.compile(loss=\"mse\", optimizer=\"sgd\")\nmodel.fit(train_set, validation_data=valid_set, epochs=5)\n```", "```py\ntest_mse = model.evaluate(test_set)\nnew_set = test_set.take(3)  # pretend we have 3 new samples\ny_pred = model.predict(new_set)  # or you could just pass a NumPy array\n```", "```py\nn_epochs = 5\nfor epoch in range(n_epochs):\n    for X_batch, y_batch in train_set:\n        [...]  # perform one gradient descent step\n```", "```py\n@tf.function\ndef train_one_epoch(model, optimizer, loss_fn, train_set):\n    for X_batch, y_batch in train_set:\n        with tf.GradientTape() as tape:\n            y_pred = model(X_batch)\n            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n            loss = tf.add_n([main_loss] + model.losses)\n        gradients = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\nloss_fn = tf.keras.losses.mean_squared_error\nfor epoch in range(n_epochs):\n    print(\"\\rEpoch {}/{}\".format(epoch + 1, n_epochs), end=\"\")\n    train_one_epoch(model, optimizer, loss_fn, train_set)\n```", "```py\nwith tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n    f.write(b\"This is the first record\")\n    f.write(b\"And this is the second record\")\n```", "```py\nfilepaths = [\"my_data.tfrecord\"]\ndataset = tf.data.TFRecordDataset(filepaths)\nfor item in dataset:\n    print(item)\n```", "```py\ntf.Tensor(b'This is the first record', shape=(), dtype=string)\ntf.Tensor(b'And this is the second record', shape=(), dtype=string)\n```", "```py\noptions = tf.io.TFRecordOptions(compression_type=\"GZIP\")\nwith tf.io.TFRecordWriter(\"my_compressed.tfrecord\", options) as f:\n    f.write(b\"Compress, compress, compress!\")\n```", "```py\ndataset = tf.data.TFRecordDataset([\"my_compressed.tfrecord\"],\n                                  compression_type=\"GZIP\")\n```", "```py\nsyntax = \"proto3\";\nmessage Person {\n    string name = 1;\n    int32 id = 2;\n    repeated string email = 3;\n}\n```", "```py\n>>> from person_pb2 import Person  # import the generated access class\n>>> person = Person(name=\"Al\", id=123, email=[\"a@b.com\"])  # create a Person\n>>> print(person)  # display the Person\nname: \"Al\"\nid: 123\nemail: \"a@b.com\"\n>>> person.name  # read a field\n'Al'\n>>> person.name = \"Alice\"  # modify a field\n>>> person.email[0]  # repeated fields can be accessed like arrays\n'a@b.com'\n>>> person.email.append(\"c@d.com\")  # add an email address\n>>> serialized = person.SerializeToString()  # serialize person to a byte string\n>>> serialized\nb'\\n\\x05Alice\\x10{\\x1a\\x07a@b.com\\x1a\\x07c@d.com'\n>>> person2 = Person()  # create a new Person\n>>> person2.ParseFromString(serialized)  # parse the byte string (27 bytes long)\n27\n>>> person == person2  # now they are equal\nTrue\n```", "```py\nsyntax = \"proto3\";\nmessage BytesList { repeated bytes value = 1; }\nmessage FloatList { repeated float value = 1 [packed = true]; }\nmessage Int64List { repeated int64 value = 1 [packed = true]; }\nmessage Feature {\n    oneof kind {\n        BytesList bytes_list = 1;\n        FloatList float_list = 2;\n        Int64List int64_list = 3;\n    }\n};\nmessage Features { map<string, Feature> feature = 1; };\nmessage Example { Features features = 1; };\n```", "```py\nfrom tensorflow.train import BytesList, FloatList, Int64List\nfrom tensorflow.train import Feature, Features, Example\n\nperson_example = Example(\n    features=Features(\n        feature={\n            \"name\": Feature(bytes_list=BytesList(value=[b\"Alice\"])),\n            \"id\": Feature(int64_list=Int64List(value=[123])),\n            \"emails\": Feature(bytes_list=BytesList(value=[b\"a@b.com\",\n                                                          b\"c@d.com\"]))\n        }))\n```", "```py\nwith tf.io.TFRecordWriter(\"my_contacts.tfrecord\") as f:\n    for _ in range(5):\n        f.write(person_example.SerializeToString())\n```", "```py\nfeature_description = {\n    \"name\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n    \"id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n    \"emails\": tf.io.VarLenFeature(tf.string),\n}\n\ndef parse(serialized_example):\n    return tf.io.parse_single_example(serialized_example, feature_description)\n\ndataset = tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]).map(parse)\nfor parsed_example in dataset:\n    print(parsed_example)\n```", "```py\n>>> tf.sparse.to_dense(parsed_example[\"emails\"], default_value=b\"\")\n<tf.Tensor: [...] dtype=string, numpy=array([b'a@b.com', b'c@d.com'], [...])>\n>>> parsed_example[\"emails\"].values\n<tf.Tensor: [...] dtype=string, numpy=array([b'a@b.com', b'c@d.com'], [...])>\n```", "```py\ndef parse(serialized_examples):\n    return tf.io.parse_example(serialized_examples, feature_description)\n\ndataset = tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]).batch(2).map(parse)\nfor parsed_examples in dataset:\n    print(parsed_examples)  # two examples at a time\n```", "```py\nmessage FeatureList { repeated Feature feature = 1; };\nmessage FeatureLists { map<string, FeatureList> feature_list = 1; };\nmessage SequenceExample {\n    Features context = 1;\n    FeatureLists feature_lists = 2;\n};\n```", "```py\nparsed_context, parsed_feature_lists = tf.io.parse_single_sequence_example(\n    serialized_sequence_example, context_feature_descriptions,\n    sequence_feature_descriptions)\nparsed_content = tf.RaggedTensor.from_sparse(parsed_feature_lists[\"content\"])\n```", "```py\nnorm_layer = tf.keras.layers.Normalization()\nmodel = tf.keras.models.Sequential([\n    norm_layer,\n    tf.keras.layers.Dense(1)\n])\nmodel.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(learning_rate=2e-3))\nnorm_layer.adapt(X_train)  # computes the mean and variance of every feature\nmodel.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=5)\n```", "```py\nnorm_layer = tf.keras.layers.Normalization()\nnorm_layer.adapt(X_train)\nX_train_scaled = norm_layer(X_train)\nX_valid_scaled = norm_layer(X_valid)\n```", "```py\nmodel = tf.keras.models.Sequential([tf.keras.layers.Dense(1)])\nmodel.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(learning_rate=2e-3))\nmodel.fit(X_train_scaled, y_train, epochs=5,\n          validation_data=(X_valid_scaled, y_valid))\n```", "```py\nfinal_model = tf.keras.Sequential([norm_layer, model])\nX_new = X_test[:3]  # pretend we have a few new instances (unscaled)\ny_pred = final_model(X_new)  # preprocesses the data and makes predictions\n```", "```py\ndataset = dataset.map(lambda X, y: (norm_layer(X), y))\n```", "```py\nimport numpy as np\n\nclass MyNormalization(tf.keras.layers.Layer):\n    def adapt(self, X):\n        self.mean_ = np.mean(X, axis=0, keepdims=True)\n        self.std_ = np.std(X, axis=0, keepdims=True)\n\n    def call(self, inputs):\n        eps = tf.keras.backend.epsilon()  # a small smoothing term\n        return (inputs - self.mean_) / (self.std_ + eps)\n```", "```py\n>>> age = tf.constant([[10.], [93.], [57.], [18.], [37.], [5.]])\n>>> discretize_layer = tf.keras.layers.Discretization(bin_boundaries=[18., 50.])\n>>> age_categories = discretize_layer(age)\n>>> age_categories\n<tf.Tensor: shape=(6, 1), dtype=int64, numpy=array([[0],[2],[2],[1],[1],[0]])>\n```", "```py\n>>> discretize_layer = tf.keras.layers.Discretization(num_bins=3)\n>>> discretize_layer.adapt(age)\n>>> age_categories = discretize_layer(age)\n>>> age_categories\n<tf.Tensor: shape=(6, 1), dtype=int64, numpy=array([[1],[2],[2],[1],[2],[0]])>\n```", "```py\n>>> onehot_layer = tf.keras.layers.CategoryEncoding(num_tokens=3)\n>>> onehot_layer(age_categories)\n<tf.Tensor: shape=(6, 3), dtype=float32, numpy=\narray([[0., 1., 0.],\n [0., 0., 1.],\n [0., 0., 1.],\n [0., 1., 0.],\n [0., 0., 1.],\n [1., 0., 0.]], dtype=float32)>\n```", "```py\n>>> two_age_categories = np.array([[1, 0], [2, 2], [2, 0]])\n>>> onehot_layer(two_age_categories)\n<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[1., 1., 0.],\n [0., 0., 1.],\n [1., 0., 1.]], dtype=float32)>\n```", "```py\n>>> onehot_layer = tf.keras.layers.CategoryEncoding(num_tokens=3 + 3)\n>>> onehot_layer(two_age_categories + [0, 3])  # adds 3 to the second feature\n<tf.Tensor: shape=(3, 6), dtype=float32, numpy=\narray([[0., 1., 0., 1., 0., 0.],\n [0., 0., 1., 0., 0., 1.],\n [0., 0., 1., 1., 0., 0.]], dtype=float32)>\n```", "```py\n>>> cities = [\"Auckland\", \"Paris\", \"Paris\", \"San Francisco\"]\n>>> str_lookup_layer = tf.keras.layers.StringLookup()\n>>> str_lookup_layer.adapt(cities)\n>>> str_lookup_layer([[\"Paris\"], [\"Auckland\"], [\"Auckland\"], [\"Montreal\"]])\n<tf.Tensor: shape=(4, 1), dtype=int64, numpy=array([[1], [3], [3], [0]])>\n```", "```py\n>>> str_lookup_layer = tf.keras.layers.StringLookup(output_mode=\"one_hot\")\n>>> str_lookup_layer.adapt(cities)\n>>> str_lookup_layer([[\"Paris\"], [\"Auckland\"], [\"Auckland\"], [\"Montreal\"]])\n<tf.Tensor: shape=(4, 4), dtype=float32, numpy=\narray([[0., 1., 0., 0.],\n [0., 0., 0., 1.],\n [0., 0., 0., 1.],\n [1., 0., 0., 0.]], dtype=float32)>\n```", "```py\n>>> str_lookup_layer = tf.keras.layers.StringLookup(num_oov_indices=5)\n>>> str_lookup_layer.adapt(cities)\n>>> str_lookup_layer([[\"Paris\"], [\"Auckland\"], [\"Foo\"], [\"Bar\"], [\"Baz\"]])\n<tf.Tensor: shape=(4, 1), dtype=int64, numpy=array([[5], [7], [4], [3], [4]])>\n```", "```py\n>>> hashing_layer = tf.keras.layers.Hashing(num_bins=10)\n>>> hashing_layer([[\"Paris\"], [\"Tokyo\"], [\"Auckland\"], [\"Montreal\"]])\n<tf.Tensor: shape=(4, 1), dtype=int64, numpy=array([[0], [1], [9], [1]])>\n```", "```py\n>>> tf.random.set_seed(42)\n>>> embedding_layer = tf.keras.layers.Embedding(input_dim=5, output_dim=2)\n>>> embedding_layer(np.array([2, 4, 2]))\n<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\narray([[-0.04663396,  0.01846724],\n [-0.02736737, -0.02768031],\n [-0.04663396,  0.01846724]], dtype=float32)>\n```", "```py\n>>> tf.random.set_seed(42)\n>>> ocean_prox = [\"<1H OCEAN\", \"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", \"ISLAND\"]\n>>> str_lookup_layer = tf.keras.layers.StringLookup()\n>>> str_lookup_layer.adapt(ocean_prox)\n>>> lookup_and_embed = tf.keras.Sequential([\n...     str_lookup_layer,\n...     tf.keras.layers.Embedding(input_dim=str_lookup_layer.vocabulary_size(),\n...                               output_dim=2)\n... ])\n...\n>>> lookup_and_embed(np.array([[\"<1H OCEAN\"], [\"ISLAND\"], [\"<1H OCEAN\"]]))\n<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\narray([[-0.01896119,  0.02223358],\n [ 0.02401174,  0.03724445],\n [-0.01896119,  0.02223358]], dtype=float32)>\n```", "```py\nX_train_num, X_train_cat, y_train = [...]  # load the training set\nX_valid_num, X_valid_cat, y_valid = [...]  # and the validation set\n\nnum_input = tf.keras.layers.Input(shape=[8], name=\"num\")\ncat_input = tf.keras.layers.Input(shape=[], dtype=tf.string, name=\"cat\")\ncat_embeddings = lookup_and_embed(cat_input)\nencoded_inputs = tf.keras.layers.concatenate([num_input, cat_embeddings])\noutputs = tf.keras.layers.Dense(1)(encoded_inputs)\nmodel = tf.keras.models.Model(inputs=[num_input, cat_input], outputs=[outputs])\nmodel.compile(loss=\"mse\", optimizer=\"sgd\")\nhistory = model.fit((X_train_num, X_train_cat), y_train, epochs=5,\n                    validation_data=((X_valid_num, X_valid_cat), y_valid))\n```", "```py\n>>> train_data = [\"To be\", \"!(to be)\", \"That's the question\", \"Be, be, be.\"]\n>>> text_vec_layer = tf.keras.layers.TextVectorization()\n>>> text_vec_layer.adapt(train_data)\n>>> text_vec_layer([\"Be good!\", \"Question: be or be?\"])\n<tf.Tensor: shape=(2, 4), dtype=int64, numpy=\narray([[2, 1, 0, 0],\n [6, 2, 1, 2]])>\n```", "```py\n>>> text_vec_layer = tf.keras.layers.TextVectorization(output_mode=\"tf_idf\")\n>>> text_vec_layer.adapt(train_data)\n>>> text_vec_layer([\"Be good!\", \"Question: be or be?\"])\n<tf.Tensor: shape=(2, 6), dtype=float32, numpy=\narray([[0.96725637, 0.6931472 , 0\\. , 0\\. , 0\\. , 0\\.        ],\n [0.96725637, 1.3862944 , 0\\. , 0\\. , 0\\. , 1.0986123 ]], dtype=float32)>\n```", "```py\n>>> import tensorflow_hub as hub\n>>> hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim50/2\")\n>>> sentence_embeddings = hub_layer(tf.constant([\"To be\", \"Not to be\"]))\n>>> sentence_embeddings.numpy().round(2)\narray([[-0.25,  0.28,  0.01,  0.1 ,  [...] ,  0.05,  0.31],\n [-0.2 ,  0.2 , -0.08,  0.02,  [...] , -0.04,  0.15]], dtype=float32)\n```", "```py\nfrom sklearn.datasets import load_sample_images\n\nimages = load_sample_images()[\"images\"]\ncrop_image_layer = tf.keras.layers.CenterCrop(height=100, width=100)\ncropped_images = crop_image_layer(images)\n```", "```py\nimport tensorflow_datasets as tfds\n\ndatasets = tfds.load(name=\"mnist\")\nmnist_train, mnist_test = datasets[\"train\"], datasets[\"test\"]\n```", "```py\nfor batch in mnist_train.shuffle(10_000, seed=42).batch(32).prefetch(1):\n    images = batch[\"image\"]\n    labels = batch[\"label\"]\n    # [...] do something with the images and labels\n```", "```py\nmnist_train = mnist_train.shuffle(buffer_size=10_000, seed=42).batch(32)\nmnist_train = mnist_train.map(lambda items: (items[\"image\"], items[\"label\"]))\nmnist_train = mnist_train.prefetch(1)\n```", "```py\ntrain_set, valid_set, test_set = tfds.load(\n    name=\"mnist\",\n    split=[\"train[:90%]\", \"train[90%:]\", \"test\"],\n    as_supervised=True\n)\ntrain_set = train_set.shuffle(buffer_size=10_000, seed=42).batch(32).prefetch(1)\nvalid_set = valid_set.batch(32).cache()\ntest_set = test_set.batch(32).cache()\ntf.random.set_seed(42)\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=(28, 28)),\n    tf.keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n              metrics=[\"accuracy\"])\nhistory = model.fit(train_set, validation_data=valid_set, epochs=5)\ntest_loss, test_accuracy = model.evaluate(test_set)\n```"]