<html><head></head><body><div id="book-content" class="calibre2"><div id="sbo-rt-content" class="calibre3"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 11. Looking Ahead" class="calibre6"><div class="preface" id="ch11_looking_ahead_1728407067187368">
<h1 class="calibre5"><span class="firstname">Chapter 11. </span>Looking Ahead</h1>

<p class="subtitle">Human history only makes sense on a logarithmic scale. It took humans countless eons to figure out farming, millennia beyond that to invent writing, centuries more to invent the steam engine, and decades more to invent the automobile, computer, and smartphone. Just a few years after that, around 2012, deep learning<a contenteditable="false" data-primary="deep learning" data-type="indexterm" id="id1193" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> appeared on the scene.</p>

<p class="subtitle">OpenAI’s GPT-2 was<a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="rapid adoption and expansion of" data-type="indexterm" id="id1194" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> announced in 2019, and then ChatGPT was announced in 2022. This ignited an explosion of development around LLMs. Many companies have jumped into the fray—Anthropic, Google, Microsoft, Meta, xAI, NVIDIA, Mistral, and more—all building new LLMs that have leap-frogged the previous ones in capability, capacity, and speed. In mere months, LLMs have morphed from document completion engines, to chat engines, to agents that can interact with the outside world.</p>

<p class="subtitle">Buckle up, readers. If<a contenteditable="false" data-primary="agency" data-see="also conversational agency" data-type="indexterm" id="id1195" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="completions" data-see="also document types" data-type="indexterm" id="id1196" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="completions" data-secondary="misleading or incorrect" data-see="also hallucinations" data-type="indexterm" id="id1197" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="content" data-see="also prompt content" data-type="indexterm" id="id1198" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="document types" data-see="also completions" data-type="indexterm" id="id1199" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="errors" data-see="also hallucinations" data-type="indexterm" id="id1200" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="evaluation" data-see="also quality" data-type="indexterm" id="id1201" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="generative pre-trained transformer models" data-see="GPT models" data-type="indexterm" id="id1202" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="large language models (LLMs)" data-see="also application design" data-type="indexterm" id="id1203" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="impact on workflow" data-see="also LLM workflows" data-type="indexterm" id="id1204" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="pre-trained transformer architecture" data-see="also ChatGPT" data-type="indexterm" id="id1205" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="quality" data-see="also evaluation" data-type="indexterm" id="id1206" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="tests" data-see="also evaluation" data-type="indexterm" id="id1207" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="workflows" data-see="LLM workflows" data-type="indexterm" id="id1208" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="RLHF model" data-see="also reinforcement learning from human feedback" data-type="indexterm" id="id1209" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="feedback" data-see="also reinforcement learning from human feedback" data-type="indexterm" id="id1210" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="RAG" data-see="retrieval-augmented generation" data-type="indexterm" id="id1211" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="window sizes" data-see="also context window" data-type="indexterm" id="id1212" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="transformer" data-see="also GPT models" data-type="indexterm" id="id1213" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> you think the pace of change is fast now, then just wait, it’s only going to get faster. (Maybe that Ray Kurzweil guy was on to something!) In this final chapter, let’s look ahead to some of the developments on our horizon and how they will change your work as a prompt engineer.</p>

<section data-type="sect1" data-pdf-bookmark="Multimodality" class="calibre6"><div class="preface" id="ch11_multimodality_1728407067187555">
<h1 class="calibre5">Multimodality</h1>

<p class="subtitle">There<a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="multimodal models" data-type="indexterm" id="LLMmulti11" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="multimodality" data-secondary="push toward multimodal models" data-type="indexterm" id="id1214" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> is a huge push toward the use of multimodal models. OpenAI kicked off this trend with<a contenteditable="false" data-primary="OpenAI GPT APIs" data-secondary="GPT-4" data-type="indexterm" id="id1215" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> GPT-4, which was able to<a contenteditable="false" data-primary="image processing" data-type="indexterm" id="id1216" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> process images as part of the prompt. Although OpenAI has not disclosed details about how exactly the model works, most likely, it closely follows the methods published in <a href="https://arxiv.org/abs/2202.10936" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">academic literature</a>.</p>

<p class="subtitle">In one such method, a<a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-type="indexterm" id="id1217" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="multimodality" data-secondary="image processing with" data-type="indexterm" id="id1218" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> convolutional network is used to convert image features into<a contenteditable="false" data-primary="embedding models" data-type="indexterm" id="id1219" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> embedding vectors of the same dimensions as those used for text tokens. The image vectors are imbued with positional information so that the relationships among the features in the image are retained. The image and text vectors are then concatenated. Finally, the<a contenteditable="false" data-primary="transformer architecture" data-type="indexterm" id="id1220" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> transformer architecture processes this information in much the same way that text-only LLMs process pure text (see <a data-type="xref" href="ch02.html#ch02_understanding_llms_1728407258904677" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Chapter 2</a>). Multimodality can naively be extended to video input—all you have to do is sample images from the video, as demonstrated in this <a href="https://oreil.ly/RhK-7" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">OpenAI cookbook</a>.</p>

<p class="subtitle">As they mature, multimodal models <a contenteditable="false" data-primary="multimodality" data-secondary="benefits of" data-type="indexterm" id="id1221" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/>are going to be extremely useful in domains that can’t be captured by text. For example, it’s easy to imagine how these models can be used to make the world much more accessible to a person with<a contenteditable="false" data-primary="vision model" data-type="indexterm" id="id1222" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> vision impairment. A vision model could help them read signs, find buildings, and navigate unfamiliar environments.</p>

<p class="subtitle">Another reason that multimodal models are important is because they give the models access to a large volume of rich training data. Over the past few years, there has been increasing concern that we might actually run out of training data! The models are large enough that they can learn increasingly intricate details about the world. However, if we overtrain with a set of data that is too small, then these models can overfit—effectively memorizing text rather than modeling how the world works. Amazingly, literally<em class="hyperlink"> the text of the entire public internet</em> may not be enough for the next generation of large models.</p>

<p class="subtitle">However, when we incorporate images and video into the training, we gain access to vastly more content. Moreover, the image and video content carry a very different type of information that can help models better understand the world around them; with access to images, it should become much simpler for models to understand tasks related to spatial reasoning, social cues, physical common sense, and much more.</p>

<p class="subtitle">As a prompt engineer, when you build future LLM applications, you are likely to include images and videos in the prompt—and even though they constitute a completely different form of information, you can make use of some of the lessons in this book when dealing with them. Remember to include only images that are relevant to the conversation at hand so that the model doesn’t get distracted. Frame the images with text that properly introduces their role in the conversation and make use of patterns and motifs that were in the training data. For instance, don’t introduce a new type of diagram to convey information when there is a common format that is more readily available on the internet.</p>

<section class="calibre6" data-type="sect2" data-pdf-bookmark="User Experience and User Interface"><div class="preface" id="ch11_user_experience_and_user_interface_1728407067187635">
<h2 class="calibre19">User Experience and User Interface</h2>

<p class="subtitle">The<a contenteditable="false" data-primary="multimodality" data-secondary="user experience and user interface" data-type="indexterm" id="Muser11" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="user experience" data-type="indexterm" id="userexp11" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> UI of many consumer applications is currently moving toward conversational interactions. It makes sense, right? Humans have been speaking to each other for 200,000 years but only clicking buttons on screens for the past 40. In this section, we’ll focus on a new element of the conversation that has caught our attention—artifacts—or, as we like to call<a contenteditable="false" data-primary="stateful objects of discourse" data-type="indexterm" id="id1223" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> them, <em class="hyperlink">stateful objects of discourse</em>.</p>

<p class="subtitle">Think about it. In day-to-day collaboration with other humans, we often talk about a <em class="hyperlink">thing</em>—the<a contenteditable="false" data-primary="objects of discourse" data-type="indexterm" id="id1224" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <em class="hyperlink">object of discourse</em>. And as we talk about it, we can talk about how we want to change it, we can actually modify it, and we can talk about how it has changed over time—meaning we can talk about its state. Pair programming is a great example here. The files are the objects of discourse, and during pairing, we can change them and talk about how they are changing.</p>

<p class="subtitle">In most chat applications today, the assistants don’t address the object of the conversation in a stateful way. If you ask ChatGPT to write a function and then later modify it, it can’t go back and update the contents of the function. Instead, it rewrites the function over and over again, from scratch. Rather than having one object whose state has evolved, ChatGPT writes <em class="hyperlink">N</em> objects into the conversation.</p>

<p class="subtitle">What’s more, it is difficult to specify which object you’re talking about, especially if you have multiple objects in play. Which function were you talking about? Which version? These problems make it difficult to work with the assistant <em class="hyperlink">on</em> something rather than just having a conversation in which ideas are expected to fly by and go out of scope.</p>

<p class="subtitle">As we were wrapping up this book, Anthropic introduced<a contenteditable="false" data-primary="Anthropic" data-secondary="Artifacts prompt" data-type="indexterm" id="id1225" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <em class="hyperlink">Artifacts</em>, which represents a step in the direction of stateful objects of discourse. In a conversation with Anthropic’s Claude, the<a contenteditable="false" data-primary="Artifacts" data-secondary="stateful object of discourse" data-type="indexterm" id="id1226" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> Artifact <em class="hyperlink">is</em> the stateful object of discourse. It can be an SVG image, an HTML file, a mermaid diagram, code, or any other type of text fragment. During a conversation, the user works with the assistant to modify the Artifact until it reaches the user’s expectations. And while the back-and-forth conversation is captured in a transcript on the left side of the screen, the Artifact they are discussing remains—statefully—on the right side of the screen (see <a data-type="xref" href="#ch11_figure_1_1728407067176842" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 11-1</a>). If the user asks for the Artifact to be modified, then the state of the Artifact is updated in place rather than regurgitated over and over again in the transcript.</p>

<figure class="calibre22"><div id="ch11_figure_1_1728407067176842" class="figure"><img alt="A screenshot of a chat  Description automatically generated" src="assets/pefl_1101.png" class="calibre23"/>
<h6 class="calibre24"><span class="firstname">Figure 11-1. </span>Working with Claude to draw a peg-legged pirate with a patch over his eye while statefully discussing the fact that the image appears to be missing actual legs and eyes</h6>
</div></figure>

<p class="subtitle">Claude’s Artifacts paradigm is very close to what we have in mind, but there’s still room for improvement. For instance, much of the change is just in the UI, rather than in the prompt engineering. When you ask for a change, Claude is still rewriting the entire Artifact from scratch; it just knows to put the Artifact in the right pane. This form of editing might not scale well to longer documents.</p>

<p class="subtitle">Additionally, it’s not easy to interact with multiple Artifacts at once. Claude’s interface assumes one Artifact at a time. If you start talking about a different Artifact, then the UI treats it as if it were just a different version of the previous interface. Another problem with multiple Artifacts is that it’s hard to refer to them. It would be nice if both the UI and the prompt included shorthand names for the items.</p>

<p class="subtitle">Finally, Claude’s interface and the prompt engineering (for that matter) don’t allow the user to edit the Artifact. If you see a small problem that you could easily fix, the only way to address it is to tell the assistant to fix the problem for you (by retyping the whole file). It would be a better experience if the user could update the Artifact and then have this update reflected in the next prompt so that the model is aware of the change.</p>

<p class="subtitle">When building LLM interfaces in your own LLM applications, it can be a good idea to lean into a conversational interface since conversation is so intuitive for humans. But if so, you need to invest time to really get it right—it’s easy to whip up something bare-bones with LLMs, but it will be a gimmicky distraction rather than something that provides true benefits, unless it’s properly thought through and fleshed out.</p>

<p class="subtitle">Model designers are aware of this need, and they innovate to support it. Tools were a great improvement—they gave the assistants the ability to take action in the world. Artifacts are similarly useful—they allow conversations to be about <em class="hyperlink">things</em> (i.e., stateful objects of discourse). What’s next?</p>

<p class="subtitle">The conversational UI is also a great way to keep users in the loop. As we discussed in <a data-type="xref" href="ch08.html#ch08_01_conversational_agency_1728429579285372" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Chapter 8</a>, models tend to stray off course if left to their own devices. But in a close conversational interaction, users can identify problems early and put the assistant back on course.<a contenteditable="false" data-primary="" data-startref="Muser11" data-type="indexterm" id="id1227" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="" data-startref="userexp11" data-type="indexterm" id="id1228" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/></p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Intelligence" class="calibre6"><div class="preface" id="ch11_intelligence_1728407067187700">
<h2 class="calibre19">Intelligence</h2>

<p class="subtitle">Has<a contenteditable="false" data-primary="multimodality" data-secondary="upcoming developments in intelligence" data-type="indexterm" id="Mintell11" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="intelligence" data-secondary="upcoming developments in LLMs" data-type="indexterm" id="Idevel11" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> anyone noticed that LLMs are getting a lot smarter? Yeah…and they’re going to keep getting smarter too. Let’s look at some upcoming developments.</p>

<p class="subtitle">For one thing, we’re getting smarter with our<a contenteditable="false" data-primary="benchmarks" data-type="indexterm" id="id1229" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> benchmarks. Benchmarks are problem sets with known answers that allow us to measure how well models perform relative to humans and relative to one another. At this point, several of the most useful benchmarks have saturated (see <a data-type="xref" href="#ch11_figure_2_1728407067176876" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 11-2</a>), meaning that the leading models tend to ace them. This makes the benchmarks useless for evaluating model improvements. There are two reasons that models saturate the benchmarks: (1) models really are getting smarter—<em class="hyperlink">a good thing</em>, and (2) models are cheating by<a contenteditable="false" data-primary="training" data-type="indexterm" id="id1230" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> training on the benchmarks—<em class="hyperlink">a very bad thing</em>. The “cheating” isn’t intentional; it’s just that after a couple of years, information from benchmarks gets duplicated (verbatim or through descriptions) all over the internet and accidentally pulled into training.</p>

<p class="subtitle">To fix this, we in the AI community are being diligent in upgrading our benchmarks (for instance, on the<a contenteditable="false" data-primary="Open LLM Leaderboard 2" data-type="indexterm" id="id1231" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <a href="https://oreil.ly/zr_z6" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Open LLM Leaderboard 2</a>). We have also started using nonmemorizable benchmarks such as<a contenteditable="false" data-primary="ARC-AGI" data-type="indexterm" id="id1232" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <a href="https://oreil.ly/YTM0M" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">ARC-AGI</a>, which is effectively a set of psychometric intelligence tests<a contenteditable="false" data-primary="psychometric intelligence tests" data-type="indexterm" id="id1233" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="tests" data-secondary="psychometric intelligence tests" data-type="indexterm" id="id1234" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> composed of patterns of shapes. They test how well the individual—or LLM—can understand and reproduce<a contenteditable="false" data-primary="repetitions and patterns" data-type="indexterm" id="id1235" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="patterns and repetitions" data-type="indexterm" id="id1236" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> novel patterns, and it’s impossible to memorize all of the test questions because they belong to a very large space of possible tests, they are algorithmically generated, and you can always generate more of them.</p>

<figure class="calibre22"><div id="ch11_figure_2_1728407067176876" class="figure"><img alt="A graph of different colored lines   Description automatically generated" src="assets/pefl_1102.png" class="calibre23"/>
<h6 class="calibre24"><span class="firstname">Figure 11-2. </span>Popular benchmarks saturate over time, making them useless as benchmarks going forward</h6>
</div></figure>

<p class="subtitle">We’re also getting smarter with model training. You can actually watch this happen when you use ChatGPT or its competitors because, thanks to better RLHF training (refer back to <a data-type="xref" href="ch03.html#ch03a_moving_toward_chat_1728432131625250" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Chapter 3</a>), the models are doing a much better job of expressing their chain-of-thought reasoning, which inevitably leads to more useful responses.</p>

<p class="subtitle">Next, we’re getting more creative with<a contenteditable="false" data-primary="knowledge distillation" data-type="indexterm" id="id1237" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> training approaches. For example, large models generally don’t utilize their full capacity, so, if you can figure out a way to convey knowledge to a small model, then you can effectively compress the information of the large model into the small model. A training approach known as <em class="hyperlink">knowledge distillation</em> uses a large model as a “teacher” of a small model. Rather than training the small model to predict the next token, knowledge distillation trains the small model to mimic the large model by predicting the full set of probabilities for the next token. This richer set of training data allows smaller models to be trained quickly, with only a slight decrease in accuracy compared to their teacher models. In exchange for the hit in accuracy, these small models are significantly cheaper and faster than the large models they were trained from.</p>

<p class="subtitle">Besides training, model improvements will come from architectural innovation. Here, we name just a few. For one, models are getting smaller and faster through <em class="hyperlink">quantization</em> approaches,<a contenteditable="false" data-primary="quantization approaches" data-type="indexterm" id="id1238" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> in which, instead of representing parameters as 32-bit floating point numbers, you can approximate them with 8-bit parameters, reducing the model size considerably and correspondingly decreasing the cost and increasing the speed.</p>

<p class="subtitle">In<a contenteditable="false" data-primary="prompt engineering" data-secondary="trends in LLMs" data-type="indexterm" id="id1239" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> your prompt-engineering work, you should be able to expect these trends to continue. If something is too expensive today, it will be cheaper tomorrow. If something is too slow today, it will be faster tomorrow. If something doesn’t fit in the context today, it will fit tomorrow. And if the model isn’t smart enough today, it will be tomorrow. However, always remember that even though models will get smarter, they will never be psychic. If the prompt doesn’t contain the information that <em class="hyperlink">you</em> would need to solve the problem, then it’s probably insufficient for the model as well.<a contenteditable="false" data-primary="" data-startref="LLMmulti11" data-type="indexterm" id="id1240" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="" data-startref="Mintell11" data-type="indexterm" id="id1241" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="" data-startref="Idevel11" data-type="indexterm" id="id1242" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/></p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Conclusion" class="calibre6"><div class="preface" id="ch11_conclusion_1728407067187760">
<h1 class="calibre5">Conclusion</h1>

<p class="subtitle">If<a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="underlying principle of" data-type="indexterm" id="id1243" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="core takeaways concerning" data-type="indexterm" id="id1244" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="core takeaways" data-type="indexterm" id="id1245" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> we had to sum up the main lessons from this book, there would be two:</p>

<ol class="stafflist">
	<li class="calibre25">
	<p class="calibre26">LLMs are nothing more than text completion engines that mimic the text they see during training.</p>
	</li>
	<li class="calibre25">
	<p class="calibre26">You should<a contenteditable="false" data-primary="empathy" data-type="indexterm" id="id1246" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> empathize with the LLM and understand how it thinks.</p>
	</li>
</ol>

<p class="subtitle">Regarding the first lesson, when we started writing this book, the only models we had access to were completion models—give them a portion of a document (a.k.a. <em class="hyperlink">the prompt</em>), and they would generate plausible text to complete the document. But then, the chat APIs came to dominate, tools came along next, and perhaps, Artifacts will be the next big thing. But even still, at their core, LLMs are just completing documents so that they resemble other documents the model has been taught to “like.” It’s just that now, the documents look like a chat transcript.</p>

<p class="subtitle">The<a contenteditable="false" data-primary="prompt engineering" data-secondary="core lesson of" data-type="indexterm" id="id1247" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> prompt-engineering lesson here is to follow the well-trodden course (the<a contenteditable="false" data-primary="Red Riding Hood principle" data-type="indexterm" id="id1248" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="Little Red Riding Hood principle" data-type="indexterm" id="id1249" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> Little Red Riding Hood principle from <a data-type="xref" href="ch04.html#ch04_designing_llm_applications_1728407230643376" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Chapter 4</a>)—make your prompts follow the patterns and motifs seen in training data and you will be much more likely to get completions that are well behaved and easy to anticipate. For instance, you can format complex text as markdown, and if there is a standard document format for information you are communicating to the LLM, then you’ll have better luck using that format than coming up with a new one the model has never seen.</p>

<p class="subtitle">Regarding<a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="understanding LLM behavior" data-type="indexterm" id="id1250" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> the second lesson, on empathy, think of an LLM as your big, dumb mechanical friend who happens to know much of the content of the internet. Here are some things that will help you understand:</p>

<dl class="stafflist">
	<dt class="calibre13">LLMs are easily distracted</dt>
	<dd class="calibre14">Don’t fill up the prompt with useless information that—<em class="hyperlink">cross your fingers</em>—might just help. Make sure every piece of information matters.</dd>
	<dt class="calibre13">LLMs should be able to decipher the prompt</dt>
	<dd class="calibre14">If, as a human, you can’t understand the fully rendered prompt, then there is a very high chance that the LLM will be equally confused.</dd>
	<dt class="calibre13">LLMs need to be led</dt>
	<dd class="calibre14">Provide explicit instructions for what is to be accomplished and, when appropriate, provide examples demonstrating how the task should proceed.</dd>
	<dt class="calibre13">LLMs aren’t psychic</dt>
	<dd class="calibre14">As the prompt engineer, it’s your job to make sure the prompt contains the information that the model needs to address the problem. Alternatively, give the model the tools and the instructions to retrieve it.</dd>
	<dt class="calibre13">LLMs don’t have internal monologues</dt>
	<dd class="calibre14">If the LLM is allowed to think about the problem out loud (in a chain of thought), then it will be much easier for it to come to a useful solution.</dd>
</dl>

<p class="subtitle">Hopefully, this book has given you all that you need to jump headlong into prompt engineering and LLM application development. Be assured, the accelerating change that we currently experience will continue. Since software will be easier to create, you will find more examples of highly individualized apps or even disposable apps. Applications will take on the nondeterministic nature of the LLMs, leading to more flexible and open-ended experiences. Development will change. You will work in tandem with an AI assistant to get your work done—if you don’t already.</p>

<p class="subtitle">Whatever shape the world finds itself in, it will be a shape of your making. As a prompt engineer, you have the tools at hand and the know-how to build a future of your choosing. Embrace the acceleration. Keep experimenting. Stay flexible. In the words of the late Sir Terry Pratchett:</p>

<blockquote class="pcalibre6 pcalibre5 calibre11">
<p class="calibre12">The whole world is tap-dancing on quicksand. In this case, the prize goes to the best dancer.<sup class="calibre37"><a data-type="noteref" id="id1251-marker" href="ch11.html#id1251" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">1</a></sup></p>
</blockquote>
</div></section>
<div data-type="footnotes" class="calibre39"><p data-type="footnote" id="id1251" class="calibre40"><sup class="calibre41"><a href="ch11.html#id1251-marker" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">1</a></sup> Terry Pratchett, <em class="hyperlink">The Fifth Elephant</em> (New York: Doubleday, 1999) </p></div></div></section></div></div></body></html>