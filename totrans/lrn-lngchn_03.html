<html><head></head><body><section data-pdf-bookmark="Chapter 3. RAG Part II: Chatting with Your Data" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch03_rag_part_ii_chatting_with_your_data_1736545666793580">&#13;
<h1><span class="label">Chapter 3. </span>RAG Part II: Chatting with Your Data</h1>&#13;
&#13;
<p>In the previous chapter, you learned how to process your data and create and store embeddings in a vector store. In this chapter, you’ll learn how to efficiently retrieve the most relevant embeddings and chunks of documents based on a user’s query. This enables you to construct a prompt that contains relevant documents as context, improving the accuracy of the LLM’s final output.</p>&#13;
&#13;
<p>This process—which involves embedding a user’s query, retrieving similar documents from a data source, and then passing them as context to the prompt sent to the LLM—is formally known as<a contenteditable="false" data-primary="retrieval-augmented generation (RAG)" data-secondary="process of" data-type="indexterm" id="id545"/> <em>retrieval-augmented generation</em> (RAG).</p>&#13;
&#13;
<p>RAG is an essential component of building chat-enabled LLM apps that are accurate, efficient, and up-to-date. In this chapter, you’ll progress from basics to advanced strategies to build an effective RAG system for various data sources (such as vector stores and databases) and data structures (structured and unstructured).</p>&#13;
&#13;
<p>But first, let’s define RAG and discuss its benefits.</p>&#13;
&#13;
<section data-pdf-bookmark="Introducing Retrieval-Augmented Generation" data-type="sect1"><div class="sect1" id="ch03_introducing_retrieval_augmented_generation_1736545666793835">&#13;
<h1>Introducing Retrieval-Augmented Generation</h1>&#13;
&#13;
<p>RAG<a contenteditable="false" data-primary="retrieval-augmented generation (RAG)" data-secondary="introduction to" data-type="indexterm" id="id546"/> is a technique used to enhance the accuracy of outputs generated by LLMs by providing context from external sources. The term was originally coined in a paper by Meta AI researchers who discovered that RAG-enabled models are more factual and specific than non-RAG models.<sup><a data-type="noteref" href="ch03.html#id547" id="id547-marker">1</a></sup></p>&#13;
&#13;
<p class="pagebreak-before less_space">Without RAG, the LLM relies solely on its pretrained data, which may be outdated. For example, let’s ask ChatGPT a question about a<a contenteditable="false" data-primary="current events" data-type="indexterm" id="id548"/> current event and see its response:</p>&#13;
&#13;
<p><em>Input</em></p>&#13;
&#13;
<pre data-type="programlisting">&#13;
Which country is the latest winner of the men’s FIFA World Cup?</pre>&#13;
&#13;
<p><em>Output</em></p>&#13;
&#13;
<pre data-type="programlisting">&#13;
The most recent FIFA World Cup winner was France, who won the tournament in 2018.</pre>&#13;
&#13;
<p>The response by the LLM is factually incorrect and outdated. The latest winner at the time of this book’s publication is Argentina, who won the World Cup in 2022. While this example question may be trivial, LLM hallucination can have disastrous consequences if its answers are relied upon for fact-checking or important decision making.</p>&#13;
&#13;
<p>To combat this problem, we need to provide the LLM with factual, up-to-date information from which it can formulate an<a contenteditable="false" data-primary="output" data-secondary="accurate" data-type="indexterm" id="id549"/> accurate response. Continuing on from the previous example, let’s go over to Wikipedia’s page for the <a href="https://oreil.ly/LpLOV">FIFA World Cup</a>, copy the introduction paragraph, and then append it as<a contenteditable="false" data-primary="context" data-secondary="accurate output with" data-type="indexterm" id="id550"/> <em>context</em> to our prompt to ChatGPT:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
Which country is the latest winner of the men's FIFA World Cup?&#13;
&#13;
See context below.&#13;
&#13;
The FIFA World Cup, often called the World Cup, is an international association&#13;
football competition among the senior men's national teams of the members of&#13;
the Fédération Internationale de Football Association (FIFA), the sport's &#13;
global governing body. The tournament has been held every four years since the &#13;
inaugural tournament in 1930, with the exception of 1942 and 1946 due to the &#13;
Second World War. The reigning champions are Argentina, who won their third &#13;
title at the 2022 tournament.</pre>&#13;
&#13;
<p>Note that the last sentence contains the necessary context the LLM can use to provide an accurate answer. Here’s the response from the LLM:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
The latest winner of the men's FIFA World Cup is Argentina, who won their third&#13;
title at the 2022 tournament.</pre>&#13;
&#13;
<p>Because of the up-to-date additional context provided, the LLM was able to generate an accurate response to the prompt. But copying and pasting relevant information as context isn’t practical nor scalable for a production AI application. We need an automated system to fetch relevant information based on a user’s query, append it as context to the prompt, and then execute the generation request to the LLM.</p>&#13;
&#13;
<section class="pagebreak-before" data-pdf-bookmark="Retrieving Relevant Documents" data-type="sect2"><div class="sect2" id="ch03_retrieving_relevant_documents_1736545666793935">&#13;
<h2 class="less_space">Retrieving Relevant Documents</h2>&#13;
&#13;
<p>A<a contenteditable="false" data-primary="retrieval-augmented generation (RAG)" data-secondary="retrieving relevant documents" data-type="indexterm" id="RAGrelevant03"/><a contenteditable="false" data-primary="documents" data-secondary="retrieving relevant documents" data-type="indexterm" id="Drelevant03"/> RAG system for an AI<a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="relevant content for LLMs" data-type="indexterm" id="LLMrelevant03"/><a contenteditable="false" data-primary="data indexing" data-secondary="relevant content for LLMs" data-type="indexterm" id="DIrelevant03"/> app typically follows<a contenteditable="false" data-primary="retrieval-augmented generation (RAG)" data-secondary="core stages of" data-type="indexterm" id="id551"/> three core stages:</p>&#13;
&#13;
<dl>&#13;
	<dt>Indexing</dt>&#13;
	<dd>&#13;
	<p>This<a contenteditable="false" data-primary="data indexing" data-secondary="process of" data-type="indexterm" id="id552"/> stage involves preprocessing the external data source and storing embeddings that represent the data in a vector store where they can be easily retrieved.</p>&#13;
	</dd>&#13;
	<dt>Retrieval</dt>&#13;
	<dd>&#13;
	<p>This stage involves retrieving the relevant embeddings and data stored in the vector store based on a user’s query.</p>&#13;
	</dd>&#13;
	<dt>Generation</dt>&#13;
	<dd>&#13;
	<p>This stage involves synthesizing the original prompt with the retrieved relevant documents as one final prompt sent to the model for a prediction.</p>&#13;
	</dd>&#13;
</dl>&#13;
&#13;
<p>The three basic stages look like <a data-type="xref" href="#ch03_figure_1_1736545666780536">Figure 3-1</a>.</p>&#13;
&#13;
<figure><div class="figure" id="ch03_figure_1_1736545666780536"><img alt="A diagram of a document  Description automatically generated" src="assets/lelc_0301.png"/>&#13;
<h6><span class="label">Figure 3-1. </span>The key stages of RAG</h6>&#13;
</div></figure>&#13;
&#13;
<p>The indexing stage of this process was covered extensively in <a data-type="xref" href="ch02.html#ch02_rag_part_i_indexing_your_data_1736545662500927">Chapter 2</a>, where you learned how to use document loaders, text splitters, embeddings, and vector stores. </p>&#13;
  &#13;
  <p class="pagebreak-before less_space">Let’s run through an example from scratch again, starting with the indexing stage:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="data indexing" data-tertiary="example of" data-type="indexterm" id="id553"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">langchain_community.document_loaders</code> <code class="kn">import</code> <code class="n">TextLoader</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_openai</code> <code class="kn">import</code> <code class="n">OpenAIEmbeddings</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_text_splitters</code> <code class="kn">import</code> <code class="n">RecursiveCharacterTextSplitter</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_postgres.vectorstores</code> <code class="kn">import</code> <code class="n">PGVector</code>&#13;
&#13;
<code class="c1"># Load the document, split it into chunks</code>&#13;
<code class="n">raw_documents</code> <code class="o">=</code> <code class="n">TextLoader</code><code class="p">(</code><code class="s1">'./test.txt'</code><code class="p">)</code><code class="o">.</code><code class="n">load</code><code class="p">()</code>&#13;
<code class="n">text_splitter</code> <code class="o">=</code> <code class="n">RecursiveCharacterTextSplitter</code><code class="p">(</code><code class="n">chunk_size</code><code class="o">=</code><code class="mi">1000</code><code class="p">,</code> &#13;
    <code class="n">chunk_overlap</code><code class="o">=</code><code class="mi">200</code><code class="p">)</code>&#13;
<code class="n">documents</code> <code class="o">=</code> <code class="n">text_splitter</code><code class="o">.</code><code class="n">split_documents</code><code class="p">(</code><code class="n">raw_documents</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># embed each chunk and insert it into the vector store</code>&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">OpenAIEmbeddings</code><code class="p">()</code>&#13;
<code class="n">connection</code> <code class="o">=</code> <code class="s1">'postgresql+psycopg://langchain:langchain@localhost:6024/langchain'</code>&#13;
<code class="n">db</code> <code class="o">=</code> <code class="n">PGVector</code><code class="o">.</code><code class="n">from_documents</code><code class="p">(</code><code class="n">documents</code><code class="p">,</code> <code class="n">model</code><code class="p">,</code> <code class="n">connection</code><code class="o">=</code><code class="n">connection</code><code class="p">)</code></pre>&#13;
&#13;
<p><em>JavaScript<a contenteditable="false" data-primary="JavaScript" data-secondary="data indexing" data-tertiary="example of" data-type="indexterm" id="id554"/></em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">TextLoader</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"langchain/document_loaders/fs/text"</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">RecursiveCharacterTextSplitter</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"@langchain/textsplitters"</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">OpenAIEmbeddings</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"@langchain/openai"</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">PGVectorStore</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"@langchain/community/vectorstores/pgvector"</code><code class="p">;</code>&#13;
&#13;
<code class="c1">// Load the document, split it into chunks</code>&#13;
<code class="kr">const</code> <code class="nx">loader</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">TextLoader</code><code class="p">(</code><code class="s2">"./test.txt"</code><code class="p">);</code>&#13;
<code class="kr">const</code> <code class="nx">raw_docs</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">loader</code><code class="p">.</code><code class="nx">load</code><code class="p">();</code>&#13;
<code class="kr">const</code> <code class="nx">splitter</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">RecursiveCharacterTextSplitter</code><code class="p">({</code>&#13;
  <code class="nx">chunkSize</code><code class="o">:</code> <code class="mi">1000</code><code class="p">,</code>&#13;
  <code class="nx">chunkOverlap</code><code class="o">:</code> <code class="mi">200</code><code class="p">,</code>&#13;
<code class="p">});</code>&#13;
<code class="kr">const</code> <code class="nx">docs</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">splitter</code><code class="p">.</code><code class="nx">splitDocuments</code><code class="p">(</code><code class="nx">docs</code><code class="p">)</code>&#13;
&#13;
<code class="c1">// embed each chunk and insert it into the vector store</code>&#13;
<code class="kr">const</code> <code class="nx">model</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">OpenAIEmbeddings</code><code class="p">();</code>&#13;
<code class="kr">const</code> <code class="nx">db</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">PGVectorStore</code><code class="p">.</code><code class="nx">fromDocuments</code><code class="p">(</code><code class="nx">docs</code><code class="p">,</code> <code class="nx">model</code><code class="p">,</code> <code class="p">{</code>&#13;
  <code class="nx">postgresConnectionOptions</code><code class="o">:</code> <code class="p">{</code>&#13;
    <code class="nx">connectionString</code><code class="o">:</code> <code class="s1">'postgresql://langchain:langchain@localhost:6024/langchain'</code>&#13;
  <code class="p">}</code>&#13;
<code class="p">})</code></pre>&#13;
&#13;
<p><a data-type="xref" href="ch02.html#ch02_rag_part_i_indexing_your_data_1736545662500927">Chapter 2</a> has more details on the indexing stage.</p>&#13;
&#13;
<p>The indexing stage is now complete. In order to execute the retrieval stage, we need to perform similarity search calculations—such as<a contenteditable="false" data-primary="cosine similarity" data-type="indexterm" id="id555"/> cosine similarity—between the user’s query and our stored embeddings, so relevant chunks of our indexed document are retrieved (see <a data-type="xref" href="#ch03_figure_2_1736545666780585">Figure 3-2</a>).</p>&#13;
&#13;
<figure><div class="figure" id="ch03_figure_2_1736545666780585"><img alt="Screenshot 2024-02-12 at 1.36.56 PM.png" src="assets/lelc_0302.png"/>&#13;
<h6><span class="label">Figure 3-2. </span>An example flow of indexing documents alongside retrieval of relevant documents from a vector store; the Hierarchical Navigable Small World (HNSW) box depicts calculating similarity of documents against the user’s query</h6>&#13;
</div></figure>&#13;
&#13;
<p><a data-type="xref" href="#ch03_figure_2_1736545666780585">Figure 3-2</a> illustrates the steps in the<a contenteditable="false" data-primary="retrieval process" data-type="indexterm" id="id556"/><a contenteditable="false" data-primary="embeddings" data-secondary="retrieving relevant embeddings" data-type="indexterm" id="id557"/><a contenteditable="false" data-primary="Hierarchical Navigable Small World (HNSW)" data-type="indexterm" id="id558"/> retrieval process:</p>&#13;
&#13;
<ol>&#13;
	<li>&#13;
	<p>Convert the user’s query into embeddings.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Calculate the embeddings in the vector store that are most similar to the user’s query.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Retrieve the relevant document embeddings and their corresponding text chunk.</p>&#13;
	</li>&#13;
</ol>&#13;
&#13;
<p class="pagebreak-before less_space">We can represent these steps programmatically using LangChain as follows:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="embeddings, retrieving relevant" data-type="indexterm" id="id559"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="c1"># create retriever</code><code>&#13;
</code><code class="n">retriever</code><code> </code><strong><code class="o">=</code></strong><code> </code><code class="n">db</code><strong><code class="o">.</code></strong><code class="n">as_retriever</code><code class="p">(</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># fetch relevant documents</code><code>&#13;
</code><code class="n">docs</code><code> </code><strong><code class="o">=</code></strong><code> </code><code class="n">retriever</code><strong><code class="o">.</code></strong><code class="n">invoke</code><code class="p">(</code><code class="s2">"""</code><code class="s2">Who are the key figures in the ancient greek </code><code class="s2">&#13;
</code><code class="s2">    history of philosophy?</code><code class="s2">"""</code><code class="p">)</code></pre>&#13;
&#13;
<p><em>JavaScript<a contenteditable="false" data-primary="JavaScript" data-secondary="embeddings, retrieving relevant" data-type="indexterm" id="id560"/></em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="c1">// create retriever</code>&#13;
<code class="kr">const</code> <code class="nx">retriever</code> <code class="o">=</code> <code class="nx">db</code><code class="p">.</code><code class="nx">asRetriever</code><code class="p">()</code>&#13;
&#13;
<code class="c1">// fetch relevant documents</code>&#13;
<code class="kr">const</code> <code class="nx">docs</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">retriever</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="sb">`Who are the key figures in the ancient </code>&#13;
<code class="sb">  greek history of philosophy?`</code><code class="p">)</code></pre>&#13;
&#13;
<p>Note that we are using a vector store method you haven’t seen before: <code>as_retriever</code>. This function abstracts the logic of embedding the user’s query and the underlying similarity search calculations performed by the vector store to retrieve the relevant documents.</p>&#13;
&#13;
<p>There is also an<a contenteditable="false" data-primary="argument k" data-type="indexterm" id="id561"/> argument <code>k</code>, which determines the number of relevant documents to fetch from the vector store. For example:</p>&#13;
&#13;
<p><em>Python</em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="c1"># create retriever with k=2</code>&#13;
<code class="n">retriever</code> <code class="o">=</code> <code class="n">db</code><code class="o">.</code><code class="n">as_retriever</code><code class="p">(</code><code class="n">search_kwargs</code><code class="o">=</code><code class="p">{</code><code class="s2">"k"</code><code class="p">:</code> <code class="mi">2</code><code class="p">})</code>&#13;
&#13;
<code class="c1"># fetch the 2 most relevant documents</code>&#13;
<code class="n">docs</code> <code class="o">=</code> <code class="n">retriever</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="s2">"""Who are the key figures in the ancient greek history </code>&#13;
<code class="s2">    of philosophy?"""</code><code class="p">)</code></pre>&#13;
&#13;
<p><em>JavaScript</em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="c1">// create retriever with k=2</code>&#13;
<code class="kr">const</code> <code class="nx">retriever</code> <code class="o">=</code> <code class="nx">db</code><code class="p">.</code><code class="nx">asRetriever</code><code class="p">({</code><code class="nx">k</code><code class="o">:</code> <code class="mi">2</code><code class="p">})</code>&#13;
&#13;
<code class="c1">// fetch the 2 most relevant documents</code>&#13;
<code class="kr">const</code> <code class="nx">docs</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">retriever</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="sb">`Who are the key figures in the ancient </code>&#13;
<code class="sb">  greek history of philosophy?`</code><code class="p">)</code></pre>&#13;
&#13;
<p>In this example, the argument <code>k</code> is specified as 2. This tells the vector store to return the two most relevant documents based on the user’s query.</p>&#13;
&#13;
<p>It may seem counterintuitive to use a low <code>k</code> value, but retrieving more documents is not always better. The more documents are retrieved, the slower your application will perform, the larger the prompt (and associated cost of generation) will be, and the greater the likelihood of retrieving chunks of text that contain irrelevant information, which will cause the LLM to hallucinate.<a contenteditable="false" data-primary="" data-startref="RAGrelevant03" data-type="indexterm" id="id562"/><a contenteditable="false" data-primary="" data-startref="Drelevant03" data-type="indexterm" id="id563"/></p>&#13;
&#13;
<p>Now that we’ve completed the retrieval stage of the RAG system, let’s move on to the final generation stage.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Generating LLM Predictions Using Relevant Documents" data-type="sect2"><div class="sect2" id="ch03_generating_llm_predictions_using_relevant_document_1736545666794007">&#13;
<h2>Generating LLM Predictions Using Relevant Documents</h2>&#13;
&#13;
<p>Once<a contenteditable="false" data-primary="generation stage" data-type="indexterm" id="genstg03"/><a contenteditable="false" data-primary="retrieval-augmented generation (RAG)" data-secondary="generating LLM predictions" data-type="indexterm" id="RAGgenerate03"/> we’ve retrieved the relevant documents based on the user’s query, the final step is to add them to the original prompt as context and then invoke the model to generate a final output (<a data-type="xref" href="#ch03_figure_3_1736545666780616">Figure 3-3</a>).</p>&#13;
&#13;
<figure><div class="figure" id="ch03_figure_3_1736545666780616"><img alt="A diagram of a diagram  Description automatically generated" src="assets/lelc_0303.png"/>&#13;
<h6><span class="label">Figure 3-3. </span>An example flow demonstrating indexing documents, retrieval of relevant documents from a vector store, and inclusion of retrieved documents as context in the LLM prompt</h6>&#13;
</div></figure>&#13;
&#13;
<p class="pagebreak-before less_space">Here’s a code example continuing on from our previous example:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="model predictions, invoking" data-type="indexterm" id="Pgenerate03"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">langchain_openai</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_core.prompts</code> <code class="kn">import</code> <code class="n">ChatPromptTemplate</code>&#13;
&#13;
<code class="n">retriever</code> <code class="o">=</code> <code class="n">db</code><code class="o">.</code><code class="n">as_retriever</code><code class="p">()</code>&#13;
&#13;
<code class="n">prompt</code> <code class="o">=</code> <code class="n">ChatPromptTemplate</code><code class="o">.</code><code class="n">from_template</code><code class="p">(</code><code class="s2">"""Answer the question based only on </code>&#13;
<code class="s2">    the following context:</code>&#13;
<code class="si">{context}</code><code class="s2"/>&#13;
&#13;
<code class="s2">Question: </code><code class="si">{question}</code><code class="s2"/>&#13;
<code class="s2">"""</code><code class="p">)</code>&#13;
&#13;
<code class="n">llm</code> <code class="o">=</code> <code class="n">ChatOpenAI</code><code class="p">(</code><code class="n">model_name</code><code class="o">=</code><code class="s2">"gpt-3.5-turbo"</code><code class="p">,</code> <code class="n">temperature</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>&#13;
&#13;
<code class="n">chain</code> <code class="o">=</code> <code class="n">prompt</code> <code class="o">|</code> <code class="n">llm</code>&#13;
&#13;
<code class="c1"># fetch relevant documents </code>&#13;
<code class="n">docs</code> <code class="o">=</code> <code class="n">retriever</code><code class="o">.</code><code class="n">get_relevant_documents</code><code class="p">(</code><code class="s2">"""Who are the key figures in the </code>&#13;
<code class="s2">    ancient greek history of philosophy?"""</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># run</code>&#13;
<code class="n">chain</code><code class="o">.</code><code class="n">invoke</code><code class="p">({</code><code class="s2">"context"</code><code class="p">:</code> <code class="n">docs</code><code class="p">,</code><code class="s2">"question"</code><code class="p">:</code> <code class="s2">"""Who are the key figures in the </code>&#13;
<code class="s2">    ancient greek history of philosophy?"""</code><code class="p">})</code></pre>&#13;
&#13;
<p><em>JavaScript<a contenteditable="false" data-primary="JavaScript" data-secondary="model predictions, invoking" data-type="indexterm" id="JSllmpred03"/></em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">import</code> <code class="p">{</code><code class="nx">ChatOpenAI</code><code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/openai'</code>&#13;
<code class="kr">import</code> <code class="p">{</code><code class="nx">ChatPromptTemplate</code><code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/core/prompts'</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">retriever</code> <code class="o">=</code> <code class="nx">db</code><code class="p">.</code><code class="nx">asRetriever</code><code class="p">()</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">prompt</code> <code class="o">=</code> <code class="nx">ChatPromptTemplate</code><code class="p">.</code><code class="nx">fromTemplate</code><code class="p">(</code><code class="sb">`Answer the question based only </code>&#13;
<code class="sb">  on the following context:</code>&#13;
<code class="sb">{context}</code>&#13;
&#13;
<code class="sb">Question: {question}</code>&#13;
<code class="sb">`</code><code class="p">)</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">llm</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">ChatOpenAI</code><code class="p">({</code><code class="nx">temperature</code><code class="o">:</code> <code class="mi">0</code><code class="p">,</code> <code class="nx">modelName</code><code class="o">:</code> <code class="s1">'gpt-3.5-turbo'</code><code class="p">})</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">chain</code> <code class="o">=</code> <code class="nx">prompt</code><code class="p">.</code><code class="nx">pipe</code><code class="p">(</code><code class="nx">llm</code><code class="p">)</code>&#13;
&#13;
<code class="c1">// fetch relevant documents</code>&#13;
<code class="kr">const</code> <code class="nx">docs</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">retriever</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="sb">`Who are the key figures in the ancient </code>&#13;
<code class="sb">  greek history of philosophy?`</code><code class="p">)</code>&#13;
&#13;
<code class="nx">await</code> <code class="nx">chain</code><code class="p">.</code><code class="nx">invoke</code><code class="p">({</code><code class="nx">context</code><code class="o">:</code> <code class="nx">docs</code><code class="p">,</code> <code class="nx">question</code><code class="o">:</code> <code class="sb">`Who are the key figures in the </code>&#13;
<code class="sb">  ancient greek history of philosophy?`</code><code class="p">})</code></pre>&#13;
&#13;
<p>Note the following changes:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>We implement dynamic <code>context</code> and <code>question</code> variables into our prompt, which allows us to define a <code>ChatPromptTemplate</code> the model can use to generate a response.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>We define a <code>ChatOpenAI</code> interface to act as our LLM. Temperature is set to 0 to eliminate the creativity in outputs from the model.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>We create a chain to compose the prompt and LLM. A reminder: the <code>|</code> operator (or <code>pipe</code> method in JS) takes the output of <code>prompt</code> and uses it as the input to <code>llm</code>.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>We <code>invoke</code> the chain passing in the <code>context</code> variable (our retrieved relevant docs) and the user’s question to generate a final output.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>We can encapsulate this retrieval logic in a single function:</p>&#13;
&#13;
<p><em>Python</em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">langchain_openai</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_core.prompts</code> <code class="kn">import</code> <code class="n">ChatPromptTemplate</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_core.runnables</code> <code class="kn">import</code> <code class="n">chain</code>&#13;
&#13;
<code class="n">retriever</code> <code class="o">=</code> <code class="n">db</code><code class="o">.</code><code class="n">as_retriever</code><code class="p">()</code>&#13;
&#13;
<code class="n">prompt</code> <code class="o">=</code> <code class="n">ChatPromptTemplate</code><code class="o">.</code><code class="n">from_template</code><code class="p">(</code><code class="s2">"""Answer the question based only on </code>&#13;
<code class="s2">    the following context:</code>&#13;
<code class="si">{context}</code><code class="s2"/>&#13;
&#13;
<code class="s2">Question: </code><code class="si">{question}</code><code class="s2"/>&#13;
<code class="s2">"""</code><code class="p">)</code>&#13;
&#13;
<code class="n">llm</code> <code class="o">=</code> <code class="n">ChatOpenAI</code><code class="p">(</code><code class="n">model</code><code class="o">=</code><code class="s2">"gpt-3.5-turbo"</code><code class="p">,</code> <code class="n">temperature</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>&#13;
&#13;
<code class="nd">@chain</code>&#13;
<code class="k">def</code> <code class="nf">qa</code><code class="p">(</code><code class="nb">input</code><code class="p">):</code>&#13;
    <code class="c1"># fetch relevant documents </code>&#13;
    <code class="n">docs</code> <code class="o">=</code> <code class="n">retriever</code><code class="o">.</code><code class="n">get_relevant_documents</code><code class="p">(</code><code class="nb">input</code><code class="p">)</code>&#13;
    <code class="c1"># format prompt</code>&#13;
    <code class="n">formatted</code> <code class="o">=</code> <code class="n">prompt</code><code class="o">.</code><code class="n">invoke</code><code class="p">({</code><code class="s2">"context"</code><code class="p">:</code> <code class="n">docs</code><code class="p">,</code> <code class="s2">"question"</code><code class="p">:</code> <code class="nb">input</code><code class="p">})</code>&#13;
    <code class="c1"># generate answer</code>&#13;
    <code class="n">answer</code> <code class="o">=</code> <code class="n">llm</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">formatted</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="n">answer</code>&#13;
&#13;
<code class="c1"># run</code>&#13;
<code class="n">qa</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="s2">"Who are the key figures in the ancient greek history of philosophy?"</code><code class="p">)</code></pre>&#13;
&#13;
<p><em>JavaScript</em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">import</code> <code class="p">{</code><code class="nx">ChatOpenAI</code><code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/openai'</code>&#13;
<code class="kr">import</code> <code class="p">{</code><code class="nx">ChatPromptTemplate</code><code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/core/prompts'</code>&#13;
<code class="kr">import</code> <code class="p">{</code><code class="nx">RunnableLambda</code><code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/core/runnables'</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">retriever</code> <code class="o">=</code> <code class="nx">db</code><code class="p">.</code><code class="nx">asRetriever</code><code class="p">()</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">prompt</code> <code class="o">=</code> <code class="nx">ChatPromptTemplate</code><code class="p">.</code><code class="nx">fromTemplate</code><code class="p">(</code><code class="sb">`Answer the question based only </code>&#13;
<code class="sb">  on the following context:</code>&#13;
<code class="sb">{context}</code>&#13;
&#13;
<code class="sb">Question: {question}</code>&#13;
<code class="sb">`</code><code class="p">)</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">llm</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">ChatOpenAI</code><code class="p">({</code><code class="nx">temperature</code><code class="o">:</code> <code class="mi">0</code><code class="p">,</code> <code class="nx">modelName</code><code class="o">:</code> <code class="s1">'gpt-3.5-turbo'</code><code class="p">})</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">qa</code> <code class="o">=</code> <code class="nx">RunnableLambda</code><code class="p">.</code><code class="nx">from</code><code class="p">(</code><code class="nx">async</code> <code class="nx">input</code> <code class="o">=&gt;</code> <code class="p">{</code>&#13;
  <code class="c1">// fetch relevant documents</code>&#13;
  <code class="kr">const</code> <code class="nx">docs</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">retriever</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="nx">input</code><code class="p">)</code>&#13;
  <code class="c1">// format prompt</code>&#13;
  <code class="kr">const</code> <code class="nx">formatted</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">prompt</code><code class="p">.</code><code class="nx">invoke</code><code class="p">({</code><code class="nx">context</code><code class="o">:</code> <code class="nx">docs</code><code class="p">,</code> <code class="nx">question</code><code class="o">:</code> <code class="nx">input</code><code class="p">})</code>&#13;
  <code class="c1">// generate answer</code>&#13;
  <code class="kr">const</code> <code class="nx">answer</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">llm</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="nx">formatted</code><code class="p">)</code>&#13;
  <code class="k">return</code> <code class="nx">answer</code>&#13;
<code class="p">})</code>&#13;
&#13;
<code class="nx">await</code> <code class="nx">qa</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="sb">`Who are the key figures in the ancient greek history of </code>&#13;
<code class="sb">  philosophy?`</code><code class="p">)</code></pre>&#13;
&#13;
<p>Notice how we now have a new runnable <code>qa</code> function that can be called with just a question and takes care to first fetch the relevant docs for context, format them into the prompt, and finally generate the answer. In the Python code, the <code>@chain</code> decorator turns the function into a runnable chain. This notion of encapsulating multiple steps into a single function will be key to building interesting apps with LLMs.</p>&#13;
&#13;
<p>You can also return the retrieved documents for further inspection:</p>&#13;
&#13;
<p><em>Python</em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="nd">@chain</code>&#13;
<code class="k">def</code> <code class="nf">qa</code><code class="p">(</code><code class="nb">input</code><code class="p">):</code>&#13;
    <code class="c1"># fetch relevant documents </code>&#13;
    <code class="n">docs</code> <code class="o">=</code> <code class="n">retriever</code><code class="o">.</code><code class="n">get_relevant_documents</code><code class="p">(</code><code class="nb">input</code><code class="p">)</code>&#13;
    <code class="c1"># format prompt</code>&#13;
    <code class="n">formatted</code> <code class="o">=</code> <code class="n">prompt</code><code class="o">.</code><code class="n">invoke</code><code class="p">({</code><code class="s2">"context"</code><code class="p">:</code> <code class="n">docs</code><code class="p">,</code> <code class="s2">"question"</code><code class="p">:</code> <code class="nb">input</code><code class="p">})</code>&#13;
    <code class="c1"># generate answer</code>&#13;
    <code class="n">answer</code> <code class="o">=</code> <code class="n">llm</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">formatted</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="p">{</code><code class="s2">"answer"</code><code class="p">:</code> <code class="n">answer</code><code class="p">,</code> <code class="s2">"docs"</code><code class="p">:</code> <code class="n">docs</code><code class="p">}</code></pre>&#13;
&#13;
<p><em>JavaScript</em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">const</code> <code class="nx">qa</code> <code class="o">=</code> <code class="nx">RunnableLambda</code><code class="p">.</code><code class="nx">from</code><code class="p">(</code><code class="nx">async</code> <code class="nx">input</code> <code class="o">=&gt;</code> <code class="p">{</code>&#13;
  <code class="c1">// fetch relevant documents</code>&#13;
  <code class="kr">const</code> <code class="nx">docs</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">retriever</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="nx">input</code><code class="p">)</code>&#13;
  <code class="c1">// format prompt</code>&#13;
  <code class="kr">const</code> <code class="nx">formatted</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">prompt</code><code class="p">.</code><code class="nx">invoke</code><code class="p">({</code><code class="nx">context</code><code class="o">:</code> <code class="nx">docs</code><code class="p">,</code> <code class="nx">question</code><code class="o">:</code> <code class="nx">input</code><code class="p">})</code>&#13;
  <code class="c1">// generate answer</code>&#13;
  <code class="kr">const</code> <code class="nx">answer</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">llm</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="nx">formatted</code><code class="p">)</code>&#13;
  <code class="k">return</code> <code class="p">{</code><code class="nx">answer</code><code class="p">,</code> <code class="nx">docs</code><code class="p">}</code>&#13;
<code class="p">})</code></pre>&#13;
&#13;
<p>Congratulations! You’ve now built a basic RAG system to power an AI app for personal use.</p>&#13;
&#13;
<p>However, a production-ready AI app used by multiple users requires a more advanced<a contenteditable="false" data-primary="retrieval-augmented generation (RAG)" data-secondary="building robust RAG systems" data-type="indexterm" id="id564"/> RAG system. In order to build a robust RAG system, we need to answer the following questions effectively:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>How do we handle the variability in the quality of a user’s input?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>How do we route queries to retrieve relevant data from a variety of data sources?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>How do we transform natural language to the query language of the target data source?</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>How do we optimize our indexing process, i.e., embedding, text splitting?</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Next we’ll discuss the latest research-backed strategies to answer these questions and build a production-ready RAG system. These strategies can be summarized in <a data-type="xref" href="#ch03_figure_4_1736545666780656">Figure 3-4</a>.</p>&#13;
&#13;
<figure><div class="figure" id="ch03_figure_4_1736545666780656"><img alt="A diagram of a software development process  Description automatically generated" src="assets/lelc_0304.png"/>&#13;
<h6><span class="label">Figure 3-4. </span>Effective strategies to optimize the accuracy of your RAG system</h6>&#13;
</div></figure>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>All code blocks in the rest of this chapter use the vector store we set up at the beginning of the chapter.<a contenteditable="false" data-primary="" data-startref="JSllmpred03" data-type="indexterm" id="id565"/><a contenteditable="false" data-primary="" data-startref="Pgenerate03" data-type="indexterm" id="id566"/><a contenteditable="false" data-primary="" data-startref="RAGgenerate03" data-type="indexterm" id="id567"/><a contenteditable="false" data-primary="" data-startref="DIrelevant03" data-type="indexterm" id="id568"/><a contenteditable="false" data-primary="" data-startref="LLMrelevant03" data-type="indexterm" id="id569"/></p>&#13;
</div>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Query Transformation" data-type="sect1"><div class="sect1" id="ch03_query_transformation_1736545666794100">&#13;
<h1>Query Transformation</h1>&#13;
&#13;
<p>One<a contenteditable="false" data-primary="retrieval-augmented generation (RAG)" data-secondary="query handling" data-tertiary="transformation" data-type="indexterm" id="RAGqt03"/><a contenteditable="false" data-primary="data queries" data-secondary="query transformation" data-type="indexterm" id="DQtransform03"/> of the major problems with a basic RAG system is that it relies too heavily on the quality of a user’s query to generate an accurate output. In a production setting, a user is likely to construct their query in an incomplete, ambiguous, or poorly worded manner that leads to<a contenteditable="false" data-primary="hallucinations" data-type="indexterm" id="id570"/> model hallucination.</p>&#13;
&#13;
<p><em>Query transformation<a contenteditable="false" data-primary="query transformation" data-secondary="definition of term" data-type="indexterm" id="id571"/></em> is a subset of strategies designed to modify the user’s input to answer the first RAG problem question: How do we handle the variability in the quality of a user’s input? <a data-type="xref" href="#ch03_figure_5_1736545666780682">Figure 3-5</a> illustrates the range of query transformation strategies, ranging from those that make a user’s input more or less abstract in order to generate an accurate LLM output. The next section begins with a middle ground strategy.</p>&#13;
&#13;
<figure><div class="figure" id="ch03_figure_5_1736545666780682"><img alt="A diagram of a question  Description automatically generated" src="assets/lelc_0305.png"/>&#13;
<h6><span class="label">Figure 3-5. </span>Various methods to transform a user’s query based on the abstraction level</h6>&#13;
</div></figure>&#13;
&#13;
&#13;
<section data-pdf-bookmark="Rewrite-Retrieve-Read" data-type="sect2"><div class="sect2" id="ch03_rewrite_retrieve_read_1736545666794173">&#13;
<h2>Rewrite-Retrieve-Read</h2>&#13;
&#13;
<p class="fix_tracking">The<a contenteditable="false" data-primary="Rewrite-Retrieve-Read strategy" data-type="indexterm" id="RRRstrat03"/><a contenteditable="false" data-primary="query transformation" data-secondary="Rewrite-Retrieve-Read strategy" data-type="indexterm" id="QTrrr03"/> Rewrite-Retrieve-Read strategy proposed by a Microsoft Research team simply prompts the LLM to rewrite the user’s query before performing retrieval.<sup><a data-type="noteref" href="ch03.html#id572" id="id572-marker">2</a></sup> To illustrate, let’s return to the chain we built in the previous section, this time invoked with a poorly worded user query:</p>&#13;
&#13;
<p><em>Python</em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="nd">@chain</code>&#13;
<code class="k">def</code> <code class="nf">qa</code><code class="p">(</code><code class="nb">input</code><code class="p">):</code>&#13;
    <code class="c1"># fetch relevant documents </code>&#13;
    <code class="n">docs</code> <code class="o">=</code> <code class="n">retriever</code><code class="o">.</code><code class="n">get_relevant_documents</code><code class="p">(</code><code class="nb">input</code><code class="p">)</code>&#13;
    <code class="c1"># format prompt</code>&#13;
    <code class="n">formatted</code> <code class="o">=</code> <code class="n">prompt</code><code class="o">.</code><code class="n">invoke</code><code class="p">({</code><code class="s2">"context"</code><code class="p">:</code> <code class="n">docs</code><code class="p">,</code> <code class="s2">"question"</code><code class="p">:</code> <code class="nb">input</code><code class="p">})</code>&#13;
    <code class="c1"># generate answer</code>&#13;
    <code class="n">answer</code> <code class="o">=</code> <code class="n">llm</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">formatted</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="n">answer</code>&#13;
&#13;
<code class="n">qa</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="s2">"""Today I woke up and brushed my teeth, then I sat down to read the </code>&#13;
<code class="s2">    news. But then I forgot the food on the cooker. Who are some key figures in </code>&#13;
<code class="s2">    the ancient greek history of philosophy?"""</code><code class="p">)</code></pre>&#13;
&#13;
<p><em>JavaScript</em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">const</code> <code class="nx">qa</code> <code class="o">=</code> <code class="nx">RunnableLambda</code><code class="p">.</code><code class="nx">from</code><code class="p">(</code><code class="nx">async</code> <code class="nx">input</code> <code class="o">=&gt;</code> <code class="p">{</code>&#13;
  <code class="c1">// fetch relevant documents</code>&#13;
  <code class="kr">const</code> <code class="nx">docs</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">retriever</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="nx">input</code><code class="p">)</code>&#13;
  <code class="c1">// format prompt</code>&#13;
  <code class="kr">const</code> <code class="nx">formatted</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">prompt</code><code class="p">.</code><code class="nx">invoke</code><code class="p">({</code><code class="nx">context</code><code class="o">:</code> <code class="nx">docs</code><code class="p">,</code> <code class="nx">question</code><code class="o">:</code> <code class="nx">input</code><code class="p">})</code>&#13;
  <code class="c1">// generate answer</code>&#13;
  <code class="kr">const</code> <code class="nx">answer</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">llm</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="nx">formatted</code><code class="p">)</code>&#13;
  <code class="k">return</code> <code class="nx">answer</code>&#13;
<code class="p">})</code>&#13;
&#13;
<code class="nx">await</code> <code class="nx">qa</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="sb">`Today I woke up and brushed my teeth, then I sat down to read </code>&#13;
<code class="sb">  the news. But then I forgot the food on the cooker. Who are some key figures </code>&#13;
<code class="sb">  in the ancient greek history of philosophy?`</code><code class="p">)</code></pre>&#13;
&#13;
<p><em>The output</em> (remember: if you rerun it, your output might be different from this):</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
Based on the given context, there is no information provided.</pre>&#13;
&#13;
<p>The model failed to answer the question because it was distracted by the irrelevant information provided in the user’s query.</p>&#13;
&#13;
<p>Now let’s implement the Rewrite-Retrieve-Read prompt:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="Rewrite-Retrieve-Read strategy" data-type="indexterm" id="id573"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="n">rewrite_prompt</code> <code class="o">=</code> <code class="n">ChatPromptTemplate</code><code class="o">.</code><code class="n">from_template</code><code class="p">(</code><code class="s2">"""Provide a better search </code>&#13;
<code class="s2">    query for web search engine to answer the given question, end the queries </code>&#13;
<code class="s2">    with ’**’. Question: </code><code class="si">{x}</code><code class="s2"> Answer:"""</code><code class="p">)</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">parse_rewriter_output</code><code class="p">(</code><code class="n">message</code><code class="p">):</code>&#13;
    <code class="k">return</code> <code class="n">message</code><code class="o">.</code><code class="n">content</code><code class="o">.</code><code class="n">strip</code><code class="p">(</code><code class="s1">'"'</code><code class="p">)</code><code class="o">.</code><code class="n">strip</code><code class="p">(</code><code class="s2">"**"</code><code class="p">)</code>&#13;
&#13;
<code class="n">rewriter</code> <code class="o">=</code> <code class="n">rewrite_prompt</code> <code class="o">|</code> <code class="n">llm</code> <code class="o">|</code> <code class="n">parse_rewriter_output</code>&#13;
&#13;
<code class="nd">@chain</code>&#13;
<code class="k">def</code> <code class="nf">qa_rrr</code><code class="p">(</code><code class="nb">input</code><code class="p">):</code>&#13;
    <code class="c1"># rewrite the query</code>&#13;
    <code class="n">new_query</code> <code class="o">=</code> <code class="n">rewriter</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="nb">input</code><code class="p">)</code>&#13;
    <code class="c1"># fetch relevant documents </code>&#13;
    <code class="n">docs</code> <code class="o">=</code> <code class="n">retriever</code><code class="o">.</code><code class="n">get_relevant_documents</code><code class="p">(</code><code class="n">new_query</code><code class="p">)</code>&#13;
    <code class="c1"># format prompt</code>&#13;
    <code class="n">formatted</code> <code class="o">=</code> <code class="n">prompt</code><code class="o">.</code><code class="n">invoke</code><code class="p">({</code><code class="s2">"context"</code><code class="p">:</code> <code class="n">docs</code><code class="p">,</code> <code class="s2">"question"</code><code class="p">:</code> <code class="nb">input</code><code class="p">})</code>&#13;
    <code class="c1"># generate answer</code>&#13;
    <code class="n">answer</code> <code class="o">=</code> <code class="n">llm</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">formatted</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="n">answer</code>&#13;
&#13;
<code class="c1"># run</code>&#13;
<code class="n">qa_rrr</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="s2">"""Today I woke up and brushed my teeth, then I sat down to read </code>&#13;
<code class="s2">    the news. But then I forgot the food on the cooker. Who are some key </code>&#13;
<code class="s2">    figures in the ancient greek history of philosophy?"""</code><code class="p">)</code></pre>&#13;
&#13;
<p><em>JavaScript<a contenteditable="false" data-primary="JavaScript" data-secondary="Rewrite-Retrieve-Read strategy" data-type="indexterm" id="id574"/></em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">const</code> <code class="nx">rewritePrompt</code> <code class="o">=</code> <code class="nx">ChatPromptTemplate</code><code class="p">.</code><code class="nx">fromTemplate</code><code class="p">(</code><code class="sb">`Provide a better search </code>&#13;
<code class="sb">  query for web search engine to answer the given question, end the queries </code>&#13;
<code class="sb">  with ’**’. Question: {question} Answer:`</code><code class="p">)</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">rewriter</code> <code class="o">=</code> <code class="nx">rewritePrompt</code><code class="p">.</code><code class="nx">pipe</code><code class="p">(</code><code class="nx">llm</code><code class="p">).</code><code class="nx">pipe</code><code class="p">(</code><code class="nx">message</code> <code class="o">=&gt;</code> <code class="p">{</code>&#13;
  <code class="k">return</code> <code class="nx">message</code><code class="p">.</code><code class="nx">content</code><code class="p">.</code><code class="nx">replaceAll</code><code class="p">(</code><code class="s1">'"'</code><code class="p">,</code> <code class="s1">''</code><code class="p">).</code><code class="nx">replaceAll</code><code class="p">(</code><code class="s1">'**'</code><code class="p">)</code>&#13;
<code class="p">})</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">qa</code> <code class="o">=</code> <code class="nx">RunnableLambda</code><code class="p">.</code><code class="nx">from</code><code class="p">(</code><code class="nx">async</code> <code class="nx">input</code> <code class="o">=&gt;</code> <code class="p">{</code>&#13;
  <code class="kr">const</code> <code class="nx">newQuery</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">rewriter</code><code class="p">.</code><code class="nx">invoke</code><code class="p">({</code><code class="nx">question</code><code class="o">:</code> <code class="nx">input</code><code class="p">});</code>&#13;
  <code class="c1">// fetch relevant documents</code>&#13;
  <code class="kr">const</code> <code class="nx">docs</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">retriever</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="nx">newQuery</code><code class="p">)</code>&#13;
  <code class="c1">// format prompt</code>&#13;
  <code class="kr">const</code> <code class="nx">formatted</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">prompt</code><code class="p">.</code><code class="nx">invoke</code><code class="p">({</code><code class="nx">context</code><code class="o">:</code> <code class="nx">docs</code><code class="p">,</code> <code class="nx">question</code><code class="o">:</code> <code class="nx">input</code><code class="p">})</code>&#13;
  <code class="c1">// generate answer</code>&#13;
  <code class="kr">const</code> <code class="nx">answer</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">llm</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="nx">formatted</code><code class="p">)</code>&#13;
  <code class="k">return</code> <code class="nx">answer</code>&#13;
<code class="p">})</code>&#13;
&#13;
<code class="nx">await</code> <code class="nx">qa</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="sb">`Today I woke up and brushed my teeth, then I sat down to read </code>&#13;
<code class="sb">  the news. But then I forgot the food on the cooker. Who are some key </code>&#13;
<code class="sb">  figures in the ancient greek history of philosophy?`</code><code class="p">)</code></pre>&#13;
&#13;
<p><em>The output:</em></p>&#13;
&#13;
<pre data-type="programlisting">Based on the given context, some key figures in the ancient greek history of &#13;
philosophy include: Themistocles (an Athenian statesman), Pythagoras, and Plato.</pre>&#13;
&#13;
<p>Notice that we have had an LLM rewrite the user’s initial distracted query into a much clearer one, and it is that more focused query that is passed to the retriever to fetch the most relevant documents. Note: this technique can be used with any retrieval method, be that a vector store such as we have here or, for instance, a web search tool. The downside of this approach is that it introduces additional latency into your chain, because now we need to perform two LLM calls in sequence.<a contenteditable="false" data-primary="" data-startref="RRRstrat03" data-type="indexterm" id="id575"/><a contenteditable="false" data-primary="" data-startref="QTrrr03" data-type="indexterm" id="id576"/></p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Multi-Query Retrieval" data-type="sect2"><div class="sect2" id="ch03_multi_query_retrieval_1736545666794235">&#13;
<h2>Multi-Query Retrieval</h2>&#13;
&#13;
<p>A<a contenteditable="false" data-primary="query transformation" data-secondary="multi-query retrieval strategy" data-type="indexterm" id="QTmulti03"/><a contenteditable="false" data-primary="multi-query retrieval strategy" data-type="indexterm" id="multistrat03"/> user’s single query can be insufficient to capture the full scope of information required to answer the query comprehensively. The multi-query retrieval strategy resolves this problem by instructing an LLM to generate multiple queries based on a user’s initial query, executing a parallel retrieval of each query from the data source and then inserting the retrieved results as prompt context to generate a final model output. <a data-type="xref" href="#ch03_figure_6_1736545666780704">Figure 3-6</a> illustrates.</p>&#13;
&#13;
<figure><div class="figure" id="ch03_figure_6_1736545666780704"><img alt="A diagram of a diagram of a document  Description automatically generated with medium confidence" src="assets/lelc_0306.png"/>&#13;
<h6><span class="label">Figure 3-6. </span>Demonstration of the multi-query retrieval strategy</h6>&#13;
</div></figure>&#13;
&#13;
<p>This strategy is particularly useful for use cases where a single question may rely on multiple perspectives to provide a comprehensive answer.</p>&#13;
&#13;
<p>Here’s a code example of multi-query retrieval in action:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="multi-query retrieval in" data-type="indexterm" id="Pmulti03"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">langchain.prompts</code> <code class="kn">import</code> <code class="n">ChatPromptTemplate</code>&#13;
&#13;
<code class="n">perspectives_prompt</code> <code class="o">=</code> <code class="n">ChatPromptTemplate</code><code class="o">.</code><code class="n">from_template</code><code class="p">(</code><code class="s2">"""You are an AI language </code>&#13;
<code class="s2">    model assistant. Your task is to generate five different versions of the </code>&#13;
<code class="s2">    given user question to retrieve relevant documents from a vector database. </code>&#13;
<code class="s2">    By generating multiple perspectives on the user question, your goal is to </code>&#13;
<code class="s2">    help the user overcome some of the limitations of the distance-based </code>&#13;
<code class="s2">    similarity search. Provide these alternative questions separated by </code>&#13;
<code class="s2">    newlines. Original question: </code><code class="si">{question}</code><code class="s2">"""</code><code class="p">)</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">parse_queries_output</code><code class="p">(</code><code class="n">message</code><code class="p">):</code>&#13;
    <code class="k">return</code> <code class="n">message</code><code class="o">.</code><code class="n">content</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="s1">'</code><code class="se">\n</code><code class="s1">'</code><code class="p">)</code>&#13;
&#13;
<code class="n">query_gen</code> <code class="o">=</code> <code class="n">perspectives_prompt</code> <code class="o">|</code> <code class="n">llm</code> <code class="o">|</code> <code class="n">parse_queries_output</code></pre>&#13;
&#13;
<p class="pagebreak-before less_space"><em>JavaScript<a contenteditable="false" data-primary="JavaScript" data-secondary="multi-query retrieval in" data-type="indexterm" id="JSmulti03"/></em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">const</code> <code class="nx">perspectivesPrompt</code> <code class="o">=</code> <code class="nx">ChatPromptTemplate</code><code class="p">.</code><code class="nx">fromTemplate</code><code class="p">(</code><code class="sb">`You are an AI </code>&#13;
<code class="sb">  language model assistant. Your task is to generate five different versions </code>&#13;
<code class="sb">  of the given user question to retrieve relevant documents from a vector </code>&#13;
<code class="sb">  database. By generating multiple perspectives on the user question, your </code>&#13;
<code class="sb">  goal is to help the user overcome some of the limitations of the </code>&#13;
<code class="sb">  distance-based similarity search. Provide these alternative questions </code>&#13;
<code class="sb">  separated by newlines. Original question: {question}`</code><code class="p">)</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">queryGen</code> <code class="o">=</code> <code class="nx">perspectivesPrompt</code><code class="p">.</code><code class="nx">pipe</code><code class="p">(</code><code class="nx">llm</code><code class="p">).</code><code class="nx">pipe</code><code class="p">(</code><code class="nx">message</code> <code class="o">=&gt;</code> <code class="p">{</code>&#13;
  <code class="k">return</code> <code class="nx">message</code><code class="p">.</code><code class="nx">content</code><code class="p">.</code><code class="nx">split</code><code class="p">(</code><code class="s1">'\n'</code><code class="p">)</code>&#13;
<code class="p">})</code></pre>&#13;
&#13;
<p>Note that the prompt template is designed to generate variations of questions based on the user’s initial query.</p>&#13;
&#13;
<p>Next we take the list of generated queries, retrieve the most relevant docs for each of them in parallel, and then combine to get the unique union of all the retrieved relevant documents:</p>&#13;
&#13;
<p><em>Python</em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="k">def</code> <code class="nf">get_unique_union</code><code class="p">(</code><code class="n">document_lists</code><code class="p">):</code>&#13;
    <code class="c1"># Flatten list of lists, and dedupe them</code>&#13;
    <code class="n">deduped_docs</code> <code class="o">=</code> <code class="p">{</code>&#13;
        <code class="n">doc</code><code class="o">.</code><code class="n">page_content</code><code class="p">:</code> <code class="n">doc</code>&#13;
        <code class="k">for</code> <code class="n">sublist</code> <code class="ow">in</code> <code class="n">document_lists</code> <code class="k">for</code> <code class="n">doc</code> <code class="ow">in</code> <code class="n">sublist</code>&#13;
    <code class="p">}</code>&#13;
    <code class="c1"># return a flat list of unique docs</code>&#13;
    <code class="k">return</code> <code class="nb">list</code><code class="p">(</code><code class="n">deduped_docs</code><code class="o">.</code><code class="n">values</code><code class="p">())</code>&#13;
&#13;
<code class="n">retrieval_chain</code> <code class="o">=</code> <code class="n">query_gen</code> <code class="o">|</code> <code class="n">retriever</code><code class="o">.</code><code class="n">batch</code> <code class="o">|</code> <code class="n">get_unique_union</code></pre>&#13;
&#13;
<p><em>JavaScript</em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">const</code> <code class="nx">retrievalChain</code> <code class="o">=</code> <code class="nx">queryGen</code>&#13;
  <code class="p">.</code><code class="nx">pipe</code><code class="p">(</code><code class="nx">retriever</code><code class="p">.</code><code class="nx">batch</code><code class="p">.</code><code class="nx">bind</code><code class="p">(</code><code class="nx">retriever</code><code class="p">))</code>&#13;
  <code class="p">.</code><code class="nx">pipe</code><code class="p">(</code><code class="nx">documentLists</code> <code class="o">=&gt;</code> <code class="p">{</code>&#13;
    <code class="kr">const</code> <code class="nx">dedupedDocs</code> <code class="o">=</code> <code class="p">{}</code>&#13;
    <code class="nx">documentLists</code><code class="p">.</code><code class="nx">flat</code><code class="p">().</code><code class="nx">forEach</code><code class="p">(</code><code class="nx">doc</code> <code class="o">=&gt;</code> <code class="p">{</code>&#13;
      <code class="nx">dedupedDocs</code><code class="p">[</code><code class="nx">doc</code><code class="p">.</code><code class="nx">pageContent</code><code class="p">]</code> <code class="o">=</code> <code class="nx">doc</code>&#13;
    <code class="p">})</code>&#13;
    <code class="k">return</code> <code class="nb">Object</code><code class="p">.</code><code class="nx">values</code><code class="p">(</code><code class="nx">dedupedDocs</code><code class="p">)</code>&#13;
  <code class="p">})</code></pre>&#13;
&#13;
<p class="fix_tracking">Because we’re retrieving documents from the same retriever with multiple (related) queries, it’s likely at least some of them are repeated. Before using them as context to answer the question, we need to deduplicate them, to end up with a single instance of each. Here we dedupe docs by using their content (a string) as the key in a dictionary (or object in JS), because a dictionary can only contain one entry for each key. After we’ve iterated through all docs, we simply get all the dictionary values, which is now free of duplicates.</p>&#13;
&#13;
<p>Notice our use as well of <code>.batch</code>, which runs all generated queries in parallel and returns a list of the results—in this case, a list of lists of documents, which we then flatten and dedupe as described earlier.</p>&#13;
&#13;
<p>This final step is to construct a prompt, including the user’s question and combined retrieved relevant documents, and a model interface to generate the prediction:</p>&#13;
&#13;
<p><em>Python</em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="n">prompt</code> <code class="o">=</code> <code class="n">ChatPromptTemplate</code><code class="o">.</code><code class="n">from_template</code><code class="p">(</code><code class="s2">"""Answer the following question based </code>&#13;
<code class="s2">    on this context:</code>&#13;
&#13;
<code class="si">{context}</code><code class="s2"/>&#13;
&#13;
<code class="s2">Question: </code><code class="si">{question}</code><code class="s2"/>&#13;
<code class="s2">"""</code><code class="p">)</code>&#13;
&#13;
<code class="nd">@chain</code>&#13;
<code class="k">def</code> <code class="nf">multi_query_qa</code><code class="p">(</code><code class="nb">input</code><code class="p">):</code>&#13;
    <code class="c1"># fetch relevant documents </code>&#13;
    <code class="n">docs</code> <code class="o">=</code> <code class="n">retrieval_chain</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="nb">input</code><code class="p">)</code>&#13;
    <code class="c1"># format prompt</code>&#13;
    <code class="n">formatted</code> <code class="o">=</code> <code class="n">prompt</code><code class="o">.</code><code class="n">invoke</code><code class="p">({</code><code class="s2">"context"</code><code class="p">:</code> <code class="n">docs</code><code class="p">,</code> <code class="s2">"question"</code><code class="p">:</code> <code class="nb">input</code><code class="p">})</code>&#13;
    <code class="c1"># generate answer</code>&#13;
    <code class="n">answer</code> <code class="o">=</code> <code class="n">llm</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">formatted</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="n">answer</code>&#13;
&#13;
<code class="c1"># run</code>&#13;
<code class="n">multi_query_qa</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="s2">"""Who are some key figures in the ancient greek history </code>&#13;
<code class="s2">    of philosophy?"""</code><code class="p">)</code></pre>&#13;
&#13;
<p><em>JavaScript</em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">const</code> <code class="nx">prompt</code> <code class="o">=</code> <code class="nx">ChatPromptTemplate</code><code class="p">.</code><code class="nx">fromTemplate</code><code class="p">(</code><code class="sb">`Answer the following </code>&#13;
<code class="sb">  question based on this context:</code>&#13;
&#13;
<code class="sb">{context}</code>&#13;
&#13;
<code class="sb">Question: {question}</code>&#13;
<code class="sb">`</code><code class="p">)</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">multiQueryQa</code> <code class="o">=</code> <code class="nx">RunnableLambda</code><code class="p">.</code><code class="nx">from</code><code class="p">(</code><code class="nx">async</code> <code class="nx">input</code> <code class="o">=&gt;</code> <code class="p">{</code>&#13;
  <code class="c1">// fetch relevant documents</code>&#13;
  <code class="kr">const</code> <code class="nx">docs</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">retrievalChain</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="nx">input</code><code class="p">)</code>&#13;
  <code class="c1">// format prompt</code>&#13;
  <code class="kr">const</code> <code class="nx">formatted</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">prompt</code><code class="p">.</code><code class="nx">invoke</code><code class="p">({</code><code class="nx">context</code><code class="o">:</code> <code class="nx">docs</code><code class="p">,</code> <code class="nx">question</code><code class="o">:</code> <code class="nx">input</code><code class="p">})</code>&#13;
  <code class="c1">// generate answer</code>&#13;
  <code class="kr">const</code> <code class="nx">answer</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">llm</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="nx">formatted</code><code class="p">)</code>&#13;
  <code class="k">return</code> <code class="nx">answer</code>&#13;
<code class="p">})</code>&#13;
&#13;
<code class="nx">await</code> <code class="nx">multiQueryQa</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="sb">`Who are some key figures in the ancient greek </code>&#13;
<code class="sb">  history of philosophy?`</code><code class="p">)</code></pre>&#13;
&#13;
<p>Notice how this isn’t that different from our previous QA chains, as all the new logic for multi-query retrieval is contained in <code>retrieval_chain</code>. This is key to making good use of these techniques—implementing each technique as a standalone chain (in this case, <code>retrieval_chain</code>), which makes it easy to adopt them and even to combine them.<a contenteditable="false" data-primary="" data-startref="QTmulti03" data-type="indexterm" id="id577"/><a contenteditable="false" data-primary="" data-startref="multistrat03" data-type="indexterm" id="id578"/><a contenteditable="false" data-primary="" data-startref="Pmulti03" data-type="indexterm" id="id579"/><a contenteditable="false" data-primary="" data-startref="JSmulti03" data-type="indexterm" id="id580"/></p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="RAG-Fusion" data-type="sect2"><div class="sect2" id="ch03_rag_fusion_1736545666794295">&#13;
<h2>RAG-Fusion</h2>&#13;
&#13;
<p>The<a contenteditable="false" data-primary="query transformation" data-secondary="RAG-Fusion strategy" data-type="indexterm" id="QTragfusion03"/><a contenteditable="false" data-primary="RAG-Fusion strategy" data-type="indexterm" id="ragfusion03"/><a contenteditable="false" data-primary="reciprocal rank fusion (RRF) algorithm" data-type="indexterm" id="rrfalg03"/><a contenteditable="false" data-primary="RRF (reciprocal rank fusion) algorithm" data-type="indexterm" id="id581"/> RAG-Fusion strategy shares similarities with the multi-query retrieval strategy, except we will apply a final reranking step to all the retrieved documents.<sup><a data-type="noteref" href="ch03.html#id582" id="id582-marker">3</a></sup> This reranking step makes use of the <em>reciprocal rank fusion</em> (RRF) algorithm, which involves combining the ranks of different search results to produce a single, unified ranking. By combining ranks from different queries, we pull the most relevant documents to the top of the final list. RRF is well-suited for combining results from queries that might have different scales or distributions of scores.</p>&#13;
&#13;
<p>Let’s demonstrate RAG-Fusion in code. First, we craft a prompt similar to the multi-query retrieval strategy to generate a list of queries based on the user query:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="RAG-Fusion strategy" data-type="indexterm" id="Pragfus03"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">langchain.prompts</code> <code class="kn">import</code> <code class="n">ChatPromptTemplate</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_openai</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code>&#13;
&#13;
<code class="n">prompt_rag_fusion</code> <code class="o">=</code> <code class="n">ChatPromptTemplate</code><code class="o">.</code><code class="n">from_template</code><code class="p">(</code><code class="s2">"""You are a helpful </code>&#13;
<code class="s2">    assistant that generates multiple search queries based on a single input </code>&#13;
<code class="s2">    query. </code><code class="se">\n</code><code class="s2"/>&#13;
<code class="s2">    Generate multiple search queries related to: </code><code class="si">{question}</code><code class="s2"> </code><code class="se">\n</code><code class="s2"/>&#13;
<code class="s2">    Output (4 queries):"""</code><code class="p">)</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">parse_queries_output</code><code class="p">(</code><code class="n">message</code><code class="p">):</code>&#13;
    <code class="k">return</code> <code class="n">message</code><code class="o">.</code><code class="n">content</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="s1">'</code><code class="se">\n</code><code class="s1">'</code><code class="p">)</code>&#13;
&#13;
<code class="n">llm</code> <code class="o">=</code> <code class="n">ChatOpenAI</code><code class="p">(</code><code class="n">temperature</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>&#13;
&#13;
<code class="n">query_gen</code> <code class="o">=</code> <code class="n">prompt_rag_fusion</code> <code class="o">|</code> <code class="n">llm</code> <code class="o">|</code> <code class="n">parse_queries_output</code></pre>&#13;
&#13;
<p><em>JavaScript<a contenteditable="false" data-primary="JavaScript" data-secondary="RAG-Fusion strategy" data-type="indexterm" id="JSrag03"/></em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">import</code> <code class="p">{</code><code class="nx">ChatPromptTemplate</code><code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/core/prompts'</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code><code class="nx">ChatOpenAI</code><code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/openai'</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code><code class="nx">RunnableLambda</code><code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/core/runnables'</code><code class="p">;</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">perspectivesPrompt</code> <code class="o">=</code> <code class="nx">ChatPromptTemplate</code><code class="p">.</code><code class="nx">fromTemplate</code><code class="p">(</code><code class="sb">`You are a helpful </code>&#13;
<code class="sb">  assistant that generates multiple search queries based on a single input </code>&#13;
<code class="sb">  query. </code><code class="err">\</code><code class="sb">n</code>&#13;
<code class="sb">  Generate multiple search queries related to: {question} </code><code class="err">\</code><code class="sb">n</code>&#13;
<code class="sb">  Output (4 queries):`</code><code class="p">)</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">queryGen</code> <code class="o">=</code> <code class="nx">perspectivesPrompt</code><code class="p">.</code><code class="nx">pipe</code><code class="p">(</code><code class="nx">llm</code><code class="p">).</code><code class="nx">pipe</code><code class="p">(</code><code class="nx">message</code> <code class="o">=&gt;</code> <code class="p">{</code>&#13;
  <code class="k">return</code> <code class="nx">message</code><code class="p">.</code><code class="nx">content</code><code class="p">.</code><code class="nx">split</code><code class="p">(</code><code class="s1">'\n'</code><code class="p">)</code>&#13;
<code class="p">})</code></pre>&#13;
&#13;
<p>Once we’ve generated our queries, we fetch relevant documents for each query and pass them into a function to<a contenteditable="false" data-primary="reranking/reordering" data-type="indexterm" id="id583"/><a contenteditable="false" data-primary="documents" data-secondary="reranking/reordering" data-type="indexterm" id="id584"/> <em>rerank</em> (that is, <em>reorder</em> according to relevancy) the final list of relevant documents.</p>&#13;
&#13;
<p>The function <code>reciprocal_rank_fusion</code> takes a list of the search results of each query, so a list of lists of documents, where each inner list of documents is sorted by their relevance to that query. The RRF algorithm then calculates a new score for each document based on its ranks (or positions) in the different lists and sorts them to create a final reranked list.</p>&#13;
&#13;
<p>After calculating the fused scores, the function sorts the documents in descending order of these scores to get the final reranked list, which is then returned:</p>&#13;
&#13;
<p><em>Python</em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="k">def</code> <code class="nf">reciprocal_rank_fusion</code><code class="p">(</code><code class="n">results</code><code class="p">:</code> <code class="nb">list</code><code class="p">[</code><code class="nb">list</code><code class="p">],</code> <code class="n">k</code><code class="o">=</code><code class="mi">60</code><code class="p">):</code>&#13;
    <code class="sd">"""reciprocal rank fusion on multiple lists of ranked documents </code>&#13;
<code class="sd">       and an optional parameter k used in the RRF formula</code>&#13;
<code class="sd">    """</code>&#13;
    &#13;
    <code class="c1"># Initialize a dictionary to hold fused scores for each document</code>&#13;
    <code class="c1"># Documents will be keyed by their contents to ensure uniqueness</code>&#13;
    <code class="n">fused_scores</code> <code class="o">=</code> <code class="p">{}</code>&#13;
    <code class="n">documents</code> <code class="o">=</code> <code class="p">{}</code>&#13;
&#13;
    <code class="c1"># Iterate through each list of ranked documents</code>&#13;
    <code class="k">for</code> <code class="n">docs</code> <code class="ow">in</code> <code class="n">results</code><code class="p">:</code>&#13;
        <code class="c1"># Iterate through each document in the list,</code>&#13;
        <code class="c1"># with its rank (position in the list)</code>&#13;
        <code class="k">for</code> <code class="n">rank</code><code class="p">,</code> <code class="n">doc</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">docs</code><code class="p">):</code>&#13;
            <code class="c1"># Use the document contents as the key for uniqueness</code>&#13;
            <code class="n">doc_str</code> <code class="o">=</code> <code class="n">doc</code><code class="o">.</code><code class="n">page_content</code>&#13;
            <code class="c1"># If the document hasn't been seen yet,</code>&#13;
            <code class="c1"># - initialize score to 0</code>&#13;
            <code class="c1"># - save it for later</code>&#13;
            <code class="k">if</code> <code class="n">doc_str</code> <code class="ow">not</code> <code class="ow">in</code> <code class="n">fused_scores</code><code class="p">:</code>&#13;
                <code class="n">fused_scores</code><code class="p">[</code><code class="n">doc_str</code><code class="p">]</code> <code class="o">=</code> <code class="mi">0</code>&#13;
                <code class="n">documents</code><code class="p">[</code><code class="n">doc_str</code><code class="p">]</code> <code class="o">=</code> <code class="n">doc</code>&#13;
            <code class="c1"># Update the score of the document using the RRF formula:</code>&#13;
            <code class="c1"># 1 / (rank + k)</code>&#13;
            <code class="n">fused_scores</code><code class="p">[</code><code class="n">doc_str</code><code class="p">]</code> <code class="o">+=</code> <code class="mi">1</code> <code class="o">/</code> <code class="p">(</code><code class="n">rank</code> <code class="o">+</code> <code class="n">k</code><code class="p">)</code>&#13;
&#13;
    <code class="c1"># Sort the documents based on their fused scores in descending order </code>&#13;
    <code class="c1"># to get the final reranked results</code>&#13;
    <code class="n">reranked_doc_strs</code> <code class="o">=</code> <code class="nb">sorted</code><code class="p">(</code>&#13;
        <code class="n">fused_scores</code><code class="p">,</code> <code class="n">key</code><code class="o">=</code><code class="k">lambda</code> <code class="n">d</code><code class="p">:</code> <code class="n">fused_scores</code><code class="p">[</code><code class="n">d</code><code class="p">],</code> <code class="n">reverse</code><code class="o">=</code><code class="kc">True</code>&#13;
    <code class="p">)</code>&#13;
    <code class="c1"># retrieve the corresponding doc for each doc_str</code>&#13;
    <code class="k">return</code> <code class="p">[</code>&#13;
        <code class="n">documents</code><code class="p">[</code><code class="n">doc_str</code><code class="p">]</code>&#13;
        <code class="k">for</code> <code class="n">doc_str</code> <code class="ow">in</code> <code class="n">reranked_doc_strs</code>&#13;
    <code class="p">]</code>&#13;
&#13;
<code class="n">retrieval_chain</code> <code class="o">=</code> <code class="n">generate_queries</code> <code class="o">|</code> <code class="n">retriever</code><code class="o">.</code><code class="n">batch</code> <code class="o">|</code> <code class="n">reciprocal_rank_fusion</code></pre>&#13;
&#13;
<p><em>JavaScript</em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kd">function</code> <code class="nx">reciprocalRankFusion</code><code class="p">(</code><code class="nx">results</code><code class="p">,</code> <code class="nx">k</code> <code class="o">=</code> <code class="mi">60</code><code class="p">)</code> <code class="p">{</code>&#13;
  <code class="c1">// Initialize a dictionary to hold fused scores for each document</code>&#13;
  <code class="c1">// Documents will be keyed by their contents to ensure uniqueness</code>&#13;
  <code class="kr">const</code> <code class="nx">fusedScores</code> <code class="o">=</code> <code class="p">{}</code>&#13;
  <code class="kr">const</code> <code class="nx">documents</code> <code class="o">=</code> <code class="p">{}</code>&#13;
&#13;
  <code class="nx">results</code><code class="p">.</code><code class="nx">forEach</code><code class="p">(</code><code class="nx">docs</code> <code class="o">=&gt;</code> <code class="p">{</code>&#13;
    <code class="nx">docs</code><code class="p">.</code><code class="nx">forEach</code><code class="p">((</code><code class="nx">doc</code><code class="p">,</code> <code class="nx">rank</code><code class="p">)</code> <code class="o">=&gt;</code> <code class="p">{</code>&#13;
      <code class="c1">// Use the document contents as the key for uniqueness</code>&#13;
      <code class="kr">const</code> <code class="nx">key</code> <code class="o">=</code> <code class="nx">doc</code><code class="p">.</code><code class="nx">pageContent</code>&#13;
      <code class="c1">// If the document hasn't been seen yet,</code>&#13;
      <code class="c1">// - initialize score to 0</code>&#13;
      <code class="c1">// - save it for later</code>&#13;
      <code class="k">if</code> <code class="p">(</code><code class="o">!</code><code class="p">(</code><code class="nx">key</code> <code class="k">in</code> <code class="nx">fusedScores</code><code class="p">))</code> <code class="p">{</code>&#13;
        <code class="nx">fusedScores</code><code class="p">[</code><code class="nx">key</code><code class="p">]</code> <code class="o">=</code> <code class="mi">0</code>&#13;
        <code class="nx">documents</code><code class="p">[</code><code class="nx">key</code><code class="p">]</code> <code class="o">=</code> <code class="mi">0</code>&#13;
      <code class="p">}</code>&#13;
      <code class="c1">// Update the score of the document using the RRF formula:</code>&#13;
      <code class="c1">// 1 / (rank + k)</code>&#13;
      <code class="nx">fusedScores</code><code class="p">[</code><code class="nx">key</code><code class="p">]</code> <code class="o">+=</code> <code class="mi">1</code> <code class="o">/</code> <code class="p">(</code><code class="nx">rank</code> <code class="o">+</code> <code class="nx">k</code><code class="p">)</code>&#13;
    <code class="p">})</code>&#13;
  <code class="p">})</code>&#13;
&#13;
  <code class="c1">// Sort the documents based on their fused scores in descending order </code>&#13;
  <code class="c1">// to get the final reranked results</code>&#13;
  <code class="kr">const</code> <code class="nx">sorted</code> <code class="o">=</code> <code class="nb">Object</code><code class="p">.</code><code class="nx">entries</code><code class="p">(</code><code class="nx">fusedScores</code><code class="p">).</code><code class="nx">sort</code><code class="p">((</code><code class="nx">a</code><code class="p">,</code> <code class="nx">b</code><code class="p">)</code> <code class="o">=&gt;</code> <code class="nx">b</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code> <code class="o">-</code> <code class="nx">a</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code>&#13;
  <code class="c1">// retrieve the corresponding doc for each key</code>&#13;
  <code class="k">return</code> <code class="nx">sorted</code><code class="p">.</code><code class="nx">map</code><code class="p">(([</code><code class="nx">key</code><code class="p">])</code> <code class="o">=&gt;</code> <code class="nx">documents</code><code class="p">[</code><code class="nx">key</code><code class="p">])</code>&#13;
<code class="p">}</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">retrievalChain</code> <code class="o">=</code> <code class="nx">queryGen</code>&#13;
  <code class="p">.</code><code class="nx">pipe</code><code class="p">(</code><code class="nx">retriever</code><code class="p">.</code><code class="nx">batch</code><code class="p">.</code><code class="nx">bind</code><code class="p">(</code><code class="nx">retriever</code><code class="p">))</code>&#13;
  <code class="p">.</code><code class="nx">pipe</code><code class="p">(</code><code class="nx">reciprocalRankFusion</code><code class="p">)</code></pre>&#13;
&#13;
<p class="pagebreak-before less_space">Notice that the function also takes a<a contenteditable="false" data-primary="k parameter" data-type="indexterm" id="id585"/><a contenteditable="false" data-primary="parameters" data-secondary="k parameter" data-type="indexterm" id="id586"/> <code>k</code> parameter, which determines how much influence documents in each query’s result sets have over the final list of documents. A higher value indicates that lower-ranked documents have more influence.</p>&#13;
&#13;
<p>Finally, we combine our new retrieval chain (now using RRF) with the full chain we’ve seen before:</p>&#13;
&#13;
<p><em>Python</em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="n">prompt</code> <code class="o">=</code> <code class="n">ChatPromptTemplate</code><code class="o">.</code><code class="n">from_template</code><code class="p">(</code><code class="s2">"""Answer the following question based </code>&#13;
<code class="s2">    on this context:</code>&#13;
&#13;
<code class="si">{context}</code><code class="s2"/>&#13;
&#13;
<code class="s2">Question: </code><code class="si">{question}</code><code class="s2"/>&#13;
<code class="s2">"""</code><code class="p">)</code>&#13;
&#13;
<code class="n">llm</code> <code class="o">=</code> <code class="n">ChatOpenAI</code><code class="p">(</code><code class="n">temperature</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>&#13;
&#13;
<code class="nd">@chain</code>&#13;
<code class="k">def</code> <code class="nf">multi_query_qa</code><code class="p">(</code><code class="nb">input</code><code class="p">):</code>&#13;
    <code class="c1"># fetch relevant documents </code>&#13;
    <code class="n">docs</code> <code class="o">=</code> <code class="n">retrieval_chain</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="nb">input</code><code class="p">)</code>&#13;
    <code class="c1"># format prompt</code>&#13;
    <code class="n">formatted</code> <code class="o">=</code> <code class="n">prompt</code><code class="o">.</code><code class="n">invoke</code><code class="p">({</code><code class="s2">"context"</code><code class="p">:</code> <code class="n">docs</code><code class="p">,</code> <code class="s2">"question"</code><code class="p">:</code> <code class="nb">input</code><code class="p">})</code>&#13;
    <code class="c1"># generate answer</code>&#13;
    <code class="n">answer</code> <code class="o">=</code> <code class="n">llm</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">formatted</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="n">answer</code>&#13;
&#13;
<code class="n">multi_query_qa</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="s2">"""Who are some key figures in the ancient greek history </code>&#13;
<code class="s2">    of philosophy?"""</code><code class="p">)</code></pre>&#13;
&#13;
<p><em>JavaScript</em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">const</code> <code class="nx">rewritePrompt</code> <code class="o">=</code> <code class="nx">ChatPromptTemplate</code><code class="p">.</code><code class="nx">fromTemplate</code><code class="p">(</code><code class="sb">`Answer the following </code>&#13;
<code class="sb">  question based on this context:</code>&#13;
&#13;
<code class="sb">{context}</code>&#13;
&#13;
<code class="sb">Question: {question}</code>&#13;
<code class="sb">`</code><code class="p">)</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">llm</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">ChatOpenAI</code><code class="p">({</code><code class="nx">temperature</code><code class="o">:</code> <code class="mi">0</code><code class="p">})</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">multiQueryQa</code> <code class="o">=</code> <code class="nx">RunnableLambda</code><code class="p">.</code><code class="nx">from</code><code class="p">(</code><code class="nx">async</code> <code class="nx">input</code> <code class="o">=&gt;</code> <code class="p">{</code>&#13;
  <code class="c1">// fetch relevant documents</code>&#13;
  <code class="kr">const</code> <code class="nx">docs</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">retrievalChain</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="nx">input</code><code class="p">)</code>&#13;
  <code class="c1">// format prompt</code>&#13;
  <code class="kr">const</code> <code class="nx">formatted</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">prompt</code><code class="p">.</code><code class="nx">invoke</code><code class="p">({</code><code class="nx">context</code><code class="o">:</code> <code class="nx">docs</code><code class="p">,</code> <code class="nx">question</code><code class="o">:</code> <code class="nx">input</code><code class="p">})</code>&#13;
  <code class="c1">// generate answer</code>&#13;
  <code class="kr">const</code> <code class="nx">answer</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">llm</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="nx">formatted</code><code class="p">)</code>&#13;
  <code class="k">return</code> <code class="nx">answer</code>&#13;
<code class="p">})</code>&#13;
&#13;
<code class="nx">await</code> <code class="nx">multiQueryQa</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="sb">`Who are some key figures in the ancient greek </code>&#13;
<code class="sb">  history of philosophy?`</code><code class="p">)</code></pre>&#13;
&#13;
<p>RAG-Fusion’s strength lies in its ability to capture the user’s intended expression, navigate complex queries, and broaden the scope of retrieved documents, enabling serendipitous discovery.<a contenteditable="false" data-primary="" data-startref="QTragfusion03" data-type="indexterm" id="id587"/><a contenteditable="false" data-primary="" data-startref="ragfusion03" data-type="indexterm" id="id588"/><a contenteditable="false" data-primary="" data-startref="rrfalg03" data-type="indexterm" id="id589"/><a contenteditable="false" data-primary="" data-startref="Pragfus03" data-type="indexterm" id="id590"/><a contenteditable="false" data-primary="" data-startref="JSrag03" data-type="indexterm" id="id591"/></p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Hypothetical Document Embeddings" data-type="sect2"><div class="sect2" id="ch03_hypothetical_document_embeddings_hyde_1736545666794358">&#13;
<h2>Hypothetical Document Embeddings</h2>&#13;
&#13;
<p><em>Hypothetical Document Embeddings</em> (HyDE) is<a contenteditable="false" data-primary="query transformation" data-secondary="Hypothetical Document Embeddings (HyDE)" data-type="indexterm" id="QThypo03"/><a contenteditable="false" data-primary="Hypothetical Document Embeddings (HyDE)" data-type="indexterm" id="hypo03"/> a strategy that involves creating a hypothetical document based on the user’s query, embedding the document, and retrieving relevant documents based on vector similarity.<sup><a data-type="noteref" href="ch03.html#id592" id="id592-marker">4</a></sup> The intuition behind HyDE is that an LLM-generated hypothetical document will be more similar to the most relevant documents than the original query, as shown in <a data-type="xref" href="#ch03_figure_7_1736545666780724">Figure 3-7</a>.</p>&#13;
&#13;
<figure><div class="figure" id="ch03_figure_7_1736545666780724"><img alt="Screenshot 2024-02-12 at 1.12.45 PM.png" src="assets/lelc_0307.png"/>&#13;
<h6><span class="label">Figure 3-7. </span>An illustration of HyDE closer in the vector space to the document <span class="keep-together">embeddings</span> than the plain query embeddings</h6>&#13;
</div></figure>&#13;
&#13;
<p>First, define a prompt to generate a hypothetical document:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="Hypothetical Document Embeddings (HyDE)" data-type="indexterm" id="Phypo03"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">langchain.prompts</code> <code class="kn">import</code> <code class="n">ChatPromptTemplate</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_core.output_parsers</code> <code class="kn">import</code> <code class="n">StrOutputParser</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_openai</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code>&#13;
&#13;
<code class="n">prompt_hyde</code> <code class="o">=</code> <code class="n">ChatPromptTemplate</code><code class="o">.</code><code class="n">from_template</code><code class="p">(</code><code class="s2">"""Please write a passage to </code>&#13;
<code class="s2">   answer the question.</code><code class="se">\n</code><code class="s2"> Question: </code><code class="si">{question}</code><code class="s2"> </code><code class="se">\n</code><code class="s2"> Passage:"""</code><code class="p">)</code>&#13;
&#13;
<code class="n">generate_doc</code> <code class="o">=</code> <code class="p">(</code>&#13;
    <code class="n">prompt_hyde</code> <code class="o">|</code> <code class="n">ChatOpenAI</code><code class="p">(</code><code class="n">temperature</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code> <code class="o">|</code> <code class="n">StrOutputParser</code><code class="p">()</code> &#13;
<code class="p">)</code></pre>&#13;
&#13;
<p><em>JavaScript<a contenteditable="false" data-primary="JavaScript" data-secondary="Hypothetical Document Embeddings (HyDE)" data-type="indexterm" id="JShypo03"/></em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">import</code> <code class="p">{</code><code class="nx">ChatOpenAI</code><code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/openai'</code>&#13;
<code class="kr">import</code> <code class="p">{</code><code class="nx">ChatPromptTemplate</code><code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/core/prompts'</code>&#13;
<code class="kr">import</code> <code class="p">{</code><code class="nx">RunnableLambda</code><code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/core/runnables'</code><code class="p">;</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">prompt</code> <code class="o">=</code> <code class="nx">ChatPromptTemplate</code><code class="p">.</code><code class="nx">fromTemplate</code><code class="p">(</code><code class="sb">`Please write a passage to </code>&#13;
<code class="sb">  answer the question</code>&#13;
<code class="sb">Question: {question}</code>&#13;
<code class="sb">Passage:`</code><code class="p">)</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">llm</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">ChatOpenAI</code><code class="p">({</code><code class="nx">temperature</code><code class="o">:</code> <code class="mi">0</code><code class="p">})</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">generateDoc</code> <code class="o">=</code> <code class="nx">prompt</code><code class="p">.</code><code class="nx">pipe</code><code class="p">(</code><code class="nx">llm</code><code class="p">).</code><code class="nx">pipe</code><code class="p">(</code><code class="nx">msg</code> <code class="o">=&gt;</code> <code class="nx">msg</code><code class="p">.</code><code class="nx">content</code><code class="p">)</code></pre>&#13;
&#13;
<p>Next, we take the hypothetical document and use it as input to the <code>retriever</code>, which will generate its embedding and search for similar documents in the vector store:</p>&#13;
&#13;
<p><em>Python</em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="n">retrieval_chain</code><code> </code><strong><code class="o">=</code></strong><code> </code><code class="n">generate_doc</code><code> </code><strong><code class="o">|</code></strong><code> </code><code class="n">retriever</code><code> </code></pre>&#13;
&#13;
<p><em>JavaScript</em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">const</code> <code class="nx">retrievalChain</code> <code class="o">=</code> <code class="nx">generateDoc</code><code class="p">.</code><code class="nx">pipe</code><code class="p">(</code><code class="nx">retriever</code><code class="p">)</code></pre>&#13;
&#13;
<p>Finally, we take the retrieved documents, pass them as context to the final prompt, and instruct the model to generate an output:</p>&#13;
&#13;
<p><em>Python</em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="n">prompt</code> <code class="o">=</code> <code class="n">ChatPromptTemplate</code><code class="o">.</code><code class="n">from_template</code><code class="p">(</code><code class="s2">"""Answer the following question based </code>&#13;
<code class="s2">    on this context:</code>&#13;
&#13;
<code class="si">{context}</code><code class="s2"/>&#13;
&#13;
<code class="s2">Question: </code><code class="si">{question}</code><code class="s2"/>&#13;
<code class="s2">"""</code><code class="p">)</code>&#13;
&#13;
<code class="n">llm</code> <code class="o">=</code> <code class="n">ChatOpenAI</code><code class="p">(</code><code class="n">temperature</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>&#13;
&#13;
<code class="nd">@chain</code>&#13;
<code class="k">def</code> <code class="nf">qa</code><code class="p">(</code><code class="nb">input</code><code class="p">):</code>&#13;
  <code class="c1"># fetch relevant documents from the hyde retrieval chain defined earlier</code>&#13;
  <code class="n">docs</code> <code class="o">=</code> <code class="n">retrieval_chain</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="nb">input</code><code class="p">)</code>&#13;
  <code class="c1"># format prompt</code>&#13;
  <code class="n">formatted</code> <code class="o">=</code> <code class="n">prompt</code><code class="o">.</code><code class="n">invoke</code><code class="p">({</code><code class="s2">"context"</code><code class="p">:</code> <code class="n">docs</code><code class="p">,</code> <code class="s2">"question"</code><code class="p">:</code> <code class="nb">input</code><code class="p">})</code>&#13;
  <code class="c1"># generate answer</code>&#13;
  <code class="n">answer</code> <code class="o">=</code> <code class="n">llm</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">formatted</code><code class="p">)</code>&#13;
  <code class="k">return</code> <code class="n">answer</code>&#13;
&#13;
<code class="n">qa</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="s2">"""Who are some key figures in the ancient greek history of </code>&#13;
<code class="s2">    philosophy?"""</code><code class="p">)</code></pre>&#13;
&#13;
<p><em>JavaScript</em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">const</code> <code class="nx">prompt</code> <code class="o">=</code> <code class="nx">ChatPromptTemplate</code><code class="p">.</code><code class="nx">fromTemplate</code><code class="p">(</code><code class="sb">`Answer the following </code>&#13;
<code class="sb">  question based on this context:</code>&#13;
&#13;
<code class="sb">{context}</code>&#13;
&#13;
<code class="sb">Question: {question}</code>&#13;
<code class="sb">`</code><code class="p">)</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">llm</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">ChatOpenAI</code><code class="p">({</code><code class="nx">temperature</code><code class="o">:</code> <code class="mi">0</code><code class="p">})</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">qa</code> <code class="o">=</code> <code class="nx">RunnableLambda</code><code class="p">.</code><code class="nx">from</code><code class="p">(</code><code class="nx">async</code> <code class="nx">input</code> <code class="o">=&gt;</code> <code class="p">{</code>&#13;
  <code class="c1">// fetch relevant documents from the hyde retrieval chain defined earlier</code>&#13;
  <code class="kr">const</code> <code class="nx">docs</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">retrievalChain</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="nx">input</code><code class="p">)</code>&#13;
  <code class="c1">// format prompt</code>&#13;
  <code class="kr">const</code> <code class="nx">formatted</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">prompt</code><code class="p">.</code><code class="nx">invoke</code><code class="p">({</code><code class="nx">context</code><code class="o">:</code> <code class="nx">docs</code><code class="p">,</code> <code class="nx">question</code><code class="o">:</code> <code class="nx">input</code><code class="p">})</code>&#13;
  <code class="c1">// generate answer</code>&#13;
  <code class="kr">const</code> <code class="nx">answer</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">llm</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="nx">formatted</code><code class="p">)</code>&#13;
  <code class="k">return</code> <code class="nx">answer</code>&#13;
<code class="p">})</code>&#13;
&#13;
<code class="nx">await</code> <code class="nx">qa</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="sb">`Who are some key figures in the ancient greek history of </code>&#13;
<code class="sb">  philosophy?`</code><code class="p">)</code></pre>&#13;
&#13;
<p>To recap what we covered in this section, query transformation consists of taking the user’s original query and doing the following:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Rewriting into one or more queries</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Combining the results of those queries into a single set of the most relevant results</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Rewriting the query can take many forms, but it’s usually done in a similar fashion: take the user’s original query—a prompt you wrote—and ask an LLM to write a new query or queries. Some examples of typical changes made are:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Removing irrelevant/unrelated text from the query.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Grounding the query with past conversation history. For instance, to make sense of a query such as <em>and what about in LA,</em> we need to combine it with a hypothetical past question about the weather in SF, to arrive at a useful query such as <em>weather in LA</em>.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Casting a wider net for relevant documents by also fetching documents for related queries.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Decomposing a complex question into multiple, simpler questions and then including results for all of them in the final prompt to generate an answer.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>The right rewriting strategy to use will depend on your use case.</p>&#13;
&#13;
<p>Now that we’ve covered the main query transformation strategies, let’s discuss the second major question to answer in order to build a robust RAG system: How do we route queries to retrieve relevant data from multiple data sources?<a contenteditable="false" data-primary="" data-startref="DQtransform03" data-type="indexterm" id="id593"/><a contenteditable="false" data-primary="" data-startref="RAGqt03" data-type="indexterm" id="id594"/><a contenteditable="false" data-primary="" data-startref="QThypo03" data-type="indexterm" id="id595"/><a contenteditable="false" data-primary="" data-startref="hypo03" data-type="indexterm" id="id596"/><a contenteditable="false" data-primary="" data-startref="Phypo03" data-type="indexterm" id="id597"/><a contenteditable="false" data-primary="" data-startref="JShypo03" data-type="indexterm" id="id598"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Query Routing" data-type="sect1"><div class="sect1" id="ch03_query_routing_1736545666794423">&#13;
<h1>Query Routing</h1>&#13;
&#13;
<p>Although<a contenteditable="false" data-primary="retrieval-augmented generation (RAG)" data-secondary="query handling" data-tertiary="routing" data-type="indexterm" id="RAGrout03"/><a contenteditable="false" data-primary="data queries" data-secondary="query routing" data-type="indexterm" id="DQrout03"/><a contenteditable="false" data-primary="routing" data-secondary="query routing" data-type="indexterm" id="Rquery03"/> using a single vector store is useful, the required data may live in a variety of data sources, including relational databases or other vector stores.</p>&#13;
&#13;
<p class="fix_tracking">For example, you may have two vector stores: one for LangChain Python documentation and another for LangChain JS documentation. Given a user’s question, we would like to <em>route </em>the query to the appropriate inferred data source to retrieve relevant docs. <em>Query routing<a contenteditable="false" data-primary="query routing strategy" data-secondary="purpose of" data-type="indexterm" id="id599"/></em> is a strategy used to forward a user’s query to the relevant data source.</p>&#13;
&#13;
<section data-pdf-bookmark="Logical Routing" data-type="sect2"><div class="sect2" id="ch03_logical_routing_1736545666794484">&#13;
<h2>Logical Routing</h2>&#13;
&#13;
<p>In<a contenteditable="false" data-primary="query routing strategy" data-secondary="logical routing" data-type="indexterm" id="QRTlogical03"/><a contenteditable="false" data-primary="logical routing" data-type="indexterm" id="lrouting03"/><a contenteditable="false" data-primary="routing" data-secondary="logical routing" data-type="indexterm" id="Rlogical03"/><em> logical routing</em>, we give the LLM knowledge of the various data sources at our disposal and then let the LLM reason which data source to apply based on the user’s query, as shown in <a data-type="xref" href="#ch03_figure_8_1736545666780744">Figure 3-8</a>.</p>&#13;
&#13;
<figure><div class="figure" id="ch03_figure_8_1736545666780744"><img alt="A diagram of a brain  Description automatically generated" src="assets/lelc_0308.png"/>&#13;
<h6><span class="label">Figure 3-8. </span>Query routing to relevant data sources</h6>&#13;
</div></figure>&#13;
&#13;
<p>In order to achieve this, we make use of function-calling models like GPT-3.5 Turbo to help classify each query into one of the available routes. A<a contenteditable="false" data-primary="function calls" data-type="indexterm" id="id600"/> <em>function call</em> involves defining a schema that the model can use to generate arguments of a function based on the query. This enables us to generate structured outputs that can be used to run other functions. The following Python code defines the schema for our router based on three docs for different languages:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="routing" data-tertiary="logical" data-type="indexterm" id="Plogical03"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">from</code><code> </code><code class="nn">typing</code><code> </code><code class="kn">import</code><code> </code><code class="n">Literal</code><code>&#13;
</code><code>&#13;
</code><code class="kn">from</code><code> </code><code class="nn">langchain_core</code><code class="nn">.</code><code class="nn">prompts</code><code> </code><code class="kn">import</code><code> </code><code class="n">ChatPromptTemplate</code><code>&#13;
</code><code class="kn">from</code><code> </code><code class="nn">langchain_core</code><code class="nn">.</code><code class="nn">pydantic_v1</code><code> </code><code class="kn">import</code><code> </code><code class="n">BaseModel</code><code class="p">,</code><code> </code><code class="n">Field</code><code>&#13;
</code><code class="kn">from</code><code> </code><code class="nn">langchain_openai</code><code> </code><code class="kn">import</code><code> </code><code class="n">ChatOpenAI</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># Data model</code><code>&#13;
</code><code class="k">class</code><code> </code><code class="nc">RouteQuery</code><code class="p">(</code><code class="n">BaseModel</code><code class="p">)</code><code class="p">:</code><code>&#13;
</code><code>    </code><code class="sd">"""Route a user query to the most relevant datasource."""</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="n">datasource</code><code class="p">:</code><code> </code><code class="n">Literal</code><code class="p">[</code><code class="s2">"</code><code class="s2">python_docs</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="s2">js_docs</code><code class="s2">"</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="n">Field</code><code class="p">(</code><code>&#13;
</code><code>        </code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code class="p">,</code><code>&#13;
</code><code>        </code><code class="n">description</code><code class="o">=</code><code class="s2">"""</code><code class="s2">Given a user question, choose which datasource would be </code><code class="s2">&#13;
</code><code class="s2">            most relevant for answering their question</code><code class="s2">"""</code><code class="p">,</code><code>&#13;
</code><code>    </code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># LLM with function call </code><code>&#13;
</code><code class="n">llm</code><code> </code><code class="o">=</code><code> </code><code class="n">ChatOpenAI</code><code class="p">(</code><code class="n">model</code><code class="o">=</code><code class="s2">"</code><code class="s2">gpt-3.5-turbo</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="n">temperature</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code><code>&#13;
</code><code class="n">structured_llm</code><code> </code><code class="o">=</code><code> </code><code class="n">llm</code><code class="o">.</code><code class="n">with_structured_output</code><code class="p">(</code><code class="n">RouteQuery</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># Prompt </code><code>&#13;
</code><code class="n">system</code><code> </code><code class="o">=</code><code> </code><code class="s2">"""</code><code class="s2">You are an expert at routing a user question to the appropriate data </code><code class="s2">&#13;
</code><code class="s2">    source.</code><code class="s2">&#13;
</code><code class="s2">&#13;
</code><code class="s2">Based on the programming language the question is referring to, route it to the </code><code class="s2">&#13;
</code><code class="s2">    relevant data source.</code><code class="s2">"""</code><code>&#13;
</code><code>&#13;
</code><code class="n">prompt</code><code> </code><code class="o">=</code><code> </code><code class="n">ChatPromptTemplate</code><code class="o">.</code><code class="n">from_messages</code><code class="p">(</code><code>&#13;
</code><code>    </code><code class="p">[</code><code>&#13;
</code><code>        </code><code class="p">(</code><code class="s2">"</code><code class="s2">system</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="n">system</code><code class="p">)</code><code class="p">,</code><code>&#13;
</code><code>        </code><code class="p">(</code><code class="s2">"</code><code class="s2">human</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="si">{question}</code><code class="s2">"</code><code class="p">)</code><code class="p">,</code><code>&#13;
</code><code>    </code><code class="p">]</code><code>&#13;
</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># Define router </code><code>&#13;
</code><code class="n">router</code><code> </code><strong><code class="o">=</code></strong><code> </code><code class="n">prompt</code><code> </code><strong><code class="o">|</code></strong><code> </code><code class="n">structured_llm</code></pre>&#13;
&#13;
<p><em>JavaScript<a contenteditable="false" data-primary="JavaScript" data-secondary="routing" data-tertiary="logical" data-type="indexterm" id="JSlogical03"/></em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">ChatOpenAI</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"@langchain/openai"</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">z</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"zod"</code><code class="p">;</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">routeQuery</code> <code class="o">=</code> <code class="nx">z</code><code class="p">.</code><code class="nx">object</code><code class="p">({</code>&#13;
  <code class="nx">datasource</code><code class="o">:</code> <code class="nx">z</code><code class="p">.</code><code class="kr">enum</code><code class="p">([</code><code class="s2">"python_docs"</code><code class="p">,</code> <code class="s2">"js_docs"</code><code class="p">]).</code><code class="nx">describe</code><code class="p">(</code><code class="sb">`Given a user </code>&#13;
<code class="sb">    question, choose which datasource would be most relevant for answering </code>&#13;
<code class="sb">    their question`</code><code class="p">),</code>&#13;
<code class="p">}).</code><code class="nx">describe</code><code class="p">(</code><code class="s2">"Route a user query to the most relevant datasource."</code><code class="p">)</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">llm</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">ChatOpenAI</code><code class="p">({</code><code class="nx">model</code><code class="o">:</code> <code class="s2">"gpt-3.5-turbo"</code><code class="p">,</code> <code class="nx">temperature</code><code class="o">:</code> <code class="mi">0</code><code class="p">})</code>&#13;
<code class="kr">const</code> <code class="nx">structuredLlm</code> <code class="o">=</code> <code class="nx">llm</code><code class="p">.</code><code class="nx">withStructuredOutput</code><code class="p">(</code><code class="nx">routeQuery</code><code class="p">,</code> <code class="p">{</code><code class="nx">name</code><code class="o">:</code> <code class="s2">"RouteQuery"</code><code class="p">})</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">prompt</code> <code class="o">=</code> <code class="nx">ChatPromptTemplate</code><code class="p">.</code><code class="nx">fromMessages</code><code class="p">([</code>&#13;
  <code class="p">[</code><code class="s1">'system'</code><code class="p">,</code> <code class="sb">`You are an expert at routing a user question to the appropriate </code>&#13;
<code class="sb">      data source.</code>&#13;
&#13;
<code class="sb">Based on the programming language the question is referring to, route it to </code>&#13;
<code class="sb">  the relevant data source.`</code><code class="p">],</code>&#13;
  <code class="p">[</code><code class="s1">'human'</code><code class="p">,</code> <code class="s1">'{question}'</code><code class="p">]</code>&#13;
<code class="p">])</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">router</code> <code class="o">=</code> <code class="nx">prompt</code><code class="p">.</code><code class="nx">pipe</code><code class="p">(</code><code class="nx">structuredLlm</code><code class="p">)</code></pre>&#13;
&#13;
<p>Now we invoke the LLM to extract the data source based on the predefined schema:</p>&#13;
&#13;
<p><em>Python</em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="n">question</code> <code class="o">=</code> <code class="s2">"""Why doesn't the following code work:</code>&#13;
&#13;
<code class="s2">from langchain_core.prompts import ChatPromptTemplate</code>&#13;
&#13;
<code class="s2">prompt = ChatPromptTemplate.from_messages(["human", "speak in </code><code class="si">{language}</code><code class="s2">"])</code>&#13;
<code class="s2">prompt.invoke("french")</code>&#13;
<code class="s2">"""</code>&#13;
&#13;
<code class="n">result</code> <code class="o">=</code> <code class="n">router</code><code class="o">.</code><code class="n">invoke</code><code class="p">({</code><code class="s2">"question"</code><code class="p">:</code> <code class="n">question</code><code class="p">})</code>&#13;
&#13;
<code class="n">result</code><code class="o">.</code><code class="n">datasource</code>&#13;
<code class="c1"># "python_docs"</code></pre>&#13;
&#13;
<p><em>JavaScript</em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">const</code> <code class="nx">question</code> <code class="o">=</code> <code class="sb">`Why doesn't the following code work:</code>&#13;
&#13;
<code class="sb">from langchain_core.prompts import ChatPromptTemplate</code>&#13;
&#13;
<code class="sb">prompt = ChatPromptTemplate.from_messages(["human", "speak in {language}"])</code>&#13;
<code class="sb">prompt.invoke("french")</code>&#13;
<code class="sb">`</code>&#13;
&#13;
<code class="nx">await</code> <code class="nx">router</code><code class="p">.</code><code class="nx">invoke</code><code class="p">({</code> <code class="nx">question</code> <code class="p">})</code></pre>&#13;
&#13;
<p><em>The output:</em></p>&#13;
&#13;
<pre data-type="programlisting">&#13;
{&#13;
    datasource: "python_docs"&#13;
}</pre>&#13;
&#13;
<p>Notice how the LLM produced JSON output, conforming to the schema we defined earlier. This will be useful in many other tasks.</p>&#13;
&#13;
<p class="pagebreak-before less_space">Once we’ve extracted the relevant data source, we can pass the value into another function to execute additional logic as required:</p>&#13;
&#13;
<p><em>Python</em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="k">def</code> <code class="nf">choose_route</code><code class="p">(</code><code class="n">result</code><code class="p">):</code>&#13;
    <code class="k">if</code> <code class="s2">"python_docs"</code> <code class="ow">in</code> <code class="n">result</code><code class="o">.</code><code class="n">datasource</code><code class="o">.</code><code class="n">lower</code><code class="p">():</code>&#13;
        <code class="c1">### Logic here </code>&#13;
        <code class="k">return</code> <code class="s2">"chain for python_docs"</code>&#13;
    <code class="k">else</code><code class="p">:</code>&#13;
        <code class="c1">### Logic here </code>&#13;
        <code class="k">return</code> <code class="s2">"chain for js_docs"</code>&#13;
&#13;
<code class="n">full_chain</code> <code class="o">=</code> <code class="n">router</code> <code class="o">|</code> <code class="n">RunnableLambda</code><code class="p">(</code><code class="n">choose_route</code><code class="p">)</code></pre>&#13;
&#13;
<p><em>JavaScript</em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kd">function</code> <code class="nx">chooseRoute</code><code class="p">(</code><code class="nx">result</code><code class="p">)</code> <code class="p">{</code>&#13;
  <code class="k">if</code> <code class="p">(</code><code class="nx">result</code><code class="p">.</code><code class="nx">datasource</code><code class="p">.</code><code class="nx">toLowerCase</code><code class="p">().</code><code class="nx">includes</code><code class="p">(</code><code class="s1">'python_docs'</code><code class="p">))</code> <code class="p">{</code>&#13;
    <code class="k">return</code> <code class="s1">'chain for python_docs'</code><code class="p">;</code>&#13;
  <code class="p">}</code> <code class="k">else</code> <code class="p">{</code>&#13;
    <code class="k">return</code> <code class="s1">'chain for js_docs'</code><code class="p">;</code>&#13;
  <code class="p">}</code>&#13;
<code class="p">}</code> &#13;
&#13;
<code class="kr">const</code> <code class="nx">fullChain</code> <code class="o">=</code> <code class="nx">router</code><code class="p">.</code><code class="nx">pipe</code><code class="p">(</code><code class="nx">chooseRoute</code><code class="p">)</code> </pre>&#13;
&#13;
<p>Notice how we don’t do an exact string comparison but instead first turn the generated output to lowercase, and then do a substring match. This makes our chain more resilient to the LLM going off script and producing output that doesn’t quite conform to the schema we asked for.</p>&#13;
&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>Resilience to the random nature of LLM outputs is an important theme to keep in mind when building your LLM applications.</p>&#13;
</div>&#13;
&#13;
<p>Logical routing is most suitable when you have a defined list of data sources from which relevant data can be retrieved and utilized by the LLM to generate an accurate output. These can range from vector stores to databases and even APIs.<a contenteditable="false" data-primary="" data-startref="QRTlogical03" data-type="indexterm" id="id601"/><a contenteditable="false" data-primary="" data-startref="lrouting03" data-type="indexterm" id="id602"/><a contenteditable="false" data-primary="" data-startref="Plogical03" data-type="indexterm" id="id603"/><a contenteditable="false" data-primary="" data-startref="JSlogical03" data-type="indexterm" id="id604"/><a contenteditable="false" data-primary="" data-startref="Rlogical03" data-type="indexterm" id="id605"/><a contenteditable="false" data-primary="" data-startref="JSlogical03" data-type="indexterm" id="id606"/><a contenteditable="false" data-primary="" data-startref="Rlogical03" data-type="indexterm" id="id607"/></p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Semantic Routing" data-type="sect2"><div class="sect2" id="ch03_semantic_routing_1736545666794543">&#13;
<h2>Semantic Routing</h2>&#13;
&#13;
<p>Unlike<a contenteditable="false" data-primary="routing" data-secondary="semantic routing" data-type="indexterm" id="Rsemantic03"/> logical routing, <em>semantic routing<a contenteditable="false" data-primary="query routing strategy" data-secondary="semantic routing" data-type="indexterm" id="QRSsemantic03"/><a contenteditable="false" data-primary="semantic routing" data-type="indexterm" id="semanticrouting03"/></em> involves embedding various prompts that represent various data sources alongside the user’s query and then performing vector similarity search to retrieve the most similar prompt. <a data-type="xref" href="#ch03_figure_9_1736545666780765">Figure 3-9</a> illustrates.</p>&#13;
&#13;
<figure><div class="figure" id="ch03_figure_9_1736545666780765"><img alt="A diagram of a diagram of a notepad and a brain  Description automatically generated" src="assets/lelc_0309.png"/>&#13;
<h6><span class="label">Figure 3-9. </span>Semantic routing to improve the accuracy of retrieved documents</h6>&#13;
</div></figure>&#13;
&#13;
<p>The following is an example of semantic routing:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="routing" data-tertiary="semantic" data-type="indexterm" id="Psemantic03"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">langchain.utils.math</code> <code class="kn">import</code> <code class="n">cosine_similarity</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_core.output_parsers</code> <code class="kn">import</code> <code class="n">StrOutputParser</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_core.prompts</code> <code class="kn">import</code> <code class="n">PromptTemplate</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_core.runnables</code> <code class="kn">import</code> <code class="n">chain</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_openai</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code><code class="p">,</code> <code class="n">OpenAIEmbeddings</code>&#13;
&#13;
<code class="c1"># Two prompts</code>&#13;
<code class="n">physics_template</code> <code class="o">=</code> <code class="s2">"""You are a very smart physics professor. You are great at </code>&#13;
<code class="s2">    answering questions about physics in a concise and easy-to-understand manner. </code>&#13;
<code class="s2">    When you don't know the answer to a question, you admit that you don't know.</code>&#13;
&#13;
<code class="s2">Here is a question:</code>&#13;
<code class="si">{query}</code><code class="s2">"""</code>&#13;
&#13;
<code class="n">math_template</code> <code class="o">=</code> <code class="s2">"""You are a very good mathematician. You are great at answering </code>&#13;
<code class="s2">    math questions. You are so good because you are able to break down hard </code>&#13;
<code class="s2">    problems into their component parts, answer the component parts, and then </code>&#13;
<code class="s2">    put them together to answer the broader question.</code>&#13;
&#13;
<code class="s2">Here is a question:</code>&#13;
<code class="si">{query}</code><code class="s2">"""</code>&#13;
&#13;
<code class="c1"># Embed prompts</code>&#13;
<code class="n">embeddings</code> <code class="o">=</code> <code class="n">OpenAIEmbeddings</code><code class="p">()</code>&#13;
<code class="n">prompt_templates</code> <code class="o">=</code> <code class="p">[</code><code class="n">physics_template</code><code class="p">,</code> <code class="n">math_template</code><code class="p">]</code>&#13;
<code class="n">prompt_embeddings</code> <code class="o">=</code> <code class="n">embeddings</code><code class="o">.</code><code class="n">embed_documents</code><code class="p">(</code><code class="n">prompt_templates</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Route question to prompt</code>&#13;
<code class="nd">@chain</code>&#13;
<code class="k">def</code> <code class="nf">prompt_router</code><code class="p">(</code><code class="n">query</code><code class="p">):</code>&#13;
    <code class="c1"># Embed question</code>&#13;
    <code class="n">query_embedding</code> <code class="o">=</code> <code class="n">embeddings</code><code class="o">.</code><code class="n">embed_query</code><code class="p">(</code><code class="n">query</code><code class="p">)</code>&#13;
    <code class="c1"># Compute similarity</code>&#13;
    <code class="n">similarity</code> <code class="o">=</code> <code class="n">cosine_similarity</code><code class="p">([</code><code class="n">query_embedding</code><code class="p">],</code> <code class="n">prompt_embeddings</code><code class="p">)[</code><code class="mi">0</code><code class="p">]</code>&#13;
    <code class="c1"># Pick the prompt most similar to the input question</code>&#13;
    <code class="n">most_similar</code> <code class="o">=</code> <code class="n">prompt_templates</code><code class="p">[</code><code class="n">similarity</code><code class="o">.</code><code class="n">argmax</code><code class="p">()]</code>&#13;
    <code class="k">return</code> <code class="n">PromptTemplate</code><code class="o">.</code><code class="n">from_template</code><code class="p">(</code><code class="n">most_similar</code><code class="p">)</code>&#13;
&#13;
<code class="n">semantic_router</code> <code class="o">=</code> <code class="p">(</code>&#13;
    <code class="n">prompt_router</code>&#13;
    <code class="o">|</code> <code class="n">ChatOpenAI</code><code class="p">()</code>&#13;
    <code class="o">|</code> <code class="n">StrOutputParser</code><code class="p">()</code>&#13;
<code class="p">)</code>&#13;
&#13;
<code class="nb">print</code><code class="p">(</code><code class="n">semantic_router</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="s2">"What's a black hole"</code><code class="p">))</code></pre>&#13;
&#13;
<p><em>JavaScript<a contenteditable="false" data-primary="JavaScript" data-secondary="routing" data-tertiary="semantic" data-type="indexterm" id="JSsemantic03"/></em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">import</code> <code class="p">{</code><code class="nx">cosineSimilarity</code><code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/core/utils/math'</code>&#13;
<code class="kr">import</code> <code class="p">{</code><code class="nx">ChatOpenAI</code><code class="p">,</code> <code class="nx">OpenAIEmbeddings</code><code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/openai'</code>&#13;
<code class="kr">import</code> <code class="p">{</code><code class="nx">PromptTemplate</code><code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/core/prompts'</code>&#13;
<code class="kr">import</code> <code class="p">{</code><code class="nx">RunnableLambda</code><code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/core/runnables'</code><code class="p">;</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">physicsTemplate</code> <code class="o">=</code> <code class="sb">`You are a very smart physics professor. You are great </code>&#13;
<code class="sb">  at answering questions about physics in a concise and easy-to-understand </code>&#13;
<code class="sb">  manner. When you don't know the answer to a question, you admit that you </code>&#13;
<code class="sb">  don't know.</code>&#13;
&#13;
<code class="sb">Here is a question:</code>&#13;
<code class="sb">{query}`</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">mathTemplate</code> <code class="o">=</code> <code class="sb">`You are a very good mathematician. You are great at </code>&#13;
<code class="sb">  answering math questions. You are so good because you are able to break down </code>&#13;
<code class="sb">  hard problems into their component parts, answer the component parts, and </code>&#13;
<code class="sb">  then put them together to answer the broader question.</code>&#13;
&#13;
<code class="sb">Here is a question:</code>&#13;
<code class="sb">{query}`</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">embeddings</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">OpenAIEmbeddings</code><code class="p">()</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">promptTemplates</code> <code class="o">=</code> <code class="p">[</code><code class="nx">physicsTemplate</code><code class="p">,</code> <code class="nx">mathTemplate</code><code class="p">]</code>&#13;
<code class="kr">const</code> <code class="nx">promptEmbeddings</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">embeddings</code><code class="p">.</code><code class="nx">embedDocuments</code><code class="p">(</code><code class="nx">promptTemplates</code><code class="p">)</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">promptRouter</code> <code class="o">=</code> <code class="nx">RunnableLambda</code><code class="p">.</code><code class="nx">from</code><code class="p">(</code><code class="nx">query</code> <code class="o">=&gt;</code> <code class="p">{</code>&#13;
  <code class="c1">// Embed question</code>&#13;
  <code class="kr">const</code> <code class="nx">queryEmbedding</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">embeddings</code><code class="p">.</code><code class="nx">embedQuery</code><code class="p">(</code><code class="nx">query</code><code class="p">)</code>&#13;
  <code class="c1">// Compute similarity</code>&#13;
  <code class="kr">const</code> <code class="nx">similarities</code> <code class="o">=</code> <code class="nx">cosineSimilarity</code><code class="p">([</code><code class="nx">queryEmbedding</code><code class="p">],</code> <code class="nx">promptEmbeddings</code><code class="p">)[</code><code class="mi">0</code><code class="p">]</code>&#13;
  <code class="c1">// Pick the prompt most similar to the input question</code>&#13;
  <code class="kr">const</code> <code class="nx">mostSimilar</code> <code class="o">=</code> <code class="nx">similarities</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">&gt;</code> <code class="nx">similarities</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code> &#13;
    <code class="o">?</code> <code class="nx">promptTemplates</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> &#13;
    <code class="o">:</code> <code class="nx">promptTemplates</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code>&#13;
  <code class="k">return</code> <code class="nx">PromptTemplate</code><code class="p">.</code><code class="nx">fromTemplate</code><code class="p">(</code><code class="nx">mostSimilar</code><code class="p">)</code>&#13;
<code class="p">})</code>&#13;
&#13;
&#13;
<code class="kr">const</code> <code class="nx">semanticRouter</code> <code class="o">=</code> <code class="nx">promptRouter</code><code class="p">.</code><code class="nx">pipe</code><code class="p">(</code><code class="k">new</code> <code class="nx">ChatOpenAI</code><code class="p">())</code>&#13;
&#13;
<code class="nx">await</code> <code class="nx">semanticRouter</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="s2">"What's a black hole"</code><code class="p">)</code></pre>&#13;
&#13;
<p>Now that you’ve seen how to route a user’s query to the relevant data source, let’s discuss the third major question when building a robust RAG system: “How do we transform natural language to the query language of the target data source?”<a contenteditable="false" data-primary="" data-startref="DQrout03" data-type="indexterm" id="id608"/><a contenteditable="false" data-primary="" data-startref="RAGrout03" data-type="indexterm" id="id609"/><a contenteditable="false" data-primary="" data-startref="JSsemantic03" data-type="indexterm" id="id610"/><a contenteditable="false" data-primary="" data-startref="Psemantic03" data-type="indexterm" id="id611"/><a contenteditable="false" data-primary="" data-startref="semanticrouting03" data-type="indexterm" id="id612"/><a contenteditable="false" data-primary="" data-startref="QRSsemantic03" data-type="indexterm" id="id613"/><a contenteditable="false" data-primary="" data-startref="Rsemantic03" data-type="indexterm" id="id614"/><a contenteditable="false" data-primary="" data-startref="Rquery03" data-type="indexterm" id="id615"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Query Construction" data-type="sect1"><div class="sect1" id="ch03_query_construction_1736545666794612">&#13;
<h1>Query Construction</h1>&#13;
&#13;
<p>As<a contenteditable="false" data-primary="retrieval-augmented generation (RAG)" data-secondary="query handling" data-tertiary="construction" data-type="indexterm" id="RAGconst03"/><a contenteditable="false" data-primary="data queries" data-secondary="query construction" data-type="indexterm" id="DQconst03"/> discussed earlier, RAG is an effective strategy to embed and retrieve relevant unstructured data from a vector store based on a query. But most data available for use in production apps is structured and typically stored in relational databases. In addition, unstructured data embedded in a vector store also contains structured metadata that possesses important information.</p>&#13;
&#13;
<p><em>Query construction</em> is<a contenteditable="false" data-primary="query construction" data-secondary="process of" data-type="indexterm" id="id616"/> the process of transforming a natural language query into the query language of the database or data source you are interacting with. See <a data-type="xref" href="#ch03_figure_10_1736545666780785">Figure 3-10</a>.</p>&#13;
&#13;
<figure><div class="figure" id="ch03_figure_10_1736545666780785"><img alt="A diagram of data types  Description automatically generated" src="assets/lelc_0310.png"/>&#13;
<h6><span class="label">Figure 3-10. </span>Illustration of query languages for various data sources</h6>&#13;
</div></figure>&#13;
&#13;
<p>For example, consider the query: <em>What</em><em> are movies about aliens in the year 1980?</em> This question contains an unstructured topic that can be retrieved via embeddings (<em>aliens</em>), but it also contains potential structured components (<em>year == 1980</em>).</p>&#13;
&#13;
<p>The following sections dive deeper into the various forms of query construction.</p>&#13;
&#13;
<section data-pdf-bookmark="Text-to-Metadata Filter" data-type="sect2"><div class="sect2" id="ch03_text_to_metadata_filter_1736545666794676">&#13;
<h2>Text-to-Metadata Filter</h2>&#13;
&#13;
<p>Most<a contenteditable="false" data-primary="query construction" data-secondary="text-to-metadata filter" data-type="indexterm" id="QCtexttometa03"/><a contenteditable="false" data-primary="text" data-secondary="text-to-metadata filter" data-type="indexterm" id="texttofilter03"/> vector stores provide the ability to limit your vector search based on metadata. During the embedding process, we can attach metadata key-value pairs to vectors in an index and then later specify filter expressions when you query the index.</p>&#13;
&#13;
<p>LangChain provides a <code>SelfQueryRetriever</code> that abstracts this logic and makes it easier to translate natural language queries into structured queries for various data sources. The self-querying utilizes an LLM to extract and execute the relevant metadata filters based on a user’s query and predefined metadata schema:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="text-to-metadata filter" data-type="indexterm" id="Ptexttometa03"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">langchain.chains.query_constructor.base</code> <code class="kn">import</code> <code class="n">AttributeInfo</code>&#13;
<code class="kn">from</code> <code class="nn">langchain.retrievers.self_query.base</code> <code class="kn">import</code> <code class="n">SelfQueryRetriever</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_openai</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code>&#13;
&#13;
<code class="n">fields</code> <code class="o">=</code> <code class="p">[</code>&#13;
    <code class="n">AttributeInfo</code><code class="p">(</code>&#13;
        <code class="n">name</code><code class="o">=</code><code class="s2">"genre"</code><code class="p">,</code>&#13;
        <code class="n">description</code><code class="o">=</code><code class="s2">"The genre of the movie"</code><code class="p">,</code>&#13;
        <code class="nb">type</code><code class="o">=</code><code class="s2">"string or list[string]"</code><code class="p">,</code>&#13;
    <code class="p">),</code>&#13;
    <code class="n">AttributeInfo</code><code class="p">(</code>&#13;
        <code class="n">name</code><code class="o">=</code><code class="s2">"year"</code><code class="p">,</code>&#13;
        <code class="n">description</code><code class="o">=</code><code class="s2">"The year the movie was released"</code><code class="p">,</code>&#13;
        <code class="nb">type</code><code class="o">=</code><code class="s2">"integer"</code><code class="p">,</code>&#13;
    <code class="p">),</code>&#13;
    <code class="n">AttributeInfo</code><code class="p">(</code>&#13;
        <code class="n">name</code><code class="o">=</code><code class="s2">"director"</code><code class="p">,</code>&#13;
        <code class="n">description</code><code class="o">=</code><code class="s2">"The name of the movie director"</code><code class="p">,</code>&#13;
        <code class="nb">type</code><code class="o">=</code><code class="s2">"string"</code><code class="p">,</code>&#13;
    <code class="p">),</code>&#13;
    <code class="n">AttributeInfo</code><code class="p">(</code>&#13;
        <code class="n">name</code><code class="o">=</code><code class="s2">"rating"</code><code class="p">,</code> <code class="n">description</code><code class="o">=</code><code class="s2">"A 1-10 rating for the movie"</code><code class="p">,</code> <code class="nb">type</code><code class="o">=</code><code class="s2">"float"</code>&#13;
    <code class="p">),</code>&#13;
<code class="p">]</code>&#13;
<code class="n">description</code> <code class="o">=</code> <code class="s2">"Brief summary of a movie"</code>&#13;
&#13;
<code class="n">llm</code> <code class="o">=</code> <code class="n">ChatOpenAI</code><code class="p">(</code><code class="n">temperature</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>&#13;
&#13;
<code class="n">retriever</code> <code class="o">=</code> <code class="n">SelfQueryRetriever</code><code class="o">.</code><code class="n">from_llm</code><code class="p">(</code>&#13;
    <code class="n">llm</code><code class="p">,</code> <code class="n">db</code><code class="p">,</code> <code class="n">description</code><code class="p">,</code> <code class="n">fields</code><code class="p">,</code>&#13;
<code class="p">)</code>&#13;
&#13;
<code class="nb">print</code><code class="p">(</code><code class="n">retriever</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code>&#13;
    <code class="s2">"What's a highly rated (above 8.5) science fiction film?"</code><code class="p">))</code></pre>&#13;
&#13;
<p><em>JavaScript<a contenteditable="false" data-primary="JavaScript" data-secondary="text-to-metadata filter" data-type="indexterm" id="JStexttometa03"/></em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">ChatOpenAI</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"@langchain/openai"</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">SelfQueryRetriever</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"langchain/retrievers/self_query"</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">FunctionalTranslator</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"@langchain/core/structured_query"</code><code class="p">;</code>&#13;
&#13;
<code class="cm">/**</code>&#13;
<code class="cm"> * First, we define the attributes we want to be able to query on.</code>&#13;
<code class="cm"> * in this case, we want to be able to query on the genre, year, director, </code>&#13;
<code class="cm"> * rating, and length of the movie.</code>&#13;
<code class="cm"> * We also provide a description of each attribute and the type of the attribute.</code>&#13;
<code class="cm"> * This is used to generate the query prompts.</code>&#13;
<code class="cm"> */</code>&#13;
<code class="kr">const</code> <code class="nx">fields</code> <code class="o">=</code> <code class="p">[</code>&#13;
  <code class="p">{</code>&#13;
    <code class="nx">name</code><code class="o">:</code> <code class="s2">"genre"</code><code class="p">,</code>&#13;
    <code class="nx">description</code><code class="o">:</code> <code class="s2">"The genre of the movie"</code><code class="p">,</code>&#13;
    <code class="nx">type</code><code class="o">:</code> <code class="s2">"string or array of strings"</code><code class="p">,</code>&#13;
  <code class="p">},</code>&#13;
  <code class="p">{</code>&#13;
    <code class="nx">name</code><code class="o">:</code> <code class="s2">"year"</code><code class="p">,</code>&#13;
    <code class="nx">description</code><code class="o">:</code> <code class="s2">"The year the movie was released"</code><code class="p">,</code>&#13;
    <code class="nx">type</code><code class="o">:</code> <code class="s2">"number"</code><code class="p">,</code>&#13;
  <code class="p">},</code>&#13;
  <code class="p">{</code>&#13;
    <code class="nx">name</code><code class="o">:</code> <code class="s2">"director"</code><code class="p">,</code>&#13;
    <code class="nx">description</code><code class="o">:</code> <code class="s2">"The director of the movie"</code><code class="p">,</code>&#13;
    <code class="nx">type</code><code class="o">:</code> <code class="s2">"string"</code><code class="p">,</code>&#13;
  <code class="p">},</code>&#13;
  <code class="p">{</code>&#13;
    <code class="nx">name</code><code class="o">:</code> <code class="s2">"rating"</code><code class="p">,</code>&#13;
    <code class="nx">description</code><code class="o">:</code> <code class="s2">"The rating of the movie (1-10)"</code><code class="p">,</code>&#13;
    <code class="nx">type</code><code class="o">:</code> <code class="s2">"number"</code><code class="p">,</code>&#13;
  <code class="p">},</code>&#13;
  <code class="p">{</code>&#13;
    <code class="nx">name</code><code class="o">:</code> <code class="s2">"length"</code><code class="p">,</code>&#13;
    <code class="nx">description</code><code class="o">:</code> <code class="s2">"The length of the movie in minutes"</code><code class="p">,</code>&#13;
    <code class="nx">type</code><code class="o">:</code> <code class="s2">"number"</code><code class="p">,</code>&#13;
  <code class="p">},</code>&#13;
<code class="p">];</code>&#13;
<code class="kr">const</code> <code class="nx">description</code> <code class="o">=</code> <code class="s2">"Brief summary of a movie"</code><code class="p">;</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">llm</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">ChatOpenAI</code><code class="p">();</code>&#13;
<code class="kr">const</code> <code class="nx">attributeInfos</code> <code class="o">=</code> <code class="nx">fields</code><code class="p">.</code><code class="nx">map</code><code class="p">((</code><code class="nx">field</code><code class="p">)</code> <code class="o">=&gt;</code> <code class="k">new</code> <code class="nx">AttributeInfo</code><code class="p">(</code><code class="nx">field</code><code class="p">.</code><code class="nx">name</code><code class="p">,</code>  &#13;
  <code class="nx">field</code><code class="p">.</code><code class="nx">description</code><code class="p">,</code> <code class="nx">field</code><code class="p">.</code><code class="nx">type</code><code class="p">));</code>&#13;
  &#13;
<code class="kr">const</code> <code class="nx">selfQueryRetriever</code> <code class="o">=</code> <code class="nx">SelfQueryRetriever</code><code class="p">.</code><code class="nx">fromLLM</code><code class="p">({</code>&#13;
  <code class="nx">llm</code><code class="p">,</code>&#13;
  <code class="nx">db</code><code class="p">,</code>&#13;
  <code class="nx">description</code><code class="p">,</code>&#13;
  <code class="nx">attributeInfo</code><code class="o">:</code> <code class="nx">attributeInfos</code><code class="p">,</code>&#13;
  <code class="cm">/**</code>&#13;
<code class="cm">   * We need to use a translator that translates the queries into a</code>&#13;
<code class="cm">   * filter format that the vector store can understand. LangChain provides one </code>&#13;
<code class="cm">   * here.</code>&#13;
<code class="cm">   */</code>&#13;
  <code class="nx">structuredQueryTranslator</code><code class="o">:</code> <code class="k">new</code> <code class="nx">FunctionalTranslator</code><code class="p">(),</code>&#13;
<code class="p">});</code>&#13;
&#13;
<code class="nx">await</code> <code class="nx">selfQueryRetriever</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code>&#13;
  <code class="s2">"What's a highly rated (above 8.5) science fiction film?"</code>&#13;
<code class="p">);</code></pre>&#13;
&#13;
<p class="pagebreak-before less_space">This results in a retriever that will take a user query, and split it into:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>A filter to apply on the metadata of each document first</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>A query to use for semantic search on the documents</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>To do this, we have to describe which fields the metadata of our documents contain; that description will be included in the prompt. The retriever will then do the following:</p>&#13;
&#13;
<ol>&#13;
	<li>&#13;
	<p>Send the query generation prompt to the LLM.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Parse metadata filter and rewritten search query from the LLM output.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Convert the metadata filter generated by the LLM to the format appropriate for our vector store.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Issue a similarity search against the vector store, filtered to only match documents whose metadata passes the generated filter.<a contenteditable="false" data-primary="" data-startref="JStexttometa03" data-type="indexterm" id="id617"/><a contenteditable="false" data-primary="" data-startref="texttofilter03" data-type="indexterm" id="id618"/><a contenteditable="false" data-primary="" data-startref="QCtexttometa03" data-type="indexterm" id="id619"/><a contenteditable="false" data-primary="" data-startref="Ptexttometa03" data-type="indexterm" id="id620"/></p>&#13;
	</li>&#13;
</ol>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Text-to-SQL" data-type="sect2"><div class="sect2" id="ch03_text_to_sql_1736545666794737">&#13;
<h2>Text-to-SQL</h2>&#13;
&#13;
<p>SQL and relational databases<a contenteditable="false" data-primary="query construction" data-secondary="text-to-SQL translations" data-type="indexterm" id="QCtexttosql03"/><a contenteditable="false" data-primary="text" data-secondary="text-to-SQL translations" data-type="indexterm" id="texttosql03"/> are important sources of structured data, but they don’t interact directly with natural language. Although we can simply use the LLM to translate a user’s query to SQL queries, there is little margin for error.</p>&#13;
&#13;
<p>Here are some useful strategies for effective text to SQL translations:</p>&#13;
&#13;
<dl>&#13;
	<dt>Database description</dt>&#13;
	<dd>&#13;
	<p>To<a contenteditable="false" data-primary="database description" data-type="indexterm" id="id621"/> ground SQL queries, an LLM must be provided with an accurate description of the database. One common text-to-SQL prompt employs an idea reported in this paper and others: provide the LLM with a <code>CREATE TABLE</code> description for each table, including column names and types.<sup><a data-type="noteref" href="ch03.html#id622" id="id622-marker">5</a></sup> We can also provide a few (for instance, three) example rows from the table.</p>&#13;
	</dd>&#13;
	<dt>Few-shot examples</dt>&#13;
	<dd>&#13;
	<p>Feeding the prompt<a contenteditable="false" data-primary="few-shot prompting" data-type="indexterm" id="id623"/> with few-shot examples of question-query matches can improve the query generation accuracy. This can be achieved by simply appending standard static examples in the prompt to guide the agent on how it should build queries based on questions.</p>&#13;
	</dd>&#13;
</dl>&#13;
&#13;
<p>See <a data-type="xref" href="#ch03_figure_11_1736545666780807">Figure 3-11</a> for a visual of the process.</p>&#13;
&#13;
<figure><div class="figure" id="ch03_figure_11_1736545666780807"><img alt="A close-up of a machine  Description automatically generated" src="assets/lelc_0311.png"/>&#13;
<h6><span class="label">Figure 3-11. </span>A user’s query transformed to a SQL query</h6>&#13;
</div></figure>&#13;
&#13;
<p>Here’s a full code example:</p>&#13;
&#13;
<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="text-to-SQL translations" data-type="indexterm" id="id624"/></em></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">langchain_community.tools</code> <code class="kn">import</code> <code class="n">QuerySQLDatabaseTool</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_community.utilities</code> <code class="kn">import</code> <code class="n">SQLDatabase</code>&#13;
<code class="kn">from</code> <code class="nn">langchain.chains</code> <code class="kn">import</code> <code class="n">create_sql_query_chain</code>&#13;
<code class="kn">from</code> <code class="nn">langchain_openai</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code>&#13;
&#13;
<code class="c1"># replace this with the connection details of your db</code>&#13;
<code class="n">db</code> <code class="o">=</code> <code class="n">SQLDatabase</code><code class="o">.</code><code class="n">from_uri</code><code class="p">(</code><code class="s2">"sqlite:///Chinook.db"</code><code class="p">)</code>&#13;
<code class="n">llm</code> <code class="o">=</code> <code class="n">ChatOpenAI</code><code class="p">(</code><code class="n">model</code><code class="o">=</code><code class="s2">"gpt-4"</code><code class="p">,</code> <code class="n">temperature</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># convert question to sql query</code>&#13;
<code class="n">write_query</code> <code class="o">=</code> <code class="n">create_sql_query_chain</code><code class="p">(</code><code class="n">llm</code><code class="p">,</code> <code class="n">db</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Execute SQL query</code>&#13;
<code class="n">execute_query</code> <code class="o">=</code> <code class="n">QuerySQLDatabaseTool</code><code class="p">(</code><code class="n">db</code><code class="o">=</code><code class="n">db</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># combined</code>&#13;
<code class="n">chain</code> <code class="o">=</code> <code class="n">write_query</code> <code class="o">|</code> <code class="n">execute_query</code>&#13;
&#13;
<code class="c1"># invoke the chain</code>&#13;
<code class="n">chain</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="s1">'How many employees are there?'</code><code class="p">);</code></pre>&#13;
&#13;
<p><em>JavaScript<a contenteditable="false" data-primary="JavaScript" data-secondary="text-to-SQL translations" data-type="indexterm" id="id625"/></em></p>&#13;
&#13;
<pre data-code-language="javascript" data-type="programlisting">&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">ChatOpenAI</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"@langchain/openai"</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">createSqlQueryChain</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"langchain/chains/sql_db"</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">SqlDatabase</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"langchain/sql_db"</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">DataSource</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"typeorm"</code><code class="p">;</code>&#13;
<code class="kr">import</code> <code class="p">{</code> <code class="nx">QuerySqlTool</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"langchain/tools/sql"</code><code class="p">;</code>&#13;
&#13;
<code class="kr">const</code> <code class="nx">datasource</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">DataSource</code><code class="p">({</code>&#13;
  <code class="nx">type</code><code class="o">:</code> <code class="s2">"sqlite"</code><code class="p">,</code>&#13;
  <code class="nx">database</code><code class="o">:</code> <code class="s2">"./Chinook.db"</code><code class="p">,</code> <code class="c1">// here should be the details of your database</code>&#13;
<code class="p">});</code>&#13;
<code class="kr">const</code> <code class="nx">db</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">SqlDatabase</code><code class="p">.</code><code class="nx">fromDataSourceParams</code><code class="p">({</code>&#13;
  <code class="nx">appDataSource</code><code class="o">:</code> <code class="nx">datasource</code><code class="p">,</code>&#13;
<code class="p">});</code>&#13;
<code class="kr">const</code> <code class="nx">llm</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">ChatOpenAI</code><code class="p">({</code> <code class="nx">model</code><code class="o">:</code> <code class="s2">"gpt-4"</code><code class="p">,</code> <code class="nx">temperature</code><code class="o">:</code> <code class="mi">0</code> <code class="p">});</code>&#13;
&#13;
<code class="c1">// convert question to sql query</code>&#13;
<code class="kr">const</code> <code class="nx">writeQuery</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">createSqlQueryChain</code><code class="p">({</code> <code class="nx">llm</code><code class="p">,</code> <code class="nx">db</code><code class="p">,</code> <code class="nx">dialect</code><code class="o">:</code> <code class="s2">"sqlite"</code> <code class="p">});</code>&#13;
&#13;
<code class="c1">// execute query</code>&#13;
<code class="kr">const</code> <code class="nx">executeQuery</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">QuerySqlTool</code><code class="p">(</code><code class="nx">db</code><code class="p">);</code>&#13;
&#13;
<code class="c1">// combined</code>&#13;
<code class="kr">const</code> <code class="nx">chain</code> <code class="o">=</code> <code class="nx">writeQuery</code><code class="p">.</code><code class="nx">pipe</code><code class="p">(</code><code class="nx">executeQuery</code><code class="p">);</code>&#13;
&#13;
<code class="c1">// invoke the chain</code>&#13;
<code class="nx">await</code> <code class="nx">chain</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="s1">'How many employees are there?'</code><code class="p">);</code></pre>&#13;
&#13;
<p>We first convert the user’s query to a SQL query appropriate to the dialect of our database. Then we execute that query on our database. Note that executing arbitrary SQL queries on your database generated by an LLM from user input is dangerous in a production application. To use these ideas in production, you need to consider a number of security measures to reduce the risk of unintended queries being run in your database. Here are some examples:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Run the queries on your database with a user with read-only permissions.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>The database user running the queries should have access only to the tables you wish to make available for querying.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Add a time-out to the queries run by this application; this would ensure that even if an expensive query is generated, it is canceled before taking up too many of your database resources.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>This is not an exhaustive list of security considerations. Security of LLM applications is an area currently in development, with more security measures being added to the recommendations as new vulnerabilities are discovered.<a contenteditable="false" data-primary="" data-startref="DQconst03" data-type="indexterm" id="id626"/><a contenteditable="false" data-primary="" data-startref="RAGconst03" data-type="indexterm" id="id627"/><a contenteditable="false" data-primary="" data-startref="QCtexttosql03" data-type="indexterm" id="id628"/><a contenteditable="false" data-primary="" data-startref="texttosql03" data-type="indexterm" id="id629"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="ch03_summary_1736545666794794">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>This chapter discussed various state-of-the-art strategies to efficiently retrieve the most relevant documents based on a user’s query and synthesize them with your prompt to aid the LLM to generate an accurate, up-to-date output.</p>&#13;
&#13;
<p>As discussed, a robust, production-ready RAG system requires a wide range of effective strategies that can execute query transformation, query construction, routing, and indexing optimization.</p>&#13;
&#13;
<p class="pagebreak-before less_space">Query transformation<a contenteditable="false" data-primary="query transformation" data-secondary="purpose of" data-type="indexterm" id="id630"/> enables your AI app to transform an ambiguous or malformed user query into a representative query that’s optimal for retrieval. Query construction<a contenteditable="false" data-primary="query construction" data-secondary="purpose of" data-type="indexterm" id="id631"/> enables your AI app to convert the user’s query into the syntax of the query language of the database or data source where structured data lives. Routing enables your AI app to dynamically route the user’s query to retrieve relevant information from the relevant data source.</p>&#13;
&#13;
<p>In <a data-type="xref" href="ch04.html#ch04_using_langgraph_to_add_memory_to_your_chatbot_1736545668266431">Chapter 4</a>, we’ll build on this knowledge to add memory to your AI chatbot so that it can remember and learn from each interaction. This will enable users to “chat” with the application in multiturn conversations like ChatGPT.</p>&#13;
</div></section>&#13;
<div data-type="footnotes"><p data-type="footnote" id="id547"><sup><a href="ch03.html#id547-marker">1</a></sup> Patrick Lewis et al., <a href="https://oreil.ly/Qzd2K">“Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks”</a>, arXiv, April 12, 2021. </p><p data-type="footnote" id="id572"><sup><a href="ch03.html#id572-marker">2</a></sup> Xinbei Ma et al., <a href="https://oreil.ly/zyw5E">“Query Rewriting for Retrieval-Augmented Large Language Models”</a>, arXiv, October 23, 2023. Research commissioned by Microsoft Research Asia. </p><p data-type="footnote" id="id582"><sup><a href="ch03.html#id582-marker">3</a></sup> Zackary Rackauckas, <a href="https://oreil.ly/k7TTY">“RAG-Fusion: A New Take on Retrieval-Augmented Generation”</a>, arXiv, February 21, 2024. From the <em>International Journal on Natural Language Computing</em>, vol. 13, no. 1 (February 2024). </p><p data-type="footnote" id="id592"><sup><a href="ch03.html#id592-marker">4</a></sup> Luyu Gao et al., <a href="https://oreil.ly/7aTnS">“Precise Zero-Shot Dense Retrieval Without Relevance Labels”</a>, arXiv, December 20, 2022. </p><p data-type="footnote" id="id622"><sup><a href="ch03.html#id622-marker">5</a></sup> Nitarshan Rajkumar et al., <a href="https://oreil.ly/WOrzt">“Evaluating the Text-to-SQL Capabilities of Large Language Models”</a>, arXiv, March 15, 2022. </p></div></div></section></body></html>