["```py\npip install tensorflow-model-analysis\n```", "```py\npip install protobuf\n```", "```py\nimport functools\nimport numpy as np\nimport tensorflow as tf\nimport pandas as pd\nfrom tensorflow import feature_column\nfrom tensorflow.keras import layers\nfrom sklearn.model_selection import train_test_split\nimport pprint\nimport tensorflow_model_analysis as tfma\nfrom google.protobuf import text_format\n```", "```py\nprediction_raw = model.predict(test_ds)\n```", "```py\narray([[0.6699142 ],\n       [0.6239286 ],\n       [0.06013593],\n\u2026..\n       [0.10424912]], dtype=float32)\n```", "```py\nprediction_list = prediction_raw.squeeze().tolist()\ntest_df['predicted'] = prediction_list\n```", "```py\n# Put predicted as first col, next to survived.\ncols = list(test_df.columns)\ncols = [cols[-1]] + cols[:-1]\ntest_df = test_df[cols]\n```", "```py\neval_config = text_format.Parse(\"\"\"    ![1](Images/1.png)\n  model_specs {                        ![2](Images/2.png)\n    prediction_key: 'predicted',\n    label_key: 'survived'\n  }\n  metrics_specs {                      ![3](Images/3.png)\n    metrics {class_name: \"AUC\"}\n    metrics {\n      class_name: \"FairnessIndicators\"\n      config: '{\"thresholds\": [0.1, 0.50, 0.90]}'\n    }\n    metrics { class_name: \"ExampleCount\" }\n  }\n\n  slicing_specs {                      ![4](Images/4.png)\n    feature_keys: ['sex', 'class']\n  }\n  slicing_specs {}\n  \"\"\", tfma.EvalConfig())              ![5](Images/5.png)\n\n```", "```py\nOUTPUT_PATH = './titanic-fairness'\n```", "```py\neval_result = tfma.analyze_raw_data(\n  data= test_df,\n  eval_config=eval_config,\n  output_path=OUTPUT_PATH)\n```", "```py\n!jupyter nbextension enable tensorflow_model_analysis \n--user \u2013py\n\n```", "```py\ntfma.addons.fairness.view.widget_view.render_fairness_indicator(\n eval_result)\n```", "```py\nsel_df = test_df[(test_df['sex'] == 'male') & (test_df['class'] == \n'Second')]\n```", "```py\nsel_df\n```", "```py\n TRAIN_DATA_URL = \nhttps://storage.googleapis.com/tf-datasets/titanic/train.csv\ntrain_file_path = tf.keras.utils.get_file(\"train.csv\", \nTRAIN_DATA_URL)\ntrain_df = pd.read_csv(train_file_path, \nheader='infer')\ntrain_df.groupby(['sex', 'class', 'survived' ]).size().\nreset_index(name='counts')\n```", "```py\npip install keras-tuner\n```", "```py\n!pip install keras-tuner\n```", "```py\nimport kerastuner as kt\n```", "```py\nhp_node_count = hp.Int('units', min_value = 16, max_value = 32, \nstep = 8)\ntf.keras.layers.Dense(units = hp_node_count, activation = 'relu')\n```", "```py\nhp_units = hp.Choice('units', values = [16, 18, 21])\n```", "```py\nhp_activation = hp.Choice('dense_activation', \n                values=['relu', 'tanh', 'sigmoid'])\n```", "```py\nhp_learning_rate = hp.Float('learning_rate', \n min_value = 1e-4, \n max_value = 1e-2, \n step = 1e-3)\noptimizer=tf.keras.optimizers.SGD(\n lr=hp_learning_rate, \n momentum=0.5)\n```", "```py\npip install -q -U keras-tuner\n```", "```py\nimport tensorflow as tf\nimport kerastuner as kt\nfrom tensorflow.keras import datasets, layers, models\nimport numpy as np\nimport matplotlib.pylab as plt\nimport os\nfrom datetime import datetime\nprint(tf.__version__)\n```", "```py\n(train_images, train_labels), (test_images, test_labels) = \ndatasets.cifar10.load_data()\n\n# Normalize pixel values to be between 0 and 1\ntrain_images, test_images = train_images / 255.0, \ntest_images / 255.0\n```", "```py\n# Plain-text name in alphabetical order. \nhttps://www.cs.toronto.edu/~kriz/cifar.html\nCLASS_NAMES = ['airplane', 'automobile', 'bird', 'cat',\n               'deer','dog', 'frog', 'horse', 'ship', 'truck']\n```", "```py\nvalidation_dataset = tf.data.Dataset.from_tensor_slices(\n (test_images[:500], test_labels[:500]))\n\ntest_dataset = tf.data.Dataset.from_tensor_slices(\n (test_images[500:], test_labels[500:]))\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices(\n (train_images, train_labels))\n```", "```py\ntrain_dataset_size = len(list(train_dataset.as_numpy_iterator()))\nprint('Training data sample size: ', train_dataset_size)\n\nvalidation_dataset_size = len(list(validation_dataset.\nas_numpy_iterator()))\nprint('Validation data sample size: ', validation_dataset_size)\n\ntest_dataset_size = len(list(test_dataset.as_numpy_iterator()))\nprint('Test data sample size: ', test_dataset_size)\n```", "```py\nTraining data sample size:  50000\nValidation data sample size:  500\nTest data sample size:  9500\n```", "```py\nstrategy = tf.distribute.MirroredStrategy()\nprint('Number of devices: {}'.format(\nstrategy.num_replicas_in_sync))\n```", "```py\nNumber of devices: 1\n```", "```py\nBUFFER_SIZE = 10000\nBATCH_SIZE_PER_REPLICA = 64\nBATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\nSTEPS_PER_EPOCH = train_dataset_size // BATCH_SIZE_PER_REPLICA\nVALIDATION_STEPS = 1\n```", "```py\ntrain_dataset = train_dataset.repeat().shuffle(BUFFER_SIZE).batch(\nBATCH_SIZE)\nvalidation_dataset = validation_dataset.shuffle(BUFFER_SIZE).batch(\nvalidation_dataset_size)\ntest_dataset = test_dataset.batch(test_dataset_size)\n```", "```py\ndef build_model(hp):\n  model = tf.keras.Sequential()\n  # Node count for next layer as hyperparameter\n  hp_node_count = hp.Int('units', min_value=16, max_value=32, \n      step=8)\n  model.add(tf.keras.layers.Conv2D(filters = hp_node_count,\n      kernel_size=(3, 3),\n      activation='relu',\n      name = 'conv_1',\n      kernel_initializer='glorot_uniform',\n      padding='same', input_shape = (32,32,3)))\n  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n  model.add(tf.keras.layers.Flatten(name = 'flat_1'))\n  # Activation function for next layer as hyperparameter\n  hp_AF = hp.Choice('dense_activation', \n      values = ['relu', 'tanh'])\n  model.add(tf.keras.layers.Dense(256, activation=hp_AF,\n      kernel_initializer='glorot_uniform',\n      name = 'dense_1'))\n  model.add(tf.keras.layers.Dense(10, \n      activation='softmax',\n      name = 'custom_class'))\n\n  model.build([None, 32, 32, 3])\n  # Compile model with optimizer\n  # Learning rate as hyperparameter\n  hp_LR = hp.Float('learning_rate', 1e-2, 1e-4)\n\n  model.compile(\n     loss=tf.keras.losses.SparseCategoricalCrossentropy(\n          from_logits=True),\n       optimizer=tf.keras.optimizers.Adam(\n          learning_rate=hp_LR),\n       metrics=['accuracy'])\n\n  return model\n```", "```py\ntuner = kt.Hyperband(hypermodel = build_model,\n                     objective='val_accuracy',\n                     max_epochs=10,\n                     factor=3,\n                     directory='hp_dir',\n                     project_name='hp_kt')\n```", "```py\nearly_stop = tf.keras.callbacks.EarlyStopping(\n monitor='val_accuracy', \n patience=5)\n```", "```py\ntuner.search(train_dataset,\n             steps_per_epoch = STEPS_PER_EPOCH,\n             validation_data = validation_dataset,\n             validation_steps = VALIDATION_STEPS,\n             epochs = 15,\n             callbacks = [early_stop]\n             )\n# Get the optimal hyperparameters\nbest_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n\nprint(f\"\"\"\nThe hyperparameter search is complete. The optimal number of units \nin conv_1 layer is {best_hps.get('units')} and the optimal \nlearning rate for the optimizer is {best_hps.get('learning_rate')} \nand the optimal activation for dense_1 layer\nis {best_hps.get('dense_activation')}.\n\"\"\")\n```", "```py\nTrial 42 Complete [00h 01m 14s]\nval_accuracy: 0.593999981880188\n\nBest val_accuracy So Far: 0.6579999923706055\nTotal elapsed time: 00h 28m 53s\nINFO:tensorflow:Oracle triggered exit\n\nThe hyperparameter search is complete. The optimal number of units \nin conv_1 layer is 24 and the optimal learning rate for the \noptimizer is 0.0013005004751682134 and the optimal activation \nfor dense_1 layer is relu.\n```", "```py\nbest_hp_model = tuner.hypermodel.build(best_hps)\n```", "```py\nMODEL_NAME = 'myCIFAR10-{}'.format(datetime.datetime.now().\nstrftime(\"%Y%m%d-%H%M%S\"))\nprint(MODEL_NAME)\ncheckpoint_dir = './' + MODEL_NAME\ncheckpoint_prefix = os.path.join(\ncheckpoint_dir, \"ckpt-{epoch}\")\nprint(checkpoint_prefix)\n```", "```py\nmyCheckPoint = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    monitor='val_accuracy',\n    mode='max',\n    save_weights_only = True,\n    save_best_only = True\n    )\n```", "```py\nbest_hp_model.fit(train_dataset,\n             steps_per_epoch = STEPS_PER_EPOCH,\n             validation_data = validation_dataset,\n             validation_steps = VALIDATION_STEPS,\n             epochs = 15,\n             callbacks = [early_stop, myCheckPoint])\n```", "```py\nbest_hp_model.load_weights(tf.train.latest_checkpoint(\ncheckpoint_dir))\n```"]