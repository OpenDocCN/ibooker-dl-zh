- en: Chapter 7\. Ensemble Learning and Random Forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Suppose you pose a complex question to thousands of random people, then aggregate
    their answers. In many cases you will find that this aggregated answer is better
    than an expert’s answer. This is called the *wisdom of the crowd*. Similarly,
    if you aggregate the predictions of a group of predictors (such as classifiers
    or regressors), you will often get better predictions than with the best individual
    predictor. A group of predictors is called an *ensemble*; thus, this technique
    is called *ensemble learning*, and an ensemble learning algorithm is called an
    *ensemble method*.
  prefs: []
  type: TYPE_NORMAL
- en: As an example of an ensemble method, you can train a group of decision tree
    classifiers, each on a different random subset of the training set. You can then
    obtain the predictions of all the individual trees, and the class that gets the
    most votes is the ensemble’s prediction (see the last exercise in [Chapter 6](ch06.html#trees_chapter)).
    Such an ensemble of decision trees is called a *random forest*, and despite its
    simplicity, this is one of the most powerful machine learning algorithms available
    today.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in [Chapter 2](ch02.html#project_chapter), you will often use ensemble
    methods near the end of a project, once you have already built a few good predictors,
    to combine them into an even better predictor. In fact, the winning solutions
    in machine learning competitions often involve several ensemble methods—most famously
    in the [Netflix Prize competition](https://en.wikipedia.org/wiki/Netflix_Prize).
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we will examine the most popular ensemble methods, including
    voting classifiers, bagging and pasting ensembles, random forests, and boosting,
    and stacking ensembles.
  prefs: []
  type: TYPE_NORMAL
- en: Voting Classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Suppose you have trained a few classifiers, each one achieving about 80% accuracy.
    You may have a logistic regression classifier, an SVM classifier, a random forest
    classifier, a *k*-nearest neighbors classifier, and perhaps a few more (see [Figure 7-1](#voting_classifier_training_diagram)).
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0701](assets/mls3_0701.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-1\. Training diverse classifiers
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'A very simple way to create an even better classifier is to aggregate the predictions
    of each classifier: the class that gets the most votes is the ensemble’s prediction.
    This majority-vote classifier is called a *hard voting* classifier (see [Figure 7-2](#voting_classifier_prediction_diagram)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0702](assets/mls3_0702.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-2\. Hard voting classifier predictions
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Somewhat surprisingly, this voting classifier often achieves a higher accuracy
    than the best classifier in the ensemble. In fact, even if each classifier is
    a *weak learner* (meaning it does only slightly better than random guessing),
    the ensemble can still be a *strong learner* (achieving high accuracy), provided
    there are a sufficient number of weak learners in the ensemble and they are sufficiently
    diverse.
  prefs: []
  type: TYPE_NORMAL
- en: 'How is this possible? The following analogy can help shed some light on this
    mystery. Suppose you have a slightly biased coin that has a 51% chance of coming
    up heads and 49% chance of coming up tails. If you toss it 1,000 times, you will
    generally get more or less 510 heads and 490 tails, and hence a majority of heads.
    If you do the math, you will find that the probability of obtaining a majority
    of heads after 1,000 tosses is close to 75%. The more you toss the coin, the higher
    the probability (e.g., with 10,000 tosses, the probability climbs over 97%). This
    is due to the *law of large numbers*: as you keep tossing the coin, the ratio
    of heads gets closer and closer to the probability of heads (51%). [Figure 7-3](#law_of_large_numbers_plot)
    shows 10 series of biased coin tosses. You can see that as the number of tosses
    increases, the ratio of heads approaches 51%. Eventually all 10 series end up
    so close to 51% that they are consistently above 50%.'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0703](assets/mls3_0703.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-3\. The law of large numbers
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Similarly, suppose you build an ensemble containing 1,000 classifiers that are
    individually correct only 51% of the time (barely better than random guessing).
    If you predict the majority voted class, you can hope for up to 75% accuracy!
    However, this is only true if all classifiers are perfectly independent, making
    uncorrelated errors, which is clearly not the case because they are trained on
    the same data. They are likely to make the same types of errors, so there will
    be many majority votes for the wrong class, reducing the ensemble’s accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Ensemble methods work best when the predictors are as independent from one another
    as possible. One way to get diverse classifiers is to train them using very different
    algorithms. This increases the chance that they will make very different types
    of errors, improving the ensemble’s accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scikit-Learn provides a `VotingClassifier` class that’s quite easy to use:
    just give it a list of name/predictor pairs, and use it like a normal classifier.
    Let’s try it on the moons dataset (introduced in [Chapter 5](ch05.html#svm_chapter)).
    We will load and split the moons dataset into a training set and a test set, then
    we’ll create and train a voting classifier composed of three diverse classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'When you fit a `VotingClassifier`, it clones every estimator and fits the clones.
    The original estimators are available via the `estimators` attribute, while the
    fitted clones are available via the `estimators_` attribute. If you prefer a dict
    rather than a list, you can use `named_estimators` or `named_estimators_` instead.
    To begin, let’s look at each fitted classifier’s accuracy on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'When you call the voting classifier’s `predict()` method, it performs hard
    voting. For example, the voting classifier predicts class 1 for the first instance
    of the test set, because two out of three classifiers predict that class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s look at the performance of the voting classifier on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: There you have it! The voting classifier outperforms all the individual classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: 'If all classifiers are able to estimate class probabilities (i.e., if they
    all have a `predict_proba()` method), then you can tell Scikit-Learn to predict
    the class with the highest class probability, averaged over all the individual
    classifiers. This is called *soft voting*. It often achieves higher performance
    than hard voting because it gives more weight to highly confident votes. All you
    need to do is set the voting classifier’s `voting` hyperparameter to `"soft"`,
    and ensure that all classifiers can estimate class probabilities. This is not
    the case for the `SVC` class by default, so you need to set its `probability`
    hyperparameter to `True` (this will make the `SVC` class use cross-validation
    to estimate class probabilities, slowing down training, and it will add a `predict_proba()`
    method). Let’s try that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We reach 92% accuracy simply by using soft voting—not bad!
  prefs: []
  type: TYPE_NORMAL
- en: Bagging and Pasting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One way to get a diverse set of classifiers is to use very different training
    algorithms, as just discussed. Another approach is to use the same training algorithm
    for every predictor but train them on different random subsets of the training
    set. When sampling is performed *with* replacement,⁠^([1](ch07.html#idm45720211152336))
    this method is called [*bagging*](https://homl.info/20)⁠^([2](ch07.html#idm45720211150768))
    (short for *bootstrap aggregating*⁠^([3](ch07.html#idm45720211149248))). When
    sampling is performed *without* replacement, it is called [*pasting*](https://homl.info/21).⁠^([4](ch07.html#idm45720211146016))
  prefs: []
  type: TYPE_NORMAL
- en: In other words, both bagging and pasting allow training instances to be sampled
    several times across multiple predictors, but only bagging allows training instances
    to be sampled several times for the same predictor. This sampling and training
    process is represented in [Figure 7-4](#bagging_training_diagram).
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0704](assets/mls3_0704.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-4\. Bagging and pasting involve training several predictors on different
    random samples of the training set
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Once all predictors are trained, the ensemble can make a prediction for a new
    instance by simply aggregating the predictions of all predictors. The aggregation
    function is typically the *statistical mode* for classification (i.e., the most
    frequent prediction, just like with a hard voting classifier), or the average
    for regression. Each individual predictor has a higher bias than if it were trained
    on the original training set, but aggregation reduces both bias and variance.⁠^([5](ch07.html#idm45720211140576))
    Generally, the net result is that the ensemble has a similar bias but a lower
    variance than a single predictor trained on the original training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in [Figure 7-4](#bagging_training_diagram), predictors can all
    be trained in parallel, via different CPU cores or even different servers. Similarly,
    predictions can be made in parallel. This is one of the reasons bagging and pasting
    are such popular methods: they scale very well.'
  prefs: []
  type: TYPE_NORMAL
- en: Bagging and Pasting in Scikit-Learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Scikit-Learn offers a simple API for both bagging and pasting: `BaggingClassifier`
    class (or `BaggingRegressor` for regression). The following code trains an ensemble
    of 500 decision tree classifiers:⁠^([6](ch07.html#idm45720211131712)) each is
    trained on 100 training instances randomly sampled from the training set with
    replacement (this is an example of bagging, but if you want to use pasting instead,
    just set `bootstrap=False`). The `n_jobs` parameter tells Scikit-Learn the number
    of CPU cores to use for training and predictions, and `–1` tells Scikit-Learn
    to use all available cores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A `BaggingClassifier` automatically performs soft voting instead of hard voting
    if the base classifier can estimate class probabilities (i.e., if it has a `predict_proba()`
    method), which is the case with decision tree classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 7-5](#decision_tree_without_and_with_bagging_plot) compares the decision
    boundary of a single decision tree with the decision boundary of a bagging ensemble
    of 500 trees (from the preceding code), both trained on the moons dataset. As
    you can see, the ensemble’s predictions will likely generalize much better than
    the single decision tree’s predictions: the ensemble has a comparable bias but
    a smaller variance (it makes roughly the same number of errors on the training
    set, but the decision boundary is less irregular).'
  prefs: []
  type: TYPE_NORMAL
- en: Bagging introduces a bit more diversity in the subsets that each predictor is
    trained on, so bagging ends up with a slightly higher bias than pasting; but the
    extra diversity also means that the predictors end up being less correlated, so
    the ensemble’s variance is reduced. Overall, bagging often results in better models,
    which explains why it’s generally preferred. But if you have spare time and CPU
    power, you can use cross-validation to evaluate both bagging and pasting and select
    the one that works best.
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0705](assets/mls3_0705.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-5\. A single decision tree (left) versus a bagging ensemble of 500
    trees (right)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Out-of-Bag Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With bagging, some training instances may be sampled several times for any given
    predictor, while others may not be sampled at all. By default a `BaggingClassifier`
    samples *m* training instances with replacement (`bootstrap=True`), where *m*
    is the size of the training set. With this process, it can be shown mathematically
    that only about 63% of the training instances are sampled on average for each
    predictor.⁠^([7](ch07.html#idm45720211046720)) The remaining 37% of the training
    instances that are not sampled are called *out-of-bag* (OOB) instances. Note that
    they are not the same 37% for all predictors.
  prefs: []
  type: TYPE_NORMAL
- en: 'A bagging ensemble can be evaluated using OOB instances, without the need for
    a separate validation set: indeed, if there are enough estimators, then each instance
    in the training set will likely be an OOB instance of several estimators, so these
    estimators can be used to make a fair ensemble prediction for that instance. Once
    you have a prediction for each instance, you can compute the ensemble’s prediction
    accuracy (or any other metric).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Scikit-Learn, you can set `oob_score=True` when creating a `BaggingClassifier`
    to request an automatic OOB evaluation after training. The following code demonstrates
    this. The resulting evaluation score is available in the `oob_score_` attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'According to this OOB evaluation, this `BaggingClassifier` is likely to achieve
    about 89.6% accuracy on the test set. Let’s verify this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We get 92% accuracy on the test. The OOB evaluation was a bit too pessimistic,
    just over 2% too low.
  prefs: []
  type: TYPE_NORMAL
- en: 'The OOB decision function for each training instance is also available through
    the `oob_decision_function_` attribute. Since the base estimator has a `predict_proba()`
    method, the decision function returns the class probabilities for each training
    instance. For example, the OOB evaluation estimates that the first training instance
    has a 67.6% probability of belonging to the positive class and a 32.4% probability
    of belonging to the negative class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Random Patches and Random Subspaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `BaggingClassifier` class supports sampling the features as well. Sampling
    is controlled by two hyperparameters: `max_features` and `bootstrap_features`.
    They work the same way as `max_samples` and `bootstrap`, but for feature sampling
    instead of instance sampling. Thus, each predictor will be trained on a random
    subset of the input features.'
  prefs: []
  type: TYPE_NORMAL
- en: This technique is particularly useful when you are dealing with high-dimensional
    inputs (such as images), as it can considerably speed up training. Sampling both
    training instances and features is called the [*random patches* method](https://homl.info/22).⁠^([8](ch07.html#idm45720210898224))
    Keeping all training instances (by setting `bootstrap=False` and `max_samples=1.0`)
    but sampling features (by setting `bootstrap_features` to `True` and/or `max_features`
    to a value smaller than `1.0`) is called the [*random subspaces* method](https://homl.info/23).⁠^([9](ch07.html#idm45720210877120))
  prefs: []
  type: TYPE_NORMAL
- en: Sampling features results in even more predictor diversity, trading a bit more
    bias for a lower variance.
  prefs: []
  type: TYPE_NORMAL
- en: Random Forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we have discussed, a [random forest](https://homl.info/24)⁠^([10](ch07.html#idm45720210870352))
    is an ensemble of decision trees, generally trained via the bagging method (or
    sometimes pasting), typically with `max_samples` set to the size of the training
    set. Instead of building a `BaggingClassifier` and passing it a `DecisionTreeClassifier`,
    you can use the `RandomForestClassifier` class, which is more convenient and optimized
    for decision trees⁠^([11](ch07.html#idm45720210863312)) (similarly, there is a
    `RandomForestRegressor` class for regression tasks). The following code trains
    a random forest classifier with 500 trees, each limited to maximum 16 leaf nodes,
    using all available CPU cores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: With a few exceptions, a `RandomForestClassifier` has all the hyperparameters
    of a `DecisionTreeClassifier` (to control how trees are grown), plus all the hyperparameters
    of a `BaggingClassifier` to control the ensemble itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'The random forest algorithm introduces extra randomness when growing trees;
    instead of searching for the very best feature when splitting a node (see [Chapter 6](ch06.html#trees_chapter)),
    it searches for the best feature among a random subset of features. By default,
    it samples <math><msqrt><mi>n</mi></msqrt></math> features (where *n* is the total
    number of features). The algorithm results in greater tree diversity, which (again)
    trades a higher bias for a lower variance, generally yielding an overall better
    model. So, the following `BaggingClassifier` is equivalent to the previous `RandomForestClassifier`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Extra-Trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you are growing a tree in a random forest, at each node only a random subset
    of the features is considered for splitting (as discussed earlier). It is possible
    to make trees even more random by also using random thresholds for each feature
    rather than searching for the best possible thresholds (like regular decision
    trees do). For this, simply set `splitter="random"` when creating a `DecisionTreeClassifier`.
  prefs: []
  type: TYPE_NORMAL
- en: A forest of such extremely random trees is called an [*extremely randomized
    trees*](https://homl.info/25)⁠^([12](ch07.html#idm45720210757984)) (or *extra-trees*
    for short) ensemble. Once again, this technique trades more bias for a lower variance.
    It also makes extra-trees classifiers much faster to train than regular random
    forests, because finding the best possible threshold for each feature at every
    node is one of the most time-consuming tasks of growing a tree.
  prefs: []
  type: TYPE_NORMAL
- en: You can create an extra-trees classifier using Scikit-Learn’s `ExtraTreesClassifier`
    class. Its API is identical to the `RandomForestClassifier` class, except `bootstrap`
    defaults to `False`. Similarly, the `ExtraTreesRegressor` class has the same API
    as the `RandomForestRegressor` class, except `bootstrap` defaults to `False`.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is hard to tell in advance whether a `RandomForestClassifier` will perform
    better or worse than an `ExtraTreesClassifier`. Generally, the only way to know
    is to try both and compare them using cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Importance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Yet another great quality of random forests is that they make it easy to measure
    the relative importance of each feature. Scikit-Learn measures a feature’s importance
    by looking at how much the tree nodes that use that feature reduce impurity on
    average, across all trees in the forest. More precisely, it is a weighted average,
    where each node’s weight is equal to the number of training samples that are associated
    with it (see [Chapter 6](ch06.html#trees_chapter)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Scikit-Learn computes this score automatically for each feature after training,
    then it scales the results so that the sum of all importances is equal to 1\.
    You can access the result using the `feature_importances_` variable. For example,
    the following code trains a `RandomForestClassifier` on the iris dataset (introduced
    in [Chapter 4](ch04.html#linear_models_chapter)) and outputs each feature’s importance.
    It seems that the most important features are the petal length (44%) and width
    (42%), while sepal length and width are rather unimportant in comparison (11%
    and 2%, respectively):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Similarly, if you train a random forest classifier on the MNIST dataset (introduced
    in [Chapter 3](ch03.html#classification_chapter)) and plot each pixel’s importance,
    you get the image represented in [Figure 7-6](#mnist_feature_importance_plot).
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0706](assets/mls3_0706.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-6\. MNIST pixel importance (according to a random forest classifier)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Random forests are very handy to get a quick understanding of what features
    actually matter, in particular if you need to perform feature selection.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Boosting* (originally called *hypothesis boosting*) refers to any ensemble
    method that can combine several weak learners into a strong learner. The general
    idea of most boosting methods is to train predictors sequentially, each trying
    to correct its predecessor. There are many boosting methods available, but by
    far the most popular are [*AdaBoost*](https://homl.info/26)⁠^([13](ch07.html#idm45720210584080))
    (short for *adaptive boosting*) and *gradient boosting*. Let’s start with AdaBoost.'
  prefs: []
  type: TYPE_NORMAL
- en: AdaBoost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One way for a new predictor to correct its predecessor is to pay a bit more
    attention to the training instances that the predecessor underfit. This results
    in new predictors focusing more and more on the hard cases. This is the technique
    used by AdaBoost.
  prefs: []
  type: TYPE_NORMAL
- en: For example, when training an AdaBoost classifier, the algorithm first trains
    a base classifier (such as a decision tree) and uses it to make predictions on
    the training set. The algorithm then increases the relative weight of misclassified
    training instances. Then it trains a second classifier, using the updated weights,
    and again makes predictions on the training set, updates the instance weights,
    and so on (see [Figure 7-7](#adaboost_training_diagram)).
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 7-8](#boosting_plot) shows the decision boundaries of five consecutive
    predictors on the moons dataset (in this example, each predictor is a highly regularized
    SVM classifier with an RBF kernel).⁠^([14](ch07.html#idm45720210574352)) The first
    classifier gets many instances wrong, so their weights get boosted. The second
    classifier therefore does a better job on these instances, and so on. The plot
    on the right represents the same sequence of predictors, except that the learning
    rate is halved (i.e., the misclassified instance weights are boosted much less
    at every iteration). As you can see, this sequential learning technique has some
    similarities with gradient descent, except that instead of tweaking a single predictor’s
    parameters to minimize a cost function, AdaBoost adds predictors to the ensemble,
    gradually making it better.'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0707](assets/mls3_0707.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-7\. AdaBoost sequential training with instance weight updates
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Once all predictors are trained, the ensemble makes predictions very much like
    bagging or pasting, except that predictors have different weights depending on
    their overall accuracy on the weighted training set.
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0708](assets/mls3_0708.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-8\. Decision boundaries of consecutive predictors
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'There is one important drawback to this sequential learning technique: training
    cannot be parallelized since each predictor can only be trained after the previous
    predictor has been trained and evaluated. As a result, it does not scale as well
    as bagging or pasting.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a closer look at the AdaBoost algorithm. Each instance weight *w*^((*i*))
    is initially set to 1/*m*. A first predictor is trained, and its weighted error
    rate *r*[1] is computed on the training set; see [Equation 7-1](#weighted_error_rate).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 7-1\. Weighted error rate of the j^(th) predictor
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>r</mi><mi>j</mi></msub> <mo>=</mo> <mstyle
    scriptlevel="0" displaystyle="true"><mrow><munderover><mo>∑</mo> <mstyle scriptlevel="0"
    displaystyle="false"><mrow><mfrac linethickness="0pt"><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mrow><msubsup><mover accent="true"><mi>y</mi><mo>^</mo></mover> <mi>j</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup>
    <mo>≠</mo> <msup><mi>y</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mfrac></mrow></mstyle>
    <mi>m</mi></munderover> <msup><mi>w</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mstyle>
    <mtext>where</mtext> <msubsup><mover accent="true"><mi>y</mi><mo>^</mo></mover>
    <mi>j</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup> <mtext>is</mtext>
    <mtext>the</mtext> <msup><mi>j</mi><mtext>th</mtext></msup> <mtext>predictor’s</mtext>
    <mtext>prediction</mtext><mtext>for</mtext> <mtext>the</mtext> <msup><mi>i</mi>
    <mtext>th</mtext></msup> <mtext>instance</mtext></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The predictor’s weight *α*[*j*] is then computed using [Equation 7-2](#predictor_weight),
    where *η* is the learning rate hyperparameter (defaults to 1).⁠^([15](ch07.html#idm45720210527056))
    The more accurate the predictor is, the higher its weight will be. If it is just
    guessing randomly, then its weight will be close to zero. However, if it is most
    often wrong (i.e., less accurate than random guessing), then its weight will be
    negative.
  prefs: []
  type: TYPE_NORMAL
- en: Equation 7-2\. Predictor weight
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><msub><mi>α</mi>
    <mi>j</mi></msub> <mo>=</mo> <mi>η</mi> <mo form="prefix">log</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mrow><mn>1</mn><mo>-</mo><msub><mi>r</mi> <mi>j</mi></msub></mrow>
    <msub><mi>r</mi> <mi>j</mi></msub></mfrac></mstyle></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: Next, the AdaBoost algorithm updates the instance weights, using [Equation 7-3](#instance_weight_update),
    which boosts the weights of the misclassified instances.
  prefs: []
  type: TYPE_NORMAL
- en: Equation 7-3\. Weight update rule
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="left"><mrow><mtext>for</mtext>
    <mi>i</mi> <mo>=</mo> <mn>1</mn> <mo>,</mo> <mn>2</mn> <mo>,</mo> <mo>⋯</mo> <mo>,</mo>
    <mi>m</mi></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><msup><mi>w</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>←</mo> <mfenced separators=""
    open="{" close=""><mtable><mtr><mtd columnalign="left"><msup><mi>w</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <msup><mover accent="true"><msub><mi>y</mi>
    <mi>j</mi></msub> <mo>^</mo></mover> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>=</mo> <msup><mi>y</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><msup><mi>w</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo form="prefix">exp</mo> <mrow><mo>(</mo> <msub><mi>α</mi> <mi>j</mi></msub>
    <mo>)</mo></mrow></mrow></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext>
    <msup><mover accent="true"><msub><mi>y</mi> <mi>j</mi></msub> <mo>^</mo></mover>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>≠</mo> <msup><mi>y</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mtd></mtr></mtable></mfenced></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: Then all the instance weights are normalized (i.e., divided by <math><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msup><mi>w</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math>).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, a new predictor is trained using the updated weights, and the whole
    process is repeated: the new predictor’s weight is computed, the instance weights
    are updated, then another predictor is trained, and so on. The algorithm stops
    when the desired number of predictors is reached, or when a perfect predictor
    is found.'
  prefs: []
  type: TYPE_NORMAL
- en: To make predictions, AdaBoost simply computes the predictions of all the predictors
    and weighs them using the predictor weights *α*[*j*]. The predicted class is the
    one that receives the majority of weighted votes (see [Equation 7-4](#adaboost_prediction)).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 7-4\. AdaBoost predictions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mrow><mo>(</mo> <mi mathvariant="bold">x</mi> <mo>)</mo></mrow> <mo>=</mo> <munder><mo
    form="prefix">argmax</mo> <mi>k</mi></munder> <mrow><munderover><mo>∑</mo> <mfrac
    linethickness="0pt"><mstyle scriptlevel="1" displaystyle="false"><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow></mstyle>
    <mstyle scriptlevel="1" displaystyle="false"><mrow><msub><mover accent="true"><mi>y</mi>
    <mo>^</mo></mover> <mi>j</mi></msub> <mrow><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mrow><mo>=</mo><mi>k</mi></mrow></mstyle></mfrac>
    <mi>N</mi></munderover> <msub><mi>α</mi> <mi>j</mi></msub></mrow> <mtext>where</mtext>
    <mi>N</mi> <mtext>is</mtext> <mtext>the</mtext> <mtext>number</mtext> <mtext>of</mtext>
    <mtext>predictors</mtext></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-Learn uses a multiclass version of AdaBoost called [*SAMME*](https://homl.info/27)⁠^([16](ch07.html#idm45720210436640))
    (which stands for *Stagewise Additive Modeling using a Multiclass Exponential
    loss function*). When there are just two classes, SAMME is equivalent to AdaBoost.
    If the predictors can estimate class probabilities (i.e., if they have a `predict_proba()`
    method), Scikit-Learn can use a variant of SAMME called *SAMME.R* (the *R* stands
    for “Real”), which relies on class probabilities rather than predictions and generally
    performs better.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code trains an AdaBoost classifier based on 30 *decision stumps*
    using Scikit-Learn’s `AdaBoostClassifier` class (as you might expect, there is
    also an `AdaBoostRegressor` class). A decision stump is a decision tree with `max_depth=1`—in
    other words, a tree composed of a single decision node plus two leaf nodes. This
    is the default base estimator for the `AdaBoostClassifier` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If your AdaBoost ensemble is overfitting the training set, you can try reducing
    the number of estimators or more strongly regularizing the base estimator.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Boosting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another very popular boosting algorithm is [*gradient boosting*](https://homl.info/28).⁠^([17](ch07.html#idm45720210377680))
    Just like AdaBoost, gradient boosting works by sequentially adding predictors
    to an ensemble, each one correcting its predecessor. However, instead of tweaking
    the instance weights at every iteration like AdaBoost does, this method tries
    to fit the new predictor to the *residual errors* made by the previous predictor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go through a simple regression example, using decision trees as the base
    predictors; this is called *gradient tree boosting*, or *gradient boosted regression
    trees* (GBRT). First, let’s generate a noisy quadratic dataset and fit a `DecisionTreeRegressor`
    to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll train a second `DecisionTreeRegressor` on the residual errors made
    by the first predictor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'And then we’ll train a third regressor on the residual errors made by the second
    predictor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have an ensemble containing three trees. It can make predictions on
    a new instance simply by adding up the predictions of all the trees:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 7-9](#gradient_boosting_plot) represents the predictions of these three
    trees in the left column, and the ensemble’s predictions in the right column.
    In the first row, the ensemble has just one tree, so its predictions are exactly
    the same as the first tree’s predictions. In the second row, a new tree is trained
    on the residual errors of the first tree. On the right you can see that the ensemble’s
    predictions are equal to the sum of the predictions of the first two trees. Similarly,
    in the third row another tree is trained on the residual errors of the second
    tree. You can see that the ensemble’s predictions gradually get better as trees
    are added to the ensemble.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use Scikit-Learn’s `GradientBoostingRegressor` class to train GBRT
    ensembles more easily (there’s also a `GradientBoostingClassifier` class for classification).
    Much like the `RandomForestRegressor` class, it has hyperparameters to control
    the growth of decision trees (e.g., `max_depth`, `min_samples_leaf`), as well
    as hyperparameters to control the ensemble training, such as the number of trees
    (`n_estimators`). The following code creates the same ensemble as the previous
    one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![mls3 0709](assets/mls3_0709.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-9\. In this depiction of gradient boosting, the first predictor (top
    left) is trained normally, then each consecutive predictor (middle left and lower
    left) is trained on the previous predictor’s residuals; the right column shows
    the resulting ensemble’s predictions
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The `learning_rate` hyperparameter scales the contribution of each tree. If
    you set it to a low value, such as `0.05`, you will need more trees in the ensemble
    to fit the training set, but the predictions will usually generalize better. This
    is a regularization technique called *shrinkage*. [Figure 7-10](#gbrt_learning_rate_plot)
    shows two GBRT ensembles trained with different hyperparameters: the one on the
    left does not have enough trees to fit the training set, while the one on the
    right has about the right amount. If we added more trees, the GBRT would start
    to overfit the training set.'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0710](assets/mls3_0710.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-10\. GBRT ensembles with not enough predictors (left) and just enough
    (right)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To find the optimal number of trees, you could perform cross-validation using
    `GridSearchCV` or `RandomizedSearchCV`, as usual, but there’s a simpler way: if
    you set the `n_iter_no_change` hyperparameter to an integer value, say 10, then
    the `GradientBoostingRegressor` will automatically stop adding more trees during
    training if it sees that the last 10 trees didn’t help. This is simply early stopping
    (introduced in [Chapter 4](ch04.html#linear_models_chapter)), but with a little
    bit of patience: it tolerates having no progress for a few iterations before it
    stops. Let’s train the ensemble using early stopping:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'If you set `n_iter_no_change` too low, training may stop too early and the
    model will underfit. But if you set it too high, it will overfit instead. We also
    set a fairly small learning rate and a high number of estimators, but the actual
    number of estimators in the trained ensemble is much lower, thanks to early stopping:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'When `n_iter_no_change` is set, the `fit()` method automatically splits the
    training set into a smaller training set and a validation set: this allows it
    to evaluate the model’s performance each time it adds a new tree. The size of
    the validation set is controlled by the `validation_fraction` hyperparameter,
    which is 10% by default. The `tol` hyperparameter determines the maximum performance
    improvement that still counts as negligible. It defaults to 0.0001.'
  prefs: []
  type: TYPE_NORMAL
- en: The `GradientBoostingRegressor` class also supports a `subsample` hyperparameter,
    which specifies the fraction of training instances to be used for training each
    tree. For example, if `subsample=0.25`, then each tree is trained on 25% of the
    training instances, selected randomly. As you can probably guess by now, this
    technique trades a higher bias for a lower variance. It also speeds up training
    considerably. This is called *stochastic gradient boosting*.
  prefs: []
  type: TYPE_NORMAL
- en: Histogram-Based Gradient Boosting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Scikit-Learn also provides another GBRT implementation, optimized for large
    datasets: *histogram-based gradient boosting* (HGB). It works by binning the input
    features, replacing them with integers. The number of bins is controlled by the
    `max_bins` hyperparameter, which defaults to 255 and cannot be set any higher
    than this. Binning can greatly reduce the number of possible thresholds that the
    training algorithm needs to evaluate. Moreover, working with integers makes it
    possible to use faster and more memory-efficient data structures. And the way
    the bins are built removes the need for sorting the features when training each
    tree.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a result, this implementation has a computational complexity of *O*(*b*×*m*)
    instead of *O*(*n*×*m*×log(*m*)), where *b* is the number of bins, *m* is the
    number of training instances, and *n* is the number of features. In practice,
    this means that HGB can train hundreds of times faster than regular GBRT on large
    datasets. However, binning causes a precision loss, which acts as a regularizer:
    depending on the dataset, this may help reduce overfitting, or it may cause underfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Scikit-Learn provides two classes for HGB: `HistGradientBoostingRegressor`
    and `HistGradientBoostingClassifier`. They’re similar to `GradientBoostingRegressor`
    and `GradientBoostingClassifier`, with a few notable differences:'
  prefs: []
  type: TYPE_NORMAL
- en: Early stopping is automatically activated if the number of instances is greater
    than 10,000\. You can turn early stopping always on or always off by setting the
    `early_stopping` hyperparameter to `True` or `False`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subsampling is not supported.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_estimators` is renamed to `max_iter`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The only decision tree hyperparameters that can be tweaked are `max_leaf_nodes`,
    `min_samples_leaf`, and `max_depth`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The HGB classes also have two nice features: they support both categorical
    features and missing values. This simplifies preprocessing quite a bit. However,
    the categorical features must be represented as integers ranging from 0 to a number
    lower than `max_bins`. You can use an `OrdinalEncoder` for this. For example,
    here’s how to build and train a complete pipeline for the California housing dataset
    introduced in [Chapter 2](ch02.html#project_chapter):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The whole pipeline is just as short as the imports! No need for an imputer,
    scaler, or a one-hot encoder, so it’s really convenient. Note that `categorical_features`
    must be set to the categorical column indices (or a Boolean array). Without any
    hyperparameter tuning, this model yields an RMSE of about 47,600, which is not
    too bad.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Several other optimized implementations of gradient boosting are available
    in the Python ML ecosystem: in particular, [XGBoost](https://github.com/dmlc/xgboost),
    [CatBoost](https://catboost.ai), and [LightGBM](https://lightgbm.readthedocs.io).
    These libraries have been around for several years. They are all specialized for
    gradient boosting, their APIs are very similar to Scikit-Learn’s, and they provide
    many additional features, including GPU acceleration; you should definitely check
    them out! Moreover, the [TensorFlow Random Forests library](https://tensorflow.org/decision_forests)
    provides optimized implementations of a variety of random forest algorithms, including
    plain random forests, extra-trees, GBRT, and several more.'
  prefs: []
  type: TYPE_NORMAL
- en: Stacking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The last ensemble method we will discuss in this chapter is called *stacking*
    (short for [*stacked generalization*](https://homl.info/29)).⁠^([18](ch07.html#idm45720209752032))
    It is based on a simple idea: instead of using trivial functions (such as hard
    voting) to aggregate the predictions of all predictors in an ensemble, why don’t
    we train a model to perform this aggregation? [Figure 7-11](#blending_prediction_diagram)
    shows such an ensemble performing a regression task on a new instance. Each of
    the bottom three predictors predicts a different value (3.1, 2.7, and 2.9), and
    then the final predictor (called a *blender*, or a *meta learner*) takes these
    predictions as inputs and makes the final prediction (3.0).'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0711](assets/mls3_0711.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-11\. Aggregating predictions using a blending predictor
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To train the blender, you first need to build the blending training set. You
    can use `cross_val_predict()` on every predictor in the ensemble to get out-of-sample
    predictions for each instance in the original training set ([Figure 7-12](#blending_layer_training_diagram)),
    and use these can be used as the input features to train the blender; and the
    targets can simply be copied from the original training set. Note that regardless
    of the number of features in the original training set (just one in this example),
    the blending training set will contain one input feature per predictor (three
    in this example). Once the blender is trained, the base predictors are retrained
    one last time on the full original training set.
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0712](assets/mls3_0712.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-12\. Training the blender in a stacking ensemble
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is actually possible to train several different blenders this way (e.g.,
    one using linear regression, another using random forest regression) to get a
    whole layer of blenders, and then add another blender on top of that to produce
    the final prediction, as shown in [Figure 7-13](#multi_layer_blending_diagram).
    You may be able to squeeze out a few more drops of performance by doing this,
    but it will cost you in both training time and system complexity.
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0713](assets/mls3_0713.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-13\. Predictions in a multilayer stacking ensemble
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Scikit-Learn provides two classes for stacking ensembles: `StackingClassifier`
    and `StackingRegressor`. For example, we can replace the `VotingClassifier` we
    used at the beginning of this chapter on the moons dataset with a `StackingClassifier`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: For each predictor, the stacking classifier will call `predict_proba()` if available;
    if not it will fall back to `decision_function()` or, as a last resort, call `predict()`.
    If you don’t provide a final estimator, `StackingClassifier` will use `LogisticRegression`
    and `StackingRegressor` will use `RidgeCV`.
  prefs: []
  type: TYPE_NORMAL
- en: If you evaluate this stacking model on the test set, you will find 92.8% accuracy,
    which is a bit better than the voting classifier using soft voting, which got
    92%.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, ensemble methods are versatile, powerful, and fairly simple to
    use. Random forests, AdaBoost, and GBRT are among the first models you should
    test for most machine learning tasks, and they particularly shine with heterogeneous
    tabular data. Moreover, as they require very little preprocessing, they’re great
    for getting a prototype up and running quickly. Lastly, ensemble methods like
    voting classifiers and stacking classifiers can help push your system’s performance
    to its limits.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have trained five different models on the exact same training data, and
    they all achieve 95% precision, is there any chance that you can combine these
    models to get better results? If so, how? If not, why?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between hard and soft voting classifiers?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is it possible to speed up training of a bagging ensemble by distributing it
    across multiple servers? What about pasting ensembles, boosting ensembles, random
    forests, or stacking ensembles?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the benefit of out-of-bag evaluation?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What makes extra-trees ensembles more random than regular random forests? How
    can this extra randomness help? Are extra-trees classifiers slower or faster than
    regular random forests?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If your AdaBoost ensemble underfits the training data, which hyperparameters
    should you tweak, and how?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If your gradient boosting ensemble overfits the training set, should you increase
    or decrease the learning rate?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the MNIST dataset (introduced in [Chapter 3](ch03.html#classification_chapter)),
    and split it into a training set, a validation set, and a test set (e.g., use
    50,000 instances for training, 10,000 for validation, and 10,000 for testing).
    Then train various classifiers, such as a random forest classifier, an extra-trees
    classifier, and an SVM classifier. Next, try to combine them into an ensemble
    that outperforms each individual classifier on the validation set, using soft
    or hard voting. Once you have found one, try it on the test set. How much better
    does it perform compared to the individual classifiers?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the individual classifiers from the previous exercise to make predictions
    on the validation set, and create a new training set with the resulting predictions:
    each training instance is a vector containing the set of predictions from all
    your classifiers for an image, and the target is the image’s class. Train a classifier
    on this new training set. Congratulations—you have just trained a blender, and
    together with the classifiers it forms a stacking ensemble! Now evaluate the ensemble
    on the test set. For each image in the test set, make predictions with all your
    classifiers, then feed the predictions to the blender to get the ensemble’s predictions.
    How does it compare to the voting classifier you trained earlier? Now try again
    using a `StackingClassifier` instead. Do you get better performance? If so, why?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab3*](https://homl.info/colab3).
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch07.html#idm45720211152336-marker)) Imagine picking a card randomly
    from a deck of cards, writing it down, then placing it back in the deck before
    picking the next card: the same card could be sampled multiple times.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch07.html#idm45720211150768-marker)) Leo Breiman, “Bagging Predictors”,
    *Machine Learning* 24, no. 2 (1996): 123–140.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch07.html#idm45720211149248-marker)) In statistics, resampling with replacement
    is called *bootstrapping*.
  prefs: []
  type: TYPE_NORMAL
- en: '^([4](ch07.html#idm45720211146016-marker)) Leo Breiman, “Pasting Small Votes
    for Classification in Large Databases and On-Line”, *Machine Learning* 36, no.
    1–2 (1999): 85–103.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch07.html#idm45720211140576-marker)) Bias and variance were introduced
    in [Chapter 4](ch04.html#linear_models_chapter).
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch07.html#idm45720211131712-marker)) `max_samples` can alternatively be
    set to a float between 0.0 and 1.0, in which case the max number of sampled instances
    is equal to the size of the training set times `max_samples`.
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch07.html#idm45720211046720-marker)) As *m* grows, this ratio approaches
    1 – exp(–1) ≈ 63%.
  prefs: []
  type: TYPE_NORMAL
- en: '^([8](ch07.html#idm45720210898224-marker)) Gilles Louppe and Pierre Geurts,
    “Ensembles on Random Patches”, *Lecture Notes in Computer Science* 7523 (2012):
    346–361.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([9](ch07.html#idm45720210877120-marker)) Tin Kam Ho, “The Random Subspace
    Method for Constructing Decision Forests”, *IEEE Transactions on Pattern Analysis
    and Machine Intelligence* 20, no. 8 (1998): 832–844.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([10](ch07.html#idm45720210870352-marker)) Tin Kam Ho, “Random Decision Forests”,
    *Proceedings of the Third International Conference on Document Analysis and Recognition*
    1 (1995): 278.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([11](ch07.html#idm45720210863312-marker)) The `BaggingClassifier` class remains
    useful if you want a bag of something other than decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: '^([12](ch07.html#idm45720210757984-marker)) Pierre Geurts et al., “Extremely
    Randomized Trees”, *Machine Learning* 63, no. 1 (2006): 3–42.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([13](ch07.html#idm45720210584080-marker)) Yoav Freund and Robert E. Schapire,
    “A Decision-Theoretic Generalization of On-Line Learning and an Application to
    Boosting”, *Journal of Computer and System Sciences* 55, no. 1 (1997): 119–139.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([14](ch07.html#idm45720210574352-marker)) This is just for illustrative purposes.
    SVMs are generally not good base predictors for AdaBoost; they are slow and tend
    to be unstable with it.
  prefs: []
  type: TYPE_NORMAL
- en: ^([15](ch07.html#idm45720210527056-marker)) The original AdaBoost algorithm
    does not use a learning rate hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: '^([16](ch07.html#idm45720210436640-marker)) For more details, see Ji Zhu et
    al., “Multi-Class AdaBoost”, *Statistics and Its Interface* 2, no. 3 (2009): 349–360.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([17](ch07.html#idm45720210377680-marker)) Gradient boosting was first introduced
    in Leo Breiman’s [1997 paper](https://homl.info/arcing) “Arcing the Edge” and
    was further developed in the [1999 paper](https://homl.info/gradboost) “Greedy
    Function Approximation: A Gradient Boosting Machine” by Jerome H. Friedman.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([18](ch07.html#idm45720209752032-marker)) David H. Wolpert, “Stacked Generalization”,
    *Neural Networks* 5, no. 2 (1992): 241–259.'
  prefs: []
  type: TYPE_NORMAL
