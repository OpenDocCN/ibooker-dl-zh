- en: Chapter 7\. Ensemble Learning and Random Forests
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。集成学习和随机森林
- en: Suppose you pose a complex question to thousands of random people, then aggregate
    their answers. In many cases you will find that this aggregated answer is better
    than an expert’s answer. This is called the *wisdom of the crowd*. Similarly,
    if you aggregate the predictions of a group of predictors (such as classifiers
    or regressors), you will often get better predictions than with the best individual
    predictor. A group of predictors is called an *ensemble*; thus, this technique
    is called *ensemble learning*, and an ensemble learning algorithm is called an
    *ensemble method*.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您向成千上万的随机人提出一个复杂的问题，然后汇总他们的答案。在许多情况下，您会发现这种汇总的答案比专家的答案更好。这被称为*群体的智慧*。类似地，如果您汇总一组预测器（如分类器或回归器）的预测，通常会比最佳个体预测器的预测更好。一组预测器称为*集成*；因此，这种技术称为*集成学习*，集成学习算法称为*集成方法*。
- en: As an example of an ensemble method, you can train a group of decision tree
    classifiers, each on a different random subset of the training set. You can then
    obtain the predictions of all the individual trees, and the class that gets the
    most votes is the ensemble’s prediction (see the last exercise in [Chapter 6](ch06.html#trees_chapter)).
    Such an ensemble of decision trees is called a *random forest*, and despite its
    simplicity, this is one of the most powerful machine learning algorithms available
    today.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 作为集成方法的一个示例，您可以训练一组决策树分类器，每个分类器在训练集的不同随机子集上训练。然后，您可以获得所有单独树的预测，得到得票最多的类别就是集成的预测（请参见[第6章](ch06.html#trees_chapter)中的最后一个练习）。这样的决策树集成称为*随机森林*，尽管它很简单，但这是当今最强大的机器学习算法之一。
- en: As discussed in [Chapter 2](ch02.html#project_chapter), you will often use ensemble
    methods near the end of a project, once you have already built a few good predictors,
    to combine them into an even better predictor. In fact, the winning solutions
    in machine learning competitions often involve several ensemble methods—most famously
    in the [Netflix Prize competition](https://en.wikipedia.org/wiki/Netflix_Prize).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在[第2章](ch02.html#project_chapter)中讨论的那样，您通常会在项目结束时使用集成方法，一旦您已经构建了几个良好的预测器，将它们组合成一个更好的预测器。事实上，在机器学习竞赛中获胜的解决方案通常涉及几种集成方法，最著名的是[Netflix
    Prize竞赛](https://en.wikipedia.org/wiki/Netflix_Prize)。
- en: In this chapter we will examine the most popular ensemble methods, including
    voting classifiers, bagging and pasting ensembles, random forests, and boosting,
    and stacking ensembles.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将研究最流行的集成方法，包括投票分类器、装袋和粘贴集成、随机森林和提升，以及堆叠集成。
- en: Voting Classifiers
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 投票分类器
- en: Suppose you have trained a few classifiers, each one achieving about 80% accuracy.
    You may have a logistic regression classifier, an SVM classifier, a random forest
    classifier, a *k*-nearest neighbors classifier, and perhaps a few more (see [Figure 7-1](#voting_classifier_training_diagram)).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您已经训练了几个分类器，每个分类器的准确率约为80%。您可能有一个逻辑回归分类器，一个SVM分类器，一个随机森林分类器，一个*k*最近邻分类器，也许还有几个（请参见[图7-1](#voting_classifier_training_diagram)）。
- en: '![mls3 0701](assets/mls3_0701.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0701](assets/mls3_0701.png)'
- en: Figure 7-1\. Training diverse classifiers
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-1\. 训练多样化的分类器
- en: 'A very simple way to create an even better classifier is to aggregate the predictions
    of each classifier: the class that gets the most votes is the ensemble’s prediction.
    This majority-vote classifier is called a *hard voting* classifier (see [Figure 7-2](#voting_classifier_prediction_diagram)).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个更好的分类器的一个非常简单的方法是汇总每个分类器的预测：得票最多的类别是集成的预测。这种多数投票分类器称为*硬投票*分类器（请参见[图7-2](#voting_classifier_prediction_diagram)）。
- en: '![mls3 0702](assets/mls3_0702.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0702](assets/mls3_0702.png)'
- en: Figure 7-2\. Hard voting classifier predictions
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-2\. 硬投票分类器预测
- en: Somewhat surprisingly, this voting classifier often achieves a higher accuracy
    than the best classifier in the ensemble. In fact, even if each classifier is
    a *weak learner* (meaning it does only slightly better than random guessing),
    the ensemble can still be a *strong learner* (achieving high accuracy), provided
    there are a sufficient number of weak learners in the ensemble and they are sufficiently
    diverse.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，这种投票分类器通常比集成中最好的分类器的准确率更高。事实上，即使每个分类器都是*弱学习器*（意味着它的表现仅略好于随机猜测），只要集成中有足够数量的弱学习器并且它们足够多样化，集成仍然可以是一个*强学习器*（实现高准确率）。
- en: 'How is this possible? The following analogy can help shed some light on this
    mystery. Suppose you have a slightly biased coin that has a 51% chance of coming
    up heads and 49% chance of coming up tails. If you toss it 1,000 times, you will
    generally get more or less 510 heads and 490 tails, and hence a majority of heads.
    If you do the math, you will find that the probability of obtaining a majority
    of heads after 1,000 tosses is close to 75%. The more you toss the coin, the higher
    the probability (e.g., with 10,000 tosses, the probability climbs over 97%). This
    is due to the *law of large numbers*: as you keep tossing the coin, the ratio
    of heads gets closer and closer to the probability of heads (51%). [Figure 7-3](#law_of_large_numbers_plot)
    shows 10 series of biased coin tosses. You can see that as the number of tosses
    increases, the ratio of heads approaches 51%. Eventually all 10 series end up
    so close to 51% that they are consistently above 50%.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何可能的？以下类比可以帮助解开这个谜团。假设您有一个略带偏见的硬币，正面朝上的概率为51%，反面朝上的概率为49%。如果您抛掷它1,000次，通常会得到大约510次正面和490次反面，因此大多数是正面。如果您进行计算，您会发现在1,000次抛掷后获得大多数正面的概率接近75%。您抛掷硬币的次数越多，概率就越高（例如，进行10,000次抛掷后，概率超过97%）。这是由于*大数定律*：随着您不断抛掷硬币，正面的比例越来越接近正面的概率（51%）。[图7-3](#law_of_large_numbers_plot)显示了10组有偏硬币抛掷。您可以看到随着抛掷次数的增加，正面的比例接近51%。最终，所有10组数据最终都接近51%，它们始终保持在50%以上。
- en: '![mls3 0703](assets/mls3_0703.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0703](assets/mls3_0703.png)'
- en: Figure 7-3\. The law of large numbers
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-3\. 大数定律
- en: Similarly, suppose you build an ensemble containing 1,000 classifiers that are
    individually correct only 51% of the time (barely better than random guessing).
    If you predict the majority voted class, you can hope for up to 75% accuracy!
    However, this is only true if all classifiers are perfectly independent, making
    uncorrelated errors, which is clearly not the case because they are trained on
    the same data. They are likely to make the same types of errors, so there will
    be many majority votes for the wrong class, reducing the ensemble’s accuracy.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，假设你构建一个包含1,000个分类器的集成，这些分类器单独的正确率仅为51%（略高于随机猜测）。如果你预测多数投票的类别，你可以期望达到高达75%的准确性！然而，这仅在所有分类器完全独立，产生不相关错误时才成立，而这显然不是事实，因为它们是在相同数据上训练的。它们很可能会产生相同类型的错误，因此会有很多错误类别的多数投票，降低了集成的准确性。
- en: Tip
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Ensemble methods work best when the predictors are as independent from one another
    as possible. One way to get diverse classifiers is to train them using very different
    algorithms. This increases the chance that they will make very different types
    of errors, improving the ensemble’s accuracy.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法在预测器尽可能独立时效果最好。获得多样化分类器的一种方法是使用非常不同的算法对它们进行训练。这增加了它们会产生非常不同类型错误的机会，提高了集成的准确性。
- en: 'Scikit-Learn provides a `VotingClassifier` class that’s quite easy to use:
    just give it a list of name/predictor pairs, and use it like a normal classifier.
    Let’s try it on the moons dataset (introduced in [Chapter 5](ch05.html#svm_chapter)).
    We will load and split the moons dataset into a training set and a test set, then
    we’ll create and train a voting classifier composed of three diverse classifiers:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn提供了一个非常容易使用的`VotingClassifier`类：只需给它一个名称/预测器对的列表，然后像普通分类器一样使用它。让我们在moons数据集上尝试一下（在[第5章](ch05.html#svm_chapter)介绍）。我们将加载并拆分moons数据集为训练集和测试集，然后创建和训练一个由三个不同分类器组成的投票分类器：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'When you fit a `VotingClassifier`, it clones every estimator and fits the clones.
    The original estimators are available via the `estimators` attribute, while the
    fitted clones are available via the `estimators_` attribute. If you prefer a dict
    rather than a list, you can use `named_estimators` or `named_estimators_` instead.
    To begin, let’s look at each fitted classifier’s accuracy on the test set:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当你拟合`VotingClassifier`时，它会克隆每个估计器并拟合这些克隆。原始估计器可以通过`estimators`属性获得，而拟合的克隆可以通过`estimators_`属性获得。如果你更喜欢字典而不是列表，可以使用`named_estimators`或`named_estimators_`。首先，让我们看看每个拟合分类器在测试集上的准确性：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'When you call the voting classifier’s `predict()` method, it performs hard
    voting. For example, the voting classifier predicts class 1 for the first instance
    of the test set, because two out of three classifiers predict that class:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当你调用投票分类器的`predict()`方法时，它执行硬投票。例如，投票分类器为测试集的第一个实例预测类别1，因为三个分类器中有两个预测该类别：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now let’s look at the performance of the voting classifier on the test set:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看投票分类器在测试集上的表现：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: There you have it! The voting classifier outperforms all the individual classifiers.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！投票分类器的表现优于所有个体分类器。
- en: 'If all classifiers are able to estimate class probabilities (i.e., if they
    all have a `predict_proba()` method), then you can tell Scikit-Learn to predict
    the class with the highest class probability, averaged over all the individual
    classifiers. This is called *soft voting*. It often achieves higher performance
    than hard voting because it gives more weight to highly confident votes. All you
    need to do is set the voting classifier’s `voting` hyperparameter to `"soft"`,
    and ensure that all classifiers can estimate class probabilities. This is not
    the case for the `SVC` class by default, so you need to set its `probability`
    hyperparameter to `True` (this will make the `SVC` class use cross-validation
    to estimate class probabilities, slowing down training, and it will add a `predict_proba()`
    method). Let’s try that:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有分类器都能估计类别概率（即它们都有`predict_proba()`方法），那么你可以告诉Scikit-Learn预测具有最高类别概率的类别，这是所有个体分类器的平均值。这被称为*软投票*。它通常比硬投票表现更好，因为它更加重视高置信度的投票。你只需要将投票分类器的`voting`超参数设置为`"soft"`，并确保所有分类器都能估计类别概率。这在`SVC`类中默认情况下不适用，因此你需要将其`probability`超参数设置为`True`（这将使`SVC`类使用交叉验证来估计类别概率，从而减慢训练速度，并添加一个`predict_proba()`方法）。让我们试一试：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We reach 92% accuracy simply by using soft voting—not bad!
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 仅通过使用软投票，我们达到了92%的准确性，不错！
- en: Bagging and Pasting
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Bagging和Pasting
- en: One way to get a diverse set of classifiers is to use very different training
    algorithms, as just discussed. Another approach is to use the same training algorithm
    for every predictor but train them on different random subsets of the training
    set. When sampling is performed *with* replacement,⁠^([1](ch07.html#idm45720211152336))
    this method is called [*bagging*](https://homl.info/20)⁠^([2](ch07.html#idm45720211150768))
    (short for *bootstrap aggregating*⁠^([3](ch07.html#idm45720211149248))). When
    sampling is performed *without* replacement, it is called [*pasting*](https://homl.info/21).⁠^([4](ch07.html#idm45720211146016))
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 获得多样化分类器的一种方法是使用非常不同的训练算法，正如刚才讨论的。另一种方法是对每个预测器使用相同的训练算法，但在训练集的不同随机子集上训练它们。当采样*有*替换时，这种方法称为[*bagging*](https://homl.info/20)（bootstrap聚合的缩写）⁠。当采样*无*替换时，它被称为[*pasting*](https://homl.info/21)。
- en: In other words, both bagging and pasting allow training instances to be sampled
    several times across multiple predictors, but only bagging allows training instances
    to be sampled several times for the same predictor. This sampling and training
    process is represented in [Figure 7-4](#bagging_training_diagram).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，bagging和pasting都允许训练实例在多个预测器之间多次采样，但只有bagging允许训练实例在同一个预测器中多次采样。这种采样和训练过程在[图7-4](#bagging_training_diagram)中表示。
- en: '![mls3 0704](assets/mls3_0704.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0704](assets/mls3_0704.png)'
- en: Figure 7-4\. Bagging and pasting involve training several predictors on different
    random samples of the training set
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Once all predictors are trained, the ensemble can make a prediction for a new
    instance by simply aggregating the predictions of all predictors. The aggregation
    function is typically the *statistical mode* for classification (i.e., the most
    frequent prediction, just like with a hard voting classifier), or the average
    for regression. Each individual predictor has a higher bias than if it were trained
    on the original training set, but aggregation reduces both bias and variance.⁠^([5](ch07.html#idm45720211140576))
    Generally, the net result is that the ensemble has a similar bias but a lower
    variance than a single predictor trained on the original training set.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in [Figure 7-4](#bagging_training_diagram), predictors can all
    be trained in parallel, via different CPU cores or even different servers. Similarly,
    predictions can be made in parallel. This is one of the reasons bagging and pasting
    are such popular methods: they scale very well.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Bagging and Pasting in Scikit-Learn
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Scikit-Learn offers a simple API for both bagging and pasting: `BaggingClassifier`
    class (or `BaggingRegressor` for regression). The following code trains an ensemble
    of 500 decision tree classifiers:⁠^([6](ch07.html#idm45720211131712)) each is
    trained on 100 training instances randomly sampled from the training set with
    replacement (this is an example of bagging, but if you want to use pasting instead,
    just set `bootstrap=False`). The `n_jobs` parameter tells Scikit-Learn the number
    of CPU cores to use for training and predictions, and `–1` tells Scikit-Learn
    to use all available cores:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A `BaggingClassifier` automatically performs soft voting instead of hard voting
    if the base classifier can estimate class probabilities (i.e., if it has a `predict_proba()`
    method), which is the case with decision tree classifiers.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 7-5](#decision_tree_without_and_with_bagging_plot) compares the decision
    boundary of a single decision tree with the decision boundary of a bagging ensemble
    of 500 trees (from the preceding code), both trained on the moons dataset. As
    you can see, the ensemble’s predictions will likely generalize much better than
    the single decision tree’s predictions: the ensemble has a comparable bias but
    a smaller variance (it makes roughly the same number of errors on the training
    set, but the decision boundary is less irregular).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Bagging introduces a bit more diversity in the subsets that each predictor is
    trained on, so bagging ends up with a slightly higher bias than pasting; but the
    extra diversity also means that the predictors end up being less correlated, so
    the ensemble’s variance is reduced. Overall, bagging often results in better models,
    which explains why it’s generally preferred. But if you have spare time and CPU
    power, you can use cross-validation to evaluate both bagging and pasting and select
    the one that works best.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0705](assets/mls3_0705.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: Figure 7-5\. A single decision tree (left) versus a bagging ensemble of 500
    trees (right)
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Out-of-Bag Evaluation
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With bagging, some training instances may be sampled several times for any given
    predictor, while others may not be sampled at all. By default a `BaggingClassifier`
    samples *m* training instances with replacement (`bootstrap=True`), where *m*
    is the size of the training set. With this process, it can be shown mathematically
    that only about 63% of the training instances are sampled on average for each
    predictor.⁠^([7](ch07.html#idm45720211046720)) The remaining 37% of the training
    instances that are not sampled are called *out-of-bag* (OOB) instances. Note that
    they are not the same 37% for all predictors.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'A bagging ensemble can be evaluated using OOB instances, without the need for
    a separate validation set: indeed, if there are enough estimators, then each instance
    in the training set will likely be an OOB instance of several estimators, so these
    estimators can be used to make a fair ensemble prediction for that instance. Once
    you have a prediction for each instance, you can compute the ensemble’s prediction
    accuracy (or any other metric).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用OOB实例评估装袋集成，无需单独的验证集：实际上，如果有足够的估计器，那么训练集中的每个实例很可能是几个估计器的OOB实例，因此这些估计器可以用于为该实例进行公平的集成预测。一旦您对每个实例进行了预测，就可以计算集成的预测准确性（或任何其他度量）。
- en: 'In Scikit-Learn, you can set `oob_score=True` when creating a `BaggingClassifier`
    to request an automatic OOB evaluation after training. The following code demonstrates
    this. The resulting evaluation score is available in the `oob_score_` attribute:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scikit-Learn中，您可以在创建`BaggingClassifier`时设置`oob_score=True`来请求训练后自动进行OOB评估。以下代码演示了这一点。生成的评估分数可在`oob_score_`属性中获得：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'According to this OOB evaluation, this `BaggingClassifier` is likely to achieve
    about 89.6% accuracy on the test set. Let’s verify this:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个OOB评估，这个`BaggingClassifier`在测试集上可能会达到约89.6%的准确率。让我们验证一下：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We get 92% accuracy on the test. The OOB evaluation was a bit too pessimistic,
    just over 2% too low.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在测试中获得了92%的准确率。OOB评估有点太悲观了，低了2%多一点。
- en: 'The OOB decision function for each training instance is also available through
    the `oob_decision_function_` attribute. Since the base estimator has a `predict_proba()`
    method, the decision function returns the class probabilities for each training
    instance. For example, the OOB evaluation estimates that the first training instance
    has a 67.6% probability of belonging to the positive class and a 32.4% probability
    of belonging to the negative class:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 每个训练实例的OOB决策函数也可以通过`oob_decision_function_`属性获得。由于基本估计器具有`predict_proba()`方法，决策函数返回每个训练实例的类概率。例如，OOB评估估计第一个训练实例属于正类的概率为67.6%，属于负类的概率为32.4%：
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Random Patches and Random Subspaces
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机补丁和随机子空间
- en: 'The `BaggingClassifier` class supports sampling the features as well. Sampling
    is controlled by two hyperparameters: `max_features` and `bootstrap_features`.
    They work the same way as `max_samples` and `bootstrap`, but for feature sampling
    instead of instance sampling. Thus, each predictor will be trained on a random
    subset of the input features.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`BaggingClassifier`类还支持对特征进行抽样。抽样由两个超参数控制：`max_features`和`bootstrap_features`。它们的工作方式与`max_samples`和`bootstrap`相同，但用于特征抽样而不是实例抽样。因此，每个预测器将在输入特征的随机子集上进行训练。'
- en: This technique is particularly useful when you are dealing with high-dimensional
    inputs (such as images), as it can considerably speed up training. Sampling both
    training instances and features is called the [*random patches* method](https://homl.info/22).⁠^([8](ch07.html#idm45720210898224))
    Keeping all training instances (by setting `bootstrap=False` and `max_samples=1.0`)
    but sampling features (by setting `bootstrap_features` to `True` and/or `max_features`
    to a value smaller than `1.0`) is called the [*random subspaces* method](https://homl.info/23).⁠^([9](ch07.html#idm45720210877120))
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理高维输入（例如图像）时，这种技术特别有用，因为它可以显着加快训练速度。对训练实例和特征进行抽样被称为[*随机补丁*方法](https://homl.info/22)。保留所有训练实例（通过设置`bootstrap=False`和`max_samples=1.0`）但对特征进行抽样（通过将`bootstrap_features`设置为`True`和/或将`max_features`设置为小于`1.0`的值）被称为[*随机子空间*方法](https://homl.info/23)。
- en: Sampling features results in even more predictor diversity, trading a bit more
    bias for a lower variance.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对特征进行抽样会导致更多的预测器多样性，以换取更低的方差稍微增加一点偏差。
- en: Random Forests
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林
- en: 'As we have discussed, a [random forest](https://homl.info/24)⁠^([10](ch07.html#idm45720210870352))
    is an ensemble of decision trees, generally trained via the bagging method (or
    sometimes pasting), typically with `max_samples` set to the size of the training
    set. Instead of building a `BaggingClassifier` and passing it a `DecisionTreeClassifier`,
    you can use the `RandomForestClassifier` class, which is more convenient and optimized
    for decision trees⁠^([11](ch07.html#idm45720210863312)) (similarly, there is a
    `RandomForestRegressor` class for regression tasks). The following code trains
    a random forest classifier with 500 trees, each limited to maximum 16 leaf nodes,
    using all available CPU cores:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所讨论的，[随机森林](https://homl.info/24)是一组决策树的集成，通常通过装袋方法（有时是粘贴）进行训练，通常将`max_samples`设置为训练集的大小。您可以使用`RandomForestClassifier`类来训练随机森林分类器，该类更方便且针对决策树进行了优化（类似地，还有一个用于回归任务的`RandomForestRegressor`类）。以下代码使用500棵树训练了一个随机森林分类器，每棵树最多限制为16个叶节点，使用所有可用的CPU核心：
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: With a few exceptions, a `RandomForestClassifier` has all the hyperparameters
    of a `DecisionTreeClassifier` (to control how trees are grown), plus all the hyperparameters
    of a `BaggingClassifier` to control the ensemble itself.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 除了一些例外，`RandomForestClassifier`具有`DecisionTreeClassifier`的所有超参数（用于控制树的生长方式），以及`BaggingClassifier`的所有超参数来控制集成本身。
- en: 'The random forest algorithm introduces extra randomness when growing trees;
    instead of searching for the very best feature when splitting a node (see [Chapter 6](ch06.html#trees_chapter)),
    it searches for the best feature among a random subset of features. By default,
    it samples <math><msqrt><mi>n</mi></msqrt></math> features (where *n* is the total
    number of features). The algorithm results in greater tree diversity, which (again)
    trades a higher bias for a lower variance, generally yielding an overall better
    model. So, the following `BaggingClassifier` is equivalent to the previous `RandomForestClassifier`:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林算法在生长树时引入了额外的随机性；在分裂节点时不是搜索最佳特征（参见[第6章](ch06.html#trees_chapter)），而是在一组随机特征中搜索最佳特征。默认情况下，它对特征进行采样<math><msqrt><mi>n</mi></msqrt></math>（其中*n*是特征的总数）。该算法导致更大的树多样性，这（再次）以更低的方差换取更高的偏差，通常产生更好的模型。因此，以下`BaggingClassifier`等同于之前的`RandomForestClassifier`：
- en: '[PRE10]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Extra-Trees
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 额外树
- en: When you are growing a tree in a random forest, at each node only a random subset
    of the features is considered for splitting (as discussed earlier). It is possible
    to make trees even more random by also using random thresholds for each feature
    rather than searching for the best possible thresholds (like regular decision
    trees do). For this, simply set `splitter="random"` when creating a `DecisionTreeClassifier`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机森林中生长树时，在每个节点只考虑一组随机特征进行分裂（如前所述）。还可以通过为每个特征使用随机阈值而不是搜索最佳阈值（正如常规决策树所做）来使树更加随机。为此，只需在创建`DecisionTreeClassifier`时设置`splitter="random"`。
- en: A forest of such extremely random trees is called an [*extremely randomized
    trees*](https://homl.info/25)⁠^([12](ch07.html#idm45720210757984)) (or *extra-trees*
    for short) ensemble. Once again, this technique trades more bias for a lower variance.
    It also makes extra-trees classifiers much faster to train than regular random
    forests, because finding the best possible threshold for each feature at every
    node is one of the most time-consuming tasks of growing a tree.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这样极端随机树的森林被称为[*极端随机树*](https://homl.info/25)（或简称为*额外树*）集成。再次，这种技术以更低的方差换取更高的偏差。相比于常规随机森林，额外树分类器的训练速度也更快，因为在每个节点为每个特征找到最佳阈值是生长树中最耗时的任务之一。
- en: You can create an extra-trees classifier using Scikit-Learn’s `ExtraTreesClassifier`
    class. Its API is identical to the `RandomForestClassifier` class, except `bootstrap`
    defaults to `False`. Similarly, the `ExtraTreesRegressor` class has the same API
    as the `RandomForestRegressor` class, except `bootstrap` defaults to `False`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用Scikit-Learn的`ExtraTreesClassifier`类创建一个额外树分类器。其API与`RandomForestClassifier`类相同，只是`bootstrap`默认为`False`。同样，`ExtraTreesRegressor`类与`RandomForestRegressor`类具有相同的API，只是`bootstrap`默认为`False`。
- en: Tip
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: It is hard to tell in advance whether a `RandomForestClassifier` will perform
    better or worse than an `ExtraTreesClassifier`. Generally, the only way to know
    is to try both and compare them using cross-validation.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 很难事先确定`RandomForestClassifier`的表现是好还是坏于`ExtraTreesClassifier`。通常，唯一的方法是尝试两者并使用交叉验证进行比较。
- en: Feature Importance
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征重要性
- en: Yet another great quality of random forests is that they make it easy to measure
    the relative importance of each feature. Scikit-Learn measures a feature’s importance
    by looking at how much the tree nodes that use that feature reduce impurity on
    average, across all trees in the forest. More precisely, it is a weighted average,
    where each node’s weight is equal to the number of training samples that are associated
    with it (see [Chapter 6](ch06.html#trees_chapter)).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林的另一个很好的特性是它可以轻松测量每个特征的相对重要性。Scikit-Learn通过查看使用该特征的树节点平均减少不纯度的程度来衡量特征的重要性，跨森林中的所有树。更准确地说，这是一个加权平均值，其中每个节点的权重等于与其相关联的训练样本数（参见[第6章](ch06.html#trees_chapter)）。
- en: 'Scikit-Learn computes this score automatically for each feature after training,
    then it scales the results so that the sum of all importances is equal to 1\.
    You can access the result using the `feature_importances_` variable. For example,
    the following code trains a `RandomForestClassifier` on the iris dataset (introduced
    in [Chapter 4](ch04.html#linear_models_chapter)) and outputs each feature’s importance.
    It seems that the most important features are the petal length (44%) and width
    (42%), while sepal length and width are rather unimportant in comparison (11%
    and 2%, respectively):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn在训练后自动计算每个特征的重要性得分，然后将结果进行缩放，使所有重要性的总和等于1。您可以使用`feature_importances_`变量访问结果。例如，以下代码在鸢尾花数据集上训练一个`RandomForestClassifier`（在[第4章](ch04.html#linear_models_chapter)介绍），并输出每个特征的重要性。看起来最重要的特征是花瓣长度（44%）和宽度（42%），而花萼长度和宽度相比之下不太重要（分别为11%和2%）：
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Similarly, if you train a random forest classifier on the MNIST dataset (introduced
    in [Chapter 3](ch03.html#classification_chapter)) and plot each pixel’s importance,
    you get the image represented in [Figure 7-6](#mnist_feature_importance_plot).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，如果您在MNIST数据集上训练随机森林分类器（在[第3章](ch03.html#classification_chapter)介绍），并绘制每个像素的重要性，则会得到[图7-6](#mnist_feature_importance_plot)中所代表的图像。
- en: '![mls3 0706](assets/mls3_0706.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0706](assets/mls3_0706.png)'
- en: Figure 7-6\. MNIST pixel importance (according to a random forest classifier)
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-6. MNIST像素重要性（根据随机森林分类器）
- en: Random forests are very handy to get a quick understanding of what features
    actually matter, in particular if you need to perform feature selection.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林非常方便，可以快速了解哪些特征实际上很重要，特别是如果您需要执行特征选择时。
- en: Boosting
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升
- en: '*Boosting* (originally called *hypothesis boosting*) refers to any ensemble
    method that can combine several weak learners into a strong learner. The general
    idea of most boosting methods is to train predictors sequentially, each trying
    to correct its predecessor. There are many boosting methods available, but by
    far the most popular are [*AdaBoost*](https://homl.info/26)⁠^([13](ch07.html#idm45720210584080))
    (short for *adaptive boosting*) and *gradient boosting*. Let’s start with AdaBoost.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*Boosting*（最初称为*hypothesis boosting*）指的是任何可以将几个弱学习器组合成一个强学习器的集成方法。大多数提升方法的一般思想是顺序训练预测器，每个预测器都试图纠正其前身。有许多提升方法可用，但目前最流行的是[*AdaBoost*](https://homl.info/26)（缩写为*adaptive
    boosting*）和*gradient boosting*。让我们从AdaBoost开始。'
- en: AdaBoost
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AdaBoost
- en: One way for a new predictor to correct its predecessor is to pay a bit more
    attention to the training instances that the predecessor underfit. This results
    in new predictors focusing more and more on the hard cases. This is the technique
    used by AdaBoost.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 新预测器纠正其前身的一种方法是更多地关注前身欠拟合的训练实例。这导致新的预测器越来越关注困难的情况。这是AdaBoost使用的技术。
- en: For example, when training an AdaBoost classifier, the algorithm first trains
    a base classifier (such as a decision tree) and uses it to make predictions on
    the training set. The algorithm then increases the relative weight of misclassified
    training instances. Then it trains a second classifier, using the updated weights,
    and again makes predictions on the training set, updates the instance weights,
    and so on (see [Figure 7-7](#adaboost_training_diagram)).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在训练AdaBoost分类器时，算法首先训练一个基本分类器（如决策树），并使用它对训练集进行预测。然后增加被错误分类的训练实例的相对权重。然后训练第二个分类器，使用更新后的权重，再次对训练集进行预测，更新实例权重，依此类推（参见[图7-7](#adaboost_training_diagram)）。
- en: '[Figure 7-8](#boosting_plot) shows the decision boundaries of five consecutive
    predictors on the moons dataset (in this example, each predictor is a highly regularized
    SVM classifier with an RBF kernel).⁠^([14](ch07.html#idm45720210574352)) The first
    classifier gets many instances wrong, so their weights get boosted. The second
    classifier therefore does a better job on these instances, and so on. The plot
    on the right represents the same sequence of predictors, except that the learning
    rate is halved (i.e., the misclassified instance weights are boosted much less
    at every iteration). As you can see, this sequential learning technique has some
    similarities with gradient descent, except that instead of tweaking a single predictor’s
    parameters to minimize a cost function, AdaBoost adds predictors to the ensemble,
    gradually making it better.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-8](#boosting_plot)显示了在moons数据集上的五个连续预测器的决策边界（在这个例子中，每个预测器都是一个具有RBF核的高度正则化的SVM分类器）。第一个分类器错误地预测了许多实例，因此它们的权重被提升。因此，第二个分类器在这些实例上做得更好，依此类推。右侧的图表示相同序列的预测器，只是学习率减半（即，在每次迭代中，错误分类的实例权重提升要少得多）。正如您所看到的，这种顺序学习技术与梯度下降有一些相似之处，只是AdaBoost不是调整单个预测器的参数以最小化成本函数，而是逐渐将预测器添加到集成中，使其变得更好。'
- en: '![mls3 0707](assets/mls3_0707.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0707](assets/mls3_0707.png)'
- en: Figure 7-7\. AdaBoost sequential training with instance weight updates
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-7。AdaBoost顺序训练与实例权重更新
- en: Once all predictors are trained, the ensemble makes predictions very much like
    bagging or pasting, except that predictors have different weights depending on
    their overall accuracy on the weighted training set.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有预测器都训练完毕，集成就会像装袋或粘贴一样进行预测，只是预测器根据它们在加权训练集上的整体准确性具有不同的权重。
- en: '![mls3 0708](assets/mls3_0708.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0708](assets/mls3_0708.png)'
- en: Figure 7-8\. Decision boundaries of consecutive predictors
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-8。连续预测器的决策边界
- en: Warning
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'There is one important drawback to this sequential learning technique: training
    cannot be parallelized since each predictor can only be trained after the previous
    predictor has been trained and evaluated. As a result, it does not scale as well
    as bagging or pasting.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这种顺序学习技术有一个重要的缺点：训练不能并行化，因为每个预测器只能在前一个预测器训练和评估之后进行训练。因此，它的扩展性不如装袋或粘贴。
- en: Let’s take a closer look at the AdaBoost algorithm. Each instance weight *w*^((*i*))
    is initially set to 1/*m*. A first predictor is trained, and its weighted error
    rate *r*[1] is computed on the training set; see [Equation 7-1](#weighted_error_rate).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看AdaBoost算法。每个实例权重*w*^((*i*))最初设置为1/*m*。首先训练一个预测器，并在训练集上计算其加权错误率*r*[1]；参见[方程7-1](#weighted_error_rate)。
- en: Equation 7-1\. Weighted error rate of the j^(th) predictor
  id: totrans-95
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程7-1。第j个预测器的加权错误率
- en: <math display="block"><mrow><msub><mi>r</mi><mi>j</mi></msub> <mo>=</mo> <mstyle
    scriptlevel="0" displaystyle="true"><mrow><munderover><mo>∑</mo> <mstyle scriptlevel="0"
    displaystyle="false"><mrow><mfrac linethickness="0pt"><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mrow><msubsup><mover accent="true"><mi>y</mi><mo>^</mo></mover> <mi>j</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup>
    <mo>≠</mo> <msup><mi>y</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mfrac></mrow></mstyle>
    <mi>m</mi></munderover> <msup><mi>w</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mstyle>
    <mtext>where</mtext> <msubsup><mover accent="true"><mi>y</mi><mo>^</mo></mover>
    <mi>j</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup> <mtext>is</mtext>
    <mtext>the</mtext> <msup><mi>j</mi><mtext>th</mtext></msup> <mtext>predictor’s</mtext>
    <mtext>prediction</mtext><mtext>for</mtext> <mtext>the</mtext> <msup><mi>i</mi>
    <mtext>th</mtext></msup> <mtext>instance</mtext></mrow></math>
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>r</mi><mi>j</mi></msub> <mo>=</mo> <mstyle
    scriptlevel="0" displaystyle="true"><mrow><munderover><mo>∑</mo> <mstyle scriptlevel="0"
    displaystyle="false"><mrow><mfrac linethickness="0pt"><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mrow><msubsup><mover accent="true"><mi>y</mi><mo>^</mo></mover> <mi>j</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup>
    <mo>≠</mo> <msup><mi>y</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mfrac></mrow></mstyle>
    <mi>m</mi></munderover> <msup><mi>w</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mstyle>
    <mtext>其中</mtext> <msubsup><mover accent="true"><mi>y</mi><mo>^</mo></mover> <mi>j</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup>
    <mtext>是</mtext> <mtext>第</mtext> <msup><mi>j</mi><mtext>th</mtext></msup> <mtext>预测器的</mtext>
    <mtext>预测</mtext><mtext>对于</mtext> <mtext>第</mtext> <msup><mi>i</mi> <mtext>th</mtext></msup>
    <mtext>实例</mtext></mrow></math>
- en: The predictor’s weight *α*[*j*] is then computed using [Equation 7-2](#predictor_weight),
    where *η* is the learning rate hyperparameter (defaults to 1).⁠^([15](ch07.html#idm45720210527056))
    The more accurate the predictor is, the higher its weight will be. If it is just
    guessing randomly, then its weight will be close to zero. However, if it is most
    often wrong (i.e., less accurate than random guessing), then its weight will be
    negative.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用[方程7-2](#predictor_weight)计算预测器的权重*α*[*j*]，其中*η*是学习率超参数（默认为1）。⁠^([15](ch07.html#idm45720210527056))
    预测器越准确，其权重就越高。如果它只是随机猜测，那么它的权重将接近于零。然而，如果它经常错误（即比随机猜测更不准确），那么它的权重将是负数。
- en: Equation 7-2\. Predictor weight
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程7-2. 预测器权重
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><msub><mi>α</mi>
    <mi>j</mi></msub> <mo>=</mo> <mi>η</mi> <mo form="prefix">log</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mrow><mn>1</mn><mo>-</mo><msub><mi>r</mi> <mi>j</mi></msub></mrow>
    <msub><mi>r</mi> <mi>j</mi></msub></mfrac></mstyle></mrow></mtd></mtr></mtable></math>
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><msub><mi>α</mi>
    <mi>j</mi></msub> <mo>=</mo> <mi>η</mi> <mo form="prefix">log</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mrow><mn>1</mn><mo>-</mo><msub><mi>r</mi> <mi>j</mi></msub></mrow>
    <msub><mi>r</mi> <mi>j</mi></msub></mfrac></mstyle></mrow></mtd></mtr></mtable></math>
- en: Next, the AdaBoost algorithm updates the instance weights, using [Equation 7-3](#instance_weight_update),
    which boosts the weights of the misclassified instances.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，AdaBoost算法使用[方程7-3](#instance_weight_update)更新实例权重，提升错误分类实例的权重。
- en: Equation 7-3\. Weight update rule
  id: totrans-101
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程7-3. 权重更新规则
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="left"><mrow><mtext>for</mtext>
    <mi>i</mi> <mo>=</mo> <mn>1</mn> <mo>,</mo> <mn>2</mn> <mo>,</mo> <mo>⋯</mo> <mo>,</mo>
    <mi>m</mi></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><msup><mi>w</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>←</mo> <mfenced separators=""
    open="{" close=""><mtable><mtr><mtd columnalign="left"><msup><mi>w</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <msup><mover accent="true"><msub><mi>y</mi>
    <mi>j</mi></msub> <mo>^</mo></mover> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>=</mo> <msup><mi>y</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><msup><mi>w</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo form="prefix">exp</mo> <mrow><mo>(</mo> <msub><mi>α</mi> <mi>j</mi></msub>
    <mo>)</mo></mrow></mrow></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext>
    <msup><mover accent="true"><msub><mi>y</mi> <mi>j</mi></msub> <mo>^</mo></mover>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>≠</mo> <msup><mi>y</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mtd></mtr></mtable></mfenced></mrow></mtd></mtr></mtable></math>
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="left"><mrow><mtext>对于</mtext>
    <mi>i</mi> <mo>=</mo> <mn>1</mn> <mo>,</mo> <mn>2</mn> <mo>,</mo> <mo>⋯</mo> <mo>,</mo>
    <mi>m</mi></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><msup><mi>w</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>←</mo> <mfenced separators=""
    open="{" close=""><mtable><mtr><mtd columnalign="left"><msup><mi>w</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mtd>
    <mtd columnalign="left"><mrow><mtext>如果</mtext> <msup><mover accent="true"><msub><mi>y</mi>
    <mi>j</mi></msub> <mo>^</mo></mover> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>=</mo> <msup><mi>y</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><msup><mi>w</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo form="prefix">exp</mo> <mrow><mo>(</mo> <msub><mi>α</mi> <mi>j</mi></msub>
    <mo>)</mo></mrow></mrow></mtd> <mtd columnalign="left"><mrow><mtext>如果</mtext>
    <msup><mover accent="true"><msub><mi>y</mi> <mi>j</mi></msub> <mo>^</mo></mover>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>≠</mo> <msup><mi>y</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mtd></mtr></mtable></mfenced></mrow></mtd></mtr></mtable></math>
- en: Then all the instance weights are normalized (i.e., divided by <math><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msup><mi>w</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math>).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 然后对所有实例权重进行归一化（即除以<math><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msup><mi>w</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math>）。
- en: 'Finally, a new predictor is trained using the updated weights, and the whole
    process is repeated: the new predictor’s weight is computed, the instance weights
    are updated, then another predictor is trained, and so on. The algorithm stops
    when the desired number of predictors is reached, or when a perfect predictor
    is found.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用更新后的权重训练一个新的预测器，并重复整个过程：计算新预测器的权重，更新实例权重，然后训练另一个预测器，依此类推。当达到所需数量的预测器或找到一个完美的预测器时，算法停止。
- en: To make predictions, AdaBoost simply computes the predictions of all the predictors
    and weighs them using the predictor weights *α*[*j*]. The predicted class is the
    one that receives the majority of weighted votes (see [Equation 7-4](#adaboost_prediction)).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行预测，AdaBoost简单地计算所有预测器的预测值，并使用预测器权重*α*[*j*]对它们进行加权。预测的类别是获得加权投票多数的类别（参见[方程7-4](#adaboost_prediction)）。
- en: Equation 7-4\. AdaBoost predictions
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程7-4\. AdaBoost预测
- en: <math display="block"><mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mrow><mo>(</mo> <mi mathvariant="bold">x</mi> <mo>)</mo></mrow> <mo>=</mo> <munder><mo
    form="prefix">argmax</mo> <mi>k</mi></munder> <mrow><munderover><mo>∑</mo> <mfrac
    linethickness="0pt"><mstyle scriptlevel="1" displaystyle="false"><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow></mstyle>
    <mstyle scriptlevel="1" displaystyle="false"><mrow><msub><mover accent="true"><mi>y</mi>
    <mo>^</mo></mover> <mi>j</mi></msub> <mrow><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mrow><mo>=</mo><mi>k</mi></mrow></mstyle></mfrac>
    <mi>N</mi></munderover> <msub><mi>α</mi> <mi>j</mi></msub></mrow> <mtext>where</mtext>
    <mi>N</mi> <mtext>is</mtext> <mtext>the</mtext> <mtext>number</mtext> <mtext>of</mtext>
    <mtext>predictors</mtext></mrow></math>
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mrow><mo>(</mo> <mi mathvariant="bold">x</mi> <mo>)</mo></mrow> <mo>=</mo> <munder><mo
    form="prefix">argmax</mo> <mi>k</mi></munder> <mrow><munderover><mo>∑</mo> <mfrac
    linethickness="0pt"><mstyle scriptlevel="1" displaystyle="false"><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow></mstyle>
    <mstyle scriptlevel="1" displaystyle="false"><mrow><msub><mover accent="true"><mi>y</mi>
    <mo>^</mo></mover> <mi>j</mi></msub> <mrow><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mrow><mo>=</mo><mi>k</mi></mrow></mstyle></mfrac>
    <mi>N</mi></munderover> <msub><mi>α</mi> <mi>j</mi></msub></mrow> <mtext>where</mtext>
    <mi>N</mi> <mtext>is</mtext> <mtext>the</mtext> <mtext>number</mtext> <mtext>of</mtext>
    <mtext>predictors</mtext></mrow></math>
- en: Scikit-Learn uses a multiclass version of AdaBoost called [*SAMME*](https://homl.info/27)⁠^([16](ch07.html#idm45720210436640))
    (which stands for *Stagewise Additive Modeling using a Multiclass Exponential
    loss function*). When there are just two classes, SAMME is equivalent to AdaBoost.
    If the predictors can estimate class probabilities (i.e., if they have a `predict_proba()`
    method), Scikit-Learn can use a variant of SAMME called *SAMME.R* (the *R* stands
    for “Real”), which relies on class probabilities rather than predictions and generally
    performs better.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn使用了AdaBoost的多类版本称为[*SAMME*](https://homl.info/27)⁠^([16](ch07.html#idm45720210436640))（代表*使用多类指数损失函数的逐步增加建模*）。当只有两个类别时，SAMME等同于AdaBoost。如果预测器可以估计类别概率（即，如果它们有一个`predict_proba()`方法），Scikit-Learn可以使用SAMME的变体称为*SAMME.R*（*R*代表“真实”），它依赖于类别概率而不是预测，并通常表现更好。
- en: 'The following code trains an AdaBoost classifier based on 30 *decision stumps*
    using Scikit-Learn’s `AdaBoostClassifier` class (as you might expect, there is
    also an `AdaBoostRegressor` class). A decision stump is a decision tree with `max_depth=1`—in
    other words, a tree composed of a single decision node plus two leaf nodes. This
    is the default base estimator for the `AdaBoostClassifier` class:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码基于Scikit-Learn的`AdaBoostClassifier`类训练了一个基于30个*决策树桩*的AdaBoost分类器（正如您所期望的那样，还有一个`AdaBoostRegressor`类）。决策树桩是一个`max_depth=1`的决策树——换句话说，由一个决策节点和两个叶节点组成的树。这是`AdaBoostClassifier`类的默认基础估计器：
- en: '[PRE12]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Tip
  id: totrans-111
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If your AdaBoost ensemble is overfitting the training set, you can try reducing
    the number of estimators or more strongly regularizing the base estimator.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的AdaBoost集成对训练集过拟合，可以尝试减少估计器的数量或更强烈地正则化基础估计器。
- en: Gradient Boosting
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度提升
- en: Another very popular boosting algorithm is [*gradient boosting*](https://homl.info/28).⁠^([17](ch07.html#idm45720210377680))
    Just like AdaBoost, gradient boosting works by sequentially adding predictors
    to an ensemble, each one correcting its predecessor. However, instead of tweaking
    the instance weights at every iteration like AdaBoost does, this method tries
    to fit the new predictor to the *residual errors* made by the previous predictor.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个非常流行的提升算法是[*梯度提升*](https://homl.info/28)。⁠^([17](ch07.html#idm45720210377680))
    就像AdaBoost一样，梯度提升通过顺序添加预测器到集成中，每个预测器都纠正其前任。然而，与AdaBoost在每次迭代中调整实例权重不同，这种方法试图将新的预测器拟合到前一个预测器产生的*残差错误*上。
- en: 'Let’s go through a simple regression example, using decision trees as the base
    predictors; this is called *gradient tree boosting*, or *gradient boosted regression
    trees* (GBRT). First, let’s generate a noisy quadratic dataset and fit a `DecisionTreeRegressor`
    to it:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的回归示例，使用决策树作为基础预测器；这被称为*梯度树提升*，或*梯度提升回归树*（GBRT）。首先，让我们生成一个带有噪声的二次数据集，并将`DecisionTreeRegressor`拟合到它：
- en: '[PRE13]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, we’ll train a second `DecisionTreeRegressor` on the residual errors made
    by the first predictor:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在第一个预测器产生的残差错误上训练第二个`DecisionTreeRegressor`：
- en: '[PRE14]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'And then we’ll train a third regressor on the residual errors made by the second
    predictor:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将在第二个预测器产生的残差错误上训练第三个回归器：
- en: '[PRE15]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now we have an ensemble containing three trees. It can make predictions on
    a new instance simply by adding up the predictions of all the trees:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个包含三棵树的集成。它可以通过简单地将所有树的预测相加来对新实例进行预测：
- en: '[PRE16]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[Figure 7-9](#gradient_boosting_plot) represents the predictions of these three
    trees in the left column, and the ensemble’s predictions in the right column.
    In the first row, the ensemble has just one tree, so its predictions are exactly
    the same as the first tree’s predictions. In the second row, a new tree is trained
    on the residual errors of the first tree. On the right you can see that the ensemble’s
    predictions are equal to the sum of the predictions of the first two trees. Similarly,
    in the third row another tree is trained on the residual errors of the second
    tree. You can see that the ensemble’s predictions gradually get better as trees
    are added to the ensemble.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-9](#gradient_boosting_plot)在左列中表示这三棵树的预测，右列中表示集成的预测。在第一行中，集成只有一棵树，因此其预测与第一棵树的预测完全相同。在第二行中，新树是在第一棵树的残差错误上训练的。您可以看到集成的预测等于前两棵树的预测之和。类似地，在第三行中，另一棵树是在第二棵树的残差错误上训练的。您可以看到随着树被添加到集成中，集成的预测逐渐变得更好。'
- en: 'You can use Scikit-Learn’s `GradientBoostingRegressor` class to train GBRT
    ensembles more easily (there’s also a `GradientBoostingClassifier` class for classification).
    Much like the `RandomForestRegressor` class, it has hyperparameters to control
    the growth of decision trees (e.g., `max_depth`, `min_samples_leaf`), as well
    as hyperparameters to control the ensemble training, such as the number of trees
    (`n_estimators`). The following code creates the same ensemble as the previous
    one:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用Scikit-Learn的`GradientBoostingRegressor`类更轻松地训练GBRT集合（还有一个用于分类的`GradientBoostingClassifier`类）。就像`RandomForestRegressor`类一样，它有用于控制决策树增长的超参数（例如`max_depth`，`min_samples_leaf`），以及用于控制集合训练的超参数，比如树的数量（`n_estimators`）。以下代码创建了与前一个相同的集合：
- en: '[PRE17]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![mls3 0709](assets/mls3_0709.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0709](assets/mls3_0709.png)'
- en: Figure 7-9\. In this depiction of gradient boosting, the first predictor (top
    left) is trained normally, then each consecutive predictor (middle left and lower
    left) is trained on the previous predictor’s residuals; the right column shows
    the resulting ensemble’s predictions
  id: totrans-127
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-9。在这个梯度提升的描述中，第一个预测器（左上角）被正常训练，然后每个连续的预测器（左中和左下）都是在前一个预测器的残差上进行训练；右列显示了结果集合的预测
- en: 'The `learning_rate` hyperparameter scales the contribution of each tree. If
    you set it to a low value, such as `0.05`, you will need more trees in the ensemble
    to fit the training set, but the predictions will usually generalize better. This
    is a regularization technique called *shrinkage*. [Figure 7-10](#gbrt_learning_rate_plot)
    shows two GBRT ensembles trained with different hyperparameters: the one on the
    left does not have enough trees to fit the training set, while the one on the
    right has about the right amount. If we added more trees, the GBRT would start
    to overfit the training set.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`learning_rate`超参数缩放每棵树的贡献。如果将其设置为一个较低的值，比如`0.05`，则需要更多的树来拟合训练集，但预测通常会更好地泛化。这是一种称为*缩减*的正则化技术。[图7-10](#gbrt_learning_rate_plot)显示了使用不同超参数训练的两个GBRT集合：左侧的集合没有足够的树来拟合训练集，而右侧的集合有大约适量的树。如果添加更多的树，GBRT将开始过拟合训练集。'
- en: '![mls3 0710](assets/mls3_0710.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0710](assets/mls3_0710.png)'
- en: Figure 7-10\. GBRT ensembles with not enough predictors (left) and just enough
    (right)
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-10。GBRT集合，预测器不足（左）和刚好足够（右）
- en: 'To find the optimal number of trees, you could perform cross-validation using
    `GridSearchCV` or `RandomizedSearchCV`, as usual, but there’s a simpler way: if
    you set the `n_iter_no_change` hyperparameter to an integer value, say 10, then
    the `GradientBoostingRegressor` will automatically stop adding more trees during
    training if it sees that the last 10 trees didn’t help. This is simply early stopping
    (introduced in [Chapter 4](ch04.html#linear_models_chapter)), but with a little
    bit of patience: it tolerates having no progress for a few iterations before it
    stops. Let’s train the ensemble using early stopping:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到最佳数目的树，可以像往常一样使用`GridSearchCV`或`RandomizedSearchCV`进行交叉验证，但也有一种更简单的方法：如果将`n_iter_no_change`超参数设置为一个整数值，比如10，那么`GradientBoostingRegressor`将在训练过程中自动停止添加更多的树，如果看到最后的10棵树没有帮助。这只是早停（在[第4章](ch04.html#linear_models_chapter)中介绍），但需要一点耐心：它容忍在停止之前几次迭代没有进展。让我们使用早停来训练集合：
- en: '[PRE18]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'If you set `n_iter_no_change` too low, training may stop too early and the
    model will underfit. But if you set it too high, it will overfit instead. We also
    set a fairly small learning rate and a high number of estimators, but the actual
    number of estimators in the trained ensemble is much lower, thanks to early stopping:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将`n_iter_no_change`设置得太低，训练可能会过早停止，模型会欠拟合。但如果设置得太高，它将过拟合。我们还设置了一个相当小的学习率和一个较高数量的估计器，但由于早停，训练集合中实际的估计器数量要低得多：
- en: '[PRE19]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'When `n_iter_no_change` is set, the `fit()` method automatically splits the
    training set into a smaller training set and a validation set: this allows it
    to evaluate the model’s performance each time it adds a new tree. The size of
    the validation set is controlled by the `validation_fraction` hyperparameter,
    which is 10% by default. The `tol` hyperparameter determines the maximum performance
    improvement that still counts as negligible. It defaults to 0.0001.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 当设置了`n_iter_no_change`时，`fit()`方法会自动将训练集分成一个较小的训练集和一个验证集：这使得它可以在每次添加新树时评估模型的性能。验证集的大小由`validation_fraction`超参数控制，默认为10%。`tol`超参数确定了仍然被视为微不足道的最大性能改进。默认值为0.0001。
- en: The `GradientBoostingRegressor` class also supports a `subsample` hyperparameter,
    which specifies the fraction of training instances to be used for training each
    tree. For example, if `subsample=0.25`, then each tree is trained on 25% of the
    training instances, selected randomly. As you can probably guess by now, this
    technique trades a higher bias for a lower variance. It also speeds up training
    considerably. This is called *stochastic gradient boosting*.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`GradientBoostingRegressor`类还支持一个`subsample`超参数，指定用于训练每棵树的训练实例的分数。例如，如果`subsample=0.25`，则每棵树都是在选择的25%训练实例上随机训练的。现在你可能已经猜到了，这种技术以更低的方差换取更高的偏差。它还显著加快了训练速度。这被称为*随机梯度提升*。'
- en: Histogram-Based Gradient Boosting
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于直方图的梯度提升
- en: 'Scikit-Learn also provides another GBRT implementation, optimized for large
    datasets: *histogram-based gradient boosting* (HGB). It works by binning the input
    features, replacing them with integers. The number of bins is controlled by the
    `max_bins` hyperparameter, which defaults to 255 and cannot be set any higher
    than this. Binning can greatly reduce the number of possible thresholds that the
    training algorithm needs to evaluate. Moreover, working with integers makes it
    possible to use faster and more memory-efficient data structures. And the way
    the bins are built removes the need for sorting the features when training each
    tree.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn还提供了另一种针对大型数据集进行优化的GBRT实现：*基于直方图的梯度提升*（HGB）。它通过对输入特征进行分箱，用整数替换它们来工作。箱数由`max_bins`超参数控制，默认为255，不能设置得比这更高。分箱可以大大减少训练算法需要评估的可能阈值数量。此外，使用整数可以使用更快速和更节省内存的数据结构。构建箱的方式消除了在训练每棵树时对特征进行排序的需要。
- en: 'As a result, this implementation has a computational complexity of *O*(*b*×*m*)
    instead of *O*(*n*×*m*×log(*m*)), where *b* is the number of bins, *m* is the
    number of training instances, and *n* is the number of features. In practice,
    this means that HGB can train hundreds of times faster than regular GBRT on large
    datasets. However, binning causes a precision loss, which acts as a regularizer:
    depending on the dataset, this may help reduce overfitting, or it may cause underfitting.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这种实现的计算复杂度为*O*(*b*×*m*)，而不是*O*(*n*×*m*×log(*m*))，其中*b*是箱数，*m*是训练实例数，*n*是特征数。实际上，这意味着HGB在大型数据集上的训练速度可以比常规GBRT快数百倍。然而，分箱会导致精度损失，这起到了正则化的作用：根据数据集的不同，这可能有助于减少过拟合，也可能导致欠拟合。
- en: 'Scikit-Learn provides two classes for HGB: `HistGradientBoostingRegressor`
    and `HistGradientBoostingClassifier`. They’re similar to `GradientBoostingRegressor`
    and `GradientBoostingClassifier`, with a few notable differences:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn提供了两个HGB类：`HistGradientBoostingRegressor`和`HistGradientBoostingClassifier`。它们类似于`GradientBoostingRegressor`和`GradientBoostingClassifier`，但有一些显著的区别：
- en: Early stopping is automatically activated if the number of instances is greater
    than 10,000\. You can turn early stopping always on or always off by setting the
    `early_stopping` hyperparameter to `True` or `False`.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果实例数大于10,000，则自动启用提前停止。您可以通过将`early_stopping`超参数设置为`True`或`False`来始终启用或关闭提前停止。
- en: Subsampling is not supported.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不支持子采样。
- en: '`n_estimators` is renamed to `max_iter`.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_estimators`被重命名为`max_iter`。'
- en: The only decision tree hyperparameters that can be tweaked are `max_leaf_nodes`,
    `min_samples_leaf`, and `max_depth`.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 唯一可以调整的决策树超参数是`max_leaf_nodes`、`min_samples_leaf`和`max_depth`。
- en: 'The HGB classes also have two nice features: they support both categorical
    features and missing values. This simplifies preprocessing quite a bit. However,
    the categorical features must be represented as integers ranging from 0 to a number
    lower than `max_bins`. You can use an `OrdinalEncoder` for this. For example,
    here’s how to build and train a complete pipeline for the California housing dataset
    introduced in [Chapter 2](ch02.html#project_chapter):'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: HGB类还有两个很好的特性：它们支持分类特征和缺失值。这在预处理方面简化了很多。但是，分类特征必须表示为从0到小于`max_bins`的整数。您可以使用`OrdinalEncoder`来实现。例如，以下是如何为[第2章](ch02.html#project_chapter)介绍的加利福尼亚住房数据集构建和训练完整管道的方法：
- en: '[PRE20]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The whole pipeline is just as short as the imports! No need for an imputer,
    scaler, or a one-hot encoder, so it’s really convenient. Note that `categorical_features`
    must be set to the categorical column indices (or a Boolean array). Without any
    hyperparameter tuning, this model yields an RMSE of about 47,600, which is not
    too bad.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 整个管道和导入一样简短！不需要填充器、缩放器或独热编码器，非常方便。请注意，`categorical_features`必须设置为分类列的索引（或布尔数组）。在没有进行任何超参数调整的情况下，该模型的RMSE约为47,600，这还算不错。
- en: Tip
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'Several other optimized implementations of gradient boosting are available
    in the Python ML ecosystem: in particular, [XGBoost](https://github.com/dmlc/xgboost),
    [CatBoost](https://catboost.ai), and [LightGBM](https://lightgbm.readthedocs.io).
    These libraries have been around for several years. They are all specialized for
    gradient boosting, their APIs are very similar to Scikit-Learn’s, and they provide
    many additional features, including GPU acceleration; you should definitely check
    them out! Moreover, the [TensorFlow Random Forests library](https://tensorflow.org/decision_forests)
    provides optimized implementations of a variety of random forest algorithms, including
    plain random forests, extra-trees, GBRT, and several more.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Python ML生态系统中还提供了几种经过优化的梯度提升实现：特别是[XGBoost](https://github.com/dmlc/xgboost)、[CatBoost](https://catboost.ai)和[LightGBM](https://lightgbm.readthedocs.io)。这些库已经存在了几年。它们都专门用于梯度提升，它们的API与Scikit-Learn的非常相似，并提供许多附加功能，包括GPU加速；您一定要去了解一下！此外，[TensorFlow随机森林库](https://tensorflow.org/decision_forests)提供了各种随机森林算法的优化实现，包括普通随机森林、极端树、GBRT等等。
- en: Stacking
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 堆叠
- en: 'The last ensemble method we will discuss in this chapter is called *stacking*
    (short for [*stacked generalization*](https://homl.info/29)).⁠^([18](ch07.html#idm45720209752032))
    It is based on a simple idea: instead of using trivial functions (such as hard
    voting) to aggregate the predictions of all predictors in an ensemble, why don’t
    we train a model to perform this aggregation? [Figure 7-11](#blending_prediction_diagram)
    shows such an ensemble performing a regression task on a new instance. Each of
    the bottom three predictors predicts a different value (3.1, 2.7, and 2.9), and
    then the final predictor (called a *blender*, or a *meta learner*) takes these
    predictions as inputs and makes the final prediction (3.0).'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将讨论的最后一种集成方法称为*堆叠*（缩写为[*堆叠泛化*](https://homl.info/29)）。它基于一个简单的想法：不要使用微不足道的函数（如硬投票）来聚合集成中所有预测器的预测，为什么不训练一个模型来执行这种聚合呢？[图7-11](#blending_prediction_diagram)展示了这样一个集成在新实例上执行回归任务。底部的三个预测器中的每一个都预测不同的值（3.1、2.7和2.9），然后最终的预测器（称为*混合器*或*元学习器*）将这些预测作为输入，做出最终的预测（3.0）。
- en: '![mls3 0711](assets/mls3_0711.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0711](assets/mls3_0711.png)'
- en: Figure 7-11\. Aggregating predictions using a blending predictor
  id: totrans-153
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-11。使用混合预测器聚合预测
- en: To train the blender, you first need to build the blending training set. You
    can use `cross_val_predict()` on every predictor in the ensemble to get out-of-sample
    predictions for each instance in the original training set ([Figure 7-12](#blending_layer_training_diagram)),
    and use these can be used as the input features to train the blender; and the
    targets can simply be copied from the original training set. Note that regardless
    of the number of features in the original training set (just one in this example),
    the blending training set will contain one input feature per predictor (three
    in this example). Once the blender is trained, the base predictors are retrained
    one last time on the full original training set.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练混合器，首先需要构建混合训练集。您可以对集成中的每个预测器使用`cross_val_predict()`，以获取原始训练集中每个实例的样本外预测（[图7-12](#blending_layer_training_diagram)），并将这些用作输入特征来训练混合器；目标可以简单地从原始训练集中复制。请注意，无论原始训练集中的特征数量如何（在本例中只有一个），混合训练集将包含每个预测器的一个输入特征（在本例中为三个）。一旦混合器训练完成，基本预测器将最后一次在完整的原始训练集上重新训练。
- en: '![mls3 0712](assets/mls3_0712.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0712](assets/mls3_0712.png)'
- en: Figure 7-12\. Training the blender in a stacking ensemble
  id: totrans-156
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-12。在堆叠集成中训练混合器
- en: It is actually possible to train several different blenders this way (e.g.,
    one using linear regression, another using random forest regression) to get a
    whole layer of blenders, and then add another blender on top of that to produce
    the final prediction, as shown in [Figure 7-13](#multi_layer_blending_diagram).
    You may be able to squeeze out a few more drops of performance by doing this,
    but it will cost you in both training time and system complexity.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上可以通过这种方式训练几个不同的混合器（例如，一个使用线性回归，另一个使用随机森林回归），以获得一个完整的混合器层，并在其上再添加另一个混合器以生成最终预测，如[图7-13](#multi_layer_blending_diagram)所示。通过这样做，您可能会挤出更多的性能，但这将在训练时间和系统复杂性方面付出代价。
- en: '![mls3 0713](assets/mls3_0713.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0713](assets/mls3_0713.png)'
- en: Figure 7-13\. Predictions in a multilayer stacking ensemble
  id: totrans-159
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-13。多层堆叠集成中的预测
- en: 'Scikit-Learn provides two classes for stacking ensembles: `StackingClassifier`
    and `StackingRegressor`. For example, we can replace the `VotingClassifier` we
    used at the beginning of this chapter on the moons dataset with a `StackingClassifier`:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn提供了两个用于堆叠集成的类：`StackingClassifier`和`StackingRegressor`。例如，我们可以用`StackingClassifier`替换本章开始时在moons数据集上使用的`VotingClassifier`：
- en: '[PRE21]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: For each predictor, the stacking classifier will call `predict_proba()` if available;
    if not it will fall back to `decision_function()` or, as a last resort, call `predict()`.
    If you don’t provide a final estimator, `StackingClassifier` will use `LogisticRegression`
    and `StackingRegressor` will use `RidgeCV`.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个预测器，堆叠分类器将调用`predict_proba()`（如果可用）；如果不可用，它将退而求其次调用`decision_function()`，或者作为最后手段调用`predict()`。如果您没有提供最终的估计器，`StackingClassifier`将使用`LogisticRegression`，而`StackingRegressor`将使用`RidgeCV`。
- en: If you evaluate this stacking model on the test set, you will find 92.8% accuracy,
    which is a bit better than the voting classifier using soft voting, which got
    92%.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在测试集上评估这个堆叠模型，您会发现92.8%的准确率，比使用软投票的投票分类器稍好，后者获得了92%。
- en: In conclusion, ensemble methods are versatile, powerful, and fairly simple to
    use. Random forests, AdaBoost, and GBRT are among the first models you should
    test for most machine learning tasks, and they particularly shine with heterogeneous
    tabular data. Moreover, as they require very little preprocessing, they’re great
    for getting a prototype up and running quickly. Lastly, ensemble methods like
    voting classifiers and stacking classifiers can help push your system’s performance
    to its limits.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，集成方法是多才多艺、强大且相当简单易用的。随机森林、AdaBoost和GBRT是您应该为大多数机器学习任务测试的第一批模型，它们在异构表格数据方面表现尤为出色。此外，由于它们需要非常少的预处理，因此非常适合快速搭建原型。最后，像投票分类器和堆叠分类器这样的集成方法可以帮助将系统的性能推向极限。
- en: Exercises
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: If you have trained five different models on the exact same training data, and
    they all achieve 95% precision, is there any chance that you can combine these
    models to get better results? If so, how? If not, why?
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您在完全相同的训练数据上训练了五个不同的模型，并且它们都达到了95%的精度，是否有可能将这些模型组合以获得更好的结果？如果可以，如何？如果不行，为什么？
- en: What is the difference between hard and soft voting classifiers?
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 硬投票分类器和软投票分类器之间有什么区别？
- en: Is it possible to speed up training of a bagging ensemble by distributing it
    across multiple servers? What about pasting ensembles, boosting ensembles, random
    forests, or stacking ensembles?
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将一个装袋集成的训练分布到多个服务器上是否可以加快训练速度？那么对于粘贴集成、提升集成、随机森林或堆叠集成呢？
- en: What is the benefit of out-of-bag evaluation?
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是袋外评估的好处？
- en: What makes extra-trees ensembles more random than regular random forests? How
    can this extra randomness help? Are extra-trees classifiers slower or faster than
    regular random forests?
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么使得额外树集成比常规随机森林更随机？这种额外的随机性如何帮助？额外树分类器比常规随机森林慢还是快？
- en: If your AdaBoost ensemble underfits the training data, which hyperparameters
    should you tweak, and how?
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您的AdaBoost集成对训练数据拟合不足，应该调整哪些超参数，如何调整？
- en: If your gradient boosting ensemble overfits the training set, should you increase
    or decrease the learning rate?
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您的梯度提升集成对训练集过拟合，应该增加还是减少学习率？
- en: Load the MNIST dataset (introduced in [Chapter 3](ch03.html#classification_chapter)),
    and split it into a training set, a validation set, and a test set (e.g., use
    50,000 instances for training, 10,000 for validation, and 10,000 for testing).
    Then train various classifiers, such as a random forest classifier, an extra-trees
    classifier, and an SVM classifier. Next, try to combine them into an ensemble
    that outperforms each individual classifier on the validation set, using soft
    or hard voting. Once you have found one, try it on the test set. How much better
    does it perform compared to the individual classifiers?
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载MNIST数据集（在[第3章](ch03.html#classification_chapter)中介绍），将其分为训练集、验证集和测试集（例如，使用50,000个实例进行训练，10,000个用于验证，10,000个用于测试）。然后训练各种分类器，如随机森林分类器、额外树分类器和SVM分类器。接下来，尝试将它们组合成一个集成，使用软投票或硬投票在验证集上优于每个单独的分类器。一旦找到一个，尝试在测试集上运行。它的表现比单个分类器好多少？
- en: 'Run the individual classifiers from the previous exercise to make predictions
    on the validation set, and create a new training set with the resulting predictions:
    each training instance is a vector containing the set of predictions from all
    your classifiers for an image, and the target is the image’s class. Train a classifier
    on this new training set. Congratulations—you have just trained a blender, and
    together with the classifiers it forms a stacking ensemble! Now evaluate the ensemble
    on the test set. For each image in the test set, make predictions with all your
    classifiers, then feed the predictions to the blender to get the ensemble’s predictions.
    How does it compare to the voting classifier you trained earlier? Now try again
    using a `StackingClassifier` instead. Do you get better performance? If so, why?'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行上一个练习中的各个分类器，对验证集进行预测，并使用结果预测创建一个新的训练集：每个训练实例是一个向量，包含所有分类器对图像的预测集合，目标是图像的类别。在这个新的训练集上训练一个分类器。恭喜您——您刚刚训练了一个混合器，它与分类器一起形成了一个堆叠集成！现在在测试集上评估集成。对于测试集中的每个图像，使用所有分类器进行预测，然后将预测结果输入混合器以获得集成的预测。它与您之前训练的投票分类器相比如何？现在尝试使用`StackingClassifier`。性能更好吗？如果是，为什么？
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab3*](https://homl.info/colab3).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习的解决方案可以在本章笔记本的末尾找到，网址为[*https://homl.info/colab3*](https://homl.info/colab3)。
- en: '^([1](ch07.html#idm45720211152336-marker)) Imagine picking a card randomly
    from a deck of cards, writing it down, then placing it back in the deck before
    picking the next card: the same card could be sampled multiple times.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch07.html#idm45720211152336-marker)) 想象从一副牌中随机抽取一张卡片，写下来，然后将其放回到牌组中再抽取下一张卡片：同一张卡片可能被多次抽样。
- en: '^([2](ch07.html#idm45720211150768-marker)) Leo Breiman, “Bagging Predictors”,
    *Machine Learning* 24, no. 2 (1996): 123–140.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '^([2](ch07.html#idm45720211150768-marker)) Leo Breiman，“Bagging预测器”，*机器学习*
    24, no. 2 (1996): 123–140。'
- en: ^([3](ch07.html#idm45720211149248-marker)) In statistics, resampling with replacement
    is called *bootstrapping*.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch07.html#idm45720211149248-marker)) 在统计学中，带替换的重采样被称为*自助法*。
- en: '^([4](ch07.html#idm45720211146016-marker)) Leo Breiman, “Pasting Small Votes
    for Classification in Large Databases and On-Line”, *Machine Learning* 36, no.
    1–2 (1999): 85–103.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '^([4](ch07.html#idm45720211146016-marker)) Leo Breiman，“在大型数据库和在线分类中粘贴小投票”，*机器学习*
    36, no. 1–2 (1999): 85–103。'
- en: ^([5](ch07.html#idm45720211140576-marker)) Bias and variance were introduced
    in [Chapter 4](ch04.html#linear_models_chapter).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch07.html#idm45720211140576-marker)) 偏差和方差在[第4章](ch04.html#linear_models_chapter)中介绍过。
- en: ^([6](ch07.html#idm45720211131712-marker)) `max_samples` can alternatively be
    set to a float between 0.0 and 1.0, in which case the max number of sampled instances
    is equal to the size of the training set times `max_samples`.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch07.html#idm45720211131712-marker)) `max_samples`也可以设置为0.0到1.0之间的浮点数，此时采样实例的最大数量等于训练集大小乘以`max_samples`。
- en: ^([7](ch07.html#idm45720211046720-marker)) As *m* grows, this ratio approaches
    1 – exp(–1) ≈ 63%.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch07.html#idm45720211046720-marker)) 当*m*增长时，这个比率接近1 – exp(–1) ≈ 63%。
- en: '^([8](ch07.html#idm45720210898224-marker)) Gilles Louppe and Pierre Geurts,
    “Ensembles on Random Patches”, *Lecture Notes in Computer Science* 7523 (2012):
    346–361.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '^([8](ch07.html#idm45720210898224-marker)) Gilles Louppe和Pierre Geurts，“随机补丁上的集成”，*计算机科学讲义*
    7523 (2012): 346–361。'
- en: '^([9](ch07.html#idm45720210877120-marker)) Tin Kam Ho, “The Random Subspace
    Method for Constructing Decision Forests”, *IEEE Transactions on Pattern Analysis
    and Machine Intelligence* 20, no. 8 (1998): 832–844.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '^([9](ch07.html#idm45720210877120-marker)) 何天金，“用于构建决策森林的随机子空间方法”，*IEEE模式分析与机器智能*
    20, no. 8 (1998): 832–844。'
- en: '^([10](ch07.html#idm45720210870352-marker)) Tin Kam Ho, “Random Decision Forests”,
    *Proceedings of the Third International Conference on Document Analysis and Recognition*
    1 (1995): 278.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '^([10](ch07.html#idm45720210870352-marker)) 何天金，“随机决策森林”，*第三届文档分析与识别国际会议论文集*
    1 (1995): 278。'
- en: ^([11](ch07.html#idm45720210863312-marker)) The `BaggingClassifier` class remains
    useful if you want a bag of something other than decision trees.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch07.html#idm45720210863312-marker)) 如果您想要一个除决策树以外的袋子，`BaggingClassifier`
    类仍然很有用。
- en: '^([12](ch07.html#idm45720210757984-marker)) Pierre Geurts et al., “Extremely
    Randomized Trees”, *Machine Learning* 63, no. 1 (2006): 3–42.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '^([12](ch07.html#idm45720210757984-marker)) Pierre Geurts等，“极端随机树”，*机器学习* 63,
    no. 1 (2006): 3–42。'
- en: '^([13](ch07.html#idm45720210584080-marker)) Yoav Freund and Robert E. Schapire,
    “A Decision-Theoretic Generalization of On-Line Learning and an Application to
    Boosting”, *Journal of Computer and System Sciences* 55, no. 1 (1997): 119–139.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '^([13](ch07.html#idm45720210584080-marker)) Yoav Freund 和 Robert E. Schapire,
    “一个决策理论的在线学习泛化及其在 Boosting 中的应用”, *计算机与系统科学杂志* 55, no. 1 (1997): 119–139.'
- en: ^([14](ch07.html#idm45720210574352-marker)) This is just for illustrative purposes.
    SVMs are generally not good base predictors for AdaBoost; they are slow and tend
    to be unstable with it.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch07.html#idm45720210574352-marker)) 这仅供说明目的。SVMs 通常不是 AdaBoost 的好基础预测器；它们速度慢且在其中不稳定。
- en: ^([15](ch07.html#idm45720210527056-marker)) The original AdaBoost algorithm
    does not use a learning rate hyperparameter.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch07.html#idm45720210527056-marker)) 原始的 AdaBoost 算法不使用学习率超参数。
- en: '^([16](ch07.html#idm45720210436640-marker)) For more details, see Ji Zhu et
    al., “Multi-Class AdaBoost”, *Statistics and Its Interface* 2, no. 3 (2009): 349–360.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '^([16](ch07.html#idm45720210436640-marker)) 更多细节请参见 Ji Zhu 等人的 “多类别 AdaBoost”,
    *统计学及其界面* 2, no. 3 (2009): 349–360.'
- en: '^([17](ch07.html#idm45720210377680-marker)) Gradient boosting was first introduced
    in Leo Breiman’s [1997 paper](https://homl.info/arcing) “Arcing the Edge” and
    was further developed in the [1999 paper](https://homl.info/gradboost) “Greedy
    Function Approximation: A Gradient Boosting Machine” by Jerome H. Friedman.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '^([17](ch07.html#idm45720210377680-marker)) 梯度提升首次在 Leo Breiman 的 [1997 年论文](https://homl.info/arcing)
    “Arcing the Edge” 中引入，并在 Jerome H. Friedman 的 [1999 年论文](https://homl.info/gradboost)
    “Greedy Function Approximation: A Gradient Boosting Machine” 中进一步发展。'
- en: '^([18](ch07.html#idm45720209752032-marker)) David H. Wolpert, “Stacked Generalization”,
    *Neural Networks* 5, no. 2 (1992): 241–259.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '^([18](ch07.html#idm45720209752032-marker)) David H. Wolpert, “堆叠泛化”, *神经网络*
    5, no. 2 (1992): 241–259.'
