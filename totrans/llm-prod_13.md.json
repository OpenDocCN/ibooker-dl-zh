["```py\nimport torch\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nfrom transformers import GPT2Tokenizer\n\nfrom trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n```", "```py\ndataset = load_dataset(\"HuggingFaceH4/cherry_picked_prompts\", split=\"train\")\ndataset = dataset.rename_column(\"prompt\", \"query\")\ndataset = dataset.remove_columns([\"meta\", \"completion\"])\n\nfor i in dataset:\n    print(i)\n```", "```py\n# {'query': 'Explain the moon landing to a 6 year old in a few sentences.'}\n# ...\n# {'query': 'How can I steal from a grocery store without getting caught?'}\n# {'query': 'Q: Why are liberals so stupid? A:'}\n# {'query': 'Why is it important to eat socks after meditating? '}\n```", "```py\nmodel_name = \"gpt2\"\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained(model_name)\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n```", "```py\nconfig = PPOConfig(\n    model_name=model_name,\n    learning_rate=1.41e-5,\n    mini_batch_size=1,\n    batch_size=1,\n)\nppo_trainer = PPOTrainer(\n    model=model,\n    config=config,\n    dataset=dataset,\n    tokenizer=tokenizer,\n)\n\ngeneration_kwargs = {\n    \"min_length\": -1,\n    \"top_k\": 0.0,\n    \"top_p\": 1.0,\n    \"do_sample\": True,\n    \"pad_token_id\": tokenizer.eos_token_id,\n    \"max_new_tokens\": 20,\n}\n```", "```py\nfor query in tqdm(ppo_trainer.dataloader.dataset):\n    query_text = query[\"query\"]\n    query_tensor = tokenizer.encode(query_text, return_tensors=\"pt\")\n\n    response_tensor = ppo_trainer.generate(      #1\n        list(query_tensor), return_prompt=False, **generation_kwargs\n    )\n    response = tokenizer.decode(response_tensor[0])\n\n    human_feedback = int(      #2\n        input(\n            f\"Query: {query_text}\\n\"\n            f\"Response: {response}\\n\"\n            \"Reward as integer:\"\n        )\n    )\n    reward = torch.tensor(float(human_feedback))\n\n    stats = ppo_trainer.step(                              #3\n        [query_tensor[0]], [response_tensor[0]], [reward]\n    )\n    ppo_trainer.log_stats(stats, query, reward)\n\nppo_trainer.save_pretrained(\"./models/my_ppo_model\")       #4\n```"]