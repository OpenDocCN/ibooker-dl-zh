- en: 7 Augmenting intent data with generative AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 使用生成式人工智能增强意图数据
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Creating new training and testing examples with generative AI
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用生成式人工智能创建新的训练和测试示例
- en: Identifying gaps in your current conversational AI data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别你当前对话式人工智能数据中的差距
- en: Use LLMs to build new intents in your conversational AI
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LLMs在对话式人工智能中构建新的意图
- en: Conversational AI users are frustrated when the AI does not understand them—especially
    if it happens multiple times! This applies to all conversational AI types, including
    question-answering bots, process-oriented bots, and routing agents. We’ve seen
    multiple strategies for improving the AI’s ability to understand. The first strategy—improving
    intent training manually (chapter 5)—gives full control to the human builder,
    but it takes time and specialized skill. The second strategy—retrieval-augmented
    generation (RAG, chapter 6)—gives much more control to generative AI, reducing
    the role of the human builder over time. This chapter introduces a hybrid approach
    where generative AI augments the builder. This applies to rules-based or generative
    AI–based systems.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 对话式人工智能用户在人工智能无法理解他们时感到沮丧——尤其是如果这种情况发生多次的话！这适用于所有对话式人工智能类型，包括问答机器人、面向流程的机器人和路由代理。我们已经看到了多种提高人工智能理解能力的策略。第一种策略——手动改进意图训练（第5章）——赋予人类构建者完全控制权，但这需要时间和专业技能。第二种策略——检索增强生成（RAG，第6章）——赋予生成式人工智能更多的控制权，随着时间的推移减少人类构建者的角色。本章介绍了一种混合方法，其中生成式人工智能增强构建者。这适用于基于规则或基于生成式人工智能的系统。
- en: Using generative AI as a “muse” for the human builder reduces the effort and
    time required of the human builder, increases the amount of test data available
    for data science activities, and gives the human builder the final say, which
    eliminates most opportunities for hallucinations (which is when AI says something
    that looks reasonable but is not true).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 将生成式人工智能作为人类构建者的“灵感之源”可以减少人类构建者的努力和时间，增加可用于数据科学活动的测试数据量，并让人类构建者拥有最终决定权，从而消除了大多数幻觉机会（即人工智能说出看似合理但实际上不正确的话）。
- en: Let’s say you are building a conversational AI solution to help your IT help
    desk. From interviews, you know that password resets are the most frequent task
    the AI needs to support. Therefore, the AI needs a strong understanding of the
    password reset intent.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在构建一个对话式人工智能解决方案来帮助你的IT帮助台。从访谈中，你知道密码重置是人工智能需要支持的最频繁的任务。因此，人工智能需要对密码重置意图有深入的理解。
- en: Because the conversational AI solution is new, you don’t have any production
    user utterances to train from. When you ask the service desk how users generally
    start their conversations, you hear “Well, they usually say something about ‘forgot
    password’ or ‘cannot login.’” You are appropriately suspicious—surely the users
    have a broader vocabulary than that—but you have a hard time imagining what that
    vocabulary might be. Generative AI can help you imagine.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由于对话式人工智能解决方案是新的，你没有任何生产用户话语来训练。当你询问服务台用户通常如何开始对话时，你听到“嗯，他们通常会说一些关于‘忘记密码’或‘无法登录’的话。”你很合适地怀疑——当然，用户的词汇量比这要广——但你很难想象这个词汇量可能是什么。生成式人工智能可以帮助你想象。
- en: Let’s look at how the human builder and generative AI can be partners.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看人类构建者和生成式人工智能如何成为合作伙伴。
- en: 7.1 Getting started
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 开始
- en: Large language models (LLMs) are skilled at performing many technical tasks,
    including classification and question answering. These are the same tasks at the
    core of conversational AI. So why don’t we just use generative AI for our core
    conversational AI tasks?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）擅长执行许多技术任务，包括分类和问答。这些任务与对话式人工智能的核心任务相同。那么，我们为什么不直接使用生成式人工智能来完成我们的核心对话式人工智能任务呢？
- en: LLMs are generalizable because they have been trained on huge amounts of data.
    This makes them a quick study on many tasks, but it also comes with some cost.
    What are the costs to using LLM as the classifier in conversational AI?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs之所以具有可推广性，是因为它们在大量数据上进行了训练。这使得它们在许多任务上学习迅速，但也带来了一些成本。使用LLM作为对话式人工智能中的分类器的成本是什么？
- en: '*Monetary*—LLMs can be expensive to run.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*货币成本*——LLMs的运行成本可能很高。'
- en: '*Speed*—Because LLMs consider billions of parameters, they can be slower (a
    time cost).'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*速度*——由于LLMs考虑了数十亿个参数，它们可能会更慢（时间成本）。'
- en: '*Reputation risk*—LLMs are so generalized that they may hallucinate output
    that makes your bot look bad or exposes you to legal risk.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*声誉风险*——LLMs过于通用，可能会产生使你的机器人看起来很糟糕或让你面临法律风险的输出。'
- en: '*Lack of transparency and explainability*—LLMs are often a “black box.”'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*缺乏透明度和可解释性*——大型语言模型（LLMs）通常是一个“黑盒”。'
- en: By contrast, conversational AI uses purpose-built technology. Because its classifier
    is trained only to do the task at hand, it is much cheaper, and it runs quickly
    because it considers fewer parameters. While that may reduce the accuracy, the
    system is guaranteed to use a set of controlled responses. These comparisons are
    summarized in table 7.1.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，对话式AI使用专为特定目的设计的科技。因为其分类器仅被训练执行当前任务，所以它成本更低，并且运行速度快，因为它考虑的参数更少。虽然这可能会降低准确性，但系统保证使用一组受控的响应。这些比较总结在表7.1中。
- en: Table 7.1 Comparing and contrasting traditional natural language processing
    (NLP) in conversational AI and generative AI on the classification task
  id: totrans-18
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表7.1 比较和对比传统自然语言处理（NLP）在对话式AI和生成式AI在分类任务上的差异
- en: '| Feature | Traditional NLP | Generative AI |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 特征 | 传统NLP | 生成式AI |'
- en: '| --- | --- | --- |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Model  | Purpose-built for excellence at only one task: classification  |
    Generalized model good at many tasks  |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 专为仅在一个任务上表现出色而定制：分类 | 适用于许多任务的通用模型 |'
- en: '| Runtime speed  | Fast  | Slow  |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 运行时速度 | 快速 | 慢速 |'
- en: '| Runtime cost  | Low  | High  |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 运行时成本 | 低 | 高 |'
- en: '| Accuracy  | Mostly accurate (trained by you on small data)  | Mostly accurate
    (pretrained on huge data)  |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 准确性 | 主要准确（在少量数据上由你训练） | 主要准确（在大量数据上预训练） |'
- en: '| Scalability  | Manageable for up to 100 intents; very difficult afterward  |
    Generalizes very well via RAG pattern  |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 可扩展性 | 可管理多达100个意图；之后非常困难 | 通过RAG模式很好地泛化 |'
- en: '| Controllability  | Strictly controlled by humans; requires extensive testing  |
    Prone to hallucinate when given full control; hallucinations are hard to detect
    automatically  |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 可控性 | 严格受人类控制；需要大量测试 | 当给予完全控制时容易产生幻觉；自动检测幻觉很困难 |'
- en: We can use a hybrid approach to get the best of both methods.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用混合方法来获得两种方法的最佳效果。
- en: '7.1.1 Why do it: Pros and cons'
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.1 为什么这样做：优点和缺点
- en: An LLM can greatly reduce the time and effort spent by a human builder. LLMs
    and humans work best together—as partners. Training a conversational AI classifier
    requires human effort, but it also requires data, and that data can be hard to
    collect. Sometimes that data cannot be collected until the conversational AI is
    deployed in production. Even in our familiar example of detecting “forgot password”
    problems, we still don’t know all the ways users might state their problem. They
    may use the “wrong” words!
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一个LLM可以大大减少人类构建者所花费的时间和精力。LLM和人类最佳合作——作为合作伙伴。训练对话式AI分类器需要人力，但也需要数据，而这些数据可能很难收集。有时，这些数据只能在对话式AI投入生产后才能收集。即使在我们的熟悉例子中检测“忘记密码”问题，我们仍然不知道用户可能表达问题的所有方式。他们可能会使用“错误”的词语！
- en: 'LLMs are especially helpful in these scenarios:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs在这些场景中特别有帮助：
- en: '*Bootstrapping*—AI suffers from a “cold start” problem. How can you train when
    you have no data? LLMs can generate an initial set of training data.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自举*—AI存在“冷启动”问题。当你没有数据时如何训练？LLM可以生成一组初始的训练数据。'
- en: '*Expanding*—Use LLMs to fill the gaps in your existing data when you don’t
    have enough data to optimize your classifier’s accuracy. This is especially useful
    for understanding rare but important intents (such as users reporting fraud).'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*扩展*—当没有足够的数据来优化你的分类器精度时，使用LLM来填补现有数据的空白。这对于理解罕见但重要的意图（例如用户报告欺诈）特别有用。'
- en: '*Robust testing*—LLMs can generate additional data for testing, increasing
    your confidence in the robustness of the conversational AI. (This is helpful even
    if generative AI creates the answers, as in RAG.)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*鲁棒测试*—LLMs可以生成额外的测试数据，增加你对对话式AI鲁棒性的信心。（即使生成式AI创建答案，如RAG，这也很有帮助。）'
- en: An LLM can help you run many experiments, some of which will generate output
    that is directly usable by your application, either as training or testing data.
    You and the LLM can help each other too. For instance, the LLM can give you themes
    and variations that your users may use. You can select your favorites and ask
    the LLM to expand on those by updating the prompt instructions or few-shot examples.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一个LLM可以帮助你运行许多实验，其中一些将生成可以直接由你的应用程序使用的输出，无论是作为训练数据还是测试数据。你和LLM也可以互相帮助。例如，LLM可以提供用户可能使用的主题和变体。你可以选择你最喜欢的，并要求LLM通过更新提示指令或少量示例来扩展这些内容。
- en: Your interactions with the LLM will be iterative and collaborative. For instance,
    you are unlikely to design the right prompt the first time. The LLM may not understand
    the task correctly or may give you content that’s not quite helpful. Expect to
    do a few rounds of experimentation before you get great results. After that, you
    can quickly generate suggestions across all your intents and improve your AI’s
    understanding of your users.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你与LLM的互动将是迭代和协作的。例如，你不太可能第一次就设计出正确的提示。LLM可能无法正确理解任务，或者可能给你提供不太有帮助的内容。在获得理想结果之前，你可能会进行几轮实验。之后，你可以快速生成所有意图的建议，并提高你AI对用户理解的能力。
- en: 7.1.2 What you need
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.2 你需要什么
- en: Many LLMs can help us with our task of generating more training or testing data
    for our “forgot password” intent. So do we just pick one and turn it loose? Not
    quite. The LLM will help you, but you should not expect it to do all the work.
    Instead, you should have an idea of where you need to start, such as knowing what
    gaps you have in your solution. You also need to select an LLM that is appropriate
    for your use case.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 许多LLM可以帮助我们生成更多用于“忘记密码”意图的训练或测试数据。那么我们是不是只需要挑选一个并放手？并非如此。LLM会帮助你，但你不应期望它完成所有工作。相反，你应该有一个起点，比如知道你解决方案中的哪些差距。你还需要选择一个适合你用例的LLM。
- en: 'Access to an LLM is an obvious prerequisite for using an LLM to augment your
    conversational AI. There are several non-obvious considerations when selecting
    that LLM:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LLM来增强你的对话AI的一个明显前提是能够访问LLM。在选择LLM时，还有一些不那么明显的考虑因素：
- en: '*Terms and conditions*—Several LLMs explicitly forbid you from using their
    LLM to “build or improve another model.” This clause is intended to keep you from
    building a competitive LLM, but using an LLM to improve conversational AI could
    be construed in this way, and your appetite for legal risk may vary. (Consult
    with your legal department—they may have already selected LLMs for your company
    to use.)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*条款和条件*——一些LLM明确禁止你使用它们的LLM来“构建或改进另一个模型”。这个条款的目的是防止你构建一个具有竞争力的LLM，但使用LLM来改进对话AI可能会被这样解读，而你愿意承担的法律风险可能各不相同。（请咨询你的法律部门——他们可能已经为你的公司选择了可用的LLM。）'
- en: '*Data privacy*—As you use the LLM, will it be allowed to keep your data and
    train on it in the future? The data in your conversational AI may be confidential
    to your corporation. If it is, you can’t just share it with any LLM.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据隐私*——当你使用LLM时，它是否允许在未来保留你的数据并在其上进行训练？你对话AI中的数据可能对你公司来说是机密的。如果是这样，你不能随意将其与任何LLM共享。'
- en: '*Capability*—Not every LLM is capable of creative generation tasks. Make sure
    you select a model that can follow instructions.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*能力*——并非每个LLM都具备创意生成任务的能力。确保你选择一个能够遵循指令的模型。'
- en: '*Open source or proprietary*—For many use cases, explainability is important.
    Open source models generally give you more insight into the model’s training process,
    such as the training data and source code for the model. Proprietary models generally
    do not expose that information but usually have more ease-of-use. This may also
    affect your ethical and regulatory compliance.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*开源或专有*——对于许多用例，可解释性很重要。开源模型通常让你对模型的训练过程有更多的了解，例如模型的训练数据和源代码。专有模型通常不会公开这些信息，但通常使用起来更方便。这也可能影响你的道德和监管合规性。'
- en: '*Latency and response times*—There are speed and accuracy tradeoffs; a larger
    model may be both more accurate and slower to run.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*延迟和响应时间*——存在速度和准确性的权衡；较大的模型可能既更准确又运行得更慢。'
- en: In this chapter and the rest of the book, we will use multiple prompts and models
    to provide examples. These examples will be adaptable to your model (or models)
    of choice. Feel free to experiment with other models, especially those that weren’t
    available as we wrote this book.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章和本书的其余部分，我们将使用多个提示和模型来提供示例。这些示例将适用于你选择的模型（或模型）。请随意尝试其他模型，特别是那些在我们撰写本书时不可用的模型。
- en: You will also need to bring some domain knowledge to the LLM. This can include
    background on the problems your users are bringing to your conversational AI,
    the intents your system needs to support, and utterances belonging to those intents.
    Bring as much real-world data as you can. Then use the LLM to augment that data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要将一些领域知识带到LLM（大型语言模型）中。这可以包括你用户向你的对话AI提出的问题的背景，你系统需要支持的目的，以及属于这些目的的表述。尽可能多地带来现实世界的数据。然后使用LLM来增强这些数据。
- en: 7.1.3 How to use the augmented data
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.3 如何使用增强数据
- en: An LLM can help you generate additional data for use by your chatbot. Using
    an LLM at build time reduces the risk and effect of hallucinations. These options
    include adding to your training data, adding to your testing data, and modifying
    your existing data (for example, changing grammatical structure and synonyms).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一个大型语言模型（LLM）可以帮助你为你的聊天机器人生成额外的数据。在构建时使用LLM可以减少幻觉的风险和影响。这些选项包括向你的训练数据添加内容、向你的测试数据添加内容以及修改你现有的数据（例如，改变语法结构和同义词）。
- en: The best source is data is from real users of a production system. We recognize
    that this introduces a chicken-and-egg problem—you may not have any data if you
    are not in production yet. Your intent classifier needs to be trained on varied
    data so that it can understand varied data. The chatbot should be tested on data
    that it was not trained on.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳数据来源是来自生产系统真实用户的数据。我们认识到这引入了一个鸡生蛋的问题——如果你还没有投入生产，你可能没有任何数据。你的意图分类器需要用多样化的数据进行训练，以便它能理解多样化的数据。聊天机器人应该用它未训练过的数据进行测试。
- en: When you don’t have any training data, you’ll tend to generate low-variance
    utterances. You’ll have a couple of key words in mind, and you’ll “anchor” yourself
    to them. Even with dozens of examples, low-variance utterances don’t convey much
    information. High-variation utterances cover a lot of ground quickly, as shown
    in table 7.2, and they generally make your classifier stronger.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当你没有任何训练数据时，你倾向于生成低变异性语句。你会在心中记住几个关键词，并“锚定”于它们。即使有几十个示例，低变异性语句也不传达很多信息。高变异性语句快速覆盖了大量内容，如表7.2所示，并且它们通常会使你的分类器更强。
- en: Table 7.2 Comparing low-variation to high-variation utterances. High-variation
    utterances increase the robustness of classifiers.
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表7.2比较低变异性与高变异性语句。高变异性语句增加了分类器的鲁棒性。
- en: '| Low-variation utterance set | High-variation utterance set |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 低变异性语句集 | 高变异性语句集 |'
- en: '| --- | --- |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| • I forgot my password • Forgot my password'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '| • 我忘记了密码 • 忘记密码'
- en: • Forgot password
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: • 忘记密码
- en: • Help I forgot my password
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: • 帮助，我忘记了密码
- en: '| • I can’t log in • Account locked out'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '| • 我无法登录 • 账户被锁定'
- en: • Forgot password
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: • 忘记密码
- en: '|'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: The high-variation utterances easily cover the low-variation utterances despite
    being fewer in number. The single utterance “forgot password” is enough to predict
    the intent of all four low-variance utterances. The reverse is not true and wouldn’t
    be true even if we added dozens more slight variations on “forgot password” to
    the low-variance set. “I can’t log in” has no direct word overlap—the low-variance
    set doesn’t cover it.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管数量较少，高变异性语句仍然可以覆盖低变异性语句。单个语句“忘记密码”就足以预测所有四个低变异性语句的意图。反之则不成立，即使我们在低变异性集中添加几十个“忘记密码”的微小变化，情况也不会改变。“我无法登录”没有直接词汇重叠——低变异性集不包含它。
- en: We prefer a small, high-variance training data set that covers a large volume
    of low-variance test utterances. Better to train on 10 strong variations than
    100 weak variations. This makes the chatbot robust to the diverse utterances it
    will see in production. It also reduces your chances of accidentally unbalancing
    the training set (which leads to weak understanding).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们更喜欢一个小而高变异性训练数据集，它能覆盖大量低变异性测试语句。与其训练100个弱变异性，不如训练10个强变异性。这使得聊天机器人对生产中将要看到的多样化语句具有鲁棒性。这也减少了你意外不平衡训练集（导致理解薄弱）的机会。
- en: We can visualize the information conveyed by the utterances in figure 7.1\.
    The first plot shows the low-variation utterances from table 7.2\. Since they
    only convey two words, you can think that the utterances are tightly clustered
    together. The second plot shows the high-variation utterances. With no word overlap,
    the utterances are spread all over the grid, but there is a lot of empty space.
    The third plot shows an ideal test set where there is broad coverage of the grid.
    The fourth plot shows an ideal training set, which covers maximum variation in
    a small number of examples. The test data set can be much larger than the training
    set. We want the test set to have variation, but it’s fine if we have near-duplicates.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在图7.1中可视化语句传达的信息。第一个图显示了表7.2中的低变异性语句。由于它们只传达两个单词，你可以认为这些语句紧密聚集在一起。第二个图显示了高变异性语句。由于没有词汇重叠，语句散布在整个网格上，但有很多空隙。第三个图显示了理想的测试集，其中网格覆盖范围广泛。第四个图显示了理想的训练集，在少量示例中覆盖最大变异性。测试数据集可以比训练数据集大得多。我们希望测试集具有变异性，但如果我们有近似重复的数据集也行。
- en: In this chapter, we’ll first demonstrate how to use generative AI to create
    high-variance utterances. Then we’ll expand on those utterances via many slight
    variations. By the end of the chapter, you’ll see how to generate utterances matching
    the third and fourth plots.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先展示如何使用生成式AI来创建高变异性的话语。然后，我们将通过许多细微的变化来扩展这些话语。到本章结束时，你将看到如何生成与第三和第四个图表匹配的话语。
- en: '![figure](../Images/CH07_F01_Freed2.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH07_F01_Freed2.png)'
- en: Figure 7.1 Visualizing coverage from different kinds of utterance sets. Our
    ideal training data is set 4, which covers a large variation in a small number
    of utterances. Set 3 is the ideal testing data.
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.1 从不同种类的话语集中可视化覆盖范围。我们的理想训练数据集是第4集，它覆盖了少量话语中的大量变化。第3集是理想的测试数据。
- en: Exercises
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习
- en: Imagine you are building a chatbot for a typical retail store. Create ten utterances
    for a `#store_location` intent, for when users ask questions like “Where is your
    store located?” Keep track of how much time this takes.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 想象你正在为一家典型的零售店构建聊天机器人。为`#store_location`意图创建十个话语，当用户询问“你的商店在哪里？”时使用。记录这需要多少时间。
- en: Create ten more utterances, without using the words “where,” “store,” “located,”
    or “location.” (Time yourself again.) Do these utterances have more variety?
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建十个更多的话语，不使用“where”、“store”、“located”或“location”这些词。（再次计时。）这些话语是否更有变化？
- en: Repeat the previous two exercises for a `#store_hours` intent. First use whatever
    words you want, and then restrict yourself from using “when,” “time,” and “hours.”
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为`#store_hours`意图重复前两个练习。首先使用你想要的任何词语，然后限制自己不要使用“when”、“time”和“hours”。
- en: 7.2 Hardening your existing intents
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 增强现有意图的强度
- en: 'We’ll start our exercise knowing which intent we need to improve: the “forgot
    password” intent. We need enough training data so that the conversational AI can
    detect this intent. Remember that our support staff didn’t know all the ways users
    might state this problem. They said, “Users usually say something about ‘forgot
    password’ or ‘cannot login.’” That won’t be enough to train the chatbot on a robust
    “forgot password” intent.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始我们的练习，知道我们需要改进哪个意图：“忘记密码”意图。我们需要足够的训练数据，以便对话式AI能够检测到这个意图。记住，我们的支持人员不知道用户可能表达这个问题的所有方式。他们说：“用户通常会提到‘忘记密码’或‘无法登录’。”这不足以训练聊天机器人以稳健的“忘记密码”意图。
- en: We will use the LLM as our partner. First, the LLM will help us generate contextual
    synonyms so that we see a broad range of vocabulary. Next, the LLM will generate
    full utterances using this vocabulary. Then we will have the LLM generate different
    grammatical variations, such as questions versus statements and past tense versus
    present tense. We’ll also have the LLM transfer lessons learned from building
    one intent (“forgot password”) into building the next intent (“find a store”).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用LLM作为我们的合作伙伴。首先，LLM将帮助我们生成上下文同义词，以便我们看到广泛的词汇。接下来，LLM将使用这些词汇生成完整的话语。然后，我们将让LLM生成不同的语法变体，例如疑问句与陈述句以及过去时与现在时。我们还将让LLM将构建一个意图（“忘记密码”）时学到的经验教训应用到构建下一个意图（“寻找商店”）中。
- en: We’ll start with the simplest step—finding synonyms.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从最简单的步骤开始——寻找同义词。
- en: Can I use a different LLM?
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 我可以使用不同的LLM吗？
- en: Yes! We will use multiple models in this book. The field of generative AI is
    moving quickly, and models used during the writing of this book may be supplanted
    by better models by the time this book is published or by the time you read it.
    The principles we’ll demonstrate are more important than the specific models we’ll
    use.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 是的！在这本书中，我们将使用多个模型。生成式AI领域正在快速发展，本书写作期间使用的模型可能在本书出版或你阅读它的时候被更好的模型所取代。我们将展示的原则比我们将使用的具体模型更重要。
- en: 7.2.1 Get creative with synonyms
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 用同义词发挥创意
- en: The first step is designing a prompt. A good prompt ensures your LLM understands
    its task, and the first task in this example is generating a broad range of synonyms.
    The process of prompt engineering requires an iterative process of experimentation.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是设计提示。一个好的提示确保你的LLM理解其任务，在这个例子中，第一个任务是生成广泛的同义词。提示工程的过程需要实验的迭代过程。
- en: Our subject matter experts advised that the utterances often include “forgot
    password.” One way of increasing your chatbot’s robustness is to make sure you
    have coverage on noun and verb phrases. Let’s ask an LLM to generate some likely
    synonyms for the noun phrases.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主题专家建议话语中经常包括“忘记密码”。提高你的聊天机器人鲁棒性的方法之一是确保你覆盖了名词短语和动词短语。让我们让一个LLM生成一些可能的名词短语同义词。
- en: How do I set up and run my LLM?
  id: totrans-78
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 我该如何设置和运行我的 LLM？
- en: There are multiple options for setting up an LLM environment. You can run LLMs
    locally on your machine, using a tool like Ollama, or run them on a commercially
    hosted platform. We used the Prompt Lab available in IBM’s watsonx.ai platform
    due to our familiarity with it, but nothing in this chapter is platform dependent.
    Use your favorite platform.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 设置 LLM 环境有多种选择。您可以在自己的机器上本地运行 LLM，使用像 Ollama 这样的工具，或者在一个商业托管平台上运行它们。由于我们熟悉它，我们使用了
    IBM 的 watsonx.ai 平台上的 Prompt Lab，但本章中没有任何内容是平台依赖的。使用您喜欢的平台。
- en: For this exercise, we will use the falcon-40b-8lang-instruct model ([https://huggingface.co/tiiuae/falcon-40b](https://huggingface.co/tiiuae/falcon-40b))
    with greedy decoding. Greedy decoding instructs the model to generate the next
    most probable word at each step and yields the same output every time.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用 falcon-40b-8lang-instruct 模型 ([https://huggingface.co/tiiuae/falcon-40b](https://huggingface.co/tiiuae/falcon-40b))
    并使用贪婪解码。贪婪解码指示模型在每一步生成下一个最可能的词，并且每次都产生相同的输出。
- en: Though we want to end up with full sentence fragments, we got better results
    by breaking the task down into pieces. User utterances are typically sentences
    or fragments, primarily built from nouns and verb phrases. Let’s start with a
    simple prompt—just asking for synonyms—to get our nouns.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们希望最终得到完整的句子片段，但通过将任务分解成几个部分，我们得到了更好的结果。用户陈述通常是句子或片段，主要由名词和动词短语组成。让我们从一个简单的提示开始——只是要求同义词——以获取我们的名词。
- en: Listing 7.1 Generating noun synonyms without context
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.1 基于上下文生成名词的同义词
- en: '[PRE0]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 Simple instruction'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 简单指令'
- en: '#2 Prompted cue for the LLM'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 对 LLM 的提示性提示'
- en: '#3 LLM output'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 LLM 输出'
- en: In the preceding listing, we used the LLM as a generic thesaurus. While the
    output nouns are synonyms for “password,” they are not synonyms that are often
    used in the context of logging in. We need to provide more context to the LLM
    to get better results.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的列表中，我们将 LLM 作为通用同义词典使用。虽然输出名词是“密码”的同义词，但它们不是在登录上下文中经常使用的同义词。我们需要向 LLM 提供更多上下文以获得更好的结果。
- en: This time let’s tell the LLM why we are asking for synonyms and what kind of
    synonyms we are looking for. The following listing shows the improved prompt and
    results.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这次让我们告诉 LLM 我们为什么要求同义词以及我们想要哪种同义词。以下列表显示了改进的提示和结果。
- en: Listing 7.2 Generating noun synonyms with context
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.2 基于上下文生成名词的同义词
- en: '[PRE1]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 Background information given to the LLM'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 给 LLM 的背景信息'
- en: '#2 Detailed instructions grounding the task to forgetting passwords'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 将任务与忘记密码联系起来的详细说明'
- en: '#3 Cue for LLM'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 LLM 提示'
- en: '#4 LLM response'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 LLM 回复'
- en: These synonyms sound much more familiar. Next, let’s create contextual verb
    synonyms.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这些同义词听起来更熟悉。接下来，让我们创建上下文动词同义词。
- en: Listing 7.3 Generating verb synonyms with context
  id: totrans-96
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.3 基于上下文生成动词的同义词
- en: '[PRE2]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#1 Updated instruction for verbs instead of nouns'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 更新动词而非名词的指令'
- en: '#2 Updated cue for verbs instead of nouns'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 更新动词而非名词的提示'
- en: '#3 LLM output'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 LLM 输出'
- en: Not bad, though the last verb is a little weird in this context. Another limitation
    is that all the generated verbs are in the past tense. This is appropriate given
    that our example was also in the past tense, but we want our LLM to generate more
    variety for us. Let’s try expanding from *verbs* to *verb phrases*.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然最后一个动词在这个上下文中有点奇怪，但结果还不错。另一个限制是所有生成的动词都是过去时。鉴于我们的例子也是过去时，这是合适的，但我们希望我们的 LLM
    为我们生成更多样化的内容。让我们尝试从 *动词* 扩展到 *动词短语*。
- en: Listing 7.4 Generating verb phrase synonyms with context
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.4 基于上下文生成动词短语的同义词
- en: '[PRE3]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '#1 Updated instruction and cue for “verb phrases”'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 更新“动词短语”的指令和提示'
- en: '#2 LLM output'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 LLM 输出'
- en: '#3 LLM output'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 LLM 输出'
- en: The LLM gave us full sentences (oops!), but now we are getting some present
    tense utterances (“cannot remember”) along with the other past tense utterances.
    We are making progress! With only a few minutes of prompting, we got the LLM to
    give us a lot of variation to think about. Before, we might have assumed our chatbot
    would just have to look for “forgot” and “password.” Now we have a dozen other
    useful words to consider when we test the bot.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 给我们提供了完整的句子（哎呀！），但现在我们得到了一些现在时的陈述（“不能记得”）以及其他过去时的陈述。我们在进步！仅仅几分钟的提示，我们就让
    LLM 给我们提供了很多值得思考的变体。以前，我们可能认为我们的聊天机器人只需查找“忘记”和“密码”。现在我们有了一打其他有用的词可以考虑，当我们测试机器人时。
- en: Let’s experiment a little more. This time we’ll increase the creativity of the
    model by moving to *sampling decoding* and increasing the temperature. We’ll also
    revise the prompt by asking for “10 synonyms” instead of “5 nouns.” Listing 7.5
    shows the nouns, and listing 7.6 shows the verb phrases.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再实验一下。这次我们将通过转向*采样解码*和增加温度来提高模型的创造性。我们还将通过要求“10个同义词”而不是“5个名词”来修改提示。列表7.5显示了名词，列表7.6显示了动词短语。
- en: NOTE  With greedy decoding, the LLM generates the same results every time. Sampling
    decoding generates non-deterministic output. If you try these prompts, you’ll
    probably get different results. This is okay! We are only using the LLM to spark
    our creativity.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: NOTE　使用贪婪解码时，LLM每次都会生成相同的结果。采样解码生成非确定性输出。如果你尝试这些提示，你可能会得到不同的结果。这是可以的！我们只是在用LLM激发我们的创造力。
- en: Listing 7.5 Generating noun synonyms with increased creativity settings
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.5 使用增加创造性的设置生成名词的同义词
- en: '[PRE4]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#1 Updated prompt and cue'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 更新后的提示和引导'
- en: '#2 LLM output'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 LLM输出'
- en: Awesome! This is a great list of nouns. Your system may not use all of them,
    but this is a thorough list for testing.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！这是一个很棒的名词列表。您的系统可能不会使用所有这些，但这是一个全面的测试列表。
- en: Let’s try verbs next.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们接下来尝试动词。
- en: Listing 7.6 Generating verb phrase synonyms with increased creativity settings
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.6 使用增加创造性的设置生成动词短语的同义词
- en: '[PRE5]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#1 Updated instruction and cue'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 更新后的指令和引导'
- en: '#2 LLM output'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 LLM输出'
- en: 'This is a much more creative list of synonyms. While there are some oddities
    in this list (“unknown,” “not applicable”) there are some nice creative sparks:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个更具创造性的同义词列表。虽然这个列表中有些奇怪之处（“未知”，“不适用”），但也有一些不错的创意火花：
- en: '*Slightly wrong verb*—“Did not remember” is odd, but it makes you think of
    “Cannot remember.”'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*稍微错误的动词*——“没有记住”很奇怪，但它让你想到“不能记住”。'
- en: '*Wrong tense*—“Didn’t know” makes you consider “Do not know.”'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*错误的时态*——“不知道”让你考虑“不知道”。'
- en: '*Sentiment*—“OMG” reminds you that utterances may include frustration.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*情感*——“OMG”让你意识到表达可能包括挫败感。'
- en: We’ve generated synonyms relevant to our domain with only a few minutes of effort,
    but we still only have piece-parts. We started with the utterance “I forgot my
    password” and can now plug in new nouns and verb phrases, but we are still stuck
    with a simple subject-verb-object structure. Our users will surely use more varied
    grammar. We don’t want the chatbot to depend on only one grammatical form. We
    want it to be resilient to more varied utterances. Let’s use LLMs to generate
    more grammatical variations.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仅用几分钟的努力就生成了与我们的领域相关的同义词，但我们仍然只有部分内容。我们从一个表达“我忘记了我的密码”开始，现在可以插入新的名词和动词短语，但我们仍然被困在简单的主语-谓语-宾语结构中。我们的用户肯定会使用更多样化的语法。我们不希望聊天机器人只依赖于一种语法形式。我们希望它能够对更多样化的表达有弹性。让我们使用LLM来生成更多的语法变化。
- en: 7.2.2 Generate new grammatical variations
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.2 生成新的语法变化
- en: Instead of asking the LLM to generate words or word pairs, let’s try having
    it generate entire utterances. We need to design a prompt that introduces word
    variation, but we don’t want to bias the model too hard toward “I forgot my password.”
    We will use a similar prompt that sets a context, but rather than directly including
    the phrase “I forgot my password,” we will describe the user’s problem instead
    (they can’t log in).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是要求LLM生成单词或单词对，让我们尝试让它生成完整的表达。我们需要设计一个提示来引入单词变化，但我们不想让模型过于偏向“我忘记了我的密码”。我们将使用一个类似的提示来设定上下文，但不是直接包含短语“我忘记了我的密码”，而是描述用户的问题（他们无法登录）。
- en: Our first attempt is shown in the following listing.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们第一次尝试的示例如下所示。
- en: Listing 7.7 Generating entire utterances
  id: totrans-128
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.7 生成完整的表达
- en: '[PRE6]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 Updated instruction and cue'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 更新后的指令和引导'
- en: '#2 LLM output'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 LLM输出'
- en: 'We are starting to get more variety. This list of utterances has several ideas
    we haven’t seen yet (like “password reset email”). Even better, we are getting
    more variety in the sentence structure. The output list has the following grammatical
    varieties:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始获得更多样化了。这个表达列表有几个我们还没有看到的想法（比如“密码重置电子邮件”）。更好的是，我们在句子结构上得到了更多的变化。输出列表有以下语法变化：
- en: '*Subject verb object statement (active voice)*—I forgot my password.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*主语-谓语-宾语陈述（主动语态）*——我忘记了我的密码。'
- en: '*Passive voice statement*—My account is locked.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*被动语态陈述*——我的账户被锁定。'
- en: '*Prepositional*—I need help with my account.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*介词短语*——我需要帮助我的账户。'
- en: '*Question*—Can you help me log in?'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*问题*——你能帮我登录吗？'
- en: '*Qualifiers*—I tried resetting my password but it didn’t work.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*限定词*——我尝试重置密码但不起作用。'
- en: The variety in our sample utterances is improving. The generated utterances
    are usable for our training or test sets, but there are still gaps. For instance,
    all these utterances are perfect sentences. What about our users who are so busy
    (or frustrated) that they only give us a few words? Can the LLM generate useful
    sentence fragments? The following listing explores this idea.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们样本话语的多样性正在提高。生成的话语可用于我们的训练或测试集，但仍存在差距。例如，所有这些话语都是完美的句子。那么，对于那些如此忙碌（或沮丧）以至于只给我们几个词的用户呢？LLM
    能生成有用的句子片段吗？以下列表探讨了这一想法。
- en: Listing 7.8 Generating sentence fragment utterances
  id: totrans-139
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.8 生成句子片段话语
- en: '[PRE7]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '#1 Updated instruction to generate fragments. Cue is unchanged.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 更新指令以生成片段。提示不变。'
- en: '#2 One-shot example below the cue'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 在提示下方的一个单样本示例'
- en: '#3 LLM output'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 LLM 输出'
- en: In our previous prompts, we were not able to generate sentence fragments. This
    time, we gave the LLM extra help. Aside from our usual updates to the prompt (replacing
    “utterances” with “fragments”), we gave the LLM one additional hint. We provided
    the first example fragment “Forgot password.” This is called one-shot learning
    because we gave the LLM one example of what we wanted, and that helped the LLM
    learn how to process our request.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的提示中，我们无法生成句子片段。这次，我们给了 LLM 额外的帮助。除了我们对提示的常规更新（将“话语”替换为“片段”）之外，我们还给了 LLM
    一个额外的提示。我们提供了第一个示例片段“忘记密码。”这被称为单样本学习，因为我们给了 LLM 我们想要的例子，这有助于 LLM 学习如何处理我们的请求。
- en: Zero-shot? One-shot? Few-shot?
  id: totrans-145
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 零样本？单样本？少样本？
- en: The “zero-shot,” “one-shot,” and “few-shot” terms refer to the number of examples
    (shots) given in the prompt. A zero-shot prompt does not give any examples. A
    one-shot prompt gives one example, and a few-shot prompt gives a few examples.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: “零样本”、“单样本”和“少样本”这些术语指的是提示中给出的示例数量（样本）。零样本提示不提供任何示例。单样本提示提供一个示例，而少样本提示提供几个示例。
- en: For training data generation, one-shot learning is a great way to get exactly
    the kind of output you want. Whenever you are having trouble getting an LLM to
    follow your instructions, consider giving a good example rather than just tweaking
    the instructions. While writing this chapter, we tried several more prompts than
    are included in this book, and none of them gave us sentence fragments until we
    used one-shot learning.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练数据生成，单样本学习是一种获取所需输出的绝佳方式。每当你在让 LLM 遵循你的指令时遇到困难，考虑提供一个好的例子，而不仅仅是微调指令。在撰写本章时，我们尝试了比书中包含的更多的提示，直到我们使用单样本学习，我们都没有得到句子片段。
- en: Further, you can use one-shot learning to take the lessons learned while building
    one intent and apply them to another intent. In the next listing, we use examples
    from a “store location” intent to generate examples for a “password reset” intent.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你可以使用单样本学习将构建一个意图时学到的经验应用到另一个意图上。在下一个列表中，我们使用“商店位置”意图的示例来生成“密码重置”意图的示例。
- en: Listing 7.9 Using one-shot learning for multiple grammatical structures
  id: totrans-149
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.9 使用单样本学习生成多种语法结构的示例
- en: '[PRE8]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '#1 Standard background for LLM, unchanged from the past several examples'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 LLM 的标准背景，与过去几个示例保持不变'
- en: '#2 One-shot example includes instruction and desired output'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 单样本示例包括指令和期望的输出'
- en: '#3 Instruction to the LLM, supplemented by the cue “Direct Question:”'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 对 LLM 的指令，补充了提示“直接问题：”'
- en: '#4 LLM output (starts after the cue “Direct Question:”)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 LLM 输出（在提示“直接问题：”之后开始）'
- en: With a single prompt, we were able to get examples of each of the grammatical
    structures we wanted (the LLM made a mistake on “direct question,” but the output
    is still useful).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一个提示，我们能够得到我们想要的每种语法结构的示例（LLM 在“直接问题”上犯了一个错误，但输出仍然有用）。
- en: Here’s one more trick for generating utterances. Rather than using detailed
    instructions, provide a few examples, and ask the LLM to generate more. We’ll
    use a different prompt format and a different model—granite-13b-instruct-v2 ([https://mng.bz/DMlR](https://mng.bz/DMlR))—and
    we’ll use sampling decoding for increased creativity and non-deterministic results.
    The following listing shows the prompt and first output.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种生成话语的额外技巧。而不是使用详细的指令，提供几个示例，并要求 LLM 生成更多。我们将使用不同的提示格式和不同的模型——granite-13b-instruct-v2
    ([https://mng.bz/DMlR](https://mng.bz/DMlR))——我们将使用采样解码以增加创造性和非确定性结果。以下列表显示了提示和第一个输出。
- en: Listing 7.10 Using a creative prompt to generate examples with a Granite model
  id: totrans-157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.10 使用创意提示生成 Granite 模型的示例
- en: '[PRE9]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 A marker indicating our instruction to the model'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 指示模型指令的标记'
- en: '#2 The actual instruction'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 实际指令'
- en: '#3 Beginning of examples'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 示例的开始'
- en: '#4 Output cue'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 输出提示'
- en: '#5 LLM output'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 LLM输出'
- en: 'Because we are using non-deterministic settings, the model output is different
    every time. Here are the outputs from the next five executions of the same prompt:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的是非确定性设置，模型输出每次都不同。以下是下一个五个执行相同提示的输出：
- en: “Can you help me recover my password”
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “你能帮我恢复我的密码吗”
- en: “I’m locked out of my account”
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “我被锁在我的账户外面”
- en: “Can’t remember username or password”
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “我记不起用户名或密码”
- en: “Hoping you can help me, I just reset my password but it’s not working”
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “希望你能帮我，我刚刚重置了我的密码但不起作用”
- en: “I’ve failed logging in 5 times in a row”
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “我已经连续5次登录失败”
- en: We didn’t specify exactly what variation we wanted, but we still got some interesting
    variations. We saw new verbs (“recover”) and concepts (“5 times in a row”). This
    highlights the value of experimenting with different LLMs, different prompts,
    and different parameter settings. Generating training data requires creativity.
    Don’t rely on one or two experiments to do the work—you and generative AI can
    work together to be creative.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有指定我们想要的精确变化，但我们仍然得到了一些有趣的变化。我们看到了新的动词（“恢复”）和概念（“连续5次”）。这突出了实验不同LLM、不同提示和不同参数设置的价值。生成训练数据需要创造力。不要依赖一个或两个实验来完成工作——你可以和生成式AI一起创造性地工作。
- en: 7.2.3 Build strong intents from LLM output
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.3 从LLM输出构建强大的意图
- en: Let’s recap our experiments so far. We’ve generated nouns and verbs as in-context
    synonyms (not just generic synonyms). We’ve generated entire utterances with a
    similar structure, then used LLMs to generate utterances with varied grammatical
    structures. We’ve used multiple models, prompts, and parameter settings. Generative
    AI has been a great partner!
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下迄今为止的实验。我们已生成作为上下文同义词的名词和动词（而不仅仅是通用同义词）。我们已生成具有相似结构的整个话语，然后使用LLM生成具有不同语法结构的话语。我们使用了多个模型、提示和参数设置。生成式AI是一个伟大的合作伙伴！
- en: From these experiments, we have a lot of possibilities for building a training
    set. Let’s select 10 utterances covering the variations we generated earlier.
    For some of the utterances, we’ll use the verbatim output from the LLMs. For other
    utterances, we’ll substitute some variations. For instance, the generated utterances
    were heavy on “password”—we can substitute “login information” or “account information.”
    The utterances were also heavy on “forgot,” so we’ll substitute “can’t remember.”
    The following listing shows one possible selection of utterances.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些实验中，我们有很多可能性来构建一个训练集。让我们选择10个话语，涵盖我们之前生成的变化。对于一些话语，我们将使用LLM的逐字输出。对于其他话语，我们将替换一些变化。例如，生成的话语中“密码”很多——我们可以用“登录信息”或“账户信息”来替换。话语中也很多“忘记”，所以我们将用“记不起”来替换。以下列表显示了话语的一个可能选择。
- en: Listing 7.11 Ten hand-selected utterances based on LLM suggestions
  id: totrans-174
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.11 基于LLM建议的十个精选话语
- en: '[PRE10]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We’ve come a long way since the initial suggestion that “most requests include
    the words ‘forgot’ and ‘password’”! If we use these utterances in our training
    set, we will have much more robust chatbot understanding than if we had stuck
    to our keyword-based advice.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 自从最初建议“大多数请求包括‘忘记’和‘密码’”以来，我们已经走了很长的路！如果我们将这些话语用于我们的训练集，我们将比坚持基于关键词的建议有更稳健的聊天机器人理解。
- en: 'We’ve been successful generating ideas across multiple prompting sessions.
    This begs the question, can we do everything in one prompt? We would expect to
    need all our tricks to date: a context for the LLM, a clear instruction, and a
    one-shot example. Let’s try to train a “store locator” intent using our best examples
    from the “forgot password” intent. The following listing demonstrates this using
    the falcon-40b-8lang-instruct model with greedy decoding again.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在多个提示会话中成功生成想法。这引发了一个问题，我们能否在一个提示中完成所有操作？我们预计需要我们迄今为止的所有技巧：为LLM提供上下文，一个清晰的指令，以及一个一次性示例。让我们尝试使用“忘记密码”意图的最佳示例来训练一个“商店定位器”意图。以下列表展示了使用falcon-40b-8lang-instruct模型和贪婪解码再次进行此操作。
- en: Listing 7.12 Using one-shot learning to copy lessons from one intent to another
  id: totrans-178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.12 使用一次性学习将一个意图的教训复制到另一个意图
- en: '[PRE11]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '#1 Standard background for LLM'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 LLM的标准背景'
- en: '#2 Instruction and cue for one-shot example (password reset)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 一次性示例（密码重置）的指令和提示'
- en: '#3 One-shot example'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 一次性示例'
- en: '#4 Instruction and cue for target (store locator)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 目标（商店定位器）的指令和提示'
- en: '#5 LLM output'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 LLM输出'
- en: '#6 LLM output'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 LLM输出'
- en: 'Not bad! This is a reasonable start for our new intent. There are several positive
    aspects to this output:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 还不错！这是我们新意图的合理起点。这个输出有几个积极方面：
- en: '*Verb variety*—No verb is repeated, aside from “is.”'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*动词多样性*——除了“是”之外，没有重复的动词。'
- en: '*Concept variety*—The examples cover both absolute and relative concepts via
    “location” and “direction.” They also cover time and space (“how long,” “how far”).'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*概念多样性*——通过“位置”和“方向”，示例涵盖了绝对和相对概念。它们还涵盖了时间和空间（“多长时间”，“多远”）。'
- en: '*Granularity variety*—Utterances range from “what city” to “what street” as
    well as “from here.”'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*粒度多样性*——表述范围从“哪个城市”到“哪个街道”以及“从这里”。'
- en: 'However, the utterances also have some limitations:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些表述也有一些局限性：
- en: '*Grammatical structure*—The utterances are all questions. There are no commands
    or fragments.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语法结构*——这些表述都是问题。没有命令或片段。'
- en: '*Noun variety*—Every example uses “store.”'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*名词多样性*——每个例子都使用“store”。'
- en: '*Obvious omissions*—I’m lost without my GPS. It’s surprising the utterances
    didn’t explicitly include something like “What’s your address” or “driving directions.”'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*明显遗漏*——没有GPS我就迷失了。令人惊讶的是，表述没有明确包含像“你的地址是什么”或“驾驶方向”这样的内容。'
- en: The task is too hard to complete in a single prompt. We asked the LLM for everything
    we wanted and even gave examples. The LLM was able to complete many of our requests
    but also ignored or failed to fulfill several of our requests. Things aren’t as
    simple as perfecting one intent and then asking the LLM replicate that to all
    the other intents. There are just too many instructions and variables in our task
    for current LLMs to get everything right in one try. That may well change in the
    future.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这个任务太难，无法在单个提示中完成。我们向LLM提出了我们想要的所有内容，甚至给出了例子。LLM能够完成我们许多请求，但也忽略了或未能完成我们的一些请求。事情并不像完美地完成一个意图然后让LLM复制到所有其他意图那样简单。我们的任务中有太多的指令和变量，对于当前的LLM来说，在一次尝试中就完全正确是非常困难的。这可能会在未来改变。
- en: This is why we suggest using an LLM as a partner rather than doing everything
    by yourself or doing everything with an LLM. You cannot offload your thinking
    onto the LLM, but you *can* have an LLM run experiments for you very quickly.
    Generating synonyms and grammar variety sounds easy, but you probably couldn’t
    do it as quickly and completely as an LLM. Have the LLM generate lots of ideas
    and then pick the best ones.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么我们建议将LLM用作合作伙伴，而不是自己或完全使用LLM来做所有事情。您不能将您的思考外包给LLM，但您*可以*让LLM非常快速地为您运行实验。生成同义词和语法多样性听起来很简单，但您可能无法像LLM那样快速和完整地完成。让LLM生成很多想法，然后挑选最好的。
- en: REMEMBER  The LLM can’t think for you, but it can give you a very good “first
    draft.”
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: REMEMBER  LLM无法为您思考，但它可以为您提供一个非常好的“初稿”。
- en: Table 7.3 summarizes dos and don’ts for using LLMs to generate training and
    test data.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.3总结了使用LLM生成训练和测试数据的“做”和“不要”。
- en: Table 7.3 Dos and don’ts for using LLMs to generate training and testing data
  id: totrans-198
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表7.3 使用LLM生成训练和测试数据的“做”和“不要”
- en: '| Do | Don’t |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 做 | 不要 |'
- en: '| --- | --- |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| • Use the LLM as a partner or creative assistant. You still drive the process.
    • Set contextual guidance and focused instructions'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '| • 将LLM用作合作伙伴或创意助手。您仍然驱动这个过程。 • 设置上下文指导和专注的指令 |'
- en: • Use examples and one-shot learning to nudge the LLM
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: • 使用示例和单次学习来引导LLM
- en: • Experiment with multiple prompts
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: • 尝试使用多个提示
- en: • Use LLM output to augment data collected from users
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: • 使用LLM输出增强从用户收集的数据
- en: '| • Accept LLM output without reviewing or refining it • Expect the LLM to
    know what you want'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '| • 不审查或精炼LLM输出就接受它 • 期望LLM知道您想要什么 |'
- en: • Perform too many tasks in a single prompt
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: • 在单个提示中执行太多任务
- en: • Feed confidential data into a proprietary LLM platform that keeps your data
    “for future training purposes”
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: • 将机密数据输入到保留的LLM平台，该平台“为了未来的训练目的”保留您的数据
- en: • Assume that LLM output is fully representative of user data
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: • 假设LLM输出完全代表了用户数据
- en: '|'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: LLMs are great for generating utterance training data when you have a clear
    problem but no representative user utterances. While we always prefer to use actual
    user utterances from a production system, we don’t always have that luxury. LLM-generated
    data helps us fill in the gaps. Their vast training sets likely include some data
    from your domain (such as customer service), but it may not include all your needs.
    They’ve seen lots of “password reset” utterances but probably none that include
    the name of your application. Given a choice between no training data, fabricated
    training data from subject matter experts, and LLM-generated training data, the
    LLM-generated option is the best bet.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 当你有一个明确的问题但没有代表性的用户表述时，LLM非常适合生成表述训练数据。虽然我们总是更喜欢使用来自生产系统的实际用户表述，但我们并不总是享有这种奢侈。LLM生成的数据帮助我们填补了空白。它们庞大的训练集可能包含一些来自你领域（如客户服务）的数据，但可能不包括你所有的需求。它们看到了很多“密码重置”的表述，但可能没有包含你应用程序的名称。在无训练数据、从主题专家那里编造的训练数据和LLM生成的训练数据之间进行选择时，LLM生成的选项是最好的选择。
- en: 7.2.4 Creating even more examples with templates
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.4 使用模板创建更多示例
- en: In the previous section, we generated a varied list of utterances by combining
    multiple different outputs from the LLM and using them to generate new utterances.
    Figure 7.2 shows an example of mutating full utterances (from listing 7.7) with
    synonyms seen in listings 7.1 to 7.6.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们通过结合LLM的多个不同输出并使用它们来生成新的表述，从而生成了一系列多样化的表述。图7.2展示了从列表7.7中突变完整表述（使用列表7.1到7.6中看到的同义词）的示例。
- en: '![figure](../Images/CH07_F02_Freed2.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH07_F02_Freed2.png)'
- en: Figure 7.2 Generating additional examples from the initial LLM output. The LLM
    generated “My password isn’t working,” but we now know a related utterance is
    “My login information isn’t working.”
  id: totrans-214
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.2 从初始LLM输出生成额外的示例。LLM生成了“我的密码不起作用”，但我们现在知道相关的表述是“我的登录信息不起作用”。
- en: TIP  Creating examples from templates is a programmatic task, not specifically
    a generative AI task. Mixing and matching multiple styles can generate the best
    results.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: TIP  创建模板中的示例是一项程序性任务，而不是特定于生成式AI的任务。混合和匹配多种风格可以产生最佳结果。
- en: Using templates is especially helpful when some of the LLM outputs only contain
    one verb or noun. We were able to introduce variety into our utterance collection
    with manual changes, but we can take this to the extreme by considering the LLM
    outputs as templates. Starting with the basic utterance “I forgot my password,”
    we then explored contextual synonyms for “forgot” and “password.” Figure 7.3 converts
    this utterance into a template of “I <verb phrase> my <noun phrase>,” which can
    generate more utterances.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 使用模板在LLM输出仅包含一个动词或名词时特别有帮助。我们能够通过手动更改引入我们的表述集合的多样性，但我们可以通过将LLM输出视为模板来将这种做法推向极致。从基本的表述“我忘记了密码”开始，我们随后探索了“忘记”和“密码”的上下文同义词。图7.3将这个表述转换成了“我<动词短语>我的<名词短语>”的模板，这可以生成更多的表述。
- en: '![figure](../Images/CH07_F03_Freed2.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH07_F03_Freed2.png)'
- en: Figure 7.3 Converting “I forgot my password” into a template that lets us replace
    verbs and nouns in context. One option from this template is “I lost my credentials.”
  id: totrans-218
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.3 将“我忘记了密码”转换成允许我们在上下文中替换动词和名词的模板。这个模板的一个选项是“我丢失了我的凭证”。
- en: This template generates 36 total utterances due to having six verb choices and
    six noun choices (6 × 6 = 36). This is a lot of data, but it is quite unbalanced—it
    all uses the exact same grammatical structure. Worse, some of the utterances may
    not ever be uttered by users. This approach is not suitable for generating training
    data, since it overweighs the bot toward a single pattern. These templated utterances
    will hide the influence of other more varied utterances like “Can you help me
    log in?” “I tried to reset my password, but it didn’t work,” and “account locked
    out.”
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模板由于有六个动词选择和六个名词选择（6×6=36），总共生成了36个表述。这是一大批数据，但它非常不平衡——它全部使用了完全相同的语法结构。更糟糕的是，一些表述可能永远不会被用户说出。这种方法不适合生成训练数据，因为它过分强调了单一模式。这些模板化的表述将隐藏其他更多样化的表述的影响，如“你能帮我登录吗？”“我尝试重置密码，但不起作用”，以及“账户被锁定”。
- en: The templated utterances are useful for your testing set if you recognize the
    imbalance. There is nothing wrong with testing your conversational AI on all 36
    utterances as a sanity test. Just don’t limit yourself to testing one template
    and assuming the intent is well-trained.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你认识到不平衡，模板语句对你的测试集是有用的。测试你的对话AI的所有36个语句作为理智测试并没有什么不妥。只是不要限制自己只测试一个模板并假设意图已经很好地训练过。
- en: A templated approach can be useful for generating testing data that helps ensure
    the chatbot can tell two intents apart in the face of extraneous information.
    In addition to our “forgot password” template, let’s assume we have a “store location”
    template that uses the verbs “need,” “forgot,” and “want” and the nouns “address,”
    “location,” and “driving directions.” The store location template is like figure
    7.3, but it uses “I <verb phrase> your <noun phrase>.” We’ll also assume that
    some users will greet the bot (“Hi,” “hello,” “good day”) or generically ask for
    help (“can you help,” “please assist”). These generic additions do not add any
    differentiating information to the user utterance. Will they somehow affect the
    chatbot? Figure 7.4 shows how we can set up a test.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 模板方法可以用来生成测试数据，这有助于确保聊天机器人能够在面对无关信息的情况下区分两个意图。除了我们的“忘记密码”模板外，假设我们还有一个“商店位置”模板，它使用动词“需要”、“忘记”和“想要”，以及名词“地址”、“位置”和“驾驶方向”。商店位置模板类似于图7.3，但它使用“I
    <verb phrase> your <noun phrase>。”我们还将假设一些用户会问候机器人（“Hi”，“hello”，“good day”）或泛地问询帮助（“can
    you help”，“please assist”）。这些泛化的添加并没有向用户语句中添加任何区分信息。它们会以某种方式影响聊天机器人吗？图7.4展示了我们如何设置测试。
- en: '![figure](../Images/CH07_F04_Freed2.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH07_F04_Freed2.png)'
- en: Figure 7.4 Using common templates to see whether greetings and closures affect
    the chatbot’s understanding. One possible utterance is “Hello I lost my credentials
    please assist.”
  id: totrans-223
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.4 使用常见模板来查看问候语和结束语是否会影响聊天机器人的理解。一个可能的语句是“Hello I lost my credentials please
    assist。”
- en: There are three verb variations and three noun variations in each intent, giving
    nine (3 × 3 = 9) possibilities for each intent. Without considering greetings,
    we could have run 18 tests (9 per intent). In this test, we have added three greeting
    variations and two closure variations, allowing us to increase the test size six-fold.
    The 108 (18 × 6 = 108) utterances will include “Hi! I forgot my password. Can
    you help?” and “Good day. I need your address. Please assist.” and 106 more variations.
    These can all be included in a test set.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 每个意图中都有三个动词变化和三个名词变化，为每个意图提供了九种可能性（3 × 3 = 9）。不考虑问候语，我们可能已经进行了18次测试（每个意图9次）。在这个测试中，我们增加了三种问候语变化和两种结束语变化，使我们能够将测试规模扩大六倍。108（18
    × 6 = 108）个语句将包括“Hi！我忘记了我的密码。你能帮忙吗？”和“Good day。我需要你的地址。请协助。”以及106种更多变化。这些都可以包含在测试集中。
- en: In theory, there is no difference between theory and practice—in practice there
    is.Yogi Berra
  id: totrans-225
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 理论上，理论与实践之间没有区别——在实践中，有区别。Yogi Berra
- en: We do not expect adding the greetings and closure variations to affect the classification,
    but we can verify this. If your training data is severely unbalanced, the chatbot
    may be affected by these extra words. Therefore, running this kind of test can
    be valuable as another sanity test, in addition to the methods shown in chapter
    5.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不期望添加问候语和结束语变化会影响分类，但我们可以验证这一点。如果你的训练数据严重不平衡，聊天机器人可能会受到这些额外单词的影响。因此，运行这类测试可以作为另一种理智测试，除了第5章中展示的方法。
- en: Exercises
  id: totrans-227
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习
- en: 'Use generative AI to create examples for a “store location” intent. How many
    nouns, verbs, and grammatical structures can you generate? Track the amount of
    time you spend on this exercise:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用生成式AI为“商店位置”意图创建示例。你能生成多少个名词、动词和语法结构？跟踪你在这项练习上花费的时间：
- en: Use a prompt with instructions only. This is a zero-shot prompt.
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用仅包含指令的提示。这是一个零次示例的提示。
- en: Use a prompt that includes examples. This is a one-shot or few-shot prompt.
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用包含示例的提示。这是一个单次或少量示例的提示。
- en: Did the model generate more varied utterances in less time than when you created
    utterances manually?
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型在更短的时间内生成更多样化的语句，比您手动创建语句时要多吗？
- en: Repeat the previous exercise for an intent that is not well understood in a
    chatbot you are building (or using). If possible, augment the bot’s training or
    test data sets with some of these new utterances, and measure the change in accuracy.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为你正在构建的聊天机器人中不太理解的意图重复之前的练习（或使用）。如果可能，通过这些新的语句之一增加机器人的训练或测试数据集，并测量准确性的变化。
- en: 7.3 Getting more creative
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 更加创新
- en: Chapter 5 demonstrated rock-solid data science principles for evaluating and
    improving your training and testing data. Those principles give you metrics that
    demonstrate your chatbot’s ability to understand and quantify the effect of the
    improvements you are trying to make. Those robust principles take time to implement.
    This section will show you a few creative ways to use an LLM before diving deeper
    into statistical approaches. These LLM-based techniques do not replace the statistical
    approaches, but they can give you a quick intuition.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 第5章展示了评估和改进你的训练和测试数据的坚实基础数据科学原则。这些原则为你提供了指标，这些指标证明了你的聊天机器人理解并量化了你试图改进的效果。这些稳健的原则需要时间来实施。本节将展示在深入研究统计方法之前，如何使用LLM的几种创意方法。这些基于LLM的技术并不取代统计方法，但它们可以给你一个快速直观的认识。
- en: 7.3.1 Brainstorm additional intents
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.1 思考更多意图
- en: The LLM can help you brainstorm new intents your system might need to handle.
    While we prefer to work from real-world data, such as a backlog of support tickets,
    a little brainstorming doesn’t hurt. If you are starting a brand-new support process,
    you may not have any data to work with and need a kickstart. The following listing
    demonstrates an intent brainstorming process.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: LLM可以帮助你思考系统可能需要处理的新意图。虽然我们更喜欢从现实世界的数据开始工作，例如支持票务的积压，但一点头脑风暴并不会有害。如果你正在启动全新的支持流程，你可能没有任何数据可以工作，需要启动。以下列表演示了一个意图头脑风暴过程。
- en: Listing 7.13 Brainstorming new intents
  id: totrans-237
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.13 思考新意图
- en: '[PRE12]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '#1 Standard background for LLM—still unchanged'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 LLM的标准背景——仍然没有改变'
- en: '#2 Instruction and cue'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 指令和提示'
- en: '#3 LLM output'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 LLM输出'
- en: This looks like a great start. We have product search, price dispute, order
    tracking, login problems, and returns. These all seem worthy of expanding into
    intents and process flows in your conversational AI.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来是一个很好的开始。我们有产品搜索、价格争议、订单跟踪、登录问题和退货。这些似乎都值得扩展到你的对话AI中的意图和流程中。
- en: 7.3.2 Check for confusion
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.2 检查混淆
- en: We can also see if the LLM agrees with the intents we have created. We can take
    the training utterances we have selected and ask the LLM to sort them into intents.
    Let’s see what happens if we remove the intent name from our “forgot password”
    intent.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以看看LLM是否同意我们创建的意图。我们可以选择我们已选的训练语句，并要求LLM将它们分类到意图中。让我们看看如果我们从“忘记密码”意图中移除意图名称会发生什么。
- en: Listing 7.14 Does the LLM predict the same intent (“forgot password”) as we
    did?
  id: totrans-245
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.14 LLM预测的意图（“忘记密码”）与我们的一致吗？
- en: '[PRE13]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '#1 Instruction for LLM'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 对LLM的指令'
- en: '#2 Input belonging to the instruction'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 指令所属的输入'
- en: '#3 Cue'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 提示'
- en: '#4 LLM output'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 LLM输出'
- en: The LLM suggested two intents where we had only used one. The intents “login”
    and “password reset” are in line with our original “forgot password” label. The
    LLM-derived intents feel too narrow, especially since both intents are likely
    to have the same answer.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: LLM建议了两个意图，而我们只使用了其中一个。意图“登录”和“密码重置”与我们的原始“忘记密码”标签一致。LLM推导出的意图感觉过于狭窄，尤其是由于这两个意图很可能有相同的答案。
- en: LLM output format is not always consistent
  id: totrans-252
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: LLM输出格式并不总是一致的
- en: In listing 7.14, the LLM “invented” an output schema with hyphenated list entry
    and arrows from utterance to intent. Since we are just reviewing the results visually,
    this is okay, but additional instructions to the LLM might help (e.g., “respond
    in a bulleted list”). We could also demonstrate our desired format with a one-shot
    example.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表7.14中，LLM“发明”了一个带有连字符列表条目和从话语到意图的箭头的输出模式。由于我们只是从视觉上审查结果，这可以接受，但给LLM的额外指令可能有所帮助（例如，“以项目符号列表形式回答”）。我们还可以用一个一次性示例来展示我们想要的格式。
- en: This test is not as robust as the other techniques shown in chapter 5, but it
    can be used as a quick sanity test on your training data. If the LLM does not
    find any cohesion in your training data, you might have a problem.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这个测试不如第5章中展示的其他技术稳健，但它可以用作对训练数据的快速合理性测试。如果LLM在你的训练数据中找不到任何连贯性，你可能存在问题。
- en: LLMs and human builders work well together. Figure 7.5 summarizes the many ways
    LLMs can help you change your conversational AI to improve its ability to understand
    your users.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: LLM和人类构建者可以很好地协同工作。图7.5总结了LLM可以帮助你改变你的对话AI以改善其理解用户能力的方式。
- en: '![figure](../Images/CH07_F05_Freed2.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH07_F05_Freed2.png)'
- en: Figure 7.5 An LLM augments the human builder in many ways.
  id: totrans-257
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.5 LLM以多种方式增强人类构建者。
- en: Exercises
  id: totrans-258
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习
- en: Take a chatbot that you are building or using. Describe its purpose. Use that
    description and a creative LLM prompt to generate example problems that the bot
    would solve. Use sampling decoding, and run the prompt multiple times to get multiple
    ideas. Do these line up with the intents or process flows the bot handles?
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以您正在构建或使用的聊天机器人为例。描述其目的。使用该描述和创意LLM提示生成机器人将解决的示例问题。使用采样解码，多次运行提示以获得多个想法。这些想法与机器人处理的意图或流程一致吗？
- en: Take a chatbot you are building. Extract a subset of utterances from the test
    data. Ask an LLM to predict the intent or process flow they belong to. Does the
    LLM prediction align with how your bot is implemented?
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以您正在构建的聊天机器人为例。从测试数据中提取一个子集的语句。要求LLM预测它们所属的意图或流程。LLM的预测与您的机器人实现方式一致吗？
- en: Summary
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: LLMs are great partners that augment human builders. Humans and LLMs are better
    together.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs是出色的合作伙伴，它们增强了人类构建者。人类和LLMs在一起会更好。
- en: Experiment with different models, prompts, and parameters to get the best output
    from LLMs. Keep iterating! Don’t expect your first attempt to be perfect.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试不同的模型、提示和参数以从LLMs中获得最佳输出。持续迭代！不要期望你的第一次尝试就完美无缺。
- en: Don’t just give instructions to an LLM. Provide examples through one-shot or
    few-shot prompts.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要只是向LLM下达指令。通过一次性或少量提示提供示例。
- en: When you identify a gap in your data, you can ask an LLM to help you fill it.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当您在数据中识别到差距时，您可以要求LLM帮助您填补。
- en: LLM output can be used directly in your training data, or you can manually refine
    it first.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM的输出可以直接用于您的训练数据，或者您可以先手动对其进行精炼。
- en: Use greedy decoding to get the same output every time. Use sampling decoding
    to get randomized responses with additional creativity. Execute the same prompt
    multiple times with sampling decoding to get a variety of responses, and use the
    most helpful output.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用贪婪解码以每次获得相同的输出。使用采样解码以获得具有额外创造性的随机响应。
