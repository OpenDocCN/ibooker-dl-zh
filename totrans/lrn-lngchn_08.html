<html><head></head><body><section data-pdf-bookmark="Chapter 8. Patterns to Make the Most of LLMs" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch08_patterns_to_make_the_most_of_llms_1736545674143600">
<h1><span class="label">Chapter 8. </span>Patterns to Make the Most of LLMs</h1>

<p>LLMs today have some major limitations, but that doesn’t mean your dream LLM app is impossible to build. The experience that you design for users of your application needs to work around, and ideally with, the limitations.</p>

<p><a data-type="xref" href="ch05.html#ch05_cognitive_architectures_with_langgraph_1736545670030774">Chapter 5</a> touched on the key trade-off we face when building<a contenteditable="false" data-primary="LLM applications" data-secondary="trade-off between agency and reliability" data-type="indexterm" id="LLMAtrade08"/> LLM apps: the<a contenteditable="false" data-primary="agency-reliability trade-off" data-type="indexterm" id="agreltrade08"/> trade-off between<a contenteditable="false" data-primary="agency" data-type="indexterm" id="id776"/> <em>agency</em> (the LLM’s capacity to act autonomously) and<a contenteditable="false" data-primary="reliability" data-type="indexterm" id="id777"/> <em>reliability</em> (the degree to which we can trust its outputs). Intuitively, any LLM application will be more useful to us if it takes more actions without our involvement, but if we let agency go too far, the application will inevitably do things we wish it hadn’t.</p>

<p><a data-type="xref" href="#ch08_figure_1_1736545674134403">Figure 8-1</a> illustrates this trade-off.</p>

<figure><div class="figure" id="ch08_figure_1_1736545674134403"><img alt="The agency-reliability trade-off" src="assets/lelc_0801.png"/>
<h6><span class="label">Figure 8-1. </span>The agency-reliability trade-off</h6>
</div></figure>

<p>To borrow a concept from other fields,<sup><a data-type="noteref" href="ch08.html#id778" id="id778-marker">1</a></sup> we can visualize the trade-off as a<a contenteditable="false" data-primary="frontiers" data-type="indexterm" id="id779"/> <em>frontier</em>—all points on the frontier’s curved line are optimal LLM architectures for some application, marking different choices between agency and reliability. (Refer to <a data-type="xref" href="ch05.html#ch05_cognitive_architectures_with_langgraph_1736545670030774">Chapter 5</a> for an overview of different LLM application architectures.) As an example, notice how the chain architecture has relatively low agency but higher reliability, whereas the Agent architecture has higher agency at the expense of lower reliability.</p>

<p>Let’s briefly touch on a number of additional (but still important) objectives that you might want your LLM application to have. Each LLM app will be designed for a different mix of one or more of these objectives:</p>

<dl>
	<dt>Latency</dt>
	<dd>
	<p>Minimize<a contenteditable="false" data-primary="latency" data-type="indexterm" id="id780"/> time to get final answer</p>
	</dd>
	<dt>Autonomy</dt>
	<dd>
	<p>Minimize<a contenteditable="false" data-primary="autonomy" data-type="indexterm" id="id781"/> interruptions for human input</p>
	</dd>
	<dt>Variance</dt>
	<dd>
	<p>Minimize<a contenteditable="false" data-primary="variance" data-type="indexterm" id="id782"/> variation between invocations</p>
	</dd>
</dl>

<p>This is not meant as an exhaustive list of all possible objectives, but rather as illustrative of the trade-offs you face when building your application. Each objective is somewhat at odds with all the others (for instance, the easiest path to higher reliability requires either higher latency or lower autonomy). Each objective would nullify the others if given full weight (for instance, the minimal latency app is the one that does nothing at all). <a data-type="xref" href="#ch08_figure_2_1736545674134438">Figure 8-2</a> illustrates this concept.</p>

<figure><div class="figure" id="ch08_figure_2_1736545674134438"><img alt="Shifting the frontier, or more agency for the same reliability" src="assets/lelc_0802.png"/>
<h6><span class="label">Figure 8-2. </span>Shifting the frontier, or more agency, for the same reliability</h6>
</div></figure>

<p>What we really want as application developers is to shift the frontier outward. For the same level of reliability, we’d like to achieve higher agency; and for the same level of agency, we’d like to achieve higher reliability. This chapter covers a number of techniques you can use to achieve this:</p>

<dl>
	<dt>Streaming/intermediate output</dt>
	<dd>
	<p>Higher latency is easier to accept if there is some communication of progress/intermediate output throughout.</p>
	</dd>
	<dt>Structured output</dt>
	<dd>
	<p>Requiring an LLM to produce output in a predefined format makes it more likely that it will conform to expectations.</p>
	</dd>
	<dt>Human in the loop</dt>
	<dd>
	<p>Higher-agency architectures benefit from human intervention while they’re running: interrupting, approving, forking, or undoing.</p>
	</dd>
	<dt>Double texting modes</dt>
	<dd>
	<p>The longer an LLM app takes to answer, the more likely it is that the user might send it new input before the previous one has finished being processed.<a contenteditable="false" data-primary="" data-startref="agreltrade08" data-type="indexterm" id="id783"/><a contenteditable="false" data-primary="" data-startref="LLMAtrade08" data-type="indexterm" id="id784"/></p>
	</dd>
</dl>

<section data-pdf-bookmark="Structured Output" data-type="sect1"><div class="sect1" id="ch08_structured_output_1736545674143795">
<h1>Structured Output</h1>

<p>It<a contenteditable="false" data-primary="structured output" data-secondary="need for" data-type="indexterm" id="id785"/><a contenteditable="false" data-primary="LLM applications" data-secondary="patterns and key concepts for successful" data-type="indexterm" id="LLMApatternss08"/> is often crucial to have LLMs return structured output, either because a downstream use of that output expects a things in a specific <em>schema</em> (a definition of the names and types of the various fields in a piece of structured output) or purely to reduce variance to what would otherwise be completely free-form text output.</p>

<p>There<a contenteditable="false" data-primary="structured output" data-secondary="strategies to achieve" data-type="indexterm" id="id786"/> are a few different strategies you can use for this with different LLMs:</p>

<dl>
	<dt>Prompting</dt>
	<dd>
	<p class="fix_tracking">This<a contenteditable="false" data-primary="prompts" data-secondary="structured output with" data-type="indexterm" id="id787"/> is when you ask the LLM (very nicely) to return output in the desired format (for instance, JSON, XML, or CSV). Prompting’s big advantage is that it works to some extent with any LLM; the downside is that it acts more as a suggestion to the LLM and not as a guarantee that the output will come out in this format.</p>
	</dd>
	<dt>Tool calling</dt>
	<dd>
	<p>This<a contenteditable="false" data-primary="tool-calling technique" data-secondary="structured output with" data-type="indexterm" id="id788"/> is available for LLMs that have been fine-tuned to pick from a list of possible output schemas, and to produce something that conforms to the chosen one. This usually involves writing, for each of the possible output schemas: a name to identify it, a description to help the LLM decide when it is the appropriate choice, and a schema for the desired output format (usually in JSONSchema notation).</p>
	</dd>
	<dt>JSON mode</dt>
	<dd>
	<p>This<a contenteditable="false" data-primary="JSON output" data-type="indexterm" id="id789"/><a contenteditable="false" data-primary="output" data-secondary="JSON output" data-type="indexterm" id="id790"/> is a mode available in some LLMs (such as recent OpenAI models) that enforces the LLM to output a valid JSON document.</p>
	</dd>
</dl>

<p>Different models may support different variants of these, with slightly different parameters. To make it easy to get LLMs to return structured output, LangChain models implement a common interface, a method called <code>.with_structured_output</code>. By invoking this method—and passing in a JSON schema or a Pydantic (in Python) or Zod (in JS) model—the model will add whatever model parameters and output parsers are necessary to produce and return the structured output. When a particular model implements more than one of the preceding strategies, you can configure which method to use.</p>

<p>Let’s create a schema to use:</p>

<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="schema creation" data-type="indexterm" id="id791"/></em></p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">pydantic</code> <code class="kn">import</code> <code class="n">BaseModel</code><code class="p">,</code> <code class="n">Field</code>


<code class="k">class</code> <code class="nc">Joke</code><code class="p">(</code><code class="n">BaseModel</code><code class="p">):</code>
    <code class="n">setup</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="n">Field</code><code class="p">(</code><code class="n">description</code><code class="o">=</code><code class="s2">"The setup of the joke"</code><code class="p">)</code>
    <code class="n">punchline</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="n">Field</code><code class="p">(</code><code class="n">description</code><code class="o">=</code><code class="s2">"The punchline to the joke"</code><code class="p">)</code></pre>

<p><em>JavaScript<a contenteditable="false" data-primary="JavaScript" data-secondary="schema creation" data-type="indexterm" id="id792"/></em></p>

<pre data-code-language="javascript" data-type="programlisting">
<code class="kr">import</code> <code class="p">{</code> <code class="nx">z</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"zod"</code><code class="p">;</code>

<code class="kr">const</code> <code class="nx">joke</code> <code class="o">=</code> <code class="nx">z</code><code class="p">.</code><code class="nx">object</code><code class="p">({</code>
  <code class="nx">setup</code><code class="o">:</code> <code class="nx">z</code><code class="p">.</code><code class="nx">string</code><code class="p">().</code><code class="nx">describe</code><code class="p">(</code><code class="s2">"The setup of the joke"</code><code class="p">),</code>
  <code class="nx">punchline</code><code class="o">:</code> <code class="nx">z</code><code class="p">.</code><code class="nx">string</code><code class="p">().</code><code class="nx">describe</code><code class="p">(</code><code class="s2">"The punchline to the joke"</code><code class="p">),</code>
<code class="p">});</code></pre>

<p>Notice how we take care to add a description to each field. This is key because—together with the name of the field—this is the information the LLM will use to decide what part of the output should go in each field. We could also have defined a schema in raw<a contenteditable="false" data-primary="JSONSchema notation" data-type="indexterm" id="id793"/> JSONSchema notation, which would look like this:</p>

<pre data-type="programlisting">
{'properties': {'setup': {'description': 'The setup of the joke',
    'title': 'Setup',
    'type': 'string'},
 'punchline': {'description': 'The punchline to the joke',
    'title': 'Punchline',
    'type': 'string'}},
 'required': ['setup', 'punchline'],
 'title': 'Joke',
 'type': 'object'}</pre>

<p class="pagebreak-before less_space">And now let’s get an LLM to generate output that conforms to this schema:</p>

<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="output" data-tertiary="structured" data-type="indexterm" id="id794"/></em></p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">langchain_openai</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">ChatOpenAI</code><code class="p">(</code><code class="n">model</code><code class="o">=</code><code class="s2">"gpt-3.5-turbo"</code><code class="p">,</code> <code class="n">temperature</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">with_structured_output</code><code class="p">(</code><code class="n">Joke</code><code class="p">)</code>

<code class="n">model</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="s2">"Tell me a joke about cats"</code><code class="p">)</code></pre>

<p><em>JavaScript<a contenteditable="false" data-primary="JavaScript" data-secondary="output" data-tertiary="structured" data-type="indexterm" id="id795"/></em></p>

<pre data-code-language="javascript" data-type="programlisting">
<code class="kr">import</code> <code class="p">{</code> <code class="nx">ChatOpenAI</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"@langchain/openai"</code><code class="p">;</code>

<code class="kd">let</code> <code class="nx">model</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">ChatOpenAI</code><code class="p">({</code>
  <code class="nx">model</code><code class="o">:</code> <code class="s2">"gpt-3.5-turbo"</code><code class="p">,</code>
  <code class="nx">temperature</code><code class="o">:</code> <code class="mi">0</code>
<code class="p">});</code>
<code class="nx">model</code> <code class="o">=</code> <code class="nx">model</code><code class="p">.</code><code class="nx">withStructuredOutput</code><code class="p">(</code><code class="nx">joke</code><code class="p">);</code>

<code class="nx">await</code> <code class="nx">structuredLlm</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="s2">"Tell me a joke about cats"</code><code class="p">);</code></pre>

<p><em>An example of output</em></p>

<pre data-type="programlisting">
{
    setup: "Why don't cats play poker in the wild?",
    punchline: "Too many cheetahs."
}</pre>

<p>A couple of things to notice:</p>

<ul>
	<li>
	<p>We create the instance of the model as usual, specifying the model name to use and other parameters.</p>
	</li>
	<li>
	<p>Low temperature is usually a good fit for structured output, as it reduces the chance the LLM will produce invalid output that doesn’t conform to the schema.</p>
	</li>
	<li>
	<p>Afterward, we attach the schema to the model, which returns a new object, which will produce output that matches the schema provided. When you pass in a Pydantic or Zod object for schema, this will be used for validation as well; that is, if the LLM produces output that doesn’t conform, a validation error will be returned to you instead of the failed output.</p>
	</li>
	<li>
	<p>Finally, we invoke the model with our (free-form) input, and receive back output that matches the structure we desired.</p>
	</li>
</ul>

<p>This pattern of using structured output can be very useful both as a standalone tool and as a part of a larger application; for instance, refer back to <a data-type="xref" href="ch05.html#ch05_cognitive_architectures_with_langgraph_1736545670030774">Chapter 5</a>, where we make use of this capability to implement the routing step of the router architecture.</p>

<section data-pdf-bookmark="Intermediate Output" data-type="sect2"><div class="sect2" id="ch08_intermediate_output_1736545674143885">
<h2>Intermediate Output</h2>

<p>The<a contenteditable="false" data-primary="structured output" data-secondary="intermediate output" data-type="indexterm" id="SOinter08"/><a contenteditable="false" data-primary="intermediate output" data-type="indexterm" id="intout08"/> more complex your LLM architecture becomes, the more likely it will take longer to execute. If you think back to the architecture diagrams in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch05.html#ch05_cognitive_architectures_with_langgraph_1736545670030774">5</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch06.html#ch06_agent_architecture_1736545671750341">6</a>, every time you see multiple steps (or nodes) connected in sequence or in a loop, that is an indication that the time it takes for a full invocation is increasing.</p>

<p>This increase in<a contenteditable="false" data-primary="latency" data-type="indexterm" id="id796"/> latency—if not addressed—can be a blocker to user adoption of LLM applications, with most users expecting computer applications to produce some output within seconds. There are several strategies to make the higher latency more palatable, but they all fall under the umbrella of <em>streaming output</em>, that is, receiving output from the application while it is still running.</p>

<p>For this section, we’ll use the last architecture described in <a data-type="xref" href="ch06.html#ch06_dealing_with_many_tools_1736545671750712">“Dealing with Many Tools”</a>. Refer back to <a data-type="xref" href="ch06.html#ch06_agent_architecture_1736545671750341">Chapter 6</a> for the full code snippet.</p>

<p>To<a contenteditable="false" data-primary="LangGraph" data-secondary="intermediate output" data-type="indexterm" id="id797"/> generate intermediate output with LangGraph, all you have to do is to invoke the graph with the<a contenteditable="false" data-primary="stream method" data-type="indexterm" id="id798"/> <code>stream</code> method, which will yield the output of each node as soon as each finishes. Let’s see what that looks like:</p>

<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="output" data-tertiary="intermediate" data-type="indexterm" id="id799"/></em></p>

<pre data-code-language="python" data-type="programlisting">
<code class="nb">input</code> <code class="o">=</code> <code class="p">{</code>
    <code class="s2">"messages"</code><code class="p">:</code> <code class="p">[</code>
        <code class="n">HumanMessage</code><code class="p">(</code><code class="s2">"""How old was the 30th president of the United States </code>
<code class="s2">            when he died?"""</code><code class="p">)</code>
    <code class="p">]</code>
<code class="p">}</code>
<code class="k">for</code> <code class="n">c</code> <code class="ow">in</code> <code class="n">graph</code><code class="o">.</code><code class="n">stream</code><code class="p">(</code><code class="nb">input</code><code class="p">,</code> <code class="n">stream_mode</code><code class="o">=</code><code class="s1">'updates'</code><code class="p">):</code>
    <code class="nb">print</code><code class="p">(</code><code class="n">c</code><code class="p">)</code></pre>

<p><em>JavaScript<a contenteditable="false" data-primary="JavaScript" data-secondary="output" data-tertiary="intermediate" data-type="indexterm" id="id800"/></em></p>

<pre data-code-language="javascript" data-type="programlisting">
<code class="kr">const</code> <code class="nx">input</code> <code class="o">=</code> <code class="p">{</code>
  <code class="nx">messages</code><code class="o">:</code> <code class="p">[</code>
    <code class="k">new</code> <code class="nx">HumanMessage</code><code class="p">(</code><code class="sb">`How old was the 30th president of the United States when </code>
<code class="sb">      he died?`</code><code class="p">)</code>
  <code class="p">]</code>
<code class="p">}</code>
<code class="kr">const</code> <code class="nx">output</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">graph</code><code class="p">.</code><code class="nx">stream</code><code class="p">(</code><code class="nx">input</code><code class="p">,</code> <code class="nx">streamMode</code><code class="o">:</code> <code class="s1">'updates'</code><code class="p">)</code>
<code class="k">for</code> <code class="nx">await</code> <code class="p">(</code><code class="kr">const</code> <code class="nx">c</code> <code class="k">of</code> <code class="nx">output</code><code class="p">)</code> <code class="p">{</code>
  <code class="nx">console</code><code class="p">.</code><code class="nx">log</code><code class="p">(</code><code class="nx">c</code><code class="p">)</code>
<code class="p">}</code></pre>

<p><em>The output:</em></p>

<pre data-type="programlisting">
{
    "select_tools": {
        "selected_tools": ['duckduckgo_search', 'calculator']
    }
}
{
    "model": {
        "messages": AIMessage(
            content="",
            tool_calls=[
                {
                    "name": "duckduckgo_search",
                    "args": {
                        "query": "30th president of the United States"
                    },
                    "id": "9ed4328dcdea4904b1b54487e343a373",
                    "type": "tool_call",
                }
            ],
        )
    }
}
{
    "tools": {
        "messages": [
            ToolMessage(
                content="Calvin Coolidge (born July 4, 1872, Plymouth, Vermont, 
                    U.S.—died January 5, 1933, Northampton, Massachusetts) was 
                    the 30th president of the United States (1923-29). Coolidge 
                    acceded to the presidency after the death in office of 
                    Warren G. Harding, just as the Harding scandals were coming 
                    to light....",
                name="duckduckgo_search",
                tool_call_id="9ed4328dcdea4904b1b54487e343a373",
            )
        ]
    }
}
{
    "model": {
        "messages": AIMessage(
            content="Calvin Coolidge, the 30th president of the United States, 
                was born on July 4, 1872, and died on January 5, 1933. To 
                calculate his age at the time of his death, we can subtract his 
                birth year from his death year. \n\nAge at death = Death year - 
                Birth year\nAge at death = 1933 - 1872\nAge at death = 61 
                years\n\nCalvin Coolidge was 61 years old when he died.",
        )
    }
}</pre>

<p>Notice how each output entry is a dictionary with the name of the node that emitted as the key and the output of that node as the value. This gives you two key pieces of information:</p>

<ul>
	<li>
	<p>Where the application currently is; that is, if you think back to the architecture diagrams shown in previous chapters, where in that diagram are we currently?</p>
	</li>
	<li>
	<p>Each update to the shared state of the application, which together build up to the final output of the graph.</p>
	</li>
</ul>

<p>In addition, LangGraph<a contenteditable="false" data-primary="LangGraph" data-secondary="stream modes supported in" data-type="indexterm" id="id801"/> supports more stream modes:</p>

<ul>
	<li>
	<p><code>updates</code>. This is the<a contenteditable="false" data-primary="updates mode (LangGraph)" data-type="indexterm" id="id802"/> default mode, described above.</p>
	</li>
	<li>
	<p><code>values</code>. This<a contenteditable="false" data-primary="values mode (LangGraph)" data-type="indexterm" id="id803"/> mode yields the current state of the graph every time it changes, that is after each set of nodes finishes executing. This can be useful when the way you display output to your users closely tracks the shape of the graph state.</p>
	</li>
	<li>
	<p><code>debug</code>. This<a contenteditable="false" data-primary="debug mode (LangGraph)" data-type="indexterm" id="id804"/> mode yields detailed events every time something happens in your graph, including:</p>

	<ul>
		<li>
		<p><code>checkpoint</code> events, whenever<a contenteditable="false" data-primary="checkpoint events" data-type="indexterm" id="id805"/> a new checkpoint of the current state is saved to the database</p>
		</li>
		<li>
		<p><code>task</code> events, emitted<a contenteditable="false" data-primary=" " data-type="indexterm" id="id806"/> whenever a node is about to start running</p>
		</li>
		<li>
		<p><code>task_result</code> events, emitted<a contenteditable="false" data-primary="task_result events" data-type="indexterm" id="id807"/> whenever a node finishes running</p>
		</li>
	</ul>
	</li>
	<li>
	<p>Finally, you can combine these modes; for instance, requesting both <code>updates</code> and <code>values</code> by passing a list.</p>
	</li>
</ul>

<p>You control the stream mode with the <code>stream_mode</code> argument to <code>stream()</code>.<a contenteditable="false" data-primary="" data-startref="SOinter08" data-type="indexterm" id="id808"/><a contenteditable="false" data-primary="" data-startref="intout08" data-type="indexterm" id="id809"/></p>
</div></section>

<section data-pdf-bookmark="Streaming LLM Output Token-by-Token" data-type="sect2"><div class="sect2" id="ch08_streaming_llm_output_token_by_token_1736545674143951">
<h2>Streaming LLM Output Token-by-Token</h2>

<p>Sometimes<a contenteditable="false" data-primary="structured output" data-secondary="streaming LLM output token-by-token" data-type="indexterm" id="id810"/><a contenteditable="false" data-primary="token-by-token streaming output" data-type="indexterm" id="id811"/> you may also want to get streaming output from each LLM call inside your larger LLM application. This can be useful for various projects, such as when building<a contenteditable="false" data-primary="interactive chatbots" data-type="indexterm" id="id812"/> an interactive chatbot, where you want each word to be displayed as soon as it is produced by the LLM.</p>

<p>You can achieve this with<a contenteditable="false" data-primary="LangGraph" data-secondary="streaming LLM output token-by-token" data-type="indexterm" id="id813"/> LangGraph as well:</p>

<p><em>Python<a contenteditable="false" data-primary="Python" data-secondary="output" data-tertiary="streaming token-by-token" data-type="indexterm" id="id814"/></em></p>

<pre data-code-language="python" data-type="programlisting">
<code class="nb">input</code> <code class="o">=</code> <code class="p">{</code>
    <code class="s2">"messages"</code><code class="p">:</code> <code class="p">[</code>
        <code class="n">HumanMessage</code><code class="p">(</code><code class="s2">"""How old was the 30th president of the United States </code>
<code class="s2">            when he died?"""</code><code class="p">)</code>
    <code class="p">]</code>
<code class="p">}</code>
<code class="n">output</code> <code class="o">=</code> <code class="n">app</code><code class="o">.</code><code class="n">astream_events</code><code class="p">(</code><code class="nb">input</code><code class="p">,</code> <code class="n">version</code><code class="o">=</code><code class="s2">"v2"</code><code class="p">)</code>

<code class="k">async</code> <code class="k">for</code> <code class="n">event</code> <code class="ow">in</code> <code class="n">output</code><code class="p">:</code>
    <code class="k">if</code> <code class="n">event</code><code class="p">[</code><code class="s2">"event"</code><code class="p">]</code> <code class="o">==</code> <code class="s2">"on_chat_model_stream"</code><code class="p">:</code>
        <code class="n">content</code> <code class="o">=</code> <code class="n">event</code><code class="p">[</code><code class="s2">"data"</code><code class="p">][</code><code class="s2">"chunk"</code><code class="p">]</code><code class="o">.</code><code class="n">content</code>
        <code class="k">if</code> <code class="n">content</code><code class="p">:</code>
            <code class="nb">print</code><code class="p">(</code><code class="n">content</code><code class="p">)</code></pre>

<p class="pagebreak-before less_space"><em>JavaScript<a contenteditable="false" data-primary="JavaScript" data-secondary="output" data-tertiary="streaming token-by-token" data-type="indexterm" id="id815"/></em></p>

<pre data-code-language="javascript" data-type="programlisting">
<code class="kr">const</code> <code class="nx">input</code> <code class="o">=</code> <code class="p">{</code>
  <code class="nx">messages</code><code class="o">:</code> <code class="p">[</code>
    <code class="k">new</code> <code class="nx">HumanMessage</code><code class="p">(</code><code class="sb">`How old was the 30th president of the United States when </code>
<code class="sb">      he died?`</code><code class="p">)</code>
  <code class="p">]</code>
<code class="p">}</code>

<code class="kr">const</code> <code class="nx">output</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">agent</code><code class="p">.</code><code class="nx">streamEvents</code><code class="p">(</code><code class="nx">input</code><code class="p">,</code> <code class="p">{</code><code class="nx">version</code><code class="o">:</code> <code class="s2">"v2"</code><code class="p">});</code>

<code class="k">for</code> <code class="nx">await</code> <code class="p">(</code><code class="kr">const</code> <code class="p">{</code> <code class="nx">event</code><code class="p">,</code> <code class="nx">data</code> <code class="p">}</code> <code class="k">of</code> <code class="nx">output</code><code class="p">)</code> <code class="p">{</code>
  <code class="k">if</code> <code class="p">(</code><code class="nx">event</code> <code class="o">===</code> <code class="s2">"on_chat_model_stream"</code><code class="p">)</code> <code class="p">{</code>
    <code class="kr">const</code> <code class="nx">msg</code> <code class="o">=</code> <code class="nx">data</code><code class="p">.</code><code class="nx">chunk</code> <code class="nx">as</code> <code class="nx">AIMessageChunk</code><code class="p">;</code>
    <code class="k">if</code> <code class="p">(</code><code class="nx">msg</code><code class="p">.</code><code class="nx">content</code><code class="p">)</code> <code class="p">{</code>
      <code class="nx">console</code><code class="p">.</code><code class="nx">log</code><code class="p">(</code><code class="nx">msg</code><code class="p">.</code><code class="nx">content</code><code class="p">);</code>
    <code class="p">}</code>
  <code class="p">}</code>
<code class="p">}</code></pre>

<p>This will emit each word (technically each token) as soon as it is received from the LLM. You can find more details on this pattern from <a href="https://oreil.ly/ExYll">LangChain</a>.</p>
</div></section>

<section data-pdf-bookmark="Human-in-the-Loop Modalities" data-type="sect2"><div class="sect2" id="ch08_human_in_the_loop_modalities_1736545674144033">
<h2>Human-in-the-Loop Modalities</h2>

<p>As<a contenteditable="false" data-primary="structured output" data-secondary="human-in-the-loop modalities" data-type="indexterm" id="SOhuman08"/> we walk the autonomy (or agency) ladder, we find ourselves increasingly giving up control (or oversight) in exchange for capability (or autonomy). The shared state pattern used in LangGraph (see <a data-type="xref" href="ch05.html#ch05_cognitive_architectures_with_langgraph_1736545670030774">Chapter 5</a> for an introduction) makes it easier to observe, interrupt, and modify the application. This makes it possible to use many different <em>human-in-the-loop</em> modes, or ways for the developer/end user of an application to influence what the LLM is up to.</p>

<p>For this section, we’ll again use the last architecture described in <a data-type="xref" href="ch06.html#ch06_dealing_with_many_tools_1736545671750712">“Dealing with Many Tools”</a>. Refer back to <a data-type="xref" href="ch06.html#ch06_agent_architecture_1736545671750341">Chapter 6</a> for the full code snippet. For all human-in-the-loop modes, we first need to attach a checkpointer<a contenteditable="false" data-primary="checkpointers" data-type="indexterm" id="id816"/><a contenteditable="false" data-primary="human-in-the-loop modalities" data-secondary="checkpointers" data-type="indexterm" id="id817"/> to the graph; refer to <a data-type="xref" href="ch04.html#ch04_adding_memory_to_stategraph_1736545668266831">“Adding Memory to StateGraph”</a> for more details on this:</p>

<p><em>Python</em></p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">langgraph.checkpoint.memory</code> <code class="kn">import</code> <code class="n">MemorySaver</code>

<code class="n">graph</code> <code class="o">=</code> <code class="n">builder</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">checkpointer</code><code class="o">=</code><code class="n">MemorySaver</code><code class="p">())</code></pre>

<p><em>JavaScript</em></p>

<pre data-code-language="javascript" data-type="programlisting">
<code class="kr">import</code> <code class="p">{</code><code class="nx">MemorySaver</code><code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/langgraph'</code>

<code class="nx">graph</code> <code class="o">=</code> <code class="nx">builder</code><code class="p">.</code><code class="nx">compile</code><code class="p">({</code> <code class="nx">checkpointer</code><code class="o">:</code> <code class="k">new</code> <code class="nx">MemorySaver</code><code class="p">()</code> <code class="p">})</code></pre>

<p class="pagebreak-before less_space">This returns an instance of the graph that stores the<a contenteditable="false" data-primary="state" data-type="indexterm" id="id818"/> state at the end of each step, so every invocation after the first doesn’t start from a blank slate. Any time the graph is called, it starts by using the checkpointer to fetch the most recent saved state—if any—and combines the new input with the previous state. And only then does it execute the first nodes. This is key to enabling human-in-the-loop modalities, which all rely on the graph remembering the previous state.</p>

<p>The<a contenteditable="false" data-primary="human-in-the-loop modalities" data-secondary="interrupt mode" data-type="indexterm" id="id819"/><a contenteditable="false" data-primary="interrupt mode" data-type="indexterm" id="id820"/> first mode, <code>interrupt</code>, is the simplest form of control—the user is looking at streaming output of the application as it is produced, and manually interrupts it when he sees fit (see <a data-type="xref" href="#ch08_figure_3_1736545674134460">Figure 8-3</a>). The state is saved as of the last complete step prior to the user hitting the interrupt button. From there the user can choose to:</p>

<ul>
	<li>
	<p>Resume from that point onward, and the computation will proceed as if it hadn’t been interrupted (see <a data-type="xref" href="#ch08_resume_1736545674144097">“Resume”</a>).</p>
	</li>
	<li>
	<p>Send new input into the application (e.g., a new message in a chatbot), which will cancel any future steps that were pending and start dealing with the new input (see <a data-type="xref" href="#ch08_restart_1736545674144152">“Restart”</a>).</p>
	</li>
	<li>
	<p>Do nothing and nothing else will run.</p>
	</li>
</ul>

<figure><div class="figure" id="ch08_figure_3_1736545674134460"><img alt="The Interrupt pattern" src="assets/lelc_0803.png"/>
<h6><span class="label">Figure 8-3. </span>The <code>interrupt</code> pattern</h6>
</div></figure>

<p class="pagebreak-before less_space">Let’s see how to do this in LangGraph:</p>

<p><em>Python</em></p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">asyncio</code>

<code class="n">event</code> <code class="o">=</code> <code class="n">asyncio</code><code class="o">.</code><code class="n">Event</code><code class="p">()</code>

<code class="nb">input</code> <code class="o">=</code> <code class="p">{</code>
    <code class="s2">"messages"</code><code class="p">:</code> <code class="p">[</code>
        <code class="n">HumanMessage</code><code class="p">(</code><code class="s2">"""How old was the 30th president of the United States </code>
<code class="s2">            when he died?"""</code><code class="p">)</code>
    <code class="p">]</code>
<code class="p">}</code>

<code class="n">config</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"configurable"</code><code class="p">:</code> <code class="p">{</code><code class="s2">"thread_id"</code><code class="p">:</code> <code class="s2">"1"</code><code class="p">}}</code>

<code class="k">async</code> <code class="k">with</code> <code class="n">aclosing</code><code class="p">(</code><code class="n">graph</code><code class="o">.</code><code class="n">astream</code><code class="p">(</code><code class="nb">input</code><code class="p">,</code> <code class="n">config</code><code class="p">))</code> <code class="k">as</code> <code class="n">stream</code><code class="p">:</code>
    <code class="k">async</code> <code class="k">for</code> <code class="n">chunk</code> <code class="ow">in</code> <code class="n">stream</code><code class="p">:</code>
        <code class="k">if</code> <code class="n">event</code><code class="o">.</code><code class="n">is_set</code><code class="p">():</code>
            <code class="k">break</code>
        <code class="k">else</code><code class="p">:</code>
            <code class="o">...</code> <code class="c1"># do something with the output</code>

<code class="c1"># Somewhere else in your application</code>

<code class="n">event</code><code class="o">.</code><code class="n">set</code><code class="p">()</code></pre>

<p><em>JavaScript</em></p>

<pre data-code-language="javascript" data-type="programlisting">
<code class="kr">const</code> <code class="nx">controller</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">AbortController</code><code class="p">()</code>

<code class="kr">const</code> <code class="nx">input</code> <code class="o">=</code> <code class="p">{</code>
  <code class="s2">"messages"</code><code class="o">:</code> <code class="p">[</code>
    <code class="k">new</code> <code class="nx">HumanMessage</code><code class="p">(</code><code class="sb">`How old was the 30th president of the United States when </code>
<code class="sb">      he died?`</code><code class="p">)</code>
  <code class="p">]</code>
<code class="p">}</code>

<code class="kr">const</code> <code class="nx">config</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"configurable"</code><code class="o">:</code> <code class="p">{</code><code class="s2">"thread_id"</code><code class="o">:</code> <code class="s2">"1"</code><code class="p">}}</code>

<code class="k">try</code> <code class="p">{</code>
  <code class="kr">const</code> <code class="nx">output</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">graph</code><code class="p">.</code><code class="nx">stream</code><code class="p">(</code><code class="nx">input</code><code class="p">,</code> <code class="p">{</code>
    <code class="p">...</code><code class="nx">config</code><code class="p">,</code>
    <code class="nx">signal</code><code class="o">:</code> <code class="nx">controller</code><code class="p">.</code><code class="nx">signal</code>
  <code class="p">});</code>
  <code class="k">for</code> <code class="nx">await</code> <code class="p">(</code><code class="kr">const</code> <code class="nx">chunk</code> <code class="k">of</code> <code class="nx">output</code><code class="p">)</code> <code class="p">{</code>
    <code class="nx">console</code><code class="p">.</code><code class="nx">log</code><code class="p">(</code><code class="nx">chunk</code><code class="p">);</code> <code class="c1">// do something with the output</code>
  <code class="p">}</code>
<code class="p">}</code> <code class="k">catch</code> <code class="p">(</code><code class="nx">e</code><code class="p">)</code> <code class="p">{</code>
  <code class="nx">console</code><code class="p">.</code><code class="nx">log</code><code class="p">(</code><code class="nx">e</code><code class="p">);</code>
<code class="p">}</code>

<code class="c1">// Somewhere else in your application</code>
<code class="nx">controller</code><code class="p">.</code><code class="nx">abort</code><code class="p">()</code></pre>

<p>This makes use of an event or signal, so that you can control interruption from outside of the running application. Notice in the Python code block the use of <code>aclosing</code>; this ensures the stream is properly closed when interrupted. Notice in JS the use of the<a contenteditable="false" data-primary="try-catch statement" data-type="indexterm" id="id821"/> <code>try-catch</code> statement, as interrupting the run will result in an <code>abort</code> exception being raised. Finally notice that usage of the checkpointer requires passing in an identifier for this thread, to distinguish this interaction with the graph from all others.</p>

<figure><div class="figure" id="ch08_figure_4_1736545674134478"><img alt="The Authorize patten" src="assets/lelc_0804.png"/>
<h6><span class="label">Figure 8-4. </span>The <code>authorize</code> pattern</h6>
</div></figure>

<p>A<a contenteditable="false" data-primary="human-in-the-loop modalities" data-secondary="authorize mode" data-type="indexterm" id="id822"/><a contenteditable="false" data-primary="authorize mode" data-type="indexterm" id="id823"/> second control mode is <code>authorize</code>, where the user defines ahead of time that they want the application to hand off control to them every time a particular node is about to be called (see <a data-type="xref" href="#ch08_figure_4_1736545674134478">Figure 8-4</a>). This is usually implemented for tool confirmation—before any tool (or particular tools) is called, the application will pause and ask for confirmation, at which point the user can, again:</p>

<ul>
	<li>
	<p>Resume computation, accepting the tool call.</p>
	</li>
	<li>
	<p>Send a new message to guide the bot in a different direction, in which case the tool will not be called.</p>
	</li>
	<li>
	<p>Do nothing.</p>
	</li>
</ul>

<p>Here’s the code:</p>

<p><em>Python</em></p>

<pre data-code-language="python" data-type="programlisting">
<code class="nb">input</code> <code class="o">=</code> <code class="p">{</code>
    <code class="s2">"messages"</code><code class="p">:</code> <code class="p">[</code>
        <code class="n">HumanMessage</code><code class="p">(</code><code class="s2">"""How old was the 30th president of the United States </code>
<code class="s2">            when he died?"""</code><code class="p">)</code>
    <code class="p">]</code>
<code class="p">}</code>

<code class="n">config</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"configurable"</code><code class="p">:</code> <code class="p">{</code><code class="s2">"thread_id"</code><code class="p">:</code> <code class="s2">"1"</code><code class="p">}}</code>

<code class="n">output</code> <code class="o">=</code> <code class="n">graph</code><code class="o">.</code><code class="n">astream</code><code class="p">(</code><code class="nb">input</code><code class="p">,</code> <code class="n">config</code><code class="p">,</code> <code class="n">interrupt_before</code><code class="o">=</code><code class="p">[</code><code class="s1">'tools'</code><code class="p">])</code>

<code class="k">async</code> <code class="k">for</code> <code class="n">c</code> <code class="ow">in</code> <code class="n">output</code><code class="p">:</code>
    <code class="o">...</code> <code class="c1"># do something with the output</code></pre>

<p><em>JavaScript</em></p>

<pre data-code-language="javascript" data-type="programlisting">
<code class="kr">const</code> <code class="nx">input</code> <code class="o">=</code> <code class="p">{</code>
  <code class="s2">"messages"</code><code class="o">:</code> <code class="p">[</code>
    <code class="k">new</code> <code class="nx">HumanMessage</code><code class="p">(</code><code class="sb">`How old was the 30th president of the United States when </code>
<code class="sb">      he died?`</code><code class="p">)</code>
  <code class="p">]</code>
<code class="p">}</code>

<code class="kr">const</code> <code class="nx">config</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"configurable"</code><code class="o">:</code> <code class="p">{</code><code class="s2">"thread_id"</code><code class="o">:</code> <code class="s2">"1"</code><code class="p">}}</code>

<code class="kr">const</code> <code class="nx">output</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">graph</code><code class="p">.</code><code class="nx">stream</code><code class="p">(</code><code class="nx">input</code><code class="p">,</code> <code class="p">{</code>
  <code class="p">...</code><code class="nx">config</code><code class="p">,</code>
  <code class="nx">interruptBefore</code><code class="o">:</code> <code class="p">[</code><code class="s1">'tools'</code><code class="p">]</code>
<code class="p">});</code>
<code class="k">for</code> <code class="nx">await</code> <code class="p">(</code><code class="kr">const</code> <code class="nx">chunk</code> <code class="k">of</code> <code class="nx">output</code><code class="p">)</code> <code class="p">{</code>
  <code class="nx">console</code><code class="p">.</code><code class="nx">log</code><code class="p">(</code><code class="nx">chunk</code><code class="p">);</code> <code class="c1">// do something with the output</code>
<code class="p">}</code></pre>

<p>This will run the graph up until it is about to enter the node called <code>tools</code>, thus giving you the chance to inspect the current state, and decide whether to proceed or not. Notice that <code>interrupt_before</code> is a list where order is not important; if you pass multiple node names, it will interrupt before entering each of them.</p>

<section data-pdf-bookmark="Resume" data-type="sect3"><div class="sect3" id="ch08_resume_1736545674144097">
<h3>Resume</h3>

<p>To<a contenteditable="false" data-primary="human-in-the-loop modalities" data-secondary="resume step" data-type="indexterm" id="id824"/> proceed from an interrupted graph—such as when using one of the previous two patterns—you just need to re-invoke the graph with null input (or <code>None</code> in Python). This is taken as a signal to continue processing the previous non-null input:</p>

<p><em>Python</em></p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">config</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"configurable"</code><code class="p">:</code> <code class="p">{</code><code class="s2">"thread_id"</code><code class="p">:</code> <code class="s2">"1"</code><code class="p">}}</code>

<code class="n">output</code> <code class="o">=</code> <code class="n">graph</code><code class="o">.</code><code class="n">astream</code><code class="p">(</code><code class="kc">None</code><code class="p">,</code> <code class="n">config</code><code class="p">,</code> <code class="n">interrupt_before</code><code class="o">=</code><code class="p">[</code><code class="s1">'tools'</code><code class="p">])</code>

<code class="k">async</code> <code class="k">for</code> <code class="n">c</code> <code class="ow">in</code> <code class="n">output</code><code class="p">:</code>
    <code class="o">...</code> <code class="c1"># do something with the output</code></pre>

<p><em>JavaScript</em></p>

<pre data-code-language="javascript" data-type="programlisting">
<code class="kr">const</code> <code class="nx">config</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"configurable"</code><code class="o">:</code> <code class="p">{</code><code class="s2">"thread_id"</code><code class="o">:</code> <code class="s2">"1"</code><code class="p">}}</code>

<code class="kr">const</code> <code class="nx">output</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">graph</code><code class="p">.</code><code class="nx">stream</code><code class="p">(</code><code class="kc">null</code><code class="p">,</code> <code class="p">{</code>
  <code class="p">...</code><code class="nx">config</code><code class="p">,</code>
  <code class="nx">interruptBefore</code><code class="o">:</code> <code class="p">[</code><code class="s1">'tools'</code><code class="p">]</code>
<code class="p">});</code>
<code class="k">for</code> <code class="nx">await</code> <code class="p">(</code><code class="kr">const</code> <code class="nx">chunk</code> <code class="k">of</code> <code class="nx">output</code><code class="p">)</code> <code class="p">{</code>
  <code class="nx">console</code><code class="p">.</code><code class="nx">log</code><code class="p">(</code><code class="nx">chunk</code><code class="p">);</code> <code class="c1">// do something with the output</code>
<code class="p">}</code></pre>
</div></section>

<section data-pdf-bookmark="Restart" data-type="sect3"><div class="sect3" id="ch08_restart_1736545674144152">
<h3>Restart</h3>

<p>If<a contenteditable="false" data-primary="human-in-the-loop modalities" data-secondary="restart step" data-type="indexterm" id="id825"/> instead you want an interrupted graph to start over from the first node, with additional new input, you just need to invoke it with new input:</p>

<p><em>Python</em></p>

<pre data-code-language="python" data-type="programlisting">
<code class="nb">input</code> <code class="o">=</code> <code class="p">{</code>
    <code class="s2">"messages"</code><code class="p">:</code> <code class="p">[</code>
        <code class="n">HumanMessage</code><code class="p">(</code><code class="s2">"""How old was the 30th president of the United States </code>
<code class="s2">            when he died?"""</code><code class="p">)</code>
    <code class="p">]</code>
<code class="p">}</code>

<code class="n">config</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"configurable"</code><code class="p">:</code> <code class="p">{</code><code class="s2">"thread_id"</code><code class="p">:</code> <code class="s2">"1"</code><code class="p">}}</code>

<code class="n">output</code> <code class="o">=</code> <code class="n">graph</code><code class="o">.</code><code class="n">astream</code><code class="p">(</code><code class="nb">input</code><code class="p">,</code> <code class="n">config</code><code class="p">)</code>

<code class="k">async</code> <code class="k">for</code> <code class="n">c</code> <code class="ow">in</code> <code class="n">output</code><code class="p">:</code>
    <code class="o">...</code> <code class="c1"># do something with the output</code></pre>

<p><em>JavaScript</em></p>

<pre data-code-language="javascript" data-type="programlisting">
<code class="kr">const</code> <code class="nx">input</code> <code class="o">=</code> <code class="p">{</code>
  <code class="s2">"messages"</code><code class="o">:</code> <code class="p">[</code>
    <code class="k">new</code> <code class="nx">HumanMessage</code><code class="p">(</code><code class="sb">`How old was the 30th president of the United States when </code>
<code class="sb">      he died?`</code><code class="p">)</code>
  <code class="p">]</code>
<code class="p">}</code>

<code class="kr">const</code> <code class="nx">config</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"configurable"</code><code class="o">:</code> <code class="p">{</code><code class="s2">"thread_id"</code><code class="o">:</code> <code class="s2">"1"</code><code class="p">}}</code>

<code class="kr">const</code> <code class="nx">output</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">graph</code><code class="p">.</code><code class="nx">stream</code><code class="p">(</code><code class="nx">input</code><code class="p">,</code> <code class="nx">config</code><code class="p">);</code>

<code class="k">for</code> <code class="nx">await</code> <code class="p">(</code><code class="kr">const</code> <code class="nx">chunk</code> <code class="k">of</code> <code class="nx">output</code><code class="p">)</code> <code class="p">{</code>
  <code class="nx">console</code><code class="p">.</code><code class="nx">log</code><code class="p">(</code><code class="nx">chunk</code><code class="p">);</code> <code class="c1">// do something with the output</code>
<code class="p">}</code></pre>

<p>This will keep the current state of the graph, merge it with the new input, and start again from the first node.</p>

<p>If you want to lose the current state, just change the <code>thread_id</code>, which will start a new interaction from a blank slate. Any string value is a valid <code>thread_id</code>; we’d recommend using UUIDs (or other unique identifiers) as thread IDs.</p>
</div></section>

<section data-pdf-bookmark="Edit state" data-type="sect3"><div class="sect3" id="ch08_edit_state_1736545674144207">
<h3>Edit state</h3>

<p>Sometimes<a contenteditable="false" data-primary="human-in-the-loop modalities" data-secondary="edit state" data-type="indexterm" id="id826"/> you might want to update the state of the graph before resuming; this is possible with the <code>update_state</code> method. You’ll usually want to first inspect the current state with <code>get_state</code>.</p>

<p>Here’s what it looks like:</p>

<p><em>Python</em></p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">config</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"configurable"</code><code class="p">:</code> <code class="p">{</code><code class="s2">"thread_id"</code><code class="p">:</code> <code class="s2">"1"</code><code class="p">}}</code>

<code class="n">state</code> <code class="o">=</code> <code class="n">graph</code><code class="o">.</code><code class="n">get_state</code><code class="p">(</code><code class="n">config</code><code class="p">)</code>

<code class="c1"># something you want to add or replace</code>
<code class="n">update</code> <code class="o">=</code> <code class="p">{</code> <code class="p">}</code>

<code class="n">graph</code><code class="o">.</code><code class="n">update_state</code><code class="p">(</code><code class="n">config</code><code class="p">,</code> <code class="n">update</code><code class="p">)</code></pre>

<p><em>JavaScript</em></p>

<pre data-code-language="javascript" data-type="programlisting">
<code class="kr">const</code> <code class="nx">config</code> <code class="o">=</code> <code class="s2">"configurable"</code><code class="o">:</code> <code class="p">{</code><code class="s2">"thread_id"</code><code class="o">:</code> <code class="s2">"1"</code><code class="p">}</code>

<code class="kr">const</code> <code class="nx">state</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">graph</code><code class="p">.</code><code class="nx">getState</code><code class="p">(</code><code class="nx">config</code><code class="p">)</code>

<code class="c1">// something you want to add or replace</code>
<code class="kr">const</code> <code class="nx">update</code> <code class="o">=</code> <code class="p">{</code> <code class="p">}</code>

<code class="nx">await</code> <code class="nx">graph</code><code class="p">.</code><code class="nx">updateState</code><code class="p">(</code><code class="nx">config</code><code class="p">,</code> <code class="nx">update</code><code class="p">)</code></pre>

<p>This will create a new checkpoint containing your update. After this, you’re ready to resume the graph from this new point. See <a data-type="xref" href="#ch08_resume_1736545674144097">“Resume”</a> to find out how.</p>
</div></section>

<section data-pdf-bookmark="Fork" data-type="sect3"><div class="sect3" id="ch08_fork_1736545674144274">
<h3>Fork</h3>

<p>You<a contenteditable="false" data-primary="human-in-the-loop modalities" data-secondary="fork step" data-type="indexterm" id="id827"/> can also browse the history of all past states the graph has passed through, and any of them can be visited again, for instance, to get an alternative answer. This can be very useful in more creative applications, where each run through the graph is expected to produce different output.</p>

<p class="pagebreak-before less_space">Let’s see what it looks like:</p>

<p><em>Python</em></p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">config</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"configurable"</code><code class="p">:</code> <code class="p">{</code><code class="s2">"thread_id"</code><code class="p">:</code> <code class="s2">"1"</code><code class="p">}}</code>

<code class="n">history</code> <code class="o">=</code> <code class="p">[</code>
    <code class="n">state</code> <code class="k">for</code> <code class="n">state</code> <code class="ow">in</code>
    <code class="n">graph</code><code class="o">.</code><code class="n">get_state_history</code><code class="p">(</code><code class="n">config</code><code class="p">)</code>
<code class="p">]</code>

<code class="c1"># replay a past state</code>
<code class="n">graph</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="kc">None</code><code class="p">,</code> <code class="n">history</code><code class="p">[</code><code class="mi">2</code><code class="p">]</code><code class="o">.</code><code class="n">config</code><code class="p">)</code></pre>

<p><em>JavaScript</em></p>

<pre data-code-language="javascript" data-type="programlisting">
<code class="kr">const</code> <code class="nx">config</code> <code class="o">=</code> <code class="s2">"configurable"</code><code class="o">:</code> <code class="p">{</code><code class="s2">"thread_id"</code><code class="o">:</code> <code class="s2">"1"</code><code class="p">}</code>

<code class="kr">const</code> <code class="nx">history</code> <code class="o">=</code> <code class="nx">await</code> <code class="nb">Array</code><code class="p">.</code><code class="nx">fromAsync</code><code class="p">(</code><code class="nx">graph</code><code class="p">.</code><code class="nx">getStateHistory</code><code class="p">(</code><code class="nx">config</code><code class="p">))</code>

<code class="c1">// replay a past state</code>
<code class="nx">await</code> <code class="nx">graph</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="kc">null</code><code class="p">,</code> <code class="nx">history</code><code class="p">[</code><code class="mi">2</code><code class="p">].</code><code class="nx">config</code><code class="p">)</code></pre>

<p>Notice how we collect the history into a list/array in both languages; <span class="keep-together"><code>get_state_history</code></span> returns an iterator of states (to allow consuming lazily). The states returned from the history method are sorted with the most recent first and the oldest last.</p>

<p>The true power of the human-in-the-loop controls comes from mixing them in whatever way suits your application.<a contenteditable="false" data-primary="" data-startref="SOhuman08" data-type="indexterm" id="id828"/></p>
</div></section>
</div></section>

<section data-pdf-bookmark="Multitasking LLMs" data-type="sect2"><div class="sect2" id="ch08_multitasking_llms_1736545674144354">
<h2>Multitasking LLMs</h2>

<p>This<a contenteditable="false" data-primary="input" data-secondary="handling concurrent" data-type="indexterm" id="Iconcurrent08"/><a contenteditable="false" data-primary="LLM applications" data-secondary="multitasking LLMs" data-type="indexterm" id="LLMAmultitask08"/><a contenteditable="false" data-primary="multitasking" data-type="indexterm" id="multitask08"/> section covers the problem of handling concurrent input for LLM applications. This is a particularly relevant problem given that LLMs are quite slow, much more so when producing long outputs or when chained in<a contenteditable="false" data-primary="agent architecture" data-secondary="multi-agent architectures" data-type="indexterm" id="id829"/> multistep architectures (like you can do with LangGraph). Even as LLMs become faster, dealing with concurrent inputs will continue to be a challenge, as latency improvements will also unlock the door for more and more complex use cases, in much the same way as even the most productive person still faces the need to prioritize competing demands on their time.</p>

<p>Let’s walk through the options.</p>

<section data-pdf-bookmark="Refuse concurrent inputs" data-type="sect3"><div class="sect3" id="ch08_refuse_concurrent_inputs_1736545674144416">
<h3>Refuse concurrent inputs</h3>

<p>Any<a contenteditable="false" data-primary="concurrent inputs, dealing with" data-type="indexterm" id="concurrentin08"/> input received while processing a previous one is rejected. This is the simplest strategy, but unlikely to cover all needs, as it effectively means handing off concurrency management to the caller.</p>
</div></section>

<section data-pdf-bookmark="Handle independently" data-type="sect3"><div class="sect3" id="ch08_handle_independently_1736545674144484">
<h3>Handle independently</h3>

<p>Another simple option is to treat any new input as an independent invocation, creating a new<a contenteditable="false" data-primary="threads" data-type="indexterm" id="id830"/> thread (a container for remembering state) and producing output in that context. This has the obvious downside of needing to be shown to the user as two separate and unreconcilable invocations, which isn’t always possible or desirable. On the other hand, it has the upside of scaling to arbitrarily large sizes, and is something you’ll use to some extent in your application almost certainly. For instance, this is how you would think about the problem of getting a chatbot to “chat” with two different users concurrently.</p>
</div></section>

<section data-pdf-bookmark="Queue concurrent inputs" data-type="sect3"><div class="sect3" id="ch08_queue_concurrent_inputs_1736545674144546">
<h3>Queue concurrent inputs</h3>

<p>Any input received while processing a previous one is<a contenteditable="false" data-primary="queueing" data-type="indexterm" id="id831"/> queued up and handled when the current one is finished. This strategy has some pros:</p>

<ul>
	<li>
	<p>It supports receiving an arbitrary number of concurrent requests.</p>
	</li>
	<li>
	<p>Because we wait for current input to finish processing, it doesn’t matter if the new input arrives almost immediately after we start handling the current input or immediately before we finish; the end result will be the same, as we will finish processing the current input before moving on to the next.</p>
	</li>
</ul>

<p>The strategy suffers from a few drawbacks as well:</p>

<ul>
	<li>
	<p>It may take a while to process all queued inputs; in fact, the queue may grow unbounded if inputs are produced at a rate faster than processed.</p>
	</li>
	<li>
	<p>The inputs may be stale by the time they get processed, given that they are queued before seeing the response to the previous one, and not altered afterwards. This strategy is not appropriate when new inputs depend on previous answers.</p>
	</li>
</ul>
</div></section>

<section data-pdf-bookmark="Interrupt" data-type="sect3"><div class="sect3" id="ch08_interrupt_1736545674144602">
<h3>Interrupt</h3>

<p>When<a contenteditable="false" data-primary="interrupt strategy" data-type="indexterm" id="id832"/> a new input is received while another is being processed, abandon processing of the current one and restart the chain with the new input. This strategy can vary by what is kept of the interrupted run. Here are a few options:</p>

<ul>
	<li>
	<p>Keep nothing. The previous input is completely forgotten, as if it had never been sent or processed.</p>
	</li>
	<li>
	<p>Keep the last completed step. In a checkpointing app (which stores progress as it moves through the computation), keep the state produced by the last completed step, discard any pending state updates from the currently executing step, and start handling the new input in that context.</p>
	</li>
	<li>
	<p>Keep the last completed step, as well as the current in-progress step. Attempt to interrupt the current step while taking care to save any incomplete updates to state that were being produced at the time. This is likely to not generalize beyond the simplest architectures.</p>
	</li>
	<li>
	<p>Wait for the current node (but not any subsequent nodes) to finish, then save and interrupt.</p>
	</li>
</ul>

<p>This option has some pros compared to queuing concurrent inputs:</p>

<ul>
	<li>
	<p>New input is handled as soon as possible, reducing latency and the chance of producing stale outputs.</p>
	</li>
	<li>
	<p>For the “keep nothing” variant, the final output doesn’t depend on when the new input was received.</p>
	</li>
</ul>

<p>But it also has drawbacks:</p>

<ul>
	<li>
	<p>Effectively, this strategy is still limited to processing one input at a time; any old input is abandoned when new input is received.</p>
	</li>
	<li>
	<p>Keeping partial state updates for the next run requires the state to be designed with that in mind; if not, then your application is likely to end up in an invalid state. For instance, OpenAI chat models require an AI message requesting tool calls to be immediately followed by tool messages with the tool outputs. If your run is interrupted in between, you either defensively clean up the intermediate state or risk being unable to progress further.</p>
	</li>
	<li>
	<p>The final outputs produced are very sensitive to when the new input is received; new input will be handled in the context of the (incomplete) progress previously made toward handling the previous input. This can result in brittle or unpredictable outcomes if you don’t design accordingly.</p>
	</li>
</ul>
</div></section>

<section data-pdf-bookmark="Fork and merge" data-type="sect3"><div class="sect3" id="ch08_fork_and_merge_1736545674144657">
<h3>Fork and merge</h3>

<p>Another option<a contenteditable="false" data-primary="fork and merge strategy" data-type="indexterm" id="id833"/> is to handle new input in parallel, forking the state of the thread as it is when the new input is received and merging the final states as inputs finish being handled. This option requires designing your state to either be mergeable without conflicts (e.g., using conflict-free replicated data types [CRDTs] or other conflict resolution algorithms) or having the user manually resolve conflicts before you’re able to make sense of the output or send new input in this thread. If either of those two requirements is met, this is likely to be the best option overall. This way, new input is handled in a timely manner, output is independent of time received, and it supports an arbitrary number of concurrent runs.</p>

<p>Some<a contenteditable="false" data-primary="" data-startref="Iconcurrent08" data-type="indexterm" id="id834"/><a contenteditable="false" data-primary="" data-startref="LLMAmultitask08" data-type="indexterm" id="id835"/><a contenteditable="false" data-primary="" data-startref="multitask08" data-type="indexterm" id="id836"/><a contenteditable="false" data-primary="" data-startref="concurrentin08" data-type="indexterm" id="id837"/><a contenteditable="false" data-primary="" data-startref="LLMApatternss08" data-type="indexterm" id="id838"/> of these strategies are implemented in LangGraph Platform, which will be covered in <a data-type="xref" href="ch09.html#ch09_deployment_launching_your_ai_application_into_pro_1736545675509604">Chapter 9</a>.</p>
</div></section>
</div></section>
</div></section>

<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="ch08_summary_1736545674144712">
<h1>Summary</h1>

<p>In this chapter, we returned to the main trade-off you face when building LLM applications: agency versus reliability. We learned that there are strategies to partially beat the odds and get more reliability without sacrificing agency, and vice versa.</p>

<p>We started by covering structured outputs, which can improve the predictability of LLM-generated text. Next, we discussed emitting streaming/intermediate output from your application, which can make high latency (an inevitable side effect of agency currently) applications pleasant to use.</p>

<p>We also walked through a variety of human-in-the-loop controls—that is, techniques to give back some oversight to the end user of your LLM application—which can often make the difference in making high-agency architectures reliable. Finally, we talked about the problem of handling concurrent input to your application, a particularly salient problem for LLM apps given their high latency.</p>

<p>In the next chapter, you’ll learn how to deploy your AI application into production.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id778"><sup><a href="ch08.html#id778-marker">1</a></sup> In finance, the <em>efficient frontier</em> in portfolio optimization; in economics, a <em>production-possibility frontier</em>; in engineering, the <em>Pareto front</em>. </p></div></div></section></body></html>