- en: Chapter 3\. Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 1](ch01.html#landscape_chapter) I mentioned that the most common
    supervised learning tasks are regression (predicting values) and classification
    (predicting classes). In [Chapter 2](ch02.html#project_chapter) we explored a
    regression task, predicting housing values, using various algorithms such as linear
    regression, decision trees, and random forests (which will be explained in further
    detail in later chapters). Now we will turn our attention to classification systems.
  prefs: []
  type: TYPE_NORMAL
- en: MNIST
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter we will be using the MNIST dataset, which is a set of 70,000
    small images of digits handwritten by high school students and employees of the
    US Census Bureau. Each image is labeled with the digit it represents. This set
    has been studied so much that it is often called the “hello world” of machine
    learning: whenever people come up with a new classification algorithm they are
    curious to see how it will perform on MNIST, and anyone who learns machine learning
    tackles this dataset sooner or later.'
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-Learn provides many helper functions to download popular datasets. MNIST
    is one of them. The following code fetches the MNIST dataset from OpenML.org:⁠^([1](ch03.html#id1283))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The `sklearn.datasets` package contains mostly three types of functions: `fetch_*`
    functions such as `fetch_openml()` to download real-life datasets, `load_*` functions
    to load small toy datasets bundled with Scikit-Learn (so they don’t need to be
    downloaded over the internet), and `make_*` functions to generate fake datasets,
    useful for tests. Generated datasets are usually returned as an `(X, y)` tuple
    containing the input data and the targets, both as NumPy arrays. Other datasets
    are returned as `sklearn.utils.Bunch` objects, which are dictionaries whose entries
    can also be accessed as attributes. They generally contain the following entries:'
  prefs: []
  type: TYPE_NORMAL
- en: '`"DESCR"`'
  prefs: []
  type: TYPE_NORMAL
- en: A description of the dataset
  prefs: []
  type: TYPE_NORMAL
- en: '`"data"`'
  prefs: []
  type: TYPE_NORMAL
- en: The input data, usually as a 2D NumPy array
  prefs: []
  type: TYPE_NORMAL
- en: '`"target"`'
  prefs: []
  type: TYPE_NORMAL
- en: The labels, usually as a 1D NumPy array
  prefs: []
  type: TYPE_NORMAL
- en: 'The `fetch_openml()` function is a bit unusual since by default it returns
    the inputs as a Pandas DataFrame and the labels as a Pandas Series (unless the
    dataset is sparse). But the MNIST dataset contains images, and DataFrames aren’t
    ideal for that, so it’s preferable to set `as_frame=False` to get the data as
    NumPy arrays instead. Let’s look at these arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]`` `array([[0, 0, 0, ..., 0, 0, 0],`  `[0, 0, 0, ..., 0, 0, 0],`  `[0,
    0, 0, ..., 0, 0, 0],`  `...,`  `[0, 0, 0, ..., 0, 0, 0],`  `[0, 0, 0, ..., 0,
    0, 0],`  `[0, 0, 0, ..., 0, 0, 0]])` `>>>` `X``.``shape` [PRE2] `array([''5'',
    ''0'', ''4'', ..., ''4'', ''5'', ''6''], dtype=object)` `>>>` `y``.``shape` ``
    `(70000,)` `` [PRE3]` [PRE4]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]` [PRE6] [PRE7]`py [PRE8]py[PRE9]py[PRE10]`py[PRE11]py` [PRE12]`py[PRE13]py[PRE14]py[PRE15]py[PRE16]py[PRE17]py[PRE18]`py[PRE19]py[PRE20]`
    [PRE21] >>> from sklearn.metrics import f1_score `>>>` `f1_score``(``y_train_5``,`
    `y_train_pred``)` `` `0.7325171197343846` `` [PRE22]` [PRE23][PRE24][PRE25][PRE26]``py[PRE27]py`
    ## The Precision/Recall Trade-Off    To understand this trade-off, let’s look
    at how the `SGDClassifier` makes its classification decisions. For each instance,
    it computes a score based on a *decision function*. If that score is greater than
    a threshold, it assigns the instance to the positive class; otherwise it assigns
    it to the negative class. [Figure 3-4](#decision_threshold_diagram) shows a few
    digits positioned from the lowest score on the left to the highest score on the
    right. Suppose the *decision threshold* is positioned at the central arrow (between
    the two 5s): you will find 4 true positives (actual 5s) on the right of that threshold,
    and 1 false positive (actually a 6). Therefore, with that threshold, the precision
    is 80% (4 out of 5). But out of 6 actual 5s, the classifier only detects 4, so
    the recall is 67% (4 out of 6). If you raise the threshold (move it to the arrow
    on the right), the false positive (the 6) becomes a true negative, thereby increasing
    the precision (up to 100% in this case), but one true positive becomes a false
    negative, decreasing recall down to 50%. Conversely, lowering the threshold increases
    recall and reduces precision.  ![Diagram illustrating the precision/recall trade-off
    with different decision thresholds; as the threshold increases, precision generally
    increases while recall decreases.](assets/hmls_0304.png)  ###### Figure 3-4\.
    The precision/recall trade-off: images are ranked by their classifier score, and
    those above the chosen decision threshold are considered positive; the higher
    the threshold, the lower the recall, but (in general) the higher the precision    Instead
    of calling the classifier’s `predict()` method, you can call its `decision_function()`
    method, which returns a score for each instance. You can then use any threshold
    you want to make predictions based on those scores:    [PRE28]py`` `array([2164.22030239])`
    `>>>` `threshold` `=` `0` [PRE29]` [PRE30][PRE31][PRE32]``py[PRE33]`py` The `SGDClassifier`
    uses a threshold equal to 0, so the preceding code returns the same result as
    the `predict()` method (i.e., `True`). Let’s raise the threshold:    [PRE34]py
    `>>>` `y_some_digit_pred` `` `array([False])` `` [PRE35]py   [PRE36] [PRE37]``
    This confirms that raising the threshold decreases recall. The image actually
    represents a 5, and the classifier detects it when the threshold is 0, but it
    misses it when the threshold is increased to 3,000.    How do you decide which
    threshold to use? One option is to use the `cross_val_predict()` function to get
    the scores of all instances in the training set, but this time specify that you
    want to return decision scores instead of predictions:    [PRE38]    With these
    scores, use the `precision_recall_curve()` function to compute precision and recall
    for all possible thresholds (the function adds a last precision of 1 and a last
    recall of 0, corresponding to an infinite threshold):    [PRE39]    Finally, use
    Matplotlib to plot precision and recall as functions of the threshold value ([Figure 3-5](#precision_recall_vs_threshold_plot)).
    Let’s show the threshold of 3,000 we selected:    [PRE40]  ![Graph showing precision
    and recall curves as functions of decision threshold, illustrating their inverse
    relationship.](assets/hmls_0305.png)  ###### Figure 3-5\. Precision and recall
    versus the decision threshold    ###### Note    You may wonder why the precision
    curve is bumpier than the recall curve in [Figure 3-5](#precision_recall_vs_threshold_plot).
    The reason is that precision may sometimes go down when you raise the threshold
    (although in general it will go up). To understand why, look back at [Figure 3-4](#decision_threshold_diagram)
    and notice what happens when you start from the central threshold and move it
    just one digit to the right: precision goes from 4/5 (80%) down to 3/4 (75%).
    On the other hand, recall can only go down when the threshold is increased, which
    explains why its curve looks smooth.    At this threshold value, precision is
    near 90% and recall is around 50%. Another way to select a good precision/recall
    trade-off is to plot precision directly against recall, as shown in [Figure 3-6](#precision_vs_recall_plot)
    (the same threshold is shown):    [PRE41]  ![Graph showing the precision-recall
    curve with a marked point where precision drops sharply as recall approaches 80%.](assets/hmls_0306.png)  ######
    Figure 3-6\. Precision versus recall    You can see that precision really starts
    to fall sharply at around 80% recall. You will probably want to select a precision/recall
    trade-off just before that drop—for example, at around 60% recall. But of course,
    the choice depends on your project.    Suppose you decide to aim for 90% precision.
    You could use the first plot to find the threshold you need to use, but that’s
    not very precise. Alternatively, you can search for the lowest threshold that
    gives you at least 90% precision. For this, you can use the NumPy array’s `argmax()`
    method. This returns the first index of the maximum value, which in this case
    means the first `True` value:    [PRE42] `>>>` `threshold_for_90_precision` ``
    `np.float64(3370.0194991439557)` `` [PRE43]   [PRE44]` [PRE45] [PRE46]`py [PRE47]py``
    [PRE48]py[PRE49][PRE50][PRE51][PRE52]py[PRE53]py` [PRE54]`py`` [PRE55]`py[PRE56][PRE57][PRE58]
    [PRE59][PRE60][PRE61][PRE62][PRE63]`` [PRE64][PRE65][PRE66] [PRE67]`py[PRE68]py[PRE69]py[PRE70]py[PRE71]py[PRE72]`py[PRE73]py[PRE74][PRE75][PRE76]py
    >>> class_id = some_digit_scores.argmax() `>>>` `class_id` `` `np.int64(5)` ``
    [PRE77]py[PRE78]py[PRE79][PRE80] [PRE81] If you want to force Scikit-Learn to
    use one-versus-one or one-versus-the-rest, you can use the `OneVsOneClassifier`
    or `OneVsRestClassifier` classes. Simply create an instance and pass a classifier
    to its constructor (it doesn’t even have to be a binary classifier). For example,
    this code creates a multiclass classifier using the OvR strategy, based on an
    `SVC`:    [PRE82]    Let’s make a prediction, and check the number of trained
    classifiers:    [PRE83]   [PRE84][PRE85]``py[PRE86]`` [PRE87]`` [PRE88]` Oops,
    that’s incorrect. Prediction errors do happen! This time Scikit-Learn used the
    OvR strategy under the hood: since there are 10 classes, it trained 10 binary
    classifiers. The `decision_function()` method now returns one value per class.
    Let’s look at the scores that the SGD classifier assigned to each class:    [PRE89]   [PRE90]
    You can see that the classifier is not very confident about its prediction: almost
    all scores are very negative, while class 3 has a score of +1,824, and class 5
    is not too far behind at –1,386\. Of course, you’ll want to evaluate this classifier
    on more than one image. Since there are roughly the same number of images in each
    class, the accuracy metric is fine. As usual, you can use the `cross_val_score()`
    function to evaluate the model:    [PRE91]py   [PRE92]`py [PRE93]py`` [PRE94]py[PRE95][PRE96][PRE97][PRE98]py[PRE99]py`
    [PRE100]`py`` [PRE101]`py[PRE102][PRE103][PRE104] [PRE105][PRE106][PRE107][PRE108]
    [PRE109]py` # Error Analysis    If this were a real project, you would now follow
    the steps in your machine learning project checklist (see [*https://homl.info/checklist*](https://homl.info/checklist)).
    You’d explore data preparation options, try out multiple models, shortlist the
    best ones, fine-tune their hyperparameters using `GridSearchCV`, and automate
    as much as possible. Here, we will assume that you have found a promising model
    and you want to find ways to improve it. One way to do this is to analyze the
    types of errors it makes.    First, look at the confusion matrix. For this, you
    first need to make predictions using the `cross_val_predict()` function; then
    you can pass the labels and predictions to the `confusion_matrix()` function,
    just like you did earlier. However, since there are now 10 classes instead of
    2, the confusion matrix will contain quite a lot of numbers, and it may be hard
    to read.    A colored diagram of the confusion matrix is much easier to analyze.
    To plot such a diagram, use the `ConfusionMatrixDisplay.from_predictions()` function
    like this:    [PRE110]py    This produces the left diagram in [Figure 3-9](#confusion_matrix_plot_1).
    This confusion matrix looks pretty good: most images are on the main diagonal,
    which means that they were classified correctly. Notice that the cell on the diagonal
    in row #5 and column #5 looks slightly darker than the other digits. This could
    be because the model made more errors on 5s, or because there are fewer 5s in
    the dataset than the other digits. That’s why it’s important to normalize the
    confusion matrix by dividing each value by the total number of images in the corresponding
    (true) class (i.e., divide by the row’s sum). This can be done simply by setting
    `normalize="true"`. We can also specify the `values_format=".0%"` argument to
    show percentages with no decimals. The following code produces the diagram on
    the right in [Figure 3-9](#confusion_matrix_plot_1):    [PRE111]py  ![Confusion
    matrices displaying model predictions, with the left showing raw counts and the
    right normalized by row to percentage accuracy for each class.](assets/hmls_0309.png)  ######
    Figure 3-9\. Confusion matrix (left) and the same CM normalized by row (right)    Now
    we can easily see that only 82% of the images of 5s were classified correctly.
    The most common error the model made with images of 5s was to misclassify them
    as 8s: this happened for 10% of all 5s. But only 2% of 8s got misclassified as
    5s; confusion matrices are generally not symmetrical! If you look carefully, you
    will notice that many digits have been misclassified as 8s, but this is not immediately
    obvious from this diagram. If you want to make the errors stand out more, you
    can try putting zero weight on the correct predictions. The following code does
    just that and produces the diagram on the left in [Figure 3-10](#confusion_matrix_plot_2):    [PRE112]py  ![Confusion
    matrix diagrams showing error rates normalized by row on the left and by column
    on the right, highlighting common misclassification patterns.](assets/hmls_0310.png)  ######
    Figure 3-10\. Confusion matrix with errors only, normalized by row (left) and
    by column (right)    Now you can see much more clearly the kinds of errors the
    classifier makes. The column for class 8 is now really bright, which confirms
    that many images got misclassified as 8s. In fact this is the most common misclassification
    for almost all classes. But be careful how you interpret the percentages in this
    diagram: remember that we’ve excluded the correct predictions. For example, the
    36% in row #7, column #9 in the left grid does *not* mean that 36% of all images
    of 7s were misclassified as 9s. It means that 36% of the *errors* the model made
    on images of 7s were misclassifications as 9s. In reality, only 3% of images of
    7s were misclassified as 9s, as you can see in the diagram on the right in [Figure 3-9](#confusion_matrix_plot_1).    It
    is also possible to normalize the confusion matrix by column rather than by row:
    if you set `normalize="pred"`, you get the diagram on the right in [Figure 3-10](#confusion_matrix_plot_2).
    For example, you can see that 56% of misclassified 7s are actually 9s.    Analyzing
    the confusion matrix often gives you insights into ways to improve your classifier.
    Looking at these plots, it seems that your efforts should be spent on reducing
    the false 8s. For example, you could try to gather more training data for digits
    that look like 8s (but are not) so that the classifier can learn to distinguish
    them from real 8s. Or you could engineer new features that would help the classifier—for
    example, writing an algorithm to count the number of closed loops (e.g., 8 has
    two, 6 has one, 5 has none). Or you could preprocess the images (e.g., using Scikit-Image,
    Pillow, or OpenCV) to make some patterns, such as closed loops, stand out more.    Analyzing
    individual errors can also be a good way to gain insights into what your classifier
    is doing and why it is failing. For example, let’s plot examples of 3s and 5s
    in a confusion matrix style ([Figure 3-11](#error_analysis_digits_plot)):    [PRE113]py  ![Confusion
    matrix showing images of handwritten digits 3 and 5, highlighting misclassifications
    by a simple classifier.](assets/hmls_0311.png)  ###### Figure 3-11\. Some images
    of 3s and 5s organized like a confusion matrix    As you can see, some of the
    digits that the classifier gets wrong (i.e., in the bottom-left and top-right
    blocks) are so badly written that even a human would have trouble classifying
    them. However, most misclassified images seem like obvious errors to us. It may
    be hard to understand why the classifier made the mistakes it did, but remember
    that the human brain is a fantastic pattern recognition system, and our visual
    system does a lot of complex preprocessing before any information even reaches
    our consciousness. So, the fact that this task feels simple does not mean that
    it is. Recall that we used a simple `SGDClassifier`, which is just a linear model:
    all it does is assign a weight per class to each pixel, and when it sees a new
    image it just sums up the weighted pixel intensities to get a score for each class.
    Since 3s and 5s differ by only a few pixels, this model will easily confuse them.    The
    main difference between 3s and 5s is the position of the small line that joins
    the top line to the bottom arc. If you draw a 3 with the junction slightly shifted
    to the left, the classifier might classify it as a 5, and vice versa. In other
    words, this classifier is quite sensitive to image shifting and rotation. One
    way to reduce the 3/5 confusion is to preprocess the images to ensure that they
    are well centered and not too rotated. However, this may not be easy since it
    requires predicting the correct rotation of each image. A much simpler approach
    consists of augmenting the training set with slightly shifted and rotated variants
    of the training images. This will force the model to learn to be more tolerant
    to such variations. This is called *data augmentation* (we’ll cover this in [Chapter 12](ch12.html#cnn_chapter);
    also see exercise 2 at the end of this chapter).    # Multilabel Classification    Until
    now, each instance has always been assigned to just one class. But in some cases
    you may want your classifier to output multiple classes for each instance. Consider
    a face-recognition classifier: what should it do if it recognizes several people
    in the same picture? It should attach one tag per person it recognizes. Say the
    classifier has been trained to recognize three faces: Alice, Bob, and Charlie.
    Then when the classifier is shown a picture of Alice and Charlie, it should output
    `[True, False, True]` (meaning “Alice yes, Bob no, Charlie yes”). Such a classification
    system that outputs multiple binary tags is called a *multilabel classification*
    system.    We won’t go into face recognition just yet, but let’s look at a simpler
    example, just for illustration purposes:    [PRE114]py    This code creates a
    `y_multilabel` array containing two target labels for each digit image: the first
    indicates whether the digit is large (7, 8, or 9), and the second indicates whether
    it is odd. Then the code creates a `KNeighborsClassifier` instance, which supports
    multilabel classification (not all classifiers do), and trains this model using
    the multiple targets array. Now you can make a prediction, and notice that it
    outputs two labels:    [PRE115]py   [PRE116]py  [PRE117]py [PRE118]py`` [PRE119]py[PRE120]py[PRE121]py[PRE122]py[PRE123]py[PRE124]py[PRE125]py`
    [PRE126]`py[PRE127]'
  prefs: []
  type: TYPE_NORMAL
