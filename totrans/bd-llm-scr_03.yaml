- en: 4 Implementing a GPT model from scratch to generate text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Coding a GPT-like large language model (LLM) that can be trained to generate
    human-like text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalizing layer activations to stabilize neural network training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding shortcut connections in deep neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing transformer blocks to create GPT models of various sizes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing the number of parameters and storage requirements of GPT models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You‚Äôve already learned and coded the *multi-head attention* mechanism, one of
    the core components of LLMs. Now, we will code the other building blocks of an
    LLM and assemble them into a GPT-like model that we will train in the next chapter
    to generate human-like text.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/4-1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1 The three main stages of coding an LLM. This chapter focuses on
    step 3 of stage 1: implementing the LLM architecture.'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The LLM architecture referenced in figure 4.1, consists of several building
    blocks. We will begin with a top-down view of the model architecture before covering
    the individual components in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Coding an LLM architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs, such as GPT (which stands for *generative pretrained transformer*), are
    large deep neural network architectures designed to generate new text one word
    (or token) at a time. However, despite their size, the model architecture is less
    complicated than you might think, since many of its components are repeated, as
    we will see later. Figure 4.2 provides a top-down view of a GPT-like LLM, with
    its main components highlighted.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/4-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 A GPT model. In addition to the embedding layers, it consists of
    one or more transformer blocks containing the masked multi-head attention module
    we previously implemented.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We have already covered several aspects of the LLM architecture, such as input
    tokenization and embedding and the masked multi-head attention module. Now, we
    will implement the core structure of the GPT model, including its *transformer
    blocks*, which we will later train to generate human-like text.
  prefs: []
  type: TYPE_NORMAL
- en: Previously, we used smaller embedding dimensions for simplicity, ensuring that
    the concepts and examples could comfortably fit on a single page. Now, we are
    scaling up to the size of a small GPT-2 model, specifically the smallest version
    with 124 million parameters, as described in ‚ÄúLanguage Models Are Unsupervised
    Multitask Learners,‚Äù by Radford et al. ([https://mng.bz/yoBq](https://mng.bz/yoBq)).
    Note that while the original report mentions 117 million parameters, this was
    later corrected. In chapter 6, we will focus on loading pretrained weights into
    our implementation and adapting it for larger GPT-2 models with 345, 762, and
    1,542 million parameters.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of deep learning and LLMs like GPT, the term ‚Äúparameters‚Äù refers
    to the trainable weights of the model. These weights are essentially the internal
    variables of the model that are adjusted and optimized during the training process
    to minimize a specific loss function. This optimization allows the model to learn
    from the training data.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in a neural network layer that is represented by a 2,048 √ó 2,048‚Äìdimensional
    matrix (or tensor) of weights, each element of this matrix is a parameter. Since
    there are 2,048 rows and 2,048 columns, the total number of parameters in this
    layer is 2,048 multiplied by 2,048, which equals 4,194,304 parameters.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-2 vs. GPT-3
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Note that we are focusing on GPT-2 because OpenAI has made the weights of the
    pretrained model publicly available, which we will load into our implementation
    in chapter 6\. GPT-3 is fundamentally the same in terms of model architecture,
    except that it is scaled up from 1.5 billion parameters in GPT-2 to 175 billion
    parameters in GPT-3, and it is trained on more data. As of this writing, the weights
    for GPT-3 are not publicly available. GPT-2 is also a better choice for learning
    how to implement LLMs, as it can be run on a single laptop computer, whereas GPT-3
    requires a GPU cluster for training and inference. According to Lambda Labs ([https://lambdalabs.com/](https://lambdalabs.com/)),
    it would take 355 years to train GPT-3 on a single V100 datacenter GPU and 665
    years on a consumer RTX 8000 GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'We specify the configuration of the small GPT-2 model via the following Python
    dictionary, which we will use in the code examples later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `GPT_CONFIG_124M` dictionary, we use concise variable names for clarity
    and to prevent long lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_size` refers to a vocabulary of 50,257 words, as used by the BPE tokenizer
    (see chapter 2).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_length` denotes the maximum number of input tokens the model can handle
    via the positional embeddings (see chapter 2).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`emb_dim` represents the embedding size, transforming each token into a 768-dimensional
    vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_heads` indicates the count of attention heads in the multi-head attention
    mechanism (see chapter 3).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_layers` specifies the number of transformer blocks in the model, which we
    will cover in the upcoming discussion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`drop_rate` indicates the intensity of the dropout mechanism (0.1 implies a
    10% random drop out of hidden units) to prevent overfitting (see chapter 3).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`qkv_bias` determines whether to include a bias vector in the `Linear` layers
    of the multi-head attention for query, key, and value computations. We will initially
    disable this, following the norms of modern LLMs, but we will revisit it in chapter
    6 when we load pretrained GPT-2 weights from OpenAI into our model (see chapter
    6).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using this configuration, we will implement a GPT placeholder architecture (`DummyGPTModel`),
    as shown in figure 4.3\. This will provide us with a big-picture view of how everything
    fits together and what other components we need to code to assemble the full GPT
    model architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/4-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 The order in which we code the GPT architecture. We start with the
    GPT backbone, a placeholder architecture, before getting to the individual core
    pieces and eventually assembling them in a transformer block for the final GPT
    architecture.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The numbered boxes in figure 4.3 illustrate the order in which we tackle the
    individual concepts required to code the final GPT architecture. We will start
    with step 1, a placeholder GPT backbone we will call `DummyGPTModel`.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.1 A placeholder GPT model architecture class
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Uses a placeholder for TransformerBlock'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Uses a placeholder for LayerNorm'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 A simple placeholder class that will be replaced by a real TransformerBlock
    later'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 This block does nothing and just returns its input.'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 A simple placeholder class that will be replaced by a real LayerNorm later'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 The parameters here are just to mimic the LayerNorm interface.'
  prefs: []
  type: TYPE_NORMAL
- en: The `DummyGPTModel` class in this code defines a simplified version of a GPT-like
    model using PyTorch‚Äôs neural network module (`nn.Module`). The model architecture
    in the `DummyGPTModel` class consists of token and positional embeddings, dropout,
    a series of transformer blocks (`DummyTransformerBlock`), a final layer normalization
    (`DummyLayerNorm`), and a linear output layer (`out_head`). The configuration
    is passed in via a Python dictionary, for instance, the `GPT_CONFIG_124M` dictionary
    we created earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `forward` method describes the data flow through the model: it computes
    token and positional embeddings for the input indices, applies dropout, processes
    the data through the transformer blocks, applies normalization, and finally produces
    logits with the linear output layer.'
  prefs: []
  type: TYPE_NORMAL
- en: The code in listing 4.1 is already functional. However, for now, note that we
    use placeholders (`DummyLayerNorm` and `DummyTransformerBlock`) for the transformer
    block and layer normalization, which we will develop later.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will prepare the input data and initialize a new GPT model to illustrate
    its usage. Building on our coding of the tokenizer (see chapter 2), let‚Äôs now
    consider a high-level overview of how data flows in and out of a GPT model, as
    shown in figure 4.4.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/4-4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 A big-picture overview showing how the input data is tokenized, embedded,
    and fed to the GPT model. Note that in our `DummyGPTClass` coded earlier, the
    token embedding is handled inside the GPT model. In LLMs, the embedded input token
    dimension typically matches the output dimension. The output embeddings here represent
    the context vectors (see chapter 3).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To implement these steps, we tokenize a batch consisting of two text inputs
    for the GPT model using the tiktoken tokenizer from chapter 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting token IDs for the two texts are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The first row corresponds to the first text, and the second row corresponds
    to the second text.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we initialize a new 124-million-parameter `DummyGPTModel` instance and
    feed it the tokenized `batch`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The model outputs, which are commonly referred to as logits, are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The output tensor has two rows corresponding to the two text samples. Each text
    sample consists of four tokens; each token is a 50,257-dimensional vector, which
    matches the size of the tokenizer‚Äôs vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: The embedding has 50,257 dimensions because each of these dimensions refers
    to a unique token in the vocabulary. When we implement the postprocessing code,
    we will convert these 50,257-dimensional vectors back into token IDs, which we
    can then decode into words.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have taken a top-down look at the GPT architecture and its inputs
    and outputs, we will code the individual placeholders, starting with the real
    layer normalization class that will replace the `DummyLayerNorm` in the previous
    code.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Normalizing activations with layer normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training deep neural networks with many layers can sometimes prove challenging
    due to problems like vanishing or exploding gradients. These problems lead to
    unstable training dynamics and make it difficult for the network to effectively
    adjust its weights, which means the learning process struggles to find a set of
    parameters (weights) for the neural network that minimizes the loss function.
    In other words, the network has difficulty learning the underlying patterns in
    the data to a degree that would allow it to make accurate predictions or decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Note ‚ÄÉIf you are new to neural network training and the concepts of gradients,
    a brief introduction to these concepts can be found in section A.4 in appendix
    A. However, a deep mathematical understanding of gradients is not required to
    follow the contents of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs now implement *layer normalization* to improve the stability and efficiency
    of neural network training. The main idea behind layer normalization is to adjust
    the activations (outputs) of a neural network layer to have a mean of 0 and a
    variance of 1, also known as unit variance. This adjustment speeds up the convergence
    to effective weights and ensures consistent, reliable training. In GPT-2 and modern
    transformer architectures, layer normalization is typically applied before and
    after the multi-head attention module, and, as we have seen with the `DummyLayerNorm`
    placeholder, before the final output layer. Figure 4.5 provides a visual overview
    of how layer normalization functions.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/4-5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 An illustration of layer normalization where the six outputs of the
    layer, also called activations, are normalized such that they have a 0 mean and
    a variance of 1.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We can recreate the example shown in figure 4.5 via the following code, where
    we implement a neural network layer with five inputs and six outputs that we apply
    to two input examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Creates two training examples with five dimensions (features) each'
  prefs: []
  type: TYPE_NORMAL
- en: 'This prints the following tensor, where the first row lists the layer outputs
    for the first input and the second row lists the layer outputs for the second
    row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The neural network layer we have coded consists of a `Linear` layer followed
    by a nonlinear activation function, `ReLU` (short for rectified linear unit),
    which is a standard activation function in neural networks. If you are unfamiliar
    with `ReLU`, it simply thresholds negative inputs to 0, ensuring that a layer
    outputs only positive values, which explains why the resulting layer output does
    not contain any negative values. Later, we will use another, more sophisticated
    activation function in GPT.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we apply layer normalization to these outputs, let‚Äôs examine the mean
    and variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The first row in the mean tensor here contains the mean value for the first
    input row, and the second output row contains the mean for the second input row.
  prefs: []
  type: TYPE_NORMAL
- en: Using `keepdim=True` in operations like mean or variance calculation ensures
    that the output tensor retains the same number of dimensions as the input tensor,
    even though the operation reduces the tensor along the dimension specified via
    `dim`. For instance, without `keepdim=True`, the returned mean tensor would be
    a two-dimensional vector `[0.1324,` `0.2170]` instead of a 2 √ó 1‚Äìdimensional matrix
    `[[0.1324],` `[0.2170]]`.
  prefs: []
  type: TYPE_NORMAL
- en: The `dim` parameter specifies the dimension along which the calculation of the
    statistic (here, mean or variance) should be performed in a tensor. As figure
    4.6 explains, for a two-dimensional tensor (like a matrix), using `dim=-1` for
    operations such as mean or variance calculation is the same as using `dim=1`.
    This is because `-1` refers to the tensor‚Äôs last dimension, which corresponds
    to the columns in a two-dimensional tensor. Later, when adding layer normalization
    to the GPT model, which produces three-dimensional tensors with the shape `[batch_size,`
    `num_tokens,` `embedding_size]`, we can still use `dim=-1` for normalization across
    the last dimension, avoiding a change from `dim=1` to `dim=2`.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/4-6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 An illustration of the dim parameter when calculating the mean of
    a tensor. For instance, if we have a two-dimensional tensor (matrix) with dimensions
    `[rows,` `columns]`, using `dim=0` will perform the operation across rows (vertically,
    as shown at the bottom), resulting in an output that aggregates the data for each
    column. Using `dim=1` or `dim=-1` will perform the operation across columns (horizontally,
    as shown at the top), resulting in an output aggregating the data for each row.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Next, let‚Äôs apply layer normalization to the layer outputs we obtained earlier.
    The operation consists of subtracting the mean and dividing by the square root
    of the variance (also known as the standard deviation):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see based on the results, the normalized layer outputs, which now
    also contain negative values, have 0 mean and a variance of 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Note that the value ‚Äì5.9605e-08 in the output tensor is the scientific notation
    for ‚Äì5.9605 √ó 10^(-8), which is ‚Äì0.000000059605 in decimal form. This value is
    very close to 0, but it is not exactly 0 due to small numerical errors that can
    accumulate because of the finite precision with which computers represent numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'To improve readability, we can also turn off the scientific notation when printing
    tensor values by setting `sci_mode` to `False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: So far, we have coded and applied layer normalization in a step-by-step process.
    Let‚Äôs now encapsulate this process in a PyTorch module that we can use in the
    GPT model later.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.2 A layer normalization class
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This specific implementation of layer normalization operates on the last dimension
    of the input tensor x, which represents the embedding dimension (`emb_dim`). The
    variable `eps` is a small constant (epsilon) added to the variance to prevent
    division by zero during normalization. The `scale` and `shift` are two trainable
    parameters (of the same dimension as the input) that the LLM automatically adjusts
    during training if it is determined that doing so would improve the model‚Äôs performance
    on its training task. This allows the model to learn appropriate scaling and shifting
    that best suit the data it is processing.
  prefs: []
  type: TYPE_NORMAL
- en: Biased variance
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In our variance calculation method, we use an implementation detail by setting
    `unbiased=False`. For those curious about what this means, in the variance calculation,
    we divide by the number of inputs *n* in the variance formula. This approach does
    not apply Bessel‚Äôs correction, which typically uses *n* ‚Äì *1* instead of *n* in
    the denominator to adjust for bias in sample variance estimation. This decision
    results in a so-called biased estimate of the variance. For LLMs, where the embedding
    dimension *n* is significantly large, the difference between using *n* and *n*
    ‚Äì *1* is practically negligible. I chose this approach to ensure compatibility
    with the GPT-2 model‚Äôs normalization layers and because it reflects TensorFlow‚Äôs
    default behavior, which was used to implement the original GPT-2 model. Using
    a similar setting ensures our method is compatible with the pretrained weights
    we will load in chapter 6\.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs now try the `LayerNorm` module in practice and apply it to the batch
    input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The results show that the layer normalization code works as expected and normalizes
    the values of each of the two inputs such that they have a mean of 0 and a variance
    of 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We have now covered two of the building blocks we will need to implement the
    GPT architecture, as shown in figure 4.7\. Next, we will look at the GELU activation
    function, which is one of the activation functions used in LLMs, instead of the
    traditional ReLU function we used previously.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/4-7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 The building blocks necessary to build the GPT architecture. So far,
    we have completed the GPT backbone and layer normalization. Next, we will focus
    on GELU activation and the feed forward network.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Layer normalization vs. batch normalization
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If you are familiar with batch normalization, a common and traditional normalization
    method for neural networks, you may wonder how it compares to layer normalization.
    Unlike batch normalization, which normalizes across the batch dimension, layer
    normalization normalizes across the feature dimension. LLMs often require significant
    computational resources, and the available hardware or the specific use case can
    dictate the batch size during training or inference. Since layer normalization
    normalizes each input independently of the batch size, it offers more flexibility
    and stability in these scenarios. This is particularly beneficial for distributed
    training or when deploying models in environments where resources are constrained.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Implementing a feed forward network with GELU activations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we will implement a small neural network submodule used as part of the
    transformer block in LLMs. We begin by implementing the *GELU* activation function,
    which plays a crucial role in this neural network submodule.
  prefs: []
  type: TYPE_NORMAL
- en: Note For additional information on implementing neural networks in PyTorch,
    see section A.5 in appendix A.
  prefs: []
  type: TYPE_NORMAL
- en: Historically, the ReLU activation function has been commonly used in deep learning
    due to its simplicity and effectiveness across various neural network architectures.
    However, in LLMs, several other activation functions are employed beyond the traditional
    ReLU. Two notable examples are GELU (*Gaussian error linear unit*) and SwiGLU
    (*Swish-gated linear unit*).
  prefs: []
  type: TYPE_NORMAL
- en: GELU and SwiGLU are more complex and smooth activation functions incorporating
    Gaussian and sigmoid-gated linear units, respectively. They offer improved performance
    for deep learning models, unlike the simpler ReLU.
  prefs: []
  type: TYPE_NORMAL
- en: 'The GELU activation function can be implemented in several ways; the exact
    version is defined as GELU(x) = x‚ãÖùõ∑(x), where ùõ∑(x) is the cumulative distribution
    function of the standard Gaussian distribution. In practice, however, it‚Äôs common
    to implement a computationally cheaper approximation (the original GPT-2 model
    was also trained with this approximation, which was found via curve fitting):'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/Equation-eqs-4.png)'
  prefs: []
  type: TYPE_IMG
- en: In code, we can implement this function as a PyTorch module.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.3 An implementation of the GELU activation function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, to get an idea of what this GELU function looks like and how it compares
    to the ReLU function, let‚Äôs plot these functions side by side:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Creates 100 sample data points in the range ‚Äì3 to 3'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in the resulting plot in figure 4.8, ReLU (right) is a piecewise
    linear function that outputs the input directly if it is positive; otherwise,
    it outputs zero. GELU (left) is a smooth, nonlinear function that approximates
    ReLU but with a non-zero gradient for almost all negative values (except at approximately
    *x* = ‚Äì0.75).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/4-8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 The output of the GELU and ReLU plots using matplotlib. The x-axis
    shows the function inputs and the y-axis shows the function outputs.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The smoothness of GELU can lead to better optimization properties during training,
    as it allows for more nuanced adjustments to the model‚Äôs parameters. In contrast,
    ReLU has a sharp corner at zero (figure 4.18, right), which can sometimes make
    optimization harder, especially in networks that are very deep or have complex
    architectures. Moreover, unlike ReLU, which outputs zero for any negative input,
    GELU allows for a small, non-zero output for negative values. This characteristic
    means that during the training process, neurons that receive negative input can
    still contribute to the learning process, albeit to a lesser extent than positive
    inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let‚Äôs use the GELU function to implement the small neural network module,
    `FeedForward`, that we will be using in the LLM‚Äôs transformer block later.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.4 A feed forward neural network module
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the `FeedForward` module is a small neural network consisting
    of two `Linear` layers and a `GELU` activation function. In the 124-million-parameter
    GPT model, it receives the input batches with tokens that have an embedding size
    of 768 each via the `GPT_CONFIG_124M` dictionary where `GPT_CONFIG_` `124M["emb_dim"]`
    `=` `768`. Figure 4.9 shows how the embedding size is manipulated inside this
    small feed forward neural network when we pass it some inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/4-9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 An overview of the connections between the layers of the feed forward
    neural network. This neural network can accommodate variable batch sizes and numbers
    of tokens in the input. However, the embedding size for each token is determined
    and fixed when initializing the weights.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Following the example in figure 4.9, let‚Äôs initialize a new `FeedForward` module
    with a token embedding size of 768 and feed it a batch input with two samples
    and three tokens each:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Creates sample input with batch dimension 2'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, the shape of the output tensor is the same as that of the input
    tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The `FeedForward` module plays a crucial role in enhancing the model‚Äôs ability
    to learn from and generalize the data. Although the input and output dimensions
    of this module are the same, it internally expands the embedding dimension into
    a higher-dimensional space through the first linear layer, as illustrated in figure
    4.10\. This expansion is followed by a nonlinear GELU activation and then a contraction
    back to the original dimension with the second linear transformation. Such a design
    allows for the exploration of a richer representation space.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/4-10.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 An illustration of the expansion and contraction of the layer outputs
    in the feed forward neural network. First, the inputs expand by a factor of 4
    from 768 to 3,072 values. Then, the second layer compresses the 3,072 values back
    into a 768-dimensional representation.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Moreover, the uniformity in input and output dimensions simplifies the architecture
    by enabling the stacking of multiple layers, as we will do later, without the
    need to adjust dimensions between them, thus making the model more scalable.
  prefs: []
  type: TYPE_NORMAL
- en: As figure 4.11 shows, we have now implemented most of the LLM‚Äôs building blocks.
    Next, we will go over the concept of shortcut connections that we insert between
    different layers of a neural network, which are important for improving the training
    performance in deep neural network architectures.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/4-11.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 The building blocks necessary to build the GPT architecture. The
    black checkmarks indicating those we have already covered.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 4.4 Adding shortcut connections
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs discuss the concept behind *shortcut connections*, also known as skip
    or residual connections. Originally, shortcut connections were proposed for deep
    networks in computer vision (specifically, in residual networks) to mitigate the
    challenge of vanishing gradients. The vanishing gradient problem refers to the
    issue where gradients (which guide weight updates during training) become progressively
    smaller as they propagate backward through the layers, making it difficult to
    effectively train earlier layers.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/4-12.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 A comparison between a deep neural network consisting of five layers
    without (left) and with shortcut connections (right). Shortcut connections involve
    adding the inputs of a layer to its outputs, effectively creating an alternate
    path that bypasses certain layers. The gradients denote the mean absolute gradient
    at each layer, which we compute in listing 4.5.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Figure 4.12 shows that a shortcut connection creates an alternative, shorter
    path for the gradient to flow through the network by skipping one or more layers,
    which is achieved by adding the output of one layer to the output of a later layer.
    This is why these connections are also known as skip connections. They play a
    crucial role in preserving the flow of gradients during the backward pass in training.
  prefs: []
  type: TYPE_NORMAL
- en: In the following list, we implement the neural network in figure 4.12 to see
    how we can add shortcut connections in the `forward` method.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.5 A neural network to illustrate shortcut connections
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Implements five layers'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Compute the output of the current layer'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Check if shortcut can be applied'
  prefs: []
  type: TYPE_NORMAL
- en: The code implements a deep neural network with five layers, each consisting
    of a `Linear` layer and a `GELU` activation function. In the forward pass, we
    iteratively pass the input through the layers and optionally add the shortcut
    connections if the `self.use_ shortcut` attribute is set to `True`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs use this code to initialize a neural network without shortcut connections.
    Each layer will be initialized such that it accepts an example with three input
    values and returns three output values. The last layer returns a single output
    value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Specifies random seed for the initial weights for reproducibility'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we implement a function that computes the gradients in the model‚Äôs backward
    pass:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Forward pass'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Calculates loss based on how close the target and output are'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Backward pass to calculate the gradients'
  prefs: []
  type: TYPE_NORMAL
- en: This code specifies a loss function that computes how close the model output
    and a user-specified target (here, for simplicity, the value 0) are. Then, when
    calling `loss.backward()`, PyTorch computes the loss gradient for each layer in
    the model. We can iterate through the weight parameters via `model.named_parameters()`.
    Suppose we have a 3 √ó 3 weight parameter matrix for a given layer. In that case,
    this layer will have 3 √ó 3 gradient values, and we print the mean absolute gradient
    of these 3 √ó 3 gradient values to obtain a single gradient value per layer to
    compare the gradients between layers more easily.
  prefs: []
  type: TYPE_NORMAL
- en: In short, the `.backward()` method is a convenient method in PyTorch that computes
    loss gradients, which are required during model training, without implementing
    the math for the gradient calculation ourselves, thereby making working with deep
    neural networks much more accessible.
  prefs: []
  type: TYPE_NORMAL
- en: Note ‚ÄÉIf you are unfamiliar with the concept of gradients and neural network
    training, I recommend reading sections A.4 and A.7 in appendix A.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs now use the `print_gradients` function and apply it to the model without
    skip connections:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The output of the `print_gradients` function shows, the gradients become smaller
    as we progress from the last layer (`layers.4`) to the first layer (`layers.0`),
    which is a phenomenon called the *vanishing gradient problem*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs now instantiate a model with skip connections and see how it compares:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The last layer `(layers.4`) still has a larger gradient than the other layers.
    However, the gradient value stabilizes as we progress toward the first layer (`layers.0`)
    and doesn‚Äôt shrink to a vanishingly small value.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, shortcut connections are important for overcoming the limitations
    posed by the vanishing gradient problem in deep neural networks. Shortcut connections
    are a core building block of very large models such as LLMs, and they will help
    facilitate more effective training by ensuring consistent gradient flow across
    layers when we train the GPT model in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we‚Äôll connect all of the previously covered concepts (layer normalization,
    GELU activations, feed forward module, and shortcut connections) in a transformer
    block, which is the final building block we need to code the GPT architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Connecting attention and linear layers in a transformer block
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let‚Äôs implement the *transformer block*, a fundamental building block
    of GPT and other LLM architectures. This block, which is repeated a dozen times
    in the 124-million-parameter GPT-2 architecture, combines several concepts we
    have previously covered: multi-head attention, layer normalization, dropout, feed
    forward layers, and GELU activations. Later, we will connect this transformer
    block to the remaining parts of the GPT architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/4-13.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.13 An illustration of a transformer block. Input tokens have been embedded
    into 768-dimensional vectors. Each row corresponds to one token‚Äôs vector representation.
    The outputs of the transformer block are vectors of the same dimension as the
    input, which can then be fed into subsequent layers in an LLM.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Figure 4.13 shows a transformer block that combines several components, including
    the masked multi-head attention module (see chapter 3) and the `FeedForward` module
    we previously implemented (see section 4.3). When a transformer block processes
    an input sequence, each element in the sequence (for example, a word or subword
    token) is represented by a fixed-size vector (in this case, 768 dimensions). The
    operations within the transformer block, including multi-head attention and feed
    forward layers, are designed to transform these vectors in a way that preserves
    their dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: The idea is that the self-attention mechanism in the multi-head attention block
    identifies and analyzes relationships between elements in the input sequence.
    In contrast, the feed forward network modifies the data individually at each position.
    This combination not only enables a more nuanced understanding and processing
    of the input but also enhances the model‚Äôs overall capacity for handling complex
    data patterns.
  prefs: []
  type: TYPE_NORMAL
- en: We can create the `TransformerBlock` in code.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.6 The transformer block component of GPT
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Shortcut connection for attention block'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Add the original input back'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Shortcut connection for feed forward block'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Adds the original input back'
  prefs: []
  type: TYPE_NORMAL
- en: The given code defines a `TransformerBlock` class in PyTorch that includes a
    multi-head attention mechanism (`MultiHeadAttention`) and a feed forward network
    (`FeedForward`), both configured based on a provided configuration dictionary
    (`cfg`), such as `GPT_CONFIG_124M`.
  prefs: []
  type: TYPE_NORMAL
- en: Layer normalization (`LayerNorm`) is applied before each of these two components,
    and dropout is applied after them to regularize the model and prevent overfitting.
    This is also known as *Pre-LayerNorm*. Older architectures, such as the original
    transformer model, applied layer normalization after the self-attention and feed
    forward networks instead, known as *Post-LayerNorm*, which often leads to worse
    training dynamics.
  prefs: []
  type: TYPE_NORMAL
- en: The class also implements the forward pass, where each component is followed
    by a shortcut connection that adds the input of the block to its output. This
    critical feature helps gradients flow through the network during training and
    improves the learning of deep models (see section 4.4).
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `GPT_CONFIG_124M` dictionary we defined earlier, let‚Äôs instantiate
    a transformer block and feed it some sample data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Creates sample input of shape [batch_size, num_tokens, emb_dim]'
  prefs: []
  type: TYPE_NORMAL
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the transformer block maintains the input dimensions in its output,
    indicating that the transformer architecture processes sequences of data without
    altering their shape throughout the network.
  prefs: []
  type: TYPE_NORMAL
- en: The preservation of shape throughout the transformer block architecture is not
    incidental but a crucial aspect of its design. This design enables its effective
    application across a wide range of sequence-to-sequence tasks, where each output
    vector directly corresponds to an input vector, maintaining a one-to-one relationship.
    However, the output is a context vector that encapsulates information from the
    entire input sequence (see chapter 3). This means that while the physical dimensions
    of the sequence (length and feature size) remain unchanged as it passes through
    the transformer block, the content of each output vector is re-encoded to integrate
    contextual information from across the entire input sequence.
  prefs: []
  type: TYPE_NORMAL
- en: With the transformer block implemented, we now have all the building blocks
    needed to implement the GPT architecture. As illustrated in figure 4.14, the transformer
    block combines layer normalization, the feed forward network, GELU activations,
    and shortcut connections. As we will eventually see, this transformer block will
    make up the main component of the GPT architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/4-14.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.14 The building blocks necessary to build the GPT architecture. The
    black checks indicate the blocks we have completed.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 4.6 Coding the GPT model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We started this chapter with a big-picture overview of a GPT architecture that
    we called `DummyGPTModel`. In this `DummyGPTModel` code implementation, we showed
    the input and outputs to the GPT model, but its building blocks remained a black
    box using a `DummyTransformerBlock` and `DummyLayerNorm` class as placeholders.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs now replace the `DummyTransformerBlock` and `DummyLayerNorm` placeholders
    with the real `TransformerBlock` and `LayerNorm` classes we coded previously to
    assemble a fully working version of the original 124-million-parameter version
    of GPT-2\. In chapter 5, we will pretrain a GPT-2 model, and in chapter 6, we
    will load in the pretrained weights from OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: Before we assemble the GPT-2 model in code, let‚Äôs look at its overall structure,
    as shown in figure 4.15, which includes all the concepts we have covered so far.
    As we can see, the transformer block is repeated many times throughout a GPT model
    architecture. In the case of the 124-million-parameter GPT-2 model, it‚Äôs repeated
    12 times, which we specify via the `n_layers` entry in the `GPT_CONFIG_124M` dictionary.
    This transform block is repeated 48 times in the largest GPT-2 model with 1,542
    million parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/4-15.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.15 An overview of the GPT model architecture showing the flow of data
    through the GPT model. Starting from the bottom, tokenized text is first converted
    into token embeddings, which are then augmented with positional embeddings. This
    combined information forms a tensor that is passed through a series of transformer
    blocks shown in the center (each containing multi-head attention and feed forward
    neural network layers with dropout and layer normalization), which are stacked
    on top of each other and repeated 12 times.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The output from the final transformer block then goes through a final layer
    normalization step before reaching the linear output layer. This layer maps the
    transformer‚Äôs output to a high-dimensional space (in this case, 50,257 dimensions,
    corresponding to the model‚Äôs vocabulary size) to predict the next token in the
    sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs now code the architecture in figure 4.15.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.7 The GPT model architecture implementation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The device setting will allow us to train the model on a CPU or GPU, depending
    on which device the input data sits on.'
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to the `TransformerBlock` class, the `GPTModel` class is relatively small
    and compact.
  prefs: []
  type: TYPE_NORMAL
- en: The `__init__` constructor of this `GPTModel` class initializes the token and
    positional embedding layers using the configurations passed in via a Python dictionary,
    `cfg`. These embedding layers are responsible for converting input token indices
    into dense vectors and adding positional information (see chapter 2).
  prefs: []
  type: TYPE_NORMAL
- en: Next, the `__init__` method creates a sequential stack of `TransformerBlock`
    modules equal to the number of layers specified in `cfg`. Following the transformer
    blocks, a `LayerNorm` layer is applied, standardizing the outputs from the transformer
    blocks to stabilize the learning process. Finally, a linear output head without
    bias is defined, which projects the transformer‚Äôs output into the vocabulary space
    of the tokenizer to generate logits for each token in the vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: The forward method takes a batch of input token indices, computes their embeddings,
    applies the positional embeddings, passes the sequence through the transformer
    blocks, normalizes the final output, and then computes the logits, representing
    the next token‚Äôs unnormalized probabilities. We will convert these logits into
    tokens and text outputs in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs now initialize the 124-million-parameter GPT model using the `GPT_CONFIG_
    124M` dictionary we pass into the `cfg` parameter and feed it with the batch text
    input we previously created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This code prints the contents of the input batch followed by the output tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Token IDs of text 1'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Token IDs of text 2'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the output tensor has the shape `[2,` `4,` `50257]`, since we
    passed in two input texts with four tokens each. The last dimension, `50257`,
    corresponds to the vocabulary size of the tokenizer. Later, we will see how to
    convert each of these 50,257-dimensional output vectors back into tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we move on to coding the function that converts the model outputs into
    text, let‚Äôs spend a bit more time with the model architecture itself and analyze
    its size. Using the `numel()` method, short for ‚Äúnumber of elements,‚Äù we can collect
    the total number of parameters in the model‚Äôs parameter tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The result is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Now, a curious reader might notice a discrepancy. Earlier, we spoke of initializing
    a 124-million-parameter GPT model, so why is the actual number of parameters 163
    million?
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason is a concept called *weight tying*, which was used in the original
    GPT-2 architecture. It means that the original GPT-2 architecture reuses the weights
    from the token embedding layer in its output layer. To understand better, let‚Äôs
    take a look at the shapes of the token embedding layer and linear output layer
    that we initialized on the `model` via the `GPTModel` earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see from the print outputs, the weight tensors for both these layers
    have the same shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The token embedding and output layers are very large due to the number of rows
    for the 50,257 in the tokenizer‚Äôs vocabulary. Let‚Äôs remove the output layer parameter
    count from the total GPT-2 model count according to the weight tying:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the model is now only 124 million parameters large, matching
    the original size of the GPT-2 model.
  prefs: []
  type: TYPE_NORMAL
- en: Weight tying reduces the overall memory footprint and computational complexity
    of the model. However, in my experience, using separate token embedding and output
    layers results in better training and model performance; hence, we use separate
    layers in our `GPTModel` implementation. The same is true for modern LLMs. However,
    we will revisit and implement the weight tying concept later in chapter 6 when
    we load the pretrained weights from OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 4.1 Number of parameters in feed forward and attention modules
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Calculate and compare the number of parameters that are contained in the feed
    forward module and those that are contained in the multi-head attention module.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, let‚Äôs compute the memory requirements of the 163 million parameters
    in our `GPTModel` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Calculates the total size in bytes (assuming float32, 4 bytes per parameter)'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Converts to megabytes'
  prefs: []
  type: TYPE_NORMAL
- en: The result is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: In conclusion, by calculating the memory requirements for the 163 million parameters
    in our `GPTModel` object and assuming each parameter is a 32-bit float taking
    up 4 bytes, we find that the total size of the model amounts to 621.83 MB, illustrating
    the relatively large storage capacity required to accommodate even relatively
    small LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we‚Äôve implemented the `GPTModel` architecture and saw that it outputs
    numeric tensors of shape `[batch_size,` `num_tokens,` `vocab_size]`, let‚Äôs write
    the code to convert these output tensors into text.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 4.2 Initializing larger GPT models
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We initialized a 124-million-parameter GPT model, which is known as ‚ÄúGPT-2 small.‚Äù
    Without making any code modifications besides updating the configuration file,
    use the `GPTModel` class to implement GPT-2 medium (using 1,024-dimensional embeddings,
    24 transformer blocks, 16 multi-head attention heads), GPT-2 large (1,280-dimensional
    embeddings, 36 transformer blocks, 20 multi-head attention heads), and GPT-2 XL
    (1,600-dimensional embeddings, 48 transformer blocks, 25 multi-head attention
    heads). As a bonus, calculate the total number of parameters in each GPT model.
  prefs: []
  type: TYPE_NORMAL
- en: 4.7 Generating text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will now implement the code that converts the tensor outputs of the GPT model
    back into text. Before we get started, let‚Äôs briefly review how a generative model
    like an LLM generates text one word (or token) at a time.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/4-16.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.16 The step-by-step process by which an LLM generates text, one token
    at a time. Starting with an initial input context (‚ÄúHello, I am‚Äù), the model predicts
    a subsequent token during each iteration, appending it to the input context for
    the next round of prediction. As shown, the first iteration adds ‚Äúa,‚Äù the second
    ‚Äúmodel,‚Äù and the third ‚Äúready,‚Äù progressively building the sentence.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Figure 4.16 illustrates the step-by-step process by which a GPT model generates
    text given an input context, such as ‚ÄúHello, I am.‚Äù With each iteration, the input
    context grows, allowing the model to generate coherent and contextually appropriate
    text. By the sixth iteration, the model has constructed a complete sentence: ‚ÄúHello,
    I am a model ready to help.‚Äù We‚Äôve seen that our current `GPTModel` implementation
    outputs tensors with shape `[batch_size,` `num_token,` `vocab_size]`. Now the
    question is: How does a GPT model go from these output tensors to the generated
    text?'
  prefs: []
  type: TYPE_NORMAL
- en: The process by which a GPT model goes from output tensors to generated text
    involves several steps, as illustrated in figure 4.17\. These steps include decoding
    the output tensors, selecting tokens based on a probability distribution, and
    converting these tokens into human-readable text.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/4-17.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.17 The mechanics of text generation in a GPT model by showing a single
    iteration in the token generation process. The process begins by encoding the
    input text into token IDs, which are then fed into the GPT model. The outputs
    of the model are then converted back into text and appended to the original input
    text.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The next-token generation process detailed in figure 4.17 illustrates a single
    step where the GPT model generates the next token given its input. In each step,
    the model outputs a matrix with vectors representing potential next tokens. The
    vector corresponding to the next token is extracted and converted into a probability
    distribution via the `softmax` function. Within the vector containing the resulting
    probability scores, the index of the highest value is located, which translates
    to the token ID. This token ID is then decoded back into text, producing the next
    token in the sequence. Finally, this token is appended to the previous inputs,
    forming a new input sequence for the subsequent iteration. This step-by-step process
    enables the model to generate text sequentially, building coherent phrases and
    sentences from the initial input context.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, we repeat this process over many iterations, such as shown in figure
    4.16, until we reach a user-specified number of generated tokens. In code, we
    can implement the token-generation process as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.8 A function for the GPT model to generate text
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '#1 idx is a (batch, n_tokens) array of indices in the current context.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Crops current context if it exceeds the supported context size, e.g., if
    LLM supports only 5 tokens, and the context size is 10, then only the last 5 tokens
    are used as context'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Focuses only on the last time step, so that (batch, n_token, vocab_size)
    becomes (batch, vocab_size)'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 probas has shape (batch, vocab_size).'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 idx_next has shape (batch, 1).'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Appends sampled index to the running sequence, where idx has shape (batch,
    n_tokens+1)'
  prefs: []
  type: TYPE_NORMAL
- en: This code demonstrates a simple implementation of a generative loop for a language
    model using PyTorch. It iterates for a specified number of new tokens to be generated,
    crops the current context to fit the model‚Äôs maximum context size, computes predictions,
    and then selects the next token based on the highest probability prediction.
  prefs: []
  type: TYPE_NORMAL
- en: To code the `generate_text_simple` function, we use a `softmax` function to
    convert the logits into a probability distribution from which we identify the
    position with the highest value via `torch.argmax`. The `softmax` function is
    monotonic, meaning it preserves the order of its inputs when transformed into
    outputs. So, in practice, the softmax step is redundant since the position with
    the highest score in the softmax output tensor is the same position in the logit
    tensor. In other words, we could apply the `torch.argmax` function to the logits
    tensor directly and get identical results. However, I provide the code for the
    conversion to illustrate the full process of transforming logits to probabilities,
    which can add additional intuition so that the model generates the most likely
    next token, which is known as *greedy decoding*.
  prefs: []
  type: TYPE_NORMAL
- en: When we implement the GPT training code in the next chapter, we will use additional
    sampling techniques to modify the softmax outputs such that the model doesn‚Äôt
    always select the most likely token. This introduces variability and creativity
    in the generated text.
  prefs: []
  type: TYPE_NORMAL
- en: This process of generating one token ID at a time and appending it to the context
    using the `generate_text_simple` function is further illustrated in figure 4.18\.
    (The token ID generation process for each iteration is detailed in figure 4.17.)
    We generate the token IDs in an iterative fashion. For instance, in iteration
    1, the model is provided with the tokens corresponding to ‚ÄúHello, I am,‚Äù predicts
    the next token (with ID 257, which is ‚Äúa‚Äù), and appends it to the input. This
    process is repeated until the model produces the complete sentence ‚ÄúHello, I am
    a model ready to help‚Äù after six iterations.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/4-18.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.18 The six iterations of a token prediction cycle, where the model
    takes a sequence of initial token IDs as input, predicts the next token, and appends
    this token to the input sequence for the next iteration. (The token IDs are also
    translated into their corresponding text for better understanding.)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Let‚Äôs now try out the `generate_text_simple` function with the `"Hello,` `I`
    `am"` context as model input. First, we encode the input context into token IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Adds batch dimension'
  prefs: []
  type: TYPE_NORMAL
- en: The encoded IDs are
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we put the model into `.eval()` mode. This disables random components
    like dropout, which are only used during training, and use the `generate_text_simple`
    function on the encoded input tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Disables dropout since we are not training the model'
  prefs: []
  type: TYPE_NORMAL
- en: The resulting output token IDs are
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `.decode` method of the tokenizer, we can convert the IDs back into
    text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: The model output in text format is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the model generated gibberish, which is not at all like the coherent
    text `Hello,` `I` `am` `a` `model` `ready` `to` `help`. What happened? The reason
    the model is unable to produce coherent text is that we haven‚Äôt trained it yet.
    So far, we have only implemented the GPT architecture and initialized a GPT model
    instance with initial random weights. Model training is a large topic in itself,
    and we will tackle it in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 4.3 Using separate dropout parameters
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'At the beginning of this chapter, we defined a global `drop_rate` setting in
    the `GPT_ CONFIG_124M` dictionary to set the dropout rate in various places throughout
    the `GPTModel` architecture. Change the code to specify a separate dropout value
    for the various dropout layers throughout the model architecture. (Hint: there
    are three distinct places where we used dropout layers: the embedding layer, shortcut
    layer, and multi-head attention module.)'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Layer normalization stabilizes training by ensuring that each layer‚Äôs outputs
    have a consistent mean and variance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shortcut connections are connections that skip one or more layers by feeding
    the output of one layer directly to a deeper layer, which helps mitigate the vanishing
    gradient problem when training deep neural networks, such as LLMs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformer blocks are a core structural component of GPT models, combining
    masked multi-head attention modules with fully connected feed forward networks
    that use the GELU activation function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT models are LLMs with many repeated transformer blocks that have millions
    to billions of parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT models come in various sizes, for example, 124, 345, 762, and 1,542 million
    parameters, which we can implement with the same `GPTModel` Python class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The text-generation capability of a GPT-like LLM involves decoding output tensors
    into human-readable text by sequentially predicting one token at a time based
    on a given input context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Without training, a GPT model generates incoherent text, which underscores the
    importance of model training for coherent text generation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
