- en: Chapter 2\. Introduction to TensorFlow Primitives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will introduce you to fundamental aspects of TensorFlow. In particular,
    you will learn how to perform basic computation using TensorFlow. A large part
    of this chapter will be spent introducing the concept of tensors, and discussing
    how tensors are represented and manipulated within TensorFlow. This discussion
    will necessitate a brief overview of some of the mathematical concepts that underlie
    tensorial mathematics. In particular, we’ll briefly review basic linear algebra
    and demonstrate how to perform basic linear algebraic operations with TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll follow this discussion of basic mathematics with a discussion of the differences
    between declarative and imperative programming styles. Unlike many programming
    languages, TensorFlow is largely declarative. Calling a TensorFlow operation adds
    a description of a computation to TensorFlow’s “computation graph.” In particular,
    TensorFlow code “describes” computations and doesn’t actually perform them. In
    order to run TensorFlow code, users need to create `tf.Session` objects. We introduce
    the concept of sessions and describe how users perform computations with them
    in TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: We end the chapter by discussing the notion of variables. Variables in TensorFlow
    hold tensors and allow for stateful computation that modifies variables to occur.
    We demonstrate how to create variables and update their values via TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Tensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tensors are fundamental mathematical constructs in fields such as physics and
    engineering. Historically, however, tensors have made fewer inroads in computer
    science, which has traditionally been more associated with discrete mathematics
    and logic. This state of affairs has started to change significantly with the
    advent of machine learning and its foundation on continuous, vectorial mathematics.
    Modern machine learning is founded upon the manipulation and calculus of tensors.
  prefs: []
  type: TYPE_NORMAL
- en: Scalars, Vectors, and Matrices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To start, we will give some simple examples of tensors that you might be familiar
    with. The simplest example of a tensor is a scalar, a single constant value drawn
    from the real numbers (recall that the real numbers are decimal numbers of arbitrary
    precision, with both positive and negative numbers permitted). Mathematically,
    we denote the real numbers by <math><mi>ℝ</mi></math> . More formally, we call
    a scalar a rank-0 tensor.
  prefs: []
  type: TYPE_NORMAL
- en: Aside on Fields
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Mathematically sophisticated readers will protest that it’s entirely meaningful
    to define tensors based on the complex numbers, or with binary numbers. More generally,
    it’s sufficient that the numbers come from a *field*: a mathematical collection
    of numbers where 0, 1, addition, multiplication, subtraction, and division are
    defined. Common fields include the real numbers <math alttext="double-struck upper
    R"><mi>ℝ</mi></math> , the rational numbers <math alttext="double-struck upper
    Q"><mi>ℚ</mi></math> , the complex numbers <math alttext="double-struck upper
    C"><mi>ℂ</mi></math> , and finite fields such as <math alttext="double-struck
    upper Z 2"><msub><mi>ℤ</mi> <mn>2</mn></msub></math> . For simplicity, in much
    of the discussion, we will assume real valued tensors, but substituting in values
    from other fields is entirely reasonable.'
  prefs: []
  type: TYPE_NORMAL
- en: If scalars are rank-0 tensors, what constitutes a rank-1 tensor? Formally, speaking,
    a rank-1 tensor is a vector; a list of real numbers. Traditionally, vectors are
    written as either column vectors
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mfenced open="(" close=")"><mtable><mtr><mtd><mi>a</mi></mtd></mtr>
    <mtr><mtd><mi>b</mi></mtd></mtr></mtable></mfenced></math>
  prefs: []
  type: TYPE_NORMAL
- en: or as row vectors
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mfenced open="(" close=")"><mtable><mtr><mtd><mi>a</mi></mtd>
    <mtd><mi>b</mi></mtd></mtr></mtable></mfenced></math>
  prefs: []
  type: TYPE_NORMAL
- en: Notationally, the collection of all column vectors of length 2 is denoted <math
    alttext="double-struck upper R Superscript 2 times 1"><msup><mi>ℝ</mi> <mrow><mn>2</mn><mo>×</mo><mn>1</mn></mrow></msup></math>
    while the set of all row vectors of length 2 is <math alttext="double-struck upper
    R Superscript 1 times 2"><msup><mi>ℝ</mi> <mrow><mn>1</mn><mo>×</mo><mn>2</mn></mrow></msup></math>
    . More computationally, we might say that the shape of a column vector is (2,
    1), while the shape of a row vector is (1, 2). If we don’t wish to specify whether
    a vector is a row vector or column vector, we can say it comes from the set <math
    alttext="double-struck upper R squared"><msup><mi>ℝ</mi> <mn>2</mn></msup></math>
    and has shape (2). This notion of tensor shape is quite important for understanding
    TensorFlow computations, and we will return to it later on in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: One of the simplest uses of vectors is to represent coordinates in the real
    world. Suppose that we decide on an origin point (say the position where you’re
    currently standing). Then any position in the world can be represented by three
    displacement values from your current position (left-right displacement, front-back
    displacement, up-down displacement). Thus, the set of vectors (vector space) <math
    alttext="double-struck upper R cubed"><msup><mi>ℝ</mi> <mn>3</mn></msup></math>
    can represent any position in the world.
  prefs: []
  type: TYPE_NORMAL
- en: For a different example, let’s suppose that a cat is described by its height,
    weight, and color. Then a video game cat can be represented as a vector
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mfenced open="(" close=")"><mtable><mtr><mtd><mi>height</mi></mtd></mtr>
    <mtr><mtd><mi>weight</mi></mtd></mtr> <mtr><mtd><mi>color</mi></mtd></mtr></mtable></mfenced></math>
  prefs: []
  type: TYPE_NORMAL
- en: in the space <math><msup><mi>ℝ</mi> <mn>3</mn></msup></math> . This type of
    representation is often called a *featurization*. That is, a featurization is
    a representation of a real-world entity as a vector (or more generally as a tensor).
    Nearly all machine learning algorithms operate on vectors or tensors. Thus the
    process of featurization is a critical part of any machine learning pipeline.
    Often, the featurization system can be the most sophisticated part of a machine
    learning system. Suppose we have a benzene molecule as illustrated in [Figure 2-1](#ch2-benzene).
  prefs: []
  type: TYPE_NORMAL
- en: '![images/Benzene-2D-flat.png](assets/tfdl_0201.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-1\. A representation of a benzene molecule.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: How can we transform this molecule into a vector suitable for a query to a machine
    learning system? There are a number of potential solutions to this problem, most
    of which exploit the idea of marking the presence of subfragments of the molecule.
    The presence or absence of specific subfragments is marked by setting indices
    in a binary vector (in <math alttext="StartSet 0 comma 1 EndSet Superscript n"><msup><mrow><mo>{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>}</mo></mrow>
    <mi>n</mi></msup></math> ) to 1/0, respectively. This process is illustrated in
    [Figure 2-2](#ch2-circfing).
  prefs: []
  type: TYPE_NORMAL
- en: '![images/molecular_fingerprint.jpg](assets/tfdl_0202.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-2\. Subfragments of the molecule to be featurized are selected (those
    containing OH). These fragments are hashed into indices in a fixed-length vector.
    These positions are set to 1 and all other positions are set to 0.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that this process sounds (and is) fairly complex. In fact, one of the most
    challenging aspects of building a machine learning system is deciding how to transform
    the data in question into a tensorial format. For some types of data, this transformation
    is obvious. For others (such as molecules), the transformation required can be
    quite subtle. For the practitioner of machine learning, it isn’t usually necessary
    to invent a new featurization method since the scholarly literature is extensive,
    but it will often be necessary to read research papers to understand best practices
    for transforming a new data stream.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have established that rank-0 tensors are scalars ( <math alttext="double-struck
    upper R"><mi>ℝ</mi></math> ) and that rank-1 tensors are vectors ( <math alttext="double-struck
    upper R Superscript n"><msup><mi>ℝ</mi> <mi>n</mi></msup></math> ), what is a
    rank-2 tensor? Traditionally, a rank-2 tensor is referred to as a matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mfenced open="(" close=")"><mtable><mtr><mtd><mi>a</mi></mtd>
    <mtd><mi>b</mi></mtd></mtr> <mtr><mtd><mi>c</mi></mtd> <mtd><mi>d</mi></mtd></mtr></mtable></mfenced></math>
  prefs: []
  type: TYPE_NORMAL
- en: This matrix has two rows and two columns. The set of all such matrices is referred
    to as <math alttext="double-struck upper R Superscript 2 times 2"><msup><mi>ℝ</mi>
    <mrow><mn>2</mn><mo>×</mo><mn>2</mn></mrow></msup></math> . Returning to our notion
    of tensor shape earlier, the shape of this matrix is (2, 2). Matrices are traditionally
    used to represent transformations of vectors. For example, the action of rotating
    a vector in the plane by angle α can be performed by the matrix
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>R</mi> <mi>α</mi></msub> <mo>=</mo> <mfenced
    open="(" close=")"><mtable><mtr><mtd><mrow><mo form="prefix">cos</mo> <mo>(</mo>
    <mi>α</mi> <mo>)</mo></mrow></mtd> <mtd><mrow><mo form="prefix">–sin</mo> <mo>(</mo>
    <mi>α</mi> <mo>)</mo></mrow></mtd></mtr> <mtr><mtd><mrow><mo form="prefix">sin</mo>
    <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd> <mtd><mrow><mo form="prefix">cos</mo>
    <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: To see this, note that the *x* unit vector (1, 0) is transformed by matrix multiplication
    into the vector (cos (α), sin (α)). (We will cover the detailed definition of
    matrix multiplication later in the chapter, but will simply display the result
    for the moment).
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mfenced open="(" close=")"><mtable><mtr><mtd><mrow><mo
    form="prefix">cos</mo> <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd> <mtd><mrow><mo
    form="prefix">–sin</mo> <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd></mtr> <mtr><mtd><mrow><mo
    form="prefix">sin</mo> <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd> <mtd><mrow><mo
    form="prefix">cos</mo> <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd></mtr></mtable></mfenced>
    <mo>·</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><mn>1</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced> <mo>=</mo> <mfenced open="("
    close=")"><mtable><mtr><mtd><mrow><mo form="prefix">cos</mo> <mo>(</mo> <mi>α</mi>
    <mo>)</mo></mrow></mtd></mtr> <mtr><mtd><mrow><mo form="prefix">sin</mo> <mo>(</mo>
    <mi>α</mi> <mo>)</mo></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: This transformation can be visualized graphically as well. [Figure 2-3](#ch2-circparam)
    demonstrates how the final vector corresponds to a rotation of the original unit
    vector.
  prefs: []
  type: TYPE_NORMAL
- en: '![unit_circle.png](assets/tfdl_0203.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-3\. Positions on the unit circle are parameterized by cosine and sine.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Matrix Mathematics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a number of standard mathematical operations on matrices that machine
    learning programs use repeatedly. We will briefly review some of the most fundamental
    of these operations.
  prefs: []
  type: TYPE_NORMAL
- en: The matrix transpose is a convenient operation that flips a matrix around its
    diagonal. Mathematically, suppose *A* is a matrix; then the transpose matrix <math><msup><mi>A</mi>
    <mi>T</mi></msup></math> is defined by equation <math><mrow><msubsup><mi>A</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow> <mi>T</mi></msubsup> <mo>=</mo> <msub><mi>A</mi>
    <mrow><mi>j</mi><mi>i</mi></mrow></msub></mrow></math> . For example, the transpose
    of the rotation matrix <math alttext="upper R Subscript alpha"><msub><mi>R</mi>
    <mi>α</mi></msub></math> is
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msubsup><mi>R</mi> <mrow><mi>α</mi></mrow> <mi>T</mi></msubsup>
    <mo>=</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><mrow><mo form="prefix">cos</mo>
    <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd> <mtd><mrow><mo form="prefix">sin</mo>
    <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd></mtr> <mtr><mtd><mrow><mo form="prefix">–sin</mo>
    <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd> <mtd><mrow><mo form="prefix">cos</mo>
    <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Addition of matrices is only defined for matrices of the same shape and is
    simply performed elementwise. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mfenced open="(" close=")"><mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><mn>2</mn></mtd></mtr> <mtr><mtd><mn>3</mn></mtd> <mtd><mn>4</mn></mtd></mtr></mtable></mfenced>
    <mo>+</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><mn>1</mn></mtd> <mtd><mn>1</mn></mtd></mtr>
    <mtr><mtd><mn>1</mn></mtd> <mtd><mn>1</mn></mtd></mtr></mtable></mfenced> <mo>=</mo>
    <mfenced open="(" close=")"><mtable><mtr><mtd><mn>2</mn></mtd> <mtd><mn>3</mn></mtd></mtr>
    <mtr><mtd><mn>4</mn></mtd> <mtd><mn>5</mn></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, matrices can be multiplied by scalars. In this case, each element
    of the matrix is simply multiplied elementwise by the scalar in question:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mn>2</mn> <mo>·</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><mn>2</mn></mtd></mtr> <mtr><mtd><mn>3</mn></mtd> <mtd><mn>4</mn></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><mn>2</mn></mtd> <mtd><mn>4</mn></mtd></mtr>
    <mtr><mtd><mn>6</mn></mtd> <mtd><mn>8</mn></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, it is sometimes possible to multiply two matrices directly. This
    notion of matrix multiplication is probably the most important mathematical concept
    associated with matrices. Note specifically that matrix multiplication is not
    the same notion as elementwise multiplication of matrices! Rather, suppose we
    have a matrix *A* of shape (*m*, *n*) with *m* rows and *n* columns. Then, *A*
    can be multiplied on the right by any matrix *B* of shape (*n*, *k*) (where *k*
    is any positive integer) to form matrix *AB* of shape (*m*, *k*). For the actual
    mathematical description, suppose *A* is a matrix of shape (*m*, *n*) and *B*
    is a matrix of shape (*n*, *k*). Then *AB* is defined by
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mrow><mo>(</mo><mi>A</mi><mi>B</mi><mo>)</mo></mrow>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub> <mo>=</mo> <munder><mo>∑</mo> <mi>k</mi></munder>
    <msub><mi>A</mi> <mrow><mi>i</mi><mi>k</mi></mrow></msub> <msub><mi>B</mi> <mrow><mi>k</mi><mi>j</mi></mrow></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'We displayed a matrix multiplication equation earlier in brief. Let’s expand
    that example now that we have the formal definition:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mfenced open="(" close=")"><mtable><mtr><mtd><mrow><mo
    form="prefix">cos</mo> <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd> <mtd><mrow><mo
    form="prefix">–sin</mo> <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd></mtr> <mtr><mtd><mrow><mo
    form="prefix">sin</mo> <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd> <mtd><mrow><mo
    form="prefix">cos</mo> <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd></mtr></mtable></mfenced>
    <mo>·</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><mn>1</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced> <mo>=</mo> <mfenced open="("
    close=")"><mtable><mtr><mtd><mrow><mo form="prefix">cos</mo> <mo>(</mo> <mi>α</mi>
    <mo>)</mo> <mo>·</mo> <mn>1</mn> <mo>–</mo> <mo form="prefix">sin</mo> <mo>(</mo>
    <mi>α</mi> <mo>)</mo> <mo>·</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mo
    form="prefix">sin</mo> <mo>(</mo> <mi>α</mi> <mo>)</mo> <mo>·</mo> <mn>1</mn>
    <mo>–</mo> <mo form="prefix">cos</mo> <mo>(</mo> <mi>α</mi> <mo>)</mo> <mo>·</mo>
    <mn>0</mn></mrow></mtd></mtr></mtable></mfenced> <mo>=</mo> <mfenced open="("
    close=")"><mtable><mtr><mtd><mrow><mo form="prefix">cos</mo> <mo>(</mo> <mi>α</mi>
    <mo>)</mo></mrow></mtd></mtr> <mtr><mtd><mrow><mo form="prefix">sin</mo> <mo>(</mo>
    <mi>α</mi> <mo>)</mo></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The fundamental takeaway is that rows of one matrix are multiplied against columns
    of the other matrix.
  prefs: []
  type: TYPE_NORMAL
- en: This definition hides a number of subtleties. Note first that matrix multiplication
    is not commutative. That is, <math><mrow><mi>A</mi> <mi>B</mi> <mo>≠</mo> <mi>B</mi>
    <mi>A</mi></mrow></math> in general. In fact, *AB* can exist when *BA* is not
    meaningful. Suppose, for example, *A* is a matrix of shape (2, 3) and *B* is a
    matrix of shape (3, 4). Then *AB* is a matrix of shape (2, 4). However, *BA* is
    not defined since the respective dimensions (4 and 2) don’t match. As another
    subtlety, note that, as in the rotation example, a matrix of shape (*m*, *n*)
    can be multiplied on the right by a matrix of shape (*n*, 1). However, a matrix
    of shape (*n*, 1) is simply a column vector. So, it is meaningful to multiply
    matrices by vectors. Matrix-vector multiplication is one of the fundamental building
    blocks of common machine learning systems.
  prefs: []
  type: TYPE_NORMAL
- en: One of the nicest properties of standard multiplication is that it is a linear
    operation. More precisely, a function *f* is called linear if <math alttext="f
    left-parenthesis x plus y right-parenthesis equals f left-parenthesis x right-parenthesis
    plus f left-parenthesis y right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mi>x</mi>
    <mo>+</mo> <mi>y</mi> <mo>)</mo> <mo>=</mo> <mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo>
    <mo>+</mo> <mi>f</mi> <mo>(</mo> <mi>y</mi> <mo>)</mo></mrow></math> and <math><mrow><mi>f</mi>
    <mo>(</mo> <mi>c</mi> <mi>x</mi> <mo>)</mo> <mo>=</mo> <mi>c</mi> <mi>f</mi> <mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow></math> where *c* is a scalar. To demonstrate that
    scalar multiplication is linear, suppose that *a*, *b*, *c*, *d* are all real
    numbers. Then we have
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>a</mi> <mo>·</mo> <mo>(</mo> <mi>b</mi> <mo>·</mo>
    <mi>c</mi> <mo>)</mo> <mo>=</mo> <mi>b</mi> <mo>·</mo> <mo>(</mo> <mi>a</mi> <mi>c</mi>
    <mo>)</mo></mrow></math><math display="block"><mrow><mi>a</mi> <mo>·</mo> <mo>(</mo>
    <mi>c</mi> <mo>+</mo> <mi>d</mi> <mo>)</mo> <mo>=</mo> <mi>a</mi> <mi>c</mi> <mo>+</mo>
    <mi>a</mi> <mi>d</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'We make use of the commutative and distributive properties of scalar multiplication
    here. Now suppose that instead, *A*, *C*, *D* are now matrices where *C*, *D*
    are of the same size and it is meaningful to multiply *A* on the right with either
    *C* or *D* (*b* remains a real number). Then matrix multiplication is a linear
    operator:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>A</mi> <mo>(</mo> <mi>b</mi> <mo>·</mo> <mi>C</mi>
    <mo>)</mo> <mo>=</mo> <mi>b</mi> <mo>·</mo> <mo>(</mo> <mi>A</mi> <mi>C</mi> <mo>)</mo></mrow></math><math
    display="block"><mrow><mi>A</mi> <mo>(</mo> <mi>C</mi> <mo>+</mo> <mi>D</mi> <mo>)</mo>
    <mo>=</mo> <mi>A</mi> <mi>C</mi> <mo>+</mo> <mi>A</mi> <mi>D</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Put another way, matrix multiplication is distributive and commutes with scalar
    multiplication. In fact, it can be shown that any linear transformation on vectors
    corresponds to a matrix multiplication. For a computer science analogy, think
    of linearity as a property demanded by an abstract method in a superclass. Then
    standard multiplication and matrix multiplication are concrete implementations
    of that abstract method for different subclasses (respectively real numbers and
    matrices).
  prefs: []
  type: TYPE_NORMAL
- en: Tensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous sections, we introduced the notion of scalars as rank-0 tensors,
    vectors as rank-1 tensors, and matrices as rank-2 tensors. What then is a rank-3
    tensor? Before passing to a general definition, it can help to think about the
    commonalities between scalars, vectors, and matrices. Scalars are single numbers.
    Vectors are lists of numbers. To pick out any particular element of a vector requires
    knowing its index. Hence, we need one index element into the vector (thus a rank-1
    tensor). Matrices are tables of numbers. To pick out any particular element of
    a matrix requires knowing its row and column. Hence, we need two index elements
    (thus a rank-2 tensor). It follows naturally that a rank-3 tensor is a set of
    numbers where there are three required indices. It can help to think of a rank-3
    tensor as a rectangular prism of numbers, as illustrated in [Figure 2-4](#ch2-rank3).
  prefs: []
  type: TYPE_NORMAL
- en: '![images/3tensor.gif](assets/tfdl_0204.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-4\. A rank-3 tensor can be visualized as a rectangular prism of numbers.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The rank-3 tensor *T* displayed in the figure is of shape (*N*, *N*, *N*). An
    arbitrary element of the tensor would then be selected by specifying (*i*, *j*,
    *k*) as indices.
  prefs: []
  type: TYPE_NORMAL
- en: There is a linkage between tensors and shapes. A rank-1 tensor has a shape of
    dimension 1, a rank-2 tensor a shape of dimension 2, and a rank-3 tensor of dimension
    3\. You might protest that this contradicts our earlier discussion of row and
    column vectors. By our definition, a column vector has shape (*n*, 1). Wouldn’t
    that make a column vector a rank-2 tensor (or a matrix)? This is exactly what
    has happened. Recall that a vector which is not specified to be a row vector or
    column vector has shape (*n*). When we specify that a vector is a row vector or
    a column vector, we in fact specify a method of transforming the underlying vector
    into a matrix. This type of dimension expansion is a common trick in tensor manipulation.
  prefs: []
  type: TYPE_NORMAL
- en: Note that another way of thinking about a rank-3 tensor is as a list of matrices
    all with the same shape. Suppose that *W* is a matrix with shape (*n*, *n*). Then
    the tensor <math alttext="upper T Subscript i j k Baseline equals left-parenthesis
    upper W 1 comma ellipsis comma upper W Subscript n Baseline right-parenthesis"><mrow><msub><mi>T</mi>
    <mrow><mi>i</mi><mi>j</mi><mi>k</mi></mrow></msub> <mo>=</mo> <mrow><mo>(</mo>
    <msub><mi>W</mi> <mn>1</mn></msub> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msub><mi>W</mi>
    <mi>n</mi></msub> <mo>)</mo></mrow></mrow></math> consists of *n* copies of the
    matrix *W*.
  prefs: []
  type: TYPE_NORMAL
- en: Note that a black-and-white image can be represented as a rank-2 tensor. Suppose
    we have a 224 × 224-pixel black and white image. Then, pixel (*i*, *j*) is 1/0
    to encode a black/white pixel, respectively. It follows that a black and white
    image can be represented as a matrix of shape (224, 224). Now, consider a 224
    × 224 color image. The color at a particular pixel is typically represented by
    three separate RGB channels. That is, pixel (*i*, *j*) is represented as a tuple
    of numbers (*r*, *g*, *b*) that encode the amount of red, green, and blue at the
    pixel, respectively. *r*, *g*, *b* are typically integers from 0 to 255\. It follows
    now that the color image can be encoded as a rank-3 tensor of shape (224, 224,
    3). Continuing the analogy, consider a color video. Suppose that each frame of
    the video is a 224 × 224 color image. Then a minute of video (at 60 fps) would
    be a rank-4 tensor of shape (224, 224, 3, 3600). Continuing even further, a collection
    of 10 such videos would then form a rank-5 tensor of shape (10, 224, 224, 3, 3600).
    In general, tensors provide for a convenient representation of numeric data. In
    practice, it’s not common to see tensors of higher order than rank-5 tensors,
    but it’s smart to design any tensor software to allow for arbitrary tensors since
    intelligent users will always come up with use cases designers don’t consider.
  prefs: []
  type: TYPE_NORMAL
- en: Tensors in Physics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Tensors are used widely in physics to encode fundamental physical quantities.
    For example, the stress tensor is commonly used in material science to define
    the stress at a point within a material. Mathematically, the stress tensor is
    a rank-2 tensor of shape (3, 3):'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>σ</mi> <mo>=</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><msub><mi>σ</mi>
    <mn>11</mn></msub></mtd> <mtd><msub><mi>τ</mi> <mn>12</mn></msub></mtd> <mtd><msub><mi>τ</mi>
    <mn>13</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>τ</mi> <mn>21</mn></msub></mtd>
    <mtd><msub><mi>σ</mi> <mn>22</mn></msub></mtd> <mtd><msub><mi>τ</mi> <mn>23</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>τ</mi> <mn>31</mn></msub></mtd> <mtd><msub><mi>τ</mi> <mn>32</mn></msub></mtd>
    <mtd><msub><mi>σ</mi> <mn>33</mn></msub></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Then, suppose that *n* is a vector of shape (3) that encodes a direction. The
    stress <math><msup><mi>T</mi> <mi>n</mi></msup></math> in direction *n* is specified
    by the vector <math><mrow><msup><mi>T</mi> <mi>n</mi></msup> <mo>=</mo> <mi>T</mi>
    <mo>·</mo> <mi>n</mi></mrow></math> (note the matrix-vector multiplication). This
    relationship is depicted pictorially in [Figure 2-5](#ch2-stress).
  prefs: []
  type: TYPE_NORMAL
- en: '![images/stress_energy_small.png](assets/tfdl_0205.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-5\. A 3D pictorial depiction of the components of stress.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As another physical example, Einstein’s field equations of general relativity
    are commonly expressed in tensorial format:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>R</mi> <mrow><mi>μ</mi><mi>ν</mi></mrow></msub>
    <mo>-</mo> <mfrac><mn>1</mn> <mn>2</mn></mfrac> <mi>R</mi> <msub><mi>g</mi> <mrow><mi>μ</mi><mi>ν</mi></mrow></msub>
    <mo>+</mo> <mi>Λ</mi> <msub><mi>g</mi> <mrow><mi>μ</mi><mi>ν</mi></mrow></msub>
    <mo>=</mo> <mfrac><mrow><mn>8</mn><mi>π</mi><mi>G</mi></mrow> <msup><mi>c</mi>
    <mn>4</mn></msup></mfrac> <msub><mi>T</mi> <mrow><mi>μ</mi><mi>ν</mi></mrow></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Here <math alttext="upper R Subscript mu nu"><msub><mi>R</mi> <mrow><mi>μ</mi><mi>ν</mi></mrow></msub></math>
    is the Ricci curvature tensor, <math><msub><mi>g</mi> <mrow><mi>μ</mi><mi>ν</mi></mrow></msub></math>
    is the metric tensor, <math><msub><mi>T</mi> <mrow><mi>μ</mi><mi>ν</mi></mrow></msub></math>
    is the stress-energy tensor, and the remaining quantities are scalars. Note, however,
    that there’s an important subtlety distinguishing these tensors and the other
    tensors we’ve discussed previously. Quantities like the metric tensor provide
    a separate tensor (in the sense of an array of numbers) for each point in space-time
    (mathematically, the metric tensor is a tensor field). The same holds for the
    stress tensor previously discussed, and for the other tensors in these equations.
    At a given point in space-time, each of these quantities becomes a symmetric rank-2
    tensor of shape (4, 4) using our notation.
  prefs: []
  type: TYPE_NORMAL
- en: Part of the power of modern tensor calculus systems such as TensorFlow is that
    some of the mathematical machinery long used for classical physics can now be
    adapted to solve applied problems in image processing and language understanding.
    At the same time, today’s tensor calculus systems are still limited compared with
    the mathematical machinery of physicists. For example, there’s no simple way to
    talk about a quantity such as the metric tensor using TensorFlow yet. We hope
    that as tensor calculus becomes more fundamental to computer science, the situation
    will change and that systems like TensorFlow will serve as a bridge between the
    physical world and the computational world.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematical Asides
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The discussion so far in this chapter has introduced tensors informally via
    example and illustration. In our definition, a tensor is simply an array of numbers.
    It’s often convenient to view a tensor as a function instead. The most common
    definition introduces a tensor as a multilinear function from a product of vector
    spaces to the real numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>T</mi> <mo>:</mo> <msub><mi>V</mi> <mn>1</mn></msub>
    <mo>×</mo> <msub><mi>V</mi> <mn>2</mn></msub> <mo>×</mo> <mo>⋯</mo> <msub><mi>V</mi>
    <mi>n</mi></msub> <mo>→</mo> <mi>ℝ</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: This definition uses a number of terms you haven’t seen. A vector space is simply
    a collection of vectors. You’ve seen a few examples of vector spaces such as <math><msup><mi>ℝ</mi>
    <mn>3</mn></msup></math> or generally <math alttext="double-struck upper R Superscript
    n"><msup><mi>ℝ</mi> <mi>n</mi></msup></math> . We won’t lose any generality by
    holding that <math alttext="upper V Subscript i Baseline equals double-struck
    upper R Superscript d Super Subscript i"><mrow><msub><mi>V</mi> <mi>i</mi></msub>
    <mo>=</mo> <msup><mi>ℝ</mi> <msub><mi>d</mi> <mi>i</mi></msub></msup></mrow></math>
    . As we defined previously, a function *f* is linear if <math alttext="f left-parenthesis
    x plus y right-parenthesis equals f left-parenthesis x right-parenthesis plus
    f left-parenthesis y right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mi>x</mi>
    <mo>+</mo> <mi>y</mi> <mo>)</mo> <mo>=</mo> <mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo>
    <mo>+</mo> <mi>f</mi> <mo>(</mo> <mi>y</mi> <mo>)</mo></mrow></math> and <math><mrow><mi>f</mi>
    <mo>(</mo> <mi>c</mi> <mi>x</mi> <mo>)</mo> <mo>=</mo> <mi>c</mi> <mi>f</mi> <mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow></math> . A multilinear function is simply a function
    that is linear in each argument. This function can be viewed as assigning individual
    entries of a multidimensional array, when provided indices into the array as arguments.
  prefs: []
  type: TYPE_NORMAL
- en: We won’t use this more mathematical definition much in this book, but it serves
    as a useful bridge to connect the deep learning concepts you will learn about
    with the centuries of mathematical research that have been undertaken on tensors
    by the physics and mathematics communities.
  prefs: []
  type: TYPE_NORMAL
- en: Covariance and Contravariance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our definition here has swept many details under the rug that would need to
    be carefully attended to for a formal treatment. For example, we don’t touch upon
    the notion of covariant and contravariant indices here. What we call a rank-*n*
    tensor is better described as a (*p*, *q*)-tensor where *n* = *p* + *q* and *p*
    is the number of contravariant indices, and *q* the number of covariant indices.
    Matrices are (1,1)-tensors, for example. As a subtlety, there are rank-2 tensors
    that are not matrices! We won’t dig into these topics carefully here since they
    don’t crop up much in machine learning, but we encourage you to understand how
    covariance and contravariance affect the machine learning systems you construct.
  prefs: []
  type: TYPE_NORMAL
- en: Basic Computations in TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve spent the last sections covering the mathematical definitions of various
    tensors. It’s now time to cover how to create and manipulate tensors using TensorFlow.
    For this section, we recommend you follow along using an interactive Python session
    (with IPython). Many of the basic TensorFlow concepts are easiest to understand
    after experimenting with them directly.
  prefs: []
  type: TYPE_NORMAL
- en: Installing TensorFlow and Getting Started
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before continuing this section, you will need to install TensorFlow on your
    machine. The details of installation will vary depending on your particular hardware,
    so we refer you to [the official TensorFlow documentation](https://www.tensorflow.org/api_docs/)
    for more details.
  prefs: []
  type: TYPE_NORMAL
- en: Although there are frontends to TensorFlow in multiple programming languages,
    we will exclusively use the TensorFlow Python API in the remainder of this book.
    We recommend that you install [Anaconda Python](https://anaconda.org/anaconda/python),
    which packages many useful numerical libraries along with the base Python executable.
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve installed TensorFlow, we recommend that you invoke it interactively
    while you’re learning the basic API (see [Example 2-1](#ch2-interactive-session)).
    When experimenting with TensorFlow interactively, it’s convenient to use `tf.InteractiveSession()`.
    Invoking this statement within IPython (an interactive Python shell) will make
    TensorFlow behave almost imperatively, allowing beginners to play with tensors
    much more easily. You will learn about imperative versus declarative style in
    greater depth later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-1\. Initialize an interactive TensorFlow session
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The rest of the code in this section will assume that an interactive session
    has been loaded.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing Constant Tensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Until now, we’ve discussed tensors as abstract mathematical entities. However,
    a system like TensorFlow must run on a real computer, so any tensors must live
    on computer memory in order to be useful to computer programmers. TensorFlow provides
    a number of functions that instantiate basic tensors in memory. The simplest of
    these are `tf.zeros()` and `tf.ones()`. `tf.zeros()` takes a tensor shape (represented
    as a Python tuple) and returns a tensor of that shape filled with zeros. Let’s
    try invoking this command in the shell ([Example 2-2](#ch2-zeros)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-2\. Create a zeros tensor
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlow returns a reference to the desired tensor rather than the value of
    the tensor itself. To force the value of the tensor to be returned, we will use
    the method `tf.Tensor.eval()` of tensor objects ([Example 2-3](#ch2-evaluate)).
    Since we have initialized `tf.InteractiveSession()`, this method will return the
    value of the zeros tensor to us.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-3\. Evaluate the value of a tensor
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note that the evaluated value of the TensorFlow tensor is itself a Python object.
    In particular, `a.eval()` is a `numpy.ndarray` object. NumPy is a sophisticated
    numerical system for Python. We won’t attempt an in-depth discussion of NumPy
    here beyond noting that TensorFlow is designed to be compatible with NumPy conventions
    to a large degree.
  prefs: []
  type: TYPE_NORMAL
- en: We can call `tf.zeros()` and `tf.ones()` to create and display tensors of various
    sizes ([Example 2-4](#ch2-display)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-4\. Evaluate and display tensors
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: What if we’d like a tensor filled with some quantity besides 0/1? The `tf.fill()`
    method provides a nice shortcut for doing so ([Example 2-5](#ch2-fill)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-5\. Filling tensors with arbitrary values
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`tf.constant` is another function, similar to `tf.fill`, which allows for construction
    of tensors that shouldn’t change during the program execution ([Example 2-6](#ch2-constant)).'
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-6\. Creating constant tensors
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Sampling Random Tensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although working with constant tensors is convenient for testing ideas, it’s
    much more common to initialize tensors with random values. The most common way
    to do this is to sample each entry in the tensor from a random distribution. `tf.random_normal`
    allows for each entry in a tensor of specified shape to be sampled from a Normal
    distribution of specified mean and standard deviation ([Example 2-7](#ch2-randnormal)).
  prefs: []
  type: TYPE_NORMAL
- en: Symmetry Breaking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many machine learning algorithms learn by performing updates to a set of tensors
    that hold weights. These update equations usually satisfy the property that weights
    initialized at the same value will continue to evolve together. Thus, if the initial
    set of tensors is initialized to a constant value, the model won’t be capable
    of learning much. Fixing this situation requires *symmetry breaking*. The easiest
    way of breaking symmetry is to sample each entry in a tensor randomly.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-7\. Sampling a tensor with random Normal entries
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: One thing to note is that machine learning systems often make use of very large
    tensors that often have tens of millions of parameters. When we sample tens of
    millions of random values from the Normal distribution, it becomes almost certain
    that some sampled values will be far from the mean. Such large samples can lead
    to numerical instability, so it’s common to sample using `tf.truncated_normal()`
    instead of `tf.random_normal()`. This function behaves the same as `tf.random_normal()`
    in terms of API, but drops and resamples all values more than two standard deviations
    from the mean.
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.random_uniform()` behaves like `tf.random_normal()` except for the fact
    that random values are sampled from the Uniform distribution over a specified
    range ([Example 2-8](#ch2-randuniform)).'
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-8\. Sampling a tensor with uniformly random entries
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Tensor Addition and Scaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TensorFlow makes use of Python’s operator overloading to make basic tensor arithmetic
    straightforward with standard Python operators ([Example 2-9](#ch2-tensoradd)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-9\. Adding tensors together
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Tensors can also be multiplied this way. Note, however, when multiplying two
    tensors we get elementwise multiplication and not matrix multiplication, which
    can be seen in [Example 2-10](#ch2-tensormul).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-10\. Elementwise tensor multiplication
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Matrix Operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TensorFlow provides a variety of amenities for working with matrices. (Matrices
    by far are the most common type of tensor used in practice.) In particular, TensorFlow
    provides shortcuts to make certain types of commonly used matrices. The most widely
    used of these is likely the identity matrix. Identity matrices are square matrices
    that are 0 everywhere except on the diagonal, where they are 1\. `tf.eye()` allows
    for fast construction of identity matrices of desired size ([Example 2-11](#ch2-tensorid)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-11\. Creating an identity matrix
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Diagonal matrices are another common type of matrix. Like identity matrices,
    diagonal matrices are only nonzero along the diagonal. Unlike identity matrices,
    they may take arbitrary values along the diagonal. Let’s construct a diagonal
    matrix with ascending values along the diagonal ([Example 2-12](#ch2-tensordiag)).
    To start, we’ll need a method to construct a vector of ascending values in TensorFlow.
    The easiest way for doing this is invoking `tf.range(start, limit, delta)`. Note
    that `limit` is excluded from the range and `delta` is the step size for the traversal.
    The resulting vector can then be fed to `tf.diag(diagonal)`, which will construct
    a matrix with the specified diagonal.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-12\. Creating diagonal matrices
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now suppose that we have a specified matrix in TensorFlow. How do we compute
    the matrix transpose? `tf.matrix_transpose()` will do the trick nicely ([Example 2-13](#ch2-tensortrans)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-13\. Taking a matrix transpose
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s suppose we have a pair of matrices we’d like to multiply using matrix
    multiplication. The easiest way to do so is by invoking `tf.matmul()` ([Example 2-14](#ch2-tensormatmul)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-14\. Performing matrix multiplication
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: You can check that this answer matches the mathematical definition of matrix
    multiplication we provided earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Tensor Types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may have noticed the `dtype` notation in the preceding examples. Tensors
    in TensorFlow come in a variety of types such as `tf.float32`, `tf.float64`, `tf.int32`,
    `tf.int64`. It’s possible to to create tensors of specified types by setting `dtype`
    in tensor construction functions. Furthermore, given a tensor, it’s possible to
    change its type using casting functions such as `tf.to_double()`, `tf.to_float()`,
    `tf.to_int32()`, `tf.to_int64()`, and others ([Example 2-15](#ch2-tensortype)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-15\. Creating tensors of different types
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Tensor Shape Manipulations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Within TensorFlow, tensors are just collections of numbers written in memory.
    The different shapes are views into the underlying set of numbers that provide
    different ways of interacting with the set of numbers. At different times, it
    can be useful to view the same set of numbers as forming tensors with different
    shapes. `tf.reshape()` allows tensors to be converted into tensors with different
    shapes ([Example 2-16](#ch2-tensorshapes)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-16\. Manipulating tensor shapes
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Notice how we can turn the original rank-1 tensor into a rank-2 tensor and then
    into a rank-3 tensor with `tf.reshape`. While all necessary shape manipulations
    can be performed with `tf.reshape()`, sometimes it can be convenient to perform
    simpler shape manipulations using functions such as `tf.expand_dims` or `tf.squeeze`.
    `tf.expand_dims` adds an extra dimension to a tensor of size 1\. It’s useful for
    increasing the rank of a tensor by one (for example, when converting a rank-1
    vector into a rank-2 row vector or column vector). `tf.squeeze`, on the other
    hand, removes all dimensions of size 1 from a tensor. It’s a useful way to convert
    a row or column vector into a flat vector.
  prefs: []
  type: TYPE_NORMAL
- en: This is also a convenient opportunity to introduce the `tf.Tensor.get_shape()`
    method ([Example 2-17](#ch2-tensorgetshape)). This method lets users query the
    shape of a tensor.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-17\. Getting the shape of a tensor
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Introduction to Broadcasting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Broadcasting is a term (introduced by NumPy) for when a tensor system’s matrices
    and vectors of different sizes can be added together. These rules allow for conveniences
    like adding a vector to every row of a matrix. Broadcasting rules can be quite
    complex, so we will not dive into a formal discussion of the rules. It’s often
    easier to experiment and see how the broadcasting works ([Example 2-18](#ch2-tensorbroadcast)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-18\. Examples of broadcasting
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the vector `b` is added to every row of matrix `a`. Notice another
    subtlety; we explicitly set the `dtype` for `b`. If the `dtype` isn’t set, TensorFlow
    will report a type error. Let’s see what would have happened if we hadn’t set
    the `dtype` ([Example 2-19](#ch2-tensornocast)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-19\. TensorFlow doesn’t perform implicit type casting
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Unlike languages like C, TensorFlow doesn’t perform implicit type casting under
    the hood. It’s often necessary to perform explicit type casts when doing arithmetic
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: Imperative and Declarative Programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most situations in computer science involve imperative programming. Consider
    a simple Python program ([Example 2-20](#ch2-pyadd)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-20\. Python program imperatively performing an addition
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This program, when translated into machine code, instructs the machine to perform
    a primitive addition operation on two registers, one containing 3, and the other
    containing 4\. The result is then 7\. This style of programming is called *imperative*
    since the program tells the computer explicitly which actions to perform.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative style of programming is *declarative*. In a declarative system,
    a computer program is a high-level description of the computation that is to be
    performed. It does not instruct the computer exactly how to perform the computation.
    [Example 2-21](#ch2-tensorpyadd) is the TensorFlow equivalent of [Example 2-20](#ch2-pyadd).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-21\. TensorFlow program declaratively performing an addition
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Note that the value of `c` isn’t `7`! Rather, it’s a symbolic tensor. This code
    specifies the computation of adding two values together to create a new tensor.
    The actual computation isn’t executed until we call `c.eval()`. In the sections
    before, we have been using the `eval()` method to simulate imperative style in
    TensorFlow since it can be challenging to understand declarative programming at
    first.
  prefs: []
  type: TYPE_NORMAL
- en: However, declarative programming is by no means an unknown concept to software
    engineering. Relational databases and SQL provide an example of a widely used
    declarative programming system. Commands like SELECT and JOIN may be implemented
    in an arbitrary fashion under the hood so long as their basic semantics are preserved.
    TensorFlow code is best thought of as analogous to a SQL program; the TensorFlow
    code specifies a computation to be performed, with details left up to TensorFlow.
    The TensorFlow developers exploit this lack of detail under the hood to tailor
    the execution style to the underlying hardware, be it CPU, GPU, or mobile device.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that the grand weakness of declarative programming is
    that the abstraction is quite leaky. For example, without detailed understanding
    of the underlying implementation of the relational database, long SQL programs
    can become unbearably inefficient. Similarly, large TensorFlow programs implemented
    without understanding of the underlying learning algorithms are unlikely to work
    well. In the rest of this section, we will start paring back the abstraction,
    a process we will continue throughout the rest of the book.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Eager
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The TensorFlow team recently added a new experimental module, TensorFlow Eager,
    that enables users to run TensorFlow calculations imperatively. In time, this
    module will likely become the preferred entry mode for new programmers learning
    TensorFlow. However, at the timing of writing, this module is still very new with
    many rough edges. As a result, we won’t teach you about Eager mode, but encourage
    you to check it out for yourself.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to emphasize that much of TensorFlow will remain declarative
    even after Eager matures, so it’s worth learning declarative TensorFlow regardless.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Graphs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Any computation in TensorFlow is represented as an instance of a `tf.Graph`
    object. Such a graph consists of a set of instances of `tf.Tensor` objects and
    `tf.Operation` objects. We have covered `tf.Tensor` in some detail, but what are
    `tf.Operation` objects? You have already seen them over the course of this chapter.
    A call to an operation like `tf.matmul` creates a `tf.Operation` instance to mark
    the need to perform the matrix multiplication operation.
  prefs: []
  type: TYPE_NORMAL
- en: When a `tf.Graph` is not explicitly specified, TensorFlow adds tensors and operations
    to a hidden global `tf.Graph` instance. This instance can be fetched by `tf.get_default_graph()`
    ([Example 2-22](#ch2-defgraph)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-22\. Getting the default TensorFlow graph
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: It is possible to specify that TensorFlow operations should be performed in
    graphs other than the default. We will demonstrate examples of this in future
    chapters.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Sessions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In TensorFlow, a `tf.Session()` object stores the context under which a computation
    is performed. At the beginning of this chapter, we used `tf.InteractiveSession()`
    to set up an environment for all TensorFlow computations. This call created a
    hidden global context for all computations performed. We then used `tf.Tensor.eval()`
    to execute our declaratively specified computations. Underneath the hood, this
    call is evaluated in context of this hidden global `tf.Session`. It can be convenient
    (and often necessary) to use an explicit context for a computation instead of
    a hidden context ([Example 2-23](#ch2-sessions)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-23\. Explicitly manipulating TensorFlow sessions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This code evaluates `b` in the context of `sess` instead of the hidden global
    session. In fact, we can make this more explicit with an alternate notation ([Example 2-24](#ch2-runsessions)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-24\. Running a computation within a session
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: In fact, calling `b.eval(session=sess)` is just syntactic sugar for calling
    `sess.run(b)`.
  prefs: []
  type: TYPE_NORMAL
- en: This entire discussion may smack a bit of sophistry. What does it matter which
    session is in play given that all the different methods seem to return the same
    answer? Explicit sessions don’t really show their value until you start to perform
    computations that have state, a topic you will learn about in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All the example code in this section has used constant tensors. While we could
    combine and recombine these tensors in any way we chose, we could never change
    the value of tensors themselves (only create new tensors with new values). The
    style of programming so far has been *functional* and not *stateful*. While functional
    computations are very useful, machine learning often depends heavily on stateful
    computations. Learning algorithms are essentially rules for updating stored tensors
    to explain provided data. If it’s not possible to update these stored tensors,
    it would be hard to learn.
  prefs: []
  type: TYPE_NORMAL
- en: The `tf.Variable()` class provides a wrapper around tensors that allows for
    stateful computations. The variable objects serve as holders for tensors. Creating
    a variable is easy enough ([Example 2-25](#ch2-createvar)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-25\. Creating a TensorFlow variable
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: What happens when we try to evaluate the variable `a` as though it were a tensor,
    as in [Example 2-26](#ch2-vareval)?
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-26\. Evaluating an uninitialized variable fails
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The evaluation fails since variables have to be explicitly initialized. The
    easiest way to initialize all variables is to invoke `tf.global_variables_initializer`.
    Running this operation within a session will initialize all variables in the program
    ([Example 2-27](#ch2-sessioninit)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-27\. Evaluating initialized variables
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: After initialization, we are able to fetch the value stored within the variable
    as though it were a plain tensor. So far, there’s not much more to variables than
    plain tensors. Variables only become interesting once we can assign to them. `tf.assign()`
    lets us do this. Using `tf.assign()` we can update the value of an existing variable
    ([Example 2-28](#ch2-varassign)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-28\. Assigning values to variables
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: What would happen if we tried to assign a value to variable `a` not of shape
    `(2,2)`? Let’s find out in [Example 2-29](#ch2-badshapeassing).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-29\. Assignment fails when shapes aren’t equal
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: You can see that TensorFlow complains. The shape of the variable is fixed upon
    initialization and must be preserved with updates. As another interesting note,
    `tf.assign` is itself a part of the underlying global `tf.Graph` instance. This
    allows TensorFlow programs to update their internal state every time they are
    run. We will make heavy use of this feature in the chapters to come.
  prefs: []
  type: TYPE_NORMAL
- en: Review
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ve introduced the mathematical concept of tensors, and briefly
    reviewed a number of mathematical concepts associated with tensors. We then demonstrated
    how to create tensors in TensorFlow and perform these same mathematical operations
    within TensorFlow. We also briefly introduced some underlying TensorFlow structures
    like the computational graph, sessions, and variables. If you haven’t completely
    grasped the concepts discussed in this chapter, don’t worry much about it. We
    will repeatedly use these same concepts over the remainder of the book, so there
    will be plenty of chances to let the ideas sink in.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will teach you how to build simple learning models for
    linear and logistic regression using TensorFlow. Subsequent chapters will build
    on these foundations to teach you how to train more sophisticated models.
  prefs: []
  type: TYPE_NORMAL
