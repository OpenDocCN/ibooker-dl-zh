- en: Chapter 2\. Introduction to TensorFlow Primitives
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章。TensorFlow基本概念介绍
- en: This chapter will introduce you to fundamental aspects of TensorFlow. In particular,
    you will learn how to perform basic computation using TensorFlow. A large part
    of this chapter will be spent introducing the concept of tensors, and discussing
    how tensors are represented and manipulated within TensorFlow. This discussion
    will necessitate a brief overview of some of the mathematical concepts that underlie
    tensorial mathematics. In particular, we’ll briefly review basic linear algebra
    and demonstrate how to perform basic linear algebraic operations with TensorFlow.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍TensorFlow的基本概念。特别是，您将学习如何使用TensorFlow执行基本计算。本章的大部分内容将用于介绍张量的概念，并讨论在TensorFlow中如何表示和操作张量。这个讨论将需要简要概述一些支撑张量数学的数学概念。特别是，我们将简要回顾基本线性代数，并演示如何使用TensorFlow执行基本线性代数运算。
- en: We’ll follow this discussion of basic mathematics with a discussion of the differences
    between declarative and imperative programming styles. Unlike many programming
    languages, TensorFlow is largely declarative. Calling a TensorFlow operation adds
    a description of a computation to TensorFlow’s “computation graph.” In particular,
    TensorFlow code “describes” computations and doesn’t actually perform them. In
    order to run TensorFlow code, users need to create `tf.Session` objects. We introduce
    the concept of sessions and describe how users perform computations with them
    in TensorFlow.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在讨论基本数学之后讨论声明式和命令式编程风格之间的区别。与许多编程语言不同，TensorFlow主要是声明式的。调用TensorFlow操作会向TensorFlow的“计算图”中添加一个计算的描述。特别是，TensorFlow代码“描述”计算，实际上并不执行它们。为了运行TensorFlow代码，用户需要创建`tf.Session`对象。我们介绍了会话的概念，并描述了用户如何在TensorFlow中使用它们进行计算。
- en: We end the chapter by discussing the notion of variables. Variables in TensorFlow
    hold tensors and allow for stateful computation that modifies variables to occur.
    We demonstrate how to create variables and update their values via TensorFlow.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过讨论变量的概念来结束本章。TensorFlow中的变量保存张量，并允许进行有状态的计算以修改变量。我们演示如何通过TensorFlow创建变量并更新它们的值。
- en: Introducing Tensors
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍张量
- en: Tensors are fundamental mathematical constructs in fields such as physics and
    engineering. Historically, however, tensors have made fewer inroads in computer
    science, which has traditionally been more associated with discrete mathematics
    and logic. This state of affairs has started to change significantly with the
    advent of machine learning and its foundation on continuous, vectorial mathematics.
    Modern machine learning is founded upon the manipulation and calculus of tensors.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 张量是物理和工程等领域中的基本数学构造。然而，从历史上看，张量在计算机科学中的应用较少，计算机科学传统上更多地与离散数学和逻辑相关。随着机器学习的出现及其基础建立在连续的、矢量化的数学上，这种情况已经开始发生显著变化。现代机器学习建立在对张量的操作和微积分之上。
- en: Scalars, Vectors, and Matrices
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标量、向量和矩阵
- en: To start, we will give some simple examples of tensors that you might be familiar
    with. The simplest example of a tensor is a scalar, a single constant value drawn
    from the real numbers (recall that the real numbers are decimal numbers of arbitrary
    precision, with both positive and negative numbers permitted). Mathematically,
    we denote the real numbers by <math><mi>ℝ</mi></math> . More formally, we call
    a scalar a rank-0 tensor.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将给出一些您可能熟悉的张量的简单示例。张量的最简单示例是标量，即从实数中取出的单个常数值（请记住，实数是任意精度的十进制数，允许包含正数和负数）。在数学上，我们用<math><mi>ℝ</mi></math>来表示实数。更正式地说，我们将标量称为秩为0的张量。
- en: Aside on Fields
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于域的说明
- en: 'Mathematically sophisticated readers will protest that it’s entirely meaningful
    to define tensors based on the complex numbers, or with binary numbers. More generally,
    it’s sufficient that the numbers come from a *field*: a mathematical collection
    of numbers where 0, 1, addition, multiplication, subtraction, and division are
    defined. Common fields include the real numbers <math alttext="double-struck upper
    R"><mi>ℝ</mi></math> , the rational numbers <math alttext="double-struck upper
    Q"><mi>ℚ</mi></math> , the complex numbers <math alttext="double-struck upper
    C"><mi>ℂ</mi></math> , and finite fields such as <math alttext="double-struck
    upper Z 2"><msub><mi>ℤ</mi> <mn>2</mn></msub></math> . For simplicity, in much
    of the discussion, we will assume real valued tensors, but substituting in values
    from other fields is entirely reasonable.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学上精通的读者可能会反对基于复数或二进制数定义张量是完全有意义的。更一般地说，只要数字来自一个*域*：一个数学上定义了0、1、加法、乘法、减法和除法的数字集合。常见的域包括实数<math
    alttext="double-struck upper R"><mi>ℝ</mi></math>、有理数<math alttext="double-struck
    upper Q"><mi>ℚ</mi></math>、复数<math alttext="double-struck upper C"><mi>ℂ</mi></math>和有限域，如<math
    alttext="double-struck upper Z 2"><msub><mi>ℤ</mi> <mn>2</mn></msub></math>。在讨论中，我们将假设张量的值为实数，但是用其他域的值替代是完全合理的。
- en: If scalars are rank-0 tensors, what constitutes a rank-1 tensor? Formally, speaking,
    a rank-1 tensor is a vector; a list of real numbers. Traditionally, vectors are
    written as either column vectors
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果标量是秩为0的张量，那么什么构成了秩为1的张量？严格来说，秩为1的张量是一个向量；一个实数列表。传统上，向量被写成列向量
- en: <math display="block"><mfenced open="(" close=")"><mtable><mtr><mtd><mi>a</mi></mtd></mtr>
    <mtr><mtd><mi>b</mi></mtd></mtr></mtable></mfenced></math>
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mfenced open="(" close=")"><mtable><mtr><mtd><mi>a</mi></mtd></mtr>
    <mtr><mtd><mi>b</mi></mtd></mtr></mtable></mfenced></math>
- en: or as row vectors
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 或者作为行向量
- en: <math display="block"><mfenced open="(" close=")"><mtable><mtr><mtd><mi>a</mi></mtd>
    <mtd><mi>b</mi></mtd></mtr></mtable></mfenced></math>
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mfenced open="(" close=")"><mtable><mtr><mtd><mi>a</mi></mtd>
    <mtd><mi>b</mi></mtd></mtr></mtable></mfenced></math>
- en: Notationally, the collection of all column vectors of length 2 is denoted <math
    alttext="double-struck upper R Superscript 2 times 1"><msup><mi>ℝ</mi> <mrow><mn>2</mn><mo>×</mo><mn>1</mn></mrow></msup></math>
    while the set of all row vectors of length 2 is <math alttext="double-struck upper
    R Superscript 1 times 2"><msup><mi>ℝ</mi> <mrow><mn>1</mn><mo>×</mo><mn>2</mn></mrow></msup></math>
    . More computationally, we might say that the shape of a column vector is (2,
    1), while the shape of a row vector is (1, 2). If we don’t wish to specify whether
    a vector is a row vector or column vector, we can say it comes from the set <math
    alttext="double-struck upper R squared"><msup><mi>ℝ</mi> <mn>2</mn></msup></math>
    and has shape (2). This notion of tensor shape is quite important for understanding
    TensorFlow computations, and we will return to it later on in this chapter.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在符号上，长度为2的所有列向量表示为<math alttext="double-struck upper R Superscript 2 times 1"><msup><mi>ℝ</mi>
    <mrow><mn>2</mn><mo>×</mo><mn>1</mn></mrow></msup></math>，而长度为2的所有行向量集合是<math
    alttext="double-struck upper R Superscript 1 times 2"><msup><mi>ℝ</mi> <mrow><mn>1</mn><mo>×</mo><mn>2</mn></mrow></msup></math>。更具体地说，我们可以说列向量的形状是（2,
    1），而行向量的形状是（1, 2）。如果我们不想指定一个向量是行向量还是列向量，我们可以说它来自于集合<math alttext="double-struck
    upper R squared"><msup><mi>ℝ</mi> <mn>2</mn></msup></math>，形状为（2）。这种张量形状的概念对于理解TensorFlow计算非常重要，我们将在本章后面再次回到这个概念。
- en: One of the simplest uses of vectors is to represent coordinates in the real
    world. Suppose that we decide on an origin point (say the position where you’re
    currently standing). Then any position in the world can be represented by three
    displacement values from your current position (left-right displacement, front-back
    displacement, up-down displacement). Thus, the set of vectors (vector space) <math
    alttext="double-struck upper R cubed"><msup><mi>ℝ</mi> <mn>3</mn></msup></math>
    can represent any position in the world.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 向量最简单的用途之一是表示现实世界中的坐标。假设我们决定一个原点（比如你当前站立的位置）。那么世界上的任何位置都可以用从你当前位置的三个位移值来表示（左右位移、前后位移、上下位移）。因此，向量空间<math
    alttext="double-struck upper R cubed"><msup><mi>ℝ</mi> <mn>3</mn></msup></math>可以表示世界上的任何位置。
- en: For a different example, let’s suppose that a cat is described by its height,
    weight, and color. Then a video game cat can be represented as a vector
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 举个不同的例子，假设一只猫由它的身高、体重和颜色描述。那么一个视频游戏中的猫可以被表示为一个向量。
- en: <math display="block"><mfenced open="(" close=")"><mtable><mtr><mtd><mi>height</mi></mtd></mtr>
    <mtr><mtd><mi>weight</mi></mtd></mtr> <mtr><mtd><mi>color</mi></mtd></mtr></mtable></mfenced></math>
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mfenced open="(" close=")"><mtable><mtr><mtd><mi>height</mi></mtd></mtr>
    <mtr><mtd><mi>weight</mi></mtd></mtr> <mtr><mtd><mi>color</mi></mtd></mtr></mtable></mfenced></math>
- en: in the space <math><msup><mi>ℝ</mi> <mn>3</mn></msup></math> . This type of
    representation is often called a *featurization*. That is, a featurization is
    a representation of a real-world entity as a vector (or more generally as a tensor).
    Nearly all machine learning algorithms operate on vectors or tensors. Thus the
    process of featurization is a critical part of any machine learning pipeline.
    Often, the featurization system can be the most sophisticated part of a machine
    learning system. Suppose we have a benzene molecule as illustrated in [Figure 2-1](#ch2-benzene).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在空间<math><msup><mi>ℝ</mi> <mn>3</mn></msup></math>。这种类型的表示通常被称为*特征化*。也就是说，特征化是将现实世界实体表示为向量（或更一般地表示为张量）。几乎所有的机器学习算法都是在向量或张量上运行的。因此，特征化过程是任何机器学习流程中的关键部分。通常，特征化系统可能是机器学习系统中最复杂的部分。假设我们有一个如[图2-1](#ch2-benzene)所示的苯分子。
- en: '![images/Benzene-2D-flat.png](assets/tfdl_0201.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![images/Benzene-2D-flat.png](assets/tfdl_0201.png)'
- en: Figure 2-1\. A representation of a benzene molecule.
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1。苯分子的表示。
- en: How can we transform this molecule into a vector suitable for a query to a machine
    learning system? There are a number of potential solutions to this problem, most
    of which exploit the idea of marking the presence of subfragments of the molecule.
    The presence or absence of specific subfragments is marked by setting indices
    in a binary vector (in <math alttext="StartSet 0 comma 1 EndSet Superscript n"><msup><mrow><mo>{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>}</mo></mrow>
    <mi>n</mi></msup></math> ) to 1/0, respectively. This process is illustrated in
    [Figure 2-2](#ch2-circfing).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何将这个分子转换成适合查询机器学习系统的向量？对于这个问题有许多潜在的解决方案，其中大多数利用了标记分子的亚片段存在的想法。特定亚片段的存在或不存在通过在二进制向量中设置索引（在<math
    alttext="StartSet 0 comma 1 EndSet Superscript n"><msup><mrow><mo>{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>}</mo></mrow>
    <mi>n</mi></msup></math>）分别设置为1/0来标记。这个过程在[图2-2](#ch2-circfing)中有所说明。
- en: '![images/molecular_fingerprint.jpg](assets/tfdl_0202.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![images/molecular_fingerprint.jpg](tfdl_0202.png)'
- en: Figure 2-2\. Subfragments of the molecule to be featurized are selected (those
    containing OH). These fragments are hashed into indices in a fixed-length vector.
    These positions are set to 1 and all other positions are set to 0.
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-2。将要进行特征化的分子的亚片段被选择（包含OH的那些）。这些片段被哈希成固定长度向量中的索引。这些位置被设置为1，所有其他位置被设置为0。
- en: Note that this process sounds (and is) fairly complex. In fact, one of the most
    challenging aspects of building a machine learning system is deciding how to transform
    the data in question into a tensorial format. For some types of data, this transformation
    is obvious. For others (such as molecules), the transformation required can be
    quite subtle. For the practitioner of machine learning, it isn’t usually necessary
    to invent a new featurization method since the scholarly literature is extensive,
    but it will often be necessary to read research papers to understand best practices
    for transforming a new data stream.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个过程听起来（也是）相当复杂。事实上，构建一个机器学习系统最具挑战性的方面之一是决定如何将所涉及的数据转换成张量格式。对于某些类型的数据，这种转换是显而易见的。对于其他类型的数据（比如分子），所需的转换可能相当微妙。对于机器学习从业者来说，通常不需要发明新的特征化方法，因为学术文献非常丰富，但通常需要阅读研究论文以了解将新数据流转换的最佳实践。
- en: 'Now that we have established that rank-0 tensors are scalars ( <math alttext="double-struck
    upper R"><mi>ℝ</mi></math> ) and that rank-1 tensors are vectors ( <math alttext="double-struck
    upper R Superscript n"><msup><mi>ℝ</mi> <mi>n</mi></msup></math> ), what is a
    rank-2 tensor? Traditionally, a rank-2 tensor is referred to as a matrix:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确定了秩为0的张量是标量（<math alttext="双划线上标 R"><mi>ℝ</mi></math>），而秩为1的张量是向量（<math
    alttext="双划线上标 R 上标 n"><msup><mi>ℝ</mi> <mi>n</mi></msup></math>），那么秩为2的张量是什么？传统上，秩为2的张量被称为矩阵：
- en: <math display="block"><mfenced open="(" close=")"><mtable><mtr><mtd><mi>a</mi></mtd>
    <mtd><mi>b</mi></mtd></mtr> <mtr><mtd><mi>c</mi></mtd> <mtd><mi>d</mi></mtd></mtr></mtable></mfenced></math>
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mfenced open="(" close=")"><mtable><mtr><mtd><mi>a</mi></mtd>
    <mtd><mi>b</mi></mtd></mtr> <mtr><mtd><mi>c</mi></mtd> <mtd><mi>d</mi></mtd></mtr></mtable></mfenced></math>
- en: This matrix has two rows and two columns. The set of all such matrices is referred
    to as <math alttext="double-struck upper R Superscript 2 times 2"><msup><mi>ℝ</mi>
    <mrow><mn>2</mn><mo>×</mo><mn>2</mn></mrow></msup></math> . Returning to our notion
    of tensor shape earlier, the shape of this matrix is (2, 2). Matrices are traditionally
    used to represent transformations of vectors. For example, the action of rotating
    a vector in the plane by angle α can be performed by the matrix
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这个矩阵有两行两列。所有这样的矩阵集合被称为<math alttext="双划线上标 R 2乘2"><msup><mi>ℝ</mi> <mrow><mn>2</mn><mo>×</mo><mn>2</mn></mrow></msup></math>。回到我们之前对张量形状的概念，这个矩阵的形状是(2,
    2)。矩阵通常用来表示向量的变换。例如，通过矩阵
- en: <math display="block"><mrow><msub><mi>R</mi> <mi>α</mi></msub> <mo>=</mo> <mfenced
    open="(" close=")"><mtable><mtr><mtd><mrow><mo form="prefix">cos</mo> <mo>(</mo>
    <mi>α</mi> <mo>)</mo></mrow></mtd> <mtd><mrow><mo form="prefix">–sin</mo> <mo>(</mo>
    <mi>α</mi> <mo>)</mo></mrow></mtd></mtr> <mtr><mtd><mrow><mo form="prefix">sin</mo>
    <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd> <mtd><mrow><mo form="prefix">cos</mo>
    <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>R</mi> <mi>α</mi></msub> <mo>=</mo> <mfenced
    open="(" close=")"><mtable><mtr><mtd><mrow><mo form="prefix">cos</mo> <mo>(</mo>
    <mi>α</mi> <mo>)</mo></mrow></mtd> <mtd><mrow><mo form="prefix">–sin</mo> <mo>(</mo>
    <mi>α</mi> <mo>)</mrow></mtd></mtr> <mtr><mtd><mrow><mo form="prefix">sin</mo>
    <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd> <mtd><mrow><mo form="prefix">cos</mo>
    <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd></mtr></mtable></mfenced></mrow></math>
- en: To see this, note that the *x* unit vector (1, 0) is transformed by matrix multiplication
    into the vector (cos (α), sin (α)). (We will cover the detailed definition of
    matrix multiplication later in the chapter, but will simply display the result
    for the moment).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要看到这一点，注意*x*单位向量（1, 0）通过矩阵乘法转换为向量（cos(α)，sin(α)）。（我们将在本章后面详细介绍矩阵乘法的定义，但目前只是显示结果）。
- en: <math display="block"><mrow><mfenced open="(" close=")"><mtable><mtr><mtd><mrow><mo
    form="prefix">cos</mo> <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd> <mtd><mrow><mo
    form="prefix">–sin</mo> <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd></mtr> <mtr><mtd><mrow><mo
    form="prefix">sin</mo> <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd> <mtd><mrow><mo
    form="prefix">cos</mo> <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd></mtr></mtable></mfenced>
    <mo>·</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><mn>1</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced> <mo>=</mo> <mfenced open="("
    close=")"><mtable><mtr><mtd><mrow><mo form="prefix">cos</mo> <mo>(</mo> <mi>α</mi>
    <mo>)</mo></mrow></mtd></mtr> <mtr><mtd><mrow><mo form="prefix">sin</mo> <mo>(</mo>
    <mi>α</mi> <mo>)</mo></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mfenced open="(" close=")"><mtable><mtr><mtd><mrow><mo
    form="prefix">cos</mo> <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd> <mtd><mrow><mo
    form="prefix">–sin</mo> <mo>(</mo> <mi>α</mi> <mo>)</mrow></mtd></mtr> <mtr><mtd><mrow><mo
    form="prefix">sin</mo> <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd> <mtd><mrow><mo
    form="prefix">cos</mo> <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd></mtr></mtable></mfenced>
    <mo>·</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><mn>1</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced> <mo>=</mo> <mfenced open="("
    close=")"><mtable><mtr><mtd><mrow><mo form="prefix">cos</mo> <mo>(</mo> <mi>α</mi>
    <mo>)</mo></mrow></mtd></mtr> <mtr><mtd><mrow><mo form="prefix">sin</mo> <mo>(</mo>
    <mi>α</mi> <mo>)</mo></mrow></mtd></mtr></mtable></mfenced></mrow></math>
- en: This transformation can be visualized graphically as well. [Figure 2-3](#ch2-circparam)
    demonstrates how the final vector corresponds to a rotation of the original unit
    vector.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这种转换也可以以图形方式可视化。[图2-3](#ch2-circparam)展示了最终向量如何对应于原始单位向量的旋转。
- en: '![unit_circle.png](assets/tfdl_0203.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![unit_circle.png](assets/tfdl_0203.png)'
- en: Figure 2-3\. Positions on the unit circle are parameterized by cosine and sine.
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-3。单位圆上的位置由余弦和正弦参数化。
- en: Matrix Mathematics
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习程序经常使用矩阵的一些标准数学运算。我们将简要回顾其中一些最基本的运算。
- en: There are a number of standard mathematical operations on matrices that machine
    learning programs use repeatedly. We will briefly review some of the most fundamental
    of these operations.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在平面上旋转向量的操作可以通过这个矩阵来执行。
- en: The matrix transpose is a convenient operation that flips a matrix around its
    diagonal. Mathematically, suppose *A* is a matrix; then the transpose matrix <math><msup><mi>A</mi>
    <mi>T</mi></msup></math> is defined by equation <math><mrow><msubsup><mi>A</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow> <mi>T</mi></msubsup> <mo>=</mo> <msub><mi>A</mi>
    <mrow><mi>j</mi><mi>i</mi></mrow></msub></mrow></math> . For example, the transpose
    of the rotation matrix <math alttext="upper R Subscript alpha"><msub><mi>R</mi>
    <mi>α</mi></msub></math> is
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的转置是一个方便的操作，它将矩阵沿对角线翻转。数学上，假设*A*是一个矩阵；那么转置矩阵<math><msup><mi>A</mi> <mi>T</mi></msup></math>由方程<math><mrow><msubsup><mi>A</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow> <mi>T</mi></msubsup> <mo>=</mo> <msub><mi>A</mi>
    <mrow><mi>j</mi><mi>i</mi></mrow></msub></mrow></math>定义。例如，旋转矩阵<math alttext="上标
    R 下标 alpha"><msub><mi>R</mi> <mi>α</mi></msub></math>的转置是
- en: <math display="block"><mrow><msubsup><mi>R</mi> <mrow><mi>α</mi></mrow> <mi>T</mi></msubsup>
    <mo>=</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><mrow><mo form="prefix">cos</mo>
    <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd> <mtd><mrow><mo form="prefix">sin</mo>
    <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd></mtr> <mtr><mtd><mrow><mo form="prefix">–sin</mo>
    <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd> <mtd><mrow><mo form="prefix">cos</mo>
    <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msubsup><mi>R</mi> <mrow><mi>α</mi></mrow> <mi>T</mi></msubsup>
    <mo>=</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><mrow><mo form="prefix">cos</mo>
    <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd> <mtd><mrow><mo form="prefix">sin</mo>
    <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd></mtr> <mtr><mtd><mrow><mo form="prefix">–sin</mo>
    <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd> <mtd><mrow><mo form="prefix">cos</mo>
    <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'Addition of matrices is only defined for matrices of the same shape and is
    simply performed elementwise. For example:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的加法仅对形状相同的矩阵定义，并且仅是逐元素执行。例如：
- en: <math display="block"><mrow><mfenced open="(" close=")"><mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><mn>2</mn></mtd></mtr> <mtr><mtd><mn>3</mn></mtd> <mtd><mn>4</mn></mtd></mtr></mtable></mfenced>
    <mo>+</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><mn>1</mn></mtd> <mtd><mn>1</mn></mtd></mtr>
    <mtr><mtd><mn>1</mn></mtd> <mtd><mn>1</mn></mtd></mtr></mtable></mfenced> <mo>=</mo>
    <mfenced open="(" close=")"><mtable><mtr><mtd><mn>2</mn></mtd> <mtd><mn>3</mn></mtd></mtr>
    <mtr><mtd><mn>4</mn></mtd> <mtd><mn>5</mn></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mfenced open="(" close=")"><mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><mn>2</mn></mtd></mtr> <mtr><mtd><mn>3</mn></mtd> <mtd><mn>4</mn></mtd></mtr></mtable></mfenced>
    <mo>+</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><mn>1</mn></mtd> <mtd><mn>1</mn></mtd></mtr>
    <mtr><mtd><mn>1</mn></mtd> <mtd><mn>1</mn></mtd></mtr></mtable></mfenced> <mo>=</mo>
    <mfenced open="(" close=")"><mtable><mtr><mtd><mn>2</mn></mtd> <mtd><mn>3</mn></mtd></mtr>
    <mtr><mtd><mn>4</mn></mtd> <mtd><mn>5</mn></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'Similarly, matrices can be multiplied by scalars. In this case, each element
    of the matrix is simply multiplied elementwise by the scalar in question:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，矩阵可以乘以标量。在这种情况下，矩阵的每个元素都简单地逐元素乘以相关的标量：
- en: <math display="block"><mrow><mn>2</mn> <mo>·</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><mn>2</mn></mtd></mtr> <mtr><mtd><mn>3</mn></mtd> <mtd><mn>4</mn></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><mn>2</mn></mtd> <mtd><mn>4</mn></mtd></mtr>
    <mtr><mtd><mn>6</mn></mtd> <mtd><mn>8</mn></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mn>2</mn> <mo>·</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><mn>2</mn></mtd></mtr> <mtr><mtd><mn>3</mn></mtd> <mtd><mn>4</mn></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><mn>2</mn></mtd> <mtd><mn>4</mn></mtd></mtr>
    <mtr><mtd><mn>6</mn></mtd> <mtd><mn>8</mn></mtd></mtr></mtable></mfenced></mrow></math>
- en: Furthermore, it is sometimes possible to multiply two matrices directly. This
    notion of matrix multiplication is probably the most important mathematical concept
    associated with matrices. Note specifically that matrix multiplication is not
    the same notion as elementwise multiplication of matrices! Rather, suppose we
    have a matrix *A* of shape (*m*, *n*) with *m* rows and *n* columns. Then, *A*
    can be multiplied on the right by any matrix *B* of shape (*n*, *k*) (where *k*
    is any positive integer) to form matrix *AB* of shape (*m*, *k*). For the actual
    mathematical description, suppose *A* is a matrix of shape (*m*, *n*) and *B*
    is a matrix of shape (*n*, *k*). Then *AB* is defined by
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，有时可以直接相乘两个矩阵。矩阵乘法的概念可能是与矩阵相关的最重要的数学概念。特别注意，矩阵乘法不同于矩阵的逐元素乘法！假设我们有一个形状为（*m*，*n*）的矩阵*A*，其中*m*行*n*列。那么，*A*可以右乘任何形状为（*n*，*k*）的矩阵*B*（其中*k*是任意正整数），形成形状为（*m*，*k*）的矩阵*AB*。对于实际的数学描述，假设*A*是形状为（*m*，*n*）的矩阵，*B*是形状为（*n*，*k*）的矩阵。那么*AB*由以下定义：
- en: <math display="block"><mrow><msub><mrow><mo>(</mo><mi>A</mi><mi>B</mi><mo>)</mo></mrow>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub> <mo>=</mo> <munder><mo>∑</mo> <mi>k</mi></munder>
    <msub><mi>A</mi> <mrow><mi>i</mi><mi>k</mi></mrow></msub> <msub><mi>B</mi> <mrow><mi>k</mi><mi>j</mi></mrow></msub></mrow></math>
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mrow><mo>(</mo><mi>A</mi><mi>B</mi><mo>)</mo></mrow>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub> <mo>=</mo> <munder><mo>∑</mo> <mi>k</mi></munder>
    <msub><mi>A</mi> <mrow><mi>i</mi><mi>k</mi></mrow></msub> <msub><mi>B</mi> <mrow><mi>k</mi><mi>j</mi></mrow></msub></mrow></math>
- en: 'We displayed a matrix multiplication equation earlier in brief. Let’s expand
    that example now that we have the formal definition:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前简要展示了一个矩阵乘法方程。现在让我们扩展这个例子，因为我们有了正式的定义：
- en: <math display="block"><mrow><mfenced open="(" close=")"><mtable><mtr><mtd><mrow><mo
    form="prefix">cos</mo> <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd> <mtd><mrow><mo
    form="prefix">–sin</mo> <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd></mtr> <mtr><mtd><mrow><mo
    form="prefix">sin</mo> <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd> <mtd><mrow><mo
    form="prefix">cos</mo> <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd></mtr></mtable></mfenced>
    <mo>·</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><mn>1</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced> <mo>=</mo> <mfenced open="("
    close=")"><mtable><mtr><mtd><mrow><mo form="prefix">cos</mo> <mo>(</mo> <mi>α</mi>
    <mo>)</mo> <mo>·</mo> <mn>1</mn> <mo>–</mo> <mo form="prefix">sin</mo> <mo>(</mo>
    <mi>α</mi> <mo>)</mo> <mo>·</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mo
    form="prefix">sin</mo> <mo>(</mo> <mi>α</mi> <mo>)</mo> <mo>·</mo> <mn>1</mn>
    <mo>–</mo> <mo form="prefix">cos</mo> <mo>(</mo> <mi>α</mi> <mo>)</mo> <mo>·</mo>
    <mn>0</mn></mrow></mtd></mtr></mtable></mfenced> <mo>=</mo> <mfenced open="("
    close=")"><mtable><mtr><mtd><mrow><mo form="prefix">cos</mo> <mo>(</mo> <mi>α</mi>
    <mo>)</mo></mrow></mtd></mtr> <mtr><mtd><mrow><mo form="prefix">sin</mo> <mo>(</mo>
    <mi>α</mi> <mo>)</mo></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mfenced open="(" close=")"><mtable><mtr><mtd><mrow><mo
    form="prefix">cos</mo> <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd> <mtd><mrow><mo
    form="prefix">–sin</mo> <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd></mtr> <mtr><mtd><mrow><mo
    form="prefix">sin</mo> <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd> <mtd><mrow><mo
    form="prefix">cos</mo> <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd></mtr></mtable></mfenced>
    <mo>·</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><mn>1</mn></td></mtr>
    <mtr><mtd><mn>0</mn></mtr></mtable></mfenced> <mo>=</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><mrow><mo
    form="prefix">cos</mo> <mo>(</mo> <mi>α</mi> <mo>)</mo> <mo>·</mo> <mn>1</mn>
    <mo>–</mo> <mo form="prefix">sin</mo> <mo>(</mo> <mi>α</mi> <mo>)</mo> <mo>·</mo>
    <mn>0</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mo form="prefix">sin</mo> <mo>(</mo>
    <mi>α</mi> <mo>)</mo> <mo>·</mo> <mn>1</mn> <mo>–</mo> <mo form="prefix">cos</mo>
    <mo>(</mo> <mi>α</mi> <mo>)</mo> <mo>·</mo> <mn>0</mn></mrow></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><mrow><mo form="prefix">cos</mo>
    <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd></mtr> <mtr><mtd><mrow><mo form="prefix">sin</mo>
    <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></mtd></mtr></mtable></mfenced></mrow></math>
- en: The fundamental takeaway is that rows of one matrix are multiplied against columns
    of the other matrix.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 基本要点是一个矩阵的行与另一个矩阵的列相乘。
- en: This definition hides a number of subtleties. Note first that matrix multiplication
    is not commutative. That is, <math><mrow><mi>A</mi> <mi>B</mi> <mo>≠</mo> <mi>B</mi>
    <mi>A</mi></mrow></math> in general. In fact, *AB* can exist when *BA* is not
    meaningful. Suppose, for example, *A* is a matrix of shape (2, 3) and *B* is a
    matrix of shape (3, 4). Then *AB* is a matrix of shape (2, 4). However, *BA* is
    not defined since the respective dimensions (4 and 2) don’t match. As another
    subtlety, note that, as in the rotation example, a matrix of shape (*m*, *n*)
    can be multiplied on the right by a matrix of shape (*n*, 1). However, a matrix
    of shape (*n*, 1) is simply a column vector. So, it is meaningful to multiply
    matrices by vectors. Matrix-vector multiplication is one of the fundamental building
    blocks of common machine learning systems.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定义隐藏了许多微妙之处。首先注意矩阵乘法不是交换的。也就是说，<math><mrow><mi>A</mi> <mi>B</mi> <mo>≠</mo>
    <mi>B</mi> <mi>A</mi></mrow></math> 通常不成立。实际上，当 *BA* 没有意义时，*AB* 可能存在。例如，假设 *A*
    是一个形状为 (2, 3) 的矩阵，*B* 是一个形状为 (3, 4) 的矩阵。那么 *AB* 是一个形状为 (2, 4) 的矩阵。然而，*BA* 没有定义，因为各自的维度（4
    和 2）不匹配。另一个微妙之处是，正如旋转示例中所示，一个形状为 (*m*, *n*) 的矩阵可以右乘一个形状为 (*n*, 1) 的矩阵。然而，一个形状为
    (*n*, 1) 的矩阵简单地是一个列向量。因此，将矩阵乘以向量是有意义的。矩阵-向量乘法是常见机器学习系统的基本构建块之一。
- en: One of the nicest properties of standard multiplication is that it is a linear
    operation. More precisely, a function *f* is called linear if <math alttext="f
    left-parenthesis x plus y right-parenthesis equals f left-parenthesis x right-parenthesis
    plus f left-parenthesis y right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mi>x</mi>
    <mo>+</mo> <mi>y</mi> <mo>)</mo> <mo>=</mo> <mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo>
    <mo>+</mo> <mi>f</mi> <mo>(</mo> <mi>y</mi> <mo>)</mo></mrow></math> and <math><mrow><mi>f</mi>
    <mo>(</mo> <mi>c</mi> <mi>x</mi> <mo>)</mo> <mo>=</mo> <mi>c</mi> <mi>f</mi> <mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow></math> where *c* is a scalar. To demonstrate that
    scalar multiplication is linear, suppose that *a*, *b*, *c*, *d* are all real
    numbers. Then we have
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 标准乘法的一个最好的特性是它是一个线性操作。更准确地说，如果一个函数 *f* 被称为线性，那么 <math alttext="f left-parenthesis
    x plus y right-parenthesis equals f left-parenthesis x right-parenthesis plus
    f left-parenthesis y right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mi>x</mi>
    <mo>+</mo> <mi>y</mi> <mo>)</mo> <mo>=</mo> <mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo>
    <mo>+</mo> <mi>f</mi> <mo>(</mo> <mi>y</mi> <mo>)</mo></mrow></math> 和 <math><mrow><mi>f</mi>
    <mo>(</mo> <mi>c</mi> <mi>x</mi> <mo>)</mo> <mo>=</mo> <mi>c</mi> <mi>f</mi> <mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow></math> 其中 *c* 是一个标量。为了证明标量乘法是线性的，假设 *a*, *b*, *c*,
    *d* 都是实数。那么我们有
- en: <math display="block"><mrow><mi>a</mi> <mo>·</mo> <mo>(</mo> <mi>b</mi> <mo>·</mo>
    <mi>c</mi> <mo>)</mo> <mo>=</mo> <mi>b</mi> <mo>·</mo> <mo>(</mo> <mi>a</mi> <mi>c</mi>
    <mo>)</mo></mrow></math><math display="block"><mrow><mi>a</mi> <mo>·</mo> <mo>(</mo>
    <mi>c</mi> <mo>+</mo> <mi>d</mi> <mo>)</mo> <mo>=</mo> <mi>a</mi> <mi>c</mi> <mo>+</mo>
    <mi>a</mi> <mi>d</mi></mrow></math>
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>a</mi> <mo>·</mo> <mo>(</mo> <mi>b</mi> <mo>·</mo>
    <mi>c</mi> <mo>)</mo> <mo>=</mo> <mi>b</mi> <mo>·</mo> <mo>(</mo> <mi>a</mi> <mi>c</mi>
    <mo>)</mo></mrow></math><math display="block"><mrow><mi>a</mi> <mo>·</mo> <mo>(</mo>
    <mi>c</mi> <mo>+</mo> <mi>d</mi> <mo>)</mo> <mo>=</mo> <mi>a</mi> <mi>c</mi> <mo>+</mo>
    <mi>a</mi> <mi>d</mi></mrow></math>
- en: 'We make use of the commutative and distributive properties of scalar multiplication
    here. Now suppose that instead, *A*, *C*, *D* are now matrices where *C*, *D*
    are of the same size and it is meaningful to multiply *A* on the right with either
    *C* or *D* (*b* remains a real number). Then matrix multiplication is a linear
    operator:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里利用了标量乘法的交换和分配性质。现在假设相反，*A*, *C*, *D* 现在是矩阵，其中 *C*, *D* 的大小相同，并且将 *A* 右乘以
    *C* 或 *D* 是有意义的（*b* 仍然是一个实数）。那么矩阵乘法是一个线性操作符：
- en: <math display="block"><mrow><mi>A</mi> <mo>(</mo> <mi>b</mi> <mo>·</mo> <mi>C</mi>
    <mo>)</mo> <mo>=</mo> <mi>b</mi> <mo>·</mo> <mo>(</mo> <mi>A</mi> <mi>C</mi> <mo>)</mo></mrow></math><math
    display="block"><mrow><mi>A</mi> <mo>(</mo> <mi>C</mi> <mo>+</mo> <mi>D</mi> <mo>)</mo>
    <mo>=</mo> <mi>A</mi> <mi>C</mi> <mo>+</mo> <mi>A</mi> <mi>D</mi></mrow></math>
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Put another way, matrix multiplication is distributive and commutes with scalar
    multiplication. In fact, it can be shown that any linear transformation on vectors
    corresponds to a matrix multiplication. For a computer science analogy, think
    of linearity as a property demanded by an abstract method in a superclass. Then
    standard multiplication and matrix multiplication are concrete implementations
    of that abstract method for different subclasses (respectively real numbers and
    matrices).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Tensors
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous sections, we introduced the notion of scalars as rank-0 tensors,
    vectors as rank-1 tensors, and matrices as rank-2 tensors. What then is a rank-3
    tensor? Before passing to a general definition, it can help to think about the
    commonalities between scalars, vectors, and matrices. Scalars are single numbers.
    Vectors are lists of numbers. To pick out any particular element of a vector requires
    knowing its index. Hence, we need one index element into the vector (thus a rank-1
    tensor). Matrices are tables of numbers. To pick out any particular element of
    a matrix requires knowing its row and column. Hence, we need two index elements
    (thus a rank-2 tensor). It follows naturally that a rank-3 tensor is a set of
    numbers where there are three required indices. It can help to think of a rank-3
    tensor as a rectangular prism of numbers, as illustrated in [Figure 2-4](#ch2-rank3).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![images/3tensor.gif](assets/tfdl_0204.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: Figure 2-4\. A rank-3 tensor can be visualized as a rectangular prism of numbers.
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The rank-3 tensor *T* displayed in the figure is of shape (*N*, *N*, *N*). An
    arbitrary element of the tensor would then be selected by specifying (*i*, *j*,
    *k*) as indices.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: There is a linkage between tensors and shapes. A rank-1 tensor has a shape of
    dimension 1, a rank-2 tensor a shape of dimension 2, and a rank-3 tensor of dimension
    3\. You might protest that this contradicts our earlier discussion of row and
    column vectors. By our definition, a column vector has shape (*n*, 1). Wouldn’t
    that make a column vector a rank-2 tensor (or a matrix)? This is exactly what
    has happened. Recall that a vector which is not specified to be a row vector or
    column vector has shape (*n*). When we specify that a vector is a row vector or
    a column vector, we in fact specify a method of transforming the underlying vector
    into a matrix. This type of dimension expansion is a common trick in tensor manipulation.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Note that another way of thinking about a rank-3 tensor is as a list of matrices
    all with the same shape. Suppose that *W* is a matrix with shape (*n*, *n*). Then
    the tensor <math alttext="upper T Subscript i j k Baseline equals left-parenthesis
    upper W 1 comma ellipsis comma upper W Subscript n Baseline right-parenthesis"><mrow><msub><mi>T</mi>
    <mrow><mi>i</mi><mi>j</mi><mi>k</mi></mrow></msub> <mo>=</mo> <mrow><mo>(</mo>
    <msub><mi>W</mi> <mn>1</mn></msub> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msub><mi>W</mi>
    <mi>n</mi></msub> <mo>)</mo></mrow></mrow></math> consists of *n* copies of the
    matrix *W*.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Note that a black-and-white image can be represented as a rank-2 tensor. Suppose
    we have a 224 × 224-pixel black and white image. Then, pixel (*i*, *j*) is 1/0
    to encode a black/white pixel, respectively. It follows that a black and white
    image can be represented as a matrix of shape (224, 224). Now, consider a 224
    × 224 color image. The color at a particular pixel is typically represented by
    three separate RGB channels. That is, pixel (*i*, *j*) is represented as a tuple
    of numbers (*r*, *g*, *b*) that encode the amount of red, green, and blue at the
    pixel, respectively. *r*, *g*, *b* are typically integers from 0 to 255\. It follows
    now that the color image can be encoded as a rank-3 tensor of shape (224, 224,
    3). Continuing the analogy, consider a color video. Suppose that each frame of
    the video is a 224 × 224 color image. Then a minute of video (at 60 fps) would
    be a rank-4 tensor of shape (224, 224, 3, 3600). Continuing even further, a collection
    of 10 such videos would then form a rank-5 tensor of shape (10, 224, 224, 3, 3600).
    In general, tensors provide for a convenient representation of numeric data. In
    practice, it’s not common to see tensors of higher order than rank-5 tensors,
    but it’s smart to design any tensor software to allow for arbitrary tensors since
    intelligent users will always come up with use cases designers don’t consider.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Tensors in Physics
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Tensors are used widely in physics to encode fundamental physical quantities.
    For example, the stress tensor is commonly used in material science to define
    the stress at a point within a material. Mathematically, the stress tensor is
    a rank-2 tensor of shape (3, 3):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>σ</mi> <mo>=</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><msub><mi>σ</mi>
    <mn>11</mn></msub></mtd> <mtd><msub><mi>τ</mi> <mn>12</mn></msub></mtd> <mtd><msub><mi>τ</mi>
    <mn>13</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>τ</mi> <mn>21</mn></msub></mtd>
    <mtd><msub><mi>σ</mi> <mn>22</mn></msub></mtd> <mtd><msub><mi>τ</mi> <mn>23</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>τ</mi> <mn>31</mn></msub></mtd> <mtd><msub><mi>τ</mi> <mn>32</mn></msub></mtd>
    <mtd><msub><mi>σ</mi> <mn>33</mn></msub></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Then, suppose that *n* is a vector of shape (3) that encodes a direction. The
    stress <math><msup><mi>T</mi> <mi>n</mi></msup></math> in direction *n* is specified
    by the vector <math><mrow><msup><mi>T</mi> <mi>n</mi></msup> <mo>=</mo> <mi>T</mi>
    <mo>·</mo> <mi>n</mi></mrow></math> (note the matrix-vector multiplication). This
    relationship is depicted pictorially in [Figure 2-5](#ch2-stress).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '![images/stress_energy_small.png](assets/tfdl_0205.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
- en: Figure 2-5\. A 3D pictorial depiction of the components of stress.
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As another physical example, Einstein’s field equations of general relativity
    are commonly expressed in tensorial format:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>R</mi> <mrow><mi>μ</mi><mi>ν</mi></mrow></msub>
    <mo>-</mo> <mfrac><mn>1</mn> <mn>2</mn></mfrac> <mi>R</mi> <msub><mi>g</mi> <mrow><mi>μ</mi><mi>ν</mi></mrow></msub>
    <mo>+</mo> <mi>Λ</mi> <msub><mi>g</mi> <mrow><mi>μ</mi><mi>ν</mi></mrow></msub>
    <mo>=</mo> <mfrac><mrow><mn>8</mn><mi>π</mi><mi>G</mi></mrow> <msup><mi>c</mi>
    <mn>4</mn></msup></mfrac> <msub><mi>T</mi> <mrow><mi>μ</mi><mi>ν</mi></mrow></msub></mrow></math>
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Here <math alttext="upper R Subscript mu nu"><msub><mi>R</mi> <mrow><mi>μ</mi><mi>ν</mi></mrow></msub></math>
    is the Ricci curvature tensor, <math><msub><mi>g</mi> <mrow><mi>μ</mi><mi>ν</mi></mrow></msub></math>
    is the metric tensor, <math><msub><mi>T</mi> <mrow><mi>μ</mi><mi>ν</mi></mrow></msub></math>
    is the stress-energy tensor, and the remaining quantities are scalars. Note, however,
    that there’s an important subtlety distinguishing these tensors and the other
    tensors we’ve discussed previously. Quantities like the metric tensor provide
    a separate tensor (in the sense of an array of numbers) for each point in space-time
    (mathematically, the metric tensor is a tensor field). The same holds for the
    stress tensor previously discussed, and for the other tensors in these equations.
    At a given point in space-time, each of these quantities becomes a symmetric rank-2
    tensor of shape (4, 4) using our notation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这里<math alttext="上标mu nu的R"><msub><mi>R</mi> <mrow><mi>μ</mi><mi>ν</mi></mrow></msub></math>是里奇曲率张量，<math><msub><mi>g</mi>
    <mrow><mi>μ</mi><mi>ν</mi></mrow></msub></math>是度规张量，<math><msub><mi>T</mi> <mrow><mi>μ</mi><mi>ν</mi></mrow></msub></math>是应力能量张量，其余量是标量。然而，需要注意的是，这些张量和我们之前讨论过的其他张量之间有一个重要的微妙区别。像度规张量这样的量为时空中的每个点提供一个单独的张量（在数字数组的意义上，数学上，度规张量是一个张量场）。之前讨论过的应力张量也是如此，这些方程中的其他张量也是如此。在时空中的特定点，这些量中的每一个都变成了一个对称的秩2张量，使用我们的符号形状为（4，4）。
- en: Part of the power of modern tensor calculus systems such as TensorFlow is that
    some of the mathematical machinery long used for classical physics can now be
    adapted to solve applied problems in image processing and language understanding.
    At the same time, today’s tensor calculus systems are still limited compared with
    the mathematical machinery of physicists. For example, there’s no simple way to
    talk about a quantity such as the metric tensor using TensorFlow yet. We hope
    that as tensor calculus becomes more fundamental to computer science, the situation
    will change and that systems like TensorFlow will serve as a bridge between the
    physical world and the computational world.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现代张量微积分系统（如TensorFlow）的一部分力量在于，一些长期用于经典物理学的数学机器现在可以被改编用于解决图像处理和语言理解等应用问题。与此同时，今天的张量微积分系统与物理学家的数学机器相比仍然有限。例如，目前还没有简单的方法来使用TensorFlow谈论度规张量这样的量。我们希望随着张量微积分对计算机科学变得更加基础，情况会发生变化，像TensorFlow这样的系统将成为物理世界和计算世界之间的桥梁。
- en: Mathematical Asides
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数学细节
- en: 'The discussion so far in this chapter has introduced tensors informally via
    example and illustration. In our definition, a tensor is simply an array of numbers.
    It’s often convenient to view a tensor as a function instead. The most common
    definition introduces a tensor as a multilinear function from a product of vector
    spaces to the real numbers:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中的讨论通过示例和插图非正式地介绍了张量。在我们的定义中，张量简单地是一组数字的数组。通常方便将张量视为一个函数。最常见的定义将张量引入为从向量空间的乘积到实数的多线性函数：
- en: <math display="block"><mrow><mi>T</mi> <mo>:</mo> <msub><mi>V</mi> <mn>1</mn></msub>
    <mo>×</mo> <msub><mi>V</mi> <mn>2</mn></msub> <mo>×</mo> <mo>⋯</mo> <msub><mi>V</mi>
    <mi>n</mi></msub> <mo>→</mo> <mi>ℝ</mi></mrow></math>
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>T</mi> <mo>:</mo> <msub><mi>V</mi> <mn>1</mn></msub>
    <mo>×</mo> <msub><mi>V</mi> <mn>2</mn></msub> <mo>×</mo> <mo>⋯</mo> <msub><mi>V</mi>
    <mi>n</mi></msub> <mo>→</mo> <mi>ℝ</mi></mrow></math>
- en: This definition uses a number of terms you haven’t seen. A vector space is simply
    a collection of vectors. You’ve seen a few examples of vector spaces such as <math><msup><mi>ℝ</mi>
    <mn>3</mn></msup></math> or generally <math alttext="double-struck upper R Superscript
    n"><msup><mi>ℝ</mi> <mi>n</mi></msup></math> . We won’t lose any generality by
    holding that <math alttext="upper V Subscript i Baseline equals double-struck
    upper R Superscript d Super Subscript i"><mrow><msub><mi>V</mi> <mi>i</mi></msub>
    <mo>=</mo> <msup><mi>ℝ</mi> <msub><mi>d</mi> <mi>i</mi></msub></msup></mrow></math>
    . As we defined previously, a function *f* is linear if <math alttext="f left-parenthesis
    x plus y right-parenthesis equals f left-parenthesis x right-parenthesis plus
    f left-parenthesis y right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mi>x</mi>
    <mo>+</mo> <mi>y</mi> <mo>)</mo> <mo>=</mo> <mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo>
    <mo>+</mo> <mi>f</mi> <mo>(</mo> <mi>y</mi> <mo>)</mo></mrow></math> and <math><mrow><mi>f</mi>
    <mo>(</mo> <mi>c</mi> <mi>x</mi> <mo>)</mo> <mo>=</mo> <mi>c</mi> <mi>f</mi> <mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow></math> . A multilinear function is simply a function
    that is linear in each argument. This function can be viewed as assigning individual
    entries of a multidimensional array, when provided indices into the array as arguments.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定义使用了一些你没有见过的术语。一个向量空间简单地是向量的集合。你已经见过一些向量空间的例子，比如<math><msup><mi>ℝ</mi> <mn>3</mn></msup></math>或者一般的<math
    alttext="双击上标n的R"><msup><mi>ℝ</mi> <mi>n</mi></msup></math>。我们可以假设<math alttext="V下标i等于双击上标d下标i的R"><mrow><msub><mi>V</mi>
    <mi>i</mi></msub> <mo>=</mo> <msup><mi>ℝ</mi> <msub><mi>d</mi> <mi>i</mi></msub></msup></mrow></math>而不会失去一般性。正如我们之前定义的，一个函数*f*是线性的，如果<math
    alttext="f左括号x加y右括号等于f左括号x右括号加f左括号y右括号"><mrow><mi>f</mi> <mo>(</mo> <mi>x</mi>
    <mo>+</mo> <mi>y</mi> <mo>)</mo> <mo>=</mo> <mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo>
    <mo>+</mo> <mi>f</mi> <mo>(</mo> <mi>y</mi> <mo>)</mo></mrow></math>和<math><mrow><mi>f</mi>
    <mo>(</mo> <mi>c</mi> <mi>x</mi> <mo>)</mo> <mo>=</mo> <mi>c</mi> <mi>f</mi> <mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow></math>。一个多线性函数简单地是一个在每个参数上都是线性的函数。当提供数组索引作为参数时，这个函数可以被看作是给定多维数组的单个条目。
- en: We won’t use this more mathematical definition much in this book, but it serves
    as a useful bridge to connect the deep learning concepts you will learn about
    with the centuries of mathematical research that have been undertaken on tensors
    by the physics and mathematics communities.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中我们不会经常使用这个更数学化的定义，但它作为一个有用的桥梁，将你将要学习的深度学习概念与物理学和数学界对张量进行的几个世纪的研究联系起来。
- en: Covariance and Contravariance
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 协变和逆变
- en: Our definition here has swept many details under the rug that would need to
    be carefully attended to for a formal treatment. For example, we don’t touch upon
    the notion of covariant and contravariant indices here. What we call a rank-*n*
    tensor is better described as a (*p*, *q*)-tensor where *n* = *p* + *q* and *p*
    is the number of contravariant indices, and *q* the number of covariant indices.
    Matrices are (1,1)-tensors, for example. As a subtlety, there are rank-2 tensors
    that are not matrices! We won’t dig into these topics carefully here since they
    don’t crop up much in machine learning, but we encourage you to understand how
    covariance and contravariance affect the machine learning systems you construct.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里的定义中忽略了许多需要仔细处理的细节，以进行正式处理。例如，我们在这里没有涉及共变和逆变指数的概念。我们所谓的秩-*n*张量最好描述为一个（*p*，*q*）-张量，其中*n*
    = *p* + *q*，*p*是逆变指数的数量，*q*是共变指数的数量。例如，矩阵是（1,1）-张量。作为一个微妙之处，有些秩为2的张量不是矩阵！我们不会在这里仔细探讨这些主题，因为它们在机器学习中并不经常出现，但我们鼓励您了解协变性和逆变性如何影响您构建的机器学习系统。
- en: Basic Computations in TensorFlow
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow中的基本计算
- en: We’ve spent the last sections covering the mathematical definitions of various
    tensors. It’s now time to cover how to create and manipulate tensors using TensorFlow.
    For this section, we recommend you follow along using an interactive Python session
    (with IPython). Many of the basic TensorFlow concepts are easiest to understand
    after experimenting with them directly.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几节中，我们已经涵盖了各种张量的数学定义。现在是时候使用TensorFlow创建和操作张量了。对于本节，我们建议您使用交互式Python会话（使用IPython）跟随进行。许多基本的TensorFlow概念在直接实验后最容易理解。
- en: Installing TensorFlow and Getting Started
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装TensorFlow并入门
- en: Before continuing this section, you will need to install TensorFlow on your
    machine. The details of installation will vary depending on your particular hardware,
    so we refer you to [the official TensorFlow documentation](https://www.tensorflow.org/api_docs/)
    for more details.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续本节之前，您需要在您的计算机上安装TensorFlow。安装的详细信息将取决于您的特定硬件，因此我们建议您查阅[官方TensorFlow文档](https://www.tensorflow.org/api_docs/)以获取更多详细信息。
- en: Although there are frontends to TensorFlow in multiple programming languages,
    we will exclusively use the TensorFlow Python API in the remainder of this book.
    We recommend that you install [Anaconda Python](https://anaconda.org/anaconda/python),
    which packages many useful numerical libraries along with the base Python executable.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管TensorFlow有多种编程语言的前端，但我们将在本书的其余部分中专门使用TensorFlow Python API。我们建议您安装[Anaconda
    Python](https://anaconda.org/anaconda/python)，它打包了许多有用的数值库以及基本的Python可执行文件。
- en: Once you’ve installed TensorFlow, we recommend that you invoke it interactively
    while you’re learning the basic API (see [Example 2-1](#ch2-interactive-session)).
    When experimenting with TensorFlow interactively, it’s convenient to use `tf.InteractiveSession()`.
    Invoking this statement within IPython (an interactive Python shell) will make
    TensorFlow behave almost imperatively, allowing beginners to play with tensors
    much more easily. You will learn about imperative versus declarative style in
    greater depth later in this chapter.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您安装了TensorFlow，我们建议您在学习基本API时交互地调用它（请参见[示例2-1](#ch2-interactive-session)）。在与TensorFlow交互时进行实验时，使用`tf.InteractiveSession()`会很方便。在IPython（一个交互式Python
    shell）中调用此语句将使TensorFlow几乎以命令方式运行，使初学者更容易地玩弄张量。稍后在本章中，您将更深入地了解命令式与声明式风格的区别。
- en: Example 2-1\. Initialize an interactive TensorFlow session
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例2-1。初始化一个交互式TensorFlow会话
- en: '[PRE0]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The rest of the code in this section will assume that an interactive session
    has been loaded.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的其余代码将假定已加载了一个交互式会话。
- en: Initializing Constant Tensors
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始化常量张量
- en: Until now, we’ve discussed tensors as abstract mathematical entities. However,
    a system like TensorFlow must run on a real computer, so any tensors must live
    on computer memory in order to be useful to computer programmers. TensorFlow provides
    a number of functions that instantiate basic tensors in memory. The simplest of
    these are `tf.zeros()` and `tf.ones()`. `tf.zeros()` takes a tensor shape (represented
    as a Python tuple) and returns a tensor of that shape filled with zeros. Let’s
    try invoking this command in the shell ([Example 2-2](#ch2-zeros)).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经将张量讨论为抽象的数学实体。然而，像TensorFlow这样的系统必须在真实计算机上运行，因此任何张量必须存在于计算机内存中，以便对计算机程序员有用。TensorFlow提供了许多在内存中实例化基本张量的函数。其中最简单的是`tf.zeros()`和`tf.ones()`。`tf.zeros()`接受一个张量形状（表示为Python元组）并返回一个填充有零的该形状的张量。让我们尝试在shell中调用此命令（请参见[示例2-2](#ch2-zeros)）。
- en: Example 2-2\. Create a zeros tensor
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例2-2。创建一个零张量
- en: '[PRE1]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: TensorFlow returns a reference to the desired tensor rather than the value of
    the tensor itself. To force the value of the tensor to be returned, we will use
    the method `tf.Tensor.eval()` of tensor objects ([Example 2-3](#ch2-evaluate)).
    Since we have initialized `tf.InteractiveSession()`, this method will return the
    value of the zeros tensor to us.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow返回所需张量的引用，而不是张量本身的值。为了强制返回张量的值，我们将使用张量对象的`tf.Tensor.eval()`方法（请参见[示例2-3](#ch2-evaluate)）。由于我们已经初始化了`tf.InteractiveSession()`，这个方法将向我们返回零张量的值。
- en: Example 2-3\. Evaluate the value of a tensor
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例2-3。评估张量的值
- en: '[PRE2]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note that the evaluated value of the TensorFlow tensor is itself a Python object.
    In particular, `a.eval()` is a `numpy.ndarray` object. NumPy is a sophisticated
    numerical system for Python. We won’t attempt an in-depth discussion of NumPy
    here beyond noting that TensorFlow is designed to be compatible with NumPy conventions
    to a large degree.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，TensorFlow张量的计算值本身是一个Python对象。特别地，`a.eval()`是一个`numpy.ndarray`对象。NumPy是Python的一个复杂数值系统。我们不会在这里尝试对NumPy进行深入讨论，只是注意到TensorFlow被设计为在很大程度上与NumPy约定兼容。
- en: We can call `tf.zeros()` and `tf.ones()` to create and display tensors of various
    sizes ([Example 2-4](#ch2-display)).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以调用`tf.zeros()`和`tf.ones()`来创建和显示各种大小的张量（请参见[示例2-4](#ch2-display)）。
- en: Example 2-4\. Evaluate and display tensors
  id: totrans-96
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例2-4。评估和显示张量
- en: '[PRE3]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: What if we’d like a tensor filled with some quantity besides 0/1? The `tf.fill()`
    method provides a nice shortcut for doing so ([Example 2-5](#ch2-fill)).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要一个填充有除0/1之外的某个数量的张量呢？`tf.fill()`方法提供了一个很好的快捷方式来做到这一点（[示例2-5](#ch2-fill)）。
- en: Example 2-5\. Filling tensors with arbitrary values
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例2-5。用任意值填充张量
- en: '[PRE4]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`tf.constant` is another function, similar to `tf.fill`, which allows for construction
    of tensors that shouldn’t change during the program execution ([Example 2-6](#ch2-constant)).'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.constant`是另一个函数，类似于`tf.fill`，允许在程序执行期间不应更改的张量的构建（[示例2-6](#ch2-constant)）。'
- en: Example 2-6\. Creating constant tensors
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例2-6。创建常量张量
- en: '[PRE5]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Sampling Random Tensors
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 抽样随机张量
- en: Although working with constant tensors is convenient for testing ideas, it’s
    much more common to initialize tensors with random values. The most common way
    to do this is to sample each entry in the tensor from a random distribution. `tf.random_normal`
    allows for each entry in a tensor of specified shape to be sampled from a Normal
    distribution of specified mean and standard deviation ([Example 2-7](#ch2-randnormal)).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用常量张量方便测试想法，但更常见的是使用随机值初始化张量。这样做的最常见方式是从随机分布中抽样张量中的每个条目。`tf.random_normal`允许从指定均值和标准差的正态分布中抽样指定形状的张量中的每个条目（[示例2-7](#ch2-randnormal)）。
- en: Symmetry Breaking
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对称性破缺
- en: Many machine learning algorithms learn by performing updates to a set of tensors
    that hold weights. These update equations usually satisfy the property that weights
    initialized at the same value will continue to evolve together. Thus, if the initial
    set of tensors is initialized to a constant value, the model won’t be capable
    of learning much. Fixing this situation requires *symmetry breaking*. The easiest
    way of breaking symmetry is to sample each entry in a tensor randomly.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习算法通过对保存权重的一组张量执行更新来学习。这些更新方程通常满足初始化为相同值的权重将继续一起演变的属性。因此，如果初始张量集初始化为一个常量值，模型将无法学习太多。解决这种情况需要*破坏对称性*。打破对称性的最简单方法是随机抽样张量中的每个条目。
- en: Example 2-7\. Sampling a tensor with random Normal entries
  id: totrans-108
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例2-7。抽样具有随机正态条目的张量
- en: '[PRE6]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: One thing to note is that machine learning systems often make use of very large
    tensors that often have tens of millions of parameters. When we sample tens of
    millions of random values from the Normal distribution, it becomes almost certain
    that some sampled values will be far from the mean. Such large samples can lead
    to numerical instability, so it’s common to sample using `tf.truncated_normal()`
    instead of `tf.random_normal()`. This function behaves the same as `tf.random_normal()`
    in terms of API, but drops and resamples all values more than two standard deviations
    from the mean.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一点是，机器学习系统通常使用具有数千万参数的非常大的张量。当我们从正态分布中抽样数千万个随机值时，几乎可以肯定会有一些抽样值远离均值。这样大的样本可能导致数值不稳定，因此通常使用`tf.truncated_normal()`而不是`tf.random_normal()`进行抽样。这个函数在API方面与`tf.random_normal()`相同，但会删除并重新抽样所有距离均值超过两个标准差的值。
- en: '`tf.random_uniform()` behaves like `tf.random_normal()` except for the fact
    that random values are sampled from the Uniform distribution over a specified
    range ([Example 2-8](#ch2-randuniform)).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.random_uniform()`的行为类似于`tf.random_normal()`，唯一的区别是随机值是从指定范围的均匀分布中抽样的（[示例2-8](#ch2-randuniform)）。'
- en: Example 2-8\. Sampling a tensor with uniformly random entries
  id: totrans-112
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例2-8。抽样具有均匀随机条目的张量
- en: '[PRE7]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Tensor Addition and Scaling
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 张量加法和缩放
- en: TensorFlow makes use of Python’s operator overloading to make basic tensor arithmetic
    straightforward with standard Python operators ([Example 2-9](#ch2-tensoradd)).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow利用Python的运算符重载，使用标准Python运算符使基本张量算术变得简单直观（[示例2-9](#ch2-tensoradd)）。
- en: Example 2-9\. Adding tensors together
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例2-9。将张量相加
- en: '[PRE8]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Tensors can also be multiplied this way. Note, however, when multiplying two
    tensors we get elementwise multiplication and not matrix multiplication, which
    can be seen in [Example 2-10](#ch2-tensormul).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 张量也可以这样相乘。但是请注意，当两个张量相乘时，我们得到的是逐元素乘法而不是矩阵乘法，可以在[示例2-10](#ch2-tensormul)中看到。
- en: Example 2-10\. Elementwise tensor multiplication
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例2-10。逐元素张量乘法
- en: '[PRE9]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Matrix Operations
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 矩阵运算
- en: TensorFlow provides a variety of amenities for working with matrices. (Matrices
    by far are the most common type of tensor used in practice.) In particular, TensorFlow
    provides shortcuts to make certain types of commonly used matrices. The most widely
    used of these is likely the identity matrix. Identity matrices are square matrices
    that are 0 everywhere except on the diagonal, where they are 1\. `tf.eye()` allows
    for fast construction of identity matrices of desired size ([Example 2-11](#ch2-tensorid)).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow提供了各种便利设施来处理矩阵。（在实践中，矩阵是最常用的张量类型。）特别是，TensorFlow提供了快捷方式来创建某些常用矩阵类型。其中最常用的可能是单位矩阵。单位矩阵是指在对角线上除了1之外其他地方都是0的方阵。`tf.eye()`允许快速构建所需大小的单位矩阵（[示例2-11](#ch2-tensorid)）。
- en: Example 2-11\. Creating an identity matrix
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例2-11。创建一个单位矩阵
- en: '[PRE10]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Diagonal matrices are another common type of matrix. Like identity matrices,
    diagonal matrices are only nonzero along the diagonal. Unlike identity matrices,
    they may take arbitrary values along the diagonal. Let’s construct a diagonal
    matrix with ascending values along the diagonal ([Example 2-12](#ch2-tensordiag)).
    To start, we’ll need a method to construct a vector of ascending values in TensorFlow.
    The easiest way for doing this is invoking `tf.range(start, limit, delta)`. Note
    that `limit` is excluded from the range and `delta` is the step size for the traversal.
    The resulting vector can then be fed to `tf.diag(diagonal)`, which will construct
    a matrix with the specified diagonal.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 对角矩阵是另一种常见类型的矩阵。与单位矩阵不同，对角矩阵只在对角线上非零。与单位矩阵不同，它们可以在对角线上取任意值。让我们构造一个沿对角线升序值的对角矩阵（[示例2-12](#ch2-tensordiag)）。首先，我们需要一种方法在TensorFlow中构造升序值的向量。这样做的最简单方法是调用`tf.range(start,
    limit, delta)`。请注意，范围中排除了`limit`，`delta`是遍历的步长。然后，生成的向量可以传递给`tf.diag(diagonal)`，它将构造具有指定对角线的矩阵。
- en: Example 2-12\. Creating diagonal matrices
  id: totrans-126
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例2-12。创建对角矩阵
- en: '[PRE11]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now suppose that we have a specified matrix in TensorFlow. How do we compute
    the matrix transpose? `tf.matrix_transpose()` will do the trick nicely ([Example 2-13](#ch2-tensortrans)).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们在TensorFlow中有一个指定的矩阵。如何计算矩阵的转置？`tf.matrix_transpose()`会很好地完成这个任务（[示例2-13](#ch2-tensortrans)）。
- en: Example 2-13\. Taking a matrix transpose
  id: totrans-129
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例2-13。取矩阵转置
- en: '[PRE12]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now, let’s suppose we have a pair of matrices we’d like to multiply using matrix
    multiplication. The easiest way to do so is by invoking `tf.matmul()` ([Example 2-14](#ch2-tensormatmul)).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们有一对矩阵，我们想要使用矩阵乘法相乘。最简单的方法是调用`tf.matmul()`（[示例2-14](#ch2-tensormatmul)）。
- en: Example 2-14\. Performing matrix multiplication
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例2-14。执行矩阵乘法
- en: '[PRE13]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: You can check that this answer matches the mathematical definition of matrix
    multiplication we provided earlier.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以检查这个答案是否与我们之前提供的矩阵乘法的数学定义相匹配。
- en: Tensor Types
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 张量类型
- en: You may have noticed the `dtype` notation in the preceding examples. Tensors
    in TensorFlow come in a variety of types such as `tf.float32`, `tf.float64`, `tf.int32`,
    `tf.int64`. It’s possible to to create tensors of specified types by setting `dtype`
    in tensor construction functions. Furthermore, given a tensor, it’s possible to
    change its type using casting functions such as `tf.to_double()`, `tf.to_float()`,
    `tf.to_int32()`, `tf.to_int64()`, and others ([Example 2-15](#ch2-tensortype)).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到了前面示例中的`dtype`表示。TensorFlow中的张量有各种类型，如`tf.float32`、`tf.float64`、`tf.int32`、`tf.int64`。可以通过在张量构造函数中设置`dtype`来创建指定类型的张量。此外，给定一个张量，可以使用转换函数如`tf.to_double()`、`tf.to_float()`、`tf.to_int32()`、`tf.to_int64()`等来更改其类型（[示例2-15](#ch2-tensortype)）。
- en: Example 2-15\. Creating tensors of different types
  id: totrans-137
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例2-15。创建不同类型的张量
- en: '[PRE14]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Tensor Shape Manipulations
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 张量形状操作
- en: Within TensorFlow, tensors are just collections of numbers written in memory.
    The different shapes are views into the underlying set of numbers that provide
    different ways of interacting with the set of numbers. At different times, it
    can be useful to view the same set of numbers as forming tensors with different
    shapes. `tf.reshape()` allows tensors to be converted into tensors with different
    shapes ([Example 2-16](#ch2-tensorshapes)).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow中，张量只是内存中写入的数字集合。不同的形状是对底层数字集合的视图，提供了与数字集合交互的不同方式。在不同的时间，将相同的数字集合视为具有不同形状的张量可能是有用的。`tf.reshape()`允许将张量转换为具有不同形状的张量（[示例2-16](#ch2-tensorshapes)）。
- en: Example 2-16\. Manipulating tensor shapes
  id: totrans-141
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例2-16。操作张量形状
- en: '[PRE15]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Notice how we can turn the original rank-1 tensor into a rank-2 tensor and then
    into a rank-3 tensor with `tf.reshape`. While all necessary shape manipulations
    can be performed with `tf.reshape()`, sometimes it can be convenient to perform
    simpler shape manipulations using functions such as `tf.expand_dims` or `tf.squeeze`.
    `tf.expand_dims` adds an extra dimension to a tensor of size 1\. It’s useful for
    increasing the rank of a tensor by one (for example, when converting a rank-1
    vector into a rank-2 row vector or column vector). `tf.squeeze`, on the other
    hand, removes all dimensions of size 1 from a tensor. It’s a useful way to convert
    a row or column vector into a flat vector.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 注意如何使用`tf.reshape`将原始秩为1的张量转换为秩为2的张量，然后再转换为秩为3的张量。虽然所有必要的形状操作都可以使用`tf.reshape()`执行，但有时使用诸如`tf.expand_dims`或`tf.squeeze`等函数执行更简单的形状操作可能更方便。`tf.expand_dims`向大小为1的张量添加额外的维度。它用于通过增加一个维度来增加张量的秩（例如，将秩为1的向量转换为秩为2的行向量或列向量）。另一方面，`tf.squeeze`从张量中删除所有大小为1的维度。这是将行向量或列向量转换为平坦向量的有用方法。
- en: This is also a convenient opportunity to introduce the `tf.Tensor.get_shape()`
    method ([Example 2-17](#ch2-tensorgetshape)). This method lets users query the
    shape of a tensor.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是一个方便的机会来介绍`tf.Tensor.get_shape()`方法（[示例2-17](#ch2-tensorgetshape)）。这个方法允许用户查询张量的形状。
- en: Example 2-17\. Getting the shape of a tensor
  id: totrans-145
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例2-17。获取张量的形状
- en: '[PRE16]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Introduction to Broadcasting
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 广播简介
- en: Broadcasting is a term (introduced by NumPy) for when a tensor system’s matrices
    and vectors of different sizes can be added together. These rules allow for conveniences
    like adding a vector to every row of a matrix. Broadcasting rules can be quite
    complex, so we will not dive into a formal discussion of the rules. It’s often
    easier to experiment and see how the broadcasting works ([Example 2-18](#ch2-tensorbroadcast)).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 广播是一个术语（由NumPy引入），用于当张量系统的矩阵和不同大小的向量可以相加时。这些规则允许像将向量添加到矩阵的每一行这样的便利。广播规则可能相当复杂，因此我们不会深入讨论规则。尝试并查看广播的工作方式通常更容易（[示例2-18](#ch2-tensorbroadcast)）。
- en: Example 2-18\. Examples of broadcasting
  id: totrans-149
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例2-18。广播的示例
- en: '[PRE17]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Notice that the vector `b` is added to every row of matrix `a`. Notice another
    subtlety; we explicitly set the `dtype` for `b`. If the `dtype` isn’t set, TensorFlow
    will report a type error. Let’s see what would have happened if we hadn’t set
    the `dtype` ([Example 2-19](#ch2-tensornocast)).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 注意向量`b`被添加到矩阵`a`的每一行。注意另一个微妙之处；我们明确为`b`设置了`dtype`。如果没有设置`dtype`，TensorFlow将报告类型错误。让我们看看如果我们没有设置`dtype`会发生什么（[示例2-19](#ch2-tensornocast)）。
- en: Example 2-19\. TensorFlow doesn’t perform implicit type casting
  id: totrans-152
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例2-19。TensorFlow不执行隐式类型转换
- en: '[PRE18]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Unlike languages like C, TensorFlow doesn’t perform implicit type casting under
    the hood. It’s often necessary to perform explicit type casts when doing arithmetic
    operations.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 与C语言不同，TensorFlow在底层不执行隐式类型转换。在进行算术运算时通常需要执行显式类型转换。
- en: Imperative and Declarative Programming
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 命令式和声明式编程
- en: Most situations in computer science involve imperative programming. Consider
    a simple Python program ([Example 2-20](#ch2-pyadd)).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学中的大多数情况涉及命令式编程。考虑一个简单的Python程序（[示例2-20](#ch2-pyadd)）。
- en: Example 2-20\. Python program imperatively performing an addition
  id: totrans-157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例2-20。以命令式方式执行加法的Python程序
- en: '[PRE19]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This program, when translated into machine code, instructs the machine to perform
    a primitive addition operation on two registers, one containing 3, and the other
    containing 4\. The result is then 7\. This style of programming is called *imperative*
    since the program tells the computer explicitly which actions to perform.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这个程序，当被翻译成机器码时，指示机器对两个寄存器执行一个原始的加法操作，一个包含3，另一个包含4。结果是7。这种编程风格被称为*命令式*，因为程序明确告诉计算机执行哪些操作。
- en: An alternative style of programming is *declarative*. In a declarative system,
    a computer program is a high-level description of the computation that is to be
    performed. It does not instruct the computer exactly how to perform the computation.
    [Example 2-21](#ch2-tensorpyadd) is the TensorFlow equivalent of [Example 2-20](#ch2-pyadd).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种编程风格是*声明式*。在声明式系统中，计算机程序是要执行的计算的高级描述。它不会明确告诉计算机如何执行计算。[示例2-21](#ch2-tensorpyadd)是[示例2-20](#ch2-pyadd)的TensorFlow等价物。
- en: Example 2-21\. TensorFlow program declaratively performing an addition
  id: totrans-161
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例2-21。以声明式方式执行加法的TensorFlow程序
- en: '[PRE20]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Note that the value of `c` isn’t `7`! Rather, it’s a symbolic tensor. This code
    specifies the computation of adding two values together to create a new tensor.
    The actual computation isn’t executed until we call `c.eval()`. In the sections
    before, we have been using the `eval()` method to simulate imperative style in
    TensorFlow since it can be challenging to understand declarative programming at
    first.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`c`的值不是`7`！相反，它是一个符号张量。这段代码指定了将两个值相加以创建一个新张量的计算。实际计算直到我们调用`c.eval()`才执行。在之前的部分，我们一直在使用`eval()`方法来模拟TensorFlow中的命令式风格，因为一开始理解声明式编程可能会有挑战。
- en: However, declarative programming is by no means an unknown concept to software
    engineering. Relational databases and SQL provide an example of a widely used
    declarative programming system. Commands like SELECT and JOIN may be implemented
    in an arbitrary fashion under the hood so long as their basic semantics are preserved.
    TensorFlow code is best thought of as analogous to a SQL program; the TensorFlow
    code specifies a computation to be performed, with details left up to TensorFlow.
    The TensorFlow developers exploit this lack of detail under the hood to tailor
    the execution style to the underlying hardware, be it CPU, GPU, or mobile device.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，声明式编程对软件工程并不是一个未知的概念。关系数据库和SQL提供了一个广泛使用的声明式编程系统的例子。像SELECT和JOIN这样的命令可以在底层以任意方式实现，只要它们的基本语义得以保留。TensorFlow代码最好被视为类似于SQL程序；TensorFlow代码指定要执行的计算，细节留给TensorFlow处理。TensorFlow开发人员利用底层缺乏细节来调整执行风格以适应底层硬件，无论是CPU、GPU还是移动设备。
- en: It’s important to note that the grand weakness of declarative programming is
    that the abstraction is quite leaky. For example, without detailed understanding
    of the underlying implementation of the relational database, long SQL programs
    can become unbearably inefficient. Similarly, large TensorFlow programs implemented
    without understanding of the underlying learning algorithms are unlikely to work
    well. In the rest of this section, we will start paring back the abstraction,
    a process we will continue throughout the rest of the book.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，声明式编程的主要弱点是抽象性很差。例如，没有对关系数据库的底层实现有详细了解，长的SQL程序可能会变得难以忍受地低效。同样，没有对底层学习算法的理解实现的大型TensorFlow程序可能不会运行良好。在本节的其余部分，我们将开始减少抽象，这个过程将贯穿整本书的其余部分。
- en: TensorFlow Eager
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow Eager
- en: The TensorFlow team recently added a new experimental module, TensorFlow Eager,
    that enables users to run TensorFlow calculations imperatively. In time, this
    module will likely become the preferred entry mode for new programmers learning
    TensorFlow. However, at the timing of writing, this module is still very new with
    many rough edges. As a result, we won’t teach you about Eager mode, but encourage
    you to check it out for yourself.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow团队最近添加了一个新的实验模块，TensorFlow Eager，使用户能够以命令式方式运行TensorFlow计算。随着时间的推移，这个模块很可能会成为新程序员学习TensorFlow的首选入口模式。然而，在撰写时，这个模块仍然非常新，并且存在许多问题。因此，我们不会教授您关于Eager模式，但鼓励您自行了解。
- en: It’s important to emphasize that much of TensorFlow will remain declarative
    even after Eager matures, so it’s worth learning declarative TensorFlow regardless.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要强调，即使Eager成熟后，TensorFlow的很多部分仍然会保持声明式，所以学习声明式的TensorFlow是值得的。
- en: TensorFlow Graphs
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow 图
- en: Any computation in TensorFlow is represented as an instance of a `tf.Graph`
    object. Such a graph consists of a set of instances of `tf.Tensor` objects and
    `tf.Operation` objects. We have covered `tf.Tensor` in some detail, but what are
    `tf.Operation` objects? You have already seen them over the course of this chapter.
    A call to an operation like `tf.matmul` creates a `tf.Operation` instance to mark
    the need to perform the matrix multiplication operation.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow中，任何计算都表示为`tf.Graph`对象的实例。这样的图由一组`tf.Tensor`对象实例和`tf.Operation`对象实例组成。我们已经详细介绍了`tf.Tensor`，但`tf.Operation`对象是什么？在本章的过程中，您已经看到了它们。对`tf.matmul`等操作的调用会创建一个`tf.Operation`实例，以标记执行矩阵乘法操作的需求。
- en: When a `tf.Graph` is not explicitly specified, TensorFlow adds tensors and operations
    to a hidden global `tf.Graph` instance. This instance can be fetched by `tf.get_default_graph()`
    ([Example 2-22](#ch2-defgraph)).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 当未明确指定`tf.Graph`时，TensorFlow会将张量和操作添加到隐藏的全局`tf.Graph`实例中。可以通过`tf.get_default_graph()`获取此实例（[示例2-22](#ch2-defgraph)）。
- en: Example 2-22\. Getting the default TensorFlow graph
  id: totrans-172
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例2-22。获取默认的TensorFlow图
- en: '[PRE21]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: It is possible to specify that TensorFlow operations should be performed in
    graphs other than the default. We will demonstrate examples of this in future
    chapters.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 可以指定TensorFlow操作应在除默认之外的图中执行。我们将在未来章节中演示这方面的示例。
- en: TensorFlow Sessions
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow会话
- en: In TensorFlow, a `tf.Session()` object stores the context under which a computation
    is performed. At the beginning of this chapter, we used `tf.InteractiveSession()`
    to set up an environment for all TensorFlow computations. This call created a
    hidden global context for all computations performed. We then used `tf.Tensor.eval()`
    to execute our declaratively specified computations. Underneath the hood, this
    call is evaluated in context of this hidden global `tf.Session`. It can be convenient
    (and often necessary) to use an explicit context for a computation instead of
    a hidden context ([Example 2-23](#ch2-sessions)).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow中，`tf.Session()`对象存储计算执行的上下文。在本章的开头，我们使用`tf.InteractiveSession()`为所有TensorFlow计算设置环境。此调用创建了一个隐藏的全局上下文，用于执行所有计算。然后我们使用`tf.Tensor.eval()`来执行我们声明指定的计算。在幕后，此调用在这个隐藏的全局`tf.Session`上下文中进行评估。使用显式上下文进行计算而不是隐藏上下文可能会更方便（通常也更必要）（[示例2-23](#ch2-sessions)）。
- en: Example 2-23\. Explicitly manipulating TensorFlow sessions
  id: totrans-177
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例2-23。显式操作TensorFlow会话
- en: '[PRE22]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This code evaluates `b` in the context of `sess` instead of the hidden global
    session. In fact, we can make this more explicit with an alternate notation ([Example 2-24](#ch2-runsessions)).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码在`sess`的上下文中评估`b`，而不是隐藏的全局会话。实际上，我们可以使用另一种符号更明确地表示这一点（[示例2-24](#ch2-runsessions)）。
- en: Example 2-24\. Running a computation within a session
  id: totrans-180
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例2-24。在会话中运行计算
- en: '[PRE23]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In fact, calling `b.eval(session=sess)` is just syntactic sugar for calling
    `sess.run(b)`.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，调用`b.eval(session=sess)`只是调用`sess.run(b)`的语法糖。
- en: This entire discussion may smack a bit of sophistry. What does it matter which
    session is in play given that all the different methods seem to return the same
    answer? Explicit sessions don’t really show their value until you start to perform
    computations that have state, a topic you will learn about in the following section.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 整个讨论可能有点诡辩。鉴于所有不同的方法似乎返回相同的答案，哪个会话正在进行并不重要？直到您开始执行具有状态的计算时，显式会话才能展现其价值，这是您将在下一节中了解的主题。
- en: TensorFlow Variables
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow变量
- en: All the example code in this section has used constant tensors. While we could
    combine and recombine these tensors in any way we chose, we could never change
    the value of tensors themselves (only create new tensors with new values). The
    style of programming so far has been *functional* and not *stateful*. While functional
    computations are very useful, machine learning often depends heavily on stateful
    computations. Learning algorithms are essentially rules for updating stored tensors
    to explain provided data. If it’s not possible to update these stored tensors,
    it would be hard to learn.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的所有示例代码都使用了常量张量。虽然我们可以以任何方式组合和重组这些张量，但我们永远无法更改张量本身的值（只能创建具有新值的新张量）。到目前为止，编程风格一直是*函数式*而不是*有状态*的。虽然函数式计算非常有用，但机器学习往往严重依赖有状态的计算。学习算法本质上是更新存储的张量以解释提供的数据的规则。如果无法更新这些存储的张量，学习将变得困难。
- en: The `tf.Variable()` class provides a wrapper around tensors that allows for
    stateful computations. The variable objects serve as holders for tensors. Creating
    a variable is easy enough ([Example 2-25](#ch2-createvar)).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.Variable()`类提供了一个围绕张量的包装器，允许进行有状态的计算。变量对象充当张量的持有者。创建变量非常容易（[示例2-25](#ch2-createvar)）。'
- en: Example 2-25\. Creating a TensorFlow variable
  id: totrans-187
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例2-25。创建TensorFlow变量
- en: '[PRE24]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: What happens when we try to evaluate the variable `a` as though it were a tensor,
    as in [Example 2-26](#ch2-vareval)?
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们尝试评估变量`a`，就像在[示例2-26](#ch2-vareval)中一样，作为张量时会发生什么？
- en: Example 2-26\. Evaluating an uninitialized variable fails
  id: totrans-190
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例2-26。评估未初始化的变量失败
- en: '[PRE25]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The evaluation fails since variables have to be explicitly initialized. The
    easiest way to initialize all variables is to invoke `tf.global_variables_initializer`.
    Running this operation within a session will initialize all variables in the program
    ([Example 2-27](#ch2-sessioninit)).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 评估失败，因为必须显式初始化变量。初始化所有变量的最简单方法是调用`tf.global_variables_initializer`。在会话中运行此操作将初始化程序中的所有变量（[示例2-27](#ch2-sessioninit)）。
- en: Example 2-27\. Evaluating initialized variables
  id: totrans-193
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例2-27。评估初始化的变量
- en: '[PRE26]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: After initialization, we are able to fetch the value stored within the variable
    as though it were a plain tensor. So far, there’s not much more to variables than
    plain tensors. Variables only become interesting once we can assign to them. `tf.assign()`
    lets us do this. Using `tf.assign()` we can update the value of an existing variable
    ([Example 2-28](#ch2-varassign)).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化后，我们可以获取存储在变量中的值，就像它是一个普通的张量一样。到目前为止，变量没有比普通张量更有趣的地方。只有当我们可以对变量进行赋值时，变量才变得有趣。`tf.assign()`让我们可以做到这一点。使用`tf.assign()`，我们可以更新现有变量的值（[示例2-28](#ch2-varassign)）。
- en: Example 2-28\. Assigning values to variables
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例2-28。为变量赋值
- en: '[PRE27]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: What would happen if we tried to assign a value to variable `a` not of shape
    `(2,2)`? Let’s find out in [Example 2-29](#ch2-badshapeassing).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们尝试为变量`a`分配一个不是形状`(2,2)`的值会发生什么？让我们在[示例2-29](#ch2-badshapeassing)中找出答案。
- en: Example 2-29\. Assignment fails when shapes aren’t equal
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例2-29。当形状不相等时，赋值失败
- en: '[PRE28]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: You can see that TensorFlow complains. The shape of the variable is fixed upon
    initialization and must be preserved with updates. As another interesting note,
    `tf.assign` is itself a part of the underlying global `tf.Graph` instance. This
    allows TensorFlow programs to update their internal state every time they are
    run. We will make heavy use of this feature in the chapters to come.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到TensorFlow会抱怨。变量的形状在初始化时是固定的，必须在更新时保持不变。另一个有趣的地方是，`tf.assign`本身是底层全局`tf.Graph`实例的一部分。这使得TensorFlow程序可以在每次运行时更新其内部状态。在接下来的章节中，我们将大量使用这个特性。
- en: Review
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回顾
- en: In this chapter, we’ve introduced the mathematical concept of tensors, and briefly
    reviewed a number of mathematical concepts associated with tensors. We then demonstrated
    how to create tensors in TensorFlow and perform these same mathematical operations
    within TensorFlow. We also briefly introduced some underlying TensorFlow structures
    like the computational graph, sessions, and variables. If you haven’t completely
    grasped the concepts discussed in this chapter, don’t worry much about it. We
    will repeatedly use these same concepts over the remainder of the book, so there
    will be plenty of chances to let the ideas sink in.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们介绍了张量的数学概念，并简要回顾了与张量相关的一些数学概念。然后我们演示了如何在TensorFlow中创建张量并在TensorFlow中执行相同的数学运算。我们还简要介绍了一些底层的TensorFlow结构，比如计算图、会话和变量。如果你还没有完全掌握本章讨论的概念，不要太担心。在本书的剩余部分中，我们将反复使用这些概念，所以会有很多机会让这些想法深入人心。
- en: In the next chapter, we will teach you how to build simple learning models for
    linear and logistic regression using TensorFlow. Subsequent chapters will build
    on these foundations to teach you how to train more sophisticated models.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将教你如何使用TensorFlow为线性回归和逻辑回归构建简单的学习模型。随后的章节将在这些基础上构建，教你如何训练更复杂的模型。
