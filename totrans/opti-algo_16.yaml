- en: 12 Reinforcement learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12 强化学习
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Grasping the fundamental principles underlying reinforcement learning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 掌握强化学习的基本原理
- en: Understanding the Markov decision process
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解马尔可夫决策过程
- en: Comprehending the actor-critic architecture and proximal policy optimization
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解演员-评论家架构和近端策略优化
- en: Getting familiar with noncontextual and contextual multi-armed bandits
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熟悉非上下文和上下文多臂老虎机
- en: Applying reinforcement learning to solve optimization problems
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将强化学习应用于解决优化问题
- en: Reinforcement learning (RL) is a powerful machine learning approach that enables
    intelligent agents to learn optimal or near-optimal behavior through interacting
    with their environments. This chapter dives into the key concepts and techniques
    within RL, shedding light on its underlying principles as essential background
    knowledge. Following this theoretical exposition, the chapter will proceed to
    illustrate practical examples of employing RL strategies to tackle optimization
    problems.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）是一种强大的机器学习方法，它使智能代理能够通过与环境的交互来学习最优或近似最优的行为。本章深入探讨了强化学习中的关键概念和技术，揭示了其基本原理作为必要背景知识。在理论阐述之后，本章将接着阐述使用强化学习策略解决优化问题的实际例子。
- en: 12.1 Demystifying reinforcement learning
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.1 揭秘强化学习
- en: Reinforcement learning (RL) is a subfield of machine learning that deals with
    how an agent can learn to make decisions and take actions in an environment to
    achieve specific goals following a trial-and-error learning approach. The core
    idea of RL is that the agent learns by interacting with the environment, receiving
    feedback in the form of rewards or penalties as a result of its actions. The agent’s
    objective is to maximize the cumulative reward over time.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）是机器学习的一个子领域，它处理代理如何通过试错学习方法在环境中做出决策和采取行动以实现特定目标。强化学习的核心思想是代理通过与环境的交互来学习，通过其动作的结果获得奖励或惩罚作为反馈。代理的目标是在时间上最大化累积奖励。
- en: Reinforcement learning
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习
- en: “Reinforcement learning problems involve learning what to do—how to map situations
    to actions—so as to maximize a numerical reward signal” (Richard Sutton and Andrew
    Barto, in their book *Reinforcement Learning* [1]).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: “强化学习问题涉及学习做什么——如何将情况映射到动作——以最大化数值奖励信号”（理查德·萨顿和安德鲁·巴特奥在他们的书《强化学习》[1]中）。
- en: 'Figure 12.1 outlines the common RL algorithms found in the literature. This
    classification divides RL problems into two main categories: Markov decision process
    (MDP) problems and multi-armed bandit (MAB) problems. The distinction between
    the two lies in how the agent’s actions interact with and affect the environment.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1概述了文献中常见的强化学习算法。这种分类将强化学习问题分为两大类：马尔可夫决策过程（MDP）问题和多臂老虎机（MAB）问题。两者之间的区别在于代理的动作如何与环境和影响环境。
- en: '![](../Images/CH12_F01_Khamis.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH12_F01_Khamis.png)'
- en: Figure 12.1 Reinforcement learning algorithm taxonomy
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1 强化学习算法分类
- en: In MDP-based problems, the agent’s actions influence the environment, and the
    agent must consider the consequences of its actions over multiple time steps,
    incorporating the notion of states and transitions. MAB problems, on the other
    hand, involve scenarios where the agent faces a series of choices (arms) and aims
    to maximize the cumulative reward over time. Such problems are often used when
    there is no explicit state representation or long-term planning required. In contextual
    multi-armed bandit (CMAB) problems, the agent is presented with context or side
    information that is used to make more informed decisions. The expected reward
    of an arm (an action) is a function of both the action and the current context.
    This means the best action can change depending on the provided context. It is
    worth mentioning that MDPs are a more comprehensive framework that accounts for
    dynamic decision-making in a broader range of situations.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于MDP的问题中，代理的动作影响环境，代理必须考虑其动作在多个时间步长的后果，包括状态和转换的概念。另一方面，MAB问题涉及代理面临一系列选择（臂）并旨在最大化时间上的累积奖励的场景。这类问题通常在没有明确的状态表示或长期规划要求时使用。在上下文多臂老虎机（CMAB）问题中，代理被提供上下文或辅助信息，这些信息用于做出更明智的决策。臂（动作）的预期奖励是动作和当前上下文的函数。这意味着最佳动作可以随着提供的上下文而改变。值得一提的是，MDP是一个更全面的框架，它考虑了更广泛情况下动态决策。
- en: Before we explore how reinforcement learning can be applied to solve optimization
    problems, it is essential to understand several relevant reinforcement learning
    techniques. The following subsections will provide a detailed overview of these
    methods, and the subsequent sections will demonstrate their use in addressing
    optimization problems.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们探讨如何将强化学习应用于解决优化问题之前，理解几个相关的强化学习技术是至关重要的。以下小节将详细概述这些方法，接下来的章节将展示它们在解决优化问题中的应用。
- en: 12.1.1 Markov decision process (MDP)
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.1 马尔可夫决策过程（MDP）
- en: The purpose of learning is to form an internal model of the external world.
    This external world, or *environment*, can be abstracted using deterministic or
    nondeterministic (stochastic) models.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 学习的目的是形成对外部世界的内部模型。这个外部世界，或*环境*，可以使用确定性或非确定性（随机）模型进行抽象。
- en: 'Consider a situation where you’re planning to commute from your initial state
    (e.g., your home) to a designated goal state (e.g., your workplace). A deterministic
    path-planning algorithm such as A* (discussed in chapter 4) might provide you
    with multiple options: taking the train, which would take about 1 hour and 48
    minutes; driving by car, which could take about 1 hour and 7 minutes; or biking,
    which could take about 3 hours and 16 minutes (figure 12.2a). These algorithms
    operate under the assumption that actions and their resulting effects are entirely
    deterministic.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一种情况，你计划从你的初始状态（例如，你的家）通勤到一个指定的目标状态（例如，你的工作场所）。一种确定性路径规划算法，如A*（在第4章中讨论），可能会为你提供多种选择：乘坐火车，大约需要1小时48分钟；开车，大约需要1小时7分钟；或者骑自行车，大约需要3小时16分钟（图12.2a）。这些算法在假设行动及其结果完全确定性的前提下运行。
- en: '![](../Images/CH12_F02_Khamis.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH12_F02_Khamis.png)'
- en: Figure 12.2 Deterministic vs. nondeterministic state transition models
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2 确定性与非确定性状态转换模型
- en: However, if uncertainties come into play during your journey planning, you should
    resort to stochastic planning algorithms that operate on nondeterministic state
    transition models to enable planning under uncertainty. In these scenarios, transition
    probabilities between states become an integral part of your decision-making process.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果在你的旅程规划过程中出现不确定性，你应该求助于基于非确定性状态转换模型的随机规划算法，以实现不确定条件下的规划。在这些情况下，状态之间的转换概率成为你决策过程的一个组成部分。
- en: For instance, let’s consider the initial state (your home), as shown in figure
    12.2b. If you choose to commute by train, there’s a 60% chance that you’ll be
    able to catch the train on time and reach your destination (your workplace) within
    the expected 1 hour and 48 minutes. However, there’s a 40% chance that you could
    miss the train and need to wait for the next one. If you do end up waiting, there’s
    a 90% chance the train will arrive on time and you’ll catch it and arrive at your
    destination within a total time of 2 hours and 25 minutes. On the other hand,
    there’s a 10% chance the train does not arrive, leading to an extended wait or
    even having to look for an alternative. On the other hand, if you choose to drive,
    there’s a 30% chance that you’ll encounter light traffic and reach your office
    in just 50 minutes. However, there’s also a 50% likelihood of medium traffic delaying
    your arrival to 1 hour and 7 minutes. In the worst-case scenario, there’s a 20%
    chance that heavy traffic could extend your travel time to 2 hours. If you choose
    to bike, the estimated travel time of 3 hours and 16 minutes is more predictable
    and less subject to change.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们考虑初始状态（你的家），如图12.2b所示。如果你选择乘坐火车通勤，有60%的几率你能按时赶上火车，并在预期的1小时48分钟内到达目的地（你的工作场所）。然而，有40%的几率你可能会错过火车，需要等待下一班。如果你最终需要等待，有90%的几率火车会准时到达，你将赶上火车并在总共2小时25分钟内到达目的地。另一方面，有10%的几率火车不会到达，导致等待时间延长，甚至可能需要寻找替代方案。另一方面，如果你选择开车，有30%的几率你会遇到轻微的交通拥堵，只需50分钟就能到达办公室。然而，也有50%的可能性中等交通会延误你的到达时间至1小时7分钟。在最坏的情况下，有20%的几率严重交通拥堵可能会将你的旅行时间延长至2小时。如果你选择骑自行车，预计的旅行时间为3小时16分钟，更加可预测且变化较小。
- en: This scenario describes an environment that is fully observable and where the
    current state and actions taken completely determine the probability distribution
    of the next state. This is called a Markov decision process (MDP).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这种场景描述了一个完全可观察的环境，其中当前状态和采取的行动完全决定了下一个状态的概率分布。这被称为马尔可夫决策过程（MDP）。
- en: 12.1.2 From MDP to reinforcement learning
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.2 从MDP到强化学习
- en: MDP provides a mathematical framework for planning under uncertainty. It is
    used to describe an environment for reinforcement learning, where an agent learns
    to make decisions by performing actions and receiving rewards. The learning process
    involves trial and error, with the agent discovering which actions yield the highest
    expected cumulative reward over time.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: MDP为在不确定性下进行规划提供了一个数学框架。它用于描述强化学习中的环境，其中智能体通过执行动作并接收奖励来学习做出决策。学习过程涉及尝试和错误，智能体发现哪些动作在一段时间内能产生最高的期望累积奖励。
- en: As shown in figure 12.3, the agent interacts with the environment by being in
    a certain state *s[t]* ∈ *S*, taking an action *a[t]* ∈ *A* at time *t* based
    on an observation *o* and by applying policy *π*, and then receiving a reward
    *r[t]* ∈ *R* and transitioning to a new state *s[t]*[+1] ∈ *S* according to the
    state transition probability *T*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如图12.3所示，智能体通过与环境的交互来采取行动，在某个状态 *s[t]* ∈ *S* 下，根据观察 *o* 和应用策略 *π*，在时间 *t* 采取动作
    *a[t]* ∈ *A*，然后根据状态转移概率 *T* 接收奖励 *r[t]* ∈ *R* 并过渡到新的状态 *s[t]*[+1] ∈ *S*。
- en: '![](../Images/CH12_F03_Khamis.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH12_F03_Khamis.png)'
- en: Figure 12.3 An agent learns through interaction with the environment.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3 智能体通过与环境的交互来学习。
- en: 'The following terms are commonly used in RL:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下术语在RL中常用：
- en: '*A state,* s—This represents the complete and unfiltered information about
    the environment at a particular time step. An observation *o* is the partial or
    limited information that the agent can perceive from the environment at a given
    time step. When the agent is able to observe the complete state of the environment,
    we say that the environment is fully observable and can be modeled as an MDP.
    When the agent can only see part of the environment, we say that the environment
    is partially observable, and it should be modeled as a *partially observable Markov
    decision process* (POMDP).'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*状态，* s—这代表在特定时间步长关于环境的完整和未过滤的信息。观察 *o* 是智能体在给定时间步长从环境中感知到的部分或有限信息。当智能体能够观察到环境的完整状态时，我们说环境是全可观察的，可以建模为MDP。当智能体只能看到环境的一部分时，我们说环境是部分可观察的，应该建模为*部分可观察马尔可夫决策过程*
    (POMDP)。'
- en: '*An action set,* A—This represents the set of possible or permissible actions
    that an agent can take in a given environment. These actions can be discrete,
    like in the case of board games, or continuous, like in the case of robot controls
    or the lane-keep assist of an assisted or automated driving vehicle.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*动作集，* A—这代表智能体在给定环境中可以采取的可能或允许的动作集合。这些动作可以是离散的，如棋类游戏的情况，也可以是连续的，如机器人控制或辅助或自动驾驶车辆的车道保持辅助。'
- en: '*The policy*—This can be seen as the agent’s brain. It is the decision-making
    strategy of the agent or the mapping from states or observations to actions. This
    policy can be deterministic, usually denoted by *μ*, or stochastic, denoted by
    *π*. A stochastic policy *π* is mainly the probability of selecting an action
    *a* ∈ *A* given a certain state *s* ∈ *S*. A stochastic policy can also be parameterized
    and denoted by *π[Θ]*. This parameterized policy is a computable function that
    depends on a set of parameters (e.g., the weights and biases of a neural network),
    which we can adjust to change the behavior via an optimization algorithm.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*策略*—这可以看作是智能体的“大脑”。它是智能体的决策策略，或从状态或观察到动作的映射。这种策略可以是确定性的，通常用 *μ* 表示，也可以是随机的，用
    *π* 表示。一个随机的策略 *π* 主要是指在某个状态 *s* ∈ *S* 下选择动作 *a* ∈ *A* 的概率。一个随机的策略也可以参数化，并用 *π[Θ]*
    表示。这种参数化策略是一个可计算的函数，它依赖于一组参数（例如，神经网络的权重和偏差），我们可以通过优化算法调整这些参数来改变行为。'
- en: '*A trajectory,* τ *(aka episode or rollout)*—This is a sequence of states and
    actions in the world, τ = (*s*[0], *a*[0], *s*[1], *a*[1], ...) .'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*轨迹，* τ *(又称事件或滚动)*—这是世界中状态和动作的序列，τ = (*s*[0], *a*[0], *s*[1], *a*[1], ...)
    .'
- en: '*The expected return*—This refers to the cumulative sum of rewards that an
    agent can expect to receive over a future time horizon. It is a measure of the
    overall desirability or value of a particular state-action sequence or policy.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*期望回报*—这指的是智能体在未来时间范围内可以期望接收的累积奖励总和。它是特定状态-动作序列或策略的整体吸引力或价值的度量。'
- en: Let’s consider a simple Reach the Treasure game where an agent tries to get
    a treasure and then exits. In this game, there are only four states, as illustrated
    in figure 12.4\. Among them, state 0 represents a fire pit, and state 3 represents
    the exit—both are terminal states. State 1 contains the treasure, symbolized by
    a diamond. The game starts with the agent positioned in state 2 and has the options
    of moving left or right as actions. Upon reaching the treasure, the agent receives
    a reward of +10\. However, falling into the fire pit results in a penalty of –10\.
    Successfully exiting grants a reward of +4.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个简单的“到达宝藏”游戏，其中智能体试图获取宝藏然后退出。在这个游戏中，只有四个状态，如图 12.4 所示。其中，状态 0 代表火坑，状态
    3 代表出口——两者都是终端状态。状态 1 包含宝藏，用钻石表示。游戏开始时，智能体位于状态 2，可以选择向左或向右移动作为动作。到达宝藏后，智能体获得 +10
    的奖励。然而，掉入火坑会导致 -10 的惩罚。成功退出会获得 +4 的奖励。
- en: '![](../Images/CH12_F04_Khamis.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH12_F04_Khamis.png)'
- en: Figure 12.4 Reach the Treasure game
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.4 “到达宝藏”游戏
- en: 'The state *s* provides the complete information about the environment, including
    the agent’s position and the locations of the fire pit, treasure, and exit (state
    0 is the fire pit, state 1 is the treasure, state 2 is the current location, and
    state 3 is the exit). The game’s observations are collected and presented as an
    observation vector, such as *obs* = [2,0], indicating that the agent is in state
    2 and does not perceive the presence of the treasure. This is a partial view of
    the environment, as the agent does not have access to the complete state information,
    such as the locations of the fire pit or exit. The policy denotes the probabilities
    assigned to moving left or right based on the observation. For instance, a policy(obs)
    of [0.4, 0.6] signifies a 40% chance of moving left and a 60% chance of moving
    right. In the single trajectory shown in figure 12.4, we can calculate the expected
    return as follows: expected return (R) = 0.4 * (10) + 0.6 * (4) + 0.4 * 0.2 *
    (–10) + 0.4 * 0.8 * (0) + 0.4 * 0.8 * 0.9 * (0) + 0.4 * 0.8 * 0.1 * (4) = 5.728.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 状态 *s* 提供了关于环境的完整信息，包括智能体的位置以及火坑、宝藏和出口的位置（状态 0 是火坑，状态 1 是宝藏，状态 2 是当前位置，状态 3
    是出口）。游戏的观察结果被收集并呈现为一个观察向量，例如 *obs* = [2,0]，表示智能体处于状态 2，并且没有感知到宝藏的存在。这是环境的一个局部视图，因为智能体无法访问完整的州信息，例如火坑或出口的位置。策略表示基于观察移动左或右的概率。例如，策略(obs)为[0.4,
    0.6]表示有 40% 的概率向左移动，有 60% 的概率向右移动。在图 12.4 所示的单个轨迹中，我们可以计算期望回报如下：期望回报 (R) = 0.4
    * (10) + 0.6 * (4) + 0.4 * 0.2 * (–10) + 0.4 * 0.8 * (0) + 0.4 * 0.8 * 0.9 * (0)
    + 0.4 * 0.8 * 0.1 * (4) = 5.728。
- en: 'The goal in RL is to learn an optimal policy that maximizes the expected cumulative
    discounted reward. Value iteration, policy iteration, and policy gradients are
    different iterative methods used in reinforcement learning to achieve this goal.
    The value function in RL defines the expected cumulative reward of the agent starting
    from a particular state or state-action pair, following a certain policy. There
    are two types of value functions: the state-value function *V*(*s*) and the action-value
    function *Q*(*s*,*a*). The state-value function *V*(*s*) estimates the expected
    cumulative future rewards an agent may obtain, starting from a particular state
    *s* and following a policy *π*. It quantifies the desirability of a state based
    on its projected future rewards. The state-value function *V*(*s*) is given by
    the following formula:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习 (RL) 中，目标是学习一个最优策略，该策略最大化期望累积折现奖励。值迭代、策略迭代和策略梯度是强化学习中用于实现此目标的不同迭代方法。在强化学习中，价值函数定义了智能体从特定状态或状态-动作对开始，遵循一定策略的期望累积奖励。有两种类型的价值函数：状态价值函数
    *V*(*s*) 和动作价值函数 *Q*(*s*,*a*)。状态价值函数 *V*(*s*) 估计智能体从特定状态 *s* 开始并遵循策略 *π* 可能获得的期望累积未来奖励。它根据其预测的未来奖励来量化状态的可取性。状态价值函数
    *V*(*s*) 由以下公式给出：
- en: '|'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/CH12_F04_Khamis-EQ01.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH12_F04_Khamis-EQ01.png)'
- en: '| 12.1 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 12.1 |'
- en: where
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '*V^π*(*s*) is the expected return when starting in s and following *π* thereafter.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*V^π*(*s*) 是从状态 s 开始并随后遵循 *π* 的期望回报。'
- en: '*E[π]*[] denotes the expected value, given that the agent follows policy *π*.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*E[π]*[] 表示在智能体遵循策略 *π* 的情况下，期望值。'
- en: '*t* is any time step.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*t* 是任意时间步。'
- en: '*γ* is the discount factor, typically a value between 0 and 1, representing
    the present value of future rewards relative to immediate rewards. The purpose
    of discounting is to prioritize immediate rewards more heavily, reflecting the
    preference for rewards received sooner rather than later. A discount factor close
    to 0 makes the agent *myopic* (i.e., focused on immediate rewards), while a discount
    factor close to 1 makes the agent more *farsighted* (i.e., considering future
    rewards).'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*γ* 是折扣因子，通常是一个介于 0 和 1 之间的值，表示未来奖励相对于即时奖励的现值。折扣的目的在于更重视即时奖励，反映了对尽早获得奖励而非延迟获得奖励的偏好。接近
    0 的折扣因子会使代理变得 *短视*（即专注于即时奖励），而接近 1 的折扣因子会使代理更加 *远见*（即考虑未来奖励）。'
- en: 'The action-value function, *Q*(*s, a*), estimates the expected cumulative future
    rewards an agent can achieve by taking a specific action *a* from a given state
    *s* and following a policy *π*. It quantifies the “goodness” of taking a specific
    action in a specific state under a given policy. The action-value function *Q*(*s,
    a*) is given by the following formula:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 动作价值函数，*Q*(*s, a*)，估计代理从给定状态 *s* 采取特定动作 *a* 并遵循策略 *π* 可以实现的期望累积未来奖励。它量化了在给定策略下，在特定状态下采取特定动作的“好”的程度。动作价值函数
    *Q*(*s, a*) 由以下公式给出：
- en: '|'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/CH12_F04_Khamis-EQ02.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH12_F04_Khamis-EQ02.png)'
- en: '| 12.2 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 12.2 |'
- en: In *policy iteration methods*, we first compute the value *V^π*(*s*) of each
    state following an initial policy, and we use these value estimates to improve
    the policy. This means that from an initial policy we repeatedly alternate between
    policy evaluation and policy improvement steps (until convergence).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *策略迭代方法* 中，我们首先计算初始策略下每个状态的价值 *V^π*(*s*)，并使用这些价值估计来改进策略。这意味着从初始策略开始，我们反复在策略评估和策略改进步骤之间交替（直到收敛）。
- en: '*Policy gradient methods* learn a parameterized policy that is used by the
    agent to choose actions. The goal is to find the values of the policy parameters
    that maximize the expected cumulative reward over time.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*策略梯度方法* 学习一个参数化的策略，该策略由代理用于选择动作。目标是找到最大化随时间累积期望奖励的策略参数值。'
- en: Policy gradient
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度
- en: Policy gradient is a model-free policy-based method that doesn’t require explicit
    value function representation. The core idea is to favor actions that lead to
    higher returns while discouraging actions that result in lower rewards. This iterative
    process refines the policy over time, aiming to find a high-performing policy.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 政策梯度是一种无模型的政策方法，不需要显式地表示价值函数。核心思想是偏好导致更高回报的动作，同时贬低导致较低奖励的动作。这个迭代过程随着时间的推移不断优化策略，旨在找到高性能的策略。
- en: 'Instead of explicitly estimating the value function, policy gradient methods
    work by computing an estimator of the policy gradient and plugging it in to a
    stochastic gradient ascent algorithm. Policy gradient loss *L^(PG)* is one of
    the most commonly used gradient estimators:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是显式地估计价值函数，策略梯度方法通过计算策略梯度的估计值并将其插入到随机梯度上升算法中来实现。策略梯度损失 *L^(PG)* 是最常用的梯度估计器之一：
- en: '|'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/CH12_F04_Khamis-EQ03.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH12_F04_Khamis-EQ03.png)'
- en: '| 12.3 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 12.3 |'
- en: where
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: The expectation *E[t]* indicates the empirical average over a finite batch of
    samples.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 期望 *E[t]* 表示对有限批次样本的经验平均。
- en: '*π[θ]* is a stochastic policy that takes the observed states from the environment
    as an input and suggests actions to take as an output.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*π[θ]* 是一个随机策略，它将环境观察到的状态作为输入，并建议采取的动作作为输出。'
- en: '*Â[t]* is an estimate of the advantage function at time step *t*. This estimate
    basically tries to assess the relative value of the selected action in the current
    state. The advantage function represents the advantage of taking a particular
    action in a given state, compared to the expected value. It is calculated as the
    difference between the expected rewards from executing the suggested action (which
    often has the highest Q-value) and the estimated value function of the current
    state:'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Â[t]* 是在时间步 *t* 时刻优势函数的估计。这个估计基本上试图评估当前状态下所选动作的相对价值。优势函数表示在给定状态下采取特定动作相对于期望值的优势。它通过执行建议的动作（通常具有最高的
    Q 值）的期望奖励与当前状态的估计价值函数之间的差异来计算：'
- en: '|'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/CH12_F04_Khamis-EQ04.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH12_F04_Khamis-EQ04.png)'
- en: '| 12.4 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 12.4 |'
- en: where
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '*Q*(*s, a*) is the action-value function (also known as the Q-value), which
    represents the expected cumulative rewards from taking action *a* in state *s*
    following the policy.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Q*(*s, a*)是动作值函数（也称为Q值），它表示按照策略在状态*s*采取动作*a*的期望累积奖励。'
- en: '*V*(*s*) is the state-value function, which represents the expected cumulative
    rewards from state *s* following the policy.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*V*(*s*)是状态值函数，它表示从状态*s*按照策略获得的期望累积奖励。'
- en: As you can see in equation 12.4, if the advantage function is positive, indicating
    that the observed return is higher than the expected value, the gradient will
    be positive. This positive gradient means that the probabilities of the actions
    taken in that state will be increased in the future to enhance their likelihood.
    On the other hand, if the advantage function is negative, the gradient will be
    negative. This negative gradient implies that the probabilities of the selected
    actions will be decreased if similar states are encountered in the future.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在方程12.4中看到的，如果优势函数是正的，表示观察到的回报高于预期值，梯度将是正的。这个正梯度意味着在该状态下采取的动作在未来被增加其可能性的概率。另一方面，如果优势函数是负的，梯度将是负的。这个负梯度意味着如果在未来遇到类似的状态，选择动作的概率将会降低。
- en: 12.1.3 Model-based vs. model-free RL
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.3 基于模型的RL与无模型RL
- en: 'Reinforcement learning is categorized into two main types: model-based RL (MBRL)
    and model-free RL (MFRL). This classification is based on whether the RL agent
    possesses a model of the environment or not. The term *model* refers to an internal
    representation of the environment, encompassing its transition dynamics and reward
    function. Table 12.1 summarizes the differences between these two categories.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习分为两种主要类型：基于模型的RL（MBRL）和无模型RL（MFRL）。这种分类基于RL代理是否拥有环境的模型。术语*模型*指的是环境的内部表示，包括其转移动力学和奖励函数。表12.1总结了这两类之间的差异。
- en: Table 12.1 Model-based RL (MBRL) versus model-free RL (MFRL)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 表12.1 基于模型的RL（MBRL）与无模型RL（MFRL）
- en: '| Aspects | Model-based RL (MBRL) | Model-free RL (MFRL) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 方面 | 基于模型的RL（MBRL） | 无模型RL（MFRL） |'
- en: '| --- | --- | --- |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Environment model | Uses a known model or learns a model of the environment
    (i.e., transition probabilities) | Skips models and directly learns what action
    to take when (without necessarily finding out the exact model of the action) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 环境模型 | 使用已知模型或学习环境的模型（即转移概率） | 跳过模型，直接学习何时采取动作（不一定找出动作的确切模型） |'
- en: '| Rewards | Typically known or learned | Unknown or partially known. Model-free
    RL learns directly from the rewards received during interaction with the environment.
    |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 奖励 | 通常已知或学习到 | 未知或部分已知。无模型RL直接从与环境交互中获得的奖励中学习。 |'
- en: '| Actions | Selected to maximize the expected cumulative reward using the model
    | Selected to maximize the expected cumulative rewards based on the history of
    experiences |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 动作 | 使用模型选择以最大化期望累积奖励 | 根据经验的历史选择以最大化期望累积奖励 |'
- en: '| Policy | Policy learning is accomplished by learning a model of the environment
    dynamics. | Policy learning is achieved through trial and error, directly optimizing
    the policy based on observed experiences. |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 策略 | 通过学习环境动态模型来实现策略学习。 | 通过试错来实现策略学习，直接根据观察到的经验优化策略。 |'
- en: '| Design and tuning | MBRL can have a higher initial design and tuning effort
    due to model complexity. However, advancements are simplifying this process. |
    Requires less initial effort. However, MFRL hyperparameter tuning can also be
    challenging, especially for complex tasks. |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 设计和调整 | 由于模型复杂性，MBRL可能需要更高的初始设计和调整努力。然而，进步正在简化这一过程。 | 需要较少的初始努力。然而，MFRL的超参数调整也可能具有挑战性，尤其是在复杂任务中。
    |'
- en: '| Examples | AlphaZero, world models, and imagination-augmented agents (I2A)
    | Q-learning, advantage actor-critic (A2C), asynchronous advantage actor-critic
    (A3C), and proximal policy optimization (PPO) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 示例 | AlphaZero、世界模型和想象增强代理（I2A） | Q学习、优势演员-评论家（A2C）、异步优势演员-评论家（A3C）和近端策略优化（PPO）
    |'
- en: Based on how RL algorithms learn and update their policies from collected experiences,
    RL algorithms can also be classified as off-policy and on-policy RL. Off-policy
    methods learn from experiences generated by a policy different from the one being
    updated, while on-policy methods learn from experiences generated by the current
    policy being updated. Both on-policy and off-policy methods are often considered
    *model-free* because they directly learn policies or value functions from experiences
    without explicitly constructing a model of the environment’s dynamics, distinguishing
    them from model-based approaches. Table 12.2 summarizes the differences between
    off-policy and on-policy model-free RL methods.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 根据强化学习算法如何从收集的经验中学习和更新其策略，强化学习算法也可以分为离策略和在线策略强化学习。离策略方法从不同于正在更新的策略生成的经验中学习，而在线策略方法从正在更新的当前策略生成的经验中学习。在线策略和离策略方法通常被认为是*无模型*的，因为它们直接从经验中学习策略或价值函数，而没有明确构建环境动态的模型，这使它们与基于模型的方法区分开来。表12.2总结了离策略和无模型在线策略强化学习方法之间的差异。
- en: Table 12.2 Off-policy versus on-policy RL methods
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 表12.2 离策略与在线策略强化学习方法
- en: '| Aspects | Off-policy RL methods | On-policy RL methods |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 方面 | 离策略强化学习方法 | 在线策略强化学习方法 |'
- en: '| --- | --- | --- |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Learning approach | Learn from experiences generated by a different policy
    than the one being updated | Learn from experiences generated by the current policy
    being updated |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 学习方法 | 从不同于正在更新的策略生成的经验中学习 | 从正在更新的当前策略生成的经验中学习 |'
- en: '| Sample efficiency | Typically more sample-efficient due to reusing past experiences
    (recorded data) | Typically less sample-efficient, as the batch of experiences
    is discarded after each policy update (past experiences are not explicitly stored)
    |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 样本效率 | 通常由于重用过去经验（记录的数据）而具有更高的样本效率 | 通常样本效率较低，因为每次策略更新后都会丢弃经验批次（过去经验没有明确存储）
    |'
- en: '| Policy evaluation | Can learn the value function and policy separately, enabling
    different algorithms (e.g., Q-learning, DDPG, TD3) | Policy evaluation and improvement
    are typically intertwined in on-policy algorithms (e.g., REINFORCE, A2C, PPO).
    |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 策略评估 | 可以分别学习价值函数和策略，使不同的算法（例如，Q学习、DDPG、TD3）成为可能 | 在在线策略算法（例如，REINFORCE、A2C、PPO）中，策略评估和改进通常是交织在一起的。
    |'
- en: '| Pros | More sample-efficient, can learn from diverse experiences, enables
    reuse of past data, useful if large amounts of prior data are available | Simpler
    and more straightforward, avoids off-policy correction, can converge to better
    local optima, suitable for scenarios with limited data or online learning |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 优点 | 更高的样本效率，可以从多样化的经验中学习，允许重用过去的数据，如果可用的先前数据量很大则很有用 | 更简单、更直接，避免了离策略校正，可以收敛到更好的局部最优解，适用于数据有限或在线学习的场景
    |'
- en: '| Cons | Requires careful off-policy correction, less suitable for online learning
    or tasks with limited data | Less sample-efficient, discards past experiences,
    limited exploration diversity, may converge to suboptimal policies |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 缺点 | 需要仔细的离策略校正，不太适合在线学习或数据有限的任务 | 样本效率较低，丢弃过去经验，探索多样性有限，可能收敛到次优策略 |'
- en: The following two subsections provide more details about A2C and PPO as examples
    of the on-policy methods used in this chapter.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 以下两个小节提供了更多关于A2C和PPO的细节，作为本章中使用的在线策略方法的示例。
- en: 12.1.4 Actor-critic methods
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.4 行为者-评论家方法
- en: 'Figure 12.5 shows the advantage actor-critic (A2C) architecture as an example
    of actor-critic methods. As the name suggests, this architecture consists of two
    models: the *actor* and the *critic*. The actor is responsible for learning and
    updating the policy. It takes the current state as input and outputs the probability
    distribution over the actions that represent the policy. The critic, on the other
    hand, focuses on evaluating the action suggested by the actor. It takes the state
    and action as input and estimates the advantage of taking that action in that
    particular state. The advantage represents how much better (or worse) the action
    is compared to the average action in that state based on expected future rewards.
    This feedback from the critic helps the actor learn and update the policy to favor
    actions with higher advantages.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.5展示了优势演员-评论家（A2C）架构，作为演员-评论家方法的例子。正如其名所示，这个架构由两个模型组成：*演员*和*评论家*。演员负责学习和更新策略。它以当前状态作为输入，输出表示策略的动作概率分布。另一方面，评论家专注于评估演员建议的动作。它以状态和动作作为输入，并估计在该特定状态下采取该动作的优势。优势表示与该状态下平均动作相比，该动作有多好（或有多坏），这是基于预期的未来奖励。来自评论家的这种反馈有助于演员学习和更新策略，以优先考虑具有更高优势的动作。
- en: '![](../Images/CH12_F05_Khamis.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图12.5](../Images/CH12_F05_Khamis.png)'
- en: Figure 12.5 The advantage actor-critic (A2C) architecture
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.5](../Images/CH12_F05_Khamis.png)'
- en: A2C is a synchronous, model-free algorithm that aims to learn both the policy
    (the actor) and the value function (the critic) simultaneously. It learns an optimal
    policy by iteratively improving the actor and critic networks. By estimating advantages,
    the algorithm can provide feedback on the quality of the actions taken by the
    actor. The critic network helps estimate the value function, providing a baseline
    for the advantages calculation. This combination allows the algorithm to update
    the policy in a more stable and efficient manner.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: A2C是一种同步、无模型的算法，旨在同时学习策略（演员）和值函数（评论家）。它通过迭代改进演员和评论家网络来学习最优策略。通过估计优势，该算法可以为演员采取的动作的质量提供反馈。评论家网络帮助估计值函数，为优势计算提供基准。这种组合使得算法能够以更稳定和高效的方式更新策略。
- en: 12.1.5 Proximal policy optimization
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.5 近端策略优化
- en: The proximal policy optimization (PPO) algorithm is an on-policy model-free
    RL designed by OpenAI [2], and it has been successfully used in many applications
    such as video gaming and robot control. PPO is based on the actor-critic architecture.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 近端策略优化（PPO）算法是由OpenAI [2] 设计的一种基于策略的无模型强化学习（RL）算法，它已在视频游戏和机器人控制等许多应用中取得了成功。PPO基于演员-评论家架构。
- en: 'In RL, the agent generates its own training data through interactions with
    the environment. Unlike supervised machine learning, which relies on static datasets,
    RL’s training data is dynamically dependent on the current policy. This dynamic
    nature leads to constantly changing data distributions, introducing potential
    instability during training. In the policy gradient method explained previously,
    if you continuously apply gradient ascent on a single batch of collected experiences,
    it can lead to updates that push the parameters of the network too far from the
    range where the data was collected. Consequently, the advantage function, which
    provides an estimate of the true advantage, becomes inaccurate, and the policy
    can be severely disrupted. To address this problem, two primary variants of PPO
    have been proposed: PPO-penalty and PPO-clip.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习（RL）中，智能体通过与环境的交互生成自己的训练数据。与依赖于静态数据集的监督机器学习不同，RL的训练数据动态地依赖于当前策略。这种动态特性导致数据分布不断变化，在训练过程中引入了潜在的不稳定性。在之前解释的政策梯度方法中，如果你对收集到的单个经验批次连续应用梯度上升，可能会导致更新将网络的参数推离数据收集范围，从而使得提供真实优势估计的优势函数变得不准确，并严重破坏策略。为了解决这个问题，已经提出了PPO的两个主要变体：PPO-penalty和PPO-clip。
- en: PPO-penalty
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: PPO-penalty
- en: 'In *PPO-penalty*, a constraint is incorporated in the objective function to
    ensure that the policy update does not deviate too much from the old policy. This
    idea is the basis of trust region policy optimization (TRPO). By enforcing a trust
    region constraint, TRPO restricts the policy update to a manageable region and
    prevents large policy shifts. PPO-penalty is primarily inspired by TRPO and uses
    the following unconstrained objective function, which can be optimized using stochastic
    gradient ascent:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *PPO-penalty* 中，目标函数中包含了一个约束，以确保策略更新不会过多偏离旧策略。这个想法是信任域策略优化 (TRPO) 的基础。通过强制执行信任域约束，TRPO
    将策略更新限制在可管理的区域内，并防止策略发生大的变化。PPO-penalty 主要受 TRPO 启发，并使用以下无约束目标函数，该函数可以使用随机梯度上升进行优化：
- en: '|'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/CH12_F05_Khamis-EQ05.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH12_F05_Khamis-EQ05.png)'
- en: '| 12.5 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 12.5 |'
- en: where *θ[old]* is the vector of policy parameters before the update, *β* is
    a fixed penalty coefficient, and the Kullback–Leibler divergence (KL) represents
    the divergence between the updated and old policies. This constraint is integrated
    into the objective function to avoid the risk of moving too far from the old policy.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *θ[old]* 是更新前的策略参数向量，*β* 是一个固定的惩罚系数，而 Kullback–Leibler 距离 (KL) 表示更新策略与旧策略之间的差异。这个约束被整合到目标函数中，以避免远离旧策略的风险。
- en: Kullback–Leibler divergence
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Kullback–Leibler 距离
- en: '*Kullback*–*Leibler (KL) divergence*, also known as *relative entropy*, is
    a metric that quantifies the dissimilarity between two probability distributions.
    KL divergence between two probability distributions *P* and *Q* is defined as
    KL(*P* || *Q*) = ∫ *P*(*x*) ⋅ log(*P*(*x*) / *Q*(x)) *dx*, where *P*(*x*) and
    *Q*(*x*) represent the probability density functions (PDFs) of the two distributions.
    The integral is taken over the entire support of the random variable *x* (i.e.,
    the range of values of the random variable where the PDF is nonzero). The following
    figure shows the KL divergence between two Gaussian distributions with different
    means and variances.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*Kullback*–*Leibler (KL) 距离*，也称为 *相对熵*，是一个度量，用于量化两个概率分布之间的差异。两个概率分布 *P* 和 *Q*
    之间的 KL 距离定义为 KL(*P* || *Q*) = ∫ *P*(*x*) ⋅ log(*P*(*x*) / *Q*(x)) *dx*，其中 *P*(*x*)
    和 *Q*(*x*) 代表两个分布的概率密度函数 (PDFs)。积分是在随机变量 *x* 的整个支持集上进行的（即 PDF 非零的随机变量的值域）。以下图显示了具有不同均值和方差的两个高斯分布之间的
    KL 距离。'
- en: '![](../Images/CH12_F05_UN01_Khamis.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH12_F05_UN01_Khamis.png)'
- en: KL divergence between two Gaussian distributions
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 两个高斯分布之间的 KL 距离
- en: The KL divergence is equal to zero if, and only if, *P* and *Q* are identical
    distributions.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果，并且仅当 *P* 和 *Q* 是相同的分布时，KL 距离等于零。
- en: PPO-clip
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: PPO-clip
- en: In *PPO-clip*, a ratio *r*(*θ*) is defined as a probability ratio between the
    updated policy and the old version of the policy. Given a sequence of sample actions
    and states, this *r*(*θ*) value will be larger than 1 if the action is more likely
    now than it was in the old version of the policy, and it will be somewhere between
    0 and 1 if it is less likely now than it was before the last gradient step.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *PPO-clip* 中，定义了一个比率 *r*(*θ*)，它是更新策略与旧策略版本之间的概率比率。给定一系列样本动作和状态，如果动作现在比旧策略版本更有可能，则
    *r*(*θ*) 值将大于 1，如果动作现在比上次梯度步之前更不可能，则它将在 0 和 1 之间。
- en: 'The central objective function to be maximized in PPO-clip takes the following
    form:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PPO-clip 中要最大化的中心目标函数具有以下形式：
- en: '|'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/CH12_F05_UN01_Khamis-EQ06.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH12_F05_UN01_Khamis-EQ06.png)'
- en: '| 12.6 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 12.6 |'
- en: where *L^(CLIP)* is the clipped surrogate objective, which is an *expectation
    operator* computed over batches of trajectories, and epsilon *ϵ* is a hyperparameter
    (e.g., *ϵ* = 0.2). As you can see in equation 12.6, the expectation operator is
    taken over the minimum of two terms. The first term represents the default objective
    used in normal policy gradients. It encourages the policy to favor actions that
    result in a high positive advantage compared to a baseline. The second term is
    a clipped or truncated version of the normal policy gradients. It applies a clipping
    operation to ensure that the update remains within a specified range, specifically
    between 1 – *ϵ* and 1 + *ϵ*.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *L^(CLIP)* 是剪裁代理目标，它是一个在轨迹批次上计算的 *期望算子*，epsilon *ϵ* 是一个超参数（例如，*ϵ* = 0.2）。正如方程
    12.6 所示，期望算子是在两个项的最小值上取的。第一项代表在正常策略梯度中使用的默认目标，它鼓励策略优先选择与基线相比具有高正优势的动作。第二项是正常策略梯度的剪裁或截断版本。它应用剪裁操作以确保更新保持在指定的范围内，具体是在
    1 – *ϵ* 和 1 + *ϵ* 之间。
- en: The clipped surrogate objective in PPO has different regions that define how
    the objective function behaves based on the advantage estimate *Â[t]* and the
    ratio of probabilities, as illustrated in figure 12.6.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: PPO中的剪辑代理目标在不同的区域定义了基于优势估计 *Â[t]* 和概率比率的客观函数的行为，如图12.6所示。
- en: '![](../Images/CH12_F06_Khamis.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH12_F06_Khamis.png)'
- en: Figure 12.6 Clipped surrogate objective in PPO
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.6 PPO中的剪辑代理目标
- en: On the left side of figure 12.6, where the advantage function is positive, the
    objective function represents cases where the selected action has a better-than-expected
    effect on the outcome. It’s important to observe how the objective function flattens
    out when the ratio (*r*) becomes too high. This occurs when the action is more
    likely under the current policy compared to the old policy. The clipping operation
    limits the update to a range where the new policy does not deviate significantly
    from the old policy, preventing excessively large policy updates that may disrupt
    training stability.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在图12.6的左侧，优势函数为正时，目标函数表示所选行为对结果有比预期更好的影响。当比率（*r*）变得过高时，目标函数会变得平坦。这发生在当前策略下行为比旧策略更有可能发生的情况下。剪辑操作将更新限制在一个范围内，新策略不会与旧策略有显著偏差，从而防止过大的策略更新，这可能会破坏训练稳定性。
- en: On the right side of figure 12.6, where the advantage function is negative,
    it represents situations where the action has an estimated negative effect on
    the outcome. The objective function flattens out when the ratio (*r*) approaches
    zero. This corresponds to actions that are much less likely under the current
    policy compared to the old policy. This flattening effect prevents overdoing updates
    that could otherwise reduce the probabilities of these actions to zero.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在图12.6的右侧，优势函数为负时，它表示行为对结果有估计的负面影响。当比率（*r*）接近零时，目标函数会变得平坦。这对应于在当前策略下比旧策略发生可能性小得多的行为。这种平坦化效应防止了过度更新，否则可能会将这些行为的概率降低到零。
- en: 12.1.6 Multi-armed bandit (MAB)
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.6 多臂老虎机（MAB）
- en: The *multi-armed bandit* (MAB) is a class of reinforcement learning problems
    with a single state. In MAB, an agent is faced with a set of actions or “arms”
    to choose from, and each action has an associated reward distribution. The agent’s
    goal is to maximize the total reward accumulated over a series of actions. The
    agent does not modify its environment through its actions, does not consider state
    transitions, and always stays in the same single state. It focuses on selecting
    the most rewarding action at each time step without considering the impact on
    the environment’s state.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '*多臂老虎机*（MAB）是一类具有单个状态的强化学习问题。在MAB中，一个代理面对一组可供选择的行为或“臂”，每个行为都有一个相关的奖励分布。代理的目标是在一系列行为中最大化累积的总奖励。代理不会通过其行为修改其环境，不考虑状态转换，并且始终保持在同一单个状态。它专注于在每个时间步选择最有利的行为，而不考虑对环境状态的影响。'
- en: Slot machine (one-armed bandit)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 老虎机（单臂强盗）
- en: A *slot machine* (aka a fruit machine, pokies, or one-armed bandit) is a popular
    gambling device typically found in casinos. It is a mechanical or electronic gaming
    machine that features multiple spinning reels with various symbols on them. Players
    insert coins or tokens into the machine and then pull a lever (hence the term
    “one-armed bandit”) or press a button to initiate the spinning of the reels. The
    objective of playing a slot machine is to align the symbols on the spinning reels
    in a winning combination. The term “bandit” originates from the analogy to a slot
    machine, where the agent pulls an arm (like pulling the lever on a slot machine)
    to receive varying rewards. This highlights how people perceive slot machines,
    especially older mechanical ones, as resembling a thief or bandit taking the player’s
    money.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*老虎机*（又称水果机、老虎机或单臂强盗）是一种流行的赌博设备，通常在赌场中找到。它是一种机械或电子游戏机，具有多个带有各种符号的旋转滚筒。玩家将硬币或代币投入机器，然后拉动杠杆（因此得名“单臂强盗”）或按按钮以启动滚筒的旋转。玩老虎机的目的是将旋转滚筒上的符号排列成获胜组合。术语“强盗”源于与老虎机的类比，其中代理拉动一个臂（就像拉动老虎机的杠杆一样）以获得不同的奖励。这突显了人们如何看待老虎机，尤其是较老的机械老虎机，就像是一个窃贼或强盗在夺取玩家的钱。'
- en: The MAB agent faces the exploration versus exploitation dilemma. To learn the
    best actions, it must explore various options (exploration). However, it also
    needs to quickly converge and swiftly focus on the most promising action (exploitation)
    based on its current beliefs. In supervised and unsupervised machine learning,
    the primary goal is to fit a prediction model (in supervised learning) or to discover
    patterns within the given data (in unsupervised learning), without an explicit
    notion of exploration, as seen in reinforcement learning, where the agent interacts
    with an environment to learn optimal behavior through trial and error. MAB problems
    provide valuable insights into learning from limited feedback and balancing exploration
    against discovering high-reward actions. In MAB, the agent’s objective is to exploit
    the actions that have historically yielded high rewards while exploring to gather
    information about potentially more rewarding actions.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: MAB代理面临着探索与利用的困境。为了学习最佳动作，它必须探索各种选项（探索）。然而，它还需要根据其当前信念快速收敛并迅速关注最有希望的动作（利用）。在监督学习和无监督学习中，主要目标是拟合预测模型（在监督学习中）或发现给定数据中的模式（在无监督学习中），没有明确的探索概念，正如在强化学习中所见，代理与环境交互以通过试错学习最佳行为。MAB问题为从有限的反馈中学习以及平衡探索与发现高奖励动作提供了宝贵的见解。在MAB中，代理的目标是在历史上产生高奖励的动作中利用，同时探索以收集有关可能更有奖励的动作的信息。
- en: 'To understand MAB, imagine you’re in a casino, and in front of you is a row
    of slot machines (one-armed bandits). Each slot machine represents an “arm” of
    the MAB. Let’s assume that you are presented with three slot machines, as shown
    in figure 12.7\. Every time you decide to play a slot machine, your situation
    (or state) is the same: “Which slot machine should I play next?” The environment
    doesn’t change or provide you with different conditions; you’re perpetually making
    decisions in this singular state. There’s no context such as “If the casino is
    crowded, then play machine A” or “If it’s raining outside, then play machine B.”
    It’s always just “Which machine should I play?” Your action is choosing a slot
    machine to play—you insert a coin and pull the lever. You are allowed to pull
    the levers of these machines for a total of 90 times, and each slot machine has
    its own payoff distribution, characterized by its mean and standard deviation.
    However, at the beginning, you are unaware of the specific details of these distributions.
    After pulling the lever, the machine might give you a payout (a positive reward)
    or nothing (a reward of zero). Over time, you’re trying to discern if one machine
    gives a higher payout more frequently than the others.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解MAB，想象你身处一家赌场，面前是一排老虎机（单臂老虎机）。每台老虎机代表MAB的一个“臂”。让我们假设你面前有三台老虎机，如图12.7所示。每次你决定玩老虎机时，你的情况（或状态）都是相同的：“我应该玩哪台老虎机？”环境不会改变或提供不同的条件；你始终在这个单一状态下做出决定。没有上下文，例如“如果赌场拥挤，就玩机器A”或“如果外面下雨，就玩机器B。”总是只是“我应该玩哪台机器？”你的动作是选择一台老虎机来玩——你投入硬币并拉动杠杆。你被允许拉动这些机器的杠杆总共90次，每台老虎机都有自己的收益分布，由其均值和标准差来表征。然而，一开始，你并不知道这些分布的具体细节。拉动杠杆后，机器可能会给你支付（正奖励）或什么也没有（零奖励）。随着时间的推移，你试图判断是否有哪台机器比其他机器更频繁地给出更高的支付。
- en: '![](../Images/CH12_F07_Khamis.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH12_F07_Khamis.png)'
- en: Figure 12.7 Three slot machines as a noncontextual multi-armed bandit (MAB)
    problem. The payoff distribution’s mean and standard deviations of these three
    machines are (8,0.5), (10, 0.7), and (5, 0.45), respectively.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.7 三台老虎机作为一个非上下文的多臂老虎机（MAB）问题。这三台机器的收益分布的均值和标准差分别为（8，0.5），（10，0.7）和（5，0.45）。
- en: The objective in this MAB problem is to maximize the total payoff or cumulative
    reward obtained over the 90 trials by choosing the most rewarding slot machine.
    In other words, you’re trying to learn the payoff distributions of the machines.
    The term *payoff distribution* refers to the probability distributions of rewards
    (or payoffs) that each machine (or arm) provides. Since you don’t have prior knowledge
    about these payoff distributions, you need to explore the slot machines by trying
    different options to gather information about their performance. The challenge
    is to strike a balance between exploration and exploitation. Exploration involves
    trying different machines to learn their payoff distributions, while exploitation
    involves choosing the machine that is believed to yield the highest rewards based
    on the available information.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个多臂老虎机（MAB）问题中的目标是，通过选择最有回报的老虎机，在90次试验中最大化获得的总回报或累积奖励。换句话说，你正在尝试学习机器的回报分布。术语*回报分布*指的是每个机器（或臂）提供的奖励（或回报）的概率分布。由于你对这些回报分布没有先验知识，你需要通过尝试不同的选项来探索老虎机，以收集有关它们性能的信息。挑战在于在探索和利用之间找到平衡。探索涉及尝试不同的机器以学习它们的回报分布，而利用则涉及根据可用信息选择被认为能带来最高回报的机器。
- en: 'You can apply various bandit algorithms or strategies to determine the best
    approach for selecting the slot machines and maximizing your cumulative reward
    over the 90 trials. Examples of these strategies include, but are not limited
    to, explore-only, exploit-only greedy, epsilon-greedy (ε-greedy), and upper confidence
    bound (UCB):'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以应用各种老虎机算法或策略来确定选择老虎机和最大化90次试验累积奖励的最佳方法。这些策略包括但不限于仅探索、仅利用贪婪、ε-贪婪（ε-greedy）和上置信界（UCB）：
- en: '*Explore-only strategy*—In this strategy, the agent randomly selects a slot
    machine to play at each trial without considering the past results. In MAB, “regret”
    is a common measure, calculated as the difference between the maximum possible
    reward and the reward obtained from each selected machine. For example, if we
    apply the explore-only strategy in the 90 trials, and considering the mean of
    the payoff distribution of each slot machine, we will get a total average return
    of 30 × 8 + 30 × 10 + 30 × 5 = 690. The maximum possible reward can be obtained
    if you use machine 2 during the 90 trials. The maximum reward in this case will
    be 90 × 10 = 900. This means that regret *ρ* = 900 – 690 = 210.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*仅探索策略*—在这种策略中，代理在每次试验中随机选择一台老虎机进行游戏，而不考虑过去的结果。在MAB中，“遗憾”是一个常见的衡量标准，计算为最大可能奖励与从每个选定的机器获得的奖励之间的差异。例如，如果我们应用仅探索策略进行90次试验，并考虑每台老虎机回报分布的平均值，我们将得到总平均回报为30
    × 8 + 30 × 10 + 30 × 5 = 690。如果你在90次试验中使用机器2，可以获得最大奖励。在这种情况下，最大奖励将是90 × 10 = 900。这意味着遗憾*ρ*
    = 900 – 690 = 210。'
- en: '*Exploit-only greedy strategy*—In this case, the agent tries each machine once
    and then selects the slot machine that has the highest estimated mean reward.
    For example, assume that in the first trial, the agent gets payoffs of 7, 6, and
    3 from machines 1, 2, and 3 respectively. The agent will then focus on using machine
    1, thinking that it is the most rewarding. This can lead the agent to get stuck
    due to a lack of exploration.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*仅利用贪婪策略*—在这种情况下，代理尝试每台机器一次，然后选择具有最高估计平均回报的老虎机。例如，假设在第一次试验中，代理从机器1、2和3分别获得7、6和3的回报。代理将随后专注于使用机器1，认为它是回报最高的。这可能导致代理由于缺乏探索而陷入困境。'
- en: ε*-greedy strategy*—Here, the agent tries to strike a balance between exploration
    and exploitation by randomly selecting a slot machine with a certain probability
    (epsilon). This is the exploration part, where the agent occasionally tries out
    all three machines to gather more information about them. With a probability of
    1 – ε, the agent chooses the machine that has the highest estimated reward based
    on past experiences. This is the exploitation part, where the agent opts for the
    action that appears to be the best based on data the agent has gathered so far.
    For example, for the 90 trials and if ε = 10%, the agent will randomly select
    a slot machine about 9 times (10% of 90). The other 81 trials (90% of 90) would
    see the agent choosing the slot machine that has, based on the trials up to that
    point, yielded the highest average reward.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ε*-贪婪策略*—在这里，智能体通过以一定概率（epsilon）随机选择老虎机来尝试在探索和利用之间取得平衡。这是探索部分，其中智能体偶尔尝试所有三台机器以收集更多关于它们的信息。以1
    – ε的概率，智能体根据过去的经验选择具有最高估计奖励的机器。这是利用部分，其中智能体根据迄今为止收集的数据选择看似最好的动作。例如，对于90次试验，如果ε
    = 10%，智能体将随机选择老虎机大约9次（90的10%）。其他81次试验（90%的90）将看到智能体选择根据到目前为止的试验，产生了最高平均奖励的老虎机。
- en: '*Upper confidence bound (UCB) strategy*—In this strategy, the agent selects
    a machine based on a trade-off between its estimated mean reward and the uncertainty
    or confidence in that estimate. It prioritizes exploring machines with high potential
    rewards but high uncertainty in their estimates to reduce uncertainty and maximize
    rewards over time. In UCB, the arm (the action *A*[t]) is chosen at time step
    *t* using the following formula:'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*上置信界（UCB）策略*—在这种策略中，智能体根据其估计的平均奖励和估计的不确定性或置信度之间的权衡来选择机器。它优先探索具有高潜在奖励但估计不确定性高的机器，以减少不确定性并最大化长期奖励。在UCB中，选择臂（动作
    *A*[t]）在时间步 *t* 使用以下公式：'
- en: '|'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/CH12_F07_Khamis-EQ07.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH12_F07_Khamis-EQ07.png)'
- en: '| 12.7 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 12.7 |'
- en: where *Q[t]*(*a*) is the estimated value of action *a* at trial *t*. *Q[t]*(*a*)
    = sum of rewards/*N[t]*(*a*). *N[t]*(*a*) is the number of trials where action
    *a* has been selected, prior to trial *t*. The first term on the right side of
    equation 12.7 represents the exploitation part. If you always pulled the arm with
    the highest *Q[t]*(*a*), you would always be exploiting the current knowledge
    without exploring other arms. The second term is the exploration part. As the
    number of trials *t* increases, the exploration term generally increases, but
    it’s reduced by how often action *a* has already been selected. The multiplier
    *c* scales the influence of this exploration term.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *Q[t]*(*a*) 是在试验 *t* 时动作 *a* 的估计值。*Q[t]*(*a*) = 奖励总和/*N[t]*(*a*)。*N[t]*(*a*)
    是在试验 *t* 之前选择动作 *a* 的试验次数。方程12.7右侧的第一个项代表利用部分。如果你总是拉动具有最高 *Q[t]*(*a*) 的臂，你将总是利用当前的知识而不探索其他臂。第二个项是探索部分。随着试验次数
    *t* 的增加，探索项通常会增加，但会被动作 *a* 已经被选择的频率所减少。乘数 *c* 调整了此探索项的影响。
- en: Each strategy employs a different approach to balance exploration and exploitation,
    leading to different levels of regret. This regret level quantifies the cumulative
    loss an agent incurs due to not always choosing the optimal action. Intuitively,
    it measures the difference between the reward an agent could have achieved by
    always pulling the best arm (i.e., by selecting the optimal action) and the reward
    the agent actually received by following a certain strategy.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 每种策略采用不同的方法来平衡探索和利用，导致不同的遗憾水平。这种遗憾水平量化了智能体由于不总是选择最佳动作而造成的累积损失。直观上，它衡量了智能体通过始终拉动最佳臂（即通过选择最佳动作）所能获得的奖励与智能体实际通过遵循某种策略所获得的奖励之间的差异。
- en: Let’s look at Python implementations of the four MAB strategies. We’ll start
    by setting the numbers of arms (actions), the payoff distributions of each slot
    machine, and the number of trials. We’ll also define a `sample_payoff` to sample
    a payoff from a slot machine and calculate the maximum possible reward.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看四种MAB策略的Python实现。我们将首先设置臂（动作）的数量、每个老虎机的收益分布和试验次数。我们还将定义一个`sample_payoff`来从老虎机中采样收益并计算最大可能奖励。
- en: Listing 12.1 MAB strategies
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.1 多臂老虎机（MAB）策略
- en: '[PRE0]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① Set the number of slot machines (arms).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ① 设置老虎机（臂）的数量。
- en: ② Specify payoff distribution.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ② 指定收益分布。
- en: ③ Set the number of trials.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 设置试验次数。
- en: ④ Function to sample a payoff from a slot machine
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 从老虎机中采样收益的函数
- en: ⑤ Calculate the maximum possible reward.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 计算最大可能奖励。
- en: 'As a continuation of listing 12.1, we’ll define a function named `explore_only()`
    that implements the explore-only strategy. In this function, `total_regret` is
    initialized, and we iterate over the specified number of trials. A slot machine
    is randomly selected by generating a random integer between 0 and *K* – 1 (inclusive),
    where *K* represents the number of slot machines. We then sample the payoff from
    the selected slot machine by calling the `sample_payoff()` function, which returns
    a reward value based on the payoff distribution of the selected machine. The regret
    is calculated by subtracting the reward obtained from the maximum possible reward
    (`max_reward`). Here, we consider the maximum value of the mean values as the
    points of maximum probability in the payoff distributions of the three machines.
    The average regret is returned as output:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 作为12.1列表的延续，我们将定义一个名为`explore_only()`的函数，该函数实现了仅探索的策略。在这个函数中，初始化`total_regret`，然后遍历指定的试验次数。通过生成一个介于0和*K*
    - 1（包含）之间的随机整数来随机选择一台投币机，其中*K*代表投币机的数量。然后，通过调用`sample_payoff()`函数从所选的投币机中采样收益，该函数根据所选机器的收益分布返回一个奖励值。遗憾值通过从最大可能奖励（`max_reward`）中减去获得的奖励来计算。在这里，我们考虑平均值的最大值作为三个机器收益分布中最大概率的点。平均遗憾值作为输出返回：
- en: '[PRE1]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① Randomly select a slot machine.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ① 随机选择一台投币机。
- en: ② Sample the payoff from the selected slot machine.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ② 从所选的投币机中采样收益。
- en: ③ Calculatethe regret.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 计算遗憾值。
- en: ④ Calculate the total regret.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 计算总遗憾值。
- en: ⑤ Calculate the average regret.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 计算平均遗憾值。
- en: 'The second strategy is defined in the `exploit_only_greedy()` function. This
    function selects the slot machine with the highest mean payoff by finding the
    index of the maximum mean payoff from the list of payoff means (`payoff_params[i]["mean"]`)
    for each machine (`i`). The `np.argmax()` function returns the index of the maximum
    mean payoff, representing the machine that is believed to provide the highest
    expected reward:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种策略在`exploit_only_greedy()`函数中定义。该函数通过从每个机器的收益平均值列表（`payoff_params[i]["mean"]`）中找到最大平均收益值的索引来选择具有最高平均收益率的投币机。`np.argmax()`函数返回最大平均收益值的索引，代表被认为能提供最高预期奖励的机器：
- en: '[PRE2]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ① Select the slot machine with the highest mean payoff for exploitation.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ① 选择具有最高平均收益率的投币机进行利用。
- en: ② Sample the payoff from the selected slot machine.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ② 从所选的投币机中采样收益。
- en: ③ Calculatethe regret.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 计算遗憾值。
- en: ④ Calculate the total regret.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 计算总遗憾值。
- en: ⑤ Calculate the average regret.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 计算平均遗憾值。
- en: 'The following `epsilon_greedy(epsilon)` function implements the epsilon-greedy
    strategy. This function checks if a randomly generated number between 0 and 1
    is less than the epsilon value. If it is, the algorithm performs exploration by
    randomly selecting a slot machine for exploration. If this condition is not satisfied,
    the algorithm performs exploitation by selecting the slot machine with the highest
    mean payoff:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 以下`epsilon_greedy(epsilon)`函数实现了epsilon-greedy策略。该函数检查生成的介于0和1之间的随机数是否小于epsilon值。如果是，算法通过随机选择一台投币机进行探索来执行探索。如果不满足此条件，算法通过选择具有最高平均收益率的投币机来执行利用：
- en: '[PRE3]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① Randomly select a slot machine for exploration.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ① 随机选择一台投币机进行探索。
- en: ② Select the slot machine with the highest mean payoff for exploitation.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ② 选择具有最高平均收益率的投币机进行利用。
- en: ③ Sample the payoff from the selected slot machine.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 从所选的投币机中采样收益。
- en: 'The following `ucb(c)` function implements the upper confidence bound (UCB)
    strategy. This function starts by initializing an array to keep track of the number
    of plays for each slot machine. The function also initializes an array to accumulate
    the sum of rewards obtained from each slot machine and initializes a variable
    to accumulate the total regret. The code includes a loop that plays each slot
    machine once to gather initial rewards and update the counts and sum of rewards:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 以下`ucb(c)`函数实现了上置信界（UCB）策略。该函数首先初始化一个数组来跟踪每个投币机的游戏次数。该函数还初始化一个数组来累计从每个投币机获得的奖励总和，并初始化一个变量来累计总遗憾值。代码包括一个循环，每次玩一次每个投币机以收集初始奖励并更新计数和奖励总和：
- en: '[PRE4]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ① Initialize an array to keep track of the number of plays for each slot machine.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ① 初始化一个数组来跟踪每个投币机的游戏次数。
- en: ② Initialize an array to accumulate the sum of rewards obtained from each slot
    machine.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ② 初始化一个数组来累计从每个投币机获得的奖励总和。
- en: ③ Initialize the total regret of each slot machine.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 初始化每个老虎机的总后悔值。
- en: ④ Play each slot machine once to initialize.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 每次玩一次老虎机以初始化。
- en: ⑤ Continue playing with the UCB strategy.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 继续使用UCB策略进行游戏。
- en: ⑥ Calculate the UCB values.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 计算UCB值。
- en: 'The following code snippet is used to run the strategies, calculate average
    regrets, and print the results:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段用于运行策略，计算平均后悔值，并打印结果：
- en: '[PRE5]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ① Set the epsilon value for the epsilon-greedy strategy, and run the strategy.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ① 设置epsilon-greedy策略的epsilon值，并运行策略。
- en: ② Set the value of the exploration parameter c for the UCB strategy, and run
    the strategy.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ② 设置UCB策略的探索参数c的值，并运行策略。
- en: ③ Print the results
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 打印结果
- en: 'Given the random sampling included in the code, your results will vary, but
    running the code will produce results something like these:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 由于代码中包含随机抽样，您的结果可能会有所不同，但运行代码将产生类似以下的结果：
- en: '[PRE6]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: MAB algorithms and concepts find applications in various real-world scenarios
    where decision-making under uncertainty and exploration–exploitation trade-offs
    are involved. Examples of real-world applications of MABs include, but are not
    limited to, resource allocation (dynamically allocating resources to different
    options to maximize performance), online advertising (dynamically allocating ad
    impressions to different options and learning which ads yield the highest click-through
    rate, which is the probability that a user clicks on an ad), design of experiments
    and clinical trials (optimizing the allocation of patients to different treatment
    options), content recommendation (personalizing content recommendations for users),
    and website optimization (optimizing different design options).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: MAB算法和概念在各种现实场景中找到应用，这些场景涉及不确定性下的决策和探索-利用权衡。MABs的现实应用示例包括但不限于资源分配（动态分配资源到不同选项以最大化性能）、在线广告（动态分配广告印象到不同选项，并学习哪些广告产生最高的点击率，这是用户点击广告的概率）、实验设计和临床试验（优化患者分配到不同治疗选项）、内容推荐（为用户个性化内容推荐）和网站优化（优化不同的设计选项）。
- en: As you saw in figure 12.1, MABs can be classified into noncontextual and contextual
    MABs. In contrast to the previously explained noncontextual MABs, a contextual
    multi-armed bandit (CMAB) uses the contextual information contained in the environment.
    In CMAB, the learner repeatedly observes a context, selects an action, and receives
    feedback in the form of a reward or loss specific to the chosen action. CMAB algorithms
    use supplementary information, known as *side information* or *context*, to make
    informed decisions in real-world scenarios. For example, in a truck selection
    problem, the shared context is the type of delivery route (city or interstate).
    Section 12.5 shows how to use CMAB to solve this problem as an example of combinatorial
    actions.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在图12.1中看到的，多臂老虎机（MABs）可以分为非上下文和上下文MABs。与之前解释的非上下文MABs相比，上下文多臂老虎机（CMAB）使用环境中包含的上下文信息。在CMAB中，学习器反复观察上下文，选择一个动作，并接收与所选动作相关的奖励或损失作为反馈。CMAB算法使用补充信息，称为*侧信息*或*上下文*，在现实场景中做出明智的决策。例如，在卡车选择问题中，共享的上下文是配送路线的类型（城市或州际）。第12.5节展示了如何使用CMAB来解决这个问题，作为组合动作的示例。
- en: Contextual multi-armed bandit applications
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文多臂老虎机应用
- en: Contextual multi-armed bandits (CMABs) have found applications in areas like
    personalized recommendations and online advertising, where the context can be
    information about a user. For example, Amazon shows how to develop and deploy
    a CMAB workflow on SageMaker using a built-in Vowpal Wabbit (VW) container to
    train and deploy contextual bandit models ([http://mng.bz/y8rE](http://mng.bz/y8rE)).
    These CMABs can be used to personalize content for a user, such as content layout,
    ads, search, product recommendations, etc. Moreover, a scalable algorithmic decision-making
    platform called WayLift was developed based on CMAB using VW for optimizing marketing
    decisions ([http://mng.bz/MZom](http://mng.bz/MZom)).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文多臂老虎机（CMABs）在个性化推荐和在线广告等领域找到了应用，其中上下文可以是一个用户的信息。例如，亚马逊展示了如何使用内置的Vowpal Wabbit（VW）容器在SageMaker上开发和部署CMAB工作流程（[http://mng.bz/y8rE](http://mng.bz/y8rE)）。这些CMABs可以用于为用户个性化内容，如内容布局、广告、搜索、产品推荐等。此外，基于CMAB并使用VW进行营销决策优化的可扩展算法决策平台WayLift被开发出来（[http://mng.bz/MZom](http://mng.bz/MZom)）。
- en: 12.2 Optimization with reinforcement learning
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.2 基于强化学习的优化
- en: RL can be used for combinatorial optimization problems by framing the problem
    as a Markov decision process (MDP) and applying RL algorithms to find an optimal
    policy that leads to the best possible solution. In reinforcement learning, an
    agent learns to make sequential decisions in an environment to maximize a notion
    of cumulative reward. This process involves finding an optimal policy that maps
    states to actions in order to maximize the expected long-term reward.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）可以通过将问题构造成马尔可夫决策过程（MDP）并应用强化学习算法来寻找一个最优策略，从而用于组合优化问题，以获得最佳可能的解决方案。在强化学习中，智能体通过在一个环境中学习做出连续决策，以最大化累积奖励的概念。这个过程涉及到找到一个最优策略，将状态映射到动作，以最大化预期的长期奖励。
- en: The convergence of reinforcement learning and optimization has recently become
    an active area of research, drawing significant attention from the academic and
    industrial communities. Researchers are actively exploring ways to apply the strengths
    of RL to tackle complex optimization problems efficiently and effectively. For
    example, the generic end-to-end pipeline presented in section 11.7 can be used
    to tackle combinatorial optimization problems such as TSP, the vehicle routing
    problem (VRP), the satisfiability problem (SAT), maximum cut (MaxCut), maximal
    independent set (MIS), etc. This pipeline includes training the model with supervised
    or reinforcement learning. Listing 11.4 showed how to solve TSP with an ML model
    pretrained using a supervised or reinforcement learning approach, as described
    by Joshi, Laurent, and Bresson [3].
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习与优化的收敛性最近已成为一个活跃的研究领域，吸引了学术界和工业界的广泛关注。研究人员正在积极探索将强化学习的优势应用于高效有效地解决复杂优化问题的方法。例如，第11.7节中提出的通用端到端管道可以用于解决组合优化问题，如旅行商问题（TSP）、车辆路径问题（VRP）、可满足性问题（SAT）、最大切割（MaxCut）、最大独立集（MIS）等。此管道包括使用监督学习或强化学习训练模型。列表11.4展示了如何使用Joshi、Laurent和Bresson
    [3]所描述的监督学习或强化学习方法预训练的机器学习模型来解决TSP。
- en: 'An end-to-end framework for solving the VRP using reinforcement learning is
    presented in a paper by Nazari et al. [4]. The capacitated vehicle routing problem
    (CVRP) was also handled by reinforcement learning in a paper by Delarue, Anderson,
    and Tjandraatmadja [5]. Another framework called RLOR is described in a paper
    by Wan, Li, and Wang[6] as a flexible framework of deep reinforcement learning
    for routing problems such as CVRP and TSP. A distributed model-free RL algorithm
    called DeepPool is described in a paper by Alabbasi, Ghosh, and Aggarwal [7] as
    learning optimal dispatch policies for ride-sharing applications by interacting
    with the environment. DeepFreight is another model-free RL algorithm for the freight
    delivery problem described in a paper by Jiayu et al. [8]. It decomposes the problem
    into two closely collaborative components: truck-dispatch and package-matching.
    The key objectives of the freight delivery system are to maximize the number of
    served requests within a certain time limit and to minimize the total fuel consumption
    of the fleet during this process. MOVI is another model-free approach for a large-scale
    taxi dispatch problem, described by Oda and Joe-Wong [9]. RL is also used to optimize
    traffic signal control (TSC) as a way to mitigate congestion. In a paper by Ruan
    et al. [10] (of which I was a co-author), a model of a real-world intersection
    with real traffic data collected in Hangzhou, China, is simulated with different
    RL-based traffic signal controllers. We also proposed a multi-agent reinforcement
    learning model to provide both macroscopic and microscopic control in mixed traffic
    scenarios [11]. The experimental results show that the proposed approach demonstrates
    superior performance compared with other baselines in terms of several metrics,
    such as throughput, average speed, and safety.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 一篇由 Nazari 等人撰写的论文中提出了一种使用强化学习解决车辆路径问题（VRP）的端到端框架[4]。在 Delarue、Anderson 和 Tjandraatmadja
    的论文[5]中，也使用强化学习处理了带能力的车辆路径问题（CVRP）。另一篇名为 RLOR 的框架由 Wan、Li 和 Wang 在论文[6]中描述，它是一个适用于
    CVRP 和 TSP 等路由问题的深度强化学习灵活框架。Alabbasi、Ghosh 和 Aggarwal 在论文[7]中描述了一个无模型的分布式强化学习算法
    DeepPool，通过与环境交互来学习最优调度策略，用于拼车应用。DeepFreight 是另一个用于货运交付问题的无模型强化学习算法，由 Jiayu 等人在论文[8]中描述。它将问题分解为两个紧密协作的组件：卡车调度和包裹匹配。货运交付系统的关键目标是最大化在特定时间限制内服务的请求数量，并在此过程中最小化整个车队的总燃油消耗。MOVI
    是另一种用于大规模出租车调度问题的无模型方法，由 Oda 和 Joe-Wong 在论文[9]中描述。强化学习也被用来优化交通信号控制（TSC），作为一种缓解拥堵的方法。在
    Ruan 等人撰写的论文[10]（我是合著者之一）中，模拟了一个收集于中国杭州的真实路口的真实交通数据模型，并使用不同的基于强化学习的交通信号控制器进行模拟。我们还提出了一种多智能体强化学习模型，以在混合交通场景中提供宏观和微观控制[11]。实验结果表明，所提出的方法在吞吐量、平均速度和安全性等几个指标上优于其他基线。
- en: A framework for learning optimization algorithms, known as “Learning to Optimize,”
    is described by Li and Malik [12]. The problem was formulated as an RL problem,
    in which any optimization algorithm can be represented as a policy. Guided policy
    search is used, and autonomous optimizers are trained for different classes of
    convex and nonconvex objective functions. These autonomous optimizers converge
    faster or reach better optima than hand-engineered optimizers. This is somewhat
    similar to the amortized optimization concept described in section 11.6, but using
    reinforcement learning.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 李和 Malik 在论文[12]中描述了一个名为“学习优化”的优化算法框架。问题被表述为一个强化学习问题，其中任何优化算法都可以表示为一个策略。使用引导策略搜索，并针对不同类别的凸和非凸目标函数训练了自主优化器。这些自主优化器比手工设计的优化器收敛更快或达到更好的最优解。这与第11.6节中描述的摊销优化概念有些相似，但使用了强化学习。
- en: RL-based dispatching is described by Toll et al. for a four-elevator system
    in a 10-floor building [13]. The elevator dispatching problem is a combinatorial
    optimization problem that involves efficiently dispatching multiple elevators
    in a multi-floor building to serve incoming requests from passengers. As explained
    in section 1.4.2, the number of possible states in this case is 10⁴ (elevator
    positions) × 2^(40) (elevator buttons) × 2^(18) (hall call buttons) = 2.88 x 10^(21)
    different states.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Toll 等人在论文[13]中描述了基于强化学习的调度方法，用于一个10层建筑中的四电梯系统。电梯调度问题是一个组合优化问题，涉及在多层建筑中高效调度多个电梯来服务乘客的请求。如第1.4.2节所述，在这种情况下可能的状态数量为
    10⁴（电梯位置）× 2^(40)（电梯按钮）× 2^(18)（大厅呼叫按钮）= 2.88 x 10^(21) 种不同状态。
- en: 'Stable-Baselines3 (SB3) provides reliable implementations of reinforcement
    learning algorithms in PyTorch for several OpenAI Gym-compatible and custom RL
    environments. To install Stable Baselines3 with pip, execute `pip install stable-baselines3`.
    Examples of RL algorithm implementations in SB3 include advantage actor-critic
    (A2C), soft actor-critic (SAC), deep deterministic policy gradient (DDPG), deep
    Q network (DQN), hindsight experience replay (HER), twin delayed DDPG (TD3), and
    proximal policy optimization (PPO). Environments and projects available in SB3
    include the following:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Stable-Baselines3 (SB3)为几个与OpenAI Gym兼容的自定义RL环境提供了强化学习算法的可靠实现。要使用pip安装Stable
    Baselines3，请执行`pip install stable-baselines3`。SB3中RL算法实现的示例包括优势演员-评论家（A2C）、软演员-评论家（SAC）、深度确定性策略梯度（DDPG）、深度Q网络（DQN）、事后经验重放（HER）、双延迟DDPG（TD3）和近端策略优化（PPO）。SB3中可用的环境和项目包括以下内容：
- en: '*mobile-env*—A Gymnasium environment for autonomous coordination in wireless
    mobile networks. It allows the simulation of various scenarios involving moving
    users in a cellular network with multiple base stations. This environment is used
    in section 12.4.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*移动环境*—一个用于无线移动网络中自主协调的体育馆环境。它允许模拟涉及多个基站中移动用户的各种场景。此环境在第12.4节中使用。'
- en: '*gym-electric-motor*—An OpenAI Gym environment for the simulation and control
    of electric drivetrains. This environment is used in exercise 6 for this chapter
    (see appendix C).'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*gym-electric-motor*—一个用于模拟和控制电动传动系统的OpenAI Gym环境。此环境在本章的练习6中使用（见附录C）。'
- en: '*highway-env*—An environment for decision-making in autonomous driving in different
    scenarios, such as highway, merge, roundabout, parking, intersection, and racetrack.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*高速公路环境*—一个用于在不同场景中（如高速公路、合并、环岛、停车、交叉口和赛道）进行自动驾驶决策的环境。'
- en: '*Generalized state dependent exploration (gSDE) for deep reinforcement learning
    in robotics*—An exploration method to train RL agents directly on real robots.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*机器人深度强化学习中的广义状态依赖探索（gSDE）*—一种直接在真实机器人上训练RL代理的探索方法。'
- en: '*RL Reach*—A platform for running reproducible RL experiments for customizable
    robotic reaching tasks.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*RL Reach*—一个用于运行可定制机器人抓取任务的RL实验的平台。'
- en: '*RL Baselines3 Zoo*—A framework to train, evaluate RL agents, tune hyperparameters,
    plot results, and record videos.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*RL Baselines3动物园*—一个用于训练、评估RL代理、调整超参数、绘制结果和记录视频的框架。'
- en: '*Furuta Pendulum Robot*—A project to build and train a rotary inverted pendulum,
    also known as a Furuta pendulum.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Furuta 悬臂机器人*—一个构建和训练旋转倒立摆（也称为Furuta 悬臂）的项目。'
- en: '*UAV_Navigation_DRL_AirSim*—A platform for training UAV navigation policies
    in complex unknown environments.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*UAV_Navigation_DRL_AirSim*—一个用于在复杂未知环境中训练无人机导航策略的平台。'
- en: '*tactile-gym*—RL environments focused on using a simulated tactile sensor as
    the primary source of observations.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*触觉健身房*—RL环境，主要使用模拟触觉传感器作为观察的主要来源。'
- en: '*SUMO-RL*—An interface to instantiate RL environments with Simulation of Urban
    MObility (SUMO) for traffic signal control.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*SUMO-RL*—一个用于实例化带有城市交通模拟（SUMO）的交通信号控制的RL环境的接口。'
- en: '*PyBullet Gym*—Environments for single and multi-agent reinforcement learning
    of quadcopter control.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*PyBullet Gym*—用于单代理和多代理四旋翼机控制强化学习的环境。'
- en: The following sections provide examples of how you can use RL methods to handle
    control problems with combinatorial actions.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 以下章节提供了如何使用RL方法处理组合动作控制问题的示例。
- en: 12.3 Balancing CartPole using A2C and PPO
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.3 使用A2C和PPO平衡CartPole
- en: Let’s consider a classic control task where the goal is to balance a pole on
    top of a cart by moving the cart left or right, as shown in figure 12.8\. This
    task can be considered an optimization problem where the objective is to maximize
    the cumulative reward by finding an optimal policy that balances the pole on the
    cart for as long as possible. The agent needs to learn how to make decisions (take
    actions) that maximize the reward signal. The agent explores different actions
    in different states and learns which actions lead to higher rewards over time.
    By iteratively updating its policy based on observed rewards, the agent aims to
    optimize its decision-making process and find the best actions for each state.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH12_F08_Khamis.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
- en: Figure 12.8 CartPole balancing problem
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'The state of this environment is described by four variables:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Cart position (continuous)—This represents the position of the cart along the
    *x*-axis. The value ranges from –4.8 to 4.8.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cart velocity (continuous)—This represents the velocity of the cart along the
    *x*-axis. The value ranges from –inf to inf.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pole angle (continuous)—This represents the angle of the pole from the vertical
    position. The value ranges from –0.418 to 0.418 radians or –23.95° to 23.95° degrees.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pole angular velocity (continuous)—This represents the angular velocity of the
    pole. The value ranges from –inf to inf.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The action space in the CartPole environment is discrete and consists of two
    possible actions:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Action 0—Move the cart to the left.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Action 1—Move the cart to the right.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The agent receives a reward of +1 for every time step when the pole remains
    upright. The episode ends if one of the following conditions is met:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: The pole angle is more than ±12 degrees from the vertical.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cart position is more than ±2.4 units from the center.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The episode reaches a maximum time step limit (typically 200 steps).
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the CartPole environment, the objective is to balance the pole on the cart
    for as long as possible, maximizing the cumulative reward. Let’s look at the code
    for learning the optimal policy to balance the CartPole using the advantage actor-critic
    (A2C) algorithm discussed in section 12.1.4.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in listing 12.2, we start by importing the necessary libraries:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '`gym` is the OpenAI Gym library, used for working with reinforcement learning
    environments.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch` is the PyTorch library used for building and training neural networks.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.nn` is a module providing the tools for defining neural networks.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.nn.functional` contains various activation and loss functions.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.optim` contains optimization algorithms for training neural networks.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tqdm` provides a progress bar for tracking the training progress.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seaborn` is used for visualization.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Listing 12.2 Balancing CartPole using the A2C algorithm
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we create the actor and critic networks using PyTorch. The `Actor` class
    is a subclass of `nn.Module` in PyTorch, representing the policy network, and
    the `__init__` method defines the architecture of the actor network. Three fully
    connected layers (`fc1`, `fc2`, `fc3`) are used. The `forward` method performs
    a forward pass through the network, applying activation functions (ReLU) and returning
    the action probabilities using the softmax function:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `Critic` class is also a subclass of `nn.Module`, representing the value
    network. The `__init__` method defines the architecture of the critic network,
    similar to the actor network. The forward method performs a forward pass through
    the network, applying activation functions (ReLU) and returning the predicted
    value:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As a continuation, the following code snippet is used to create an instance
    of the CartPole environment using the OpenAI Gym library and to retrieve important
    information about the environment:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ① Create the CartPole environment.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: ② Set the random seed to help make the environment’s behavior reproducible.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: ③ Retrieve the dimensionality of the observation space.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: ④ Retrieve the number of actions in the action space.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '`state_dim` represents the state space or the observation space and in this
    example has a value of 4 (the four states being cart position, cart velocity,
    pole angle, and pole angular velocity). `n_actions` represents the dimensionality
    of the action space or the number of actions, which in this example is 2 (push
    left and push right). We can now initialize the actor and critic models as well
    as the Adam optimizer for the actor and critic models using learning rate `lr=1e-3`.
    The discount factor, gamma, determines the importance of future rewards compared
    to immediate rewards in reinforcement learning algorithms:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ① Create an instance of the Actor class.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: ② Create an instance of the Critic class.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: ③ Initialize the Adam optimizer for the actor model.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: ④ Initialize the Adam optimizer for the critic model.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Set the discount rate.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'After this initialization, we can start the training process, where an agent
    interacts with the environment for a specified number of episodes. During the
    training, the agent computes the advantage function, updates the actor and critic
    models, and keeps track of the training statistics. The following code snippet
    is used to initialize the training process:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ① Set the total number of episodes to run.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: ② Create an empty list to store the total rewards obtained in each episode.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: ③ Create a dictionary to store the training statistics, including the actor
    loss, critic loss, and total return for each episode.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: ④ Initialize the tqdm progress bar.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'The training loop iterates over the specified number of episodes. In each episode,
    the environment is reset to its initial state, and the random seeds are initialized
    to ensure that the sequence of random numbers generated by the environment remains
    consistent across different runs of the code:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The agent then interacts with the environment by taking actions based on its
    policy, accumulating rewards, and updating its parameters to improve its performance
    in balancing the pole on the cart until the episode is complete. The actor network
    is used to determine action probabilities given the current state, and a categorical
    distribution is created using these probabilities. An action is then stochastically
    sampled from this distribution and executed in the environment. The resulting
    next state, reward, done flag, and additional information are received from the
    environment, completing one step of the agent-environment interaction loop:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ① Obtain action probabilities given the current state.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: ② Create a categorical distribution.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: ③ Sample an action from the categorical distribution.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: ④ Pass the action to the environment.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: The advantage function is then computed using the rewards, next state value,
    and current state value.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ① Compute the advantage function.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: ② Update the total reward.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: ③ Move to the next state.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: 'This advantage function is used to update the critic model. The critic loss
    is calculated as the mean squared error of the advantage. The critic parameters
    are updated using the Adam optimizer:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The actor loss is calculated as the negative log probability of the chosen
    action multiplied by the advantage. The actor parameters are updated using the
    Adam optimizer:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The total reward for the episode is then appended to the `episode_rewards`
    list:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The actor loss, critic loss, and total reward for the episode are added to
    the `stats` dictionary. The statistics are printed for each episode:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ① Print the tracking statistics.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: ② Update the tqdm progress bar.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: ③ Close the tqdm progress bar.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now visualize the learning process by plotting the episode rewards obtained
    during each training episode using a scatter plot with a trend line:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ① Create a DataFrame for episode rewards.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: ② Set the size of the figure to be displayed.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: ③ Set the style of the seaborn plots to have a white grid background.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: ④ Create the scatter plot with a trend line.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: When you run this code, you’ll get a scatter plot and trend line something like
    the one in figure 12.9.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH12_F09_Khamis.jpg)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
- en: Figure 12.9 Episode rewards with a trend line
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, there are fluctuations in the rewards during the learning process,
    as depicted by the scatter plot showing increasing and decreasing rewards in different
    episodes. However, the overall trend or pattern in the rewards over the episodes
    is improving. Fluctuations in the rewards during the learning process are expected
    and considered normal behavior in reinforcement learning. Initially, the agent
    may explore different actions, which can lead to both successful and unsuccessful
    episodes, resulting in varying rewards. As the training progresses, the agent
    refines its policy and tends to exploit more promising actions, leading to more
    consistent rewards.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: 'Stable Baselines3 (SB3) provides more abstract and reliable implementations
    of different reinforcement learning algorithms based on PyTorch. As a continuation
    of listing 12.2, the following code snippet shows the steps for handling the CartPole
    environment using the A2C implementation in SB3\. The A2C agent uses MlpPolicy
    as a specific type of policy network, which in turn uses a multilayer perceptron
    (MLP) architecture. The created agent will interact with the environment for a
    total of 10,000 timesteps to learn the optimal policy:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ① Import the gym module.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: ② Import the A2C model
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: ③ To evaluate the performance of the trained model
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: ④ Create the CartPole-v1 environment.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Create an A2C model with the MlpPolicy (multilayer perceptron) and the environment.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Start the learning process for the model.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Test and evaluate the trained model.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Render the environment in a way that a human can visualize it.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: This code snippet sets up the environment, trains an A2C model, evaluates its
    performance for 1,000 steps, and then visualizes the model’s actions in the environment.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: We can use PPO instead of A2C to balance the CartPole. The next listing is quite
    similar to the previous one, but it uses PPO instead of A2C.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.3 Balancing CartPole using the PPO algorithm
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ① Import the PPO model from SB3.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: ② Create an instance of the CartPole-v1 environment.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: ③ Initialize a PPO model with the MlpPolicy to handle agent’s networks.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: ④ Start the learning process for 10,000 timesteps.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: 'During the training, the code renders logger output in the following format:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The logger output presents the following information:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '`rollout/`'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ep_len_mean`—The mean episode length during rollouts'
  id: totrans-314
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ep_rew_mean`—The mean episodic training reward during rollouts'
  id: totrans-315
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`time/`'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fps`—The number of frames per seconds achieved during training, indicating
    the computational efficiency of the algorithm'
  id: totrans-317
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`iterations`—the number of completed training iterations'
  id: totrans-318
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`time_elapsed`—The time elapsed in seconds since the beginning of training'
  id: totrans-319
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`total_timesteps`—The total number of timesteps (steps in the environments)
    the agent has experienced during training'
  id: totrans-320
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train/`'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`approx_kl`—The approximate Kullback-Leibler (KL) divergence between the old
    and new policy distributions, measuring the extent of policy changes during training'
  id: totrans-322
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clip_fraction`—The mean fraction of surrogate loss that was clipped (above
    the `clip_range` threshold) for PPO.'
  id: totrans-323
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clip_range`—The current value of the clipping factor for the surrogate loss
    of PPO'
  id: totrans-324
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`entropy_loss`—The mean value of the entropy loss (negative of the average
    policy entropy)'
  id: totrans-325
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`explained_variance`—The fraction of the return variance explained by the value
    function'
  id: totrans-326
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learning_rate`—The current learning rate value'
  id: totrans-327
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`loss`—The current total loss value'
  id: totrans-328
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_updates`—The number of gradient updates applied so far'
  id: totrans-329
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`policy_gradient_loss`—The current value of the policy gradient loss'
  id: totrans-330
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`value_loss`—The current value for the value function loss for on-policy algorithms'
  id: totrans-331
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We usually keep an eye on the reward and loss values. As a continuation, the
    following code snippet shows how to evaluate the policy and render the environment
    state:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ① Evaluate the policy of the trained model.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: ② Retrieve the vectorized environment associated with the model.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: ③ Reset the environment to its initial state and get the initial observation.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: ④ Test the trained agent.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding code, after training the model, we evaluate the policy
    of the trained model over 10 episodes and return the mean and standard deviation
    of the rewards. We then allow the agent to interact with the environment for 1,000
    steps and render the environment. The output will be an animated version of figure
    12.8 showing the behavior of the CartPole learning the balancing policy.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: 12.4 Autonomous coordination in mobile networks using PPO
  id: totrans-339
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Schneider et al. described the mobile-env environment as an open platform for
    reinforcement learning in wireless mobile networks [14]. This environment enables
    the representation of users moving within a designated area and potentially connecting
    to one or multiple base stations. It supports both multi-agent and centralized
    reinforcement learning policies.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: In the mobile-env environment, we have a mobile network formed by a number of
    base stations or cells (BSs) and user equipment (UE), as illustrated in figure
    12.10\. Our objective is to decide what connections should be established among
    the UEs and BSs in order to maximize the quality of experience (QoE) globally.
    For individual UEs, a higher QoE is achieved by establishing connections with
    as many BSs as possible, resulting in higher data rates. However, since BSs distribute
    resources among connected UEs (e.g., scheduling physical resource blocks), UEs
    end up competing for limited resources, leading to conflicting goals.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH12_F10_Khamis.jpg)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
- en: Figure 12.10 The mobile-env environment with a number of base stations and user
    equipment
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve maximum QoE globally, the policy must consider two crucial factors:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: The data rate (DR in GB/s) of each connection is determined by the channel’s
    quality (e.g., the signal-to-noise ratio) between the UE and BS.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The QoE of individual UEs does not necessarily increase linearly with higher
    data rates.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s create a mobile network environment and train a PPO agent to learn the
    coordination policy. We’ll start in listing 12.4 by importing `gymnasium`, which
    is a maintained fork of OpenAI’s Gym library. `mobile_env` is imported to create
    an environment related to mobile networks. `IPython.display` enables the use of
    interactive display features within IPython or Jupyter Notebook environments.
    We’ll create a small instance of `mobile_env` that contains three base stations
    and five users.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.4 Mobile network coordination using PPO
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ① Create an instance of the environment.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: ② Print the number of users and the number of base stations.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll create an instance of the PPO agent using the multilayer perceptron
    (MLP) policy (`MlpPolicy`) with the mobile environment (`env`) as the environment.
    The training progress will be logged to the `results_sb` directory for TensorBoard
    visualization. You can install and set up `tensorboard logdir` as follows:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The agent is trained for a total of 30,000 time steps:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The following code snippet can be used to render the environment state after
    training. During the episode, the trained model is used to predict the action
    to be taken based on the current observation. The action is then executed in the
    environment, and the environment responds with the next observation (`obs`), the
    reward received (`reward`), a Boolean flag indicating whether the episode is terminated
    (`terminated`), a flag indicating whether the episode was terminated due to the
    episode time limit (`truncated`), and environment information (`info`):'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ① Reset the environment, returning the initial observation and environment information.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: ② Flag to track if the episode is complete.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: ③ Predict the action to be taken based on the current observation.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: ④ Execute the action and get the environment response.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Update the flag
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Render the environment state.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: The output is an updated version of figure 12.10 showing three stations, five
    users, and the dynamically changing connections between them. The average data
    rate and average utility of the established connections are also rendered.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: Instead of having a single RL agent centrally control cell selection for all
    users, an alternative approach is to adopt a multi-agent RL. In this setup, multiple
    RL agents work in parallel, with each agent being responsible for the cell selection
    of a specific user. For instance, in the mobile-small-ma-v0 environment, we will
    use five RL agents, each catering to the cell selection needs of a single user.
    This approach allows for more distributed and decentralized control, enhancing
    the scalability and efficiency of the system. We’ll use Ray and Ray RLlib in this
    example. Ray is an open source unified framework for scaling AI and Python applications
    like machine learning. Ray RLlib is an open source library for RL, offering support
    for production-level, highly distributed RL workloads while maintaining unified
    and simple APIs for a large variety of industry applications. To install RLlib
    with pip, execute `pip install -U "ray[rllib]"`.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: 'As a continuation of listing 12.4, we can import the Ray libraries to learn
    the optimal coordination policy following a multi-agent approach:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The following function will create and return a wrapped environment suitable
    for RLlib’s multi-agent setup. Here we create a small instance of `mobile_env`:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We’ll now initialize Ray using the following function.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: ① Specify the number of CPUs.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: ② Disable the Ray web-based dashboard.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: ③ Ignore the reinitialization error if Ray is already initialized.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: ④ Ignore the forwarding logs.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now configure an RLlib training setup to use the proximal policy optimization
    (PPO) algorithm on `mobile-env`’s small scenario in a multi-agent environment:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ① Set the environment.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: ② Configure all agents to share the same policy.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: ③ Specify that each worker should use one CPU core.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: ④ Indicate that there should be one worker dedicated to performing rollouts.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet configures and initiates a training session using
    the RLlib framework. It sets up a tuner (trainer) for the PPO algorithm and executes
    the training:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: ① Specify PPO
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: ② Specify where the training results and checkpoints (saved model states) will
    be stored.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: ③ Define the stopping condition for the training.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: ④ Configure how checkpoints are saved.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Specify the training parameters.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Start the training process.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: 'After training, we can load the best trained agent from the results:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: ① Extract the best training result from the result_grid based on the metric
    of average episode reward.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: ② Load the agent from the best checkpoint (model state) obtained in the training.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we can evaluate a trained model on a given environment and render the
    results:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: ① Initialize the environment.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: ② Reset the environment to its initial state and fetch the initial observation
    and additional info.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: ③ Initiate a loop to run one episode with the trained model.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: ④ Initialize an empty dictionary to hold actions for each agent in the multi-agent
    environment.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Iterate through each agent’s observations.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Return the new observations, rewards, termination flags, truncation flags,
    and additional information.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Determine if the episode has ended. An episode ends if it is terminated or
    if truncated is True.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Visualize the current state of the environment.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: Running this code produces an animated rendering of the environment shown in
    figure 12.11.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH12_F11_Khamis.jpg)'
  id: totrans-405
  prefs: []
  type: TYPE_IMG
- en: Figure 12.11 Coordination of the mobile environment using a multi-agent PPO
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: 'This rendering shows the established connection with the five user units and
    three base stations and the obtained average data rates and utilities. For more
    information about the decentralized multi-agent version of the PPO-based coordinator,
    see Schneider et al.’s article “mobile-env: An open platform for reinforcement
    learning in wireless mobile networks” and the associated GitHub repo [14].'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: 12.5 Solving the truck selection problem using contextual bandits
  id: totrans-408
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s consider a scenario where a delivery service provider is planning to
    assign trucks for different delivery routes, as illustrated in figure 12.12\.
    The goal is to maximize the efficiency of the fleet according to the type of delivery
    route. The delivery routes are categorized as city deliveries or interstate deliveries,
    and the company has to select the optimal type of truck according to the following
    decision variables: size, engine type, and tire type. The available options for
    each decision variable are as follows:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: Size—Small, medium, or large
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Engine type—Petrol, diesel, or electric
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tire type—All-season, snow, or off-road
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/CH12_F12_Khamis.png)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
- en: Figure 12.12 A CMAB-based recommender system to select delivery truck size,
    engine type, and tire type based on a specified context
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: The reward is based on how suitable the truck selection is for a given delivery
    route. The reward function receives as arguments the delivery route type and the
    selected actions for each variable. In order to reflect real-world conditions
    and add complexity to the problem, we add noise to the reward value, representing
    uncertainties in weather, road conditions, and so on. The objective is to select
    the best combination from the available choices in such a way as to maximize the
    total reward. This means choosing the most suitable truck size, engine type, and
    tire type for each type of delivery route.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: A contextual multi-armed bandit (CMAB) can be used to handle this problem. Vowpal
    Wabbit (VW), an open source ML library developed originally at Yahoo and currently
    at Microsoft, supports a wide range of machine-learning algorithms, including
    CMAB. You can install VW using `pip install vowpalwabbit`.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use CMAB to find the optimal values of the decision variables that maximize
    the reward based on the given context. The next listing starts by importing the
    necessary libraries and defining variables for the contextual bandit problem.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.5 Contextual bandit for delivery truck selection
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: ① Set the shared context.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: ② Set the action options or the arms.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: 'We then define the following reward function that takes as inputs the shared
    context and indices representing the chosen size, engine, and tire options. It
    returns the reward value as an output:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: ① Higher value indicates better fuel efficiency.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: ② Higher value indicates better performance.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: ③ Higher value indicates better comfort.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: ④ Initial reward is based on the selected options.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Add noise to the reward representing uncertainties in weather, road conditions,
    and so on.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Return the reward.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: 'As a continuation, the following `generate_combinations` function generates
    combinations of actions and examples for the contextual bandit problem. This function
    takes four inputs: the shared context and three lists representing size, engine,
    and tire options for the actions. The function returns the list of examples and
    the list of descriptions once all combinations have been processed. Nested loops
    are used to iterate over each combination of size, engine, and tire options. The
    `enumerate` function is used to simultaneously retrieve the index (`i`, `j`, `k`)
    and the corresponding options (size, engine, tire):'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We now need to sample from a probability mass function (PMF) representing truck
    actions. The `sample_truck_pmf` function samples an index from a given PMF and
    returns the index along with its probability. The indices are used to retrieve
    the corresponding size, engine, and tire indices from the indices list:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: ① Convert the pmf to a Torch tensor.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: ② Perform a multinomial sampling.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: ③ Capture the probability of the selected action.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: ④ Return the sampled index and its corresponding probability.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we need to create a VW workspace for training the contextual bandit model:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-439
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The workspace is defined using the following parameters:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: '`--cb_explore_adf`—This parameter specifies the exploration algorithm for contextual
    bandit learning using the action-dependent features (ADF). In many real-world
    applications, there may be features associated with each action (or arm), and
    these are the *action-dependent features*. This enables the model to explore different
    actions based on the observed context.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--epsilon 0.2`—This parameter sets the exploration rate or epsilon value to
    0.2\. It determines the probability of the model exploring a random action instead
    of selecting the action with the highest predicted reward. A higher epsilon value
    encourages more exploration.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--interactions AA AU AAU`—This parameter creates feature interactions between
    namespaces in VW. These interactions help the model capture more complex relationships
    between features.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-l 0.05`—This parameter sets the learning rate to 0.05\. It determines the
    rate at which the model’s internal parameters are updated during the learning
    process. A higher learning rate makes the model converge faster, but if you adjust
    the learning rate too high, you risk over-fitting and end up worse on average.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--power_t 0`—This argument sets the power value to 0\. It affects the learning
    rate decay over time. A power value of 0 indicates a constant learning rate.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`quiet=True`—This argument sets the `quiet` mode to `True`, suppressing the
    display of unnecessary information or progress updates during the training process.
    It helps keep the output concise and clean.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code snippet is used to train the CMAB model:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: ① Select a random shared context.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: ② Generate examples and indices.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: ③ Obtain the model’s predictions for the chosen actions.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: ④ Obtain the chosen index and its corresponding probability.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Retrieve the corresponding size, engine, and tire indices.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Obtain and append the reward.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Update the examples corresponding to the chosen index.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Learn and update the model’s internal parameters based on the observed rewards.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: ⑨ Finalize the VW workspace.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: 'After training, we can use the following function to test the trained CMAB
    model. This code evaluates the trained model by generating examples, making predictions,
    sampling actions, and calculating the expected reward based on a given shared
    context:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The code will produce output something like the following:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: This output represents the chosen action based on the given shared context during
    testing. In this case, the chosen action specifies a truck with a medium size,
    an electric engine, and a snow tire. The obtained reward is 1.012\. Note that
    the maximum reward with noise ≈ 1.0 + 0.15 = 1.15\. Given that the maximum values
    from `size_value`, `engine_value`, and `tire_value` are 1, and considering that
    the noise is a random value with a standard deviation of 0.05, a value of +3 standard
    deviations (3 × 0.05 = 0.15) would cover about 99.7% of cases in a normal distribution.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: '12.6 Journey’s end: A final reflection'
  id: totrans-463
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this book, we have embarked on a comprehensive journey through a diverse
    landscape of search and optimization algorithms. We first explored deterministic
    search algorithms that tirelessly traverse problem spaces, seeking optimal solutions
    through both blind and informed methods. Then we climbed the peaks and valleys
    of trajectory-based algorithms, witnessing the power of simulated annealing and
    the ingenious designs of tabu search for escaping local optima. Continuing on
    our path, we ventured into the realm of evolutionary computing algorithms, witnessing
    the power of genetic algorithms and their variants in solving complex continuous
    and discrete optimization problems. Along the way, we embarked on a fascinating
    journey with swarm intelligence algorithms, starting with particle swarm optimization
    and offering a glimpse into other algorithms such as the ant colony optimization
    and artificial bee colony algorithms. Finally, we embraced the realm of machine
    learning–based methods, where supervised, unsupervised, and reinforcement learning
    algorithms are used to handle combinatorial optimization problems. Each algorithm
    covered in this book carries its own set of strengths and weaknesses. Remember,
    the choice of technique is determined by the task at hand, the characteristics
    of the problem, and the available resources.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: I hope that the knowledge you’ve gained from this book empowers you to solve
    real-world problems and embrace the boundless potential of search and optimization
    in different domains. The fascinating world of search and optimization algorithms
    continues to expand and evolve. It is up to us to harness this knowledge, to further
    our capabilities, to solve the problems of today, and to shape the future.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-466
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reinforcement learning (RL) can be formulated as an optimization problem wherein
    the agent aims to learn and/or refine its policy to maximize the expected cumulative
    reward within a specific environment.
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reinforcement learning problems can be classified into two main categories:
    Markov decision processes (MDPs) and multi-armed bandit (MAB) problems. MDP problems
    involve environments where the agent''s actions impact the environment and its
    future states. MAB problems focus on maximizing cumulative rewards from a set
    of independent choices (often referred to as "arms") that can be made repeatedly
    over time. MABs don''t consider the impact of choices on future options, unlike
    MDPs.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In MDP-based problems, reinforcement learning uses MDP as a foundational mathematical
    framework to model decision-making problems under uncertainty. MDP is used to
    describe an environment for RL where an agent learns to make decisions by performing
    actions in an environment to achieve a goal.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RL is classified into model-based and model-free RL, based on the presence or
    absence of a model of the environment. The model refers to an internal representation
    or understanding of how the environment behaves—specifically, the transition dynamics
    and reward function.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on how RL algorithms learn and update their policy from collected experiences,
    RL algorithms can be classified into off-policy and on-policy RL.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advantage actor-critic (A2C) and proximal policy optimization (PPO) are model-free
    on-policy RL methods.
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By using a clipped objective function, PPO strikes a balance between encouraging
    exploration and maintaining stability during policy updates. The clipping operation
    restricts the update to a bounded range, preventing large policy changes that
    could be detrimental to performance. This mechanism ensures that the policy update
    remains within a reasonable and controlled distance from the previous policy,
    promoting smoother and more stable learning.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlike MDPs, Multi-armed bandits (MABs) don't consider the impact of choices
    on future states, and the agent does not need to worry about transitioning between
    states because there is only one state. Explore-only, exploit-only greedy, ε-greedy,
    and upper confidence bound (UCB) are examples of MAB strategies for determining
    the best approach for selecting the actions to maximize the cumulative reward
    over the time.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contextual multi-armed bandits (CMABs) are an extension of MAB where the decision-making
    is influenced by additional contextual information about each choice or environment.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning can be applied to solve various combinatorial optimization
    problems, including the traveling salesman problem, traffic signal control, elevator
    dispatching, optimal dispatch policies for ride-sharing, the freight delivery
    problem, personalized recommendations, CartPole balancing, coordinating autonomous
    vehicles in mobile networks, and truck selection.
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
