- en: Chapter 5\. Implementing Neural Networks in PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction to PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will learn the basics of PyTorch, one of the most popular
    deep learning frameworks in use today. PyTorch was introduced by Facebook’s AI
    Research Lab in 2016 and gained users rapidly, both in industry and in research,
    through the following years. One reason for PyTorch’s widespread adoption was
    its intuitive, Pythonic feel, which fit naturally into the preexisting workstreams
    and coding paradigms followed by deep learning practitioners.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, this chapter will discuss the data structures utilized by PyTorch,
    how to define neural models in PyTorch, and how to connect data with models for
    training and testing. Finally, we implement a practical example in PyTorch—a classifier
    for the MNIST digits dataset, complete with code for training and testing the
    classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Installing PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Installing a CPU-compatible version of PyTorch is relatively simple. The PyTorch
    docs recommend using conda, a package management system. Within conda, you can
    create multiple environments, where an environment is a context that encapsulates
    all of your package installs. Access to a package does not transfer across environments—this
    allows the user to have a clean separation between different contexts by downloading
    packages within individual environments. We recommend that you create a conda
    environment for deep learning purposes that you can switch into whenever necessary.
    We refer you to the conda docs for guidance on how to download conda and further
    notes on environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have installed conda, created your deep learning environment, and
    switched into it, the PyTorch docs recommend running the following code from your
    command line to download a CPU-compatible version of PyTorch on macOS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that with this install come torchvision and torchaudio, which are specialized
    packages for working with image data and audio data, respectively. If you are
    on a Linux system, the docs recommend running the following code from your command
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you can navigate to a Python shell (still in your deep learning environment),
    and the following command should run with no issues:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: It is important to get this command running with no errors in your Python shell
    before moving on to running the code in the following sections, as they all require
    the ability to import the PyTorch package.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch Tensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tensors are the primary data structure by which PyTorch stores and manipulates
    numerical information. Tensors can be seen as a generalization of arrays and matrices,
    which we covered in detail in our introduction to linear algebra in [Chapter 1](ch01.xhtml#fundamentals_of_linear_algebra_for_deep_learning).
    Specifically, tensors, as a generalization of 2D matrices and 1D arrays, can store
    multidimensional data such as batches of three-channel images. Note that this
    requires 4D data storage, since each image is 3D (including the channel dimension),
    and a fourth dimension that is required to index each individual image. Tensors
    can even represent dimensionalities beyond the 4D space, although the usage of
    such tensors in practice is uncommon.
  prefs: []
  type: TYPE_NORMAL
- en: In PyTorch, tensors are utilized universally. They are used to represent the
    inputs to models, the weight layers within the models themselves, and the outputs
    of models. The standard linear algebra operations of transposition, addition,
    multiplication, inversion, etc., can all be run on tensors.
  prefs: []
  type: TYPE_NORMAL
- en: Tensor Init
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'How do we initialize tensors? We can initialize a tensor from a variety of
    data types. Some examples are Python lists and Python numerical primitives:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Tensors can also be initialized from NumPy arrays, allowing PyTorch to be integrated
    easily into existing data science and machine learning workflows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, tensors can be formed via some common PyTorch API endpoints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Tensor Attributes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the examples we just saw, we passed a tuple as the argument to each function
    call. The number of indices in the tuple is the dimensionality of the tensor to
    be created, while the number at each index represents the desired size of that
    particular dimension. To access the dimensionality of a tensor, we can call its
    shape attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Calling the shape attribute on any of the previous examples should return the
    same tuple as the input argument, assuming the tensor has not been significantly
    modified in-between.
  prefs: []
  type: TYPE_NORMAL
- en: 'What are some other attributes of tensors? In addition to dimension, tensors
    also store information on the type of data being stored: floating point, complex,
    integer, and boolean. There exist subtypes within each of these categories, but
    we won’t go into the differences between each subtype here. It’s also important
    to note that a tensor cannot contain a mix and match of various data types—all
    data within a single tensor must be of the same data type. To access the data
    type of a tensor, we can call its `dtype` attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, although we haven’t shown this yet, we can set the data type
    of a tensor during initialization. Extending one of our previous examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition to the data type and shape of a tensor, we can also learn the device
    on which the tensor is allocated. These devices include the famous CPU, which
    is standard with any computer and is the default storage for any tensor, and the
    GPU, or graphics processing unit, which is a specialized data processing unit
    often used in the image space. GPUs massively speed up many common tensor operations
    such as multiplication via parallel processing over hundreds of small, specialized
    cores, thus making them immensely useful for most deep learning applications.
    To access the tensor device, we can call its device attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly to data type, we can set the device of a tensor upon initialization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This is a common approach to checking whether a GPU is available via code and
    using a GPU if it is available.  If the GPU is not available, it will use a CPU
    without error.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have defined a tensor with a certain set of attributes and would like
    to modify these attributes, you can use the `to` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: And finally, as we’ll cover in [“Gradients in PyTorch”](#gradients-in-pytorch),
    PyTorch tensors can be initialized with the argument `requires_grad`, which when
    set to `True`, stores the tensor’s gradient in an attribute called `grad`.
  prefs: []
  type: TYPE_NORMAL
- en: Tensor Operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The PyTorch API provides us with many possible tensor operations, ranging from
    tensor arithmetic to tensor indexing. In this section we will cover some of the
    more useful tensor operations—ones that you will likely use often in your deep
    learning applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most basic operations is multiplying a tensor by some scalar `c`.
    This can be achieved via the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in an element-wise product of the scalar with the entries of the
    tensor. Another one of the most basic tensor operations is tensor addition and
    subtraction. To do this, we can simply add tensors via `+`. Subtraction follows
    directly from being able to do addition and multiplying the second tensor by the
    scalar –1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is an element-wise sum of the two tensors. This can be seen as a
    direct generalization of matrix addition for any dimensionality. Note that this
    direct generalization implicitly assumes the same constraint we discussed for
    matrix addition a while ago: that the two tensors being summed are of the same
    dimension. PyTorch, similarly, will accept any two broadcastable inputs with no
    issues, where broadcasting is a procedure by which the two inputs are resolved
    to a common shape, and broadcastable refers to whether it is even possible for
    the two inputs to be resolved to a common shape. If the two tensors are already
    of the same shape, no broadcasting is necessary. We refer you to the [PyTorch
    documentation](https://oreil.ly/rHEdO) for more information on how the API determines
    if the two inputs are broadcastable, and how broadcasting is performed in such
    cases.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tensor multiplication is another useful operation to become familiar with.
    Tensor multiplication works the same as matrix and vector multiplication when
    the dimensionality of each tensor is less than or equal to 2\. However, tensor
    multiplication also works on tensors of arbitrarily high dimensionality, given
    the two tensors are compatible. We can think of tensor multiplication in high
    dimensions as batched matrix multiplications: imagine we have two tensors, the
    first is of shape (2,1,2) and the second is of shape (2,2,2). We can further represent
    the first tensor as a length-two list of 1 × 2 matrices, while the second is a
    length-two list of 2 × 2 matrices. Their product is a length-two list, where index
    *i* of the product is the matrix product of index *i* of the first tensor and
    index *i* of the second tensor, as shown in [Figure 5-1](#to_help_visualize_the_general_tensor).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0501.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-1\. To help visualize the general tensor multiplication method, this
    figure shows the matrix multiplication that occurs before restacking.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Restacking the resultant list into a 3D tensor, we see that the product is
    of shape (2,1,2). Now, we can generalize this to four dimensions, where instead
    of imagining we have a list of matrices, we represent each 4D tensor as a grid
    of matrices and the *(i,j)*-th index of the product is the matrix product of the
    *(i,j)*-th indices of the two 4D input tensors. We represent this mathematically:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper P Subscript i comma j comma x comma z Baseline equals sigma-summation
    Underscript y Endscripts upper A Subscript i comma j comma x comma y Baseline
    asterisk upper B Subscript i comma j comma y comma z"><mrow><msub><mi>P</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>x</mi><mo>,</mo><mi>z</mi></mrow></msub>
    <mo>=</mo> <msub><mo>∑</mo> <mi>y</mi></msub> <msub><mi>A</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>x</mi><mo>,</mo><mi>y</mi></mrow></msub>
    <mo>*</mo> <msub><mi>B</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>z</mi></mrow></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'This procedure is generalizable to any dimensionality, assuming that the two
    input tensors follow the constraints of matrix multiplication. As with tensor
    addition, there are exceptions that involve broadcasting, though we won’t cover
    those in detail here. We refer you to the PyTorch documentation for detailed information
    on broadcasting. To multiply two tensors in PyTorch, you can use the `torch matmul`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition to arithmetic operations on tensors, we can also index and slice
    tensors. If you have prior experience with NumPy, you’ll notice that PyTorch indexing
    is very similar and is based on linear algebra fundamentals. If you have a 3D
    tensor, you can access the value at position *(i,j,k)* via the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'To access larger slices of the tensor, say the matrix at position 0 in a 3D
    tensor, you can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'where the two lines of code are interpreted to be equivalent by the PyTorch
    API. This is because using a single indexer, such as `x3_t[0]`, implicitly assumes
    that the user would like to access all indices *(i,j,k)* that satisfy the condition
    *i* = 0 (i.e., the top matrix in the stack of matrices that is the original 3D
    tensor). Usage of the `:` symbol makes this implicit assumption clear by telling
    PyTorch directly that the user would not like to subset the data at that dimension.
    We can also use the `:` symbol to subset the data, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'where the last line of code is interpreted as: find all indices *(i,j,k)* such
    that *i* = 0, <math alttext="j greater-than-or-equal-to 1"><mrow><mi>j</mi> <mo>≥</mo>
    <mn>1</mn></mrow></math> , and <math alttext="j less-than 3"><mrow><mi>j</mi>
    <mo><</mo> <mn>3</mn></mrow></math> (`:` follows the standard Python list indexing
    convention of being inclusive at the start of the defined range and exclusive
    at the end). In plain English, we want to access the second and third rows of
    the top matrix in the stack of matrices that is the original 3D tensor. Note that
    this usage of `:` is consistent with standard Python list indexing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to accessing indices or slices of a tensor, we can also set those
    indices and slices to new values. In the single index case, this is as simple
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'To set larger slices of the tensor, the most straightforward way is to define
    a tensor that is of the same dimensionality as the slice, and use the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, via broadcasting, we can do things like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The first line sets the entirety of those two rows to 1, and the second sets
    both rows of the slice to the single row passed in as `sub_tensor`. In the next
    section, we will show how to compute the gradients of a function in PyTorch, and
    how to access the values of those gradients.
  prefs: []
  type: TYPE_NORMAL
- en: Gradients in PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Just as a recap, let’s recall derivatives and partial derivatives from calculus.
    The partial derivative of a function, which could be as simple as a polynomial
    function of a few variables to something as complex as a neural network, with
    respect to one of the function’s inputs represents the rate of change of the output
    of the function as that input’s value changes slightly. So, large magnitude derivatives
    indicate that the output is very volatile with small changes in the input (think
    <math alttext="f left-parenthesis x right-parenthesis equals x Superscript 10"><mrow><mi>f</mi>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <msup><mi>x</mi> <mn>10</mn></msup></mrow></math>
    when x is of moderate size), while small magnitude derivatives indicate that the
    output is relatively stable with small changes in the input (think <math alttext="f
    left-parenthesis x right-parenthesis equals StartFraction x Over 10 EndFraction"><mrow><mi>f</mi>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mi>x</mi> <mn>10</mn></mfrac></mrow></math>
    ). If the function takes in more than one input, the gradient is the vector that
    is composed of all of these partial derivatives:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="f left-parenthesis x comma y comma z right-parenthesis equals
    x squared plus y squared plus z squared"><mrow><mi>f</mi> <mrow><mo>(</mo> <mi>x</mi>
    <mo>,</mo> <mi>y</mi> <mo>,</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo> <msup><mi>x</mi>
    <mn>2</mn></msup> <mo>+</mo> <msup><mi>y</mi> <mn>2</mn></msup> <mo>+</mo> <msup><mi>z</mi>
    <mn>2</mn></msup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="StartFraction normal partial-differential f Over normal partial-differential
    x EndFraction equals normal nabla Subscript x Baseline f left-parenthesis x comma
    y comma z right-parenthesis equals 2 x"><mrow><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow>
    <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac> <mo>=</mo> <msub><mi>∇</mi> <mi>x</mi></msub>
    <mi>f</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo> <mi>z</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mn>2</mn> <mi>x</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="normal nabla f equals left-bracket 2 x Baseline 2 y Baseline
    2 z right-bracket"><mrow><mi>∇</mi> <mi>f</mi> <mo>=</mo> <mo>[</mo> <mn>2</mn>
    <mi>x</mi> <mn>2</mn> <mi>y</mi> <mn>2</mn> <mi>z</mi> <mo>]</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing from this example, how would we represent this in PyTorch? We can
    use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The call to `backward()` computes the partial derivative of the output *f* with
    respect to each of the input variables. We should expect the values for `x.grad`,
    `y.grad`, and `z.grad` to be 4.0, 6.0, and 3.0, respectively. In the case of neural
    networks, we can represent the neural network as <math alttext="f left-parenthesis
    x comma theta right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mi>θ</mi> <mo>)</mo></mrow></math> , where *f* is the neural network, *x* is
    some vector representing the input, and  <math alttext="theta"><mi>θ</mi></math>
    is the parameters of *f*. Instead of computing the gradient of the output of *f*
    with respect to *x* as done in the previous example, we compute the gradient of
    the loss of the output of *f* with respect to <math alttext="theta"><mi>θ</mi></math>
    . Adjusting <math alttext="theta"><mi>θ</mi></math>  via the gradient will eventually
    lead to a setting of <math alttext="theta"><mi>θ</mi></math>  that results in
    a small loss for the training data and one that hopefully generalizes to data
    that *f* hasn’t seen before. In the next section, we will introduce the building
    blocks of neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: The PyTorch nn Module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The PyTorch `nn` module provides all of the baseline functionality necessary
    for defining, training, and testing a model. To import the `nn` module, all you
    need to do is run the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In this section, we will cover some of the most common uses of the `nn` module.
    For example, to initialize a weight matrix needed for a feed-forward neural network,
    you can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This defines a single layer with bias in a feed-forward neural network, which
    is a matrix of weights that takes as input a vector of dimension 256 and outputs
    a vector of dimension 10\. The last line of code demonstrates how we can easily
    apply this layer to an input vector and store the output in a new tensor. If we
    wanted to do the same thing using only our knowledge from prior sections, we would
    need to manually define a weight matrix `W` and bias vector `b` via `torch`.tensor
    and explicitly compute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The `nn` module’s Linear layer allows us to abstract away these manual operations
    so we can write clean, concise code.
  prefs: []
  type: TYPE_NORMAL
- en: 'A feed-forward neural network can be thought of as simply a composition of
    such layers, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This code represents a neural network that is the function composition `layer2(layer1(vec))`,
    or mathematically: <math alttext="upper W 2 left-parenthesis upper W 1 asterisk
    x plus b 1 right-parenthesis plus b 2"><mrow><msub><mi>W</mi> <mn>2</mn></msub>
    <mrow><mo>(</mo> <msub><mi>W</mi> <mn>1</mn></msub> <mo>*</mo> <mi>x</mi> <mo>+</mo>
    <msub><mi>b</mi> <mn>1</mn></msub> <mo>)</mo></mrow> <mo>+</mo> <msub><mi>b</mi>
    <mn>2</mn></msub></mrow></math> . To represent more complex, nonlinear functions,
    the `nn` module additionally provides nonlinearities such as ReLU, which can be
    accessed via `nn.ReLU`, and tanh, which can be accessed via `nn.Tanh`. These nonlinearities
    are applied in between layers, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We’ve gone over almost everything necessary to define a model in PyTorch. The
    last thing to cover is the `nn.Module` class—the base class from which all neural
    networks are subclassed in PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `nn.Module` class has one important method that your specific model’s subclass
    will override. This method is the forward method, and it defines how the layers
    initialized in your model’s constructor interact with the input to generate the
    model’s output. Here is an example of some code that can be used to encapsulate
    the simple two-layer neural network we just defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ve written our first neural network in PyTorch! `BaseClassifier` is a bug-free
    model class that can be instantiated after defining `in_dim`, `feature_dim`, and
    `out_dim`. The constructor takes in these three variables as arguments in the
    constructor, which makes the model flexible in terms of layer size. This is the
    sort of model that can be used effectively as a first-pass classifier for datasets
    such as MNIST, as we will demonstrate in [“Building the MNIST Classifier in PyTorch”](#building-mnist-sect1).
    To generate the output of a model on some input, we can use the model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Note that we implicitly call the forward function when using the classifier
    model as a function in the final line. Comparing this to the initial approach
    of manually defining each layer’s parameters as a torch tensor and computing the
    output via `matmul` operations, this is a much more clean, modular, and reusable
    approach to defining neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to being able to define the model, instantiate it, and run data
    through it, we must be able to train and test the model. To train (and test) the
    model, we need a loss metric to evaluate the model. During training, once we calculate
    this loss metric, we can use our knowledge from the previous section and call
    `backward()` on the computed loss. This will store the gradient in each parameter
    p’s `grad` attribute. Since we have defined a classifier model, we can use the
    cross-entropy `loss` metric from PyTorch `nn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, `target` is a tensor of shape (`no_examples`), and each
    index represents the ground truth class of the input corresponding with that index.
    Now that we’ve computed the gradient of the loss of the minibatch of examples
    with respect to all of the parameters in the classifier, we can perform the gradient
    descent step. When defining a neural network as a subclass of `nn.Module`, we
    can access all of its parameters via the `parameters()` function—another convenience
    provided by the PyTorch API. To view the shape of each parameter in the neural
    network, you can run the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the first layer has 256 × 784 weights and a bias vector of length
    256\. The last layer has 10 × 256 weights and a bias vector of length 10.
  prefs: []
  type: TYPE_NORMAL
- en: 'During gradient descent, we need to adjust the parameters based on their gradients.
    We could do this manually, but PyTorch has abstracted away this functionality
    into the `torch.optim` module. This module provides functionality for determining
    the optimizer, which may be more complex than classic gradient descent, and updating
    the parameters of the model. You can define the optimizer as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This code creates an optimizer that will update the parameters of the classifier
    via SGD at the end of each minibatch. To actually perform this update, you can
    use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: In the simple case of a feed-forward network as defined in `BaseClassifier`,
    the testing mode of such a network is the same as the training mode—we can just
    call `classifier(test_x)` on any minibatch in the test set to evaluate the model.
    However, as we’ll discuss later, this is not true for all neural architectures.
  prefs: []
  type: TYPE_NORMAL
- en: This code works for a single minibatch—performing training over the entire dataset
    would require manually shuffling the dataset at each epoch and splitting the dataset
    into minibatches that can be iterated through. Thankfully, PyTorch has also abstracted
    this process out into what are called PyTorch datasets and dataloaders. In the
    next section, we will cover these modules in detail.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch Datasets and Dataloaders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The PyTorch `Dataset` is a base class that can be used to access your specific
    data. In practice, you would subclass the `Dataset` class by overriding two important
    methods: `__len__()` and `__getitem__()`. The first method, as you can probably
    tell from its name, refers to the length of the dataset—i.e., the number of examples
    that the model will be trained or tested on. If we think of the dataset as a list
    of examples, the second method takes as input an index and returns the example
    at that index. Each example consists of both the data point (e.g., image) and
    label (e.g., value from 0 to 9 in the case of MNIST). Here is some example code
    for a dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we assume that the directory containing our dataset consists
    of images that follow the naming convention *img-idx.png*, where *idx* refers
    to the index of the image. Additionally, we assume that our ground-truth labels
    are stored in a saved NumPy array, which can be loaded and indexed using *idx*
    to find each image’s corresponding label.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `DataLoader` class in PyTorch takes as input a dataset instantiation, and
    abstracts away all of the heavy lifting required to load in the dataset by the
    minibatch and shuffle the dataset between epochs. Although we won’t go behind
    the scenes in too much depth, the `DataLoader` class does make use of Python’s
    multiprocessing built-in module to efficiently load minibatches in parallel. Here
    is some example code that puts everything together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'To iterate through these dataloaders, use the following code as a template:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The data returned is a tensor of shape (64,784) and the labels returned are
    of shape (64,). As you can tell, the dataloader also does the work of stacking
    all of the examples into a single tensor that can simply be run through the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: where `out` is of shape (64,10) in the case of MNIST. In the next section, we
    will put together all of our learnings to build a neural architecture that can
    be trained and tested on the MNIST dataset, provide code samples for training
    and testing the model by building off of work in this section, and show example
    training and testing loss curves.
  prefs: []
  type: TYPE_NORMAL
- en: Building the MNIST Classifier in PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It’s time to build an MNIST classifier in PyTorch. For the most part, we can
    reuse a lot of the code presented and explained earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that, by default, the minibatch tensors and model parameters are on CPU,
    so there was no need to call the `to` function on each of these to change the
    device. Also, the MNIST dataset provided by PyTorch unfortunately does not come
    with a validation set, so we’ll do our best to use insights solely from the training
    loss curve to inform our final hyperparameter decision for the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Additionally, note that we call `classifier.train()` and `classifier.eval()`
    at the beginning of the training and test functions, respectively. The calls to
    these functions communicate to the PyTorch backend whether the model is in training
    mode or inference mode. You might be wondering why we need to call `classifier.train()`
    and `classifier.eval()` if there is no difference between the behavior of the
    neural network at train and test time. Although this is true in our first-pass
    example, the training and testing modes for other neural architectures are not
    necessarily the same. For example, if dropout layers are added to the model architecture,
    the dropout layers need to be ignored during the testing phase. We add in the
    calls to `train()` and `eval()` here since it is generally considered good practice
    to do so.
  prefs: []
  type: TYPE_NORMAL
- en: As a first step, we need to set some starting hyperparameters for model training.
    We start with a slightly conservative learning rate in `1e-4` and inspect the
    training loss curve and testing accuracy after 40 epochs, or iterations through
    the entire dataset. [Figure 5-2](#we_see_signs_of_underfitting_as_the_model_performance)
    shows a graph of the training loss curve through the epochs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0502.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-2\. We see signs of underfitting as the model performance on the training
    set is failing to level out, meaning we have not yet settled into a local optimum
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can see that this loss curve is not particularly close to leveling out near
    the end of training, which we’d hope to start seeing for a model training at a
    sufficient learning rate. And although we don’t have a validation set to confirm
    our suspicions, we have strong reason to suspect that a higher learning rate would
    help. After setting the learning rate to a slightly more aggressive `1e-3`, we
    observe a training loss curve that is much more in line with what we’d hope to
    see ([Figure 5-3](#this_leveling_out_of_the_loss_curve_is_more_like_what_wed_expect_to_see_with_an_appropriate_learning_rate)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0503.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-3\. This leveling out of the loss curve is more like what we’d expect
    to see with an appropriate learning rate for the problem
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The loss curve starts to level out only near the end of training. This trend
    indicates that the model is likely in the sweet spot between underfitting to the
    training data, like our previous attempt, and overfitting to the training data.
    Evaluating the trained model at 40 epochs on the test set achieves an accuracy
    of 91%! Although this is nowhere close to the top performers on MNIST today, which
    primarily use convolutional neural classifiers, it is a great start. We recommend
    you try some extensions to the code, such as increasing the number of hidden layers
    and substituting in a more sophisticated optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the basics of PyTorch and its functionality. Specifically,
    we learned the concept of tensors in PyTorch, and how these tensors store numerical
    information. Additionally, we learned how to manipulate tensors via tensor operations,
    access the data within a tensor, and set a few important attributes. We also discussed
    gradients in PyTorch and how they can be stored within a tensor. We built our
    first neural network via standard `nn` functionality in the the PyTorch `nn` module
    section. Comparing the `nn`-based approach with an approach that used PyTorch
    tensors solely out of the box showed much of the effective abstraction that the
    `nn` module provides, lending to its ease of use. And finally, we put all of our
    learnings together in the final section, where we trained and tested an MNIST
    digits feed-forward neural classifier to 91% accuracy on the PyTorch-provided
    test set. Although we covered much of the fundamentals and have equipped you with
    the knowledge you need to get your hands dirty, we have only scratched the surface
    of all that the PyTorch API has to offer—we encourage you to explore further and
    improve upon the models we built in this section. We recommend that you visit
    the PyTorch documentation to learn more and build your own neural nets, including
    trying other architectures, on a variety of online datasets, such as the CIFAR-10
    image recognition datasets. In the next section, we will cover neural network
    implementation, one of the other most popular deep learning frameworks in use
    today.
  prefs: []
  type: TYPE_NORMAL
