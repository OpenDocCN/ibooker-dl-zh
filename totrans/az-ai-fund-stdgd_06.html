<html><head></head><body><section data-pdf-bookmark="Chapter 6. Features of Computer Vision Workloads on Azure" data-type="chapter" epub:type="chapter"><div class="chapter" id="i06_chapter6_features_of_computer_vision_workloads_on_azure_1742068262904737">
<h1><span class="label">Chapter 6. </span>Features of Computer Vision <span class="keep-together">Workloads on Azure</span></h1>

<p>In<a contenteditable="false" data-primary="computer vision" data-type="indexterm" id="icd601"/> this<a contenteditable="false" data-primary="AI (artificial intelligence)" data-secondary="categories of" data-tertiary="computer vision" data-type="indexterm" id="icd1001"/> chapter, we’ll cover the essentials of computer vision, a core area of AI that represents 15%–20% of the exam content. This technology enables you to harness Microsoft Azure’s suite of tools designed to “see” and interpret the world through data. We’ll start with an overview of key computer vision solutions, techniques, and foundational concepts. Then, we’ll dive into how Azure approaches image classification, object detection, OCR, and facial analysis. We’ll examine these topics in greater depth, incorporating hands-on examples along the way.</p>

<section data-pdf-bookmark="Computer Vision Services for Azure" data-type="sect1"><div class="sect1" id="i06_chapter6_computer_vision_services_for_azure_1742068262904927">
<h1>Computer Vision Services for Azure</h1>

<p>When<a contenteditable="false" data-primary="computer vision" data-secondary="Azure services for" data-tertiary="options" data-type="indexterm" id="icd602"/> you’re diving into computer vision on Azure, you’ve got several options to explore, each catering to different goals and levels of customization:</p>

<dl>
	<dt><em>Azure AI Vision</em></dt>
	<dd>
	<p>If your focus is only on computer vision, Azure AI Vision <a contenteditable="false" data-primary="Azure AI Vision" data-secondary="defined" data-type="indexterm" id="id637"/>is built precisely for that. It’s got all the tools you need to handle visual data, analyze images and videos, and pull useful insights. Let’s say you’re developing a system for a parking garage that needs to track available spaces and detect unauthorized vehicles through camera feeds. Azure AI Vision gives you a dedicated, powerful toolkit to tackle visual processing tasks like these with options to help you manage your budget along the way.</p>
	</dd>
	<dt><em>Azure AI Services</em></dt>
	<dd>
	<p>If you’re<a contenteditable="false" data-primary="Azure AI Services" data-type="indexterm" id="id638"/> interested in tapping into multiple AI capabilities beyond computer vision—translation or search, for example—Azure AI Services is your go-to. Think of it as a one-stop shop for various AI tools. You manage everything with a single endpoint and access key. This setup will save time. For instance, imagine you’re building a travel app that translates text, tags images, and detects landmarks. Azure AI Services lets you combine all these features. This keeps everything streamlined and simple to manage.</p>
	</dd>
	<dt><em>Azure AI Custom Vision</em></dt>
	<dd>
	<p>When you need tailored image recognition capabilities,<a contenteditable="false" data-primary="Azure AI Custom Vision" data-type="indexterm" id="id639"/><a contenteditable="false" data-primary="Custom Vision Service" data-type="indexterm" id="id640"/> Azure AI Custom Vision is your go-to solution. It empowers you to create and train custom image recognition models using tags specific to your project’s needs. For instance, if you’re in agriculture, you could build a model to identify crop diseases from images, helping farmers take proactive measures and improve yields. Custom Vision is accessible via SDKs, an API, or an intuitive web portal.</p>
	</dd>
	<dt><em>Azure AI Face Service</em></dt>
	<dd>
	<p>This <a contenteditable="false" data-primary="Azure AI Face Service" data-type="indexterm" id="id641"/><a contenteditable="false" data-primary="Face Service" data-type="indexterm" id="id642"/>service provides advanced AI algorithms to detect, recognize, and analyze human faces in images—even if someone is wearing sunglasses or viewed from an angle. It’s an excellent tool if your projects require identity verification, touchless access control, or automated face blurring for privacy in public spaces. The service can return detailed facial analysis, which makes it useful for applications where in-depth facial recognition is needed. However, access is restricted: only Microsoft-managed customers and partners meeting specific eligibility and usage criteria can use the service as part of Microsoft’s responsible AI principles. To apply, interested users must complete the <a href="https://oreil.ly/sO2xB">Face Recognition intake form</a>.</p>
	</dd>
	<dt>Azure AI Video Indexer</dt>
	<dd>
	<p>This<a contenteditable="false" data-primary="Azure AI Video Indexer" data-type="indexterm" id="id643"/><a contenteditable="false" data-primary="Video Indexer Service" data-type="indexterm" id="id644"/> tool allows for extracting insights from videos, such as object detection, OCR, and content moderation. This is done by leveraging more than 30 AI models. There are also audio capabilities, such as for transcription, translation, and emotion detection.</p>
	</dd>
</dl>

<p>Each of these options offers a unique approach to computer vision, whether you need a broad AI toolkit, a dedicated vision platform, customized model building, or advanced facial <a contenteditable="false" data-primary="computer vision" data-secondary="Azure services for" data-startref="icd602" data-tertiary="options" data-type="indexterm" id="id645"/>recognition capabilities.</p>

<section data-pdf-bookmark="What You Can Do with Azure’s Computer Vision Services" data-type="sect2"><div class="sect2" id="i06_chapter6_what_you_can_do_with_azure_s_computer_vision_servi_1742068262905044">
<h2>What You Can Do with Azure’s Computer Vision Services</h2>

<p>Azure’s<a contenteditable="false" data-primary="computer vision" data-secondary="Azure services for" data-tertiary="capabilities of" data-type="indexterm" id="icd603"/> computer vision services offer a powerful set of tools for analyzing and understanding images in various ways. If you’re looking to automatically generate descriptions, you can start with <em>image captioning</em>. <a contenteditable="false" data-primary="image captioning" data-type="indexterm" id="icd1024"/>This feature doesn’t just identify what’s in an image—it goes a step further by giving each description a confidence score ranging from 0 to 1. This lets you know how certain the model is about its analysis. For instance, if you have a picture of a sunny beach with people swimming, image captioning might generate a caption like “a beach scene with people <span class="keep-together">swimming</span>” and show a confidence score that indicates how likely it is to be accurate—say 0.9. This makes it easy to verify how much trust to place in the results.</p>

<p><em>Tagging</em> is<a contenteditable="false" data-primary="tagging" data-type="indexterm" id="id646"/> another helpful feature. This adds a layer of searchable terms to each image. Tagging highlights specific keywords related to elements found in the image, such as <em>beach</em>, <em>ocean</em>, or <em>sun</em>, and each tag includes a confidence score. This feature is invaluable when organizing large image libraries or quickly searching for images with certain characteristics.</p>

<p>Azure’s computer vision also works <a contenteditable="false" data-primary="object detection" data-secondary="defined" data-type="indexterm" id="id647"/>with <em>object detection</em>. This identifies and locates specific objects within an image, such as cars, people, or furniture. Object detection goes beyond just recognizing objects by also pinpointing their exact locations within the image, which is great for any task that requires spatial awareness. This can be used in scenarios like monitoring inventory, analyzing traffic patterns, or even enabling automated checkout systems in retail.</p>

<p><em>Facial detection</em> and <em>face recognition</em> add<a contenteditable="false" data-primary="facial detection and recognition" data-type="indexterm" id="id648"/> even  more specialized capabilities, as noted before with the Azure AI Face Service. With facial detection, Azure locates the presence of faces within an image, but it doesn’t go further to identify who those faces belong to. This is ideal for applications like crowd counting or assessing emotions from facial expressions. Face recognition takes things further by recognizing individual faces—matching them to known identities. This is useful for security applications, personalized experiences, or any situation where you need to verify someone’s <span class="keep-together">identity</span>.</p>

<p>Last, Azure’s <em>Optical Character Recognition (OCR)</em> service <a contenteditable="false" data-primary="Optical Character Recognition (OCR)" data-secondary="defined" data-type="indexterm" id="id649"/><a contenteditable="false" data-primary="OCR (Optical Character Recognition)" data-secondary="defined" data-type="indexterm" id="id650"/>allows you to extract text from images. This transforms any visual text into machine-readable characters. OCR is ideal for digitizing printed documents, scanning receipts, or reading handwritten notes. Whether you’re automating data entry or making scanned documents searchable, OCR can simplify processes by quickly converting images of text into usable data.</p>

<p><a data-type="xref" href="#i06_chapter6_table_1_1742068262896692">Table 6-1</a> provides a summary of Azure’s computer vision <a contenteditable="false" data-primary="image captioning" data-startref="icd1024" data-type="indexterm" id="id651"/><a contenteditable="false" data-primary="object detection" data-secondary="overview of" data-type="indexterm" id="id652"/>capabilities.</p>

<table class="border" id="i06_chapter6_table_1_1742068262896692">
	<caption><span class="label">Table 6-1. </span>Computer vision capabilities in Azure</caption>
	<thead>
		<tr>
			<th>Feature</th>
			<th>Description</th>
			<th>Use cases</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Image captioning</p>
			</td>
			<td>
			<p>Identifies the content of an image and provides a description along with a confidence score from 0 to 1</p>
			</td>
			<td>
			<p>Describing photos</p>

			<p>Auto-generating captions</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Tagging</p>
			</td>
			<td>
			<p>Adds specific key terms or labels to an image, each with a confidence score, making it easier to categorize and search for images based on content</p>
			</td>
			<td>
			<p>Photo library organization</p>

			<p>Digital assets management</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Object detection</p>
			</td>
			<td>
			<p>Identifies and locates specific objects within an image, providing spatial data on their positions</p>
			</td>
			<td>
			<p>Inventory management</p>

			<p>Traffic analysis</p>

			<p>Automated retail checkout</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Facial detection</p>
			</td>
			<td>
			<p>Detects the presence of faces in an image without identifying individuals</p>
			</td>
			<td>
			<p>Crowd counting</p>

			<p>Emotion detection in groups</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Face recognition</p>
			</td>
			<td>
			<p>Recognizes individual faces, matching them to known identities for verification purposes</p>
			</td>
			<td>
			<p>Security access</p>

			<p>Personalized customer experiences</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>OCR</p>
			</td>
			<td>
			<p>Extracts text from images, converting visual text into machine-readable characters</p>
			</td>
			<td>
			<p>Data entry automation</p>

			<p>Document digitization</p>

			<p>Receipt scanning</p>
			</td>
		</tr>
	</tbody>
</table>

<p>Let’s walk through an example using the Azure AI Vision<a contenteditable="false" data-primary="Azure AI Vision" data-secondary="image captioning" data-type="indexterm" id="id653"/> service. Go to the “<a href="https://oreil.ly/aYYYy">Add captions to images” section in the Vision Studio</a>. You will see sample photos at the top and a place to upload your own photos, as shown in <a data-type="xref" href="#i06_chapter6_figure_1_1742068262892610">Figure 6-1</a>.</p>

<figure><div class="figure" id="i06_chapter6_figure_1_1742068262892610"><img src="assets/aaif_0601.png"/>
<h6><span class="label">Figure 6-1. </span>The image caption service in Azure Vision Studio</h6>
</div></figure>

<p>We’ll select the baseball player. The caption will appear: “A baseball player holding a bat.” Then, select JSON and review the output, which is shown here:</p>

<pre data-type="programlisting">
{
  “apim-request-id”: “163f2514-c22f-41f1-86d9-2d3ec77ca461”,
  “content-length”: “158”,
  “content-type”: “application/json; charset=utf-8”,
  “modelVersion”: “2023-10-01”,
  “captionResult”: {
    “text”: “a baseball player holding a bat”,
    “confidence”: 0.8212961554527283
  },
  “metadata”: {
    “width”: 250,
    “height”: 322
  }
}</pre>

<p>The JSON shows the identification information, length, content type, and model version for the AI. It also shows the image caption as well as the confidence score, which is 0.821. Finally, you’ll see some metadata for the dimensions of the <a contenteditable="false" data-primary="computer vision" data-secondary="Azure services for" data-startref="icd603" data-tertiary="capabilities of" data-type="indexterm" id="id654"/>image.</p>
</div></section>

<section data-pdf-bookmark="How Computer Vision Works" data-type="sect2"><div class="sect2" id="i06_chapter6_how_computer_vision_works_1742068262905118">
<h2>How Computer Vision Works</h2>

<p>At <a contenteditable="false" data-primary="computer vision" data-secondary="overview of" data-type="indexterm" id="icd604"/>the heart of computer vision is the pixel. When a computer “looks” at an image, it’s actually reading numbers that tell it how bright each pixel is and what color it should be. </p>

<p>Let’s take a more in-depth look at how computer vision <a contenteditable="false" data-primary="pixels" data-type="indexterm" id="id655"/>works:</p>

<dl>
	<dt>Images are made of pixels</dt>
	<dd>
	<p>Imagine an image as a huge grid made up of pixels. An image that is 100 × 100 pixels, for instance, has exactly 10,000 pixels arranged in rows and columns. Each pixel has a color made up of three values: red, green, and blue (RGB), ranging in integer value from 0 to 255. By mixing these colors, the computer constructs the images we see.</p>
	</dd>
	<dt>Find patterns in pixels</dt>
	<dd>
	<p>To recognize objects, computer vision algorithms search for patterns within this pixel grid. They use math-based methods, like convolution, to identify parts of the image, such as edges and textures. Convolution <a contenteditable="false" data-primary="convolution" data-type="indexterm" id="id656"/>works by analyzing small groups of pixels (kernels) to enhance certain features, like an edge, and help the computer pick out shapes in the image.</p>
	</dd>
	<dt>Spot important details</dt>
	<dd>
	<p>After recognizing patterns in the pixels, computer vision models focus on key parts—features—of the image. These features could be the curve of a face, the outline of a vehicle, or the edges of a building. By narrowing down the focus, the model zeroes in on only the most important points. This makes it easier to tell what the image contains.</p>
	</dd>
	<dt><em>Teach a model to recognize images</em></dt>
	<dd>
	<p>Now <a contenteditable="false" data-primary="training ML models" data-secondary="image recognition" data-type="indexterm" id="id657"/>that the model has picked out these features, it’s ready for the next step: learning. The model has been trained on loads of images, each labeled with what’s in it. By studying this labeled data, it learns to connect certain pixel patterns with objects like “cat” or “car” and later can recognize similar objects in new images.</p>
	</dd>
	<dt><em>DL and pixel data</em></dt>
	<dd>
	<p>For DL<a contenteditable="false" data-primary="DL (deep learning)" data-secondary="computer vision" data-type="indexterm" id="id658"/> models like <a contenteditable="false" data-primary="CNNs (convolutional neural networks)" data-type="indexterm" id="id659"/>convolutional neural networks (CNNs)—which we’ll learn more about later in this chapter—pixels are just the beginning. Each layer in a CNN digs deeper into the pixel data, with early layers detecting edges, middle layers recognizing shapes, and later layers identifying full objects. As the network learns, it adjusts the relationships between pixels. This makes the model better at detecting patterns each time it’s used.</p>
	</dd>
	<dt><em>Fine-tune for accuracy</em></dt>
	<dd>
	<p>In <a contenteditable="false" data-primary="fine-tuning models" data-type="indexterm" id="id660"/>applications where accuracy is critical—like facial recognition or medical imaging—every pixel matters. Advanced algorithms take a closer look at these pixels, which improves the model’s precision. By fine-tuning pixel data, the model gradually detects with greater detail, getting better at interpreting images with each improvement.</p>
	</dd>
</dl>

<p>Let’s go further into pixels and the processing of images. For this, we’ll take a look at an example, which is an array:</p>

<pre data-type="programlisting">
0   0   0   0   0   0   0
0   0   0   0   0   0   0
0   0  180 180 180  0   0
0   0  180 180 180  0   0
0   0  180 180 180  0   0
0   0   0   0   0   0   0
0   0   0   0   0   0   0</pre>

<p>This array has seven rows and seven columns, creating a 7 × 7–pixel image—its resolution. Each pixel in this image is represented by a number ranging from 0 (black) to 255 (white), with values between these extremes representing different shades of gray. In this case, the pixel values of 180 create a lighter gray square in the middle of a darker background, which is shown in <a data-type="xref" href="#i06_chapter6_figure_2_1742068262892648">Figure 6-2</a>.</p>

<p>A grid of pixel values like this forms a simple, two-dimensional image by arranging rows and columns along x- and y-coordinates. This single layer is enough for grayscale images, but color images require a bit more complexity. To represent color, we use three separate layers. Each one represents a different color component: red, green, and blue. As an example, imagine we have a similar 7 <em>× </em>7–pixel array in three color channels.</p>

<p>Red:</p>

<pre data-type="programlisting">
120  120  120  120  120  120  120
120  120  120  120  120  120  120
120  120  200  200  200  120  120
120  120  200  200  200  120  120
120  120  200  200  200  120  120
120  120  120  120  120  120  120
120  120  120  120  120  120  120</pre>

<p>Green:</p>

<pre data-type="programlisting">
20    20   20   20   20   20   20
20    20   20   20   20   20   20
20    20  180  180  180   20   20
20    20  180  180  180   20   20
20    20  180  180  180   20   20
20    20   20   20   20   20   20
20    20   20   20   20   20   20</pre>

<p>Blue:</p>

<pre data-type="programlisting">
250  250  250  250  250  250  250
250  250  250  250  250  250  250
250  250   30   30   30  250  250
250  250   30   30   30  250  250
250  250   30   30   30  250  250
250  250  250  250  250  250  250
250  250  250  250  250  250  250</pre>

<figure><div class="figure" id="i06_chapter6_figure_2_1742068262892648"><img src="assets/aaif_0602.png"/>
<h6><span class="label">Figure 6-2. </span>A 7 × 7 grid display outlining each pixel</h6>
</div></figure>

<p class="pagebreak-before">This example is similar to <a data-type="xref" href="#i06_chapter6_figure_2_1742068262892648">Figure 6-2</a> in shape. But when these three layers combine, they form a color image. The greenish-blue squares on the outer edges come from mixing these values for each color:</p>

<ul>
	<li>
	<p>Red: 120</p>
	</li>
	<li>
	<p>Green: 20</p>
	</li>
	<li>
	<p>Blue: 250</p>
	</li>
</ul>

<p>The teal squares in the middle are created by mixing different values for each color:</p>

<ul>
	<li>
	<p>Red: 200</p>
	</li>
	<li>
	<p>Green: 180</p>
	</li>
	<li>
	<p>Blue: 30</p>
	</li>
</ul>

<p>This blending of the red, green, and blue channels brings color images to life, <a contenteditable="false" data-primary="computer vision" data-secondary="overview of" data-startref="icd604" data-type="indexterm" id="id661"/>pixel by pixel.</p>
</div></section>

<section data-pdf-bookmark="Image Filters" data-type="sect2"><div class="sect2" id="i06_chapter6_image_filters_1742068262905183">
<h2>Image Filters</h2>

<p>When<a contenteditable="false" data-primary="computer vision" data-secondary="image filters" data-type="indexterm" id="icd605"/><a contenteditable="false" data-primary="image filters" data-type="indexterm" id="icd610"/> you want to enhance an image, applying filters can significantly transform it by adjusting each pixel’s value to create various effects. Filters <a contenteditable="false" data-primary="kernels" data-type="indexterm" id="id662"/>use <em>kernels</em>—small grids of numbers that define how each pixel will be altered. Different kernels create different effects, such as blurring or sharpening. Each serves a unique purpose in image <span class="keep-together">processing</span>.</p>

<p>Consider a 3 × 3–kernel matrix for sharpening that looks like this:</p>

<pre data-type="programlisting">
 0  -1   0
-1   6  -1
 0  -1   0</pre>

<p>This kernel is applied by moving it across the image, calculating a new pixel value for each 3 × 3 section, and filling these results into a new version of the image. Let’s walk through an example using a simple grayscale image:</p>

<pre data-type="programlisting">
  0    0    0    0    0    0    0
  0   50   50   50   50   50    0
  0   50  100  100  100   50    0
  0   50  100  150  100   50    0
  0   50  100  100  100   50    0
  0   50   50   50   50   50    0
  0    0    0    0    0    0    0</pre>

<p class="pagebreak-before">Starting with the top left corner, each pixel in a 3 × 3 section is multiplied by the corresponding kernel value, then summed up to produce a new pixel value for the output image. For example:</p>

<pre data-type="programlisting">
(0 * 0) + (0 * -1) + (0 * 0) +
(0 * -1) + (50 * 6) + (50 * -1) +
(0 * 0) + (50 * -1) + (100 * 0) = 200</pre>

<p>You then move the kernel one pixel to the right and repeat this process across the entire image. This specific kernel is used for sharpening, emphasizing details, and making edges more prominent.</p>

<p>Keep in mind that there are various filters to achieve different results:</p>

<dl>
	<dt>Blurring</dt>
	<dd>
	<p>Blurring filters, <a contenteditable="false" data-primary="blurring filters" data-type="indexterm" id="id663"/>like a simple averaging kernel, reduce noise and smooth out an image. A typical blurring kernel might look like this:</p>

	<pre data-type="programlisting">
    1/9  1/9  1/9
    1/9  1/9  1/9
    1/9  1/9  1/9</pre>
	</dd>
	<dd>
	<p>This kernel averages the pixels in each 3 × 3 section, softening details and creating a blurred effect.</p>
	</dd>
	<dt>Edge detection</dt>
	<dd>
	<p>Edge detection filters, <a contenteditable="false" data-primary="edge detection filters" data-type="indexterm" id="id664"/>such as the <a contenteditable="false" data-primary="Sobel filters" data-type="indexterm" id="id665"/>Sobel or <a contenteditable="false" data-primary="Laplacian filters" data-type="indexterm" id="id666"/>Laplacian filters, are designed to identify boundaries within an image by emphasizing rapid changes in pixel intensity—for example:</p>

	<pre data-type="programlisting">
    -1 -1 -1
    -1  8 -1
    -1 -1 -1</pre>
	</dd>
	<dd>
	<p>This kernel detects edges by highlighting where pixel values change sharply, helping to isolate shapes and lines.</p>
	</dd>
	<dt>Color inversion</dt>
	<dd>
	<p>Color inversion filters<a contenteditable="false" data-primary="color inversion filters" data-type="indexterm" id="id667"/> flip the pixel values, creating a negative of the image. These filters don’t use a kernel like the other filters. Inverting colors means that each pixel’s value is subtracted from the maximum intensity, transforming light areas to dark and vice versa.</p>
	</dd>
	<dt>Sharpening</dt>
	<dd>
	<p>The<a contenteditable="false" data-primary="sharpening filter" data-type="indexterm" id="id668"/> sharpening filter, as shown in the example above, enhances details by making edges stand out. This is useful for highlighting features or making an image appear crisper.</p>
	</dd>
</dl>

<p class="pagebreak-before"><a data-type="xref" href="#i06_chapter6_figure_3_1742068262892675">Figure 6-3</a> has a side-by-side comparison showing the effect of a sharpening filter on an image of an apple. This process, known as <em>convolutional filtering</em>, involves <a contenteditable="false" data-primary="convolutional filtering" data-type="indexterm" id="id669"/>sweeping the filter across the image to apply effects like blurring, edge detection, color inversion, and sharpening. By experimenting with different kernels, you gain many creative possibilities for transforming <a contenteditable="false" data-primary="computer vision" data-secondary="image filters" data-startref="icd605" data-type="indexterm" id="id670"/><a contenteditable="false" data-primary="image filters" data-startref="icd610" data-type="indexterm" id="id671"/>your images.</p>

<figure><div class="figure" id="i06_chapter6_figure_3_1742068262892675"><img src="assets/aaif_0603.png"/>
<h6><span class="label">Figure 6-3. </span>The result of using a filter on an image to sharpen it</h6>
</div></figure>
</div></section>

<section data-pdf-bookmark="Image Classification" data-type="sect2"><div class="sect2" id="i06_chapter6_image_classification_1742068262905243">
<h2>Image Classification</h2>

<p><em>Image classification</em> is<a contenteditable="false" data-primary="computer vision" data-secondary="image classification" data-type="indexterm" id="id672"/><a contenteditable="false" data-primary="image classification" data-type="indexterm" id="id673"/> an important aspect of AI that focuses on identifying the main content within an image and sorting it into specific categories. For example, if you want a model to distinguish between animals, it can be taught to identify whether an image shows a cat, dog, or bird. This capability goes beyond just detecting items; it’s about recognizing complex patterns. Sometimes this means picking up on visual details that may be too subtle for the human eye.</p>

<p>Supervised learning <a contenteditable="false" data-primary="supervised learning" data-secondary="image classification" data-type="indexterm" id="id674"/>is typically used to train a model for image classification. In this approach, the model learns from a labeled dataset—each image is paired with its correct label—so the model can learn what each category looks like. This way, the AI becomes adept at recognizing and correctly labeling new images that it hasn’t seen before. On the other hand, unsupervised learning, <a contenteditable="false" data-primary="unsupervised learning" data-type="indexterm" id="id675"/>which doesn’t rely on labeled data, is generally less effective for tasks that need precise category distinctions.</p>

<p>In practice, one popular application of image classification is in health care, where AI models can analyze medical images to detect specific abnormalities. This might include spotting tumors in X-rays or identifying certain conditions in MRI scans. Not only does this streamline the diagnostic process, but it also enables quicker and potentially more accurate detection, which can be crucial for early intervention.</p>
</div></section>

<section data-pdf-bookmark="Object Detection" data-type="sect2"><div class="sect2" id="i06_chapter6_object_detection_1742068262905300">
<h2>Object Detection</h2>

<p><em>Object detection </em>is<a contenteditable="false" data-primary="computer vision" data-secondary="object detection" data-type="indexterm" id="id676"/><a contenteditable="false" data-primary="object detection" data-secondary="overview of" data-type="indexterm" id="id677"/> an AI technique that not only identifies what objects are in an image but also pinpoints their exact locations. This is achieved through <em>bounding boxes</em>, <a contenteditable="false" data-primary="bounding boxes" data-type="indexterm" id="id678"/>which are rectangular outlines that frame each identified object and are mapped by pixel coordinates. These boxes show precisely where each object sits within the image, giving a more detailed understanding than simply knowing what objects are present.</p>

<p>Azure AI’s vision service offers robust object detection capabilities. Tools like <a contenteditable="false" data-primary="Azure AI Vision" data-secondary="object detection" data-type="indexterm" id="id679"/>Vision Studio make integrating this feature into various applications more accessible. The Azure AI vision service allows users to detect objects and track their locations using bounding boxes. Object detection builds on image analysis models, but it involves more complex training since the model must learn not just to recognize objects but also to locate them accurately within the image. <a data-type="xref" href="#i06_chapter6_figure_4_1742068262892697">Figure 6-4</a> shows an example of using object detection with Vision Studio.</p>

<figure><div class="figure" id="i06_chapter6_figure_4_1742068262892697"><img src="assets/aaif_0604.png"/>
<h6><span class="label">Figure 6-4. </span>Object detection using Azure AI Vision Studio</h6>
</div></figure>

<p class="pagebreak-before">The uploaded image shows how the model detected individual products by drawing bounding boxes around each item on the shelves. Each bounding box, outlined in blue, represents a “detected product,” showing where each item is located within the image. Alongside this visual display, Vision Studio provides detailed JSON data on the right, which includes the pixel coordinates of each bounding box. What’s more, each detected object comes with a confidence score that tells us how sure it is that each item is a “product.”</p>
</div></section>

<section data-pdf-bookmark="OCR" data-type="sect2"><div class="sect2" id="i06_chapter6_ocr_1742068262905356">
<h2>OCR</h2>

<p>Traditional<a contenteditable="false" data-primary="computer vision" data-secondary="OCR" data-type="indexterm" id="icd606"/><a contenteditable="false" data-primary="OCR (Optical Character Recognition)" data-secondary="overview of" data-type="indexterm" id="icd611"/><a contenteditable="false" data-primary="Optical Character Recognition (OCR)" data-secondary="overview of" data-type="indexterm" id="icd611a"/> OCR uses pattern recognition to match text shapes, but with AI, things have stepped up a notch. Now, ML algorithms analyze every shape and line, comparing them to vast libraries of text samples. This means that AI can handle tricky fonts or messy handwriting better than older OCR methods ever could.</p>

<p>That said, OCR isn’t perfect. Sometimes, it misreads characters—like mistaking a lowercase “l” for the number “1.” These errors can crop up especially if the text is smudged or the font is unusual.</p>

<p>Yet OCR can provide major advantages. Let’s take an example. Suppose a health care provider has decades’ worth of patient records stored in dusty file cabinets. Digging up a file means sorting through piles of paper—a tedious, time-consuming process that often leads to errors or lost documents. With OCR, though, all those paper records can be quickly scanned, converted to digital format, and made fully searchable. This can mean saving substantial amounts of money and time.</p>

<p>Azure’s Read OCR<a contenteditable="false" data-primary="Azure Read OCR engine" data-type="indexterm" id="id680"/><a contenteditable="false" data-primary="Read OCR engine" data-type="indexterm" id="id681"/> engine is built with advanced ML models to not just pull text from documents but also adapt to multiple languages and formats. Read OCR is flexible, letting you choose between cloud-based processing or on-premises deployment. If you’re working with single images or photos “in the wild,” Read OCR offers a fast synchronous API, so you can embed it into your software.</p>

<p>There are two main versions of Read OCR that cover different scenarios. The first version is optimized for general images, such as labels, signs, or posters that you’d find in everyday settings. This version (OCR for Images 4.0) is ideal for situations that need fast text extraction.</p>

<p>The second version is tailored for text-heavy scanned or digital documents like books, reports, or articles. This <a contenteditable="false" data-primary="Microsoft" data-secondary="Document Intelligence model" data-type="indexterm" id="id682"/><a contenteditable="false" data-primary="Document Intelligence model" data-type="indexterm" id="id683"/>Document Intelligence model uses an asynchronous API, which means it’s built to handle high volumes and works great if you’re automating large-scale document processing.</p>

<p>Let’s look at an example of the OCR capabilities of Azure AI Foundry<a contenteditable="false" data-primary="Azure AI Foundry" data-secondary="OCR" data-type="indexterm" id="id684"/>. This example converts an image of a Social Security card into readable text, as shown in <a data-type="xref" href="#i06_chapter6_figure_5_1742068262892719">Figure 6-5</a>.</p>

<figure><div class="figure" id="i06_chapter6_figure_5_1742068262892719"><img src="assets/aaif_0605.png"/>
<h6><span class="label">Figure 6-5. </span>OCR output from Azure AI Foundry of a Social Security card image</h6>
</div></figure>

<p>As you can see, Azure AI Foundry ​successfully identifies the text. If you select JSON, you will get an extensive set of features. First, it will show a line of the text:</p>

<pre data-type="programlisting">
    “lines”: [
      {
        “text”: “SOCIAL SECURITY”,
        “boundingPolygon”: [
          {
            “x”: 500,
            “y”: 3742
          },
          {
            “x”: 2696,
            “y”: 2980
          },
          {
            “x”: 2810,
            “y”: 3118
          },
          {
            “x”: 560,
            “y”: 4032
          }
        ],</pre>

<p class="pagebreak-before">For each word, the location is also provided. Then, the OCR breaks this down into the values for each of the words, along with the confidence scores:</p>

<pre data-type="programlisting">
“words”: [
          {
            “text”: “SOCIAL”,
            “boundingPolygon”: [
              {
                “x”: 501,
                “y”: 3906
              },
              {
                “x”: 1304,
                “y”: 3402
              },
              {
                “x”: 1381,
                “y”: 3536
              },
              {
                “x”: 596,
                “y”: 4032
              }
            ],
            “confidence”: 0.963
          },</pre>

<p>This OCR functionality ties<a contenteditable="false" data-primary="intelligent document processing (IDP)" data-type="indexterm" id="id685"/><a contenteditable="false" data-primary="IDP (intelligent document processing)" data-type="indexterm" id="id686"/> into intelligent document processing (IDP), which is like OCR’s next-level cousin. It uses OCR as a foundation but dives deeper, extracting structure and key information beyond just words. Microsoft’s Document Intelligence model, <a contenteditable="false" data-primary="Microsoft" data-secondary="Document Intelligence model" data-type="indexterm" id="id687"/><a contenteditable="false" data-primary="Document Intelligence model" data-type="indexterm" id="id688"/>for example, builds on Read OCR to analyze relationships, identify key entities, and provide detailed document insights. (We covered this system in the section <a data-type="xref" href="ch02.html#i02_chapter2_azure_ai_foundry_1742068260813706">“Studios”</a>.)</p>

<p>Let’s look at an example of <a contenteditable="false" data-primary="Azure AI Document Intelligence" data-type="indexterm" id="id689"/><a contenteditable="false" data-primary="Document Intelligence Service" data-type="indexterm" id="id690"/>the Azure AI <a href="https://oreil.ly/Pd_kX">Document Intelligence Studio</a>. Select the Invoices section and you will see the dashboard. Click “Run analysis” and you will see the screen shown in <a data-type="xref" href="#i06_chapter6_figure_6_1742068262892740">Figure 6-6</a>.</p>

<p>On the left side of the screen, the OCR has identified the various fields on the document. The Document Intelligence technology then determines what types of values they have. For example, there are fields for the billing and recipient addresses and so on. There are also confidence scores for <a contenteditable="false" data-primary="computer vision" data-secondary="OCR" data-startref="icd606" data-type="indexterm" id="id691"/><a contenteditable="false" data-primary="OCR (Optical Character Recognition)" data-secondary="overview of" data-startref="icd611" data-type="indexterm" id="id692"/><a contenteditable="false" data-primary="Optical Character Recognition (OCR)" data-secondary="overview of" data-startref="icd611a" data-type="indexterm" id="id693"/>each.</p>

<figure><div class="figure" id="i06_chapter6_figure_6_1742068262892740"><img src="assets/aaif_0606.png"/>
<h6><span class="label">Figure 6-6. </span>Analysis of an invoice using Document Intelligence Studio</h6>
</div></figure>
</div></section>

<section data-pdf-bookmark="Facial Detection and Analysis" data-type="sect2"><div class="sect2" id="i06_chapter6_facial_detection_and_analysis_1742068262905416">
<h2>Facial Detection and Analysis</h2>

<p>Facial<a contenteditable="false" data-primary="computer vision" data-secondary="facial detection and recognition" data-type="indexterm" id="icd607"/><a contenteditable="false" data-primary="facial detection and recognition" data-type="indexterm" id="icd612"/> detection and analysis are crucial technologies for computer vision. Facial detection identifies human faces within an image or video feed, often as a preliminary step in other processes, such as unlocking your phone with your face. It determines whether a face is present but does not provide additional details. Facial analysis, on the other hand, goes a step further by interpreting various features of the face, such as age, gender, and emotional expressions, based on patterns in facial structures.</p>

<p>Facial detection and analysis rely on AI, particularly DL,<a contenteditable="false" data-primary="DL (deep learning)" data-secondary="facial detection and recognition" data-type="indexterm" id="id694"/> which uses layers of neural networks to analyze patterns in data. Models are trained on vast datasets of human faces, which allows the models to recognize and interpret facial features accurately. Techniques like <a contenteditable="false" data-primary="CNNs (convolutional neural networks)" data-secondary="facial detection and recognition" data-type="indexterm" id="id695"/>CNNs play a significant role here, scanning and learning from pixel patterns in images. The AI models then apply this learning to recognize facial features and expressions in real time or in prerecorded images, constantly improving with each use.</p>

<p>You’ll find facial detection and analysis in many everyday applications. Security systems use it for surveillance and to identify individuals of interest in crowded places while retail settings apply it to gather demographic data on customers and even tailor advertisements. In health care, facial analysis is being explored to assess emotional well-being and identify symptoms associated with certain conditions. Gaming and entertainment sectors use it to enhance user experiences, adapting the game or content interactions based on the user’s reactions.</p>

<p>Azure AI Studio includes powerful face recognition capabilities. <a data-type="xref" href="#i06_chapter6_figure_7_1742068262892761">Figure 6-7</a> shows an example of this.</p>

<figure><div class="figure" id="i06_chapter6_figure_7_1742068262892761"><img src="assets/aaif_0607.png"/>
<h6><span class="label">Figure 6-7. </span>Azure AI Studio identifies and analyzes the face of a woman in an image</h6>
</div></figure>

<p>In this example, the AI has recognized the face of a woman. It has created a bounding box<a contenteditable="false" data-primary="bounding boxes" data-type="indexterm" id="id696"/> for her face, and there are various dots on her face to indicate different features. If you select JSON, you will see information about this:</p>

<pre data-type="programlisting">
“faceLandmarks”: {
      “pupilLeft”: {
        “x”: 510.5,
        “y”: 256.4
      },
      “pupilRight”: {
        “x”: 574.7,
        “y”: 263.7
      },
      “noseTip”: {
        “x”: 538.1,
        “y”: 301.7
      },
      “mouthLeft”: {
        “x”: 502.2,
        “y”: 325
      },</pre>

<p>This captures information about the identification and locations of the left pupil, right pupil, nose tip, and so <a contenteditable="false" data-primary="computer vision" data-secondary="facial detection and recognition" data-startref="icd607" data-type="indexterm" id="id697"/><a contenteditable="false" data-primary="facial detection and recognition" data-startref="icd612" data-type="indexterm" id="id698"/>on.</p>
</div></section>

<section data-pdf-bookmark="Convolutional Neural Networks" data-type="sect2"><div class="sect2" id="i06_chapter6_convolutional_neural_networks_1742068262905477">
<h2>Convolutional Neural Networks</h2>

<p>Imagine<a contenteditable="false" data-primary="computer vision" data-secondary="CNNs" data-type="indexterm" id="icd608"/><a contenteditable="false" data-primary="CNNs (convolutional neural networks)" data-type="indexterm" id="icd613"/> trying to teach a computer to recognize animals in photos, such as whether the animal is a cat, a dog, or even a lion. <em>Convolutional neural networks (CNNs)</em> make this possible. They’re a popular type of DL model because of how they analyze data. Think of CNNs as digital detectives that filter through an image to pick out important clues like shapes, edges, and textures. They then piece these clues together to make a prediction.</p>

<p>Here’s how it works in practice. The model’s filters start with random weights. With each round of training, the model learns which details help it recognize each feature, such as the animal type. After enough practice, it becomes highly accurate at picking up the clues—so much so that it can correctly label a new image of, say, a lion, even if it’s never seen that exact lion before.</p>

<p>Here’s a breakdown of the CNN <a contenteditable="false" data-primary="CNNs (convolutional neural networks)" data-secondary="layers of" data-type="indexterm" id="id699"/>layers:</p>

<dl>
	<dt>Convolutional layers</dt>
	<dd>
	<p>Each <a contenteditable="false" data-primary="convolutional layers, in CNNs" data-type="indexterm" id="id700"/>filter in these layers scans the image and captures specific patterns, such as fur texture or ear shape. Over time, the CNN learns which patterns are most useful for recognizing an animal in the image.</p>
	</dd>
	<dt><em>Pooling layers</em></dt>
	<dd>
	<p>To keep<a contenteditable="false" data-primary="pooling layers, in CNNs" data-type="indexterm" id="id701"/> from getting overloaded with data, CNNs use pooling (often max pooling), which shrinks the feature maps to retain only the most essential details. This process makes the model faster and less sensitive to minor variations, such as changes in an animal’s pose.</p>
	</dd>
	<dt><em>Fully connected layers</em></dt>
	<dd>
	<p>Here, <a contenteditable="false" data-primary="fully connected layers, in CNNs" data-type="indexterm" id="id702"/>the model brings together all the information it’s collected. It’s like flattening a 3D puzzle and connecting all the pieces to make a single prediction. By this stage, the model has a strong sense of what the object is.</p>
	</dd>
	<dt>Activation functions</dt>
	<dd>
	<p>In <a contenteditable="false" data-primary="activation functions, in CNNs" data-type="indexterm" id="id703"/>the final steps, the model needs to weigh its options and come to a conclusion. Functions like <code>softmax</code> are used. They assign probabilities to each possible label, so the model might output probabilities like [0.3, 0.6, 0.1]—indicating a 60% chance that the image is a dog, for example.</p>
	</dd>
	<dt><em>Backpropagation and optimization</em></dt>
	<dd>
	<p>When<a contenteditable="false" data-primary="backpropagation" data-type="indexterm" id="id704"/> the model’s <a contenteditable="false" data-primary="optimization, of CNNs" data-type="indexterm" id="id705"/>guess isn’t right, it’s not the end of the road. It reviews where it missed the mark and adjusts the filter weights to improve its next guess. This feedback loop, called <em>backpropagation</em>, helps the model focus on the most relevant features for each type of image.</p>
	</dd>
</dl>

<p>Training a CNN means feeding it thousands—often millions—of labeled images and letting it practice. With each training round, the model’s accuracy improves as it learns from its mistakes. By the end, the CNN understands how to best identify each type of object, even if a new image looks slightly different from the training set.</p>

<p class="pagebreak-before">While CNNs are often used for labeling images, they’re also perfect for other tasks that require understanding complex visual data:</p>

<dl>
	<dt><em>Object detection</em></dt>
	<dd>
	<p>Rather<a contenteditable="false" data-primary="object detection" data-secondary="CNNs" data-type="indexterm" id="id706"/> than<a contenteditable="false" data-primary="CNNs (convolutional neural networks)" data-secondary="object detection" data-type="indexterm" id="id707"/> just saying “This is a cat,” CNNs can identify multiple animals in an image, drawing boxes around each one.</p>
	</dd>
	<dt>Image segmentation</dt>
	<dd>
	<p>In<a contenteditable="false" data-primary="image segmentation" data-type="indexterm" id="id708"/> detailed<a contenteditable="false" data-primary="CNNs (convolutional neural networks)" data-secondary="image segmentation" data-type="indexterm" id="id709"/> tasks like medical imaging, CNNs can classify each pixel in an image, identifying distinct regions, which can be helpful, for instance, in spotting different types of tissues in a medical scan.</p>
	</dd>
</dl>

<p>In short, CNNs allow machines to interpret images in ways that were <a contenteditable="false" data-primary="computer vision" data-secondary="CNNs" data-startref="icd608" data-type="indexterm" id="id710"/><a contenteditable="false" data-primary="CNNs (convolutional neural networks)" data-startref="icd613" data-type="indexterm" id="id711"/>previously impossible.</p>
</div></section>

<section data-pdf-bookmark="The Evolution of Computer Vision: From CNNs to Multimodal Models" data-type="sect2"><div class="sect2" id="i06_chapter6_the_evolution_of_computer_vision_from_cnns_to_mul_1742068262905546">
<h2>The Evolution of Computer Vision: From CNNs to Multimodal Models</h2>

<p>A <a contenteditable="false" data-primary="computer vision" data-secondary="evolution of" data-type="indexterm" id="id712"/>new kind of architecture—called <em>transformers</em>—has<a contenteditable="false" data-primary="transformers" data-secondary="overview of" data-type="indexterm" id="id713"/> been taking over the NLP<a contenteditable="false" data-primary="NLP (natural language processing)" data-secondary="transformers" data-type="indexterm" id="id714"/> world. Transformers function by turning words or phrases into numerical codes called <em>embeddings</em>. Imagine<a contenteditable="false" data-primary="embeddings" data-type="indexterm" id="id715"/> embeddings as coordinates in a virtual landscape where words with similar meanings gather near one another. For example, “cat” and “kitten” would be positioned closely together, unlike “cat” and “airplane,” which would sit farther apart in this semantic space.</p>

<p>Transformers’ success with language led to a new <a contenteditable="false" data-primary="multimodal models" data-type="indexterm" id="id716"/>breakthrough: <em>multimodal models</em>. These models combine images and text, enabling them to understand both at the same time. Here’s how they do it:</p>

<ol>
	<li>
	<p>They analyze images to pull out key visual features.</p>
	</li>
	<li>
	<p>They convert text into embeddings.</p>
	</li>
	<li>
	<p>They learn to link these visual and text-based elements.</p>
	</li>
</ol>

<p>Microsoft’s Florence <a contenteditable="false" data-primary="Florence model" data-type="indexterm" id="id717"/>is<a contenteditable="false" data-primary="Microsoft" data-secondary="Florence model" data-type="indexterm" id="id718"/> one example of a multimodal model. It’s trained on millions of images with captions, letting it take on various tasks like:</p>

<ul>
	<li>
	<p>Sorting images into categories</p>
	</li>
	<li>
	<p>Spotting specific objects in images</p>
	</li>
	<li>
	<p>Crafting natural image descriptions</p>
	</li>
	<li>
	<p>Adding relevant tags</p>
	</li>
</ul>

<p>These multimodal models represent the cutting edge of AI today, creating exciting opportunities for systems that can handle both images and words with ease.</p>
</div></section>
</div></section>

<section data-pdf-bookmark="Responsible AI and Computer Vision" data-type="sect1"><div class="sect1" id="i06_chapter6_responsible_ai_and_computer_vision_1742068262905608">
<h1>Responsible AI and Computer Vision</h1>

<p>In <a data-type="xref" href="ch03.html#i03_chapter3_overview_of_ai_workloads_and_key_use_cases_1742068261133633">Chapter 3</a>, we<a contenteditable="false" data-primary="computer vision" data-secondary="responsible AI and" data-type="indexterm" id="icd609"/><a contenteditable="false" data-primary="responsible AI" data-secondary="computer vision and" data-type="indexterm" id="icd614"/> examined the principles of responsible AI. This is certainly an important topic in the context of computer vision, where ethical issues take center stage. However, this technology also raises ethical concerns. Privacy is a major issue as many users may not know their data is being collected or how it will be used. There’s also the potential for misuse, such as mass surveillance without consent. Bias<a contenteditable="false" data-primary="bias" data-type="indexterm" id="id719"/> in AI models is another ethical concern because models trained on limited or nondiverse datasets can lead to inaccurate or unfair outcomes, especially across different demographic groups. For these reasons, deploying facial detection and analysis solutions responsibly requires transparency, strict data governance, and ongoing efforts to mitigate bias and protect individual privacy. Let’s dive deeper into the key factors that shape responsible AI practices in this category.</p>

<section data-pdf-bookmark="Fairness in Facial Recognition" data-type="sect2"><div class="sect2" id="i06_chapter6_fairness_in_face_recognition_1742068262905667">
<h2>Fairness in Facial Recognition</h2>

<p>Ensuring<a contenteditable="false" data-primary="explainable AI" data-secondary="fairness" data-type="indexterm" id="id720"/> fairness<a contenteditable="false" data-primary="fairness" data-type="indexterm" id="id721"/><a contenteditable="false" data-primary="responsible AI" data-secondary="fairness" data-type="indexterm" id="id722"/><a contenteditable="false" data-primary="AI (artificial intelligence)" data-secondary="principles for responsible AI" data-tertiary="fairness" data-type="indexterm" id="id723"/> in facial recognition systems is a vital ethical consideration. Research has shown that these systems often exhibit biases, particularly against individuals with darker skin tones and women. A <a href="https://oreil.ly/orfFS">2025 study by Ketan Kotwal and Sébastien Marcel</a> found that<a contenteditable="false" data-primary="Kotwal, Ketan" data-type="indexterm" id="id724"/><a contenteditable="false" data-primary="Marcel, Sébastien" data-type="indexterm" id="id725"/> these systems often achieve higher accuracy for male subjects compared to female subjects. Moreover, the study highlighted challenges in skin tone classification, noting that lighter-skinned individuals are generally easier to verify than those with darker skin tones. This bias often arises from insufficiently diverse training datasets. Addressing this issue requires developing more inclusive datasets that represent a wide variety of skin tones, genders, and ethnic backgrounds. Moreover, employing fairness-aware algorithms and conducting regular audits can help identify and reduce biases.</p>

<p>Because of the issues, Microsoft has implemented restrictions with its face recognition technology. For example, it prohibits US police departments from using this technology. Microsoft has also retired certain features, such as those that infer emotions, gender, and age.</p>
</div></section>

<section data-pdf-bookmark="Privacy and Security" data-type="sect2"><div class="sect2" id="i06_chapter6_privacy_and_security_1742068262905736">
<h2>Privacy and Security</h2>

<p>Facial<a contenteditable="false" data-primary="responsible AI" data-secondary="security and privacy" data-type="indexterm" id="id726"/><a contenteditable="false" data-primary="security and privacy" data-type="indexterm" id="id727"/><a contenteditable="false" data-primary="AI (artificial intelligence)" data-secondary="principles for responsible AI" data-tertiary="security and privacy" data-type="indexterm" id="id728"/><a contenteditable="false" data-primary="explainable AI" data-secondary="security and privacy" data-type="indexterm" id="id729"/> recognition technology also raises pressing concerns about privacy and security. Unauthorized surveillance and data collection without consent infringe on individuals’ privacy rights. Furthermore, the storage of facial data carries risks, such as identity theft in the event of a breach. To address these concerns, organizations must establish strict guidelines requiring informed consent before collecting facial data. Adopting robust encryption methods and adhering to data protection laws like the European Union’s General Data Protection Regulation (GDPR) <a contenteditable="false" data-primary="GDPR (General Data Protection Regulation)" data-type="indexterm" id="id730"/>can further secure sensitive information. Providing transparency about how face recognition systems are used and offering individuals the option to opt out are additional measures that can help protect privacy and build trust.</p>
</div></section>

<section data-pdf-bookmark="Transparency" data-type="sect2"><div class="sect2" id="i06_chapter6_transparency_1742068262905790">
<h2>Transparency</h2>

<p>Transparency<a contenteditable="false" data-primary="responsible AI" data-secondary="transparency" data-type="indexterm" id="id731"/><a contenteditable="false" data-primary="transparency" data-type="indexterm" id="id732"/><a contenteditable="false" data-primary="AI (artificial intelligence)" data-secondary="principles for responsible AI" data-tertiary="transparency" data-type="indexterm" id="id733"/><a contenteditable="false" data-primary="explainable AI" data-secondary="transparency" data-type="indexterm" id="id734"/> is key for fostering trust and accountability in computer vision systems. Users should understand how these systems function, the types of data they collect, and the decision-making processes involved. To promote transparency, developers should provide clear documentation about their algorithms, including the data sources and methodologies employed. Open communication with stakeholders and the public can also help demystify the technology. What’s more, incorporating explainable AI techniques can make the decision-making processes of computer vision systems more accessible to nontechnical audiences, enhancing trust and <a contenteditable="false" data-primary="computer vision" data-secondary="responsible AI and" data-startref="icd609" data-type="indexterm" id="id735"/><a contenteditable="false" data-primary="responsible AI" data-secondary="computer vision and" data-startref="icd614" data-type="indexterm" id="id736"/>understanding.</p>
</div></section>
</div></section>

<section data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="i06_chapter6_conclusion_1742068262905842">
<h1>Conclusion</h1>

<p>To wrap things up, Azure’s computer vision tools pack serious power, putting advanced AI right at your fingertips. Knowing how Azure handles everything from object detection to OCR and facial analysis is not only impressive but also key for passing the AI-900 exam. This exam expects you to understand the capabilities Azure offers—like how multimodal models can blend text and images to create more contextually aware AI. And with multimodal models opening up new possibilities, Azure’s computer vision tools aren’t just practical—they’re shaping the next era of AI, one smart application at <a contenteditable="false" data-primary="AI (artificial intelligence)" data-secondary="categories of" data-startref="icd1001" data-tertiary="computer vision" data-type="indexterm" id="id737"/>a <a contenteditable="false" data-primary="computer vision" data-startref="icd601" data-type="indexterm" id="id738"/>time.</p>
</div></section>
<section class="less_space pagebreak-before" data-pdf-bookmark="Quiz" data-type="sect1"><div class="sect1" id="chapter6quiz">
	<h1>Quiz</h1>
	<p>To check your answers, please refer to the <a data-type="xref" href="app02.html#answers_chapter_6_sample_questions_1745932457451863">“Chapter 6 Answer Key”</a>.</p>
	<ol>
		<li>
		  <p>Which of the following is a fundamental concept in computer vision involving dividing an image into a grid of colored points? </p>
		  <ol type="a">
			<li>
			  <p>Filters </p>
			</li>
			<li>
			  <p>Pixels </p>
			</li>
			<li>
			  <p>Neural networks </p>
			</li>
			<li>
			  <p>Labels </p>
			</li>
		  </ol>
		</li>
		<li>
		  <p>Which method is used in object detection to pinpoint the location of objects within an image? </p>
		  <ol type="a">
			<li>
			  <p>OCR </p>
			</li>
			<li>
			  <p>Bounding boxes </p>
			</li>
			<li>
			  <p>Facial detection </p>
			</li>
			<li>
			  <p>Image classification </p>
			</li>
		  </ol>
		</li>
		<li>
		  <p>What is the purpose of convolution in computer vision? </p>
		  <ol type="a">
			<li>
			  <p>To detect color inversion </p>
			</li>
			<li>
			  <p>To resize images </p>
			</li>
			<li>
			  <p>To identify patterns in pixel data </p>
			</li>
			<li>
			  <p>To assign labels </p>
			</li>
		  </ol>
		</li>
		<li>
		  <p>In Microsoft Azure’s computer vision tools, which service primarily handles tasks like object detection and facial analysis? </p>
		  <ol type="a">
			<li>
			  <p>Azure Cognitive Search Service</p>
			</li>
			<li>
			  <p>Azure AI Vision </p>
			</li>
			<li>
			  <p>Azure Machine Learning </p>
			</li>
			<li>
			  <p>Azure Kubernetes Service </p>
			</li>
		  </ol>
		</li>
		<li>
		  <p>What type of neural network is most commonly used for computer vision tasks like image classification? </p>
		  <ol type="a">
			<li>
			  <p>Recurrent neural network (RNN) </p>
			</li>
			<li>
			  <p>Convolutional neural network (CNN) </p>
			</li>
			<li>
			  <p>Generative adversarial network (GAN) </p>
			</li>
			<li>
			  <p>Transformer </p>
			</li>
		  </ol>
		</li>
		<li class="less_space pagebreak-before">
		  <p>Which component of a CNN reduces the size of feature maps to focus on essential details? </p>
		  <ol type="a">
			<li>
			  <p>Convolutional layers </p>
			</li>
			<li>
			  <p>Fully connected layers </p>
			</li>
			<li>
			  <p>Pooling layers </p>
			</li>
			<li>
			  <p>Activation functions </p>
			</li>
		  </ol>
		</li>
		<li>
		  <p>Which computer vision technique reads and interprets text within images? </p>
		  <ol type="a">
			<li>
			  <p>Image classification </p>
			</li>
			<li>
			  <p>OCR </p>
			</li>
			<li>
			  <p>Facial detection </p>
			</li>
			<li>
			  <p>Object detection </p>
			</li>
		  </ol>
		</li>
		<li>
		  <p>What is the primary ethical concern associated with facial detection and analysis in AI? </p>
		  <ol type="a">
			<li>
			  <p>Lack of color accuracy </p>
			</li>
			<li>
			  <p>Privacy and consent issues </p>
			</li>
			<li>
			  <p>High computational costs </p>
			</li>
			<li>
			  <p>Limited dataset availability </p>
			</li>
		  </ol>
		</li>
		<li>
		  <p>In Azure’s AI Vision Studio, what data accompanies object detection to indicate the model’s confidence? </p>
		  <ol type="a">
			<li>
			  <p>Pixel count </p>
			</li>
			<li>
			  <p>Confidence score </p>
			</li>
			<li>
			  <p>Bounding box color </p>
			</li>
			<li>
			  <p>File type </p>
			</li>
		  </ol>
		</li>
		<li>
		  <p>What role do activation functions play in CNNs during the image recognition process? </p>
		  <ol type="a">
			<li>
			  <p>Identifying edge patterns </p>
			</li>
			<li>
			  <p>Reducing image size </p>
			</li>
			<li>
			  <p>Assigning probabilities to predictions </p>
			</li>
			<li>
			  <p>Adding color to images </p>
			</li>
		  </ol>
		</li>
	  </ol>
	
	</div></section>
</div></section></body></html>