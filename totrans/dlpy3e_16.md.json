["```py\nTranslate English to French:\n\nsea otter => loutre de mer\npeppermint => menthe poivrÃ©e\nplush giraffe => peluche girafe\ncheese => \n```", "```py\nTranslate English to French:\n\ncheese => \n```", "```py\nimport keras\nimport pathlib\n\nextract_dir = keras.utils.get_file(\n    fname=\"mini-c4\",\n    origin=(\n        \"https://hf.co/datasets/mattdangerw/mini-c4/resolve/main/mini-c4.zip\"\n    ),\n    extract=True,\n)\nextract_dir = pathlib.Path(extract_dir) / \"mini-c4\" \n```", "```py\n>>> with open(extract_dir / \"shard0.txt\", \"r\") as f:\n>>>     print(f.readline().replace(\"\\\\n\", \"\\n\")[:100])\nBeginners BBQ Class Taking Place in Missoula!\nDo you want to get better at making delicious BBQ? You\n```", "```py\nimport keras_hub\nimport numpy as np\n\nvocabulary_file = keras.utils.get_file(\n    origin=\"https://hf.co/mattdangerw/spiece/resolve/main/vocabulary.proto\",\n)\ntokenizer = keras_hub.tokenizers.SentencePieceTokenizer(vocabulary_file) \n```", "```py\n>>> tokenizer.tokenize(\"The quick brown fox.\")\narray([  450,  4996, 17354,  1701, 29916, 29889], dtype=int32)\n>>> tokenizer.detokenize([450, 4996, 17354, 1701, 29916, 29889])\n\"The quick brown fox.\"\n```", "```py\nimport tensorflow as tf\n\nbatch_size = 64\nsequence_length = 256\nsuffix = np.array([tokenizer.token_to_id(\"<|endoftext|>\")])\n\ndef read_file(filename):\n    ds = tf.data.TextLineDataset(filename)\n    # Restores newlines\n    ds = ds.map(lambda x: tf.strings.regex_replace(x, r\"\\\\n\", \"\\n\"))\n    # Tokenizes data\n    ds = ds.map(tokenizer, num_parallel_calls=8)\n    # Adds the <|endoftext|> token\n    return ds.map(lambda x: tf.concat([x, suffix], -1))\n\nfiles = [str(file) for file in extract_dir.glob(\"*.txt\")]\nds = tf.data.Dataset.from_tensor_slices(files)\n# Combines our file shards into a single dataset\nds = ds.interleave(read_file, cycle_length=32, num_parallel_calls=32)\n# Windows tokens into even samples of 256 tokens\nds = ds.rebatch(sequence_length + 1, drop_remainder=True)\n# Splits labels, offset by one\nds = ds.map(lambda x: (x[:-1], x[1:]))\nds = ds.batch(batch_size).prefetch(8) \n```", "```py\nnum_batches = 58746\nnum_val_batches = 500\nnum_train_batches = num_batches - num_val_batches\nval_ds = ds.take(num_val_batches).repeat()\ntrain_ds = ds.skip(num_val_batches).repeat() \n```", "```py\nfrom keras import layers\n\nclass TransformerDecoder(keras.Layer):\n    def __init__(self, hidden_dim, intermediate_dim, num_heads):\n        super().__init__()\n        key_dim = hidden_dim // num_heads\n        # Self-attention layers\n        self.self_attention = layers.MultiHeadAttention(\n            num_heads, key_dim, dropout=0.1\n        )\n        self.self_attention_layernorm = layers.LayerNormalization()\n        # Feedforward layers\n        self.feed_forward_1 = layers.Dense(intermediate_dim, activation=\"relu\")\n        self.feed_forward_2 = layers.Dense(hidden_dim)\n        self.feed_forward_layernorm = layers.LayerNormalization()\n        self.dropout = layers.Dropout(0.1)\n\n    def call(self, inputs):\n        # Self-attention computation\n        residual = x = inputs\n        x = self.self_attention(query=x, key=x, value=x, use_causal_mask=True)\n        x = self.dropout(x)\n        x = x + residual\n        x = self.self_attention_layernorm(x)\n        # Feedforward computation\n        residual = x\n        x = self.feed_forward_1(x)\n        x = self.feed_forward_2(x)\n        x = self.dropout(x)\n        x = x + residual\n        x = self.feed_forward_layernorm(x)\n        return x \n```", "```py\nfrom keras import ops\n\nclass PositionalEmbedding(keras.Layer):\n    def __init__(self, sequence_length, input_dim, output_dim):\n        super().__init__()\n        self.token_embeddings = layers.Embedding(input_dim, output_dim)\n        self.position_embeddings = layers.Embedding(sequence_length, output_dim)\n\n    def call(self, inputs, reverse=False):\n        if reverse:\n            token_embeddings = self.token_embeddings.embeddings\n            return ops.matmul(inputs, ops.transpose(token_embeddings))\n        positions = ops.cumsum(ops.ones_like(inputs), axis=-1) - 1\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions \n```", "```py\n# Enables mixed precision (see chapter 18)\nkeras.config.set_dtype_policy(\"mixed_float16\")\n\nvocab_size = tokenizer.vocabulary_size()\nhidden_dim = 512\nintermediate_dim = 2056\nnum_heads = 8\nnum_layers = 8\n\ninputs = keras.Input(shape=(None,), dtype=\"int32\", name=\"inputs\")\nembedding = PositionalEmbedding(sequence_length, vocab_size, hidden_dim)\nx = embedding(inputs)\nx = layers.LayerNormalization()(x)\nfor i in range(num_layers):\n    x = TransformerDecoder(hidden_dim, intermediate_dim, num_heads)(x)\noutputs = embedding(x, reverse=True)\nmini_gpt = keras.Model(inputs, outputs) \n```", "```py\nclass WarmupSchedule(keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self):\n        # Peak learning rate\n        self.rate = 2e-4\n        self.warmup_steps = 1_000.0\n\n    def __call__(self, step):\n        step = ops.cast(step, dtype=\"float32\")\n        scale = ops.minimum(step / self.warmup_steps, 1.0)\n        return self.rate * scale \n```", "```py\nimport matplotlib.pyplot as plt\n\nschedule = WarmupSchedule()\nx = range(0, 5_000, 100)\ny = [ops.convert_to_numpy(schedule(step)) for step in x]\nplt.plot(x, y)\nplt.xlabel(\"Train Step\")\nplt.ylabel(\"Learning Rate\")\nplt.show() \n```", "```py\nnum_epochs = 8\n# Set these to a lower value if you don't want to wait for training.\nsteps_per_epoch = num_train_batches // num_epochs\nvalidation_steps = num_val_batches\n\nmini_gpt.compile(\n    optimizer=keras.optimizers.Adam(schedule),\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[\"accuracy\"],\n)\nmini_gpt.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=num_epochs,\n    steps_per_epoch=steps_per_epoch,\n    validation_steps=validation_steps,\n) \n```", "```py\ndef generate(prompt, max_length=64):\n    tokens = list(ops.convert_to_numpy(tokenizer(prompt)))\n    prompt_length = len(tokens)\n    for _ in range(max_length - prompt_length):\n        prediction = mini_gpt(ops.convert_to_numpy([tokens]))\n        prediction = ops.convert_to_numpy(prediction[0, -1])\n        tokens.append(np.argmax(prediction).item())\n    return tokenizer.detokenize(tokens) \n```", "```py\n>>> prompt = \"A piece of advice\"\n>>> generate(prompt)\nA piece of advice, and the best way to get a feel for yourself is to get a sense\nof what you are doing.\nIf you are a business owner, you can get a sense of what you are doing. You can\nget a sense of what you are doing, and you can get a sense of what\n```", "```py\ndef compiled_generate(prompt, max_length=64):\n    tokens = list(ops.convert_to_numpy(tokenizer(prompt)))\n    prompt_length = len(tokens)\n    # Pads tokens to the full sequence length\n    tokens = tokens + [0] * (max_length - prompt_length)\n    for i in range(prompt_length, max_length):\n        prediction = mini_gpt.predict(np.array([tokens]), verbose=0)\n        prediction = prediction[0, i - 1]\n        tokens[i] = np.argmax(prediction).item()\n    return tokenizer.detokenize(tokens) \n```", "```py\n>>> import timeit\n>>> tries = 10\n>>> timeit.timeit(lambda: compiled_generate(prompt), number=tries) / tries\n0.4866470648999893\n```", "```py\ndef compiled_generate(prompt, sample_fn, max_length=64):\n    tokens = list(ops.convert_to_numpy(tokenizer(prompt)))\n    prompt_length = len(tokens)\n    tokens = tokens + [0] * (max_length - prompt_length)\n    for i in range(prompt_length, max_length):\n        prediction = mini_gpt.predict(np.array([tokens]), verbose=0)\n        prediction = prediction[0, i - 1]\n        next_token = ops.convert_to_numpy(sample_fn(prediction))\n        tokens[i] = np.array(next_token).item()\n    return tokenizer.detokenize(tokens) \n```", "```py\ndef greedy_search(preds):\n    return ops.argmax(preds)\n\ncompiled_generate(prompt, greedy_search) \n```", "```py\ndef random_sample(preds, temperature=1.0):\n    preds = preds / temperature\n    return keras.random.categorical(preds[None, :], num_samples=1)[0] \n```", "```py\n>>> compiled_generate(prompt, random_sample)\nA piece of advice, just read my knees and stick with getables and a hello to me.\nHowever, the bar napkin doesn't last as long. I happen to be waking up close and\npull it up as I wanted too and I still get it, really, shouldn't be a reaction\n```", "```py\n>>> from functools import partial\n>>> compiled_generate(prompt, partial(random_sample, temperature=2.0))\nA piece of advice tran writes using ignore unnecessary pivot - come without\nintrodu accounts indicugelÃ¢ per\\u3000divuren sendSolisÅ¼silen om transparent\nGill Guide pover integer song arrays coding\\u3000LIST**â€¦Allow index criteria\nDraw Reference Ex artifactincluding lib tak Br basunker increases entirelytembre\nAnyÐºÐ°TextView cardinal spiritual heavenToen\n>>> compiled_generate(prompt, partial(random_sample, temperature=0.8))\nA piece of advice I wrote about the same thing today. I have been a writer for\ntwo years now. I am writing this blog and I just wrote about it. I am writing\nthis blog and it was really interesting. I have been writing about the book and\nI have read many things about my life.\nThe\n>>> compiled_generate(prompt, partial(random_sample, temperature=0.2))\nA piece of advice, and a lot of people are saying that they have to be careful\nabout the way they think about it.\nI think it's a good idea to have a good understanding of the way you think about\nit.\nI think it's a good idea to have a good understanding of the\n```", "```py\ndef top_k(preds, k=5, temperature=1.0):\n    preds = preds / temperature\n    top_preds, top_indices = ops.top_k(preds, k=k, sorted=False)\n    choice = keras.random.categorical(top_preds[None, :], num_samples=1)[0]\n    return ops.take_along_axis(top_indices, choice, axis=-1) \n```", "```py\n>>> compiled_generate(prompt, partial(top_k, k=5))\nA piece of advice that I can't help it. I'm not going to be able to do anything\nfor a few months, but I'm trying to get a little better. It's a little too much.\nI have a few other questions on this site, but I'm sure I\n>>> compiled_generate(prompt, partial(top_k, k=20))\nA piece of advice and guidance from the Audi Bank in 2015\\. With all the above,\nit's not just a bad idea, but it's very good to see that is going to be a great\nyear for you in 2017.\nThat's really going to\n```", "```py\n>>> compiled_generate(prompt, partial(top_k, k=5, temperature=0.5))\nA piece of advice that you can use to get rid of the problem.\nThe first thing you need to do is to get the job done. It is important that you\nhave a plan that will help you get rid of it.\nThe first thing you need to do is to get rid of the problem yourself.\n```", "```py\ngemma_lm = keras_hub.models.CausalLM.from_preset(\n    \"gemma3_1b\",\n    dtype=\"float32\",\n) \n```", "```py\n>>> gemma_lm.summary()\nPreprocessor: \"gemma3_causal_lm_preprocessor\"\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ Layer (type)                                 â”ƒ                        Config â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ gemma3_tokenizer (Gemma3Tokenizer)           â”‚           Vocab size: 262,144 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nModel: \"gemma3_causal_lm\"\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ Layer (type)          â”ƒ Output Shape      â”ƒ     Param # â”ƒ Connected to       â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ padding_mask          â”‚ (None, None)      â”‚           0 â”‚ -                  â”‚\nâ”‚ (InputLayer)          â”‚                   â”‚             â”‚                    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ token_ids             â”‚ (None, None)      â”‚           0 â”‚ -                  â”‚\nâ”‚ (InputLayer)          â”‚                   â”‚             â”‚                    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ gemma3_backbone       â”‚ (None, None,      â”‚ 999,885,952 â”‚ padding_mask[0][0â€¦ â”‚\nâ”‚ (Gemma3Backbone)      â”‚ 1152)             â”‚             â”‚ token_ids[0][0]    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ token_embedding       â”‚ (None, None,      â”‚ 301,989,888 â”‚ gemma3_backbone[0â€¦ â”‚\nâ”‚ (ReversibleEmbedding) â”‚ 262144)           â”‚             â”‚                    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n Total params: 999,885,952 (3.72 GB)\n Trainable params: 999,885,952 (3.72 GB)\n Non-trainable params: 0 (0.00 B)\n```", "```py\n>>> gemma_lm.compile(sampler=\"greedy\")\n>>> gemma_lm.generate(\"A piece of advice\", max_length=40)\nA piece of advice from a former student of mine:\n\n<blockquote>\"I'm not sure if you've heard of it, but I've been told that the\nbest way to learn\n>>> gemma_lm.generate(\"How can I make brownies?\", max_length=40)\nHow can I make brownies?\n\n[User 0001]\n\nI'm trying to make brownies for my son's birthday party. I've never made\nbrownies before.\n```", "```py\n>>> gemma_lm.generate(\n>>>     \"The following brownie recipe is easy to make in just a few \"\n>>>     \"steps.\\n\\nYou can start by\",\n>>>     max_length=40,\n>>> )\nThe following brownie recipe is easy to make in just a few steps.\n\nYou can start by melting the butter and sugar in a saucepan over medium heat.\n\nThen add the eggs and vanilla extract\n```", "```py\n>>> gemma_lm.generate(\n>>>     \"Tell me about the 542nd president of the United States.\",\n>>>     max_length=40,\n>>> )\nTell me about the 542nd president of the United States.\n\nThe 542nd president of the United States was James A. Garfield.\n```", "```py\nimport json\n\nPROMPT_TEMPLATE = \"\"\"\"[instruction]\\n{}[end]\\n[response]\\n\"\"\"\nRESPONSE_TEMPLATE = \"\"\"{}[end]\"\"\"\n\ndataset_path = keras.utils.get_file(\n    origin=(\n        \"https://hf.co/datasets/databricks/databricks-dolly-15k/\"\n        \"resolve/main/databricks-dolly-15k.jsonl\"\n    ),\n)\ndata = {\"prompts\": [], \"responses\": []}\nwith open(dataset_path) as file:\n    for line in file:\n        features = json.loads(line)\n        if features[\"context\"]:\n            continue\n        data[\"prompts\"].append(PROMPT_TEMPLATE.format(features[\"instruction\"]))\n        data[\"responses\"].append(RESPONSE_TEMPLATE.format(features[\"response\"])) \n```", "```py\n>>> data[\"prompts\"][0]\n[instruction]\nWhich is a species of fish? Tope or Rope[end]\n[response] \n>>> data[\"responses\"][0]\nTope[end]\n```", "```py\nds = tf.data.Dataset.from_tensor_slices(data).shuffle(2000).batch(2)\nval_ds = ds.take(100)\ntrain_ds = ds.skip(100) \n```", "```py\n>>> preprocessor = gemma_lm.preprocessor\n>>> preprocessor.sequence_length = 512\n>>> batch = next(iter(train_ds))\n>>> x, y, sample_weight = preprocessor(batch)\n>>> x[\"token_ids\"].shape\n(2, 512)\n>>> x[\"padding_mask\"].shape\n(2, 512)\n>>> y.shape\n(2, 512)\n>>> sample_weight.shape\n(2, 512)\n```", "```py\n>>> x[\"token_ids\"][0, :5], y[0, :5]\n(Array([     2,  77074,  22768, 236842,    107], dtype=int32),\n Array([ 77074,  22768, 236842,    107,  24249], dtype=int32))\n```", "```py\nclass Linear(keras.Layer):\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        self.kernel = self.add_weight(shape=(input_dim, output_dim))\n\n    def call(self, inputs):\n        return ops.matmul(inputs, self.kernel) \n```", "```py\nclass LoraLinear(keras.Layer):\n    def __init__(self, input_dim, output_dim, rank):\n        super().__init__()\n        self.kernel = self.add_weight(\n            shape=(input_dim, output_dim), trainable=False\n        )\n        self.alpha = self.add_weight(shape=(input_dim, rank))\n        self.beta = self.add_weight(shape=(rank, output_dim))\n\n    def call(self, inputs):\n        frozen = ops.matmul(inputs, self.kernel)\n        update = ops.matmul(ops.matmul(inputs, self.alpha), self.beta)\n        return frozen + update \n```", "```py\ngemma_lm.backbone.enable_lora(rank=8) \n```", "```py\n>>> gemma_lm.summary()\nPreprocessor: \"gemma3_causal_lm_preprocessor\"\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ Layer (type)                                 â”ƒ                        Config â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ gemma3_tokenizer (Gemma3Tokenizer)           â”‚           Vocab size: 262,144 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nModel: \"gemma3_causal_lm\"\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ Layer (type)          â”ƒ Output Shape      â”ƒ     Param # â”ƒ Connected to       â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ padding_mask          â”‚ (None, None)      â”‚           0 â”‚ -                  â”‚\nâ”‚ (InputLayer)          â”‚                   â”‚             â”‚                    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ token_ids             â”‚ (None, None)      â”‚           0 â”‚ -                  â”‚\nâ”‚ (InputLayer)          â”‚                   â”‚             â”‚                    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ gemma3_backbone       â”‚ (None, None,      â”‚ 1,001,190,â€¦ â”‚ padding_mask[0][0â€¦ â”‚\nâ”‚ (Gemma3Backbone)      â”‚ 1152)             â”‚             â”‚ token_ids[0][0]    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ token_embedding       â”‚ (None, None,      â”‚ 301,989,888 â”‚ gemma3_backbone[0â€¦ â”‚\nâ”‚ (ReversibleEmbedding) â”‚ 262144)           â”‚             â”‚                    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n Total params: 1,001,190,528 (3.73 GB)\n Trainable params: 1,304,576 (4.98 MB)\n Non-trainable params: 999,885,952 (3.72 GB)\n```", "```py\ngemma_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=keras.optimizers.Adam(5e-5),\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\ngemma_lm.fit(train_ds, validation_data=val_ds, epochs=1) \n```", "```py\n>>> gemma_lm.generate(\n...     \"[instruction]\\nHow can I make brownies?[end]\\n\"\n...     \"[response]\\n\",\n...     max_length=512,\n... )\n[instruction]\nHow can I make brownies?[end]\n[response]\nYou can make brownies by mixing together 1 cup of flour, 1 cup of sugar, 1/2\ncup of butter, 1/2 cup of milk, 1/2 cup of chocolate chips, and 1/2 cup of\nchocolate chips. Then, you can bake it in a 9x13 pan for 30 minutes at 350\ndegrees Fahrenheit. You can also add a little bit of vanilla extract to the\nbatter to make it taste better.[end]\n>>> gemma_lm.generate(\n...     \"[instruction]\\nWhat is a proper noun?[end]\\n\"\n...     \"[response]\\n\",\n...     max_length=512,\n... )\n[instruction]\nWhat is a proper noun?[end]\n[response]\nA proper noun is a word that refers to a specific person, place, or thing.\nProper nouns are usually capitalized and are used to identify specific\nindividuals, places, or things. Proper nouns are often used in formal writing\nand are often used in titles, such as \"The White House\" or \"The Eiffel Tower.\"\nProper nouns are also used in titles of books, movies, and other works of\nliterature.[end]\n```", "```py\n>>> gemma_lm.generate(\n...     \"[instruction]\\nWho is the 542nd president of the United States?[end]\\n\"\n...     \"[response]\\n\",\n...     max_length=512,\n... )\n[instruction]\nWho is the 542nd president of the United States?[end]\n[response]\nThe 542nd president of the United States was James A. Garfield.[end]\n```", "```py\nfor prompts in dataset:\n    # Takes an action\n    responses = model.generate(prompts)\n    # Receives a reward\n    rewards = reward_model.predict(responses)\n    good_responses = []\n    for response, score in zip(responses, rewards):\n        if score > cutoff:\n            good_responses.append(response)\n    # Updates the model parameters. We do not update the reward model.\n    model.fit(good_responses) \n```", "```py\ngemma_lm = keras_hub.models.CausalLM.from_preset(\n    \"gemma3_instruct_4b\",\n    dtype=\"bfloat16\",\n) \n```", "```py\nPROMPT_TEMPLATE = \"\"\"<start_of_turn>user\n{}<end_of_turn>\n<start_of_turn>model\n\"\"\" \n```", "```py\n>>> prompt = \"Why can't you assign values in Jax tensors? Be brief!\"\n>>> gemma_lm.generate(PROMPT_TEMPLATE.format(prompt), max_length=512)\n<start_of_turn>user\nWhy can't you assign values in Jax tensors? Be brief!<end_of_turn>\n<start_of_turn>model\nJax tensors are designed for efficient automatic differentiation. Directly\nassigning values disrupts this process, making it difficult to track gradients\ncorrectly. Instead, Jax uses operations to modify tensor values, preserving the\ndifferentiation pipeline.<end_of_turn>\n```", "```py\n>>> prompt = \"Who is the 542nd president of the United States?\"\n>>> gemma_lm.generate(PROMPT_TEMPLATE.format(prompt), max_length=512)\n<start_of_turn>user\nWho is the 542nd president of the United States?<end_of_turn>\n<start_of_turn>model\nThis is a trick question! As of today, November 2, 2023, the United States has\nonly had 46 presidents. There hasn't been a 542nd president yet. ðŸ˜Š \n\nYou're playing with a very large number!<end_of_turn>\n```", "```py\nimport matplotlib.pyplot as plt\n\nimage_url = (\n    \"https://github.com/mattdangerw/keras-nlp-scripts/\"\n    \"blob/main/learned-python.png?raw=true\"\n)\nimage_path = keras.utils.get_file(origin=image_url)\n\nimage = np.array(keras.utils.load_img(image_path))\nplt.axis(\"off\")\nplt.imshow(image)\nplt.show() \n```", "```py\n>>> # Limits the maximum input size of the model\n>>> gemma_lm.preprocessor.max_images_per_prompt = 1\n>>> gemma_lm.preprocessor.sequence_length = 512\n>>> prompt = \"What is going on in this image? Be concise!<start_of_image>\"\n>>> gemma_lm.generate({\n...     \"prompts\": PROMPT_TEMPLATE.format(prompt),\n...     \"images\": [image],\n... })\n<start_of_turn>user\nWhat is going on in this image? Be concise!\n\n<start_of_image>\n\n<end_of_turn>\n<start_of_turn>model\nA snake wearing glasses is sitting in a leather armchair, surrounded by a large\nbookshelf, and reading a book. It's a whimsical, slightly surreal image.\n<end_of_turn>\n>>> prompt = \"What is the snake wearing?<start_of_image>\"\n>>> gemma_lm.generate({\n...     \"prompts\": PROMPT_TEMPLATE.format(prompt),\n...     \"images\": [image],\n... })\n<start_of_turn>user\nWhat is the snake wearing?\n\n<start_of_image>\n\n<end_of_turn>\n<start_of_turn>model\nThe snake is wearing a pair of glasses! They are red-framed and perched on its\nhead.<end_of_turn>\n```", "```py\nUse the following pieces of context to answer the question.\nQuestion: What are some good ways to improve sleep?\nContext: {text from a medical journal on improving sleep}\nAnswer: \n```", "```py\nprompt = \"\"\"Judy wrote a 2-page letter to 3 friends twice a week for 3 months.\nHow many letters did she write?\nBe brief, and add \"ANSWER:\" before your final answer.\"\"\"\n\n# Turns on random sampling to get a diverse range of outputs\ngemma_lm.compile(sampler=\"random\") \n```", "```py\n>>> gemma_lm.generate(PROMPT_TEMPLATE.format(prompt))\n<start_of_turn>user\nJudy wrote a 2-page letter to 3 friends twice a week for 3 months.\nHow many letters did she write?\nBe brief, and add \"ANSWER:\" before your final answer.<end_of_turn>\n<start_of_turn>model\nHere's how to solve the problem:\n\n* **Letters per week:** 3 friends * 2 letters/week = 6 letters/week\n* **Letters per month:** 6 letters/week * 4 weeks/month = 24 letters/month\n* **Letters in 3 months:** 24 letters/month * 3 months = 72 letters\n* **Total letters:** 72 letters * 2 = 144 letters\n\nANSWER: 144<end_of_turn>\n>>> gemma_lm.generate(PROMPT_TEMPLATE.format(prompt))\n<start_of_turn>user\nJudy wrote a 2-page letter to 3 friends twice a week for 3 months.\nHow many letters did she write?\nBe brief, and add \"ANSWER:\" before your final answer.<end_of_turn>\n<start_of_turn>model\nHere's how to solve the problem:\n\n* **Letters per week:** 3 friends * 2 letters/week = 6 letters/week\n* **Letters per month:** 6 letters/week * 4 weeks/month = 24 letters/month\n* **Total letters:** 24 letters/month * 3 months = 72 letters\n\nANSWER: 72<end_of_turn>\n```"]