- en: Chapter 4\. Generative Adversarial Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 2014, Ian Goodfellow et al. presented a paper entitled â€œGenerative Adversarial
    Netsâ€^([1](ch04.xhtml#idm45387021611344)) at the Neural Information Processing
    Systems conference (NeurIPS) in Montreal. The introduction of generative adversarial
    networks (or GANs, as they are more commonly known) is now regarded as a key turning
    point in the history of generative modeling, as the core ideas presented in this
    paper have spawned some of the most successful and impressive generative models
    ever created.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will first lay out the theoretical underpinning of GANs, then we
    will see how to build our own GAN using Keras.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Letâ€™s start with a short story to illustrate some of the fundamental concepts
    used in the GAN training process.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: The story of Brickki bricks and the forgers describes the training process of
    a generative adversarial network.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: A GAN is a battle between two adversaries, the *generator* and the *discriminator*.
    The generator tries to convert random noise into observations that look as if
    they have been sampled from the original dataset, and the discriminator tries
    to predict whether an observation comes from the original dataset or is one of
    the generatorâ€™s forgeries. Examples of the inputs and outputs to the two networks
    are shown in [FigureÂ 4-2](#gan_diagram_simple).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0402.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
- en: Figure 4-2\. Inputs and outputs of the two networks in a GAN
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: At the start of the process, the generator outputs noisy images and the discriminator
    predicts randomly. The key to GANs lies in how we alternate the training of the
    two networks, so that as the generator becomes more adept at fooling the discriminator,
    the discriminator must adapt in order to maintain its ability to correctly identify
    which observations are fake. This drives the generator to find new ways to fool
    the discriminator, and so the cycle continues.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Deep Convolutional GAN (DCGAN)
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To see this in action, letâ€™s start building our first GAN in Keras, to generate
    pictures of bricks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: We will be closely following one of the first major papers on GANs, â€œUnsupervised
    Representation Learning with Deep Convolutional Generative Adversarial Networks.â€^([2](ch04.xhtml#idm45387021585984))
    In this 2015 paper, the authors show how to build a deep convolutional GAN to
    generate realistic images from a variety of datasets. They also introduce several
    changes that significantly improve the quality of the generated images.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Running the Code for This Example
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for this example can be found in the Jupyter notebook located at *notebooks/04_gan/01_dcgan/dcgan.ipynb*
    in the book repository.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: The Bricks Dataset
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, youâ€™ll need to download the training data. Weâ€™ll be using the [Images
    of LEGO Bricks dataset](https://oreil.ly/3vp9f) that is available through Kaggle.
    This is a computer-rendered collection of 40,000 photographic images of 50 different
    toy bricks, taken from multiple angles. Some example images of Brickki products
    are shown in [FigureÂ 4-3](Images/#gan_bricks_images).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0403.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: Figure 4-3\. Examples of images from the Bricks dataset
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can download the dataset by running the Kaggle dataset downloader script
    in the book repository, as shown in [ExampleÂ 4-1](#downloading-lego-dataset).
    This will save the images and accompanying metadata locally to the */data* folder.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-1\. Downloading the Bricks dataset
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`We use the Keras function `image_dataset_from_directory` to create a TensorFlow
    Dataset pointed at the directory where the images are stored, as shown in [ExampleÂ 4-2](#preprocessing-lego-data).
    This allows us to read batches of images into memory only when required (e.g.,
    during training), so that we can work with large datasets and not worry about
    having to fit the entire dataset into memory. It also resizes the images to 64
    Ã— 64, interpolating between pixel values.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-2\. Creating a TensorFlow Dataset from image files in a directory
  id: totrans-23
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The original data is scaled in the range [0, 255] to denote the pixel intensity.
    When training GANs we rescale the data to the range [â€“1, 1] so that we can use
    the tanh activation function on the final layer of the generator, which tends
    to provide stronger gradients than the sigmoid function ([ExampleÂ 4-3](#preprocessing-lego-data_2)).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-3\. Preprocessing the Bricks dataset
  id: totrans-26
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Letâ€™s now take a look at how we build the discriminator.`  `## The Discriminator
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of the discriminator is to predict if an image is real or fake. This
    is a supervised image classification problem, so we can use a similar architecture
    to those we worked with in [ChapterÂ 2](ch02.xhtml#chapter_deep_learning): stacked
    convolutional layers, with a single output node.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: The full architecture of the discriminator we will be building is shown in [TableÂ 4-1](#gan_bricks_discriminator).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-1\. Model summary of the discriminator
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '| Layer (type) | Output shape | Param # |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
- en: '| InputLayer | (None, 64, 64, 1) | 0 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
- en: '| Conv2D | (None, 32, 32, 64) | 1,024 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
- en: '| LeakyReLU | (None, 32, 32, 64) | 0 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
- en: '| Dropout | (None, 32, 32, 64) | 0 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
- en: '| Conv2D | (None, 16, 16, 128) | 131,072 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
- en: '| BatchNormalization | (None, 16, 16, 128) | 512 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
- en: '| LeakyReLU | (None, 16, 16, 128) | 0 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
- en: '| Dropout | (None, 16, 16, 128) | 0 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
- en: '| Conv2D | (None, 8, 8, 256) | 524,288 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
- en: '| BatchNormalization | (None, 8, 8, 256) | 1,024 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
- en: '| LeakyReLU | (None, 8, 8, 256) | 0 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
- en: '| Dropout | (None, 8, 8, 256) | 0 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
- en: '| Conv2D | (None, 4, 4, 512) | 2,097,152 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
- en: '| BatchNormalization | (None, 4, 4, 512) | 2,048 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
- en: '| LeakyReLU | (None, 4, 4, 512) | 0 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
- en: '| Dropout | (None, 4, 4, 512) | 0 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
- en: '| Conv2D | (None, 1, 1, 1) | 8,192 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
- en: '| Flatten | (None, 1) | 0 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
- en: '| Total params | 2,765,312 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
- en: '| Trainable params | 2,763,520 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
- en: '| Non-trainable params | 1,792 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
- en: The Keras code to build the discriminator is provided in [ExampleÂ 4-4](#the-discriminator-ex).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-4\. The discriminator
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[![1](Images/1.png)](#co_generative_adversarial_networks_CO1-1)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Define the `Input` layer of the discriminator (the image).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_generative_adversarial_networks_CO1-2)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Stack `Conv2D` layers on top of each other, with `BatchNormalization`, `LeakyReLU`
    activation, and `Dropout` layers sandwiched in between.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_generative_adversarial_networks_CO1-3)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Flatten the last convolutional layerâ€”by this point, the shape of the tensor
    is 1 Ã— 1 Ã— 1, so there is no need for a final `Dense` layer.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_generative_adversarial_networks_CO1-4)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: The Keras model that defines the discriminatorâ€”a model that takes an input image
    and outputs a single number between 0 and 1.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Notice how we use a stride of 2 in some of the `Conv2D` layers to reduce the
    spatial shape of the tensor as it passes through the network (64 in the original
    image, then 32, 16, 8, 4, and finally 1), while increasing the number of channels
    (1 in the grayscale input image, then 64, 128, 256, and finally 512), before collapsing
    to a single prediction.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: We use a sigmoid activation on the final `Conv2D` layer to output a number between
    0 and 1.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: The Generator
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now letâ€™s build the generator. The input to the generator will be a vector drawn
    from a multivariate standard normal distribution. The output is an image of the
    same size as an image in the original training data.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'This description may remind you of the decoder in a variational autoencoder.
    In fact, the generator of a GAN fulfills exactly the same purpose as the decoder
    of a VAE: converting a vector in the latent space to an image. The concept of
    mapping from a latent space back to the original domain is very common in generative
    modeling, as it gives us the ability to manipulate vectors in the latent space
    to change high-level features of images in the original domain.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of the generator we will be building is shown in [TableÂ 4-2](#gan_bricks_generator).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-2\. Model summary of the generator
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '| Layer (type) | Output shape | Param # |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
- en: '| InputLayer | (None, 100) | 0 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: InputLayerï¼ˆæ— ï¼Œ100ï¼‰0
- en: '| Reshape | (None, 1, 1, 100) | 0 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: Reshapeï¼ˆæ— ï¼Œ1ï¼Œ1ï¼Œ100ï¼‰0
- en: '| Conv2DTranspose | (None, 4, 4, 512) | 819,200 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: Conv2DTransposeï¼ˆæ— ï¼Œ4ï¼Œ4ï¼Œ512ï¼‰819,200
- en: '| BatchNormalization | (None, 4, 4, 512) | 2,048 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: BatchNormalizationï¼ˆæ— ï¼Œ4ï¼Œ4ï¼Œ512ï¼‰2,048
- en: '| ReLU | (None, 4, 4, 512) | 0 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: ReLUï¼ˆæ— ï¼Œ4ï¼Œ4ï¼Œ512ï¼‰0
- en: '| Conv2DTranspose | (None, 8, 8, 256) | 2,097,152 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: Conv2DTransposeï¼ˆæ— ï¼Œ8ï¼Œ8ï¼Œ256ï¼‰2,097,152
- en: '| BatchNormalization | (None, 8, 8, 256) | 1,024 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: BatchNormalizationï¼ˆæ— ï¼Œ8ï¼Œ8ï¼Œ256ï¼‰1,024
- en: '| ReLU | (None, 8, 8, 256) | 0 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: ReLUï¼ˆæ— ï¼Œ8ï¼Œ8ï¼Œ256ï¼‰0
- en: '| Conv2DTranspose | (None, 16, 16, 128) | 524,288 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: Conv2DTransposeï¼ˆæ— ï¼Œ16ï¼Œ16ï¼Œ128ï¼‰524,288
- en: '| BatchNormalization | (None, 16, 16, 128) | 512 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: BatchNormalizationï¼ˆæ— ï¼Œ16ï¼Œ16ï¼Œ128ï¼‰512
- en: '| ReLU | (None, 16, 16, 128) | 0 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: ReLUï¼ˆæ— ï¼Œ16ï¼Œ16ï¼Œ128ï¼‰0
- en: '| Conv2DTranspose | (None, 32, 32, 64) | 131,072 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: Conv2DTransposeï¼ˆæ— ï¼Œ32ï¼Œ32ï¼Œ64ï¼‰131,072
- en: '| BatchNormalization | (None, 32, 32, 64) | 256 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: BatchNormalizationï¼ˆæ— ï¼Œ32ï¼Œ32ï¼Œ64ï¼‰256
- en: '| ReLU | (None, 32, 32, 64) | 0 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: ReLUï¼ˆæ— ï¼Œ32ï¼Œ32ï¼Œ64ï¼‰0
- en: '| Conv2DTranspose | (None, 64, 64, 1) | 1,024 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: Conv2DTransposeï¼ˆæ— ï¼Œ64ï¼Œ64ï¼Œ1ï¼‰1,024
- en: '| Total params | 3,576,576 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: æ€»å‚æ•°3,576,576
- en: '| Trainable params | 3,574,656 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: å¯è®­ç»ƒå‚æ•°3,574,656
- en: '| Non-trainable params | 1,920 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: ä¸å¯è®­ç»ƒå‚æ•°1,920
- en: The code for building the generator is given in [ExampleÂ 4-5](#the-generator-ex).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºç”Ÿæˆå™¨çš„ä»£ç åœ¨[ç¤ºä¾‹4-5](#the-generator-ex)ä¸­ç»™å‡ºã€‚
- en: Example 4-5\. The generator
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹4-5ã€‚ç”Ÿæˆå™¨
- en: '[PRE4]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](Images/1.png)](#co_generative_adversarial_networks_CO2-1)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_generative_adversarial_networks_CO2-1)'
- en: Define the `Input` layer of the generatorâ€”a vector of length 100.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: å®šä¹‰ç”Ÿæˆå™¨çš„â€œInputâ€å±‚-é•¿åº¦ä¸º100çš„å‘é‡ã€‚
- en: '[![2](Images/2.png)](#co_generative_adversarial_networks_CO2-2)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_generative_adversarial_networks_CO2-2)'
- en: We use a `Reshape` layer to give a 1 Ã— 1 Ã— 100 tensor, so that we can start
    applying convolutional transpose operations.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªâ€œReshapeâ€å±‚æ¥ç»™å‡ºä¸€ä¸ª1Ã—1Ã—100çš„å¼ é‡ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥å¼€å§‹åº”ç”¨å·ç§¯è½¬ç½®æ“ä½œã€‚
- en: '[![3](Images/3.png)](#co_generative_adversarial_networks_CO2-3)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_generative_adversarial_networks_CO2-3)'
- en: We pass this through four `Conv2DTranspose` layers, with `BatchNormalization`
    and `LeakyReLU` layers sandwiched in between.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€šè¿‡å››ä¸ªâ€œConv2DTransposeâ€å±‚ä¼ é€’è¿™äº›æ•°æ®ï¼Œå…¶ä¸­å¤¹åœ¨ä¸­é—´çš„æ˜¯â€œBatchNormalizationâ€å’Œâ€œLeakyReLUâ€å±‚ã€‚
- en: '[![4](Images/4.png)](#co_generative_adversarial_networks_CO2-4)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_generative_adversarial_networks_CO2-4)'
- en: The final `Conv2DTranspose` layer uses a tanh activation function to transform
    the output to the range [â€“1, 1], to match the original image domain.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆçš„â€œConv2DTransposeâ€å±‚ä½¿ç”¨tanhæ¿€æ´»å‡½æ•°å°†è¾“å‡ºè½¬æ¢ä¸ºèŒƒå›´[-1,1]ï¼Œä»¥åŒ¹é…åŸå§‹å›¾åƒåŸŸã€‚
- en: '[![5](Images/5.png)](#co_generative_adversarial_networks_CO2-5)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_generative_adversarial_networks_CO2-5)'
- en: The Keras model that defines the generatorâ€”a model that accepts a vector of
    length 100 and outputs a tensor of shape `[64, 64, 1]`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å®šä¹‰ç”Ÿæˆå™¨çš„Kerasæ¨¡å‹-æ¥å—é•¿åº¦ä¸º100çš„å‘é‡å¹¶è¾“å‡ºå½¢çŠ¶ä¸º`[64ï¼Œ64ï¼Œ1]`çš„å¼ é‡ã€‚
- en: Notice how we use a stride of 2 in some of the `Conv2DTranspose` layers to increase
    the spatial shape of the tensor as it passes through the network (1 in the original
    vector, then 4, 8, 16, 32, and finally 64), while decreasing the number of channels
    (512 then 256, 128, 64, and finally 1 to match the grayscale output).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œæˆ‘ä»¬åœ¨ä¸€äº›â€œConv2DTransposeâ€å±‚ä¸­ä½¿ç”¨æ­¥å¹…ä¸º2ï¼Œä»¥å¢åŠ é€šè¿‡ç½‘ç»œä¼ é€’æ—¶å¼ é‡çš„ç©ºé—´å½¢çŠ¶ï¼ˆåŸå§‹å‘é‡ä¸­ä¸º1ï¼Œç„¶åä¸º4ï¼Œ8ï¼Œ16ï¼Œ32ï¼Œæœ€ç»ˆä¸º64ï¼‰ï¼ŒåŒæ—¶å‡å°‘é€šé“æ•°ï¼ˆ512ï¼Œç„¶åä¸º256ï¼Œ128ï¼Œ64ï¼Œæœ€ç»ˆä¸º1ä»¥åŒ¹é…ç°åº¦è¾“å‡ºï¼‰ã€‚
- en: Training the DCGAN
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒDCGAN
- en: As we have seen, the architectures of the generator and discriminator in a DCGAN
    are very simple and not so different from the VAE models that we looked at in
    [ChapterÂ 3](ch03.xhtml#chapter_vae). The key to understanding GANs lies in understanding
    the training process for the generator and discriminator.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œåœ¨DCGANä¸­ç”Ÿæˆå™¨å’Œé‰´åˆ«å™¨çš„æ¶æ„éå¸¸ç®€å•ï¼Œå¹¶ä¸”ä¸æˆ‘ä»¬åœ¨ç¬¬3ç« ä¸­çœ‹åˆ°çš„VAEæ¨¡å‹å¹¶æ²¡æœ‰å¤ªå¤§ä¸åŒã€‚ç†è§£GANçš„å…³é”®åœ¨äºç†è§£ç”Ÿæˆå™¨å’Œé‰´åˆ«å™¨çš„è®­ç»ƒè¿‡ç¨‹ã€‚
- en: We can train the discriminator by creating a training set where some of the
    images are *real* observations from the training set and some are *fake* outputs
    from the generator. We then treat this as a supervised learning problem, where
    the labels are 1 for the real images and 0 for the fake images, with binary cross-entropy
    as the loss function.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡åˆ›å»ºä¸€ä¸ªè®­ç»ƒé›†æ¥è®­ç»ƒé‰´åˆ«å™¨ï¼Œå…¶ä¸­ä¸€äº›å›¾åƒæ˜¯æ¥è‡ªè®­ç»ƒé›†çš„*çœŸå®*è§‚å¯Ÿç»“æœï¼Œä¸€äº›æ˜¯æ¥è‡ªç”Ÿæˆå™¨çš„*å‡*è¾“å‡ºã€‚ç„¶åæˆ‘ä»¬å°†å…¶è§†ä¸ºä¸€ä¸ªç›‘ç£å­¦ä¹ é—®é¢˜ï¼Œå…¶ä¸­çœŸå®å›¾åƒçš„æ ‡ç­¾ä¸º1ï¼Œå‡å›¾åƒçš„æ ‡ç­¾ä¸º0ï¼ŒæŸå¤±å‡½æ•°ä¸ºäºŒå…ƒäº¤å‰ç†µã€‚
- en: How should we train the generator? We need to find a way of scoring each generated
    image so that it can optimize toward high-scoring images. Luckily, we have a discriminator
    that does exactly that! We can generate a batch of images and pass these through
    the discriminator to get a score for each image. The loss function for the generator
    is then simply the binary cross-entropy between these probabilities and a vector
    of ones, because we want to train the generator to produce images that the discriminator
    thinks are real.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åº”è¯¥å¦‚ä½•è®­ç»ƒç”Ÿæˆå™¨ï¼Ÿæˆ‘ä»¬éœ€è¦æ‰¾åˆ°ä¸€ç§è¯„åˆ†æ¯ä¸ªç”Ÿæˆçš„å›¾åƒçš„æ–¹æ³•ï¼Œä»¥ä¾¿å®ƒå¯ä»¥ä¼˜åŒ–åˆ°é«˜åˆ†å›¾åƒã€‚å¹¸è¿çš„æ˜¯ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªé‰´åˆ«å™¨æ­£æ˜¯è¿™æ ·åšçš„ï¼æˆ‘ä»¬å¯ä»¥ç”Ÿæˆä¸€æ‰¹å›¾åƒå¹¶å°†å…¶é€šè¿‡é‰´åˆ«å™¨ä»¥è·å¾—æ¯ä¸ªå›¾åƒçš„åˆ†æ•°ã€‚ç„¶åç”Ÿæˆå™¨çš„æŸå¤±å‡½æ•°å°±æ˜¯è¿™äº›æ¦‚ç‡ä¸ä¸€ä¸ªå…¨ä¸º1çš„å‘é‡ä¹‹é—´çš„äºŒå…ƒäº¤å‰ç†µï¼Œå› ä¸ºæˆ‘ä»¬å¸Œæœ›è®­ç»ƒç”Ÿæˆå™¨ç”Ÿæˆé‰´åˆ«å™¨è®¤ä¸ºæ˜¯çœŸå®çš„å›¾åƒã€‚
- en: Crucially, we must alternate the training of these two networks, making sure
    that we only update the weights of one network at a time. For example, during
    the generator training process, only the generatorâ€™s weights are updated. If we
    allowed the discriminatorâ€™s weights to change as well, the discriminator would
    just adjust so that it is more likely to predict the generated images to be real,
    which is not the desired outcome. We want generated images to be predicted close
    to 1 (real) because the generator is strong, not because the discriminator is
    weak.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: è‡³å…³é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å¿…é¡»äº¤æ›¿è®­ç»ƒè¿™ä¸¤ä¸ªç½‘ç»œï¼Œç¡®ä¿æˆ‘ä»¬ä¸€æ¬¡åªæ›´æ–°ä¸€ä¸ªç½‘ç»œçš„æƒé‡ã€‚ä¾‹å¦‚ï¼Œåœ¨ç”Ÿæˆå™¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œåªæœ‰ç”Ÿæˆå™¨çš„æƒé‡ä¼šè¢«æ›´æ–°ã€‚å¦‚æœæˆ‘ä»¬å…è®¸é‰´åˆ«å™¨çš„æƒé‡ä¹Ÿå‘ç”Ÿå˜åŒ–ï¼Œé‚£ä¹ˆé‰´åˆ«å™¨å°†åªæ˜¯è°ƒæ•´è‡ªå·±ï¼Œä»¥ä¾¿æ›´æœ‰å¯èƒ½é¢„æµ‹ç”Ÿæˆçš„å›¾åƒæ˜¯çœŸå®çš„ï¼Œè¿™ä¸æ˜¯æœŸæœ›çš„ç»“æœã€‚æˆ‘ä»¬å¸Œæœ›ç”Ÿæˆçš„å›¾åƒè¢«é¢„æµ‹æ¥è¿‘1ï¼ˆçœŸå®ï¼‰ï¼Œå› ä¸ºç”Ÿæˆå™¨å¼ºå¤§ï¼Œè€Œä¸æ˜¯å› ä¸ºé‰´åˆ«å™¨å¼±ã€‚
- en: A diagram of the training process for the discriminator and generator is shown
    in [FigureÂ 4-5](#gan_bricks_training).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: é‰´åˆ«å™¨å’Œç”Ÿæˆå™¨çš„è®­ç»ƒè¿‡ç¨‹çš„å›¾ç¤ºå¦‚[å›¾4-5](#gan_bricks_training)æ‰€ç¤ºã€‚
- en: '![](Images/gdl2_0405.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
- en: Figure 4-5\. Training the DCGANâ€”gray boxes indicate that the weights are frozen
    during training
  id: totrans-114
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Keras provides us with the ability to create a custom `train_step` function
    to implement this logic. [ExampleÂ 4-7](#building-the-gan-ex) shows the full `DCGAN`
    model class.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-7\. Compiling the DCGAN
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](Images/1.png)](#co_generative_adversarial_networks_CO3-1)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: The loss function for the generator and discriminator is `BinaryCrossentropy`.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_generative_adversarial_networks_CO3-2)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: To train the network, first sample a batch of vectors from a multivariate standard
    normal distribution.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_generative_adversarial_networks_CO3-3)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Next, pass these through the generator to produce a batch of generated images.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_generative_adversarial_networks_CO3-4)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Now ask the discriminator to predict the realness of the batch of real imagesâ€¦â€‹
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_generative_adversarial_networks_CO3-5)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: â€¦â€‹and the batch of generated images.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](Images/6.png)](#co_generative_adversarial_networks_CO3-6)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: The discriminator loss is the average binary cross-entropy across both the real
    images (with label 1) and the fake images (with label 0).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](Images/7.png)](#co_generative_adversarial_networks_CO3-7)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: The generator loss is the binary cross-entropy between the discriminator predictions
    for the generated images and a label of 1.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[![8](Images/8.png)](#co_generative_adversarial_networks_CO3-8)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Update the weights of the discriminator and generator separately.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: The discriminator and generator are constantly fighting for dominance, which
    can make the DCGAN training process unstable. Ideally, the training process will
    find an equilibrium that allows the generator to learn meaningful information
    from the discriminator and the quality of the images will start to improve. After
    enough epochs, the discriminator tends to end up dominating, as shown in [FigureÂ 4-6](#gan_bricks_loss_accuracy),
    but this may not be a problem as the generator may have already learned to produce
    sufficiently high-quality images by this point.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0406.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
- en: Figure 4-6\. Loss and accuracy of the discriminator and generator during training
  id: totrans-136
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Adding Noise to the Labels
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A useful trick when training GANs is to add a small amount of random noise to
    the training labels. This helps to improve the stability of the training process
    and sharpen the generated images. This *label smoothing* acts as way to tame the
    discriminator, so that it is presented with a more challenging task and doesnâ€™t
    overpower the generator.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Analysis of the DCGAN
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By observing images produced by the generator at specific epochs during training
    ([FigureÂ 4-7](#gan_bricks_by_epoch)), it is clear that the generator is becoming
    increasingly adept at producing images that could have been drawn from the training
    set.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0407.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
- en: Figure 4-7\. Output from the generator at specific epochs during training
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is somewhat miraculous that a neural network is able to convert random noise
    into something meaningful. It is worth remembering that we havenâ€™t provided the
    model with any additional features beyond the raw pixels, so it has to work out
    high-level concepts such as how to draw shadows, cuboids, and circles entirely
    by itself.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'Another requirement of a successful generative model is that it doesnâ€™t only
    reproduce images from the training set. To test this, we can find the image from
    the training set that is closest to a particular generated example. A good measure
    for distance is the *L1 distance*, defined as:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[FigureÂ 4-8](#gan_bricks_closest) shows the closest observations in the training
    set for a selection of generated images. We can see that while there is some degree
    of similarity between the generated images and the training set, they are not
    identical. This shows that the generator has understood these high-level features
    and can generate examples that are distinct from those it has already seen.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0408.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
- en: Figure 4-8\. Closest matches of generated images from the training set
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'GAN Training: Tips and Tricks'
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While GANs are a major breakthrough for generative modeling, they are also notoriously
    difficult to train. We will explore some of the most common problems and challenges
    encountered when training GANs in this section, alongside potential solutions.
    In the next section, we will look at some more fundamental adjustments to the
    GAN framework that we can make to remedy many of these problems.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Discriminator overpowers the generator
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If the discriminator becomes too strong, the signal from the loss function becomes
    too weak to drive any meaningful improvements in the generator. In the worst-case
    scenario, the discriminator perfectly learns to separate real images from fake
    images and the gradients vanish completely, leading to no training whatsoever,
    as can be seen in [FigureÂ 4-9](#gan_discriminator_dominant_ex).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0409.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
- en: Figure 4-9\. Example output when the discriminator overpowers the generator
  id: totrans-154
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If you find your discriminator loss function collapsing, you need to find ways
    to weaken the discriminator. Try the following suggestions:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Increase the `rate` parameter of the `Dropout` layers in the discriminator to
    dampen the amount of information that flows through the network.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce the learning rate of the discriminator.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce the number of convolutional filters in the discriminator.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add noise to the labels when training the discriminator.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flip the labels of some images at random when training the discriminator.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generator overpowers the discriminator
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If the discriminator is not powerful enough, the generator will find ways to
    easily trick the discriminator with a small sample of nearly identical images.
    This is known as *mode collapse*.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose we were to train the generator over several batches without
    updating the discriminator in between. The generator would be inclined to find
    a single observation (also known as a *mode*) that always fools the discriminator
    and would start to map every point in the latent input space to this image. Moreover,
    the gradients of the loss function would collapse to near 0, so it wouldnâ€™t be
    able to recover from this state.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Even if we then tried to retrain the discriminator to stop it being fooled by
    this one point, the generator would simply find another mode that fools the discriminator,
    since it has already become numb to its input and therefore has no incentive to
    diversify its output.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: The effect of mode collapse can be seen in [FigureÂ 4-10](#gan_mode_collapse).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0410.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
- en: Figure 4-10\. Example of mode collapse when the generator overpowers the discriminator
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you find that your generator is suffering from mode collapse, you can try
    strengthening the discriminator using the opposite suggestions to those listed
    in the previous section. Also, you can try reducing the learning rate of both
    networks and increasing the batch size.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Uninformative loss
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since the deep learning model is compiled to minimize the loss function, it
    would be natural to think that the smaller the loss function of the generator,
    the better the quality of the images produced. However, since the generator is
    only graded against the current discriminator and the discriminator is constantly
    improving, we cannot compare the loss function evaluated at different points in
    the training process. Indeed, in [FigureÂ 4-6](#gan_bricks_loss_accuracy), the
    loss function of the generator actually increases over time, even though the quality
    of the images is clearly improving. This lack of correlation between the generator
    loss and image quality sometimes makes GAN training difficult to monitor.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we have seen, even with simple GANs, there are a large number of hyperparameters
    to tune. As well as the overall architecture of both the discriminator and the
    generator, there are the parameters that govern batch normalization, dropout,
    learning rate, activation layers, convolutional filters, kernel size, striding,
    batch size, and latent space size to consider. GANs are highly sensitive to very
    slight changes in all of these parameters, and finding a set of parameters that
    works is often a case of educated trial and error, rather than following an established
    set of guidelines.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œå³ä½¿æ˜¯ç®€å•çš„GANï¼Œä¹Ÿæœ‰å¤§é‡çš„è¶…å‚æ•°éœ€è¦è°ƒæ•´ã€‚é™¤äº†é‰´åˆ«å™¨å’Œç”Ÿæˆå™¨çš„æ•´ä½“æ¶æ„å¤–ï¼Œè¿˜æœ‰æ§åˆ¶æ‰¹é‡å½’ä¸€åŒ–ã€ä¸¢å¼ƒã€å­¦ä¹ ç‡ã€æ¿€æ´»å±‚ã€å·ç§¯æ»¤æ³¢å™¨ã€å†…æ ¸å¤§å°ã€æ­¥å¹…ã€æ‰¹é‡å¤§å°å’Œæ½œåœ¨ç©ºé—´å¤§å°çš„å‚æ•°éœ€è¦è€ƒè™‘ã€‚GANå¯¹æ‰€æœ‰è¿™äº›å‚æ•°çš„å¾®å°å˜åŒ–éå¸¸æ•æ„Ÿï¼Œæ‰¾åˆ°ä¸€ç»„æœ‰æ•ˆçš„å‚æ•°é€šå¸¸æ˜¯ç»è¿‡æœ‰æ•™å…»çš„è¯•é”™è¿‡ç¨‹ï¼Œè€Œä¸æ˜¯éµå¾ªä¸€å¥—å·²å»ºç«‹çš„æŒ‡å¯¼æ–¹é’ˆã€‚
- en: This is why it is important to understand the inner workings of the GAN and
    know how to interpret the loss functionâ€”so that you can identify sensible adjustments
    to the hyperparameters that might improve the stability of the model.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯ä¸ºä»€ä¹ˆé‡è¦ç†è§£GANçš„å†…éƒ¨å·¥ä½œåŸç†å¹¶çŸ¥é“å¦‚ä½•è§£é‡ŠæŸå¤±å‡½æ•°â€”â€”è¿™æ ·ä½ å°±å¯ä»¥è¯†åˆ«å‡ºå¯èƒ½æ”¹å–„æ¨¡å‹ç¨³å®šæ€§çš„è¶…å‚æ•°çš„åˆç†è°ƒæ•´ã€‚
- en: Tackling GAN challenges
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è§£å†³GANçš„æŒ‘æˆ˜
- en: In recent years, several key advancements have drastically improved the overall
    stability of GAN models and diminished the likelihood of some of the problems
    listed earlier, such as mode collapse.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: è¿‘å¹´æ¥ï¼Œä¸€äº›å…³é”®è¿›å±•å¤§å¤§æé«˜äº†GANæ¨¡å‹çš„æ•´ä½“ç¨³å®šæ€§ï¼Œå¹¶å‡å°‘äº†ä¸€äº›æ—©æœŸåˆ—å‡ºçš„é—®é¢˜çš„å¯èƒ½æ€§ï¼Œæ¯”å¦‚æ¨¡å¼å´©æºƒã€‚
- en: In the remainder of this chapter we shall examine the Wasserstein GAN with Gradient
    Penalty (WGAN-GP), which makes several key adjustments to the GAN framework we
    have explored thus far to improve the stability and quality of the image generation
    process.`  `# Wasserstein GAN with Gradient Penalty (WGAN-GP)
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« çš„å…¶ä½™éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†ç ”ç©¶å¸¦æœ‰æ¢¯åº¦æƒ©ç½šçš„Wasserstein GANï¼ˆWGAN-GPï¼‰ï¼Œè¯¥æ¨¡å‹å¯¹æˆ‘ä»¬è¿„ä»Šä¸ºæ­¢æ¢ç´¢çš„GANæ¡†æ¶è¿›è¡Œäº†å‡ ä¸ªå…³é”®è°ƒæ•´ï¼Œä»¥æ”¹å–„å›¾åƒç”Ÿæˆè¿‡ç¨‹çš„ç¨³å®šæ€§å’Œè´¨é‡ã€‚`
    `#å¸¦æœ‰æ¢¯åº¦æƒ©ç½šçš„Wasserstein GANï¼ˆWGAN-GPï¼‰
- en: In this section we will build a WGAN-GP to generate faces from the CelebA dataset
    that we utilized in [ChapterÂ 3](ch03.xhtml#chapter_vae).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªWGAN-GPæ¥ä»æˆ‘ä»¬åœ¨[ç¬¬3ç« ](ch03.xhtml#chapter_vae)ä¸­ä½¿ç”¨çš„CelebAæ•°æ®é›†ä¸­ç”Ÿæˆäººè„¸ã€‚
- en: Running the Code for This Example
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¿è¡Œæ­¤ç¤ºä¾‹çš„ä»£ç 
- en: The code for this example can be found in the Jupyter notebook located at *notebooks/04_gan/02_wgan_gp/wgan_gp.ipynb*
    in the book repository.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç¤ºä¾‹çš„ä»£ç å¯ä»¥åœ¨ä¹¦åº“ä¸­çš„*notebooks/04_gan/02_wgan_gp/wgan_gp.ipynb*ä¸­æ‰¾åˆ°ã€‚
- en: The code has been adapted from the excellent [WGAN-GP tutorial](https://oreil.ly/dHYbC)
    created by Aakash Kumar Nain, available on the Keras website.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ®µä»£ç æ˜¯ä»ç”±Aakash Kumar Nainåˆ›å»ºçš„ä¼˜ç§€çš„[WGAN-GPæ•™ç¨‹](https://oreil.ly/dHYbC)ä¸­æ”¹ç¼–è€Œæ¥ï¼Œè¯¥æ•™ç¨‹å¯åœ¨Kerasç½‘ç«™ä¸Šæ‰¾åˆ°ã€‚
- en: 'The Wasserstein GAN (WGAN), introduced in a 2017 paper by Arjovsky et al.,^([4](ch04.xhtml#idm45387021016704))
    was one of the first big steps toward stabilizing GAN training. With a few changes,
    the authors were able to show how to train GANs that have the following two properties
    (quoted from the paper):'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: Wasserstein GANï¼ˆWGANï¼‰æ˜¯ç”±Arjovskyç­‰äººåœ¨2017å¹´çš„ä¸€ç¯‡è®ºæ–‡ä¸­å¼•å…¥çš„ï¼Œæ˜¯ç¨³å®šGANè®­ç»ƒçš„ç¬¬ä¸€æ­¥ã€‚é€šè¿‡ä¸€äº›æ”¹å˜ï¼Œä½œè€…ä»¬èƒ½å¤Ÿå±•ç¤ºå¦‚ä½•è®­ç»ƒå…·æœ‰ä»¥ä¸‹ä¸¤ä¸ªç‰¹æ€§çš„GANï¼ˆå¼•ç”¨è‡ªè®ºæ–‡ï¼‰ï¼š
- en: A meaningful loss metric that correlates with the generatorâ€™s convergence and
    sample quality
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªä¸ç”Ÿæˆå™¨çš„æ”¶æ•›å’Œæ ·æœ¬è´¨é‡ç›¸å…³çš„æœ‰æ„ä¹‰çš„æŸå¤±åº¦é‡
- en: Improved stability of the optimization process
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–è¿‡ç¨‹çš„ç¨³å®šæ€§æé«˜
- en: Specifically, the paper introduces the *Wasserstein loss function* for both
    the discriminator and the generator. Using this loss function instead of binary
    cross-entropy results in a more stable convergence of the GAN.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: å…·ä½“æ¥è¯´ï¼Œè¯¥è®ºæ–‡ä¸ºé‰´åˆ«å™¨å’Œç”Ÿæˆå™¨å¼•å…¥äº†*WassersteinæŸå¤±å‡½æ•°*ã€‚ä½¿ç”¨è¿™ä¸ªæŸå¤±å‡½æ•°è€Œä¸æ˜¯äºŒå…ƒäº¤å‰ç†µä¼šå¯¼è‡´GANæ›´ç¨³å®šåœ°æ”¶æ•›ã€‚
- en: In this section weâ€™ll define the Wasserstein loss function and then see what
    other changes we need to make to the model architecture and training process to
    incorporate our new loss function.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å®šä¹‰WassersteinæŸå¤±å‡½æ•°ï¼Œç„¶åçœ‹çœ‹æˆ‘ä»¬éœ€è¦å¯¹æ¨¡å‹æ¶æ„å’Œè®­ç»ƒè¿‡ç¨‹åšå“ªäº›å…¶ä»–æ›´æ”¹ä»¥æ•´åˆæˆ‘ä»¬çš„æ–°æŸå¤±å‡½æ•°ã€‚
- en: You can find the full model class in the Jupyter notebook located at *chapter05/wgan-gp/faces/train.ipynb*
    in the book repository.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨ä¹¦åº“ä¸­çš„*chapter05/wgan-gp/faces/train.ipynb*ä¸­æ‰¾åˆ°å®Œæ•´çš„æ¨¡å‹ç±»ã€‚
- en: Wasserstein Loss
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: WassersteinæŸå¤±
- en: Letâ€™s first remind ourselves of the definition of binary cross-entropy lossâ€”the
    function that we are currently using to train the discriminator and generator
    of the GAN ([Equation 4-1](#binary-cross-entropy-loss)).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é¦–å…ˆå›é¡¾ä¸€ä¸‹äºŒå…ƒäº¤å‰ç†µæŸå¤±çš„å®šä¹‰â€”â€”æˆ‘ä»¬ç›®å‰ç”¨æ¥è®­ç»ƒGANçš„å‡½æ•°ï¼ˆ[æ–¹ç¨‹4-1](#binary-cross-entropy-loss)ï¼‰ã€‚
- en: Equation 4-1\. Binary cross-entropy loss
  id: totrans-189
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹4-1. äºŒå…ƒäº¤å‰ç†µæŸå¤±
- en: <math alttext="minus StartFraction 1 Over n EndFraction sigma-summation Underscript
    i equals 1 Overscript n Endscripts left-parenthesis y Subscript i Baseline log
    left-parenthesis p Subscript i Baseline right-parenthesis plus left-parenthesis
    1 minus y Subscript i Baseline right-parenthesis log left-parenthesis 1 minus
    p Subscript i Baseline right-parenthesis right-parenthesis" display="block"><mrow><mo>-</mo>
    <mfrac><mn>1</mn> <mi>n</mi></mfrac> <munderover><mo>âˆ‘</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo
    form="prefix">log</mo> <mrow><mo>(</mo> <msub><mi>p</mi> <mi>i</mi></msub> <mo>)</mo></mrow>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo>
    <msub><mi>p</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="minus StartFraction 1 Over n EndFraction sigma-summation Underscript
    i equals 1 Overscript n Endscripts left-parenthesis y Subscript i Baseline log
    left-parenthesis p Subscript i Baseline right-parenthesis plus left-parenthesis
    1 minus y Subscript i Baseline right-parenthesis log left-parenthesis 1 minus
    p Subscript i Baseline right-parenthesis right-parenthesis" display="block"><mrow><mo>-</mo>
    <mfrac><mn>1</mn> <mi>n</mi></mfrac> <munderover><mo>âˆ‘</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo
    form="prefix">log</mo> <mrow><mo>(</mo> <msub><mi>p</mi> <mi>i</mi></msub> <mo>)</mo></mrow>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo>
    <msub><mi>p</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
- en: To train the GAN discriminator <math alttext="upper D"><mi>D</mi></math> , we
    calculate the loss when comparing predictions for real images <math alttext="p
    Subscript i Baseline equals upper D left-parenthesis x Subscript i Baseline right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>i</mi></msub> <mo>=</mo> <mi>D</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow></mrow></math> to the response <math alttext="y Subscript i Baseline
    equals 1"><mrow><msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo> <mn>1</mn></mrow></math>
    and predictions for generated images <math alttext="p Subscript i Baseline equals
    upper D left-parenthesis upper G left-parenthesis z Subscript i Baseline right-parenthesis
    right-parenthesis"><mrow><msub><mi>p</mi> <mi>i</mi></msub> <mo>=</mo> <mi>D</mi>
    <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <msub><mi>z</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math> to the response <math alttext="y
    Subscript i Baseline equals 0"><mrow><msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo>
    <mn>0</mn></mrow></math> . Therefore, for the GAN discriminator, minimizing the
    loss function can be written as shown in [Equation 4-2](#GAN-discriminator-loss-minimization).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è®­ç»ƒGANé‰´åˆ«å™¨Dï¼Œæˆ‘ä»¬è®¡ç®—äº†å¯¹æ¯”çœŸå®å›¾åƒx_içš„é¢„æµ‹p_iä¸å“åº”y_i=1ä»¥åŠå¯¹æ¯”ç”Ÿæˆå›¾åƒG(z_i)çš„é¢„æµ‹p_iä¸å“åº”y_i=0çš„æŸå¤±ã€‚å› æ­¤ï¼Œå¯¹äºGANé‰´åˆ«å™¨ï¼Œæœ€å°åŒ–æŸå¤±å‡½æ•°å¯ä»¥å†™æˆ[æ–¹ç¨‹4-2](#GAN-discriminator-loss-minimization)æ‰€ç¤ºã€‚
- en: Equation 4-2\. GAN discriminator loss minimization
  id: totrans-192
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹4-2. GANé‰´åˆ«å™¨æŸå¤±æœ€å°åŒ–
- en: <math alttext="min Underscript upper D Endscripts minus left-parenthesis double-struck
    upper E Subscript x tilde p Sub Subscript upper X Subscript Baseline left-bracket
    log upper D left-parenthesis x right-parenthesis right-bracket plus double-struck
    upper E Subscript z tilde p Sub Subscript upper Z Subscript Baseline left-bracket
    log left-parenthesis 1 minus upper D left-parenthesis upper G left-parenthesis
    z right-parenthesis right-parenthesis right-parenthesis right-bracket right-parenthesis"
    display="block"><mrow><munder><mo movablelimits="true" form="prefix">min</mo>
    <mi>D</mi></munder> <mo>-</mo> <mrow><mo>(</mo> <msub><mi>ğ”¼</mi> <mrow><mi>x</mi><mo>âˆ¼</mo><msub><mi>p</mi>
    <mi>X</mi></msub></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <mrow><mi>D</mi>
    <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>+</mo> <msub><mi>ğ”¼</mi>
    <mrow><mi>z</mi><mo>âˆ¼</mo><msub><mi>p</mi> <mi>Z</mi></msub></mrow></msub> <mrow><mo>[</mo>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>D</mi> <mo>(</mo>
    <mi>G</mi> <mo>(</mo> <mi>z</mi> <mo>)</mo> <mo>)</mo> <mo>)</mo></mrow> <mo>]</mo></mrow>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="min Underscript upper D Endscripts minus left-parenthesis double-struck
    upper E Subscript x tilde p Sub Subscript upper X Subscript Baseline left-bracket
    log upper D left-parenthesis x right-parenthesis right-bracket plus double-struck
    upper E Subscript z tilde p Sub Subscript upper Z Subscript Baseline left-bracket
    log left-parenthesis 1 minus upper D left-parenthesis upper G left-parenthesis
    z right-parenthesis right-parenthesis right-parenthesis right-bracket right-parenthesis"
    display="block"><mrow><munder><mo movablelimits="true" form="prefix">min</mo>
    <mi>D</mi></munder> <mo>-</mo> <mrow><mo>(</mo> <msub><mi>ğ”¼</mi> <mrow><mi>x</mi><mo>âˆ¼</mo><msub><mi>p</mi>
    <mi>X</mi></msub></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <mrow><mi>D</mi>
    <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>+</mo> <msub><mi>ğ”¼</mi>
    <mrow><mi>z</mi><mo>âˆ¼</mo><msub><mi>p</mi> <mi>Z</mi></msub></mrow></msub> <mrow><mo>[</mo>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>D</mi> <mo>(</mo>
    <mi>G</mi> <mo>(</mo> <mi>z</mi> <mo>)</mo> <mo>)</mo> <mo>)</mo></mrow> <mo>]</mo></mrow>
    <mo>)</mo></mrow></mrow></math>
- en: To train the GAN generator <math alttext="upper G"><mi>G</mi></math> , we calculate
    the loss when comparing predictions for generated images <math alttext="p Subscript
    i Baseline equals upper D left-parenthesis upper G left-parenthesis z Subscript
    i Baseline right-parenthesis right-parenthesis"><mrow><msub><mi>p</mi> <mi>i</mi></msub>
    <mo>=</mo> <mi>D</mi> <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <msub><mi>z</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math> to the response
    <math alttext="y Subscript i Baseline equals 1"><mrow><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>=</mo> <mn>1</mn></mrow></math> . Therefore, for the GAN generator, minimizing
    the loss function can be written as shown in [Equation 4-3](#GAN-generator-loss-minimization).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è®­ç»ƒGANç”Ÿæˆå™¨Gï¼Œæˆ‘ä»¬è®¡ç®—äº†å¯¹æ¯”ç”Ÿæˆå›¾åƒG(z_i)çš„é¢„æµ‹p_iä¸å“åº”y_i=1çš„æŸå¤±ã€‚å› æ­¤ï¼Œå¯¹äºGANç”Ÿæˆå™¨ï¼Œæœ€å°åŒ–æŸå¤±å‡½æ•°å¯ä»¥å†™æˆ[æ–¹ç¨‹4-3](#GAN-generator-loss-minimization)æ‰€ç¤ºã€‚
- en: Equation 4-3\. GAN generator loss minimization
  id: totrans-195
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹4-3. GANç”Ÿæˆå™¨æŸå¤±æœ€å°åŒ–
- en: <math alttext="min Underscript upper G Endscripts minus left-parenthesis double-struck
    upper E Subscript z tilde p Sub Subscript upper Z Subscript Baseline left-bracket
    log upper D left-parenthesis upper G left-parenthesis z right-parenthesis right-parenthesis
    right-bracket right-parenthesis" display="block"><mrow><munder><mo movablelimits="true"
    form="prefix">min</mo> <mi>G</mi></munder> <mo>-</mo> <mrow><mo>(</mo> <msub><mi>ğ”¼</mi>
    <mrow><mi>z</mi><mo>âˆ¼</mo><msub><mi>p</mi> <mi>Z</mi></msub></mrow></msub> <mrow><mo>[</mo>
    <mo form="prefix">log</mo> <mrow><mi>D</mi> <mo>(</mo> <mi>G</mi> <mo>(</mo> <mi>z</mi>
    <mo>)</mo> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>)</mo></mrow></mrow></math>
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="min Underscript upper G Endscripts minus left-parenthesis double-struck
    upper E Subscript z tilde p Sub Subscript upper Z Subscript Baseline left-bracket
    log upper D left-parenthesis upper G left-parenthesis z right-parenthesis right-parenthesis
    right-bracket right-parenthesis" display="block"><mrow><munder><mo movablelimits="true"
    form="prefix">min</mo> <mi>G</mi></munder> <mo>-</mo> <mrow><mo>(</mo> <msub><mi>ğ”¼</mi>
    <mrow><mi>z</mi><mo>âˆ¼</mo><msub><mi>p</mi> <mi>Z</mi></msub></mrow></msub> <mrow><mo>[</mo>
    <mo form="prefix">log</mo> <mrow><mi>D</mi> <mo>(</mo> <mi>G</mi> <mo>(</mo> <mi>z</mi>
    <mo>)</mo> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>)</mo></mrow></mrow></math>
- en: Now letâ€™s compare this to the Wasserstein loss function.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬å°†å…¶ä¸WassersteinæŸå¤±å‡½æ•°è¿›è¡Œæ¯”è¾ƒã€‚
- en: First, the Wasserstein loss requires that we use <math alttext="y Subscript
    i Baseline equals 1"><mrow><msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo> <mn>1</mn></mrow></math>
    and <math alttext="y Subscript i"><msub><mi>y</mi> <mi>i</mi></msub></math> =
    â€“1 as labels, rather than 1 and 0\. We also remove the sigmoid activation from
    the final layer of the discriminator, so that predictions <math alttext="p Subscript
    i"><msub><mi>p</mi> <mi>i</mi></msub></math> are no longer constrained to fall
    in the range [0, 1] but instead can now be any number in the range ( <math alttext="negative
    normal infinity"><mrow><mo>-</mo> <mi>âˆ</mi></mrow></math> , <math alttext="normal
    infinity"><mi>âˆ</mi></math> ). For this reason, the discriminator in a WGAN is
    usually referred to as a *critic* that outputs a *score* rather than a probability.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼ŒWassersteinæŸå¤±è¦æ±‚æˆ‘ä»¬ä½¿ç”¨y_i=1å’Œy_i=-1ä½œä¸ºæ ‡ç­¾ï¼Œè€Œä¸æ˜¯1å’Œ0ã€‚æˆ‘ä»¬è¿˜ä»é‰´åˆ«å™¨çš„æœ€åä¸€å±‚ä¸­ç§»é™¤äº†sigmoidæ¿€æ´»ï¼Œä½¿å¾—é¢„æµ‹p_iä¸å†å—é™äº[0,
    1]èŒƒå›´ï¼Œè€Œæ˜¯å¯ä»¥æ˜¯ä»»ä½•èŒƒå›´å†…çš„ä»»æ„æ•°å­—ï¼ˆè´Ÿæ— ç©·ï¼Œæ­£æ— ç©·ï¼‰ã€‚å› æ­¤ï¼Œåœ¨WGANä¸­ï¼Œé‰´åˆ«å™¨é€šå¸¸è¢«ç§°ä¸º*è¯„è®ºå®¶*ï¼Œè¾“å‡º*åˆ†æ•°*è€Œä¸æ˜¯æ¦‚ç‡ã€‚
- en: 'The Wasserstein loss function is defined as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: WassersteinæŸå¤±å‡½æ•°å®šä¹‰å¦‚ä¸‹ï¼š
- en: <math alttext="minus StartFraction 1 Over n EndFraction sigma-summation Underscript
    i equals 1 Overscript n Endscripts left-parenthesis y Subscript i Baseline p Subscript
    i Baseline right-parenthesis" display="block"><mrow><mo>-</mo> <mfrac><mn>1</mn>
    <mi>n</mi></mfrac> <munderover><mo>âˆ‘</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>i</mi></msub> <msub><mi>p</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="minus StartFraction 1 Over n EndFraction sigma-summation Underscript
    i equals 1 Overscript n Endscripts left-parenthesis y Subscript i Baseline p Subscript
    i Baseline right-parenthesis" display="block"><mrow><mo>-</mo> <mfrac><mn>1</mn>
    <mi>n</mi></mfrac> <munderover><mo>âˆ‘</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>i</mi></msub> <msub><mi>p</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>
- en: 'To train the WGAN critic <math alttext="upper D"><mi>D</mi></math> , we calculate
    the loss when comparing predictions for real images <math alttext="p Subscript
    i Baseline equals upper D left-parenthesis x Subscript i Baseline right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>i</mi></msub> <mo>=</mo> <mi>D</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow></mrow></math> to the response <math alttext="y Subscript i Baseline
    equals 1"><mrow><msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo> <mn>1</mn></mrow></math>
    and predictions for generated images <math alttext="p Subscript i Baseline equals
    upper D left-parenthesis upper G left-parenthesis z Subscript i Baseline right-parenthesis
    right-parenthesis"><mrow><msub><mi>p</mi> <mi>i</mi></msub> <mo>=</mo> <mi>D</mi>
    <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <msub><mi>z</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math> to the response <math alttext="y
    Subscript i"><msub><mi>y</mi> <mi>i</mi></msub></math> = â€“1\. Therefore, for the
    WGAN critic, minimizing the loss function can be written as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è®­ç»ƒWGANè¯„è®ºå®¶<math alttext="upper D"><mi>D</mi></math>ï¼Œæˆ‘ä»¬è®¡ç®—çœŸå®å›¾åƒçš„é¢„æµ‹ä¸å“åº”ä¹‹é—´çš„æŸå¤±<math
    alttext="p Subscript i Baseline equals upper D left-parenthesis x Subscript i
    Baseline right-parenthesis"><mrow><msub><mi>p</mi> <mi>i</mi></msub> <mo>=</mo>
    <mi>D</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>ä¸å“åº”<math
    alttext="y Subscript i Baseline equals 1"><mrow><msub><mi>y</mi> <mi>i</msub>
    <mo>=</mo> <mn>1</mn></mrow></math>ï¼Œä»¥åŠç”Ÿæˆå›¾åƒçš„é¢„æµ‹ä¸å“åº”ä¹‹é—´çš„æŸå¤±<math alttext="p Subscript
    i Baseline equals upper D left-parenthesis upper G left-parenthesis z Subscript
    i Baseline right-parenthesis right-parenthesis"><mrow><msub><mi>p</mi> <mi>i</msub>
    <mo>=</mo> <mi>D</mi> <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <msub><mi>z</mi>
    <mi>i</msub> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>ä¸å“åº”<math alttext="y
    Subscript i"><msub><mi>y</mi> <mi>i</msub></math> = -1ã€‚å› æ­¤ï¼Œå¯¹äºWGANè¯„è®ºå®¶ï¼Œæœ€å°åŒ–æŸå¤±å‡½æ•°å¯ä»¥å†™æˆå¦‚ä¸‹å½¢å¼ï¼š
- en: <math alttext="min Underscript upper D Endscripts minus left-parenthesis double-struck
    upper E Subscript x tilde p Sub Subscript upper X Subscript Baseline left-bracket
    upper D left-parenthesis x right-parenthesis right-bracket minus double-struck
    upper E Subscript z tilde p Sub Subscript upper Z Subscript Baseline left-bracket
    upper D left-parenthesis upper G left-parenthesis z right-parenthesis right-parenthesis
    right-bracket right-parenthesis" display="block"><mrow><munder><mo movablelimits="true"
    form="prefix">min</mo> <mi>D</mi></munder> <mo>-</mo> <mrow><mo>(</mo> <msub><mi>ğ”¼</mi>
    <mrow><mi>x</mi><mo>âˆ¼</mo><msub><mi>p</mi> <mi>X</mi></msub></mrow></msub> <mrow><mo>[</mo>
    <mi>D</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>-</mo>
    <msub><mi>ğ”¼</mi> <mrow><mi>z</mi><mo>âˆ¼</mo><msub><mi>p</mi> <mi>Z</mi></msub></mrow></msub>
    <mrow><mo>[</mo> <mi>D</mi> <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <mi>z</mi>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>)</mo></mrow></mrow></math>
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="min Underscript upper D Endscripts minus left-parenthesis double-struck
    upper E Subscript x tilde p Sub Subscript upper X Subscript Baseline left-bracket
    upper D left-parenthesis x right-parenthesis right-bracket minus double-struck
    upper E Subscript z tilde p Sub Subscript upper Z Subscript Baseline left-bracket
    upper D left-parenthesis upper G left-parenthesis z right-parenthesis right-parenthesis
    right-bracket right-parenthesis" display="block"><mrow><munder><mo movablelimits="true"
    form="prefix">min</mo> <mi>D</mi></munder> <mo>-</mo> <mrow><mo>(</mo> <msub><mi>ğ”¼</mi>
    <mrow><mi>x</mi><mo>âˆ¼</mo><msub><mi>p</mi> <mi>X</mi></msub></mrow></msub> <mrow><mo>[</mo>
    <mi>D</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>-</mo>
    <msub><mi>ğ”¼</mi> <mrow><mi>z</mi><mo>âˆ¼</mo><msub><mi>p</mi> <mi>Z</mi></msub></mrow></msub>
    <mrow><mo>[</mo> <mi>D</mi> <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <mi>z</mi>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>)</mo></mrow></mrow></math>
- en: In other words, the WGAN critic tries to maximize the difference between its
    predictions for real images and generated images.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢å¥è¯è¯´ï¼ŒWGANè¯„è®ºå®¶è¯•å›¾æœ€å¤§åŒ–å…¶å¯¹çœŸå®å›¾åƒå’Œç”Ÿæˆå›¾åƒçš„é¢„æµ‹ä¹‹é—´çš„å·®å¼‚ã€‚
- en: 'To train the WGAN generator, we calculate the loss when comparing predictions
    for generated images <math alttext="p Subscript i Baseline equals upper D left-parenthesis
    upper G left-parenthesis z Subscript i Baseline right-parenthesis right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>i</mi></msub> <mo>=</mo> <mi>D</mi> <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo>
    <msub><mi>z</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
    to the response <math alttext="y Subscript i Baseline equals 1"><mrow><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>=</mo> <mn>1</mn></mrow></math> . Therefore, for the WGAN
    generator, minimizing the loss function can be written as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è®­ç»ƒWGANç”Ÿæˆå™¨ï¼Œæˆ‘ä»¬è®¡ç®—ç”Ÿæˆå›¾åƒçš„é¢„æµ‹ä¸å“åº”ä¹‹é—´çš„æŸå¤±<math alttext="p Subscript i Baseline equals
    upper D left-parenthesis upper G left-parenthesis z Subscript i Baseline right-parenthesis
    right-parenthesis"><mrow><msub><mi>p</mi> <mi>i</mi></msub> <mo>=</mo> <mi>D</mi>
    <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <msub><mi>z</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>ä¸å“åº”<math alttext="y Subscript
    i Baseline equals 1"><mrow><msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo> <mn>1</mn></mrow></math>ã€‚å› æ­¤ï¼Œå¯¹äºWGANç”Ÿæˆå™¨ï¼Œæœ€å°åŒ–æŸå¤±å‡½æ•°å¯ä»¥å†™æˆå¦‚ä¸‹å½¢å¼ï¼š
- en: <math alttext="min Underscript upper G Endscripts minus left-parenthesis double-struck
    upper E Subscript z tilde p Sub Subscript upper Z Subscript Baseline left-bracket
    upper D left-parenthesis upper G left-parenthesis z right-parenthesis right-parenthesis
    right-bracket right-parenthesis" display="block"><mrow><munder><mo movablelimits="true"
    form="prefix">min</mo> <mi>G</mi></munder> <mo>-</mo> <mrow><mo>(</mo> <msub><mi>ğ”¼</mi>
    <mrow><mi>z</mi><mo>âˆ¼</mo><msub><mi>p</mi> <mi>Z</mi></msub></mrow></msub> <mrow><mo>[</mo>
    <mi>D</mi> <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>)</mo></mrow></mrow></math>
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="min Underscript upper G Endscripts minus left-parenthesis double-struck
    upper E Subscript z tilde p Sub Subscript upper Z Subscript Baseline left-bracket
    upper D left-parenthesis upper G left-parenthesis z right-parenthesis right-parenthesis
    right-bracket right-parenthesis" display="block"><mrow><munder><mo movablelimits="true"
    form="prefix">min</mo> <mi>G</mi></munder> <mo>-</mo> <mrow><mo>(</mo> <msub><mi>ğ”¼</mi>
    <mrow><mi>z</mi><mo>âˆ¼</mo><msub><mi>p</mi> <mi>Z</mi></msub></mrow></msub> <mrow><mo>[</mo>
    <mi>D</mi> <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>)</mo></mrow></mrow></math>
- en: In other words, the WGAN generator tries to produce images that are scored as
    highly as possible by the critic (i.e., the critic is fooled into thinking they
    are real).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢å¥è¯è¯´ï¼ŒWGANç”Ÿæˆå™¨è¯•å›¾ç”Ÿæˆè¯„è®ºå®¶å°½å¯èƒ½é«˜åˆ†çš„å›¾åƒï¼ˆå³ï¼Œè¯„è®ºå®¶è¢«æ¬ºéª—ä»¥ä¸ºå®ƒä»¬æ˜¯çœŸå®çš„ï¼‰ã€‚
- en: The Lipschitz Constraint
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ©æ™®å¸ŒèŒ¨çº¦æŸ
- en: It may surprise you that we are now allowing the critic to output any number
    in the range ( <math alttext="negative normal infinity"><mrow><mo>-</mo> <mi>âˆ</mi></mrow></math>
    , <math alttext="normal infinity"><mi>âˆ</mi></math> ), rather than applying a
    sigmoid function to restrict the output to the usual [0, 1] range. The Wasserstein
    loss can therefore be very large, which is unsettlingâ€”usually, large numbers in
    neural networks are to be avoided!
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿè®¸è®©ä½ æƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬ç°åœ¨å…è®¸è¯„è®ºå®¶è¾“å‡ºèŒƒå›´å†…çš„ä»»ä½•æ•°å­—ï¼ˆ<math alttext="negative normal infinity"><mrow><mo>-</mo>
    <mi>âˆ</mi></mrow></math>ï¼Œ<math alttext="normal infinity"><mi>âˆ</mi></math>ï¼‰ï¼Œè€Œä¸æ˜¯åº”ç”¨Sigmoidå‡½æ•°å°†è¾“å‡ºé™åˆ¶åœ¨é€šå¸¸çš„[0,1]èŒƒå›´å†…ã€‚å› æ­¤ï¼ŒWassersteinæŸå¤±å¯èƒ½éå¸¸å¤§ï¼Œè¿™ä»¤äººä¸å®‰â€”â€”é€šå¸¸æƒ…å†µä¸‹ï¼Œç¥ç»ç½‘ç»œä¸­çš„å¤§æ•°å€¼åº”è¯¥é¿å…ï¼
- en: In fact, the authors of the WGAN paper show that for the Wasserstein loss function
    to work, we also need to place an additional constraint on the critic. Specifically,
    it is required that the critic is a *1-Lipschitz continuous function*. Letâ€™s pick
    this apart to understand what it means in more detail.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: äº‹å®ä¸Šï¼ŒWGANè®ºæ–‡çš„ä½œè€…è¡¨æ˜ï¼Œä¸ºäº†ä½¿WassersteinæŸå¤±å‡½æ•°èµ·ä½œç”¨ï¼Œæˆ‘ä»¬è¿˜éœ€è¦å¯¹è¯„è®ºå®¶æ–½åŠ é¢å¤–çš„çº¦æŸã€‚å…·ä½“æ¥è¯´ï¼Œè¯„è®ºå®¶å¿…é¡»æ˜¯*1-Lipschitzè¿ç»­å‡½æ•°*ã€‚è®©æˆ‘ä»¬è¯¦ç»†è§£é‡Šä¸€ä¸‹è¿™æ„å‘³ç€ä»€ä¹ˆã€‚
- en: 'The critic is a function <math alttext="upper D"><mi>D</mi></math> that converts
    an image into a prediction. We say that this function is 1-Lipschitz if it satisfies
    the following inequality for any two input images, <math alttext="x 1"><msub><mi>x</mi>
    <mn>1</mn></msub></math> and <math alttext="x 2"><msub><mi>x</mi> <mn>2</mn></msub></math>
    :'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: è¯„è®ºå®¶æ˜¯ä¸€ä¸ªå°†å›¾åƒè½¬æ¢ä¸ºé¢„æµ‹çš„å‡½æ•°<math alttext="upper D"><mi>D</mi></math>ã€‚å¦‚æœå¯¹äºä»»æ„ä¸¤ä¸ªè¾“å…¥å›¾åƒ<math
    alttext="x 1"><msub><mi>x</mi> <mn>1</mn></msub></math>å’Œ<math alttext="x 2"><msub><mi>x</mi>
    <mn>2</mn></msub></math>ï¼Œè¯¥å‡½æ•°æ»¡è¶³ä»¥ä¸‹ä¸ç­‰å¼ï¼Œåˆ™æˆ‘ä»¬ç§°è¯¥å‡½æ•°ä¸º1-Lipschitzï¼š
- en: <math alttext="StartFraction StartAbsoluteValue upper D left-parenthesis x 1
    right-parenthesis minus upper D left-parenthesis x 2 right-parenthesis EndAbsoluteValue
    Over StartAbsoluteValue x 1 minus x 2 EndAbsoluteValue EndFraction less-than-or-equal-to
    1" display="block"><mrow><mfrac><mrow><mrow><mo>|</mo><mi>D</mi></mrow><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow><mo>-</mo><mi>D</mi><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>2</mn></msub> <mo>)</mo></mrow><mrow><mo>|</mo></mrow></mrow> <mrow><mrow><mo>|</mo></mrow><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>-</mo><msub><mi>x</mi> <mn>2</mn></msub> <mrow><mo>|</mo></mrow></mrow></mfrac>
    <mo>â‰¤</mo> <mn>1</mn></mrow></math>
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartFraction StartAbsoluteValue upper D left-parenthesis x 1
    right-parenthesis minus upper D left-parenthesis x 2 right-parenthesis EndAbsoluteValue
    Over StartAbsoluteValue x 1 minus x 2 EndAbsoluteValue EndFraction less-than-or-equal-to
    1" display="block"><mrow><mfrac><mrow><mrow><mo>|</mo><mi>D</mi></mrow><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow><mo>-</mo><mi>D</mi><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>2</mn></msub> <mo>)</mo></mrow><mrow><mo>|</mo></mrow></mrow> <mrow><mrow><mo>|</mo></mrow><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>-</mo><msub><mi>x</mi> <mn>2</mn></msub> <mrow><mo>|</mo></mrow></mrow></mfrac>
    <mo>â‰¤</mo> <mn>1</mn></mrow></math>
- en: Here, <math alttext="StartAbsoluteValue x 1 minus x 2 EndAbsoluteValue"><mrow><mrow><mo>|</mo></mrow>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>-</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mrow><mo>|</mo></mrow></mrow></math> is the average pixelwise absolute difference
    between two images and <math alttext="StartAbsoluteValue upper D left-parenthesis
    x 1 right-parenthesis minus upper D left-parenthesis x 2 right-parenthesis EndAbsoluteValue"><mrow><mrow><mo>|</mo>
    <mi>D</mi></mrow> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>)</mo></mrow>
    <mo>-</mo> <mi>D</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>)</mo></mrow>
    <mrow><mo>|</mo></mrow></mrow></math> is the absolute difference between the critic
    predictions. Essentially, we require a limit on the rate at which the predictions
    of the critic can change between two images (i.e., the absolute value of the gradient
    must be at most 1 everywhere). We can see this applied to a Lipschitz continuous
    1D function in [FigureÂ 4-11](#lipschitz)â€”at no point does the line enter the cone,
    wherever you place the cone on the line. In other words, there is a limit on the
    rate at which the line can rise or fall at any point.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œ<math alttext="StartAbsoluteValue x 1 minus x 2 EndAbsoluteValue"><mrow><mrow><mo>|</mo></mrow>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>-</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mrow><mo>|</mo></mrow></mrow></math>æ˜¯ä¸¤ä¸ªå›¾åƒä¹‹é—´çš„å¹³å‡åƒç´ ç»å¯¹å·®å¼‚ï¼Œ<math alttext="StartAbsoluteValue
    upper D left-parenthesis x 1 right-parenthesis minus upper D left-parenthesis
    x 2 right-parenthesis EndAbsoluteValue"><mrow><mrow><mo>|</mo> <mi>D</mi></mrow>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>)</mo></mrow> <mo>-</mo>
    <mi>D</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>)</mo></mrow>
    <mrow><mo>|</mo></mrow></mrow></math>æ˜¯è¯„è®ºå®¶é¢„æµ‹ä¹‹é—´çš„ç»å¯¹å·®å¼‚ã€‚åŸºæœ¬ä¸Šï¼Œæˆ‘ä»¬è¦æ±‚è¯„è®ºå®¶çš„é¢„æµ‹åœ¨ä¸¤ä¸ªå›¾åƒä¹‹é—´å˜åŒ–çš„é€Ÿç‡æœ‰é™ï¼ˆå³ï¼Œæ¢¯åº¦çš„ç»å¯¹å€¼å¿…é¡»åœ¨ä»»ä½•åœ°æ–¹æœ€å¤šä¸º1ï¼‰ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™åº”ç”¨äºåˆ©æ™®å¸ŒèŒ¨è¿ç»­çš„ä¸€ç»´å‡½æ•°ä¸­ï¼Œå¦‚[å›¾4-11](#lipschitz)æ‰€ç¤ºâ€”â€”åœ¨ä»»ä½•ä½ç½®æ”¾ç½®é”¥ä½“æ—¶ï¼Œçº¿éƒ½ä¸ä¼šè¿›å…¥é”¥ä½“ã€‚æ¢å¥è¯è¯´ï¼Œçº¿åœ¨ä»»ä½•ç‚¹ä¸Šå‡æˆ–ä¸‹é™çš„é€Ÿç‡éƒ½æœ‰é™åˆ¶ã€‚
- en: '![](Images/gdl2_0411.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0411.png)'
- en: 'Figure 4-11\. A Lipschitz continuous function (source: [Wikipedia](https://oreil.ly/Ki7ds))'
  id: totrans-214
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾4-11ã€‚åˆ©æ™®å¸ŒèŒ¨è¿ç»­å‡½æ•°ï¼ˆæ¥æºï¼š[ç»´åŸºç™¾ç§‘](https://oreil.ly/Ki7ds)ï¼‰
- en: Tip
  id: totrans-215
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æç¤º
- en: For those who want to delve deeper into the mathematical rationale behind why
    the Wasserstein loss only works when this constraint is enforced, Jonathan Hui
    offers [an excellent explanation](https://oreil.ly/devy5).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºé‚£äº›æƒ³æ·±å…¥äº†è§£ä¸ºä»€ä¹ˆåªæœ‰åœ¨å¼ºåˆ¶æ‰§è¡Œè¿™ä¸ªçº¦æŸæ—¶ï¼ŒWassersteinæŸå¤±æ‰æœ‰æ•ˆçš„æ•°å­¦åŸç†çš„äººï¼ŒJonathan Huiæä¾›äº†[ä¸€ä¸ªä¼˜ç§€çš„è§£é‡Š](https://oreil.ly/devy5)ã€‚
- en: Enforcing the Lipschitz Constraint
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¼ºåˆ¶åˆ©æ™®å¸ŒèŒ¨çº¦æŸ
- en: In the original WGAN paper, the authors show how it is possible to enforce the
    Lipschitz constraint by clipping the weights of the critic to lie within a small
    range, [â€“0.01, 0.01], after each training batch.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åŸå§‹çš„WGANè®ºæ–‡ä¸­ï¼Œä½œè€…å±•ç¤ºäº†é€šè¿‡åœ¨æ¯ä¸ªè®­ç»ƒæ‰¹æ¬¡åå°†è¯„è®ºå®¶çš„æƒé‡å‰ªè¾‘åˆ°ä¸€ä¸ªå°èŒƒå›´[-0.01, 0.01]å†…æ¥å¼ºåˆ¶æ‰§è¡Œåˆ©æ™®å¸ŒèŒ¨çº¦æŸçš„å¯èƒ½æ€§ã€‚
- en: One of the criticisms of this approach is that the capacity of the critic to
    learn is greatly diminished, since we are clipping its weights. In fact, even
    in the original WGAN paper the authors write, â€œWeight clipping is a clearly terrible
    way to enforce a Lipschitz constraint.â€ A strong critic is pivotal to the success
    of a WGAN, since without accurate gradients, the generator cannot learn how to
    adapt its weights to produce better samples.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•çš„æ‰¹è¯„ä¹‹ä¸€æ˜¯ï¼Œè¯„è®ºå®¶å­¦ä¹ çš„èƒ½åŠ›å¤§å¤§é™ä½ï¼Œå› ä¸ºæˆ‘ä»¬æ­£åœ¨å‰ªè¾‘å®ƒçš„æƒé‡ã€‚äº‹å®ä¸Šï¼Œå³ä½¿åœ¨åŸå§‹çš„WGANè®ºæ–‡ä¸­ï¼Œä½œè€…ä»¬ä¹Ÿå†™é“ï¼Œâ€œæƒé‡å‰ªè¾‘æ˜¾ç„¶æ˜¯ä¸€ç§å¼ºåˆ¶åˆ©æ™®å¸ŒèŒ¨çº¦æŸçš„å¯æ€•æ–¹å¼ã€‚â€å¼ºå¤§çš„è¯„è®ºå®¶å¯¹äºWGANçš„æˆåŠŸè‡³å…³é‡è¦ï¼Œå› ä¸ºæ²¡æœ‰å‡†ç¡®çš„æ¢¯åº¦ï¼Œç”Ÿæˆå™¨æ— æ³•å­¦ä¹ å¦‚ä½•è°ƒæ•´å…¶æƒé‡ä»¥ç”Ÿæˆæ›´å¥½çš„æ ·æœ¬ã€‚
- en: Therefore, other researchers have looked for alternative ways to enforce the
    Lipschitz constraint and improve the capacity of the WGAN to learn complex features.
    One such method is the Wasserstein GAN with Gradient Penalty.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå…¶ä»–ç ”ç©¶äººå‘˜å¯»æ‰¾äº†å…¶ä»–æ–¹æ³•æ¥å¼ºåˆ¶æ‰§è¡Œåˆ©æ™®å¸ŒèŒ¨çº¦æŸï¼Œå¹¶æé«˜WGANå­¦ä¹ å¤æ‚ç‰¹å¾çš„èƒ½åŠ›ã€‚å…¶ä¸­ä¸€ç§æ–¹æ³•æ˜¯å¸¦æœ‰æ¢¯åº¦æƒ©ç½šçš„Wasserstein GANã€‚
- en: In the paper introducing this variant,^([5](ch04.xhtml#idm45387019298976)) the
    authors show how the Lipschitz constraint can be enforced directly by including
    a *gradient penalty* term in the loss function for the critic that penalizes the
    model if the gradient norm deviates from 1\. This results in a far more stable
    training process.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼•å…¥è¿™ç§å˜ä½“çš„è®ºæ–‡ä¸­ï¼Œä½œè€…å±•ç¤ºäº†å¦‚ä½•é€šè¿‡åœ¨è¯„è®ºå®¶çš„æŸå¤±å‡½æ•°ä¸­ç›´æ¥åŒ…å«*æ¢¯åº¦æƒ©ç½š*é¡¹æ¥å¼ºåˆ¶æ‰§è¡Œåˆ©æ™®å¸ŒèŒ¨çº¦æŸï¼Œå¦‚æœæ¢¯åº¦èŒƒæ•°åç¦»1ï¼Œæ¨¡å‹å°†å—åˆ°æƒ©ç½šã€‚è¿™å¯¼è‡´äº†ä¸€ä¸ªæ›´åŠ ç¨³å®šçš„è®­ç»ƒè¿‡ç¨‹ã€‚
- en: In the next section, weâ€™ll see how to build this extra term into the loss function
    for our critic.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•å°†è¿™ä¸ªé¢å¤–é¡¹æ„å»ºåˆ°æˆ‘ä»¬è¯„è®ºå®¶çš„æŸå¤±å‡½æ•°ä¸­ã€‚
- en: The Gradient Penalty Loss
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¢¯åº¦æƒ©ç½šæŸå¤±
- en: '[FigureÂ 4-12](#wgangp_critic) is a diagram of the training process for the
    critic of a WGAN-GP. If we compare this to the original discriminator training
    process from [FigureÂ 4-5](#gan_bricks_training), we can see that the key addition
    is the gradient penalty loss included as part of the overall loss function, alongside
    the Wasserstein loss from the real and fake images.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '[å›¾4-12](#wgangp_critic)æ˜¯WGAN-GPè¯„è®ºå®¶çš„è®­ç»ƒè¿‡ç¨‹çš„å›¾è¡¨ã€‚å¦‚æœæˆ‘ä»¬å°†å…¶ä¸[å›¾4-5](#gan_bricks_training)ä¸­åŸå§‹é‰´åˆ«å™¨è®­ç»ƒè¿‡ç¨‹è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°çš„å…³é”®æ·»åŠ æ˜¯æ¢¯åº¦æƒ©ç½šæŸå¤±ä½œä¸ºæ•´ä½“æŸå¤±å‡½æ•°çš„ä¸€éƒ¨åˆ†ï¼Œä¸çœŸå®å’Œè™šå‡å›¾åƒçš„WassersteinæŸå¤±ä¸€èµ·ã€‚'
- en: '![](Images/gdl2_0412.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0412.png)'
- en: Figure 4-12\. The WGAN-GP critic training process
  id: totrans-226
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾4-12ã€‚WGAN-GPè¯„è®ºå®¶è®­ç»ƒè¿‡ç¨‹
- en: The gradient penalty loss measures the squared difference between the norm of
    the gradient of the predictions with respect to the input images and 1\. The model
    will naturally be inclined to find weights that ensure the gradient penalty term
    is minimized, thereby encouraging the model to conform to the Lipschitz constraint.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦æƒ©ç½šæŸå¤±è¡¡é‡äº†é¢„æµ‹æ¢¯åº¦çš„èŒƒæ•°ä¸è¾“å…¥å›¾åƒä¹‹é—´çš„å¹³æ–¹å·®å¼‚å’Œ1ä¹‹é—´çš„å·®å¼‚ã€‚æ¨¡å‹è‡ªç„¶å€¾å‘äºæ‰¾åˆ°ç¡®ä¿æ¢¯åº¦æƒ©ç½šé¡¹æœ€å°åŒ–çš„æƒé‡ï¼Œä»è€Œé¼“åŠ±æ¨¡å‹ç¬¦åˆåˆ©æ™®å¸ŒèŒ¨çº¦æŸã€‚
- en: It is intractable to calculate this gradient everywhere during the training
    process, so instead the WGAN-GP evaluates the gradient at only a handful of points.
    To ensure a balanced mix, we use a set of interpolated images that lie at randomly
    chosen points along lines connecting the batch of real images to the batch of
    fake images pairwise, as shown in [FigureÂ 4-13](#wgangp_randomweighting).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0413.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
- en: Figure 4-13\. Interpolating between images
  id: totrans-230
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In [ExampleÂ 4-8](#gradient-penalty-loss-ex), we show how the gradient penalty
    is calculated in code.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-8\. The gradient penalty loss function
  id: totrans-232
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[![1](Images/1.png)](#co_generative_adversarial_networks_CO4-1)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Each image in the batch gets a random number, between 0 and 1, stored as the
    vector `alpha`.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_generative_adversarial_networks_CO4-2)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: A set of interpolated images is calculated.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_generative_adversarial_networks_CO4-3)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: The critic is asked to score each of these interpolated images.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_generative_adversarial_networks_CO4-4)'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: The gradient of the predictions is calculated with respect to the input images.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_generative_adversarial_networks_CO4-5)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: The L2 norm of this vector is calculated.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](Images/6.png)](#co_generative_adversarial_networks_CO4-6)'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: The function returns the average squared distance between the L2 norm and 1.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Training the WGAN-GP
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A key benefit of using the Wasserstein loss function is that we no longer need
    to worry about balancing the training of the critic and the generatorâ€”in fact,
    when using the Wasserstein loss, the critic must be trained to convergence before
    updating the generator, to ensure that the gradients for the generator update
    are accurate. This is in contrast to a standard GAN, where it is important not
    to let the discriminator get too strong.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, with Wasserstein GANs, we can simply train the critic several times
    between generator updates, to ensure it is close to convergence. A typical ratio
    used is three to five critic updates per generator update.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: We have now introduced both of the key concepts behind the WGAN-GPâ€”the Wasserstein
    loss and the gradient penalty term that is included in the critic loss function.
    The training step of the WGAN model that incorporates all of these ideas is shown
    in [ExampleÂ 4-9](#training-wgan-ex).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-9\. Training the WGAN-GP
  id: totrans-250
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[![1](Images/1.png)](#co_generative_adversarial_networks_CO5-1)'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Perform three critic updates.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_generative_adversarial_networks_CO5-2)'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the Wasserstein loss for the criticâ€”the difference between the average
    prediction for the fake images and the real images.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_generative_adversarial_networks_CO5-3)'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the gradient penalty term (see [ExampleÂ 4-8](#gradient-penalty-loss-ex)).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_generative_adversarial_networks_CO5-4)'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: The critic loss function is a weighted sum of the Wasserstein loss and the gradient
    penalty.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_generative_adversarial_networks_CO5-5)'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Update the weights of the critic.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](Images/6.png)](#co_generative_adversarial_networks_CO5-6)'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the Wasserstein loss for the generator.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](Images/7.png)](#co_generative_adversarial_networks_CO5-7)'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Update the weights of the generator.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Batch Normalization in a WGAN-GP
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One last consideration we should note before training a WGAN-GP is that batch
    normalization shouldnâ€™t be used in the critic. This is because batch normalization
    creates correlation between images in the same batch, which makes the gradient
    penalty loss less effective. Experiments have shown that WGAN-GPs can still produce
    excellent results even without batch normalization in the critic.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'We have now covered all of the key differences between a standard GAN and a
    WGAN-GP. To recap:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: A WGAN-GP uses the Wasserstein loss.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The WGAN-GP is trained using labels of 1 for real and â€“1 for fake.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no sigmoid activation in the final layer of the critic.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Include a gradient penalty term in the loss function for the critic.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train the critic multiple times for each update of the generator.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are no batch normalization layers in the critic.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analysis of the WGAN-GP
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Letâ€™s take a look at some example outputs from the generator, after 25 epochs
    of training ([FigureÂ 4-14](#wgangp_examples)).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0414.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
- en: Figure 4-14\. WGAN-GP face examples
  id: totrans-278
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The model has learned the significant high-level attributes of a face, and there
    is no sign of mode collapse.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: We can also see how the loss functions of the model evolve over time ([FigureÂ 4-15](#wgangp_losses))â€”the
    loss functions of both the critic and generator are highly stable and convergent.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: If we compare the WGAN-GP output to the VAE output from the previous chapter,
    we can see that the GAN images are generally sharperâ€”especially the definition
    between the hair and the background. This is true in general; VAEs tend to produce
    softer images that blur color boundaries, whereas GANs are known to produce sharper,
    more well-defined images.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0415.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-15\. WGAN-GP loss curves: the critic loss (`epoch_c_loss`) is broken
    down into the Wasserstein loss (`epoch_c_wass`) and the gradient penalty loss
    (`epoch_c_gp`)'
  id: totrans-283
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is also true that GANs are generally more difficult to train than VAEs and
    take longer to reach a satisfactory quality. However, many state-of-the-art generative
    models today are GAN-based, as the rewards for training large-scale GANs on GPUs
    over a longer period of time are significant.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Conditional GAN (CGAN)
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in this chapter, we have built GANs that are able to generate realistic
    images from a given training set. However, we havenâ€™t been able to control the
    type of image we would like to generateâ€”for example, a male or female face, or
    a large or small brick. We can sample a random point from the latent space, but
    we do not have the ability to easily understand what kind of image will be produced
    given the choice of latent variable.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: In the final part of this chapter we shall turn our attention to building a
    GAN where we are able to control the outputâ€”a so called *conditional GAN*. This
    idea, first introduced in â€œConditional Generative Adversarial Netsâ€ by Mirza and
    Osindero in 2014,^([6](ch04.xhtml#idm45387018921312)) is a relatively simple extension
    to the GAN architecture.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Running the Code for This Example
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for this example can be found in the Jupyter notebook located at *notebooks/04_gan/03_cgan/cgan.ipynb*
    in the book repository.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: The code has been adapted from the excellent [CGAN tutorial](https://oreil.ly/Ey11I)
    created by Sayak Paul, available on the Keras website.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: CGAN Architecture
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this example, we will condition our CGAN on the *blond hair* attribute of
    the faces dataset. That is, we will be able to explicitly specify whether we want
    to generate an image with blond hair or not. This label is provided as part of
    the CelebA dataset.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: The high-level CGAN architecture is shown in [FigureÂ 4-16](#cgan_architecture).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0416.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
- en: Figure 4-16\. Inputs and outputs of the generator and critic in a CGAN
  id: totrans-295
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The key difference between a standard GAN and a CGAN is that in a CGAN we pass
    in extra information to the generator and critic relating to the label. In the
    generator, this is simply appended to the latent space sample as a one-hot encoded
    vector. In the critic, we add the label information as extra channels to the RGB
    image. We do this by repeating the one-hot encoded vector to fill the same shape
    as the input images.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: CGANs work because the critic now has access to extra information regarding
    the content of the image, so the generator must ensure that its output agrees
    with the provided label, in order to keep fooling the critic. If the generator
    produced perfect images that disagreed with the image label the critic would be
    able to tell that they were fake simply because the images and labels did not
    match.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-298
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In our example, our one-hot encoded label will have length 2, because there
    are two classes (Blonde and Not Blond). However, you can have as many labels as
    you likeâ€”for example, you could train a CGAN on the Fashion-MNIST dataset to output
    one of the 10 different fashion items, by incorporating a one-hot encoded label
    vector of length 10 into the input of the generator and 10 additional one-hot
    encoded label channels into the input of the critic.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: The only change we need to make to the architecture is to concatenate the label
    information to the existing inputs of the generator and the critic, as shown in
    [ExampleÂ 4-10](#cgan_input_shapes).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-10\. Input layers in the CGAN
  id: totrans-301
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[![1](Images/1.png)](#co_generative_adversarial_networks_CO6-1)'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: The image channels and label channels are passed in separately to the critic
    and concatenated.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_generative_adversarial_networks_CO6-2)'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: The latent vector and the label classes are passed in separately to the generator
    and concatenated before being reshaped.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: Training the CGAN
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We must also make some changes to the `train_step` of the CGAN to match the
    new input formats of the generator and critic, as shown in [ExampleÂ 4-11](#cgan_train_step).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-11\. The `train_step` of the CGAN
  id: totrans-309
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[![1](Images/1.png)](#co_generative_adversarial_networks_CO7-1)'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: The images and labels are unpacked from the input data.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_generative_adversarial_networks_CO7-2)'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: The one-hot encoded vectors are expanded to one-hot encoded images that have
    the same spatial size as the input images (64 Ã— 64).
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_generative_adversarial_networks_CO7-3)'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: The generator is now fed with a list of two inputsâ€”the random latent vectors
    and the one-hot encoded label vectors.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_generative_adversarial_networks_CO7-4)'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: The critic is now fed with a list of two inputsâ€”the fake/real images and the
    one-hot encoded label channels.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_generative_adversarial_networks_CO7-5)'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: The gradient penalty function also requires the one-hot encoded label channels
    to be passed through as it uses the critic.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](Images/6.png)](#co_generative_adversarial_networks_CO7-6)'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: The changes made to the critic training step also apply to the generator training
    step.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: Analysis of the CGAN
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can control the CGAN output by passing a particular one-hot encoded label
    into the input of the generator. For example, to generate a face with nonblond
    hair, we pass in the vector `[1, 0]`. To generate a face with blond hair, we pass
    in the vector `[0, 1]`.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: The output from the CGAN can be seen in [FigureÂ 4-17](#cgan_output). Here, we
    keep the random latent vectors the same across the examples and change only the
    conditional label vector. It is clear that the CGAN has learned to use the label
    vector to control only the hair color attribute of the images. It is impressive
    that the rest of the image barely changesâ€”this is proof that GANs are able to
    organize points in the latent space in such a way that individual features can
    be decoupled from each other.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0417.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
- en: Figure 4-17\. Output from the CGAN when the *Blond* and *Not Blond* vectors
    are appended to the latent sample
  id: totrans-327
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  id: totrans-328
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If labels are available for your dataset, it is generally a good idea to include
    them as input to your GAN even if you do not necessarily need to condition the
    generated output on the label, as they tend to improve the quality of images generated.
    You can think of the labels as just a highly informative extension to the pixel
    input.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-330
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter we explored three different generative adversarial network
    (GAN) models: the deep convolutional GAN (DCGAN), the more sophisticated Wasserstein
    GAN with Gradient Penalty (WGAN-GP), and the conditional GAN (CGAN).'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: All GANs are characterized by a generator versus discriminator (or critic) architecture,
    with the discriminator trying to â€œspot the differenceâ€ between real and fake images
    and the generator aiming to fool the discriminator. By balancing how these two
    adversaries are trained, the GAN generator can gradually learn how to produce
    similar observations to those in the training set.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: We first saw how to train a DCGAN to generate images of toy bricks. It was able
    to learn how to realistically represent 3D objects as images, including accurate
    representations of shadow, shape, and texture. We also explored the different
    ways in which GAN training can fail, including mode collapse and vanishing gradients.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: We then explored how the Wasserstein loss function remedied many of these problems
    and made GAN training more predictable and reliable. The WGAN-GP places the 1-Lipschitz
    requirement at the heart of the training process by including a term in the loss
    function to pull the gradient norm toward 1.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: We applied the WGAN-GP to the problem of face generation and saw how by simply
    choosing points from a standard normal distribution, we can generate new faces.
    This sampling process is very similar to a VAE, though the faces produced by a
    GAN are quite differentâ€”often sharper, with greater distinction between different
    parts of the image.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we built a CGAN that allowed us to control the type of image that is
    generated. This works by passing in the label as input to the critic and generator,
    thereby giving the network the additional information it needs in order to condition
    the generated output on a given label.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: Overall, we have seen how the GAN framework is extremely flexible and able to
    be adapted to many interesting problem domains. In particular, GANs have driven
    significant progress in the field of image generation with many interesting extensions
    to the underlying framework, as we shall see in [ChapterÂ 10](ch10.xhtml#chapter_image_generation).
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore a different family of generative model
    that is ideal for modeling sequential dataâ€”autoregressive models.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch04.xhtml#idm45387021611344-marker)) Ian J. Goodfellow et al., â€œGenerative
    Adversarial Nets,â€ June 10, 2014, [*https://arxiv.org/abs/1406.2661*](https://arxiv.org/abs/1406.2661)
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch04.xhtml#idm45387021585984-marker)) Alec Radford et al., â€œUnsupervised
    Representation Learning with Deep Convolutional Generative Adversarial Networks,â€
    January 7, 2016, [*https://arxiv.org/abs/1511.06434*](https://arxiv.org/abs/1511.06434).
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch04.xhtml#idm45387021101472-marker)) Augustus Odena et al., â€œDeconvolution
    and Checkerboard Artifacts,â€ October 17, 2016, [*https://distill.pub/2016/deconv-checkerboard*](https://distill.pub/2016/deconv-checkerboard).
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch04.xhtml#idm45387021016704-marker)) Martin Arjovsky et al., â€œWasserstein
    GAN,â€ January 26, 2017, [*https://arxiv.org/abs/1701.07875*](https://arxiv.org/abs/1701.07875).
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch04.xhtml#idm45387019298976-marker)) Ishaan Gulrajani et al., â€œImproved
    Training of Wasserstein GANs,â€ March 31, 2017, [*https://arxiv.org/abs/1704.00028*](https://arxiv.org/abs/1704.00028).
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch04.xhtml#idm45387018921312-marker)) Mehdi Mirza and Simon Osindero,
    â€œConditional Generative Adversarial Nets,â€ November 6, 2014, [*https://arxiv.org/abs/1411.1784*](https://arxiv.org/abs/1411.1784).`
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
