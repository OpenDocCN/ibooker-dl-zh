- en: Chapter 4\. Generative Adversarial Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 2014, Ian Goodfellow et al. presented a paper entitled “Generative Adversarial
    Nets”^([1](ch04.xhtml#idm45387021611344)) at the Neural Information Processing
    Systems conference (NeurIPS) in Montreal. The introduction of generative adversarial
    networks (or GANs, as they are more commonly known) is now regarded as a key turning
    point in the history of generative modeling, as the core ideas presented in this
    paper have spawned some of the most successful and impressive generative models
    ever created.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will first lay out the theoretical underpinning of GANs, then we
    will see how to build our own GAN using Keras.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s start with a short story to illustrate some of the fundamental concepts
    used in the GAN training process.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: The story of Brickki bricks and the forgers describes the training process of
    a generative adversarial network.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: A GAN is a battle between two adversaries, the *generator* and the *discriminator*.
    The generator tries to convert random noise into observations that look as if
    they have been sampled from the original dataset, and the discriminator tries
    to predict whether an observation comes from the original dataset or is one of
    the generator’s forgeries. Examples of the inputs and outputs to the two networks
    are shown in [Figure 4-2](#gan_diagram_simple).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0402.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
- en: Figure 4-2\. Inputs and outputs of the two networks in a GAN
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: At the start of the process, the generator outputs noisy images and the discriminator
    predicts randomly. The key to GANs lies in how we alternate the training of the
    two networks, so that as the generator becomes more adept at fooling the discriminator,
    the discriminator must adapt in order to maintain its ability to correctly identify
    which observations are fake. This drives the generator to find new ways to fool
    the discriminator, and so the cycle continues.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Deep Convolutional GAN (DCGAN)
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To see this in action, let’s start building our first GAN in Keras, to generate
    pictures of bricks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: We will be closely following one of the first major papers on GANs, “Unsupervised
    Representation Learning with Deep Convolutional Generative Adversarial Networks.”^([2](ch04.xhtml#idm45387021585984))
    In this 2015 paper, the authors show how to build a deep convolutional GAN to
    generate realistic images from a variety of datasets. They also introduce several
    changes that significantly improve the quality of the generated images.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Running the Code for This Example
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for this example can be found in the Jupyter notebook located at *notebooks/04_gan/01_dcgan/dcgan.ipynb*
    in the book repository.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: The Bricks Dataset
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, you’ll need to download the training data. We’ll be using the [Images
    of LEGO Bricks dataset](https://oreil.ly/3vp9f) that is available through Kaggle.
    This is a computer-rendered collection of 40,000 photographic images of 50 different
    toy bricks, taken from multiple angles. Some example images of Brickki products
    are shown in [Figure 4-3](Images/#gan_bricks_images).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0403.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: Figure 4-3\. Examples of images from the Bricks dataset
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can download the dataset by running the Kaggle dataset downloader script
    in the book repository, as shown in [Example 4-1](#downloading-lego-dataset).
    This will save the images and accompanying metadata locally to the */data* folder.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-1\. Downloading the Bricks dataset
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`We use the Keras function `image_dataset_from_directory` to create a TensorFlow
    Dataset pointed at the directory where the images are stored, as shown in [Example 4-2](#preprocessing-lego-data).
    This allows us to read batches of images into memory only when required (e.g.,
    during training), so that we can work with large datasets and not worry about
    having to fit the entire dataset into memory. It also resizes the images to 64
    × 64, interpolating between pixel values.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-2\. Creating a TensorFlow Dataset from image files in a directory
  id: totrans-23
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The original data is scaled in the range [0, 255] to denote the pixel intensity.
    When training GANs we rescale the data to the range [–1, 1] so that we can use
    the tanh activation function on the final layer of the generator, which tends
    to provide stronger gradients than the sigmoid function ([Example 4-3](#preprocessing-lego-data_2)).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-3\. Preprocessing the Bricks dataset
  id: totrans-26
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let’s now take a look at how we build the discriminator.`  `## The Discriminator
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of the discriminator is to predict if an image is real or fake. This
    is a supervised image classification problem, so we can use a similar architecture
    to those we worked with in [Chapter 2](ch02.xhtml#chapter_deep_learning): stacked
    convolutional layers, with a single output node.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: The full architecture of the discriminator we will be building is shown in [Table 4-1](#gan_bricks_discriminator).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-1\. Model summary of the discriminator
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '| Layer (type) | Output shape | Param # |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
- en: '| InputLayer | (None, 64, 64, 1) | 0 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
- en: '| Conv2D | (None, 32, 32, 64) | 1,024 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
- en: '| LeakyReLU | (None, 32, 32, 64) | 0 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
- en: '| Dropout | (None, 32, 32, 64) | 0 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
- en: '| Conv2D | (None, 16, 16, 128) | 131,072 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
- en: '| BatchNormalization | (None, 16, 16, 128) | 512 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
- en: '| LeakyReLU | (None, 16, 16, 128) | 0 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
- en: '| Dropout | (None, 16, 16, 128) | 0 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
- en: '| Conv2D | (None, 8, 8, 256) | 524,288 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
- en: '| BatchNormalization | (None, 8, 8, 256) | 1,024 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
- en: '| LeakyReLU | (None, 8, 8, 256) | 0 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
- en: '| Dropout | (None, 8, 8, 256) | 0 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
- en: '| Conv2D | (None, 4, 4, 512) | 2,097,152 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
- en: '| BatchNormalization | (None, 4, 4, 512) | 2,048 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
- en: '| LeakyReLU | (None, 4, 4, 512) | 0 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
- en: '| Dropout | (None, 4, 4, 512) | 0 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
- en: '| Conv2D | (None, 1, 1, 1) | 8,192 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
- en: '| Flatten | (None, 1) | 0 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
- en: '| Total params | 2,765,312 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
- en: '| Trainable params | 2,763,520 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
- en: '| Non-trainable params | 1,792 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
- en: The Keras code to build the discriminator is provided in [Example 4-4](#the-discriminator-ex).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-4\. The discriminator
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[![1](Images/1.png)](#co_generative_adversarial_networks_CO1-1)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Define the `Input` layer of the discriminator (the image).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_generative_adversarial_networks_CO1-2)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Stack `Conv2D` layers on top of each other, with `BatchNormalization`, `LeakyReLU`
    activation, and `Dropout` layers sandwiched in between.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_generative_adversarial_networks_CO1-3)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Flatten the last convolutional layer—by this point, the shape of the tensor
    is 1 × 1 × 1, so there is no need for a final `Dense` layer.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_generative_adversarial_networks_CO1-4)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: The Keras model that defines the discriminator—a model that takes an input image
    and outputs a single number between 0 and 1.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Notice how we use a stride of 2 in some of the `Conv2D` layers to reduce the
    spatial shape of the tensor as it passes through the network (64 in the original
    image, then 32, 16, 8, 4, and finally 1), while increasing the number of channels
    (1 in the grayscale input image, then 64, 128, 256, and finally 512), before collapsing
    to a single prediction.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: We use a sigmoid activation on the final `Conv2D` layer to output a number between
    0 and 1.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: The Generator
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now let’s build the generator. The input to the generator will be a vector drawn
    from a multivariate standard normal distribution. The output is an image of the
    same size as an image in the original training data.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'This description may remind you of the decoder in a variational autoencoder.
    In fact, the generator of a GAN fulfills exactly the same purpose as the decoder
    of a VAE: converting a vector in the latent space to an image. The concept of
    mapping from a latent space back to the original domain is very common in generative
    modeling, as it gives us the ability to manipulate vectors in the latent space
    to change high-level features of images in the original domain.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of the generator we will be building is shown in [Table 4-2](#gan_bricks_generator).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-2\. Model summary of the generator
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '| Layer (type) | Output shape | Param # |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
- en: '| InputLayer | (None, 100) | 0 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: InputLayer（无，100）0
- en: '| Reshape | (None, 1, 1, 100) | 0 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: Reshape（无，1，1，100）0
- en: '| Conv2DTranspose | (None, 4, 4, 512) | 819,200 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: Conv2DTranspose（无，4，4，512）819,200
- en: '| BatchNormalization | (None, 4, 4, 512) | 2,048 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: BatchNormalization（无，4，4，512）2,048
- en: '| ReLU | (None, 4, 4, 512) | 0 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: ReLU（无，4，4，512）0
- en: '| Conv2DTranspose | (None, 8, 8, 256) | 2,097,152 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: Conv2DTranspose（无，8，8，256）2,097,152
- en: '| BatchNormalization | (None, 8, 8, 256) | 1,024 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: BatchNormalization（无，8，8，256）1,024
- en: '| ReLU | (None, 8, 8, 256) | 0 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: ReLU（无，8，8，256）0
- en: '| Conv2DTranspose | (None, 16, 16, 128) | 524,288 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: Conv2DTranspose（无，16，16，128）524,288
- en: '| BatchNormalization | (None, 16, 16, 128) | 512 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: BatchNormalization（无，16，16，128）512
- en: '| ReLU | (None, 16, 16, 128) | 0 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: ReLU（无，16，16，128）0
- en: '| Conv2DTranspose | (None, 32, 32, 64) | 131,072 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: Conv2DTranspose（无，32，32，64）131,072
- en: '| BatchNormalization | (None, 32, 32, 64) | 256 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: BatchNormalization（无，32，32，64）256
- en: '| ReLU | (None, 32, 32, 64) | 0 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: ReLU（无，32，32，64）0
- en: '| Conv2DTranspose | (None, 64, 64, 1) | 1,024 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: Conv2DTranspose（无，64，64，1）1,024
- en: '| Total params | 3,576,576 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: 总参数3,576,576
- en: '| Trainable params | 3,574,656 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: 可训练参数3,574,656
- en: '| Non-trainable params | 1,920 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: 不可训练参数1,920
- en: The code for building the generator is given in [Example 4-5](#the-generator-ex).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 构建生成器的代码在[示例4-5](#the-generator-ex)中给出。
- en: Example 4-5\. The generator
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例4-5。生成器
- en: '[PRE4]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](Images/1.png)](#co_generative_adversarial_networks_CO2-1)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_generative_adversarial_networks_CO2-1)'
- en: Define the `Input` layer of the generator—a vector of length 100.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 定义生成器的“Input”层-长度为100的向量。
- en: '[![2](Images/2.png)](#co_generative_adversarial_networks_CO2-2)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_generative_adversarial_networks_CO2-2)'
- en: We use a `Reshape` layer to give a 1 × 1 × 100 tensor, so that we can start
    applying convolutional transpose operations.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个“Reshape”层来给出一个1×1×100的张量，这样我们就可以开始应用卷积转置操作。
- en: '[![3](Images/3.png)](#co_generative_adversarial_networks_CO2-3)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_generative_adversarial_networks_CO2-3)'
- en: We pass this through four `Conv2DTranspose` layers, with `BatchNormalization`
    and `LeakyReLU` layers sandwiched in between.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过四个“Conv2DTranspose”层传递这些数据，其中夹在中间的是“BatchNormalization”和“LeakyReLU”层。
- en: '[![4](Images/4.png)](#co_generative_adversarial_networks_CO2-4)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_generative_adversarial_networks_CO2-4)'
- en: The final `Conv2DTranspose` layer uses a tanh activation function to transform
    the output to the range [–1, 1], to match the original image domain.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的“Conv2DTranspose”层使用tanh激活函数将输出转换为范围[-1,1]，以匹配原始图像域。
- en: '[![5](Images/5.png)](#co_generative_adversarial_networks_CO2-5)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_generative_adversarial_networks_CO2-5)'
- en: The Keras model that defines the generator—a model that accepts a vector of
    length 100 and outputs a tensor of shape `[64, 64, 1]`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 定义生成器的Keras模型-接受长度为100的向量并输出形状为`[64，64，1]`的张量。
- en: Notice how we use a stride of 2 in some of the `Conv2DTranspose` layers to increase
    the spatial shape of the tensor as it passes through the network (1 in the original
    vector, then 4, 8, 16, 32, and finally 64), while decreasing the number of channels
    (512 then 256, 128, 64, and finally 1 to match the grayscale output).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在一些“Conv2DTranspose”层中使用步幅为2，以增加通过网络传递时张量的空间形状（原始向量中为1，然后为4，8，16，32，最终为64），同时减少通道数（512，然后为256，128，64，最终为1以匹配灰度输出）。
- en: Training the DCGAN
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练DCGAN
- en: As we have seen, the architectures of the generator and discriminator in a DCGAN
    are very simple and not so different from the VAE models that we looked at in
    [Chapter 3](ch03.xhtml#chapter_vae). The key to understanding GANs lies in understanding
    the training process for the generator and discriminator.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，在DCGAN中生成器和鉴别器的架构非常简单，并且与我们在第3章中看到的VAE模型并没有太大不同。理解GAN的关键在于理解生成器和鉴别器的训练过程。
- en: We can train the discriminator by creating a training set where some of the
    images are *real* observations from the training set and some are *fake* outputs
    from the generator. We then treat this as a supervised learning problem, where
    the labels are 1 for the real images and 0 for the fake images, with binary cross-entropy
    as the loss function.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过创建一个训练集来训练鉴别器，其中一些图像是来自训练集的*真实*观察结果，一些是来自生成器的*假*输出。然后我们将其视为一个监督学习问题，其中真实图像的标签为1，假图像的标签为0，损失函数为二元交叉熵。
- en: How should we train the generator? We need to find a way of scoring each generated
    image so that it can optimize toward high-scoring images. Luckily, we have a discriminator
    that does exactly that! We can generate a batch of images and pass these through
    the discriminator to get a score for each image. The loss function for the generator
    is then simply the binary cross-entropy between these probabilities and a vector
    of ones, because we want to train the generator to produce images that the discriminator
    thinks are real.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该如何训练生成器？我们需要找到一种评分每个生成的图像的方法，以便它可以优化到高分图像。幸运的是，我们有一个鉴别器正是这样做的！我们可以生成一批图像并将其通过鉴别器以获得每个图像的分数。然后生成器的损失函数就是这些概率与一个全为1的向量之间的二元交叉熵，因为我们希望训练生成器生成鉴别器认为是真实的图像。
- en: Crucially, we must alternate the training of these two networks, making sure
    that we only update the weights of one network at a time. For example, during
    the generator training process, only the generator’s weights are updated. If we
    allowed the discriminator’s weights to change as well, the discriminator would
    just adjust so that it is more likely to predict the generated images to be real,
    which is not the desired outcome. We want generated images to be predicted close
    to 1 (real) because the generator is strong, not because the discriminator is
    weak.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 至关重要的是，我们必须交替训练这两个网络，确保我们一次只更新一个网络的权重。例如，在生成器训练过程中，只有生成器的权重会被更新。如果我们允许鉴别器的权重也发生变化，那么鉴别器将只是调整自己，以便更有可能预测生成的图像是真实的，这不是期望的结果。我们希望生成的图像被预测接近1（真实），因为生成器强大，而不是因为鉴别器弱。
- en: A diagram of the training process for the discriminator and generator is shown
    in [Figure 4-5](#gan_bricks_training).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴别器和生成器的训练过程的图示如[图4-5](#gan_bricks_training)所示。
- en: '![](Images/gdl2_0405.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
- en: Figure 4-5\. Training the DCGAN—gray boxes indicate that the weights are frozen
    during training
  id: totrans-114
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Keras provides us with the ability to create a custom `train_step` function
    to implement this logic. [Example 4-7](#building-the-gan-ex) shows the full `DCGAN`
    model class.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-7\. Compiling the DCGAN
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](Images/1.png)](#co_generative_adversarial_networks_CO3-1)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: The loss function for the generator and discriminator is `BinaryCrossentropy`.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_generative_adversarial_networks_CO3-2)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: To train the network, first sample a batch of vectors from a multivariate standard
    normal distribution.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_generative_adversarial_networks_CO3-3)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Next, pass these through the generator to produce a batch of generated images.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_generative_adversarial_networks_CO3-4)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Now ask the discriminator to predict the realness of the batch of real images…​
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_generative_adversarial_networks_CO3-5)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: …​and the batch of generated images.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](Images/6.png)](#co_generative_adversarial_networks_CO3-6)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: The discriminator loss is the average binary cross-entropy across both the real
    images (with label 1) and the fake images (with label 0).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](Images/7.png)](#co_generative_adversarial_networks_CO3-7)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: The generator loss is the binary cross-entropy between the discriminator predictions
    for the generated images and a label of 1.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[![8](Images/8.png)](#co_generative_adversarial_networks_CO3-8)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Update the weights of the discriminator and generator separately.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: The discriminator and generator are constantly fighting for dominance, which
    can make the DCGAN training process unstable. Ideally, the training process will
    find an equilibrium that allows the generator to learn meaningful information
    from the discriminator and the quality of the images will start to improve. After
    enough epochs, the discriminator tends to end up dominating, as shown in [Figure 4-6](#gan_bricks_loss_accuracy),
    but this may not be a problem as the generator may have already learned to produce
    sufficiently high-quality images by this point.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0406.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
- en: Figure 4-6\. Loss and accuracy of the discriminator and generator during training
  id: totrans-136
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Adding Noise to the Labels
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A useful trick when training GANs is to add a small amount of random noise to
    the training labels. This helps to improve the stability of the training process
    and sharpen the generated images. This *label smoothing* acts as way to tame the
    discriminator, so that it is presented with a more challenging task and doesn’t
    overpower the generator.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Analysis of the DCGAN
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By observing images produced by the generator at specific epochs during training
    ([Figure 4-7](#gan_bricks_by_epoch)), it is clear that the generator is becoming
    increasingly adept at producing images that could have been drawn from the training
    set.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0407.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
- en: Figure 4-7\. Output from the generator at specific epochs during training
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is somewhat miraculous that a neural network is able to convert random noise
    into something meaningful. It is worth remembering that we haven’t provided the
    model with any additional features beyond the raw pixels, so it has to work out
    high-level concepts such as how to draw shadows, cuboids, and circles entirely
    by itself.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'Another requirement of a successful generative model is that it doesn’t only
    reproduce images from the training set. To test this, we can find the image from
    the training set that is closest to a particular generated example. A good measure
    for distance is the *L1 distance*, defined as:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[Figure 4-8](#gan_bricks_closest) shows the closest observations in the training
    set for a selection of generated images. We can see that while there is some degree
    of similarity between the generated images and the training set, they are not
    identical. This shows that the generator has understood these high-level features
    and can generate examples that are distinct from those it has already seen.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0408.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
- en: Figure 4-8\. Closest matches of generated images from the training set
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'GAN Training: Tips and Tricks'
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While GANs are a major breakthrough for generative modeling, they are also notoriously
    difficult to train. We will explore some of the most common problems and challenges
    encountered when training GANs in this section, alongside potential solutions.
    In the next section, we will look at some more fundamental adjustments to the
    GAN framework that we can make to remedy many of these problems.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Discriminator overpowers the generator
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If the discriminator becomes too strong, the signal from the loss function becomes
    too weak to drive any meaningful improvements in the generator. In the worst-case
    scenario, the discriminator perfectly learns to separate real images from fake
    images and the gradients vanish completely, leading to no training whatsoever,
    as can be seen in [Figure 4-9](#gan_discriminator_dominant_ex).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0409.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
- en: Figure 4-9\. Example output when the discriminator overpowers the generator
  id: totrans-154
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If you find your discriminator loss function collapsing, you need to find ways
    to weaken the discriminator. Try the following suggestions:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Increase the `rate` parameter of the `Dropout` layers in the discriminator to
    dampen the amount of information that flows through the network.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce the learning rate of the discriminator.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce the number of convolutional filters in the discriminator.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add noise to the labels when training the discriminator.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flip the labels of some images at random when training the discriminator.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generator overpowers the discriminator
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If the discriminator is not powerful enough, the generator will find ways to
    easily trick the discriminator with a small sample of nearly identical images.
    This is known as *mode collapse*.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose we were to train the generator over several batches without
    updating the discriminator in between. The generator would be inclined to find
    a single observation (also known as a *mode*) that always fools the discriminator
    and would start to map every point in the latent input space to this image. Moreover,
    the gradients of the loss function would collapse to near 0, so it wouldn’t be
    able to recover from this state.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Even if we then tried to retrain the discriminator to stop it being fooled by
    this one point, the generator would simply find another mode that fools the discriminator,
    since it has already become numb to its input and therefore has no incentive to
    diversify its output.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: The effect of mode collapse can be seen in [Figure 4-10](#gan_mode_collapse).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0410.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
- en: Figure 4-10\. Example of mode collapse when the generator overpowers the discriminator
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you find that your generator is suffering from mode collapse, you can try
    strengthening the discriminator using the opposite suggestions to those listed
    in the previous section. Also, you can try reducing the learning rate of both
    networks and increasing the batch size.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Uninformative loss
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since the deep learning model is compiled to minimize the loss function, it
    would be natural to think that the smaller the loss function of the generator,
    the better the quality of the images produced. However, since the generator is
    only graded against the current discriminator and the discriminator is constantly
    improving, we cannot compare the loss function evaluated at different points in
    the training process. Indeed, in [Figure 4-6](#gan_bricks_loss_accuracy), the
    loss function of the generator actually increases over time, even though the quality
    of the images is clearly improving. This lack of correlation between the generator
    loss and image quality sometimes makes GAN training difficult to monitor.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we have seen, even with simple GANs, there are a large number of hyperparameters
    to tune. As well as the overall architecture of both the discriminator and the
    generator, there are the parameters that govern batch normalization, dropout,
    learning rate, activation layers, convolutional filters, kernel size, striding,
    batch size, and latent space size to consider. GANs are highly sensitive to very
    slight changes in all of these parameters, and finding a set of parameters that
    works is often a case of educated trial and error, rather than following an established
    set of guidelines.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，即使是简单的GAN，也有大量的超参数需要调整。除了鉴别器和生成器的整体架构外，还有控制批量归一化、丢弃、学习率、激活层、卷积滤波器、内核大小、步幅、批量大小和潜在空间大小的参数需要考虑。GAN对所有这些参数的微小变化非常敏感，找到一组有效的参数通常是经过有教养的试错过程，而不是遵循一套已建立的指导方针。
- en: This is why it is important to understand the inner workings of the GAN and
    know how to interpret the loss function—so that you can identify sensible adjustments
    to the hyperparameters that might improve the stability of the model.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么重要理解GAN的内部工作原理并知道如何解释损失函数——这样你就可以识别出可能改善模型稳定性的超参数的合理调整。
- en: Tackling GAN challenges
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决GAN的挑战
- en: In recent years, several key advancements have drastically improved the overall
    stability of GAN models and diminished the likelihood of some of the problems
    listed earlier, such as mode collapse.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，一些关键进展大大提高了GAN模型的整体稳定性，并减少了一些早期列出的问题的可能性，比如模式崩溃。
- en: In the remainder of this chapter we shall examine the Wasserstein GAN with Gradient
    Penalty (WGAN-GP), which makes several key adjustments to the GAN framework we
    have explored thus far to improve the stability and quality of the image generation
    process.`  `# Wasserstein GAN with Gradient Penalty (WGAN-GP)
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将研究带有梯度惩罚的Wasserstein GAN（WGAN-GP），该模型对我们迄今为止探索的GAN框架进行了几个关键调整，以改善图像生成过程的稳定性和质量。`
    `#带有梯度惩罚的Wasserstein GAN（WGAN-GP）
- en: In this section we will build a WGAN-GP to generate faces from the CelebA dataset
    that we utilized in [Chapter 3](ch03.xhtml#chapter_vae).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建一个WGAN-GP来从我们在[第3章](ch03.xhtml#chapter_vae)中使用的CelebA数据集中生成人脸。
- en: Running the Code for This Example
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行此示例的代码
- en: The code for this example can be found in the Jupyter notebook located at *notebooks/04_gan/02_wgan_gp/wgan_gp.ipynb*
    in the book repository.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例的代码可以在书库中的*notebooks/04_gan/02_wgan_gp/wgan_gp.ipynb*中找到。
- en: The code has been adapted from the excellent [WGAN-GP tutorial](https://oreil.ly/dHYbC)
    created by Aakash Kumar Nain, available on the Keras website.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码是从由Aakash Kumar Nain创建的优秀的[WGAN-GP教程](https://oreil.ly/dHYbC)中改编而来，该教程可在Keras网站上找到。
- en: 'The Wasserstein GAN (WGAN), introduced in a 2017 paper by Arjovsky et al.,^([4](ch04.xhtml#idm45387021016704))
    was one of the first big steps toward stabilizing GAN training. With a few changes,
    the authors were able to show how to train GANs that have the following two properties
    (quoted from the paper):'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: Wasserstein GAN（WGAN）是由Arjovsky等人在2017年的一篇论文中引入的，是稳定GAN训练的第一步。通过一些改变，作者们能够展示如何训练具有以下两个特性的GAN（引用自论文）：
- en: A meaningful loss metric that correlates with the generator’s convergence and
    sample quality
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个与生成器的收敛和样本质量相关的有意义的损失度量
- en: Improved stability of the optimization process
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化过程的稳定性提高
- en: Specifically, the paper introduces the *Wasserstein loss function* for both
    the discriminator and the generator. Using this loss function instead of binary
    cross-entropy results in a more stable convergence of the GAN.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，该论文为鉴别器和生成器引入了*Wasserstein损失函数*。使用这个损失函数而不是二元交叉熵会导致GAN更稳定地收敛。
- en: In this section we’ll define the Wasserstein loss function and then see what
    other changes we need to make to the model architecture and training process to
    incorporate our new loss function.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将定义Wasserstein损失函数，然后看看我们需要对模型架构和训练过程做哪些其他更改以整合我们的新损失函数。
- en: You can find the full model class in the Jupyter notebook located at *chapter05/wgan-gp/faces/train.ipynb*
    in the book repository.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在书库中的*chapter05/wgan-gp/faces/train.ipynb*中找到完整的模型类。
- en: Wasserstein Loss
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Wasserstein损失
- en: Let’s first remind ourselves of the definition of binary cross-entropy loss—the
    function that we are currently using to train the discriminator and generator
    of the GAN ([Equation 4-1](#binary-cross-entropy-loss)).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先回顾一下二元交叉熵损失的定义——我们目前用来训练GAN的函数（[方程4-1](#binary-cross-entropy-loss)）。
- en: Equation 4-1\. Binary cross-entropy loss
  id: totrans-189
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程4-1. 二元交叉熵损失
- en: <math alttext="minus StartFraction 1 Over n EndFraction sigma-summation Underscript
    i equals 1 Overscript n Endscripts left-parenthesis y Subscript i Baseline log
    left-parenthesis p Subscript i Baseline right-parenthesis plus left-parenthesis
    1 minus y Subscript i Baseline right-parenthesis log left-parenthesis 1 minus
    p Subscript i Baseline right-parenthesis right-parenthesis" display="block"><mrow><mo>-</mo>
    <mfrac><mn>1</mn> <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo
    form="prefix">log</mo> <mrow><mo>(</mo> <msub><mi>p</mi> <mi>i</mi></msub> <mo>)</mo></mrow>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo>
    <msub><mi>p</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="minus StartFraction 1 Over n EndFraction sigma-summation Underscript
    i equals 1 Overscript n Endscripts left-parenthesis y Subscript i Baseline log
    left-parenthesis p Subscript i Baseline right-parenthesis plus left-parenthesis
    1 minus y Subscript i Baseline right-parenthesis log left-parenthesis 1 minus
    p Subscript i Baseline right-parenthesis right-parenthesis" display="block"><mrow><mo>-</mo>
    <mfrac><mn>1</mn> <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo
    form="prefix">log</mo> <mrow><mo>(</mo> <msub><mi>p</mi> <mi>i</mi></msub> <mo>)</mo></mrow>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo>
    <msub><mi>p</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
- en: To train the GAN discriminator <math alttext="upper D"><mi>D</mi></math> , we
    calculate the loss when comparing predictions for real images <math alttext="p
    Subscript i Baseline equals upper D left-parenthesis x Subscript i Baseline right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>i</mi></msub> <mo>=</mo> <mi>D</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow></mrow></math> to the response <math alttext="y Subscript i Baseline
    equals 1"><mrow><msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo> <mn>1</mn></mrow></math>
    and predictions for generated images <math alttext="p Subscript i Baseline equals
    upper D left-parenthesis upper G left-parenthesis z Subscript i Baseline right-parenthesis
    right-parenthesis"><mrow><msub><mi>p</mi> <mi>i</mi></msub> <mo>=</mo> <mi>D</mi>
    <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <msub><mi>z</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math> to the response <math alttext="y
    Subscript i Baseline equals 0"><mrow><msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo>
    <mn>0</mn></mrow></math> . Therefore, for the GAN discriminator, minimizing the
    loss function can be written as shown in [Equation 4-2](#GAN-discriminator-loss-minimization).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练GAN鉴别器D，我们计算了对比真实图像x_i的预测p_i与响应y_i=1以及对比生成图像G(z_i)的预测p_i与响应y_i=0的损失。因此，对于GAN鉴别器，最小化损失函数可以写成[方程4-2](#GAN-discriminator-loss-minimization)所示。
- en: Equation 4-2\. GAN discriminator loss minimization
  id: totrans-192
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程4-2. GAN鉴别器损失最小化
- en: <math alttext="min Underscript upper D Endscripts minus left-parenthesis double-struck
    upper E Subscript x tilde p Sub Subscript upper X Subscript Baseline left-bracket
    log upper D left-parenthesis x right-parenthesis right-bracket plus double-struck
    upper E Subscript z tilde p Sub Subscript upper Z Subscript Baseline left-bracket
    log left-parenthesis 1 minus upper D left-parenthesis upper G left-parenthesis
    z right-parenthesis right-parenthesis right-parenthesis right-bracket right-parenthesis"
    display="block"><mrow><munder><mo movablelimits="true" form="prefix">min</mo>
    <mi>D</mi></munder> <mo>-</mo> <mrow><mo>(</mo> <msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo><msub><mi>p</mi>
    <mi>X</mi></msub></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <mrow><mi>D</mi>
    <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>+</mo> <msub><mi>𝔼</mi>
    <mrow><mi>z</mi><mo>∼</mo><msub><mi>p</mi> <mi>Z</mi></msub></mrow></msub> <mrow><mo>[</mo>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>D</mi> <mo>(</mo>
    <mi>G</mi> <mo>(</mo> <mi>z</mi> <mo>)</mo> <mo>)</mo> <mo>)</mo></mrow> <mo>]</mo></mrow>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="min Underscript upper D Endscripts minus left-parenthesis double-struck
    upper E Subscript x tilde p Sub Subscript upper X Subscript Baseline left-bracket
    log upper D left-parenthesis x right-parenthesis right-bracket plus double-struck
    upper E Subscript z tilde p Sub Subscript upper Z Subscript Baseline left-bracket
    log left-parenthesis 1 minus upper D left-parenthesis upper G left-parenthesis
    z right-parenthesis right-parenthesis right-parenthesis right-bracket right-parenthesis"
    display="block"><mrow><munder><mo movablelimits="true" form="prefix">min</mo>
    <mi>D</mi></munder> <mo>-</mo> <mrow><mo>(</mo> <msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo><msub><mi>p</mi>
    <mi>X</mi></msub></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <mrow><mi>D</mi>
    <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>+</mo> <msub><mi>𝔼</mi>
    <mrow><mi>z</mi><mo>∼</mo><msub><mi>p</mi> <mi>Z</mi></msub></mrow></msub> <mrow><mo>[</mo>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>D</mi> <mo>(</mo>
    <mi>G</mi> <mo>(</mo> <mi>z</mi> <mo>)</mo> <mo>)</mo> <mo>)</mo></mrow> <mo>]</mo></mrow>
    <mo>)</mo></mrow></mrow></math>
- en: To train the GAN generator <math alttext="upper G"><mi>G</mi></math> , we calculate
    the loss when comparing predictions for generated images <math alttext="p Subscript
    i Baseline equals upper D left-parenthesis upper G left-parenthesis z Subscript
    i Baseline right-parenthesis right-parenthesis"><mrow><msub><mi>p</mi> <mi>i</mi></msub>
    <mo>=</mo> <mi>D</mi> <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <msub><mi>z</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math> to the response
    <math alttext="y Subscript i Baseline equals 1"><mrow><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>=</mo> <mn>1</mn></mrow></math> . Therefore, for the GAN generator, minimizing
    the loss function can be written as shown in [Equation 4-3](#GAN-generator-loss-minimization).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练GAN生成器G，我们计算了对比生成图像G(z_i)的预测p_i与响应y_i=1的损失。因此，对于GAN生成器，最小化损失函数可以写成[方程4-3](#GAN-generator-loss-minimization)所示。
- en: Equation 4-3\. GAN generator loss minimization
  id: totrans-195
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程4-3. GAN生成器损失最小化
- en: <math alttext="min Underscript upper G Endscripts minus left-parenthesis double-struck
    upper E Subscript z tilde p Sub Subscript upper Z Subscript Baseline left-bracket
    log upper D left-parenthesis upper G left-parenthesis z right-parenthesis right-parenthesis
    right-bracket right-parenthesis" display="block"><mrow><munder><mo movablelimits="true"
    form="prefix">min</mo> <mi>G</mi></munder> <mo>-</mo> <mrow><mo>(</mo> <msub><mi>𝔼</mi>
    <mrow><mi>z</mi><mo>∼</mo><msub><mi>p</mi> <mi>Z</mi></msub></mrow></msub> <mrow><mo>[</mo>
    <mo form="prefix">log</mo> <mrow><mi>D</mi> <mo>(</mo> <mi>G</mi> <mo>(</mo> <mi>z</mi>
    <mo>)</mo> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>)</mo></mrow></mrow></math>
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="min Underscript upper G Endscripts minus left-parenthesis double-struck
    upper E Subscript z tilde p Sub Subscript upper Z Subscript Baseline left-bracket
    log upper D left-parenthesis upper G left-parenthesis z right-parenthesis right-parenthesis
    right-bracket right-parenthesis" display="block"><mrow><munder><mo movablelimits="true"
    form="prefix">min</mo> <mi>G</mi></munder> <mo>-</mo> <mrow><mo>(</mo> <msub><mi>𝔼</mi>
    <mrow><mi>z</mi><mo>∼</mo><msub><mi>p</mi> <mi>Z</mi></msub></mrow></msub> <mrow><mo>[</mo>
    <mo form="prefix">log</mo> <mrow><mi>D</mi> <mo>(</mo> <mi>G</mi> <mo>(</mo> <mi>z</mi>
    <mo>)</mo> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>)</mo></mrow></mrow></math>
- en: Now let’s compare this to the Wasserstein loss function.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将其与Wasserstein损失函数进行比较。
- en: First, the Wasserstein loss requires that we use <math alttext="y Subscript
    i Baseline equals 1"><mrow><msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo> <mn>1</mn></mrow></math>
    and <math alttext="y Subscript i"><msub><mi>y</mi> <mi>i</mi></msub></math> =
    –1 as labels, rather than 1 and 0\. We also remove the sigmoid activation from
    the final layer of the discriminator, so that predictions <math alttext="p Subscript
    i"><msub><mi>p</mi> <mi>i</mi></msub></math> are no longer constrained to fall
    in the range [0, 1] but instead can now be any number in the range ( <math alttext="negative
    normal infinity"><mrow><mo>-</mo> <mi>∞</mi></mrow></math> , <math alttext="normal
    infinity"><mi>∞</mi></math> ). For this reason, the discriminator in a WGAN is
    usually referred to as a *critic* that outputs a *score* rather than a probability.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，Wasserstein损失要求我们使用y_i=1和y_i=-1作为标签，而不是1和0。我们还从鉴别器的最后一层中移除了sigmoid激活，使得预测p_i不再受限于[0,
    1]范围，而是可以是任何范围内的任意数字（负无穷，正无穷）。因此，在WGAN中，鉴别器通常被称为*评论家*，输出*分数*而不是概率。
- en: 'The Wasserstein loss function is defined as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Wasserstein损失函数定义如下：
- en: <math alttext="minus StartFraction 1 Over n EndFraction sigma-summation Underscript
    i equals 1 Overscript n Endscripts left-parenthesis y Subscript i Baseline p Subscript
    i Baseline right-parenthesis" display="block"><mrow><mo>-</mo> <mfrac><mn>1</mn>
    <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>i</mi></msub> <msub><mi>p</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="minus StartFraction 1 Over n EndFraction sigma-summation Underscript
    i equals 1 Overscript n Endscripts left-parenthesis y Subscript i Baseline p Subscript
    i Baseline right-parenthesis" display="block"><mrow><mo>-</mo> <mfrac><mn>1</mn>
    <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>i</mi></msub> <msub><mi>p</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>
- en: 'To train the WGAN critic <math alttext="upper D"><mi>D</mi></math> , we calculate
    the loss when comparing predictions for real images <math alttext="p Subscript
    i Baseline equals upper D left-parenthesis x Subscript i Baseline right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>i</mi></msub> <mo>=</mo> <mi>D</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow></mrow></math> to the response <math alttext="y Subscript i Baseline
    equals 1"><mrow><msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo> <mn>1</mn></mrow></math>
    and predictions for generated images <math alttext="p Subscript i Baseline equals
    upper D left-parenthesis upper G left-parenthesis z Subscript i Baseline right-parenthesis
    right-parenthesis"><mrow><msub><mi>p</mi> <mi>i</mi></msub> <mo>=</mo> <mi>D</mi>
    <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <msub><mi>z</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math> to the response <math alttext="y
    Subscript i"><msub><mi>y</mi> <mi>i</mi></msub></math> = –1\. Therefore, for the
    WGAN critic, minimizing the loss function can be written as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练WGAN评论家<math alttext="upper D"><mi>D</mi></math>，我们计算真实图像的预测与响应之间的损失<math
    alttext="p Subscript i Baseline equals upper D left-parenthesis x Subscript i
    Baseline right-parenthesis"><mrow><msub><mi>p</mi> <mi>i</mi></msub> <mo>=</mo>
    <mi>D</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>与响应<math
    alttext="y Subscript i Baseline equals 1"><mrow><msub><mi>y</mi> <mi>i</msub>
    <mo>=</mo> <mn>1</mn></mrow></math>，以及生成图像的预测与响应之间的损失<math alttext="p Subscript
    i Baseline equals upper D left-parenthesis upper G left-parenthesis z Subscript
    i Baseline right-parenthesis right-parenthesis"><mrow><msub><mi>p</mi> <mi>i</msub>
    <mo>=</mo> <mi>D</mi> <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <msub><mi>z</mi>
    <mi>i</msub> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>与响应<math alttext="y
    Subscript i"><msub><mi>y</mi> <mi>i</msub></math> = -1。因此，对于WGAN评论家，最小化损失函数可以写成如下形式：
- en: <math alttext="min Underscript upper D Endscripts minus left-parenthesis double-struck
    upper E Subscript x tilde p Sub Subscript upper X Subscript Baseline left-bracket
    upper D left-parenthesis x right-parenthesis right-bracket minus double-struck
    upper E Subscript z tilde p Sub Subscript upper Z Subscript Baseline left-bracket
    upper D left-parenthesis upper G left-parenthesis z right-parenthesis right-parenthesis
    right-bracket right-parenthesis" display="block"><mrow><munder><mo movablelimits="true"
    form="prefix">min</mo> <mi>D</mi></munder> <mo>-</mo> <mrow><mo>(</mo> <msub><mi>𝔼</mi>
    <mrow><mi>x</mi><mo>∼</mo><msub><mi>p</mi> <mi>X</mi></msub></mrow></msub> <mrow><mo>[</mo>
    <mi>D</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>-</mo>
    <msub><mi>𝔼</mi> <mrow><mi>z</mi><mo>∼</mo><msub><mi>p</mi> <mi>Z</mi></msub></mrow></msub>
    <mrow><mo>[</mo> <mi>D</mi> <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <mi>z</mi>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>)</mo></mrow></mrow></math>
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="min Underscript upper D Endscripts minus left-parenthesis double-struck
    upper E Subscript x tilde p Sub Subscript upper X Subscript Baseline left-bracket
    upper D left-parenthesis x right-parenthesis right-bracket minus double-struck
    upper E Subscript z tilde p Sub Subscript upper Z Subscript Baseline left-bracket
    upper D left-parenthesis upper G left-parenthesis z right-parenthesis right-parenthesis
    right-bracket right-parenthesis" display="block"><mrow><munder><mo movablelimits="true"
    form="prefix">min</mo> <mi>D</mi></munder> <mo>-</mo> <mrow><mo>(</mo> <msub><mi>𝔼</mi>
    <mrow><mi>x</mi><mo>∼</mo><msub><mi>p</mi> <mi>X</mi></msub></mrow></msub> <mrow><mo>[</mo>
    <mi>D</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>-</mo>
    <msub><mi>𝔼</mi> <mrow><mi>z</mi><mo>∼</mo><msub><mi>p</mi> <mi>Z</mi></msub></mrow></msub>
    <mrow><mo>[</mo> <mi>D</mi> <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <mi>z</mi>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>)</mo></mrow></mrow></math>
- en: In other words, the WGAN critic tries to maximize the difference between its
    predictions for real images and generated images.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，WGAN评论家试图最大化其对真实图像和生成图像的预测之间的差异。
- en: 'To train the WGAN generator, we calculate the loss when comparing predictions
    for generated images <math alttext="p Subscript i Baseline equals upper D left-parenthesis
    upper G left-parenthesis z Subscript i Baseline right-parenthesis right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>i</mi></msub> <mo>=</mo> <mi>D</mi> <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo>
    <msub><mi>z</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
    to the response <math alttext="y Subscript i Baseline equals 1"><mrow><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>=</mo> <mn>1</mn></mrow></math> . Therefore, for the WGAN
    generator, minimizing the loss function can be written as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练WGAN生成器，我们计算生成图像的预测与响应之间的损失<math alttext="p Subscript i Baseline equals
    upper D left-parenthesis upper G left-parenthesis z Subscript i Baseline right-parenthesis
    right-parenthesis"><mrow><msub><mi>p</mi> <mi>i</mi></msub> <mo>=</mo> <mi>D</mi>
    <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <msub><mi>z</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>与响应<math alttext="y Subscript
    i Baseline equals 1"><mrow><msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo> <mn>1</mn></mrow></math>。因此，对于WGAN生成器，最小化损失函数可以写成如下形式：
- en: <math alttext="min Underscript upper G Endscripts minus left-parenthesis double-struck
    upper E Subscript z tilde p Sub Subscript upper Z Subscript Baseline left-bracket
    upper D left-parenthesis upper G left-parenthesis z right-parenthesis right-parenthesis
    right-bracket right-parenthesis" display="block"><mrow><munder><mo movablelimits="true"
    form="prefix">min</mo> <mi>G</mi></munder> <mo>-</mo> <mrow><mo>(</mo> <msub><mi>𝔼</mi>
    <mrow><mi>z</mi><mo>∼</mo><msub><mi>p</mi> <mi>Z</mi></msub></mrow></msub> <mrow><mo>[</mo>
    <mi>D</mi> <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>)</mo></mrow></mrow></math>
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="min Underscript upper G Endscripts minus left-parenthesis double-struck
    upper E Subscript z tilde p Sub Subscript upper Z Subscript Baseline left-bracket
    upper D left-parenthesis upper G left-parenthesis z right-parenthesis right-parenthesis
    right-bracket right-parenthesis" display="block"><mrow><munder><mo movablelimits="true"
    form="prefix">min</mo> <mi>G</mi></munder> <mo>-</mo> <mrow><mo>(</mo> <msub><mi>𝔼</mi>
    <mrow><mi>z</mi><mo>∼</mo><msub><mi>p</mi> <mi>Z</mi></msub></mrow></msub> <mrow><mo>[</mo>
    <mi>D</mi> <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>)</mo></mrow></mrow></math>
- en: In other words, the WGAN generator tries to produce images that are scored as
    highly as possible by the critic (i.e., the critic is fooled into thinking they
    are real).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，WGAN生成器试图生成评论家尽可能高分的图像（即，评论家被欺骗以为它们是真实的）。
- en: The Lipschitz Constraint
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利普希茨约束
- en: It may surprise you that we are now allowing the critic to output any number
    in the range ( <math alttext="negative normal infinity"><mrow><mo>-</mo> <mi>∞</mi></mrow></math>
    , <math alttext="normal infinity"><mi>∞</mi></math> ), rather than applying a
    sigmoid function to restrict the output to the usual [0, 1] range. The Wasserstein
    loss can therefore be very large, which is unsettling—usually, large numbers in
    neural networks are to be avoided!
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 也许让你惊讶的是，我们现在允许评论家输出范围内的任何数字（<math alttext="negative normal infinity"><mrow><mo>-</mo>
    <mi>∞</mi></mrow></math>，<math alttext="normal infinity"><mi>∞</mi></math>），而不是应用Sigmoid函数将输出限制在通常的[0,1]范围内。因此，Wasserstein损失可能非常大，这令人不安——通常情况下，神经网络中的大数值应该避免！
- en: In fact, the authors of the WGAN paper show that for the Wasserstein loss function
    to work, we also need to place an additional constraint on the critic. Specifically,
    it is required that the critic is a *1-Lipschitz continuous function*. Let’s pick
    this apart to understand what it means in more detail.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，WGAN论文的作者表明，为了使Wasserstein损失函数起作用，我们还需要对评论家施加额外的约束。具体来说，评论家必须是*1-Lipschitz连续函数*。让我们详细解释一下这意味着什么。
- en: 'The critic is a function <math alttext="upper D"><mi>D</mi></math> that converts
    an image into a prediction. We say that this function is 1-Lipschitz if it satisfies
    the following inequality for any two input images, <math alttext="x 1"><msub><mi>x</mi>
    <mn>1</mn></msub></math> and <math alttext="x 2"><msub><mi>x</mi> <mn>2</mn></msub></math>
    :'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 评论家是一个将图像转换为预测的函数<math alttext="upper D"><mi>D</mi></math>。如果对于任意两个输入图像<math
    alttext="x 1"><msub><mi>x</mi> <mn>1</mn></msub></math>和<math alttext="x 2"><msub><mi>x</mi>
    <mn>2</mn></msub></math>，该函数满足以下不等式，则我们称该函数为1-Lipschitz：
- en: <math alttext="StartFraction StartAbsoluteValue upper D left-parenthesis x 1
    right-parenthesis minus upper D left-parenthesis x 2 right-parenthesis EndAbsoluteValue
    Over StartAbsoluteValue x 1 minus x 2 EndAbsoluteValue EndFraction less-than-or-equal-to
    1" display="block"><mrow><mfrac><mrow><mrow><mo>|</mo><mi>D</mi></mrow><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow><mo>-</mo><mi>D</mi><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>2</mn></msub> <mo>)</mo></mrow><mrow><mo>|</mo></mrow></mrow> <mrow><mrow><mo>|</mo></mrow><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>-</mo><msub><mi>x</mi> <mn>2</mn></msub> <mrow><mo>|</mo></mrow></mrow></mfrac>
    <mo>≤</mo> <mn>1</mn></mrow></math>
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartFraction StartAbsoluteValue upper D left-parenthesis x 1
    right-parenthesis minus upper D left-parenthesis x 2 right-parenthesis EndAbsoluteValue
    Over StartAbsoluteValue x 1 minus x 2 EndAbsoluteValue EndFraction less-than-or-equal-to
    1" display="block"><mrow><mfrac><mrow><mrow><mo>|</mo><mi>D</mi></mrow><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow><mo>-</mo><mi>D</mi><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>2</mn></msub> <mo>)</mo></mrow><mrow><mo>|</mo></mrow></mrow> <mrow><mrow><mo>|</mo></mrow><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>-</mo><msub><mi>x</mi> <mn>2</mn></msub> <mrow><mo>|</mo></mrow></mrow></mfrac>
    <mo>≤</mo> <mn>1</mn></mrow></math>
- en: Here, <math alttext="StartAbsoluteValue x 1 minus x 2 EndAbsoluteValue"><mrow><mrow><mo>|</mo></mrow>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>-</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mrow><mo>|</mo></mrow></mrow></math> is the average pixelwise absolute difference
    between two images and <math alttext="StartAbsoluteValue upper D left-parenthesis
    x 1 right-parenthesis minus upper D left-parenthesis x 2 right-parenthesis EndAbsoluteValue"><mrow><mrow><mo>|</mo>
    <mi>D</mi></mrow> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>)</mo></mrow>
    <mo>-</mo> <mi>D</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>)</mo></mrow>
    <mrow><mo>|</mo></mrow></mrow></math> is the absolute difference between the critic
    predictions. Essentially, we require a limit on the rate at which the predictions
    of the critic can change between two images (i.e., the absolute value of the gradient
    must be at most 1 everywhere). We can see this applied to a Lipschitz continuous
    1D function in [Figure 4-11](#lipschitz)—at no point does the line enter the cone,
    wherever you place the cone on the line. In other words, there is a limit on the
    rate at which the line can rise or fall at any point.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，<math alttext="StartAbsoluteValue x 1 minus x 2 EndAbsoluteValue"><mrow><mrow><mo>|</mo></mrow>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>-</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mrow><mo>|</mo></mrow></mrow></math>是两个图像之间的平均像素绝对差异，<math alttext="StartAbsoluteValue
    upper D left-parenthesis x 1 right-parenthesis minus upper D left-parenthesis
    x 2 right-parenthesis EndAbsoluteValue"><mrow><mrow><mo>|</mo> <mi>D</mi></mrow>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>)</mo></mrow> <mo>-</mo>
    <mi>D</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>)</mo></mrow>
    <mrow><mo>|</mo></mrow></mrow></math>是评论家预测之间的绝对差异。基本上，我们要求评论家的预测在两个图像之间变化的速率有限（即，梯度的绝对值必须在任何地方最多为1）。我们可以看到这应用于利普希茨连续的一维函数中，如[图4-11](#lipschitz)所示——在任何位置放置锥体时，线都不会进入锥体。换句话说，线在任何点上升或下降的速率都有限制。
- en: '![](Images/gdl2_0411.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0411.png)'
- en: 'Figure 4-11\. A Lipschitz continuous function (source: [Wikipedia](https://oreil.ly/Ki7ds))'
  id: totrans-214
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-11。利普希茨连续函数（来源：[维基百科](https://oreil.ly/Ki7ds)）
- en: Tip
  id: totrans-215
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: For those who want to delve deeper into the mathematical rationale behind why
    the Wasserstein loss only works when this constraint is enforced, Jonathan Hui
    offers [an excellent explanation](https://oreil.ly/devy5).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些想深入了解为什么只有在强制执行这个约束时，Wasserstein损失才有效的数学原理的人，Jonathan Hui提供了[一个优秀的解释](https://oreil.ly/devy5)。
- en: Enforcing the Lipschitz Constraint
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强制利普希茨约束
- en: In the original WGAN paper, the authors show how it is possible to enforce the
    Lipschitz constraint by clipping the weights of the critic to lie within a small
    range, [–0.01, 0.01], after each training batch.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始的WGAN论文中，作者展示了通过在每个训练批次后将评论家的权重剪辑到一个小范围[-0.01, 0.01]内来强制执行利普希茨约束的可能性。
- en: One of the criticisms of this approach is that the capacity of the critic to
    learn is greatly diminished, since we are clipping its weights. In fact, even
    in the original WGAN paper the authors write, “Weight clipping is a clearly terrible
    way to enforce a Lipschitz constraint.” A strong critic is pivotal to the success
    of a WGAN, since without accurate gradients, the generator cannot learn how to
    adapt its weights to produce better samples.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的批评之一是，评论家学习的能力大大降低，因为我们正在剪辑它的权重。事实上，即使在原始的WGAN论文中，作者们也写道，“权重剪辑显然是一种强制利普希茨约束的可怕方式。”强大的评论家对于WGAN的成功至关重要，因为没有准确的梯度，生成器无法学习如何调整其权重以生成更好的样本。
- en: Therefore, other researchers have looked for alternative ways to enforce the
    Lipschitz constraint and improve the capacity of the WGAN to learn complex features.
    One such method is the Wasserstein GAN with Gradient Penalty.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，其他研究人员寻找了其他方法来强制执行利普希茨约束，并提高WGAN学习复杂特征的能力。其中一种方法是带有梯度惩罚的Wasserstein GAN。
- en: In the paper introducing this variant,^([5](ch04.xhtml#idm45387019298976)) the
    authors show how the Lipschitz constraint can be enforced directly by including
    a *gradient penalty* term in the loss function for the critic that penalizes the
    model if the gradient norm deviates from 1\. This results in a far more stable
    training process.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在引入这种变体的论文中，作者展示了如何通过在评论家的损失函数中直接包含*梯度惩罚*项来强制执行利普希茨约束，如果梯度范数偏离1，模型将受到惩罚。这导致了一个更加稳定的训练过程。
- en: In the next section, we’ll see how to build this extra term into the loss function
    for our critic.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到如何将这个额外项构建到我们评论家的损失函数中。
- en: The Gradient Penalty Loss
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度惩罚损失
- en: '[Figure 4-12](#wgangp_critic) is a diagram of the training process for the
    critic of a WGAN-GP. If we compare this to the original discriminator training
    process from [Figure 4-5](#gan_bricks_training), we can see that the key addition
    is the gradient penalty loss included as part of the overall loss function, alongside
    the Wasserstein loss from the real and fake images.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-12](#wgangp_critic)是WGAN-GP评论家的训练过程的图表。如果我们将其与[图4-5](#gan_bricks_training)中原始鉴别器训练过程进行比较，我们可以看到的关键添加是梯度惩罚损失作为整体损失函数的一部分，与真实和虚假图像的Wasserstein损失一起。'
- en: '![](Images/gdl2_0412.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0412.png)'
- en: Figure 4-12\. The WGAN-GP critic training process
  id: totrans-226
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-12。WGAN-GP评论家训练过程
- en: The gradient penalty loss measures the squared difference between the norm of
    the gradient of the predictions with respect to the input images and 1\. The model
    will naturally be inclined to find weights that ensure the gradient penalty term
    is minimized, thereby encouraging the model to conform to the Lipschitz constraint.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度惩罚损失衡量了预测梯度的范数与输入图像之间的平方差异和1之间的差异。模型自然倾向于找到确保梯度惩罚项最小化的权重，从而鼓励模型符合利普希茨约束。
- en: It is intractable to calculate this gradient everywhere during the training
    process, so instead the WGAN-GP evaluates the gradient at only a handful of points.
    To ensure a balanced mix, we use a set of interpolated images that lie at randomly
    chosen points along lines connecting the batch of real images to the batch of
    fake images pairwise, as shown in [Figure 4-13](#wgangp_randomweighting).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0413.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
- en: Figure 4-13\. Interpolating between images
  id: totrans-230
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In [Example 4-8](#gradient-penalty-loss-ex), we show how the gradient penalty
    is calculated in code.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-8\. The gradient penalty loss function
  id: totrans-232
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[![1](Images/1.png)](#co_generative_adversarial_networks_CO4-1)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Each image in the batch gets a random number, between 0 and 1, stored as the
    vector `alpha`.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_generative_adversarial_networks_CO4-2)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: A set of interpolated images is calculated.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_generative_adversarial_networks_CO4-3)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: The critic is asked to score each of these interpolated images.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_generative_adversarial_networks_CO4-4)'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: The gradient of the predictions is calculated with respect to the input images.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_generative_adversarial_networks_CO4-5)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: The L2 norm of this vector is calculated.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](Images/6.png)](#co_generative_adversarial_networks_CO4-6)'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: The function returns the average squared distance between the L2 norm and 1.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Training the WGAN-GP
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A key benefit of using the Wasserstein loss function is that we no longer need
    to worry about balancing the training of the critic and the generator—in fact,
    when using the Wasserstein loss, the critic must be trained to convergence before
    updating the generator, to ensure that the gradients for the generator update
    are accurate. This is in contrast to a standard GAN, where it is important not
    to let the discriminator get too strong.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, with Wasserstein GANs, we can simply train the critic several times
    between generator updates, to ensure it is close to convergence. A typical ratio
    used is three to five critic updates per generator update.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: We have now introduced both of the key concepts behind the WGAN-GP—the Wasserstein
    loss and the gradient penalty term that is included in the critic loss function.
    The training step of the WGAN model that incorporates all of these ideas is shown
    in [Example 4-9](#training-wgan-ex).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-9\. Training the WGAN-GP
  id: totrans-250
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[![1](Images/1.png)](#co_generative_adversarial_networks_CO5-1)'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Perform three critic updates.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_generative_adversarial_networks_CO5-2)'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the Wasserstein loss for the critic—the difference between the average
    prediction for the fake images and the real images.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_generative_adversarial_networks_CO5-3)'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the gradient penalty term (see [Example 4-8](#gradient-penalty-loss-ex)).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_generative_adversarial_networks_CO5-4)'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: The critic loss function is a weighted sum of the Wasserstein loss and the gradient
    penalty.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_generative_adversarial_networks_CO5-5)'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Update the weights of the critic.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](Images/6.png)](#co_generative_adversarial_networks_CO5-6)'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the Wasserstein loss for the generator.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](Images/7.png)](#co_generative_adversarial_networks_CO5-7)'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Update the weights of the generator.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Batch Normalization in a WGAN-GP
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One last consideration we should note before training a WGAN-GP is that batch
    normalization shouldn’t be used in the critic. This is because batch normalization
    creates correlation between images in the same batch, which makes the gradient
    penalty loss less effective. Experiments have shown that WGAN-GPs can still produce
    excellent results even without batch normalization in the critic.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'We have now covered all of the key differences between a standard GAN and a
    WGAN-GP. To recap:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: A WGAN-GP uses the Wasserstein loss.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The WGAN-GP is trained using labels of 1 for real and –1 for fake.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no sigmoid activation in the final layer of the critic.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Include a gradient penalty term in the loss function for the critic.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train the critic multiple times for each update of the generator.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are no batch normalization layers in the critic.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analysis of the WGAN-GP
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s take a look at some example outputs from the generator, after 25 epochs
    of training ([Figure 4-14](#wgangp_examples)).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0414.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
- en: Figure 4-14\. WGAN-GP face examples
  id: totrans-278
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The model has learned the significant high-level attributes of a face, and there
    is no sign of mode collapse.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: We can also see how the loss functions of the model evolve over time ([Figure 4-15](#wgangp_losses))—the
    loss functions of both the critic and generator are highly stable and convergent.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: If we compare the WGAN-GP output to the VAE output from the previous chapter,
    we can see that the GAN images are generally sharper—especially the definition
    between the hair and the background. This is true in general; VAEs tend to produce
    softer images that blur color boundaries, whereas GANs are known to produce sharper,
    more well-defined images.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0415.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-15\. WGAN-GP loss curves: the critic loss (`epoch_c_loss`) is broken
    down into the Wasserstein loss (`epoch_c_wass`) and the gradient penalty loss
    (`epoch_c_gp`)'
  id: totrans-283
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is also true that GANs are generally more difficult to train than VAEs and
    take longer to reach a satisfactory quality. However, many state-of-the-art generative
    models today are GAN-based, as the rewards for training large-scale GANs on GPUs
    over a longer period of time are significant.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Conditional GAN (CGAN)
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in this chapter, we have built GANs that are able to generate realistic
    images from a given training set. However, we haven’t been able to control the
    type of image we would like to generate—for example, a male or female face, or
    a large or small brick. We can sample a random point from the latent space, but
    we do not have the ability to easily understand what kind of image will be produced
    given the choice of latent variable.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: In the final part of this chapter we shall turn our attention to building a
    GAN where we are able to control the output—a so called *conditional GAN*. This
    idea, first introduced in “Conditional Generative Adversarial Nets” by Mirza and
    Osindero in 2014,^([6](ch04.xhtml#idm45387018921312)) is a relatively simple extension
    to the GAN architecture.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Running the Code for This Example
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for this example can be found in the Jupyter notebook located at *notebooks/04_gan/03_cgan/cgan.ipynb*
    in the book repository.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: The code has been adapted from the excellent [CGAN tutorial](https://oreil.ly/Ey11I)
    created by Sayak Paul, available on the Keras website.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: CGAN Architecture
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this example, we will condition our CGAN on the *blond hair* attribute of
    the faces dataset. That is, we will be able to explicitly specify whether we want
    to generate an image with blond hair or not. This label is provided as part of
    the CelebA dataset.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: The high-level CGAN architecture is shown in [Figure 4-16](#cgan_architecture).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0416.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
- en: Figure 4-16\. Inputs and outputs of the generator and critic in a CGAN
  id: totrans-295
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The key difference between a standard GAN and a CGAN is that in a CGAN we pass
    in extra information to the generator and critic relating to the label. In the
    generator, this is simply appended to the latent space sample as a one-hot encoded
    vector. In the critic, we add the label information as extra channels to the RGB
    image. We do this by repeating the one-hot encoded vector to fill the same shape
    as the input images.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: CGANs work because the critic now has access to extra information regarding
    the content of the image, so the generator must ensure that its output agrees
    with the provided label, in order to keep fooling the critic. If the generator
    produced perfect images that disagreed with the image label the critic would be
    able to tell that they were fake simply because the images and labels did not
    match.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-298
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In our example, our one-hot encoded label will have length 2, because there
    are two classes (Blonde and Not Blond). However, you can have as many labels as
    you like—for example, you could train a CGAN on the Fashion-MNIST dataset to output
    one of the 10 different fashion items, by incorporating a one-hot encoded label
    vector of length 10 into the input of the generator and 10 additional one-hot
    encoded label channels into the input of the critic.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: The only change we need to make to the architecture is to concatenate the label
    information to the existing inputs of the generator and the critic, as shown in
    [Example 4-10](#cgan_input_shapes).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-10\. Input layers in the CGAN
  id: totrans-301
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[![1](Images/1.png)](#co_generative_adversarial_networks_CO6-1)'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: The image channels and label channels are passed in separately to the critic
    and concatenated.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_generative_adversarial_networks_CO6-2)'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: The latent vector and the label classes are passed in separately to the generator
    and concatenated before being reshaped.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: Training the CGAN
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We must also make some changes to the `train_step` of the CGAN to match the
    new input formats of the generator and critic, as shown in [Example 4-11](#cgan_train_step).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-11\. The `train_step` of the CGAN
  id: totrans-309
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[![1](Images/1.png)](#co_generative_adversarial_networks_CO7-1)'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: The images and labels are unpacked from the input data.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_generative_adversarial_networks_CO7-2)'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: The one-hot encoded vectors are expanded to one-hot encoded images that have
    the same spatial size as the input images (64 × 64).
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_generative_adversarial_networks_CO7-3)'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: The generator is now fed with a list of two inputs—the random latent vectors
    and the one-hot encoded label vectors.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_generative_adversarial_networks_CO7-4)'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: The critic is now fed with a list of two inputs—the fake/real images and the
    one-hot encoded label channels.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_generative_adversarial_networks_CO7-5)'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: The gradient penalty function also requires the one-hot encoded label channels
    to be passed through as it uses the critic.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](Images/6.png)](#co_generative_adversarial_networks_CO7-6)'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: The changes made to the critic training step also apply to the generator training
    step.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: Analysis of the CGAN
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can control the CGAN output by passing a particular one-hot encoded label
    into the input of the generator. For example, to generate a face with nonblond
    hair, we pass in the vector `[1, 0]`. To generate a face with blond hair, we pass
    in the vector `[0, 1]`.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: The output from the CGAN can be seen in [Figure 4-17](#cgan_output). Here, we
    keep the random latent vectors the same across the examples and change only the
    conditional label vector. It is clear that the CGAN has learned to use the label
    vector to control only the hair color attribute of the images. It is impressive
    that the rest of the image barely changes—this is proof that GANs are able to
    organize points in the latent space in such a way that individual features can
    be decoupled from each other.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0417.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
- en: Figure 4-17\. Output from the CGAN when the *Blond* and *Not Blond* vectors
    are appended to the latent sample
  id: totrans-327
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  id: totrans-328
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If labels are available for your dataset, it is generally a good idea to include
    them as input to your GAN even if you do not necessarily need to condition the
    generated output on the label, as they tend to improve the quality of images generated.
    You can think of the labels as just a highly informative extension to the pixel
    input.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-330
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter we explored three different generative adversarial network
    (GAN) models: the deep convolutional GAN (DCGAN), the more sophisticated Wasserstein
    GAN with Gradient Penalty (WGAN-GP), and the conditional GAN (CGAN).'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: All GANs are characterized by a generator versus discriminator (or critic) architecture,
    with the discriminator trying to “spot the difference” between real and fake images
    and the generator aiming to fool the discriminator. By balancing how these two
    adversaries are trained, the GAN generator can gradually learn how to produce
    similar observations to those in the training set.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: We first saw how to train a DCGAN to generate images of toy bricks. It was able
    to learn how to realistically represent 3D objects as images, including accurate
    representations of shadow, shape, and texture. We also explored the different
    ways in which GAN training can fail, including mode collapse and vanishing gradients.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: We then explored how the Wasserstein loss function remedied many of these problems
    and made GAN training more predictable and reliable. The WGAN-GP places the 1-Lipschitz
    requirement at the heart of the training process by including a term in the loss
    function to pull the gradient norm toward 1.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: We applied the WGAN-GP to the problem of face generation and saw how by simply
    choosing points from a standard normal distribution, we can generate new faces.
    This sampling process is very similar to a VAE, though the faces produced by a
    GAN are quite different—often sharper, with greater distinction between different
    parts of the image.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we built a CGAN that allowed us to control the type of image that is
    generated. This works by passing in the label as input to the critic and generator,
    thereby giving the network the additional information it needs in order to condition
    the generated output on a given label.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: Overall, we have seen how the GAN framework is extremely flexible and able to
    be adapted to many interesting problem domains. In particular, GANs have driven
    significant progress in the field of image generation with many interesting extensions
    to the underlying framework, as we shall see in [Chapter 10](ch10.xhtml#chapter_image_generation).
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore a different family of generative model
    that is ideal for modeling sequential data—autoregressive models.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch04.xhtml#idm45387021611344-marker)) Ian J. Goodfellow et al., “Generative
    Adversarial Nets,” June 10, 2014, [*https://arxiv.org/abs/1406.2661*](https://arxiv.org/abs/1406.2661)
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch04.xhtml#idm45387021585984-marker)) Alec Radford et al., “Unsupervised
    Representation Learning with Deep Convolutional Generative Adversarial Networks,”
    January 7, 2016, [*https://arxiv.org/abs/1511.06434*](https://arxiv.org/abs/1511.06434).
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch04.xhtml#idm45387021101472-marker)) Augustus Odena et al., “Deconvolution
    and Checkerboard Artifacts,” October 17, 2016, [*https://distill.pub/2016/deconv-checkerboard*](https://distill.pub/2016/deconv-checkerboard).
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch04.xhtml#idm45387021016704-marker)) Martin Arjovsky et al., “Wasserstein
    GAN,” January 26, 2017, [*https://arxiv.org/abs/1701.07875*](https://arxiv.org/abs/1701.07875).
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch04.xhtml#idm45387019298976-marker)) Ishaan Gulrajani et al., “Improved
    Training of Wasserstein GANs,” March 31, 2017, [*https://arxiv.org/abs/1704.00028*](https://arxiv.org/abs/1704.00028).
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch04.xhtml#idm45387018921312-marker)) Mehdi Mirza and Simon Osindero,
    “Conditional Generative Adversarial Nets,” November 6, 2014, [*https://arxiv.org/abs/1411.1784*](https://arxiv.org/abs/1411.1784).`
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
