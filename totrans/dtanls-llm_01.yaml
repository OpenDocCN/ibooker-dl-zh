- en: 1 Analyzing data with large language models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An introduction to language models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data analysis with language models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using language models efficiently
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language models are powerful neural networks that can be used for various data-processing
    tasks. This chapter introduces language models and shows how and why to use them
    for data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 What can language models do?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will start this section with a little poem and an associated picture (figure
    [1.1](#fig__ai4data)) connecting the two main topics of this book, data analysis
    and large language models:'
  prefs: []
  type: TYPE_NORMAL
- en: '*In the silent hum of the server’s light,'
  prefs: []
  type: TYPE_NORMAL
- en: Data flows through the veins of night.
  prefs: []
  type: TYPE_NORMAL
- en: Rows and columns, a structured sea,
  prefs: []
  type: TYPE_NORMAL
- en: With stories hidden, waiting to be free.*
  prefs: []
  type: TYPE_NORMAL
- en: '*Each number sings of pasts untold,'
  prefs: []
  type: TYPE_NORMAL
- en: Trends and truths in patterns bold.
  prefs: []
  type: TYPE_NORMAL
- en: And here arrives a curious friend,
  prefs: []
  type: TYPE_NORMAL
- en: A language model, eager to comprehend.*
  prefs: []
  type: TYPE_NORMAL
- en: '*It listens close, with circuits keen,'
  prefs: []
  type: TYPE_NORMAL
- en: To turn raw facts into insight unseen.
  prefs: []
  type: TYPE_NORMAL
- en: From scatter plots to sentences clear,
  prefs: []
  type: TYPE_NORMAL
- en: Data’s language is all it can hear.*
  prefs: []
  type: TYPE_NORMAL
- en: '*The figures dance, the texts reply,'
  prefs: []
  type: TYPE_NORMAL
- en: As code meets meaning under AI’s eye.
  prefs: []
  type: TYPE_NORMAL
- en: They merge their worlds, a seamless blend,
  prefs: []
  type: TYPE_NORMAL
- en: Where logic and language have no end.*
  prefs: []
  type: TYPE_NORMAL
- en: '*For in this bond, both deep and wide,'
  prefs: []
  type: TYPE_NORMAL
- en: Data’s essence finds a guide.
  prefs: []
  type: TYPE_NORMAL
- en: And in the neural net’s embrace,
  prefs: []
  type: TYPE_NORMAL
- en: Data analysis gains a poetic grace.*
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F01_Trummer.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1 Illustration by GPT-4o, connecting the topics “data analysis” and
    “large language models”
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The poem and the picture were generated by GPT-4o (“o” for “omni”), a language
    model by OpenAI that processes multimodal data, based solely on the instructions
    “Write a poem connecting data analysis and large language models!” followed by
    “Now draw a corresponding picture!” Both the picture and the poem seem to relate
    to the requested topics. Although the poem may not win any literature awards,
    its text is coherent, it is structured as we would expect from a poem, and it
    rhymes! Perhaps most importantly, all it took to generate the poem and the picture
    were short instructions expressed in natural language. Whereas prior machine learning
    methods relied on large amounts of task-specific training data, this requirement
    is now obsolete. And, of course, the task is specific enough to convince us that
    the language model is not copying existing solutions from the web and generates
    original content instead.
  prefs: []
  type: TYPE_NORMAL
- en: Writing poems and generating pictures are only two of many possible use cases
    (albeit possibly the most entertaining ones). Models like GPT-4o can solve various
    tasks, such as summarizing text documents, writing program code, and answering
    questions about pictures. In this book, you will learn how to use language models
    to accomplish a plethora of data-analysis tasks ranging from extracting information
    from large collections of text documents to writing code for data analysis. After
    reading this book, you will be able to quickly build data-analysis pipelines that
    are based on language models and extract useful insights from a variety of data
    formats.
  prefs: []
  type: TYPE_NORMAL
- en: What does GPT stand for?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*GPT* stands for *Generative Pretrained Transformer*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Generative*: GPT is a large neural network that generates content (e.g., text
    or code) in response to input text. This fact distinguishes it from other neural
    networks that, for example, can only classify input text into a fixed set of predefined
    categories.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Pretrained*: GPT is pretrained on large amounts of data, solving generic tasks
    such as predicting the next word in text. Typically, the pretraining task is different
    from the tasks it is primarily used for. However, pretraining helps it learn more
    specialized tasks faster.'
  prefs: []
  type: TYPE_NORMAL
- en: The *Transformer* is a new neural network architecture that is particularly
    useful for learning tasks that involve variable-length input or output (such as
    text documents). It is currently the dominant architecture for generative AI approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 What you will learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This book is about using language models for data analysis. We can categorize
    data-analysis tasks by the type of data we’re analyzing and by the type of analysis.
    This book covers a wide range of data types and analysis tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'We focus on *multimodal* data analysis: that is, we use language models to
    analyze various types of data. More precisely, we cover the following data types
    in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Text*—Think of emails, newspaper articles, and comments on a web forum. Text
    data is ubiquitous and contains valuable information. In this book, we will see
    how to use language models to automatically classify text documents based on their
    content, how to extract specific pieces of information from text, and how to group
    text documents about related topics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Images*—A picture is worth a thousand words, as they say. Images help us to
    understand complex concepts, capture fond memories of our last holiday, and illustrate
    current events. Language models can easily extract information from pictures.
    For instance, we will use language models to answer arbitrary questions about
    images or identify people who appear in pictures based on a database of profiles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Videos*—A large percentage of the data on the web is video data. Even on your
    smartphone, video data is probably taking up a significant part of your phone’s
    total storage capacity. In this book, we will see that language models can be
    applied to analyze videos as well: for instance, to generate suitable video titles
    based on the video content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Audio*—To many people, speech is the most natural form of communication. Audio
    recordings capture speeches and conversations and complement videos. In this book,
    we will see how to transcribe audio recordings, how to translate spoken language
    into other languages, and how to build a query interface that answers spoken questions
    about data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Tables*—Imagine a data set containing information about customers. It is natural
    to represent that data as a table, featuring columns for the customer’s address,
    phone number, and credit card information, while different rows store information
    about different customers. In this book, we will see how to use language models
    to write code that performs complex operations on such tabular data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Graphs*—From social networks to metro networks, many data sets are conveniently
    represented as graphs, modeling entities (such as people or metro stations) and
    their connections (representing friendships or metro connections). We will see
    how we can use language models to generate code that analyzes large graphs in
    various ways.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Structured vs. unstructured data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Data types are often categorized into two groups: *structured* and *unstructured
    data*. Structured data has a structure that facilitates efficient data processing
    via specialized tools. Examples of structured data include tables and graph data.
    For such data, we typically use the language model as an interface to specialized
    data-processing tools. Unstructured data, including text, images, videos, and
    audio files, does not have a structure that can be easily exploited for efficient
    processing. So, for unstructured data, we typically need to use the language model
    directly on the data.'
  prefs: []
  type: TYPE_NORMAL
- en: For most of this book, we will use OpenAI models via OpenAI’s Python library.
    Toward the end of the book, we will also discuss language models from other providers.
    As libraries from different providers tend to offer similar functionality, getting
    used to other models shouldn’t take long.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, using language models incurs monetary fees proportional to the amount
    of data being processed. The fees depend on the language model used, the model
    configuration, and the way in which the input to the language model is formulated.
    In this book, not only will you learn to solve various data-analysis tasks via
    language models, but we will discuss how to do so with minimal costs.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 How to use language models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: State-of-the-art language models are used via a method called *prompting*. We
    discuss prompting next, followed by the interfaces we can use for prompting.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.1 Prompting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Until a few years ago, machine learning models were trained for one specific
    task. For instance, we might have a model trained to classify the text of a review
    as either “positive” (i.e., the review author is satisfied) or “negative” (i.e.,
    the author is dissatisfied). To use that model, we only need the review text as
    input. There’s no need to describe the task (classifying the review) as part of
    the input because the model has been specialized to do that task and that task
    only.
  prefs: []
  type: TYPE_NORMAL
- en: This has changed in recent years with the emergence of large language models
    such as GPT. Such models are no longer trained for specific tasks. Instead, they
    are intended to serve as universal task solvers that can, in principle, solve
    any task the user desires. When using such a model, it is up to the user to describe
    to the model in precise terms what the model should do.
  prefs: []
  type: TYPE_NORMAL
- en: 'The prompt is the input to the language model. The prompt can contain multimodal
    data: for example, text and images. At a minimum, to get the language model to
    solve a specific task, the prompt should contain a text instructing the model
    on what to do. Beyond those instructions, the prompt should contain all relevant
    context. For instance, if the instructions ask the model to determine whether
    a car is visible in a picture, the prompt must also contain the picture. The instructions
    in the prompt should be specific and clarify, for instance, the expected output
    format. For example, if we want the model to output “1” if a car is present and
    “0” otherwise, enabling us to easily add the numbers generated by the model to
    count cars, we need to explicitly clarify that in the prompt (otherwise, the model
    might answer “Yes, there is a car in the picture,” which makes it harder to count
    in the post-processing stage). Besides instructions and context, the prompt may
    contain examples to help the language model understand the task.'
  prefs: []
  type: TYPE_NORMAL
- en: Few-shot vs. zero-shot learning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We can help the language model better understand a task by providing examples
    as part of the prompt. Those examples are similar to the task we want the model
    to solve and specify the input and desired output. This approach is sometimes
    called *few-shot learning*, as the model learns the task based on a few samples.
    On the other hand, we can use *zero-shot learning*, meaning the model learns the
    task without any (zero) samples based only on the task description.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.2 Example prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s illustrate prompts with an example. A classical use case for language
    models is analyzing product reviews to determine the sentiment underlying the
    review: whether the review is positive (i.e., the customer recommends the product)
    or negative (i.e., the customer is unhappy with the product). Assume that we have
    a review to classify as positive or negative. If we have a specialized model trained
    for review classification for the specific product category we’re interested in,
    all it takes is to send our review to that model. As the model is specialized
    to the target problem, it already “knows” what to do with the input and the required
    output format. However, because we use large language models, we have to provide
    a bit more context along with the review.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our prompt should contain all relevant information for the model, describing
    the task to solve and all context. In the example scenario, we probably want to
    include the following pieces of information:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Review text*—The text of the review we want to classify.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Task description*—A description of the task to solve.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Output formats*—What is the required output format?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Relevant context*—For example, are we reviewing laptops or lawn mowers?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optionally, we can include a few example reviews with their associated correct
    classification. This may help the model classify reviews more accurately.
  prefs: []
  type: TYPE_NORMAL
- en: The following prompt includes all the relevant pieces of information for an
    example review.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 1.1 Prompt for classifying a laptop review
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Context'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Task description and output format'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 First example'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Second example'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Review'
  prefs: []
  type: TYPE_NORMAL
- en: This prompt starts with a description of relevant context (**1**). Customers
    are reviewing laptops, so, for example, if they label items as “heavy,” that’s
    probably a bad sign (unlike analyzing reviews for, let’s say, steamrollers). The
    task description (**2**) tells the model what to do with the reviews and specifies
    the desired output format (output “satisfied” or “dissatisfied”) as well. Next,
    we have a list of examples. Strictly speaking, adding examples in the prompt may
    not be necessary for this simple task. However, adding examples in the prompt
    can sometimes increase the accuracy of the output. Here, we add two example reviews
    (**3** and **4**), together with the desired output for those reviews. Finally,
    we add the review (**5**) that we want the model to classify. Given the preceding
    prompt, state-of-the-art language models are likely to output “dissatisfied” when
    sent this prompt as input. That, of course, is indeed the desired output.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.3 Interfaces
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So how can we send prompts to a language model? Providers such as OpenAI typically
    offer web interfaces, enabling users to send single prompts to their language
    models. In chapter 2, we will use OpenAI’s web interface to send prompts instructing
    the model to analyze text or to write code for data processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The web interface works well as long as we send only a few prompts. However,
    analyzing a large collection of text documents would require sending many prompts
    (one per text document). Clearly, we don’t want to enter thousands of prompts
    by hand. This is where OpenAI’s Python library comes in handy. Using this library
    enables us to send prompts to OpenAI’s models directly from Python and to process
    the model’s answer in Python. This enables us to automate data loading, prompt
    generation, and any kind of post-processing we need to do on the model’s answers.
    It also allows us to integrate language models with other useful tools: for example,
    to use the language model to write code for data processing and immediately execute
    that code using other tools.'
  prefs: []
  type: TYPE_NORMAL
- en: We will review OpenAI’s Python library in chapter 3\. We will use this library
    throughout most of this book. Other providers of language models, including Google,
    Anthropic, and Cohere, offer similar Python libraries to send prompts to their
    language models. We will discuss those libraries in more detail in chapter 8.
  prefs: []
  type: TYPE_NORMAL
- en: 1.4 Using language models for data analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So how do we use language models specifically for data analysis? This book
    considers two possibilities. First, we can use the language model *directly* on
    the data. This means the language model receives the data we want to analyze as
    part of the prompt (along with instructions on which analysis to perform). Second,
    we can use the language model *indirectly* to analyze data. Here, the language
    model does not directly “see” the data: that is, we do not include the data in
    its entirety in the prompt. Instead, we use the language model to write code for
    data processing, executed in specialized data-processing tools. Which approach
    to use depends on the data properties and the task. Let’s have a closer look at
    both methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.4.1 Using language models directly on data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The most natural approach to analyzing data with language models is to put
    the data directly into the prompt. This is what we did in section [1.3.2](#sub__ExamplePrompt):
    to analyze a review, we include the review text in the prompt, along with instructions
    on what to do with the text. We can use the same approach for other types of data
    besides text. For example, when using multimodal models such as GPT-4o, we can
    simply include the pictures to analyze, together with analysis instructions, in
    the prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: Typically, we do not want to analyze a single picture or review but a whole
    collection of them. For instance, assume that we want to classify an entire collection
    of reviews, determining for each of them whether the review is positive or negative.
    In such cases, we generally take the following approach, implemented in Python
    using OpenAI’s Python library (or an equivalent library allowing users to send
    prompts to other providers’ models). We load the reviews to classify and generate
    one prompt for each review. Then, we send those prompts to the language model,
    extract the classification result from the answer generated by the model for each
    review, and save the results in a file on disk.
  prefs: []
  type: TYPE_NORMAL
- en: In this scenario, we want to solve the same task (review classification) for
    multiple text documents (i.e., reviews). As you can imagine, the prompts for different
    reviews should therefore bear some similarity. Although the text of the review
    to classify changes each time, the task description and other parts of the prompt
    remain the same.
  prefs: []
  type: TYPE_NORMAL
- en: To generate prompts in Python, we use a *prompt template*. A prompt template
    specifies a prompt associated with a specific task to solve. In our example, we
    would use a prompt template to classify reviews as positive or negative. A prompt
    template contains placeholders to represent parts of the prompt that change depending
    on the input data. Considering our prompt template for review classification,
    we should probably include a placeholder for the review text. Then, when generating
    prompts in Python, we replace that placeholder with the text of the current review
    to classify.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, we can use the following prompt template to classify reviews.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 1.2 Prompt template for classifying laptop reviews
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Context'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Task description and output format'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 First example'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Second example'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Placeholder for review text'
  prefs: []
  type: TYPE_NORMAL
- en: This prompt template generalizes the prompt we saw for classifying one specific
    review (have a look at listing [1.1](#code__ReviewPrompt) in section [1.3.2](#sub__ExamplePrompt)).
    Again, we provide context (the fact that we’re classifying laptop reviews) (**1**)
    and instructions describing the task to solve, as well as the output format (**2**).
    We also provide a few example reviews with associated classification results (**3**
    and **4**). Although the review to classify changes, depending on the input, we
    do not need to change the example reviews. Those reviews merely illustrate what
    task the language model needs to solve. Finally (**5**), we have a placeholder
    for the review text. When iterating over different reviews, we generate a prompt
    for each of them by substituting the review text for this placeholder.
  prefs: []
  type: TYPE_NORMAL
- en: The example prompt template has only a single placeholder. In general, several
    parts of the prompt may change depending on the input data. If so, we introduce
    placeholders for each of those parts and substitute all of them to generate prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Figure [1.2](#fig__directDataAnalysis) summarizes how we use prompt templates
    when analyzing data directly with language models. For each data item (e.g., a
    review to classify), we substitute for placeholders in the prompt template to
    generate a prompt (we can also say that we *instantiate* a prompt). We then send
    this prompt to the language model to solve the data-analysis task we’re interested
    in.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F02_Trummer.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.2 Using language models directly for data analysis. A prompt template
    describes the analysis task. It contains placeholders that are replaced with data
    to analyze. After substituting for the placeholders, the resulting prompt is submitted
    to the language model to produce output.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 1.4.2 Data analysis via external tools
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Putting data directly into the prompt is not always the most efficient approach.
    For some types of data, specialized tools are available that process certain operations
    on that data very efficiently. In those cases, it is often more efficient to use
    the language model to write code for data processing (rather than analyzing the
    data directly). The code generated by the language model can then be executed
    by the specialized tool.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will apply this approach to structured data. For structured data such as
    data tables and graphs, specialized data-processing tools are available that support
    a wide range of analysis operations. Those operations, such as filtering and aggregating
    data, can be performed very efficiently on structured data. Even if it was possible
    to perform the same operations reliably with language models (which is not the
    case), we would not want to do it because the fees we pay to providers like OpenAI
    are proportional to the size of the input data. Processing large structured data
    sets (such as tables with millions of rows) using language models is prohibitively
    expensive. In the following chapters, we discuss the following types of tools
    for structured data processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Relational database management system*—Stores and processes relational data:
    that is, collections of data tables. Most relational database management systems
    support *SQL*, the Structured Query Language. We will use language models to translate
    questions about data to queries in SQL.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Graph data management system*—Handles graph data representing entities and
    the relationships between them. Different graph data management systems support
    different query languages. In chapter 5, we see how to use language models to
    translate questions about data into queries in the *Cypher* language, supported
    by the Neo4j graph data management system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For instance, let’s assume we want to enable lay users ompt template for translating
    questto analyze a relational database: that is, a collection of data tables. Perhaps
    a table contains the results of a survey, and we want to let users aggregate answers
    from different groups of respondents. The survey results are stored in a relational
    database management system (the most suitable type of tool for this data type).
    Using language models, we can enable users to ask questions about the data in
    natural language (that is, in plain English). The language model takes care of
    translating those questions into formal queries. More precisely, given that the
    data is stored in a relational database management system, we want to translate
    those questions into SQL queries.'
  prefs: []
  type: TYPE_NORMAL
- en: Again, we introduce a prompt template for the task we’re interested in. Here,
    we’re interested in text-to-SQL translation, meaning we want to use the language
    model to translate questions in natural language to SQL queries. Although the
    task (text-to-SQL translation) and the data (the database containing survey results)
    remain fixed, the user’s questions will change over time. Therefore, we introduce
    a placeholder for the user question in our prompt template. In principle, the
    following prompt template should enable us to translate questions on our survey
    data into SQL queries.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 1.3 Prompt template for translating questions to SQL
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Description of database'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Question to translate'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Task description'
  prefs: []
  type: TYPE_NORMAL
- en: 'First the prompt describes the structure of our data (**1**). This is required
    to enable the system to write correct queries (e.g., queries that refer to the
    correct names of tables and columns in those tables). The description in the example
    template is abbreviated. We will see how to accurately describe the structure
    of a relational database in later chapters. Next, the prompt template contains
    the question to translate (**2**). This is a placeholder to enable users to ask
    different questions using the same prompt template. Finally, the prompt template
    contains a (concise) task description (**3**): we want to translate questions
    to SQL queries!'
  prefs: []
  type: TYPE_NORMAL
- en: Figure [1.3](#fig__textToSQL) summarizes the process for text-to-SQL translation.
    Given a corresponding prompt template, we substitute the user question for the
    placeholder, translate the question to an SQL query via the language model, and
    finally execute the query in a relational database management system. The query
    result is shown to the user.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F03_Trummer.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.3 Using language models indirectly to build a natural language interface
    for tabular data. The prompt template contains placeholders for questions about
    data. After substituting for placeholders, the resulting prompt is used as input
    for the language model. The model translates the question into an SQL query that
    is executed via a relational database management system.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 1.5 Minimizing costs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When processing data with language models, we typically pay fees to a model
    provider. The larger the amount of data we process, the higher the fees. Before
    analyzing large amounts of data, we want to make sure we’re not overpaying. For
    instance, using larger language models (the neural network implementing the language
    model has more “neurons,” so to speak) is often more expensive, but for complex
    tasks, it may pay off with higher-quality results. But if the large model is not
    needed to solve our current task well, we should save the money and use a smaller
    model. Fortunately, there are quite a few ways in which we can optimize the tradeoff
    between processing costs and result quality. We discuss the different options
    next. All of them are covered in more detail in later book chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 1.5.1 Picking the best model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OpenAI offers many different versions of the GPT model, ranging from relatively
    small models to giant models like GPT-4\. At the time of writing, using GPT-4
    is over 100 times more expensive, per input token, than using the cheapest version.
  prefs: []
  type: TYPE_NORMAL
- en: What are tokens?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The processing fees for language models like GPT-4 are proportional to the number
    of tokens read and generated by the model. A *token* is the atomic unit at which
    the language model represents text internally. Typically, one token corresponds
    to approximately four characters.
  prefs: []
  type: TYPE_NORMAL
- en: Given those price differences, it is clearly a good idea to think hard about
    which specific model satisfies our needs. For instance, for a simple task like
    review classification, we probably don’t need to use OpenAI’s most expensive model.
    But if we want to use the model to write complex code for data processing, using
    the most expensive version may be worth it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, we don’t need to restrict ourselves to models offered by OpenAI.
    Language models are offered by many providers, including Google, Anthropic, and
    Cohere. In principle, we might even choose to host our own model, using models
    that are publicly available: for example, on the Hugging Face platform. Some of
    those models are generic (similar to OpenAI’s GPT models), whereas others are
    trained for more specific tasks. If we happen to be interested in tasks for which
    specialized models exist, we may want to use one of them. We discuss models from
    other providers in more detail in chapter 8.'
  prefs: []
  type: TYPE_NORMAL
- en: Picking the right model for your needs is not an easy task. As a first step,
    you might want to look at benchmarks such as Stanford’s Holistic Evaluation of
    Language Models (HELM, [https://crfm.stanford.edu/helm/](https://crfm.stanford.edu/helm/);
    see figure [1.4](#fig__helm)). This benchmark compares the quality of results
    produced by different language models on different types of tasks. Ultimately,
    you may have to try a few models on your task and a data sample to ensure that
    you choose the optimal one. In chapter 9, we will see how to benchmark different
    models systematically for an example task.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F04_Trummer.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.4 Holistic Evaluation of Language Models (HELM): comparing language
    models offered by different providers according to various metrics'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 1.5.2 Optimally configuring models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The OpenAI Python library offers a variety of tuning parameters to influence
    model behavior. For instance, we can influence the probability that certain words
    appear in the output of a model. This can be useful, for instance, when classifying
    reviews. If the output of the model should be one of only a few possible choices
    (such as “positive” and “negative”), it makes sense to restrict possible outputs
    to those choices. That way, we avoid cases in which the model generates output
    that does not correspond to any of the class names. To take another example, we
    can fine-tune the criteria used to decide when the model stops generating output.
    For instance, if we know that the output should consist of a single token (e.g.,
    the name of a class when classifying reviews), we can explicitly limit the output
    length to a single token. This prevents the model from generating more output
    than necessary (saving us money in the process, as costs depend on the amount
    of output generated).
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss those and many other tuning parameters in more detail in chapter
    3\. In chapter 9, we will see how to use those tuning parameters to get better
    performance from our language models.
  prefs: []
  type: TYPE_NORMAL
- en: Another option to configure models is to fine-tune them. This means, essentially,
    that we’re creating our own variant of an existing model. By training the model
    with a small amount of task-specific training data, we get a model that potentially
    performs better at our task than the vanilla version. For instance, if we want
    to classify reviews, we might train the model with a few hundred example reviews
    and associated classification results. This may enable us to use a much smaller
    and cheaper model, fine-tuned for our specific task, that performs as well on
    this task as a much larger model that has not been fine-tuned.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, fine-tuning also costs money, and it may not be immediately clear
    whether it is worth it for a specific task. We discuss fine-tuning and the associated
    tradeoffs in more detail in chapter 9.
  prefs: []
  type: TYPE_NORMAL
- en: 1.5.3 Prompt engineering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The prompt template can significantly affect the quality of the results produced
    by the language model. A good prompt template clearly specifies the task to solve
    and provides all relevant context. We will see how to map various tasks to suitable
    prompt templates throughout the following chapters, covering a variety of data
    types. After working through those examples, you should be able to design your
    own prompt templates for novel tasks, following the same principles.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the model choice, it can be hard to pick the best prompt template
    for a given task without doing any testing. In chapter 9, we will test prompt
    templates in an example scenario and illustrate how different prompt templates
    lead to different outcomes. In some cases, investing a little time in finding
    the best prompt template may enable you to get satisfactory performance with fairly
    cheap models (whereas working with the unoptimized prompt template may make a
    more expensive model necessary).
  prefs: []
  type: TYPE_NORMAL
- en: Where to get prompt templates
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Finding a good prompt template for a new task may take some time. If you do
    not want to spend that time, have somebody else do it for you! More precisely,
    you can find platforms on the web that enable users to buy and sell prompt templates.
    One of them is PromptBase ([https://promptbase.com](https://promptbase.com)).
    Say you want to translate English questions into SQL queries. By entering corresponding
    keywords, you will find not one but multiple alternative prompt templates on that
    platform. If the prompt template seems like a good match based on the associated
    description, you can buy it and use it for your data-analysis needs.
  prefs: []
  type: TYPE_NORMAL
- en: 1.6 Advanced software frameworks and agents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Throughout most of this book, we will use OpenAI’s Python library and similar
    libraries from other providers. For instance, these libraries enable you to send
    prompts to language models and receive the models’ answers. Although they are
    entirely sufficient for many use cases, you may want to consider more advanced
    software frameworks when developing complex applications that are based on language
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this book, we discuss two advanced software frameworks for working with
    language models: LangChain ([https://langchain.com](https://langchain.com)) and
    LlamaIndex ([www.llamain](http://www.llamaindex.ai)[dex.ai](http://www.llamaindex.ai)).
    Both make it easier to develop Python applications for data analysis with language
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: Besides many other features, these frameworks make it easy to create agents
    that use language models. This approach is useful for complex data-analysis tasks
    requiring, for instance, combining data from multiple sources. For most of this
    book, we solve data-analysis tasks with a single invocation of the language model,
    whether it is analyzing a text document or translating a question about data to
    a formal query. If the task requires multiple steps, such as performing preprocessing
    before calling the language model or post-processing on the model’s answer, we
    must hard-code the corresponding processing logic.
  prefs: []
  type: TYPE_NORMAL
- en: This approach works as long as we can reliably predict the sequence of steps
    required for data processing. However, in some cases, it can be difficult to predict
    which steps are required. For instance, we may get questions from users that refer
    either to a text document or to a relational database. So, depending on the question,
    we need to either write an SQL query or extract information from text documents.
    Or perhaps we might need information from both the text and the relational database,
    extracting information related to the question from the text and then using the
    information we obtain to formulate an SQL query.
  prefs: []
  type: TYPE_NORMAL
- en: 'In such cases, it is not possible to hard-code all possible sequences of steps
    in advance. Instead, we want to design an approach that is flexible enough to
    decide independently what step is required next. This can be done using agents
    and language models. With this approach, the language model is used to decompose
    complex analysis tasks into subproblems. Furthermore, the language model may choose
    to invoke *tools*: arbitrary functions whose interfaces are described in natural
    language. Such tools can, for instance, encapsulate the invocation of an SQL query
    on a relational database. After invoking a corresponding tool, the language model
    is given access to the invocation result (e.g., the query result) and can use
    that result to plan the next steps. We will see how to use agents to solve complex
    data-analysis tasks where it is unclear, a priori, which data sources and processing
    methods are required to solve them.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Language models can solve novel tasks without specialized training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The prompt is the input to the language model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompts may combine text with other types of data, such as images.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A prompt contains a task description, context, and (optionally) examples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language models can analyze certain types of data directly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When analyzing data directly, the data must appear in the prompt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prompt templates contain placeholders: for example, to represent data items.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By substituting for placeholders in a prompt template, we obtain a prompt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language models can also help to analyze data via external tools.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language models can instruct other tools on how to process data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Models are available in many different sizes with significant cost differences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Models can be configured using various configuration parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LangChain and LlamaIndex help to develop complex applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agents use language models to solve complex problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
