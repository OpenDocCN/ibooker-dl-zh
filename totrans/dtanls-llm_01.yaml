- en: 1 Analyzing data with large language models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 使用大语言模型分析数据
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: An introduction to language models
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言模型简介
- en: Data analysis with language models
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用语言模型进行数据分析
- en: Using language models efficiently
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高效使用语言模型
- en: Language models are powerful neural networks that can be used for various data-processing
    tasks. This chapter introduces language models and shows how and why to use them
    for data analysis.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型是强大的神经网络，可用于各种数据处理任务。本章介绍了语言模型，并展示了如何以及为什么使用它们进行数据分析。
- en: 1.1 What can language models do?
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 语言模型能做什么？
- en: 'We will start this section with a little poem and an associated picture (figure
    [1.1](#fig__ai4data)) connecting the two main topics of this book, data analysis
    and large language models:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从这个小诗和相关的图片（图[1.1](#fig__ai4data)）开始本节，这两个主题是本书的两个主要主题，数据分析和大语言模型：
- en: '*In the silent hum of the server’s light,'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*在服务器灯光的寂静嗡嗡声中，'
- en: Data flows through the veins of night.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 数据在夜晚的血管中流淌。
- en: Rows and columns, a structured sea,
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 行和列，一个有结构的海洋，
- en: With stories hidden, waiting to be free.*
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏着故事，等待被释放。
- en: '*Each number sings of pasts untold,'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*每个数字都在唱着未讲述的过去，'
- en: Trends and truths in patterns bold.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在明显的模式中找到趋势和真理。
- en: And here arrives a curious friend,
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 而现在，一个好奇的朋友到来，
- en: A language model, eager to comprehend.*
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一个渴望理解的语言模型。
- en: '*It listens close, with circuits keen,'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*它倾听得非常近，电路敏锐，'
- en: To turn raw facts into insight unseen.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 将原始事实转化为未见过的洞察。
- en: From scatter plots to sentences clear,
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 从散点图到清晰的句子，
- en: Data’s language is all it can hear.*
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的语言是它能听到的全部。
- en: '*The figures dance, the texts reply,'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*图表起舞，文本回应，'
- en: As code meets meaning under AI’s eye.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当代码在人工智能的注视下遇到意义。
- en: They merge their worlds, a seamless blend,
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 他们融合了他们的世界，无缝的融合，
- en: Where logic and language have no end.*
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在逻辑和语言没有尽头的地方。
- en: '*For in this bond, both deep and wide,'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*因为在这个紧密而广泛的关系中，'
- en: Data’s essence finds a guide.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的本质找到了一个向导。
- en: And in the neural net’s embrace,
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络怀抱中，
- en: Data analysis gains a poetic grace.*
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析获得了诗意的优雅。
- en: '![figure](../Images/CH01_F01_Trummer.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH01_F01_Trummer.png)'
- en: Figure 1.1 Illustration by GPT-4o, connecting the topics “data analysis” and
    “large language models”
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.1 GPT-4o的插图，将“数据分析”和“大语言模型”这两个主题联系起来
- en: The poem and the picture were generated by GPT-4o (“o” for “omni”), a language
    model by OpenAI that processes multimodal data, based solely on the instructions
    “Write a poem connecting data analysis and large language models!” followed by
    “Now draw a corresponding picture!” Both the picture and the poem seem to relate
    to the requested topics. Although the poem may not win any literature awards,
    its text is coherent, it is structured as we would expect from a poem, and it
    rhymes! Perhaps most importantly, all it took to generate the poem and the picture
    were short instructions expressed in natural language. Whereas prior machine learning
    methods relied on large amounts of task-specific training data, this requirement
    is now obsolete. And, of course, the task is specific enough to convince us that
    the language model is not copying existing solutions from the web and generates
    original content instead.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这首诗和这幅画都是由 GPT-4o（“o”代表“omni”）生成的，OpenAI 的一个语言模型，它处理多模态数据，仅基于指令“写一首将数据分析和大语言模型联系起来的诗！”然后是“现在画一幅相应的画！”这幅画和这首诗似乎都与请求的主题相关。尽管这首诗可能不会赢得任何文学奖项，但它的文本是连贯的，它按照我们期望的诗的结构来组织，并且押韵！也许最重要的是，生成这首诗和这幅画只需要用自然语言表达的简短指令。而之前的机器学习方法依赖于大量特定任务的训练数据，这种需求现在已经过时了。当然，任务足够具体，足以让我们相信语言模型不是从网络上复制现有的解决方案，而是生成原创内容。
- en: Writing poems and generating pictures are only two of many possible use cases
    (albeit possibly the most entertaining ones). Models like GPT-4o can solve various
    tasks, such as summarizing text documents, writing program code, and answering
    questions about pictures. In this book, you will learn how to use language models
    to accomplish a plethora of data-analysis tasks ranging from extracting information
    from large collections of text documents to writing code for data analysis. After
    reading this book, you will be able to quickly build data-analysis pipelines that
    are based on language models and extract useful insights from a variety of data
    formats.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 写诗和生成图片只是许多可能的用例（尽管可能是最有趣的一些）。像GPT-4o这样的模型可以解决各种任务，例如总结文本文档、编写程序代码以及回答关于图片的问题。在这本书中，你将学习如何使用语言模型完成从从大量文本文档中提取信息到为数据分析编写代码的各种数据分析任务。阅读完这本书后，你将能够快速构建基于语言模型的数据分析管道，并从各种数据格式中提取有用的见解。
- en: What does GPT stand for?
  id: totrans-32
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: GPT代表什么？
- en: '*GPT* stands for *Generative Pretrained Transformer*.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*GPT*代表*生成式预训练变换器*。'
- en: '*Generative*: GPT is a large neural network that generates content (e.g., text
    or code) in response to input text. This fact distinguishes it from other neural
    networks that, for example, can only classify input text into a fixed set of predefined
    categories.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*生成式*：GPT是一个大型神经网络，可以根据输入文本生成内容（例如文本或代码）。这一事实使其与其他神经网络区分开来，例如，其他神经网络只能将输入文本分类到一组预定义的固定类别中。'
- en: '*Pretrained*: GPT is pretrained on large amounts of data, solving generic tasks
    such as predicting the next word in text. Typically, the pretraining task is different
    from the tasks it is primarily used for. However, pretraining helps it learn more
    specialized tasks faster.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*预训练*：GPT在大量的数据上进行了预训练，解决诸如预测文本中的下一个单词等通用任务。通常，预训练任务与其主要使用的任务不同。然而，预训练有助于它更快地学习更专业的任务。'
- en: The *Transformer* is a new neural network architecture that is particularly
    useful for learning tasks that involve variable-length input or output (such as
    text documents). It is currently the dominant architecture for generative AI approaches.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*Transformer*是一种新的神经网络架构，特别适用于涉及可变长度输入或输出的学习任务（例如文本文档）。它目前是生成式人工智能方法的占主导地位的架构。'
- en: 1.2 What you will learn
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 你将学到什么
- en: This book is about using language models for data analysis. We can categorize
    data-analysis tasks by the type of data we’re analyzing and by the type of analysis.
    This book covers a wide range of data types and analysis tasks.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书是关于使用语言模型进行数据分析的。我们可以根据我们分析的数据类型和分析类型来对数据分析任务进行分类。这本书涵盖了广泛的数据类型和分析任务。
- en: 'We focus on *multimodal* data analysis: that is, we use language models to
    analyze various types of data. More precisely, we cover the following data types
    in this book:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们专注于*多模态*数据分析：也就是说，我们使用语言模型来分析各种类型的数据。更确切地说，这本书中涵盖了以下数据类型：
- en: '*Text*—Think of emails, newspaper articles, and comments on a web forum. Text
    data is ubiquitous and contains valuable information. In this book, we will see
    how to use language models to automatically classify text documents based on their
    content, how to extract specific pieces of information from text, and how to group
    text documents about related topics.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*文本*——想想电子邮件、报纸文章和网上的论坛评论。文本数据无处不在，包含有价值的信息。在这本书中，我们将看到如何使用语言模型根据其内容自动分类文本文档，如何从文本中提取特定信息，以及如何将关于相关主题的文本文档分组。'
- en: '*Images*—A picture is worth a thousand words, as they say. Images help us to
    understand complex concepts, capture fond memories of our last holiday, and illustrate
    current events. Language models can easily extract information from pictures.
    For instance, we will use language models to answer arbitrary questions about
    images or identify people who appear in pictures based on a database of profiles.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图像*——正如人们所说，“一图胜千言”。图像帮助我们理解复杂的概念，捕捉我们上次假期的美好回忆，以及描绘当前事件。语言模型可以轻松地从图片中提取信息。例如，我们将使用语言模型来回答关于图像的任意问题或根据数据库中的个人资料识别图片中出现的人。'
- en: '*Videos*—A large percentage of the data on the web is video data. Even on your
    smartphone, video data is probably taking up a significant part of your phone’s
    total storage capacity. In this book, we will see that language models can be
    applied to analyze videos as well: for instance, to generate suitable video titles
    based on the video content.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*视频*——网络上的大量数据是视频数据。即使在你的智能手机上，视频数据可能也占据了手机总存储容量的很大一部分。在这本书中，我们将看到语言模型也可以应用于分析视频：例如，根据视频内容生成合适的视频标题。'
- en: '*Audio*—To many people, speech is the most natural form of communication. Audio
    recordings capture speeches and conversations and complement videos. In this book,
    we will see how to transcribe audio recordings, how to translate spoken language
    into other languages, and how to build a query interface that answers spoken questions
    about data.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*音频*——对许多人来说，语音是最自然的交流形式。音频记录捕捉演讲和对话，并补充视频。在这本书中，我们将看到如何转录音频记录，如何将口语翻译成其他语言，以及如何构建一个回答关于数据的语音问题的查询界面。'
- en: '*Tables*—Imagine a data set containing information about customers. It is natural
    to represent that data as a table, featuring columns for the customer’s address,
    phone number, and credit card information, while different rows store information
    about different customers. In this book, we will see how to use language models
    to write code that performs complex operations on such tabular data.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*表格*——想象一个包含客户信息的数据库。将数据表示为表格是自然的，其中包含客户地址、电话号码和信用卡信息的列，而不同的行存储有关不同客户的信息。在这本书中，我们将看到如何使用语言模型编写执行此类表格数据复杂操作的代码。'
- en: '*Graphs*—From social networks to metro networks, many data sets are conveniently
    represented as graphs, modeling entities (such as people or metro stations) and
    their connections (representing friendships or metro connections). We will see
    how we can use language models to generate code that analyzes large graphs in
    various ways.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图*——从社交网络到地铁网络，许多数据集都方便地表示为图，模拟实体（如人或地铁站）及其连接（代表友谊或地铁连接）。我们将看到如何使用语言模型生成分析大型图的代码。'
- en: Structured vs. unstructured data
  id: totrans-46
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 结构化数据与非结构化数据
- en: 'Data types are often categorized into two groups: *structured* and *unstructured
    data*. Structured data has a structure that facilitates efficient data processing
    via specialized tools. Examples of structured data include tables and graph data.
    For such data, we typically use the language model as an interface to specialized
    data-processing tools. Unstructured data, including text, images, videos, and
    audio files, does not have a structure that can be easily exploited for efficient
    processing. So, for unstructured data, we typically need to use the language model
    directly on the data.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 数据类型通常分为两组：*结构化数据*和*非结构化数据*。结构化数据具有结构，便于通过专用工具进行高效数据处理。结构化数据的例子包括表格和图数据。对于此类数据，我们通常将语言模型用作访问专用数据处理工具的接口。非结构化数据，包括文本、图像、视频和音频文件，没有结构，不能轻易用于高效处理。因此，对于非结构化数据，我们通常需要在数据上直接使用语言模型。
- en: For most of this book, we will use OpenAI models via OpenAI’s Python library.
    Toward the end of the book, we will also discuss language models from other providers.
    As libraries from different providers tend to offer similar functionality, getting
    used to other models shouldn’t take long.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的大部分内容中，我们将通过OpenAI的Python库使用OpenAI模型。在本书的结尾，我们还将讨论来自其他提供商的语言模型。由于不同提供商的库通常提供类似的功能，适应其他模型不应花费太多时间。
- en: Typically, using language models incurs monetary fees proportional to the amount
    of data being processed. The fees depend on the language model used, the model
    configuration, and the way in which the input to the language model is formulated.
    In this book, not only will you learn to solve various data-analysis tasks via
    language models, but we will discuss how to do so with minimal costs.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，使用语言模型会产生与处理的数据量成比例的货币费用。费用取决于所使用的语言模型、模型配置以及语言模型输入的表述方式。在这本书中，你不仅将学习通过语言模型解决各种数据分析任务，我们还将讨论如何以最低的成本完成这些任务。
- en: 1.3 How to use language models
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 如何使用语言模型
- en: State-of-the-art language models are used via a method called *prompting*. We
    discuss prompting next, followed by the interfaces we can use for prompting.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最先进的语言模型通过一种称为*提示（prompting）*的方法使用。我们将在接下来讨论提示，然后是我们可以用于提示的接口。
- en: 1.3.1 Prompting
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.1 提示（Prompting）
- en: Until a few years ago, machine learning models were trained for one specific
    task. For instance, we might have a model trained to classify the text of a review
    as either “positive” (i.e., the review author is satisfied) or “negative” (i.e.,
    the author is dissatisfied). To use that model, we only need the review text as
    input. There’s no need to describe the task (classifying the review) as part of
    the input because the model has been specialized to do that task and that task
    only.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 直到几年前，机器学习模型都是针对一个特定任务进行训练的。例如，我们可能有一个模型被训练来将评论文本分类为“正面”（即，评论作者满意）或“负面”（即，作者不满意）。要使用该模型，我们只需要评论文本作为输入。不需要将任务（分类评论）作为输入的一部分进行描述，因为该模型已经专门化来执行该任务，并且仅执行该任务。
- en: This has changed in recent years with the emergence of large language models
    such as GPT. Such models are no longer trained for specific tasks. Instead, they
    are intended to serve as universal task solvers that can, in principle, solve
    any task the user desires. When using such a model, it is up to the user to describe
    to the model in precise terms what the model should do.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，随着大型语言模型如GPT的出现，这种情况发生了变化。这些模型不再针对特定任务进行训练。相反，它们旨在作为通用任务求解器，原则上可以解决用户想要的任何任务。当使用此类模型时，用户需要精确地告诉模型模型应该做什么。
- en: 'The prompt is the input to the language model. The prompt can contain multimodal
    data: for example, text and images. At a minimum, to get the language model to
    solve a specific task, the prompt should contain a text instructing the model
    on what to do. Beyond those instructions, the prompt should contain all relevant
    context. For instance, if the instructions ask the model to determine whether
    a car is visible in a picture, the prompt must also contain the picture. The instructions
    in the prompt should be specific and clarify, for instance, the expected output
    format. For example, if we want the model to output “1” if a car is present and
    “0” otherwise, enabling us to easily add the numbers generated by the model to
    count cars, we need to explicitly clarify that in the prompt (otherwise, the model
    might answer “Yes, there is a car in the picture,” which makes it harder to count
    in the post-processing stage). Besides instructions and context, the prompt may
    contain examples to help the language model understand the task.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 提示是语言模型的输入。提示可以包含多模态数据：例如，文本和图像。至少，为了使语言模型解决一个特定任务，提示应该包含一个文本，指导模型要做什么。除了这些指令之外，提示还应包含所有相关上下文。例如，如果指令要求模型判断图片中是否可见汽车，提示必须也包含该图片。提示中的指令应该是具体和明确的，例如，预期的输出格式。例如，如果我们希望模型在存在汽车时输出“1”，不存在时输出“0”，以便我们能够轻松地将模型生成的数字添加到计数汽车中，我们需要在提示中明确说明这一点（否则，模型可能会回答“是的，图片中有汽车”，这在后处理阶段会使计数更困难）。除了指令和上下文之外，提示还可以包含示例，以帮助语言模型理解任务。
- en: Few-shot vs. zero-shot learning
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 少样本学习与零样本学习
- en: We can help the language model better understand a task by providing examples
    as part of the prompt. Those examples are similar to the task we want the model
    to solve and specify the input and desired output. This approach is sometimes
    called *few-shot learning*, as the model learns the task based on a few samples.
    On the other hand, we can use *zero-shot learning*, meaning the model learns the
    task without any (zero) samples based only on the task description.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在提示中提供示例来帮助语言模型更好地理解一个任务。这些示例与我们要模型解决的任务相似，并指定了输入和期望的输出。这种方法有时被称为*少样本学习*，因为模型基于少量样本来学习任务。另一方面，我们可以使用*零样本学习*，这意味着模型仅基于任务描述（没有样本）来学习任务。
- en: 1.3.2 Example prompt
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.2 示例提示
- en: 'Let’s illustrate prompts with an example. A classical use case for language
    models is analyzing product reviews to determine the sentiment underlying the
    review: whether the review is positive (i.e., the customer recommends the product)
    or negative (i.e., the customer is unhappy with the product). Assume that we have
    a review to classify as positive or negative. If we have a specialized model trained
    for review classification for the specific product category we’re interested in,
    all it takes is to send our review to that model. As the model is specialized
    to the target problem, it already “knows” what to do with the input and the required
    output format. However, because we use large language models, we have to provide
    a bit more context along with the review.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来说明提示。语言模型的一个经典用例是分析产品评论，以确定评论背后的情感：评论是积极的（即，客户推荐该产品）还是消极的（即，客户对产品不满意）。假设我们有一个要分类为积极或消极的评论。如果我们有一个针对我们感兴趣的特定产品类别训练的专门用于评论分类的模型，我们只需要将我们的评论发送到该模型。由于模型专门针对目标问题，它已经“知道”如何处理输入和所需的输出格式。然而，由于我们使用大型语言模型，我们必须在评论中提供更多背景信息。
- en: 'Our prompt should contain all relevant information for the model, describing
    the task to solve and all context. In the example scenario, we probably want to
    include the following pieces of information:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的提示应该包含模型所需的所有相关信息，描述要解决的问题和所有背景。在示例场景中，我们可能希望包括以下信息：
- en: '*Review text*—The text of the review we want to classify.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*评论文本*——我们要分类的评论的文本。'
- en: '*Task description*—A description of the task to solve.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*任务描述*——要解决的问题的描述。'
- en: '*Output formats*—What is the required output format?'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输出格式*——所需的输出格式是什么？'
- en: '*Relevant context*—For example, are we reviewing laptops or lawn mowers?'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*相关背景*——例如，我们是在审查笔记本电脑还是割草机？'
- en: Optionally, we can include a few example reviews with their associated correct
    classification. This may help the model classify reviews more accurately.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，我们可以包括一些带有相关正确分类的示例评论。这可能有助于模型更准确地分类评论。
- en: The following prompt includes all the relevant pieces of information for an
    example review.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 以下提示包含了一个示例评论的所有相关信息的全部。
- en: Listing 1.1 Prompt for classifying a laptop review
  id: totrans-67
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表1.1 对笔记本电脑评论进行分类的提示
- en: '[PRE0]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 Context'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 背景'
- en: '#2 Task description and output format'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 任务描述和输出格式'
- en: '#3 First example'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 第一个示例'
- en: '#4 Second example'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 第二个示例'
- en: '#5 Review'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 评论'
- en: This prompt starts with a description of relevant context (**1**). Customers
    are reviewing laptops, so, for example, if they label items as “heavy,” that’s
    probably a bad sign (unlike analyzing reviews for, let’s say, steamrollers). The
    task description (**2**) tells the model what to do with the reviews and specifies
    the desired output format (output “satisfied” or “dissatisfied”) as well. Next,
    we have a list of examples. Strictly speaking, adding examples in the prompt may
    not be necessary for this simple task. However, adding examples in the prompt
    can sometimes increase the accuracy of the output. Here, we add two example reviews
    (**3** and **4**), together with the desired output for those reviews. Finally,
    we add the review (**5**) that we want the model to classify. Given the preceding
    prompt, state-of-the-art language models are likely to output “dissatisfied” when
    sent this prompt as input. That, of course, is indeed the desired output.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个提示从相关背景描述开始（**1**）。客户正在审查笔记本电脑，例如，如果他们将物品标记为“重”，这可能是一个坏信号（与分析蒸汽压路机的评论不同）。任务描述（**2**）告诉模型如何处理评论，并指定所需的输出格式（输出“满意”或“不满意”）。接下来，我们有一个示例列表。严格来说，在这个简单任务中，在提示中添加示例可能不是必要的。然而，在提示中添加示例有时可以提高输出的准确性。在这里，我们添加了两个示例评论（**3**和**4**），以及这些评论的期望输出。最后，我们添加了我们要模型分类的评论（**5**）。鉴于前面的提示，最先进的语言模型在接收到这个提示作为输入时，很可能会输出“不满意”。当然，这正是我们期望的输出。
- en: 1.3.3 Interfaces
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.3 接口
- en: So how can we send prompts to a language model? Providers such as OpenAI typically
    offer web interfaces, enabling users to send single prompts to their language
    models. In chapter 2, we will use OpenAI’s web interface to send prompts instructing
    the model to analyze text or to write code for data processing.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何向语言模型发送提示呢？像OpenAI这样的提供商通常提供Web界面，使用户能够向他们的语言模型发送单个提示。在第2章中，我们将使用OpenAI的Web界面来发送提示，指示模型分析文本或为数据处理编写代码。
- en: 'The web interface works well as long as we send only a few prompts. However,
    analyzing a large collection of text documents would require sending many prompts
    (one per text document). Clearly, we don’t want to enter thousands of prompts
    by hand. This is where OpenAI’s Python library comes in handy. Using this library
    enables us to send prompts to OpenAI’s models directly from Python and to process
    the model’s answer in Python. This enables us to automate data loading, prompt
    generation, and any kind of post-processing we need to do on the model’s answers.
    It also allows us to integrate language models with other useful tools: for example,
    to use the language model to write code for data processing and immediately execute
    that code using other tools.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 只要我们只发送几个提示，Web界面就能很好地工作。然而，分析大量文本文档就需要发送许多提示（每个文本文档一个）。显然，我们不想手动输入数千个提示。这就是OpenAI的Python库派上用场的地方。使用这个库，我们可以直接从Python发送提示到OpenAI的模型，并在Python中处理模型的答案。这使得我们能够自动化数据加载、提示生成以及我们需要在模型答案上进行的任何后处理。它还允许我们将语言模型与其他有用的工具集成：例如，使用语言模型编写数据处理代码，并立即使用其他工具执行该代码。
- en: We will review OpenAI’s Python library in chapter 3\. We will use this library
    throughout most of this book. Other providers of language models, including Google,
    Anthropic, and Cohere, offer similar Python libraries to send prompts to their
    language models. We will discuss those libraries in more detail in chapter 8.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第3章中回顾OpenAI的Python库。我们将在这本书的大部分内容中使用这个库。其他语言模型的提供者，包括Google、Anthropic和Cohere，也提供了类似的Python库来向他们的语言模型发送提示。我们将在第8章中更详细地讨论这些库。
- en: 1.4 Using language models for data analysis
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4 使用语言模型进行数据分析
- en: 'So how do we use language models specifically for data analysis? This book
    considers two possibilities. First, we can use the language model *directly* on
    the data. This means the language model receives the data we want to analyze as
    part of the prompt (along with instructions on which analysis to perform). Second,
    we can use the language model *indirectly* to analyze data. Here, the language
    model does not directly “see” the data: that is, we do not include the data in
    its entirety in the prompt. Instead, we use the language model to write code for
    data processing, executed in specialized data-processing tools. Which approach
    to use depends on the data properties and the task. Let’s have a closer look at
    both methods.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何具体使用语言模型进行数据分析呢？这本书考虑了两种可能性。首先，我们可以直接在数据上使用语言模型。这意味着语言模型将我们想要分析的数据作为提示的一部分接收（包括要执行的分析说明）。其次，我们可以间接使用语言模型来分析数据。在这里，语言模型并不直接“看到”数据：也就是说，我们不会将数据全部包含在提示中。相反，我们使用语言模型编写数据处理代码，这些代码在专门的数据处理工具中执行。使用哪种方法取决于数据属性和任务。让我们更详细地看看这两种方法。
- en: 1.4.1 Using language models directly on data
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.1 直接在数据上使用语言模型
- en: 'The most natural approach to analyzing data with language models is to put
    the data directly into the prompt. This is what we did in section [1.3.2](#sub__ExamplePrompt):
    to analyze a review, we include the review text in the prompt, along with instructions
    on what to do with the text. We can use the same approach for other types of data
    besides text. For example, when using multimodal models such as GPT-4o, we can
    simply include the pictures to analyze, together with analysis instructions, in
    the prompt.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用语言模型分析数据的最佳方法是将数据直接放入提示中。这正是我们在[1.3.2](#sub__ExamplePrompt)节中做的事情：为了分析一条评论，我们在提示中包含了评论文本，以及关于如何处理文本的说明。我们也可以用同样的方法处理除了文本之外的其他类型的数据。例如，当使用多模态模型如GPT-4o时，我们可以在提示中简单地包含要分析的图片，以及分析说明。
- en: Typically, we do not want to analyze a single picture or review but a whole
    collection of them. For instance, assume that we want to classify an entire collection
    of reviews, determining for each of them whether the review is positive or negative.
    In such cases, we generally take the following approach, implemented in Python
    using OpenAI’s Python library (or an equivalent library allowing users to send
    prompts to other providers’ models). We load the reviews to classify and generate
    one prompt for each review. Then, we send those prompts to the language model,
    extract the classification result from the answer generated by the model for each
    review, and save the results in a file on disk.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们不想分析单个图片或评论，而是一组图片或评论。例如，假设我们想要分类整个评论集合，确定每个评论是正面还是负面。在这种情况下，我们通常采取以下方法，使用
    Python 和 OpenAI 的 Python 库（或允许用户向其他提供商的模型发送提示的等效库）实现。我们加载要分类的评论，并为每个评论生成一个提示。然后，我们将这些提示发送给语言模型，从模型为每个评论生成的答案中提取分类结果，并将结果保存到磁盘上的文件中。
- en: In this scenario, we want to solve the same task (review classification) for
    multiple text documents (i.e., reviews). As you can imagine, the prompts for different
    reviews should therefore bear some similarity. Although the text of the review
    to classify changes each time, the task description and other parts of the prompt
    remain the same.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们想要为多个文本文档（即评论）解决相同的任务（评论分类）。正如你可以想象的那样，因此不同的评论提示应该有一些相似之处。尽管每次要分类的评论文本都会变化，但任务描述和提示的其他部分保持不变。
- en: To generate prompts in Python, we use a *prompt template*. A prompt template
    specifies a prompt associated with a specific task to solve. In our example, we
    would use a prompt template to classify reviews as positive or negative. A prompt
    template contains placeholders to represent parts of the prompt that change depending
    on the input data. Considering our prompt template for review classification,
    we should probably include a placeholder for the review text. Then, when generating
    prompts in Python, we replace that placeholder with the text of the current review
    to classify.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中生成提示时，我们使用一个 *提示模板*。提示模板指定与特定任务关联的提示。在我们的例子中，我们将使用提示模板来分类评论是正面还是负面。提示模板包含占位符，以表示根据输入数据而变化的提示的部分。考虑到我们的评论分类提示模板，我们可能需要包括一个用于评论文本的占位符。然后，在
    Python 中生成提示时，我们将用当前要分类的评论文本替换该占位符。
- en: For instance, we can use the following prompt template to classify reviews.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以使用以下提示模板来分类评论。
- en: Listing 1.2 Prompt template for classifying laptop reviews
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 1.2 用于分类笔记本电脑评论的提示模板
- en: '[PRE1]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 Context'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 上下文'
- en: '#2 Task description and output format'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 任务描述和输出格式'
- en: '#3 First example'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 第一个示例'
- en: '#4 Second example'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 第二个示例'
- en: '#5 Placeholder for review text'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 评论文本的占位符'
- en: This prompt template generalizes the prompt we saw for classifying one specific
    review (have a look at listing [1.1](#code__ReviewPrompt) in section [1.3.2](#sub__ExamplePrompt)).
    Again, we provide context (the fact that we’re classifying laptop reviews) (**1**)
    and instructions describing the task to solve, as well as the output format (**2**).
    We also provide a few example reviews with associated classification results (**3**
    and **4**). Although the review to classify changes, depending on the input, we
    do not need to change the example reviews. Those reviews merely illustrate what
    task the language model needs to solve. Finally (**5**), we have a placeholder
    for the review text. When iterating over different reviews, we generate a prompt
    for each of them by substituting the review text for this placeholder.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这个提示模板概括了我们之前看到的用于分类单个特定评论的提示（查看第 [1.3.2](#sub__ExamplePrompt) 节中的列表 [1.1](#code__ReviewPrompt)）。再次强调，我们提供了上下文（我们正在分类笔记本电脑评论）(**1**)，以及描述要解决的问题的任务说明，以及输出格式(**2**)。我们还提供了一些带有相关分类结果的示例评论(**3**
    和 **4**)。尽管要分类的评论根据输入而变化，但我们不需要更改示例评论。这些评论仅仅说明了语言模型需要解决的问题。最后(**5**)，我们有一个用于评论文本的占位符。当遍历不同的评论时，我们通过用这个占位符替换评论文本来为每个评论生成一个提示。
- en: The example prompt template has only a single placeholder. In general, several
    parts of the prompt may change depending on the input data. If so, we introduce
    placeholders for each of those parts and substitute all of them to generate prompts.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 示例提示模板只有一个占位符。一般来说，根据输入数据，提示的几个部分可能会发生变化。如果是这样，我们为这些部分中的每一个引入占位符，并用它们替换所有部分以生成提示。
- en: Figure [1.2](#fig__directDataAnalysis) summarizes how we use prompt templates
    when analyzing data directly with language models. For each data item (e.g., a
    review to classify), we substitute for placeholders in the prompt template to
    generate a prompt (we can also say that we *instantiate* a prompt). We then send
    this prompt to the language model to solve the data-analysis task we’re interested
    in.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图[1.2](#fig__directDataAnalysis)总结了我们在使用语言模型直接分析数据时如何使用提示模板。对于每个数据项（例如，一个需要分类的评论），我们在提示模板中的占位符进行替换以生成一个提示（我们也可以说我们*实例化*了一个提示）。然后我们将这个提示发送给语言模型来解决我们感兴趣的数据分析任务。
- en: '![figure](../Images/CH01_F02_Trummer.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH01_F02_Trummer.png)'
- en: Figure 1.2 Using language models directly for data analysis. A prompt template
    describes the analysis task. It contains placeholders that are replaced with data
    to analyze. After substituting for the placeholders, the resulting prompt is submitted
    to the language model to produce output.
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.2 直接使用语言模型进行数据分析。一个提示模板描述了分析任务。它包含占位符，这些占位符将被用于分析的数据所替换。在替换占位符后，生成的提示被提交给语言模型以产生输出。
- en: 1.4.2 Data analysis via external tools
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.2 通过外部工具进行数据分析
- en: Putting data directly into the prompt is not always the most efficient approach.
    For some types of data, specialized tools are available that process certain operations
    on that data very efficiently. In those cases, it is often more efficient to use
    the language model to write code for data processing (rather than analyzing the
    data directly). The code generated by the language model can then be executed
    by the specialized tool.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 直接将数据放入提示中并不总是最有效的方法。对于某些类型的数据，有专门的工具可用，这些工具可以非常高效地处理该数据上的某些操作。在这些情况下，使用语言模型编写数据处理代码（而不是直接分析数据）通常更有效。语言模型生成的代码然后可以被专用工具执行。
- en: 'We will apply this approach to structured data. For structured data such as
    data tables and graphs, specialized data-processing tools are available that support
    a wide range of analysis operations. Those operations, such as filtering and aggregating
    data, can be performed very efficiently on structured data. Even if it was possible
    to perform the same operations reliably with language models (which is not the
    case), we would not want to do it because the fees we pay to providers like OpenAI
    are proportional to the size of the input data. Processing large structured data
    sets (such as tables with millions of rows) using language models is prohibitively
    expensive. In the following chapters, we discuss the following types of tools
    for structured data processing:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将应用这种方法来处理结构化数据。对于如数据表和图这样的结构化数据，有专门的数据处理工具可用，它们支持广泛的分析操作。这些操作，如过滤和聚合数据，可以在结构化数据上非常高效地执行。即使使用语言模型可靠地执行相同的操作（这并不总是可能的），我们也不愿意这样做，因为我们支付给像OpenAI这样的提供商的费用与输入数据的大小成比例。使用语言模型处理大型结构化数据集（如包含数百万行的表）是极其昂贵的。在接下来的章节中，我们将讨论以下类型的结构化数据处理工具：
- en: '*Relational database management system*—Stores and processes relational data:
    that is, collections of data tables. Most relational database management systems
    support *SQL*, the Structured Query Language. We will use language models to translate
    questions about data to queries in SQL.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*关系数据库管理系统*—存储和处理关系数据：即数据表的集合。大多数关系数据库管理系统支持*SQL*，即结构化查询语言。我们将使用语言模型将关于数据的问题翻译成SQL查询。'
- en: '*Graph data management system*—Handles graph data representing entities and
    the relationships between them. Different graph data management systems support
    different query languages. In chapter 5, we see how to use language models to
    translate questions about data into queries in the *Cypher* language, supported
    by the Neo4j graph data management system.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图数据管理系统*—处理表示实体及其之间关系的图数据。不同的图数据管理系统支持不同的查询语言。在第5章中，我们将看到如何使用语言模型将关于数据的问题翻译成Neo4j图数据管理系统支持的*Cypher*语言的查询。'
- en: 'For instance, let’s assume we want to enable lay users ompt template for translating
    questto analyze a relational database: that is, a collection of data tables. Perhaps
    a table contains the results of a survey, and we want to let users aggregate answers
    from different groups of respondents. The survey results are stored in a relational
    database management system (the most suitable type of tool for this data type).
    Using language models, we can enable users to ask questions about the data in
    natural language (that is, in plain English). The language model takes care of
    translating those questions into formal queries. More precisely, given that the
    data is stored in a relational database management system, we want to translate
    those questions into SQL queries.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们想要使普通用户能够使用一个用于翻译问题的模板来分析关系数据库：即一组数据表。也许一个表包含调查结果，我们希望让用户从不同的受访者群体中汇总答案。调查结果存储在关系数据库管理系统中（最适合此类数据类型的工具）。使用语言模型，我们可以让用户用自然语言（即普通英语）提出关于数据的问题。语言模型负责将这些问题翻译成正式查询。更确切地说，鉴于数据存储在关系数据库管理系统中，我们希望将这些问题翻译成SQL查询。
- en: Again, we introduce a prompt template for the task we’re interested in. Here,
    we’re interested in text-to-SQL translation, meaning we want to use the language
    model to translate questions in natural language to SQL queries. Although the
    task (text-to-SQL translation) and the data (the database containing survey results)
    remain fixed, the user’s questions will change over time. Therefore, we introduce
    a placeholder for the user question in our prompt template. In principle, the
    following prompt template should enable us to translate questions on our survey
    data into SQL queries.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们引入一个提示模板来描述我们感兴趣的任务。在这里，我们感兴趣的是文本到SQL翻译，这意味着我们希望使用语言模型将自然语言中的问题翻译成SQL查询。尽管任务（文本到SQL翻译）和数据（包含调查结果的数据库）保持不变，但用户的问题会随时间变化。因此，我们在提示模板中引入一个用户问题的占位符。原则上，以下提示模板应该能够使我们能够将关于我们的调查数据的问题翻译成SQL查询。
- en: Listing 1.3 Prompt template for translating questions to SQL
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 1.3 将问题翻译到SQL的提示模板
- en: '[PRE2]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#1 Description of database'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 数据库描述'
- en: '#2 Question to translate'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 要翻译的问题'
- en: '#3 Task description'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 任务描述'
- en: 'First the prompt describes the structure of our data (**1**). This is required
    to enable the system to write correct queries (e.g., queries that refer to the
    correct names of tables and columns in those tables). The description in the example
    template is abbreviated. We will see how to accurately describe the structure
    of a relational database in later chapters. Next, the prompt template contains
    the question to translate (**2**). This is a placeholder to enable users to ask
    different questions using the same prompt template. Finally, the prompt template
    contains a (concise) task description (**3**): we want to translate questions
    to SQL queries!'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，提示描述了我们的数据结构（**1**）。这是为了使系统能够编写正确的查询（例如，引用表中正确名称的查询）。示例模板中的描述被简化了。我们将在后面的章节中看到如何准确描述关系数据库的结构。接下来，提示模板包含要翻译的问题（**2**）。这是一个占位符，使用户能够使用相同的提示模板提出不同的问题。最后，提示模板包含一个（简洁的）任务描述（**3**）：我们希望将问题翻译成SQL查询！
- en: Figure [1.3](#fig__textToSQL) summarizes the process for text-to-SQL translation.
    Given a corresponding prompt template, we substitute the user question for the
    placeholder, translate the question to an SQL query via the language model, and
    finally execute the query in a relational database management system. The query
    result is shown to the user.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [1.3](#fig__textToSQL) 总结了文本到SQL翻译的过程。给定相应的提示模板，我们用用户问题替换占位符，通过语言模型将问题翻译成SQL查询，并在关系数据库管理系统中执行查询。查询结果展示给用户。
- en: '![figure](../Images/CH01_F03_Trummer.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH01_F03_Trummer.png)'
- en: Figure 1.3 Using language models indirectly to build a natural language interface
    for tabular data. The prompt template contains placeholders for questions about
    data. After substituting for placeholders, the resulting prompt is used as input
    for the language model. The model translates the question into an SQL query that
    is executed via a relational database management system.
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 1.3 使用语言模型间接构建表格数据的自然语言界面。提示模板包含关于数据的占位符。在替换占位符后，生成的提示用作语言模型的输入。模型将问题翻译成通过关系数据库管理系统执行的SQL查询。
- en: 1.5 Minimizing costs
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.5 最小化成本
- en: When processing data with language models, we typically pay fees to a model
    provider. The larger the amount of data we process, the higher the fees. Before
    analyzing large amounts of data, we want to make sure we’re not overpaying. For
    instance, using larger language models (the neural network implementing the language
    model has more “neurons,” so to speak) is often more expensive, but for complex
    tasks, it may pay off with higher-quality results. But if the large model is not
    needed to solve our current task well, we should save the money and use a smaller
    model. Fortunately, there are quite a few ways in which we can optimize the tradeoff
    between processing costs and result quality. We discuss the different options
    next. All of them are covered in more detail in later book chapters.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用语言模型处理数据时，我们通常需要向模型提供商支付费用。我们处理的数据量越大，费用就越高。在分析大量数据之前，我们想要确保我们没有支付过高的费用。例如，使用更大的语言模型（从某种意义上说，实现语言模型的神经网络有更多的“神经元”）通常更昂贵，但对于复杂任务，它可能通过更高质量的结果来获得回报。但如果大型模型不是解决我们当前任务所必需的，我们应该节省金钱并使用较小的模型。幸运的是，我们有相当多的方法可以优化处理成本和结果质量之间的权衡。我们将在下一节讨论不同的选项。所有这些内容在后续的章节中都有更详细的介绍。
- en: 1.5.1 Picking the best model
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.5.1 选择最佳模型
- en: OpenAI offers many different versions of the GPT model, ranging from relatively
    small models to giant models like GPT-4\. At the time of writing, using GPT-4
    is over 100 times more expensive, per input token, than using the cheapest version.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI提供了许多不同版本的GPT模型，从小型模型到像GPT-4这样的巨型模型不等。在撰写本文时，使用GPT-4的成本，按每个输入标记计算，比使用最便宜的版本高100多倍。
- en: What are tokens?
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 标记（tokens）是什么？
- en: The processing fees for language models like GPT-4 are proportional to the number
    of tokens read and generated by the model. A *token* is the atomic unit at which
    the language model represents text internally. Typically, one token corresponds
    to approximately four characters.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于GPT-4的语言模型的处理费用与模型读取和生成的标记数量成正比。*标记*是语言模型在内部表示文本的原子单位。通常，一个标记对应大约四个字符。
- en: Given those price differences, it is clearly a good idea to think hard about
    which specific model satisfies our needs. For instance, for a simple task like
    review classification, we probably don’t need to use OpenAI’s most expensive model.
    But if we want to use the model to write complex code for data processing, using
    the most expensive version may be worth it.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些价格差异，仔细思考哪个具体模型满足我们的需求显然是一个好主意。例如，对于像评论分类这样的简单任务，我们可能不需要使用OpenAI最昂贵的模型。但如果我们想使用该模型编写用于数据处理复杂代码，使用最昂贵的版本可能是有价值的。
- en: 'Of course, we don’t need to restrict ourselves to models offered by OpenAI.
    Language models are offered by many providers, including Google, Anthropic, and
    Cohere. In principle, we might even choose to host our own model, using models
    that are publicly available: for example, on the Hugging Face platform. Some of
    those models are generic (similar to OpenAI’s GPT models), whereas others are
    trained for more specific tasks. If we happen to be interested in tasks for which
    specialized models exist, we may want to use one of them. We discuss models from
    other providers in more detail in chapter 8.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们不需要局限于OpenAI提供的模型。许多提供商都提供语言模型，包括Google、Anthropic和Cohere。原则上，我们甚至可以选择托管自己的模型，使用公开可用的模型：例如，在Hugging
    Face平台上。其中一些模型是通用的（类似于OpenAI的GPT模型），而其他模型则是为更具体的任务训练的。如果我们恰好对存在专门模型的任务感兴趣，我们可能想要使用其中之一。我们将在第8章中更详细地讨论其他提供商的模型。
- en: Picking the right model for your needs is not an easy task. As a first step,
    you might want to look at benchmarks such as Stanford’s Holistic Evaluation of
    Language Models (HELM, [https://crfm.stanford.edu/helm/](https://crfm.stanford.edu/helm/);
    see figure [1.4](#fig__helm)). This benchmark compares the quality of results
    produced by different language models on different types of tasks. Ultimately,
    you may have to try a few models on your task and a data sample to ensure that
    you choose the optimal one. In chapter 9, we will see how to benchmark different
    models systematically for an example task.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为满足您的需求选择合适的模型并非易事。作为第一步，您可能想要查看像斯坦福大学的语言模型全面评估（HELM，[https://crfm.stanford.edu/helm/](https://crfm.stanford.edu/helm/)；见图1.4）这样的基准。这个基准比较了不同语言模型在不同类型任务上产生的结果质量。最终，您可能需要在您的任务和数据样本上尝试几个模型，以确保您选择了最优的模型。在第9章中，我们将看到如何为一个示例任务系统地评估不同的模型。
- en: '![figure](../Images/CH01_F04_Trummer.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH01_F04_Trummer.png)'
- en: 'Figure 1.4 Holistic Evaluation of Language Models (HELM): comparing language
    models offered by different providers according to various metrics'
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.4 语言模型整体评估（HELM）：根据各种指标比较不同提供商提供的语言模型
- en: 1.5.2 Optimally configuring models
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.5.2 优化模型配置
- en: The OpenAI Python library offers a variety of tuning parameters to influence
    model behavior. For instance, we can influence the probability that certain words
    appear in the output of a model. This can be useful, for instance, when classifying
    reviews. If the output of the model should be one of only a few possible choices
    (such as “positive” and “negative”), it makes sense to restrict possible outputs
    to those choices. That way, we avoid cases in which the model generates output
    that does not correspond to any of the class names. To take another example, we
    can fine-tune the criteria used to decide when the model stops generating output.
    For instance, if we know that the output should consist of a single token (e.g.,
    the name of a class when classifying reviews), we can explicitly limit the output
    length to a single token. This prevents the model from generating more output
    than necessary (saving us money in the process, as costs depend on the amount
    of output generated).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的Python库提供了各种调整参数来影响模型行为。例如，我们可以影响某些词语出现在模型输出中的概率。这在例如分类评论时可能很有用。如果模型的输出应该是仅几个可能选择之一（例如“正面”和“负面”），那么将可能的输出限制到这些选择是有意义的。这样，我们就避免了模型生成与任何类名都不对应的输出的情况。以另一个例子来说，我们可以微调决定模型何时停止生成输出的标准。例如，如果我们知道输出应该由单个标记组成（例如，分类评论时的类名），我们可以明确地将输出长度限制为单个标记。这防止了模型生成比必要的更多输出（在这个过程中节省了我们的费用，因为费用取决于生成的输出量）。
- en: We will discuss those and many other tuning parameters in more detail in chapter
    3\. In chapter 9, we will see how to use those tuning parameters to get better
    performance from our language models.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第3章中更详细地讨论那些以及许多其他调整参数。在第9章中，我们将看到如何使用这些调整参数来从我们的语言模型中获得更好的性能。
- en: Another option to configure models is to fine-tune them. This means, essentially,
    that we’re creating our own variant of an existing model. By training the model
    with a small amount of task-specific training data, we get a model that potentially
    performs better at our task than the vanilla version. For instance, if we want
    to classify reviews, we might train the model with a few hundred example reviews
    and associated classification results. This may enable us to use a much smaller
    and cheaper model, fine-tuned for our specific task, that performs as well on
    this task as a much larger model that has not been fine-tuned.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 配置模型的另一种选项是微调它们。这本质上意味着我们正在创建现有模型的自己的变体。通过使用少量特定任务的训练数据来训练模型，我们得到一个模型，它在我们的任务上可能比原始版本表现更好。例如，如果我们想分类评论，我们可能会用几百个示例评论及其相关的分类结果来训练模型。这可能使我们能够使用一个更小、更便宜的模型，该模型针对我们的特定任务进行了微调，并且在这个任务上的表现与一个未经过微调的更大模型相当。
- en: Of course, fine-tuning also costs money, and it may not be immediately clear
    whether it is worth it for a specific task. We discuss fine-tuning and the associated
    tradeoffs in more detail in chapter 9.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，微调也会花费金钱，并且对于特定任务来说，是否值得可能并不立即明朗。我们将在第9章中更详细地讨论微调和相关的权衡。
- en: 1.5.3 Prompt engineering
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.5.3 提示工程
- en: The prompt template can significantly affect the quality of the results produced
    by the language model. A good prompt template clearly specifies the task to solve
    and provides all relevant context. We will see how to map various tasks to suitable
    prompt templates throughout the following chapters, covering a variety of data
    types. After working through those examples, you should be able to design your
    own prompt templates for novel tasks, following the same principles.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 提示模板可以显著影响语言模型产生的结果质量。一个好的提示模板清楚地指定了要解决的问题，并提供了所有相关上下文。在接下来的章节中，我们将看到如何将各种任务映射到合适的提示模板，涵盖各种数据类型。在完成这些示例后，你应该能够根据相同的原则为新颖的任务设计自己的提示模板。
- en: Similar to the model choice, it can be hard to pick the best prompt template
    for a given task without doing any testing. In chapter 9, we will test prompt
    templates in an example scenario and illustrate how different prompt templates
    lead to different outcomes. In some cases, investing a little time in finding
    the best prompt template may enable you to get satisfactory performance with fairly
    cheap models (whereas working with the unoptimized prompt template may make a
    more expensive model necessary).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 与模型选择类似，如果不进行任何测试，可能很难为特定任务选择最佳的提示模板。在第9章中，我们将在一个示例场景中测试提示模板，并说明不同的提示模板如何导致不同的结果。在某些情况下，花一点时间寻找最佳的提示模板可能会让你能够以相当低廉的模型获得令人满意的表现（而使用未经优化的提示模板可能需要更昂贵的模型）。
- en: Where to get prompt templates
  id: totrans-134
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在哪里获取提示模板
- en: Finding a good prompt template for a new task may take some time. If you do
    not want to spend that time, have somebody else do it for you! More precisely,
    you can find platforms on the web that enable users to buy and sell prompt templates.
    One of them is PromptBase ([https://promptbase.com](https://promptbase.com)).
    Say you want to translate English questions into SQL queries. By entering corresponding
    keywords, you will find not one but multiple alternative prompt templates on that
    platform. If the prompt template seems like a good match based on the associated
    description, you can buy it and use it for your data-analysis needs.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为新任务找到一个好的提示模板可能需要一些时间。如果你不想花那个时间，可以让别人帮你做！更准确地说，你可以在网上找到允许用户买卖提示模板的平台。其中之一是PromptBase
    ([https://promptbase.com](https://promptbase.com))。假设你想将英语问题翻译成SQL查询。通过输入相应的关键词，你将在该平台上找到不止一个替代提示模板。如果根据相关的描述，提示模板看起来是一个很好的匹配，你可以购买它并用于你的数据分析需求。
- en: 1.6 Advanced software frameworks and agents
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.6 高级软件框架和代理
- en: Throughout most of this book, we will use OpenAI’s Python library and similar
    libraries from other providers. For instance, these libraries enable you to send
    prompts to language models and receive the models’ answers. Although they are
    entirely sufficient for many use cases, you may want to consider more advanced
    software frameworks when developing complex applications that are based on language
    models.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的大部分内容中，我们将使用OpenAI的Python库和其他提供商的类似库。例如，这些库允许你向语言模型发送提示并接收模型的答案。尽管它们对于许多用例来说已经足够，但在开发基于语言模型的复杂应用程序时，你可能需要考虑更高级的软件框架。
- en: 'In this book, we discuss two advanced software frameworks for working with
    language models: LangChain ([https://langchain.com](https://langchain.com)) and
    LlamaIndex ([www.llamain](http://www.llamaindex.ai)[dex.ai](http://www.llamaindex.ai)).
    Both make it easier to develop Python applications for data analysis with language
    models.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们讨论了两个用于处理语言模型的先进软件框架：LangChain ([https://langchain.com](https://langchain.com))
    和 LlamaIndex ([www.llamain](http://www.llamaindex.ai)[dex.ai](http://www.llamaindex.ai))。两者都使得使用语言模型开发数据分析的Python应用程序变得更加容易。
- en: Besides many other features, these frameworks make it easy to create agents
    that use language models. This approach is useful for complex data-analysis tasks
    requiring, for instance, combining data from multiple sources. For most of this
    book, we solve data-analysis tasks with a single invocation of the language model,
    whether it is analyzing a text document or translating a question about data to
    a formal query. If the task requires multiple steps, such as performing preprocessing
    before calling the language model or post-processing on the model’s answer, we
    must hard-code the corresponding processing logic.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 除了许多其他功能外，这些框架使得创建使用语言模型的代理变得容易。这种方法对于需要结合来自多个来源的数据的复杂数据分析任务非常有用。在本书的大部分内容中，我们通过单次调用语言模型来解决数据分析任务，无论是分析文本文档还是将关于数据的问题翻译成正式查询。如果任务需要多个步骤，例如在调用语言模型之前进行预处理或在模型答案上进行后处理，我们必须硬编码相应的处理逻辑。
- en: This approach works as long as we can reliably predict the sequence of steps
    required for data processing. However, in some cases, it can be difficult to predict
    which steps are required. For instance, we may get questions from users that refer
    either to a text document or to a relational database. So, depending on the question,
    we need to either write an SQL query or extract information from text documents.
    Or perhaps we might need information from both the text and the relational database,
    extracting information related to the question from the text and then using the
    information we obtain to formulate an SQL query.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 只要我们能够可靠地预测数据处理所需的步骤序列，这种方法就会有效。然而，在某些情况下，预测所需的步骤可能很困难。例如，我们可能会收到来自用户的关于文本文档或关系型数据库的问题。因此，根据问题，我们可能需要编写SQL查询或从文本文档中提取信息。或者，我们可能需要从文本和关系型数据库中获取信息，从文本中提取与问题相关的信息，然后使用我们获得的信息来制定SQL查询。
- en: 'In such cases, it is not possible to hard-code all possible sequences of steps
    in advance. Instead, we want to design an approach that is flexible enough to
    decide independently what step is required next. This can be done using agents
    and language models. With this approach, the language model is used to decompose
    complex analysis tasks into subproblems. Furthermore, the language model may choose
    to invoke *tools*: arbitrary functions whose interfaces are described in natural
    language. Such tools can, for instance, encapsulate the invocation of an SQL query
    on a relational database. After invoking a corresponding tool, the language model
    is given access to the invocation result (e.g., the query result) and can use
    that result to plan the next steps. We will see how to use agents to solve complex
    data-analysis tasks where it is unclear, a priori, which data sources and processing
    methods are required to solve them.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，无法事先硬编码所有可能的步骤序列。相反，我们希望设计一个足够灵活的方法，能够独立决定下一步需要采取什么步骤。这可以通过代理和语言模型来实现。使用这种方法，语言模型被用来将复杂分析任务分解为子问题。此外，语言模型可以选择调用*工具*：接口用自然语言描述的任意函数。例如，这些工具可以封装对关系型数据库上的SQL查询的调用。在调用相应的工具后，语言模型可以访问调用结果（例如，查询结果），并可以使用该结果来规划下一步。我们将看到如何使用代理来解决复杂的数据分析任务，在这些任务中，事先并不清楚需要哪些数据源和处理方法。
- en: Summary
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Language models can solve novel tasks without specialized training.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言模型可以在没有专门训练的情况下解决新任务。
- en: The prompt is the input to the language model.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示是语言模型的输入。
- en: Prompts may combine text with other types of data, such as images.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示可以结合文本和其他类型的数据，例如图像。
- en: A prompt contains a task description, context, and (optionally) examples.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示包含任务描述、上下文和（可选）示例。
- en: Language models can analyze certain types of data directly.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言模型可以直接分析某些类型的数据。
- en: When analyzing data directly, the data must appear in the prompt.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当直接分析数据时，数据必须出现在提示中。
- en: 'Prompt templates contain placeholders: for example, to represent data items.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示模板包含占位符：例如，用于表示数据项。
- en: By substituting for placeholders in a prompt template, we obtain a prompt.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过替换提示模板中的占位符，我们获得一个提示。
- en: Language models can also help to analyze data via external tools.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言模型还可以通过外部工具帮助分析数据。
- en: Language models can instruct other tools on how to process data.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言模型可以指导其他工具如何处理数据。
- en: Models are available in many different sizes with significant cost differences.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型有多种不同的大小，成本差异显著。
- en: Models can be configured using various configuration parameters.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型可以使用各种配置参数进行配置。
- en: LangChain and LlamaIndex help to develop complex applications.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LangChain和LlamaIndex有助于开发复杂的应用程序。
- en: Agents use language models to solve complex problems.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理使用语言模型来解决复杂问题。
