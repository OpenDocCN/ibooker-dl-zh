- en: 11 Reducing opt-outs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Identifying the reasons behind a user’s desire for human agents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to prevent users from immediately wanting to opt out
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to keep users engaged with your conversational AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using generative AI to create friendlier dialogue messages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deciding when to involve a human agent (and when not to)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The term “opt out” refers to a user attempting to exit a virtual agent experience,
    often with the intention of reaching a human agent. You might also see this described
    as *escalating* or *zeroing out* (pushing zero on a phone’s dial pad to get an
    operator). Opt-outs can be costly. Chatbots are an investment, and they must demonstrate
    a return on business value in order to remain viable. Containment loss due to
    too many opt-outs can sink a business case.
  prefs: []
  type: TYPE_NORMAL
- en: Users will opt out for a variety of reasons that often require different strategies
    and approaches to resolve. Regardless of the type of bot you are managing, be
    it voice, text, FAQ, process-oriented, or even routing agents, identifying where
    your users opt out within the conversation can give you clues about why they did
    so. Learning why users opt out will help you design an experience that minimizes
    opting out, which should improve your containment.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll explore some conversational AI solutions that suffered
    from containment loss due to users opting out and discuss how each challenge was
    resolved. Different use cases may have different solutions, depending on the organization’s
    priorities, resources, and constraints, but there are common patterns and principles
    that can increase the value of your conversational AI and make users more likely
    to stay with it.
  prefs: []
  type: TYPE_NORMAL
- en: 11.1 What drives opt-out behavior?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some users encounter a virtual agent, and the first thing they do is request
    a human—they don’t even try to interact with the conversational AI. We call this
    an *immediate opt-out*. Other times, a user will initially go along with (opt
    *in* to) a virtual agent experience but attempt to opt out later in the conversation.
    Their reasons tend to be quite different from the initial opt-out drivers and
    are often an indication that there is some problem with the overall conversational
    design or perhaps just with a particular step in a flow. Weak understanding can
    also be a root cause.
  prefs: []
  type: TYPE_NORMAL
- en: 11.1.1 Immediate opt-out drivers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By their nature, immediate opt-outs provide very little information about the
    user’s reason. Collecting data on this is difficult, but it can be obtained through
    surveys or following up on the agent escalation (a very manual, time-consuming
    effort). Our research uncovered a few different drivers for this behavior, which
    were not mutually exclusive.
  prefs: []
  type: TYPE_NORMAL
- en: Prior poor experience with an IVR, chatbot, or virtual agent
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Interactive voice response (IVR) allows a user to interact with a computer using
    a phone’s keypad or simple voice commands. Early IVR systems were around by the
    1970s, but in the 2000s, they became cheaper to deploy and have since been ubiquitous
    in the modern world. You would be hard-pressed to find a single person who hasn’t
    been annoyed by a company greeting that spends thirty seconds telling you how
    to use their phone menu. Worse still is the warning that it is *“*important to
    listen to all of the options” (as their menu may have changed)!
  prefs: []
  type: TYPE_NORMAL
- en: Chatbots have also been with us for quite some time, though successfully getting
    them to do something functionally useful is relatively recent, and they are still
    evolving into true virtual assistants.
  prefs: []
  type: TYPE_NORMAL
- en: Users may not be able to tell the difference between an IVR, simple chatbot,
    or robust virtual assistant. Quite frankly, they don’t care. Prior bad experiences
    are going to bias many people against automated systems. There are even corners
    of the internet that specialize in “hacks” that people have discovered to bypass
    automated systems and get directly routed to a company’s human agent queue.
  prefs: []
  type: TYPE_NORMAL
- en: The user judges their problem is too complex for a machine
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Sometimes a user believes their situation is so unique or complex that it is
    beyond the capabilities of an automated system. Sometimes they are right; other
    times they are not. Judgments about complexity and uniqueness are relative to
    the individual, and they may not know that thousands of others have experienced
    a similar problem. This can be related to prior experience, but that isn’t always
    the case.
  prefs: []
  type: TYPE_NORMAL
- en: These users opt out because they believe they will end up needing an agent anyways.
    They see the automated system as a waste of their time, prolonging or obstructing
    their path to resolution.
  prefs: []
  type: TYPE_NORMAL
- en: Preference for human interaction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Connecting with other humans is fundamental to our survival as a species, so
    some people prefer dealing with a real person. Their needs may have nothing to
    do with the solution’s capability and can include loneliness, sensitive or embarrassing
    topics, mistrust of machines and automated systems, language or accessibility
    barriers, etc. This is increasingly becoming a generational phenomenon. Older
    users are more likely to opt out because they may find it difficult to interact
    with a machine, whereas digital natives usually have an easier time navigating
    automated systems. Regardless of age, you will probably always have users who
    prefer human interaction.
  prefs: []
  type: TYPE_NORMAL
- en: 11.1.2 Motivations for later opt-outs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Users who opt out after initially engaging with a virtual assistant often do
    so because they are struggling with a particular interaction. Such occurrences
    can usually be tied to a specific task, action, or step within the conversational
    flow, making it a bit easier to identify a root cause.
  prefs: []
  type: TYPE_NORMAL
- en: Bot does not understand the user’s request
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When the bot does not understand a user’s request, the user may opt out. This
    will most often occur early in a conversation, but it can occur anywhere.
  prefs: []
  type: TYPE_NORMAL
- en: It is standard practice to allow a certain number of retry attempts for user
    input before escalating them. The average is three, but your use case may have
    a higher or lower threshold. This practice is a critical tool for containment,
    but user tolerance for machine errors can be lower and less forgiving than it
    would be with a human. Users may opt out after being asked to repeat themselves
    once or twice, feeling that the bot is incapable of understanding.
  prefs: []
  type: TYPE_NORMAL
- en: User does not understand what the bot is asking
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A poorly worded question from the bot may confuse the user. The user may not
    be clear about the type of response the bot is looking for or the expected format.
    A user may ask the bot to repeat, but if the same question is presented, the user
    might still be confused. This will result in the user feeling stuck, and they
    will likely ask for a human to help them get unstuck.
  prefs: []
  type: TYPE_NORMAL
- en: User does not have or know the requested information
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If a user is asked to provide information they do not have, they may say, “I
    don’t know” or “I don’t have that,” or they may simply ask for an agent. If a
    task flow cannot move forward without certain information, and no alternatives
    are presented, the user knows they aren’t going to reach their goal without agent
    intervention.
  prefs: []
  type: TYPE_NORMAL
- en: User feels like they are not progressing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A user might opt out if they feel that the conversation is stuck in a loop,
    is bouncing between menus, or has reached a dead end. This could be caused by
    an actual bug in your dialogue logic, or it could be that the conversational design
    is failing to indicate progress toward the user’s goal. In any case, they are
    going to get frustrated and look for a way out.
  prefs: []
  type: TYPE_NORMAL
- en: User does not like the answer or outcome, or they had a different expectation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Your bot may provide a response that is technically correct, but it still makes
    the user unhappy. They may ask for an agent in the hope that they can reach a
    different outcome. They may also feel that the information was insufficient and
    will ask for an agent if the experience does not appear to provide an opportunity
    for follow-up or additional requests.
  prefs: []
  type: TYPE_NORMAL
- en: 11.1.3 Gathering data on opt-out behavior
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to determine if your solution is losing containment due to opt-outs,
    you must collect data. Some conversational platforms come with the ability to
    report on the actions or tasks that were invoked during a conversation. They may
    also provide data on whether or not the interaction concluded successfully.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, the out-of-the-box analytics aren’t sufficient for providing actionable
    metrics. In those cases, you can instrument your dialogue with context variables
    at key points in the flow. Good instrumentation of your dialogue flows can help
    you identify and prioritize areas for improvement. (You may need a data warehouse
    and an enterprise or custom reporting tool to track this data over time.)
  prefs: []
  type: TYPE_NORMAL
- en: For a complex task-oriented solution, you might use breadcrumbs to mark the
    start or completion of major flows and subflows. Figure 11.1 shows an example
    of opt-out data grouped by the major dialogue task flow in which the request for
    an agent occurred.
  prefs: []
  type: TYPE_NORMAL
- en: If your dialogue is instrumented to track the exact step where an opt-out occurs,
    you can look for trends to help you uncover the root cause, as seen in figure
    11.2\.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing you might do with this information, especially for a process-oriented
    bot like the one in our example, is determine which task flows are considered
    immediate opt-outs. A simple question and answer (Q&A) bot may only have an initial
    greeting. Everything after that would be an “other” opt-out.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH11_F01_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 A breakdown of opt-out requests by current task flow shows that
    immediate opt-outs (requests for an agent during the Initial Greeting task flow)
    occurred more frequently than in any other part of the conversational journey
    for this self-service, process-oriented bot.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![figure](../Images/CH11_F02_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 A breakdown of opt-out occurrences by step can aid in root cause
    analysis. In this chart, immediate opt-outs show prominently on the left, but
    trends indicate that there may also be a problem with collecting address details
    in multiple downstream flows.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In our process-oriented bot example, a request for an agent during the Initial
    Greeting task flow was considered an immediate opt-out—the user was not willing
    to engage. All other opt-outs were associated with some other task flow within
    the conversation. Figure 11.3 shows our example bot’s opt-out requests according
    to where the task sits within the full conversational flow.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH11_F03_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 A high-level flow diagram shows how far a user gets into a process
    before opting out. This information can aid in root cause investigation.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The rest of this chapter will focus on strategies for addressing the problem
    of opt-outs, including approaches aimed at reducing initial opt-outs, later opt-outs,
    and strategies to keep the user in channel (opt-out retention).
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Reflect on what you have learned about why users opt out of a virtual assistant:'
  prefs: []
  type: TYPE_NORMAL
- en: Do you have the ability to differentiate between an immediate opt-out and a
    later opt-out?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are you able to identify patterns in your dialogue flows where opt-outs are
    occurring?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 11.2 Reducing immediate opt-outs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’ve likely heard the saying, “You never get a second chance to make a first
    impression.” Immediate opt-outs are a sign that a user is not impressed. You only
    have a brief chance to convince a user that they are in the right place and that
    your virtual assistant is competent, capable, and efficient. This section provides
    strategies to reduce the likelihood of a user asking for an agent right away.
  prefs: []
  type: TYPE_NORMAL
- en: The line between what constitutes an “immediate opt-out” versus an “other opt-out”
    within your dialogue flow is not arbitrary, but it is flexible. It may occur in
    the first step or in the first few steps. The distinction is meant to identify
    a point in a conversational flow where the user has agency to either agree to
    opt in or attempt to opt out. For an FAQ-style bot, a request for agent in the
    very first utterance would be considered an “immediate opt-out,” and everything
    after that would be an “other opt-out.”
  prefs: []
  type: TYPE_NORMAL
- en: What follows are three strategies that will have the greatest effect on reducing
    immediate opt-outs.
  prefs: []
  type: TYPE_NORMAL
- en: '11.2.1 Start with a great experience: Greetings and introductions'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'What makes a user feel good about an automated interaction? Universal customer
    care principles apply: users should feel that they have reached the right place,
    that they are in good hands, and that their time is valued.'
  prefs: []
  type: TYPE_NORMAL
- en: The first immediate opt-out driver we discussed in this chapter was prior poor
    experience with an IVR, chatbot, or virtual agent. Your bot’s greeting or introduction
    will set the tone for the conversational experience. This is your chance to gain
    the user’s trust—to convince them that that your virtual agent can be just as
    effective and efficient as a human agent at helping them reach their goal.
  prefs: []
  type: TYPE_NORMAL
- en: 'In chapter 1, we teased a dramatic improvement that addressed the challenge
    of “immediate opt-out” by users. We worked with a regional utility company’s virtual
    assistant that was losing over half of the callers to immediate opt-outs. The
    assistant was an extension of a larger IVR (voice) system (the main customer service
    line). This pilot program was intended to handle two self-service tasks: stop
    a utility service or transfer the service to a new address.'
  prefs: []
  type: TYPE_NORMAL
- en: The assumption was that everyone who reached our virtual assistant intended
    to do one of these two things (stop or transfer their utility service), based
    on the IVR menu selections that delivered the caller to our solution. Information
    about the user and their menu selection was passed to the virtual assistant, which
    immediately launched the corresponding use case flow. The following listing shows
    the user’s experience, first with the IVR and then with the handoff to the virtual
    assistant.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.1 Handoff from IVR to virtual assistant
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the early production logs, we noticed a significant volume of users opting
    out, often right away. Though our utility company’s virtual agent pilot was technically
    very capable, many callers wouldn’t give it a chance. We listened to call recordings
    and discovered that the user experience as a whole felt disjointed. Customers
    dialed a number, reached an IVR (with a particular IVR “voice”), and navigated
    to a menu of service-related topics. If the caller chose to stop or transfer their
    service, they were routed to our virtual agent, but the transition was abrupt—a
    different voice bypassed greeting the caller (the original justification was that
    the caller had already been greeted by the IVR) and dove right into the task with
    seemingly optimal efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: We could hear the confusion in the initial silence of some callers when they
    reached the virtual assistant. There were long hesitations, audible sighs, or
    stammering (“uh…,” “um…,” “hmm”). They hadn’t been introduced to this new agent.
    They were confused by the first question, which was procedurally appropriate but
    conversationally came off as impersonal and clunky. A human agent wouldn’t have
    opened the conversation like that. They would have introduced themselves in a
    welcoming tone. If the IVR collected information about the caller’s goal, a human
    agent would have confirmed this with the user before proceeding (“I see you’re
    calling about transferring your utility service, is that correct?”).
  prefs: []
  type: TYPE_NORMAL
- en: We redesigned the experience from the opening line, starting with a context-aware
    greeting and introduction. The virtual agent was given a persona and a more conversational
    tone, as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.2 Updated virtual assistant greeting and introduction
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Context-aware greeting'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Virtual agent introduction'
  prefs: []
  type: TYPE_NORMAL
- en: Beginning the interaction with a greeting and introduction differentiated our
    virtual assistant from the menu-driven IVR system. This gave the caller time to
    adjust to the transition to a new voice and a different style of interaction.
  prefs: []
  type: TYPE_NORMAL
- en: A context-aware greeting, such as acknowledging the time of day or greeting
    a user by name, can transform a robotic, impersonal exchange into a more warm
    and welcoming experience. A name or persona may not be appropriate in all use
    cases, but for this one, it conveyed a sense of ownership and accountability.
    “Alice” is here to serve customers.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.2 Convey capabilities and set expectations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our immediate opt-out drivers, we identified users who opt out because they
    feel like their problem is too unique or complicated for a machine. Sometimes
    this is a fair judgment, but sometimes it is not.
  prefs: []
  type: TYPE_NORMAL
- en: Setting expectations up front is vital. It is not uncommon for companies to
    launch a pilot virtual assistant with limited scope or capabilities. When your
    solution only provides a subset of the topics or tasks a user might want, you
    need to communicate this up front. By doing so, users will either be assured that
    they are in the right place, or they will recognize that they are not. Especially
    in these scenarios, the greeting (or solution entry point) is a good time to announce
    the chatbot’s purpose and capabilities (or get a quick confirmation from the user
    before pushing forward).
  prefs: []
  type: TYPE_NORMAL
- en: Our utility company’s virtual agent greeting was expanded to confirm the user’s
    goal (using context indicating their IVR selection). We included a brief preview
    of the journey the user was about to embark upon. Because this was a voice channel—more
    prone to unexpected disconnects—we also set some expectations for what a successful
    completion would look (sound) like. The following listing shows the additional
    verbiage.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.3 Updated greeting conveying capabilities and seting expectations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Affirms the bot’s purpose or capability'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Previews the user’s journey'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Sets expectations for the journey’s success'
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.3 Incentivize self-service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A technically savvy or recurring user may realize the efficiency of using an
    automated solution, but one-time or occasional users don’t know what to expect.
    They may perceive the process to be difficult or time-consuming. These users may
    also be motivated to immediately opt out due to a preference for human interaction.
    Incentivizing self-service may reduce immediate opt-outs.
  prefs: []
  type: TYPE_NORMAL
- en: In our utility company use case, the average call time for completing a stop
    service request with a human agent was about 5 to 7 minutes (plus hold time).
    Our self-service flow could be at least that fast. The following listing shows
    how we made an additional tweak to our greeting, letting the caller know that
    they could accomplish their goal in a short amount of time.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.4 Incentivizing the caller by offering a fast resolution
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**#1 Incentivizes self-service**  **To further incentivize the caller, we wanted
    to assure them that they would be in good hands, even if they ran into problems
    with the automated system. The following listing shows the updated message.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.5 Preventing immediate opt-outs by assuring escalation can happen
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**#1 Assures the caller that problems will be escalated**  **### 11.2.4 Allow
    the user to opt in'
  prefs: []
  type: TYPE_NORMAL
- en: Wherever possible, users should be given a sense of agency. This could look
    like obtaining their consent to proceed with the virtual agent experience. It
    may not be appropriate for all use cases, but this approach has the benefit of
    clearly identifying users who have agreed to opt in to the experience.
  prefs: []
  type: TYPE_NORMAL
- en: Our utility company’s virtual agent greeting was updated one last time. We ask
    the user if they are ready to proceed.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.6 Inviting the user to opt in
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Allows the user to opt in'
  prefs: []
  type: TYPE_NORMAL
- en: Though we were hoping to hear a “yes” in response to our question, we also had
    to be prepared to handle other responses. We had a hypothesis that some users
    entered our solution by mistake. Sometimes it was a misunderstanding of what was
    going to happen when they selected the menu option in the IVR. Other times, it
    was simply a case of “fat finger”—an erroneous and often unnoticed dial pad selection.
    These “other” responses confirmed our hypothesis—our users would say “back” or
    “no” or express a different goal. They had arrived here by accident, and we wanted
    to get them back on the right path to accomplish their goal. Figure 11.4 shows
    how we redesigned the greeting flow so that each non-yes scenario could be handled
    appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH11_F04_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 The dialogue logic in this sample greeting flow can handle various
    responses to the question, “Are you ready to proceed?” If a response is explicit,
    such as “no,” “go back,” or “speak to an agent,” a predefined flow is invoked.
    Otherwise, the response is sent to the classifier for intent detection and is
    handled by the corresponding flow.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Review the section of your dialogue where a user might immediately opt out,
    and ask yourself the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Does my virtual assistant greet the user warmly and introduce itself?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does my virtual assistant explain its purpose—what it can and cannot do?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does my virtual assistant offer a comparatively low-friction experience to meet
    the user’s needs (as good as or better than alternative channels or human intervention)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 11.3 Reducing other opt-outs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Opt-outs that occur later in a conversation can often be tied to a specific
    problem or gap in the conversational design. In this section, we’ll discuss strategies
    and approaches to help minimize opt-outs that occur later in a conversation.
  prefs: []
  type: TYPE_NORMAL
- en: 11.3.1 Try hard to understand
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In chapter 4, we discussed the importance of understanding what your users want.
    Opt-outs can be an indication of problems in your bot’s intent recognition. When
    a user engages with a chatbot that greets them with an open-ended question, such
    as “How can I help you?” they are going to have an expectation that they can ask
    any question related to your company’s business, or even the general domain.
  prefs: []
  type: TYPE_NORMAL
- en: If the solution is not prepared for a range of reasonable requests, your user
    is going to be frustrated when asked to repeat or rephrase. Do not allow your
    solution to become stale. Invest in keeping your training relevant and representative
    of current user needs. Conversational search or RAG patterns may be a great fit
    for some use cases—especially those with broad domains and question-answer bots.
  prefs: []
  type: TYPE_NORMAL
- en: 11.3.2 Try hard to be understood
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Your word and phrasing choices are important, especially when you are soliciting
    information from a user. A lengthy output or poorly worded question may be hard
    for the user to parse. The user may not understand the type of information they
    are being asked to provide, or they may not have it immediately available. It
    is common for users to opt out if they find themselves in this situation. When
    you solicit information, be clear about how the user should answer. Multiple choice
    questions are sometimes misinterpreted as yes/no questions:'
  prefs: []
  type: TYPE_NORMAL
- en: “Are you calling about starting, stopping, or transferring your electric service?”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Are you looking for reconsideration, claim disputes, or review?”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 11.5 shows an example of a choice question that might be interpreted
    as a yes/no question and one approach for preventing this type of confusion.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH11_F05_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 The way you structure a question can help the user understand what
    type of question you are asking—and what sort of answer you are looking for.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 11.3.3 Be flexible and accommodating
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Rigid, confusing, or overly restrictive conditions that define a “valid” user
    input may cause users to opt out. Your dialogue should be flexible enough to handle
    a range of “correct” responses and accommodate the user by disambiguating if necessary.
    An example of disambiguation is shown in figure 11.6.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH11_F06_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.6 A resilient dialogue flow can handle a range of valid responses.
    If necessary, a friendly disambiguation step can be invoked to get clarity about
    the user’s goal.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Special considerations for voice solutions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If speech recognition is part of the experience, validate your transcription
    accuracy so that egregious misunderstandings can be avoided. Because language
    models are text-based, mistranscribed speech inputs can compound problems in understanding.
    You could also miss important topic trends if the speech service does not faithfully
    capture the user’s input.
  prefs: []
  type: TYPE_NORMAL
- en: For voice solutions, it is imperative that you keep the cognitive load to a
    minimum; craft your questions to be direct and concise. If information needs to
    be included with the question, make sure the question comes at the end of the
    output. This will prompt the user that it is their turn to speak, as shown in
    figure 11.7\. Be prepared to handle requests to repeat the question or information.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH11_F07_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.7 The bad example (left) puts the call to action “How can I help you?”
    in the middle of the output. This might spur the user to begin speaking while
    the output is still trying to play. The good example (right) puts informational
    messaging up front. At the end of the message, the call to action is a clear invitation
    for the user to begin speaking.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Use technology to ease pain points
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are situations where you can employ technology to ease user pain points
    and facilitate understanding. For example, an insurance company had a voice assistant
    for members. Member ID numbers could be 9 or 13 digits, and sometimes they were
    preceded by a letter. We didn’t want to burden the caller with a message about
    how long the number might be or whether or not to include the preceding letter,
    if there even was one. That’s too much cognitive load, especially on a first pass.
    We simply asked, “What’s your member ID?” Some users would speak this number,
    including the letter. Others would key in the number using the dial pad, and they
    would ignore any preceding letter. We wanted to accommodate natural user behavior,
    so we made our logic more robust. The caller could include or exclude the preceding
    alpha character as long as we detected the right number of digits to perform a
    lookup. Instead of pestering the user with retries, the solution would fill in
    trivial information and attempt to perform a lookup if it had enough information
    (e.g., “123456789” and “X123456789” were both acceptable). If needed, our retry
    messaging would change the message slightly to guide the user to provide a valid
    input.
  prefs: []
  type: TYPE_NORMAL
- en: Another tool for improving robustness and accuracy, if your technology allows
    it, is custom speech models. Speech transcriptions of numbers or letters are difficult
    to collect over a voice channel due to phoneme similarity (e.g., “8” can often
    sound like “H,” so we selected speech models that were optimized to such inputs).
  prefs: []
  type: TYPE_NORMAL
- en: 11.3.4 Convey progress
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Self-service task flows can range in complexity from answering a question to
    simple triage to multistep, multiflow, dynamic user journeys. In these more complex
    flows, your dialogue design should signal the progression of the task or process.
    If the user feels that this is going to go on forever or that they are looping
    through an unending series of menus, they may believe that a conversation with
    a human would be more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Progress can be signaled in a variety of ways and should be optimized for the
    medium. Before initiating a long task flow, set the user’s expectations about
    what you will need from them and how long it will take. A text-based platform
    can be more verbose or provide visual indicators, such as numbering questions
    or showing progress bars. A phone channel will need to be more succinct but can
    give indicators such as “We’re almost done” or “Just a few more questions.”
  prefs: []
  type: TYPE_NORMAL
- en: 11.3.5 Anticipate additional user needs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When a user receives an answer from your bot or reaches the conclusion of a
    task flow, they may find that their goal is still unmet. Does your dialogue flow
    have potential outcomes that are technically correct or appropriate but that will
    tend to leave the user’s problem unresolved? Think about the position the user
    is in. They have a need, they get an answer, but the need still exists. If the
    dialogue flow has concluded without addressing that need, the user is going to
    opt out. What other choice do they have?
  prefs: []
  type: TYPE_NORMAL
- en: Can you extend your solution’s self-serve capability to initiate an alternate
    resolution path? Figure 11.8 shows some example scenarios and possible ways to
    avert the need for escalation.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH11_F08_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.8 You can avoid escalations by presenting the next best course of
    action whenever you anticipate a user could still have an unmet need at the conclusion
    of a dialogue flow.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 11.3.6 Don’t be rude
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When you’re building a virtual assistant, remember that you are attempting to
    simulate a more human-like experience. Empathy comes naturally to humans, but
    it must be intentionally integrated into a machine-driven interaction. Assume
    that your users are making an effort on their end of the conversation. When you
    need to retry or repair the conversation, don’t make the user feel they are to
    blame. A conversational designer should craft error messages in a way that guides
    the user back on track without implying fault.
  prefs: []
  type: TYPE_NORMAL
- en: For example, our insurance company solution could look up claims with a claim
    number, which was thirteen digits. Originally, it seemed logical to provide “informative”
    information in the retry output. This turned out to sound rude over the phone,
    as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.7 Retry scenario with rude customer experience
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Sometimes the user failed to provide enough digits; other times, the speech
    service failed to detect all of the spoken digits or mistranscribed the user response.
    Regardless of fault, the solution needs to re-ask the user to provide the information.
    On a first retry, it is often enough to simply ask for the information again.
    You can progressively guide the user if they are struggling to give the right
    kind of response or to provide information in the correct format.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.8 Updated experience for retry scenario
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Exercises
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Review the downstream flows within your dialogue where users might opt out,
    and ask yourself the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: How often do users opt out because the virtual assistant does not seem to understand
    their request?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are users opting out because they do not understand what the virtual assistant
    is asking of them?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is my virtual assistant pleasant and helpful? Does it convey competence, efficiency,
    and empathy?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is there automation available to streamline or expedite the user to their end
    goal?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do users request a human because they have unmet needs after interacting with
    the virtual assistant, even after a seemingly “successful” flow?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 11.4 Opt-out retention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The strategies discussed up to this point are intended to reduce immediate opt-outs
    but may not eliminate them entirely. To improve containment, you can also try
    to retain the user in the virtual agent experience after an immediate opt-out
    request or at key points in a dialogue flow.
  prefs: []
  type: TYPE_NORMAL
- en: A good faith attempt to keep the user in channel can work in many scenarios.
    This must be undertaken with care—a customer should never feel that they are being
    held hostage by the system. When that happens, by the time the user eventually
    does get to a human agent, they may be frustrated or hostile.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of an opt-out retention flow is to
  prefs: []
  type: TYPE_NORMAL
- en: Discover the user’s true goal or need
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assess whether the bot is capable of meeting that need
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If so, convince the user that the bot is capable of meeting that need
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If not, route them to the next best action (e.g., another virtual agent or a
    human)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You might implement such a flow at the beginning of a conversational interaction
    to help improve containment loss due to immediate opt-outs. It can also be used
    strategically after a user has appeared to successfully complete a flow. If users
    are asking for an agent right after appearing to complete a “happy path,” you
    might be missing something. Either the user didn’t like the answer, or it did
    not help them toward their goal.
  prefs: []
  type: TYPE_NORMAL
- en: 11.4.1 Start right away by collecting opt-out data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want to get started right away, or if you are still in the predeployment
    build phase, you can implement a simplified opt-out flow that asks the user to
    provide the reason for opting out. In the most simplified version, the assistant
    escalates no matter what is said. Figure 11.9 shows an example data-collection
    opt-out flow. This is a fairly non-intrusive strategy to find out why users are
    opting out.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH11_F09_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.9 A simple flow to collect the user’s reason for opting out
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Once you collect metrics, you may realize that you need better training, or
    your solution strategy has a mismatch between what you built it for and how users
    want to interact with it.
  prefs: []
  type: TYPE_NORMAL
- en: 11.4.2 Implementing an opt-out retention flow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once you understand why users are opting out, you can begin to address these
    requests in more meaningful ways. The first thing you’ll want to do is update
    your classifier so that you can take the action most appropriate for the request.
    Such actions could include
  prefs: []
  type: TYPE_NORMAL
- en: Expanding your bot’s current capability to handle these new requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handing the user off to a different virtual agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Escalating the user to a human agent when necessary (if available)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing information about next-best alternatives if agent escalation is not
    available
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 11.10 shows a dialogue flow that can identify which requests are in scope
    versus out of scope, with a fallback path for requests that are not understood
    by the classifier.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH11_F10_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.10 A typical opt-out retention pattern will try to find out what the
    user needs. It will either recognize an in-scope request (a request that it is
    equipped to handle) or an out-of-scope request (a request that it understands
    but is not equipped to handle), or it will not recognize the request. When an
    in-scope request is recognized, the bot may make an incentivized offer to keep
    the user in-channel. When an out-of-scope request is made, the bot can route the
    user directly to the appropriate skill or agent queue. When a request is not recognized,
    the user is routed to a default or general hold queue.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For our utility company use case, we added logic to ask the user to tell us
    more about what they needed (and trained our classifier over time to identify
    the range of requests). Many users would still insist on escalating by demanding
    an agent a second time. These users would be transferred to the general live agent
    hold queue.
  prefs: []
  type: TYPE_NORMAL
- en: Some users expressed a goal that was understood but was truly outside the scope
    of our system. These users could be routed to a specialized agent (if available),
    minimizing the need for additional transfers. For example, some businesses have
    a single customer service department to handle all problems. Others route users
    to different customer service agents or departments to handle specific scenarios,
    such as a dedicated billing department or agents who specialize in handling appointment
    scheduling.
  prefs: []
  type: TYPE_NORMAL
- en: When a user expressed a goal that aligned with our solution’s purpose, we made
    an incentivized offer and gave them a choice to proceed with self-service. This
    gives the user agency and makes them feel heard. By providing information about
    hold times for a human agent, they can decide how best to use their time. By correctly
    identifying their need, we gain their confidence. Many users are willing to proceed
    as long as the bot is able to demonstrate competence, as shown in the following
    listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.9 Opt-out retention flow identifying goal to keep user in-channel
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The virtual agent would then proceed to collect as much information as possible,
    until an error occurred or the caller requested an agent a second time. This enabled
    us to self-serve the customer either to completion, or to collect as much information
    as possible, reducing the amount of time the human agent would need to spend resolving
    the call.
  prefs: []
  type: TYPE_NORMAL
- en: This new design had great success. Out of callers who immediately opted out
    but expressed an in-scope intent, 38% were convinced to stay with the virtual
    agent and be successfully authenticated. When paired with other updates described
    in this chapter, the overall use case completion rate increased from 27% to 30%.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Reflect on what you have learned about strategies to keep the user in-channel:'
  prefs: []
  type: TYPE_NORMAL
- en: Does your current solution have opt-out trend patterns where you don’t understand
    why the user asked for an agent?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Would asking the user to provide more information about what they need help
    you target areas for improvement?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 11.5 Improving dialogue with generative AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In chapter 5, we showed how to use retrieval-augmented generation (RAG) to generate
    chatbot responses at runtime. But if you prefer static and controlled dialogue,
    you can still use generative AI to assist your conversational designer during
    the build phase of a project. Conversational designers are excellent at crafting
    dialogue that meets the needs of both the system and users. If you don’t have
    a designer handy, you can use generative AI to help you craft dialogue messages
    that achieve your goals.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll demonstrate several techniques for improving static output
    responses using generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: 11.5.1 Improving error messages with generative AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It’s difficult to write good error dialogue. Our first instinct can be to provide
    information that’s technically true but brusque to the user. This is especially
    difficult in voice channels, where the user may make a mistake and not be aware
    of it. For instance, a user being asked to provide an identifier of a certain
    length may forget one of the digits (or mistakenly add an additional digit).
  prefs: []
  type: TYPE_NORMAL
- en: For a Social Security number (nine digits in length), a true-but-brusque error
    message would be “I did not get nine digits. Enter a valid Social Security number
    now.” Let’s use generative AI to improve that message.
  prefs: []
  type: TYPE_NORMAL
- en: Since this is a creative task, we’ll use an instructible model (mixtral-8x7b-instruct-v01-q)
    and sampling decoding. With sampling decoding, the responses will be nondeterministic.
    We’ll run this prompt multiple times to generate multiple responses to choose
    from.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.10 Improving error messages with generative AI—iteration 1
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Instruction and grounding for the model'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Instruction and grounding for the model'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Scenario and input message to improve'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 LLM output'
  prefs: []
  type: TYPE_NORMAL
- en: 'Running that prompt three times generated these outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: “Please enter your 9-digit SSN.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Please enter your full 9 digit Social Security Number.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Please enter 9 digits for the Social Security Number. Thank you!”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Those messages are a definite improvement. They could be even better with an
    apology included. Let’s augment the instructions to include an apology.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.11 Improving error messages with generative AI—iteration 2
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Original instruction'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 New instruction— add an apology'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Original scenario and input message'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 LLM output'
  prefs: []
  type: TYPE_NORMAL
- en: 'Running that prompt six times generated these outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: “Sorry, please enter a 9-digit social security number.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Apologies. Please enter correct Social Security number.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Apologies for trouble, try again with 9 digits.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Apologies for that, let’s try again. Enter a 9-digit SSN please.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “I apologize, please enter a 9-digit Social Security Number.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “My apologies, that’s not quite right. Please try again with 9 digits.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can take one of these messages as is or mix and match to generate a new one.
    Perhaps use “Sorry, let’s try again. Please enter your 9-digit social security
    number.”
  prefs: []
  type: TYPE_NORMAL
- en: 11.5.2 Improving greeting messages with generative AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We experienced a greeting message that confused some of the users calling in
    to the utility company. The users were passed from one IVR to another in a jarring
    manner. Could generative AI help with an updated greeting? Let’s see in the following
    listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.12 Improving greeting messages with generative AI—iteration 1
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '#1 LLM output'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first output is pretty good, but we can generate a few more:'
  prefs: []
  type: TYPE_NORMAL
- en: '“Hello! Welcome to ABC Energy. I understand that you’d like to discontinue
    your service. Before we proceed, may I ask which type of account you’re calling
    about: residential or commercial? Additionally, could you please provide your
    current address so we can properly process your request? Thank you.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Hello! Welcome to ABC Energy. I’m here to assist you today. To begin, could
    you please tell me whether this is a residential or commercial account? Once I
    have that information, I can help you with any questions or concerns you may have
    about stopping your service.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output messages give us several ideas on how to improve the greeting. They
    all include a salutation, an identification of the company, and a preamble. Then
    they include the key question, as well as some commentary after the question.
    Only one problem: we should end the message with a question—this is the invitation
    for the user to speak. (This is important in voice solutions, as we don’t want
    to ask the user a question but continue speaking—this tends to result in the user
    answering while the bot continues speaking.) We could manually remove that last
    part or try an updated instruction to the LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: The following listing attempts a new instruction.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.13 Improving greeting messages with generative AI—iteration 2
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '#1 New instruction'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 LLM output'
  prefs: []
  type: TYPE_NORMAL
- en: This message is better, but it still ends with “Thank you!” We instructed the
    LLM not to do this, but it still did. In fact, we ran this prompt multiple times,
    and it ended the output this way each time. We even changed the instruction to
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This is ultimately a minor annoyance. We can remove the “Thank you!” and proceed
    with the updated message.
  prefs: []
  type: TYPE_NORMAL
- en: The greeting message could be improved by outlining the capabilities of the
    bot and letting the user know what expectations we have of them. The following
    listing improves the greeting once more.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.14 Improving greeting messages with generative AI—iteration 3
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Detailed instruction'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Detailed instruction'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Simplistic greeting that we ask the LLM to improve'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 LLM output'
  prefs: []
  type: TYPE_NORMAL
- en: 'An excellent greeting! Recall that we are using sampling decoding and getting
    nondeterministic responses. You’ll most likely get different output. Here are
    a few more outputs from the LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: “Welcome to ABC Energy! I can assist you in discontinuing your electric service
    today. To ensure accuracy, I’ll need to ask a few questions regarding your account
    details. Please note that no charges will be applied until the process is finalized.
    Are you prepared to proceed?”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Welcome to ABC Energy! I can assist you in discontinuing your electric service.
    To ensure accuracy, I’ll ask several questions related to your account. Please
    note that no charges will be applied during this process. Are you prepared to
    proceed with providing the necessary details?”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can experiment with different instructions to change the greeting or proceed
    with your favorite message.
  prefs: []
  type: TYPE_NORMAL
- en: One improvement is to incentivize the user by letting them know the process
    is quick. We can augment the instructions to note that. The next listing adds
    the instruction “The user should be incentivized that the questions are easy and
    don’t take long to answer.”
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.15 Improving greeting messages with generative AI—iteration 4
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '#1 New instruction'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Original greeting that we ask the LLM to improve'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 LLM output (new output in bold)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The LLM seamlessly transforms the message with brevity: the new message explains
    that the next questions in the process are designed be “quick and easy” (for the
    user).'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s augment the greeting one more time. The user should be assured that customer
    service will step in if needed. In the next listing, we’ll add the instruction
    “The output must let the user know a customer service representative will get
    involved if the user cannot complete the automated process.”
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.16 Improving greeting messages with generative AI—iteration 5
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '#1 New instruction'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Original greeting that we ask the LLM to improve'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 LLM output (new output in bold)'
  prefs: []
  type: TYPE_NORMAL
- en: The greeting conveys everything we need, but it’s starting to get long. Let’s
    ask the bot to shorten it by adding one more instruction.
  prefs: []
  type: TYPE_NORMAL
- en: The following listing adds “The output should be 40 words or less.”
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.17 Improving greeting messages with generative AI—iteration 6
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '#1 New instruction'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Original greeting that we ask the LLM to improve'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 LLM output'
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI is an excellent partner for improving dialogue messages!
  prefs: []
  type: TYPE_NORMAL
- en: 11.6 Sometimes it’s okay to escalate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There may be scenarios where your virtual assistant is going to deliver information
    that the user is not happy about. For example, if a user submitted a claim and
    hears it was denied, they probably aren’t going to be satisfied and will want
    to rectify the situation. If the automated solution doesn’t provide a path to
    resolution or reconsideration, they may be inclined to opt out at this juncture.
  prefs: []
  type: TYPE_NORMAL
- en: With good planning, you can design proactive flows that anticipate follow-on
    user needs. That way, even if escalation is the appropriate next-best action,
    your metrics can distinguish between an opt-out and an intentional transfer for
    business reasons. In the next chapter, we’ll discuss how to optimize the handoff
    to a human agent.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Opt-outs are a major source of containment loss, which causes a virtual agent
    to fail on delivering business value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Users who opt out early in a conversation tend to do so for reasons related
    to their perception of a virtual agent’s capability and whether they are confident
    that the virtual agent can usher the user to their end goal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Opt-outs that occur later in a conversation are indicators that a virtual agent
    might have weak understanding or problems with the dialogue design.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Opt-out retention is a great strategy for improving containment; it can also
    provide valuable data about what users expect your bot to be able to do.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative AI can supplement the process of crafting tactful, efficient responses
    throughout your dialogue flows.****
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
