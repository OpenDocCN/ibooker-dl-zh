- en: Chapter 8\. Unsupervised Learning Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Yann LeCun, Turing Award winner and Meta’s Chief AI Scientist, famously said
    that “if intelligence was a cake, unsupervised learning would be the cake, supervised
    learning would be the icing on the cake, and reinforcement learning would be the
    cherry on the cake” (NeurIPS 2016). In other words, there is a huge potential
    in unsupervised learning that we have only barely started to sink our teeth into.
    Indeed, the vast majority of the available data is unlabeled: we have the input
    features **X**, but we do not have the labels **y**.'
  prefs: []
  type: TYPE_NORMAL
- en: Say you want to create a system that will take a few pictures of each item on
    a manufacturing production line and detect which items are defective. You can
    fairly easily create a system that will take pictures automatically, and this
    might give you thousands of pictures every day. You can then build a reasonably
    large dataset in just a few weeks. But wait, there are no labels! If you want
    to train a regular binary classifier that will predict whether an item is defective
    or not, you will need to label every single picture as “defective” or “normal”.
    This will generally require human experts to sit down and manually go through
    all the pictures. This is a long, costly, and tedious task, so it will usually
    only be done on a small subset of the available pictures. As a result, the labeled
    dataset will be quite small, and the classifier’s performance will be disappointing.
    Moreover, every time the company makes any change to its products, the whole process
    will need to be started over from scratch. Wouldn’t it be great if the algorithm
    could just exploit the unlabeled data without needing humans to label every picture?
    Enter unsupervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Chapter 7](ch07.html#dimensionality_chapter) we looked at the most common
    unsupervised learning task: dimensionality reduction. In this chapter we will
    look at a few more unsupervised tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs: []
  type: TYPE_NORMAL
- en: The goal is to group similar instances together into *clusters*. Clustering
    is a great tool for data analysis, customer segmentation, recommender systems,
    search engines, image segmentation, semi-supervised learning, dimensionality reduction,
    and more.
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection (also called *outlier detection*)
  prefs: []
  type: TYPE_NORMAL
- en: The objective is to learn what “normal” data looks like, and then use that to
    detect abnormal instances. These instances are called *anomalies*, or *outliers*,
    while the normal instances are called *inliers*. Anomaly detection is useful in
    a wide variety of applications, such as fraud detection, detecting defective products
    in manufacturing, identifying new trends in time series, or removing outliers
    from a dataset before training another model, which can significantly improve
    the performance of the resulting model.
  prefs: []
  type: TYPE_NORMAL
- en: Density estimation
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the task of estimating the *probability density function* (PDF) of
    the random process that generated the dataset.⁠^([1](ch08.html#id1959)) Density
    estimation is commonly used for anomaly detection: instances located in very low-density
    regions are likely to be anomalies. It is also useful for data analysis and visualization.'
  prefs: []
  type: TYPE_NORMAL
- en: Ready for some cake? We will start with two clustering algorithms, *k*-means
    and DBSCAN, then we’ll discuss Gaussian mixture models and see how they can be
    used for density estimation, clustering, and anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clustering Algorithms: k-means and DBSCAN'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you enjoy a hike in the mountains, you stumble upon a plant you have never
    seen before. You look around and you notice a few more. They are not identical,
    yet they are sufficiently similar for you to know that they most likely belong
    to the same species (or at least the same genus). You may need a botanist to tell
    you what species that is, but you certainly don’t need an expert to identify groups
    of similar-looking objects. This is called *clustering*: it is the task of identifying
    similar instances and assigning them to *clusters*, or groups of similar instances.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like in classification, each instance gets assigned to a group. However,
    unlike classification, clustering is an unsupervised task, there are no labels,
    so the algorithm needs to figure out on its own how to group instances. Consider
    [Figure 8-1](#classification_vs_clustering_plot): on the left is the iris dataset
    (introduced in [Chapter 4](ch04.html#linear_models_chapter)), where each instance’s
    species (i.e., its class) is represented with a different marker. It is a labeled
    dataset, for which classification algorithms such as logistic regression, SVMs,
    or random forest classifiers are well suited. On the right is the same dataset,
    but without the labels, so you cannot use a classification algorithm anymore.
    This is where clustering algorithms step in: many of them can easily detect the
    lower-left cluster. It is also quite easy to see with our own eyes, but it is
    not so obvious that the upper-right cluster is composed of two distinct subclusters.
    That said, the dataset has two additional features (sepal length and width) that
    are not represented here, and clustering algorithms can make good use of all features,
    so in fact they identify the three clusters fairly well (e.g., using a Gaussian
    mixture model, only 5 instances out of 150 are assigned to the wrong cluster).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram comparing classification (left) with labeled data and clustering
    (right) with unlabeled data on the iris dataset, highlighting how clustering identifies
    groups without prior labels.](assets/hmls_0801.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8-1\. Classification (left) versus clustering (right): in clustering,
    the dataset is unlabeled so the algorithm must identify the clusters without guidance'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Clustering is used in a wide variety of applications, including:'
  prefs: []
  type: TYPE_NORMAL
- en: Customer segmentation
  prefs: []
  type: TYPE_NORMAL
- en: You can cluster your customers based on their purchases and their activity on
    your website. This is useful to understand who your customers are and what they
    need, so you can adapt your products and marketing campaigns to each segment.
    For example, customer segmentation can be useful in *recommender systems* to suggest
    content that other users in the same cluster enjoyed.
  prefs: []
  type: TYPE_NORMAL
- en: Data analysis
  prefs: []
  type: TYPE_NORMAL
- en: When you analyze a new dataset, it can be helpful to run a clustering algorithm,
    and then analyze each cluster separately.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs: []
  type: TYPE_NORMAL
- en: Once a dataset has been clustered, it is usually possible to measure each instance’s
    *affinity* with each cluster; affinity is any measure of how well an instance
    fits into a cluster. Each instance’s feature vector **x** can then be replaced
    with the vector of its cluster affinities. If there are *k* clusters, then this
    vector is *k*-dimensional. The new vector is typically much lower-dimensional
    than the original feature vector, but it can preserve enough information for further
    processing.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering
  prefs: []
  type: TYPE_NORMAL
- en: The cluster affinities can often be useful as extra features. For example, we
    used *k*-means in [Chapter 2](ch02.html#project_chapter) to add geographic cluster
    affinity features to the California housing dataset, and they helped us get better
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: '*Anomaly detection* (also called *outlier detection*)'
  prefs: []
  type: TYPE_NORMAL
- en: Any instance that has a low affinity to all the clusters is likely to be an
    anomaly. For example, if you have clustered the users of your website based on
    their behavior, you can detect users with unusual behavior, such as an unusual
    number of requests per second.
  prefs: []
  type: TYPE_NORMAL
- en: Semi-supervised learning
  prefs: []
  type: TYPE_NORMAL
- en: If you only have a few labels, you could perform clustering and propagate the
    labels to all the instances in the same cluster. This technique can greatly increase
    the number of labels available for a subsequent supervised learning algorithm,
    and thus improve its performance.
  prefs: []
  type: TYPE_NORMAL
- en: Search engines
  prefs: []
  type: TYPE_NORMAL
- en: Some search engines let you search for images that are similar to a reference
    image. To build such a system, you would first apply a clustering algorithm to
    all the images in your database; similar images would end up in the same cluster.
    Then when a user provides a reference image, all you’d need to do is use the trained
    clustering model to find this image’s cluster, and you could then simply return
    all the images from this cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Image segmentation
  prefs: []
  type: TYPE_NORMAL
- en: By clustering pixels according to their color, then replacing each pixel’s color
    with the mean color of its cluster, it is possible to considerably reduce the
    number of different colors in an image. Image segmentation is used in many object
    detection and tracking systems, as it makes it easier to detect the contour of
    each object.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is no universal definition of what a cluster is: it really depends on
    the context, and different algorithms will capture different kinds of clusters.
    Some algorithms look for instances centered around a particular point, called
    a *centroid*. Others look for continuous regions of densely packed instances:
    these clusters can take on any shape. Some algorithms are hierarchical, looking
    for clusters of clusters. And the list goes on.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will look at two popular clustering algorithms, *k*-means
    and DBSCAN, and explore some of their applications, such as nonlinear dimensionality
    reduction, semi-supervised learning, and anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: k-Means Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Consider the unlabeled dataset represented in [Figure 8-2](#blobs_plot): you
    can clearly see five blobs of instances. The *k*-means algorithm is a simple algorithm
    capable of clustering this kind of dataset very quickly and efficiently, often
    in just a few iterations. It was proposed by Stuart Lloyd at Bell Labs in 1957
    as a technique for pulse-code modulation, but it was only [published](https://homl.info/36)
    outside of the company in 1982.⁠^([2](ch08.html#id1973)) In 1965, Edward W. Forgy
    had published virtually the same algorithm, so *k*-means is sometimes referred
    to as the Lloyd–Forgy algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scatterplot displaying five distinct clusters of data points, illustrating
    an unlabeled dataset suitable for k-means clustering.](assets/hmls_0802.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. An unlabeled dataset composed of five blobs of instances
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s train a *k*-means clusterer on this dataset. It will try to find each
    blob’s center and assign each instance to the closest blob:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note that you have to specify the number of clusters *k* that the algorithm
    must find. In this example, it is pretty obvious from looking at the data that
    *k* should be set to 5, but in general it is not that easy. We will discuss this
    shortly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each instance will be assigned to one of the five clusters. In the context
    of clustering, an instance’s *label* is the index of the cluster to which the
    algorithm assigns this instance; this is not to be confused with the class labels
    in classification, which are used as targets (remember that clustering is an unsupervised
    learning task). The `KMeans` instance preserves the predicted labels of the instances
    it was trained on, available via the `labels_` instance variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also take a look at the five centroids that the algorithm found:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You can easily assign new instances to the cluster whose centroid is closest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If you plot the cluster’s decision boundaries, you get a Voronoi tessellation:
    see [Figure 8-3](#voronoi_plot), where each centroid is represented with an ⓧ.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram showing Voronoi tessellation with k-means centroids marked, highlighting
    decision boundaries between clusters.](assets/hmls_0803.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-3\. k-means decision boundaries (Voronoi tessellation)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The vast majority of the instances were clearly assigned to the appropriate
    cluster, but a few instances were probably mislabeled, especially near the boundary
    between the top-left cluster and the central cluster. Indeed, the *k*-means algorithm
    does not behave very well when the blobs have very different diameters because
    all it cares about when assigning an instance to a cluster is the distance to
    the centroid.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of assigning each instance to a single cluster, which is called *hard
    clustering*, it can be useful to give each instance a score per cluster, which
    is called *soft clustering*. The score can be the distance between the instance
    and the centroid or a similarity score (or affinity), such as the Gaussian radial
    basis function we used in [Chapter 2](ch02.html#project_chapter). In the `KMeans`
    class, the `transform()` method measures the distance from each instance to every
    centroid:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, the first instance in `X_new` is located at a distance of
    about 2.81 from the first centroid, 0.33 from the second centroid, 2.90 from the
    third centroid, 1.49 from the fourth centroid, and 2.89 from the fifth centroid.
    If you have a high-dimensional dataset and you transform it this way, you end
    up with a *k*-dimensional dataset: this transformation can be a very efficient
    nonlinear dimensionality reduction technique. Alternatively, you can use these
    distances as extra features to train another model, as in [Chapter 2](ch02.html#project_chapter).'
  prefs: []
  type: TYPE_NORMAL
- en: The k-means algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So, how does the algorithm work? Well, suppose you were given the centroids.
    You could easily label all the instances in the dataset by assigning each of them
    to the cluster whose centroid is closest. Conversely, if you were given all the
    instance labels, you could easily locate each cluster’s centroid by computing
    the mean of the instances in that cluster. But you are given neither the labels
    nor the centroids, so how can you proceed? Start by placing the centroids randomly
    (e.g., by picking *k* instances at random from the dataset and using their locations
    as centroids). Then label the instances, update the centroids, label the instances,
    update the centroids, and so on until the centroids stop moving. The algorithm
    is guaranteed to converge in a finite number of steps (usually quite small). That’s
    because the mean squared distance between the instances and their closest centroids
    can only go down at each step, and since it cannot be negative, it’s guaranteed
    to converge.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the algorithm in action in [Figure 8-4](#kmeans_algorithm_plot):
    the centroids are initialized randomly (top left), then the instances are labeled
    (top right), then the centroids are updated (center left), the instances are relabeled
    (center right), and so on. As you can see, in just three iterations the algorithm
    has reached a clustering that seems close to optimal.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The computational complexity of the algorithm is generally linear with regard
    to the number of instances *m*, the number of clusters *k*, and the number of
    dimensions *n*. However, this is only true when the data has a clustering structure.
    If it does not, then in the worst-case scenario the complexity can increase exponentially
    with the number of instances. In practice, this rarely happens, and *k*-means
    is generally one of the fastest clustering algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating the k-means algorithm''s process with random centroid
    initialization and subsequent instance labeling, showing potential clustering
    outcomes over iterations.](assets/hmls_0804.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-4\. The *k*-means algorithm
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Although the algorithm is guaranteed to converge, it may not converge to the
    right solution (i.e., it may converge to a local optimum): whether it does or
    not depends on the centroid initialization. [Figure 8-5](#kmeans_variability_plot)
    shows two suboptimal solutions that the algorithm can converge to if you are not
    lucky with the random initialization step.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram showing two suboptimal k-means clustering solutions caused by different
    random centroid initializations.](assets/hmls_0805.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-5\. Suboptimal solutions due to unlucky centroid initializations
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s take a look at a few ways you can mitigate this risk by improving the
    centroid initialization.
  prefs: []
  type: TYPE_NORMAL
- en: Centroid initialization methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you happen to know approximately where the centroids should be (e.g., if
    you ran another clustering algorithm earlier), then you can set the `init` hyperparameter
    to a NumPy array containing the list of centroids:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Another solution is to run the algorithm multiple times with different random
    initializations and keep the best solution. The number of random initializations
    is controlled by the `n_init` hyperparameter: by default it is equal to `10` when
    using `init="random"`, which means that the whole algorithm described earlier
    runs 10 times when you call `fit()`, and Scikit-Learn keeps the best solution.
    But how exactly does it know which solution is the best? It uses a performance
    metric! That metric is called the model’s *inertia*, which is defined in [Equation
    8-1](#inertia_equation).'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 8-1\. A model’s inertia is the sum of all squared distances between
    each instance **x**^((*i*)) and the closest centroid **c**^((*i*)) predicted by
    the model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <mrow><mtext>inertia</mtext> <mo>=</mo> <munder><mo>∑</mo> <mi>i</mi></munder>
    <msup><mrow><mo>∥</mo><msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>-</mo><msup><mi mathvariant="bold">c</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo rspace="0.1em">∥</mo></mrow> <mn>2</mn></msup></mrow>
  prefs: []
  type: TYPE_NORMAL
- en: 'The inertia is roughly equal to 219.6 for the model on the left in [Figure 8-5](#kmeans_variability_plot),
    600.4 for the model on the right in [Figure 8-5](#kmeans_variability_plot), and
    only 211.6 for the model in [Figure 8-3](#voronoi_plot). The `KMeans` class runs
    the initialization algorithm `n_init` times and keeps the model with the lowest
    inertia. In this example, the model in [Figure 8-3](#voronoi_plot) will be selected
    (unless we are very unlucky with `n_init` consecutive random initializations).
    If you are curious, a model’s inertia is accessible via the `inertia_` instance
    variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `score()` method returns the negative inertia (it’s negative because a
    predictor’s `score()` method must always respect Scikit-Learn’s “greater is better”
    rule—if a predictor is better than another, its `score()` method should return
    a greater score):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'An important improvement to the *k*-means algorithm, *k-means++*, was proposed
    in a [2006 paper](https://homl.info/37) by David Arthur and Sergei Vassilvitskii.⁠^([3](ch08.html#id1983))
    They introduced a smarter initialization step that tends to select centroids that
    are distant from one another. This change makes the *k*-means algorithm much more
    likely to locate all important clusters, and less likely to converge to a suboptimal
    solution (just like spreading out fishing boats increases the chance of locating
    more schools of fish). The paper showed that the additional computation required
    for the smarter initialization step is well worth it because it makes it possible
    to drastically reduce the number of times the algorithm needs to be run to find
    the optimal solution. The *k*-means++ initialization algorithm works like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Take one centroid **c**^((1)), chosen uniformly at random from the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take a new centroid **c**^((*i*)), choosing an instance **x**^((*i*)) with probability
    $upper D left-parenthesis bold x Superscript left-parenthesis i right-parenthesis
    Baseline right-parenthesis squared$ / $sigma-summation Underscript j equals 1
    Overscript m Endscripts upper D left-parenthesis bold x Superscript left-parenthesis
    j right-parenthesis Baseline right-parenthesis squared$ , where D(**x**^((*i*)))
    is the distance between the instance **x**^((*i*)) and the closest centroid that
    was already chosen. This probability distribution ensures that instances farther
    away from already chosen centroids are much more likely to be selected as centroids.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat the previous step until all *k* centroids have been chosen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When you set `init="k-means++"` (which is the default), the `KMeans` class
    actually uses a variant of *k*-means++ called *greedy k-means++*: instead of sampling
    a single centroid at each iteration, it samples multiple and picks the best one.
    When using this algorithm, `n_init` defaults to 1.'
  prefs: []
  type: TYPE_NORMAL
- en: Accelerated k-means and mini-batch k-means
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another improvement to the *k*-means algorithm was proposed in a [2003 paper](https://homl.info/38)
    by Charles Elkan.⁠^([4](ch08.html#id1989)) On some large datasets with many clusters,
    the algorithm can be accelerated by avoiding many unnecessary distance calculations.
    Elkan achieved this by exploiting the triangle inequality (i.e., that a straight
    line is always the shortest distance between two points⁠^([5](ch08.html#id1990)))
    and by keeping track of lower and upper bounds for distances between instances
    and centroids. However, Elkan’s algorithm does not always accelerate training,
    and sometimes it can even slow down training significantly; it depends on the
    dataset. Still, if you want to give it a try, set `algorithm="elkan"`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Yet another important variant of the *k*-means algorithm was proposed in a
    [2010 paper](https://homl.info/39) by David Sculley.⁠^([6](ch08.html#id1993))
    Instead of using the full dataset at each iteration, the algorithm is capable
    of using mini-batches, moving the centroids just slightly at each iteration. This
    speeds up the algorithm and makes it possible to cluster huge datasets that do
    not fit in memory. Scikit-Learn implements this algorithm in the `MiniBatchKMeans`
    class, which you can use just like the `KMeans` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: If the dataset does not fit in memory, the simplest option is to use the `memmap`
    class, as we did for incremental PCA in [Chapter 7](ch07.html#dimensionality_chapter).
    Alternatively, you can pass one mini-batch at a time to the `partial_fit()` method,
    but this will require much more work, since you will need to perform multiple
    initializations and select the best one yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the optimal number of clusters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we’ve set the number of clusters *k* to 5 because it was obvious by
    looking at the data that this was the correct number of clusters. But in general,
    it won’t be so easy to know how to set *k*, and the result might be quite bad
    if you set it to the wrong value. As you can see in [Figure 8-6](#bad_n_clusters_plot),
    for this dataset setting *k* to 3 or 8 results in fairly bad models.
  prefs: []
  type: TYPE_NORMAL
- en: You might be thinking that you could just pick the model with the lowest inertia.
    Unfortunately, it is not that simple. The inertia for *k* = 3 is about 653.2,
    which is much higher than for *k* = 5 (211.7). But with *k* = 8, the inertia is
    just 127.1\. The inertia is not a good performance metric when trying to choose
    *k* because it keeps getting lower as we increase *k*. Indeed, the more clusters
    there are, the closer each instance will be to its closest centroid, and therefore
    the lower the inertia will be. Let’s plot the inertia as a function of *k*. When
    we do this, the curve often contains an inflexion point called the *elbow* (see
    [Figure 8-7](#inertia_vs_k_plot)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram showing clustering outcomes; with _k_ = 3, distinct clusters merge,
    and with _k_ = 8, some clusters split into multiple parts.](assets/hmls_0806.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8-6\. Bad choices for the number of clusters: when k is too small, separate
    clusters get merged (left), and when k is too large, some clusters get chopped
    into multiple pieces (right)'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Line graph showing inertia decreasing sharply with increasing clusters \(k\),
    with an elbow at \(k = 4\).](assets/hmls_0807.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-7\. Plotting the inertia as a function of the number of clusters *k*
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As you can see, the inertia drops very quickly as we increase *k* up to 4,
    but then it decreases much more slowly as we keep increasing *k*. This curve has
    roughly the shape of an arm, and there is an elbow at *k* = 4\. So, if we did
    not know better, we might think 4 was a good choice: any lower value would be
    dramatic, while any higher value would not help much, and we might just be splitting
    perfectly good clusters in half for no good reason.'
  prefs: []
  type: TYPE_NORMAL
- en: This technique for choosing the best value for the number of clusters is rather
    coarse. A more precise (but also more computationally expensive) approach is to
    use the *silhouette score*, which is the mean *silhouette coefficient* over all
    the instances. An instance’s silhouette coefficient is equal to (*b* – *a*) /
    max(*a*, *b*), where *a* is the mean distance to the other instances in the same
    cluster (i.e., the mean intra-cluster distance) and *b* is the mean nearest-cluster
    distance (i.e., the mean distance to the instances of the next closest cluster,
    defined as the one that minimizes *b*, excluding the instance’s own cluster).
    The silhouette coefficient can vary between –1 and +1\. A coefficient close to
    +1 means that the instance is well inside its own cluster and far from other clusters,
    while a coefficient close to 0 means that it is close to a cluster boundary; finally,
    a coefficient close to –1 means that the instance may have been assigned to the
    wrong cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compute the silhouette score, you can use Scikit-Learn’s `silhouette_score()`
    function, giving it all the instances in the dataset and the labels they were
    assigned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Let’s compare the silhouette scores for different numbers of clusters (see [Figure 8-8](#silhouette_score_vs_k_plot)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Line plot showing silhouette scores for various cluster counts, with peaks
    at _k_ = 4 and 5, indicating optimal clustering choices.](assets/hmls_0808.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-8\. Selecting the number of clusters *k* using the silhouette score
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As you can see, this visualization is much richer than the previous one: although
    it confirms that *k* = 4 is a very good choice, it also highlights the fact that
    *k* = 5 is quite good as well, and much better than *k* = 6 or 7\. This was not
    visible when comparing inertias.'
  prefs: []
  type: TYPE_NORMAL
- en: An even more informative visualization is obtained when we plot every instance’s
    silhouette coefficient, sorted by the clusters they are assigned to and by the
    value of the coefficient. This is called a *silhouette diagram* (see [Figure 8-9](#silhouette_analysis_plot)).
    Each diagram contains one knife shape per cluster. The shape’s height indicates
    the number of instances in the cluster, and its width represents the sorted silhouette
    coefficients of the instances in the cluster (wider is better).
  prefs: []
  type: TYPE_NORMAL
- en: 'The vertical dashed lines represent the mean silhouette score for each number
    of clusters. When most of the instances in a cluster have a lower coefficient
    than this score (i.e., if many of the instances stop short of the dashed line,
    ending to the left of it), then the cluster is rather bad since this means its
    instances are much too close to other clusters. Here we can see that when *k*
    = 3 or 6, we get bad clusters. But when *k* = 4 or 5, the clusters look pretty
    good: most instances extend beyond the dashed line, to the right and closer to
    1.0\. When *k* = 4, the cluster at index 0 (at the bottom) is rather big. When
    *k* = 5, all clusters have similar sizes. So, even though the overall silhouette
    score from *k* = 4 is slightly greater than for *k* = 5, it seems like a good
    idea to use *k* = 5 to get clusters of similar sizes.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Silhouette diagrams comparing cluster quality for different values of _k_,
    showing that clusters are more balanced when _k_ = 5, despite slightly higher
    overall scores for _k_ = 4.](assets/hmls_0809.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-9\. Analyzing the silhouette diagrams for various values of *k*
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Limits of k-Means
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite its many merits, most notably being fast and scalable, *k*-means is
    not perfect. As we saw, it is necessary to run the algorithm several times to
    avoid suboptimal solutions, plus you need to specify the number of clusters, which
    can be quite a hassle. Moreover, *k*-means does not behave very well when the
    clusters have varying sizes, different densities, or nonspherical shapes. For
    example, [Figure 8-10](#bad_kmeans_plot) shows how *k*-means clusters a dataset
    containing three ellipsoidal clusters of different dimensions, densities, and
    orientations.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, neither of these solutions is any good. The solution on the
    left is better, but it still chops off 25% of the middle cluster and assigns it
    to the cluster on the right. The solution on the right is just terrible, even
    though its inertia is lower. So, depending on the data, different clustering algorithms
    may perform better. On these types of elliptical clusters, Gaussian mixture models
    work great.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram comparing two k-means clustering attempts on ellipsoidal data, highlighting
    poor performance in clustering accuracy.](assets/hmls_0810.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-10\. k-means fails to cluster these ellipsoidal blobs properly
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is important to scale the input features (see [Chapter 2](ch02.html#project_chapter))
    before you run *k*-means, or the clusters may be very stretched and *k*-means
    will perform poorly. Scaling the features does not guarantee that all the clusters
    will be nice and spherical, but it generally helps *k*-means.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s look at a few ways we can benefit from clustering. We will use *k*-means,
    but feel free to experiment with other clustering algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Using Clustering for Image Segmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Image segmentation* is the task of partitioning an image into multiple segments.
    There are several variants:'
  prefs: []
  type: TYPE_NORMAL
- en: In *color segmentation*, pixels with a similar color get assigned to the same
    segment. This is sufficient in many applications. For example, if you want to
    analyze satellite images to measure how much total forest area there is in a region,
    color segmentation may be just fine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In *semantic segmentation*, all pixels that are part of the same object type
    get assigned to the same segment. For example, in a self-driving car’s vision
    system, all pixels that are part of a pedestrian’s image might be assigned to
    the “pedestrian” segment (there would be one segment containing all the pedestrians).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In *instance segmentation*, all pixels that are part of the same individual
    object are assigned to the same segment. In this case there would be a different
    segment for each pedestrian.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The state of the art in semantic or instance segmentation today is achieved
    using complex architectures based on convolutional neural networks (see [Chapter 12](ch12.html#cnn_chapter))
    or vision transformers (see [Chapter 16](ch16.html#vit_chapter)). In this chapter
    we are going to focus on the (much simpler) color segmentation task, using *k*-means.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start by importing the Pillow package (successor to the Python Imaging
    Library, PIL), which we’ll then use to load the *ladybug.png* image (see the upper-left
    image in [Figure 8-11](#image_segmentation_plot)), assuming it’s located at `filepath`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The image is represented as a 3D array. The first dimension’s size is the height;
    the second is the width; and the third is the number of color channels, in this
    case red, green, and blue (RGB). In other words, for each pixel there is a 3D
    vector containing the intensities of red, green, and blue as unsigned 8-bit integers
    between 0 and 255\. Some images may have fewer channels (such as grayscale images,
    which only have one), and some images may have more channels (such as images with
    an additional *alpha channel* for transparency, or satellite images, which often
    contain channels for additional light frequencies, like infrared).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code reshapes the array to get a long list of RGB colors, then
    it clusters these colors using *k*-means with eight clusters. It creates a `segmented_img`
    array containing the nearest cluster center for each pixel (i.e., the mean color
    of each pixel’s cluster), and lastly it reshapes this array to the original image
    shape. The third line uses advanced NumPy indexing; for example, if the first
    10 labels in `kmeans_.labels_` are equal to 1, then the first 10 colors in `segmented_img`
    are equal to `kmeans.cluster_centers_[1]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This outputs the image shown in the upper right of [Figure 8-11](#image_segmentation_plot).
    You can experiment with various numbers of clusters, as shown in the figure. When
    you use fewer than eight clusters, notice that the ladybug’s flashy red color
    fails to get a cluster of its own: it gets merged with colors from the environment.
    This is because *k*-means prefers clusters of similar sizes. The ladybug is small—much
    smaller than the rest of the image—so even though its color is flashy, *k*-means
    fails to dedicate a cluster to it.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image showing the effect of _k_-means clustering with 10, 8, 6, 4, and 2
    color clusters on an original image of a ladybug on a dandelion, illustrating
    how fewer clusters merge colors and lose detail.](assets/hmls_0811.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-11\. Image segmentation using *k*-means with various numbers of color
    clusters
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: That wasn’t too hard, was it? Now let’s look at another application of clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Using Clustering for Semi-Supervised Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another use case for clustering is in semi-supervised learning, when we have
    plenty of unlabeled instances and very few labeled instances. For example, clustering
    can help choose which additional instances to label (e.g., near the cluster centroids).
    It can also be used to propagate the most common label in each cluster to the
    unlabeled instances in that cluster. Let’s try these ideas on the digits dataset,
    which is a simple MNIST-like dataset containing 1,797 grayscale 8 × 8 images representing
    the digits 0 to 9\. First, let’s load and split the dataset (it’s already shuffled):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We will pretend we only have labels for 50 instances. To get a baseline performance,
    let’s train a logistic regression model on these 50 labeled instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then measure the accuracy of this model on the test set (note that the
    test set must be labeled):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The model’s accuracy is just 75.8%. That’s not great: indeed, if you try training
    the model on the full training set, you will find that it will reach about 90.9%
    accuracy. Let’s see how we can do better. First, let’s cluster the training set
    into 50 clusters. Then, for each cluster, we’ll find the image closest to the
    centroid. We’ll call these images the *representative images*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 8-12](#representative_images_plot) shows the 50 representative images.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fifty handwritten digit images, each representing a distinct cluster for
    analysis.](assets/hmls_0812.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-12\. Fifty representative digit images (one per cluster)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s look at each image and manually label them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have a dataset with just 50 labeled instances, but instead of being
    random instances, each of them is a representative image of its cluster. Let’s
    see if the performance is any better:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Wow! We jumped from 75.8% accuracy to 83.4%, although we are still only training
    the model on 50 instances. Since it is often costly and painful to label instances,
    especially when it has to be done manually by experts, it is a good idea to label
    representative instances rather than just random instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'But perhaps we can go one step further: what if we propagated the labels to
    all the other instances in the same cluster? This is called *label propagation*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s train the model again and look at its performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We got another significant accuracy boost! Let’s see if we can do even better
    by ignoring the 50% of instances that are farthest from their cluster center:
    this should eliminate some outliers. The following code first computes the distance
    from each instance to its closest cluster center, then for each cluster it sets
    the 50% largest distances to –1\. Lastly, it creates a set without these instances
    marked with a –1 distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s train the model again on this partially propagated dataset and see
    what accuracy we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Nice! With just 50 labeled instances (only 5 examples per class on average!)
    we got 88.4% accuracy, pretty close to the performance we got on the fully labeled
    digits dataset. This is partly thanks to the fact that we dropped some outliers,
    and partly because the propagated labels are actually pretty good—their accuracy
    is about 98.9%, as the following code shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Scikit-Learn also offers two classes that can propagate labels automatically:
    `LabelSpreading` and `LabelPropagation` in the `sklearn.​semi_supervised` package.
    Both classes construct a similarity matrix between all the instances, and iteratively
    propagate labels from labeled instances to similar unlabeled instances. There’s
    also a different class called `SelfTrainingClassifier` in the same package: you
    give it a base classifier (e.g., `RandomForestClassifier`) and it trains it on
    the labeled instances, then uses it to predict labels for the unlabeled samples.
    It then updates the training set with the labels it is most confident about, and
    repeats this process of training and labeling until it cannot add labels anymore.
    These techniques are not magic bullets, but they can occasionally give your model
    a little boost.'
  prefs: []
  type: TYPE_NORMAL
- en: Before we move on to Gaussian mixture models, let’s take a look at DBSCAN, another
    popular clustering algorithm that illustrates a very different approach based
    on local density estimation. This approach allows the algorithm to identify clusters
    of arbitrary shapes.
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The *density-based spatial clustering of applications with noise* (DBSCAN)
    algorithm defines clusters as continuous regions of high density. Here is how
    it works:'
  prefs: []
  type: TYPE_NORMAL
- en: For each instance, the algorithm counts how many instances are located within
    a small distance ε (epsilon) from it. This region is called the instance’s *ε-neighborhood*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If an instance has at least `min_samples` instances in its ε-neighborhood (including
    itself), then it is considered a *core instance*. In other words, core instances
    are those that are located in dense regions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All instances in the neighborhood of a core instance belong to the same cluster.
    This neighborhood may include other core instances; therefore, a long sequence
    of neighboring core instances forms a single cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any instance that is not a core instance and does not have one in its neighborhood
    is considered an anomaly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This algorithm works well if all the clusters are well separated by low-density
    regions. The `DBSCAN` class in Scikit-Learn is as simple to use as you might expect.
    Let’s test it on the moons dataset, introduced in [Chapter 5](ch05.html#trees_chapter):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The labels of all the instances are now available in the `labels_` instance
    variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that some instances have a cluster index equal to –1, which means that
    they are considered as anomalies by the algorithm. The indices of the core instances
    are available in the `core_sample_indices_` instance variable, and the core instances
    themselves are available in the `components_` instance variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This clustering is represented in the lefthand plot of [Figure 8-13](#dbscan_plot).
    As you can see, it identified quite a lot of anomalies, plus seven different clusters.
    How disappointing! Fortunately, if we widen each instance’s neighborhood by increasing
    `eps` to 0.2, we get the clustering on the right, which looks perfect. Let’s continue
    with this model.
  prefs: []
  type: TYPE_NORMAL
- en: '![DBSCAN clustering results with `eps` values of 0.05 and 0.20, showing improved
    clustering with fewer anomalies in the right plot.](assets/hmls_0813.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-13\. DBSCAN clustering using two different neighborhood radiuses
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Surprisingly, the `DBSCAN` class does not have a `predict()` method, although
    it has a `fit_predict()` method. In other words, it cannot predict which cluster
    a new instance belongs to. This decision was made because different classification
    algorithms can be better for different tasks, so the authors decided to let the
    user choose which one to use. Moreover, it’s not hard to implement. For example,
    let’s train a `KNeighborsClassifier`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, given a few new instances, we can predict which clusters they most likely
    belong to and even estimate a probability for each cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we only trained the classifier on the core instances, but we could
    also have chosen to train it on all the instances, or all but the anomalies: this
    choice depends on the final task.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The decision boundary is represented in [Figure 8-14](#cluster_classification_plot)
    (the crosses represent the four instances in `X_new`). Notice that since there
    is no anomaly in the training set, the classifier always chooses a cluster, even
    when that cluster is far away. It is fairly straightforward to introduce a maximum
    distance, in which case the two instances that are far away from both clusters
    are classified as anomalies. To do this, use the `kneighbors()` method of the
    `KNeighborsClassifier`. Given a set of instances, it returns the distances and
    the indices of the *k*-nearest neighbors in the training set (two matrices, each
    with *k* columns):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '![Diagram showing a decision boundary between two clusters, illustrating how
    DBSCAN identifies clusters of varying shapes.](assets/hmls_0814.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-14\. Decision boundary between two clusters
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In short, DBSCAN is a very simple yet powerful algorithm capable of identifying
    any number of clusters of any shape. It is robust to outliers, and it has just
    two hyperparameters (`eps` and `min_samples`). If the density varies significantly
    across the clusters, however, or if there’s no sufficiently low-density region
    around some clusters, DBSCAN can struggle to capture all the clusters properly.
    Moreover, its computational complexity is roughly *O*(*m*²*n*), so it does not
    scale well to large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You may also want to try *hierarchical DBSCAN* (HDBSCAN), using `sklearn.cluster.HDBSCAN`:
    it is often better than DBSCAN at finding clusters of varying densities.'
  prefs: []
  type: TYPE_NORMAL
- en: Other Clustering Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Scikit-Learn implements several more clustering algorithms that you should
    take a look at. I cannot cover them all in detail here, but here is a brief overview:'
  prefs: []
  type: TYPE_NORMAL
- en: Agglomerative clustering
  prefs: []
  type: TYPE_NORMAL
- en: A hierarchy of clusters is built from the bottom up. Think of many tiny bubbles
    floating on water and gradually attaching to each other until there’s one big
    group of bubbles. Similarly, at each iteration, agglomerative clustering connects
    the nearest pair of clusters (starting with individual instances). If you drew
    a tree with a branch for every pair of clusters that merged, you would get a binary
    tree of clusters, where the leaves are the individual instances. This approach
    can capture clusters of various shapes; it also produces a flexible and informative
    cluster tree instead of forcing you to choose a particular cluster scale, and
    it can be used with any pairwise distance. It can scale nicely to large numbers
    of instances if you provide a connectivity matrix, which is a sparse *m* × *m*
    matrix that indicates which pairs of instances are neighbors (e.g., returned by
    `sklearn.neighbors.kneighbors_graph()`). Without a connectivity matrix, the algorithm
    does not scale well to large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: BIRCH
  prefs: []
  type: TYPE_NORMAL
- en: 'The balanced iterative reducing and clustering using hierarchies (BIRCH) algorithm
    was designed specifically for very large datasets, and it can be faster than batch
    *k*-means, with similar results, as long as the number of features is not too
    large (<20). During training, it builds a tree structure containing just enough
    information to quickly assign each new instance to a cluster, without having to
    store all the instances in the tree: this approach allows it to use limited memory
    while handling huge datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: Mean-shift
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm starts by placing a circle centered on each instance; then for
    each circle it computes the mean of all the instances located within it, and it
    shifts the circle so that it is centered on the mean. Next, it iterates this mean-shifting
    step until all the circles stop moving (i.e., until each of them is centered on
    the mean of the instances it contains). Mean-shift shifts the circles in the direction
    of higher density, until each of them has found a local density maximum. Finally,
    all the instances whose circles have settled in the same place (or close enough)
    are assigned to the same cluster. Mean-shift has some of the same features as
    DBSCAN, like how it can find any number of clusters of any shape, it has very
    few hyperparameters (just one—the radius of the circles, called the *bandwidth*),
    and it relies on local density estimation. But unlike DBSCAN, mean-shift tends
    to chop clusters into pieces when they have internal density variations. Unfortunately,
    its computational complexity is *O*(*m*²*n*), so it is not suited for large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Affinity propagation
  prefs: []
  type: TYPE_NORMAL
- en: 'In this algorithm, instances repeatedly exchange messages between one another
    until every instance has elected another instance (or itself) to represent it.
    These elected instances are called *exemplars*. Each exemplar and all the instances
    that elected it form one cluster. In real-life politics, you typically want to
    vote for a candidate whose opinions are similar to yours, but you also want them
    to win the election, so you might choose a candidate you don’t fully agree with,
    but who is more popular. You typically evaluate popularity through polls. Affinity
    propagation works in a similar way, and it tends to choose exemplars located near
    the center of clusters, similar to *k*-means. But unlike with *k*-means, you don’t
    have to pick a number of clusters ahead of time: it is determined during training.
    Moreover, affinity propagation can deal nicely with clusters of different sizes.
    Sadly, this algorithm has a computational complexity of *O*(*m*²), so it is not
    suited for large datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: Spectral clustering
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm takes a similarity matrix between the instances and creates a
    low-dimensional embedding from it (i.e., it reduces the matrix’s dimensionality),
    then it uses another clustering algorithm in this low-dimensional space (Scikit-Learn’s
    implementation uses *k*-means). Spectral clustering can capture complex cluster
    structures, and it can also be used to cut graphs (e.g., to identify clusters
    of friends on a social network). It does not scale well to large numbers of instances,
    and it does not behave well when the clusters have very different sizes.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s dive into Gaussian mixture models, which can be used for density estimation,
    clustering, and anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian Mixtures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A *Gaussian mixture model* (GMM) is a probabilistic model that assumes that
    the instances were generated from a mixture of several Gaussian distributions
    whose parameters are unknown. All the instances generated from a single Gaussian
    distribution form a cluster that typically looks like an ellipsoid. Each cluster
    can have a different ellipsoidal shape, size, density, and orientation, just like
    in [Figure 8-10](#bad_kmeans_plot).⁠^([7](ch08.html#id2046)) When you observe
    an instance, you know it was generated from one of the Gaussian distributions,
    but you are not told which one, and you do not know what the parameters of these
    distributions are.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several GMM variants. In the simplest variant, implemented in the
    `GaussianMixture` class, you must know in advance the number *k* of Gaussian distributions.
    The dataset **X** is assumed to have been generated through the following probabilistic
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: For each instance, a cluster is picked randomly from among *k* clusters. The
    probability of choosing the *j*^(th) cluster is the cluster’s weight *ϕ*^((*j*)).⁠^([8](ch08.html#id2050))
    The index of the cluster chosen for the *i*^(th) instance is denoted *z*^((*i*)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the *i*^(th) instance was assigned to the *j*^(th) cluster (i.e., *z*^((*i*))
    = *j*), then the location **x**^((*i*)) of this instance is sampled randomly from
    the Gaussian distribution with mean **μ**^((*j*)) and covariance matrix **Σ**^((*j*)).
    This is denoted **x**^((*i*)) ~ 𝒩(μ*^((*j*)), **Σ**^((*j*))).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So what can you do with such a model? Well, given the dataset **X**, you typically
    want to start by estimating the weights **ϕ** and all the distribution parameters
    **μ**^((1)) to **μ**^((*k*)) and **Σ**^((1)) to **Σ**^((*k*)). Scikit-Learn’s
    `GaussianMixture` class makes this super easy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at the parameters that the algorithm estimated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Great, it worked fine! Indeed, two of the three clusters were generated with
    500 instances each, while the third cluster only contains 250 instances. So the
    true cluster weights are 0.4, 0.4, and 0.2, respectively, and that’s roughly what
    the algorithm found (in a different order). Similarly, the true means and covariance
    matrices are quite close to those found by the algorithm. But how? This class
    relies on the *expectation-maximization* (EM) algorithm, which has many similarities
    with the *k*-means algorithm: it also initializes the cluster parameters randomly,
    then it repeats two steps until convergence, first assigning instances to clusters
    (this is called the *expectation step*) and then updating the clusters (this is
    called the *maximization step*). Sounds familiar, right? In the context of clustering,
    you can think of EM as a generalization of *k*-means that not only finds the cluster
    centers (**μ**^((1)) to **μ**^((*k*))), but also their size, shape, and orientation
    (**Σ**^((1)) to **Σ**^((*k*))), as well as their relative weights (*ϕ*^((1)) to
    *ϕ*^((*k*))). Unlike *k*-means, though, EM uses soft cluster assignments, not
    hard assignments. For each instance, during the expectation step, the algorithm
    estimates the probability that it belongs to each cluster (based on the current
    cluster parameters). Then, during the maximization step, each cluster is updated
    using *all* the instances in the dataset, with each instance weighted by the estimated
    probability that it belongs to that cluster. These probabilities are called the
    *responsibilities* of the clusters for the instances. During the maximization
    step, each cluster’s update will mostly be impacted by the instances it is most
    responsible for.'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Unfortunately, just like *k*-means, EM can end up converging to poor solutions,
    so it needs to be run several times, keeping only the best solution. This is why
    we set `n_init` to 10\. Be careful: by default `n_init` is set to 1.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can check whether the algorithm converged and how many iterations it took:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that you have an estimate of the location, size, shape, orientation, and
    relative weight of each cluster, the model can easily assign each instance to
    the most likely cluster (hard clustering) or estimate the probability that it
    belongs to a particular cluster (soft clustering). Just use the `predict()` method
    for hard clustering, or the `predict_proba()` method for soft clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'A Gaussian mixture model is a *generative model*, meaning you can sample new
    instances from it (note that they are ordered by cluster index):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'It is also possible to estimate the density of the model at any given location.
    This is achieved using the `score_samples()` method: for each instance it is given,
    this method estimates the log of the *probability density function* (PDF) at that
    location. The greater the score, the higher the density:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'If you compute the exponential of these scores, you get the value of the PDF
    at the location of the given instances. These are not probabilities, but probability
    *densities*: they can take on any positive value, not just a value between 0 and
    1\. To estimate the probability that an instance will fall within a particular
    region, you would have to integrate the PDF over that region (if you do so over
    the entire space of possible instance locations, the result will be 1).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 8-15](#gaussian_mixtures_plot) shows the cluster means, the decision
    boundaries (dashed lines), and the density contours of this model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram of a Gaussian mixture model showing three cluster means, decision
    boundaries as dashed lines, and density contours indicating distribution.](assets/hmls_0815.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-15\. Cluster means, decision boundaries, and density contours of a
    trained Gaussian mixture model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Nice! The algorithm clearly found an excellent solution. Of course, we made
    its task easy by generating the data using a set of 2D Gaussian distributions
    (unfortunately, real-life data is not always so Gaussian and low-dimensional).
    We also gave the algorithm the correct number of clusters. When there are many
    dimensions, or many clusters, or few instances, EM can struggle to converge to
    the optimal solution. You might need to reduce the difficulty of the task by limiting
    the number of parameters that the algorithm has to learn. One way to do this is
    to limit the range of shapes and orientations that the clusters can have. This
    can be achieved by imposing constraints on the covariance matrices. To do this,
    set the `covariance_type` hyperparameter to one of the following values:'
  prefs: []
  type: TYPE_NORMAL
- en: '`"spherical"`'
  prefs: []
  type: TYPE_NORMAL
- en: All clusters must be spherical, but they can have different diameters (i.e.,
    different variances).
  prefs: []
  type: TYPE_NORMAL
- en: '`"diag"`'
  prefs: []
  type: TYPE_NORMAL
- en: Clusters can take on any ellipsoidal shape of any size, but the ellipsoid’s
    axes must be parallel to the coordinate axes (i.e., the covariance matrices must
    be diagonal).
  prefs: []
  type: TYPE_NORMAL
- en: '`"tied"`'
  prefs: []
  type: TYPE_NORMAL
- en: All clusters must have the same ellipsoidal shape, size, and orientation (i.e.,
    all clusters share the same covariance matrix).
  prefs: []
  type: TYPE_NORMAL
- en: By default, `covariance_type` is equal to `"full"`, which means that each cluster
    can take on any shape, size, and orientation (it has its own unconstrained covariance
    matrix). [Figure 8-16](#covariance_type_plot) plots the solutions found by the
    EM algorithm when `covariance_type` is set to `"tied"` or `"spherical"`.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram showing Gaussian mixture models for tied covariance (left) with oval
    clusters and spherical covariance (right) with circular clusters.](assets/hmls_0816.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-16\. Gaussian mixtures for tied clusters (left) and spherical clusters
    (right)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The computational complexity of training a `GaussianMixture` model depends on
    the number of instances *m*, the number of dimensions *n*, the number of clusters
    *k*, and the constraints on the covariance matrices. If `covariance_type` is `"spherical"`
    or `"diag"`, it is *O*(*kmn*), assuming the data has a clustering structure. If
    `covariance_type` is `"tied"` or `"full"`, it is *O*(*kmn*² + *kn*³), so it will
    not scale to large numbers of features.
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian mixture models can also be used for anomaly detection. We’ll see how
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Using Gaussian Mixtures for Anomaly Detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using a Gaussian mixture model for anomaly detection is quite simple: any instance
    located in a low-density region can be considered an anomaly. You must define
    what density threshold you want to use. For example, in a manufacturing company
    that tries to detect defective products, the ratio of defective products is usually
    well known. Say it is equal to 2%. You then set the density threshold to be the
    value that results in having 2% of the instances located in areas below that threshold
    density. If you notice that you get too many false positives (i.e., perfectly
    good products that are flagged as defective), you can lower the threshold. Conversely,
    if you have too many false negatives (i.e., defective products that the system
    does not flag as defective), you can increase the threshold. This is the usual
    precision/recall trade-off (see [Chapter 3](ch03.html#classification_chapter)).
    Here is how you would identify the outliers using the second percentile lowest
    density as the threshold (i.e., approximately 2% of the instances will be flagged
    as anomalies):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 8-17](#mixture_anomaly_detection_plot) represents these anomalies as
    stars.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Contour plot illustrating anomaly detection with a Gaussian mixture model,
    where anomalies are marked with red stars.](assets/hmls_0817.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-17\. Anomaly detection using a Gaussian mixture model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'A closely related task is *novelty detection*: it differs from anomaly detection
    in that the algorithm is assumed to be trained on a “clean” dataset, uncontaminated
    by outliers, whereas anomaly detection does not make this assumption. Indeed,
    outlier detection is often used to clean up a dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Gaussian mixture models try to fit all the data, including the outliers; if
    you have too many of them this will bias the model’s view of “normality”, and
    some outliers may wrongly be considered as normal. If this happens, you can try
    to fit the model once, use it to detect and remove the most extreme outliers,
    then fit the model again on the cleaned-up dataset. Another approach is to use
    robust covariance estimation methods (see the `EllipticEnvelope` class).
  prefs: []
  type: TYPE_NORMAL
- en: Just like *k*-means, the `GaussianMixture` algorithm requires you to specify
    the number of clusters. So how can you find that number?
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the Number of Clusters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With *k*-means, you can use the inertia or the silhouette score to select the
    appropriate number of clusters. But with Gaussian mixtures, it is not possible
    to use these metrics because they are not reliable when the clusters are not spherical
    or have different sizes. Instead, you can try to find the model that minimizes
    a *theoretical information criterion*, such as the *Bayesian information criterion*
    (BIC) or the *Akaike information criterion* (AIC), defined in [Equation 8-2](#information_criteria_equation).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 8-2\. Bayesian information criterion (BIC) and Akaike information criterion
    (AIC)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: $StartLayout 1st Row 1st Column Blank 2nd Column upper B upper I upper C equals
    normal l normal o normal g left-parenthesis m right-parenthesis p minus 2 normal
    l normal o normal g left-parenthesis ModifyingAbove script upper L With caret
    right-parenthesis 2nd Row 1st Column Blank 2nd Column upper A upper I upper C
    equals 2 p minus 2 normal l normal o normal g left-parenthesis ModifyingAbove
    script upper L With caret right-parenthesis EndLayout$
  prefs: []
  type: TYPE_NORMAL
- en: 'In these equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '*m* is the number of instances, as always.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p* is the number of parameters learned by the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $ModifyingAbove script upper L With caret$ is the maximized value of the *likelihood
    function* of the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both the BIC and the AIC penalize models that have more parameters to learn
    (e.g., more clusters) and reward models that fit the data well. They often end
    up selecting the same model. When they differ, the model selected by the BIC tends
    to be simpler (fewer parameters) than the one selected by the AIC, but tends to
    not fit the data quite as well (this is especially true for larger datasets).
  prefs: []
  type: TYPE_NORMAL
- en: 'To compute the BIC and AIC, call the `bic()` and `aic()` methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 8-19](#aic_bic_vs_k_plot) shows the BIC for different numbers of clusters
    *k*. As you can see, both the BIC and the AIC are lowest when *k* = 3, so it is
    most likely the best choice.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Plot showing AIC and BIC values for different numbers of clusters _k_, with
    both criteria reaching their minimum at _k_ = 3.](assets/hmls_0819.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-19\. AIC and BIC for different numbers of clusters *k*
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Bayesian Gaussian Mixture Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Rather than manually searching for the optimal number of clusters, you can
    use the `BayesianGaussianMixture` class, which is capable of giving weights equal
    (or close) to zero to unnecessary clusters. Set the number of clusters `n_components`
    to a value that you have good reason to believe is greater than the optimal number
    of clusters (this assumes some minimal knowledge about the problem at hand), and
    the algorithm will eliminate the unnecessary clusters automatically. For example,
    let’s set the number of clusters to 10 and see what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Perfect: the algorithm automatically detected that only three clusters are
    needed, and the resulting clusters are almost identical to the ones in [Figure 8-15](#gaussian_mixtures_plot).'
  prefs: []
  type: TYPE_NORMAL
- en: 'A final note about Gaussian mixture models: although they work great on clusters
    with ellipsoidal shapes, they don’t do so well with clusters of very different
    shapes. For example, let’s see what happens if we use a Bayesian Gaussian mixture
    model to cluster the moons dataset (see [Figure 8-20](#moons_vs_bgm_plot)).'
  prefs: []
  type: TYPE_NORMAL
- en: Oops! The algorithm desperately searched for ellipsoids, so it found eight different
    clusters instead of two. The density estimation is not too bad, so this model
    could perhaps be used for anomaly detection, but it failed to identify the two
    moons. To conclude this chapter, let’s take a quick look at a few algorithms capable
    of dealing with arbitrarily shaped clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram showing a Gaussian mixture model''s attempt to fit nonellipsoidal
    clusters, illustrating its limitation in detecting two moon-shaped clusters by
    identifying eight separate sections instead.](assets/hmls_0820.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-20\. Fitting a Gaussian mixture to nonellipsoidal clusters
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Other Algorithms for Anomaly and Novelty Detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Scikit-Learn implements other algorithms dedicated to anomaly detection or
    novelty detection:'
  prefs: []
  type: TYPE_NORMAL
- en: Fast-MCD (minimum covariance determinant)
  prefs: []
  type: TYPE_NORMAL
- en: Implemented by the `EllipticEnvelope` class, this algorithm is useful for outlier
    detection, in particular to clean up a dataset. It assumes that the normal instances
    (inliers) are generated from a single Gaussian distribution (not a mixture). It
    also assumes that the dataset is contaminated with outliers that were not generated
    from this Gaussian distribution. When the algorithm estimates the parameters of
    the Gaussian distribution (i.e., the shape of the elliptic envelope around the
    inliers), it is careful to ignore the instances that are most likely outliers.
    This technique gives a better estimation of the elliptic envelope and thus makes
    the algorithm better at identifying the outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Isolation forest
  prefs: []
  type: TYPE_NORMAL
- en: 'This is an efficient algorithm for outlier detection, especially in high-dimensional
    datasets. The algorithm builds a random forest in which each decision tree is
    grown randomly: at each node, it picks a feature randomly, then it picks a random
    threshold value (between the min and max values) to split the dataset in two.
    The dataset gradually gets chopped into pieces this way, until all instances end
    up isolated from the other instances. Anomalies are usually far from other instances,
    so on average (across all the decision trees) they tend to get isolated in fewer
    steps than normal instances.'
  prefs: []
  type: TYPE_NORMAL
- en: Local outlier factor (LOF)
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm is also good for outlier detection. It compares the density of
    instances around a given instance to the density around its neighbors. An anomaly
    is often more isolated than its *k*-nearest neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: One-class SVM
  prefs: []
  type: TYPE_NORMAL
- en: 'This algorithm is better suited for novelty detection. Recall that a kernelized
    SVM classifier separates two classes by first (implicitly) mapping all the instances
    to a high-dimensional space, then separating the two classes using a linear SVM
    classifier within this high-dimensional space (see the online chapter on SVMs
    at [*https://homl.info*](https://homl.info)). Since we just have one class of
    instances, the one-class SVM algorithm instead tries to separate the instances
    in high-dimensional space from the origin. In the original space, this will correspond
    to finding a small region that encompasses all the instances. If a new instance
    does not fall within this region, it is an anomaly. There are a few hyperparameters
    to tweak: the usual ones for a kernelized SVM, plus a margin hyperparameter that
    corresponds to the probability of a new instance being mistakenly considered as
    novel when it is in fact normal. It works great, especially with high-dimensional
    datasets, but like all SVMs it does not scale to large datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: PCA and other dimensionality reduction techniques with an `inverse_transform()`
    method
  prefs: []
  type: TYPE_NORMAL
- en: If you compare the reconstruction error of a normal instance with the reconstruction
    error of an anomaly, the latter will usually be much larger. This is a simple
    and often quite efficient anomaly detection approach (see this chapter’s exercises
    for an example).
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How would you define clustering? Can you name a few clustering algorithms?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are some of the main applications of clustering algorithms?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Describe two techniques to select the right number of clusters when using *k*-means.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is label propagation? Why would you implement it, and how?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you name two clustering algorithms that can scale to large datasets? And
    two that look for regions of high density?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you think of a use case where active learning would be useful? How would
    you implement it?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between anomaly detection and novelty detection?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a Gaussian mixture? What tasks can you use it for?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you name two techniques to find the right number of clusters when using
    a Gaussian mixture model?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The classic Olivetti faces dataset contains 400 grayscale 64 × 64–pixel images
    of faces. Each image is flattened to a 1D vector of size 4,096\. Forty different
    people were photographed (10 times each), and the usual task is to train a model
    that can predict which person is represented in each picture. Load the dataset
    using the `sklearn.datasets.fetch_olivetti_faces()` function, then split it into
    a training set, a validation set, and a test set (note that the dataset is already
    scaled between 0 and 1). Since the dataset is quite small, you will probably want
    to use stratified sampling to ensure that there are the same number of images
    per person in each set. Next, cluster the images using *k*-means, and ensure that
    you have a good number of clusters (using one of the techniques discussed in this
    chapter). Visualize the clusters: do you see similar faces in each cluster?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Continuing with the Olivetti faces dataset, train a classifier to predict which
    person is represented in each picture, and evaluate it on the validation set.
    Next, use *k*-means as a dimensionality reduction tool, and train a classifier
    on the reduced set. Search for the number of clusters that allows the classifier
    to get the best performance: what performance can you reach? What if you append
    the features from the reduced set to the original features (again, searching for
    the best number of clusters)?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a Gaussian mixture model on the Olivetti faces dataset. To speed up the
    algorithm, you should probably reduce the dataset’s dimensionality (e.g., use
    PCA, preserving 99% of the variance). Use the model to generate some new faces
    (using the `sample()` method), and visualize them (if you used PCA, you will need
    to use its `inverse_transform()` method). Try to modify some images (e.g., rotate,
    flip, darken) and see if the model can detect the anomalies (i.e., compare the
    output of the `score_samples()` method for normal images and for anomalies).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Some dimensionality reduction techniques can also be used for anomaly detection.
    For example, take the Olivetti faces dataset and reduce it with PCA, preserving
    99% of the variance. Then compute the reconstruction error for each image. Next,
    take some of the modified images you built in the previous exercise and look at
    their reconstruction error: notice how much larger it is. If you plot a reconstructed
    image, you will see why: it tries to reconstruct a normal face.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab-p*](https://homl.info/colab-p).
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch08.html#id1959-marker)) If you are not familiar with probability theory,
    I highly recommend the free online classes by Khan Academy.
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch08.html#id1973-marker)) Stuart P. Lloyd, “Least Squares Quantization
    in PCM”, *IEEE Transactions on Information Theory* 28, no. 2 (1982): 129–137.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([3](ch08.html#id1983-marker)) David Arthur and Sergei Vassilvitskii, “k-Means++:
    The Advantages of Careful Seeding”, *Proceedings of the 18th Annual ACM-SIAM Symposium
    on Discrete Algorithms* (2007): 1027–1035.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([4](ch08.html#id1989-marker)) Charles Elkan, “Using the Triangle Inequality
    to Accelerate k-Means”, *Proceedings of the 20th International Conference on Machine
    Learning* (2003): 147–153.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch08.html#id1990-marker)) The triangle inequality is AC ≤ AB + BC, where
    A, B, and C are three points and AB, AC, and BC are the distances between these
    points.
  prefs: []
  type: TYPE_NORMAL
- en: '^([6](ch08.html#id1993-marker)) David Sculley, “Web-Scale K-Means Clustering”,
    *Proceedings of the 19th International Conference on World Wide Web* (2010): 1177–1178.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch08.html#id2046-marker)) In contrast, as we saw earlier, *k*-means implicitly
    assumes that clusters all have a similar size and density, and are all roughly
    round.
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch08.html#id2050-marker)) Phi (*ϕ* or *φ*) is the 21st letter of the Greek
    alphabet.
  prefs: []
  type: TYPE_NORMAL
