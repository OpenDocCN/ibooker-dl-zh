- en: Chapter 8\. Unsupervised Learning Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Yann LeCun, Turing Award winner and Meta‚Äôs Chief AI Scientist, famously said
    that ‚Äúif intelligence was a cake, unsupervised learning would be the cake, supervised
    learning would be the icing on the cake, and reinforcement learning would be the
    cherry on the cake‚Äù (NeurIPS 2016). In other words, there is a huge potential
    in unsupervised learning that we have only barely started to sink our teeth into.
    Indeed, the vast majority of the available data is unlabeled: we have the input
    features **X**, but we do not have the labels **y**.'
  prefs: []
  type: TYPE_NORMAL
- en: Say you want to create a system that will take a few pictures of each item on
    a manufacturing production line and detect which items are defective. You can
    fairly easily create a system that will take pictures automatically, and this
    might give you thousands of pictures every day. You can then build a reasonably
    large dataset in just a few weeks. But wait, there are no labels! If you want
    to train a regular binary classifier that will predict whether an item is defective
    or not, you will need to label every single picture as ‚Äúdefective‚Äù or ‚Äúnormal‚Äù.
    This will generally require human experts to sit down and manually go through
    all the pictures. This is a long, costly, and tedious task, so it will usually
    only be done on a small subset of the available pictures. As a result, the labeled
    dataset will be quite small, and the classifier‚Äôs performance will be disappointing.
    Moreover, every time the company makes any change to its products, the whole process
    will need to be started over from scratch. Wouldn‚Äôt it be great if the algorithm
    could just exploit the unlabeled data without needing humans to label every picture?
    Enter unsupervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Chapter¬†7](ch07.html#dimensionality_chapter) we looked at the most common
    unsupervised learning task: dimensionality reduction. In this chapter we will
    look at a few more unsupervised tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs: []
  type: TYPE_NORMAL
- en: The goal is to group similar instances together into *clusters*. Clustering
    is a great tool for data analysis, customer segmentation, recommender systems,
    search engines, image segmentation, semi-supervised learning, dimensionality reduction,
    and more.
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection (also called *outlier detection*)
  prefs: []
  type: TYPE_NORMAL
- en: The objective is to learn what ‚Äúnormal‚Äù data looks like, and then use that to
    detect abnormal instances. These instances are called *anomalies*, or *outliers*,
    while the normal instances are called *inliers*. Anomaly detection is useful in
    a wide variety of applications, such as fraud detection, detecting defective products
    in manufacturing, identifying new trends in time series, or removing outliers
    from a dataset before training another model, which can significantly improve
    the performance of the resulting model.
  prefs: []
  type: TYPE_NORMAL
- en: Density estimation
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the task of estimating the *probability density function* (PDF) of
    the random process that generated the dataset.‚Å†^([1](ch08.html#id1959)) Density
    estimation is commonly used for anomaly detection: instances located in very low-density
    regions are likely to be anomalies. It is also useful for data analysis and visualization.'
  prefs: []
  type: TYPE_NORMAL
- en: Ready for some cake? We will start with two clustering algorithms, *k*-means
    and DBSCAN, then we‚Äôll discuss Gaussian mixture models and see how they can be
    used for density estimation, clustering, and anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clustering Algorithms: k-means and DBSCAN'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you enjoy a hike in the mountains, you stumble upon a plant you have never
    seen before. You look around and you notice a few more. They are not identical,
    yet they are sufficiently similar for you to know that they most likely belong
    to the same species (or at least the same genus). You may need a botanist to tell
    you what species that is, but you certainly don‚Äôt need an expert to identify groups
    of similar-looking objects. This is called *clustering*: it is the task of identifying
    similar instances and assigning them to *clusters*, or groups of similar instances.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like in classification, each instance gets assigned to a group. However,
    unlike classification, clustering is an unsupervised task, there are no labels,
    so the algorithm needs to figure out on its own how to group instances. Consider
    [Figure¬†8-1](#classification_vs_clustering_plot): on the left is the iris dataset
    (introduced in [Chapter¬†4](ch04.html#linear_models_chapter)), where each instance‚Äôs
    species (i.e., its class) is represented with a different marker. It is a labeled
    dataset, for which classification algorithms such as logistic regression, SVMs,
    or random forest classifiers are well suited. On the right is the same dataset,
    but without the labels, so you cannot use a classification algorithm anymore.
    This is where clustering algorithms step in: many of them can easily detect the
    lower-left cluster. It is also quite easy to see with our own eyes, but it is
    not so obvious that the upper-right cluster is composed of two distinct subclusters.
    That said, the dataset has two additional features (sepal length and width) that
    are not represented here, and clustering algorithms can make good use of all features,
    so in fact they identify the three clusters fairly well (e.g., using a Gaussian
    mixture model, only 5 instances out of 150 are assigned to the wrong cluster).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram comparing classification (left) with labeled data and clustering
    (right) with unlabeled data on the iris dataset, highlighting how clustering identifies
    groups without prior labels.](assets/hmls_0801.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8-1\. Classification (left) versus clustering (right): in clustering,
    the dataset is unlabeled so the algorithm must identify the clusters without guidance'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Clustering is used in a wide variety of applications, including:'
  prefs: []
  type: TYPE_NORMAL
- en: Customer segmentation
  prefs: []
  type: TYPE_NORMAL
- en: You can cluster your customers based on their purchases and their activity on
    your website. This is useful to understand who your customers are and what they
    need, so you can adapt your products and marketing campaigns to each segment.
    For example, customer segmentation can be useful in *recommender systems* to suggest
    content that other users in the same cluster enjoyed.
  prefs: []
  type: TYPE_NORMAL
- en: Data analysis
  prefs: []
  type: TYPE_NORMAL
- en: When you analyze a new dataset, it can be helpful to run a clustering algorithm,
    and then analyze each cluster separately.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs: []
  type: TYPE_NORMAL
- en: Once a dataset has been clustered, it is usually possible to measure each instance‚Äôs
    *affinity* with each cluster; affinity is any measure of how well an instance
    fits into a cluster. Each instance‚Äôs feature vector **x** can then be replaced
    with the vector of its cluster affinities. If there are *k* clusters, then this
    vector is *k*-dimensional. The new vector is typically much lower-dimensional
    than the original feature vector, but it can preserve enough information for further
    processing.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering
  prefs: []
  type: TYPE_NORMAL
- en: The cluster affinities can often be useful as extra features. For example, we
    used *k*-means in [Chapter¬†2](ch02.html#project_chapter) to add geographic cluster
    affinity features to the California housing dataset, and they helped us get better
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: '*Anomaly detection* (also called *outlier detection*)'
  prefs: []
  type: TYPE_NORMAL
- en: Any instance that has a low affinity to all the clusters is likely to be an
    anomaly. For example, if you have clustered the users of your website based on
    their behavior, you can detect users with unusual behavior, such as an unusual
    number of requests per second.
  prefs: []
  type: TYPE_NORMAL
- en: Semi-supervised learning
  prefs: []
  type: TYPE_NORMAL
- en: If you only have a few labels, you could perform clustering and propagate the
    labels to all the instances in the same cluster. This technique can greatly increase
    the number of labels available for a subsequent supervised learning algorithm,
    and thus improve its performance.
  prefs: []
  type: TYPE_NORMAL
- en: Search engines
  prefs: []
  type: TYPE_NORMAL
- en: Some search engines let you search for images that are similar to a reference
    image. To build such a system, you would first apply a clustering algorithm to
    all the images in your database; similar images would end up in the same cluster.
    Then when a user provides a reference image, all you‚Äôd need to do is use the trained
    clustering model to find this image‚Äôs cluster, and you could then simply return
    all the images from this cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Image segmentation
  prefs: []
  type: TYPE_NORMAL
- en: By clustering pixels according to their color, then replacing each pixel‚Äôs color
    with the mean color of its cluster, it is possible to considerably reduce the
    number of different colors in an image. Image segmentation is used in many object
    detection and tracking systems, as it makes it easier to detect the contour of
    each object.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is no universal definition of what a cluster is: it really depends on
    the context, and different algorithms will capture different kinds of clusters.
    Some algorithms look for instances centered around a particular point, called
    a *centroid*. Others look for continuous regions of densely packed instances:
    these clusters can take on any shape. Some algorithms are hierarchical, looking
    for clusters of clusters. And the list goes on.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will look at two popular clustering algorithms, *k*-means
    and DBSCAN, and explore some of their applications, such as nonlinear dimensionality
    reduction, semi-supervised learning, and anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: k-Means Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Consider the unlabeled dataset represented in [Figure¬†8-2](#blobs_plot): you
    can clearly see five blobs of instances. The *k*-means algorithm is a simple algorithm
    capable of clustering this kind of dataset very quickly and efficiently, often
    in just a few iterations. It was proposed by Stuart Lloyd at Bell Labs in 1957
    as a technique for pulse-code modulation, but it was only [published](https://homl.info/36)
    outside of the company in 1982.‚Å†^([2](ch08.html#id1973)) In 1965, Edward W. Forgy
    had published virtually the same algorithm, so *k*-means is sometimes referred
    to as the Lloyd‚ÄìForgy algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scatterplot displaying five distinct clusters of data points, illustrating
    an unlabeled dataset suitable for k-means clustering.](assets/hmls_0802.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. An unlabeled dataset composed of five blobs of instances
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let‚Äôs train a *k*-means clusterer on this dataset. It will try to find each
    blob‚Äôs center and assign each instance to the closest blob:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note that you have to specify the number of clusters *k* that the algorithm
    must find. In this example, it is pretty obvious from looking at the data that
    *k* should be set to 5, but in general it is not that easy. We will discuss this
    shortly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each instance will be assigned to one of the five clusters. In the context
    of clustering, an instance‚Äôs *label* is the index of the cluster to which the
    algorithm assigns this instance; this is not to be confused with the class labels
    in classification, which are used as targets (remember that clustering is an unsupervised
    learning task). The `KMeans` instance preserves the predicted labels of the instances
    it was trained on, available via the `labels_` instance variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2][PRE3]``py[PRE4]``py You can easily assign new instances to the cluster
    whose centroid is closest:    [PRE5]py `>>>` `kmeans``.``predict``(``X_new``)`
    `` `array([1, 1, 2, 2], dtype=int32)` `` [PRE6]py   [PRE7]py`` [PRE8]py In this
    example, the first instance in `X_new` is located at a distance of about 2.81
    from the first centroid, 0.33 from the second centroid, 2.90 from the third centroid,
    1.49 from the fourth centroid, and 2.89 from the fifth centroid. If you have a
    high-dimensional dataset and you transform it this way, you end up with a *k*-dimensional
    dataset: this transformation can be a very efficient nonlinear dimensionality
    reduction technique. Alternatively, you can use these distances as extra features
    to train another model, as in [Chapter¬†2](ch02.html#project_chapter).    ### The
    k-means algorithm    So, how does the algorithm work? Well, suppose you were given
    the centroids. You could easily label all the instances in the dataset by assigning
    each of them to the cluster whose centroid is closest. Conversely, if you were
    given all the instance labels, you could easily locate each cluster‚Äôs centroid
    by computing the mean of the instances in that cluster. But you are given neither
    the labels nor the centroids, so how can you proceed? Start by placing the centroids
    randomly (e.g., by picking *k* instances at random from the dataset and using
    their locations as centroids). Then label the instances, update the centroids,
    label the instances, update the centroids, and so on until the centroids stop
    moving. The algorithm is guaranteed to converge in a finite number of steps (usually
    quite small). That‚Äôs because the mean squared distance between the instances and
    their closest centroids can only go down at each step, and since it cannot be
    negative, it‚Äôs guaranteed to converge.    You can see the algorithm in action
    in [Figure¬†8-4](#kmeans_algorithm_plot): the centroids are initialized randomly
    (top left), then the instances are labeled (top right), then the centroids are
    updated (center left), the instances are relabeled (center right), and so on.
    As you can see, in just three iterations the algorithm has reached a clustering
    that seems close to optimal.    ###### Note    The computational complexity of
    the algorithm is generally linear with regard to the number of instances *m*,
    the number of clusters *k*, and the number of dimensions *n*. However, this is
    only true when the data has a clustering structure. If it does not, then in the
    worst-case scenario the complexity can increase exponentially with the number
    of instances. In practice, this rarely happens, and *k*-means is generally one
    of the fastest clustering algorithms.  ![Diagram illustrating the k-means algorithm''s
    process with random centroid initialization and subsequent instance labeling,
    showing potential clustering outcomes over iterations.](assets/hmls_0804.png)  ######
    Figure 8-4\. The *k*-means algorithm    Although the algorithm is guaranteed to
    converge, it may not converge to the right solution (i.e., it may converge to
    a local optimum): whether it does or not depends on the centroid initialization.
    [Figure¬†8-5](#kmeans_variability_plot) shows two suboptimal solutions that the
    algorithm can converge to if you are not lucky with the random initialization
    step.  ![Diagram showing two suboptimal k-means clustering solutions caused by
    different random centroid initializations.](assets/hmls_0805.png)  ###### Figure
    8-5\. Suboptimal solutions due to unlucky centroid initializations    Let‚Äôs take
    a look at a few ways you can mitigate this risk by improving the centroid initialization.    ###
    Centroid initialization methods    If you happen to know approximately where the
    centroids should be (e.g., if you ran another clustering algorithm earlier), then
    you can set the `init` hyperparameter to a NumPy array containing the list of
    centroids:    [PRE9]py    Another solution is to run the algorithm multiple times
    with different random initializations and keep the best solution. The number of
    random initializations is controlled by the `n_init` hyperparameter: by default
    it is equal to `10` when using `init="random"`, which means that the whole algorithm
    described earlier runs 10 times when you call `fit()`, and Scikit-Learn keeps
    the best solution. But how exactly does it know which solution is the best? It
    uses a performance metric! That metric is called the model‚Äôs *inertia*, which
    is defined in [Equation 8-1](#inertia_equation).    ##### Equation 8-1\. A model‚Äôs
    inertia is the sum of all squared distances between each instance **x**^((*i*))
    and the closest centroid **c**^((*i*)) predicted by the model  <mrow><mtext>inertia</mtext>
    <mo>=</mo> <munder><mo>‚àë</mo> <mi>i</mi></munder> <msup><mrow><mo>‚à•</mo><msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo><msup><mi
    mathvariant="bold">c</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo
    rspace="0.1em">‚à•</mo></mrow> <mn>2</mn></msup></mrow>  The inertia is roughly
    equal to 219.6 for the model on the left in [Figure¬†8-5](#kmeans_variability_plot),
    600.4 for the model on the right in [Figure¬†8-5](#kmeans_variability_plot), and
    only 211.6 for the model in [Figure¬†8-3](#voronoi_plot). The `KMeans` class runs
    the initialization algorithm `n_init` times and keeps the model with the lowest
    inertia. In this example, the model in [Figure¬†8-3](#voronoi_plot) will be selected
    (unless we are very unlucky with `n_init` consecutive random initializations).
    If you are curious, a model‚Äôs inertia is accessible via the `inertia_` instance
    variable:    [PRE10]py   [PRE11]` [PRE12] from sklearn.cluster import MiniBatchKMeans  minibatch_kmeans
    = MiniBatchKMeans(n_clusters=5, random_state=42) minibatch_kmeans.fit(X) [PRE13]
    >>> from sklearn.metrics import silhouette_score `>>>` `silhouette_score``(``X``,`
    `kmeans``.``labels_``)` `` `np.float64(0.655517642572828)` `` [PRE14]` [PRE15][PRE16][PRE17][PRE18][PRE19]
    [PRE20]`py` [PRE21]  [PRE22][PRE23][PRE24][PRE25][PRE26]` [PRE27][PRE28][PRE29]
    >>> import PIL `>>>` `image` `=` `np``.``asarray``(``PIL``.``Image``.``open``(``filepath``))`
    [PRE30] [PRE31][PRE32]`` [PRE33] X = image.reshape(-1, 3) kmeans = KMeans(n_clusters=8,
    random_state=42).fit(X) segmented_img = kmeans.cluster_centers_[kmeans.labels_]
    segmented_img = segmented_img.reshape(image.shape) [PRE34]` [PRE35][PRE36][PRE37][PRE38]``py[PRE39]py`
    ## Using Clustering for Semi-Supervised Learning    Another use case for clustering
    is in semi-supervised learning, when we have plenty of unlabeled instances and
    very few labeled instances. For example, clustering can help choose which additional
    instances to label (e.g., near the cluster centroids). It can also be used to
    propagate the most common label in each cluster to the unlabeled instances in
    that cluster. Let‚Äôs try these ideas on the digits dataset, which is a simple MNIST-like
    dataset containing 1,797 grayscale 8 √ó 8 images representing the digits 0 to 9\.
    First, let‚Äôs load and split the dataset (it‚Äôs already shuffled):    [PRE40]py    We
    will pretend we only have labels for 50 instances. To get a baseline performance,
    let‚Äôs train a logistic regression model on these 50 labeled instances:    [PRE41]py    We
    can then measure the accuracy of this model on the test set (note that the test
    set must be labeled):    [PRE42]py   [PRE43]`py The model‚Äôs accuracy is just 75.8%.
    That‚Äôs not great: indeed, if you try training the model on the full training set,
    you will find that it will reach about 90.9% accuracy. Let‚Äôs see how we can do
    better. First, let‚Äôs cluster the training set into 50 clusters. Then, for each
    cluster, we‚Äôll find the image closest to the centroid. We‚Äôll call these images
    the *representative images*:    [PRE44]py    [Figure¬†8-12](#representative_images_plot)
    shows the 50 representative images.  ![Fifty handwritten digit images, each representing
    a distinct cluster for analysis.](assets/hmls_0812.png)  ###### Figure 8-12\.
    Fifty representative digit images (one per cluster)    Let‚Äôs look at each image
    and manually label them:    [PRE45]py    Now we have a dataset with just 50 labeled
    instances, but instead of being random instances, each of them is a representative
    image of its cluster. Let‚Äôs see if the performance is any better:    [PRE46]py
    `>>>` `log_reg``.``score``(``X_test``,` `y_test``)` `` `0.8337531486146096` ``
    [PRE47]py   [PRE48] [PRE49][PRE50]``py[PRE51]`` [PRE52]`` [PRE53]` We got another
    significant accuracy boost! Let‚Äôs see if we can do even better by ignoring the
    50% of instances that are farthest from their cluster center: this should eliminate
    some outliers. The following code first computes the distance from each instance
    to its closest cluster center, then for each cluster it sets the 50% largest distances
    to ‚Äì1\. Lastly, it creates a set without these instances marked with a ‚Äì1 distance:    [PRE54]    Now
    let‚Äôs train the model again on this partially propagated dataset and see what
    accuracy we get:    [PRE55] `>>>` `log_reg``.``score``(``X_test``,` `y_test``)`
    `` `0.8841309823677582` `` [PRE56]   [PRE57] [PRE58]`py [PRE59]py`` [PRE60]py[PRE61][PRE62][PRE63][PRE64]py[PRE65]py
    [PRE66] [PRE67][PRE68]``py[PRE69]`` [PRE70]`` This clustering is represented in
    the lefthand plot of [Figure¬†8-13](#dbscan_plot). As you can see, it identified
    quite a lot of anomalies, plus seven different clusters. How disappointing! Fortunately,
    if we widen each instance‚Äôs neighborhood by increasing `eps` to 0.2, we get the
    clustering on the right, which looks perfect. Let‚Äôs continue with this model.  ![DBSCAN
    clustering results with `eps` values of 0.05 and 0.20, showing improved clustering
    with fewer anomalies in the right plot.](assets/hmls_0813.png)  ###### Figure
    8-13\. DBSCAN clustering using two different neighborhood radiuses    Surprisingly,
    the `DBSCAN` class does not have a `predict()` method, although it has a `fit_predict()`
    method. In other words, it cannot predict which cluster a new instance belongs
    to. This decision was made because different classification algorithms can be
    better for different tasks, so the authors decided to let the user choose which
    one to use. Moreover, it‚Äôs not hard to implement. For example, let‚Äôs train a `KNeighborsClassifier`:    [PRE71]    Now,
    given a few new instances, we can predict which clusters they most likely belong
    to and even estimate a probability for each cluster:    [PRE72] `array([1, 0,
    1, 0])` `>>>` `knn``.``predict_proba``(``X_new``)` `` `array([[0.18, 0.82],`  `[1\.  ,
    0\.  ],`  `[0.12, 0.88],`  `[1\.  , 0\.  ]])` `` [PRE73]   [PRE74]` [PRE75] [PRE76]`py
    [PRE77]py`` [PRE78]py[PRE79] [PRE80][PRE81]``py[PRE82]`py[PRE83][PRE84][PRE85]``py[PRE86]py[PRE87][PRE88][PRE89]
    [PRE90][PRE91][PRE92][PRE93][PRE94]``  [PRE95][PRE96][PRE97]`` [PRE98][PRE99][PRE100]`
    [PRE101][PRE102][PRE103] # Gaussian Mixtures    A *Gaussian mixture model* (GMM)
    is a probabilistic model that assumes that the instances were generated from a
    mixture of several Gaussian distributions whose parameters are unknown. All the
    instances generated from a single Gaussian distribution form a cluster that typically
    looks like an ellipsoid. Each cluster can have a different ellipsoidal shape,
    size, density, and orientation, just like in [Figure¬†8-10](#bad_kmeans_plot).‚Å†^([7](ch08.html#id2046))
    When you observe an instance, you know it was generated from one of the Gaussian
    distributions, but you are not told which one, and you do not know what the parameters
    of these distributions are.    There are several GMM variants. In the simplest
    variant, implemented in the `GaussianMixture` class, you must know in advance
    the number *k* of Gaussian distributions. The dataset **X** is assumed to have
    been generated through the following probabilistic process:    *   For each instance,
    a cluster is picked randomly from among *k* clusters. The probability of choosing
    the *j*^(th) cluster is the cluster‚Äôs weight *œï*^((*j*)).‚Å†^([8](ch08.html#id2050))
    The index of the cluster chosen for the *i*^(th) instance is denoted *z*^((*i*)).           *   If
    the *i*^(th) instance was assigned to the *j*^(th) cluster (i.e., *z*^((*i*))
    = *j*), then the location **x**^((*i*)) of this instance is sampled randomly from
    the Gaussian distribution with mean **Œº**^((*j*)) and covariance matrix **Œ£**^((*j*)).
    This is denoted **x**^((*i*)) ~ ùí©(Œº*^((*j*)), **Œ£**^((*j*))).              So
    what can you do with such a model? Well, given the dataset **X**, you typically
    want to start by estimating the weights **œï** and all the distribution parameters
    **Œº**^((1)) to **Œº**^((*k*)) and **Œ£**^((1)) to **Œ£**^((*k*)). Scikit-Learn‚Äôs
    `GaussianMixture` class makes this super easy:    [PRE104]py    Let‚Äôs look at
    the parameters that the algorithm estimated:    [PRE105]py `array([[-1.40764129,  1.42712848],`  `[
    3.39947665,  1.05931088],`  `[ 0.05145113,  0.07534576]])` `>>>` `gm``.``covariances_`
    `` `array([[[ 0.63478217,  0.72970097],`  `[ 0.72970097,  1.16094925]],`   `[[
    1.14740131, -0.03271106],`  `[-0.03271106,  0.95498333]],`   `[[ 0.68825143,  0.79617956],`  `[
    0.79617956,  1.21242183]]])` `` [PRE106]py   [PRE107]`py[PRE108]`py[PRE109]py[PRE110][PRE111][PRE112]py[PRE113]py[PRE114][PRE115]
    A Gaussian mixture model is a *generative model*, meaning you can sample new instances
    from it (note that they are ordered by cluster index):    [PRE116]py `array([[-2.32491052,  1.04752548],`  `[-1.16654983,  1.62795173],`  `[
    1.84860618,  2.07374016],`  `[ 3.98304484,  1.49869936],`  `[ 3.8163406 ,  0.53038367],`  `[
    0.38079484, -0.56239369]])` `>>>` `y_new` `` `array([0, 0, 1, 1, 1, 2])` `` [PRE117]py   [PRE118]
    [PRE119][PRE120]``py[PRE121]`` If you compute the exponential of these scores,
    you get the value of the PDF at the location of the given instances. These are
    not probabilities, but probability *densities*: they can take on any positive
    value, not just a value between 0 and 1\. To estimate the probability that an
    instance will fall within a particular region, you would have to integrate the
    PDF over that region (if you do so over the entire space of possible instance
    locations, the result will be 1).    [Figure¬†8-15](#gaussian_mixtures_plot) shows
    the cluster means, the decision boundaries (dashed lines), and the density contours
    of this model.  ![Diagram of a Gaussian mixture model showing three cluster means,
    decision boundaries as dashed lines, and density contours indicating distribution.](assets/hmls_0815.png)  ######
    Figure 8-15\. Cluster means, decision boundaries, and density contours of a trained
    Gaussian mixture model    Nice! The algorithm clearly found an excellent solution.
    Of course, we made its task easy by generating the data using a set of 2D Gaussian
    distributions (unfortunately, real-life data is not always so Gaussian and low-dimensional).
    We also gave the algorithm the correct number of clusters. When there are many
    dimensions, or many clusters, or few instances, EM can struggle to converge to
    the optimal solution. You might need to reduce the difficulty of the task by limiting
    the number of parameters that the algorithm has to learn. One way to do this is
    to limit the range of shapes and orientations that the clusters can have. This
    can be achieved by imposing constraints on the covariance matrices. To do this,
    set the `covariance_type` hyperparameter to one of the following values:    `"spherical"`      All
    clusters must be spherical, but they can have different diameters (i.e., different
    variances).      `"diag"`      Clusters can take on any ellipsoidal shape of any
    size, but the ellipsoid‚Äôs axes must be parallel to the coordinate axes (i.e.,
    the covariance matrices must be diagonal).      `"tied"`      All clusters must
    have the same ellipsoidal shape, size, and orientation (i.e., all clusters share
    the same covariance matrix).      By default, `covariance_type` is equal to `"full"`,
    which means that each cluster can take on any shape, size, and orientation (it
    has its own unconstrained covariance matrix). [Figure¬†8-16](#covariance_type_plot)
    plots the solutions found by the EM algorithm when `covariance_type` is set to
    `"tied"` or `"spherical"`.  ![Diagram showing Gaussian mixture models for tied
    covariance (left) with oval clusters and spherical covariance (right) with circular
    clusters.](assets/hmls_0816.png)  ###### Figure 8-16\. Gaussian mixtures for tied
    clusters (left) and spherical clusters (right)    ###### Note    The computational
    complexity of training a `GaussianMixture` model depends on the number of instances
    *m*, the number of dimensions *n*, the number of clusters *k*, and the constraints
    on the covariance matrices. If `covariance_type` is `"spherical"` or `"diag"`,
    it is *O*(*kmn*), assuming the data has a clustering structure. If `covariance_type`
    is `"tied"` or `"full"`, it is *O*(*kmn*¬≤ + *kn*¬≥), so it will not scale to large
    numbers of features.    Gaussian mixture models can also be used for anomaly detection.
    We‚Äôll see how in the next section.    ## Using Gaussian Mixtures for Anomaly Detection    Using
    a Gaussian mixture model for anomaly detection is quite simple: any instance located
    in a low-density region can be considered an anomaly. You must define what density
    threshold you want to use. For example, in a manufacturing company that tries
    to detect defective products, the ratio of defective products is usually well
    known. Say it is equal to 2%. You then set the density threshold to be the value
    that results in having 2% of the instances located in areas below that threshold
    density. If you notice that you get too many false positives (i.e., perfectly
    good products that are flagged as defective), you can lower the threshold. Conversely,
    if you have too many false negatives (i.e., defective products that the system
    does not flag as defective), you can increase the threshold. This is the usual
    precision/recall trade-off (see [Chapter¬†3](ch03.html#classification_chapter)).
    Here is how you would identify the outliers using the second percentile lowest
    density as the threshold (i.e., approximately 2% of the instances will be flagged
    as anomalies):    [PRE122]    [Figure¬†8-17](#mixture_anomaly_detection_plot) represents
    these anomalies as stars.  ![Contour plot illustrating anomaly detection with
    a Gaussian mixture model, where anomalies are marked with red stars.](assets/hmls_0817.png)  ######
    Figure 8-17\. Anomaly detection using a Gaussian mixture model    A closely related
    task is *novelty detection*: it differs from anomaly detection in that the algorithm
    is assumed to be trained on a ‚Äúclean‚Äù dataset, uncontaminated by outliers, whereas
    anomaly detection does not make this assumption. Indeed, outlier detection is
    often used to clean up a dataset.    ###### Tip    Gaussian mixture models try
    to fit all the data, including the outliers; if you have too many of them this
    will bias the model‚Äôs view of ‚Äúnormality‚Äù, and some outliers may wrongly be considered
    as normal. If this happens, you can try to fit the model once, use it to detect
    and remove the most extreme outliers, then fit the model again on the cleaned-up
    dataset. Another approach is to use robust covariance estimation methods (see
    the `EllipticEnvelope` class).    Just like *k*-means, the `GaussianMixture` algorithm
    requires you to specify the number of clusters. So how can you find that number?    ##
    Selecting the Number of Clusters    With *k*-means, you can use the inertia or
    the silhouette score to select the appropriate number of clusters. But with Gaussian
    mixtures, it is not possible to use these metrics because they are not reliable
    when the clusters are not spherical or have different sizes. Instead, you can
    try to find the model that minimizes a *theoretical information criterion*, such
    as the *Bayesian information criterion* (BIC) or the *Akaike information criterion*
    (AIC), defined in [Equation 8-2](#information_criteria_equation).    ##### Equation
    8-2\. Bayesian information criterion (BIC) and Akaike information criterion (AIC)  $StartLayout
    1st Row 1st Column Blank 2nd Column upper B upper I upper C equals normal l normal
    o normal g left-parenthesis m right-parenthesis p minus 2 normal l normal o normal
    g left-parenthesis ModifyingAbove script upper L With caret right-parenthesis
    2nd Row 1st Column Blank 2nd Column upper A upper I upper C equals 2 p minus 2
    normal l normal o normal g left-parenthesis ModifyingAbove script upper L With
    caret right-parenthesis EndLayout$  In these equations:    *   *m* is the number
    of instances, as always.           *   *p* is the number of parameters learned
    by the model.           *   $ModifyingAbove script upper L With caret$ is the
    maximized value of the *likelihood function* of the model.              Both the
    BIC and the AIC penalize models that have more parameters to learn (e.g., more
    clusters) and reward models that fit the data well. They often end up selecting
    the same model. When they differ, the model selected by the BIC tends to be simpler
    (fewer parameters) than the one selected by the AIC, but tends to not fit the
    data quite as well (this is especially true for larger datasets).    To compute
    the BIC and AIC, call the `bic()` and `aic()` methods:    [PRE123]   `` `[Figure¬†8-19](#aic_bic_vs_k_plot)
    shows the BIC for different numbers of clusters *k*. As you can see, both the
    BIC and the AIC are lowest when *k* = 3, so it is most likely the best choice.  ![Plot
    showing AIC and BIC values for different numbers of clusters _k_, with both criteria
    reaching their minimum at _k_ = 3.](assets/hmls_0819.png)  ###### Figure 8-19\.
    AIC and BIC for different numbers of clusters *k*` ``  [PRE124]`` [PRE125]` ##
    Bayesian Gaussian Mixture Models    Rather than manually searching for the optimal
    number of clusters, you can use the `BayesianGaussianMixture` class, which is
    capable of giving weights equal (or close) to zero to unnecessary clusters. Set
    the number of clusters `n_components` to a value that you have good reason to
    believe is greater than the optimal number of clusters (this assumes some minimal
    knowledge about the problem at hand), and the algorithm will eliminate the unnecessary
    clusters automatically. For example, let‚Äôs set the number of clusters to 10 and
    see what happens:    [PRE126][PRE127]`` `...` [PRE128] `>>>` `bgm``.``weights_``.``round``(``2``)`
    `` `array([0.4 , 0.21, 0.39, 0\.  , 0\.  , 0\.  , 0\.  , 0\.  , 0\.  , 0\.  ])`
    `` [PRE129]` [PRE130][PRE131]   [PRE132]  [PRE133] ``## Other Algorithms for Anomaly
    and Novelty Detection    Scikit-Learn implements other algorithms dedicated to
    anomaly detection or novelty detection:    Fast-MCD (minimum covariance determinant)      Implemented
    by the `EllipticEnvelope` class, this algorithm is useful for outlier detection,
    in particular to clean up a dataset. It assumes that the normal instances (inliers)
    are generated from a single Gaussian distribution (not a mixture). It also assumes
    that the dataset is contaminated with outliers that were not generated from this
    Gaussian distribution. When the algorithm estimates the parameters of the Gaussian
    distribution (i.e., the shape of the elliptic envelope around the inliers), it
    is careful to ignore the instances that are most likely outliers. This technique
    gives a better estimation of the elliptic envelope and thus makes the algorithm
    better at identifying the outliers.      Isolation forest      This is an efficient
    algorithm for outlier detection, especially in high-dimensional datasets. The
    algorithm builds a random forest in which each decision tree is grown randomly:
    at each node, it picks a feature randomly, then it picks a random threshold value
    (between the min and max values) to split the dataset in two. The dataset gradually
    gets chopped into pieces this way, until all instances end up isolated from the
    other instances. Anomalies are usually far from other instances, so on average
    (across all the decision trees) they tend to get isolated in fewer steps than
    normal instances.      Local outlier factor (LOF)      This algorithm is also
    good for outlier detection. It compares the density of instances around a given
    instance to the density around its neighbors. An anomaly is often more isolated
    than its *k*-nearest neighbors.      One-class SVM      This algorithm is better
    suited for novelty detection. Recall that a kernelized SVM classifier separates
    two classes by first (implicitly) mapping all the instances to a high-dimensional
    space, then separating the two classes using a linear SVM classifier within this
    high-dimensional space (see the online chapter on SVMs at [*https://homl.info*](https://homl.info)).
    Since we just have one class of instances, the one-class SVM algorithm instead
    tries to separate the instances in high-dimensional space from the origin. In
    the original space, this will correspond to finding a small region that encompasses
    all the instances. If a new instance does not fall within this region, it is an
    anomaly. There are a few hyperparameters to tweak: the usual ones for a kernelized
    SVM, plus a margin hyperparameter that corresponds to the probability of a new
    instance being mistakenly considered as novel when it is in fact normal. It works
    great, especially with high-dimensional datasets, but like all SVMs it does not
    scale to large datasets.      PCA and other dimensionality reduction techniques
    with an `inverse_transform()` method      If you compare the reconstruction error
    of a normal instance with the reconstruction error of an anomaly, the latter will
    usually be much larger. This is a simple and often quite efficient anomaly detection
    approach (see this chapter‚Äôs exercises for an example).`` [PRE134]` [PRE135]`
    [PRE136]`` [PRE137][PRE138]py[PRE139]py`` [PRE140]`py [PRE141]`py` [PRE142]`py``
    [PRE143]`py[PRE144][PRE145][PRE146] [PRE147][PRE148][PRE149][PRE150][PRE151]``  [PRE152]
    ``# Exercises    1.  How would you define clustering? Can you name a few clustering
    algorithms?           2.  What are some of the main applications of clustering
    algorithms?           3.  Describe two techniques to select the right number of
    clusters when using *k*-means.           4.  What is label propagation? Why would
    you implement it, and how?           5.  Can you name two clustering algorithms
    that can scale to large datasets? And two that look for regions of high density?           6.  Can
    you think of a use case where active learning would be useful? How would you implement
    it?           7.  What is the difference between anomaly detection and novelty
    detection?           8.  What is a Gaussian mixture? What tasks can you use it
    for?           9.  Can you name two techniques to find the right number of clusters
    when using a Gaussian mixture model?           10.  The classic Olivetti faces
    dataset contains 400 grayscale 64 √ó 64‚Äìpixel images of faces. Each image is flattened
    to a 1D vector of size 4,096\. Forty different people were photographed (10 times
    each), and the usual task is to train a model that can predict which person is
    represented in each picture. Load the dataset using the `sklearn.datasets.fetch_olivetti_faces()`
    function, then split it into a training set, a validation set, and a test set
    (note that the dataset is already scaled between 0 and 1). Since the dataset is
    quite small, you will probably want to use stratified sampling to ensure that
    there are the same number of images per person in each set. Next, cluster the
    images using *k*-means, and ensure that you have a good number of clusters (using
    one of the techniques discussed in this chapter). Visualize the clusters: do you
    see similar faces in each cluster?           11.  Continuing with the Olivetti
    faces dataset, train a classifier to predict which person is represented in each
    picture, and evaluate it on the validation set. Next, use *k*-means as a dimensionality
    reduction tool, and train a classifier on the reduced set. Search for the number
    of clusters that allows the classifier to get the best performance: what performance
    can you reach? What if you append the features from the reduced set to the original
    features (again, searching for the best number of clusters)?           12.  Train
    a Gaussian mixture model on the Olivetti faces dataset. To speed up the algorithm,
    you should probably reduce the dataset‚Äôs dimensionality (e.g., use PCA, preserving
    99% of the variance). Use the model to generate some new faces (using the `sample()`
    method), and visualize them (if you used PCA, you will need to use its `inverse_transform()`
    method). Try to modify some images (e.g., rotate, flip, darken) and see if the
    model can detect the anomalies (i.e., compare the output of the `score_samples()`
    method for normal images and for anomalies).           13.  Some dimensionality
    reduction techniques can also be used for anomaly detection. For example, take
    the Olivetti faces dataset and reduce it with PCA, preserving 99% of the variance.
    Then compute the reconstruction error for each image. Next, take some of the modified
    images you built in the previous exercise and look at their reconstruction error:
    notice how much larger it is. If you plot a reconstructed image, you will see
    why: it tries to reconstruct a normal face.              Solutions to these exercises
    are available at the end of this chapter‚Äôs notebook, at [*https://homl.info/colab-p*](https://homl.info/colab-p).    ^([1](ch08.html#id1959-marker))
    If you are not familiar with probability theory, I highly recommend the free online
    classes by Khan Academy.    ^([2](ch08.html#id1973-marker)) Stuart P. Lloyd, ‚ÄúLeast
    Squares Quantization in PCM‚Äù, *IEEE Transactions on Information Theory* 28, no.
    2 (1982): 129‚Äì137.    ^([3](ch08.html#id1983-marker)) David Arthur and Sergei
    Vassilvitskii, ‚Äúk-Means++: The Advantages of Careful Seeding‚Äù, *Proceedings of
    the 18th Annual ACM-SIAM Symposium on Discrete Algorithms* (2007): 1027‚Äì1035.    ^([4](ch08.html#id1989-marker))
    Charles Elkan, ‚ÄúUsing the Triangle Inequality to Accelerate k-Means‚Äù, *Proceedings
    of the 20th International Conference on Machine Learning* (2003): 147‚Äì153.    ^([5](ch08.html#id1990-marker))
    The triangle inequality is AC ‚â§ AB + BC, where A, B, and C are three points and
    AB, AC, and BC are the distances between these points.    ^([6](ch08.html#id1993-marker))
    David Sculley, ‚ÄúWeb-Scale K-Means Clustering‚Äù, *Proceedings of the 19th International
    Conference on World Wide Web* (2010): 1177‚Äì1178.    ^([7](ch08.html#id2046-marker))
    In contrast, as we saw earlier, *k*-means implicitly assumes that clusters all
    have a similar size and density, and are all roughly round.    ^([8](ch08.html#id2050-marker))
    Phi (*œï* or *œÜ*) is the 21st letter of the Greek alphabet.`` [PRE153]` [PRE154][PRE155][PRE156]
    [PRE157]`py[PRE158]py[PRE159]'
  prefs: []
  type: TYPE_NORMAL
