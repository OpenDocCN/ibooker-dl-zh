["```py\n# Encodes the input into a mean and variance parameter\nz_mean, z_log_variance = encoder(input_img)\n# Draws a latent point using a small random epsilon\nz = z_mean + exp(z_log_variance) * epsilon\n# Decodes z back to an image\nreconstructed_img = decoder(z)\n# Instantiates the autoencoder model, which maps an input image to its\n# reconstruction\nmodel = Model(input_img, reconstructed_img) \n```", "```py\nimport keras\nfrom keras import layers\n\n# Dimensionality of the latent space: a 2D plane\nlatent_dim = 2\n\nimage_inputs = keras.Input(shape=(28, 28, 1))\nx = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(\n    image_inputs\n)\nx = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\nx = layers.Flatten()(x)\nx = layers.Dense(16, activation=\"relu\")(x)\n# The input image ends up being encoded into these two parameters.\nz_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\nz_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\nencoder = keras.Model(image_inputs, [z_mean, z_log_var], name=\"encoder\") \n```", "```py\n>>> encoder.summary()\nModel: \"encoder\"\n┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer           │ (None, 28, 28, 1) │           0 │ -                  │\n│ (InputLayer)          │                   │             │                    │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ conv2d (Conv2D)       │ (None, 14, 14,    │         320 │ input_layer[0][0]  │\n│                       │ 32)               │             │                    │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ conv2d_1 (Conv2D)     │ (None, 7, 7, 64)  │      18,496 │ conv2d[0][0]       │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ flatten (Flatten)     │ (None, 3136)      │           0 │ conv2d_1[0][0]     │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ dense (Dense)         │ (None, 16)        │      50,192 │ flatten[0][0]      │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ z_mean (Dense)        │ (None, 2)         │          34 │ dense[0][0]        │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ z_log_var (Dense)     │ (None, 2)         │          34 │ dense[0][0]        │\n└───────────────────────┴───────────────────┴─────────────┴────────────────────┘\n Total params: 69,076 (269.83 KB)\n Trainable params: 69,076 (269.83 KB)\n Non-trainable params: 0 (0.00 B)\n```", "```py\nfrom keras import ops\n\nclass Sampler(keras.Layer):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        # We need a seed generator to use functions from keras.random\n        # in call().\n        self.seed_generator = keras.random.SeedGenerator()\n        self.built = True\n\n    def call(self, z_mean, z_log_var):\n        batch_size = ops.shape(z_mean)[0]\n        z_size = ops.shape(z_mean)[1]\n        epsilon = keras.random.normal(\n            # Draws a batch of random normal vectors\n            (batch_size, z_size), seed=self.seed_generator\n        )\n        # Applies the VAE sampling formula\n        return z_mean + ops.exp(0.5 * z_log_var) * epsilon \n```", "```py\n# Input where we'll feed z\nlatent_inputs = keras.Input(shape=(latent_dim,))\n# Produces the same number of coefficients we had at the level of the\n# Flatten layer in the encoder\nx = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n# Reverts the Flatten layer of the encoder\nx = layers.Reshape((7, 7, 64))(x)\n# Reverts the Conv2D layers of the encoder\nx = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(\n    x\n)\nx = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(\n    x\n)\n# The output ends up with shape (28, 28, 1).\ndecoder_outputs = layers.Conv2D(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\ndecoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\") \n```", "```py\n>>> decoder.summary()\nModel: \"decoder\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_1 (InputLayer)        │ (None, 2)                │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_1 (Dense)                   │ (None, 3136)             │         9,408 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ reshape (Reshape)                 │ (None, 7, 7, 64)         │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ conv2d_transpose                  │ (None, 14, 14, 64)       │        36,928 │\n│ (Conv2DTranspose)                 │                          │               │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ conv2d_transpose_1                │ (None, 28, 28, 32)       │        18,464 │\n│ (Conv2DTranspose)                 │                          │               │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ conv2d_2 (Conv2D)                 │ (None, 28, 28, 1)        │           289 │\n└───────────────────────────────────┴──────────────────────────┴───────────────┘\n Total params: 65,089 (254.25 KB)\n Trainable params: 65,089 (254.25 KB)\n Non-trainable params: 0 (0.00 B)\n```", "```py\nclass VAE(keras.Model):\n    def __init__(self, encoder, decoder, **kwargs):\n        super().__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n        self.sampler = Sampler()\n        # We'll use these metrics to keep track of the loss averages\n        # over each epoch.\n        self.reconstruction_loss_tracker = keras.metrics.Mean(\n            name=\"reconstruction_loss\"\n        )\n        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n\n    def call(self, inputs):\n        return self.encoder(inputs)\n\n    def compute_loss(self, x, y, y_pred, sample_weight=None, training=True):\n        # Argument x is the model's input.\n        original = x\n        # Argument y_pred is the output of call().\n        z_mean, z_log_var = y_pred\n        # This is our reconstructed image.\n        reconstruction = self.decoder(self.sampler(z_mean, z_log_var))\n\n        # We sum the reconstruction loss over the spatial dimensions\n        # (axes 1 and 2) and take its mean over the batch dimension.\n        reconstruction_loss = ops.mean(\n            ops.sum(\n                keras.losses.binary_crossentropy(x, reconstruction), axis=(1, 2)\n            )\n        )\n        # Adds the regularization term (Kullback–Leibler divergence)\n        kl_loss = -0.5 * (\n            1 + z_log_var - ops.square(z_mean) - ops.exp(z_log_var)\n        )\n        total_loss = reconstruction_loss + ops.mean(kl_loss)\n\n        # Updates the state of our loss-tracking metrics\n        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n        self.kl_loss_tracker.update_state(kl_loss)\n        return total_loss \n```", "```py\nimport numpy as np\n\n(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n# We train on all MNIST digits, so we concatenate the training and test\n# samples.\nmnist_digits = np.concatenate([x_train, x_test], axis=0)\nmnist_digits = np.expand_dims(mnist_digits, -1).astype(\"float32\") / 255\n\nvae = VAE(encoder, decoder)\n# We don't pass a loss argument in compile(), since the loss is already\n# part of the train_step().\nvae.compile(optimizer=keras.optimizers.Adam())\n# We don't pass targets in fit(), since train_step() doesn't expect\n# any.\nvae.fit(mnist_digits, epochs=30, batch_size=128) \n```", "```py\nimport matplotlib.pyplot as plt\n\n# We'll display a grid of 30 × 30 digits (900 digits total).\nn = 30\ndigit_size = 28\nfigure = np.zeros((digit_size * n, digit_size * n))\n\n# Samples points linearly on a 2D grid\ngrid_x = np.linspace(-1, 1, n)\ngrid_y = np.linspace(-1, 1, n)[::-1]\n\n# Iterates over grid locations\nfor i, yi in enumerate(grid_y):\n    for j, xi in enumerate(grid_x):\n        # For each location, samples a digit and adds it to our figure\n        z_sample = np.array([[xi, yi]])\n        x_decoded = vae.decoder.predict(z_sample)\n        digit = x_decoded[0].reshape(digit_size, digit_size)\n        figure[\n            i * digit_size : (i + 1) * digit_size,\n            j * digit_size : (j + 1) * digit_size,\n        ] = digit\n\nplt.figure(figsize=(15, 15))\nstart_range = digit_size // 2\nend_range = n * digit_size + start_range\npixel_range = np.arange(start_range, end_range, digit_size)\nsample_range_x = np.round(grid_x, 1)\nsample_range_y = np.round(grid_y, 1)\nplt.xticks(pixel_range, sample_range_x)\nplt.yticks(pixel_range, sample_range_y)\nplt.xlabel(\"z[0]\")\nplt.ylabel(\"z[1]\")\nplt.axis(\"off\")\nplt.imshow(figure, cmap=\"Greys_r\") \n```", "```py\nimport os\n\nfpath = keras.utils.get_file(\n    origin=\"https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\",\n    extract=True,\n) \n```", "```py\nbatch_size = 32\nimage_size = 128\nimages_dir = os.path.join(fpath, \"jpg\")\ndataset = keras.utils.image_dataset_from_directory(\n    images_dir,\n    # We won't need the labels, just the images.\n    labels=None,\n    image_size=(image_size, image_size),\n    # Crops images when resizing them to preserve their aspect ratio\n    crop_to_aspect_ratio=True,\n)\ndataset = dataset.rebatch(\n    # We'd like all batches to have the same size, so we drop the last\n    # (irregular) batch.\n    batch_size,\n    drop_remainder=True,\n) \n```", "```py\nfrom matplotlib import pyplot as plt\n\nfor batch in dataset:\n    img = batch.numpy()[0]\n    break\nplt.imshow(img.astype(\"uint8\")) \n```", "```py\n# Utility function to apply a block of layers with a residual\n# connection\ndef residual_block(x, width):\n    input_width = x.shape[3]\n    if input_width == width:\n        residual = x\n    else:\n        residual = layers.Conv2D(width, 1)(x)\n    x = layers.BatchNormalization(center=False, scale=False)(x)\n    x = layers.Conv2D(width, 3, padding=\"same\", activation=\"swish\")(x)\n    x = layers.Conv2D(width, 3, padding=\"same\")(x)\n    x = x + residual\n    return x\n\ndef get_model(image_size, widths, block_depth):\n    noisy_images = keras.Input(shape=(image_size, image_size, 3))\n    noise_rates = keras.Input(shape=(1, 1, 1))\n\n    x = layers.Conv2D(widths[0], 1)(noisy_images)\n    n = layers.UpSampling2D(image_size, interpolation=\"nearest\")(noise_rates)\n    x = layers.Concatenate()([x, n])\n\n    skips = []\n    # Dowsampling stage\n    for width in widths[:-1]:\n        for _ in range(block_depth):\n            x = residual_block(x, width)\n            skips.append(x)\n        x = layers.AveragePooling2D(pool_size=2)(x)\n\n    # Middle stage\n    for _ in range(block_depth):\n        x = residual_block(x, widths[-1])\n\n    # Upsampling stage\n    for width in reversed(widths[:-1]):\n        x = layers.UpSampling2D(size=2, interpolation=\"bilinear\")(x)\n        for _ in range(block_depth):\n            x = layers.Concatenate()([x, skips.pop()])\n            x = residual_block(x, width)\n\n    # We set the kernel initializer for the last layer to \"zeros,\"\n    # making the model predict only zeros after initialization (that\n    # is, our default assumption before training is \"no noise\").\n    pred_noise_masks = layers.Conv2D(3, 1, kernel_initializer=\"zeros\")(x)\n\n    # Creates the functional model\n    return keras.Model([noisy_images, noise_rates], pred_noise_masks) \n```", "```py\ndef diffusion_schedule(\n    diffusion_times,\n    min_signal_rate=0.02,\n    max_signal_rate=0.95,\n):\n    start_angle = ops.cast(ops.arccos(max_signal_rate), \"float32\")\n    end_angle = ops.cast(ops.arccos(min_signal_rate), \"float32\")\n    diffusion_angles = start_angle + diffusion_times * (end_angle - start_angle)\n    signal_rates = ops.cos(diffusion_angles)\n    noise_rates = ops.sin(diffusion_angles)\n    return noise_rates, signal_rates \n```", "```py\ndiffusion_times = ops.arange(0.0, 1.0, 0.01)\nnoise_rates, signal_rates = diffusion_schedule(diffusion_times)\n\n# These lines are only necessary if you're using PyTorch, in which case\n# tensor conversion to NumPy is no longer trivial.\ndiffusion_times = ops.convert_to_numpy(diffusion_times)\nnoise_rates = ops.convert_to_numpy(noise_rates)\nsignal_rates = ops.convert_to_numpy(signal_rates)\n\nplt.plot(diffusion_times, noise_rates, label=\"Noise rate\")\nplt.plot(diffusion_times, signal_rates, label=\"Signal rate\")\n\nplt.xlabel(\"Diffusion time\")\nplt.legend() \n```", "```py\nclass DiffusionModel(keras.Model):\n    def __init__(self, image_size, widths, block_depth, **kwargs):\n        super().__init__(**kwargs)\n        self.image_size = image_size\n        self.denoising_model = get_model(image_size, widths, block_depth)\n        self.seed_generator = keras.random.SeedGenerator()\n        # Our loss function\n        self.loss = keras.losses.MeanAbsoluteError()\n        # We'll use this to normalize input images.\n        self.normalizer = keras.layers.Normalization() \n```", "```py\n def denoise(self, noisy_images, noise_rates, signal_rates):\n        # Calls the denoising model\n        pred_noise_masks = self.denoising_model([noisy_images, noise_rates])\n        # Reconstructs the predicted clean image\n        pred_images = (\n            noisy_images - noise_rates * pred_noise_masks\n        ) / signal_rates\n        return pred_images, pred_noise_masks \n```", "```py\n def call(self, images):\n        images = self.normalizer(images)\n        # Samples random noise masks\n        noise_masks = keras.random.normal(\n            (batch_size, self.image_size, self.image_size, 3),\n            seed=self.seed_generator,\n        )\n        # Samples random diffusion times\n        diffusion_times = keras.random.uniform(\n            (batch_size, 1, 1, 1),\n            minval=0.0,\n            maxval=1.0,\n            seed=self.seed_generator,\n        )\n        noise_rates, signal_rates = diffusion_schedule(diffusion_times)\n        # Adds noise to the images\n        noisy_images = signal_rates * images + noise_rates * noise_masks\n        # Denoises them\n        pred_images, pred_noise_masks = self.denoise(\n            noisy_images, noise_rates, signal_rates\n        )\n        return pred_images, pred_noise_masks, noise_masks\n\n    def compute_loss(self, x, y, y_pred, sample_weight=None, training=True):\n        _, pred_noise_masks, noise_masks = y_pred\n        return self.loss(noise_masks, pred_noise_masks) \n```", "```py\n def generate(self, num_images, diffusion_steps):\n        noisy_images = keras.random.normal(\n            # Starts from pure noise\n            (num_images, self.image_size, self.image_size, 3),\n            seed=self.seed_generator,\n        )\n        step_size = 1.0 / diffusion_steps\n        for step in range(diffusion_steps):\n            # Computes appropriate noise rates and signal rates\n            diffusion_times = ops.ones((num_images, 1, 1, 1)) - step * step_size\n            noise_rates, signal_rates = diffusion_schedule(diffusion_times)\n            # Calls denoising model\n            pred_images, pred_noises = self.denoise(\n                noisy_images, noise_rates, signal_rates\n            )\n            # Prepares noisy images for the next iteration\n            next_diffusion_times = diffusion_times - step_size\n            next_noise_rates, next_signal_rates = diffusion_schedule(\n                next_diffusion_times\n            )\n            noisy_images = (\n                next_signal_rates * pred_images + next_noise_rates * pred_noises\n            )\n        # Denormalizes images so their values fit between 0 and 255\n        images = (\n            self.normalizer.mean + pred_images * self.normalizer.variance**0.5\n        )\n        return ops.clip(images, 0.0, 255.0) \n```", "```py\nclass VisualizationCallback(keras.callbacks.Callback):\n    def __init__(self, diffusion_steps=20, num_rows=3, num_cols=6):\n        self.diffusion_steps = diffusion_steps\n        self.num_rows = num_rows\n        self.num_cols = num_cols\n\n    def on_epoch_end(self, epoch=None, logs=None):\n        generated_images = self.model.generate(\n            num_images=self.num_rows * self.num_cols,\n            diffusion_steps=self.diffusion_steps,\n        )\n\n        plt.figure(figsize=(self.num_cols * 2.0, self.num_rows * 2.0))\n        for row in range(self.num_rows):\n            for col in range(self.num_cols):\n                i = row * self.num_cols + col\n                plt.subplot(self.num_rows, self.num_cols, i + 1)\n                img = ops.convert_to_numpy(generated_images[i]).astype(\"uint8\")\n                plt.imshow(img)\n                plt.axis(\"off\")\n        plt.tight_layout()\n        plt.show()\n        plt.close() \n```", "```py\nmodel = DiffusionModel(image_size, widths=[32, 64, 96, 128], block_depth=2)\n# Computes the mean and variance necessary to perform normalization —\n# don't forget it!\nmodel.normalizer.adapt(dataset) \n```", "```py\nmodel.compile(\n    optimizer=keras.optimizers.AdamW(\n        # Configures the learning rate decay schedule\n        learning_rate=keras.optimizers.schedules.InverseTimeDecay(\n            initial_learning_rate=1e-3,\n            decay_steps=1000,\n            decay_rate=0.1,\n        ),\n        # Turns on Polyak averaging\n        use_ema=True,\n        # Configures how often to overwrite the model's weights with\n        # their exponential moving average\n        ema_overwrite_frequency=100,\n    ),\n) \n```", "```py\nmodel.fit(\n    dataset,\n    epochs=100,\n    callbacks=[\n        VisualizationCallback(),\n        keras.callbacks.ModelCheckpoint(\n            filepath=\"diffusion_model.weights.h5\",\n            save_weights_only=True,\n            save_best_only=True,\n        ),\n    ],\n) \n```", "```py\nimport keras_hub\n\nheight, width = 512, 512\ntask = keras_hub.models.TextToImage.from_preset(\n    \"stable_diffusion_3_medium\",\n    image_shape=(height, width, 3),\n    # A trick to keep memory usage down. More details in chapter 18.\n    dtype=\"float16\",\n)\nprompt = \"A NASA astraunaut riding an origami elephant in New York City\"\ntask.generate(prompt) \n```", "```py\ntask.generate(\n    {\n        \"prompts\": prompt,\n        \"negative_prompts\": \"blue color\",\n    }\n) \n```", "```py\nimport numpy as np\nfrom PIL import Image\n\ndef display(images):\n    return Image.fromarray(np.concatenate(images, axis=1))\n\ndisplay([task.generate(prompt, num_steps=x) for x in [5, 10, 15, 20, 25]]) \n```", "```py\nfrom keras import random\n\ndef get_text_embeddings(prompt):\n    token_ids = task.preprocessor.generate_preprocess([prompt])\n    # We don't care about negative prompts here, but the model expects\n    # them.\n    negative_token_ids = task.preprocessor.generate_preprocess([\"\"])\n    return task.backbone.encode_text_step(token_ids, negative_token_ids)\n\ndef denoise_with_text_embeddings(embeddings, num_steps=28, guidance_scale=7.0):\n    # Creates pure noise to denoise into an image\n    latents = random.normal((1, height // 8, width // 8, 16))\n    for step in range(num_steps):\n        latents = task.backbone.denoise_step(\n            latents,\n            embeddings,\n            step,\n            num_steps,\n            guidance_scale,\n        )\n    return task.backbone.decode_step(latents)[0]\n\n# Rescales our images back to [0, 255]\ndef scale_output(x):\n    x = ops.convert_to_numpy(x)\n    x = np.clip((x + 1.0) / 2.0, 0.0, 1.0)\n    return np.round(x * 255.0).astype(\"uint8\")\n\nembeddings = get_text_embeddings(prompt)\nimage = denoise_with_text_embeddings(embeddings)\nscale_output(image) \n```", "```py\n>>> [x.shape for x in embeddings]\n[(1, 154, 4096), (1, 154, 4096), (1, 2048), (1, 2048)]\n```", "```py\nfrom keras import ops\n\ndef slerp(t, v1, v2):\n    v1, v2 = ops.cast(v1, \"float32\"), ops.cast(v2, \"float32\")\n    v1_norm = ops.linalg.norm(ops.ravel(v1))\n    v2_norm = ops.linalg.norm(ops.ravel(v2))\n    dot = ops.sum(v1 * v2 / (v1_norm * v2_norm))\n    theta_0 = ops.arccos(dot)\n    sin_theta_0 = ops.sin(theta_0)\n    theta_t = theta_0 * t\n    sin_theta_t = ops.sin(theta_t)\n    s0 = ops.sin(theta_0 - theta_t) / sin_theta_0\n    s1 = sin_theta_t / sin_theta_0\n    return s0 * v1 + s1 * v2\n\ndef interpolate_text_embeddings(e1, e2, start=0, stop=1, num=10):\n    embeddings = []\n    for t in np.linspace(start, stop, num):\n        embeddings.append(\n            (\n                # The second and fourth text embeddings are for the\n                # negative prompt, which we do not use.\n                slerp(t, e1[0], e2[0]),\n                e1[1],\n                slerp(t, e1[2], e2[2]),\n                e1[3],\n            )\n        )\n    return embeddings \n```", "```py\nprompt1 = \"A friendly dog looking up in a field of flowers\"\nprompt2 = \"A horrifying, tentacled creature hovering over a field of flowers\"\ne1 = get_text_embeddings(prompt1)\ne2 = get_text_embeddings(prompt2)\n\nimages = []\n# Zooms in to the middle of the overall interpolation from [0, 1]\nfor et in interpolate_text_embeddings(e1, e2, start=0.5, stop=0.6, num=9):\n    image = denoise_with_text_embeddings(et)\n    images.append(scale_output(image))\ndisplay(images) \n```"]