<html><head></head><body>
  <h1 class="tochead" id="heading_id_2">12 <a id="idTextAnchor000"/>Reinforcement learning</h1>

  <p class="co-summary-head">This chapter covers<a id="marker-450"/><a id="idIndexMarker000"/></p>

  <ul class="calibre5">
    <li class="co-summary-bullet">Grasping the fundamental principles underlying reinforcement learning</li>

    <li class="co-summary-bullet">Understanding the Markov decision process</li>

    <li class="co-summary-bullet">Comprehending the actor-critic architecture and proximal policy optimization</li>

    <li class="co-summary-bullet">Getting familiar with noncontextual and contextual multi-armed bandits</li>

    <li class="co-summary-bullet">Applying reinforcement learning to solve optimization problems</li>
  </ul>

  <p class="body">Reinforcement learning (RL) is a powerful machine learning approach that enables intelligent agents to learn optimal or near-optimal behavior through interacting with their environments. This chapter dives into the key concepts and techniques within RL, shedding light on its underlying principles as essential background knowledge. Following this theoretical exposition, the chapter will proceed to illustrate practical examples of employing RL strategies to tackle optimization problems.<a id="idIndexMarker001"/><a id="idIndexMarker002"/></p>

  <h2 class="fm-head" id="heading_id_3">12.1 Demystifying reinforcement learning</h2>

  <p class="body"><a id="marker-451"/>Reinforcement learning (RL) is a subfield of machine learning that deals with how an agent can learn to make decisions and take actions in an environment to achieve specific goals following a trial-and-error learning approach. The core idea of RL is that the agent learns by interacting with the environment, receiving feedback in the form of rewards or penalties as a result of its actions. The agent’s objective is to maximize the cumulative reward over time.<a id="idIndexMarker003"/></p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title">Reinforcement learning</p>

    <p class="fm-sidebar-text">“Reinforcement learning problems involve learning what to do—how to map situations to actions—so as to maximize a numerical reward signal” (Richard Sutton and Andrew Barto, in their book <i class="fm-italics">Reinforcement Learning</i> [1]).</p>
  </div>

  <p class="body">Figure 12.1 outlines the common RL algorithms found in the literature. This classification divides RL problems into two main categories: Markov decision process (MDP) problems and multi-armed bandit (MAB) problems. The distinction between the two lies in how the agent’s actions interact with and affect the environment. <a id="idIndexMarker004"/><a id="idIndexMarker005"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH12_F01_Khamis.png"/></p>

    <p class="figurecaption">Figure 12.1 Reinforcement learning algorithm taxonomy</p>
  </div>

  <p class="body">In MDP-based problems, the agent’s actions influence the environment, and the agent must consider the consequences of its actions over multiple time steps, incorporating the notion of states and transitions. MAB problems, on the other hand, involve scenarios where the agent faces a series of choices (arms) and aims to maximize the cumulative reward over time. Such problems are often used when there is no explicit state representation or long-term planning required. In contextual multi-armed bandit (CMAB) problems, the agent is presented with context or side information that is used to make more informed decisions. The expected reward of an arm (an action) is a function of both the action and the current context. This means the best action can change depending on the provided context. It is worth mentioning that MDPs are a more comprehensive framework that accounts for dynamic decision-making in a broader range of situations. <a id="idIndexMarker006"/></p>

  <p class="body">Before we explore how reinforcement learning can be applied to solve optimization problems, it is essential to understand several relevant reinforcement learning techniques. The following subsections will provide a detailed overview of these methods, and the subsequent sections will demonstrate their use in addressing optimization problems.</p>

  <h3 class="fm-head1" id="heading_id_4">12.1.1 Markov decision process (MDP)</h3>

  <p class="body">The purpose of learning is to form an internal model of the external world. This external world, or <i class="fm-italics">environment</i>, can be abstracted using deterministic or nondeterministic (stochastic) models. <a id="idIndexMarker007"/><a id="idIndexMarker008"/><a id="marker-452"/></p>

  <p class="body">Consider a situation where you’re planning to commute from your initial state (e.g., your home) to a designated goal state (e.g., your workplace). A deterministic path-planning algorithm such as A* (discussed in chapter 4) might provide you with multiple options: taking the train, which would take about 1 hour and 48 minutes; driving by car, which could take about 1 hour and 7 minutes; or biking, which could take about 3 hours and 16 minutes (figure 12.2a). These algorithms operate under the assumption that actions and their resulting effects are entirely deterministic.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH12_F02_Khamis.png"/></p>

    <p class="figurecaption">Figure 12.2 Deterministic vs. nondeterministic state transition models</p>
  </div>

  <p class="body">However, if uncertainties come into play during your journey planning, you should resort to stochastic planning algorithms that operate on nondeterministic state transition models to enable planning under uncertainty. In these scenarios, transition probabilities between states become an integral part of your decision-making process.</p>

  <p class="body">For instance, let’s consider the initial state (your home), as shown in figure 12.2b. If you choose to commute by train, there’s a 60% chance that you’ll be able to catch the train on time and reach your destination (your workplace) within the expected 1 hour and 48 minutes. However, there’s a 40% chance that you could miss the train and need to wait for the next one. If you do end up waiting, there’s a 90% chance the train will arrive on time and you’ll catch it and arrive at your destination within a total time of 2 hours and 25 minutes. On the other hand, there’s a 10% chance the train does not arrive, leading to an extended wait or even having to look for an alternative. On the other hand, if you choose to drive, there’s a 30% chance that you’ll encounter light traffic and reach your office in just 50 minutes. However, there’s also a 50% likelihood of medium traffic delaying your arrival to 1 hour and 7 minutes. In the worst-case scenario, there’s a 20% chance that heavy traffic could extend your travel time to 2 hours. If you choose to bike, the estimated travel time of 3 hours and 16 minutes is more predictable and less subject to change.</p>

  <p class="body">This scenario describes an environment that is fully observable and where the current state and actions taken completely determine the probability distribution of the next state. This is called a Markov decision process (MDP).</p>

  <h3 class="fm-head1" id="heading_id_5">12.1.2 From MDP to reinforcement learning</h3>

  <p class="body"><a id="marker-453"/>MDP provides a mathematical framework for planning under uncertainty. It is used to describe an environment for reinforcement learning, where an agent learns to make decisions by performing actions and receiving rewards. The learning process involves trial and error, with the agent discovering which actions yield the highest expected cumulative reward over time. <a id="idIndexMarker009"/><a id="idIndexMarker010"/></p>

  <p class="body">As shown in figure 12.3, the agent interacts with the environment by being in a certain state <span class="times"><i class="fm-italics">s<sub class="fm-subscript">t</sub></i> <span class="cambria">∈</span> <i class="fm-italics">S</i></span>, taking an action <span class="times"><i class="fm-italics">a<sub class="fm-subscript">t</sub></i> <span class="cambria">∈</span> <i class="fm-italics">A</i></span> at time <i class="timesitalic">t</i> based on an observation <i class="timesitalic">o</i> and by applying policy <i class="timesitalic">π</i>, and then receiving a reward <span class="times"><i class="fm-italics">r<sub class="fm-subscript">t</sub></i> <span class="cambria">∈</span> <i class="fm-italics">R</i></span> and transitioning to a new state <span class="times"><i class="fm-italics">s<sub class="fm-subscript">t</sub></i><sub class="fm-subscript">+1</sub> <span class="cambria">∈</span> <i class="fm-italics">S</i></span> according to the state transition probability <i class="timesitalic">T</i>.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH12_F03_Khamis.png"/></p>

    <p class="figurecaption">Figure 12.3 An agent learns through interaction with the environment.</p>
  </div>

  <p class="body">The following terms are commonly used in RL:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">A state,</i> s—This represents the complete and unfiltered information about the environment at a particular time step. An observation <i class="fm-italics">o</i> is the partial or limited information that the agent can perceive from the environment at a given time step. When the agent is able to observe the complete state of the environment, we say that the environment is fully observable and can be modeled as an MDP. When the agent can only see part of the environment, we say that the environment is partially observable, and it should be modeled as a <i class="fm-italics">partially observable Markov decision process</i> (POMDP).<a id="idIndexMarker011"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">An action set,</i> A—This represents the set of possible or permissible actions that an agent can take in a given environment. These actions can be discrete, like in the case of board games, or continuous, like in the case of robot controls or the lane-keep assist of an assisted or automated driving vehicle.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">The policy</i>—This can be seen as the agent’s brain. It is the decision-making strategy of the agent or the mapping from states or observations to actions. This policy can be deterministic, usually denoted by <i class="timesitalic">μ</i>, or stochastic, denoted by <i class="timesitalic">π</i>. A stochastic policy <i class="timesitalic">π</i> is mainly the probability of selecting an action <span class="times"><i class="fm-italics">a</i> <span class="cambria">∈</span> <i class="fm-italics">A</i></span> given a certain state <span class="times"><i class="fm-italics">s</i> <span class="cambria">∈</span> <i class="fm-italics">S</i></span>. A stochastic policy can also be parameterized and denoted by <i class="timesitalic">π<sub class="fm-subscript">Θ</sub></i>. This parameterized policy is a computable function that depends on a set of parameters (e.g., the weights and biases of a neural network), which we can adjust to change the behavior via an optimization algorithm.<a id="idIndexMarker012"/><a id="marker-454"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">A trajectory,</i> <span class="cambria">τ</span> <i class="fm-italics">(aka episode or rollout)</i>—This is a sequence of states and actions in the world, <span class="times">τ = (<i class="fm-italics">s</i><sub class="fm-subscript">0</sub>, <i class="fm-italics">a</i><sub class="fm-subscript">0</sub>, <i class="fm-italics">s</i><sub class="fm-subscript">1</sub>, <i class="fm-italics">a</i><sub class="fm-subscript">1</sub>, ...)</span> .<a id="idIndexMarker013"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">The expected return</i>—This refers to the cumulative sum of rewards that an agent can expect to receive over a future time horizon. It is a measure of the overall desirability or value of a particular state-action sequence or policy.</p>
    </li>
  </ul>

  <p class="body">Let’s consider a simple Reach the Treasure game where an agent tries to get a treasure and then exits. In this game, there are only four states, as illustrated in figure 12.4. Among them, state 0 represents a fire pit, and state 3 represents the exit—both are terminal states. State 1 contains the treasure, symbolized by a diamond. The game starts with the agent positioned in state 2 and has the options of moving left or right as actions. Upon reaching the treasure, the agent receives a reward of +10. However, falling into the fire pit results in a penalty of –10. Successfully exiting grants a reward of +4.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH12_F04_Khamis.png"/></p>

    <p class="figurecaption">Figure 12.4 Reach the Treasure game</p>
  </div>

  <p class="body"><a id="marker-455"/>The state <i class="timesitalic">s</i> provides the complete information about the environment, including the agent’s position and the locations of the fire pit, treasure, and exit (state 0 is the fire pit, state 1 is the treasure, state 2 is the current location, and state 3 is the exit). The game’s observations are collected and presented as an observation vector, such as <i class="fm-italics">obs</i> = [2,0], indicating that the agent is in state 2 and does not perceive the presence of the treasure. This is a partial view of the environment, as the agent does not have access to the complete state information, such as the locations of the fire pit or exit. The policy denotes the probabilities assigned to moving left or right based on the observation. For instance, a policy(obs) of [0.4, 0.6] signifies a 40% chance of moving left and a 60% chance of moving right. In the single trajectory shown in figure 12.4, we can calculate the expected return as follows: <span class="times">expected return (R) = 0.4 * (10) + 0.6 * (4) + 0.4 * 0.2 * (–10) + 0.4 * 0.8 * (0) + 0.4 * 0.8 * 0.9 * (0) + 0.4 * 0.8 * 0.1 * (4) = 5.728</span>.</p>

  <p class="body">The goal in RL is to learn an optimal policy that maximizes the expected cumulative discounted reward. Value iteration, policy iteration, and policy gradients are different iterative methods used in reinforcement learning to achieve this goal. The value function in RL defines the expected cumulative reward of the agent starting from a particular state or state-action pair, following a certain policy. There are two types of value functions: the state-value function <span class="times"><i class="fm-italics">V</i>(<i class="fm-italics">s</i>)</span> and the action-value function <span class="times"><i class="fm-italics">Q</i>(<i class="fm-italics">s</i>,<i class="fm-italics">a</i>)</span>. The state-value function <span class="times"><i class="fm-italics">V</i>(<i class="fm-italics">s</i>)</span> estimates the expected cumulative future rewards an agent may obtain, starting from a particular state <i class="timesitalic">s</i> and following a policy <i class="timesitalic">π</i>. It quantifies the desirability of a state based on its projected future rewards. The state-value function <span class="times"><i class="fm-italics">V</i>(<i class="fm-italics">s</i>)</span> is given by the following formula:<a id="idIndexMarker014"/><a id="idIndexMarker015"/><a id="marker-456"/></p>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="90%"/>
      <col class="contenttable-0-col" span="1" width="10%"/>
    </colgroup>

    <tbody class="calibre6">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figured"><img alt="" class="calibre4" src="../Images/CH12_F04_Khamis-EQ01.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-equation-caption">12.1</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body">where</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><span class="times"><i class="fm-italics">V<sup class="fm-superscript">π</sup></i>(<i class="fm-italics">s</i>)</span> is the expected return when starting in s and following <i class="timesitalic">π</i> thereafter.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><span class="times"><i class="fm-italics">E<sub class="fm-subscript">π</sub></i>[]</span> denotes the expected value, given that the agent follows policy <i class="timesitalic">π</i>.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="timesitalic">t</i> is any time step.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="timesitalic">γ</i> is the discount factor, typically a value between 0 and 1, representing the present value of future rewards relative to immediate rewards. The purpose of discounting is to prioritize immediate rewards more heavily, reflecting the preference for rewards received sooner rather than later. A discount factor close to 0 makes the agent <i class="fm-italics">myopic</i> (i.e., focused on immediate rewards), while a discount factor close to 1 makes the agent more <i class="fm-italics">farsighted</i> (i.e., considering future rewards). <a id="idIndexMarker016"/><a id="idIndexMarker017"/></p>
    </li>
  </ul>

  <p class="body">The action-value function, <span class="times"><i class="fm-italics">Q</i>(<i class="fm-italics">s, a</i>)</span>, estimates the expected cumulative future rewards an agent can achieve by taking a specific action <i class="fm-italics">a</i> from a given state <i class="timesitalic">s</i> and following a policy <i class="timesitalic">π</i>. It quantifies the “goodness” of taking a specific action in a specific state under a given policy. The action-value function <span class="times"><i class="fm-italics">Q</i>(<i class="fm-italics">s, a</i>)</span> is given by the following formula:</p>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="90%"/>
      <col class="contenttable-0-col" span="1" width="10%"/>
    </colgroup>

    <tbody class="calibre6">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figured"><img alt="" class="calibre4" src="../Images/CH12_F04_Khamis-EQ02.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-equation-caption">12.2</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body">In <i class="fm-italics">policy iteration methods</i>, we first compute the value <i class="fm-italics">V<sup class="fm-superscript">π</sup></i>(<i class="fm-italics">s</i>) of each state following an initial policy, and we use these value estimates to improve the policy. This means that from an initial policy we repeatedly alternate between policy evaluation and policy improvement steps (until convergence). <a id="idIndexMarker018"/></p>

  <p class="body"><i class="fm-italics">Policy gradient methods</i> learn a parameterized policy that is used by the agent to choose actions. The goal is to find the values of the policy parameters that maximize the expected cumulative reward over time. <a id="idIndexMarker019"/></p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title">Policy gradient</p>

    <p class="fm-sidebar-text">Policy gradient is a model-free policy-based method that doesn’t require explicit value function representation. The core idea is to favor actions that lead to higher returns while discouraging actions that result in lower rewards. This iterative process refines the policy over time, aiming to find a high-performing policy.<a id="idIndexMarker020"/></p>
  </div>

  <p class="body"><a id="marker-457"/>Instead of explicitly estimating the value function, policy gradient methods work by computing an estimator of the policy gradient and plugging it in to a stochastic gradient ascent algorithm. Policy gradient loss <i class="timesitalic">L<sup class="fm-superscript">PG</sup></i> is one of the most commonly used gradient estimators:<a id="idIndexMarker021"/></p>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="90%"/>
      <col class="contenttable-0-col" span="1" width="10%"/>
    </colgroup>

    <tbody class="calibre6">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figured"><img alt="" class="calibre4" src="../Images/CH12_F04_Khamis-EQ03.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-equation-caption">12.3</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body">where</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">The expectation <i class="timesitalic">E<sub class="fm-subscript">t</sub></i> indicates the empirical average over a finite batch of samples.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="timesitalic">π<sub class="fm-subscript">θ</sub></i> is a stochastic policy that takes the observed states from the environment as an input and suggests actions to take as an output.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="timesitalic">Â<sub class="fm-subscript">t</sub></i> is an estimate of the advantage function at time step <i class="timesitalic">t</i>. This estimate basically tries to assess the relative value of the selected action in the current state. The advantage function represents the advantage of taking a particular action in a given state, compared to the expected value. It is calculated as the difference between the expected rewards from executing the suggested action (which often has the highest Q-value) and the estimated value function of the current state:</p>
    </li>
  </ul>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="90%"/>
      <col class="contenttable-0-col" span="1" width="10%"/>
    </colgroup>

    <tbody class="calibre6">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figured"><img alt="" class="calibre4" src="../Images/CH12_F04_Khamis-EQ04.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-equation-caption">12.4</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body">where</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><span class="times"><i class="fm-italics">Q</i>(<i class="fm-italics">s, a</i>)</span> is the action-value function (also known as the Q-value), which represents the expected cumulative rewards from taking action <i class="timesitalic">a</i> in state <i class="timesitalic">s</i> following the policy.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><span class="times"><i class="fm-italics">V</i>(<i class="fm-italics">s</i>)</span> is the state-value function, which represents the expected cumulative rewards from state <i class="timesitalic">s</i> following the policy.</p>
    </li>
  </ul>

  <p class="body">As you can see in equation 12.4, if the advantage function is positive, indicating that the observed return is higher than the expected value, the gradient will be positive. This positive gradient means that the probabilities of the actions taken in that state will be increased in the future to enhance their likelihood. On the other hand, if the advantage function is negative, the gradient will be negative. This negative gradient implies that the probabilities of the selected actions will be decreased if similar states are encountered in the future.</p>

  <h3 class="fm-head1" id="heading_id_6">12.1.3 Model-based vs. model-free RL</h3>

  <p class="body">Reinforcement learning is categorized into two main types: model-based RL (MBRL) and model-free RL (MFRL). This classification is based on whether the RL agent possesses a model of the environment or not. The term <i class="fm-italics">model</i> refers to an internal representation of the environment, encompassing its transition dynamics and reward function. Table 12.1 summarizes the differences between these two categories.<a id="idIndexMarker022"/><a id="idIndexMarker023"/><a id="idIndexMarker024"/><a id="marker-458"/></p>

  <p class="fm-table-caption">Table 12.1 Model-based RL (MBRL) versus model-free RL (MFRL)</p>

  <table border="1" class="contenttable-1-table" id="table001" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="20%"/>
      <col class="contenttable-0-col" span="1" width="40%"/>
      <col class="contenttable-0-col" span="1" width="40%"/>
    </colgroup>

    <thead class="calibre6">
      <tr class="contenttable-0-tr">
        <th class="contenttable-1-th">
          <p class="fm-table-head">Aspects</p>
        </th>

        <th class="contenttable-1-th">
          <p class="fm-table-head">Model-based RL (MBRL)</p>
        </th>

        <th class="contenttable-1-th">
          <p class="fm-table-head">Model-free RL (MFRL)</p>
        </th>
      </tr>
    </thead>

    <tbody class="calibre6">
      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Environment model</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Uses a known model or learns a model of the environment (i.e., transition probabilities)</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Skips models and directly learns what action to take when (without necessarily finding out the exact model of the action)</p>
        </td>
      </tr>

      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Rewards</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Typically known or learned</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Unknown or partially known. Model-free RL learns directly from the rewards received during interaction with the environment.</p>
        </td>
      </tr>

      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Actions</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Selected to maximize the expected cumulative reward using the model</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Selected to maximize the expected cumulative rewards based on the history of experiences</p>
        </td>
      </tr>

      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Policy</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Policy learning is accomplished by learning a model of the environment dynamics.</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Policy learning is achieved through trial and error, directly optimizing the policy based on observed experiences.</p>
        </td>
      </tr>

      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Design and tuning</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">MBRL can have a higher initial design and tuning effort due to model complexity. However, advancements are simplifying this process.</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Requires less initial effort. However, MFRL hyperparameter tuning can also be challenging, especially for complex tasks.</p>
        </td>
      </tr>

      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Examples</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">AlphaZero, world models, and imagination-augmented agents (I2A)</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Q-learning, advantage actor-critic (A2C), asynchronous advantage actor-critic (A3C), and proximal policy optimization (PPO)</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body">Based on how RL algorithms learn and update their policies from collected experiences, RL algorithms can also be classified as off-policy and on-policy RL. Off-policy methods learn from experiences generated by a policy different from the one being updated, while on-policy methods learn from experiences generated by the current policy being updated. Both on-policy and off-policy methods are often considered <i class="fm-italics">model-free</i> because they directly learn policies or value functions from experiences without explicitly constructing a model of the environment’s dynamics, distinguishing them from model-based approaches. Table 12.2 summarizes the differences between off-policy and on-policy model-free RL methods.<a id="idIndexMarker025"/><a id="idIndexMarker026"/></p>

  <p class="fm-table-caption">Table 12.2 Off-policy versus on-policy RL methods</p>

  <table border="1" class="contenttable-1-table" id="table002" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="20%"/>
      <col class="contenttable-0-col" span="1" width="40%"/>
      <col class="contenttable-0-col" span="1" width="40%"/>
    </colgroup>

    <thead class="calibre6">
      <tr class="contenttable-0-tr">
        <th class="contenttable-1-th">
          <p class="fm-table-head">Aspects</p>
        </th>

        <th class="contenttable-1-th">
          <p class="fm-table-head">Off-policy RL methods</p>
        </th>

        <th class="contenttable-1-th">
          <p class="fm-table-head">On-policy RL methods</p>
        </th>
      </tr>
    </thead>

    <tbody class="calibre6">
      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Learning approach</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Learn from experiences generated by a different policy than the one being updated</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Learn from experiences generated by the current policy being updated</p>
        </td>
      </tr>

      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Sample efficiency</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Typically more sample-efficient due to reusing past experiences (recorded data)</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Typically less sample-efficient, as the batch of experiences is discarded after each policy update (past experiences are not explicitly stored)</p>
        </td>
      </tr>

      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Policy evaluation</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Can learn the value function and policy separately, enabling different algorithms (e.g., Q-learning, DDPG, TD3)</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Policy evaluation and improvement are typically intertwined in on-policy algorithms (e.g., REINFORCE, A2C, PPO).</p>
        </td>
      </tr>

      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Pros</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">More sample-efficient, can learn from diverse experiences, enables reuse of past data, useful if large amounts of prior data are available</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Simpler and more straightforward, avoids off-policy correction, can converge to better local optima, suitable for scenarios with limited data or online learning</p>
        </td>
      </tr>

      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Cons</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Requires careful off-policy correction, less suitable for online learning or tasks with limited data</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Less sample-efficient, discards past experiences, limited exploration diversity, may converge to suboptimal policies</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body">The following two subsections provide more details about A2C and PPO as examples of the on-policy methods used in this chapter.</p>

  <h3 class="fm-head1" id="heading_id_7">12.1.4 Actor-critic methods</h3>

  <p class="body"><a id="marker-459"/>Figure 12.5 shows the advantage actor-critic (A2C) architecture as an example of actor-critic methods. As the name suggests, this architecture consists of two models: the <i class="fm-italics">actor</i> and the <i class="fm-italics">critic</i>. The actor is responsible for learning and updating the policy. It takes the current state as input and outputs the probability distribution over the actions that represent the policy. The critic, on the other hand, focuses on evaluating the action suggested by the actor. It takes the state and action as input and estimates the advantage of taking that action in that particular state. The advantage represents how much better (or worse) the action is compared to the average action in that state based on expected future rewards. This feedback from the critic helps the actor learn and update the policy to favor actions with higher advantages.<a id="idIndexMarker027"/><a id="idIndexMarker028"/><a id="idIndexMarker029"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH12_F05_Khamis.png"/></p>

    <p class="figurecaption">Figure 12.5 The advantage actor-critic (A2C) architecture</p>
  </div>

  <p class="body">A2C is a synchronous, model-free algorithm that aims to learn both the policy (the actor) and the value function (the critic) simultaneously. It learns an optimal policy by iteratively improving the actor and critic networks. By estimating advantages, the algorithm can provide feedback on the quality of the actions taken by the actor. The critic network helps estimate the value function, providing a baseline for the advantages calculation. This combination allows the algorithm to update the policy in a more stable and efficient manner.</p>

  <h3 class="fm-head1" id="heading_id_8">12.1.5 Proximal policy optimization</h3>

  <p class="body"><a id="marker-460"/>The proximal policy optimization (PPO) algorithm is an on-policy model-free RL designed by OpenAI [2], and it has been successfully used in many applications such as video gaming and robot control. PPO is based on the actor-critic architecture.<a id="idIndexMarker030"/><a id="idIndexMarker031"/></p>

  <p class="body">In RL, the agent generates its own training data through interactions with the environment. Unlike supervised machine learning, which relies on static datasets, RL’s training data is dynamically dependent on the current policy. This dynamic nature leads to constantly changing data distributions, introducing potential instability during training. In the policy gradient method explained previously, if you continuously apply gradient ascent on a single batch of collected experiences, it can lead to updates that push the parameters of the network too far from the range where the data was collected. Consequently, the advantage function, which provides an estimate of the true advantage, becomes inaccurate, and the policy can be severely disrupted. To address this problem, two primary variants of PPO have been proposed: PPO-penalty and PPO-clip.</p>

  <p class="fm-head2">PPO-penalty</p>

  <p class="body">In <i class="fm-italics">PPO-penalty</i>, a constraint is incorporated in the objective function to ensure that the policy update does not deviate too much from the old policy. This idea is the basis of trust region policy optimization (TRPO). By enforcing a trust region constraint, TRPO restricts the policy update to a manageable region and prevents large policy shifts. PPO-penalty is primarily inspired by TRPO and uses the following unconstrained objective function, which can be optimized using stochastic gradient ascent: <a id="idIndexMarker032"/><a id="idIndexMarker033"/><a id="idIndexMarker034"/><a id="idIndexMarker035"/></p>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="90%"/>
      <col class="contenttable-0-col" span="1" width="10%"/>
    </colgroup>

    <tbody class="calibre6">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figured"><img alt="" class="calibre4" src="../Images/CH12_F05_Khamis-EQ05.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-equation-caption">12.5</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body">where <i class="fm-italics">θ<sub class="fm-subscript">old</sub></i> is the vector of policy parameters before the update, <i class="timesitalic">β</i> is a fixed penalty coefficient, and the Kullback–Leibler divergence (KL) represents the divergence between the updated and old policies. This constraint is integrated into the objective function to avoid the risk of moving too far from the old policy.<a id="idIndexMarker036"/><a id="idIndexMarker037"/><a id="marker-461"/></p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title">Kullback–Leibler divergence</p>

    <p class="fm-sidebar-text"><i class="fm-italics">Kullback</i>–<i class="fm-italics">Leibler (KL) divergence</i>, also known as <i class="fm-italics">relative entropy</i>, is a metric that quantifies the dissimilarity between two probability distributions. KL divergence between two probability distributions <i class="timesitalic">P</i> and <i class="timesitalic">Q</i> is defined as <span class="times">KL(<i class="fm-italics">P</i> || <i class="fm-italics">Q</i>) = ∫ <i class="fm-italics">P</i>(<i class="fm-italics">x</i>) ⋅ log(<i class="fm-italics">P</i>(<i class="fm-italics">x</i>) / <i class="fm-italics">Q</i>(x)) <i class="fm-italics">dx</i></span>, where <span class="times"><i class="fm-italics">P</i>(<i class="fm-italics">x</i>)</span> and <span class="times"><i class="fm-italics">Q</i>(<i class="fm-italics">x</i>)</span> represent the probability density functions (PDFs) of the two distributions. The integral is taken over the entire support of the random variable <i class="timesitalic">x</i> (i.e., the range of values of the random variable where the PDF is nonzero). The following figure shows the KL divergence between two Gaussian distributions with different means and variances.<a id="idIndexMarker038"/><a id="idIndexMarker039"/><a id="idIndexMarker040"/></p>

    <p class="sidebarafigures"><img alt="" class="calibre2" src="../Images/CH12_F05_UN01_Khamis.png"/></p>

    <p class="sidebaracaptions">KL divergence between two Gaussian distributions</p>

    <p class="fm-sidebar-text">The KL divergence is equal to zero if, and only if, <i class="timesitalic">P</i> and <i class="timesitalic">Q</i> are identical distributions.</p>
  </div>

  <p class="fm-head2">PPO-clip</p>

  <p class="body"><a id="marker-462"/>In <i class="fm-italics">PPO-clip</i>, a ratio <span class="times"><i class="fm-italics">r</i>(<i class="fm-italics">θ</i>)</span> is defined as a probability ratio between the updated policy and the old version of the policy. Given a sequence of sample actions and states, this <span class="times"><i class="fm-italics">r</i>(<i class="fm-italics">θ</i>)</span> value will be larger than 1 if the action is more likely now than it was in the old version of the policy, and it will be somewhere between 0 and 1 if it is less likely now than it was before the last gradient step. <a id="idIndexMarker041"/><a id="idIndexMarker042"/><a id="idIndexMarker043"/></p>

  <p class="body">The central objective function to be maximized in PPO-clip takes the following form:</p>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="90%"/>
      <col class="contenttable-0-col" span="1" width="10%"/>
    </colgroup>

    <tbody class="calibre6">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figured"><img alt="" class="calibre4" src="../Images/CH12_F05_UN01_Khamis-EQ06.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-equation-caption">12.6</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body">where <i class="timesitalic">L<sup class="fm-superscript">CLIP</sup></i> is the clipped surrogate objective, which is an <i class="fm-italics">expectation operator</i> computed over batches of trajectories, and epsilon <i class="timesitalic">ϵ</i> is a hyperparameter (e.g., <span class="times"><i class="timesitalic">ϵ</i> = 0.2)</span>. As you can see in equation 12.6, the expectation operator is taken over the minimum of two terms. The first term represents the default objective used in normal policy gradients. It encourages the policy to favor actions that result in a high positive advantage compared to a baseline. The second term is a clipped or truncated version of the normal policy gradients. It applies a clipping operation to ensure that the update remains within a specified range, specifically between <span class="times">1 – <i class="timesitalic">ϵ</i></span> and <span class="times">1 + <i class="timesitalic">ϵ</i></span>. <a id="idIndexMarker044"/></p>

  <p class="body"><a id="marker-463"/>The clipped surrogate objective in PPO has different regions that define how the objective function behaves based on the advantage estimate <i class="timesitalic">Â<sub class="fm-subscript">t</sub></i> and the ratio of probabilities, as illustrated in figure 12.6.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH12_F06_Khamis.png"/></p>

    <p class="figurecaption">Figure 12.6 Clipped surrogate objective in PPO</p>
  </div>

  <p class="body">On the left side of figure 12.6, where the advantage function is positive, the objective function represents cases where the selected action has a better-than-expected effect on the outcome. It’s important to observe how the objective function flattens out when the ratio (<i class="timesitalic">r</i>) becomes too high. This occurs when the action is more likely under the current policy compared to the old policy. The clipping operation limits the update to a range where the new policy does not deviate significantly from the old policy, preventing excessively large policy updates that may disrupt training stability.</p>

  <p class="body">On the right side of figure 12.6, where the advantage function is negative, it represents situations where the action has an estimated negative effect on the outcome. The objective function flattens out when the ratio (<i class="timesitalic">r</i>) approaches zero. This corresponds to actions that are much less likely under the current policy compared to the old policy. This flattening effect prevents overdoing updates that could otherwise reduce the probabilities of these actions to zero.</p>

  <h3 class="fm-head1" id="heading_id_9">12.1.6 Multi-armed bandit (MAB)</h3>

  <p class="body"><a id="marker-464"/>The <i class="fm-italics">multi-armed bandit</i> (MAB) is a class of reinforcement learning problems with a single state. In MAB, an agent is faced with a set of actions or “arms” to choose from, and each action has an associated reward distribution. The agent’s goal is to maximize the total reward accumulated over a series of actions. The agent does not modify its environment through its actions, does not consider state transitions, and always stays in the same single state. It focuses on selecting the most rewarding action at each time step without considering the impact on the environment’s state. <a id="idIndexMarker045"/><a id="idIndexMarker046"/></p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title">Slot machine (one-armed bandit)</p>

    <p class="fm-sidebar-text">A <i class="fm-italics">slot machine</i> (aka a fruit machine, pokies, or one-armed bandit) is a popular gambling device typically found in casinos. It is a mechanical or electronic gaming machine that features multiple spinning reels with various symbols on them. Players insert coins or tokens into the machine and then pull a lever (hence the term “one-armed bandit”) or press a button to initiate the spinning of the reels. The objective of playing a slot machine is to align the symbols on the spinning reels in a winning combination. The term “bandit” originates from the analogy to a slot machine, where the agent pulls an arm (like pulling the lever on a slot machine) to receive varying rewards. This highlights how people perceive slot machines, especially older mechanical ones, as resembling a thief or bandit taking the player’s money.<a id="idIndexMarker047"/></p>
  </div>

  <p class="body">The MAB agent faces the exploration versus exploitation dilemma. To learn the best actions, it must explore various options (exploration). However, it also needs to quickly converge and swiftly focus on the most promising action (exploitation) based on its current beliefs. In supervised and unsupervised machine learning, the primary goal is to fit a prediction model (in supervised learning) or to discover patterns within the given data (in unsupervised learning), without an explicit notion of exploration, as seen in reinforcement learning, where the agent interacts with an environment to learn optimal behavior through trial and error. MAB problems provide valuable insights into learning from limited feedback and balancing exploration against discovering high-reward actions. In MAB, the agent’s objective is to exploit the actions that have historically yielded high rewards while exploring to gather information about potentially more rewarding actions.</p>

  <p class="body">To understand MAB, imagine you’re in a casino, and in front of you is a row of slot machines (one-armed bandits). Each slot machine represents an “arm” of the MAB. Let’s assume that you are presented with three slot machines, as shown in figure 12.7. Every time you decide to play a slot machine, your situation (or state) is the same: “Which slot machine should I play next?” The environment doesn’t change or provide you with different conditions; you’re perpetually making decisions in this singular state. There’s no context such as “If the casino is crowded, then play machine A” or “If it’s raining outside, then play machine B.” It’s always just “Which machine should I play?” Your action is choosing a slot machine to play—you insert a coin and pull the lever. You are allowed to pull the levers of these machines for a total of 90 times, and each slot machine has its own payoff distribution, characterized by its mean and standard deviation. However, at the beginning, you are unaware of the specific details of these distributions. After pulling the lever, the machine might give you a payout (a positive reward) or nothing (a reward of zero). Over time, you’re trying to discern if one machine gives a higher payout more frequently than the others.<a id="marker-465"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH12_F07_Khamis.png"/></p>

    <p class="figurecaption">Figure 12.7 Three slot machines as a noncontextual multi-armed bandit (MAB) problem. The payoff distribution’s mean and standard deviations of these three machines are (8,0.5), (10, 0.7), and (5, 0.45), respectively.</p>
  </div>

  <p class="body">The objective in this MAB problem is to maximize the total payoff or cumulative reward obtained over the 90 trials by choosing the most rewarding slot machine. In other words, you’re trying to learn the payoff distributions of the machines. The term <i class="fm-italics">payoff distribution</i> refers to the probability distributions of rewards (or payoffs) that each machine (or arm) provides. Since you don’t have prior knowledge about these payoff distributions, you need to explore the slot machines by trying different options to gather information about their performance. The challenge is to strike a balance between exploration and exploitation. Exploration involves trying different machines to learn their payoff distributions, while exploitation involves choosing the machine that is believed to yield the highest rewards based on the available information.</p>

  <p class="body">You can apply various bandit algorithms or strategies to determine the best approach for selecting the slot machines and maximizing your cumulative reward over the 90 trials. Examples of these strategies include, but are not limited to, explore-only, exploit-only greedy, epsilon-greedy (<span class="cambria">ε</span>-greedy), and upper confidence bound (UCB):</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Explore-only strategy</i>—In this strategy, the agent randomly selects a slot machine to play at each trial without considering the past results. In MAB, “regret” is a common measure, calculated as the difference between the maximum possible reward and the reward obtained from each selected machine. For example, if we apply the explore-only strategy in the 90 trials, and considering the mean of the payoff distribution of each slot machine, we will get a total average return of <span class="times">30 × 8 + 30 × 10 + 30 × 5 = 690</span>. The maximum possible reward can be obtained if you use machine 2 during the 90 trials. The maximum reward in this case will be <span class="times">90 × 10 = 900</span>. This means that regret <span class="times"><i class="fm-italics">ρ</i> = 900 – 690 = 210</span>. <a id="idIndexMarker048"/><a id="marker-466"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Exploit-only greedy strategy</i>—In this case, the agent tries each machine once and then selects the slot machine that has the highest estimated mean reward. For example, assume that in the first trial, the agent gets payoffs of 7, 6, and 3 from machines 1, 2, and 3 respectively. The agent will then focus on using machine 1, thinking that it is the most rewarding. This can lead the agent to get stuck due to a lack of exploration. <a id="idIndexMarker049"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><span class="cambria">ε</span><i class="fm-italics">-greedy strategy</i>—Here, the agent tries to strike a balance between exploration and exploitation by randomly selecting a slot machine with a certain probability (epsilon). This is the exploration part, where the agent occasionally tries out all three machines to gather more information about them. With a probability of 1 – <span class="cambria">ε</span>, the agent chooses the machine that has the highest estimated reward based on past experiences. This is the exploitation part, where the agent opts for the action that appears to be the best based on data the agent has gathered so far. For example, for the 90 trials and if <span class="times">ε = 10%</span>, the agent will randomly select a slot machine about 9 times (10% of 90). The other 81 trials (90% of 90) would see the agent choosing the slot machine that has, based on the trials up to that point, yielded the highest average reward.<a id="idIndexMarker050"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Upper confidence bound (UCB) strategy</i>—In this strategy, the agent selects a machine based on a trade-off between its estimated mean reward and the uncertainty or confidence in that estimate. It prioritizes exploring machines with high potential rewards but high uncertainty in their estimates to reduce uncertainty and maximize rewards over time. In UCB, the arm (the action <span class="times"><i class="fm-italics">A</i><sub class="fm-subscript">t</sub>)</span> is chosen at time step <i class="timesitalic">t</i> using the following formula:<a id="idIndexMarker051"/></p>
    </li>
  </ul>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="90%"/>
      <col class="contenttable-0-col" span="1" width="10%"/>
    </colgroup>

    <tbody class="calibre6">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figured"><img alt="" class="calibre4" src="../Images/CH12_F07_Khamis-EQ07.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-equation-caption">12.7</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body-ind">where <span class="times"><i class="fm-italics">Q<sub class="fm-subscript">t</sub></i>(<i class="fm-italics">a</i>)</span> is the estimated value of action <i class="timesitalic">a</i> at trial <i class="timesitalic">t</i>. <span class="times"><i class="fm-italics">Q<sub class="fm-subscript">t</sub></i>(<i class="fm-italics">a</i>) = sum of rewards/<i class="fm-italics">N<sub class="fm-subscript">t</sub></i>(<i class="fm-italics">a</i>)</span>. <span class="times"><i class="fm-italics">N<sub class="fm-subscript">t</sub></i>(<i class="fm-italics">a</i>)</span> is the number of trials where action <i class="timesitalic">a</i> has been selected, prior to trial <i class="timesitalic">t</i>. The first term on the right side of equation 12.7 represents the exploitation part. If you always pulled the arm with the highest <span class="times"><i class="fm-italics">Q<sub class="fm-subscript">t</sub></i>(<i class="fm-italics">a</i>)</span>, you would always be exploiting the current knowledge without exploring other arms. The second term is the exploration part. As the number of trials <i class="timesitalic">t</i> increases, the exploration term generally increases, but it’s reduced by how often action <i class="timesitalic">a</i> has already been selected. The multiplier <i class="timesitalic">c</i> scales the influence of this exploration term.</p>

  <p class="body">Each strategy employs a different approach to balance exploration and exploitation, leading to different levels of regret. This regret level quantifies the cumulative loss an agent incurs due to not always choosing the optimal action. Intuitively, it measures the difference between the reward an agent could have achieved by always pulling the best arm (i.e., by selecting the optimal action) and the reward the agent actually received by following a certain strategy.</p>

  <p class="body"><a id="marker-467"/>Let’s look at Python implementations of the four MAB strategies. We’ll start by setting the numbers of arms (actions), the payoff distributions of each slot machine, and the number of trials. We’ll also define a <code class="fm-code-in-text">sample_payoff</code> to sample a payoff from a slot machine and calculate the maximum possible reward.<a id="idIndexMarker052"/></p>

  <p class="fm-code-listing-caption">Listing 12.1 MAB strategies</p>
  <pre class="programlisting">import numpy as np
  
K = 3                                                             <span class="fm-combinumeral">①</span>
payoff_params = [
    {"mean": 8, "std_dev": 0.5},
    {"mean": 10, "std_dev": 0.7},
    {"mean": 5, "std_dev": 0.45}
]                                                                 <span class="fm-combinumeral">②</span>
num_trials = 90                                                   <span class="fm-combinumeral">③</span>
  
def sample_payoff(slot_machine):
    return np.random.normal(payoff_params[slot_machine]["mean"], 
<span class="fm-code-continuation-arrow">➥</span>        payoff_params[slot_machine]["std_dev"])                 <span class="fm-combinumeral">④</span>
  
max_reward = max([payoff_params[i]["mean"] for i in range(K)])    <span class="fm-combinumeral">⑤</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Set the number of slot machines (arms).</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Specify payoff distribution.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Set the number of trials.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Function to sample a payoff from a slot machine</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Calculate the maximum possible reward.</p>

  <p class="body">As a continuation of listing 12.1, we’ll define a function named <code class="fm-code-in-text">explore_only()</code> that implements the explore-only strategy. In this function, <code class="fm-code-in-text">total_regret</code> is initialized, and we iterate over the specified number of trials. A slot machine is randomly selected by generating a random integer between 0 and <i class="fm-italics">K</i> – 1 (inclusive), where <i class="fm-italics">K</i> represents the number of slot machines. We then sample the payoff from the selected slot machine by calling the <code class="fm-code-in-text">sample_payoff()</code> function, which returns a reward value based on the payoff distribution of the selected machine. The regret is calculated by subtracting the reward obtained from the maximum possible reward (<code class="fm-code-in-text">max_reward</code>). Here, we consider the maximum value of the mean values as the points of maximum probability in the payoff distributions of the three machines. The average regret is returned as output: <a id="idIndexMarker053"/><a id="idIndexMarker054"/><a id="marker-468"/></p>
  <pre class="programlisting">def explore_only():
    total_regret = 0
    for _ in range(num_trials):
        selected_machine = np.random.randint(K)   <span class="fm-combinumeral">①</span>
        reward = sample_payoff(selected_machine)  <span class="fm-combinumeral">②</span>
        regret = max_reward - reward              <span class="fm-combinumeral">③</span>
        total_regret += regret                    <span class="fm-combinumeral">④</span>
    average_regret = total_regret / num_trials    <span class="fm-combinumeral">⑤</span>
    return average_regret</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Randomly select a slot machine.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Sample the payoff from the selected slot machine.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Calculatethe regret.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Calculate the total regret.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Calculate the average regret.</p>

  <p class="body">The second strategy is defined in the <code class="fm-code-in-text">exploit_only_greedy()</code> function. This function selects the slot machine with the highest mean payoff by finding the index of the maximum mean payoff from the list of payoff means (<code class="fm-code-in-text">payoff_params[i]["mean"]</code>) for each machine (<code class="fm-code-in-text">i</code>). The <code class="fm-code-in-text">np.argmax()</code> function returns the index of the maximum mean payoff, representing the machine that is believed to provide the highest expected reward:<a id="idIndexMarker055"/><a id="idIndexMarker056"/></p>
  <pre class="programlisting">def exploit_only_greedy():
    total_regret = 0
    for _ in range(num_trials):
        selected_machine = np.argmax([payoff_params[i]["mean"] for i in  
        range(K)])                                                    <span class="fm-combinumeral">①</span>
        reward = sample_payoff(selected_machine)                      <span class="fm-combinumeral">②</span>
        regret = max_reward – reward                                  <span class="fm-combinumeral">③</span>
        total_regret += regret                                        <span class="fm-combinumeral">④</span>
    average_regret = total_regret / num_trials                        <span class="fm-combinumeral">⑤</span>
    return average_regret</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Select the slot machine with the highest mean payoff for exploitation.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Sample the payoff from the selected slot machine.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Calculatethe regret.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Calculate the total regret.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Calculate the average regret.</p>

  <p class="body">The following <code class="fm-code-in-text">epsilon_greedy(epsilon)</code> function implements the epsilon-greedy strategy. This function checks if a randomly generated number between 0 and 1 is less than the epsilon value. If it is, the algorithm performs exploration by randomly selecting a slot machine for exploration. If this condition is not satisfied, the algorithm performs exploitation by selecting the slot machine with the highest mean payoff:<a id="idIndexMarker057"/></p>
  <pre class="programlisting">def epsilon_greedy(epsilon):
    total_regret = 0
    for _ in range(num_trials):
        if np.random.random() &lt; epsilon:
            selected_machine = np.random.randint(K)                <span class="fm-combinumeral">①</span>
        else:
            selected_machine = np.argmax([payoff_params[i]["mean"] 
            <span class="fm-code-continuation-arrow">➥</span> for i in range(K)])                                 <span class="fm-combinumeral">②</span>
        reward = sample_payoff(selected_machine)                   <span class="fm-combinumeral">③</span>
        regret = max_reward - reward
        total_regret += regret
    average_regret = total_regret / num_trials
    return average_regret</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Randomly select a slot machine for exploration.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Select the slot machine with the highest mean payoff for exploitation.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Sample the payoff from the selected slot machine.</p>

  <p class="body">The following <code class="fm-code-in-text">ucb(c)</code> function implements the upper confidence bound (UCB) strategy. This function starts by initializing an array to keep track of the number of plays for each slot machine. The function also initializes an array to accumulate the sum of rewards obtained from each slot machine and initializes a variable to accumulate the total regret. The code includes a loop that plays each slot machine once to gather initial rewards and update the counts and sum of rewards:<a id="idIndexMarker058"/><a id="marker-469"/></p>
  <pre class="programlisting">def ucb(c):
    num_plays = np.zeros(K)                                                       <span class="fm-combinumeral">①</span>
    sum_rewards = np.zeros(K)                                                     <span class="fm-combinumeral">②</span>
    total_regret = 0                                                              <span class="fm-combinumeral">③</span>
    
    for i in range(K):                                                            <span class="fm-combinumeral">④</span>
        reward = sample_payoff(i)
        num_plays[i] += 1
        sum_rewards[i] += reward
    
    for t in range(K, num_trials):                                                <span class="fm-combinumeral">⑤</span>
        ucb_values = sum_rewards / num_plays + c * np.sqrt(np.log(t) / num_plays) <span class="fm-combinumeral">⑥</span>
        selected_machine = np.argmax(ucb_values)
        reward = sample_payoff(selected_machine)
        num_plays[selected_machine] += 1
        sum_rewards[selected_machine] += reward
        optimal_reward = max_reward
        regret = optimal_reward - reward
        total_regret += regret
  
    average_regret = total_regret / num_trials
    return average_regret</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Initialize an array to keep track of the number of plays for each slot machine.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Initialize an array to accumulate the sum of rewards obtained from each slot machine.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Initialize the total regret of each slot machine.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Play each slot machine once to initialize.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Continue playing with the UCB strategy.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Calculate the UCB values.</p>

  <p class="body">The following code snippet is used to run the strategies, calculate average regrets, and print the results:</p>
  <pre class="programlisting">avg_regret_explore = explore_only()
avg_regret_exploit = exploit_only_greedy()
avg_regret_epsilon_greedy = epsilon_greedy(0.1)                    <span class="fm-combinumeral">①</span>
avg_regret_ucb = ucb(2)                                            <span class="fm-combinumeral">②</span>
  
  
print(f"Average Regret - Explore only Strategy: {round(avg_regret_ <span class="fm-combinumeral">③</span>
explore,4)}")                                                      <span class="fm-combinumeral">③</span>
print(f"Average Regret - Exploit only Greedy Strategy:
<span class="fm-code-continuation-arrow">➥</span>    {round(avg_regret_exploit,4)}")                              <span class="fm-combinumeral">③</span>
print(f"Average Regret - Epsilon-greedy Strategy:
<span class="fm-code-continuation-arrow">➥</span>    {round(avg_regret_epsilon_greedy,4)}")                       <span class="fm-combinumeral">③</span>
print(f"Average Regret - UCB Strategy: {round(avg_regret_ucb,4)}") <span class="fm-combinumeral">③</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Set the epsilon value for the epsilon-greedy strategy, and run the strategy.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Set the value of the exploration parameter c for the UCB strategy, and run the strategy.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Print the results</p>

  <p class="body">Given the random sampling included in the code, your results will vary, but running the code will produce results something like these:</p>
  <pre class="programlisting">Average Regret - Explore only Strategy: 2.246
Average Regret - Exploit only Greedy Strategy: 0.048
Average Regret - Epsilon-greedy Strategy: 0.3466
Average Regret - UCB Strategy: 0.0378</pre>

  <p class="body">MAB algorithms and concepts find applications in various real-world scenarios where decision-making under uncertainty and exploration–exploitation trade-offs are involved. Examples of real-world applications of MABs include, but are not limited to, resource allocation (dynamically allocating resources to different options to maximize performance), online advertising (dynamically allocating ad impressions to different options and learning which ads yield the highest click-through rate, which is the probability that a user clicks on an ad), design of experiments and clinical trials (optimizing the allocation of patients to different treatment options), content recommendation (personalizing content recommendations for users), and website optimization (optimizing different design options).</p>

  <p class="body">As you saw in figure 12.1, MABs can be classified into noncontextual and contextual MABs. In contrast to the previously explained noncontextual MABs, a contextual multi-armed bandit (CMAB) uses the contextual information contained in the environment. In CMAB, the learner repeatedly observes a context, selects an action, and receives feedback in the form of a reward or loss specific to the chosen action. CMAB algorithms use supplementary information, known as <i class="fm-italics">side information</i> or <i class="fm-italics">context</i>, to make informed decisions in real-world scenarios. For example, in a truck selection problem, the shared context is the type of delivery route (city or interstate). Section 12.5 shows how to use CMAB to solve this problem as an example of combinatorial actions.<a id="idIndexMarker059"/><a id="idIndexMarker060"/><a id="idIndexMarker061"/><a id="marker-470"/></p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title">Contextual multi-armed bandit applications</p>

    <p class="fm-sidebar-text">Contextual multi-armed bandits (CMABs) have found applications in areas like personalized recommendations and online advertising, where the context can be information about a user. For example, Amazon shows how to develop and deploy a CMAB workflow on SageMaker using a built-in Vowpal Wabbit (VW) container to train and deploy contextual bandit models (<a class="url" href="http://mng.bz/y8rE">http://mng.bz/y8rE</a>). These CMABs can be used to personalize content for a user, such as content layout, ads, search, product recommendations, etc. Moreover, a scalable algorithmic decision-making platform called WayLift was developed based on CMAB using VW for optimizing marketing decisions (<a class="url" href="http://mng.bz/MZom">http://mng.bz/MZom</a>).<a id="idIndexMarker062"/><a id="idIndexMarker063"/></p>
  </div>

  <h2 class="fm-head" id="heading_id_10">12.2 Optimization with reinforcement learning</h2>

  <p class="body">RL can be used for combinatorial optimization problems by framing the problem as a Markov decision process (MDP) and applying RL algorithms to find an optimal policy that leads to the best possible solution. In reinforcement learning, an agent learns to make sequential decisions in an environment to maximize a notion of cumulative reward. This process involves finding an optimal policy that maps states to actions in order to maximize the expected long-term reward.<a id="idIndexMarker064"/><a id="idIndexMarker065"/></p>

  <p class="body">The convergence of reinforcement learning and optimization has recently become an active area of research, drawing significant attention from the academic and industrial communities. Researchers are actively exploring ways to apply the strengths of RL to tackle complex optimization problems efficiently and effectively. For example, the generic end-to-end pipeline presented in section 11.7 can be used to tackle combinatorial optimization problems such as TSP, the vehicle routing problem (VRP), the satisfiability problem (SAT), maximum cut (MaxCut), maximal independent set (MIS), etc. This pipeline includes training the model with supervised or reinforcement learning. Listing 11.4 showed how to solve TSP with an ML model pretrained using a supervised or reinforcement learning approach, as described by Joshi, Laurent, and Bresson [3].<a id="idIndexMarker066"/><a id="idIndexMarker067"/><a id="idIndexMarker068"/><a id="idIndexMarker069"/></p>

  <p class="body">An end-to-end framework for solving the VRP using reinforcement learning is presented in a paper by Nazari et al. [4]. The capacitated vehicle routing problem (CVRP) was also handled by reinforcement learning in a paper by Delarue, Anderson, and Tjandraatmadja [5]. Another framework called RLOR is described in a paper by Wan, Li, and Wang[6] as a flexible framework of deep reinforcement learning for routing problems such as CVRP and TSP. A distributed model-free RL algorithm called DeepPool is described in a paper by Alabbasi, Ghosh, and Aggarwal [7] as learning optimal dispatch policies for ride-sharing applications by interacting with the environment. DeepFreight is another model-free RL algorithm for the freight delivery problem described in a paper by Jiayu et al. [8]. It decomposes the problem into two closely collaborative components: truck-dispatch and package-matching. The key objectives of the freight delivery system are to maximize the number of served requests within a certain time limit and to minimize the total fuel consumption of the fleet during this process. MOVI is another model-free approach for a large-scale taxi dispatch problem, described by Oda and Joe-Wong [9]. RL is also used to optimize traffic signal control (TSC) as a way to mitigate congestion. In a paper by Ruan et al. [10] (of which I was a co-author), a model of a real-world intersection with real traffic data collected in Hangzhou, China, is simulated with different RL-based traffic signal controllers. We also proposed a multi-agent reinforcement learning model to provide both macroscopic and microscopic control in mixed traffic scenarios [11]. The experimental results show that the proposed approach demonstrates superior performance compared with other baselines in terms of several metrics, such as throughput, average speed, and safety.<a id="idIndexMarker070"/><a id="idIndexMarker071"/><a id="idIndexMarker072"/><a id="idIndexMarker073"/><a id="idIndexMarker074"/><a id="marker-471"/></p>

  <p class="body">A framework for learning optimization algorithms, known as “Learning to Optimize,” is described by Li and Malik [12]. The problem was formulated as an RL problem, in which any optimization algorithm can be represented as a policy. Guided policy search is used, and autonomous optimizers are trained for different classes of convex and nonconvex objective functions. These autonomous optimizers converge faster or reach better optima than hand-engineered optimizers. This is somewhat similar to the amortized optimization concept described in section 11.6, but using reinforcement learning.</p>

  <p class="body">RL-based dispatching is described by Toll et al. for a four-elevator system in a 10-floor building [13]. The elevator dispatching problem is a combinatorial optimization problem that involves efficiently dispatching multiple elevators in a multi-floor building to serve incoming requests from passengers. As explained in section 1.4.2, the number of possible states in this case is <span class="times">10<sup class="fm-superscript">4</sup></span> (elevator positions) <span class="times">× 2<sup class="fm-superscript">40</sup></span> (elevator buttons) <span class="times">× 2<sup class="fm-superscript">18</sup></span> (hall call buttons) <span class="times">= 2.88 x 10<sup class="fm-superscript">21</sup></span> different states.</p>

  <p class="body">Stable-Baselines3 (SB3) provides reliable implementations of reinforcement learning algorithms in PyTorch for several OpenAI Gym-compatible and custom RL environments. To install Stable Baselines3 with pip, execute <code class="fm-code-in-text">pip install stable-baselines3</code>. Examples of RL algorithm implementations in SB3 include advantage actor-critic (A2C), soft actor-critic (SAC), deep deterministic policy gradient (DDPG), deep Q network (DQN), hindsight experience replay (HER), twin delayed DDPG (TD3), and proximal policy optimization (PPO). Environments and projects available in SB3 include the following:<a id="idIndexMarker075"/><a id="idIndexMarker076"/><a id="idIndexMarker077"/><a id="idIndexMarker078"/><a id="idIndexMarker079"/><a id="idIndexMarker080"/><a id="idIndexMarker081"/><a id="idIndexMarker082"/><a id="marker-472"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">mobile-env</i>—A Gymnasium environment for autonomous coordination in wireless mobile networks. It allows the simulation of various scenarios involving moving users in a cellular network with multiple base stations. This environment is used in section 12.4.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">gym-electric-motor</i>—An OpenAI Gym environment for the simulation and control of electric drivetrains. This environment is used in exercise 6 for this chapter (see appendix C).<a id="idIndexMarker083"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">highway-env</i>—An environment for decision-making in autonomous driving in different scenarios, such as highway, merge, roundabout, parking, intersection, and racetrack.<a id="idIndexMarker084"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Generalized state dependent exploration (gSDE) for deep reinforcement learning in robotics</i>—An exploration method to train RL agents directly on real robots.<a id="idIndexMarker085"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">RL Reach</i>—A platform for running reproducible RL experiments for customizable robotic reaching tasks.<a id="idIndexMarker086"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">RL Baselines3 Zoo</i>—A framework to train, evaluate RL agents, tune hyperparameters, plot results, and record videos.<a id="idIndexMarker087"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Furuta Pendulum Robot</i>—A project to build and train a rotary inverted pendulum, also known as a Furuta pendulum.<a id="idIndexMarker088"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">UAV_Navigation_DRL_AirSim</i>—A platform for training UAV navigation policies in complex unknown environments.<a id="idIndexMarker089"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">tactile-gym</i>—RL environments focused on using a simulated tactile sensor as the primary source of observations.<a id="idIndexMarker090"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">SUMO-RL</i>—An interface to instantiate RL environments with Simulation of Urban MObility (SUMO) for traffic signal control.<a id="idIndexMarker091"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">PyBullet Gym</i>—Environments for single and multi-agent reinforcement learning of quadcopter control.<a id="idIndexMarker092"/></p>
    </li>
  </ul>

  <p class="body">The following sections provide examples of how you can use RL methods to handle control problems with combinatorial actions.<a id="idIndexMarker093"/><a id="idIndexMarker094"/></p>

  <h2 class="fm-head" id="heading_id_11">12.3 Balancing CartPole using A2C and PPO</h2>

  <p class="body">Let’s consider a classic control task where the goal is to balance a pole on top of a cart by moving the cart left or right, as shown in figure 12.8. This task can be considered an optimization problem where the objective is to maximize the cumulative reward by finding an optimal policy that balances the pole on the cart for as long as possible. The agent needs to learn how to make decisions (take actions) that maximize the reward signal. The agent explores different actions in different states and learns which actions lead to higher rewards over time. By iteratively updating its policy based on observed rewards, the agent aims to optimize its decision-making process and find the best actions for each state.<a id="idIndexMarker095"/><a id="idIndexMarker096"/><a id="idIndexMarker097"/><a id="idIndexMarker098"/><a id="idIndexMarker099"/><a id="marker-473"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH12_F08_Khamis.png"/></p>

    <p class="figurecaption">Figure 12.8 CartPole balancing problem</p>
  </div>

  <p class="body">The state of this environment is described by four variables:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Cart position (continuous)—This represents the position of the cart along the <i class="fm-italics">x</i>-axis. The value ranges from –4.8 to 4.8.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Cart velocity (continuous)—This represents the velocity of the cart along the <i class="fm-italics">x</i>-axis. The value ranges from –inf to inf.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Pole angle (continuous)—This represents the angle of the pole from the vertical position. The value ranges from –0.418 to 0.418 radians or –23.95° to 23.95° degrees.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Pole angular velocity (continuous)—This represents the angular velocity of the pole. The value ranges from –inf to inf.</p>
    </li>
  </ul>

  <p class="body">The action space in the CartPole environment is discrete and consists of two possible actions:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Action 0—Move the cart to the left.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Action 1—Move the cart to the right.</p>
    </li>
  </ul>

  <p class="body">The agent receives a reward of +1 for every time step when the pole remains upright. The episode ends if one of the following conditions is met:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">The pole angle is more than ±12 degrees from the vertical.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The cart position is more than ±2.4 units from the center.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The episode reaches a maximum time step limit (typically 200 steps).</p>
    </li>
  </ul>

  <p class="body">In the CartPole environment, the objective is to balance the pole on the cart for as long as possible, maximizing the cumulative reward. Let’s look at the code for learning the optimal policy to balance the CartPole using the advantage actor-critic (A2C) algorithm discussed in section 12.1.4.</p>

  <p class="body">As shown in listing 12.2, we start by importing the necessary libraries:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">gym</code> is the OpenAI Gym library, used for working with reinforcement learning environments. <a id="idIndexMarker100"/><a id="marker-474"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">torch</code> is the PyTorch library used for building and training neural networks. <a id="idIndexMarker101"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">torch.nn</code> is a module providing the tools for defining neural networks. <a id="idIndexMarker102"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">torch.nn.functional</code> contains various activation and loss functions. <a id="idIndexMarker103"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">torch.optim</code> contains optimization algorithms for training neural networks. <a id="idIndexMarker104"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">tqdm</code> provides a progress bar for tracking the training progress.<a id="idIndexMarker105"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">seaborn</code> is used for visualization.<a id="idIndexMarker106"/></p>
    </li>
  </ul>

  <p class="fm-code-listing-caption">Listing 12.2 Balancing CartPole using the A2C algorithm</p>
  <pre class="programlisting">import gym
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import matplotlib.pyplot as plt
from tqdm import tqdm
import pandas as pd
import seaborn as sns</pre>

  <p class="body">Next, we create the actor and critic networks using PyTorch. The <code class="fm-code-in-text">Actor</code> class is a subclass of <code class="fm-code-in-text">nn.Module</code> in PyTorch, representing the policy network, and the <code class="fm-code-in-text">__init__</code> method defines the architecture of the actor network. Three fully connected layers (<code class="fm-code-in-text">fc1</code>, <code class="fm-code-in-text">fc2</code>, <code class="fm-code-in-text">fc3</code>) are used. The <code class="fm-code-in-text">forward</code> method performs a forward pass through the network, applying activation functions (ReLU) and returning the action probabilities using the softmax function:<a id="idIndexMarker107"/><a id="idIndexMarker108"/><a id="idIndexMarker109"/><a id="idIndexMarker110"/></p>
  <pre class="programlisting">class Actor(nn.Module):
  
    def __init__(self, state_dim,  action_dim):
        super(Actor, self).__init__()
        self.fc1 = nn.Linear(state_dim, 64)
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, action_dim)
  
    def forward(self, state):
        x1 = F.relu(self.fc1(state))
        x2 = F.relu(self.fc2(x1))
        action_probs = F.softmax(self.fc3(x2), dim=-1)
        return action_probs</pre>

  <p class="body">The <code class="fm-code-in-text">Critic</code> class is also a subclass of <code class="fm-code-in-text">nn.Module</code>, representing the value network. The <code class="fm-code-in-text">__init__</code> method defines the architecture of the critic network, similar to the actor network. The forward method performs a forward pass through the network, applying activation functions (ReLU) and returning the predicted value:<a id="idIndexMarker111"/><a id="idIndexMarker112"/><a id="idIndexMarker113"/><a id="marker-475"/></p>
  <pre class="programlisting">class Critic(nn.Module):
  
    def __init__(self, state_dim):
        super(Critic, self).__init__()
        self.fc1 = nn.Linear(state_dim, 64)
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 1)
  
    def forward(self, state):
        x1 = F.relu(self.fc1(state))
        x2 = F.relu(self.fc2(x1))
        value = self.fc3(x2)
        return value</pre>

  <p class="body">As a continuation, the following code snippet is used to create an instance of the CartPole environment using the OpenAI Gym library and to retrieve important information about the environment:</p>
  <pre class="programlisting">env = gym.make("CartPole-v1")                               <span class="fm-combinumeral">①</span>
env.seed(0)                                                 <span class="fm-combinumeral">②</span>
  
state_dim = env.observation_space.shape[0]                  <span class="fm-combinumeral">③</span>
n_actions = env.action_space.n                              <span class="fm-combinumeral">④</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Create the CartPole environment.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Set the random seed to help make the environment’s behavior reproducible.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Retrieve the dimensionality of the observation space.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Retrieve the number of actions in the action space.</p>

  <p class="body"><code class="fm-code-in-text">state_dim</code> represents the state space or the observation space and in this example has a value of 4 (the four states being cart position, cart velocity, pole angle, and pole angular velocity). <code class="fm-code-in-text">n_actions</code> represents the dimensionality of the action space or the number of actions, which in this example is 2 (push left and push right). We can now initialize the actor and critic models as well as the Adam optimizer for the actor and critic models using learning rate <code class="fm-code-in-text">lr=1e-3</code>. The discount factor, gamma, determines the importance of future rewards compared to immediate rewards in reinforcement learning algorithms:<a id="idIndexMarker114"/><a id="idIndexMarker115"/><a id="idIndexMarker116"/></p>
  <pre class="programlisting">actor = Actor(state_dim, n_actions)                          <span class="fm-combinumeral">①</span>
critic = Critic(state_dim)                                   <span class="fm-combinumeral">②</span>
adam_actor = torch.optim.Adam(actor.parameters(), lr=1e-3)   <span class="fm-combinumeral">③</span>
adam_critic = torch.optim.Adam(critic.parameters(), lr=1e-3) <span class="fm-combinumeral">④</span>
gamma = 0.99                                                 <span class="fm-combinumeral">⑤</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Create an instance of the Actor class.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Create an instance of the Critic class.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Initialize the Adam optimizer for the actor model.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Initialize the Adam optimizer for the critic model.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Set the discount rate.</p>

  <p class="body">After this initialization, we can start the training process, where an agent interacts with the environment for a specified number of episodes. During the training, the agent computes the advantage function, updates the actor and critic models, and keeps track of the training statistics. The following code snippet is used to initialize the training process:</p>
  <pre class="programlisting">num_episodes=500                                                    <span class="fm-combinumeral">①</span>
episode_rewards = []                                                <span class="fm-combinumeral">②</span>
stats={'actor loss':[], 'critic loss':[], 'return':[]}              <span class="fm-combinumeral">③</span>
pbar = tqdm(total=num_episodes, ncols=80, bar_format='{l_bar}{bar}| {n_fmt}/ 
<span class="fm-code-continuation-arrow">➥</span> {total_fmt}')                                                    <span class="fm-combinumeral">④</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Set the total number of episodes to run.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Create an empty list to store the total rewards obtained in each episode.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Create a dictionary to store the training statistics, including the actor loss, critic loss, and total return for each episode.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Initialize the tqdm progress bar.</p>

  <p class="body"><a id="marker-476"/>The training loop iterates over the specified number of episodes. In each episode, the environment is reset to its initial state, and the random seeds are initialized to ensure that the sequence of random numbers generated by the environment remains consistent across different runs of the code:</p>
  <pre class="programlisting">for episode in range(num_episodes):
    done = False
    total_reward = 0
    state = env.reset()
    env.seed(0)</pre>

  <p class="body">The agent then interacts with the environment by taking actions based on its policy, accumulating rewards, and updating its parameters to improve its performance in balancing the pole on the cart until the episode is complete. The actor network is used to determine action probabilities given the current state, and a categorical distribution is created using these probabilities. An action is then stochastically sampled from this distribution and executed in the environment. The resulting next state, reward, done flag, and additional information are received from the environment, completing one step of the agent-environment interaction loop:</p>
  <pre class="programlisting">while not done:        
    probs = actor(torch.from_numpy(state).float())                          <span class="fm-combinumeral">①</span>
    dist = torch.distributions.Categorical(probs=probs)                     <span class="fm-combinumeral">②</span>
    action = dist.sample()                                                  <span class="fm-combinumeral">③</span>
  
    next_state, reward, done, info = env.step(action.detach().data.numpy()) <span class="fm-combinumeral">④</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Obtain action probabilities given the current state.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Create a categorical distribution.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Sample an action from the categorical distribution.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Pass the action to the environment.</p>

  <p class="body">The advantage function is then computed using the rewards, next state value, and current state value.</p>
  <pre class="programlisting">advantage = reward + (1-
<span class="fm-code-continuation-arrow">➥</span> done)*gamma*critic(torch.from_numpy(next_state).float()) - 
<span class="fm-code-continuation-arrow">➥</span>    critic(torch.from_numpy(state).float())       <span class="fm-combinumeral">①</span>
  
total_reward += reward                              <span class="fm-combinumeral">②</span>
state = next_state                                  <span class="fm-combinumeral">③</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Compute the advantage function.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Update the total reward.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Move to the next state.</p>

  <p class="body">This advantage function is used to update the critic model. The critic loss is calculated as the mean squared error of the advantage. The critic parameters are updated using the Adam optimizer:</p>
  <pre class="programlisting">critic_loss = advantage.pow(2).mean()
adam_critic.zero_grad()
critic_loss.backward()
adam_critic.step()</pre>

  <p class="body">The actor loss is calculated as the negative log probability of the chosen action multiplied by the advantage. The actor parameters are updated using the Adam optimizer:</p>
  <pre class="programlisting">actor_loss = -dist.log_prob(action)*advantage.detach()
adam_actor.zero_grad()
actor_loss.backward()
adam_actor.step()</pre>

  <p class="body">The total reward for the episode is then appended to the <code class="fm-code-in-text">episode_rewards</code> list:<a id="idIndexMarker117"/></p>
  <pre class="programlisting">episode_rewards.append(total_reward)</pre>

  <p class="body">The actor loss, critic loss, and total reward for the episode are added to the <code class="fm-code-in-text">stats</code> dictionary. The statistics are printed for each episode: <a id="idIndexMarker118"/></p>
  <pre class="programlisting">    stats['actor loss'].append(actor_loss)
    stats['critic loss'].append(critic_loss)
    stats['return'].append(total_reward)
    print('Actor loss= ', round(stats['actor loss'][episode].item(), 4), 'Critic 
       <span class="fm-code-continuation-arrow">➥</span> loss= ', round(stats['critic loss'][episode].item(), 4), 'Return= ', 
       <span class="fm-code-continuation-arrow">➥</span> stats['return'][episode])                           <span class="fm-combinumeral">①</span>
    pbar.set_description(f"Episode {episode + 1}")            <span class="fm-combinumeral">②</span>
    pbar.set_postfix({"Reward": episode_rewards})             <span class="fm-combinumeral">②</span>
    pbar.update(1)                                            <span class="fm-combinumeral">②</span>
pbar.close()                                                  <span class="fm-combinumeral">③</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Print the tracking statistics.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Update the tqdm progress bar.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Close the tqdm progress bar.</p>

  <p class="body"><a id="marker-477"/>Let’s now visualize the learning process by plotting the episode rewards obtained during each training episode using a scatter plot with a trend line:</p>
  <pre class="programlisting">data = pd.DataFrame({"Episode": range(1, num_episodes + 1), "Reward": 
<span class="fm-code-continuation-arrow">➥</span>    episode_rewards})                                                     <span class="fm-combinumeral">①</span>
  
plt.figure(figsize=(12,6))                                                  <span class="fm-combinumeral">②</span>
sns.set(style="whitegrid")                                                  <span class="fm-combinumeral">③</span>
sns.regplot(data=data, x="Episode", y="Reward", scatter_kws={"alpha": 0.5}) <span class="fm-combinumeral">④</span>
plt.xlabel("Episode")
plt.ylabel("Reward")
plt.title("Episode Rewards with Trend Line")
plt.show()</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Create a DataFrame for episode rewards.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Set the size of the figure to be displayed.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Set the style of the seaborn plots to have a white grid background.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Create the scatter plot with a trend line.</p>

  <p class="body">When you run this code, you’ll get a scatter plot and trend line something like the one in figure 12.9.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH12_F09_Khamis.jpg"/></p>

    <p class="figurecaption">Figure 12.9 Episode rewards with a trend line</p>
  </div>

  <p class="body">As you can see, there are fluctuations in the rewards during the learning process, as depicted by the scatter plot showing increasing and decreasing rewards in different episodes. However, the overall trend or pattern in the rewards over the episodes is improving. Fluctuations in the rewards during the learning process are expected and considered normal behavior in reinforcement learning. Initially, the agent may explore different actions, which can lead to both successful and unsuccessful episodes, resulting in varying rewards. As the training progresses, the agent refines its policy and tends to exploit more promising actions, leading to more consistent rewards.</p>

  <p class="body"><a id="marker-478"/>Stable Baselines3 (SB3) provides more abstract and reliable implementations of different reinforcement learning algorithms based on PyTorch. As a continuation of listing 12.2, the following code snippet shows the steps for handling the CartPole environment using the A2C implementation in SB3. The A2C agent uses <span class="fm-code-in-text">MlpPolicy</span> as a specific type of policy network, which in turn uses a multilayer perceptron (MLP) architecture. The created agent will interact with the environment for a total of 10,000 timesteps to learn the optimal policy:<a id="idIndexMarker119"/><a id="idIndexMarker120"/><a id="idIndexMarker121"/><a id="idIndexMarker122"/></p>
  <pre class="programlisting">import gymnasium as gym                                              <span class="fm-combinumeral">①</span>
from stable_baselines3 import A2C                                    <span class="fm-combinumeral">②</span>
from stable_baselines3.common.evaluation import evaluate_policy      <span class="fm-combinumeral">③</span>
  
env = gym.make("CartPole-v1", render_mode="rgb_array")               <span class="fm-combinumeral">④</span>
  
model = A2C("MlpPolicy", env, verbose=1)                             <span class="fm-combinumeral">⑤</span>
  
model.learn(total_timesteps=10000, progress_bar=True)                <span class="fm-combinumeral">⑥</span>
  
mean_reward, std_reward = evaluate_policy(model, model.get_env(),
<span class="fm-code-continuation-arrow">➥</span>    n_eval_episodes=10)                                            <span class="fm-combinumeral">⑦</span>
vec_env = model.get_env()                                            <span class="fm-combinumeral">⑦</span>
obs = vec_env.reset()                                                <span class="fm-combinumeral">⑦</span>
for i in range(1000):                                                <span class="fm-combinumeral">⑦</span>
    action, _states = model.predict(obs, deterministic=True)         <span class="fm-combinumeral">⑦</span>
    obs, rewards, dones, info = vec_env.step(action)                 <span class="fm-combinumeral">⑦</span>
    vec_env.render("human")                                          <span class="fm-combinumeral">⑧</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Import the gym module.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Import the A2C model</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> To evaluate the performance of the trained model</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Create the CartPole-v1 environment.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Create an A2C model with the MlpPolicy (multilayer perceptron) and the environment.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Start the learning process for the model.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Test and evaluate the trained model.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Render the environment in a way that a human can visualize it.</p>

  <p class="body">This code snippet sets up the environment, trains an A2C model, evaluates its performance for 1,000 steps, and then visualizes the model’s actions in the environment.</p>

  <p class="body"><a id="marker-479"/>We can use PPO instead of A2C to balance the CartPole. The next listing is quite similar to the previous one, but it uses PPO instead of A2C.<a id="idIndexMarker123"/></p>

  <p class="fm-code-listing-caption">Listing 12.3 Balancing CartPole using the PPO algorithm</p>
  <pre class="programlisting">import gymnasium as gym
from stable_baselines3 import PPO                                  <span class="fm-combinumeral">①</span>
from stable_baselines3.common.evaluation import evaluate_policy
  
env = gym.make("CartPole-v1", render_mode="rgb_array")             <span class="fm-combinumeral">②</span>
  
model = PPO("MlpPolicy", env, verbose=1)                           <span class="fm-combinumeral">③</span>
  
model.learn(total_timesteps=10000, progress_bar=True)              <span class="fm-combinumeral">④</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Import the PPO model from SB3.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Create an instance of the CartPole-v1 environment.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Initialize a PPO model with the MlpPolicy to handle agent’s networks.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Start the learning process for 10,000 timesteps.</p>

  <p class="body">During the training, the code renders logger output in the following format:</p>
  <pre class="programlisting">------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 61.8         |
|    ep_rew_mean          | 61.8         |
| time/                   |              |
|    fps                  | 362          |
|    iterations           | 5            |
|    time_elapsed         | 28           |
|    total_timesteps      | 10240        |
| train/                  |              |
|    approx_kl            | 0.0064375857 |
|    clip_fraction        | 0.051        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.61        |
|    explained_variance   | 0.245        |
|    learning_rate        | 0.0003       |
|    loss                 | 26.1         |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.0141      |
|    value_loss           | 65.2         |
------------------------------------------</pre>

  <p class="body">The logger output presents the following information:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">rollout/</code></p>

      <ul class="calibre9">
        <li class="fm-list-bullet">
          <p class="list"><code class="fm-code-in-text">ep_len_mean</code>—The mean episode length during rollouts</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list"><code class="fm-code-in-text">ep_rew_mean</code>—The mean episodic training reward during rollouts</p>
        </li>
      </ul>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">time/</code></p>

      <ul class="calibre9">
        <li class="fm-list-bullet">
          <p class="list"><code class="fm-code-in-text">fps</code>—The number of frames per seconds achieved during training, indicating the computational efficiency of the algorithm</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list"><code class="fm-code-in-text">iterations</code>—the number of completed training iterations</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list"><code class="fm-code-in-text">time_elapsed</code>—The time elapsed in seconds since the beginning of training</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list"><code class="fm-code-in-text">total_timesteps</code>—The total number of timesteps (steps in the environments) the agent has experienced during training</p>
        </li>
      </ul>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">train/</code><a id="marker-480"/></p>

      <ul class="calibre9">
        <li class="fm-list-bullet">
          <p class="list"><code class="fm-code-in-text">approx_kl</code>—The approximate Kullback-Leibler (KL) divergence between the old and new policy distributions, measuring the extent of policy changes during training</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list"><code class="fm-code-in-text">clip_fraction</code>—The mean fraction of surrogate loss that was clipped (above the <code class="fm-code-in-text">clip_range</code> threshold) for PPO.</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list"><code class="fm-code-in-text">clip_range</code>—The current value of the clipping factor for the surrogate loss of PPO</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list"><code class="fm-code-in-text">entropy_loss</code>—The mean value of the entropy loss (negative of the average policy entropy)</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list"><code class="fm-code-in-text">explained_variance</code>—The fraction of the return variance explained by the value function</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list"><code class="fm-code-in-text">learning_rate</code>—The current learning rate value</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list"><code class="fm-code-in-text">loss</code>—The current total loss value</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list"><code class="fm-code-in-text">n_updates</code>—The number of gradient updates applied so far</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list"><code class="fm-code-in-text">policy_gradient_loss</code>—The current value of the policy gradient loss</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list"><code class="fm-code-in-text">value_loss</code>—The current value for the value function loss for on-policy algorithms</p>
        </li>
      </ul>
    </li>
  </ul>

  <p class="body">We usually keep an eye on the reward and loss values. As a continuation, the following code snippet shows how to evaluate the policy and render the environment state:</p>
  <pre class="programlisting">mean_reward, std_reward = evaluate_policy(model, model.get_env(), 
<span class="fm-code-continuation-arrow">➥</span>    n_eval_episodes=10)                                         <span class="fm-combinumeral">①</span>
vec_env = model.get_env()                                         <span class="fm-combinumeral">②</span>
obs = vec_env.reset()                                             <span class="fm-combinumeral">③</span>
for i in range(1000):                                             <span class="fm-combinumeral">④</span>
    action, _states = model.predict(obs, deterministic=True) 
    obs, rewards, dones, info = vec_env.step(action) 
    vec_env.render("human")</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Evaluate the policy of the trained model.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Retrieve the vectorized environment associated with the model.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Reset the environment to its initial state and get the initial observation.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Test the trained agent.</p>

  <p class="body">As shown in the preceding code, after training the model, we evaluate the policy of the trained model over 10 episodes and return the mean and standard deviation of the rewards. We then allow the agent to interact with the environment for 1,000 steps and render the environment. The output will be an animated version of figure 12.8 showing the behavior of the CartPole learning the balancing policy.</p>

  <h2 class="fm-head" id="heading_id_12">12.4 Autonomous coordination in mobile networks using PPO</h2>

  <p class="body"><a id="marker-481"/>Schneider et al. described the mobile-env environment as an open platform for reinforcement learning in wireless mobile networks [14]. This environment enables the representation of users moving within a designated area and potentially connecting to one or multiple base stations. It supports both multi-agent and centralized reinforcement learning policies.<a id="idIndexMarker124"/><a id="idIndexMarker125"/><a id="idIndexMarker126"/><a id="idIndexMarker127"/></p>

  <p class="body">In the mobile-env environment, we have a mobile network formed by a number of base stations or cells (BSs) and user equipment (UE), as illustrated in figure 12.10. Our objective is to decide what connections should be established among the UEs and BSs in order to maximize the quality of experience (QoE) globally. For individual UEs, a higher QoE is achieved by establishing connections with as many BSs as possible, resulting in higher data rates. However, since BSs distribute resources among connected UEs (e.g., scheduling physical resource blocks), UEs end up competing for limited resources, leading to conflicting goals.<a id="idIndexMarker128"/><a id="idIndexMarker129"/><a id="idIndexMarker130"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH12_F10_Khamis.jpg"/></p>

    <p class="figurecaption">Figure 12.10 The mobile-env environment with a number of base stations and user equipment</p>
  </div>

  <p class="body">To achieve maximum QoE globally, the policy must consider two crucial factors:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">The data rate (DR in GB/s) of each connection is determined by the channel’s quality (e.g., the signal-to-noise ratio) between the UE and BS.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The QoE of individual UEs does not necessarily increase linearly with higher data rates.</p>
    </li>
  </ul>

  <p class="body">Let’s create a mobile network environment and train a PPO agent to learn the coordination policy. We’ll start in listing 12.4 by importing <code class="fm-code-in-text">gymnasium</code>, which is a maintained fork of OpenAI’s Gym library. <code class="fm-code-in-text">mobile_env</code> is imported to create an environment related to mobile networks. <code class="fm-code-in-text">IPython.display</code> enables the use of interactive display features within IPython or Jupyter Notebook environments. We’ll create a small instance of <code class="fm-code-in-text">mobile_env</code> that contains three base stations and five users.<a id="idIndexMarker131"/><a id="idIndexMarker132"/><a id="marker-482"/><a id="idIndexMarker133"/><a id="idIndexMarker134"/></p>

  <p class="fm-code-listing-caption">Listing 12.4 Mobile network coordination using PPO</p>
  <pre class="programlisting">import gymnasium
import matplotlib.pyplot as plt
import mobile_env
from IPython import display
  
from stable_baselines3 import PPO
from stable_baselines3.ppo import MlpPolicy
  
env = gymnasium.make("mobile-small-central-v0", render_mode="rgb_array")     <span class="fm-combinumeral">①</span>
print(f"\nSmall environment with {env.NUM_USERS} users and {env.NUM_STATIONS}
<span class="fm-code-continuation-arrow">➥</span>    cells.")                                                               <span class="fm-combinumeral">②</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Create an instance of the environment.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Print the number of users and the number of base stations.</p>

  <p class="body">Next, we’ll create an instance of the PPO agent using the multilayer perceptron (MLP) policy (<code class="fm-code-in-text">MlpPolicy</code>) with the mobile environment (<code class="fm-code-in-text">env</code>) as the environment. The training progress will be logged to the <code class="fm-code-in-text">results_sb</code> directory for TensorBoard visualization. You can install and set up <code class="fm-code-in-text">tensorboard logdir</code> as follows:<a id="idIndexMarker135"/><a id="idIndexMarker136"/><a id="idIndexMarker137"/><a id="idIndexMarker138"/></p>
  <pre class="programlisting">pip install tensorboard
tensorboard --logdir .</pre>

  <p class="body">The agent is trained for a total of 30,000 time steps:</p>
  <pre class="programlisting">model = PPO(MlpPolicy, env, tensorboard_log='results_sb', verbose=1)
model.learn(total_timesteps=30000, progress_bar=True)</pre>

  <p class="body">The following code snippet can be used to render the environment state after training. During the episode, the trained model is used to predict the action to be taken based on the current observation. The action is then executed in the environment, and the environment responds with the next observation (<code class="fm-code-in-text">obs</code>), the reward received (<code class="fm-code-in-text">reward</code>), a Boolean flag indicating whether the episode is terminated (<code class="fm-code-in-text">terminated</code>), a flag indicating whether the episode was terminated due to the episode time limit (<code class="fm-code-in-text">truncated</code>), and environment information (<code class="fm-code-in-text">info</code>):</p>
  <pre class="programlisting">obs, info = env.reset()                                         <span class="fm-combinumeral">①</span>
done = False                                                    <span class="fm-combinumeral">②</span>
  
while not done:
    action, _ = model.predict(obs)                              <span class="fm-combinumeral">③</span>
    obs, reward, terminated, truncated, info = env.step(action) <span class="fm-combinumeral">④</span>
    done = terminated or truncated                              <span class="fm-combinumeral">⑤</span>
  
    plt.imshow(env.render())                                    <span class="fm-combinumeral">⑥</span>
    display.display(plt.gcf())                                  <span class="fm-combinumeral">⑥</span>
    display.clear_output(wait=True)                             <span class="fm-combinumeral">⑥</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Reset the environment, returning the initial observation and environment information.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Flag to track if the episode is complete.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Predict the action to be taken based on the current observation.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Execute the action and get the environment response.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Update the flag</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Render the environment state.</p>

  <p class="body">The output is an updated version of figure 12.10 showing three stations, five users, and the dynamically changing connections between them. The average data rate and average utility of the established connections are also rendered.</p>

  <p class="body">Instead of having a single RL agent centrally control cell selection for all users, an alternative approach is to adopt a multi-agent RL. In this setup, multiple RL agents work in parallel, with each agent being responsible for the cell selection of a specific user. For instance, in the mobile-small-ma-v0 environment, we will use five RL agents, each catering to the cell selection needs of a single user. This approach allows for more distributed and decentralized control, enhancing the scalability and efficiency of the system. We’ll use Ray and Ray RLlib in this example. Ray is an open source unified framework for scaling AI and Python applications like machine learning. Ray RLlib is an open source library for RL, offering support for production-level, highly distributed RL workloads while maintaining unified and simple APIs for a large variety of industry applications. To install RLlib with pip, execute <code class="fm-code-in-text">pip install -U "ray[rllib]"</code>.<a id="marker-483"/><a id="idIndexMarker139"/></p>

  <p class="body">As a continuation of listing 12.4, we can import the Ray libraries to learn the optimal coordination policy following a multi-agent approach:</p>
  <pre class="programlisting">import ray
from ray.tune.registry import register_env
import ray.air
from ray.rllib.algorithms.ppo import PPOConfig
from ray.rllib.policy.policy import PolicySpec
from ray.tune.stopper import MaximumIterationStopper
from ray.rllib.algorithms.algorithm import Algorithm
from mobile_env.wrappers.multi_agent import RLlibMAWrapper</pre>

  <p class="body">The following function will create and return a wrapped environment suitable for RLlib’s multi-agent setup. Here we create a small instance of <code class="fm-code-in-text">mobile_env</code>:</p>
  <pre class="programlisting">def register(config):  
    env = gymnasium.make("mobile-small-ma-v0")
    return RLlibMAWrapper(env)</pre>

  <p class="body">We’ll now initialize Ray using the following function.</p>
  <pre class="programlisting">ray.init(
  num_cpus=2,                <span class="fm-combinumeral">①</span>
  include_dashboard=False,   <span class="fm-combinumeral">②</span>
  ignore_reinit_error=True,  <span class="fm-combinumeral">③</span>
  log_to_driver=False,       <span class="fm-combinumeral">④</span>
)</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Specify the number of CPUs.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Disable the Ray web-based dashboard.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Ignore the reinitialization error if Ray is already initialized.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Ignore the forwarding logs.</p>

  <p class="body">We can now configure an RLlib training setup to use the proximal policy optimization (PPO) algorithm on <code class="fm-code-in-text">mobile-env</code>’s small scenario in a multi-agent environment:<a id="marker-484"/><a id="idIndexMarker140"/></p>
  <pre class="programlisting">config = (
    PPOConfig()
    .environment(env="mobile-small-ma-v0")                  <span class="fm-combinumeral">①</span>
    .multi_agent(
        policies={"shared_policy": PolicySpec()},
        policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: 
       <span class="fm-code-continuation-arrow">➥</span> "shared_policy",
    )                                                       <span class="fm-combinumeral">②</span>
  
    .resources(num_cpus_per_worker=1)                       <span class="fm-combinumeral">③</span>
    .rollouts(num_rollout_workers=1)                        <span class="fm-combinumeral">④</span>
)</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Set the environment.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Configure all agents to share the same policy.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Specify that each worker should use one CPU core.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Indicate that there should be one worker dedicated to performing rollouts.</p>

  <p class="body">The following code snippet configures and initiates a training session using the RLlib framework. It sets up a tuner (trainer) for the PPO algorithm and executes the training:</p>
  <pre class="programlisting">tuner = ray.tune.Tuner(
    "PPO",                                                                  <span class="fm-combinumeral">①</span>
    run_config=ray.air.RunConfig(
        storage_path="./results_rllib",                                     <span class="fm-combinumeral">②</span>
        stop=MaximumIterationStopper(max_iter=10),                          <span class="fm-combinumeral">③</span>
        checkpoint_config=ray.air.CheckpointConfig(checkpoint_at_end=True), <span class="fm-combinumeral">④</span>
    ),
    param_space=config,                                                     <span class="fm-combinumeral">⑤</span>
)
  
result_grid = tuner.fit()                                                   <span class="fm-combinumeral">⑥</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Specify PPO</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Specify where the training results and checkpoints (saved model states) will be stored.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Define the stopping condition for the training.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Configure how checkpoints are saved.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Specify the training parameters.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Start the training process.</p>

  <p class="body">After training, we can load the best trained agent from the results:</p>
  <pre class="programlisting">best_result = result_grid.get_best_result(metric="episode_reward_mean", 
<span class="fm-code-continuation-arrow">➥</span> mode="max")                                           <span class="fm-combinumeral">①</span>
ppo = Algorithm.from_checkpoint(best_result.checkpoint)  <span class="fm-combinumeral">②</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Extract the best training result from the result_grid based on the metric of average episode reward.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Load the agent from the best checkpoint (model state) obtained in the training.</p>

  <p class="body">Lastly, we can evaluate a trained model on a given environment and render the results:</p>
  <pre class="programlisting">env = gymnasium.make("mobile-small-ma-v0", render_mode="rgb_array")  <span class="fm-combinumeral">①</span>
obs, info = env.reset()                                              <span class="fm-combinumeral">②</span>
done = False
  
while not done:                                                      <span class="fm-combinumeral">③</span>
    action = {}                                                      <span class="fm-combinumeral">④</span>
    for agent_id, agent_obs in obs.items():                          <span class="fm-combinumeral">⑤</span>
        action[agent_id] = ppo.compute_single_action(agent_obs,
            <span class="fm-code-continuation-arrow">➥</span> policy_id="shared_policy")                            <span class="fm-combinumeral">⑤</span>
  
    obs, reward, terminated, truncated, info = env.step(action)      <span class="fm-combinumeral">⑥</span>
    done = terminated or truncated                                   <span class="fm-combinumeral">⑦</span>
  
    plt.imshow(env.render())                                         <span class="fm-combinumeral">⑧</span>
    display.display(plt.gcf())                                       <span class="fm-combinumeral">⑧</span>
    display.clear_output(wait=True)                                  <span class="fm-combinumeral">⑧</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Initialize the environment.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Reset the environment to its initial state and fetch the initial observation and additional info.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Initiate a loop to run one episode with the trained model.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Initialize an empty dictionary to hold actions for each agent in the multi-agent environment.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Iterate through each agent’s observations.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Return the new observations, rewards, termination flags, truncation flags, and additional information.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Determine if the episode has ended. An episode ends if it is terminated or if truncated is True.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Visualize the current state of the environment.</p>

  <p class="body">Running this code produces an animated rendering of the environment shown in figure 12.11.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH12_F11_Khamis.jpg"/></p>

    <p class="figurecaption">Figure 12.11 Coordination of the mobile environment using a multi-agent PPO</p>
  </div>

  <p class="body"><a id="marker-485"/>This rendering shows the established connection with the five user units and three base stations and the obtained average data rates and utilities. For more information about the decentralized multi-agent version of the PPO-based coordinator, see Schneider et al.’s article “mobile-env: An open platform for reinforcement learning in wireless mobile networks” and the associated GitHub repo [14].<a id="idIndexMarker141"/><a id="idIndexMarker142"/><a id="idIndexMarker143"/><a id="idIndexMarker144"/></p>

  <h2 class="fm-head" id="heading_id_13">12.5 Solving the truck selection problem using contextual bandits</h2>

  <p class="body">Let’s consider a scenario where a delivery service provider is planning to assign trucks for different delivery routes, as illustrated in figure 12.12. The goal is to maximize the efficiency of the fleet according to the type of delivery route. The delivery routes are categorized as city deliveries or interstate deliveries, and the company has to select the optimal type of truck according to the following decision variables: size, engine type, and tire type. The available options for each decision variable are as follows:<a id="idIndexMarker145"/><a id="idIndexMarker146"/><a id="idIndexMarker147"/><a id="idIndexMarker148"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Size—Small, medium, or large</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Engine type—Petrol, diesel, or electric</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Tire type—All-season, snow, or off-road</p>
    </li>
  </ul>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH12_F12_Khamis.png"/></p>

    <p class="figurecaption">Figure 12.12 A CMAB-based recommender system to select delivery truck size, engine type, and tire type based on a specified context</p>
  </div>

  <p class="body">The reward is based on how suitable the truck selection is for a given delivery route. The reward function receives as arguments the delivery route type and the selected actions for each variable. In order to reflect real-world conditions and add complexity to the problem, we add noise to the reward value, representing uncertainties in weather, road conditions, and so on. The objective is to select the best combination from the available choices in such a way as to maximize the total reward. This means choosing the most suitable truck size, engine type, and tire type for each type of delivery route.<a id="marker-486"/></p>

  <p class="body">A contextual multi-armed bandit (CMAB) can be used to handle this problem. Vowpal Wabbit (VW), an open source ML library developed originally at Yahoo and currently at Microsoft, supports a wide range of machine-learning algorithms, including CMAB. You can install VW using <code class="fm-code-in-text">pip install vowpalwabbit</code>. <a id="idIndexMarker149"/><a id="idIndexMarker150"/><a id="idIndexMarker151"/></p>

  <p class="body">We’ll use CMAB to find the optimal values of the decision variables that maximize the reward based on the given context. The next listing starts by importing the necessary libraries and defining variables for the contextual bandit problem.</p>

  <p class="fm-code-listing-caption">Listing 12.5 Contextual bandit for delivery truck selection</p>
  <pre class="programlisting">import vowpalwabbit
import torch
import matplotlib.pyplot as plt
import pandas as pd
import random
import numpy as np
from tqdm import tqdm
  
shared_contexts = ['city', 'interstate']                          <span class="fm-combinumeral">①</span>
  
size_types = ['small', 'medium', 'large']                         <span class="fm-combinumeral">②</span>
engine_types = ['petrol', 'diesel', 'electric']                   <span class="fm-combinumeral">②</span>
tire_types = ['all_season', 'snow', 'performance', 'all_terrain'] <span class="fm-combinumeral">②</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Set the shared context.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Set the action options or the arms.</p>

  <p class="body">We then define the following reward function that takes as inputs the shared context and indices representing the chosen size, engine, and tire options. It returns the reward value as an output:</p>
  <pre class="programlisting">def reward_function(shared_context, size_index, engine_index, tire_index):
    size_value = [0.8, 1.0, 0.9]                                   <span class="fm-combinumeral">①</span>
    engine_value = [0.7, 0.9, 1.0]                                 <span class="fm-combinumeral">②</span>
    tire_value = [0.9, 0.8, 1.0, 0.95]                             <span class="fm-combinumeral">③</span>
  
    reward = (
        size_value[size_index]
        * engine_value[engine_index]
        * tire_value[tire_index]
    )                                                              <span class="fm-combinumeral">④</span>
  
    noise_scale = 0.05                                             <span class="fm-combinumeral">⑤</span>
    noise_value = np.random.normal(loc=0, scale=noise_scale)       <span class="fm-combinumeral">⑤</span>
    reward += noise_value                                          <span class="fm-combinumeral">⑤</span>
  
    return reward `                                                <span class="fm-combinumeral">⑥</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Higher value indicates better fuel efficiency.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Higher value indicates better performance.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Higher value indicates better comfort.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Initial reward is based on the selected options.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Add noise to the reward representing uncertainties in weather, road conditions, and so on.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Return the reward.</p>

  <p class="body"><a id="marker-487"/>As a continuation, the following <code class="fm-code-in-text">generate_combinations</code> function generates combinations of actions and examples for the contextual bandit problem. This function takes four inputs: the shared context and three lists representing size, engine, and tire options for the actions. The function returns the list of examples and the list of descriptions once all combinations have been processed. Nested loops are used to iterate over each combination of size, engine, and tire options. The <code class="fm-code-in-text">enumerate</code> function is used to simultaneously retrieve the index (<code class="fm-code-in-text">i</code>, <code class="fm-code-in-text">j</code>, <code class="fm-code-in-text">k</code>) and the corresponding options (size, engine, tire):<a id="idIndexMarker152"/><a id="idIndexMarker153"/></p>
  <pre class="programlisting">def generate_combinations(shared_context, size_types, engine_types, tire_types):
    examples = [f"shared |User {shared_context}"]
    descriptions = []
    for i, size in enumerate(size_types):
        for j, engine in enumerate(engine_types):
            for k, tire in enumerate(tire_types):
                examples.append(f"|Action truck_size={size} engine={engine} 
                    <span class="fm-code-continuation-arrow">➥</span> tire={tire}")
                descriptions.append((i, j, k))
    return examples, descriptions</pre>

  <p class="body"><a id="marker-488"/>We now need to sample from a probability mass function (PMF) representing truck actions. The <code class="fm-code-in-text">sample_truck_pmf</code> function samples an index from a given PMF and returns the index along with its probability. The indices are used to retrieve the corresponding size, engine, and tire indices from the indices list:<a id="idIndexMarker154"/><a id="idIndexMarker155"/></p>
  <pre class="programlisting">def sample_truck_pmf(pmf):
    pmf_tensor = torch.tensor(pmf)                   <span class="fm-combinumeral">①</span>
    index = torch.multinomial(pmf_tensor, 1).item()  <span class="fm-combinumeral">②</span>
    chosen_prob = pmf[index]                         <span class="fm-combinumeral">③</span>
    
    return index, chosen_prob                        <span class="fm-combinumeral">④</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Convert the pmf to a Torch tensor.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Perform a multinomial sampling.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Capture the probability of the selected action.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Return the sampled index and its corresponding probability.</p>

  <p class="body">Now we need to create a VW workspace for training the contextual bandit model:</p>
  <pre class="programlisting">cb_vw = vowpalwabbit.Workspace(
    "--cb_explore_adf --epsilon 0.2 --interactions AA AU AAU -l 0.05 --power_t 0",
    quiet=True,
)</pre>

  <p class="body">The workspace is defined using the following parameters:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">--cb_explore_adf</code>—This parameter specifies the exploration algorithm for contextual bandit learning using the action-dependent features (ADF). In many real-world applications, there may be features associated with each action (or arm), and these are the <i class="fm-italics">action-dependent features</i>. This enables the model to explore different actions based on the observed context.<a id="idIndexMarker156"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">--epsilon 0.2</code>—This parameter sets the exploration rate or epsilon value to 0.2. It determines the probability of the model exploring a random action instead of selecting the action with the highest predicted reward. A higher epsilon value encourages more exploration.<a id="idIndexMarker157"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">--interactions AA AU AAU</code>—This parameter creates feature interactions between namespaces in VW.<a id="idIndexMarker158"/> These interactions help the model capture more complex relationships between features.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">-l 0.05</code>—This parameter sets the learning rate to 0.05. It determines the rate at which the model’s internal parameters are updated during the learning process. A higher learning rate makes the model converge faster, but if you adjust the learning rate too high, you risk over-fitting and end up worse on average.<a id="idIndexMarker159"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">--power_t 0</code>—This argument sets the power value to 0. It affects the learning rate decay over time. A power value of 0 indicates a constant learning rate.<a id="idIndexMarker160"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">quiet=True</code>—This argument sets the <code class="fm-code-in-text">quiet</code> mode to <code class="fm-code-in-text">True</code>, suppressing the display of unnecessary information or progress updates during the training process. It helps keep the output concise and clean.<a id="idIndexMarker161"/><a id="marker-489"/></p>
    </li>
  </ul>

  <p class="body">The following code snippet is used to train the CMAB model:</p>
  <pre class="programlisting">num_iterations = 2500
cb_rewards = []
with tqdm(total=num_iterations, desc="Training") as pbar:
    for _ in range(num_iterations):
        shared_context = random.choice(shared_contexts)                    <span class="fm-combinumeral">①</span>
        examples, indices = generate_combinations(
            shared_context, size_types, engine_types, tire_types
        )                                                                  <span class="fm-combinumeral">②</span>
        cb_prediction = cb_vw.predict(examples)                            <span class="fm-combinumeral">③</span>
        chosen_index, prob = sample_truck_pmf(cb_prediction)               <span class="fm-combinumeral">④</span>
        size_index, engine_index, tire_index = indices[chosen_index]       <span class="fm-combinumeral">⑤</span>
        reward = reward_function(shared_context, size_index, engine_index, <span class="fm-combinumeral">⑥</span>
           <span class="fm-code-continuation-arrow">➥</span> tire_index)                                                  <span class="fm-combinumeral">⑥</span>
        cb_rewards.append(reward)                                          <span class="fm-combinumeral">⑥</span>
        examples[chosen_index + 1] = f"0:{-1*reward}:{prob} {examples[chosen_
           <span class="fm-code-continuation-arrow">➥</span> index + 1]}"                                                 <span class="fm-combinumeral">⑦</span>
        cb_vw.learn(examples)                                              <span class="fm-combinumeral">⑧</span>
        pbar.set_postfix({'Reward': reward})
        pbar.update(1)
cb_vw.finish()                                                             <span class="fm-combinumeral">⑨</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Select a random shared context.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Generate examples and indices.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Obtain the model’s predictions for the chosen actions.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Obtain the chosen index and its corresponding probability.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Retrieve the corresponding size, engine, and tire indices.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Obtain and append the reward.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Update the examples corresponding to the chosen index.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Learn and update the model’s internal parameters based on the observed rewards.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑨</span> Finalize the VW workspace.</p>

  <p class="body">After training, we can use the following function to test the trained CMAB model. This code evaluates the trained model by generating examples, making predictions, sampling actions, and calculating the expected reward based on a given shared context:</p>
  <pre class="programlisting">def test_model(shared_context, size_types, engine_types, tire_types):
    examples, indices = generate_combinations(shared_context, size_types, 
       <span class="fm-code-continuation-arrow">➥</span> engine_types, tire_types)
    cb_prediction = cb_vw.predict(examples)
    chosen_index, prob = sample_truck_pmf(cb_prediction)
    chosen_action = examples[chosen_index]
    size_index, engine_index, tire_index = indices[chosen_index]
    expected_reward = reward_function(shared_context, size_index,
                <span class="fm-code-continuation-arrow">➥</span> engine_index, tire_index)
    print("Chosen Action:", chosen_action)
    print("Expected Reward:", expected_reward)
  
test_shared_context = 'city'
test_model(test_shared_context, size_types, engine_types, tire_types)</pre>

  <p class="body">The code will produce output something like the following:</p>
  <pre class="programlisting">Chosen Action: Action truck_size=medium engine=electric tire=snow
Expected Reward: 1.012</pre>

  <p class="body">This output represents the chosen action based on the given shared context during testing. In this case, the chosen action specifies a truck with a medium size, an electric engine, and a snow tire. The obtained reward is 1.012. Note that the maximum reward with noise ≈ 1.0 + 0.15 = 1.15. Given that the maximum values from <code class="fm-code-in-text">size_value</code>, <code class="fm-code-in-text">engine_value</code>, and <code class="fm-code-in-text">tire_value</code> are 1, and considering that the noise is a random value with a standard deviation of 0.05, a value of +3 standard deviations (3 × 0.05 = 0.15) would cover about 99.7% of cases in a normal distribution.<a id="idIndexMarker162"/><a id="idIndexMarker163"/><a id="idIndexMarker164"/><a id="idIndexMarker165"/><a id="marker-490"/></p>

  <h2 class="fm-head" id="heading_id_14">12.6 Journey’s end: A final reflection</h2>

  <p class="body">In this book, we have embarked on a comprehensive journey through a diverse landscape of search and optimization algorithms. We first explored deterministic search algorithms that tirelessly traverse problem spaces, seeking optimal solutions through both blind and informed methods. Then we climbed the peaks and valleys of trajectory-based algorithms, witnessing the power of simulated annealing and the ingenious designs of tabu search for escaping local optima. Continuing on our path, we ventured into the realm of evolutionary computing algorithms, witnessing the power of genetic algorithms and their variants in solving complex continuous and discrete optimization problems. Along the way, we embarked on a fascinating journey with swarm intelligence algorithms, starting with particle swarm optimization and offering a glimpse into other algorithms such as the ant colony optimization and artificial bee colony algorithms. Finally, we embraced the realm of machine learning–based methods, where supervised, unsupervised, and reinforcement learning algorithms are used to handle combinatorial optimization problems. Each algorithm covered in this book carries its own set of strengths and weaknesses. Remember, the choice of technique is determined by the task at hand, the characteristics of the problem, and the available resources.</p>

  <p class="body">I hope that the knowledge you’ve gained from this book empowers you to solve real-world problems and embrace the boundless potential of search and optimization in different domains. The fascinating world of search and optimization algorithms continues to expand and evolve. It is up to us to harness this knowledge, to further our capabilities, to solve the problems of today, and to shape the future.</p>

  <h2 class="fm-head" id="heading_id_15">Summary</h2>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Reinforcement learning (RL) can be formulated as an optimization problem wherein the agent aims to learn and/or refine its policy to maximize the expected cumulative reward within a specific environment.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Reinforcement learning problems can be classified into two main categories: Markov decision processes (MDPs) and multi-armed bandit (MAB) problems. MDP problems involve environments where the agent's actions impact the environment and its future states. MAB problems focus on maximizing cumulative rewards from a set of independent choices (often referred to as "arms") that can be made repeatedly over time. MABs don't consider the impact of choices on future options, unlike MDPs.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">In MDP-based problems, reinforcement learning uses MDP as a foundational mathematical framework to model decision-making problems under uncertainty. MDP is used to describe an environment for RL where an agent learns to make decisions by performing actions in an environment to achieve a goal.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">RL is classified into model-based and model-free RL, based on the presence or absence of a model of the environment. The model refers to an internal representation or understanding of how the environment behaves—specifically, the transition dynamics and reward function.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Based on how RL algorithms learn and update their policy from collected experiences, RL algorithms can be classified into off-policy and on-policy RL.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Advantage actor-critic (A2C) and proximal policy optimization (PPO) are model-free on-policy RL methods.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">By using a clipped objective function, PPO strikes a balance between encouraging exploration and maintaining stability during policy updates. The clipping operation restricts the update to a bounded range, preventing large policy changes that could be detrimental to performance. This mechanism ensures that the policy update remains within a reasonable and controlled distance from the previous policy, promoting smoother and more stable learning.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Unlike MDPs, Multi-armed bandits (MABs) don't consider the impact of choices on future states, and the agent does not need to worry about transitioning between states because there is only one state. Explore-only, exploit-only greedy, <span class="cambria">ε</span>-greedy, and upper confidence bound (UCB) are examples of MAB strategies for determining the best approach for selecting the actions to maximize the cumulative reward over the time.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Contextual multi-armed bandits (CMABs) are an extension of MAB where the decision-making is influenced by additional contextual information about each choice or environment.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Reinforcement learning can be applied to solve various combinatorial optimization problems, including the traveling salesman problem, traffic signal control, elevator dispatching, optimal dispatch policies for ride-sharing, the freight delivery problem, personalized recommendations, CartPole balancing, coordinating autonomous vehicles in mobile networks, and truck selection.<a id="idIndexMarker166"/><a id="marker-491"/></p>
    </li>
  </ul>
</body></html>