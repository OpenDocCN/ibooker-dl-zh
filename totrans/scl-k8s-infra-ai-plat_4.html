<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 5. Responsible AI"><div class="chapter" id="ch05_responsible_ai_1738498450930720">
      <h1><span class="label">Chapter 5. </span>Responsible AI</h1>
      <p>Up until now, we focused on building a model and preparing it for deployment. Before going live, though, there are additional important considerations to cover.</p>
      <p>Large language models (LLMs) are not just powerful from a technical standpoint but also from a societal standpoint. They’re being used to amplify spam, aid scammers, and even generate plausible propaganda at scale. They are trained on massive corpuses of data, which include material we want to draw from as well as all kinds of bias, racism, sexism, and potentially illegal or harmful material (for example, early versions of ChatGPT cheerfully explained to users how to build bombs). Because of this, there has been increased emphasis on and scrutiny of ethical and responsible training and use of LLMs.</p>
      <section data-type="sect1" data-pdf-bookmark="Data Safety and Transparency"><div class="sect1" id="ch05_data_safety_and_transparency_1738498450930808">
        <h1>Data Safety and Transparency</h1>
        <p>Like any other machine learning model, training data is of utmost importance to LLMs. Unlike other machine learning models, however, LLMs generate text, which creates a new attack vector for bad actors. One way to combat this and help prevent unethical data usage is to publish clear information about how an LLM was trained, what data it used, and who developed it. This information makes it possible for users, researchers, and regulators to effectively scrutinize the model’s behavior and to report or manage instances of harm or misuse. There are several platforms and frameworks available to help achieve this, such as UNESCO’s <a href="https://oreil.ly/HSfmk">Global AI Ethics and Governance Observatory</a>, with more being developed and iterated on by government groups, private think tanks, organizations building AI, and academic labs.</p>
        <p>Because of the wide variety of possible outputs of an LLM, there are various outputs that a model could generate that could be a security vulnerability (like leaking personally identifiable information [PII]), a safety or ethical violation (such as generating racist or sexist content), or harmful information (like how to commit suicide or to build a bomb). These topics are challenging to consider, and the open-ended nature of LLMs multiplies the challenge, but it is important to consider them due to the wide deployment and novel capabilities of this technology.</p>
        <p>As AI becomes more regulated, transparency will be key to ensuring compliance with laws and standards related to data privacy, use, and security. Transparent data practices help companies demonstrate that they are following their legal obligations such as HIPAA for health data in the US, GDPR in the EU, and censoring PII. The regulatory frameworks around the world are in a constant state of flux, so the UN Trade and Development’s page for <a href="https://oreil.ly/gCUls">data privacy laws around the world</a> is a good resource to consult for the latest views on the global regulatory landscape.</p>
        <p>To evaluate safety, one popular tool currently available is LM-Eval. LM-Eval allows you to test generative AI models across a wide range of task-specific benchmarks and also to build your own benchmarks to test your trained model against. This tool is compatible with Kubernetes via the TrustyAI Kubernetes Operator, and is installed by default in <a href="https://oreil.ly/4s0C_">Red Hat OpenShift AI</a>. Check out the TrustyAI website to learn more about using <a href="https://oreil.ly/ZeZyV">LM-Eval in Kubernetes</a> and see how <a href="https://oreil.ly/dsTgo">LM-Eval fits in with the rest of your cluster</a>.</p>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="AI Guardrails"><div class="sect1" id="ch05_ai_guardrails_1738498450930873">
        <h1>AI Guardrails</h1>
        <p>AI guardrails are a new software concept that act as safety mechanisms for LLMs. These are solutions that act as middleware between an incoming request and an LLM, and add limitations to prevent outputs that would be harmful or go against some norm. For example, a security guardrail could detect errant PII in an output and remove it before returning it to a user. Guardrails should be able to enforce an enterprise’s policies and guidelines, enable contextual understanding, and be regularly updated.</p>
        <p>One community building an entire suite of open source AI safety tools is <a href="https://oreil.ly/hMSM-">TrustyAI</a>, who also created LM-Eval, mentioned in the previous section. They also provide AI guardrail tooling, such as <code>trustyai-detoxify</code>, a Python module within the larger TrustyAI Python library that provides guardrails around toxic language, among other safety tools centered on toxic language. Additionally, they maintain <a href="https://oreil.ly/AUOiB">TrustyAI Guardrails</a>, which acts as a server for calling detectors (such as a toxic language detector) to help developers implement their own guardrails.</p>
        <p>LLM guardrail detectors are tools that ensure LLMs operate within predefined ethical, safety, and operational boundaries. These are designed to identify and mitigate undesirable outcomes, such as generating harmful or inappropriate content, ensuring that AI systems behave responsibly. There are a wide array of detectors available today, and you can access many public ones via the <a href="https://hub.guardrailsai.com">Guardrails AI Hub</a>. Some broad examples of detectors include:</p>
        <dl>
          <dt>PII detectors</dt>
          <dd>
            <p>These screen outputs for PII and prevent it from reaching the user.</p>
          </dd>
          <dt>Hate, abuse, profanity (HAP) detectors</dt>
          <dd>
            <p>These screen outputs for toxic content, bias, or harmful content like hate speech, discrimination, or misinformation.</p>
          </dd>
          <dt>Bias detectors</dt>
          <dd>
            <p>These analyze outputs for signs of unfair biases related to race, gender, religion, or other attributes.</p>
          </dd>
          <dt>Prompt injection detectors</dt>
          <dd>
            <p>These recognize and counter potential adversarial prompts or attempts to manipulate the model into generating harmful <span class="keep-together">content</span>.</p>
          </dd>
        </dl>
        <p>TrustyAI provides a <a href="https://oreil.ly/kB0fi">Kubernetes Operator</a> that allows you to seamlessly integrate it with your cluster, as well as a <a href="https://oreil.ly/FzfM_">custom explainer for KServe</a>.</p>
        <p>While these toolkits are not exhaustive, they are currently available resources to help address abuses of LLMs, keeping your users and your enterprise safe. To dive deeper into mitigating bias and harm throughout the AI development lifecycle, consult Aileen Nielsen’s book <a class="orm:hideurl" href="https://www.oreilly.com/library/view/practical-fairness/9781492075721/"><em>Practical Fairness</em></a> (O’Reilly, 2020).</p>
      </div></section>
    </div></section></div>
</div>
</body></html>