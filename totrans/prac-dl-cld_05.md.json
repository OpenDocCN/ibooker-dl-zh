["```py\nimport tensorflow_datasets as tfds\nprint(tfds.list_builders())\n```", "```py\n['amazon_us_reviews', 'bair_robot_pushing_small', 'bigearthnet', 'caltech101',\n'cats_vs_dogs', 'celeb_a', 'imagenet2012', \u2026 , 'open_images_v4',\n'oxford_flowers102', 'stanford_dogs','voc2007', 'wikipedia', 'wmt_translate',\n'xnli']\n```", "```py\n# Import necessary packages\nimport tensorflow_datasets as tfds\n\n# Downloading and loading the dataset\ndataset = tfds.load(name=\"cats_vs_dogs\", split=tfds.Split.TRAIN)\n\n# Building a performance data pipeline\ndataset = dataset.map(preprocess).cache().repeat().shuffle(1024).batch(32).\nprefetch(tf.data.experimental.AUTOTUNE)\n\nmodel.fit(dataset, ...)\n```", "```py\nsummary_writer = tf.summary.FileWriter('./logs')\n```", "```py\n# Get TensorBoard to run\n%load_ext tensorboard\n\n# Start TensorBoard\n%tensorboard --logdir ./log\n```", "```py\n# Save model for What If Tool\ntf.saved_model.save(model, \"/tmp/model/1/\")\n```", "```py\n$ mkdir tensorboard\n$ tensorboard --logdir ./log --alsologtostderr\n```", "```py\n$ mkdir what-if-stuff\n```", "```py\n$ tree .\n\u251c\u2500\u2500 colo\n\u2502   \u2514\u2500\u2500 model\n\u2502         \u2514\u2500\u2500 1\n\u2502         \u251c\u2500\u2500 assets\n\u2502         \u251c\u2500\u2500 saved_model.pb\n\u2502         \u2514\u2500\u2500 variables\n```", "```py\n$ sudo docker run -p 8500:8500 \\\n--mount type=bind,source=/home/{*`your_username`*}/what-if-stuff/colo/model/,\n target=/models/colo \\\n-e MODEL_NAME=colo -t tensorflow/serving\n```", "```py\nfrom witwidget.notebook.visualization import WitConfigBuilder\n\n*`# features are the test examples that we want to load into the tool`*\nmodels = [model2, model3, model4]\nconfig_builder =\nWitConfigBuilder(test_examples).set_estimator_and_feature_spec(model1, features)\n\nfor each_model in models:\n    config_builder =\n config_builder.set_compare_estimator_and_feature_spec(each_model, features)\n```", "```py\nfrom tf_explain.core.grad_cam import GradCAM\nFrom tf.keras.applications.MobileNet import MobileNet\n\nmodel = MobileNet(weights='imagenet', include_top=True)\n\n# Set Grad CAM System\nexplainer = GradCAM()\n\n# Image Processing\nIMAGE_PATH = 'dog.jpg'\ndog_index = 263\nimg = tf.keras.preprocessing.image.load_img(IMAGE_PATH, target_size=(224, 224))\nimg = tf.keras.preprocessing.image.img_to_array(img)\ndata = ([img], None)\n\n# Passing the image through Grad CAM\ngrid = explainer.explain(data, model, 'conv1', index)\nname = IMAGE_PATH.split(\".jpg\")[0]\nexplainer.save(grid, '/tmp', name + '_grad_cam.png')\n```", "```py\ndataset_name = \"cats_vs_dogs\"\ntrain, info_train = tfds.load(dataset_name, split=tfds.Split.TRAIN,\n                    with_info=True)\n```", "```py\n# Load the dataset\ndataset_name = \"cats_vs_dogs\"\n\n# Dividing data into train (80), val (10) and test (10)\nsplit_train, split_val, split_test = tfds.Split.TRAIN.subsplit(weighted=[80, 10,\n                                     10])\ntrain, info_train = tfds.load(dataset_name, split=split_train , with_info=True)\nval, info_val = tfds.load(dataset_name, split=split_val, with_info=True)\ntest, info_test = tfds.load(dataset_name, split=split_test, with_info=True)\n```", "```py\n# Define Early Stopping callback\nearlystop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_acc',\n\t\t\t\t\t min_delta=0.0001, patience=10)\n\n# Add to the training model\nmodel.fit_generator(... callbacks=[earlystop_callback])\n```", "```py\n# Seed for Tensorflow\ntf.random.set_seed(1234)\n\n# Seed for Numpy\nimport numpy as np\nnp.random.seed(1234)\n\n# Seed for Keras\nseed = 1234\nfit(train_data, augment=True, seed=seed)\nflow_from_dataframe(train_dataframe, shuffle=True, seed=seed)\n```", "```py\n# Import necessary packages\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\n# tfds makes a lot of progress bars, which takes up a lot of screen space, hence\n# disabling them\ntfds.disable_progress_bar()\n\ntf.random.set_seed(1234)\n\n# Variables\nBATCH_SIZE = 32\nNUM_EPOCHS= 20\nIMG_H = IMG_W = 224\nIMG_SIZE = 224\nLOG_DIR = './log'\nSHUFFLE_BUFFER_SIZE = 1024\nIMG_CHANNELS = 3\n\ndataset_name = \"oxford_flowers102\"\n\ndef preprocess(ds):\n  x = tf.image.resize_with_pad(ds['image'], IMG_SIZE, IMG_SIZE)\n  x = tf.cast(x, tf.float32)\n  x = (x/127.5) - 1\n  return x, ds['label']\n\ndef augmentation(image,label):\n  image = tf.image.random_brightness(image, .1)\n  image = tf.image.random_contrast(image, lower=0.0, upper=1.0)\n  image = tf.image.random_flip_left_right(image)\n  return image, label\n\ndef get_dataset(dataset_name):\n  split_train, split_val = tfds.Split.TRAIN.subsplit(weighted=[9,1])\n  train, info_train = tfds.load(dataset_name, split=split_train , with_info=True)\n  val, info_val = tfds.load(dataset_name, split=split_val, with_info=True)\n  NUM_CLASSES = info_train.features['label'].num_classes\n  assert NUM_CLASSES >= info_val.features['label'].num_classes\n  NUM_EXAMPLES = info_train.splits['train'].num_examples * 0.9\n  IMG_H, IMG_W, IMG_CHANNELS = info_train.features['image'].shape\n  train = train.map(preprocess).cache().\n          repeat().shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n  train = train.map(augmentation)\n  train = train.prefetch(tf.data.experimental.AUTOTUNE)\n  val = val.map(preprocess).cache().repeat().batch(BATCH_SIZE)\n  val = val.prefetch(tf.data.experimental.AUTOTUNE)\n  return train, info_train, val, info_val, IMG_H, IMG_W, IMG_CHANNELS,\n         NUM_CLASSES, NUM_EXAMPLES\n\ntrain, info_train, val, info_val, IMG_H, IMG_W, IMG_CHANNELS, NUM_CLASSES,\nNUM_EXAMPLES = get_dataset(dataset_name)\n\n# Allow TensorBoard callbacks\ntensorboard_callback = tf.keras.callbacks.TensorBoard(LOG_DIR,\n                                                      histogram_freq=1,\n                                                      write_graph=True,\n                                                      write_grads=True,\n                                                      batch_size=BATCH_SIZE,\n                                                      write_images=True)\n\ndef transfer_learn(train, val, unfreeze_percentage, learning_rate):\n   mobile_net = tf.keras.applications.ResNet50(input_shape=(IMG_SIZE, IMG_SIZE,\n                IMG_CHANNELS), include_top=False)\n   mobile_net.trainable=False\n   # Unfreeze some of the layers according to the dataset being used\n   num_layers = len(mobile_net.layers)\n   for layer_index in range(int(num_layers - unfreeze_percentage*num_layers),\n                             num_layers ):\n   \t\tmobile_net.layers[layer_index].trainable = True\n   model_with_transfer_learning = tf.keras.Sequential([mobile_net,\n                          tf.keras.layers.GlobalAveragePooling2D(),\n                          tf.keras.layers.Flatten(),\n                          tf.keras.layers.Dense(64),\n                          tf.keras.layers.Dropout(0.3),\n                          tf.keras.layers.Dense(NUM_CLASSES, \n                                                activation='softmax')],)\n  model_with_transfer_learning.compile(\n               optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n                   loss='sparse_categorical_crossentropy',\n                   metrics=[\"accuracy\"])\n  model_with_transfer_learning.summary()\n  earlystop_callback = tf.keras.callbacks.EarlyStopping(\n                                   monitor='val_accuracy', \n                                   min_delta=0.0001, \n                                   patience=5)\n  model_with_transfer_learning.fit(train,\n                                   epochs=NUM_EPOCHS,\n                                   steps_per_epoch=int(NUM_EXAMPLES/BATCH_SIZE),\n                                   validation_data=val,\n                                   validation_steps=1,\n                                   validation_freq=1,\n                                   callbacks=[tensorboard_callback,\n                                              earlystop_callback])\n  return model_with_transfer_learning\n\n# Start TensorBoard\n%tensorboard --logdir ./log\n\n# Select the last % layers to be trained while using the transfer learning\n# technique. These layers are the closest to the output layers.\nunfreeze_percentage = .33\nlearning_rate = 0.001\n\nmodel = transfer_learn(train, val, unfreeze_percentage, learning_rate)\n```", "```py\ndef create_model():\n  model = tf.keras.Sequential([\n     tf.keras.layers.Conv2D(32, (3, 3), activation='relu',\n          input_shape=(IMG_SIZE, IMG_SIZE, IMG_CHANNELS)),\n     tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n     tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n     tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n     tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n     tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n     tf.keras.layers.Dropout(rate=0.3),\n     tf.keras.layers.Flatten(),\n     tf.keras.layers.Dense(128, activation='relu'),\n     tf.keras.layers.Dropout(rate=0.3),\n     tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n  ])\n  return model \n\ndef scratch(train, val, learning_rate):\n  model = create_model()\n  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n                 loss='sparse_categorical_crossentropy',\n                 metrics=['accuracy'])\n\n  earlystop_callback = tf.keras.callbacks.EarlyStopping(\n                 monitor='val_accuracy', \n                 min_delta=0.0001, \n                 patience=5)\n\n  model.fit(train,\n           epochs=NUM_EPOCHS,\n           steps_per_epoch=int(NUM_EXAMPLES/BATCH_SIZE),\n           validation_data=val, \n           validation_steps=1,\n           validation_freq=1,\n           callbacks=[tensorboard_callback, earlystop_callback])\n  return model\n```", "```py\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np\n\nfrom kerastuner.engine.hypermodel import HyperModel\nfrom kerastuner.engine.hyperparameters import HyperParameters\n\n# Input data\n(x, y), (val_x, val_y) = keras.datasets.mnist.load_data()\nx = x.astype('float32') / 255.\nval_x = val_x.astype('float32') / 255.\n\n# Defining hyper parameters\nhp = HyperParameters()\nhp.Choice('learning_rate', [0.1, 0.001])\nhp.Int('num_layers', 2, 10)\n\n# Defining model with expandable number of layers\ndef build_model(hp):\n    model = keras.Sequential()\n    model.add(layers.Flatten(input_shape=(28, 28)))\n    for _ in range(hp.get('num_layers')):\n        model.add(layers.Dense(32, activation='relu'))\n    model.add(layers.Dense(10, activation='softmax'))\n    model.compile(\n        optimizer=keras.optimizers.Adam(hp.get('learning_rate')),\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy'])\n    return model\n\nhypermodel = RandomSearch(\n             build_model,\n             max_trials=20, # Number of combinations allowed\n             hyperparameters=hp,\n             allow_new_entries=False,\n             objective='val_accuracy')\n\nhypermodel.search(x=x,\n             y=y,\n             epochs=5,\n             validation_data=(val_x, val_y))\n\n# Show summary of overall best model\nhypermodel.results_summary()\n```", "```py\n > Hp values:\n  |-learning_rate: 0.001\n  |-num_layers: 6\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Name         \u2502 Best model \u2502 Current model \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 accuracy     \u2502 0.9911     \u2502 0.9911        \u2502\n\u2502 loss         \u2502 0.0292     \u2502 0.0292        \u2502\n\u2502 val_loss     \u2502 0.227      \u2502 0.227         \u2502\n\u2502 val_accuracy \u2502 0.9406     \u2502 0.9406        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```", "```py\nHypertuning complete - results in ./untitled_project\n[Results summary]\n |-Results in ./untitled_project\n |-Ran 20 trials\n |-Ran 20 executions (1 per trial)\n |-Best val_accuracy: 0.9406\n```", "```py\ntuner.enable_cloud(api_key=api_key)\n```", "```py\nfrom PIL import Image\nfrom autoaugment import ImageNetPolicy\nimg = Image.open(\"cat.jpg\")\npolicy = ImageNetPolicy()\nimgs = [policy(img) for _ in range(8) ]\n```", "```py\n!pip3 install autokeras\n!pip3 install graphviz\nfrom keras.datasets import mnist\nfrom autokeras.image.image_supervised import ImageClassifier\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train = x_train.reshape(x_train.shape + (1,))\nx_test = x_test.reshape(x_test.shape + (1,))\n\nclf = ImageClassifier(path=\".\",verbose=True, augment=False)\nclf.fit(x_train, y_train, time_limit= 30 * 60) # 30 minutes\nclf.final_fit(x_train, y_train, x_test, y_test, retrain=True)\ny = clf.evaluate(x_test, y_test)\nprint(y)\n\n*`# Save the model as a pickle file`*\nclf.export_autokeras_model(\"model.pkl\")\n\nvisualize('.')\n```"]