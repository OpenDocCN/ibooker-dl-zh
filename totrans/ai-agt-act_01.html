<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">2</span></span> <span class="chapter-title-text">Harnessing the power of large language models</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">Understanding the basics of LLMs</li> 
    <li class="readable-text" id="p3">Connecting to and consuming the OpenAI API</li> 
    <li class="readable-text" id="p4">Exploring and using open source LLMs with LM Studio</li> 
    <li class="readable-text" id="p5">Prompting LLMs with prompt engineering</li> 
    <li class="readable-text" id="p6">Choosing the optimal LLM for your specific needs</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>The term <em>large language models</em> (LLMs<span class="light"/>) has now become a ubiquitous descriptor of a form of AI. These LLMs have been developed using generative pretrained transformers (GPTs). While other architectures also power LLMs, the GPT form is currently the most successful.</p> 
  </div> 
  <div class="readable-text intended-text" id="p8"> 
   <p>LLMs and GPTs are <em>generative</em> models, which means they are trained to <em>generate</em> rather than predict or classify content. To illustrate this further, consider figure 2.1, which shows the difference between generative and predictive/classification models. Generative models create something from the input, whereas predictive and classifying models classify it.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p9">  
   <img alt="figure" src="../Images/2-1.png" width="1100" height="504"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.1</span> The difference between generative and predictive models</h5>
  </div> 
  <div class="readable-text" id="p10"> 
   <p>We can further define an LLM by its constituent parts, as shown in figure 2.2. In this diagram, <em>data</em> represents the content used to train the model, and <em>architecture</em> is an attribute of the model itself, such as the number of parameters or size of the model. Models are further trained specifically to the desired use case, including chat, completions, or instruction. Finally, <em>fine-tuning</em> is a feature added to models that refines the input data and model training to better match a particular use case or domain.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p11">  
   <img alt="figure" src="../Images/2-2.png" width="852" height="484"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.2</span> The main elements that describe an LLM</h5>
  </div> 
  <div class="readable-text" id="p12"> 
   <p>The transformer architecture of GPTs, which is a specific architecture of LLMs, allows the models to be scaled to billions of parameters in size. This requires these large models to be trained on terabytes of documents to build a foundation. From there, these models will be successively trained using various methods for the desired use case of the model.</p> 
  </div> 
  <div class="readable-text intended-text" id="p13"> 
   <p>ChatGPT, for example, is trained effectively on the public internet and then fine-tuned using several training strategies. The final fine-tuning training is completed using an advanced form called <em>reinforcement learning with human feedback</em> (RLHF). This produces a model use case called chat completions.</p> 
  </div> 
  <div class="readable-text intended-text" id="p14"> 
   <p><em>Chat completions</em> LLMs are designed to improve through iteration and refinement—in other words, chatting. These models have also been benchmarked to be the best in task completion, reasoning, and planning, which makes them ideal for building agents and assistants. Completion models are trained/designed only to provide generated content on input text, so they don’t benefit from iteration.</p> 
  </div> 
  <div class="readable-text intended-text" id="p15"> 
   <p>For our journey to build powerful agents in this book, we focus on the class of LLMs called chat completions models. That, of course, doesn’t preclude you from trying other model forms for your agents. However, you may have to significantly alter the code samples provided to support other model forms.</p> 
  </div> 
  <div class="readable-text intended-text" id="p16"> 
   <p>We’ll uncover more details about LLMs and GPTs later in this chapter when we look at running an open source LLM locally. In the next section, we look at how to connect to an LLM using a growing standard from OpenAI.</p> 
  </div> 
  <div class="readable-text" id="p17"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_11"><span class="num-string">2.1</span> Mastering the OpenAI API</h2> 
  </div> 
  <div class="readable-text" id="p18"> 
   <p>Numerous AI agents and assistant projects use the OpenAI API SDK to connect to an LLM. While not standard, the basic concepts describing a connection now follow the OpenAI pattern. Therefore, we must understand the core concepts of an LLM connection using the OpenAI SDK.</p> 
  </div> 
  <div class="readable-text intended-text" id="p19"> 
   <p>This chapter will look at connecting to an LLM model using the OpenAI Python SDK/package. We’ll discuss connecting to a GPT-4 model, the model response, counting tokens, and how to define consistent messages. Starting in the following subsection, we’ll examine how to use OpenAI.</p> 
  </div> 
  <div class="readable-text" id="p20"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_12"><span class="num-string">2.1.1</span> Connecting to the chat completions model</h3> 
  </div> 
  <div class="readable-text" id="p21"> 
   <p>To complete the exercises in this section and subsequent ones, you must set up a Python developer environment and get access to an LLM. Appendix A walks you through setting up an OpenAI account and accessing GPT-4 or other models. Appendix B demonstrates setting up a Python development environment with Visual Studio Code (VS Code), including installing needed extensions. Review these sections if you want to follow along with the scenarios.</p> 
  </div> 
  <div class="readable-text intended-text" id="p22"> 
   <p>Start by opening the source code <code>chapter_2</code> folder in VS Code and creating a new Python virtual environment. Again, refer to appendix B if you need assistance.</p> 
  </div> 
  <div class="readable-text intended-text" id="p23"> 
   <p>Then, install the OpenAI and Python dot environment packages using the command in the following listing. This will install the required packages into the virtual environment.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p24"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 2.1</span> <code>pip</code> installs </h5> 
   <div class="code-area-container"> 
    <pre class="code-area">pip install openai python-dotenv</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p25"> 
   <p>Next, open the <code>connecting.py</code> file in VS Code, and inspect the code shown in listing 2.2. Be sure to set the model’s name to an appropriate name—for example, gpt-4. At the time of writing, the <code>gpt-4-1106-preview</code> was used to represent GPT-4 Turbo.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p26"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 2.2</span> <code>connecting.py</code> </h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import os
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()                          <span class="aframe-location"/> #1
api_key = os.getenv('OPENAI_API_KEY')
if not api_key:                            <span class="aframe-location"/> #2
    raise ValueError("No API key found. Please check your .env file.")
client = OpenAI(api_key=api_key)                       <span class="aframe-location"/> #3

def ask_chatgpt(user_message):
    response = client.chat.completions.create(     <span class="aframe-location"/> #4
        model="gpt-4-1106-preview",
        messages=[{"role": "system",
 "content": "You are a helpful assistant."},
        {"role": "user", "content": user_message}],
        temperature=0.7,
        )
    return response.choices[0].message.content    <span class="aframe-location"/> #5

user = "What is the capital of France?"
response = ask_chatgpt(user)               <span class="aframe-location"/> #6
print(response)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Loads the secrets stored in the .env file
     <br/>#2 Checks to see whether the key is set
     <br/>#3 Creates a client with the key
     <br/>#4 Uses the create function to generate a response
     <br/>#5 Returns just the content of the response
     <br/>#6 Executes the request and returns the response
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p27"> 
   <p>A lot is happening here, so let’s break it down by section, starting with the beginning and loading the environment variables. In the <code>chapter_2</code> folder is another file called <code>.env</code>, which holds environment variables. These variables are set automatically by calling the <code>load_dotenv</code> function.</p> 
  </div> 
  <div class="readable-text intended-text" id="p28"> 
   <p>You must set your OpenAI API key in the <code>.env</code> file, as shown in the next listing. Again, refer to appendix A to find out how to get a key and find a model name.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p29"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 2.3</span> <code>.env</code> </h5> 
   <div class="code-area-container"> 
    <pre class="code-area">OPENAI_API_KEY='your-openai-api-key'</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p30"> 
   <p>After setting the key, you can debug the file by pressing the F5 key or selecting Run &gt; Start Debugging from the VS Code menu. This will run the code, and you should see something like “The capital of France is Paris.”</p> 
  </div> 
  <div class="readable-text intended-text" id="p31"> 
   <p>Remember that the response from a generative model depends on the probability. The model will probably give us a correct and consistent answer in this case.</p> 
  </div> 
  <div class="readable-text intended-text" id="p32"> 
   <p>You can play with these probabilities by adjusting the temperature of the request. If you want a model to be more consistent, turn the temperature down to 0, but if you want the model to produce more variation, turn the temperature up. We’ll explore setting the temperature further in the next section.</p> 
  </div> 
  <div class="readable-text" id="p33"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_13"><span class="num-string">2.1.2</span> Understanding the request and response</h3> 
  </div> 
  <div class="readable-text" id="p34"> 
   <p>Digging into the chat completions request and response features can be helpful. We’ll focus on the request first, as shown next. The request encapsulates the intended model, the messages, and the temperature.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p35"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 2.4</span> The chat completions request</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">response = client.chat.completions.create(
    model="gpt-4-1106-preview",                <span class="aframe-location"/> #1
    messages=[{"role": "system", 
"content": "You are a helpful assistant."},                    <span class="aframe-location"/> #2
              {"role": "user", "content": user_message}],     <span class="aframe-location"/> #3
    temperature=0.7,    <span class="aframe-location"/> #4
    )</pre> 
    <div class="code-annotations-overlay-container">
     #1 The model or deployment used to respond to the request
     <br/>#2 The system role message
     <br/>#3 The user role message
     <br/>#4 The temperature or variability of the request
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p36"> 
   <p>Within the request, the <code>messages</code> block describes a set of messages and roles used in a request. Messages for a chat completions model can be defined in three roles:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p37"> <em>System role</em> —A message that describes the request’s rules and guidelines. It can often be used to describe the role of the LLM in making the request. </li> 
   <li class="readable-text" id="p38"> <em>User role</em> —Represents and contains the message from the user. </li> 
   <li class="readable-text" id="p39"> <em>Assistant role</em> —Can be used to capture the message history of previous responses from the LLM. It can also inject a message history when perhaps none existed. </li> 
  </ul> 
  <div class="readable-text" id="p40"> 
   <p>The message sent in a single request can encapsulate an entire conversation, as shown in the JSON in the following listing.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p41"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 2.5</span> Messages with history</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">[
    {
        "role": "system",
        "content": "You are a helpful assistant."
    },
    {
        "role": "user",
        "content": "What is the capital of France?"
    },
    {
        "role": "assistant",
        "content": "The capital of France is Paris."
    },
    {
        "role": "user",
        "content": "What is an interesting fact of Paris."
    }
],</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p42"> 
   <p>You can see how this can be applied by opening <code>message_history.py</code> in VS Code and debugging it by pressing F5. After the file runs, be sure to check the output. Then, try to run the sample a few more times to see how the results change.</p> 
  </div> 
  <div class="readable-text intended-text" id="p43"> 
   <p>The results will change from each run to the next due to the high temperature of <code>.7</code>. Go ahead and reduce the temperature to <code>.0</code>, and run the <code>message_history.py</code> sample a few more times. Keeping the temperature at <code>0</code> will show the same or similar results each time.</p> 
  </div> 
  <div class="readable-text intended-text" id="p44"> 
   <p>Setting a request’s temperature will often depend on your particular use case. Sometimes, you may want to limit the responses’ stochastic nature (randomness). Reducing the temperature to <code>0</code> will give consistent results. Likewise, a value of <code>1.0</code> will give the most variability in the responses.</p> 
  </div> 
  <div class="readable-text intended-text" id="p45"> 
   <p>Next, we also want to know what information is being returned for each request. The next listing shows the output format for the response. You can see this output by running the <code>message_history.py</code> file in VS Code.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p46"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 2.6</span> Chat completions response</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">{
    "id": "chatcmpl-8WWL23up3IRfK1nrDFQ3EHQfhx0U6",
    "choices": [                                     <span class="aframe-location"/> #1
        {
            "finish_reason": "stop",
            "index": 0,
            "message": {
                "content": "… omitted",
                "role": "assistant",      <span class="aframe-location"/> #2
                "function_call": null,
                "tool_calls": null
            },
            "logprobs": null
        }
    ],
    "created": 1702761496,
    "model": "gpt-4-1106-preview",    <span class="aframe-location"/> #3
    "object": "chat.completion",
    "system_fingerprint": "fp_3905aa4f79",
    "usage": {
        "completion_tokens": 78,    <span class="aframe-location"/> #4
        "prompt_tokens": 48,         #4
        "total_tokens": 126          #4
    }
}</pre> 
    <div class="code-annotations-overlay-container">
     #1 A model may return more than one response.
     <br/>#2 Responses returned in the assistant role
     <br/>#3 Indicates the model used
     <br/>#4 Counts the number of input (prompt) and output (completion) tokens used
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p47"> 
   <p>It can be helpful to track the number of <em>input tokens</em> (those used in prompts) and the <em>output tokens</em> (the number returned through completions). Sometimes, minimizing and reducing the number of tokens can be essential. Having fewer tokens typically means LLM interactions will be cheaper, respond faster, and produce better and more consistent results.</p> 
  </div> 
  <div class="readable-text intended-text" id="p48"> 
   <p>That covers the basics of connecting to an LLM and returning responses. Throughout this book, we’ll review and expand on how to interact with LLMs. Until then, we’ll explore in the next section how to load and use open source LLMs.</p> 
  </div> 
  <div class="readable-text" id="p49"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_14"><span class="num-string">2.2</span> Exploring open source LLMs with LM Studio</h2> 
  </div> 
  <div class="readable-text" id="p50"> 
   <p>Commercial LLMs, such as GPT-4 from OpenAI, are an excellent place to start to learn how to use modern AI and build agents. However, commercial agents are an external resource that comes at a cost, reduces data privacy and security, and introduces dependencies. Other external influences can further complicate these factors.</p> 
  </div> 
  <div class="readable-text intended-text" id="p51"> 
   <p>It’s unsurprising that the race to build comparable open source LLMs is growing more competitive every day. As a result, there are now open source LLMs that may be adequate for numerous tasks and agent systems. There have even been so many advances in tooling in just a year that hosting LLMs locally is now very easy, as we’ll see in the next section.</p> 
  </div> 
  <div class="readable-text" id="p52"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_15"><span class="num-string">2.2.1</span> Installing and running LM Studio</h3> 
  </div> 
  <div class="readable-text" id="p53"> 
   <p>LM Studio is a free download that supports downloading and hosting LLMs and other models locally for Windows, Mac, and Linux. The software is easy to use and offers several helpful features to get you started quickly. Here is a quick summary of steps to download and set up LM Studio:</p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p54"> Download LM Studio from <a href="https://lmstudio.ai/">https://lmstudio.ai/</a>. </li> 
   <li class="readable-text" id="p55"> After downloading, install the software per your operating system. Be aware that some versions of LM Studio may be in beta and require installation of additional tools or libraries. </li> 
   <li class="readable-text" id="p56"> Launch the software. </li> 
  </ol> 
  <div class="readable-text" id="p57"> 
   <p>Figure 2.3 shows the LM Studio window running. From there, you can review the current list of hot models, search for others, and even download. The home page content can be handy for understanding the details and specifications of the top models.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p58">  
   <img alt="figure" src="../Images/2-3.png" width="1012" height="779"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.3</span> LM Studio software showing the main home page</h5>
  </div> 
  <div class="readable-text intended-text" id="p59"> 
   <p>An appealing feature of LM Studio is its ability to analyze your hardware and align it with the requirements of a given model. The software will let you know how well you can run a given model. This can be a great time saver in guiding what models you experiment with.</p> 
  </div> 
  <div class="readable-text" id="p60"> 
   <p>Enter some text to search for a model, and click Go. You’ll be taken to the search page interface, as shown in figure 2.4. From this page, you can see all the model variations and other specifications, such as context token size. After you click the Compatibility Guess button, the software will even tell you if the model will run on your system.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p61">  
   <img alt="figure" src="../Images/2-4.png" width="1012" height="556"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.4</span> The LM Studio search page</h5>
  </div> 
  <div class="readable-text intended-text" id="p62"> 
   <p>Click to download any model that will run on your system. You may want to stick with models designed for chat completions, but if your system is limited, work with what you have. In addition, if you’re unsure of which model to use, go ahead and download to try them. LM Studio is a great way to explore and experiment with many models.</p> 
  </div> 
  <div class="readable-text" id="p63"> 
   <p>After the model is downloaded, you can then load and run the model on the chat page or as a server on the server page. Figure 2.5 shows loading and running a model on the chat page. It also shows the option for enabling and using a GPU if you have one.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p64">  
   <img alt="figure" src="../Images/2-5.png" width="1012" height="959"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.5</span> The LM Studio chat page with a loaded, locally running LLM</h5>
  </div> 
  <div class="readable-text intended-text" id="p65"> 
   <p>To load and run a model, open the drop-down menu at the top middle of the page, and select a downloaded model. A progress bar will appear showing the model loading, and when it’s ready, you can start typing into the UI.</p> 
  </div> 
  <div class="readable-text intended-text" id="p66"> 
   <p>The software even allows you to use some or all of your GPU, if detected, for the model inference. A GPU will generally speed up the model response times in some capacities. You can see how adding a GPU can affect the model’s performance by looking at the performance status at the bottom of the page, as shown in figure 2.5.</p> 
  </div> 
  <div class="readable-text intended-text" id="p67"> 
   <p>Chatting with a model and using or playing with various prompts can help you determine how well a model will work for your given use case. A more systematic approach is using the prompt flow tool for evaluating prompts and LLMs. We’ll describe how to use prompt flow in chapter 9.</p> 
  </div> 
  <div class="readable-text intended-text" id="p68"> 
   <p>LM Studio also allows a model to be run on a server and made accessible using the OpenAI package. We’ll see how to use the server feature and serve a model in the next section.</p> 
  </div> 
  <div class="readable-text" id="p69"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_16"><span class="num-string">2.2.2</span> Serving an LLM locally with LM Studio</h3> 
  </div> 
  <div class="readable-text" id="p70"> 
   <p>Running an LLM locally as a server is easy with LM Studio. Just open the server page, load a model, and then click the Start Server button, as shown in figure 2.6. From there, you can copy and paste any of the examples to connect with your model.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p71">  
   <img alt="figure" src="../Images/2-6.png" width="1012" height="894"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.6</span> The LM Studio server page and a server running an LLM</h5>
  </div> 
  <div class="readable-text intended-text" id="p72"> 
   <p>You can review an example of the Python code by opening <code>chapter_2/lmstudio_ server.py</code> in VS Code. The code is also shown here in listing 2.7. Then, run the code in the VS Code debugger (press F5).</p> 
  </div> 
  <div class="browsable-container listing-container" id="p73"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 2.7</span> <code>lmstudio_server.py</code> </h5> 
   <div class="code-area-container"> 
    <pre class="code-area">from openai import OpenAI

client = OpenAI(base_url="http://localhost:1234/v1", api_key="not-needed")

completion = client.chat.completions.create(
  model="local-model",                          <span class="aframe-location"/> #1
  messages=[
    {"role": "system", "content": "Always answer in rhymes."},
    {"role": "user", "content": "Introduce yourself."}      <span class="aframe-location"/> #2
  ],
  temperature=0.7,
)

print(completion.choices[0].message)     <span class="aframe-location"/> #3</pre> 
    <div class="code-annotations-overlay-container">
     #1 Currently not used; can be anything
     <br/>#2 Feel free to change the message as you like.
     <br/>#3 Default code outputs the whole message.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p74"> 
   <p>If you encounter problems connecting to the server or experience any other problems, be sure your configuration for the Server Model Settings matches the model type. For example, in figure 2.6, shown earlier, the loaded model differs from the server settings. The corrected settings are shown in figure 2.7.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p75">  
   <img alt="figure" src="../Images/2-7.png" width="1007" height="547"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.7</span> Choosing the correct Server Model Settings for the loaded model</h5>
  </div> 
  <div class="readable-text" id="p76"> 
   <p>Now, you can use a locally hosted LLM or a commercial model to build, test, and potentially even run your agents. The following section will examine how to build prompts using prompt engineering more effectively.</p> 
  </div> 
  <div class="readable-text" id="p77"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_17"><span class="num-string">2.3</span> Prompting LLMs with prompt engineering</h2> 
  </div> 
  <div class="readable-text" id="p78"> 
   <p>A prompt defined for LLMs is the message content used in the request for better response output. <em>Prompt engineering</em> is a new and emerging field that attempts to structure a methodology for building prompts. Unfortunately, prompt building isn’t a well-established science, and there is a growing and diverse set of methods defined as prompt engineering.</p> 
  </div> 
  <div class="readable-text intended-text" id="p79"> 
   <p>Fortunately, organizations such as OpenAI have begun documenting a universal set of strategies, as shown in figure 2.8. These strategies cover various tactics, some requiring additional infrastructure and considerations. As such, the prompt engineering strategies relating to more advanced concepts will be covered in the indicated chapters.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p80">  
   <img alt="figure" src="../Images/2-8.png" width="1100" height="885"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.8</span> OpenAI prompt engineering strategies reviewed in this book, by chapter location</h5>
  </div> 
  <div class="readable-text intended-text" id="p81"> 
   <p>Each strategy in figure 2.8 unfolds into tactics that can further refine the specific method of prompt engineering. This chapter will examine the fundamental Write Clear Instructions strategy. Figure 2.9 shows the tactics for this strategy in more detail, along with examples for each tactic. We’ll look at running these examples using a code demo in the following sections.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p82">  
   <img alt="figure" src="../Images/2-9.png" width="1012" height="789"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.9</span> The tactics for the Write Clear Instructions strategy</h5>
  </div> 
  <div class="readable-text intended-text" id="p83"> 
   <p>The Write Clear Instructions strategy is about being careful and specific about what you ask for. Asking an LLM to perform a task is no different from asking a person to complete the same task. Generally, the more information and context relevant to a task you can specify in a request, the better the response.</p> 
  </div> 
  <div class="readable-text intended-text" id="p84"> 
   <p>This strategy has been broken down into specific tactics you can apply to prompts. To understand how to use those, a code demo (<code>prompt_engineering.py</code>) with various prompt examples is in the <code>chapter 2</code> source code folder.</p> 
  </div> 
  <div class="readable-text intended-text" id="p85"> 
   <p>Open the <code>prompt_engineering.py</code> file in VS Code, as shown in listing 2.8. This code starts by loading all the JSON Lines files in the <code>prompts</code> folder. Then, it displays the list of files as choices and allows the user to select a prompt option. After selecting the option, the prompts are submitted to an LLM, and the response is printed.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p86"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 2.8</span> <code>prompt_engineering.py</code> <code>(main())</code></h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">def main():
    directory = "prompts"
    text_files = list_text_files_in_directory(directory)   <span class="aframe-location"/> #1

    if not text_files:
        print("No text files found in the directory.")
        return

    def print_available():                                    <span class="aframe-location"/> #2
        print("Available prompt tactics:")
        for i, filename in enumerate(text_files, start=1):
            print(f"{i}. {filename}")

    while True:
        try:
            print_available()                   #2              
            choice = int(input("Enter … 0 to exit): "))          <span class="aframe-location"/> #3
            if choice == 0:
                break
            elif 1 &lt;= choice &lt;= len(text_files):
                selected_file = text_files[choice - 1]
                file_path = os.path.join(directory,
      selected_file)
                prompts = 
<span class="">↪</span> load_and_parse_json_file(file_path)                         <span class="aframe-location"/> #4
                print(f"Running prompts for {selected_file}")
                for i, prompt in enumerate(prompts):
                    print(f"PROMPT {i+1} --------------------")
                    print(prompt)
                    print(f"REPLY ---------------------------")
                    print(prompt_llm(prompt))                      <span class="aframe-location"/> #5
            else:
                print("Invalid choice. Please enter a valid number.")
        except ValueError:
            print("Invalid input. Please enter a number.")</pre> 
    <div class="code-annotations-overlay-container">
     #1 Collects all the files for the given folder
     <br/>#2 Prints the list of files as choices
     <br/>#3 Inputs the user’s choice
     <br/>#4 Loads the prompt and parses it into messages
     <br/>#5 Submits the prompt to an OpenAI LLM
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p87"> 
   <p>A commented-out section from the listing demonstrates how to connect to a local LLM. This will allow you to explore the same prompt engineering tactics applied to open source LLMs running locally. By default, this example uses the OpenAI model we configured previously in section 2.1.1. If you didn’t complete that earlier, please go back and do it before running this one.</p> 
  </div> 
  <div class="readable-text intended-text" id="p88"> 
   <p>Figure 2.10 shows the output of running the prompt engineering tactics tester, the <code>prompt_engineering.py</code> file in VS Code. When you run the tester, you can enter a value for the tactic you want to test and watch it run.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p89">  
   <img alt="figure" src="../Images/2-10.png" width="1093" height="316"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.10</span> The output of the prompt engineering tactics tester</h5>
  </div> 
  <div class="readable-text" id="p90"> 
   <p>In the following sections, we’ll explore each prompt tactic in more detail. We’ll also examine the various examples.</p> 
  </div> 
  <div class="readable-text" id="p91"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_18"><span class="num-string">2.3.1</span> Creating detailed queries</h3> 
  </div> 
  <div class="readable-text" id="p92"> 
   <p>The basic premise of this tactic is to provide as much detail as possible but also to be careful not to give irrelevant details. The following listing shows the JSON Lines file examples for exploring this tactic.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p93"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 2.9</span> <code>detailed_queries.jsonl</code> </h5> 
   <div class="code-area-container"> 
    <pre class="code-area">[                       <span class="aframe-location"/> #1
    {
        "role": "system",
        "content": "You are a helpful assistant."
    },
    {
        "role": "user",
        "content": "What is an agent?"     <span class="aframe-location"/> #2
    }
]
[
    {
        "role": "system",
        "content": "You are a helpful assistant."
    },
    {
        "role": "user",
        "content": """
What is a GPT Agent? 
Please give me 3 examples of a GPT agent
"""                                       <span class="aframe-location"/> #3
    }
]</pre> 
    <div class="code-annotations-overlay-container">
     #1 The first example doesn’t use detailed queries.
     <br/>#2 First ask the LLM a very general question.
     <br/>#3 Ask a more specific question, and ask for examples.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p94"> 
   <p>This example demonstrates the difference between using detailed queries and not. It also goes a step further by asking for examples. Remember, the more relevance and context you can provide in your prompt, the better the overall response. Asking for examples is another way of enforcing the relationship between the question and the expected output.</p> 
  </div> 
  <div class="readable-text" id="p95"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_19"><span class="num-string">2.3.2</span> Adopting personas</h3> 
  </div> 
  <div class="readable-text" id="p96"> 
   <p>Adopting personas grants the ability to define an overarching context or set of rules to the LLM. The LLM can then use that context and/or rules to frame all later output responses. This is a compelling tactic and one that we’ll make heavy use of throughout this book.</p> 
  </div> 
  <div class="readable-text intended-text" id="p97"> 
   <p>Listing 2.10 shows an example of employing two personas to answer the same question. This can be an enjoyable technique for exploring a wide range of novel applications, from getting demographic feedback to specializing in a specific task or even rubber ducking.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p98"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">GPT rubber ducking</h5> 
   </div> 
   <div class="readable-text" id="p99"> 
    <p><em>Rubber ducking</em> is a problem-solving technique in which a person explains a problem to an inanimate object, like a rubber duck, to understand or find a solution. This method is prevalent in programming and debugging, as articulating the problem aloud often helps clarify the problem and can lead to new insights or solutions.</p> 
   </div> 
   <div class="readable-text" id="p100"> 
    <p>GPT rubber ducking uses the same technique, but instead of an inanimate object, we use an LLM. This strategy can be expanded further by giving the LLM a persona specific to the desired solution domain.</p> 
   </div> 
  </div> 
  <div class="browsable-container listing-container" id="p101"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 2.10</span> <code>adopting_personas.jsonl</code> </h5> 
   <div class="code-area-container"> 
    <pre class="code-area">[
    {
        "role": "system",
        "content": """
You are a 20 year old female who attends college 
in computer science. Answer all your replies as 
a junior programmer.
"""                        <span class="aframe-location"/> #1
    },
    {
        "role": "user",
        "content": "What is the best subject to study."
    }
]
[
    {
        "role": "system",
        "content": """
You are a 38 year old male registered nurse. 
Answer all replies as a medical professional.
"""                                            <span class="aframe-location"/> #2
    },
    {
        "role": "user",
        "content": "What is the best subject to study."
    }
]</pre> 
    <div class="code-annotations-overlay-container">
     #1 First persona
     <br/>#2 Second persona
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p102"> 
   <p>A core element of agent profiles is the persona. We’ll employ various personas to assist agents in completing their tasks. When you run this tactic, pay particular attention to the way the LLM outputs the response.</p> 
  </div> 
  <div class="readable-text" id="p103"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_20"><span class="num-string">2.3.3</span> Using delimiters</h3> 
  </div> 
  <div class="readable-text" id="p104"> 
   <p>Delimiters are a useful way of isolating and getting the LLM to focus on some part of a message. This tactic is often combined with other tactics but can work well independently. The following listing demonstrates two examples, but there are several other ways of describing delimiters, from XML tags to using markdown.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p105"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 2.11</span> <code>using_delimiters.jsonl</code> </h5> 
   <div class="code-area-container"> 
    <pre class="code-area">[
    {
        "role": "system",
        "content": """
Summarize the text delimited by triple quotes 
with a haiku.
"""              <span class="aframe-location"/> #1
    },
    {
        "role": "user",
        "content": "A gold chain is cool '''but a silver chain is better'''"
    }
]
[
    {
        "role": "system",
        "content": """
You will be provided with a pair of statements 
(delimited with XML tags) about the same topic. 
First summarize the arguments of each statement. 
Then indicate which of them makes a better statement
 and explain why.
"""                       <span class="aframe-location"/> #2
    },
    {
        "role": "user",
        "content": """
&lt;statement&gt;gold chains are cool&lt;/statement&gt;
&lt;statement&gt;silver chains are better&lt;/statement&gt;
"""
    }
]</pre> 
    <div class="code-annotations-overlay-container">
     #1 The delimiter is defined by character type and repetition.
     <br/>#2 The delimiter is defined by XML standards.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p106"> 
   <p>When you run this tactic, pay attention to the parts of the text the LLM focuses on when it outputs the response. This tactic can be beneficial for describing information in a hierarchy or other relationship patterns.</p> 
  </div> 
  <div class="readable-text" id="p107"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_21"><span class="num-string">2.3.4</span> Specifying steps</h3> 
  </div> 
  <div class="readable-text" id="p108"> 
   <p>Specifying steps is another powerful tactic that can have many uses, including in agents, as shown in listing 2.12. It’s especially powerful when developing prompts or agent profiles for complex multistep tasks. You can specify steps to break down these complex prompts into a step-by-step process that the LLM can follow. In turn, these steps can guide the LLM through multiple interactions over a more extended conversation and many iterations.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p109"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 2.12</span> <code>specifying_steps.jsonl</code> </h5> 
   <div class="code-area-container"> 
    <pre class="code-area">[
    {
        "role": "system",
        "content": """
Use the following step-by-step instructions to respond to user inputs.
Step 1 - The user will provide you with text in triple single quotes. 
Summarize this text in one sentence with a prefix that says 'Summary: '.
Step 2 - Translate the summary from Step 1 into Spanish, 
with a prefix that says 'Translation: '.
"""                                         <span class="aframe-location"/> #1
    },
    {
        "role": "user",
        "content": "'''I am hungry and would like to order an appetizer.'''"
    }
]
[
    {
        "role": "system",
        "content": """
Use the following step-by-step instructions to respond to user inputs.
Step 1 - The user will provide you with text. Answer any questions in 
the text in one sentence with a prefix that says 'Answer: '.

Step 2 - Translate the Answer from Step 1 into a dad joke,
 with a prefix that says 'Dad Joke: '."""                     <span class="aframe-location"/> #2
    },
    {
        "role": "user",
        "content": "What is the tallest structure in Paris?"
    }
]</pre> 
    <div class="code-annotations-overlay-container">
     #1 Notice the tactic of using delimiters.
     <br/>#2 Steps can be completely different operations.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p110"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_22"><span class="num-string">2.3.5</span> Providing examples</h3> 
  </div> 
  <div class="readable-text" id="p111"> 
   <p>Providing examples is an excellent way to guide the desired output of an LLM. There are numerous ways to demonstrate examples to an LLM. The system message/prompt can be a helpful way to emphasize general output. In the following listing, the example is added as the last LLM assistant reply, given the prompt “Teach me about Python.”</p> 
  </div> 
  <div class="browsable-container listing-container" id="p112"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 2.13</span> <code>providing_examples.jsonl</code> </h5> 
   <div class="code-area-container"> 
    <pre class="code-area">[
    {
        "role": "system",
        "content": """
Answer all replies in a consistent style that follows the format, 
length and style of your previous responses.
Example:
  user:
       Teach me about Python.
  assistant:                                               <span class="aframe-location"/> #1
       Python is a programming language developed in 1989
 by Guido van Rossum.

  Future replies:
       The response was only a sentence so limit
 all future replies to a single sentence.
"""                                          <span class="aframe-location"/> #2
    },
    {
        "role": "user",
        "content": "Teach me about Java."
    }
]</pre> 
    <div class="code-annotations-overlay-container">
     #1 Injects the sample output as the “previous” assistant reply
     <br/>#2 Adds a limit output tactic to restrict the size of the output and match the example
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p113"> 
   <p>Providing examples can also be used to request a particular output format from a complex series of tasks that derive the output. For example, asking an LLM to produce code that matches a sample output is an excellent use of examples. We’ll employ this tactic throughout the book, but other methods exist for guiding output.</p> 
  </div> 
  <div class="readable-text" id="p114"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_23"><span class="num-string">2.3.6</span> Specifying output length</h3> 
  </div> 
  <div class="readable-text" id="p115"> 
   <p>The tactic of specifying output length can be helpful in not just limiting tokens but also in guiding the output to a desired format. Listing 2.14 shows an example of using two different techniques for this tactic. The first limits the output to fewer than 10 words. This can have the added benefit of making the response more concise and directed, which can be desirable for some use cases. The second example demonstrates limiting output to a concise set of bullet points. This method can help narrow down the output and keep answers short. More concise answers generally mean the output is more focused and contains less filler.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p116"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 2.14</span> <code>specifying_output_length.jsonl</code> </h5> 
   <div class="code-area-container"> 
    <pre class="code-area">[
    {
        "role": "system",
        "content": """
Summarize all replies into 10 or fewer words.
"""                                              <span class="aframe-location"/> #1
    },
    {
        "role": "user",
        "content": "Please tell me an exciting fact about Paris?"
    }
]
[
    {
        "role": "system",
        "content": """
Summarize all replies into 3 bullet points.
"""                                          <span class="aframe-location"/> #2
    },
    {
        "role": "user",
        "content": "Please tell me an exciting fact about Paris?"
    }
]</pre> 
    <div class="code-annotations-overlay-container">
     #1 Restricting the output makes the answer more concise.
     <br/>#2 Restricts the answer to a short set of bullets
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p117"> 
   <p>Keeping answers brief can have additional benefits when developing multi-agent systems. Any agent system that converses with other agents can benefit from more concise and focused replies. It tends to keep the LLM more focused and reduces noisy communication.</p> 
  </div> 
  <div class="readable-text intended-text" id="p118"> 
   <p>Be sure to run through all the examples of the prompt tactics for this strategy. As mentioned, we’ll cover other prompt engineering strategies and tactics in future chapters. We’ll finish this chapter by looking at how to pick the best LLM for your use case.</p> 
  </div> 
  <div class="readable-text" id="p119"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_24"><span class="num-string">2.4</span> Choosing the optimal LLM for your specific needs</h2> 
  </div> 
  <div class="readable-text" id="p120"> 
   <p>While being a successful crafter of AI agents doesn’t require an in-depth understanding of LLMs, it’s helpful to be able to evaluate the specifications. Like a computer user, you don’t need to know how to build a processor to understand the differences in processor models. This analogy holds well for LLMs, and while the criteria may be different, it still depends on some primary considerations.</p> 
  </div> 
  <div class="readable-text intended-text" id="p121"> 
   <p>From our previous discussion and look at LM Studio, we can extract some fundamental criteria that will be important to us when considering LLMs. <span class="aframe-location"/>Figure 2.11 explains the essential criteria to define what makes an LLM worth considering for creating a GPT agent or any LLM task.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p122">  
   <img alt="figure" src="../Images/2-11.png" width="1012" height="889"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.11</span> The important criteria to consider when consuming an LLM</h5>
  </div> 
  <div class="readable-text intended-text" id="p123"> 
   <p>For our purposes of building AI agents, we need to look at each of these criteria in terms related to the task. Model context size and speed could be considered the sixth and seventh criteria, but they are usually considered variations of a model deployment architecture and infrastructure. An eighth criterion to consider for an LLM is cost, but this depends on many other factors. Here is a summary of how these criteria relate to building AI agents:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p124"> <em>Model performance</em> —You’ll generally want to understand the LLM’s performance for a given set of tasks. For example, if you’re building an agent specific to coding, then an LLM that performs well on code will be essential. </li> 
   <li class="readable-text" id="p125"> <em>Model parameters (size)</em> —The size of a model is often an excellent indication of inference performance and how well the model responds. However, the size of a model will also dictate your hardware requirements. If you plan to use your own locally hosted model, the model size will also primarily dictate the computer and GPU you need. Fortunately, we’re seeing small, very capable open source models being released regularly. </li> 
   <li class="readable-text" id="p126"> <em>Use case (model type)</em> —The type of model has several variations. Chat completions models such as ChatGPT are effective for iterating and reasoning through a problem, whereas models such as completion, question/answer, and instruct are more related to specific tasks. A chat completions model is essential for agent applications, especially those that iterate. </li> 
   <li class="readable-text" id="p127"> <em>Training input</em> —Understanding the content used to train a model will often dictate the domain of a model. While general models can be effective across tasks, more specific or fine-tuned models can be more relevant to a domain. This may be a consideration for a domain-specific agent where a smaller, more fine-tuned model may perform as well as or better than a larger model such as GPT-4. </li> 
   <li class="readable-text" id="p128"> <em>Training method</em> —It’s perhaps less of a concern, but it can be helpful to understand what method was used to train a model. How a model is trained can affect its ability to generalize, reason, and plan. This can be essential for planning agents but perhaps less significant for agents than for a more task-specific assistant. </li> 
   <li class="readable-text" id="p129"> <em>Context token size</em> —The context size of a model is more specific to the model architecture and type. It dictates the size of context or memory the model may hold. A smaller context window of less than 4,000 tokens is typically more than enough for simple tasks. However, a large context window can be essential when using multiple agents—all conversing over a task. The models will typically be deployed with variations on the context window size. </li> 
   <li class="readable-text" id="p130"> <em>Model speed (model deployment)</em> —The speed of a model is dictated by its <em>inference speed</em> (or how fast a model replies to a request), which in turn is dictated by the infrastructure it runs on. If your agent isn’t directly interacting with users, raw real-time speed may not be necessary. On the other hand, an LLM agent interacting in real time needs to be as quick as possible. For commercial models, speed will be determined and supported by the provider. Your infrastructure will determine the speed for those wanting to run their LLMs. </li> 
   <li class="readable-text" id="p131"> <em>Model cost (project budget)</em> —The cost is often dictated by the project. Whether learning to build an agent or implementing enterprise software, cost is always a consideration. A significant tradeoff exists between running your LLMs versus using a commercial API. </li> 
  </ul> 
  <div class="readable-text" id="p132"> 
   <p>There is a lot to consider when choosing which model you want to build a production agent system on. However, picking and working with a single model is usually best for research and learning purposes. If you’re new to LLMs and agents, you’ll likely want to choose a commercial option such as GPT-4 Turbo. Unless otherwise stated, the work in this book will depend on GPT-4 Turbo.</p> 
  </div> 
  <div class="readable-text intended-text" id="p133"> 
   <p>Over time, models will undoubtedly be replaced by better models. So you may need to upgrade or swap out models. To do this, though, you must understand the performance metrics of your LLMs and agents. Fortunately, in chapter 9, we’ll explore evaluating LLMs, prompts, and agent profiles with prompt flow.</p> 
  </div> 
  <div class="readable-text" id="p134"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_25"><span class="num-string">2.5</span> Exercises</h2> 
  </div> 
  <div class="readable-text" id="p135"> 
   <p>Use the following exercises to help you engage with the material in this chapter:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p136"> <em>Exercise 1</em>—Consuming Different LLMs </li> 
  </ul> 
  <div class="readable-text list-body-item" id="p137"> 
   <p><em>Objective </em>—Use the <code>connecting.py</code> code example to consume a different LLM from OpenAI or another provider.</p> 
  </div> 
  <div class="readable-text list-body-item" id="p138"> 
   <p><em>Tasks</em>:</p> 
  </div> 
  <ul> 
   <li class=" buletless-item" style="list-style-type: none;"> 
    <ul> 
     <li class="readable-text" id="p139"> Modify <code>connecting.py</code> to connect to a different LLM. </li> 
     <li class="readable-text" id="p140"> Choose an LLM from OpenAI or another provider. </li> 
     <li class="readable-text" id="p141"> Update the API keys and endpoints in the code. </li> 
     <li class="readable-text" id="p142"> Execute the modified code and validate the response. </li> 
    </ul></li> 
   <li class="readable-text" id="p143"> <em>Exercise 2</em>—Exploring Prompt Engineering Tactics </li> 
  </ul> 
  <div class="readable-text list-body-item" id="p144"> 
   <p><em>Objective </em>—Explore various prompt engineering tactics, and create variations for each.</p> 
  </div> 
  <div class="readable-text list-body-item" id="p145"> 
   <p><em>Tasks:</em></p> 
  </div> 
  <ul> 
   <li class=" buletless-item" style="list-style-type: none;"> 
    <ul> 
     <li class="readable-text" id="p146"> Review the prompt engineering tactics covered in the chapter. </li> 
     <li class="readable-text" id="p147"> Write variations for each tactic, experimenting with different phrasing and structures. </li> 
     <li class="readable-text" id="p148"> Test the variations with an LLM to observe different outcomes. </li> 
     <li class="readable-text" id="p149"> Document the results, and analyze the effectiveness of each variation. </li> 
    </ul></li> 
   <li class="readable-text" id="p150"> <em>Exercise 3</em>—Downloading and Running an LLM with LM Studio </li> 
  </ul> 
  <div class="readable-text list-body-item" id="p151"> 
   <p><em>Objective </em>—Download an LLM using LM Studio, and connect it to prompt engineering tactics.</p> 
  </div> 
  <div class="readable-text list-body-item" id="p152"> 
   <p><em>Tasks:</em></p> 
  </div> 
  <ul> 
   <li class=" buletless-item" style="list-style-type: none;"> 
    <ul> 
     <li class="readable-text" id="p153"> Install LM Studio on your machine. </li> 
     <li class="readable-text" id="p154"> Download an LLM using LM Studio. </li> 
     <li class="readable-text" id="p155"> Serve the model using LM Studio. </li> 
     <li class="readable-text" id="p156"> Write Python code to connect to the served model. </li> 
     <li class="readable-text" id="p157"> Integrate the prompt engineering tactics example with the served model. </li> 
    </ul></li> 
   <li class="readable-text" id="p158"> <em>Exercise 4</em>—Comparing Commercial and Open source LLMs </li> 
  </ul> 
  <div class="readable-text list-body-item" id="p159"> 
   <p><em>Objective </em>—Compare the performance of a commercial LLM such as GPT-4 Turbo with an open source model using prompt engineering examples.</p> 
  </div> 
  <div class="readable-text list-body-item" id="p160"> 
   <p><em>Tasks:</em></p> 
  </div> 
  <ul> 
   <li class=" buletless-item" style="list-style-type: none;"> 
    <ul> 
     <li class="readable-text" id="p161"> Implement the prompt engineering examples using GPT-4 Turbo. </li> 
     <li class="readable-text" id="p162"> Repeat the implementation using an open source LLM. </li> 
     <li class="readable-text" id="p163"> Evaluate the models based on criteria such as response accuracy, coherence, and speed. </li> 
     <li class="readable-text" id="p164"> Document the evaluation process, and summarize the findings. </li> 
    </ul></li> 
   <li class="readable-text" id="p165"> <em>Exercise 5</em>—Hosting Alternatives for LLMs </li> 
  </ul> 
  <div class="readable-text list-body-item" id="p166"> 
   <p><em>Objective </em>—Contrast and compare alternatives for hosting an LLM versus using a commercial model.</p> 
  </div> 
  <div class="readable-text list-body-item" id="p167"> 
   <p><em>Tasks:</em></p> 
  </div> 
  <ul> 
   <li class=" buletless-item" style="list-style-type: none;"> 
    <ul> 
     <li class="readable-text" id="p168"> Research different hosting options for LLMs (e.g., local servers, cloud services). </li> 
     <li class="readable-text" id="p169"> Evaluate the benefits and drawbacks of each hosting option. </li> 
     <li class="readable-text" id="p170"> Compare these options to using a commercial model in terms of cost, performance, and ease of use. </li> 
     <li class="readable-text" id="p171"> Write a report summarizing the comparison and recommending the best approach based on specific use cases. </li> 
    </ul></li> 
  </ul> 
  <div class="readable-text" id="p172"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_26">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p173"> LLMs use a type of architecture called generative pretrained transformers (GPTs). </li> 
   <li class="readable-text" id="p174"> Generative models (e.g., LLMs and GPTs) differ from predictive/classification models by learning how to represent data and not simply classify it. </li> 
   <li class="readable-text" id="p175"> LLMs are a collection of data, architecture, and training for specific use cases, called <em>fine-tuning.</em> </li> 
   <li class="readable-text" id="p176"> The OpenAI API SDK can be used to connect to an LLM from models, such as GPT-4, and also used to consume open source LLMs. </li> 
   <li class="readable-text" id="p177"> You can quickly set up Python environments and install the necessary packages for LLM integration. </li> 
   <li class="readable-text" id="p178"> LLMs can handle various requests and generate unique responses that can be used to enhance programming skills related to LLM integration. </li> 
   <li class="readable-text" id="p179"> Open source LLMs are an alternative to commercial models and can be hosted locally using tools such as LM Studio. </li> 
   <li class="readable-text" id="p180"> Prompt engineering is a collection of techniques that help craft more effective prompts to improve LLM responses. </li> 
   <li class="readable-text" id="p181"> LLMs can be used to power agents and assistants, from simple chatbots to fully capable autonomous workers. </li> 
   <li class="readable-text" id="p182"> Selecting the most suitable LLM for specific needs depends on the performance, parameters, use case, training input, and other criteria. </li> 
   <li class="readable-text" id="p183"> Running LLMs locally requires a variety of skills, from setting up GPUs to understanding various configuration options. </li> 
  </ul>
 </div></div></body></html>