["```py\nimport torch\n\nsentences = [\n    'Today is a sunny day',\n    'Today is a rainy day'\n]\n\n# Tokenization function\ndef tokenize(text):\n    return text.lower().split()\n\n# Build the vocabulary\ndef build_vocab(sentences):\n    vocab = {}\n    for sentence in sentences:\n        tokens = tokenize(sentence)\n        for token in tokens:\n            if token not in vocab:\n                vocab[token] = len(vocab) + 1 \n    return vocab\n\n# Create the vocabulary index\nvocab = build_vocab(sentences)\n\nprint(\"Vocabulary Index:\", vocab)\n```", "```py\nVocabulary Index: {'today': 1, 'is': 2, 'a': 3, 'sunny': 4, 'day': 5, 'rainy': 6}\n```", "```py\n!pip install transformers\n```", "```py\nfrom transformers import BertTokenizerFast\n\nsentences = [\n    'Today is a sunny day',\n    'Today is a rainy day'\n]\n\n# Initialize the tokenizer\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n\n# Tokenize the sentences and encode them\nencoded_inputs = tokenizer(sentences, padding=True, truncation=True, \n                           return_tensors='pt')\n\n# To see the tokens for each input (helpful for understanding the output)\ntokens = [tokenizer.convert_ids_to_tokens(ids) \n           for ids in encoded_inputs[\"input_ids\"]]\n\n# To get the word index similar to Keras' tokenizer\nword_index = tokenizer.get_vocab()\n\nprint(\"Tokens:\", tokens)\nprint(\"Token IDs:\", encoded_inputs['input_ids'])\nprint(\"Word Index:\", dict(list(word_index.items())[:10]))  \n# show only the first 10 for brevity\n\n```", "```py\nTokens: [['[CLS]', 'today', 'is', 'a', 'sunny', 'day', '[SEP]'], \n         ['[CLS]', 'today', 'is', 'a', 'rainy', 'day', '[SEP]']]\n\nToken IDs: tensor([\n        [  101,  2651,  2003,  1037, 11559,  2154,   102],\n        [  101,  2651,  2003,  1037, 16373,  2154,   102]])\n\nWord Index: {'protestant': 8330, 'initial': 3988, '##pt': 13876, \n             'charters': 23010, '243': 22884, 'ref': 25416, '##dies': 18389, \n             '##uchi': 15217, 'sainte': 16947, 'annette': 22521}\n```", "```py\n# Tokenize the sentences and encode them\nencoded_inputs = tokenizer(sentences, padding=True, truncation=True, \n                           return_tensors='pt')\n\n```", "```py\n# To see the tokens for each input (helpful for understanding the output)\ntokens = [tokenizer.convert_ids_to_tokens(ids) \n           for ids in encoded_inputs[\"input_ids\"]]\n```", "```py\nTokens: [['[CLS]', 'today', 'is', 'a', 'sunny', 'day', '[SEP]'], \n         ['[CLS]', 'today', 'is', 'a', 'rainy', 'day', '[SEP]']]\n```", "```py\nToken IDs: tensor([\n        [  101,  2651,  2003,  1037, 11559,  2154,   102],\n        [  101,  2651,  2003,  1037, 16373,  2154,   102]])\n```", "```py\ndef text_to_sequence(text, vocab):\n    return [vocab.get(token, 0) for token in tokenize(text)]  \n# 0 for unknown words\n\n```", "```py\nVocabulary Index: {'today': 1, 'is': 2, 'a': 3, 'sunny': 4, 'day': 5, 'rainy': 6}\n```", "```py\n[1, 2, 3, 4, 5]\n[1, 2, 3, 6, 5]\n```", "```py\ntest_data = [\n    'Today is a snowy day',\n    'Will it be rainy tomorrow?'\n]\n```", "```py\nfor test_sentence in test_data:\n  test_seq = text_to_sequence(test_sentence, vocab)\n  print(test_seq)\n```", "```py\n[1, 2, 3, 0, 5]\n[0, 0, 0, 6, 0]\n```", "```py\nsentences = [\n    'Today is a sunny day',\n    'Today is a rainy day',\n    'Is it sunny today?',\n    'I really enjoyed walking in the snow today'\n]\n```", "```py\n[1, 2, 3, 4, 5]\n[1, 2, 3, 6, 5]\n[2, 0, 4, 0]\n[0, 0, 0, 0, 0, 0, 0, 1]\n```", "```py\nvocab = build_vocab(sentences)\n```", "```py\n[1, 2, 3, 4, 5]\n[1, 2, 3, 6, 5]\n[2, 7, 4, 8]\n[9, 10, 11, 12, 13, 14, 15, 1]\n```", "```py\ndef pad_sequences(sequences, maxlen):\n    return [seq + [0] * (maxlen - len(seq)) if len(seq) < maxlen \n            else seq[:maxlen] for seq in sequences]\n```", "```py\nfor sentence in sentences:\n  seq = text_to_sequence(sentence, vocab)\n  padded_seq = pad_sequences([seq], maxlen=10)  # Example maxlen\n  print(padded_seq)\n```", "```py\n[[1, 2, 3, 4, 5, 0, 0, 0, 0, 0]]\n[[1, 2, 3, 6, 5, 0, 0, 0, 0, 0]]\n[[2, 7, 4, 8, 0, 0, 0, 0, 0, 0]]\n[[9, 10, 11, 12, 13, 14, 15, 1, 0, 0]]\n```", "```py\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(sentence)\nsentence = soup.get_text()\n```", "```py\nstopwords = [\"a\", \"about\", \"above\", ... \"yours\", \"yourself\", \"yourselves\"]\n```", "```py\nwords = sentence.split()\nfiltered_sentence = \"\"\nfor word in words:\n    if word not in stopwords:\n        filtered_sentence = filtered_sentence + word + \" \"\nsentences.append(filtered_sentence)\n```", "```py\nimport string\ntable = str.maketrans('', '', string.punctuation)\nwords = sentence.split()\nfiltered_sentence = \"\"\nfor word in words:\n    word = word.translate(table)\n    if word not in stopwords:\n        filtered_sentence = filtered_sentence + word + \" \"\nsentences.append(filtered_sentence)\n```", "```py\nimport os\nimport urllib.request\nimport tarfile\n\ndef download_and_extract(url, destination):\n    if not os.path.exists(destination):\n        os.makedirs(destination, exist_ok=True)\n    file_path = os.path.join(destination, \"aclImdb_v1.tar.gz\")\n\n    if not os.path.exists(file_path):\n        print(\"Downloading the dataset...\")\n        urllib.request.urlretrieve(url, file_path)\n        print(\"Download complete.\")\n\n    if \"aclImdb\" not in os.listdir(destination):\n        print(\"Extracting the dataset...\")\n        with tarfile.open(file_path, 'r:gz') as tar:\n            tar.extractall(path=destination)\n        print(\"Extraction complete.\")\n\n# URL for the dataset\ndataset_url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\ndownload_and_extract(dataset_url, \"./data\")\n```", "```py\nfrom collections import Counter\nimport os\n\n# Simple tokenizer\ndef tokenize(text):\n    return text.lower().split()\n\n# Build vocabulary\ndef build_vocab(path):\n    counter = Counter()\n    for folder in [\"pos\", \"neg\"]:\n        folder_path = os.path.join(path, folder)\n        for filename in os.listdir(folder_path):\n            with open(os.path.join(folder_path, filename), 'r', \n                                   encoding='utf-8') as file:\n                counter.update(tokenize(file.read()))\n    return {word: i+1 for i, word in enumerate(counter)} # Starting index from 1\n\nvocab = build_vocab(\"./data/aclImdb/train/\")\n```", "```py\n{'a': 1, 'year': 2, 'or': 3, 'so': 4, 'ago,': 5, 'i': 6, 'was': 7…\n```", "```py\ndef text_to_sequence(text, vocab):\n    return [vocab.get(token, 0) for token in tokenize(text)]  # 0 for unknown\n\ndef pad_sequences(sequences, maxlen):\n    return [seq + [0] * (maxlen - len(seq)) \n           if len(seq) < maxlen else seq[:maxlen] for seq in sequences]\n\n# Example use\ntext = \"This is an example.\"\nseq = text_to_sequence(text, vocab)\npadded_seq = pad_sequences([seq], maxlen=256)  # Example maxlen\nprint(seq)\n```", "```py\n# Build vocabulary\ndef build_vocab(path):\n    counter = Counter()\n    for folder in [\"pos\", \"neg\"]:\n        folder_path = os.path.join(path, folder)\n        for filename in os.listdir(folder_path):\n            with open(os.path.join(folder_path, filename), 'r', \n                                   encoding='utf-8') as file:\n                counter.update(tokenize(file.read()))\n\n    # Sort words by frequency in descending order\n    sorted_words = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n\n    # Create vocabulary with indices starting from 1\n    vocab = {word: idx + 1 for idx, (word, _) in enumerate(sorted_words)}\n    vocab['<pad>'] = 0  # Add padding token with index 0\n    return vocab\n```", "```py\n{'the': 1, 'a': 2, 'and': 3, 'of': 4, 'to': 5, 'is': 6, 'in': 7, 'i': 8, \n'this': 9, 'that': 10, 'it': 11, '/><br': 12, 'was': 13, 'as': 14,\n'for': 15, 'with': 16, 'but': 17, 'on': 18, 'movie': 19, 'his': 20,\n```", "```py\n# Simple tokenizer\nfrom bs4 import BeautifulSoup\n\n# Note that the list of stopwords is defined in the source code. \n# It’s an array of words. You can define your own or just get the one from \n# the book’s github.\n\ndef tokenize(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    cleaned_text = soup.get_text()  # Extract text from HTML\n    return [word.lower() for word in cleaned_text.split() if word.lower() \n            not in stopwords]\n```", "```py\n{'movie': 1, 'not': 2, 'film': 3, 'one': 4, 'like': 5, 'just': 6, \"it's\": 7, \n 'even': 8, 'good': 9, 'no': 10, 'really': 11, 'can': 12, 'see': 13, '-': 14, \n 'get': 15, 'will': 16, 'much': 17, 'story': 18, 'also': 19, 'first': 20\n```", "```py\nsentences = [\n    'Today is a sunny day',\n    'Today is a rainy day',\n    'Is it sunny today?'\n]\n\n[[1094, 6112, 246, 0, 0, 0, 0, 0]]\n[[1094, 6730, 246, 0, 0, 0, 0, 0]]\n[[6112, 25065, 0, 0, 0, 0, 0, 0]]\n\n```", "```py\nreverse_word_index = dict(\n    [(value, key) for (key, value) in vocab.items()])\n\ndecoded_review = ' '.join([reverse_word_index.get(i, '?') for i in seq])\n\nprint(decoded_review)\n\n```", "```py\ntoday sunny day\n```", "```py\nimport csv\nsentences=[]\nlabels=[]\nwith open('/tmp/binary-emotion.csv', encoding='UTF-8') as csvfile:\n    reader = csv.reader(csvfile, delimiter=\",\")\n    for row in reader:\n        labels.append(int(row[0]))\n        sentence = row[1].lower()\n        sentence = sentence.replace(\",\", \" , \")\n        sentence = sentence.replace(\".\", \" . \")\n        sentence = sentence.replace(\"-\", \" - \")\n        sentence = sentence.replace(\"/\", \" / \")\n        soup = BeautifulSoup(sentence)\n        sentence = soup.get_text()\n        words = sentence.split()\n        filtered_sentence = \"\"\n        for word in words:\n            word = word.translate(table)\n            if word not in stopwords:\n                filtered_sentence = filtered_sentence + word + \" \"\n        sentences.append(filtered_sentence)\n```", "```py\ntraining_size = 28000\n\ntraining_sentences = sentences[0:training_size]\ntesting_sentences = sentences[training_size:]\ntraining_labels = labels[0:training_size]\ntesting_labels = labels[training_size:]\n```", "```py\nfrom collections import Counter\n\n# Assuming the tokenize function is defined elsewhere\ndef tokenize(text):\n    # Tokenization logic, removing HTML and stopwords as discussed earlier\n    soup = BeautifulSoup(text, \"html.parser\")\n    cleaned_text = soup.get_text()\n    tokens = cleaned_text.lower().split()\n    filtered_tokens = [token for token in tokens if token not in stopwords]\n    return filtered_tokens\n\ndef build_vocab(sentences):\n    counter = Counter()\n    for text in sentences:\n        counter.update(tokenize(text))\n\n    # Sort words by frequency in descending order\n    sorted_words = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n\n    # Create vocabulary with indices starting from 1\n    vocab = {word: idx + 1 for idx, (word, _) in enumerate(sorted_words)}\n    vocab['<pad>'] = 0  # Add padding token with index 0\n    return vocab\n\nvocab = build_vocab(training_sentences)\nprint(vocab)\n```", "```py\nprint(testing_sentences[1])\nseq = text_to_sequence(testing_sentences[1], vocab)\nprint(seq)\n```", "```py\nmade many new friends twitter around usa another bike across usa trip amazing \n see people\n[146, 259, 30, 110, 53, 198, 2161, 111, 752, 970, 2161, 407, 217, 26, 73]\n```", "```py\n{\"firstName\" : \"Laurence\",\n \"lastName\" : \"Moroney\"}\n```", "```py\n[\n {\"firstName\" : \"Laurence\",\n \"lastName\" : \"Moroney\"},\n {\"firstName\" : \"Sharon\",\n \"lastName\" : \"Agathon\"}\n]\n```", "```py\n[\n {\"firstName\" : \"Laurence\",\n \"lastName\" : \"Moroney\",\n \"emails\": [\"lmoroney@gmail.com\", \"lmoroney@galactica.net\"]\n },\n {\"firstName\" : \"Sharon\",\n \"lastName\" : \"Agathon\",\n \"emails\": [\"sharon@galactica.net\", \"boomer@cylon.org\"]\n }\n]\n```", "```py\n{\"is_sarcastic\": 1 or 0, \n \"headline\": String containing headline, \n \"article_link\": String Containing link}\n```", "```py\nimport json\nwith open(\"/tmp/sarcasm.json\", 'r') as f:\n    datastore = json.load(f)\n    for item in datastore:\n        sentence = item['headline'].lower()\n        label= item['is_sarcastic']\n        link = item['article_link']\n```", "```py\nwith open(\"/tmp/sarcasm.json\", 'r') as f:\n    datastore = json.load(f)\n\nsentences = [] \nlabels = []\nurls = []\nfor item in datastore:\n    sentence = item['headline'].lower()\n    sentence = sentence.replace(\",\", \" , \")\n    sentence = sentence.replace(\".\", \" . \")\n    sentence = sentence.replace(\"-\", \" - \")\n    sentence = sentence.replace(\"/\", \" / \")\n    soup = BeautifulSoup(sentence)\n    sentence = soup.get_text()\n    words = sentence.split()\n    filtered_sentence = \"\"\n    for word in words:\n        word = word.translate(table)\n        if word not in stopwords:\n            filtered_sentence = filtered_sentence + word + \" \"\n    sentences.append(filtered_sentence)\n    labels.append(item['is_sarcastic'])\n    urls.append(item['article_link'])\n```", "```py\ntraining_size = 23000\n\ntraining_sentences = sentences[0:training_size]\ntesting_sentences = sentences[training_size:]\ntraining_labels = labels[0:training_size]\ntesting_labels = labels[training_size:]\n```"]