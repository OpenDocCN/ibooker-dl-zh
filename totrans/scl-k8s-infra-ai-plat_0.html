<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" class="pagenumrestart" data-pdf-bookmark="Chapter 1. Introduction"><div class="chapter" id="ch01_introduction_1738498450407954">
      <h1><span class="label">Chapter 1. </span>Introduction</h1>
      <p>AI, especially generative AI, has followed a similar adoption cycle to many new technologies. Those organizations producing cutting-edge technologies and those in or adjacent to the tech space typically have been quicker to adopt this technology, while other industries have had a slower rate of adoption. However, more recent advancements have helped business leaders realize that they must figure out how to leverage generative AI for their businesses or risk being left behind by their competitors.</p>
      <p>Now, enterprises are expending more resources to leverage AI for their businesses. This often takes the form of teams of data scientists implementing proofs of concept (POCs) of AI-based applications for their businesses.</p>
      <p>A particularly common proof of concept project in the enterprise is building chatbots. Typically, these projects make use of retrieval-augmented generation (RAG), combining proprietary data with off-the-shelf large language models (LLMs) to give the chatbot expertise on a specific problem domain.</p>
      <p>But these enterprises are often facing a difficult challenge: they have POCs of AI-based applications for their businesses, but they can’t move them into production. In fact, the <a href="https://oreil.ly/X1kML">vast majority of these projects never make it to production</a>.</p>
      <p>To improve the success rate of these POCs and improve the return on investment of AI initiatives in the enterprise, businesses must develop a better understanding of the challenges that arise when running AI-enabled applications in production. With this understanding, leaders will be better able to architect solutions for promoting POCs into production and managing their product <span class="keep-together">lifecycles</span>.</p>
      <p>Addressing and overcoming these challenges is at the core of the relatively recent discipline of <em>machine learning operations (MLOps)</em>. This publication will walk you through why this is a critical next step and how to leverage MLOps on Kubernetes.</p>
      <p>In this report, we’ll unpack four fundamentals of building AI-powered applications:</p>
      <ul>
        <li>
          <p>Training models in the experimental phase</p>
        </li>
        <li>
          <p>Making model creation repeatable and declarative</p>
        </li>
        <li>
          <p>Operating models in production as a part of AI-powered <span class="keep-together">applications</span></p>
        </li>
        <li>
          <p>Ensuring that models you create are trustworthy and built responsibly</p>
        </li>
      </ul>
      <p>This report will take a Kubernetes-centric approach, highlighting projects that are built to be Kubernetes native and when used together allow you to apply MLOps principles and practices to building AI-powered applications.</p>
      <section data-type="sect1" data-pdf-bookmark="What Is MLOps?"><div class="sect1" id="ch01_what_is_mlops_1738498450408122">
        <h1>What Is MLOps?</h1>
        <p>MLOps has its origin in the world of <em>DevOps</em>, a best-practice development model that seeks to deliver high-quality software to production quickly. It seeks to do this by bringing development and operations roles closer together. This fosters collaboration and shared knowledge across the software development and production lifecycles while bringing awareness of production issues to the teams and individuals best equipped to solve them. This approach requires that developers concern themselves with how the software they write performs in production and that they’re actively involved in operating that software in production as well.</p>
        <p>With the proliferation of AI/ML and an ever-increasing number of models being created, MLOps has emerged as a new paradigm for delivering high-quality models to production quickly, applying <a href="https://oreil.ly/0QBop">DevOps principles</a> to AI models and <span class="keep-together">AI-powered</span> <span class="keep-together">applications</span> instead of traditional software applications. However, MLOps doesn’t just apply DevOps principles to the AI development lifecycle but builds upon them to define foundational best practices for building and running AI-powered applications. A team implementing MLOps practices should adhere to the following core principles, which are expanded upon and explained in-depth in the book <a class="orm:hideurl" href="https://learning.oreilly.com/library/view/designing-machine-learning/9781098107956"><em>Designing Machine Learning Systems</em></a> by Chip Huyen (O’Reilly, 2022):</p>
        <dl>
          <dt>Continuous integration and delivery (CI/CD)</dt>
          <dd>
            <p>A robust suite of CI/CD automation tools to repeatably build, test, and deploy AI-powered applications.</p>
          </dd>
          <dt>Exploratory workflow orchestration</dt>
          <dd>
            <p>A robust data science workflow orchestration tool to automate the end-to-end model development lifecycle from data preparation through model training, tuning, and evaluation.</p>
          </dd>
          <dt>Reproducible artifacts</dt>
          <dd>
            <p>Artifacts from a given version of an intelligent application must be made reproducible, and all components used to create these artifacts must be versioned and well-documented.</p>
          </dd>
          <dt>Cross-team collaboration</dt>
          <dd>
            <p>Building AI-powered applications requires strong collaboration between multiple roles consisting of, at a minimum, data engineers, data scientists, application developers, and operations teams. MLOps emphasizes close communication and collaboration between these groups.</p>
          </dd>
          <dt>Model and data lineage</dt>
          <dd>
            <p>Model and data lineage along with other key metadata for an intelligent application must be well tracked, especially for the purpose of building trust in AI applications but also for debugging and explainability.</p>
          </dd>
          <dt>Monitoring</dt>
          <dd>
            <p>MLOps requires that AI-powered applications be monitored across their production lifecycle. In addition to traditional application monitoring, AI-powered applications must be monitored for data distribution drifts, model degradation, bias, compliance, and more. Because many models also use expensive specialized hardware, such as GPU clusters, monitoring for the efficient use of this hardware is critically important as well.</p>
          </dd>
          <dt>Iteration-supporting process</dt>
          <dd>
            <p>MLOps processes must allow for frequent iterations throughout the development and production lifecycles of an intelligent application. Data scientists must be able to train a model, evaluate its performance, and quickly retrain the model based on results of the evaluation. Similarly, models must be periodically retrained after they are released to production in order to incorporate new data in their training sets, as new data may diverge from the original training data. This divergence can be caught as it happens by adhering to the previously mentioned monitoring principle.</p>
          </dd>
        </dl>
        <p>Now that we understand the foundational principles, let’s consider the AI development lifecycle. While this lifecycle can take many forms, all tend to roughly follow this pattern (illustrated in <span class="keep-together"><a data-type="xref" href="#ch01_figure_1_1738498450402392">Figure 1-1</a></span>):</p>
        <dl>
          <dt>Project initiation</dt>
          <dd>
            <p>Business stakeholders, application developers, data scientists, and data engineers collaborate to identify the desired business outcome for the intelligent application, the raw data that can be used to build the model that will achieve this outcome, and the architecture for how the resulting AI model will be integrated into a software application to deliver the desired solution.</p>
          </dd>
          <dt>Data preparation</dt>
          <dd>
            <p>Data engineers and data scientists produce the necessary training and tuning data artifacts upon which to build the model. These artifacts do not need to be static: they could be real-time data or database entries that are constantly updating from data ingestion pipelines, for example.</p>
          </dd>
          <dt>Model experimentation</dt>
          <dd>
            <p>Data scientists consume the prepared feature data to create a sufficiently performant model, frequently iterating on different model architectures, training hyperparameters, and feature data combinations.</p>
          </dd>
          <dt>Application integration</dt>
          <dd>
            <p>Application developers work closely with data scientists to integrate a trained model into application code, which will consume the trained model via an API.</p>
          </dd>
          <dt>Production service</dt>
          <dd>
            <p>The application is promoted to production, where it adds value and is continuously iterated upon to improve its performance and add new features.</p>
          </dd>
        </dl>
        <figure><div id="ch01_figure_1_1738498450402392" class="figure">
          <img src="assets/skia_0101.png" width="1126" height="350"/>
          <h6><span class="label">Figure 1-1. </span>This flowchart demonstrates the iterative nature of the AI development lifecycle</h6>
        </div></figure>
        <p>Given the strong demand for a platform that enables this lifecycle and MLOps best practices, it isn’t surprising that a dizzying number of AI development platforms exist on the market today. This report will dive into the open source Kubernetes container orchestration platform, highlighting how it can be used across the AI development lifecycle to apply the foundational MLOps principles to your <span class="keep-together">workflows</span>.</p>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Why Use Kubernetes for Your MLOps Platform?"><div class="sect1" id="ch01_why_use_kubernetes_for_your_mlops_platform_1738498450408203">
        <h1>Why Use Kubernetes for Your <span class="keep-together">MLOps Platform?</span></h1>
        <p>Kubernetes has many strengths, which make it an excellent platform on which to build and run AI-powered applications while adhering to MLOps principles. Because Kubernetes applications are written in a declarative manner, it allows teams to consistently produce repeatable results when building AI models. This, combined with built-in robust <a href="https://oreil.ly/Fjk7z">GitOps tooling</a>, makes it very easy to version control model training artifacts. </p>
        <p>Creating models requires access to specialized hardware, and how that hardware is used is often unpredictable and bursty in nature. Kubernetes is able to abstract away the process of provisioning hardware resources such that a data scientist can focus on developing the model instead of on configuring the hardware environment. On the production side, running AI-powered applications requires separately scaling different parts of the application, <span class="keep-together">including</span> the <span class="keep-together">compute</span> resources serving the backend model and frontend APIs that provide a user access to the model. Because Kubernetes abstracts away hardware provisioning, scaling different pieces of the deployed application becomes easier.</p>
        <p>In addition to manual scaling, Kubernetes allows users to automatically scale highly specialized and expensive compute resources. This fine-grained resource management is critical for properly managing costs. </p>
        <p>Another consideration is specialized hardware such as accelerators, particularly in large-scale training and tuning jobs, which can be quite fragile. In a model training or fine-tuning job that requires multiple days to execute, a hardware failure that forces you to restart training can be quite costly. Kubernetes has self-healing features, which, coupled with checkpointing support in common training libraries, eliminate this problem, making Kubernetes a robust fault-tolerant platform. </p>
        <p>While many AI development platforms are tied to a specific cloud platform, Kubernetes is able to run anywhere you need it. This includes cloud providers, private datacenters, edge locations, and hybrid configurations, which allow Kubernetes to serve as the single deployment platform upon which to build your applications.</p>
        <p>On the monitoring side, Kubernetes integrates with several monitoring tools such as Prometheus, DataDog, and Grafana, which can help track performance and resource usage of models. This is especially important for LLMs due to their size and cost to operate. These deep integrations provide MLOps administrators with proactive monitoring and alerts to ensure that models run optimally for critical AI workloads.</p>
        <p>Finally, rolling out updates to models, especially LLMs, can be a difficult and costly practice. Kubernetes simplifies this process with features like rolling updates (which pushes updates incrementally) and canary deployments, helping to minimize the downtime of these models.</p>
      </div></section>
    </div></section></div>
</div>
</body></html>