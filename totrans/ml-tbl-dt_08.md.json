["```py\nimport pandas as pd\nsummary_listings = pd.read_csv(\"listings.csv\")\n```", "```py\nsummary_listings.dtypes\n```", "```py\nsummary_listings['name'].iloc[0]\n```", "```py\n'Rental unit in Sumida · ★4.78 · 1 bedroom · 2 beds · 1 bath'\n```", "```py\nI have a series of strings in this format: 'Rental unit in Sumida · ★4.78\n · 1 bedroom · 2 beds · 1 bath' Show me a series of regex commands in \norder to extract the following information: 1) the area of Tokyo of the \nlisting 2) the star ratings expressed as floats 3) the number of bedrooms \n4) the number of beds 5) the number of baths.\n```", "```py\nimport pandas as pd\nimport re\n\nclassification_list = [\n   'aparthotel', 'barn', 'bed and breakfast', 'boutique hotel',\n   'bungalow', 'cabin', 'camper/rv', 'chalet', 'condo', 'cottage',\n   'earthen home', 'farm stay', 'guest suite', 'guesthouse', 'home',\n   'hostel', 'hotel', 'houseboat', 'hut', 'loft', 'place to stay',\n   'rental unit', 'resort', 'ryokan', 'serviced apartment',\n   'tiny home', 'townhouse', 'treehouse', 'vacation home', 'villa']\n\nsummary_listings = summary_listings.assign(\n    type_of_accommodation=(\n        summary_listings['name']\n        .str.extract(\n            f\"({'|'.join(classification_list)})\", \n            flags=re.IGNORECASE)),                          ①\n    area_of_tokyo=(\n        summary_listings['name']\n        .str.extract(\n            r'in\\s(.*?)\\s·', \n            flags=re.IGNORECASE)),                          ②\n    score=(\n        summary_listings['name']\n        .str.extract(\n            r'★(\\d+\\.\\d+)',\n            flags=re.IGNORECASE)\n        .astype(float)),                                    ③\n    number_of_bedrooms=(\n         summary_listings['name']\n         .str.extract(\n            r'(\\d+)\\s*(?:bedroom|bedrooms)', \n            flags=re.IGNORECASE)\n         .fillna(0)\n         .astype(int)),                                     ④\n    number_of_beds=(\n         summary_listings['name']\n         .str.extract(\n             r'(\\d+)\\s+(?:beds?\\b)', \n             flags=re.IGNORECASE)\n         .fillna(0)\n         .astype(int)),                                     ⑤\n    number_of_baths=(\n         summary_listings['name']\n         .str.extract(\n             r'(?P<baths>\\d+)\\s*(shared\\s+)?' +\n             r'(?:half-)?baths?\\b', \n             flags=re.IGNORECASE)[\"baths\"]\n         .fillna(0)\n         .astype(int)),                                     ⑥\n)\n```", "```py\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\n\nsummary_listings = summary_listings.assign(\n    is_new=(summary_listings['name']\n               .str.contains('new', case=False)\n               .astype(int)),                              ①\n    is_studio=(summary_listings['name']\n               .str.contains('studio', case=False)\n               .astype(int)),                              ②\n    has_shared_bath=(summary_listings['name']\n                     .str.contains('shared', case=False)\n                     .astype(int)),                        ③\n    has_half_bath=(summary_listings['name']\n                   .str.contains('half', case=False)\n                   .astype(int)),                          ④\n)\n\nsummary_listings['days_since_last_review'] = (\n    datetime.today() – \n    pd.to_datetime(\n        summary_listings['last_review'])\n).dt.days\nsummary_listings['days_since_last_review'] = (\n    summary_listings['days_since_last_review'] -\n    summary_listings['days_since_last_review'].min()\n)                                                          ⑤\n\nzero_reviews = summary_listings['number_of_reviews'] == 0\nratio = summary_listings['number_of_reviews_ltm'] / \nsummary_listings['number_of_reviews']\nsummary_listings['number_of_reviews_ltm_ratio'] = (\n    np.where(zero_reviews, 0, ratio)\n)                                                          ⑥\n```", "```py\ncalendar = pd.read_csv(\"calendar.csv\")\n\ncalendar[\"adjusted_price\"] = (\n    calendar[\"adjusted_price\"]\n    .apply(lambda x: float(\n        x.replace('$', '')\n         .replace(',', ''))\n         )\n)                                                         ①\n\nprice_stats = (\n    calendar.groupby('listing_id')['adjusted_price']      ②\n    .agg(['mean', 'min', 'max'])\n)                                                         ③\n```", "```py\nprice_stats.head()\n```", "```py\ndef bin_2_cat(feature, bins=32):\n    min_value = feature.min()\n    bin_size = (feature.max() - min_value) / bins\n    return ((feature - min_value) / bin_size).astype(int) ①\n\nsummary_listings['coordinates'] = (\n    bin_2_cat(summary_listings['latitude']) * \n    1000 +   \n    bin_2_cat(summary_listings['longitude'])\n)                                                         ②\n\nprint(summary_listings['coordinates'].nunique())\n```", "```py\nimperial_palace_lat = 35.6841822633\nimperial_palace_lon = 139.751471994\n\ndef degrees_to_meters(distance_degrees, latitude):\n    conversion_factor = 111000                              ①\n    distance_meters = (distance_degrees * conversion_factor \n                       * np.cos(np.radians(latitude)))      ②\n    return distance_meters \n\ndistance_degrees = (\n    np.abs(\n        summary_listings['latitude'] \n        imperial_palace_lat) + \n    np.abs(\n        summary_listings['longitude'] \n        imperial_palace_lon)\n)                                                           ③\n\nsummary_listings['imperial_palace_distance'] = (\n    degrees_to_meters(distance_degrees,\n    summary_listings['latitude']\n)\n```", "```py\nsummary_listings.imperial_palace_distance.mean()\n```", "```py\n(summary_listings[\n    ['id', 'name', 'neighbourhood', 'imperial_palace_distance']\n].iloc[np.argmin(summary_listings['imperial_palace_distance'])])\n```", "```py\nid                                                       874407512426725982\nname                        Home in Shibuya City · ★New · 3 bedrooms · ...\nneighbourhood                                                    Chiyoda Ku\nimperial_palace_distance                                         137.394271\nName: 10255, dtype: object\n```", "```py\nfrom sklearn.neighbors import KDTree\n\nrelevant_spots = pd.read_csv(\"relevant_spots_Tokyo.csv\")\n\nvenue_categories = ['Convenience Store', 'Train Station', \n                    'Airport', 'Bus Station', 'Subway']\nmin_distances = {'listing_id': summary_listings['id']}      ①\n\nfor venue in venue_categories:\n    venue_filter = relevant_spots['venueCategory'] == venue\n    venues = relevant_spots[\n        ['latitude', 'longitude']\n    ][venue_filter]                                         ②\n    tree = KDTree(venues, metric='manhattan')               ③\n    distance, index = tree.query(\n        summary_listings[['latitude', 'longitude']],\n        k=1\n     )                                                      ④\n    min_distances[\n        'nearest_' + \n        venue.lower().replace(\" \", \"_\")\n    ] = degrees_to_meters(\n            np.ravel(distance), \n            summary_listings['latitude']\n        )\nmin_distances = pd.DataFrame(min_distances)                 ⑤\n```", "```py\nmin_distances.head()\n```", "```py\nsummary_listings_features = [\n    'neighbourhood',\n    'coordinates',\n    'room_type',\n    'minimum_nights', 'number_of_reviews', 'days_since_last_review',\n    'reviews_per_month', 'calculated_host_listings_count',\n    'availability_365', 'number_of_reviews_ltm', \n'number_of_reviews_ltm_ratio',\n    'number_of_bedrooms', 'number_of_beds', 'number_of_baths',\n    'type_of_accommodation', 'score', 'is_new',\n    'is_studio', 'has_shared_bath', 'has_half_bath',\n    'imperial_palace_distance'\n]\n\nsummarized = summary_listings[['id'] + \nsummary_listings_features].rename({'id': 'listing_id'}, axis=1)\n\nX = summarized.merge(min_distances, on='listing_id').set_index('listing_id')\n\nX = X.reindex(price_stats.index)                              ①\nprice_stats_ordered = price_stats.reindex(X.index)            ②\ny = price_stats_ordered['mean'].copy()                        ③\n```", "```py\nX.head()\n```", "```py\nX.isna().sum()\n```", "```py\ndays_since_last_review            1252\nreviews_per_month                 1252\nscore                             2381\n```", "```py\n(X.number_of_reviews==0).sum()\n```", "```py\nX[[\"days_since_last_review\", \"reviews_per_month\", \"score\"]].describe()\n```", "```py\nX.fillna(-1, inplace=True)\n```", "```py\nimport matplotlib.pyplot as plt\n\nnumeric = ['minimum_nights', 'number_of_reviews', \n           'days_since_last_review', 'reviews_per_month',\n           'calculated_host_listings_count', \n           'availability_365', 'score', \n           'number_of_reviews_ltm', \n           'number_of_reviews_ltm_ratio', \n           'number_of_bedrooms', 'number_of_beds', \n           'number_of_baths', 'imperial_palace_distance', \n           'nearest_convenience_store',\n           'nearest_train_station', 'nearest_airport', \n           'nearest_bus_station', 'nearest_subway']\n\nnum_plots = len(numeric)\nnum_rows = (num_plots + 2) // 3                          ①\nnum_cols = min(num_plots, 3)                             ②\n\nfig, axes = plt.subplots(\n    num_rows,\n    num_cols,\n    figsize=(8, 12)\n)                                                        ③\naxes = axes.flatten()                                    ④\n\nfor i, feat in enumerate(numeric):\n    X[[feat]].boxplot(ax=axes[i])\n\nfig.tight_layout()\nplt.show()\n```", "```py\nfrom scipy.stats.mstats import winsorize\n\nlower_cut_percentile = 0.00                           ①\nupper_cut_percentile = 0.001                          ②\n\nX['minimum_nights'] = winsorize(X['minimum_nights'].values, \n                                limits=(lower_cut_percentile, \nupper_cut_percentile))\n\nX[['minimum_nights']].boxplot()\n```", "```py\nX['number_of_reviews'] = winsorize(X['number_of_reviews'].values, \n                                limits=(lower_cut_percentile, \nupper_cut_percentile))\n\nX[['number_of_reviews']].boxplot()\n```", "```py\nprint(f»minimum: {y.min()}»)\nprint(f\"average: {y.mean().round(2)}\")\nprint(f\"maximum: {y.max()}\")\n```", "```py\nminimum: 1450.0\naverage: 36573.1\nmaximum: 1306500.0\n```", "```py\nperc = [1, 5, 10, 25, 50, 75, 90, 95, 99]\nfor p in perc:\n    print(f\"percentile {p:2}: {np.percentile(y, p).round(2)}\")\n```", "```py\npercentile  1: 3000.0\npercentile  5: 5198.02\npercentile 10: 7315.67\npercentile 25: 11870.07\npercentile 50: 19830.78\npercentile 75: 37741.64\npercentile 90: 83936.03\npercentile 95: 84857.11\npercentile 99: 304531.4\n```", "```py\nvalid_samples = (y >= 5200) & (y <=84857)\n```", "```py\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nvalid_y = y[valid_samples]                                    ①\nsns.kdeplot(valid_y, fill=True)\n\nmedian = np.median(valid_y)                                   ②\nplt.axvline(median, color='r', linestyle='--', linewidth=2, label='Median')\n\nplt.xlabel('Values')\nplt.ylabel('Density')\nplt.title('Distribution Curve with Median')\nplt.legend()\nplt.show()\n```", "```py\nX = X[valid_samples]\ny = y[valid_samples]\n```", "```py\nX['neighbourhood'].value_counts()\n```", "```py\nneighbourhoods = (\n    summary_listings[\n        ['neighbourhood', 'latitude', 'longitude']\n    ]\n    .groupby('neighbourhood')\n    .agg({'latitude': 'mean', \n          'longitude': 'mean',\n          'neighbourhood': 'count'})\n)                                                              ①\n\nless_than_30 = (\n    neighbourhoods[neighbourhoods['neighbourhood'] < 30]\n)\nmore_than_30 = (\n    neighbourhoods[neighbourhoods['neighbourhood'] > 30]\n)                                                              ②\n\nkdtree = KDTree(\n    more_than_30[['latitude', 'longitude']]\n)                                                              ③\nchange_list = {}                                               ④\n\nfor i in range(len(less_than_30)):\n    row = less_than_30.iloc[[i]]\n    _, idx = kdtree.query(\n        row[['latitude', 'longitude']]\n    )                                                          ⑤\n    change_list[row.index[0]] = more_than_30.index[idx[0, 0]]\n\nX[\"neighbourhood_more_than_30\"] = (\n    X[\"neighbourhood\"].replace(change_list)\n)                                                              ⑥\n```", "```py\nprint(change_list)\nprint(X[\"neighbourhood_more_than_30\"].value_counts())\n```", "```py\nfrom sklearn.model_selection import StratifiedKFold\n\ncv = StratifiedKFold(5, shuffle=True, random_state=0)          ①\ncv_splits = cv.split(\n    X, y=X[\"neighbourhood_more_than_30\"]\n)                                                              ②\n```", "```py\nprint(cv_splits)\n```", "```py\n<generator object _BaseKFold.split at 0x78356223c660>\n```", "```py\ncategorical = [\n    'room_type',\n    'neighbourhood_more_than_30', \n    'type_of_accommodation',\n    'coordinates'\n]\nnumeric = [\n    'minimum_nights',\n    'number_of_reviews', \n    'days_since_last_review',\n    'reviews_per_month',\n    'calculated_host_listings_count', \n    'availability_365',\n    'score', \n    'number_of_reviews_ltm', \n    'number_of_reviews_ltm_ratio', \n    'number_of_bedrooms', \n    'number_of_beds',\n    'number_of_baths', \n    'imperial_palace_distance', \n    'nearest_convenience_store',\n    'nearest_train_station',\n    'nearest_airport', \n    'nearest_bus_station',\n    'nearest_subway'\n]\nbinary = [\n    'is_new',\n    'is_studio',\n    'has_shared_bath',\n    'has_half_bath'\n]\n```", "```py\nfor feat in categorical:\n    print(f\"{feat} has {X[feat].nunique()} unique values\")\n```", "```py\nroom_type has 4 unique values\nneighbourhood_more_than_30 has 24 unique values\ntype_of_accommodation has 29 unique values\ncoordinates has 296 unique values\n```", "```py\nonehot_encoding = ['room_type']\nordinal_encoding = ['neighbourhood_more_than_30', 'type_of_accommodation']\ntarget_encoding = ['coordinates']\n```", "```py\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\nfrom category_encoders.target_encoder import TargetEncoder\n\nonehot_encoder = OneHotEncoder(handle_unknown='ignore')          ①\nordinal_enconder = OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n                                  unknown_value=np.nan)          ②\ntarget_encoder = TargetEncoder(\n    cols=target_encoding, \n    handle_unknown=\"value\", \n    smoothing=0.5\n)                                                                ③\n\ncolumn_transform = ColumnTransformer(\n    [('onehot_encoding', onehot_encoder, onehot_encoding),\n     ('ordinal_encoding', ordinal_enconder, ordinal_encoding),\n     ('target_encoding', target_encoder, target_encoding),\n     ('numeric', 'passthrough', numeric),\n     ('binary', 'passthrough', binary)],                         ④\n    remainder='drop',                                            ⑤\n    verbose_feature_names_out=True,                              ⑥\n    sparse_threshold=0.0)                                        ⑦\n```", "```py\nXt = column_transform.fit_transform(X, y)\ncolumn_transform.get_feature_names_out()\n```", "```py\narray(['onehot_encoding__room_type_Entire home/apt',\n       'onehot_encoding__room_type_Hotel room',\n       'onehot_encoding__room_type_Private room',\n       'onehot_encoding__room_type_Shared room',\n       'ordinal_encoding__neighbourhood_more_than_30',\n       'ordinal_encoding__type_of_accommodation',\n       'target_encoding__coordinates', 'numeric__minimum_nights',\n       'numeric__number_of_reviews', 'numeric__days_since_last_review',\n       'numeric__reviews_per_month',\n       'numeric__calculated_host_listings_count',\n       'numeric__availability_365', 'numeric__score',\n       'numeric__number_of_reviews_ltm',\n       'numeric__number_of_reviews_ltm_ratio',\n       'numeric__number_of_bedrooms', 'numeric__number_of_beds',\n       'numeric__number_of_baths', 'numeric__imperial_palace_distance',\n       'numeric__nearest_convenience_store',\n       'numeric__nearest_train_station', 'numeric__nearest_airport',\n       'numeric__nearest_bus_station', 'numeric__nearest_subway',\n       'binary__is_new', 'binary__is_studio', 'binary__has_shared_bath',\n       'binary__has_half_bath'], dtype=object)\n```", "```py\ndata = pd.DataFrame(\n    Xt, \n    columns=column_transform.get_feature_names_out(),\n    index=y.index\n)\ndata = data.assign(target=y).reset_index()\ndata.to_csv(\"airbnb_tokyo.csv\", index=False)\n```", "```py\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n\nlm = LinearRegression(fit_intercept=False)                  ①\nlm.fit(Xt, y)                                               ②\n\ncv_splits = cv.split(\n    X,\n    y = X[\"neighbourhood_more_than_30\"]\n)                                                           ③\ny_pred_cv = cross_val_predict(\n    lm, Xt, y, cv=cv_splits\n)                                                           ④\nprediction_range = y_pred_cv.min()} - {y_pred_cv.max()\nprint(f\"prediction range: {prediction_range}\")              ⑤\n\nr2 = r2_score(y, y_pred_cv)\nrmse = np.sqrt(mean_squared_error(y, y_pred_cv))\nmae = mean_absolute_error(y, y_pred_cv)                     ⑥\n\nprint(f'R-squared: {r2:.3f}')\nprint(f'RMSE: {rmse:.3f}')\nprint(f'MAE: {mae:.3f}')\n\nplt.scatter(y, y_pred_cv)                                   ⑦\nplt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', label='Ideal Fit')\nplt.axhline(\n   0, color='orange', linestyle='--', label='Zero Line'\n)                                                           ⑧\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Linear Regression - Fitted Results')\nplt.legend()\nplt.show()\n```", "```py\nprediction range: -34929.50241836217 - 136479.60736257263\nR-squared: 0.320\nRMSE: 17197.323\nMAE: 12568.371\n```", "```py\n(y_pred_cv <= 0).sum() / len(y_pred_cv)\n```", "```py\n0.005178767055074196\n```", "```py\nprint(np.where(y_pred_cv > 100_000))\n```", "```py\n(array([5509, 8307]),)\n```", "```py\nprint(np.where(y_pred_cv < -25_000))\n```", "```py\n(array([182]),)\n```", "```py\ndef report_case(model, data, feature_names, case_no):\n    case_values = data[case_no]                             ①\n    coef_values = case_values * model.coef_                 ②\n    for feature_name, value, coef_value in zip(\n            feature_names, case_values, coef_values):\n        print(f\"{feature_name:50s}\" +\n              f\"({value:10.2f}) : \" +\n              f\"{coef_value:+0.2f}\")                        ③\n    print(\"-\" * 80)\n    print(\" \"*66 + f\"{np.sum(coef_values):+0.2f}\")          ④\n```", "```py\nreport_case(model=lm, \n            data=Xt, \n            feature_names=column_transform.get_feature_names_out(), \n            case_no=8307)\n```", "```py\nonehot_encoding__room_type_Entire home/apt         (      1.00) : -8295.89\nonehot_encoding__room_type_Hotel room              (      0.00) : -0.00\nonehot_encoding__room_type_Private room            (      0.00) : -0.00\nonehot_encoding__room_type_Shared room             (      0.00) : -0.00\nordinal_encoding__neighbourhood_more_than_30       (     12.00) : +576.48\nordinal_encoding__type_of_accommodation            (     20.00) : +2377.99\ntarget_encoding__coordinates                       (  29649.71) : +26556.25\nnumeric__minimum_nights                            (      1.00) : -268.05\nnumeric__number_of_reviews                         (      0.00) : -0.00\nnumeric__days_since_last_review                    (     -1.00) : -0.66\nnumeric__reviews_per_month                         (     -1.00) : -172.50\nnumeric__calculated_host_listings_count            (     15.00) : +1470.92\nnumeric__availability_365                          (    354.00) : +16503.11\nnumeric__score                                     (     -1.00) : +524.08\nnumeric__number_of_reviews_ltm                     (      0.00) : -0.00\nnumeric__number_of_reviews_ltm_ratio               (      0.00) : +0.00\nnumeric__number_of_bedrooms                        (     18.00) : +64407.67\nnumeric__number_of_beds                            (     18.00) : +31283.70\nnumeric__number_of_baths                           (      2.00) : -1787.41\nnumeric__imperial_palace_distance                  (   2279.80) : -859.33\nnumeric__nearest_convenience_store                 (    149.84) : +549.07\nnumeric__nearest_train_station                     (    545.08) : -1043.20\nnumeric__nearest_airport                           (    389.85) : -137.44\nnumeric__nearest_bus_station                       (    322.04) : -266.55\nnumeric__nearest_subway                            (    221.93) : -17.29\nbinary__is_new                                     (      0.00) : -0.00\nbinary__is_studio                                  (      0.00) : +0.00\nbinary__has_shared_bath                            (      0.00) : -0.00\nbinary__has_half_bath                              (      0.00) : -0.00\n----------------------------------------------------------------------------\n                                                                  +131400.95\n```", "```py\nreport_case(model=lm, \n            data=Xt, \n            feature_names=column_transform.get_feature_names_out(), \n            case_no=182)\n```", "```py\nonehot_encoding__room_type_Entire home/apt         (      0.00) : -0.00\nonehot_encoding__room_type_Hotel room              (      0.00) : -0.00\nonehot_encoding__room_type_Private room            (      1.00) : -11573.69\nonehot_encoding__room_type_Shared room             (      0.00) : -0.00\nordinal_encoding__neighbourhood_more_than_30       (      6.00) : +288.24\nordinal_encoding__type_of_accommodation            (     14.00) : +1664.59\ntarget_encoding__coordinates                       (  27178.66) : +24343.02\nnumeric__minimum_nights                            (    120.00) : -32166.38\nnumeric__number_of_reviews                         (    122.00) : -1241.88\nnumeric__days_since_last_review                    (    132.00) : +87.20\nnumeric__reviews_per_month                         (      1.33) : +229.43\nnumeric__calculated_host_listings_count            (      4.00) : +392.25\nnumeric__availability_365                          (      0.00) : +0.00\nnumeric__score                                     (      4.98) : -2609.92\nnumeric__number_of_reviews_ltm                     (      4.00) : -18.33\nnumeric__number_of_reviews_ltm_ratio               (      0.03) : +54.17\nnumeric__number_of_bedrooms                        (      1.00) : +3578.20\nnumeric__number_of_beds                            (      0.00) : +0.00\nnumeric__number_of_baths                           (      0.00) : -0.00\nnumeric__imperial_palace_distance                  (  32506.70) : -12252.79\nnumeric__nearest_convenience_store                 (   5020.81) : +18397.51\nnumeric__nearest_train_station                     (   5689.32) : -10888.48\nnumeric__nearest_airport                           (  11438.81) : -4032.76\nnumeric__nearest_bus_station                       (   4999.17) : -4137.76\nnumeric__nearest_subway                            (  16976.69) : -1322.52\nbinary__is_new                                     (      0.00) : -0.00\nbinary__is_studio                                  (      0.00) : +0.00\nbinary__has_shared_bath                            (      0.00) : -0.00\nbinary__has_half_bath                              (      0.00) : -0.00\n----------------------------------------------------------------------------\n                                                                  -31209.90\n```", "```py\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\nfrom XGBoost import XGBRegressor\n\nxgb = XGBRegressor(booster='gbtree',                              ①\n                   objective='reg:gamma',                         ②\n                   n_estimators=300,\n                   max_depth=6)\n\ncv_splits = cv.split(\n    X, y=X[\"neighbourhood_more_than_30\"]\n)                                                                 ③\ny_pred_cv = cross_val_predict(\n    xgb, Xt, y, cv=cv_splits\n)                                                                 ④\nprediction_range = y_pred_cv.min()} - {y_pred_cv.max()\nprint(f\"prediction range: {prediction_range}\")                    ⑤\n\nr2 = r2_score(y, y_pred_cv)\nrmse = np.sqrt(mean_squared_error(y, y_pred_cv))\nmae = mean_absolute_error(y, y_pred_cv)                           ⑥\n\nprint(f'R-squared: {r2:.3f}')\nprint(f'RMSE: {rmse:.3f}')\nprint(f'MAE: {mae:.3f}')\n\nplt.scatter(y, y_pred_cv)                                         ⑦\nplt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', label='Ideal Fit')\nplt.axhline(0, color='orange', linestyle='--', label='Zero Line') ⑧\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('XGBoost - Fitted Results')\nplt.legend()\nplt.show()\n```", "```py\nprediction range: 3291.401123046875 - 123069.8828125\nR-squared: 0.693\nRMSE: 11562.836\nMAE: 7227.435\n```", "```py\nimport optuna\n\ndef objective(trial):                                       ①\n\n    params = {                                              ②\n        'booster': trial.suggest_categorical(\n            'booster', \n            ['gbtree', 'gblinear']\n        ),\n        'objective': trial.suggest_categorical(\n            'objective', \n            ['reg:squarederror', 'reg:gamma', 'reg:tweedie']\n        ),\n        'n_estimators': trial.suggest_int(\n            'n_estimators', 100, 1000\n        ),\n        'learning_rate': trial.suggest_float(\n            'learning_rate', 0.01, 1.0, log=True\n        ),\n        'subsample': trial.suggest_float(\n            'subsample', 0.3, 1.0\n        ),\n        'colsample_bytree': trial.suggest_float(\n            'colsample_bytree', 0.3, 1.0\n        ),\n        'max_depth': trial.suggest_int('max_depth', 1, 7),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),\n        'reg_lambda': trial.suggest_float(\n             'reg_lambda', 1e-9, 100.0, log=True\n        ),\n        'reg_alpha': trial.suggest_float(\n             'reg_alpha', 1e-9, 100.0, log=True\n        ),\n    }\n\n    if params['booster'] == 'gblinear':                      ③\n        keys_to_remove = [\n            \"colsample_bytree\", \"max_depth\", \n            \"min_child_weight\", \"subsample\"\n        ]\n        params = {\n            key:value for key, value in params.items()\n            if key not in keys_to_remove\n        }\n\n    if params['objective'] == 'reg:tweedie':                 ④\n        # Must be between in range [1, 2) : 1=poisson 2=gamma\n        params['tweedie_variance_power'] = trial.suggest_float(\n            'tweedie_variance_power', 1.01, 1.99\n        )\n\n    xgb = XGBRegressor(**params)                             ⑤\n    model_pipeline = Pipeline(\n        [('processing', column_transform), \n         ('xgb', xgb)]\n    )\n    cv_splits = cv.split(X, y=X[\"neighbourhood_more_than_30\"])\n\n    cv_scores = cross_validate(\n        estimator=model_pipeline, \n        X=X, \n        y=y,\n        scoring='neg_mean_absolute_error',\n        cv=cv_splits\n    )                                                        ⑥\n    cv_evaluation = np.mean(\n        np.abs(cv_scores['test_score'])\n    )                                                        ⑦\n    return cv_evaluation                                     ⑧\n\nsqlite_db = \"sqlite:///sqlite.db\"\nstudy_name = \"optimize_XGBoost_tokyo_airbnb\"\nstudy = optuna.create_study(\n    storage=sqlite_db, \n    study_name=study_name, \n    direction=\"minimize\",\n    load_if_exists=True\n)                                                            ⑨\n\nstudy.optimize(objective, n_trials=100)                      ⑩\nprint(study.best_value)                                      ⑪\nprint(study.best_params)                                     ⑫\n```", "```py\n6616.859370931483\n{'booster': 'gbtree', \n 'colsample_bytree': 0.946407058507176,\n 'learning_rate': 0.06867015067874482,\n 'max_depth': 7,\n 'min_child_weight': 5,\n 'n_estimators': 901,\n 'objective': 'reg:tweedie',\n 'reg_alpha': 0.0006368936493084075,\n 'reg_lambda': 3.8302865696045996,\n 'subsample': 0.8956307610431394,\n 'tweedie_variance_power': 1.560801988491813\n}\n```", "```py\nfig = optuna.visualization.plot_optimization_history(study)\nfig.show()\n```", "```py\nfig = optuna.visualization.plot_param_importances(study)\nfig.show()\n```", "```py\nbest_params = study.best_params\nprint(best_params)\n\nxgb = XGBRegressor(**best_params)                               ①\nmodel_pipeline = Pipeline([('processing', column_transform), ('xgb', xgb)])\n\ncv_splits = cv.split(X, y=X[\"neighbourhood_more_than_30\"])      ②\n\nr2_scores = []\nrmse_scores = []\nmae_scores = []\n\nfor train_index, test_index in cv_splits:                       ③\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    model_pipeline.fit(X_train, y_train)\n    y_pred = model_pipeline.predict(X_test)\n\n    r2_scores.append(r2_score(y_test, y_pred))\n    rmse_scores.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n    mae_scores.append(mean_absolute_error(y_test, y_pred))\n\nprint(f\"Mean cv R-squared: {np.mean(r2_scores):.3f}\")\nprint(f\"Mean cv RMSE: {np.mean(rmse_scores):.3f}\")\nprint(f\"Mean cv MAE: {np.mean(mae_scores):.3f}\")\n\nmodel_pipeline.fit(X, y)                                        ④\n```", "```py\n{'booster': 'gbtree', \n 'colsample_bytree': 0.946407058507176,\n 'learning_rate': 0.06867015067874482,\n 'max_depth': 7,\n 'min_child_weight': 5,\n 'n_estimators': 901,\n 'objective': 'reg:tweedie',\n 'reg_alpha': 0.0006368936493084075,\n 'reg_lambda': 3.8302865696045996,\n 'subsample': 0.8956307610431394,\n 'tweedie_variance_power': 1.560801988491813\n}\n\nMean cv R-squared: 0.727\nMean cv RMSE: 10886.568\nMean cv MAE: 6667.187\n```", "```py\nfrom XGBoost import DMatrix\n\nbooster = model_pipeline['xgb'].get_booster()              ①\n\nXt = model_pipeline['processing'].transform(X)             ②\nfeature_names = (\n    model_pipeline['processing']\n    .get_feature_names_out()\n)                                                          ③\nXd = DMatrix(Xt)                                           ④\n\nshap_values = booster.predict(Xd, pred_contribs=True)      ⑤\npreds = booster.predict(Xd)                                ⑥\n```", "```py\nnp.prod(np.exp(shap_values[0])), preds[0]\n\n(10627.659, 10627.469)\n```", "```py\nnp.corrcoef(preds, np.prod(np.exp(shap_values), axis=1))\n\narray([[1., 1.],\n       [1., 1.]])\n```", "```py\nfrom shap import TreeExplainer\n\nexplainer = TreeExplainer(model_pipeline['xgb'], data=Xt, model_output='raw', feature_perturbation='interventional')\ninterventional_shap_values = explainer.shap_values(Xt)\n```", "```py\nimport shap\n\nshap.summary_plot(\n    shap_values[:,:-1],\n    Xt,\n    plot_type=\"bar\",\n    feature_names=feature_names,\n    max_display=10,\n    show=False\n)                                                          ①\nplt.xticks(fontsize=8)\nplt.yticks(fontsize=8)\nplt.xlabel(\"SHAP Importance\", fontsize=10)\nplt.show()\n```", "```py\nshap.summary_plot(shap_values[:,:-1], Xt, \n                  plot_type=\"violin\", \n                  feature_names=feature_names,\n                  show=False)                              ①\nplt.yticks(fontsize=8)\nplt.show()\n```", "```py\ndef generate_prediction_explanation(\n    index,\n    X,\n    feature_names, \n    shapley_values,\n    predictions\n):\n    explanation = {}                                        ①\n    explanation[\"prediction\"] = predictions[index]          ②\n    for feature, original_value, shap_value in zip(\n        feature_names, \n        X[index],\n        shapley_values[index, :]\n    ):                                                      ③\n        explanation[feature] = {\n            \"original_value\": original_value, \n            \"shap_value\": shap_value\n        }\n    return explanation\n\nindex_to_explain = 5                                        ④\nexplanation_json = generate_prediction_explanation(\n    index_to_explain, \n    feature_names, \n    Xt,\n    shap_values,\n    preds\n)\nprint(explanation_json)\n```", "```py\nYou are an expert data scientist, and you need to interpret the predictions\n of a regression model based on the shape values provided in a JSON file.\n You build the explanations as a narration of how the most important \nvariables contribute to the prediction. Here is the JSON file:\n{'prediction': 55225.176, 'onehot_encoding__room_type_Entire home/apt': \n{'original_value': 1.0, 'shap_value': 0.03404991}, \n'onehot_encoding__room_type_Hotel room': {'original_value': 0.0, \n'shap_value': 0.00020163489}, … }\n```"]