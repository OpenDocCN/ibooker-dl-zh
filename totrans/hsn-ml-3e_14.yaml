- en: Chapter 12\. Custom Models and Training with TensorFlow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章. 使用TensorFlow进行自定义模型和训练
- en: 'Up until now, we’ve used only TensorFlow’s high-level API, Keras, but it already
    got us pretty far: we built various neural network architectures, including regression
    and classification nets, Wide & Deep nets, and self-normalizing nets, using all
    sorts of techniques, such as batch normalization, dropout, and learning rate schedules.
    In fact, 95% of the use cases you will encounter will not require anything other
    than Keras (and tf.data; see [Chapter 13](ch13.html#data_chapter)). But now it’s
    time to dive deeper into TensorFlow and take a look at its lower-level [Python
    API](https://homl.info/tf2api). This will be useful when you need extra control
    to write custom loss functions, custom metrics, layers, models, initializers,
    regularizers, weight constraints, and more. You may even need to fully control
    the training loop itself; for example, to apply special transformations or constraints
    to the gradients (beyond just clipping them) or to use multiple optimizers for
    different parts of the network. We will cover all these cases in this chapter,
    and we will also look at how you can boost your custom models and training algorithms
    using TensorFlow’s automatic graph generation feature. But first, let’s take a
    quick tour of TensorFlow.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只使用了TensorFlow的高级API，Keras，但它已经让我们走得很远：我们构建了各种神经网络架构，包括回归和分类网络，Wide
    & Deep网络，自正则化网络，使用各种技术，如批量归一化，dropout和学习率调度。事实上，您将遇到的95%用例不需要除了Keras（和tf.data）之外的任何东西（请参见[第13章](ch13.html#data_chapter)）。但现在是时候深入研究TensorFlow，看看它的低级[Python
    API](https://homl.info/tf2api)。当您需要额外控制以编写自定义损失函数，自定义指标，层，模型，初始化程序，正则化器，权重约束等时，这将非常有用。您甚至可能需要完全控制训练循环本身；例如，应用特殊的转换或约束到梯度（超出仅仅剪切它们）或为网络的不同部分使用多个优化器。我们将在本章中涵盖所有这些情况，并且还将看看如何使用TensorFlow的自动生成图功能来提升您的自定义模型和训练算法。但首先，让我们快速浏览一下TensorFlow。
- en: A Quick Tour of TensorFlow
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow的快速浏览
- en: As you know, TensorFlow is a powerful library for numerical computation, particularly
    well suited and fine-tuned for large-scale machine learning (but you can use it
    for anything else that requires heavy computations). It was developed by the Google
    Brain team and it powers many of Google’s large-scale services, such as Google
    Cloud Speech, Google Photos, and Google Search. It was open sourced in November
    2015, and it is now the most widely used deep learning library in the industry:^([1](ch12.html#idm45720196277360))
    countless projects use TensorFlow for all sorts of machine learning tasks, such
    as image classification, natural language processing, recommender systems, and
    time series forecasting.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所知，TensorFlow是一个强大的用于数值计算的库，特别适用于大规模机器学习（但您也可以用它来进行需要大量计算的任何其他任务）。它由Google
    Brain团队开发，驱动了谷歌许多大规模服务，如Google Cloud Speech，Google Photos和Google Search。它于2015年11月开源，现在是业界最广泛使用的深度学习库：无数项目使用TensorFlow进行各种机器学习任务，如图像分类，自然语言处理，推荐系统和时间序列预测。
- en: 'So what does TensorFlow offer? Here’s a summary:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 那么TensorFlow提供了什么？以下是一个摘要：
- en: Its core is very similar to NumPy, but with GPU support.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它的核心与NumPy非常相似，但支持GPU。
- en: It supports distributed computing (across multiple devices and servers).
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它支持分布式计算（跨多个设备和服务器）。
- en: It includes a kind of just-in-time (JIT) compiler that allows it to optimize
    computations for speed and memory usage. It works by extracting the *computation
    graph* from a Python function, optimizing it (e.g., by pruning unused nodes),
    and running it efficiently (e.g., by automatically running independent operations
    in parallel).
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它包括一种即时（JIT）编译器，允许它优化计算以提高速度和内存使用。它通过从Python函数中提取*计算图*，优化它（例如通过修剪未使用的节点），并有效地运行它（例如通过自动并行运行独立操作）来工作。
- en: Computation graphs can be exported to a portable format, so you can train a
    TensorFlow model in one environment (e.g., using Python on Linux) and run it in
    another (e.g., using Java on an Android device).
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算图可以导出为可移植格式，因此您可以在一个环境中训练TensorFlow模型（例如在Linux上使用Python），并在另一个环境中运行它（例如在Android设备上使用Java）。
- en: It implements reverse-mode autodiff (see [Chapter 10](ch10.html#ann_chapter)
    and [Appendix B](app02.html#autodiff_appendix)) and provides some excellent optimizers,
    such as RMSProp and Nadam (see [Chapter 11](ch11.html#deep_chapter)), so you can
    easily minimize all sorts of loss functions.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它实现了反向模式自动微分（请参见[第10章](ch10.html#ann_chapter)和[附录B](app02.html#autodiff_appendix)）并提供了一些优秀的优化器，如RMSProp和Nadam（请参见[第11章](ch11.html#deep_chapter)），因此您可以轻松最小化各种损失函数。
- en: 'TensorFlow offers many more features built on top of these core features: the
    most important is of course Keras,⁠^([2](ch12.html#idm45720196265168)) but it
    also has data loading and preprocessing ops (tf.data, tf.io, etc.), image processing
    ops (tf.image), signal processing ops (tf.signal), and more (see [Figure 12-1](#tensorflow_api_diagram)
    for an overview of TensorFlow’s Python API).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow提供了许多建立在这些核心功能之上的功能：最重要的当然是Keras，但它还有数据加载和预处理操作（tf.data，tf.io等），图像处理操作（tf.image），信号处理操作（tf.signal）等等（请参见[图12-1](#tensorflow_api_diagram)以获取TensorFlow的Python
    API概述）。
- en: Tip
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: We will cover many of the packages and functions of the TensorFlow API, but
    it’s impossible to cover them all, so you should really take some time to browse
    through the API; you will find that it is quite rich and well documented.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖TensorFlow API的许多包和函数，但不可能覆盖所有内容，因此您应该花些时间浏览API；您会发现它非常丰富且有很好的文档。
- en: 'At the lowest level, each TensorFlow operation (*op* for short) is implemented
    using highly efficient C++ code.⁠^([3](ch12.html#idm45720196260752)) Many operations
    have multiple implementations called *kernels*: each kernel is dedicated to a
    specific device type, such as CPUs, GPUs, or even TPUs (*tensor processing units*).
    As you may know, GPUs can dramatically speed up computations by splitting them
    into many smaller chunks and running them in parallel across many GPU threads.
    TPUs are even faster: they are custom ASIC chips built specifically for deep learning
    operations⁠^([4](ch12.html#idm45720196255744)) (we will discuss how to use TensorFlow
    with GPUs or TPUs in [Chapter 19](ch19.html#deployment_chapter)).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在最低级别上，每个TensorFlow操作（简称op）都是使用高效的C++代码实现的。许多操作有多个称为内核的实现：每个内核专门用于特定设备类型，如CPU、GPU，甚至TPU（张量处理单元）。正如您可能知道的，GPU可以通过将计算分成许多较小的块并在许多GPU线程上并行运行来显着加快计算速度。TPU速度更快：它们是专门用于深度学习操作的定制ASIC芯片（我们将在[第19章](ch19.html#deployment_chapter)讨论如何使用GPU或TPU与TensorFlow）。
- en: '![mls3 1201](assets/mls3_1201.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1201](assets/mls3_1201.png)'
- en: Figure 12-1\. TensorFlow’s Python API
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-1\. TensorFlow的Python API
- en: TensorFlow’s architecture is shown in [Figure 12-2](#tensorflow_architecture_diagram).
    Most of the time your code will use the high-level APIs (especially Keras and
    tf.data), but when you need more flexibility you will use the lower-level Python
    API, handling tensors directly. In any case, TensorFlow’s execution engine will
    take care of running the operations efficiently, even across multiple devices
    and machines if you tell it to.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow的架构如[图12-2](#tensorflow_architecture_diagram)所示。大部分时间，您的代码将使用高级API（特别是Keras和tf.data），但当您需要更灵活性时，您将使用较低级别的Python
    API，直接处理张量。无论如何，TensorFlow的执行引擎将有效地运行操作，即使跨多个设备和机器，如果您告诉它的话。
- en: 'TensorFlow runs not only on Windows, Linux, and macOS, but also on mobile devices
    (using *TensorFlow Lite*), including both iOS and Android (see [Chapter 19](ch19.html#deployment_chapter)).
    Note that APIs for other languages are also available, if you do not want to use
    the Python API: there are C++, Java, and Swift APIs. There is even a JavaScript
    implementation called *TensorFlow.js* that makes it possible to run your models
    directly in your browser.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow不仅可以在Windows、Linux和macOS上运行，还可以在移动设备上运行（使用TensorFlow Lite），包括iOS和Android（请参阅[第19章](ch19.html#deployment_chapter)）。请注意，如果您不想使用Python
    API，还可以使用其他语言的API：有C++、Java和Swift的API。甚至还有一个名为TensorFlow.js的JavaScript实现，可以直接在浏览器中运行您的模型。
- en: '![mls3 1202](assets/mls3_1202.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1202](assets/mls3_1202.png)'
- en: Figure 12-2\. TensorFlow’s architecture
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-2\. TensorFlow的架构
- en: 'There’s more to TensorFlow than the library. TensorFlow is at the center of
    an extensive ecosystem of libraries. First, there’s TensorBoard for visualization
    (see [Chapter 10](ch10.html#ann_chapter)). Next, there’s [TensorFlow Extended
    (TFX)](https://tensorflow.org/tfx), which is a set of libraries built by Google
    to productionize TensorFlow projects: it includes tools for data validation, preprocessing,
    model analysis, and serving (with TF Serving; see [Chapter 19](ch19.html#deployment_chapter)).
    Google’s *TensorFlow Hub* provides a way to easily download and reuse pretrained
    neural networks. You can also get many neural network architectures, some of them
    pretrained, in TensorFlow’s [model garden](https://github.com/tensorflow/models).
    Check out the [TensorFlow Resources](https://tensorflow.org/resources) and [*https://github.com/jtoy/awesome-tensorflow*](https://github.com/jtoy/awesome-tensorflow)
    for more TensorFlow-based projects. You will find hundreds of TensorFlow projects
    on GitHub, so it is often easy to find existing code for whatever you are trying
    to do.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow不仅仅是一个库。TensorFlow是一个庞大生态系统中心。首先，有用于可视化的TensorBoard（请参阅[第10章](ch10.html#ann_chapter)）。接下来，有由Google构建的用于将TensorFlow项目投入生产的一套库，称为[TensorFlow
    Extended (TFX)](https://tensorflow.org/tfx)：它包括用于数据验证、预处理、模型分析和服务的工具（使用TF Serving；请参阅[第19章](ch19.html#deployment_chapter)）。Google的TensorFlow
    Hub提供了一种轻松下载和重复使用预训练神经网络的方式。您还可以在TensorFlow的[model garden](https://github.com/tensorflow/models)中获得许多神经网络架构，其中一些是预训练的。查看[TensorFlow资源](https://tensorflow.org/resources)和[*https://github.com/jtoy/awesome-tensorflow*](https://github.com/jtoy/awesome-tensorflow)以获取更多基于TensorFlow的项目。您可以在GitHub上找到数百个TensorFlow项目，因此通常很容易找到您正在尝试做的任何事情的现有代码。
- en: Tip
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: More and more ML papers are released along with their implementations, and sometimes
    even with pretrained models. Check out [*https://paperswithcode.com*](https://paperswithcode.com)
    to easily find them.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 越来越多的机器学习论文随着它们的实现发布，有时甚至附带预训练模型。请查看[*https://paperswithcode.com*](https://paperswithcode.com)以轻松找到它们。
- en: Last but not least, TensorFlow has a dedicated team of passionate and helpful
    developers, as well as a large community contributing to improving it. To ask
    technical questions, you should use [*https://stackoverflow.com*](https://stackoverflow.com)
    and tag your question with *tensorflow* and *python*. You can file bugs and feature
    requests through [GitHub](https://github.com/tensorflow/tensorflow). For general
    discussions, join the [TensorFlow Forum](https://discuss.tensorflow.org).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但并非最不重要的是，TensorFlow拥有一支充满激情和乐于助人的开发团队，以及一个庞大的社区为其改进做出贡献。要提出技术问题，您应该使用[*https://stackoverflow.com*](https://stackoverflow.com)，并在问题中标记*tensorflow*和*python*。您可以通过[GitHub](https://github.com/tensorflow/tensorflow)提交错误和功能请求。要进行一般讨论，请加入[TensorFlow论坛](https://discuss.tensorflow.org)。
- en: OK, it’s time to start coding!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，现在是开始编码的时候了！
- en: Using TensorFlow like NumPy
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 像NumPy一样使用TensorFlow
- en: 'TensorFlow’s API revolves around *tensors*, which flow from operation to operation—hence
    the name Tensor*Flow*. A tensor is very similar to a NumPy `ndarray`: it is usually
    a multidimensional array, but it can also hold a scalar (a simple value, such
    as `42`). These tensors will be important when we create custom cost functions,
    custom metrics, custom layers, and more, so let’s see how to create and manipulate
    them.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow的API围绕着*张量*展开，这些张量从操作流向操作，因此得名Tensor*Flow*。张量与NumPy的`ndarray`非常相似：通常是一个多维数组，但也可以保存标量（例如`42`）。当我们创建自定义成本函数、自定义指标、自定义层等时，这些张量将非常重要，让我们看看如何创建和操作它们。
- en: Tensors and Operations
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 张量和操作
- en: 'You can create a tensor with `tf.constant()`. For example, here is a tensor
    representing a matrix with two rows and three columns of floats:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`tf.constant()`创建一个张量。例如，这里是一个表示具有两行三列浮点数的矩阵的张量：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Just like an `ndarray`, a `tf.Tensor` has a shape and a data type (`dtype`):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 就像`ndarray`一样，`tf.Tensor`有一个形状和一个数据类型（`dtype`）：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Indexing works much like in NumPy:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 索引工作方式与NumPy类似：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Most importantly, all sorts of tensor operations are available:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，各种张量操作都是可用的：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Note that writing `t + 10` is equivalent to calling `tf.add(t, 10)` (indeed,
    Python calls the magic method `t.__add__(10)`, which just calls `tf.add(t, 10)`).
    Other operators, like `-` and `*`, are also supported. The `@` operator was added
    in Python 3.5, for matrix multiplication: it is equivalent to calling the `tf.matmul()`
    function.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，编写`t + 10`等同于调用`tf.add(t, 10)`（实际上，Python调用了魔术方法`t.__add__(10)`，它只是调用了`tf.add(t,
    10)`）。其他运算符，如`-`和`*`，也受支持。`@`运算符在Python 3.5中添加，用于矩阵乘法：它等同于调用`tf.matmul()`函数。
- en: Note
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Many functions and classes have aliases. For example, `tf.add()` and `tf.math.add()`
    are the same function. This allows TensorFlow to have concise names for the most
    common operations⁠^([5](ch12.html#idm45720196020064)) while preserving well-organized
    packages.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 许多函数和类都有别名。例如，`tf.add()`和`tf.math.add()`是相同的函数。这使得TensorFlow可以为最常见的操作保留简洁的名称，同时保持良好组织的包。
- en: 'A tensor can also hold a scalar value. In this case, the shape is empty:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 张量也可以保存标量值。在这种情况下，形状为空：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'The Keras API has its own low-level API, located in `tf.keras.backend`. This
    package is usually imported as `K`, for conciseness. It used to include functions
    like `K.square()`, `K.exp()`, and `K.sqrt()`, which you may run across in existing
    code: this was useful to write portable code back when Keras supported multiple
    backends, but now that Keras is TensorFlow-only, you should call TensorFlow’s
    low-level API directly (e.g., `tf.square()` instead of `K.square()`). Technically
    `K.square()` and its friends are still there for backward compatibility, but the
    documentation of the `tf.keras.backend` package only lists a handful of utility
    functions, such as `clear_session()` (mentioned in [Chapter 10](ch10.html#ann_chapter)).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Keras API有自己的低级API，位于`tf.keras.backend`中。这个包通常被导入为`K`，以简洁为主。它曾经包括函数如`K.square()`、`K.exp()`和`K.sqrt()`，您可能在现有代码中遇到：这在Keras支持多个后端时编写可移植代码很有用，但现在Keras只支持TensorFlow，您应该直接调用TensorFlow的低级API（例如，使用`tf.square()`而不是`K.square()`）。从技术上讲，`K.square()`及其相关函数仍然存在以保持向后兼容性，但`tf.keras.backend`包的文档只列出了一些实用函数，例如`clear_session()`（在[第10章](ch10.html#ann_chapter)中提到）。
- en: 'You will find all the basic math operations you need (`tf.add()`, `tf.multiply()`,
    `tf.square()`, `tf.exp()`, `tf.sqrt()`, etc.) and most operations that you can
    find in NumPy (e.g., `tf.reshape()`, `tf.squeeze()`, `tf.tile()`). Some functions
    have a different name than in NumPy; for instance, `tf.reduce_mean()`, `tf.reduce_sum()`,
    `tf.reduce_max()`, and `tf.math.log()` are the equivalent of `np.mean()`, `np.sum()`,
    `np.max()`, and `np.log()`. When the name differs, there is usually a good reason
    for it. For example, in TensorFlow you must write `tf.transpose(t)`; you cannot
    just write `t.T` like in NumPy. The reason is that the `tf.transpose()` function
    does not do exactly the same thing as NumPy’s `T` attribute: in TensorFlow, a
    new tensor is created with its own copy of the transposed data, while in NumPy,
    `t.T` is just a transposed view on the same data. Similarly, the `tf.reduce_sum()`
    operation is named this way because its GPU kernel (i.e., GPU implementation)
    uses a reduce algorithm that does not guarantee the order in which the elements
    are added: because 32-bit floats have limited precision, the result may change
    ever so slightly every time you call this operation. The same is true of `tf.reduce_mean()`
    (but of course `tf.reduce_max()` is deterministic).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 您将找到所有您需要的基本数学运算（`tf.add()`、`tf.multiply()`、`tf.square()`、`tf.exp()`、`tf.sqrt()`等）以及大多数您可以在NumPy中找到的操作（例如`tf.reshape()`、`tf.squeeze()`、`tf.tile()`）。一些函数的名称与NumPy中的名称不同；例如，`tf.reduce_mean()`、`tf.reduce_sum()`、`tf.reduce_max()`和`tf.math.log()`相当于`np.mean()`、`np.sum()`、`np.max()`和`np.log()`。当名称不同时，通常有很好的理由。例如，在TensorFlow中，您必须编写`tf.transpose(t)`；您不能像在NumPy中那样只写`t.T`。原因是`tf.transpose()`函数与NumPy的`T`属性并不完全相同：在TensorFlow中，将创建一个具有其自己的转置数据副本的新张量，而在NumPy中，`t.T`只是相同数据的一个转置视图。同样，`tf.reduce_sum()`操作之所以被命名为这样，是因为其GPU核心（即GPU实现）使用的减少算法不保证元素添加的顺序：因为32位浮点数的精度有限，每次调用此操作时结果可能会发生微小变化。`tf.reduce_mean()`也是如此（当然`tf.reduce_max()`是确定性的）。
- en: Tensors and NumPy
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 张量和NumPy
- en: 'Tensors play nice with NumPy: you can create a tensor from a NumPy array, and
    vice versa. You can even apply TensorFlow operations to NumPy arrays and NumPy
    operations to tensors:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 张量与NumPy兼容：您可以从NumPy数组创建张量，反之亦然。您甚至可以将TensorFlow操作应用于NumPy数组，将NumPy操作应用于张量：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Warning
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Notice that NumPy uses 64-bit precision by default, while TensorFlow uses 32-bit.
    This is because 32-bit precision is generally more than enough for neural networks,
    plus it runs faster and uses less RAM. So when you create a tensor from a NumPy
    array, make sure to set `dtype=tf.float32`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，NumPy默认使用64位精度，而TensorFlow使用32位。这是因为32位精度通常对神经网络来说足够了，而且运行速度更快，使用的内存更少。因此，当您从NumPy数组创建张量时，请确保设置`dtype=tf.float32`。
- en: Type Conversions
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 类型转换
- en: 'Type conversions can significantly hurt performance, and they can easily go
    unnoticed when they are done automatically. To avoid this, TensorFlow does not
    perform any type conversions automatically: it just raises an exception if you
    try to execute an operation on tensors with incompatible types. For example, you
    cannot add a float tensor and an integer tensor, and you cannot even add a 32-bit
    float and a 64-bit float:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 类型转换可能会严重影响性能，并且当它们自动完成时很容易被忽略。为了避免这种情况，TensorFlow不会自动执行任何类型转换：如果您尝试在具有不兼容类型的张量上执行操作，它只会引发异常。例如，您不能将浮点张量和整数张量相加，甚至不能将32位浮点数和64位浮点数相加：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This may be a bit annoying at first, but remember that it’s for a good cause!
    And of course you can use `tf.cast()` when you really need to convert types:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能一开始有点烦人，但请记住这是为了一个好的目的！当然，当您真正需要转换类型时，您可以使用`tf.cast()`：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Variables
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变量
- en: 'The `tf.Tensor` values we’ve seen so far are immutable: we cannot modify them.
    This means that we cannot use regular tensors to implement weights in a neural
    network, since they need to be tweaked by backpropagation. Plus, other parameters
    may also need to change over time (e.g., a momentum optimizer keeps track of past
    gradients). What we need is a `tf.Variable`:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们看到的`tf.Tensor`值是不可变的：我们无法修改它们。这意味着我们不能使用常规张量来实现神经网络中的权重，因为它们需要通过反向传播进行调整。此外，其他参数可能也需要随时间变化（例如，动量优化器会跟踪过去的梯度）。我们需要的是`tf.Variable`：
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'A `tf.Variable` acts much like a `tf.Tensor`: you can perform the same operations
    with it, it plays nicely with NumPy as well, and it is just as picky with types.
    But it can also be modified in place using the `assign()` method (or `assign_add()`
    or `assign_sub()`, which increment or decrement the variable by the given value).
    You can also modify individual cells (or slices), by using the cell’s (or slice’s)
    `assign()` method or by using the `scatter_update()` or `scatter_nd_update()`
    methods:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.Variable`的行为很像`tf.Tensor`：您可以执行相同的操作，它与NumPy很好地配合，对类型也一样挑剔。但是它也可以使用`assign()`方法（或`assign_add()`或`assign_sub()`，它们会增加或减少给定值来就地修改变量）。您还可以使用单个单元格（或切片）的`assign()`方法或使用`scatter_update()`或`scatter_nd_update()`方法来修改单个单元格（或切片）：'
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Direct assignment will not work:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 直接赋值不起作用：
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In practice you will rarely have to create variables manually; Keras provides
    an `add_weight()` method that will take care of it for you, as you’ll see. Moreover,
    model parameters will generally be updated directly by the optimizers, so you
    will rarely need to update variables manually.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，您很少需要手动创建变量；Keras提供了一个`add_weight()`方法，它会为您处理，您将看到。此外，模型参数通常会直接由优化器更新，因此您很少需要手动更新变量。
- en: Other Data Structures
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他数据结构
- en: 'TensorFlow supports several other data structures, including the following
    (see the “Other Data Structures” section in this chapter’s notebook or [Appendix C](app03.html#structures_appendix)
    for more details):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow支持几种其他数据结构，包括以下内容（请参阅本章笔记本中的“其他数据结构”部分或[附录C](app03.html#structures_appendix)了解更多详细信息）：
- en: Sparse tensors (`tf.SparseTensor`)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏张量（`tf.SparseTensor`）
- en: Efficiently represent tensors containing mostly zeros. The `tf.sparse` package
    contains operations for sparse tensors.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 高效地表示大部分为零的张量。`tf.sparse`包含了稀疏张量的操作。
- en: Tensor arrays (`tf.TensorArray`)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 张量数组（`tf.TensorArray`）
- en: Are lists of tensors. They have a fixed length by default but can optionally
    be made extensible. All tensors they contain must have the same shape and data
    type.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 是张量列表。它们默认具有固定长度，但可以选择性地扩展。它们包含的所有张量必须具有相同的形状和数据类型。
- en: Ragged tensors (`tf.RaggedTensor`)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 不规则张量（`tf.RaggedTensor`）
- en: Represent lists of tensors, all of the same rank and data type, but with varying
    sizes. The dimensions along which the tensor sizes vary are called the *ragged
    dimensions*. The `tf.ragged` package contains operations for ragged tensors.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 表示张量列表，所有张量的秩和数据类型相同，但大小不同。张量大小变化的维度称为*不规则维度*。`tf.ragged`包含了不规则张量的操作。
- en: String tensors
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 字符串张量
- en: Are regular tensors of type `tf.string`. These represent byte strings, not Unicode
    strings, so if you create a string tensor using a Unicode string (e.g., a regular
    Python 3 string like `"café"`), then it will get encoded to UTF-8 automatically
    (e.g., `b"caf\xc3\xa9"`). Alternatively, you can represent Unicode strings using
    tensors of type `tf.int32`, where each item represents a Unicode code point (e.g.,
    `[99, 97, 102, 233]`). The `tf.strings` package (with an `s`) contains ops for
    byte strings and Unicode strings (and to convert one into the other). It’s important
    to note that a `tf.string` is atomic, meaning that its length does not appear
    in the tensor’s shape. Once you convert it to a Unicode tensor (i.e., a tensor
    of type `tf.int32` holding Unicode code points), the length appears in the shape.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 是类型为`tf.string`的常规张量。这些表示字节字符串，而不是Unicode字符串，因此如果您使用Unicode字符串（例如，像`"café"`这样的常规Python
    3字符串）创建字符串张量，那么它将自动编码为UTF-8（例如，`b"caf\xc3\xa9"`）。或者，您可以使用类型为`tf.int32`的张量来表示Unicode字符串，其中每个项目表示一个Unicode代码点（例如，`[99,
    97, 102, 233]`）。`tf.strings`包（带有`s`）包含用于字节字符串和Unicode字符串的操作（以及将一个转换为另一个的操作）。重要的是要注意`tf.string`是原子的，这意味着其长度不会出现在张量的形状中。一旦您将其转换为Unicode张量（即，一个包含Unicode代码点的`tf.int32`类型的张量），长度将出现在形状中。
- en: Sets
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 集合
- en: Are represented as regular tensors (or sparse tensors). For example, `tf.constant([[1,
    2], [3, 4]])` represents the two sets {1, 2} and {3, 4}. More generally, each
    set is represented by a vector in the tensor’s last axis. You can manipulate sets
    using operations from the `tf.sets` package.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 表示为常规张量（或稀疏张量）。例如，`tf.constant([[1, 2], [3, 4]])`表示两个集合{1, 2}和{3, 4}。更一般地，每个集合由张量的最后一个轴中的向量表示。您可以使用`tf.sets`包中的操作来操作集合。
- en: Queues
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 队列
- en: 'Store tensors across multiple steps. TensorFlow offers various kinds of queues:
    basic first-in, first-out (FIFO) queues (`FIFOQueue`), plus queues that can prioritize
    some items (`PriorityQueue`), shuffle their items (`RandomShuffleQueue`), and
    batch items of different shapes by padding (`PaddingFIFOQueue`). These classes
    are all in the `tf.queue` package.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个步骤中存储张量。TensorFlow提供各种类型的队列：基本的先进先出（FIFO）队列（`FIFOQueue`），以及可以优先处理某些项目的队列（`PriorityQueue`），对其项目进行洗牌的队列（`RandomShuffleQueue`），以及通过填充来批处理不同形状的项目的队列（`PaddingFIFOQueue`）。这些类都在`tf.queue`包中。
- en: With tensors, operations, variables, and various data structures at your disposal,
    you are now ready to customize your models and training algorithms!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 有了张量、操作、变量和各种数据结构，你现在可以定制你的模型和训练算法了！
- en: Customizing Models and Training Algorithms
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义模型和训练算法
- en: You’ll start by creating a custom loss function, which is a straightforward
    and common use case.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 你将首先创建一个自定义损失函数，这是一个简单而常见的用例。
- en: Custom Loss Functions
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义损失函数
- en: 'Suppose you want to train a regression model, but your training set is a bit
    noisy. Of course, you start by trying to clean up your dataset by removing or
    fixing the outliers, but that turns out to be insufficient; the dataset is still
    noisy. Which loss function should you use? The mean squared error might penalize
    large errors too much and cause your model to be imprecise. The mean absolute
    error would not penalize outliers as much, but training might take a while to
    converge, and the trained model might not be very precise. This is probably a
    good time to use the Huber loss (introduced in [Chapter 10](ch10.html#ann_chapter))
    instead of the good old MSE. The Huber loss is available in Keras (just use an
    instance of the `tf.keras.losses.Huber` class), but let’s pretend it’s not there.
    To implement it, just create a function that takes the labels and the model’s
    predictions as arguments, and uses TensorFlow operations to compute a tensor containing
    all the losses (one per sample):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想训练一个回归模型，但你的训练集有点嘈杂。当然，你首先尝试通过删除或修复异常值来清理数据集，但结果还不够好；数据集仍然很嘈杂。你应该使用哪种损失函数？均方误差可能会过分惩罚大误差，导致模型不够精确。平均绝对误差不会像惩罚异常值那样严重，但训练可能需要一段时间才能收敛，训练出的模型可能不够精确。这可能是使用Huber损失的好时机（在[第10章](ch10.html#ann_chapter)介绍）。Huber损失在Keras中是可用的（只需使用`tf.keras.losses.Huber`类的实例），但让我们假装它不存在。要实现它，只需创建一个函数，该函数将标签和模型预测作为参数，并使用TensorFlow操作来计算包含所有损失的张量（每个样本一个）：
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Warning
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: For better performance, you should use a vectorized implementation, as in this
    example. Moreover, if you want to benefit from TensorFlow’s graph optimization
    features, you should use only TensorFlow operations.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更好的性能，你应该使用矢量化的实现，就像这个例子一样。此外，如果你想要从TensorFlow的图优化功能中受益，你应该只使用TensorFlow操作。
- en: It is also possible to return the mean loss instead of the individual sample
    losses, but this is not recommended as it makes it impossible to use class weights
    or sample weights when you need them (see [Chapter 10](ch10.html#ann_chapter)).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以返回平均损失而不是单个样本损失，但这不推荐，因为这样做会使在需要时无法使用类权重或样本权重（参见[第10章](ch10.html#ann_chapter)）。
- en: 'Now you can use this Huber loss function when you compile the Keras model,
    then train your model as usual:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以在编译Keras模型时使用这个Huber损失函数，然后像往常一样训练你的模型：
- en: '[PRE12]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: And that’s it! For each batch during training, Keras will call the `huber_fn()`
    function to compute the loss, then it will use reverse-mode autodiff to compute
    the gradients of the loss with regard to all the model parameters, and finally
    it will perform a gradient descent step (in this example using a Nadam optimizer).
    Moreover, it will keep track of the total loss since the beginning of the epoch,
    and it will display the mean loss.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！在训练期间的每个批次中，Keras将调用`huber_fn()`函数来计算损失，然后使用反向模式自动微分来计算损失相对于所有模型参数的梯度，最后执行梯度下降步骤（在这个例子中使用Nadam优化器）。此外，它将跟踪自从epoch开始以来的总损失，并显示平均损失。
- en: But what happens to this custom loss when you save the model?
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 但是当你保存模型时，这个自定义损失会发生什么？
- en: Saving and Loading Models That Contain Custom Components
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保存和加载包含自定义组件的模型
- en: 'Saving a model containing a custom loss function works fine, but when you load
    it, you’ll need to provide a dictionary that maps the function name to the actual
    function. More generally, when you load a model containing custom objects, you
    need to map the names to the objects:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 保存包含自定义损失函数的模型可以正常工作，但是当你加载它时，你需要提供一个将函数名称映射到实际函数的字典。更一般地，当你加载包含自定义对象的模型时，你需要将名称映射到对象：
- en: '[PRE13]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Tip
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'If you decorate the `huber_fn()` function with `@keras.utils.​reg⁠ister_keras_serializable()`,
    it will automatically be available to the `load_model()` function: there’s no
    need to include it in the `custom_objects` dictionary.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你用`@keras.utils.​reg⁠ister_keras_serializable()`装饰`huber_fn()`函数，它将自动可用于`load_model()`函数：不需要将其包含在`custom_objects`字典中。
- en: 'With the current implementation, any error between –1 and 1 is considered “small”.
    But what if you want a different threshold? One solution is to create a function
    that creates a configured loss function:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 使用当前的实现，任何在-1和1之间的错误都被认为是“小”。但是如果你想要一个不同的阈值呢？一个解决方案是创建一个函数来创建一个配置好的损失函数：
- en: '[PRE14]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Unfortunately, when you save the model, the `threshold` will not be saved.
    This means that you will have to specify the `threshold` value when loading the
    model (note that the name to use is `"huber_fn"`, which is the name of the function
    you gave Keras, not the name of the function that created it):'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，当你保存模型时，`threshold`不会被保存。这意味着在加载模型时你将需要指定`threshold`的值（注意要使用的名称是`"huber_fn"`，这是你给Keras的函数的名称，而不是创建它的函数的名称）：
- en: '[PRE15]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You can solve this by creating a subclass of the `tf.keras.losses.Loss` class,
    and then implementing its `get_config()` method:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过创建`tf.keras.losses.Loss`类的子类，然后实现它的`get_config()`方法来解决这个问题：
- en: '[PRE16]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let’s walk through this code:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这段代码：
- en: 'The constructor accepts `**kwargs` and passes them to the parent constructor,
    which handles standard hyperparameters: the `name` of the loss and the `reduction`
    algorithm to use to aggregate the individual instance losses. By default this
    is `"AUTO"`, which is equivalent to `"SUM_OVER_BATCH_SIZE"`: the loss will be
    the sum of the instance losses, weighted by the sample weights, if any, and divided
    by the batch size (not by the sum of weights, so this is *not* the weighted mean).⁠^([6](ch12.html#idm45720194957792))
    Other possible values are `"SUM"` and `"NONE"`.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构造函数接受`**kwargs`并将它们传递给父构造函数，父构造函数处理标准超参数：损失的`name`和用于聚合单个实例损失的`reduction`算法。默认情况下，这是`"AUTO"`，等同于`"SUM_OVER_BATCH_SIZE"`：损失将是实例损失的总和，加权后再除以批量大小（而不是加权平均）。其他可能的值是`"SUM"`和`"NONE"`。
- en: The `call()` method takes the labels and predictions, computes all the instance
    losses, and returns them.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`call()`方法接受标签和预测值，计算所有实例损失，并返回它们。'
- en: The `get_config()` method returns a dictionary mapping each hyperparameter name
    to its value. It first calls the parent class’s `get_config()` method, then adds
    the new hyperparameters to this dictionary.⁠^([7](ch12.html#idm45720194952928))
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get_config()`方法返回一个字典，将每个超参数名称映射到其值。它首先调用父类的`get_config()`方法，然后将新的超参数添加到此字典中。'
- en: 'You can then use any instance of this class when you compile the model:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 然后您可以在编译模型时使用此类的任何实例：
- en: '[PRE17]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'When you save the model, the threshold will be saved along with it; and when
    you load the model, you just need to map the class name to the class itself:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 当您保存模型时，阈值将与模型一起保存；当您加载模型时，您只需要将类名映射到类本身：
- en: '[PRE18]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'When you save a model, Keras calls the loss instance’s `get_config()` method
    and saves the config in the SavedModel format. When you load the model, it calls
    the `from_config()` class method on the `HuberLoss` class: this method is implemented
    by the base class (`Loss`) and creates an instance of the class, passing `**config`
    to the constructor.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 当您保存模型时，Keras会调用损失实例的`get_config()`方法，并以SavedModel格式保存配置。当您加载模型时，它会在`HuberLoss`类上调用`from_config()`类方法：这个方法由基类（`Loss`）实现，并创建一个类的实例，将`**config`传递给构造函数。
- en: That’s it for losses! As you’ll see now, custom activation functions, initializers,
    regularizers, and constraints are not much different.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 损失就是这样了！正如您现在将看到的，自定义激活函数、初始化器、正则化器和约束并没有太大不同。
- en: Custom Activation Functions, Initializers, Regularizers, and Constraints
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义激活函数、初始化器、正则化器和约束
- en: 'Most Keras functionalities, such as losses, regularizers, constraints, initializers,
    metrics, activation functions, layers, and even full models, can be customized
    in much the same way. Most of the time, you will just need to write a simple function
    with the appropriate inputs and outputs. Here are examples of a custom activation
    function (equivalent to `tf.keras.activations.softplus()` or `tf.nn.softplus()`),
    a custom Glorot initializer (equivalent to `tf.keras.initializers.glorot_normal()`),
    a custom ℓ[1] regularizer (equivalent to `tf.keras.regularizers.l1(0.01)`), and
    a custom constraint that ensures weights are all positive (equivalent to `tf.keras.​con⁠straints.nonneg()`
    or `tf.nn.relu()`):'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数Keras功能，如损失、正则化器、约束、初始化器、指标、激活函数、层，甚至完整模型，都可以以类似的方式进行自定义。大多数情况下，您只需要编写一个带有适当输入和输出的简单函数。这里有一个自定义激活函数的示例（相当于`tf.keras.activations.softplus()`或`tf.nn.softplus()`）、一个自定义Glorot初始化器的示例（相当于`tf.keras.initializers.glorot_normal()`）、一个自定义ℓ[1]正则化器的示例（相当于`tf.keras.regularizers.l1(0.01)`）以及一个确保权重都为正的自定义约束的示例（相当于`tf.keras.​con⁠straints.nonneg()`或`tf.nn.relu()`）：
- en: '[PRE19]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'As you can see, the arguments depend on the type of custom function. These
    custom functions can then be used normally, as shown here:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，参数取决于自定义函数的类型。然后可以像这里展示的那样正常使用这些自定义函数：
- en: '[PRE20]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The activation function will be applied to the output of this `Dense` layer,
    and its result will be passed on to the next layer. The layer’s weights will be
    initialized using the value returned by the initializer. At each training step
    the weights will be passed to the regularization function to compute the regularization
    loss, which will be added to the main loss to get the final loss used for training.
    Finally, the constraint function will be called after each training step, and
    the layer’s weights will be replaced by the constrained weights.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数将应用于此`Dense`层的输出，并将其结果传递给下一层。层的权重将使用初始化器返回的值进行初始化。在每个训练步骤中，权重将传递给正则化函数以计算正则化损失，然后将其添加到主损失中以获得用于训练的最终损失。最后，在每个训练步骤之后，将调用约束函数，并将层的权重替换为受约束的权重。
- en: 'If a function has hyperparameters that need to be saved along with the model,
    then you will want to subclass the appropriate class, such as `tf.keras.regu⁠larizers.​​Reg⁠⁠ularizer`,
    `tf.keras.constraints.Constraint`, `tf.keras.initializers.​Ini⁠tializer`, or `tf.keras.layers.Layer`
    (for any layer, including activation functions). Much as you did for the custom
    loss, here is a simple class for ℓ[1] regularization that saves its `factor` hyperparameter
    (this time you do not need to call the parent constructor or the `get_config()`
    method, as they are not defined by the parent class):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个函数有需要与模型一起保存的超参数，那么您将希望子类化适当的类，比如`tf.keras.regu⁠larizers.​​Reg⁠⁠ularizer`、`tf.keras.constraints.Constraint`、`tf.keras.initializers.​Ini⁠tializer`或`tf.keras.layers.Layer`（适用于任何层，包括激活函数）。就像您为自定义损失所做的那样，这里是一个简单的ℓ[1]正则化类，它保存了其`factor`超参数（这次您不需要调用父构造函数或`get_config()`方法，因为它们不是由父类定义的）：
- en: '[PRE21]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Note that you must implement the `call()` method for losses, layers (including
    activation functions), and models, or the `__call__()` method for regularizers,
    initializers, and constraints. For metrics, things are a bit different, as you
    will see now.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您必须为损失、层（包括激活函数）和模型实现`call()`方法，或者为正则化器、初始化器和约束实现`__call__()`方法。对于指标，情况有些不同，您将立即看到。
- en: Custom Metrics
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义指标
- en: 'Losses and metrics are conceptually not the same thing: losses (e.g., cross
    entropy) are used by gradient descent to *train* a model, so they must be differentiable
    (at least at the points where they are evaluated), and their gradients should
    not be zero everywhere. Plus, it’s OK if they are not easily interpretable by
    humans. In contrast, metrics (e.g., accuracy) are used to *evaluate* a model:
    they must be more easily interpretable, and they can be nondifferentiable or have
    zero gradients everywhere.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 损失和指标在概念上并不相同：损失（例如，交叉熵）被梯度下降用来*训练*模型，因此它们必须是可微的（至少在评估它们的点上），它们的梯度不应该在任何地方都为零。此外，如果它们不容易被人类解释也是可以的。相反，指标（例如，准确率）用于*评估*模型：它们必须更容易被解释，可以是不可微的或者在任何地方梯度为零。
- en: 'That said, in most cases, defining a custom metric function is exactly the
    same as defining a custom loss function. In fact, we could even use the Huber
    loss function we created earlier as a metric;⁠^([8](ch12.html#idm45720194513680))
    it would work just fine (and persistence would also work the same way, in this
    case only saving the name of the function, `"huber_fn"`, not the threshold):'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，在大多数情况下，定义一个自定义指标函数与定义一个自定义损失函数完全相同。实际上，我们甚至可以使用我们之前创建的Huber损失函数作为指标；它会工作得很好（在这种情况下，持久性也会以相同的方式工作，只保存函数的名称“huber_fn”，而不是阈值）：
- en: '[PRE22]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'For each batch during training, Keras will compute this metric and keep track
    of its mean since the beginning of the epoch. Most of the time, this is exactly
    what you want. But not always! Consider a binary classifier’s precision, for example.
    As you saw in [Chapter 3](ch03.html#classification_chapter), precision is the
    number of true positives divided by the number of positive predictions (including
    both true positives and false positives). Suppose the model made five positive
    predictions in the first batch, four of which were correct: that’s 80% precision.
    Then suppose the model made three positive predictions in the second batch, but
    they were all incorrect: that’s 0% precision for the second batch. If you just
    compute the mean of these two precisions, you get 40%. But wait a second—that’s
    *not* the model’s precision over these two batches! Indeed, there were a total
    of four true positives (4 + 0) out of eight positive predictions (5 + 3), so the
    overall precision is 50%, not 40%. What we need is an object that can keep track
    of the number of true positives and the number of false positives and that can
    compute the precision based on these numbers when requested. This is precisely
    what the `tf.keras.metrics.Precision` class does:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间的每个批次，Keras将计算这个指标并跟踪自开始时的平均值。大多数情况下，这正是你想要的。但并非总是如此！例如，考虑一个二元分类器的精度。正如你在[第3章](ch03.html#classification_chapter)中看到的，精度是真正例的数量除以正例的预测数量（包括真正例和假正例）。假设模型在第一个批次中做出了五个正面预测，其中四个是正确的：这是80%的精度。然后假设模型在第二个批次中做出了三个正面预测，但它们全部是错误的：这是第二个批次的0%精度。如果你只计算这两个精度的平均值，你会得到40%。但等一下——这*不是*这两个批次的模型精度！事实上，总共有四个真正例（4
    + 0）中的八个正面预测（5 + 3），所以总体精度是50%，而不是40%。我们需要的是一个对象，它可以跟踪真正例的数量和假正例的数量，并且可以在需要时基于这些数字计算精度。这正是`tf.keras.metrics.Precision`类所做的：
- en: '[PRE23]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In this example, we created a `Precision` object, then we used it like a function,
    passing it the labels and predictions for the first batch, then for the second
    batch (you can optionally pass sample weights as well, if you want). We used the
    same number of true and false positives as in the example we just discussed. After
    the first batch, it returns a precision of 80%; then after the second batch, it
    returns 50% (which is the overall precision so far, not the second batch’s precision).
    This is called a *streaming metric* (or *stateful metric*), as it is gradually
    updated, batch after batch.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们创建了一个`Precision`对象，然后像一个函数一样使用它，为第一个批次传递标签和预测，然后为第二个批次（如果需要，还可以传递样本权重）。我们使用了与刚才讨论的示例中相同数量的真正例和假正例。在第一个批次之后，它返回80%的精度；然后在第二个批次之后，它返回50%（这是到目前为止的总体精度，而不是第二个批次的精度）。这被称为*流式指标*（或*有状态指标*），因为它逐渐更新，批次之后。
- en: 'At any point, we can call the `result()` method to get the current value of
    the metric. We can also look at its variables (tracking the number of true and
    false positives) by using the `variables` attribute, and we can reset these variables
    using the `reset_states()` method:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何时候，我们可以调用`result()`方法来获取指标的当前值。我们还可以通过使用`variables`属性查看其变量（跟踪真正例和假正例的数量），并可以使用`reset_states()`方法重置这些变量：
- en: '[PRE24]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'If you need to define your own custom streaming metric, create a subclass of
    the `tf.keras.metrics.Metric` class. Here is a basic example that keeps track
    of the total Huber loss and the number of instances seen so far. When asked for
    the result, it returns the ratio, which is just the mean Huber loss:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要定义自己的自定义流式指标，创建`tf.keras.metrics.Metric`类的子类。这里是一个基本示例，它跟踪总Huber损失和迄今为止看到的实例数量。当要求结果时，它返回比率，这只是平均Huber损失：
- en: '[PRE25]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Let’s walk through this code:⁠^([9](ch12.html#idm45720194258944))
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们走一遍这段代码：
- en: The constructor uses the `add_weight()` method to create the variables needed
    to keep track of the metric’s state over multiple batches—in this case, the sum
    of all Huber losses (`total`) and the number of instances seen so far (`count`).
    You could just create variables manually if you preferred. Keras tracks any `tf.Variable`
    that is set as an attribute (and more generally, any “trackable” object, such
    as layers or models).
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构造函数使用`add_weight()`方法创建需要在多个批次中跟踪指标状态的变量——在这种情况下，所有Huber损失的总和（`total`）和迄今为止看到的实例数量（`count`）。如果愿意，你也可以手动创建变量。Keras跟踪任何设置为属性的`tf.Variable`（更一般地，任何“可跟踪”的对象，如层或模型）。
- en: The `update_state()` method is called when you use an instance of this class
    as a function (as we did with the `Precision` object). It updates the variables,
    given the labels and predictions for one batch (and sample weights, but in this
    case we ignore them).
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你将这个类的实例用作函数时（就像我们用`Precision`对象做的那样），`update_state()`方法会被调用。它根据一个批次的标签和预测更新变量（以及样本权重，但在这种情况下我们忽略它们）。
- en: The `result()` method computes and returns the final result, in this case the
    mean Huber metric over all instances. When you use the metric as a function, the
    `update_state()` method gets called first, then the `result()` method is called,
    and its output is returned.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`result()`方法计算并返回最终结果，在这种情况下是所有实例上的平均 Huber 指标。当你将指标用作函数时，首先调用`update_state()`方法，然后调用`result()`方法，并返回其输出。'
- en: We also implement the `get_config()` method to ensure the `threshold` gets saved
    along with the model.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还实现了`get_config()`方法，以确保`threshold`与模型一起保存。
- en: The default implementation of the `reset_states()` method resets all variables
    to 0.0 (but you can override it if needed).
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reset_states()`方法的默认实现将所有变量重置为0.0（但如果需要，你可以覆盖它）。'
- en: Note
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Keras will take care of variable persistence seamlessly; no action is required.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 会无缝处理变量持久性；不需要任何操作。
- en: 'When you define a metric using a simple function, Keras automatically calls
    it for each batch, and it keeps track of the mean during each epoch, just like
    we did manually. So the only benefit of our `HuberMetric` class is that the `threshold`
    will be saved. But of course, some metrics, like precision, cannot simply be averaged
    over batches: in those cases, there’s no other option than to implement a streaming
    metric.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用简单函数定义指标时，Keras 会自动为每个批次调用它，并在每个时期期间跟踪平均值，就像我们手动做的那样。因此，我们的`HuberMetric`类的唯一好处是`threshold`将被保存。但当然，有些指标，比如精度，不能简单地在批次上进行平均：在这些情况下，除了实现流式指标之外别无选择。
- en: Now that you’ve built a streaming metric, building a custom layer will seem
    like a walk in the park!
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经构建了一个流式指标，构建一个自定义层将会变得轻而易举！
- en: Custom Layers
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义层
- en: You may occasionally want to build an architecture that contains an exotic layer
    for which TensorFlow does not provide a default implementation. Or you may simply
    want to build a very repetitive architecture, in which a particular block of layers
    is repeated many times, and it would be convenient to treat each block as a single
    layer. For such cases, you’ll want to build a custom layer.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候你可能想要构建一个包含一种 TensorFlow 没有提供默认实现的奇特层的架构。或者你可能只是想要构建一个非常重复的架构，在这种架构中，一个特定的层块被重复多次，将每个块视为单个层会很方便。对于这些情况，你会想要构建一个自定义层。
- en: 'There are some layers that have no weights, such as `tf.keras.layers.Flatten`
    or `tf.keras.layers.ReLU`. If you want to create a custom layer without any weights,
    the simplest option is to write a function and wrap it in a `tf.keras.layers.Lambda`
    layer. For example, the following layer will apply the exponential function to
    its inputs:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些没有权重的层，比如`tf.keras.layers.Flatten`或`tf.keras.layers.ReLU`。如果你想创建一个没有任何权重的自定义层，最简单的方法是编写一个函数并将其包装在`tf.keras.layers.Lambda`层中。例如，以下层将对其输入应用指数函数：
- en: '[PRE26]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This custom layer can then be used like any other layer, using the sequential
    API, the functional API, or the subclassing API. You can also use it as an activation
    function, or you could use `activation=tf.exp`. The exponential layer is sometimes
    used in the output layer of a regression model when the values to predict have
    very different scales (e.g., 0.001, 10., 1,000.). In fact, the exponential function
    is one of the standard activation functions in Keras, so you can just use `activation="exponential"`.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这个自定义层可以像任何其他层一样使用，使用序贯 API、函数式 API 或子类 API。你也可以将它用作激活函数，或者你可以使用`activation=tf.exp`。指数层有时用于回归模型的输出层，当要预测的值具有非常不同的规模时（例如，0.001、10.、1,000.）。事实上，指数函数是
    Keras 中的标准激活函数之一，所以你可以简单地使用`activation="exponential"`。
- en: 'As you might guess, to build a custom stateful layer (i.e., a layer with weights),
    you need to create a subclass of the `tf.keras.layers.Layer` class. For example,
    the following class implements a simplified version of the `Dense` layer:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会猜到，要构建一个自定义的有状态层（即带有权重的层），你需要创建`tf.keras.layers.Layer`类的子类。例如，以下类实现了`Dense`层的简化版本：
- en: '[PRE27]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let’s walk through this code:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这段代码：
- en: 'The constructor takes all the hyperparameters as arguments (in this example,
    `units` and `activation`), and importantly it also takes a `**kwargs` argument.
    It calls the parent constructor, passing it the `kwargs`: this takes care of standard
    arguments such as `input_shape`, `trainable`, and `name`. Then it saves the hyperparameters
    as attributes, converting the `activation` argument to the appropriate activation
    function using the `tf.keras.activations.get()` function (it accepts functions,
    standard strings like `"relu"` or `"swish"`, or simply `None`).'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构造函数将所有超参数作为参数（在这个例子中是`units`和`activation`），并且重要的是它还接受一个`**kwargs`参数。它调用父构造函数，将`kwargs`传递给它：这会处理标准参数，如`input_shape`、`trainable`和`name`。然后它将超参数保存为属性，使用`tf.keras.activations.get()`函数将`activation`参数转换为适当的激活函数（它接受函数、标准字符串如`"relu"`或`"swish"`，或者简单地`None`）。
- en: 'The `build()` method’s role is to create the layer’s variables by calling the
    `add_weight()` method for each weight. The `build()` method is called the first
    time the layer is used. At that point, Keras will know the shape of this layer’s
    inputs, and it will pass it to the `build()` method,⁠^([10](ch12.html#idm45720193832656))
    which is often necessary to create some of the weights. For example, we need to
    know the number of neurons in the previous layer in order to create the connection
    weights matrix (i.e., the `"kernel"`): this corresponds to the size of the last
    dimension of the inputs. At the end of the `build()` method (and only at the end),
    you must call the parent’s `build()` method: this tells Keras that the layer is
    built (it just sets `self.built = True`).'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `call()` method performs the desired operations. In this case, we compute
    the matrix multiplication of the inputs `X` and the layer’s kernel, we add the
    bias vector, and we apply the activation function to the result, and this gives
    us the output of the layer.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `get_config()` method is just like in the previous custom classes. Note
    that we save the activation function’s full configuration by calling `tf.keras.​activa⁠tions.serialize()`.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can now use a `MyDense` layer just like any other layer!
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-154
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Keras automatically infers the output shape, except when the layer is dynamic
    (as you will see shortly). In this (rare) case, you need to implement the `compute_output_shape()`
    method, which must return a `TensorShape` object.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a layer with multiple inputs (e.g., `Concatenate`), the argument
    to the `call()` method should be a tuple containing all the inputs. To create
    a layer with multiple outputs, the `call()` method should return the list of outputs.
    For example, the following toy layer takes two inputs and returns three outputs:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This layer may now be used like any other layer, but of course only using the
    functional and subclassing APIs, not the sequential API (which only accepts layers
    with one input and one output).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'If your layer needs to have a different behavior during training and during
    testing (e.g., if it uses `Dropout` or `BatchNormalization` layers), then you
    must add a `training` argument to the `call()` method and use this argument to
    decide what to do. For example, let’s create a layer that adds Gaussian noise
    during training (for regularization) but does nothing during testing (Keras has
    a layer that does the same thing, `tf.keras.layers.GaussianNoise`):'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: With that, you can now build any custom layer you need! Now let’s look at how
    to create custom models.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Custom Models
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We already looked at creating custom model classes in [Chapter 10](ch10.html#ann_chapter),
    when we discussed the subclassing API.⁠^([11](ch12.html#idm45720193550128)) It’s
    straightforward: subclass the `tf.keras.Model` class, create layers and variables
    in the constructor, and implement the `call()` method to do whatever you want
    the model to do. For example, suppose we want to build the model represented in
    [Figure 12-3](#custom_model_diagram).'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1203](assets/mls3_1203.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12-3\. Custom model example: an arbitrary model with a custom `ResidualBlock`
    layer containing a skip connection'
  id: totrans-165
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The inputs go through a first dense layer, then through a *residual block*
    composed of two dense layers and an addition operation (as you will see in [Chapter 14](ch14.html#cnn_chapter),
    a residual block adds its inputs to its outputs), then through this same residual
    block three more times, then through a second residual block, and the final result
    goes through a dense output layer. Don’t worry if this model does not make much
    sense; it’s just an example to illustrate the fact that you can easily build any
    kind of model you want, even one that contains loops and skip connections. To
    implement this model, it is best to first create a `ResidualBlock` layer, since
    we are going to create a couple of identical blocks (and we might want to reuse
    it in another model):'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This layer is a bit special since it contains other layers. This is handled
    transparently by Keras: it automatically detects that the `hidden` attribute contains
    trackable objects (layers in this case), so their variables are automatically
    added to this layer’s list of variables. The rest of this class is self-explanatory.
    Next, let’s use the subclassing API to define the model itself:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这个层有点特殊，因为它包含其他层。Keras会自动处理这一点：它会自动检测`hidden`属性包含可跟踪对象（在这种情况下是层），因此它们的变量会自动添加到此层的变量列表中。这个类的其余部分是不言自明的。接下来，让我们使用子类API来定义模型本身：
- en: '[PRE31]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We create the layers in the constructor and use them in the `call()` method.
    This model can then be used like any other model (compile it, fit it, evaluate
    it, and use it to make predictions). If you also want to be able to save the model
    using the `save()` method and load it using the `tf.keras.models.load_model()`
    function, you must implement the `get_config()` method (as we did earlier) in
    both the `ResidualBlock` class and the `ResidualRegressor` class. Alternatively,
    you can save and load the weights using the `save_weights()` and `load_weights()`
    methods.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在构造函数中创建层，并在`call()`方法中使用它们。然后可以像任何其他模型一样使用此模型（编译、拟合、评估和使用它进行预测）。如果您还希望能够使用`save()`方法保存模型，并使用`tf.keras.models.load_model()`函数加载模型，则必须在`ResidualBlock`类和`ResidualRegressor`类中实现`get_config()`方法（就像我们之前做的那样）。或者，您可以使用`save_weights()`和`load_weights()`方法保存和加载权重。
- en: The `Model` class is a subclass of the `Layer` class, so models can be defined
    and used exactly like layers. But a model has some extra functionalities, including
    of course its `compile()`, `fit()`, `evaluate()`, and `predict()` methods (and
    a few variants), plus the `get_layer()` method (which can return any of the model’s
    layers by name or by index) and the `save()` method (and support for `tf.keras.models.load_model()`
    and `tf.keras.models.clone_model()`).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`Model`类是`Layer`类的子类，因此模型可以像层一样定义和使用。但是模型具有一些额外的功能，包括当然包括`compile()`、`fit()`、`evaluate()`和`predict()`方法（以及一些变体），还有`get_layer()`方法（可以通过名称或索引返回模型的任何层）和`save()`方法（以及对`tf.keras.models.load_model()`和`tf.keras.models.clone_model()`的支持）。'
- en: Tip
  id: totrans-172
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If models provide more functionality than layers, why not just define every
    layer as a model? Well, technically you could, but it is usually cleaner to distinguish
    the internal components of your model (i.e., layers or reusable blocks of layers)
    from the model itself (i.e., the object you will train). The former should subclass
    the `Layer` class, while the latter should subclass the `Model` class.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型提供的功能比层更多，为什么不将每个层都定义为模型呢？技术上您可以这样做，但通常更清晰的做法是区分模型的内部组件（即层或可重用的层块）和模型本身（即您将训练的对象）。前者应该是`Layer`类的子类，而后者应该是`Model`类的子类。
- en: 'With that, you can naturally and concisely build almost any model that you
    find in a paper, using the sequential API, the functional API, the subclassing
    API, or even a mix of these. “Almost” any model? Yes, there are still a few things
    that we need to look at: first, how to define losses or metrics based on model
    internals, and second, how to build a custom training loop.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，您可以自然而简洁地构建几乎任何您在论文中找到的模型，使用顺序API、函数API、子类API，甚至这些的混合。“几乎”任何模型？是的，还有一些事情我们需要看一下：首先是如何基于模型内部定义损失或指标，其次是如何构建自定义训练循环。
- en: Losses and Metrics Based on Model Internals
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于模型内部的损失和指标
- en: The custom losses and metrics we defined earlier were all based on the labels
    and the predictions (and optionally sample weights). There will be times when
    you want to define losses based on other parts of your model, such as the weights
    or activations of its hidden layers. This may be useful for regularization purposes
    or to monitor some internal aspect of your model.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前定义的自定义损失和指标都是基于标签和预测（以及可选的样本权重）。有时您可能希望基于模型的其他部分（例如其隐藏层的权重或激活）定义损失。这可能对正则化目的或监视模型的某些内部方面很有用。
- en: 'To define a custom loss based on model internals, compute it based on any part
    of the model you want, then pass the result to the `add_loss()` method. For example,
    let’s build a custom regression MLP model composed of a stack of five hidden layers
    plus an output layer. This custom model will also have an auxiliary output on
    top of the upper hidden layer. The loss associated with this auxiliary output
    will be called the *reconstruction loss* (see [Chapter 17](ch17.html#autoencoders_chapter)):
    it is the mean squared difference between the reconstruction and the inputs. By
    adding this reconstruction loss to the main loss, we will encourage the model
    to preserve as much information as possible through the hidden layers—even information
    that is not directly useful for the regression task itself. In practice, this
    loss sometimes improves generalization (it is a regularization loss). It is also
    possible to add a custom metric using the model’s `add_metric()` method. Here
    is the code for this custom model with a custom reconstruction loss and a corresponding
    metric:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 要基于模型内部定义自定义损失，可以根据模型的任何部分计算损失，然后将结果传递给`add_loss()`方法。例如，让我们构建一个由五个隐藏层堆叠加一个输出层组成的自定义回归MLP模型。这个自定义模型还将在最上面的隐藏层之上具有一个辅助输出。与这个辅助输出相关联的损失将被称为*重建损失*（参见[第17章](ch17.html#autoencoders_chapter)）：它是重建和输入之间的均方差差异。通过将这个重建损失添加到主要损失中，我们将鼓励模型通过隐藏层尽可能保留更多信息，即使这些信息对于回归任务本身并不直接有用。在实践中，这种损失有时会改善泛化能力（它是一种正则化损失）。还可以使用模型的`add_metric()`方法添加自定义指标。以下是具有自定义重建损失和相应指标的自定义模型的代码：
- en: '[PRE32]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Let’s go through this code:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下这段代码：
- en: The constructor creates the DNN with five dense hidden layers and one dense
    output layer. We also create a `Mean` streaming metric to keep track of the reconstruction
    error during training.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构造函数创建了一个具有五个密集隐藏层和一个密集输出层的DNN。我们还创建了一个`Mean`流式指标，用于在训练过程中跟踪重建误差。
- en: The `build()` method creates an extra dense layer that will be used to reconstruct
    the inputs of the model. It must be created here because its number of units must
    be equal to the number of inputs, and this number is unknown before the `build()`
    method is called.⁠^([12](ch12.html#idm45720192957584))
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`build()`方法创建一个额外的密集层，用于重构模型的输入。它必须在这里创建，因为其单元数必须等于输入的数量，在调用`build()`方法之前这个数量是未知的。'
- en: The `call()` method processes the inputs through all five hidden layers, then
    passes the result through the reconstruction layer, which produces the reconstruction.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`call()`方法通过所有五个隐藏层处理输入，然后将结果传递给重构层，该层生成重构。'
- en: Then the `call()` method computes the reconstruction loss (the mean squared
    difference between the reconstruction and the inputs), and adds it to the model’s
    list of losses using the `add_loss()` method.⁠^([13](ch12.html#idm45720192953232))
    Notice that we scale down the reconstruction loss by multiplying it by 0.05 (this
    is a hyperparameter you can tune). This ensures that the reconstruction loss does
    not dominate the main loss.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后`call()`方法计算重构损失（重构和输入之间的均方差），并使用`add_loss()`方法将其添加到模型的损失列表中。请注意，我们通过将重构损失乘以0.05来缩小重构损失（这是一个可以调整的超参数）。这确保了重构损失不会主导主要损失。
- en: 'Next, during training only, the `call()` method updates the reconstruction
    metric and adds it to the model so it can be displayed. This code example can
    actually be simplified by calling `self.add_metric(recon_loss)` instead: Keras
    will automatically track the mean for you.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，在训练过程中，`call()`方法更新重构度量并将其添加到模型中以便显示。这段代码示例实际上可以通过调用`self.add_metric(recon_loss)`来简化：Keras将自动为您跟踪均值。
- en: Finally, the `call()` method passes the output of the hidden layers to the output
    layer and returns its output.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，`call()`方法将隐藏层的输出传递给输出层，并返回其输出。
- en: 'Both the total loss and the reconstruction loss will go down during training:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，总损失和重构损失都会下降：
- en: '[PRE33]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: In most cases, everything we have discussed so far will be sufficient to implement
    whatever model you want to build, even with complex architectures, losses, and
    metrics. However, for some architectures, such as GANs (see [Chapter 17](ch17.html#autoencoders_chapter)),
    you will have to customize the training loop itself. Before we get there, we must
    look at how to compute gradients automatically in TensorFlow.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，到目前为止我们讨论的一切将足以实现您想构建的任何模型，即使是具有复杂架构、损失和指标。然而，对于一些架构，如GANs（参见[第17章](ch17.html#autoencoders_chapter)），您将不得不自定义训练循环本身。在我们到达那里之前，我们必须看看如何在TensorFlow中自动计算梯度。
- en: Computing Gradients Using Autodiff
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用自动微分计算梯度
- en: 'To understand how to use autodiff (see [Chapter 10](ch10.html#ann_chapter)
    and [Appendix B](app02.html#autodiff_appendix)) to compute gradients automatically,
    let’s consider a simple toy function:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何使用自动微分（参见[第10章](ch10.html#ann_chapter)和[附录B](app02.html#autodiff_appendix)）自动计算梯度，让我们考虑一个简单的玩具函数：
- en: '[PRE34]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'If you know calculus, you can analytically find that the partial derivative
    of this function with regard to `w1` is `6 * w1` `+` `2 * w2`. You can also find
    that its partial derivative with regard to `w2` is `2 * w1`. For example, at the
    point `(w1, w2)` `=` `(5, 3)`, these partial derivatives are equal to 36 and 10,
    respectively, so the gradient vector at this point is (36, 10). But if this were
    a neural network, the function would be much more complex, typically with tens
    of thousands of parameters, and finding the partial derivatives analytically by
    hand would be a virtually impossible task. One solution could be to compute an
    approximation of each partial derivative by measuring how much the function’s
    output changes when you tweak the corresponding parameter by a tiny amount:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你懂微积分，你可以分析地找到这个函数相对于`w1`的偏导数是`6 * w1` `+` `2 * w2`。你也可以找到它相对于`w2`的偏导数是`2
    * w1`。例如，在点`(w1, w2)` `=` `(5, 3)`，这些偏导数分别等于36和10，因此在这一点的梯度向量是（36，10）。但如果这是一个神经网络，这个函数会复杂得多，通常有数万个参数，通过手工分析找到偏导数将是一个几乎不可能的任务。一个解决方案是通过测量当你微调相应参数一点点时函数的输出如何变化来计算每个偏导数的近似值：
- en: '[PRE35]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Looks about right! This works rather well and is easy to implement, but it
    is just an approximation, and importantly you need to call `f()` at least once
    per parameter (not twice, since we could compute `f(w1, w2)` just once). Having
    to call `f()` at least once per parameter makes this approach intractable for
    large neural networks. So instead, we should use reverse-mode autodiff. TensorFlow
    makes this pretty simple:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来不错！这个方法运行得相当好，而且易于实现，但它只是一个近似值，重要的是你需要至少针对每个参数调用一次`f()`（不是两次，因为我们可以只计算一次`f(w1,
    w2)`）。每个参数至少调用一次`f()`使得这种方法在大型神经网络中变得难以处理。因此，我们应该使用反向模式自动微分。TensorFlow使这变得非常简单：
- en: '[PRE36]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We first define two variables `w1` and `w2`, then we create a `tf.GradientTape`
    context that will automatically record every operation that involves a variable,
    and finally we ask this tape to compute the gradients of the result `z` with regard
    to both variables `[w1, w2]`. Let’s take a look at the gradients that TensorFlow
    computed:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 首先我们定义两个变量`w1`和`w2`，然后我们创建一个`tf.GradientTape`上下文，它将自动记录涉及变量的每个操作，最后我们要求这个磁带计算结果`z`相对于两个变量`[w1,
    w2]`的梯度。让我们看看TensorFlow计算的梯度：
- en: '[PRE37]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Perfect! Not only is the result accurate (the precision is only limited by the
    floating-point errors), but the `gradient()` method only goes through the recorded
    computations once (in reverse order), no matter how many variables there are,
    so it is incredibly efficient. It’s like magic!
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！结果不仅准确（精度仅受浮点误差限制），而且`gradient()`方法只需通过记录的计算一次（按相反顺序），无论有多少变量，因此非常高效。就像魔术一样！
- en: Tip
  id: totrans-199
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: In order to save memory, only put the strict minimum inside the `tf.GradientTape()`
    block. Alternatively, pause recording by creating a `with tape.stop_recording()`
    block inside the `tf.GradientTape()` block.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 为了节省内存，在`tf.GradientTape()`块中只放入严格的最小值。或者，通过在`tf.GradientTape()`块内创建一个`with
    tape.stop_recording()`块来暂停记录。
- en: 'The tape is automatically erased immediately after you call its `gradient()`
    method, so you will get an exception if you try to call `gradient()` twice:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用其`gradient()`方法后，磁带会立即被擦除，因此如果尝试两次调用`gradient()`，将会收到异常：
- en: '[PRE38]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: If you need to call `gradient()` more than once, you must make the tape persistent
    and delete it each time you are done with it to free resources:⁠^([14](ch12.html#idm45720192614784))
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要多次调用`gradient()`，您必须使磁带持久化，并在每次完成后删除它以释放资源：
- en: '[PRE39]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'By default, the tape will only track operations involving variables, so if
    you try to compute the gradient of `z` with regard to anything other than a variable,
    the result will be `None`:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，磁带只会跟踪涉及变量的操作，因此，如果您尝试计算`z`相对于除变量以外的任何东西的梯度，结果将是`None`：
- en: '[PRE40]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'However, you can force the tape to watch any tensors you like, to record every
    operation that involves them. You can then compute gradients with regard to these
    tensors, as if they were variables:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，您可以强制磁带监视任何您喜欢的张量，记录涉及它们的每个操作。然后，您可以计算相对于这些张量的梯度，就像它们是变量一样：
- en: '[PRE41]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This can be useful in some cases, like if you want to implement a regularization
    loss that penalizes activations that vary a lot when the inputs vary little: the
    loss will be based on the gradient of the activations with regard to the inputs.
    Since the inputs are not variables, you’ll need to tell the tape to watch them.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，这可能很有用，比如如果您想要实现一个正则化损失，惩罚激活在输入变化很小时变化很大的情况：损失将基于激活相对于输入的梯度。由于输入不是变量，您需要告诉磁带监视它们。
- en: 'Most of the time a gradient tape is used to compute the gradients of a single
    value (usually the loss) with regard to a set of values (usually the model parameters).
    This is where reverse-mode autodiff shines, as it just needs to do one forward
    pass and one reverse pass to get all the gradients at once. If you try to compute
    the gradients of a vector, for example a vector containing multiple losses, then
    TensorFlow will compute the gradients of the vector’s sum. So if you ever need
    to get the individual gradients (e.g., the gradients of each loss with regard
    to the model parameters), you must call the tape’s `jacobian()` method: it will
    perform reverse-mode autodiff once for each loss in the vector (all in parallel
    by default). It is even possible to compute second-order partial derivatives (the
    Hessians, i.e., the partial derivatives of the partial derivatives), but this
    is rarely needed in practice (see the “Computing Gradients Using Autodiff” section
    of this chapter’s notebook for an example).'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数情况下，梯度磁带用于计算单个值（通常是损失）相对于一组值（通常是模型参数）的梯度。这就是反向模式自动微分的优势所在，因为它只需要进行一次前向传递和一次反向传递就可以一次性获得所有梯度。如果尝试计算向量的梯度，例如包含多个损失的向量，那么TensorFlow将计算向量总和的梯度。因此，如果您需要获取各个梯度（例如，每个损失相对于模型参数的梯度），您必须调用磁带的`jacobian()`方法：它将为向量中的每个损失执行一次反向模式自动微分（默认情况下全部并行）。甚至可以计算二阶偏导数（Hessians，即偏导数的偏导数），但在实践中很少需要（请参阅本章笔记本的“使用自动微分计算梯度”部分以获取示例）。
- en: 'In some cases you may want to stop gradients from backpropagating through some
    part of your neural network. To do this, you must use the `tf.stop_gradient()`
    function. The function returns its inputs during the forward pass (like `tf.identity()`),
    but it does not let gradients through during backpropagation (it acts like a constant):'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，您可能希望阻止梯度通过神经网络的某些部分进行反向传播。为此，您必须使用`tf.stop_gradient()`函数。该函数在前向传递期间返回其输入（类似于`tf.identity()`），但在反向传播期间不允许梯度通过（它的作用类似于常数）：
- en: '[PRE42]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Finally, you may occasionally run into some numerical issues when computing
    gradients. For example, if you compute the gradients of the square root function
    at *x* = 10^(–50), the result will be infinite. In reality, the slope at that
    point is not infinite, but it’s more than 32-bit floats can handle:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当计算梯度时，您可能偶尔会遇到一些数值问题。例如，如果在*x*=10^（-50）处计算平方根函数的梯度，结果将是无穷大。实际上，该点的斜率并不是无穷大，但它超过了32位浮点数的处理能力：
- en: '[PRE43]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: To solve this, it’s often a good idea to add a tiny value to *x* (such as 10^(–6))
    when computing its square root.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，在计算平方根时，通常建议向*x*（例如10^（-6））添加一个微小值。
- en: 'The exponential function is also a frequent source of headaches, as it grows
    extremely fast. For example, the way `my_softplus()` was defined earlier is not
    numerically stable. If you compute `my_softplus(100.0)`, you will get infinity
    rather than the correct result (about 100). But it’s possible to rewrite the function
    to make it numerically stable: the softplus function is defined as log(1 + exp(*z*)),
    which is also equal to log(1 + exp(–|*z*|)) + max(*z*, 0) (see the notebook for
    the mathematical proof) and the advantage of this second form is that the exponential
    term cannot explode. So, here’s a better implementation of the `my_softplus()`
    function:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 指数函数也经常引起头痛，因为它增长非常快。例如，之前定义的`my_softplus()`的方式在数值上不稳定。如果计算`my_softplus(100.0)`，您将得到无穷大而不是正确的结果（约为100）。但是可以重写该函数以使其在数值上稳定：softplus函数被定义为log(1
    + exp(*z*))，这也等于log(1 + exp(–|*z*|)) + max(*z*, 0)（请参阅数学证明的笔记本），第二种形式的优势在于指数项不会爆炸。因此，这是`my_softplus()`函数的更好实现：
- en: '[PRE44]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'In some rare cases, a numerically stable function may still have numerically
    unstable gradients. In such cases, you will have to tell TensorFlow which equation
    to use for the gradients, rather than letting it use autodiff. For this, you must
    use the `@tf.​cus⁠tom_gradient` decorator when defining the function, and return
    both the function’s usual result and a function that computes the gradients. For
    example, let’s update the `my_softplus()` function to also return a numerically
    stable gradients function:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些罕见的情况下，一个数值稳定的函数可能仍然具有数值不稳定的梯度。在这种情况下，你将不得不告诉TensorFlow使用哪个方程来计算梯度，而不是让它使用自动微分。为此，你必须在定义函数时使用`@tf.​cus⁠tom_gradient`装饰器，并返回函数的通常结果以及计算梯度的函数。例如，让我们更新`my_softplus()`函数，使其也返回一个数值稳定的梯度函数：
- en: '[PRE45]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'If you know differential calculus (see the tutorial notebook on this topic),
    you can find that the derivative of log(1 + exp(*z*)) is exp(*z*) / (1 + exp(*z*)).
    But this form is not stable: for large values of *z*, it ends up computing infinity
    divided by infinity, which returns NaN. However, with a bit of algebraic manipulation,
    you can show that it’s also equal to 1 – 1 / (1 + exp(*z*)), which *is* stable.
    The `my_softplus_gradients()` function uses this equation to compute the gradients.
    Note that this function will receive as input the gradients that were backpropagated
    so far, down to the `my_softplus()` function, and according to the chain rule
    we must multiply them with this function’s gradients.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你懂微积分（参见关于这个主题的教程笔记本），你会发现log(1 + exp(*z*))的导数是exp(*z*) / (1 + exp(*z*))。但这种形式是不稳定的：对于较大的*z*值，它最终会计算出无穷大除以无穷大，返回NaN。然而，通过一点代数操作，你可以证明它也等于1
    - 1 / (1 + exp(*z*))，这是稳定的。`my_softplus_gradients()`函数使用这个方程来计算梯度。请注意，这个函数将接收到目前为止反向传播的梯度，一直到`my_softplus()`函数，并根据链式法则，我们必须将它们与这个函数的梯度相乘。
- en: Now when we compute the gradients of the `my_softplus()` function, we get the
    proper result, even for large input values.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在当我们计算`my_softplus()`函数的梯度时，即使对于较大的输入值，我们也会得到正确的结果。
- en: Congratulations! You can now compute the gradients of any function (provided
    it is differentiable at the point where you compute it), even blocking backpropagation
    when needed, and write your own gradient functions! This is probably more flexibility
    than you will ever need, even if you build your own custom training loops. You’ll
    see how to do that next.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！现在你可以计算任何函数的梯度（只要在计算时它是可微的），甚至在需要时阻止反向传播，并编写自己的梯度函数！这可能比你需要的灵活性更多，即使你构建自己的自定义训练循环。接下来你将看到如何做到这一点。
- en: Custom Training Loops
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义训练循环
- en: 'In some cases, the `fit()` method may not be flexible enough for what you need
    to do. For example, the [Wide & Deep paper](https://homl.info/widedeep) we discussed
    in [Chapter 10](ch10.html#ann_chapter) uses two different optimizers: one for
    the wide path and the other for the deep path. Since the `fit()` method only uses
    one optimizer (the one that we specify when compiling the model), implementing
    this paper requires writing your own custom loop.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，`fit()`方法可能不够灵活以满足你的需求。例如，我们在[第10章](ch10.html#ann_chapter)中讨论的[Wide &
    Deep论文](https://homl.info/widedeep)使用了两种不同的优化器：一种用于宽路径，另一种用于深路径。由于`fit()`方法只使用一个优化器（在编译模型时指定的那个），实现这篇论文需要编写自己的自定义循环。
- en: You may also like to write custom training loops simply to feel more confident
    that they do precisely what you intend them to do (perhaps you are unsure about
    some details of the `fit()` method). It can sometimes feel safer to make everything
    explicit. However, remember that writing a custom training loop will make your
    code longer, more error-prone, and harder to maintain.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能也喜欢编写自定义训练循环，只是为了更有信心地确保它们确实按照你的意图执行（也许你对`fit()`方法的一些细节不确定）。有时候，让一切都显式化可能会感觉更安全。然而，请记住，编写自定义训练循环会使你的代码变得更长、更容易出错，并且更难维护。
- en: Tip
  id: totrans-226
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Unless you’re learning or you really need the extra flexibility, you should
    prefer using the `fit()` method rather than implementing your own training loop,
    especially if you work in a team.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 除非你在学习或确实需要额外的灵活性，否则应该优先使用`fit()`方法而不是实现自己的训练循环，特别是如果你在团队中工作。
- en: 'First, let’s build a simple model. There’s no need to compile it, since we
    will handle the training loop manually:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们构建一个简单的模型。不需要编译它，因为我们将手动处理训练循环：
- en: '[PRE46]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Next, let’s create a tiny function that will randomly sample a batch of instances
    from the training set (in [Chapter 13](ch13.html#data_chapter) we will discuss
    the tf.data API, which offers a much better alternative):'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们创建一个小函数，从训练集中随机抽取一个批次的实例（在[第13章](ch13.html#data_chapter)中，我们将讨论tf.data
    API，它提供了一个更好的替代方案）：
- en: '[PRE47]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Let’s also define a function that will display the training status, including
    the number of steps, the total number of steps, the mean loss since the start
    of the epoch (we will use the `Mean` metric to compute it), and other metrics:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们还定义一个函数，用于显示训练状态，包括步数、总步数、自开始时的平均损失（我们将使用`Mean`指标来计算），以及其他指标：
- en: '[PRE48]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'This code is self-explanatory, unless you are unfamiliar with Python string
    formatting: `{m.result():.4f}` will format the metric’s result as a float with
    four digits after the decimal point, and using `\r` (carriage return) along with
    `end=""` ensures that the status bar always gets printed on the same line.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码很容易理解，除非你不熟悉Python的字符串格式化：`{m.result():.4f}`将指标的结果格式化为小数点后四位的浮点数，使用`\r`（回车）和`end=""`确保状态栏始终打印在同一行上。
- en: 'With that, let’s get down to business! First, we need to define some hyperparameters
    and choose the optimizer, the loss function, and the metrics (just the MAE in
    this example):'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，让我们开始吧！首先，我们需要定义一些超参数，并选择优化器、损失函数和指标（在这个例子中只有MAE）：
- en: '[PRE49]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: And now we are ready to build the custom loop!
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备构建自定义循环了！
- en: '[PRE50]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'There’s a lot going on in this code, so let’s walk through it:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码中有很多内容，让我们来逐步解释一下：
- en: 'We create two nested loops: one for the epochs, the other for the batches within
    an epoch.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then we sample a random batch from the training set.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inside the `tf.GradientTape()` block, we make a prediction for one batch, using
    the model as a function, and we compute the loss: it is equal to the main loss
    plus the other losses (in this model, there is one regularization loss per layer).
    Since the `mean_squared_error()` function returns one loss per instance, we compute
    the mean over the batch using `tf.reduce_mean()` (if you wanted to apply different
    weights to each instance, this is where you would do it). The regularization losses
    are already reduced to a single scalar each, so we just need to sum them (using
    `tf.add_n()`, which sums multiple tensors of the same shape and data type).'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we ask the tape to compute the gradients of the loss with regard to each
    trainable variable—*not* all variables!—and we apply them to the optimizer to
    perform a gradient descent step.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then we update the mean loss and the metrics (over the current epoch), and we
    display the status bar.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of each epoch, we reset the states of the mean loss and the metrics.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you want to apply gradient clipping (see [Chapter 11](ch11.html#deep_chapter)),
    set the optimizer’s `clipnorm` or `clipvalue` hyperparameter. If you want to apply
    any other transformation to the gradients, simply do so before calling the `apply_gradients()`
    method. And if you want to add weight constraints to your model (e.g., by setting
    `kernel_constraint` or `bias_constraint` when creating a layer), you should update
    the training loop to apply these constraints just after `apply_gradients()`, like
    so:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Warning
  id: totrans-248
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Don’t forget to set `training=True` when calling the model in the training loop,
    especially if your model behaves differently during training and testing (e.g.,
    if it uses `BatchNormalization` or `Dropout`). If it’s a custom model, make sure
    to propagate the `training` argument to the layers that your model calls.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, there are quite a lot of things you need to get right, and it’s
    easy to make a mistake. But on the bright side, you get full control.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you know how to customize any part of your models⁠^([15](ch12.html#idm45720191368768))
    and training algorithms, let’s see how you can use TensorFlow’s automatic graph
    generation feature: it can speed up your custom code considerably, and it will
    also make it portable to any platform supported by TensorFlow (see [Chapter 19](ch19.html#deployment_chapter)).'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Functions and Graphs
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Back in TensorFlow 1, graphs were unavoidable (as were the complexities that
    came with them) because they were a central part of TensorFlow’s API. Since TensorFlow 2
    (released in 2019), graphs are still there, but not as central, and they’re much
    (much!) simpler to use. To show just how simple, let’s start with a trivial function
    that computes the cube of its input:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We can obviously call this function with a Python value, such as an int or
    a float, or we can call it with a tensor:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Now, let’s use `tf.function()` to convert this Python function to a *TensorFlow*
    *function*:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'This TF function can then be used exactly like the original Python function,
    and it will return the same result (but always as tensors):'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Under the hood, `tf.function()` analyzed the computations performed by the
    `cube()` function and generated an equivalent computation graph! As you can see,
    it was rather painless (we will look at how this works shortly). Alternatively,
    we could have used `tf.function` as a decorator; this is actually more common:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The original Python function is still available via the TF function’s `python_function`
    attribute, in case you ever need it:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'TensorFlow optimizes the computation graph, pruning unused nodes, simplifying
    expressions (e.g., 1 + 2 would get replaced with 3), and more. Once the optimized
    graph is ready, the TF function efficiently executes the operations in the graph,
    in the appropriate order (and in parallel when it can). As a result, a TF function
    will usually run much faster than the original Python function, especially if
    it performs complex computations.⁠^([16](ch12.html#idm45720191194320)) Most of
    the time you will not really need to know more than that: when you want to boost
    a Python function, just transform it into a TF function. That’s all!'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow优化计算图，修剪未使用的节点，简化表达式（例如，1 + 2将被替换为3）等。一旦优化的图准备就绪，TF函数将有效地执行图中的操作，按适当的顺序（并在可能时并行执行）。因此，TF函数通常比原始Python函数运行得快得多，特别是如果它执行复杂计算。大多数情况下，您实际上不需要知道更多：当您想要提升Python函数时，只需将其转换为TF函数。就这样！
- en: Moreover, if you set `jit_compile=True` when calling `tf.function()`, then TensorFlow
    will use *accelerated linear algebra* (XLA) to compile dedicated kernels for your
    graph, often fusing multiple operations. For example, if your TF function calls
    `tf.reduce_sum(a * b + c)`, then without XLA the function would first need to
    compute `a * b` and store the result in a temporary variable, then add `c` to
    that variable, and lastly call `tf.reduce_sum()` on the result. With XLA, the
    whole computation gets compiled into a single kernel, which will compute `tf.reduce_sum(a
    * b + c)` in one shot, without using any large temporary variable. Not only will
    this be much faster, it will also use dramatically less RAM.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果在调用`tf.function()`时设置`jit_compile=True`，那么TensorFlow将使用*加速线性代数*（XLA）为您的图编译专用内核，通常融合多个操作。例如，如果您的TF函数调用`tf.reduce_sum(a
    * b + c)`，那么没有XLA，函数首先需要计算`a * b`并将结果存储在临时变量中，然后将`c`添加到该变量中，最后调用`tf.reduce_sum()`处理结果。使用XLA，整个计算将编译为单个内核，该内核将一次性计算`tf.reduce_sum(a
    * b + c)`，而不使用任何大型临时变量。这不仅速度更快，而且使用的RAM大大减少。
- en: When you write a custom loss function, a custom metric, a custom layer, or any
    other custom function and you use it in a Keras model (as we’ve done throughout
    this chapter), Keras automatically converts your function into a TF function—no
    need to use `tf.function()`. So most of the time, the magic is 100% transparent.
    And if you want Keras to use XLA, you just need to set `jit_compile=True` when
    calling the `compile()` method. Easy!
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 当您编写自定义损失函数、自定义指标、自定义层或任何其他自定义函数，并在Keras模型中使用它（就像我们在本章中一直做的那样），Keras会自动将您的函数转换为TF函数——无需使用`tf.function()`。因此，大多数情况下，这种魔术是100%透明的。如果您希望Keras使用XLA，只需在调用`compile()`方法时设置`jit_compile=True`。简单！
- en: Tip
  id: totrans-268
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: You can tell Keras *not* to convert your Python functions to TF functions by
    setting `dynamic=True` when creating a custom layer or a custom model. Alternatively,
    you can set `run_eagerly=True` when calling the model’s `compile()` method.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在创建自定义层或自定义模型时设置`dynamic=True`来告诉Keras*不*将您的Python函数转换为TF函数。或者，您可以在调用模型的`compile()`方法时设置`run_eagerly=True`。
- en: 'By default, a TF function generates a new graph for every unique set of input
    shapes and data types and caches it for subsequent calls. For example, if you
    call `tf_cube(tf.constant(10))`, a graph will be generated for int32 tensors of
    shape []. Then if you call `tf_cube(tf.constant(20))`, the same graph will be
    reused. But if you then call `tf_cube(tf.constant([10, 20]))`, a new graph will
    be generated for int32 tensors of shape [2]. This is how TF functions handle polymorphism
    (i.e., varying argument types and shapes). However, this is only true for tensor
    arguments: if you pass numerical Python values to a TF function, a new graph will
    be generated for every distinct value: for example, calling `tf_cube(10)` and
    `tf_cube(20)` will generate two graphs.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，TF函数为每个唯一的输入形状和数据类型生成一个新图，并将其缓存以供后续调用。例如，如果您调用`tf_cube(tf.constant(10))`，将为形状为[]的int32张量生成一个图。然后，如果您调用`tf_cube(tf.constant(20))`，将重用相同的图。但是，如果您随后调用`tf_cube(tf.constant([10,
    20]))`，将为形状为[2]的int32张量生成一个新图。这就是TF函数处理多态性（即不同的参数类型和形状）的方式。但是，这仅适用于张量参数：如果将数值Python值传递给TF函数，则将为每个不同的值生成一个新图：例如，调用`tf_cube(10)`和`tf_cube(20)`将生成两个图。
- en: Warning
  id: totrans-271
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: If you call a TF function many times with different numerical Python values,
    then many graphs will be generated, slowing down your program and using up a lot
    of RAM (you must delete the TF function to release it). Python values should be
    reserved for arguments that will have few unique values, such as hyperparameters
    like the number of neurons per layer. This allows TensorFlow to better optimize
    each variant of your model.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您多次使用不同的数值Python值调用TF函数，则将生成许多图，减慢程序速度并使用大量RAM（您必须删除TF函数才能释放它）。Python值应保留用于将具有少量唯一值的参数，例如每层神经元的数量之类的超参数。这样可以使TensorFlow更好地优化模型的每个变体。
- en: AutoGraph and Tracing
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AutoGraph和跟踪
- en: 'So how does TensorFlow generate graphs? It starts by analyzing the Python function’s
    source code to capture all the control flow statements, such as `for` loops, `while`
    loops, and `if` statements, as well as `break`, `continue`, and `return` statements.
    This first step is called *AutoGraph*. The reason TensorFlow has to analyze the
    source code is that Python does not provide any other way to capture control flow
    statements: it offers magic methods like `__add__()` and `__mul__()` to capture
    operators like `+` and `*`, but there are no `__while__()` or `__if__()` magic
    methods. After analyzing the function’s code, AutoGraph outputs an upgraded version
    of that function in which all the control flow statements are replaced by the
    appropriate TensorFlow operations, such as `tf.while_loop()` for loops and `tf.cond()`
    for `if` statements. For example, in [Figure 12-4](#autograph_tracing_diagram),
    AutoGraph analyzes the source code of the `sum_squares()` Python function, and
    it generates the `tf__sum_squares()` function. In this function, the `for` loop
    is replaced by the definition of the `loop_body()` function (containing the body
    of the original `for` loop), followed by a call to the `for_stmt()` function.
    This call will build the appropriate `tf.while_loop()` operation in the computation
    graph.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1204](assets/mls3_1204.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
- en: Figure 12-4\. How TensorFlow generates graphs using AutoGraph and tracing
  id: totrans-276
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Next, TensorFlow calls this “upgraded” function, but instead of passing the
    argument, it passes a *symbolic tensor*—a tensor without any actual value, only
    a name, a data type, and a shape. For example, if you call `sum_squares(tf.constant(10))`,
    then the `tf__sum_squares()` function will be called with a symbolic tensor of
    type int32 and shape []. The function will run in *graph mode*, meaning that each
    TensorFlow operation will add a node in the graph to represent itself and its
    output tensor(s) (as opposed to the regular mode, called *eager execution*, or
    *eager mode*). In graph mode, TF operations do not perform any computations. Graph
    mode was the default mode in TensorFlow 1\. In [Figure 12-4](#autograph_tracing_diagram),
    you can see the `tf__sum_squares()` function being called with a symbolic tensor
    as its argument (in this case, an int32 tensor of shape []) and the final graph
    being generated during tracing. The nodes represent operations, and the arrows
    represent tensors (both the generated function and the graph are simplified).
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-278
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In order to view the generated function’s source code, you can call `tf.autograph.to_code(sum_squares.python_function)`.
    The code is not meant to be pretty, but it can sometimes help for debugging.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: TF Function Rules
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Most of the time, converting a Python function that performs TensorFlow operations
    into a TF function is trivial: decorate it with `@tf.function` or let Keras take
    care of it for you. However, there are a few rules to respect:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 'If you call any external library, including NumPy or even the standard library,
    this call will run only during tracing; it will not be part of the graph. Indeed,
    a TensorFlow graph can only include TensorFlow constructs (tensors, operations,
    variables, datasets, and so on). So, make sure you use `tf.reduce_sum()` instead
    of `np.sum()`, `tf.sort()` instead of the built-in `sorted()` function, and so
    on (unless you really want the code to run only during tracing). This has a few
    additional implications:'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you define a TF function `f(*x*)` that just returns `np.random.rand()`, a
    random number will only be generated when the function is traced, so `f(tf.constant(2.))`
    and `f(tf.constant(3.))` will return the same random number, but `f(tf.constant([2.,
    3.]))` will return a different one. If you replace `np.random.rand()` with `tf.random.uniform([])`,
    then a new random number will be generated upon every call, since the operation
    will be part of the graph.
  id: totrans-283
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If your non-TensorFlow code has side effects (such as logging something or updating
    a Python counter), then you should not expect those side effects to occur every
    time you call the TF function, as they will only occur when the function is traced.
  id: totrans-284
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You can wrap arbitrary Python code in a `tf.py_function()` operation, but doing
    so will hinder performance, as TensorFlow will not be able to do any graph optimization
    on this code. It will also reduce portability, as the graph will only run on platforms
    where Python is available (and where the right libraries are installed).
  id: totrans-285
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You can call other Python functions or TF functions, but they should follow
    the same rules, as TensorFlow will capture their operations in the computation
    graph. Note that these other functions do not need to be decorated with `@tf.function`.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the function creates a TensorFlow variable (or any other stateful TensorFlow
    object, such as a dataset or a queue), it must do so upon the very first call,
    and only then, or else you will get an exception. It is usually preferable to
    create variables outside of the TF function (e.g., in the `build()` method of
    a custom layer). If you want to assign a new value to the variable, make sure
    you call its `assign()` method instead of using the `=` operator.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The source code of your Python function should be available to TensorFlow. If
    the source code is unavailable (for example, if you define your function in the
    Python shell, which does not give access to the source code, or if you deploy
    only the compiled **.pyc* Python files to production), then the graph generation
    process will fail or have limited functionality.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow will only capture `for` loops that iterate over a tensor or a `tf.data.Dataset`
    (see [Chapter 13](ch13.html#data_chapter)). Therefore, make sure you use `for
    i in tf.range(*x*)` rather than `for i in range(*x*)`, or else the loop will not
    be captured in the graph. Instead, it will run during tracing. (This may be what
    you want if the `for` loop is meant to build the graph; for example, to create
    each layer in a neural network.)
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As always, for performance reasons, you should prefer a vectorized implementation
    whenever you can, rather than using loops.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s time to sum up! In this chapter we started with a brief overview of TensorFlow,
    then we looked at TensorFlow’s low-level API, including tensors, operations, variables,
    and special data structures. We then used these tools to customize almost every
    component in the Keras API. Finally, we looked at how TF functions can boost performance,
    how graphs are generated using AutoGraph and tracing, and what rules to follow
    when you write TF functions (if you would like to open the black box a bit further
    and explore the generated graphs, you will find technical details in [Appendix D](app04.html#tffunctions_appendix)).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at how to efficiently load and preprocess
    data with TensorFlow.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How would you describe TensorFlow in a short sentence? What are its main features?
    Can you name other popular deep learning libraries?
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is TensorFlow a drop-in replacement for NumPy? What are the main differences
    between the two?
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do you get the same result with `tf.range(10)` and `tf.constant(np.​ara⁠nge(10))`?
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you name six other data structures available in TensorFlow, beyond regular
    tensors?
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can define a custom loss function by writing a function or by subclassing
    the `tf.keras.losses.Loss` class. When would you use each option?
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Similarly, you can define a custom metric in a function or as a subclass of
    `tf.keras.metrics.Metric`. When would you use each option?
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When should you create a custom layer versus a custom model?
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are some use cases that require writing your own custom training loop?
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can custom Keras components contain arbitrary Python code, or must they be convertible
    to TF functions?
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the main rules to respect if you want a function to be convertible
    to a TF function?
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When would you need to create a dynamic Keras model? How do you do that? Why
    not make all your models dynamic?
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Implement a custom layer that performs *layer normalization* (we will use this
    type of layer in [Chapter 15](ch15.html#rnn_chapter)):'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `build()` method should define two trainable weights **α** and **β**, both
    of shape `input_shape[-1:]` and data type `tf.float32`. **α** should be initialized
    with 1s, and **β** with 0s.
  id: totrans-306
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The `call()` method should compute the mean *μ* and standard deviation *σ* of
    each instance’s features. For this, you can use `tf.nn.moments(inputs, axes=-1,
    keepdims=True)`, which returns the mean *μ* and the variance *σ*² of all instances
    (compute the square root of the variance to get the standard deviation). Then
    the function should compute and return **α** ⊗ (**X** – *μ*)/(*σ* + *ε*) + **β**,
    where ⊗ represents itemwise multiplication (`*`) and *ε* is a smoothing term (a
    small constant to avoid division by zero, e.g., 0.001).
  id: totrans-307
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Ensure that your custom layer produces the same (or very nearly the same) output
    as the `tf.keras.layers.LayerNormalization` layer.
  id: totrans-308
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Train a model using a custom training loop to tackle the Fashion MNIST dataset
    (see [Chapter 10](ch10.html#ann_chapter)):'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Display the epoch, iteration, mean training loss, and mean accuracy over each
    epoch (updated at each iteration), as well as the validation loss and accuracy
    at the end of each epoch.
  id: totrans-310
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Try using a different optimizer with a different learning rate for the upper
    layers and the lower layers.
  id: totrans-311
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab3*](https://homl.info/colab3).
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch12.html#idm45720196277360-marker)) However, Facebook’s PyTorch library
    is currently more popular in academia: more papers cite PyTorch than TensorFlow
    or Keras. Moreover, Google’s JAX library is gaining momentum, especially in academia.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch12.html#idm45720196265168-marker)) TensorFlow includes another deep
    learning API called the *estimators API*, but it is now deprecated.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch12.html#idm45720196260752-marker)) If you ever need to (but you probably
    won’t), you can write your own operations using the C++ API.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch12.html#idm45720196255744-marker)) To learn more about TPUs and how
    they work, check out [*https://homl.info/tpus*](https://homl.info/tpus).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch12.html#idm45720196020064-marker)) A notable exception is `tf.math.log()`,
    which is commonly used but doesn’t have a `tf.log()` alias, as it might be confused
    with logging.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '^([6](ch12.html#idm45720194957792-marker)) It would not be a good idea to use
    a weighted mean: if you did, then two instances with the same weight but in different
    batches would have a different impact on training, depending on the total weight
    of each batch.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch12.html#idm45720194952928-marker)) The `{**x, [...]}` syntax was added
    in Python 3.5, to merge all the key/value pairs from dictionary `x` into another
    dictionary. Since Python 3.9, you can use the nicer `x | y` syntax instead (where
    `x` and `y` are two dictionaries).
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch12.html#idm45720194513680-marker)) However, the Huber loss is seldom
    used as a metric—the MAE or MSE is generally preferred.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch12.html#idm45720194258944-marker)) This class is for illustration purposes
    only. A simpler and better implementation would just subclass the `tf.keras.metrics.Mean`
    class; see the “Streaming Metrics” section of this chapter’s notebook for an example.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch12.html#idm45720193832656-marker)) The Keras API calls this argument
    `input_shape`, but since it also includes the batch dimension, I prefer to call
    it `batch_input_shape`.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: ^([11](ch12.html#idm45720193550128-marker)) The name “subclassing API” in Keras
    usually refers only to the creation of custom models by subclassing, although
    many other things can be created by subclassing, as you’ve seen in this chapter.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '^([12](ch12.html#idm45720192957584-marker)) Due to TensorFlow issue #46858,
    the call to `super().build()` may fail in this case, unless the issue was fixed
    by the time you read this. If not, you need to replace this line with `self.built
    = True`.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: ^([13](ch12.html#idm45720192953232-marker)) You can also call `add_loss()` on
    any layer inside the model, as the model recursively gathers losses from all of
    its layers.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: ^([14](ch12.html#idm45720192614784-marker)) If the tape goes out of scope, for
    example when the function that used it returns, Python’s garbage collector will
    delete it for you.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: ^([15](ch12.html#idm45720191368768-marker)) With the exception of optimizers,
    as very few people ever customize these; see the “Custom Optimizers” section in
    the notebook for an example.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: ^([16](ch12.html#idm45720191194320-marker)) However, in this trivial example,
    the computation graph is so small that there is nothing at all to optimize, so
    `tf_cube()` actually runs much slower than `cube()`.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
