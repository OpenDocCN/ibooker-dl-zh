<html><head></head><body>
  <div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">3</span> </span><span class="chapter-title-text"><span class="CharOverride-2">Selecting and evaluating AI tools</span></span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">Distinctions among different types of AI, or ways of using AI, and how to select the most appropriate one</li> 
    <li class="readable-text" id="p3">How to assess AI’s performance and select models</li> 
    <li class="readable-text" id="p4">Common ways to measure AI’s performance at a task</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p5"> 
   <p>This chapter provides guidance on selecting an AI model or tool and assessing its performance at a given task. We kick off by discussing three common distinctions between different types of AI: proprietary versus open source AI, off-the-shelf versus fine-tuned AI, and AI apps versus foundation models. We explain what these mean and how to pick the most suitable type. Afterward, we discuss a common process to assess AI’s performance, which uses different datasets for validation and testing. We also discuss some common performance measures such as accuracy. The appendix includes a catalog of popular generative AI tools.</p> 
  </div> 
  <div class="readable-text" id="p6"> 
   <h2 class=" readable-text-h2">Proprietary vs. open source </h2> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>In proprietary AI, the user isn’t allowed to modify or even see the code that powers the underlying ML models. The inner workings of the technology are kept secret to prevent others from copying it. One common way of using proprietary AI is through customer-facing apps such as ChatGPT. These tend to charge users a monthly subscription to access the service, although some provide a free tier that grants access to a reduced number of features. </p> 
  </div> 
  <div class="readable-text intended-text" id="p8"> 
   <p>Another common way of using proprietary AI is via APIs. These let users interact with AI programmatically to build apps that utilize it. The AI software runs on a remote server behind closed doors, so the user can’t see the code. APIs are typically billed based on usage (e.g., the number of input and output tokens).</p> 
  </div> 
  <div class="readable-text intended-text" id="p9"> 
   <p>In contrast, in open source AI, the provider publicly discloses the internal details of the ML model, including the code to use it and the values of all the model’s parameters. The user is often authorized to modify or customize the model if needed. In addition, users can self-host these models using their own infrastructure; for example, you can download a copy of the model to your local computer or your own cloud computing instance and run the code yourself. This doesn’t mean you <em>must</em> self-host the model as it may also be available through APIs, but you have the option to self-host it. An example of open source AI is the family of Llama models produced by Meta, which are openly available for download on multiple websites.</p> 
  </div> 
  <div class="readable-text intended-text" id="p10"> 
   <p>Open source AI is sometimes not quite as open as it may sound. For starters, their manufacturers don’t disclose the data used to train these models. So, while you can see the parameters of the final model, you’d be unable to train that exact model yourself as you wouldn’t know which data to use. Mistral AI, a company that provides open source AI, explains <span class="CharOverride-3">(</span><a href="https://mng.bz/rKQy"><span class="Hyperlink CharOverride-4">https:</span><span class="Hyperlink CharOverride-4">/</span><span class="Hyperlink CharOverride-4">/mng.bz/rKQy</span></a><span class="CharOverride-3">)</span>:</p> 
  </div> 
  <div class="readable-text" id="p11"> 
   <blockquote>
    <div>
     We do not communicate on our training datasets. We keep proprietary some intermediary assets (code and resources) required to produce both the Open-Source models and the Optimized models. Among others, this involves the training logic for models, and the datasets used in training.
    </div>
   </blockquote> 
  </div> 
  <div class="readable-text" id="p12"> 
   <p>Note that, just like proprietary models, open source models are improved (or aligned) by using reinforcement learning with human feedback (see chapter 1). This is performed using data created manually by human labelers, which remains undisclosed in most cases.</p> 
  </div> 
  <div class="readable-text intended-text" id="p13"> 
   <p>The licenses to use open source AI often come with restrictions. For example, you are not allowed to use a Llama model—even your own copy—for an app with more than 700 million monthly users (see <a href="https://mng.bz/VVoG"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/VVoG</span></a>). In that case, you would have to discuss licensing options with Meta, and you may be asked to pay. Moreover, you’re not allowed to use a Llama model or its outputs to improve other LLMs; in other words, you can’t use Llama to build products that compete with it.</p> 
  </div> 
  <div class="readable-text intended-text" id="p14"> 
   <p>Building large ML models is expensive, so the most powerful open source AI is built by for-profit companies that charge or intend to charge for services. These services often include consulting or access to premium, proprietary models.</p> 
  </div> 
  <div class="readable-text" id="p15"> 
   <h3 class=" readable-text-h3"> How to decide</h3> 
  </div> 
  <div class="readable-text" id="p16"> 
   <p>Proprietary AI is most suitable when you need a done-for-you solution. Using proprietary AI doesn’t usually require specialized knowledge, such as machine learning, coding, and DevOps. </p> 
  </div> 
  <div class="readable-text intended-text" id="p17"> 
   <p>One of the main reasons to use open source AI is to be able to self-host it (run it on your own servers), which can provide better transparency and governance, as you have full visibility over the code and full control over which data exits the organization. Your company may not want to send any sensitive data to a third party, such as OpenAI, or it may want to audit the code to ensure it doesn’t do anything it’s not supposed to.</p> 
  </div> 
  <div class="readable-text intended-text" id="p18"> 
   <p>The cost of self-hosting AI, however, tends to be higher than paying for APIs, as you need to maintain the required infrastructure, so it is usually not cost-effective unless done at a very large scale. You also need to be very careful—malicious open source models have been published in the past that executed unintended code in the user’s machine (see <a href="https://mng.bz/xKeX"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/xKeX</span></a>).</p> 
  </div> 
  <div class="readable-text intended-text" id="p19"> 
   <p>Another reason to use open source AI is customization. If you want to modify a model (e.g., by fine-tuning it, which is covered in the next section), then open source AI lets you do so most freely. Table 3.1 summarizes the best uses of proprietary and open source AI.</p> 
  </div> 
  <div class="browsable-container browsable-table-container" id="p20"> 
   <h5 class=" browsable-container-h5">Table 3.1<span class="CharOverride-5"> </span>Proprietary vs. open source AI</h5> 
   <table> 
    <colgroup> 
     <col class="_idGenTableRowColumn-1"/> 
     <col class="_idGenTableRowColumn-2"/> 
    </colgroup> 
    <thead> 
     <tr class="No-Table-Style _idGenTableRowColumn-3"> 
      <th class="No-Table-Style TableHead CellOverride-1" scope="col"> <p class="TableHead"><span class="CharOverride-6">Proprietary AIBest for . . .</span></p> </th> 
      <th class="No-Table-Style TableHead CellOverride-2" scope="col"> <p class="TableHead"><span class="CharOverride-6">Open source AIBest for . . .</span></p> </th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr class="No-Table-Style _idGenTableRowColumn-4"> 
      <td class="No-Table-Style CellOverride-3"> <p class="TableBody ParaOverride-5">• Done-for-you solution</p> <p class="TableBody ParaOverride-5">• Easy start</p> <p class="TableBody ParaOverride-5">• No specialized knowledge required</p> <p class="TableBody ParaOverride-5">• Small-scale use, in which pay-as-you-go AI is cheaper than maintaining your own infrastructure</p> </td> 
      <td class="No-Table-Style CellOverride-4 _idGenCellOverride-1"> <p class="TableBody ParaOverride-5">• Self-hosting so that you enjoy better governance and transparency</p> <p class="TableBody ParaOverride-5">• Large-scale use, in which maintaining your own infrastructure is cheaper than pay-as-you-go AI</p> <p class="TableBody ParaOverride-5">• Model customization (e.g., fine-tuning)</p> </td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p21"> 
   <p>In terms of the quality of outputs, proprietary AI used to hold an edge over open source AI. However, the gap has been narrowing, and many people claim that open source AI is already or will soon be as capable as its proprietary counterparts.</p> 
  </div> 
  <div class="readable-text" id="p22"> 
   <h2 class=" readable-text-h2">Off-the-shelf vs. fine-tuning</h2> 
  </div> 
  <div class="readable-text" id="p23"> 
   <p><span class="CharOverride-7">When it comes to improving the performance of generative AI at a certain task, there are two main schools of thought. One of them is using off-the-shelf models—without any alterations—and it relies on prompt engineering techniques to make them more performant and customized to your intended task. For example, it has become popular to include a few demonstrations of how to perform a task inside the prompt, which is known as </span><em>few-shot prompting</em><span class="CharOverride-7"> (as opposed to </span><em>zero-shot prompting</em><span class="CharOverride-7"> in which you don’t provide any examples). This helps disambiguate the request. Researchers from OpenAI argued </span><span class="CharOverride-4">(</span><a href="https://arxiv.org/pdf/2005.14165"><span class="Hyperlink CharOverride-4">https:</span><span class="Hyperlink CharOverride-4">/</span><span class="Hyperlink CharOverride-4">/arxiv.org/pdf/2005.14165</span></a><span class="CharOverride-4">)</span><span class="CharOverride-7">:</span><span class="CharOverride-7"/></p> 
  </div> 
  <div class="readable-text" id="p24"> 
   <blockquote>
    <div>
     If someone is asked to “make a table of world records for the 200m dash”, this request can be ambiguous, as it may not be clear exactly what format the table should have or what should be included (and even with careful clarification, understanding precisely what is desired can be difficult).
    </div>
   </blockquote> 
  </div> 
  <div class="readable-text" id="p25"> 
   <p>The researchers went on to show that including a few examples of how to perform the task within the prompt steered the LLM in the right direction.</p> 
  </div> 
  <div class="readable-text intended-text" id="p26"> 
   <p>In addition, the RAG approach (see chapter 1) has become a popular way of providing the LLM with a large amount of contextual information to help it perform a task. The increasingly large context window of state-of-the-art LLMs has made RAG particularly effective.</p> 
  </div> 
  <div class="readable-text intended-text" id="p27"> 
   <p>Improved prompts can help customize image generation. For example, the image generator Midjourney lets users upload images as part of their prompts to indicate the desired style of the generated images.</p> 
  </div> 
  <div class="readable-text intended-text" id="p28"> 
   <p>The other school of thought suggests altering the model to make it more suitable for the intended task, which is known as <em>fine-tuning.</em> The model’s internal parameters are adjusted, so you utilize an altered copy of the original model to generate your outputs.</p> 
  </div> 
  <div class="readable-text intended-text" id="p29"> 
   <p>Fine-tuning requires training data, which is used to continue the training of the original model for a little longer. For example, to fine-tune an LLM, you must create a sample of text in your intended style. This data is fed to the training algorithm to refine the LLM. The amount of data used for fine-tuning is usually much smaller compared to the data used to train the original LLM—you may need just a handful of documents to do so. Open source models are ideal for fine-tuning as you have access to the entire model with its parameters, and you can then alter the parameters to better suit your needs. </p> 
  </div> 
  <div class="readable-text intended-text" id="p30"> 
   <p>Perhaps the biggest challenge of fine-tuning is overdoing it—if you specialize your model too much on your fine-tuning training data, it might end up memorizing specific examples present in the data and not perform well with other instances. This is known as <em>overfitting</em>.</p> 
  </div> 
  <div class="readable-text intended-text" id="p31"> 
   <p>There are a handful of techniques to prevent overfitting (see the sidebar). You need to be mindful of these techniques and configure the fine-tuning algorithm appropriately to prevent overfitting. We’ll discuss later in this chapter how you can use validation and test sets to evaluate and compare different AI models, which can help select the best strategy to fine-tune a model and ensure the final model hasn’t overfitted the data. </p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p32"> 
    <h5 class=" callout-container-h5 readable-text-h5">Techniques to control overfitting</h5> 
   </div> 
   <div class="readable-text" id="p33"> 
    <p><em class="CharOverride-8">Early stopping</em><em>—</em>You train the model on your fine-tuning data only for a few iterations. You stop once performance stops improving, as measured on a separate piece of data (called the <em class="CharOverride-8">validation set</em>).</p> 
   </div> 
   <div class="readable-text" id="p34"> 
    <p><em class="CharOverride-8">Limited scope of updates</em><em>—</em>You only allow some parts of the model to be updated. For example, one popular method called LoRA inserts small layers with new learnable parameters into the model, while keeping its original parameters intact.</p> 
   </div> 
   <div class="readable-text" id="p35"> 
    <p><em class="CharOverride-8">Regularization</em><em>—</em>You add a term to the loss function that penalizes too high or too low parameter values. This reduces the risk of overfitting by preventing parameters from being overly specialized to specific training examples.</p> 
   </div> 
   <div class="readable-text" id="p36"> 
    <p><em class="CharOverride-8">Dropout</em><em>—</em>Pieces of the model are randomly removed on each iteration of the training process, which prevents internal units of the model from overly specializing to the training examples.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p37"> 
   <p>A method known as LoRA has become popular for fine-tuning (see <a href="https://arxiv.org/abs/2106.09685"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/arxiv.org/abs/2106.09685</span></a>). LoRA inserts small layers with new learnable parameters to adjust the existing model, instead of modifying its original parameters. This makes fine-tuning faster as few parameter updates must be calculated on each iteration. It also helps control overfitting as you only modify a limited number of parameters (see “Limited scope of updates” in the sidebar). </p> 
  </div> 
  <div class="readable-text intended-text" id="p38"> 
   <p>The libraries developed by Hugging Face are very popular for fine-tuning existing models (<a href="https://huggingface.co/docs/trl/main/en/index"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/huggingface.co/docs/trl/main/en/index</span></a>). Hugging Face also contains a large inventory of open source models you can fine-tune. Many users run their fine-tuning using Jupyter notebooks connected to cloud-computing instances. Google Collab is particularly commonly used for this, as it provides easy-to-access notebooks and lets you use some of its computing power for free, which might be enough to fine-tune some models.</p> 
  </div> 
  <div class="readable-text intended-text" id="p39"> 
   <p>Fine-tuning requires some specialized machine learning knowledge, so I recommend you learn the basics of ML to get it right. You might also require infrastructure to run the fine-tuning process, and you’ll then have to use your own customized copy of the model.</p> 
  </div> 
  <div class="readable-text intended-text" id="p40"> 
   <p>In some cases, it is also possible to fine-tune proprietary AI. For example, OpenAI lets you upload your own fine-tuning dataset and create a fine-tuned version of its models, which you can access through the API. The company charges a premium for using fine-tuned models compared to using OpenAI’s original models. The process is friendly, although not as customizable as fine-tuning open source models.</p> 
  </div> 
  <div class="readable-text" id="p41"> 
   <h3 class=" readable-text-h3"> How to decide</h3> 
  </div> 
  <div class="readable-text" id="p42"> 
   <p><span class="CharOverride-7">Prompt engineering is the most straightforward way of improving a model’s performance. Common advice is that it’s the first thing you should try (check out </span><a href="https://mng.bz/AQZx"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/AQZx</span></a><span class="CharOverride-7"> and </span><a href="https://mng.bz/ZlQA"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/ZlQA</span></a><span class="CharOverride-7"> for more info). As context windows have become large, prompts can be quite rich. So, it is often advisable to use fine-tuning as a last resort when the output still isn’t quite what you expect, even after trying multiple ways of improving the prompts. Note, however, that prompt engineering works best with the most advanced and costly models, as they can adapt better to a wider range of tasks and fit longer prompts within their context windows. Table 3.2 compares off-the-shelf with fine-tuned AI.</span><span class="CharOverride-7"/></p> 
  </div> 
  <div class="browsable-container browsable-table-container" id="p43"> 
   <h5 class=" browsable-container-h5">Table 3.2<span class="CharOverride-5"> </span>Off-the-shelf vs. fine-tuned AI</h5> 
   <table> 
    <colgroup> 
     <col class="_idGenTableRowColumn-5"/> 
     <col class="_idGenTableRowColumn-6"/> 
    </colgroup> 
    <thead> 
     <tr class="No-Table-Style _idGenTableRowColumn-3"> 
      <th class="No-Table-Style TableHead CellOverride-1" scope="col"> <p class="TableHead"><span class="CharOverride-6">Off-the-shelf AIBest when . . .</span> </p> </th> 
      <th class="No-Table-Style TableHead CellOverride-2" scope="col"> <p class="TableHead"><span class="CharOverride-6">Fine-tuned AIBest when . . .</span></p> </th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr class="No-Table-Style _idGenTableRowColumn-7"> 
      <td class="No-Table-Style CellOverride-3"> <p class="TableBody ParaOverride-5">• Prompt engineering techniques work well.</p> <p class="TableBody ParaOverride-5">• It is okay to use proprietary AI.</p> <p class="TableBody ParaOverride-5">• You can afford large models.</p> <p class="TableBody ParaOverride-5">• You prioritize ease of use.</p> </td> 
      <td class="No-Table-Style CellOverride-4 _idGenCellOverride-1"> <p class="TableBody ParaOverride-5">• You want highly customized outputs, and you’ve exhausted other options.</p> <p class="TableBody ParaOverride-5">• You need to use smaller models (for example, for self-hosting them).</p> <p class="TableBody ParaOverride-5">• You have ML expertise and access to computing resources.</p> </td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p44"> 
   <p>Fine-tuning can be a good choice for smaller models, for example, because you want to reduce your costs. This is particularly relevant when you must self-host your own models. In this case, using a small, fine-tuned model might be more effective than using prompt engineering with a larger model.</p> 
  </div> 
  <div class="readable-text" id="p45"> 
   <h2 class=" readable-text-h2">Customer-facing AI apps vs. foundation models</h2> 
  </div> 
  <div class="readable-text" id="p46"> 
   <p>Customer-facing AI apps help final customers perform tasks. These include general-purpose commercial chatbots such as ChatGPT and special-purpose apps such as GitHub Copilot and Cursor, which help software engineers write code. </p> 
  </div> 
  <div class="readable-text intended-text" id="p47"> 
   <p>In contrast, foundation models are large, multipurpose AI models. These models are used behind the scenes to power customer-facing apps. For example, foundation models such as GPT-4o are used to power customer-facing ChatGPT. </p> 
  </div> 
  <div class="readable-text intended-text" id="p48"> 
   <p>Some companies build both customer-facing apps and provide access to their underlying foundation models through APIs so that software developers can build their own apps on top. </p> 
  </div> 
  <div class="readable-text" id="p49"> 
   <h3 class=" readable-text-h3"> How to decide</h3> 
  </div> 
  <div class="readable-text" id="p50"> 
   <p>Customer-facing apps are the most suitable choice when you want AI to assist you in performing a specific task, as they’re friendly to use and particularly tailored to the task. Foundation models are best used as a building block when you want to create your own app based on powerful AI. Table 3.3 compares customer-facing AI apps with foundation models.</p> 
  </div> 
  <div class="browsable-container browsable-table-container" id="p51"> 
   <h5 class=" browsable-container-h5">Table 3.3<span class="CharOverride-5"> </span>Customer-facing AI apps vs. foundation models</h5> 
   <table> 
    <colgroup> 
     <col class="_idGenTableRowColumn-1"/> 
     <col class="_idGenTableRowColumn-2"/> 
    </colgroup> 
    <thead> 
     <tr class="No-Table-Style _idGenTableRowColumn-3"> 
      <th class="No-Table-Style TableHead CellOverride-1" scope="col"> <p class="TableHead"><span class="CharOverride-6">Customer-facing AI appsSuitable for . . .</span></p> </th> 
      <th class="No-Table-Style TableHead CellOverride-2" scope="col"> <p class="TableHead"><span class="CharOverride-6">Foundation modelsSuitable for . . .</span></p> </th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr class="No-Table-Style _idGenTableRowColumn-8"> 
      <td class="No-Table-Style CellOverride-3"> <p class="TableBody ParaOverride-5">• Assistance with a specific task</p> <p class="TableBody ParaOverride-5">• End users</p> </td> 
      <td class="No-Table-Style CellOverride-4 _idGenCellOverride-1"> <p class="TableBody ParaOverride-5">• Powering AI-based apps</p> <p class="TableBody ParaOverride-5">• Engineers</p> </td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p52"> 
   <h2 class=" readable-text-h2">Model validation, selection, and testing</h2> 
  </div> 
  <div class="readable-text" id="p53"> 
   <p>If you want to accurately compare and select AI models, it’s a good idea to build a benchmark to assess their respective performances. Also, for reasons that will become apparent soon, we often overestimate machine learning’s performance, so it’s good to follow a well-designed assessment process to prevent bad surprises.</p> 
  </div> 
  <div class="readable-text intended-text" id="p54"> 
   <p>This section describes the ideal protocol to evaluate AI’s performance at a task. In this protocol, AI models are built and evaluated using three different collections of data, known as <em>datasets.</em> In the following, we describe the role of each type of dataset and how it should be used.</p> 
  </div> 
  <div class="readable-text" id="p55"> 
   <h3 class=" readable-text-h3"> Training set</h3> 
  </div> 
  <div class="readable-text" id="p56"> 
   <p>The training set is the dataset used to build the model. It contains a large collection of examples of how to perform the task. For example, for image generation, it comprises numerous images paired with captions that describe their content. For text generation, it comprises a large amount of text. A much smaller training set is also used to fine-tune a model.</p> 
  </div> 
  <div class="readable-text intended-text" id="p57"> 
   <p>During training or fine-tuning, the training algorithm tries to find model parameters that minimize the loss on the training set (see chapter 1). The loss is a mathematical function that quantifies how far off the model is from performing the required task well, such as predicting the next token in the case of LLMs. </p> 
  </div> 
  <div class="readable-text intended-text" id="p58"> 
   <p>The loss is usually designed to have nice mathematical properties, such as differentiability, so it’s not always the most intuitive way of understanding a model’s performance. In addition, the loss does not always quantify how good the model is at your intended task. For example, if you want to use AI to solve coding problems, the training loss does not explicitly quantify its coding abilities; instead, it quantifies how well it autocompletes text, which is only indirectly related to coding abilities.</p> 
  </div> 
  <div class="readable-text intended-text" id="p59"> 
   <p>You don’t have to worry much about creating a training set unless you’re fine-tuning a model or training one from scratch. However, you might need to be mindful of what data was used for training when creating the validation and test sets (more on this in a minute).</p> 
  </div> 
  <div class="readable-text" id="p60"> 
   <h3 class=" readable-text-h3"> Validation set</h3> 
  </div> 
  <div class="readable-text" id="p61"> 
   <p>The validation set is used to compare the performance of different models. For example, you could use a validation set to compare the performance of GPT-4o and Llama 3 at performing a task. This helps you pick the best model, which is known as <em>model selection.</em> </p> 
  </div> 
  <div class="readable-text intended-text" id="p62"> 
   <p>The performance on the validation set is usually calculated using a measure close to your actual business objective. For example, you could calculate how often the model solves coding problems correctly. Note this is often different from the loss function used for training or fine-tuning the model. There’s a list of common performance measures later in this chapter.</p> 
  </div> 
  <div class="readable-text intended-text" id="p63"> 
   <p>It’s important that data in the validation set is not present inside the training set. Otherwise, you might overestimate the model’s performance. This is because a poor model that overfits the training data (it memorizes specific instances) may go undetected, as some of the memorized data will also appear in the validation set it’s evaluated on. If the validation set is included in the training set, it’s a bit like an exam that contains questions present verbatim in the textbook—students could memorize answers without genuinely learning and pass the exam.</p> 
  </div> 
  <div class="readable-text intended-text" id="p64"> 
   <p>You need to be particularly careful about this when using LLMs as they’re trained on a huge amount of publicly available data that includes solutions to many problems. Suppose you want to use an LLM to help you solve crossword puzzles. You create a validation set by gathering clues from real <em>New York Times</em> crosswords published in the past. You then count how often the LLM identifies the right word based on the clues. The problem is that there are numerous websites that explicitly provide the solutions to all past <em>New York Times</em> crosswords, clue by clue. So, at least in theory, an LLM could memorize the exact solution to each past clue. Your validation data would thus assess the LLM’s performance at solving problems whose solution it had the answer to. A better way of doing this would be to create a validation set containing new clues that haven’t appeared in past puzzles. This way, the LLM wouldn’t be able to “cheat.” Alternatively, you could make sure that the puzzles in the validation set were published after the LLM’s training data cut-off date.</p> 
  </div> 
  <div class="readable-text intended-text" id="p65"> 
   <p>The validation set can also be used to help you make high-level decisions when you’re training or fine-tuning your own model. For example, you can train two models with different numbers of layers or different learning rates (how much the model’s parameters are updated on every training iteration), and then pick the model with highest performance on the validation set. You could also use a validation set to compare different prompt engineering approaches.</p> 
  </div> 
  <div class="readable-text" id="p66"> 
   <h3 class=" readable-text-h3"> Test set</h3> 
  </div> 
  <div class="readable-text" id="p67"> 
   <p>Using a validation set is not enough to properly assess a model’s performance. Because you’re specifically selecting the model that works best on the validation set alone, you might get an overly optimistic idea of its performance. After all, you discarded the models that weren’t as good on that specific piece of data. What if the selected model only works well on the validation data by chance and is not a better model in general?</p> 
  </div> 
  <div class="readable-text intended-text" id="p68"> 
   <p>So, after you’re done picking the best model using the validation set, you must perform a final check using <em>another</em> dataset, called the <em>test set.</em> The test set gives you an idea of the model’s performance on data that has genuinely never been used to make modeling decisions. This final assessment is a sanity check. </p> 
  </div> 
  <div class="readable-text intended-text" id="p69"> 
   <p>The test set can only be used once. If after the test you find performance disappointing and want to update the model or consider alternatives, you must collect a new test set to perform a new assessment. Otherwise, you end up using the test set repeatedly for model selection, so it turns into a validation set.</p> 
  </div> 
  <div class="readable-text intended-text" id="p70"> 
   <p>It is up to you to choose how thorough you want to be when following this process. I know of hedge funds that are very stringent about following it, as a lot of money is at stake. For example, they try not to even look at the data inside the test set to, say, plot a graph. This way, they prevent knowledge about the test data from creeping into modeling decisions, so the test data is as independent as possible.</p> 
  </div> 
  <div class="readable-text" id="p71"> 
   <h2 class=" readable-text-h2">Performance measures</h2> 
  </div> 
  <div class="readable-text" id="p72"> 
   <p>This section describes some common performance measures that can be used to evaluate AI’s performance at an intended task. </p> 
  </div> 
  <div class="readable-text" id="p73"> 
   <h3 class=" readable-text-h3"> Accuracy</h3> 
  </div> 
  <div class="readable-text" id="p74"> 
   <p><em>Accuracy</em> is the percentage of tasks performed correctly. For example, 90% accuracy means that 9 out of 10 solutions are correct, as measured on the validation or test sets. </p> 
  </div> 
  <div class="readable-text intended-text" id="p75"> 
   <p>Accuracy is commonly used for classification tasks. For example, it is often used to assess how good AI is at categorizing an image or detecting a tweet’s sentiment. You can also use it for other problem-solving tasks. For instance, you could use accuracy to measure an LLM’s ability to solve coding problems—you’d need to count the number of correctly solved problems and divide it by the total number of problems in your validation or test set. </p> 
  </div> 
  <div class="readable-text" id="p76"> 
   <h3 class=" readable-text-h3"> Precision and recall</h3> 
  </div> 
  <div class="readable-text" id="p77"> 
   <p>In information retrieval, we are interested in identifying relevant instances out of a much larger pool. For example, a law firm may use a RAG approach to retrieve relevant legal cases, according to a query, from a large database of past cases. As another example, a bank may want to identify fraudulent transactions out of a (hopefully) much larger pool of transactions.</p> 
  </div> 
  <div class="readable-text intended-text" id="p78"> 
   <p>Two common performance measures are recall and precision. However, as we’ll discuss in a minute, neither can be used by itself.</p> 
  </div> 
  <div class="readable-text intended-text" id="p79"> 
   <p><em>Recall</em> measures how many relevant instances are identified. For example, 90% recall means that 9 out of 10 relevant instances are retrieved, the remaining being missed. </p> 
  </div> 
  <div class="readable-text intended-text" id="p80"> 
   <p><em>Precision</em> measures how relevant the retrieved instances are. For example, 90% precision means that 9 out of 10 retrieved instances are truly relevant, the remaining being irrelevant or a false positive.</p> 
  </div> 
  <div class="readable-text intended-text" id="p81"> 
   <p>The challenge is that there is a tradeoff between precision and recall. Consider a system that retrieves too much stuff. For example, it could determine that almost every past legal case is relevant to every query. This system would achieve very high recall, perhaps close to 100%. However, it would be plagued with false positives, so its precision would be very low.</p> 
  </div> 
  <div class="readable-text intended-text" id="p82"> 
   <p>In contrast, consider a system that doesn’t retrieve much stuff at all. For example, it may consider almost every past legal case irrelevant regardless of the query. This system would achieve close to 100% precision, but its recall would be very low.</p> 
  </div> 
  <div class="readable-text intended-text" id="p83"> 
   <p>So, to properly quantify AI’s performance at information retrieval, you must somehow combine recall and precision into a single measure. A popular way to do this is to calculate the <em>F-measure</em>, which is the harmonic mean (a sort of average) between precision <em>P</em> and recall <em>R</em>:</p> 
  </div> 
  <div class="browsable-container equation-container" id="p84"> 
   <p style="display: flex; align-items: center; justify-content: center; text-align: center;">F = 2(P R)/(P + R) </p> 
  </div> 
  <div class="readable-text" id="p85"> 
   <p>The higher the F-measure becomes, the higher the recall and precision. It takes its maximum value when recall and precision are both 100%.</p> 
  </div> 
  <div class="readable-text intended-text" id="p86"> 
   <p>I’m not a big fan of the F-measure for two reasons. First, it gives equal importance to recall and precision. This is arbitrary. In reality, a business may not care equally about them. I advise you to be wary of any promises of a measure that is universally good for information retrieval, be it the F-measure or something else, as the relative appetite for precision and recall is business specific.</p> 
  </div> 
  <div class="readable-text intended-text" id="p87"> 
   <p>Second, the F-measure is difficult to interpret, as the harmonic mean is not very intuitive. Technically, the F-measure is the reciprocal of the average of the reciprocals, which leads to the above formula after some algebraic manipulation. Good luck at communicating that to the business!</p> 
  </div> 
  <div class="readable-text intended-text" id="p88"> 
   <p>In my opinion, your best bet is to try to understand the business’s preferences with respect to precision and recall and come up with a custom measure that considers that. In the following paragraphs, I explain one of my preferred ways of doing this.</p> 
  </div> 
  <div class="readable-text intended-text" id="p89"> 
   <p>The first step is to understand the business’s minimum desirable level of recall (it can also be done with precision, but we’ll use recall here). For example, the business may want to make sure to always recall at least 95% of relevant legal cases or fraudulent transactions. </p> 
  </div> 
  <div class="readable-text intended-text" id="p90"> 
   <p>Afterward, you tune the system so that it attains the desired level of recall. One way to do this is to have AI output relevance as a numerical score, with values ranging from 0 (totally irrelevant) to 1 (totally relevant). Instances above a certain relevance threshold are considered relevant. You pick the threshold that helps you attain the desired level of recall. For example, it could be that setting a threshold of, say, 0.7, above which an instance is considered relevant, helps you attain the required 95% recall (you can use the validation set to calculate the threshold).</p> 
  </div> 
  <div class="readable-text intended-text" id="p91"> 
   <p>Finally, you use the other measure—precision in this case—to report performance. You can thus compare different models (all attaining the desired recall) by how precise they are.</p> 
  </div> 
  <div class="readable-text" id="p92"> 
   <h3 class=" readable-text-h3"> Mean absolute error and root mean squared error</h3> 
  </div> 
  <div class="readable-text" id="p93"> 
   <p>If you use AI to make a numerical prediction, such as the amount of rainfall, you must calculate how far off predictions are from actual values. One straightforward way of doing this is to calculate the absolute difference between predicted and known values in the training or test sets and average the results. This is known as the <em>mean absolute error</em>, or MAE. </p> 
  </div> 
  <div class="readable-text intended-text" id="p94"> 
   <p>An alternative is to square the differences, which makes them all positive, average the results, and then take the square root of this number to (sort of) undo the effect of squaring. This is known as the <em class="IndexSee">root mean squared error</em>, or RMSE. This measure is quite popular owing to its nice mathematical properties (in particular, its differentiability) and because it penalizes larger deviations more due to the squaring of the difference. However, it’s not as easy to interpret as MAE. </p> 
  </div> 
  <div class="readable-text" id="p95"> 
   <h2 class=" readable-text-h2">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p96">Proprietary AI is a good choice when you need an easy-to-use, done-for-you solution.</li> 
   <li class="readable-text" id="p97">Open source AI is a good choice when you need to self-host or customize models.</li> 
   <li class="readable-text" id="p98">If AI isn’t working quite the way you expect, or you need to customize it, it’s usually recommended to still use off-the-shelf models and enhance your prompts. If that doesn’t work, you may want to fine-tune a model to your own data. Fine-tuning is also a good option when you prefer to use a smaller model.</li> 
   <li class="readable-text" id="p99">Customer-facing AI apps are designed to be friendly and useful to end users. They’re powered by foundation models behind the scenes, which are large, general-purpose AI models you can use to build your own AI-based apps.</li> 
   <li class="readable-text" id="p100">Make sure to use a validation set (with data not present in the training set) to compare and select models. You should also perform a sanity check afterward using a separate test set, once you’ve selected your favorite model. Do not use the test set twice.</li> 
   <li class="readable-text" id="p101">The accuracy of a model measures how often it performs a task correctly. Measures such as precision and recall are used for information retrieval (e.g., fetching relevant legal cases according to a query from a much larger pool of legal cases). Precision and recall cannot be used by themselves; they must be combined in a way that matches business preferences about their relative importance. You can use the mean absolute error (MAE) or the root mean squared error (RMSE) to evaluate the performance of a model at predicting a number (such as the amount of rainfall).</li> 
  </ul>
 </body></html>