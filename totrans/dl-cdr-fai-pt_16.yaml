- en: Chapter 12\. A Language Model from Scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’re now ready to go deep…deep into deep learning! You already learned how
    to train a basic neural network, but how do you go from there to creating state-of-the-art
    models? In this part of the book, we’re going to uncover all of the mysteries,
    starting with language models.
  prefs: []
  type: TYPE_NORMAL
- en: You saw in [Chapter 10](ch10.xhtml#chapter_nlp) how to fine-tune a pretrained
    language model to build a text classifier. In this chapter, we will explain exactly
    what is inside that model and what an RNN is. First, let’s gather some data that
    will allow us to quickly prototype our various models.
  prefs: []
  type: TYPE_NORMAL
- en: The Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whenever we start working on a new problem, we always first try to think of
    the simplest dataset we can that will allow us to try out methods quickly and
    easily, and interpret the results. When we started working on language modeling
    a few years ago, we didn’t find any datasets that would allow for quick prototyping,
    so we made one. We call it *Human Numbers*, and it simply contains the first 10,000
    numbers written out in English.
  prefs: []
  type: TYPE_NORMAL
- en: Jeremy Says
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most common practical mistakes I see even among highly experienced
    practitioners is failing to use appropriate datasets at appropriate times during
    the analysis process. In particular, most people tend to start with datasets that
    are too big and too complicated.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can download, extract, and take a look at our dataset in the usual way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s open those two files and see what’s inside. At first, we’ll join all
    of the texts together and ignore the train/valid split given by the dataset (we’ll
    come back to that later):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We take all those lines and concatenate them in one big stream. To mark when
    we go from one number to the next, we use a `.` as a separator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can tokenize this dataset by splitting on spaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To numericalize, we have to create a list of all the unique tokens (our *vocab*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can convert our tokens into numbers by looking up the index of each
    in the vocab:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have a small dataset on which language modeling should be an easy
    task, we can build our first model.
  prefs: []
  type: TYPE_NORMAL
- en: Our First Language Model from Scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One simple way to turn this into a neural network would be to specify that we
    are going to predict each word based on the previous three words. We could create
    a list of every sequence of three words as our independent variables, and the
    next word after each sequence as the dependent variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can do that with plain Python. Let’s do it first with tokens just to confirm
    what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will do it with tensors of the numericalized values, which is what the
    model will actually use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We can batch those easily using the `DataLoader` class. For now, we will split
    the sequences randomly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We can now create a neural network architecture that takes three words as input,
    and returns a prediction of the probability of each possible next word in the
    vocab. We will use three standard linear layers, but with two tweaks.
  prefs: []
  type: TYPE_NORMAL
- en: The first tweak is that the first linear layer will use only the first word’s
    embedding as activations, the second layer will use the second word’s embedding
    plus the first layer’s output activations, and the third layer will use the third
    word’s embedding plus the second layer’s output activations. The key effect is
    that every word is interpreted in the information context of any words preceding
    it.
  prefs: []
  type: TYPE_NORMAL
- en: The second tweak is that each of these three layers will use the same weight
    matrix. The way that one word impacts the activations from previous words should
    not change depending on the position of a word. In other words, activation values
    will change as data moves through the layers, but the layer weights themselves
    will not change from layer to layer. So, a layer does not learn one sequence position;
    it must learn to handle all positions.
  prefs: []
  type: TYPE_NORMAL
- en: Since layer weights do not change, you might think of the sequential layers
    as “the same layer” repeated. In fact, PyTorch makes this concrete; we can create
    just one layer and use it multiple times.
  prefs: []
  type: TYPE_NORMAL
- en: Our Language Model in PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can now create the language model module that we described earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'As you see, we have created three layers:'
  prefs: []
  type: TYPE_NORMAL
- en: The embedding layer (`i_h`, for *input* to *hidden*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The linear layer to create the activations for the next word (`h_h`, for *hidden*
    to *hidden*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A final linear layer to predict the fourth word (`h_o`, for *hidden* to *output*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This might be easier to represent in pictorial form, so let’s define a simple
    pictorial representation of basic neural networks. [Figure 12-1](#img_simple_nn)
    shows how we’re going to represent a neural net with one hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![Pictorial representation of a simple neural network](Images/dlcf_1201.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-1\. Pictorial representation of a simple neural network
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Each shape represents activations: rectangle for input, circle for hidden (inner)
    layer activations, and triangle for output activations. We will use those shapes
    (summarized in [Figure 12-2](#img_shapes)) in all the diagrams in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Shapes used in our pictorial representations](Images/dlcf_1202.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-2\. Shapes used in our pictorial representations
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: An arrow represents the actual layer computation—i.e., the linear layer followed
    by the activation function. Using this notation, [Figure 12-3](#lm_rep) shows
    what our simple language model looks like.
  prefs: []
  type: TYPE_NORMAL
- en: '![Representation of our basic language model](Images/dlcf_1203.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-3\. Representation of our basic language model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To simplify things, we’ve removed the details of the layer computation from
    each arrow. We’ve also color-coded the arrows, such that all arrows with the same
    color have the same weight matrix. For instance, all the input layers use the
    same embedding matrix, so they all have the same color (green).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try training this model and see how it goes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1.824297 | 1.970941 | 0.467554 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1.386973 | 1.823242 | 0.467554 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1.417556 | 1.654497 | 0.494414 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1.376440 | 1.650849 | 0.494414 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: 'To see if this is any good, let’s check what a very simple model would give
    us. In this case, we could always predict the most common token, so let’s find
    out which token is most often the target in our validation set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The most common token has the index 29, which corresponds to the token `thousand`.
    Always predicting this token would give us an accuracy of roughly 15%, so we are
    faring way better!
  prefs: []
  type: TYPE_NORMAL
- en: Alexis Says
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'My first guess was that the separator would be the most common token, since
    there is one for every number. But looking at `tokens` reminded me that large
    numbers are written with many words, so on the way to 10,000 you write “thousand”
    a lot: five thousand, five thousand and one, five thousand and two, etc. Oops!
    Looking at your data is great for noticing subtle features as well as embarrassingly
    obvious ones.'
  prefs: []
  type: TYPE_NORMAL
- en: This is a nice first baseline. Let’s see how we can refactor it with a loop.
  prefs: []
  type: TYPE_NORMAL
- en: Our First Recurrent Neural Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Looking at the code for our module, we could simplify it by replacing the duplicated
    code that calls the layers with a `for` loop. In addition to making our code simpler,
    this will have the benefit that we will be able to apply our module equally well
    to token sequences of different lengths—we won’t be restricted to token lists
    of length three:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s check that we get the same results using this refactoring:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1.816274 | 1.964143 | 0.460185 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1.423805 | 1.739964 | 0.473259 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1.430327 | 1.685172 | 0.485382 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1.388390 | 1.657033 | 0.470406 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: We can also refactor our pictorial representation in exactly the same way, as
    shown in [Figure 12-4](#basic_rnn) (we’re also removing the details of activation
    sizes here, and using the same arrow colors as in [Figure 12-3](#lm_rep)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Basic recurrent neural network](Images/dlcf_1204.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-4\. Basic recurrent neural network
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You will see that a set of activations is being updated each time through the
    loop, stored in the variable `h`—this is called the *hidden state*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Jargon: Hidden State'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The activations that are updated at each step of a recurrent neural network.
  prefs: []
  type: TYPE_NORMAL
- en: A neural network that is defined using a loop like this is called a *recurrent
    neural network* (RNN). It is important to realize that an RNN is not a complicated
    new architecture, but simply a refactoring of a multilayer neural network using
    a `for` loop.
  prefs: []
  type: TYPE_NORMAL
- en: Alexis Says
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'My true opinion: if they were called “looping neural networks,” or LNNs, they
    would seem 50% less daunting!'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know what an RNN is, let’s try to make it a little bit better.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the RNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Looking at the code for our RNN, one thing that seems problematic is that we
    are initializing our hidden state to zero for every new input sequence. Why is
    that a problem? We made our sample sequences short so they would fit easily into
    batches. But if we order those samples correctly, the sample sequences will be
    read in order by the model, exposing the model to long stretches of the original
    sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another thing we can look at is having more signal: why predict only the fourth
    word when we could use the intermediate predictions to also predict the second
    and third words? Let’s see how we can implement those changes, starting with adding
    some state.'
  prefs: []
  type: TYPE_NORMAL
- en: Maintaining the State of an RNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because we initialize the model’s hidden state to zero for each new sample,
    we are throwing away all the information we have about the sentences we have seen
    so far, which means that our model doesn’t actually know where we are up to in
    the overall counting sequence. This is easily fixed; we can simply move the initialization
    of the hidden state to `__init__`.
  prefs: []
  type: TYPE_NORMAL
- en: But this fix will create its own subtle, but important, problem. It effectively
    makes our neural network as deep as the entire number of tokens in our document.
    For instance, if there were 10,000 tokens in our dataset, we would be creating
    a 10,000-layer neural network.
  prefs: []
  type: TYPE_NORMAL
- en: To see why this is the case, consider the original pictorial representation
    of our recurrent neural network in [Figure 12-3](#lm_rep), before refactoring
    it with a `for` loop. You can see each layer corresponds with one token input.
    When we talk about the representation of a recurrent neural network before refactoring
    with the `for` loop, we call this the *unrolled representation*. It is often helpful
    to consider the unrolled representation when trying to understand an RNN.
  prefs: []
  type: TYPE_NORMAL
- en: The problem with a 10,000-layer neural network is that if and when you get to
    the 10,000th word of the dataset, you will still need to calculate the derivatives
    all the way back to the first layer. This is going to be slow indeed, and memory-intensive.
    It is unlikely that you’ll be able to store even one mini-batch on your GPU.
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this problem is to tell PyTorch that we do not want to backpropagate
    the derivatives through the entire implicit neural network. Instead, we will keep
    just the last three layers of gradients. To remove all of the gradient history
    in PyTorch, we use the `detach` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the new version of our RNN. It is now stateful, because it remembers
    its activations between different calls to `forward`, which represent its use
    for different samples in the batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This model will have the same activations whatever sequence length we pick,
    because the hidden state will remember the last activation from the previous batch.
    The only thing that will be different is the gradients computed at each step:
    they will be calculated on only sequence length tokens in the past, instead of
    the whole stream. This approach is called *backpropagation through time* (BPTT).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Jargon: Backpropagation Through Time'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Treating a neural net with effectively one layer per time step (usually refactored
    using a loop) as one big model, and calculating gradients on it in the usual way.
    To avoid running out of memory and time, we usually use *truncated* BPTT, which
    “detaches” the history of computation steps in the hidden state every few time
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: To use `LMModel3`, we need to make sure the samples are going to be seen in
    a certain order. As we saw in [Chapter 10](ch10.xhtml#chapter_nlp), if the first
    line of the first batch is our `dset[0]`, the second batch should have `dset[1]`
    as the first line, so that the model sees the text flowing.
  prefs: []
  type: TYPE_NORMAL
- en: '`LMDataLoader` was doing this for us in [Chapter 10](ch10.xhtml#chapter_nlp).
    This time we’re going to do it ourselves.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we are going to rearrange our dataset. First we divide the samples
    into `m = len(dset) // bs` groups (this is the equivalent of splitting the whole
    concatenated dataset into, for example, 64 equally sized pieces, since we’re using
    `bs=64` here). `m` is the length of each of these pieces. For instance, if we’re
    using our whole dataset (although we’ll actually split it into train versus valid
    in a moment), we have this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The first batch will be composed of the samples
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: the second batch of the samples
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: and so forth. This way, at each epoch, the model will see a chunk of contiguous
    text of size `3*m` (since each text is of size 3) on each line of the batch.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following function does that reindexing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we just pass `drop_last=True` when building our `DataLoaders` to drop
    the last batch that does not have a shape of `bs`. We also pass `shuffle=False`
    to make sure the texts are read in order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The last thing we add is a little tweak of the training loop via a `Callback`.
    We will talk more about callbacks in [Chapter 16](ch16.xhtml#chapter_accel_sgd);
    this one will call the `reset` method of our model at the beginning of each epoch
    and before each validation phase. Since we implemented that method to set the
    hidden state of the model to zero, this will make sure we start with a clean state
    before reading those continuous chunks of text. We can also start training a bit
    longer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1.677074 | 1.827367 | 0.467548 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1.282722 | 1.870913 | 0.388942 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1.090705 | 1.651793 | 0.462500 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1.005092 | 1.613794 | 0.516587 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0.965975 | 1.560775 | 0.551202 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 0.916182 | 1.595857 | 0.560577 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 0.897657 | 1.539733 | 0.574279 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 0.836274 | 1.585141 | 0.583173 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 0.805877 | 1.629808 | 0.586779 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 0.795096 | 1.651267 | 0.588942 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: This is already better! The next step is to use more targets and compare them
    to the intermediate predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Creating More Signal
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another problem with our current approach is that we predict only one output
    word for each three input words. As a result, the amount of signal that we are
    feeding back to update weights with is not as large as it could be. It would be
    better if we predicted the next word after every single word, rather than every
    three words, as shown in [Figure 12-5](#stateful_rep).
  prefs: []
  type: TYPE_NORMAL
- en: '![RNN predicting after every token](Images/dlcf_1205.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-5\. RNN predicting after every token
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This is easy enough to add. We need to first change our data so that the dependent
    variable has each of the three next words after each of our three input words.
    Instead of `3`, we use an attribute, `sl` (for sequence length), and make it a
    bit bigger:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Looking at the first element of `seqs`, we can see that it contains two lists
    of the same size. The second list is the same as the first, but offset by one
    element:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we need to modify our model so that it outputs a prediction after every
    word, rather than just at the end of a three-word sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'This model will return outputs of shape `bs x sl x vocab_sz` (since we stacked
    on `dim=1`). Our targets are of shape `bs x sl`, so we need to flatten those before
    using them in `F.cross_entropy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now use this loss function to train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 3.103298 | 2.874341 | 0.212565 | 00:01 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2.231964 | 1.971280 | 0.462158 | 00:01 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1.711358 | 1.813547 | 0.461182 | 00:01 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1.448516 | 1.828176 | 0.483236 | 00:01 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1.288630 | 1.659564 | 0.520671 | 00:01 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 1.161470 | 1.714023 | 0.554932 | 00:01 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 1.055568 | 1.660916 | 0.575033 | 00:01 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 0.960765 | 1.719624 | 0.591064 | 00:01 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 0.870153 | 1.839560 | 0.614665 | 00:01 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 0.808545 | 1.770278 | 0.624349 | 00:01 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 0.758084 | 1.842931 | 0.610758 | 00:01 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 0.719320 | 1.799527 | 0.646566 | 00:01 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 0.683439 | 1.917928 | 0.649821 | 00:01 |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | 0.660283 | 1.874712 | 0.628581 | 00:01 |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | 0.646154 | 1.877519 | 0.640055 | 00:01 |'
  prefs: []
  type: TYPE_TB
- en: We need to train for longer, since the task has changed a bit and is more complicated
    now. But we end up with a good result…at least, sometimes. If you run it a few
    times, you’ll see that you can get quite different results on different runs.
    That’s because effectively we have a very deep network here, which can result
    in very large or very small gradients. We’ll see in the next part of this chapter
    how to deal with this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the obvious way to get a better model is to go deeper: we have only one
    linear layer between the hidden state and the output activations in our basic
    RNN, so maybe we’ll get better results with more.'
  prefs: []
  type: TYPE_NORMAL
- en: Multilayer RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a multilayer RNN, we pass the activations from our recurrent neural network
    into a second recurrent neural network, as in [Figure 12-6](#stacked_rnn_rep).
  prefs: []
  type: TYPE_NORMAL
- en: '![2-layer RNN](Images/dlcf_1206.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-6\. 2-layer RNN
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The unrolled representation is shown in [Figure 12-7](#unrolled_stack_rep) (similar
    to [Figure 12-3](#lm_rep)).
  prefs: []
  type: TYPE_NORMAL
- en: '![2-layer unrolled RNN](Images/dlcf_1207.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-7\. 2-layer unrolled RNN
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s see how to implement this in practice.
  prefs: []
  type: TYPE_NORMAL
- en: The Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can save some time by using PyTorch’s `RNN` class, which implements exactly
    what we created earlier, but also gives us the option to stack multiple RNNs,
    as we have discussed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 3.055853 | 2.591640 | 0.437907 | 00:01 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2.162359 | 1.787310 | 0.471598 | 00:01 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1.710663 | 1.941807 | 0.321777 | 00:01 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1.520783 | 1.999726 | 0.312012 | 00:01 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1.330846 | 2.012902 | 0.413249 | 00:01 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 1.163297 | 1.896192 | 0.450684 | 00:01 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 1.033813 | 2.005209 | 0.434814 | 00:01 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 0.919090 | 2.047083 | 0.456706 | 00:01 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 0.822939 | 2.068031 | 0.468831 | 00:01 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 0.750180 | 2.136064 | 0.475098 | 00:01 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 0.695120 | 2.139140 | 0.485433 | 00:01 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 0.655752 | 2.155081 | 0.493652 | 00:01 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 0.629650 | 2.162583 | 0.498535 | 00:01 |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | 0.613583 | 2.171649 | 0.491048 | 00:01 |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | 0.604309 | 2.180355 | 0.487874 | 00:01 |'
  prefs: []
  type: TYPE_TB
- en: Now that’s disappointing…our previous single-layer RNN performed better. Why?
    The reason is that we have a deeper model, leading to exploding or vanishing activations.
  prefs: []
  type: TYPE_NORMAL
- en: Exploding or Disappearing Activations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In practice, creating accurate models from this kind of RNN is difficult. We
    will get better results if we call `detach` less often, and have more layers—this
    gives our RNN a longer time horizon to learn from and richer features to create.
    But it also means we have a deeper model to train. The key challenge in the development
    of deep learning has been figuring out how to train these kinds of models.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is challenging because of what happens when you multiply by a matrix many
    times. Think about what happens when you multiply by a number many times. For
    example, if you multiply by 2, starting at 1, you get the sequence 1, 2, 4, 8,…and
    after 32 steps, you are already at 4,294,967,296\. A similar issue happens if
    you multiply by 0.5: you get 0.5, 0.25, 0.125…and after 32 steps, it’s 0.00000000023\.
    As you can see, multiplying by a number even slightly higher or lower than 1 results
    in an explosion or disappearance of our starting number, after just a few repeated
    multiplications.'
  prefs: []
  type: TYPE_NORMAL
- en: Because matrix multiplication is just multiplying numbers and adding them up,
    exactly the same thing happens with repeated matrix multiplications. And that’s
    all a deep neural network is—each extra layer is another matrix multiplication.
    This means that it is very easy for a deep neural network to end up with extremely
    large or extremely small numbers.
  prefs: []
  type: TYPE_NORMAL
- en: This is a problem, because the way computers store numbers (known as *floating
    point*) means that they become less and less accurate the further away the numbers
    get from zero. The diagram in [Figure 12-8](#float_prec), from the excellent article
    [“What You Never Wanted to Know about Floating Point but Will Be Forced to Find
    Out”](https://oreil.ly/c_kG9), shows how the precision of floating-point numbers
    varies over the number line.
  prefs: []
  type: TYPE_NORMAL
- en: '![Precision of floating-point numbers](Images/dlcf_1208.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-8\. Precision of floating-point numbers
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This inaccuracy means that often the gradients calculated for updating the weights
    end up as zero or infinity for deep networks. This is commonly referred to as
    the *vanishing gradients* or *exploding gradients* problem. It means that in SGD,
    the weights are either not updated at all or jump to infinity. Either way, they
    won’t improve with training.
  prefs: []
  type: TYPE_NORMAL
- en: Researchers have developed ways to tackle this problem, which we will be discussing
    later in the book. One option is to change the definition of a layer in a way
    that makes it less likely to have exploding activations. We’ll look at the details
    of how this is done in [Chapter 13](ch13.xhtml#chapter_convolutions), when we
    discuss batch normalization, and [Chapter 14](ch14.xhtml#chapter_resnet), when
    we discuss ResNets, although these details don’t generally matter in practice
    (unless you are a researcher who is creating new approaches to solving this problem).
    Another strategy for dealing with this is by being careful about initialization,
    which is a topic we’ll investigate in [Chapter 17](ch17.xhtml#chapter_foundations).
  prefs: []
  type: TYPE_NORMAL
- en: 'For RNNs, two types of layers are frequently used to avoid exploding activations:
    *gated recurrent units* (GRUs) and *long short-term memory* (LSTM) layers. Both
    of these are available in PyTorch and are drop-in replacements for the RNN layer.
    We will cover only LSTMs in this book; plenty of good tutorials online explain
    GRUs, which are a minor variant on the LSTM design.'
  prefs: []
  type: TYPE_NORMAL
- en: LSTM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LSTM is an architecture that was introduced back in 1997 by Jürgen Schmidhuber
    and Sepp Hochreiter. In this architecture, there are not one, but two, hidden
    states. In our base RNN, the hidden state is the output of the RNN at the previous
    time step. That hidden state is then responsible for two things:'
  prefs: []
  type: TYPE_NORMAL
- en: Having the right information for the output layer to predict the correct next
    token
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retaining memory of everything that happened in the sentence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider, for example, the sentences “Henry has a dog and he likes his dog very
    much” and “Sophie has a dog and she likes her dog very much.” It’s very clear
    that the RNN needs to remember the name at the beginning of the sentence to be
    able to predict *he/she* or *his/her*.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, RNNs are really bad at retaining memory of what happened much earlier
    in the sentence, which is the motivation to have another hidden state (called
    *cell state*) in the LSTM. The cell state will be responsible for keeping *long
    short-term memory*, while the hidden state will focus on the next token to predict.
    Let’s take a closer look at how this is achieved and build an LSTM from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Building an LSTM from Scratch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to build an LSTM, we first have to understand its architecture. [Figure 12-9](#lstm)
    shows its inner structure.
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph showing the inner architecture of an LSTM](Images/dlcf_1209.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-9\. Architecture of an LSTM
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In this picture, our input <math alttext="x Subscript t"><msub><mi>x</mi> <mi>t</mi></msub></math>
    enters on the left with the previous hidden state ( <math alttext="h Subscript
    t minus 1"><msub><mi>h</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>
    ) and cell state ( <math alttext="c Subscript t minus 1"><msub><mi>c</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>
    ). The four orange boxes represent four layers (our neural nets), with the activation
    being either sigmoid ( <math alttext="sigma"><mi>σ</mi></math> ) or tanh. tanh
    is just a sigmoid function rescaled to the range –1 to 1\. Its mathematical expression
    can be written like this:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="hyperbolic tangent left-parenthesis x right-parenthesis equals
    StartFraction e Superscript x Baseline plus e Superscript negative x Baseline
    Over e Superscript x Baseline minus e Superscript negative x Baseline EndFraction
    equals 2 sigma left-parenthesis 2 x right-parenthesis minus 1" display="block"><mrow><mo
    form="prefix">tanh</mo> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mfrac><mrow><msup><mi>e</mi> <mi>x</mi></msup> <mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mi>x</mi></mrow></msup></mrow>
    <mrow><msup><mi>e</mi> <mi>x</mi></msup> <mo>-</mo><msup><mi>e</mi> <mrow><mo>-</mo><mi>x</mi></mrow></msup></mrow></mfrac>
    <mo>=</mo> <mn>2</mn> <mi>σ</mi> <mrow><mo>(</mo> <mn>2</mn> <mi>x</mi> <mo>)</mo></mrow>
    <mo>-</mo> <mn>1</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math alttext="sigma"><mi>σ</mi></math> is the sigmoid function. The green
    circles in the figure are elementwise operations. What goes out on the right is
    the new hidden state ( <math alttext="h Subscript t"><msub><mi>h</mi> <mi>t</mi></msub></math>
    ) and new cell state ( <math alttext="c Subscript t"><msub><mi>c</mi> <mi>t</mi></msub></math>
    ), ready for our next input. The new hidden state is also used as output, which
    is why the arrow splits to go up.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go over the four neural nets (called *gates*) one by one and explain the
    diagram—but before this, notice how very little the cell state (at the top) is
    changed. It doesn’t even go directly through a neural net! This is exactly why
    it will carry on a longer-term state.
  prefs: []
  type: TYPE_NORMAL
- en: First, the arrows for input and old hidden state are joined together. In the
    RNN we wrote earlier in this chapter, we were adding them together. In the LSTM,
    we stack them in one big tensor. This means the dimension of our embeddings (which
    is the dimension of <math alttext="x Subscript t"><msub><mi>x</mi> <mi>t</mi></msub></math>
    ) can be different from the dimension of our hidden state. If we call those `n_in`
    and `n_hid`, the arrow at the bottom is of size `n_in + n_hid`; thus all the neural
    nets (orange boxes) are linear layers with `n_in + n_hid` inputs and `n_hid` outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first gate (looking from left to right) is called the *forget gate*. Since
    it’s a linear layer followed by a sigmoid, its output will consist of scalars
    between 0 and 1\. We multiply this result by the cell state to determine which
    information to keep and which to throw away: values closer to 0 are discarded,
    and values closer to 1 are kept. This gives the LSTM the ability to forget things
    about its long-term state. For instance, when crossing a period or an `xxbos`
    token, we would expect it to (have learned to) reset its cell state.'
  prefs: []
  type: TYPE_NORMAL
- en: The second gate is called the *input gate*. It works with the third gate (which
    doesn’t really have a name but is sometimes called the *cell gate*) to update
    the cell state. For instance, we may see a new gender pronoun, in which case we’ll
    need to replace the information about gender that the forget gate removed. Similar
    to the forget gate, the input gate decides which elements of the cell state to
    update (values close to 1) or not (values close to 0). The third gate determines
    what those updated values are, in the range of –1 to 1 (thanks to the tanh function).
    The result is added to the cell state.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last gate is the *output gate*. It determines which information from the
    cell state to use to generate the output. The cell state goes through a tanh before
    being combined with the sigmoid output from the output gate, and the result is
    the new hidden state. In terms of code, we can write the same steps like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'In practice, we can then refactor the code. Also, in terms of performance,
    it’s better to do one big matrix multiplication than four smaller ones (that’s
    because we launch the special fast kernel on the GPU only once, and it gives the
    GPU more work to do in parallel). The stacking takes a bit of time (since we have
    to move one of the tensors around on the GPU to have it all in a contiguous array),
    so we use two separate layers for the input and the hidden state. The optimized
    and refactored code then looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we use the PyTorch `chunk` method to split our tensor into four pieces.
    It works like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Let’s now use this architecture to train a language model!
  prefs: []
  type: TYPE_NORMAL
- en: Training a Language Model Using LSTMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here is the same network as `LMModel5`, using a two-layer LSTM. We can train
    it at a higher learning rate, for a shorter time, and get better accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 3.000821 | 2.663942 | 0.438314 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2.139642 | 2.184780 | 0.240479 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1.607275 | 1.812682 | 0.439779 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1.347711 | 1.830982 | 0.497477 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1.123113 | 1.937766 | 0.594401 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 0.852042 | 2.012127 | 0.631592 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 0.565494 | 1.312742 | 0.725749 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 0.347445 | 1.297934 | 0.711263 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 0.208191 | 1.441269 | 0.731201 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 0.126335 | 1.569952 | 0.737305 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 0.079761 | 1.427187 | 0.754150 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 0.052990 | 1.494990 | 0.745117 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 0.039008 | 1.393731 | 0.757894 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | 0.031502 | 1.373210 | 0.758464 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | 0.028068 | 1.368083 | 0.758464 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: Now that’s better than a multilayer RNN! We can still see there is a bit of
    overfitting, however, which is a sign that a bit of regularization might help.
  prefs: []
  type: TYPE_NORMAL
- en: Regularizing an LSTM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recurrent neural networks, in general, are hard to train, because of the problem
    of vanishing activations and gradients we saw before. Using LSTM (or GRU) cells
    makes training easier than with vanilla RNNs, but they are still very prone to
    overfitting. Data augmentation, while a possibility, is less often used for text
    data than for images because in most cases it requires another model to generate
    random augmentations (e.g., by translating the text into another language and
    then back into the original language). Overall, data augmentation for text data
    is currently not a well-explored space.
  prefs: []
  type: TYPE_NORMAL
- en: However, we can use other regularization techniques instead to reduce overfitting,
    which were thoroughly studied for use with LSTMs in the paper [“Regularizing and
    Optimizing LSTM Language Models”](https://oreil.ly/Rf-OG) by Stephen Merity et
    al. This paper showed how effective use of dropout, activation regularization,
    and temporal activation regularization could allow an LSTM to beat state-of-the-art
    results that previously required much more complicated models. The authors called
    an LSTM using these techniques an *AWD-LSTM*. We’ll look at each of these techniques
    in turn.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Dropout* is a regularization technique that was introduced by Geoffrey Hinton
    et al. in [“Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors”](https://oreil.ly/-_xie).
    The basic idea is to randomly change some activations to zero at training time.
    This makes sure all neurons actively work toward the output, as seen in [Figure 12-10](#img_dropout)
    (from [“Dropout: A Simple Way to Prevent Neural Networks from Overfitting”](https://oreil.ly/pYNxF)
    by Nitish Srivastava et al.).'
  prefs: []
  type: TYPE_NORMAL
- en: '![A figure from the article showing how neurons go off with dropout](Images/dlcf_1210.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-10\. Applying dropout in a neural network (courtesy of Nitish Srivastava
    et al.)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Hinton used a nice metaphor when he explained, in an interview, the inspiration
    for dropout:'
  prefs: []
  type: TYPE_NORMAL
- en: I went to my bank. The tellers kept changing, and I asked one of them why. He
    said he didn’t know but they got moved around a lot. I figured it must be because
    it would require cooperation between employees to successfully defraud the bank.
    This made me realize that randomly removing a different subset of neurons on each
    example would prevent conspiracies and thus reduce overfitting.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In the same interview, he also explained that neuroscience provided additional
    inspiration:'
  prefs: []
  type: TYPE_NORMAL
- en: We don’t really know why neurons spike. One theory is that they want to be noisy
    so as to regularize, because we have many more parameters than we have data points.
    The idea of dropout is that if you have noisy activations, you can afford to use
    a much bigger model.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'This explains the idea behind why dropout helps to generalize: first it helps
    the neurons to cooperate better together; then it makes the activations more noisy,
    thus making the model more robust.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see, however, that if we were to just zero those activations without
    doing anything else, our model would have problems training: if we go from the
    sum of five activations (that are all positive numbers since we apply a ReLU)
    to just two, this won’t have the same scale. Therefore, if we apply dropout with
    a probability `p`, we rescale all activations by dividing them by `1-p` (on average
    `p` will be zeroed, so it leaves `1-p`), as shown in [Figure 12-11](#img_dropout1).'
  prefs: []
  type: TYPE_NORMAL
- en: '![A figure from the article introducing dropout showing how a neuron is on/off](Images/dlcf_1211.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-11\. Why we scale the activations when applying dropout (courtesy
    of Nitish Srivastava et al.)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This is a full implementation of the dropout layer in PyTorch (although PyTorch’s
    native layer is actually written in C, not Python):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: The `bernoulli_` method is creating a tensor of random zeros (with probability
    `p`) and ones (with probability `1-p`), which is then multiplied with our input
    before dividing by `1-p`. Note the use of the `training` attribute, which is available
    in any PyTorch `nn.Module`, and tells us if we are doing training or inference.
  prefs: []
  type: TYPE_NORMAL
- en: Do Your Own Experiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters of the book, we’d be adding a code example for `bernoulli_`
    here, so you can see exactly how it works. But now that you know enough to do
    this yourself, we’re going to be doing fewer and fewer examples for you, and instead
    expecting you to do your own experiments to see how things work. In this case,
    you’ll see in the end-of-chapter questionnaire that we’re asking you to experiment
    with `bernoulli_`—but don’t wait for us to ask you to experiment to develop your
    understanding of the code we’re studying; go ahead and do it anyway!
  prefs: []
  type: TYPE_NORMAL
- en: Using dropout before passing the output of our LSTM to the final layer will
    help reduce overfitting. Dropout is also used in many other models, including
    the default CNN head used in `fastai.vision`, and is available in `fastai.tabular`
    by passing the `ps` parameter (where each “p” is passed to each added `Dropout`
    layer), as we’ll see in [Chapter 15](ch15.xhtml#chapter_arch_details).
  prefs: []
  type: TYPE_NORMAL
- en: Dropout has different behavior in training and validation mode, which we specified
    using the `training` attribute in `Dropout`. Calling the `train` method on a `Module`
    sets `training` to `True` (both for the module you call the method on and for
    every module it recursively contains), and `eval` sets it to `False`. This is
    done automatically when calling the methods of `Learner`, but if you are not using
    that class, remember to switch from one to the other as needed.
  prefs: []
  type: TYPE_NORMAL
- en: Activation Regularization and Temporal Activation Regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Activation regularization* (AR) and *temporal activation regularization* (TAR)
    are two regularization methods very similar to weight decay, discussed in [Chapter 8](ch08.xhtml#chapter_collab).
    When applying weight decay, we add a small penalty to the loss that aims at making
    the weights as small as possible. For activation regularization, it’s the final
    activations produced by the LSTM that we will try to make as small as possible,
    instead of the weights.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To regularize the final activations, we have to store those somewhere, then
    add the means of the squares of them to the loss (along with a multiplier `alpha`,
    which is just like `wd` for weight decay):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Temporal activation regularization is linked to the fact we are predicting
    tokens in a sentence. That means it’s likely that the outputs of our LSTMs should
    somewhat make sense when we read them in order. TAR is there to encourage that
    behavior by adding a penalty to the loss to make the difference between two consecutive
    activations as small as possible: our activations tensor has a shape `bs x sl
    x n_hid`, and we read consecutive activations on the sequence length axis (the
    dimension in the middle). With this, TAR can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '`alpha` and `beta` are then two hyperparameters to tune. To make this work,
    we need our model with dropout to return three things: the proper output, the
    activations of the LSTM pre-dropout, and the activations of the LSTM post-dropout.
    AR is often applied on the dropped-out activations (to not penalize the activations
    we turned into zeros afterward), while TAR is applied on the non-dropped-out activations
    (because those zeros create big differences between two consecutive time steps).
    A callback called `RNNRegularizer` will then apply this regularization for us.'
  prefs: []
  type: TYPE_NORMAL
- en: Training a Weight-Tied Regularized LSTM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can combine dropout (applied before we go into our output layer) with AR
    and TAR to train our previous LSTM. We just need to return three things instead
    of one: the normal output of our LSTM, the dropped-out activations, and the activations
    from our LSTMs. The last two will be picked up by the callback `RNNRegularization`
    for the contributions it has to make to the loss.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another useful trick we can add from [the AWD-LSTM paper](https://oreil.ly/ETQ5X)
    is *weight tying*. In a language model, the input embeddings represent a mapping
    from English words to activations, and the output hidden layer represents a mapping
    from activations to English words. We might expect, intuitively, that these mappings
    could be the same. We can represent this in PyTorch by assigning the same weight
    matrix to each of these layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'In `LMMModel7`, we include these final tweaks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'We can create a regularized `Learner` using the `RNNRegularizer` callback:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'A `TextLearner` automatically adds those two callbacks for us (with those values
    for `alpha` and `beta` as defaults), so we can simplify the preceding line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then train the model, and add additional regularization by increasing
    the weight decay to `0.1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 2.693885 | 2.013484 | 0.466634 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1.685549 | 1.187310 | 0.629313 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.973307 | 0.791398 | 0.745605 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.555823 | 0.640412 | 0.794108 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0.351802 | 0.557247 | 0.836100 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 0.244986 | 0.594977 | 0.807292 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 0.192231 | 0.511690 | 0.846761 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 0.162456 | 0.520370 | 0.858073 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 0.142664 | 0.525918 | 0.842285 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 0.128493 | 0.495029 | 0.858073 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 0.117589 | 0.464236 | 0.867188 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 0.109808 | 0.466550 | 0.869303 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 0.104216 | 0.455151 | 0.871826 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | 0.100271 | 0.452659 | 0.873617 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | 0.098121 | 0.458372 | 0.869385 | 00:02 |'
  prefs: []
  type: TYPE_TB
- en: Now this is far better than our previous model!
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You have now seen everything that is inside the AWD-LSTM architecture we used
    in text classification in [Chapter 10](ch10.xhtml#chapter_nlp). It uses dropout
    in a lot more places:'
  prefs: []
  type: TYPE_NORMAL
- en: Embedding dropout (just after the embedding layer)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Input dropout (after the embedding layer)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weight dropout (applied to the weights of the LSTM at each training step)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden dropout (applied to the hidden state between two layers)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This makes it even more regularized. Since fine-tuning those five dropout values
    (including the dropout before the output layer) is complicated, we have determined
    good defaults and allow the magnitude of dropout to be tuned overall with the
    `drop_mult` parameter you saw in that chapter (which is multiplied by each dropout).
  prefs: []
  type: TYPE_NORMAL
- en: Another architecture that is very powerful, especially in “sequence-to-sequence”
    problems (problems in which the dependent variable is itself a variable-length
    sequence, such as language translation), is the Transformers architecture. You
    can find it in a bonus chapter on [the book’s website](https://book.fast.ai).
  prefs: []
  type: TYPE_NORMAL
- en: Questionnaire
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If the dataset for your project is so big and complicated that working with
    it takes a significant amount of time, what should you do?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we concatenate the documents in our dataset before creating a language
    model?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To use a standard fully connected network to predict the fourth word given the
    previous three words, what two tweaks do we need to make to our model?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can we share a weight matrix across multiple layers in PyTorch?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a module that predicts the third word given the previous two words of
    a sentence, without peeking.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a recurrent neural network?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is hidden state?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the equivalent of hidden state in `LMModel1`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To maintain the state in an RNN, why is it important to pass the text to the
    model in order?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is an “unrolled” representation of an RNN?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why can maintaining the hidden state in an RNN lead to memory and performance
    problems? How do we fix this problem?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is BPTT?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write code to print out the first few batches of the validation set, including
    converting the token IDs back into English strings, as we showed for batches of
    IMDb data in [Chapter 10](ch10.xhtml#chapter_nlp).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does the `ModelResetter` callback do? Why do we need it?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the downsides of predicting just one output word for each three input
    words?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we need a custom loss function for `LMModel4`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is the training of `LMModel4` unstable?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the unrolled representation, we can see that a recurrent neural network has
    many layers. So why do we need to stack RNNs to get better results?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Draw a representation of a stacked (multilayer) RNN.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why should we get better results in an RNN if we call `detach` less often? Why
    might this not happen in practice with a simple RNN?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why can a deep network result in very large or very small activations? Why does
    this matter?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In a computer’s floating-point representation of numbers, which numbers are
    the most precise?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do vanishing gradients prevent training?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why does it help to have two hidden states in the LSTM architecture? What is
    the purpose of each one?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are these two states called in an LSTM?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is tanh, and how is it related to sigmoid?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'What is the purpose of this code in `LSTMCell`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: What does `chunk` do in PyTorch?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Study the refactored version of `LSTMCell` carefully to ensure you understand
    how and why it does the same thing as the nonrefactored version.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why can we use a higher learning rate for `LMModel6`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the three regularization techniques used in an AWD-LSTM model?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is dropout?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we scale the weights with dropout? Is this applied during training, inference,
    or both?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'What is the purpose of this line from `Dropout`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Experiment with `bernoulli_` to understand how it works.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you set your model in training mode in PyTorch? In evaluation mode?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write the equation for activation regularization (in math or code, as you prefer).
    How is it different from weight decay?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write the equation for temporal activation regularization (in math or code,
    as you prefer). Why wouldn’t we use this for computer vision problems?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is weight tying in a language model?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further Research
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In `LMModel2`, why can `forward` start with `h=0`? Why don’t we need to say
    `h=torch.zeros(...)`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write the code for an LSTM from scratch (you may refer to [Figure 12-9](#lstm)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Search the internet for the GRU architecture and implement it from scratch,
    and try training a model. See if you can get results similar to those we saw in
    this chapter. Compare your results to the results of PyTorch’s built-in `GRU`
    module.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take a look at the source code for AWD-LSTM in fastai, and try to map each of
    the lines of code to the concepts shown in this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
