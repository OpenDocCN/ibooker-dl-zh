- en: Chapter 12\. A Language Model from Scratch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章。从头开始的语言模型
- en: We’re now ready to go deep…deep into deep learning! You already learned how
    to train a basic neural network, but how do you go from there to creating state-of-the-art
    models? In this part of the book, we’re going to uncover all of the mysteries,
    starting with language models.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备深入…深入深度学习！您已经学会了如何训练基本的神经网络，但是如何从那里创建最先进的模型呢？在本书的这一部分，我们将揭开所有的神秘，从语言模型开始。
- en: You saw in [Chapter 10](ch10.xhtml#chapter_nlp) how to fine-tune a pretrained
    language model to build a text classifier. In this chapter, we will explain exactly
    what is inside that model and what an RNN is. First, let’s gather some data that
    will allow us to quickly prototype our various models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 您在[第10章](ch10.xhtml#chapter_nlp)中看到了如何微调预训练的语言模型以构建文本分类器。在本章中，我们将解释该模型的内部结构以及RNN是什么。首先，让我们收集一些数据，这些数据将允许我们快速原型化各种模型。
- en: The Data
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据
- en: Whenever we start working on a new problem, we always first try to think of
    the simplest dataset we can that will allow us to try out methods quickly and
    easily, and interpret the results. When we started working on language modeling
    a few years ago, we didn’t find any datasets that would allow for quick prototyping,
    so we made one. We call it *Human Numbers*, and it simply contains the first 10,000
    numbers written out in English.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们开始处理一个新问题时，我们总是首先尝试想出一个最简单的数据集，这样可以让我们快速轻松地尝试方法并解释结果。几年前我们开始进行语言建模时，我们没有找到任何可以快速原型的数据集，所以我们自己制作了一个。我们称之为*Human
    Numbers*，它简单地包含了用英语写出的前10000个数字。
- en: Jeremy Says
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Jeremy说
- en: One of the most common practical mistakes I see even among highly experienced
    practitioners is failing to use appropriate datasets at appropriate times during
    the analysis process. In particular, most people tend to start with datasets that
    are too big and too complicated.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我在高度经验丰富的从业者中经常看到的一个常见实际错误是在分析过程中未能在适当的时间使用适当的数据集。特别是，大多数人倾向于从太大、太复杂的数据集开始。
- en: 'We can download, extract, and take a look at our dataset in the usual way:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按照通常的方式下载、提取并查看我们的数据集：
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let’s open those two files and see what’s inside. At first, we’ll join all
    of the texts together and ignore the train/valid split given by the dataset (we’ll
    come back to that later):'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打开这两个文件，看看里面有什么。首先，我们将把所有文本连接在一起，忽略数据集给出的训练/验证拆分（我们稍后会回到这一点）：
- en: '[PRE3]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We take all those lines and concatenate them in one big stream. To mark when
    we go from one number to the next, we use a `.` as a separator:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将所有这些行连接在一个大流中。为了标记我们从一个数字到下一个数字的转变，我们使用`.`作为分隔符：
- en: '[PRE5]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can tokenize this dataset by splitting on spaces:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在空格上拆分来对这个数据集进行标记化：
- en: '[PRE7]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To numericalize, we have to create a list of all the unique tokens (our *vocab*):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了数值化，我们必须创建一个包含所有唯一标记（我们的*词汇表*）的列表：
- en: '[PRE9]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then we can convert our tokens into numbers by looking up the index of each
    in the vocab:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以通过查找每个词在词汇表中的索引，将我们的标记转换为数字：
- en: '[PRE11]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now that we have a small dataset on which language modeling should be an easy
    task, we can build our first model.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个小数据集，语言建模应该是一个简单的任务，我们可以构建我们的第一个模型。
- en: Our First Language Model from Scratch
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们的第一个从头开始的语言模型
- en: One simple way to turn this into a neural network would be to specify that we
    are going to predict each word based on the previous three words. We could create
    a list of every sequence of three words as our independent variables, and the
    next word after each sequence as the dependent variable.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 将这转换为神经网络的一个简单方法是指定我们将基于前三个单词预测每个单词。我们可以创建一个包含每个三个单词序列的列表作为我们的自变量，以及每个序列后面的下一个单词作为因变量。
- en: 'We can do that with plain Python. Let’s do it first with tokens just to confirm
    what it looks like:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用普通的Python来做到这一点。首先让我们用标记来确认它是什么样子的：
- en: '[PRE13]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now we will do it with tensors of the numericalized values, which is what the
    model will actually use:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用数值化值的张量来做到这一点，这正是模型实际使用的：
- en: '[PRE15]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can batch those easily using the `DataLoader` class. For now, we will split
    the sequences randomly:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`DataLoader`类轻松地对这些进行批处理。现在，我们将随机拆分序列：
- en: '[PRE17]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We can now create a neural network architecture that takes three words as input,
    and returns a prediction of the probability of each possible next word in the
    vocab. We will use three standard linear layers, but with two tweaks.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以创建一个神经网络架构，它以三个单词作为输入，并返回词汇表中每个可能的下一个单词的概率预测。我们将使用三个标准线性层，但有两个调整。
- en: The first tweak is that the first linear layer will use only the first word’s
    embedding as activations, the second layer will use the second word’s embedding
    plus the first layer’s output activations, and the third layer will use the third
    word’s embedding plus the second layer’s output activations. The key effect is
    that every word is interpreted in the information context of any words preceding
    it.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个调整是，第一个线性层将仅使用第一个词的嵌入作为激活，第二层将使用第二个词的嵌入加上第一层的输出激活，第三层将使用第三个词的嵌入加上第二层的输出激活。关键效果是每个词都在其前面的任何单词的信息上下文中被解释。
- en: The second tweak is that each of these three layers will use the same weight
    matrix. The way that one word impacts the activations from previous words should
    not change depending on the position of a word. In other words, activation values
    will change as data moves through the layers, but the layer weights themselves
    will not change from layer to layer. So, a layer does not learn one sequence position;
    it must learn to handle all positions.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个调整是，这三个层中的每一个将使用相同的权重矩阵。一个词对来自前面单词的激活的影响方式不应该取决于单词的位置。换句话说，激活值会随着数据通过层移动而改变，但是层权重本身不会从一层到另一层改变。因此，一个层不会学习一个序列位置；它必须学会处理所有位置。
- en: Since layer weights do not change, you might think of the sequential layers
    as “the same layer” repeated. In fact, PyTorch makes this concrete; we can create
    just one layer and use it multiple times.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 由于层权重不会改变，您可能会认为顺序层是“重复的相同层”。事实上，PyTorch 使这一点具体化；我们可以创建一个层并多次使用它。
- en: Our Language Model in PyTorch
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的 PyTorch 语言模型
- en: 'We can now create the language model module that we described earlier:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以创建我们之前描述的语言模型模块：
- en: '[PRE18]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'As you see, we have created three layers:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，我们已经创建了三个层：
- en: The embedding layer (`i_h`, for *input* to *hidden*)
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入层（`i_h`，表示 *输入* 到 *隐藏*）
- en: The linear layer to create the activations for the next word (`h_h`, for *hidden*
    to *hidden*)
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性层用于创建下一个单词的激活（`h_h`，表示 *隐藏* 到 *隐藏*）
- en: A final linear layer to predict the fourth word (`h_o`, for *hidden* to *output*)
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个最终的线性层来预测第四个单词（`h_o`，表示 *隐藏* 到 *输出*）
- en: This might be easier to represent in pictorial form, so let’s define a simple
    pictorial representation of basic neural networks. [Figure 12-1](#img_simple_nn)
    shows how we’re going to represent a neural net with one hidden layer.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能更容易以图示形式表示，因此让我们定义一个基本神经网络的简单图示表示。[图 12-1](#img_simple_nn) 显示了我们将如何用一个隐藏层表示神经网络。
- en: '![Pictorial representation of a simple neural network](Images/dlcf_1201.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![简单神经网络的图示表示](Images/dlcf_1201.png)'
- en: Figure 12-1\. Pictorial representation of a simple neural network
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-1。简单神经网络的图示表示
- en: 'Each shape represents activations: rectangle for input, circle for hidden (inner)
    layer activations, and triangle for output activations. We will use those shapes
    (summarized in [Figure 12-2](#img_shapes)) in all the diagrams in this chapter.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 每个形状代表激活：矩形代表输入，圆圈代表隐藏（内部）层激活，三角形代表输出激活。我们将在本章中的所有图表中使用这些形状（在 [图 12-2](#img_shapes)
    中总结）。
- en: '![Shapes used in our pictorial representations](Images/dlcf_1202.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![我们图示表示中使用的形状](Images/dlcf_1202.png)'
- en: Figure 12-2\. Shapes used in our pictorial representations
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-2。我们图示表示中使用的形状
- en: An arrow represents the actual layer computation—i.e., the linear layer followed
    by the activation function. Using this notation, [Figure 12-3](#lm_rep) shows
    what our simple language model looks like.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 箭头代表实际的层计算——即线性层后跟激活函数。使用这种符号，[图 12-3](#lm_rep) 显示了我们简单语言模型的外观。
- en: '![Representation of our basic language model](Images/dlcf_1203.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![我们基本语言模型的表示](Images/dlcf_1203.png)'
- en: Figure 12-3\. Representation of our basic language model
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-3。我们基本语言模型的表示
- en: To simplify things, we’ve removed the details of the layer computation from
    each arrow. We’ve also color-coded the arrows, such that all arrows with the same
    color have the same weight matrix. For instance, all the input layers use the
    same embedding matrix, so they all have the same color (green).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化事情，我们已经从每个箭头中删除了层计算的细节。我们还对箭头进行了颜色编码，使所有具有相同颜色的箭头具有相同的权重矩阵。例如，所有输入层使用相同的嵌入矩阵，因此它们都具有相同的颜色（绿色）。
- en: 'Let’s try training this model and see how it goes:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试训练这个模型，看看效果如何：
- en: '[PRE19]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | accuracy | time |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 1.824297 | 1.970941 | 0.467554 | 00:02 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1.824297 | 1.970941 | 0.467554 | 00:02 |'
- en: '| 1 | 1.386973 | 1.823242 | 0.467554 | 00:02 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1.386973 | 1.823242 | 0.467554 | 00:02 |'
- en: '| 2 | 1.417556 | 1.654497 | 0.494414 | 00:02 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1.417556 | 1.654497 | 0.494414 | 00:02 |'
- en: '| 3 | 1.376440 | 1.650849 | 0.494414 | 00:02 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1.376440 | 1.650849 | 0.494414 | 00:02 |'
- en: 'To see if this is any good, let’s check what a very simple model would give
    us. In this case, we could always predict the most common token, so let’s find
    out which token is most often the target in our validation set:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看这是否有效，请查看一个非常简单的模型会给我们什么结果。在这种情况下，我们总是可以预测最常见的标记，因此让我们找出在我们的验证集中最常见的目标是哪个标记：
- en: '[PRE20]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The most common token has the index 29, which corresponds to the token `thousand`.
    Always predicting this token would give us an accuracy of roughly 15%, so we are
    faring way better!
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的标记的索引是 29，对应于标记 `thousand`。总是预测这个标记将给我们大约 15% 的准确率，所以我们表现得更好！
- en: Alexis Says
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Alexis 说
- en: 'My first guess was that the separator would be the most common token, since
    there is one for every number. But looking at `tokens` reminded me that large
    numbers are written with many words, so on the way to 10,000 you write “thousand”
    a lot: five thousand, five thousand and one, five thousand and two, etc. Oops!
    Looking at your data is great for noticing subtle features as well as embarrassingly
    obvious ones.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我的第一个猜测是分隔符会是最常见的标记，因为每个数字都有一个分隔符。但查看 `tokens` 提醒我，大数字用许多单词写成，所以在通往 10,000 的路上，你会经常写“thousand”：five
    thousand, five thousand and one, five thousand and two 等等。糟糕！查看数据对于注意到微妙特征以及尴尬明显的特征都很有帮助。
- en: This is a nice first baseline. Let’s see how we can refactor it with a loop.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个不错的第一个基线。让我们看看如何用循环重构它。
- en: Our First Recurrent Neural Network
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的第一个循环神经网络
- en: 'Looking at the code for our module, we could simplify it by replacing the duplicated
    code that calls the layers with a `for` loop. In addition to making our code simpler,
    this will have the benefit that we will be able to apply our module equally well
    to token sequences of different lengths—we won’t be restricted to token lists
    of length three:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 查看我们模块的代码，我们可以通过用 `for` 循环替换调用层的重复代码来简化它。除了使我们的代码更简单外，这样做的好处是我们将能够同样适用于不同长度的标记序列——我们不会被限制在长度为三的标记列表上：
- en: '[PRE22]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Let’s check that we get the same results using this refactoring:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下，看看我们使用这种重构是否得到相同的结果：
- en: '[PRE23]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | accuracy | time |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 1.816274 | 1.964143 | 0.460185 | 00:02 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1.816274 | 1.964143 | 0.460185 | 00:02 |'
- en: '| 1 | 1.423805 | 1.739964 | 0.473259 | 00:02 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1.423805 | 1.739964 | 0.473259 | 00:02 |'
- en: '| 2 | 1.430327 | 1.685172 | 0.485382 | 00:02 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1.430327 | 1.685172 | 0.485382 | 00:02 |'
- en: '| 3 | 1.388390 | 1.657033 | 0.470406 | 00:02 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1.388390 | 1.657033 | 0.470406 | 00:02 |'
- en: We can also refactor our pictorial representation in exactly the same way, as
    shown in [Figure 12-4](#basic_rnn) (we’re also removing the details of activation
    sizes here, and using the same arrow colors as in [Figure 12-3](#lm_rep)).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以以完全相同的方式重构我们的图示表示，如[图12-4](#basic_rnn)所示（这里我们也删除了激活大小的细节，并使用与[图12-3](#lm_rep)相同的箭头颜色）。
- en: '![Basic recurrent neural network](Images/dlcf_1204.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![基本循环神经网络](Images/dlcf_1204.png)'
- en: Figure 12-4\. Basic recurrent neural network
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-4\. 基本循环神经网络
- en: You will see that a set of activations is being updated each time through the
    loop, stored in the variable `h`—this is called the *hidden state*.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到一组激活在每次循环中被更新，存储在变量`h`中—这被称为*隐藏状态*。
- en: 'Jargon: Hidden State'
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 术语：隐藏状态
- en: The activations that are updated at each step of a recurrent neural network.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在循环神经网络的每一步中更新的激活。
- en: A neural network that is defined using a loop like this is called a *recurrent
    neural network* (RNN). It is important to realize that an RNN is not a complicated
    new architecture, but simply a refactoring of a multilayer neural network using
    a `for` loop.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这样的循环定义的神经网络称为*循环神经网络*（RNN）。重要的是要意识到RNN并不是一个复杂的新架构，而只是使用`for`循环对多层神经网络进行重构。
- en: Alexis Says
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Alexis说
- en: 'My true opinion: if they were called “looping neural networks,” or LNNs, they
    would seem 50% less daunting!'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我的真实看法：如果它们被称为“循环神经网络”或LNNs，它们看起来会少恐怖50%！
- en: Now that we know what an RNN is, let’s try to make it a little bit better.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了什么是RNN，让我们试着让它变得更好一点。
- en: Improving the RNN
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进RNN
- en: Looking at the code for our RNN, one thing that seems problematic is that we
    are initializing our hidden state to zero for every new input sequence. Why is
    that a problem? We made our sample sequences short so they would fit easily into
    batches. But if we order those samples correctly, the sample sequences will be
    read in order by the model, exposing the model to long stretches of the original
    sequence.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 观察我们的RNN代码，有一个看起来有问题的地方是，我们为每个新的输入序列将隐藏状态初始化为零。为什么这是个问题呢？我们将样本序列设置得很短，以便它们可以轻松地适应批处理。但是，如果我们正确地对这些样本进行排序，模型将按顺序读取样本序列，使模型暴露于原始序列的长时间段。
- en: 'Another thing we can look at is having more signal: why predict only the fourth
    word when we could use the intermediate predictions to also predict the second
    and third words? Let’s see how we can implement those changes, starting with adding
    some state.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以考虑增加更多信号：为什么只预测第四个单词，而不使用中间预测来预测第二和第三个单词呢？让我们看看如何实现这些变化，首先从添加一些状态开始。
- en: Maintaining the State of an RNN
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 维护RNN的状态
- en: Because we initialize the model’s hidden state to zero for each new sample,
    we are throwing away all the information we have about the sentences we have seen
    so far, which means that our model doesn’t actually know where we are up to in
    the overall counting sequence. This is easily fixed; we can simply move the initialization
    of the hidden state to `__init__`.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们为每个新样本将模型的隐藏状态初始化为零，这样我们就丢失了关于迄今为止看到的句子的所有信息，这意味着我们的模型实际上不知道我们在整体计数序列中的进度。这很容易修复；我们只需将隐藏状态的初始化移动到`__init__`中。
- en: But this fix will create its own subtle, but important, problem. It effectively
    makes our neural network as deep as the entire number of tokens in our document.
    For instance, if there were 10,000 tokens in our dataset, we would be creating
    a 10,000-layer neural network.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，这种修复方法将产生自己微妙但重要的问题。它实际上使我们的神经网络变得和文档中的令牌数量一样多。例如，如果我们的数据集中有10,000个令牌，我们将创建一个有10,000层的神经网络。
- en: To see why this is the case, consider the original pictorial representation
    of our recurrent neural network in [Figure 12-3](#lm_rep), before refactoring
    it with a `for` loop. You can see each layer corresponds with one token input.
    When we talk about the representation of a recurrent neural network before refactoring
    with the `for` loop, we call this the *unrolled representation*. It is often helpful
    to consider the unrolled representation when trying to understand an RNN.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解为什么会出现这种情况，请考虑我们循环神经网络的原始图示表示，即在[图12-3](#lm_rep)中，在使用`for`循环重构之前。您可以看到每个层对应一个令牌输入。当我们谈论使用`for`循环重构之前的循环神经网络的表示时，我们称之为*展开表示*。在尝试理解RNN时，考虑展开表示通常是有帮助的。
- en: The problem with a 10,000-layer neural network is that if and when you get to
    the 10,000th word of the dataset, you will still need to calculate the derivatives
    all the way back to the first layer. This is going to be slow indeed, and memory-intensive.
    It is unlikely that you’ll be able to store even one mini-batch on your GPU.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 10,000层神经网络的问题在于，当您到达数据集的第10,000个单词时，您仍然需要计算直到第一层的所有导数。这将非常缓慢，且占用内存。您可能无法在GPU上存储一个小批量。
- en: The solution to this problem is to tell PyTorch that we do not want to backpropagate
    the derivatives through the entire implicit neural network. Instead, we will keep
    just the last three layers of gradients. To remove all of the gradient history
    in PyTorch, we use the `detach` method.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的方法是告诉PyTorch我们不希望通过整个隐式神经网络反向传播导数。相反，我们将保留梯度的最后三层。为了在PyTorch中删除所有梯度历史，我们使用`detach`方法。
- en: 'Here is the new version of our RNN. It is now stateful, because it remembers
    its activations between different calls to `forward`, which represent its use
    for different samples in the batch:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们RNN的新版本。现在它是有状态的，因为它在不同调用`forward`时记住了其激活，这代表了它在批处理中用于不同样本的情况：
- en: '[PRE24]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This model will have the same activations whatever sequence length we pick,
    because the hidden state will remember the last activation from the previous batch.
    The only thing that will be different is the gradients computed at each step:
    they will be calculated on only sequence length tokens in the past, instead of
    the whole stream. This approach is called *backpropagation through time* (BPTT).'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们选择什么序列长度，这个模型将具有相同的激活，因为隐藏状态将记住上一批次的最后激活。唯一不同的是在每一步计算的梯度：它们将仅在过去的序列长度标记上计算，而不是整个流。这种方法称为*时间穿梭反向传播*（BPTT）。
- en: 'Jargon: Backpropagation Through Time'
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 术语：时间穿梭反向传播
- en: Treating a neural net with effectively one layer per time step (usually refactored
    using a loop) as one big model, and calculating gradients on it in the usual way.
    To avoid running out of memory and time, we usually use *truncated* BPTT, which
    “detaches” the history of computation steps in the hidden state every few time
    steps.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 将一个神经网络有效地视为每个时间步长一个层（通常使用循环重构），并以通常的方式在其上计算梯度。为了避免内存和时间不足，我们通常使用*截断* BPTT，每隔几个时间步“分离”隐藏状态的计算历史。
- en: To use `LMModel3`, we need to make sure the samples are going to be seen in
    a certain order. As we saw in [Chapter 10](ch10.xhtml#chapter_nlp), if the first
    line of the first batch is our `dset[0]`, the second batch should have `dset[1]`
    as the first line, so that the model sees the text flowing.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`LMModel3`，我们需要确保样本按照一定顺序进行查看。正如我们在[第10章](ch10.xhtml#chapter_nlp)中看到的，如果第一批的第一行是我们的`dset[0]`，那么第二批应该将`dset[1]`作为第一行，以便模型看到文本流动。
- en: '`LMDataLoader` was doing this for us in [Chapter 10](ch10.xhtml#chapter_nlp).
    This time we’re going to do it ourselves.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`LMDataLoader`在[第10章](ch10.xhtml#chapter_nlp)中为我们做到了这一点。这次我们要自己做。'
- en: 'To do this, we are going to rearrange our dataset. First we divide the samples
    into `m = len(dset) // bs` groups (this is the equivalent of splitting the whole
    concatenated dataset into, for example, 64 equally sized pieces, since we’re using
    `bs=64` here). `m` is the length of each of these pieces. For instance, if we’re
    using our whole dataset (although we’ll actually split it into train versus valid
    in a moment), we have this:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将重新排列我们的数据集。首先，我们将样本分成`m = len(dset) // bs`组（这相当于将整个连接数据集分成，例如，64个大小相等的部分，因为我们在这里使用`bs=64`）。`m`是每个这些部分的长度。例如，如果我们使用整个数据集（尽管我们实际上将在一会儿将其分成训练和验证），我们有：
- en: '[PRE25]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The first batch will be composed of the samples
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 第一批将由样本组成
- en: '[PRE27]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: the second batch of the samples
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 样本的第二批
- en: '[PRE28]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: and so forth. This way, at each epoch, the model will see a chunk of contiguous
    text of size `3*m` (since each text is of size 3) on each line of the batch.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 等等。这样，每个时期，模型将在每批次的每行上看到大小为`3*m`的连续文本块（因为每个文本的大小为3）。
- en: 'The following function does that reindexing:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数执行重新索引：
- en: '[PRE29]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then we just pass `drop_last=True` when building our `DataLoaders` to drop
    the last batch that does not have a shape of `bs`. We also pass `shuffle=False`
    to make sure the texts are read in order:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在构建`DataLoaders`时只需传递`drop_last=True`来删除最后一个形状不为`bs`的批次。我们还传递`shuffle=False`以确保文本按顺序阅读：
- en: '[PRE30]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The last thing we add is a little tweak of the training loop via a `Callback`.
    We will talk more about callbacks in [Chapter 16](ch16.xhtml#chapter_accel_sgd);
    this one will call the `reset` method of our model at the beginning of each epoch
    and before each validation phase. Since we implemented that method to set the
    hidden state of the model to zero, this will make sure we start with a clean state
    before reading those continuous chunks of text. We can also start training a bit
    longer:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们添加的最后一件事是通过`Callback`对训练循环进行微调。我们将在[第16章](ch16.xhtml#chapter_accel_sgd)中更多地讨论回调；这个回调将在每个时期的开始和每个验证阶段之前调用我们模型的`reset`方法。由于我们实现了该方法来将模型的隐藏状态设置为零，这将确保我们在阅读这些连续文本块之前以干净的状态开始。我们也可以开始训练更长一点：
- en: '[PRE31]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | accuracy | time |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 1.677074 | 1.827367 | 0.467548 | 00:02 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1.677074 | 1.827367 | 0.467548 | 00:02 |'
- en: '| 1 | 1.282722 | 1.870913 | 0.388942 | 00:02 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1.282722 | 1.870913 | 0.388942 | 00:02 |'
- en: '| 2 | 1.090705 | 1.651793 | 0.462500 | 00:02 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1.090705 | 1.651793 | 0.462500 | 00:02 |'
- en: '| 3 | 1.005092 | 1.613794 | 0.516587 | 00:02 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1.005092 | 1.613794 | 0.516587 | 00:02 |'
- en: '| 4 | 0.965975 | 1.560775 | 0.551202 | 00:02 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.965975 | 1.560775 | 0.551202 | 00:02 |'
- en: '| 5 | 0.916182 | 1.595857 | 0.560577 | 00:02 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.916182 | 1.595857 | 0.560577 | 00:02 |'
- en: '| 6 | 0.897657 | 1.539733 | 0.574279 | 00:02 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 0.897657 | 1.539733 | 0.574279 | 00:02 |'
- en: '| 7 | 0.836274 | 1.585141 | 0.583173 | 00:02 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 0.836274 | 1.585141 | 0.583173 | 00:02 |'
- en: '| 8 | 0.805877 | 1.629808 | 0.586779 | 00:02 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 0.805877 | 1.629808 | 0.586779 | 00:02 |'
- en: '| 9 | 0.795096 | 1.651267 | 0.588942 | 00:02 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 0.795096 | 1.651267 | 0.588942 | 00:02 |'
- en: This is already better! The next step is to use more targets and compare them
    to the intermediate predictions.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这已经更好了！下一步是使用更多目标并将它们与中间预测进行比较。
- en: Creating More Signal
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建更多信号
- en: Another problem with our current approach is that we predict only one output
    word for each three input words. As a result, the amount of signal that we are
    feeding back to update weights with is not as large as it could be. It would be
    better if we predicted the next word after every single word, rather than every
    three words, as shown in [Figure 12-5](#stateful_rep).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当前方法的另一个问题是，我们仅为每三个输入单词预测一个输出单词。因此，我们反馈以更新权重的信号量不如可能的那么大。如果我们在每个单词后预测下一个单词，而不是每三个单词，将会更好，如[图12-5](#stateful_rep)所示。
- en: '![RNN predicting after every token](Images/dlcf_1205.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![RNN在每个标记后进行预测](Images/dlcf_1205.png)'
- en: Figure 12-5\. RNN predicting after every token
  id: totrans-140
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-5。RNN在每个标记后进行预测
- en: 'This is easy enough to add. We need to first change our data so that the dependent
    variable has each of the three next words after each of our three input words.
    Instead of `3`, we use an attribute, `sl` (for sequence length), and make it a
    bit bigger:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这很容易添加。我们需要首先改变我们的数据，使得因变量在每个三个输入词后的每个三个词中都有。我们使用一个属性`sl`（用于序列长度），并使其稍微变大：
- en: '[PRE32]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Looking at the first element of `seqs`, we can see that it contains two lists
    of the same size. The second list is the same as the first, but offset by one
    element:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 查看`seqs`的第一个元素，我们可以看到它包含两个相同大小的列表。第二个列表与第一个相同，但偏移了一个元素：
- en: '[PRE33]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now we need to modify our model so that it outputs a prediction after every
    word, rather than just at the end of a three-word sequence:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要修改我们的模型，使其在每个单词之后输出一个预测，而不仅仅是在一个三个词序列的末尾：
- en: '[PRE35]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This model will return outputs of shape `bs x sl x vocab_sz` (since we stacked
    on `dim=1`). Our targets are of shape `bs x sl`, so we need to flatten those before
    using them in `F.cross_entropy`:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型将返回形状为`bs x sl x vocab_sz`的输出（因为我们在`dim=1`上堆叠）。我们的目标的形状是`bs x sl`，所以在使用`F.cross_entropy`之前，我们需要将它们展平：
- en: '[PRE36]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We can now use this loss function to train the model:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用这个损失函数来训练模型：
- en: '[PRE37]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | accuracy | time |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 3.103298 | 2.874341 | 0.212565 | 00:01 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 3.103298 | 2.874341 | 0.212565 | 00:01 |'
- en: '| 1 | 2.231964 | 1.971280 | 0.462158 | 00:01 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2.231964 | 1.971280 | 0.462158 | 00:01 |'
- en: '| 2 | 1.711358 | 1.813547 | 0.461182 | 00:01 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1.711358 | 1.813547 | 0.461182 | 00:01 |'
- en: '| 3 | 1.448516 | 1.828176 | 0.483236 | 00:01 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1.448516 | 1.828176 | 0.483236 | 00:01 |'
- en: '| 4 | 1.288630 | 1.659564 | 0.520671 | 00:01 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 1.288630 | 1.659564 | 0.520671 | 00:01 |'
- en: '| 5 | 1.161470 | 1.714023 | 0.554932 | 00:01 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 1.161470 | 1.714023 | 0.554932 | 00:01 |'
- en: '| 6 | 1.055568 | 1.660916 | 0.575033 | 00:01 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 1.055568 | 1.660916 | 0.575033 | 00:01 |'
- en: '| 7 | 0.960765 | 1.719624 | 0.591064 | 00:01 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 0.960765 | 1.719624 | 0.591064 | 00:01 |'
- en: '| 8 | 0.870153 | 1.839560 | 0.614665 | 00:01 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 0.870153 | 1.839560 | 0.614665 | 00:01 |'
- en: '| 9 | 0.808545 | 1.770278 | 0.624349 | 00:01 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 0.808545 | 1.770278 | 0.624349 | 00:01 |'
- en: '| 10 | 0.758084 | 1.842931 | 0.610758 | 00:01 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 0.758084 | 1.842931 | 0.610758 | 00:01 |'
- en: '| 11 | 0.719320 | 1.799527 | 0.646566 | 00:01 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 0.719320 | 1.799527 | 0.646566 | 00:01 |'
- en: '| 12 | 0.683439 | 1.917928 | 0.649821 | 00:01 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 0.683439 | 1.917928 | 0.649821 | 00:01 |'
- en: '| 13 | 0.660283 | 1.874712 | 0.628581 | 00:01 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 13 | 0.660283 | 1.874712 | 0.628581 | 00:01 |'
- en: '| 14 | 0.646154 | 1.877519 | 0.640055 | 00:01 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 14 | 0.646154 | 1.877519 | 0.640055 | 00:01 |'
- en: We need to train for longer, since the task has changed a bit and is more complicated
    now. But we end up with a good result…at least, sometimes. If you run it a few
    times, you’ll see that you can get quite different results on different runs.
    That’s because effectively we have a very deep network here, which can result
    in very large or very small gradients. We’ll see in the next part of this chapter
    how to deal with this.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要训练更长时间，因为任务有点变化，现在更加复杂。但我们最终得到了一个好结果...至少有时候是这样。如果你多次运行它，你会发现在不同的运行中可以得到非常不同的结果。这是因为实际上我们在这里有一个非常深的网络，这可能导致非常大或非常小的梯度。我们将在本章的下一部分看到如何处理这个问题。
- en: 'Now, the obvious way to get a better model is to go deeper: we have only one
    linear layer between the hidden state and the output activations in our basic
    RNN, so maybe we’ll get better results with more.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，获得更好模型的明显方法是加深：在我们基本的RNN中，隐藏状态和输出激活之间只有一个线性层，所以也许我们用更多的线性层会得到更好的结果。
- en: Multilayer RNNs
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层RNNs
- en: In a multilayer RNN, we pass the activations from our recurrent neural network
    into a second recurrent neural network, as in [Figure 12-6](#stacked_rnn_rep).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在多层RNN中，我们将来自我们递归神经网络的激活传递到第二个递归神经网络中，就像[图12-6](#stacked_rnn_rep)中所示。
- en: '![2-layer RNN](Images/dlcf_1206.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![2层RNN](Images/dlcf_1206.png)'
- en: Figure 12-6\. 2-layer RNN
  id: totrans-174
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-6\. 2层RNN
- en: The unrolled representation is shown in [Figure 12-7](#unrolled_stack_rep) (similar
    to [Figure 12-3](#lm_rep)).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 展开的表示在[图12-7](#unrolled_stack_rep)中显示（类似于[图12-3](#lm_rep)）。
- en: '![2-layer unrolled RNN](Images/dlcf_1207.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![2层展开的RNN](Images/dlcf_1207.png)'
- en: Figure 12-7\. 2-layer unrolled RNN
  id: totrans-177
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-7\. 2层展开的RNN
- en: Let’s see how to implement this in practice.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在实践中实现这一点。
- en: The Model
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型
- en: 'We can save some time by using PyTorch’s `RNN` class, which implements exactly
    what we created earlier, but also gives us the option to stack multiple RNNs,
    as we have discussed:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用PyTorch的`RNN`类来节省一些时间，该类实现了我们之前创建的内容，但也给了我们堆叠多个RNN的选项，正如我们之前讨论的那样：
- en: '[PRE38]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | accuracy | time |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 3.055853 | 2.591640 | 0.437907 | 00:01 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 3.055853 | 2.591640 | 0.437907 | 00:01 |'
- en: '| 1 | 2.162359 | 1.787310 | 0.471598 | 00:01 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2.162359 | 1.787310 | 0.471598 | 00:01 |'
- en: '| 2 | 1.710663 | 1.941807 | 0.321777 | 00:01 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1.710663 | 1.941807 | 0.321777 | 00:01 |'
- en: '| 3 | 1.520783 | 1.999726 | 0.312012 | 00:01 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1.520783 | 1.999726 | 0.312012 | 00:01 |'
- en: '| 4 | 1.330846 | 2.012902 | 0.413249 | 00:01 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 1.330846 | 2.012902 | 0.413249 | 00:01 |'
- en: '| 5 | 1.163297 | 1.896192 | 0.450684 | 00:01 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 1.163297 | 1.896192 | 0.450684 | 00:01 |'
- en: '| 6 | 1.033813 | 2.005209 | 0.434814 | 00:01 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 1.033813 | 2.005209 | 0.434814 | 00:01 |'
- en: '| 7 | 0.919090 | 2.047083 | 0.456706 | 00:01 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 0.919090 | 2.047083 | 0.456706 | 00:01 |'
- en: '| 8 | 0.822939 | 2.068031 | 0.468831 | 00:01 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 0.822939 | 2.068031 | 0.468831 | 00:01 |'
- en: '| 9 | 0.750180 | 2.136064 | 0.475098 | 00:01 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 0.750180 | 2.136064 | 0.475098 | 00:01 |'
- en: '| 10 | 0.695120 | 2.139140 | 0.485433 | 00:01 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 0.695120 | 2.139140 | 0.485433 | 00:01 |'
- en: '| 11 | 0.655752 | 2.155081 | 0.493652 | 00:01 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 0.655752 | 2.155081 | 0.493652 | 00:01 |'
- en: '| 12 | 0.629650 | 2.162583 | 0.498535 | 00:01 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 0.629650 | 2.162583 | 0.498535 | 00:01 |'
- en: '| 13 | 0.613583 | 2.171649 | 0.491048 | 00:01 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 13 | 0.613583 | 2.171649 | 0.491048 | 00:01 |'
- en: '| 14 | 0.604309 | 2.180355 | 0.487874 | 00:01 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 14 | 0.604309 | 2.180355 | 0.487874 | 00:01 |'
- en: Now that’s disappointing…our previous single-layer RNN performed better. Why?
    The reason is that we have a deeper model, leading to exploding or vanishing activations.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这令人失望...我们之前的单层RNN表现更好。为什么？原因是我们有一个更深的模型，导致激活爆炸或消失。
- en: Exploding or Disappearing Activations
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活爆炸或消失
- en: In practice, creating accurate models from this kind of RNN is difficult. We
    will get better results if we call `detach` less often, and have more layers—this
    gives our RNN a longer time horizon to learn from and richer features to create.
    But it also means we have a deeper model to train. The key challenge in the development
    of deep learning has been figuring out how to train these kinds of models.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'This is challenging because of what happens when you multiply by a matrix many
    times. Think about what happens when you multiply by a number many times. For
    example, if you multiply by 2, starting at 1, you get the sequence 1, 2, 4, 8,…and
    after 32 steps, you are already at 4,294,967,296\. A similar issue happens if
    you multiply by 0.5: you get 0.5, 0.25, 0.125…and after 32 steps, it’s 0.00000000023\.
    As you can see, multiplying by a number even slightly higher or lower than 1 results
    in an explosion or disappearance of our starting number, after just a few repeated
    multiplications.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Because matrix multiplication is just multiplying numbers and adding them up,
    exactly the same thing happens with repeated matrix multiplications. And that’s
    all a deep neural network is—each extra layer is another matrix multiplication.
    This means that it is very easy for a deep neural network to end up with extremely
    large or extremely small numbers.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: This is a problem, because the way computers store numbers (known as *floating
    point*) means that they become less and less accurate the further away the numbers
    get from zero. The diagram in [Figure 12-8](#float_prec), from the excellent article
    [“What You Never Wanted to Know about Floating Point but Will Be Forced to Find
    Out”](https://oreil.ly/c_kG9), shows how the precision of floating-point numbers
    varies over the number line.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '![Precision of floating-point numbers](Images/dlcf_1208.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
- en: Figure 12-8\. Precision of floating-point numbers
  id: totrans-207
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This inaccuracy means that often the gradients calculated for updating the weights
    end up as zero or infinity for deep networks. This is commonly referred to as
    the *vanishing gradients* or *exploding gradients* problem. It means that in SGD,
    the weights are either not updated at all or jump to infinity. Either way, they
    won’t improve with training.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Researchers have developed ways to tackle this problem, which we will be discussing
    later in the book. One option is to change the definition of a layer in a way
    that makes it less likely to have exploding activations. We’ll look at the details
    of how this is done in [Chapter 13](ch13.xhtml#chapter_convolutions), when we
    discuss batch normalization, and [Chapter 14](ch14.xhtml#chapter_resnet), when
    we discuss ResNets, although these details don’t generally matter in practice
    (unless you are a researcher who is creating new approaches to solving this problem).
    Another strategy for dealing with this is by being careful about initialization,
    which is a topic we’ll investigate in [Chapter 17](ch17.xhtml#chapter_foundations).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'For RNNs, two types of layers are frequently used to avoid exploding activations:
    *gated recurrent units* (GRUs) and *long short-term memory* (LSTM) layers. Both
    of these are available in PyTorch and are drop-in replacements for the RNN layer.
    We will cover only LSTMs in this book; plenty of good tutorials online explain
    GRUs, which are a minor variant on the LSTM design.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: LSTM
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LSTM is an architecture that was introduced back in 1997 by Jürgen Schmidhuber
    and Sepp Hochreiter. In this architecture, there are not one, but two, hidden
    states. In our base RNN, the hidden state is the output of the RNN at the previous
    time step. That hidden state is then responsible for two things:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Having the right information for the output layer to predict the correct next
    token
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retaining memory of everything that happened in the sentence
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider, for example, the sentences “Henry has a dog and he likes his dog very
    much” and “Sophie has a dog and she likes her dog very much.” It’s very clear
    that the RNN needs to remember the name at the beginning of the sentence to be
    able to predict *he/she* or *his/her*.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑句子“Henry has a dog and he likes his dog very much”和“Sophie has a dog and
    she likes her dog very much。”很明显，RNN需要记住句子开头的名字才能预测*he/she*或*his/her*。
- en: In practice, RNNs are really bad at retaining memory of what happened much earlier
    in the sentence, which is the motivation to have another hidden state (called
    *cell state*) in the LSTM. The cell state will be responsible for keeping *long
    short-term memory*, while the hidden state will focus on the next token to predict.
    Let’s take a closer look at how this is achieved and build an LSTM from scratch.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，RNN在保留句子中较早发生的记忆方面表现非常糟糕，这就是在LSTM中有另一个隐藏状态（称为*cell state*）的动机。cell state将负责保持*长期短期记忆*，而隐藏状态将专注于预测下一个标记。让我们更仔细地看看如何实现这一点，并从头开始构建一个LSTM。
- en: Building an LSTM from Scratch
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从头开始构建一个LSTM
- en: In order to build an LSTM, we first have to understand its architecture. [Figure 12-9](#lstm)
    shows its inner structure.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建一个LSTM，我们首先必须了解其架构。[图12-9](#lstm)显示了其内部结构。
- en: '![A graph showing the inner architecture of an LSTM](Images/dlcf_1209.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![显示LSTM内部架构的图表](Images/dlcf_1209.png)'
- en: Figure 12-9\. Architecture of an LSTM
  id: totrans-220
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-9\. LSTM的架构
- en: 'In this picture, our input <math alttext="x Subscript t"><msub><mi>x</mi> <mi>t</mi></msub></math>
    enters on the left with the previous hidden state ( <math alttext="h Subscript
    t minus 1"><msub><mi>h</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>
    ) and cell state ( <math alttext="c Subscript t minus 1"><msub><mi>c</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>
    ). The four orange boxes represent four layers (our neural nets), with the activation
    being either sigmoid ( <math alttext="sigma"><mi>σ</mi></math> ) or tanh. tanh
    is just a sigmoid function rescaled to the range –1 to 1\. Its mathematical expression
    can be written like this:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在这张图片中，我们的输入<math alttext="x Subscript t"><msub><mi>x</mi> <mi>t</mi></msub></math>从左侧进入，带有先前的隐藏状态（<math
    alttext="h Subscript t minus 1"><msub><mi>h</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>）和cell
    state（<math alttext="c Subscript t minus 1"><msub><mi>c</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>）。四个橙色框代表四个层（我们的神经网络），激活函数可以是sigmoid（<math
    alttext="sigma"><mi>σ</mi></math>）或tanh。tanh只是一个重新缩放到范围-1到1的sigmoid函数。它的数学表达式可以写成这样：
- en: <math alttext="hyperbolic tangent left-parenthesis x right-parenthesis equals
    StartFraction e Superscript x Baseline plus e Superscript negative x Baseline
    Over e Superscript x Baseline minus e Superscript negative x Baseline EndFraction
    equals 2 sigma left-parenthesis 2 x right-parenthesis minus 1" display="block"><mrow><mo
    form="prefix">tanh</mo> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mfrac><mrow><msup><mi>e</mi> <mi>x</mi></msup> <mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mi>x</mi></mrow></msup></mrow>
    <mrow><msup><mi>e</mi> <mi>x</mi></msup> <mo>-</mo><msup><mi>e</mi> <mrow><mo>-</mo><mi>x</mi></mrow></msup></mrow></mfrac>
    <mo>=</mo> <mn>2</mn> <mi>σ</mi> <mrow><mo>(</mo> <mn>2</mn> <mi>x</mi> <mo>)</mo></mrow>
    <mo>-</mo> <mn>1</mn></mrow></math>
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="双曲正切左括号 x 右括号等于开始分数 e 上标 x 基线加 e 上标 负 x 基线 除以 e 上标 x 基线减 e 上标
    负 x 基线 等于 2 sigma 左括号 2 x 右括号 减 1" display="block"><mrow><mo form="prefix">tanh</mo>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><msup><mi>e</mi>
    <mi>x</mi></msup> <mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mi>x</mi></mrow></msup></mrow>
    <mrow><msup><mi>e</mi> <mi>x</mi></msup> <mo>-</mo><msup><mi>e</mi> <mrow><mo>-</mo><mi>x</mi></mrow></msup></mrow></mfrac>
    <mo>=</mo> <mn>2</mn> <mi>σ</mi> <mrow><mo>(</mo> <mn>2</mn> <mi>x</mi> <mo>)</mo></mrow>
    <mo>-</mo> <mn>1</mn></mrow></math>
- en: where <math alttext="sigma"><mi>σ</mi></math> is the sigmoid function. The green
    circles in the figure are elementwise operations. What goes out on the right is
    the new hidden state ( <math alttext="h Subscript t"><msub><mi>h</mi> <mi>t</mi></msub></math>
    ) and new cell state ( <math alttext="c Subscript t"><msub><mi>c</mi> <mi>t</mi></msub></math>
    ), ready for our next input. The new hidden state is also used as output, which
    is why the arrow splits to go up.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math alttext="sigma"><mi>σ</mi></math>是sigmoid函数。图中的绿色圆圈是逐元素操作。右侧输出的是新的隐藏状态（<math
    alttext="h Subscript t"><msub><mi>h</mi> <mi>t</mi></msub></math>）和新的cell state（<math
    alttext="c Subscript t"><msub><mi>c</mi> <mi>t</mi></msub></math>），准备接受我们的下一个输入。新的隐藏状态也被用作输出，这就是为什么箭头分开向上移动。
- en: Let’s go over the four neural nets (called *gates*) one by one and explain the
    diagram—but before this, notice how very little the cell state (at the top) is
    changed. It doesn’t even go directly through a neural net! This is exactly why
    it will carry on a longer-term state.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一查看四个神经网络（称为*门*）并解释图表——但在此之前，请注意cell state（顶部）几乎没有改变。它甚至没有直接通过神经网络！这正是为什么它将继续保持较长期的状态。
- en: First, the arrows for input and old hidden state are joined together. In the
    RNN we wrote earlier in this chapter, we were adding them together. In the LSTM,
    we stack them in one big tensor. This means the dimension of our embeddings (which
    is the dimension of <math alttext="x Subscript t"><msub><mi>x</mi> <mi>t</mi></msub></math>
    ) can be different from the dimension of our hidden state. If we call those `n_in`
    and `n_hid`, the arrow at the bottom is of size `n_in + n_hid`; thus all the neural
    nets (orange boxes) are linear layers with `n_in + n_hid` inputs and `n_hid` outputs.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，将输入和旧隐藏状态的箭头连接在一起。在本章前面编写的RNN中，我们将它们相加。在LSTM中，我们将它们堆叠在一个大张量中。这意味着我们的嵌入的维度（即<math
    alttext="x Subscript t"><msub><mi>x</mi> <mi>t</mi></msub></math>的维度）可以与隐藏状态的维度不同。如果我们将它们称为`n_in`和`n_hid`，底部的箭头大小为`n_in
    + n_hid`；因此所有的神经网络（橙色框）都是具有`n_in + n_hid`输入和`n_hid`输出的线性层。
- en: 'The first gate (looking from left to right) is called the *forget gate*. Since
    it’s a linear layer followed by a sigmoid, its output will consist of scalars
    between 0 and 1\. We multiply this result by the cell state to determine which
    information to keep and which to throw away: values closer to 0 are discarded,
    and values closer to 1 are kept. This gives the LSTM the ability to forget things
    about its long-term state. For instance, when crossing a period or an `xxbos`
    token, we would expect it to (have learned to) reset its cell state.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个门（从左到右看）称为*遗忘门*。由于它是一个线性层后面跟着一个sigmoid，它的输出将由0到1之间的标量组成。我们将这个结果乘以细胞状态，以确定要保留哪些信息，要丢弃哪些信息：接近0的值被丢弃，接近1的值被保留。这使得LSTM有能力忘记关于其长期状态的事情。例如，当穿过一个句号或一个`xxbos`标记时，我们期望它（已经学会）重置其细胞状态。
- en: The second gate is called the *input gate*. It works with the third gate (which
    doesn’t really have a name but is sometimes called the *cell gate*) to update
    the cell state. For instance, we may see a new gender pronoun, in which case we’ll
    need to replace the information about gender that the forget gate removed. Similar
    to the forget gate, the input gate decides which elements of the cell state to
    update (values close to 1) or not (values close to 0). The third gate determines
    what those updated values are, in the range of –1 to 1 (thanks to the tanh function).
    The result is added to the cell state.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个门称为*输入门*。它与第三个门（没有真正的名称，但有时被称为*细胞门*）一起更新细胞状态。例如，我们可能看到一个新的性别代词，这时我们需要替换遗忘门删除的关于性别的信息。与遗忘门类似，输入门决定要更新的细胞状态元素（接近1的值）或不更新（接近0的值）。第三个门确定这些更新值是什么，范围在-1到1之间（由于tanh函数）。结果被添加到细胞状态中。
- en: 'The last gate is the *output gate*. It determines which information from the
    cell state to use to generate the output. The cell state goes through a tanh before
    being combined with the sigmoid output from the output gate, and the result is
    the new hidden state. In terms of code, we can write the same steps like this:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个门是*输出门*。它确定从细胞状态中使用哪些信息来生成输出。细胞状态经过tanh后与输出门的sigmoid输出结合，结果就是新的隐藏状态。在代码方面，我们可以这样写相同的步骤：
- en: '[PRE40]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'In practice, we can then refactor the code. Also, in terms of performance,
    it’s better to do one big matrix multiplication than four smaller ones (that’s
    because we launch the special fast kernel on the GPU only once, and it gives the
    GPU more work to do in parallel). The stacking takes a bit of time (since we have
    to move one of the tensors around on the GPU to have it all in a contiguous array),
    so we use two separate layers for the input and the hidden state. The optimized
    and refactored code then looks like this:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们可以重构代码。此外，就性能而言，做一次大矩阵乘法比做四次小矩阵乘法更好（因为我们只在GPU上启动一次特殊的快速内核，这样可以让GPU并行处理更多工作）。堆叠需要一点时间（因为我们必须在GPU上移动一个张量，使其全部在一个连续的数组中），所以我们为输入和隐藏状态使用两个单独的层。优化和重构后的代码如下：
- en: '[PRE41]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Here we use the PyTorch `chunk` method to split our tensor into four pieces.
    It works like this:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用PyTorch的`chunk`方法将张量分成四部分。它的工作原理如下：
- en: '[PRE42]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Let’s now use this architecture to train a language model!
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用这个架构来训练一个语言模型！
- en: Training a Language Model Using LSTMs
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用LSTMs训练语言模型
- en: 'Here is the same network as `LMModel5`, using a two-layer LSTM. We can train
    it at a higher learning rate, for a shorter time, and get better accuracy:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这是与`LMModel5`相同的网络，使用了两层LSTM。我们可以以更高的学习率进行训练，时间更短，获得更好的准确性：
- en: '[PRE46]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | accuracy | time |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 3.000821 | 2.663942 | 0.438314 | 00:02 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 3.000821 | 2.663942 | 0.438314 | 00:02 |'
- en: '| 1 | 2.139642 | 2.184780 | 0.240479 | 00:02 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2.139642 | 2.184780 | 0.240479 | 00:02 |'
- en: '| 2 | 1.607275 | 1.812682 | 0.439779 | 00:02 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1.607275 | 1.812682 | 0.439779 | 00:02 |'
- en: '| 3 | 1.347711 | 1.830982 | 0.497477 | 00:02 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1.347711 | 1.830982 | 0.497477 | 00:02 |'
- en: '| 4 | 1.123113 | 1.937766 | 0.594401 | 00:02 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 1.123113 | 1.937766 | 0.594401 | 00:02 |'
- en: '| 5 | 0.852042 | 2.012127 | 0.631592 | 00:02 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.852042 | 2.012127 | 0.631592 | 00:02 |'
- en: '| 6 | 0.565494 | 1.312742 | 0.725749 | 00:02 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 0.565494 | 1.312742 | 0.725749 | 00:02 |'
- en: '| 7 | 0.347445 | 1.297934 | 0.711263 | 00:02 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 0.347445 | 1.297934 | 0.711263 | 00:02 |'
- en: '| 8 | 0.208191 | 1.441269 | 0.731201 | 00:02 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 0.208191 | 1.441269 | 0.731201 | 00:02 |'
- en: '| 9 | 0.126335 | 1.569952 | 0.737305 | 00:02 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 0.126335 | 1.569952 | 0.737305 | 00:02 |'
- en: '| 10 | 0.079761 | 1.427187 | 0.754150 | 00:02 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 0.079761 | 1.427187 | 0.754150 | 00:02 |'
- en: '| 11 | 0.052990 | 1.494990 | 0.745117 | 00:02 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 0.052990 | 1.494990 | 0.745117 | 00:02 |'
- en: '| 12 | 0.039008 | 1.393731 | 0.757894 | 00:02 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 0.039008 | 1.393731 | 0.757894 | 00:02 |'
- en: '| 13 | 0.031502 | 1.373210 | 0.758464 | 00:02 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 13 | 0.031502 | 1.373210 | 0.758464 | 00:02 |'
- en: '| 14 | 0.028068 | 1.368083 | 0.758464 | 00:02 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 14 | 0.028068 | 1.368083 | 0.758464 | 00:02 |'
- en: Now that’s better than a multilayer RNN! We can still see there is a bit of
    overfitting, however, which is a sign that a bit of regularization might help.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这比多层RNN好多了！然而，我们仍然可以看到有一点过拟合，这表明一点正则化可能会有所帮助。
- en: Regularizing an LSTM
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化LSTM
- en: Recurrent neural networks, in general, are hard to train, because of the problem
    of vanishing activations and gradients we saw before. Using LSTM (or GRU) cells
    makes training easier than with vanilla RNNs, but they are still very prone to
    overfitting. Data augmentation, while a possibility, is less often used for text
    data than for images because in most cases it requires another model to generate
    random augmentations (e.g., by translating the text into another language and
    then back into the original language). Overall, data augmentation for text data
    is currently not a well-explored space.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络总体上很难训练，因为我们之前看到的激活和梯度消失问题。使用LSTM（或GRU）单元比使用普通RNN更容易训练，但它们仍然很容易过拟合。数据增强虽然是一种可能性，但在文本数据中使用得比图像数据少，因为在大多数情况下，它需要另一个模型来生成随机增强（例如，将文本翻译成另一种语言，然后再翻译回原始语言）。总的来说，目前文本数据的数据增强并不是一个被充分探索的领域。
- en: However, we can use other regularization techniques instead to reduce overfitting,
    which were thoroughly studied for use with LSTMs in the paper [“Regularizing and
    Optimizing LSTM Language Models”](https://oreil.ly/Rf-OG) by Stephen Merity et
    al. This paper showed how effective use of dropout, activation regularization,
    and temporal activation regularization could allow an LSTM to beat state-of-the-art
    results that previously required much more complicated models. The authors called
    an LSTM using these techniques an *AWD-LSTM*. We’ll look at each of these techniques
    in turn.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Dropout
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Dropout* is a regularization technique that was introduced by Geoffrey Hinton
    et al. in [“Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors”](https://oreil.ly/-_xie).
    The basic idea is to randomly change some activations to zero at training time.
    This makes sure all neurons actively work toward the output, as seen in [Figure 12-10](#img_dropout)
    (from [“Dropout: A Simple Way to Prevent Neural Networks from Overfitting”](https://oreil.ly/pYNxF)
    by Nitish Srivastava et al.).'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '![A figure from the article showing how neurons go off with dropout](Images/dlcf_1210.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
- en: Figure 12-10\. Applying dropout in a neural network (courtesy of Nitish Srivastava
    et al.)
  id: totrans-266
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Hinton used a nice metaphor when he explained, in an interview, the inspiration
    for dropout:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: I went to my bank. The tellers kept changing, and I asked one of them why. He
    said he didn’t know but they got moved around a lot. I figured it must be because
    it would require cooperation between employees to successfully defraud the bank.
    This made me realize that randomly removing a different subset of neurons on each
    example would prevent conspiracies and thus reduce overfitting.
  id: totrans-268
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In the same interview, he also explained that neuroscience provided additional
    inspiration:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: We don’t really know why neurons spike. One theory is that they want to be noisy
    so as to regularize, because we have many more parameters than we have data points.
    The idea of dropout is that if you have noisy activations, you can afford to use
    a much bigger model.
  id: totrans-270
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'This explains the idea behind why dropout helps to generalize: first it helps
    the neurons to cooperate better together; then it makes the activations more noisy,
    thus making the model more robust.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see, however, that if we were to just zero those activations without
    doing anything else, our model would have problems training: if we go from the
    sum of five activations (that are all positive numbers since we apply a ReLU)
    to just two, this won’t have the same scale. Therefore, if we apply dropout with
    a probability `p`, we rescale all activations by dividing them by `1-p` (on average
    `p` will be zeroed, so it leaves `1-p`), as shown in [Figure 12-11](#img_dropout1).'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '![A figure from the article introducing dropout showing how a neuron is on/off](Images/dlcf_1211.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
- en: Figure 12-11\. Why we scale the activations when applying dropout (courtesy
    of Nitish Srivastava et al.)
  id: totrans-274
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This is a full implementation of the dropout layer in PyTorch (although PyTorch’s
    native layer is actually written in C, not Python):'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The `bernoulli_` method is creating a tensor of random zeros (with probability
    `p`) and ones (with probability `1-p`), which is then multiplied with our input
    before dividing by `1-p`. Note the use of the `training` attribute, which is available
    in any PyTorch `nn.Module`, and tells us if we are doing training or inference.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Do Your Own Experiments
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters of the book, we’d be adding a code example for `bernoulli_`
    here, so you can see exactly how it works. But now that you know enough to do
    this yourself, we’re going to be doing fewer and fewer examples for you, and instead
    expecting you to do your own experiments to see how things work. In this case,
    you’ll see in the end-of-chapter questionnaire that we’re asking you to experiment
    with `bernoulli_`—but don’t wait for us to ask you to experiment to develop your
    understanding of the code we’re studying; go ahead and do it anyway!
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的前几章中，我们会在这里添加一个`bernoulli_`的代码示例，这样您就可以看到它的确切工作原理。但是现在您已经了解足够多，可以自己做这个，我们将为您提供越来越少的示例，而是期望您自己进行实验以了解事物是如何工作的。在这种情况下，您将在章节末尾的问卷中看到，我们要求您尝试使用`bernoulli_`，但不要等到我们要求您进行实验才开发您对我们正在研究的代码的理解；无论如何都可以开始做。
- en: Using dropout before passing the output of our LSTM to the final layer will
    help reduce overfitting. Dropout is also used in many other models, including
    the default CNN head used in `fastai.vision`, and is available in `fastai.tabular`
    by passing the `ps` parameter (where each “p” is passed to each added `Dropout`
    layer), as we’ll see in [Chapter 15](ch15.xhtml#chapter_arch_details).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在将我们的LSTM的输出传递到最终层之前使用dropout将有助于减少过拟合。在许多其他模型中也使用了dropout，包括`fastai.vision`中使用的默认CNN头部，并且通过传递`ps`参数（其中每个“p”都传递给每个添加的`Dropout`层）在`fastai.tabular`中也可用，正如我们将在[第15章](ch15.xhtml#chapter_arch_details)中看到的。
- en: Dropout has different behavior in training and validation mode, which we specified
    using the `training` attribute in `Dropout`. Calling the `train` method on a `Module`
    sets `training` to `True` (both for the module you call the method on and for
    every module it recursively contains), and `eval` sets it to `False`. This is
    done automatically when calling the methods of `Learner`, but if you are not using
    that class, remember to switch from one to the other as needed.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练和验证模式下，dropout的行为不同，我们使用`Dropout`中的`training`属性进行指定。在`Module`上调用`train`方法会将`training`设置为`True`（对于您调用该方法的模块以及递归包含的每个模块），而`eval`将其设置为`False`。在调用`Learner`的方法时会自动执行此操作，但如果您没有使用该类，请记住根据需要在两者之间切换。
- en: Activation Regularization and Temporal Activation Regularization
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活正则化和时间激活正则化
- en: '*Activation regularization* (AR) and *temporal activation regularization* (TAR)
    are two regularization methods very similar to weight decay, discussed in [Chapter 8](ch08.xhtml#chapter_collab).
    When applying weight decay, we add a small penalty to the loss that aims at making
    the weights as small as possible. For activation regularization, it’s the final
    activations produced by the LSTM that we will try to make as small as possible,
    instead of the weights.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 激活正则化（AR）和时间激活正则化（TAR）是两种与权重衰减非常相似的正则化方法，在[第8章](ch08.xhtml#chapter_collab)中讨论过。在应用权重衰减时，我们会对损失添加一个小的惩罚，旨在使权重尽可能小。对于激活正则化，我们将尝试使LSTM生成的最终激活尽可能小，而不是权重。
- en: 'To regularize the final activations, we have to store those somewhere, then
    add the means of the squares of them to the loss (along with a multiplier `alpha`,
    which is just like `wd` for weight decay):'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对最终激活进行正则化，我们必须将它们存储在某个地方，然后将它们的平方的平均值添加到损失中（以及一个乘数`alpha`，就像权重衰减的`wd`一样）：
- en: '[PRE49]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Temporal activation regularization is linked to the fact we are predicting
    tokens in a sentence. That means it’s likely that the outputs of our LSTMs should
    somewhat make sense when we read them in order. TAR is there to encourage that
    behavior by adding a penalty to the loss to make the difference between two consecutive
    activations as small as possible: our activations tensor has a shape `bs x sl
    x n_hid`, and we read consecutive activations on the sequence length axis (the
    dimension in the middle). With this, TAR can be expressed as follows:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 时间激活正则化与我们在句子中预测标记有关。这意味着当我们按顺序阅读它们时，我们的LSTM的输出应该在某种程度上是有意义的。TAR通过向损失添加惩罚来鼓励这种行为，使两个连续激活之间的差异尽可能小：我们的激活张量的形状为`bs
    x sl x n_hid`，我们在序列长度轴上（中间维度）读取连续激活。有了这个，TAR可以表示如下：
- en: '[PRE50]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '`alpha` and `beta` are then two hyperparameters to tune. To make this work,
    we need our model with dropout to return three things: the proper output, the
    activations of the LSTM pre-dropout, and the activations of the LSTM post-dropout.
    AR is often applied on the dropped-out activations (to not penalize the activations
    we turned into zeros afterward), while TAR is applied on the non-dropped-out activations
    (because those zeros create big differences between two consecutive time steps).
    A callback called `RNNRegularizer` will then apply this regularization for us.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，`alpha`和`beta`是要调整的两个超参数。为了使这项工作成功，我们需要让我们的带有dropout的模型返回三个东西：正确的输出，LSTM在dropout之前的激活以及LSTM在dropout之后的激活。通常在dropout后的激活上应用AR（以免惩罚我们之后转换为零的激活），而TAR应用在未经dropout的激活上（因为这些零会在两个连续时间步之间产生很大的差异）。然后，一个名为`RNNRegularizer`的回调将为我们应用这种正则化。
- en: Training a Weight-Tied Regularized LSTM
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练带有权重绑定的正则化LSTM
- en: 'We can combine dropout (applied before we go into our output layer) with AR
    and TAR to train our previous LSTM. We just need to return three things instead
    of one: the normal output of our LSTM, the dropped-out activations, and the activations
    from our LSTMs. The last two will be picked up by the callback `RNNRegularization`
    for the contributions it has to make to the loss.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将dropout（应用在我们进入输出层之前）与AR和TAR相结合，以训练我们之前的LSTM。我们只需要返回三个东西而不是一个：我们的LSTM的正常输出，dropout后的激活以及我们的LSTM的激活。最后两个将由回调`RNNRegularization`捕获，以便为其对损失的贡献做出贡献。
- en: 'Another useful trick we can add from [the AWD-LSTM paper](https://oreil.ly/ETQ5X)
    is *weight tying*. In a language model, the input embeddings represent a mapping
    from English words to activations, and the output hidden layer represents a mapping
    from activations to English words. We might expect, intuitively, that these mappings
    could be the same. We can represent this in PyTorch by assigning the same weight
    matrix to each of these layers:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从[AWD-LSTM论文](https://oreil.ly/ETQ5X)中添加另一个有用的技巧是*权重绑定*。在语言模型中，输入嵌入表示从英语单词到激活的映射，输出隐藏层表示从激活到英语单词的映射。直觉上，我们可能会期望这些映射是相同的。我们可以通过将相同的权重矩阵分配给这些层来在PyTorch中表示这一点：
- en: '[PRE51]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'In `LMMModel7`, we include these final tweaks:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在`LMMModel7`中，我们包括了这些最终的调整：
- en: '[PRE52]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We can create a regularized `Learner` using the `RNNRegularizer` callback:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`RNNRegularizer`回调函数创建一个正则化的`Learner`：
- en: '[PRE53]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'A `TextLearner` automatically adds those two callbacks for us (with those values
    for `alpha` and `beta` as defaults), so we can simplify the preceding line:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '`TextLearner`会自动为我们添加这两个回调函数（使用`alpha`和`beta`的默认值），因此我们可以简化前面的行：'
- en: '[PRE54]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We can then train the model, and add additional regularization by increasing
    the weight decay to `0.1`:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以训练模型，并通过增加权重衰减到`0.1`来添加额外的正则化：
- en: '[PRE55]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | accuracy | time |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 2.693885 | 2.013484 | 0.466634 | 00:02 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 2.693885 | 2.013484 | 0.466634 | 00:02 |'
- en: '| 1 | 1.685549 | 1.187310 | 0.629313 | 00:02 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1.685549 | 1.187310 | 0.629313 | 00:02 |'
- en: '| 2 | 0.973307 | 0.791398 | 0.745605 | 00:02 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.973307 | 0.791398 | 0.745605 | 00:02 |'
- en: '| 3 | 0.555823 | 0.640412 | 0.794108 | 00:02 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.555823 | 0.640412 | 0.794108 | 00:02 |'
- en: '| 4 | 0.351802 | 0.557247 | 0.836100 | 00:02 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.351802 | 0.557247 | 0.836100 | 00:02 |'
- en: '| 5 | 0.244986 | 0.594977 | 0.807292 | 00:02 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.244986 | 0.594977 | 0.807292 | 00:02 |'
- en: '| 6 | 0.192231 | 0.511690 | 0.846761 | 00:02 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 0.192231 | 0.511690 | 0.846761 | 00:02 |'
- en: '| 7 | 0.162456 | 0.520370 | 0.858073 | 00:02 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 0.162456 | 0.520370 | 0.858073 | 00:02 |'
- en: '| 8 | 0.142664 | 0.525918 | 0.842285 | 00:02 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 0.142664 | 0.525918 | 0.842285 | 00:02 |'
- en: '| 9 | 0.128493 | 0.495029 | 0.858073 | 00:02 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 0.128493 | 0.495029 | 0.858073 | 00:02 |'
- en: '| 10 | 0.117589 | 0.464236 | 0.867188 | 00:02 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 0.117589 | 0.464236 | 0.867188 | 00:02 |'
- en: '| 11 | 0.109808 | 0.466550 | 0.869303 | 00:02 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 0.109808 | 0.466550 | 0.869303 | 00:02 |'
- en: '| 12 | 0.104216 | 0.455151 | 0.871826 | 00:02 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 0.104216 | 0.455151 | 0.871826 | 00:02 |'
- en: '| 13 | 0.100271 | 0.452659 | 0.873617 | 00:02 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 13 | 0.100271 | 0.452659 | 0.873617 | 00:02 |'
- en: '| 14 | 0.098121 | 0.458372 | 0.869385 | 00:02 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 14 | 0.098121 | 0.458372 | 0.869385 | 00:02 |'
- en: Now this is far better than our previous model!
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这比我们之前的模型好多了！
- en: Conclusion
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'You have now seen everything that is inside the AWD-LSTM architecture we used
    in text classification in [Chapter 10](ch10.xhtml#chapter_nlp). It uses dropout
    in a lot more places:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在已经看到了我们在[第10章](ch10.xhtml#chapter_nlp)中用于文本分类的AWD-LSTM架构内部的所有内容。它在更多地方使用了丢失：
- en: Embedding dropout (just after the embedding layer)
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入丢失（就在嵌入层之后）
- en: Input dropout (after the embedding layer)
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入丢失（在嵌入层之后）
- en: Weight dropout (applied to the weights of the LSTM at each training step)
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重丢失（应用于每个训练步骤中LSTM的权重）
- en: Hidden dropout (applied to the hidden state between two layers)
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏丢失（应用于两个层之间的隐藏状态）
- en: This makes it even more regularized. Since fine-tuning those five dropout values
    (including the dropout before the output layer) is complicated, we have determined
    good defaults and allow the magnitude of dropout to be tuned overall with the
    `drop_mult` parameter you saw in that chapter (which is multiplied by each dropout).
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得它更加规范化。由于微调这五个丢失值（包括输出层之前的丢失）很复杂，我们已经确定了良好的默认值，并允许通过您在该章节中看到的`drop_mult`参数来整体调整丢失的大小。
- en: Another architecture that is very powerful, especially in “sequence-to-sequence”
    problems (problems in which the dependent variable is itself a variable-length
    sequence, such as language translation), is the Transformers architecture. You
    can find it in a bonus chapter on [the book’s website](https://book.fast.ai).
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个非常强大的架构，特别适用于“序列到序列”问题（依赖变量本身是一个变长序列的问题，例如语言翻译），是Transformer架构。您可以在[书籍网站](https://book.fast.ai)的额外章节中找到它。
- en: Questionnaire
  id: totrans-327
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问卷
- en: If the dataset for your project is so big and complicated that working with
    it takes a significant amount of time, what should you do?
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您的项目数据集非常庞大且复杂，处理它需要大量时间，您应该怎么做？
- en: Why do we concatenate the documents in our dataset before creating a language
    model?
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么在创建语言模型之前我们要将数据集中的文档连接起来？
- en: To use a standard fully connected network to predict the fourth word given the
    previous three words, what two tweaks do we need to make to our model?
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要使用标准的全连接网络来预测前三个单词给出的第四个单词，我们需要对模型进行哪两个调整？
- en: How can we share a weight matrix across multiple layers in PyTorch?
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何在PyTorch中跨多个层共享权重矩阵？
- en: Write a module that predicts the third word given the previous two words of
    a sentence, without peeking.
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个模块，预测句子前两个单词给出的第三个单词，而不偷看。
- en: What is a recurrent neural network?
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是循环神经网络？
- en: What is hidden state?
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 隐藏状态是什么？
- en: What is the equivalent of hidden state in `LMModel1`?
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`LMModel1`中隐藏状态的等价物是什么？'
- en: To maintain the state in an RNN, why is it important to pass the text to the
    model in order?
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了在RNN中保持状态，为什么按顺序将文本传递给模型很重要？
- en: What is an “unrolled” representation of an RNN?
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是RNN的“展开”表示？
- en: Why can maintaining the hidden state in an RNN lead to memory and performance
    problems? How do we fix this problem?
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么在RNN中保持隐藏状态会导致内存和性能问题？我们如何解决这个问题？
- en: What is BPTT?
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是BPTT？
- en: Write code to print out the first few batches of the validation set, including
    converting the token IDs back into English strings, as we showed for batches of
    IMDb data in [Chapter 10](ch10.xhtml#chapter_nlp).
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写代码打印出验证集的前几个批次，包括将标记ID转换回英文字符串，就像我们在[第10章](ch10.xhtml#chapter_nlp)中展示的IMDb数据批次一样。
- en: What does the `ModelResetter` callback do? Why do we need it?
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ModelResetter`回调函数的作用是什么？我们为什么需要它？'
- en: What are the downsides of predicting just one output word for each three input
    words?
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每三个输入词预测一个输出词的缺点是什么？
- en: Why do we need a custom loss function for `LMModel4`?
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们需要为`LMModel4`设计一个自定义损失函数？
- en: Why is the training of `LMModel4` unstable?
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么`LMModel4`的训练不稳定？
- en: In the unrolled representation, we can see that a recurrent neural network has
    many layers. So why do we need to stack RNNs to get better results?
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在展开表示中，我们可以看到递归神经网络有许多层。那么为什么我们需要堆叠RNN以获得更好的结果？
- en: Draw a representation of a stacked (multilayer) RNN.
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制一个堆叠（多层）RNN的表示。
- en: Why should we get better results in an RNN if we call `detach` less often? Why
    might this not happen in practice with a simple RNN?
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们不经常调用`detach`，为什么在RNN中应该获得更好的结果？为什么在实践中可能不会发生这种情况？
- en: Why can a deep network result in very large or very small activations? Why does
    this matter?
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么深度网络可能导致非常大或非常小的激活？这为什么重要？
- en: In a computer’s floating-point representation of numbers, which numbers are
    the most precise?
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在计算机的浮点数表示中，哪些数字是最精确的？
- en: Why do vanishing gradients prevent training?
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么消失的梯度会阻止训练？
- en: Why does it help to have two hidden states in the LSTM architecture? What is
    the purpose of each one?
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在LSTM架构中有两个隐藏状态为什么有帮助？每个的目的是什么？
- en: What are these two states called in an LSTM?
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在LSTM中这两个状态被称为什么？
- en: What is tanh, and how is it related to sigmoid?
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: tanh是什么，它与sigmoid有什么关系？
- en: 'What is the purpose of this code in `LSTMCell`:'
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`LSTMCell`中这段代码的目的是什么：'
- en: '[PRE56]'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: What does `chunk` do in PyTorch?
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在PyTorch中`chunk`是做什么的？
- en: Study the refactored version of `LSTMCell` carefully to ensure you understand
    how and why it does the same thing as the nonrefactored version.
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仔细研究`LSTMCell`的重构版本，确保你理解它如何以及为什么与未重构版本执行相同的操作。
- en: Why can we use a higher learning rate for `LMModel6`?
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们可以为`LMModel6`使用更高的学习率？
- en: What are the three regularization techniques used in an AWD-LSTM model?
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AWD-LSTM模型中使用的三种正则化技术是什么？
- en: What is dropout?
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是dropout？
- en: Why do we scale the weights with dropout? Is this applied during training, inference,
    or both?
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们要用dropout来缩放权重？这是在训练期间、推理期间还是两者都应用？
- en: 'What is the purpose of this line from `Dropout`:'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Dropout`中这行代码的目的是什么：'
- en: '[PRE57]'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Experiment with `bernoulli_` to understand how it works.
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试使用`bernoulli_`来了解它的工作原理。
- en: How do you set your model in training mode in PyTorch? In evaluation mode?
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何在PyTorch中将模型设置为训练模式？在评估模式下呢？
- en: Write the equation for activation regularization (in math or code, as you prefer).
    How is it different from weight decay?
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 写出激活正则化的方程（数学或代码，任你选择）。它与权重衰减有什么不同？
- en: Write the equation for temporal activation regularization (in math or code,
    as you prefer). Why wouldn’t we use this for computer vision problems?
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 写出时间激活正则化的方程（数学或代码，任你选择）。为什么我们不会在计算机视觉问题中使用这个？
- en: What is weight tying in a language model?
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 语言模型中的权重绑定是什么？
- en: Further Research
  id: totrans-369
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步研究
- en: In `LMModel2`, why can `forward` start with `h=0`? Why don’t we need to say
    `h=torch.zeros(...)`?
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`LMModel2`中，为什么`forward`可以从`h=0`开始？为什么我们不需要写`h=torch.zeros(...)`？
- en: Write the code for an LSTM from scratch (you may refer to [Figure 12-9](#lstm)).
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从头开始编写一个LSTM的代码（你可以参考[图12-9](#lstm)）。
- en: Search the internet for the GRU architecture and implement it from scratch,
    and try training a model. See if you can get results similar to those we saw in
    this chapter. Compare your results to the results of PyTorch’s built-in `GRU`
    module.
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 搜索互联网了解GRU架构并从头开始实现它，尝试训练一个模型。看看能否获得类似于本章中看到的结果。将你的结果与PyTorch内置的`GRU`模块的结果进行比较。
- en: Take a look at the source code for AWD-LSTM in fastai, and try to map each of
    the lines of code to the concepts shown in this chapter.
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看fastai中AWD-LSTM的源代码，并尝试将每行代码映射到本章中展示的概念。
