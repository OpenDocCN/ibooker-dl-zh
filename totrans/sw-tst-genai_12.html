<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="heading_id_2">10 <a id="idTextAnchor002"/><a id="idTextAnchor003"/><a id="idTextAnchor004"/><a id="idTextAnchor005"/><a id="idTextAnchor006"/>Introducing customized LLMs</h1>
<p class="co-summary-head">This chapter covers<a id="idIndexMarker000"/><a id="marker-203"/><a id="idIndexMarker001"/></p>
<ul class="calibre5">
<li class="co-summary-bullet">How a lack of context affects an LLM’s performance</li>
<li class="co-summary-bullet">How RAG works and its value</li>
<li class="co-summary-bullet">How the fine-tuning of LLMs works and its value</li>
<li class="co-summary-bullet">Comparing RAG and fine-tuning approaches</li>
</ul>
<p class="body">Over the past few chapters, we saw how to hone our skills to identify distinct, focused tasks that large language models (LLMs) can support. Combined with a range of prompt-engineering techniques, we’ve been successful in getting LLMs to return responses that are valuable for our testing activities. However, despite the lessons we learned, the responses we receive might still not be completely aligned with our needs and context. Although it would be foolish to think that we can completely align an LLM with our context, there are more advanced options that can be utilized along with prompt engineering to further maximize the output of an LLM in support of our testing. So, in this final part, we’re going to examine ways in which we can enhance LLMs so that they can become more embedded in our context, specifically focusing on retrieval-augmented generation (RAG) and fine-tuning. But before we dig into the specific details and actions of how these approaches work, we’ll first examine why more commonly used LLMs such as ChatGPT, Claude, and Gemini may struggle to tune to our context and then slowly familiarize ourselves with the more advanced topics of RAG and fine-tuning, comparing them to determine which one is more suitable in a given situation.<a id="idIndexMarker002"/></p>
<h2 class="fm-head" id="heading_id_3">10.1 The challenge with LLMs and context</h2>
<p class="body"><a id="marker-204"/>We learned that the key challenge of working with LLMs is getting them to respond with an output that<a id="idIndexMarker003"/></p>
<ul class="calibre5">
<li class="fm-list-bullet">
<p class="list">Is relevant to our context</p>
</li>
<li class="fm-list-bullet">
<p class="list">Follows the rules of our prompt</p>
</li>
<li class="fm-list-bullet">
<p class="list">Contains no hallucinations</p>
</li>
</ul>
<p class="body">The prompt-engineering techniques we’ve used have helped us achieve these goals so far. For example, if we return to our example prompt from chapter 5, we can see a weak prompt such as</p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-MW.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table">Create tests for a file upload feature</p>
</td>
</tr>
</tbody>
</table>
<p class="body">which returns a weak response with quite generalized results:</p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-openai.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table-list-a">   1.  Test for File Upload Functionality:</p>
<p class="body-table-list-b">   •   Test Case 1.1: Upload a Valid File</p>
<p class="body-table-list-c">   <span class="cambria">−</span>   Input: Choose a file with a valid format (e.g., .jpg, .pdf).</p>
<p class="body-table-list-c">   <span class="cambria">−</span>   Expected Outcome: The file is successfully uploaded, and a confirmation message is displayed.</p>
<p class="body-table-list-b">   •   Test Case 1.2: Attempt to Upload an Invalid File</p>
<p class="body-table-list-c">   <span class="cambria">−</span>   Input: Choose a file with an invalid format (e.g., .exe, .xhtml).</p>
<p class="body-table-list-c">   <span class="cambria">−</span>   Expected Outcome: An error message is displayed, indicating that the file format is not supported.</p>
<p class="body-table-list-a">   2.  Test for File Size Limit:</p>
<p class="body-table-list-b">   •   Test Case 2.1: Upload a File Below the Size Limit</p>
<p class="body-table-list-c">   <span class="cambria">−</span>   Input: Choose a file smaller than the maximum allowed size.</p>
<p class="body-table-list-c">   <span class="cambria">−</span>   Expected Outcome: The file is successfully uploaded, and a confirmation message is displayed.</p>
<p class="body-table-list-b">   •   Test Case 2.2: Attempt to Upload a File Exceeding the Size Limit</p>
<p class="body-table-list-c">   <span class="cambria">−</span>   Input: Choose a file larger than the maximum allowed size.</p>
<p class="body-table-list-c">   <span class="cambria">−</span>   Expected Outcome: An error message is displayed, indicating that the file exceeds the maximum size limit.</p>
</td>
</tr>
</tbody>
</table>
<p class="body">However, a prompt with more detail and the right prompt-engineering tactics can result in a more optimal response. A prompt such as</p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-MW.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table">You are an expert software tester. Generate as many test cases as possible for the behavior delimited by three hashes.</p>
<p class="body-table">###</p>
<p class="body-table">An Authorization API that will take a token from a different API and check to see if the token is valid and in date before either responding positively or negatively depending on the result.</p>
<p class="body-table">###</p>
</td>
</tr>
</tbody>
</table>
<p class="body"><a id="marker-205"/>returns more specific results, such as</p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-openai.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table-list-a">   1.  Valid Token Check:</p>
<p class="body-table-list-b">   •   Test with a valid token that is within the expiration date. Expected result: API responds positively.</p>
<p class="body-table-list-a">   2.  Expired Token Check:</p>
<p class="body-table-list-b">   •   Test with a valid token that has expired. Expected result: API responds negatively.</p>
<p class="body-table-list-a">   3.  Invalid Token Check:</p>
<p class="body-table-list-b">   •   Test with an invalid token format (e.g., incorrect characters, wrong length). Expected result: API responds negatively.</p>
<p class="body-table-list-a">   4.  Empty Token Check:</p>
<p class="body-table-list-b">   •   Test with an empty token input. Expected result: API responds negatively or returns an appropriate error message.</p>
<p class="body-table-list-a">   5.  Null Token Check:</p>
<p class="body-table-list-b">   •   Test with a null token input. Expected result: API responds negatively or returns an appropriate error message.</p>
</td>
</tr>
</tbody>
</table>
<p class="body">Our second prompt is more successful because we’ve shared more of our context within as well as used specific prompt engineering tactics. Therefore, if we want to get the most out of an LLM, we not only need to rely on good prompt-engineering techniques, but also provide the most relevant context possible. The reasons for this are twofold:</p>
<ul class="calibre5">
<li class="fm-list-bullet">
<p class="list">It is more than likely that an LLM has not been trained on our context. Therefore, an LLM has no increased weighting or bias toward our context.</p>
</li>
<li class="fm-list-bullet">
<p class="list">An LLM is trained on such a massive amount of generalized data that when asked a generic question, it will rely on stronger, more generalized patterns it has identified from the training process.</p>
</li>
</ul>
<p class="body">So, if we want to get the best out of an LLM, on the surface, the answer seems simple enough: provide it with as much context detail as possible (which we have been doing to some degree already). However, if we were to do this, we’d quickly run into some limitations around the amount we can send in a prompt.</p>
<h3 class="fm-head1" id="heading_id_4">10.1.1 Tokens, context windows, and limitations</h3>
<p class="body">Before we talk about this prompting limitation, there are some other concepts around LLMs that we need to be aware of—namely, tokens and context windows. Understanding these two aspects of LLMs will help us appreciate why current LLMs have an upper limit on how much context can be provided and how that affects our strategies of use.<a id="idIndexMarker004"/><a id="idIndexMarker005"/><a id="idIndexMarker006"/><a id="marker-206"/><a id="idIndexMarker007"/></p>
<p class="fm-head2">Tokens</p>
<p class="body">Imagine we are sending the following prompt to an LLM:<a id="idIndexMarker008"/></p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-MW.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table">List me five of the most populated cities in the world.</p>
</td>
</tr>
</tbody>
</table>
<p class="body">How does an LLM, which can only interpret information using machine code, parse this prompt and return a response? This is done through a process known as <i class="fm-italics">tokenization,</i> in which natural language text is converted into matching integers that can be read by an LLM. To understand how this works, let’s consider our populated cities prompt. If we were to put this through the tokenization process, the sentence would be sliced into smaller, discrete parts. For example, our prompt could be broken into 12 sections:<a id="idIndexMarker009"/></p>
<pre class="programlisting">List
 me
 five
 of
 the
 most
 populated
 cities
 in
 the
 world
.</pre>
<p class="body">As we can see, each word in the sentence and the period at the end have been sliced into their own smaller sections, known as <i class="fm-italics">tokens.</i> Notice how they also include the whitespace to the left of each word. It’s a general rule of thumb that each word in a sentence is split into its token; however, some tokenizers (the tools used for making this conversion) can sometimes break up larger words into individual tokens or group smaller words together.</p>
<p class="body">Once a sentence has been sliced into tokens, each one is converted into an integer with a unique number being used as an identifier for each word. For example, completing the tokenization of our prompt would result in a list of numbers (the commas and whitespace have been added to help readability):</p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-openai.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<pre class="programlisting">861, 757, 4330, 315, 279, 1455, 35459, 9919, 304, 279, 1917, 13</pre>
</td>
</tr>
</tbody>
</table>
<p class="body">Each number in this list correlates to a specific slice taken from our prompt. For example, the token <code class="fm-code-in-text">the</code> has an id of 279 and we can see that it appears twice in the list for each instance when <code class="fm-code-in-text">the</code> was used in the sentence. Once the prompt has completed the tokenization process, the model we use is then able to process the list of integers and start determining how to respond. Then the process of tokenization is used again to create the response text we receive from a model, so our model might respond with a series of integers:</p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-openai.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<pre class="programlisting">53954, 16417, 11, 6457, 198, 16939, 6151, 11, 6890, 198, 2059, 31170, 11, 5734, 198, 50, 3496, 38026, 11, 16327, 198, 44, 30955, 11, 6890</pre>
</td>
</tr>
</tbody>
</table>
<p class="body">When converted back into text, it would result in the following response (line breaks are also included in the integer list, and each mention of iteration of 11 is a line break):</p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-openai.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table">Tokyo, Japan</p>
<p class="body-table">Delhi, India</p>
<p class="body-table">Shanghai, China</p>
<p class="body-table">São Paulo, Brazil</p>
<p class="body-table">Mumbai, India</p>
</td>
</tr>
</tbody>
</table>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Experimenting with tokenizers</p>
<p class="fm-sidebar-text">To better understand how the tokenization process works and how words, numbers, and symbols are sliced, we can experiment with preview tools for tokenizing such as the one at <a class="url" href="https://gpt-tokenizer.dev/">https://gpt-tokenizer.dev/</a>.</p>
</div>
<p class="body">So, tokens are an important aspect of LLMs because they inform us not just of how a model can parse a prompt and form a response, but also of how large a prompt can be sent to said model before we run into problems. This brings us to context windows and the crux of our context challenge.</p>
<p class="fm-head2">Context windows</p>
<p class="body">Given that the tokenization process takes natural language and turns it into a series of numbers for an LLM to process, the longer a prompt is, the more tokens there are to process. The problem with prompts that have larger sets of tokens is that it affects how effectively an LLM processes our prompt and the resources it consumes. The larger a prompt, the more complex it becomes to generate a response, which means more hardware usage. All of this will come at a cost, either in hosting fees if utilizing a private LLM or API costs for sending prompts (platforms such as OpenAI charge based on the number of sent and received tokens).<a id="marker-207"/><a id="idIndexMarker010"/></p>
<p class="body">Add to this that a larger context window doesn’t necessarily mean a better-performing LLM, and we start to see that providers of LLMs have a tradeoff to make. As a result, LLMs will likely have some sort of limitation built into the model on the number of tokens it can receive at a given time. This is known as a model’s <code class="fm-code-in-text">context</code> <code class="fm-code-in-text">window</code>. Different models contain different-sized context windows, which are sometimes also referred to as the context length. All of this depends on the type of model that has been trained, the hardware it is running on, and how it has been deployed with other supporting applications. For example, OpenAI’s ChatGPT 4 is estimated to have a context window of 128k tokens, whereas that for Meta’s Llama-2 is 4k (before modifications are made). Therefore, when it comes to determining which LLM to use in a given situation, we must be aware of the context length. Choosing a model that is limited in size to save cost might limit what context we can add to a prompt.<a id="idIndexMarker011"/><a id="idIndexMarker012"/><a id="idIndexMarker013"/><a id="idIndexMarker014"/></p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Not all context windows are the same</p>
<p class="fm-sidebar-text">One thing to note when discussing context windows is that just because a model is able to take, for example, a 128k token request, it doesn’t mean that the response will have the same limitations. In fact, it may be that the response has a much smaller window to help keep costs down. This won’t necessarily affect our learning in the following chapters, but it’s a useful detail to keep in mind when expecting a model to return a large response.</p>
</div>
<h3 class="fm-head1" id="heading_id_5">10.1.2 Embedding context as a solution</h3>
<p class="body">Now that we understand that LLMs interpret our requests through tokenization and that the size of tokens we can send to an LLM is limited, we can start to see the problem we face when adding further context to our prompts. Although LLMs are developing rapidly, becoming more efficient and offering larger context windows, it’s simply not cost-effective to, for example, add a complete code base of an application with prompt’s instructions. We’d either end up hitting the upper limits of a model’s capabilities or burning our budget at a rapid pace. Instead, to maximize accuracy, we need to consider how we embed context into our prompts and our LLMs in an intelligent way. Fortunately, there has been a lot of work in the AI community that we can use to embed our context further in a way that improves accuracy and doesn’t break the bank (or the model) in the process.<a id="idIndexMarker015"/><a id="marker-208"/><a id="idIndexMarker016"/></p>
<h2 class="fm-head" id="heading_id_6">10.2 Embedding context further into prompts and LLMs</h2>
<p class="body">To improve the ability to increase an LLM’s exposure to our context, we can utilize one of two techniques. The first is RAG, and the second, fine-tuning. Throughout the remaining chapters, we’ll explore how these two approaches work, how they differ from one another, and how we can determine which is a more suitable approach to improving an LLM’s responses. Although both approaches are different in application, the end goal of both is similar: to help us improve a model’s performance by allowing us to add more context to an LLM’s workflow. Retrieval-augmented generation looks to solve the problem by focusing on ways in which we can enhance our prompting, whereas fine-tuning looks to bake our context directly into the model itself. Let’s take a look at both briefly so that we can become more familiar with them and determine which approach is more suitable in a given situation.<a id="idIndexMarker017"/><a id="idIndexMarker018"/></p>
<h3 class="fm-head1" id="heading_id_7">10.2.1 RAG</h3>
<p class="body">As we learned earlier, if an LLM has limitations in the size of a prompt it can receive due to a context window, it’s inadvisable to attempt to throw all our context into a single prompt hoping that it will improve an LLM’s response. However, that doesn’t mean we can’t be selective with the type of context we provide in a prompt. What this means is that accuracy can be improved in an LLM not by brute-forcing our context onto an LLM, but by crafting our prompts so that they contain all the relevant information about our context to support our instructions. In more concrete terms, this means that if we wanted an LLM to generate boilerplate page objects for an automated check, it would be better to provide the specific HTML for a page and any relevant code for said page than add the entire code base to a prompt.<a id="idIndexMarker019"/><a id="idIndexMarker020"/><a id="idIndexMarker021"/><a id="marker-209"/></p>
<p class="body">On the surface, this seems like an effective and simple approach: write our prompt, find the relevant supporting information, combine the two into a final prompt, and send it to an LLM. The problem though is that this can be a labor-intensive activity, researching and determining what information to add and what to ignore. Fortunately, this is where RAG can help us. As shown in figure 10.1, RAG works by automating the process of embedding relevant information into our prompts through using our prompt’s initial instructions to determine what information to add to our prompt.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre39" src="../../OEBPS/Images/CH10_F01_Winteringham2.png"/></p>
<p class="figurecaption">Figure 10.1 A high-level workflow diagram for RAG</p>
</div>
<p class="body">To help us better understand the process, let’s return to our Page-object-generation prompt example. With a RAG framework, the approach would work like this:</p>
<ol class="calibre18">
<li class="fm-list-bullet">
<p class="list">A corpus of information will have been created. In our example, it may contain labeled HTML documents for each page of our application.</p>
</li>
<li class="fm-list-bullet">
<p class="list">A prompt is created in which we ask for an LLM to generate a Page object for our booking listing page.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Our RAG framework analyzes our prompt and programmatically finds the most relevant document in our corpus of HTML documents. If the RAG framework is working correctly, it will determine that the HTML document that contains the booking listing is the most relevant.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The most relevant HTML document is added to the prompt that we initially created, and the prompt is then sent to an LLM to return a response.</p>
</li>
</ol>
<p class="body">The RAG helps us contextualize our prompts further by analyzing what we’re asking and then automatically identifying the right type of context. What makes this useful is that it helps us create a prompt with the information that is of most use for an LLM to create a more accurate response than if the information didn’t exist. It’s also a useful approach because RAG allows us to embed any type of data that is easy to parse and search for relevancy, whether it’s code, documentation, database entries, or raw metrics. We also can control how relevancy is determined, meaning that we still have control over the type of information that might be added to a prompt.</p>
<p class="body">For these reasons and its relative ease of setup, RAG has become a popular approach to enhancing our interaction with LLMs. Once we begin to appreciate how RAG works, we can begin to see how it might be useful in a testing context. We’ve already explored the idea of using RAG to extract sections of a code base to support prompts that are looking to create automation, but it can also be used to support queries around risk analysis, understanding how our products work and generating test ideas. There is also the potential to use testing artifacts in RAG frameworks, such as exploratory testing notes, test scripts, or automation code, to bolster the prompting ideas we explored in the previous chapters. At its core, if the data we want to use can be stored in a consistent format and easily queried, then it has the potential to be used in a RAG framework.</p>
<h3 class="fm-head1" id="heading_id_8">10.2.2 Fine-tuning LLMs</h3>
<p class="body"><a id="marker-210"/>Although RAG focuses on ways to enhance a prompt by adding targeted contextual material, fine-tuning focuses on enhancing the model we’re using itself. Fine-tuning utilizes a collection of tools and techniques used to further train a model that has already been initially trained in hope that it will bias the model further toward the data it has been fine-tuned on, as summarized in figure 10.2.<a id="idIndexMarker022"/><a id="idIndexMarker023"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre40" src="../../OEBPS/Images/CH10_F02_Winteringham2.png"/></p>
<p class="figurecaption">Figure 10.2 A visualization outlining the difference between training and fine-tuning</p>
</div>
<p class="body">As the figure shows, fine-tuning takes place against a model that has already been trained on a large corpus of data. For example, the free version of ChatGPT is a fine-tuned version of OpenAI’s gpt-3.5-turbo model. gpt-3.5 went through a series of training processes to get its core abilities, but when interacted with, it lacks the tone and characteristics of a chatbot. For example, after sending the following prompt to gpt-3.5 (before fine-tuning):</p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-MW.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table">You are an expert travel agent. Give me a list of the top three places to visit in London.</p>
</td>
</tr>
</tbody>
</table>
<p class="body"><a id="marker-211"/>I received the following response:</p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-openai.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table-list-a">   1.  The British Museum - This iconic museum houses a vast collection of art and artifacts from all corners of the globe, including the Rosetta Stone and the controversial Elgin Marbles. It is a must-visit for history and culture enthusiasts.</p>
<p class="body-table-list-a">   2.  The Tower of London - This historic fortress on the banks of the River Thames has served as a royal palace, prison, and armory. Visitors can explore the Crown Jewels, see the famous ravens, and learn about the Tower’s fascinating history.</p>
<p class="body-table-list-a">   3.  The West End - London’s renowned theater district is home to world-class productions and musicals. Take in a show at one of the many historic venues like the Royal Opera House or the National Theatre, and experience the magic of London’s vibrant performing arts scene.</p>
</td>
</tr>
</tbody>
</table>
<p class="body">Whereas, upon sending the prompt to ChatGPT (a fine-tuned version of gpt-3.5), I received the following response:</p>
<table border="0" class="contenttable-0-table" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="7%"/>
<col class="contenttable-0-col" span="1" width="93%"/>
</colgroup>
<tbody class="contenttable-0-tbody">
<tr class="contenttable-0-tr">
<td class="contenttable-0-td">
<div class="figure2">
<p class="figure1"><img alt="" class="calibre7" height="40" src="../../OEBPS/Images/logo-openai.png" width="40"/></p>
</div>
</td>
<td class="contenttable-0-td">
<p class="body-table">Absolutely! London offers a plethora of attractions, but here are three must-visit places:</p>
<p class="body-table-list-a">   1.  The British Museum: It’s a treasure trove of human history and culture, housing millions of artifacts spanning centuries and continents. From the Rosetta Stone to the Egyptian mummies and the Elgin Marbles, the British Museum is a fascinating journey through civilizations.</p>
<p class="body-table-list-a">   2.  The Tower of London: Steeped in history and legend, the Tower of London is an iconic fortress that has served as a royal palace, prison, and treasury over the centuries. Visitors can explore its ancient walls, marvel at the Crown Jewels, and hear tales of famous prisoners and executions.</p>
<p class="body-table-list-a">   3.  Westminster Abbey: A masterpiece of Gothic architecture, Westminster Abbey is not only a place of worship but also the site of royal coronations, weddings, and burials. Visitors can admire its stunning stained glass windows, intricate stone carvings, and the tombs of famous figures like Isaac Newton, Charles Darwin, and Elizabeth I.</p>
<p class="body-table">These attractions offer a rich glimpse into London’s history, culture, and heritage.</p>
</td>
</tr>
</tbody>
</table>
<p class="body">Factually, they are similar. But looking at the second response, we can see that it has a bit more flavor to it. It has a more casual and friendly tone, all of which comes from the fine-tuning process it has undergone to make it feel more chat-like.</p>
<p class="body">Qualities such as ChatGPT’s more casual tone are achieved by training a model with a curated dataset in hope of biasing its internal parameters toward a desired tone, style, or specific context. To recap, a model consists of millions or billions of parameters that determine the probability of which word should come after another word or collection of words. These parameters are in place when a fine-tuning session is started, and the fine-tuning process seeks to influence the parameters further toward an end goal. We’ll talk about this more in chapter 11, but we can see how this approach works in figure 10.3.</p>
<p class="body">Fine-tuning takes information from a data set and uses it to form a prompt sent to our model that is being fine-tuned. The response is then evaluated against an expected response. If the responses don’t align, the model is tweaked so that its chances of an aligned response the next time are increased. This is then done thousands of, or perhaps millions of, times to slowly tune a model closer to our end goal of how we want the fine-tuned model to respond, which means that a very large corpus of data is required to successfully train a model.</p>
<p class="body">Applying fine-tuning to a model can have a range of benefits. We have already seen them in the fine-tuning of GPT into ChatGPT, but we’ve also taken advantage of fine-tuned GPT models in the form of GitHub Copilot. These examples demonstrate the range of uses that fine-tuning can provide in a testing context. Thus, we can tune models based on natural language text that might come from documentation or testing artifacts. This could be used to embed domain language into its responses and promote responses that are more tuned to our context. They can also be tuned on our code base to help us with additional risk analysis, comprehension of what our code is doing, or acting as a more aligned code assistant.</p>
<p class="body"><a id="marker-212"/>When discussing fine-tuning, we need to be careful not to fall into the trap of thinking we’re teaching a model about our context. LLMs don’t think like humans do. But, as an analogy, teaching a model about our context is a close one. The challenge is that it’s not an exact process, meaning that multiple iterations are likely required to get the result we want, and as our context changes, further tuning sessions would likely be required.<a id="idIndexMarker024"/><a id="idIndexMarker025"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre41" src="../../OEBPS/Images/CH10_F03_Winteringham2.png"/></p>
<p class="figurecaption">Figure 10.3 A visual model of how fine-tuning works</p>
</div>
<h3 class="fm-head1" id="heading_id_9">10.2.3 Comparing the two approaches</h3>
<p class="body">Knowing which approach to take depends very much on the goals we want to achieve, and the constraints put upon us. The two approaches aren’t entirely the same. But if we ever come into a position where we need to decide which approach to take, it can be beneficial to use some general attributes to help us determine the pros and cons of each.<a id="idIndexMarker026"/><a id="marker-213"/><a id="idIndexMarker027"/></p>
<p class="fm-head2">Learning curve</p>
<p class="body">Although learning new skills is always relative to a person’s abilities and skillsets, for those of us who have followed along with the book so far, getting comfortable with RAG frameworks is a smaller learning leap than fine-tuning. It can be argued that using a RAG framework is a form of advanced prompt engineering and many off-the-shelf tools are available that can be used to jump into using RAG with minimum effort.<a id="idIndexMarker028"/><a id="idIndexMarker029"/></p>
<p class="body">Fine-tuning, however, has a steeper learning curve because it is a collection of different actions, tools, and considerations that are much larger in scope than using RAG. In the chapter on fine-tuning, you’ll learn different steps to take in curating and preparing data for tuning, executing a fine-tuning session, and evaluating success. Each part requires knowledge of tools, frameworks, and approaches to complete each step. Fortunately, the ecosystem around fine-tuning is making the fine-tuning process more accessible and easy to get started with.</p>
<p class="fm-head2">Cost</p>
<p class="body">There are two aspects to cost that we need to consider: tooling and talent. As we’ve learned, the learning curve for RAG can be easier than that for fine-tuning. This means that the associated costs around training or hiring talent around implementing RAG are potentially lesser. As for tooling, the cost to get initially set up with RAG can be quite low. However, costs for RAG framework tools and the use of third-party LLMs can cause costs to balloon, especially if we are being charged for the number of tokens sent and received via an LLM API platform.<a id="idIndexMarker030"/><a id="marker-214"/><a id="idIndexMarker031"/></p>
<p class="body">Unlike the popular RAG frameworks, a lot of fine-tuning tools are open source, which can make initial investment in tooling cheaper. Platforms are appearing that make the fine-tuning process easier, but they do come at a price. For tooling, the cost can be found more in the hardware needed to support fine-tuning. Running tuning sessions requires substantial CPU, GPU, and RAM resources, and if we want to tune at scale, then more investment is required. There is also the cost of hosting a tuned model once it’s ready for use. Finally, as fine-tuning consists of a combination of activities, training or hiring can be much more expensive, depending on how much detail we want to go into each part of the tuning process.</p>
<p class="fm-head2">Speed to production</p>
<p class="body">Given the tools available to support a RAG framework out of the box, getting RAG set up and running can be rather fast. When iterating with RAG, the focus will be on the following two areas: the prompt we want to send to our LLM (that includes the additional data) and the data we want to store and extract relevant information from when required. Although there is a lot of space for improvement, getting these aspects of a RAG framework set up to an initial satisfactory state doesn’t take too much time.<a id="idIndexMarker032"/><a id="idIndexMarker033"/></p>
<p class="body">Fine-tuning, however, can be relatively slower because there are more activities involved. For example, curating and preparing data sets for fine-tuning can be a complex activity in its own right. Depending on hardware, fine-tuning can also take time to complete with even small tunings taking many hours. Add to this that we would likely need to run multiple tunes because we tweak tuning and model settings, as well as the data set we are using, so it can take a while before we reach a satisfactory tuned model.</p>
<p class="fm-head2">Control</p>
<p class="body">Although most of the comparisons so far have been favorable toward RAG frameworks, those benefits do have a tradeoff. When we refer to control as a quality characteristic of using LLMs, this implies how much influence we have on improving the process, what insight we have into how a model is performing, and what control we have over the LLM behavior. Also, there are considerations for privacy controls as well.<a id="idIndexMarker034"/><a id="marker-215"/><a id="idIndexMarker035"/></p>
<p class="body">Most of the RAG tools available for purchase are hosted on platforms that can be quite opaque. This can mean there is less control over how data is stored for retrieval or how the relevancy algorithms work. For example, one technology that is used in RAG is vector databases. How data is stored and relationships maintained in vector databases can be out of our control but have a big influence on what relevant data is returned. Add to this that a lot of these tools tend to encourage us to use platforms such as OpenAI’s API, then we have even less control over which models we want to use and how an LLM will respond.</p>
<p class="body">Fine-tuning is very much about experimentation, which means we must have full control over all aspects of how the tuning is done. Because fine-tuning contains many steps, we have a lot of control over what happens within each part of the process. We have control over what data we want to use and what format it should be in, and we can control which type of model we want to tune and how. Furthermore, because the result is a tuned model that can be deployed elsewhere, we have a lot more control over where a model is deployed and who has access to it, making it more suitable for enterprise-based applications.</p>
<p class="body">These comparisons help us get a flavor of how the two approaches compare, which has been summarized in figure 10.4.</p>
<p class="body">Of course, these comparisons are highly context-dependent, but they do demonstrate that RAG can be a faster, more cost-effective approach to take on first. However, if we want more control over how we want an LLM to respond, then turning to fine-tuning can reward us further if we’re willing to invest.<a id="idIndexMarker036"/><a id="idIndexMarker037"/></p>
<h3 class="fm-head1" id="heading_id_10">10.2.4 Combining RAG and fine-tuning</h3>
<p class="body">We’ve explored how these two approaches differ, but before we conclude the chapter, it’s worth stating that these two techniques are not mutually exclusive. Given that RAG focuses on prompts, whereas fine-tuning focuses on changes to a model, both can be combined to further improve responses. The tradeoff is that much more complexity is introduced into building, training, and debugging. It is much more expensive to bring a tuned model inside a RAG framework to production, and if it doesn’t work as expected (or desired), how do we determine what needs our attention? This is the challenge of working with indeterministic systems, whether we choose to focus on RAG, fine-tuning, or a combination of both. The approach to evaluating an LLM’s use as a testing assistant requires constant, healthy skepticism.<a id="idIndexMarker038"/><a id="idIndexMarker039"/><a id="idIndexMarker040"/><a id="idIndexMarker041"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre42" src="../../OEBPS/Images/CH10_F04_Winteringham2.png"/></p>
<p class="figurecaption">Figure 10.4 A quick comparison of RAG and fine-tuning</p>
</div>
<h2 class="fm-head" id="heading_id_11">Summary</h2>
<ul class="calibre5">
<li class="fm-list-bullet">
<p class="list">One of the key challenges of using LLMs is getting them to return context-sensitive, valuable results.</p>
</li>
<li class="fm-list-bullet">
<p class="list">To get a well-aligned response, an LLM needs to be given as much relevant context as possible.</p>
</li>
<li class="fm-list-bullet">
<p class="list">LLMs interpret natural language text by turning text into numbers, known as tokens, through the tokenization process.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Based on the sophistication of an LLM model and the hardware it is run on, an LLM will only be able to take a certain number of tokens at a given time.<a id="marker-216"/></p>
</li>
<li class="fm-list-bullet">
<p class="list">The number of tokens an LLM can take at a given time is known as the context window.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Because LLMs have a limited context window, we must come up with different strategies to allow us to embed context in without incurring massive costs.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Two approaches that can be used to improve context awareness are retrieval-augmented generation (RAG) and fine-tuning.</p>
</li>
<li class="fm-list-bullet">
<p class="list">RAG is a process in which additional relevant information is added to a prompt to improve an LLM’s response.</p>
</li>
<li class="fm-list-bullet">
<p class="list">RAG works by connecting to a corpus of data and finding the most relevant material based on the provided prompt query. It’s then all combined into a single prompt for an LLM.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Fine-tuning utilizes training techniques to tune an already trained model with additional data.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Fine-tuning allows us to modify the tone or detail or way in which an LLM responds.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Fine-tuning can help us promote our context within an LLM’s parameters and make it more context-sensitive to our needs.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Learning how to utilize RAG frameworks tends to be faster and easier than fine-tuning.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Fine-tuning requires knowledge of different processes and tools to carry out the full fine-tuning process.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Cost of tooling and talent for RAG is relatively lower than that for fine-tuning.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Existing RAG platforms make it easy to get set up and running with RAG.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Fine-tuning requires more investment in time to get a model ready for production.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Fine-tuning offers much more control than RAG in terms of the model or framework we use at the end.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><a id="marker-217"/>RAG and fine-tuning can be used together.</p>
</li>
</ul>
</div></body></html>