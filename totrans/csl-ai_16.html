<html><head></head><body>
<div id="sbo-rt-content"><div class="readable-text" id="p1">
<h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">13</span> </span> <span class="chapter-title-text">Causality and large language models</span></h1>
</div>
<div class="introduction-summary">
<h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3>
<ul>
<li class="readable-text" id="p2">Using causal information in LLMs to enhance a causal analysis</li>
<li class="readable-text" id="p3">Connecting the components of an LLM to a causal ideas</li>
<li class="readable-text" id="p4">Building a causal LLM</li>
</ul>
</div>
<div class="readable-text" id="p5">
<p>Large language models (LLMs) represent a significant advancement in the field of artificial intelligence. These models are large neural networks designed to generate and understand human-readable text. They are “large” because their scale is truly impressive—cutting-edge LLMs have parameters numbering in the billions and trillions. As generative models, their main function is to generate coherent and contextually relevant natural language. They can also generate structured text, such as programming code, markup languages, mathematical symbols, database queries, and many other useful things in text form.</p>
</div>
<div class="readable-text intended-text" id="p6">
<p>LLMs are just one example of a broad class of generative AI. For example, we can use the neural network architecture underlying cutting-edge LLMs to model other sequences, such as time-series or DNA. LLMs are a type of <em>foundation model</em>, meaning large-scale models that serve as a base or foundation upon which more specialized models or applications can be built. Some LLMs are multimodal, meaning they work with text as well as other content modalities, such as images. In this chapter, we’ll focus specifically on LLMs, but much of what we discuss can be generalized to these related ideas.</p>
</div>
<div class="readable-text intended-text" id="p7">
<p>To start, let’s explore some use cases for using LLMs to enhance a causal analysis.</p>
</div>
<div class="readable-text" id="p8">
<h2 class="readable-text-h2" id="sigil_toc_id_320"><span class="num-string">13.1</span> LLMs as a causal knowledgebase</h2>
</div>
<div class="readable-text" id="p9">
<p>As I’ve mentioned several times in this book, in causal modeling, we don’t model the data, we model the data generating process (DGP). In other words, causal modeling requires causal information about the DGP that we can use as assumptions that enable identification of our causal queries. Cutting-edge LLMs are excellent resources for sourcing and validating that causal information. In this section, I’ll demonstrate some proven use cases for using LLMs to query causal information that augments a causal analysis.</p>
</div>
<div class="readable-text intended-text" id="p10">
<p>The following results come from OpenAI’s LLM GPT-4 via the ChatGPT interface. Because of the random generation from the model, and OpenAI’s periodic updates to the model and ChatGPT software, your results will vary from mine to some degree.</p>
</div>
<div class="readable-text" id="p11">
<h3 class="readable-text-h3" id="sigil_toc_id_321"><span class="num-string">13.1.1</span> Building a causal DAG</h3>
</div>
<div class="readable-text" id="p12">
<p>Let’s start with using the LLM to augment the first workflow we discussed in this book: building the causal DAG. Specifying assumptions in the form of a causal DAG can be daunting. What if your DAG is wrong? Modern LLMs are invaluable as a way of leveraging public expert knowledge to build a DAG.</p>
</div>
<div class="readable-text prompt" id="p13">
<p><strong class="prompt-head-image"><img alt="image" height="60px" src="../Images/init.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>You are a helpful causal analysis assistant. I’m interested in modeling the relationship between smoking and lung cancer. Provide me a causal DAG that captures this causal relationship.</p>
</div>
<div class="readable-text response" id="p15">
<p><strong><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong><span class="aframe-location"/></strong>Certainly! A Directed Acyclic Graph (DAG) is a powerful tool to represent causal relationships. Here’s a basic causal DAG to represent the relationship between smoking and lung cancer:</p>
</div>
<ol class="response">
<li class="readable-text buletless-item" id="p16"> Nodes: 
    <ul>
<li> <strong>S</strong>: Smoking behavior (e.g., smoker, non-smoker, or frequency of smoking) </li>
<li> <strong>LC</strong>: Lung Cancer (presence or absence) </li>
</ul></li>
<li class="readable-text buletless-item" id="p17"> Directed Edge: 
    <ul>
<li> A directed edge from <strong>S</strong> to <strong>LC</strong> represents that smoking behavior can cause or influence the occurrence of lung cancer. </li>
</ul></li>
<li class="readable-text buletless-item" id="p18"> Potential Confounders: 
    <ul>
<li class="buletless-item"> Other nodes can be introduced into the DAG to account for potential confounders. Some potential confounders might be: 
      <ol style="list-style: lower-alpha">
<li> <strong>G</strong>: Genetics (a family history of lung cancer) </li>
<li> <strong>A</strong>: Age (older age might be associated with a higher risk) </li>
<li> <strong>E</strong>: Environmental exposure (like asbestos, pollution, etc.) </li>
<li> <strong>O</strong>: Other health behaviors (like diet, exercise) </li>
</ol></li>
</ul></li>
<li class="readable-text buletless-item" id="p19"> Potential Mediators: 
    <ul>
<li> Mediators are intermediate variables through which a cause exerts its effect on an outcome. </li>
<li class="buletless-item"> For instance, one could hypothesize that smoking might cause lung damage, which in turn causes lung cancer. In this case: 
      <ol style="list-style: lower-alpha">
<li> <strong>LD</strong>: Lung Damage </li>
</ol></li>
</ul></li>
</ol>
<div class="readable-text response" id="p20">
<p>The DAG might look like this with the confounders and the mediator:</p>
</div>
<ol class="response">
<li class="readable-text" id="p21"> <strong>S </strong>&lt;-<strong> G</strong> –&gt; <strong>LC</strong> </li>
<li class="readable-text" id="p22"> <strong>S </strong>&lt;-<strong> A</strong> –&gt; <strong>LC</strong> </li>
<li class="readable-text" id="p23"> <strong>S </strong>&lt;-<strong> E</strong> –&gt; <strong>LC</strong> </li>
<li class="readable-text" id="p24"> <strong>S </strong>&lt;-<strong> O</strong> –&gt; <strong>LC</strong> </li>
<li class="readable-text" id="p25"> <strong>S</strong> –&gt; <strong>LC</strong> </li>
<li class="readable-text" id="p26"> <strong>S</strong> –&gt; <strong>LD</strong> –&gt; <strong>LC</strong> </li>
</ol>
<div class="readable-text response" id="p27">
<p>In the process of modeling, it’s important to: </p>
</div>
<ul class="response">
<li class="readable-text" id="p28"> <strong>Identify all potential confounders</strong> to avoid biased results. <strong>Remember that correlation does not imply causation</strong>. Just because two nodes are correlated does not mean one causes the other. </li>
<li class="readable-text" id="p29"> <strong>Validate your DAG with domain experts</strong>. They can provide insights into relationships you might not have considered. </li>
<li class="readable-text" id="p30"> <strong>Use data</strong> to test and validate the causal relationships in your DAG, e.g., through controlled experiments or statistical methods like instrumental variables or propensity score matching when experiments are not feasible. </li>
<li class="readable-text" id="p31"> Bear in mind that this is a simplified DAG for the relationship between smoking and lung cancer. Depending on the granularity and depth of your study, you might need to adjust and expand this framework. </li>
</ul>
<div class="readable-text" id="p32">
<p>Continuing, I can prompt the LLM to provide potential instrumental variables.</p>
</div>
<div class="readable-text prompt" id="p33">
<p><strong class="prompt-head-image"><img alt="image" height="60px" src="../Images/init.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>Suggest two good candidates for instrumental variables. Be brief in your response.</p>
</div>
<div class="readable-text response" id="p35">
<p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><span class="aframe-location"/>Certainly. For the relationship between smoking and lung cancer, two potential instrumental variables (IVs) could be:</p>
</div>
<ol class="response">
<li class="readable-text" id="p36"> <strong>Cigarette Tax or Price</strong>: If certain regions or periods have higher cigarette prices or taxes, this can influence smoking behavior without directly influencing lung cancer rates except through the effect of reduced smoking. The assumption here is that the only way cigarette prices could affect lung cancer is by influencing smoking behavior. </li>
<li class="readable-text" id="p37"> <strong>Anti-Smoking Campaigns or Legislation</strong>: Introduction or intensity of anti-smoking campaigns or legislation in certain regions or times can be used as an instrument. If these campaigns or laws are exogenously determined (not based on current lung cancer rates), they can affect smoking rates without having a direct effect on lung cancer, except through their impact on smoking behavior. </li>
</ol>
<div class="readable-text response" id="p38">
<p>For both IVs, it’s crucial to validate the two key assumptions of instrument validity:</p>
</div>
<ol class="response">
<li class="readable-text" id="p39"> The instrument is related to the exposure (e.g., cigarette tax affects smoking behavior). </li>
<li class="readable-text" id="p40"> The instrument affects the outcome (e.g., lung cancer) only through its effect on the exposure and not through other pathways. </li>
</ol>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p41">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Prompting best practice</h5>
</div>
<div class="readable-text" id="p42">
<p>The prompt preface “You are a helpful causal analysis assistant” is an example of what is sometimes called a “system prompt.” System prompts provide general context about what is expected from the LLM. System prompting, and other prompt engineering techniques (e.g., chain-of-thought prompting, in-context learning) are best practices for getting good generations from LLMs. For the sake of brevity, I’ll use system prompts but omit some of the other prompt engineering best practices in this section—it is smart to include them in practical settings.</p>
</div>
</div>
<div class="readable-text" id="p43">
<p>Continuing to interact with the LLM in this way, perhaps through multiple sessions, you can come up with a candidate DAG that makes sense to you and your stakeholders. Next, you can prompt the LLM to give you code to run an analysis.</p>
</div>
<div class="readable-text" id="p44">
<h3 class="readable-text-h3" id="sigil_toc_id_322"><span class="num-string">13.1.2</span> Generating code for DAGs, models, and causal analyses</h3>
</div>
<div class="readable-text" id="p45">
<p>Modern LLMs are trained on vast corpora of code from a wide variety of programming languages. This greatly speeds up causal analyses, particularly if the libraries you’re using are represented in the training data. For example, we can prompt ChatGPT to implement its suggested lung cancer DAG in DoWhy and estimate the causal effect.</p>
</div>
<div class="readable-text prompt" id="p46">
<p><strong class="prompt-head-image"><img alt="image" height="60px" src="../Images/init.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>Provide me with code that uses the DoWhy library in Python to implement the causal DAG that you suggested above. Include cigarette price as an instrumental variable and provide code for estimating the causal effect.</p>
</div>
<div class="readable-text response" id="p48">
<p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong><span class="aframe-location"/>Certainly! Here’s a basic example using the DoWhy library in Python for implementing the causal DAG for the relationship between smoking and lung cancer, using cigarette price as an instrumental variable.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p50">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Setting up your environment</h5>
</div>
<div class="readable-text" id="p51">
<p>The code in this chapter was written with DoWhy version 0.11, Transformers version 4.38.2, Accelerate version 0.28.0, pandas 2.0.3, NumPy 1.25.2, and pyro-ppl (Pyro) version 1.9.0. Visit the chapter notes at <a href="https://www.altdeep.ai/p/causalaibook">https://www.altdeep.ai/p/causalaibook</a> for a link to a notebook containing the chapter’s code.</p>
</div>
</div>
<div class="readable-text" id="p52">
<p>Chat GTP then went on to generate the following code.</p>
</div>
<div class="browsable-container listing-container" id="p53">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 13.1</span> DoWhy code generated by ChatGPT (verbatim)</h5>
<div class="code-area-container">
<pre class="code-area">import numpy as np
import pandas as pd
import dowhy
from dowhy import CausalModel
from dowhy.datasets import linear_dataset
from dowhy.do_why import CausalModel   <span class="aframe-location"/> #1

# Let's create a sample dataset for illustration purposes
# Normally, you'd import your own dataset: data = pd.read_csv('your_dataset.csv')
n_points = 1000
data = pd.DataFrame({
    "S": np.random.binomial(n=1, p=0.5, size=n_points),    <span class="aframe-location"/> #2
    "LC": np.random.binomial(n=1, p=0.5, size=n_points),   <span class="aframe-location"/> #3
    "Price": np.random.normal(loc=5, scale=1, size=n_points),    <span class="aframe-location"/> #4
})

# Create a causal DAG
model=CausalModel(
        data = data,
        treatment='S',
        outcome='LC',
        common_causes=['G', 'A', 'E', 'O'],  # Potential confounders
        instruments=['Price']  # Instrumental Variable
)

# Identify the causal effect
identified_estimand = model.identify_effect(proceed_when_unidentifiable=True)

# Estimate the causal effect using Instrumental Variable method
estimate = model.estimate_effect(identified_estimand,
                                 method_name="iv.instrumental_variable",
                                 method_params={'iv_instrument_name': 'Price'})

# Print the causal effect estimate
print(estimate)</pre>
<div class="code-annotations-overlay-container">
     #1 This line is wrong, but the code runs if you remove it.
     <br/>#2 Simulate smoker status: 0 for non-smoker, 1 for smoker.
     <br/>#3 Simulate lung cancer status: 0 for no lung cancer, 1 for lung cancer.
     <br/>#4 Simulate cigarette price with a mean price of 5.
     <br/>
</div>
</div>
</div>
<div class="readable-text response" id="p54">
<p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><span class="aframe-location"/>This is a simplified example. In a real-world application: </p>
</div>
<ol class="response">
<li class="readable-text" id="p55"> Your data would be imported rather than simulated as above. </li>
<li class="readable-text" id="p56"> Ensure that your DAG correctly models the relationships based on domain knowledge. </li>
<li class="readable-text" id="p57"> Assumptions behind the instrumental variables need to be rigorously checked. </li>
<li class="readable-text" id="p58"> Further analytical steps (e.g., statistical tests, checking robustness) would be necessary. </li>
</ol>
<div class="readable-text" id="p59">
<p>When you generate code, you may get errors. For example, DoWhy has gone through several iterations, and there is nothing forcing the LLM to keep up with the latest version of the library. In this case, there is an error in the line <code>from dowhy.do_why import CausalModel</code>. There is no module called <code>do_why</code>, and <code>CausalModel</code> is already imported in a previous line. We’ll address errors produced by the LLM in section 13.1.7. For now, if you delete that line, the code will run.</p>
</div>
<div class="readable-text intended-text" id="p60">
<p>ChatGPT will work well with pgmpy and Pyro, the libraries we’ve worked with extensively in the book. As an exercise, I suggest taking one of the models you built in pgmpy, and prompting the LLM to convert it to a model in Pyro.</p>
</div>
<div class="readable-text intended-text" id="p61">
<p>Next, we’ll look at what the LLM can tell us beyond the structure of the DAG.</p>
</div>
<div class="readable-text" id="p62">
<h3 class="readable-text-h3" id="sigil_toc_id_323"><span class="num-string">13.1.3</span> Explanations and mechanism</h3>
</div>
<div class="readable-text" id="p63">
<p>The advantage of DAGs is that graphs are an intuitive way to represent what causes what. But they do not tell us anything about the nature of the mechanism by which a cause drives an effect.</p>
</div>
<div class="readable-text intended-text" id="p64">
<p>Structural causal models provide a mathematical way of characterizing the mechanism underlying a causal relationship. Characterizing mechanisms in mathematical terms makes sense in natural sciences, where we have plenty of equations describing natural laws. It is harder in, for example, business domains where we lack equations beyond mere approximations (what is the equation relating ad spending and engagement?).</p>
</div>
<div class="readable-text intended-text" id="p65">
<p>LLMs leverage common and expert knowledge to provide mechanistic explanations that go beyond the DAG but that are in a form more accessible than formal math.</p>
</div>
<div class="readable-text prompt" id="p66">
<p><strong class="prompt-head-image"><img alt="image" height="60px" src="../Images/init.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>You are a helpful causal explanation generator. Provide a characterization of the causal relationship between ad spending and brand awareness. Be concise enough to fit a single slide in a slide deck.</p>
</div>
<div class="readable-text response" id="p68">
<p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong><span class="aframe-location"/>Causal Relationship Between Ad Spending and Brand Awareness</p>
</div>
<ul class="response">
<li class="readable-text" id="p69"> <strong>Direct Causality</strong>: As ad spending increases, brand awareness typically rises. 
    <ol style="list-style: lower-alpha">
<li> <strong>Mechanism</strong>: Larger ad spends mean more frequent and widespread exposure of the brand to the target audience. </li>
</ol></li>
<li class="readable-text" id="p70"> <strong>Diminishing Returns</strong>: After a certain point, additional ad spending might not lead to proportionally higher brand awareness. 
    <ol style="list-style: lower-alpha">
<li> <strong>Reason</strong>: Market saturation, ad fatigue, or redundancy in audience reach. </li>
</ol></li>
<li class="readable-text" id="p71"> <strong>External Factors</strong>: Ad efficiency isn’t solely based on spend. 
    <ol style="list-style: lower-alpha">
<li> <strong>Influences</strong>: Ad quality, placement, target audience relevancy, competition, and market conditions can affect the outcome. </li>
</ol></li>
</ul>
<div class="readable-text" id="p72">
<p>Consider that some counterfactual queries can be identified, with simple categorical assumptions about the functional relationship between the cause and effect. For example, the probability of necessity is identifiable from intervention data and partially identifiable from observational data if the relationship between <em>X</em> and <em>Y</em> is monotonic:</p>
</div>
<div class="readable-text prompt" id="p73">
<p><strong class="prompt-head-image"><img alt="image" height="60px" src="../Images/init.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>Is this a monotonic relationship? Limit your answer to one sentence.</p>
</div>
<div class="readable-text response" id="p75">
<p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><span class="aframe-location"/><span class="aframe-location"/>Yes, the relationship between ad spending and brand awareness is generally monotonic, as increased ad spending will not decrease brand awareness, though the rate of increase might diminish.</p>
</div>
<div class="readable-text" id="p77">
<p>We needn’t limit ourselves to one explanation; we can use the LLM as a generator of explanatory hypotheses. Consider, for example<em>, abductive reasoning tasks </em>where one infers likely causes of a particular observed set of effects (e.g., in science, criminal investigation, and medical diagnosis).</p>
</div>
<div class="readable-text intended-text" id="p78">
<p>To illustrate, let’s pose a modified version of a medical diagnostic question sourced from the US Medical License Exam to ChatGPT. I’ll prompt for multiple hypotheses about the cause of the observed symptoms, which is what a doctor would call a <em>differential diagnosis</em>:</p>
</div>
<div class="readable-text prompt" id="p79">
<p><strong class="prompt-head-image"><img alt="image" height="60px" src="../Images/init.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>You are a causal explanation assistant. Your job is to generate 3 one sentence-long hypotheses that explain the observations. This is a medical diagnosis example.</p>
</div>
<div class="readable-text prompt" id="p81">
<p>Observations: A 48-year-old woman presents with diplopia, ocular pain, amenorrhea, hot flashes, weight loss, tachycardia, hypertension, moist palms, enlarged thyroid, prominent eyes, lid retraction, conjunctival injection, and inability to converge eyes. Visual acuity is 20/20 bilaterally. </p>
</div>
<ol class="response">
<li class="readable-text" id="p82"><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong> <strong><span class="aframe-location"/></strong>The patient may have Graves’ disease, an autoimmune disorder affecting the thyroid, leading to hyperthyroidism and its associated symptoms, including ocular manifestations. </li>
</ol>
<ol class="response faux-ol-li" style="list-style: none;">
<li class="readable-text faux-li has-faux-ol-li-counter" id="p83"><span class="faux-ol-li-counter">2. </span> The woman’s symptoms, including amenorrhea and hot flashes, suggest a possible concomitant early menopause or another endocrine disorder affecting her reproductive hormones. </li>
<li class="readable-text faux-li has-faux-ol-li-counter" id="p84"><span class="faux-ol-li-counter">3. </span> The ocular symptoms, including diplopia, lid retraction, and inability to converge eyes, hint at a potential thyroid eye disease or orbital myopathy. </li>
</ol>
<div class="readable-text" id="p85">
<p>One of the challenges of causal modeling is working with domain experts to encode domain assumptions into the structure of the model. These types of generated explanations and hypotheses are valuable assets for communicating with domain experts who might agree with, or better yet, iterate upon these generated explanations.</p>
</div>
<div class="readable-text" id="p86">
<h3 class="readable-text-h3" id="sigil_toc_id_324"><span class="num-string">13.1.4</span> The causal frame problem and AI alignment </h3>
</div>
<div class="readable-text" id="p87">
<p>Given a query of interest, we have to decide what variables are relevant to the problem. Philosoper and AI researcher Clark Glymour does a good job of describing the challenge behind this task: “Given an enormous amount of stuff, and some task to be done using some of the stuff, what is the relevant stuff for the task?”<a href="#footnote-305"><sup class="footnote-reference" id="footnote-source-1">1</sup></a></p>
</div>
<div class="readable-text intended-text" id="p88">
<p>LLMs are useful at establishing a <em>causal frame</em>—the problem of selecting the causal variables relevant to a causal query. In chapter 3’s discussion on building the DAG, I mentioned <em>causal sufficiency </em>as a d-separation–based criteria for selecting variables for inclusion in the DAG. But when we prompted the LLM to provide a DAG containing variables relevant to smoking’s effect on lung cancer, it is highly doubtful that the LLM was using causal sufficiency to select those variables. More likely, it was using common knowledge found in its training data about this relationship. That knowledge was sufficient to provide a reasonably minimal set of relevant variables.</p>
</div>
<div class="readable-text intended-text" id="p89">
<p>To illustrate the utility of this ability, consider the following ChatGPT interaction:</p>
</div>
<div class="readable-text prompt" id="p90">
<p><strong class="prompt-head-image"><img alt="image" height="60px" src="../Images/init.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>You are a helpful causal reasoning assistant. Consider the following story. </p>
</div>
<div class="readable-text prompt" id="p92">
<p>Jake is on an office network. There was an open shared folder on the server. Jake is transferring files across the network and accidentally clicks on the folder, starting a process that consumes a significant amount of bandwidth. This causes the network to slow down. How responsible is Jake for the network slowdown?</p>
</div>
<div class="readable-text response" id="p93">
<p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>Based on the provided story, let’s analyze the situation using causal reasoning:</p>
</div>
<ol class="response">
<li class="readable-text" id="p94"> <strong>Direct Cause</strong>: Jake started a process that consumes a significant amount of bandwidth by accidentally clicking on a file. </li>
<li class="readable-text" id="p95"> <strong>Effect</strong>: The network slowed down due to the consumed bandwidth. </li>
</ol>
<div class="readable-text response" id="p96">
<p>Given these facts, we can establish that Jake’s action was the direct cause of the network slowdown.</p>
</div>
<div class="readable-text response" id="p97">
<p>However, responsibility can be considered from different dimensions:</p>
</div>
<ol class="response">
<li class="readable-text" id="p98"> <strong>Intent</strong>: Jake’s action was accidental. He did not intend to slow down the network. </li>
<li class="readable-text" id="p99"> <strong>Prevention Mechanisms</strong>: The story does not specify if there were any warnings or user prompts before starting such a process that consumes significant bandwidth. If there were no warnings, one could argue that the system was not designed to prevent such accidental actions. </li>
<li class="readable-text" id="p100"> <strong>Knowledge &amp; Training</strong>: Was Jake trained or informed about the implications of transferring large files or about the particular file that consumes a lot of bandwidth? If not, his knowledge gap contributed to the accident. </li>
<li class="readable-text" id="p101"> <strong>System Design</strong>: The open shared folder on the server was accessible to Jake. If the file was known to be a bandwidth hog, it’s worth asking why such a file was placed in an open shared folder without any warnings or restrictions. </li>
</ol>
<div class="readable-text response" id="p102">
<p>To directly answer the question: Jake is responsible for the network slowdown in the sense that his action was the direct cause. However, the degree of his responsibility might be mitigated if there were systemic design flaws, lack of training, or absence of preventive measures. If other factors, like the ones mentioned above, played a role in the incident, then the responsibility would be shared among Jake and those other factors.</p>
</div>
<div class="readable-text" id="p103">
<p>Here, the response includes the systemic design flaws, lack of training, or absence of preventive measures as factors relevant to the causal frame, despite the fact that I made no mention of these factors in the prompt.</p>
</div>
<div class="readable-text intended-text" id="p104">
<p>More importantly, consider the many factors the LLM could have included but did not: perhaps Jake’s mouse had a physical glitch that caused him to click on the file, or perhaps a hacker removed security features on that file, or any other possible mitigating factors. A human would likely exclude these from the causal frame unless explicitly mentioned, and so does the LLM. </p>
</div>
<div class="readable-text intended-text" id="p105">
<p>If humans are already good at establishing the causal frame, why would we care whether the LLM can do so? Consider the problem of root cause analysis from network logs. Suppose the same story about Jake were rewritten in the form of network logs, such as in the following:</p>
</div>
<div class="browsable-container listing-container" id="p106">
<div class="code-area-container">
<pre class="code-area">Aug 22 14:02:34 User "Jake" logged in from 192.168.0.105.
Aug 22 14:03:12 Shared folder accessed by user "Jake" at //SERVER/shared-folder.
Aug 22 14:03:20 File transfer initiated by "Jake" from local: /home/jake/documents to remote: //SERVER/shared-folder/documents.
Aug 22 14:03:25 WARNING: Unusual network activity detected. User "Jake" launched file "heavy_process.exe" on //SERVER/shared-folder.
Aug 22 14:03:30 Network performance alert: Bandwidth usage spikes at 95%.
Aug 22 14:04:00 Network slowdown detected: Packet loss rate 35%.</pre>
</div>
</div>
<div class="readable-text" id="p107">
<p>This is harder for a human to parse than the natural language story, even for an experienced network administrator. Further, there would likely be many other network events unrelated to the events of this narrative that the human would have to read through and disregard. But the LLM has plenty of network logs in its training data, and extracting relevant events from a longer body of text is an established capability of LLMs.</p>
</div>
<div class="readable-text intended-text" id="p108">
<p>In most of this book, we’ve focused on using causal models to infer objective causal truth about the world, such as whether smoking causes lung cancer. In this example, the objective truth, that Jake caused the network slowdown, is trivial. The question of how responsible Jake is for the slowdown is highly subjective. But this is still a valuable question to answer, especially if you are trying to find a root cause in a large network log file.</p>
</div>
<div class="readable-text intended-text" id="p109">
<p><em>AI alignment</em> is the task of making sure an AI system’s behaviors are consistent with human goals and preferences. Here, we want a specific type of AI alignment. We want the LLM to go beyond confirming obvious causal facts to aligning with the ways humans tend to make causal judgments. In this case, I asked the LLM an open question about Jake’s responsibility, and it gave a response that many humans would agree with. I could also have instructed the LLM to follow certain criteria when generating a judgement. For example, an attorney could prompt the LLM to apply certain legal theories and case law in generating a legal opinion.</p>
</div>
<div class="readable-text intended-text" id="p110">
<p>Next, we’ll look at how to use the LLM to understand and contextualize more nuanced causal concepts.</p>
</div>
<div class="readable-text" id="p111">
<h3 class="readable-text-h3" id="sigil_toc_id_325"><span class="num-string">13.1.5</span> Understanding and contextualizing causal concepts</h3>
</div>
<div class="readable-text" id="p112">
<p>It can be challenging to understand formal causal concepts and apply them within your domain. The LLM can serve as a tool for explaining and contextualizing these concepts. Let’s illustrate with the counterfactual concept of <em>necessary causation</em>.</p>
</div>
<div class="readable-text prompt" id="p113">
<p><strong class="prompt-head-image"><img alt="image" height="60px" src="../Images/init.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>You are a helpful explainer of causal inference topics. Provide a one paragraph explanation of the concept of necessary causation.</p>
</div>
<div class="readable-text response" id="p115">
<p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><span class="aframe-location"/>Certainly! Necessary causation refers to a causal relationship where the cause is a necessary condition for the effect to occur. In other words, without the presence of the cause, the effect cannot happen. However, the presence of the cause does not guarantee the effect will occur, as there may be other conditions that also need to be met. Think of it like a key to a lock: the key (cause) is necessary to open the lock (effect), but just having the key doesn’t mean the lock will open automatically; you still need to insert and turn it. If the lock remains locked without the key, then the key is a necessary cause for unlocking.</p>
</div>
<div class="readable-text" id="p117">
<p>A cutting-edge LLM would be trained on a wide variety of theory and research from across domains. It can be useful to contextualize an idea within various domains.</p>
</div>
<div class="readable-text prompt" id="p118">
<p><strong class="prompt-head-image"><img alt="image" height="60px" src="../Images/init.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>In one paragraph, explain how necessary causation is conceptualized in legal theory.</p>
</div>
<div class="readable-text response" id="p120">
<p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>In legal theory, necessary causation, often termed “but-for causation,” is a foundational concept used to establish a direct link between a defendant’s actions and the harm suffered by a plaintiff. For an action to be a but-for cause of an outcome, the outcome would not have occurred in the absence of the action. In other words, if one can say “but for the defendant’s action, the harm would not have occurred,” then necessary causation is established. However, it’s worth noting that while but-for causation is a critical component in many legal contexts, it’s not always sufficient on its own to establish liability. Other factors, such as foreseeability and proximate cause, often come into play to determine the full scope of legal responsibility.</p>
</div>
<div class="readable-text prompt" id="p122">
<p><strong class="prompt-head-image"><img alt="image" height="60px" src="../Images/init.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>From the perspective of cognitive science, explain in one brief paragraph the role necessary causation plays in human judgments.</p>
</div>
<div class="readable-text response" id="p124">
<p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>From a cognitive science perspective, necessary causation plays a pivotal role in human judgments and decision-making. Our brains are wired to seek patterns and causal relationships in the environment to predict and navigate future events. When we perceive an event as a necessary cause of another, it simplifies our cognitive processing by creating a deterministic link between the two events. This means that when we encounter the cause, we can anticipate the effect, allowing for more efficient planning and response. However, this predisposition can also lead to cognitive biases, where we might mistakenly attribute an effect to a perceived necessary cause, even when other contributing factors are at play.</p>
</div>
<div class="readable-text prompt" id="p126">
<p><strong class="prompt-head-image"><img alt="image" height="60px" src="../Images/init.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>From the perspective of philosophy, explain in only one sentence how Kant and Hume differed in their beliefs about necessary causation.</p>
</div>
<div class="readable-text response" id="p128">
<p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>Kant believed in the existence of a priori synthetic knowledge and that our minds impose causal order on our experiences, while Hume argued that causation is not a feature of the world itself but a habit of thought based on the regular succession of events.</p>
</div>
<div class="readable-text" id="p130">
<p>Now that we’ve unpacked the concept of necessary causation, let’s apply it to a practical problem in our domain of interest. Suppose you are a data scientist at Netflix:</p>
</div>
<div class="readable-text prompt" id="p131">
<p><strong class="prompt-head-image"><img alt="image" height="60px" src="../Images/init.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>Give me an example of necessary causation that involves a Netflix user unsubscribing from Netflix.</p>
</div>
<div class="readable-text response" id="p133">
<p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>A necessary causation for a Netflix user unsubscribing from Netflix might be the exclusive content that is only available on the platform. If a particular show or movie that the user is deeply interested in is removed from Netflix while remaining available somewhere else, then the removal of that content becomes a necessary cause for that user’s decision to unsubscribe. Without the removal of that content, the user might not have unsubscribed.</p>
</div>
<div class="readable-text" id="p135">
<p>Now that the LLM has helped us understand necessary causation, we can get its help in formalizing the concept.</p>
</div>
<div class="readable-text" id="p136">
<h3 class="readable-text-h3" id="sigil_toc_id_326"><span class="num-string">13.1.6</span> Formalization of causal queries</h3>
</div>
<div class="readable-text" id="p137">
<p>For humans, natural language is the medium of our causal reasoning. But we need to convert that natural language into formal terms so that we can apply mathematical and algorithmic analysis. This can be a challenge. Fortunately, LLMs can help greatly in this respect.</p>
</div>
<div class="readable-text intended-text" id="p138">
<p>For example, in chapter 8, I introduced several examples of the language of parallel world counterfactuals and how to convert them into counterfactual notation. Necessary causation, more specifically the <em>probability of necessity</em>,<em> </em>was one of these counterfactuals. Let’s put this task to the LLM in our Netflix context:</p>
</div>
<div class="readable-text prompt" id="p139">
<p><strong class="prompt-head-image"><img alt="image" height="60px" src="../Images/init.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>You are a helpful causal inference assistant. I’m considering a counterfactual inference problem in the context of Netflix. I have the following query: “There is a set of users who unsubscribed after content was removed from the platform. What is the probability a given member of this set of users would not have unsubscribed had the content not been removed?” Write this query as a probabilistic expression with Pearl’s counterfactual notation.</p>
</div>
<div class="readable-text response" id="p141">
<p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>Certainly! Pearl’s counterfactual notation offers a concise way to express such queries.</p>
</div>
<div class="readable-text response" id="p142">
<p>Let’s define some variables:</p>
</div>
<ol class="response">
<li class="readable-text" id="p143"> <em>X</em> =removal of the content (binary, 1 if removed, 0 if not removed). </li>
<li class="readable-text" id="p144"> <em>Y</em> =whether user unsubscribes (binary, 1 if user unsubscribes, 0 if they did not unsubscribe). </li>
</ol>
<div class="readable-text response" id="p145">
<p>Your query is essentially asking for the probability that a user would not have subscribed, had the content not been removed, given that it was removed and they did unsubscribe.</p>
</div>
<div class="readable-text response" id="p146">
<p>Using Pearl’s counterfactual notation, this can be written as:<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p147">
<img alt="figure" height="22" src="../Images/ness-ch13-eqs-0x.png" width="260"/>
</div>
<div class="readable-text response" id="p148">
<p>Where:</p>
</div>
<ul class="response">
<li class="readable-text" id="p149"> <em>Y</em><sub><em>x</em></sub><sub>=0</sub> is the counterfactual outcome for <em>Y</em> had <em>X</em> been set to 0 (i.e., the content is not removed). </li>
<li class="readable-text" id="p150"> <em>X</em>=1 denotes that the content was removed. </li>
<li class="readable-text" id="p151"> <em>Y</em>=1 denotes that the user unsubscribed. </li>
</ul>
<div class="readable-text response" id="p152">
<p>So, <em>P</em>(<em>Y</em><em><sub>x=</sub></em><sub>0 </sub>= 0| <em>X </em>= 1, <em>Y </em>= 1) is the probability that a user would not have unsubscribed had the content not been removed, given that it was removed and they did unsubscribe.</p>
</div>
<div class="readable-text" id="p153">
<p>The LLM can take us from conceptualization to formalization. But beware, as it can also “hallucinate.”</p>
</div>
<div class="readable-text" id="p154">
<h3 class="readable-text-h3" id="sigil_toc_id_327"><span class="num-string">13.1.7</span> Beware: LLMs hallucinate</h3>
</div>
<div class="readable-text" id="p155">
<p>We’ve seen that LLMs can aid in code generation. Let’s ask for code that implements the probability of necessity.</p>
</div>
<div class="readable-text prompt" id="p156">
<p><strong class="prompt-head-image"><img alt="image" height="60px" src="../Images/init.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>Provide Python code that, for a given pair of treatment and outcome variables, calculates the counterfactual query “probability of necessity.”</p>
</div>
<div class="readable-text response" id="p158">
<p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>The Probability of Necessity (PN) is the proportion of cases where the treatment is present, and the outcome is achieved ONLY when the treatment is present. In simpler terms, it indicates the probability that the outcome occurs because of the treatment.</p>
</div>
<div class="readable-text response" id="p159">
<p>Given that definition, you can calculate PN using the following formula:<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p160">
<img alt="figure" height="55" src="../Images/ness-ch13-eqs-1x.png" width="664"/>
</div>
<div class="readable-text response" id="p161">
<p>Where:</p>
</div>
<ul class="response">
<li class="readable-text" id="p162"> <em>P</em>(<em>Treatment</em>∩<em>Outcome</em>) is the probability of both the treatment and outcome occurring. </li>
<li class="readable-text" id="p163"> <em>P</em>(¬<em>Treatment</em>∩<em>Outcome</em>) is the probability of the outcome occurring but not the treatment. </li>
</ul>
<div class="readable-text" id="p164">
<p>The response goes on to provide Python code that is well formatted, well commented, and compiles. <em>But this answer is wrong</em>. This is what is commonly called a “hallucination,” a convincing lie. This example is especially convincing—that formula is true <em>only</em> when the relationship between the treatment and outcome is monotonic, when there are no confounders, and when <em>P</em>(<em>Treatment</em>) = <em>P</em>(<em>Outcome</em>). Those assumptions are so strong that we can’t forgive ChatGPT for not mentioning them. Other generations from the model were even more off base.</p>
</div>
<div class="readable-text intended-text" id="p165">
<p>We can use the LLM to augment our causal analysis, particularly with common and expert causal knowledge as well as generated code. But hallucination means this isn’t a panacea. We need to rely on our own causal expertise to spot when hallucination occurs and understand when it threatens the quality of our analysis.</p>
</div>
<div class="readable-text intended-text" id="p166">
<p>To understand why this hallucination occurred, let’s first examine how LLMs work.</p>
</div>
<div class="readable-text" id="p167">
<h2 class="readable-text-h2" id="sigil_toc_id_328"><span class="num-string">13.2</span> A causality-themed LLM primer</h2>
</div>
<div class="readable-text" id="p168">
<p>To understand how to deploy LLMs for causal applications, it is important to understand how they work, as well as their limitations. This section provides a quick high-level causality-themed tour of the core ideas.</p>
</div>
<div class="readable-text" id="p169">
<h3 class="readable-text-h3" id="sigil_toc_id_329"><span class="num-string">13.2.1</span> A probabilistic ML view of LLMs</h3>
</div>
<div class="readable-text" id="p170">
<p>In the context of LLMs, a “token” refers to a sequence of characters that the model reads, which can be as short as one character or as long as one word. Tokens are the units into which input text is divided into manageable pieces for the model.</p>
</div>
<div class="readable-text intended-text" id="p171">
<p>Hugging Face’s Transformers library has a publicly available version of GPT-2, which is far inferior to cutting-edge models but has a similar <em>Transformer</em> architecture. The Transformer architecture is a type of deep learning model designed to process and understand text and other sequential data, by focusing on the relationships between words in a sentence regardless of their position. Let’s tokenize the expression “Can LLMs reason counterfactually?”</p>
</div>
<div class="browsable-container listing-container" id="p172">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 13.2</span> Viewing example tokens that an LLM operates upon</h5>
<div class="code-area-container">
<pre class="code-area">from transformers import GPT2Tokenizer
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')  <span class="aframe-location"/> #1
tokens = tokenizer.tokenize("Can LLMs reason counterfactually?")   <span class="aframe-location"/> #2
print(tokens)    <span class="aframe-location"/> #3</pre>
<div class="code-annotations-overlay-container">
     #1 Initialize the GPT-2 tokenizer.
     <br/>#2 Tokenize the sequence.
     <br/>#3 Print out the tokens.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p173">
<p>This prints out the following tokens:</p>
</div>
<div class="browsable-container listing-container" id="p174">
<div class="code-area-container">
<pre class="code-area">['Can', 'ĠLL', 'Ms', 'Ġreason', 'Ġcounter', 'fact', 'ually', '?']</pre>
</div>
</div>
<div class="readable-text" id="p175">
<p>The “Ġ” corresponds to a space. Note that punctuation marks are tokens, and that words like “counterfactual” are broken up into multiple tokens. Each token corresponds to an integer indexing the token in a large “vocabulary.” GPT-2 has a vocabulary size of 50,257.</p>
</div>
<div class="browsable-container listing-container" id="p176">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 13.3</span> Converting tokens to integers</h5>
<div class="code-area-container">
<pre class="code-area">input_ids = tokenizer.encode(    <span class="aframe-location"/> #1
    "Can LLMs reason counterfactually?",    #1
    return_tensors='pt'    #1
)   #1
print(input_ids)</pre>
<div class="code-annotations-overlay-container">
     #1 “Encode” the tokens into integers that index the token in a list of tokens called the “vocabulary”.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p177">
<p>This <em>encodes </em>the tokens into a sequence of integers:</p>
</div>
<div class="browsable-container listing-container" id="p178">
<div class="code-area-container">
<pre class="code-area">tensor([[ 6090, 27140, 10128,  1738,  3753, 22584,   935,    30]])</pre>
</div>
</div>
<div class="readable-text" id="p179">
<p>The Transformer architecture works with these numeric values.</p>
</div>
<div class="readable-text intended-text" id="p180">
<p>LLMs define a joint probability distribution on sequences of tokens. For the phrase “Can LLMs reason counterfactually?” the model defines a probability distribution:<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p181">
<img alt="figure" height="23" src="../Images/ness-ch13-eqs-2x.png" width="682"/>
</div>
<div class="readable-text" id="p182">
<p>The models will also consider the chances that this sequence ended at the question mark, rather than continuing. For that, the LLM’s vocabulary includes a special token to mark the end of a sequence. For GPT-2, this is token is <code>&lt;|endoftext|&gt;</code>:<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p183">
<img alt="figure" height="60" src="../Images/ness-ch13-eqs-3x.png" width="537"/>
</div>
<div class="readable-text" id="p184">
<p>Further, autoregressive LLMs, such as the GPT and Llama series of Transformer models, model text in the order of the text sequence, so they factorize this joint probability as follows:<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p185">
<img alt="figure" height="60" src="../Images/ness-ch13-eqs-4x.png" width="858"/>
</div>
<div class="readable-text" id="p186">
<p>We can calculate each of these probabilities on the log scale with the Transformers library. In generating the log probability, we first calculate <em>logits</em> for each term in the vocabulary. For a probability value <em>p</em>, the corresponding logit is log(<em>p</em> / (1–<em>p</em>)).</p>
</div>
<div class="browsable-container listing-container" id="p187">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 13.4</span> Calculate the log probability of each token in the sequence</h5>
<div class="code-area-container">
<pre class="code-area">import torch
from transformers import GPT2LMHeadModel

model = GPT2LMHeadModel.from_pretrained('gpt2-medium')   <span class="aframe-location"/> #1
model.eval()    #1

input_text = "Can LLMs reason counterfactually?&lt;|endoftext|&gt;"    <span class="aframe-location"/> #2
input_ids = tokenizer.encode(input_text, return_tensors='pt')    #2

with torch.no_grad():   <span class="aframe-location"/> #3
    outputs = model(input_ids)    #3
    logits = outputs.logits    #3

log_probs = torch.nn.functional.log_softmax(logits, dim=-1)    <span class="aframe-location"/> #4
for idx, token in enumerate(input_ids[0]):    #4
    token_log_prob = log_probs[0][idx][token].item()    #4
    print(f"Token: {tokenizer.decode(token)}" + #4
           " | Log Probability: {token_log_prob}")     #4</pre>
<div class="code-annotations-overlay-container">
     #1 Initialize the GPT-2 model and set to evaluation mode.
     <br/>#2 Tokenize and encode the phrase, including the end-of-sequence token.
     <br/>#3 Given the phrase, the model produces logits for every element in the vocabulary.
     <br/>#4 For each position in the sequence, get the log probability corresponding to the token that was actually present in that position.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p188">
<p>This prints the following output:</p>
</div>
<div class="browsable-container listing-container" id="p189">
<div class="code-area-container">
<pre class="code-area">Token: Can | Log Probability: -10.451835632324219
Token:  LL | Log Probability: -9.275650978088379
Token: Ms | Log Probability: -14.926365852355957
Token:  reason | Log Probability: -10.416162490844727
Token:  counter | Log Probability: -8.359155654907227
Token: fact | Log Probability: -22.62082290649414
Token: ually | Log Probability: -11.302435874938965
Token: ? | Log Probability: -10.131906509399414
Token: &lt;|endoftext|&gt; | Log Probability: -11.475025177001953</pre>
</div>
</div>
<div class="readable-text" id="p190">
<p>Summing these together provides the joint probability of the sequence under the model.</p>
</div>
<div class="readable-text intended-text" id="p191">
<p>Of course, as a generative model, GPT-2 can generate the next token conditional on the tokens that came before it. The <em>prompt</em> the user provides is the beginning of the sequence, and the model’s response extends the sequence.</p>
</div>
<div class="browsable-container listing-container" id="p192">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 13.5</span> Generation from the LLM</h5>
<div class="code-area-container">
<pre class="code-area">prompt = "Counterfactual reasoning would enable AI to"    <span class="aframe-location"/> #1
input_ids = tokenizer.encode(prompt, return_tensors='pt')    #1

output = model.generate(   <span class="aframe-location"/> #2
    input_ids,     #2
    max_length=25,    #2
    do_sample=True,    #2
    pad_token_id=tokenizer.eos_token_id     #2
)

generated_text = tokenizer.decode(output[0], skip_special_tokens=True)   <span class="aframe-location"/> #3
print(generated_text)   #3</pre>
<div class="code-annotations-overlay-container">
     #1 Specify and encode the prompt.
     <br/>#2 Generate from the model. The “do_sample=True” argument means we’re doing random selection from the probability distribution of the next token, given all the previous tokens.
     <br/>#3 Decode and print the output.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p193">
<p>This prints out the following:</p>
</div>
<div class="browsable-container listing-container" id="p194">
<div class="code-area-container">
<pre class="code-area">Counterfactual reasoning would enable AI to figure out what people want before they ask them. It would also enable self-awareness</pre>
</div>
</div>
<div class="readable-text" id="p195">
<p>Again, note that ChatGPT generation has a random element, so this will likely produce something different for you.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p196">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Note about confusing terminology</h5>
</div>
<div class="readable-text" id="p197">
<p>Models like the GPT models are often called “causal language models,” but these are not causal models in the way we’ve discussed in this book. They are not a causal model of a DGP. “Causal” here refers to the autoregressive nature of the model—the model evaluates the probability of a token in a sequence conditional only on the tokens that came before it.</p>
</div>
</div>
<div class="readable-text" id="p198">
<p>All this is to say that the LLM is at a basic level a probability model of the joint probability of a sequence of tokens. The canonical training procedures for these models attempt to fit the joint probability distribution of the tokens. Models like GPT optimize the model’s ability to predict a given token in a training document given the previous tokens. Understanding that the LLM is a probability model over tokens doesn’t explain why LLMs can generate <em>coherent </em>text (meaning text with logical and consistent interrelation of ideas that forms a comprehensible whole). For that, we need to understand <em>attention</em>.</p>
</div>
<div class="readable-text" id="p199">
<h3 class="readable-text-h3" id="sigil_toc_id_330"><span class="num-string">13.2.2</span> The attention mechanism</h3>
</div>
<div class="readable-text" id="p200">
<p>One of the main drivers behind the success of LLMs is use of <em>Transformer architectures </em>and other neural network architectures that rely on a mechanism called <em>attention</em>. The attention mechanism allows the model to weigh the importance of different parts of an input sequence differently. That allows the model to learn to “focus” on specific parts of a sequence that are more relevant to a given task, while “ignoring” or assigning lesser weight to less pertinent parts.</p>
</div>
<div class="readable-text intended-text" id="p201">
<p>Consider, for example, the following conditional counterfactual statement about leaves being a necessary cause of a fire:</p>
</div>
<div class="readable-text" id="p202">
<blockquote>
<div>
     The Labor Day weekend wildfire started in the forest and spread rapidly due to the dry leaves on the ground. Had there been a controlled burn, the fire wouldn’t have spread so rapidly.
    </div>
</blockquote>
</div>
<div class="readable-text" id="p203">
<p>The attention mechanism helps the model recognize that “leaves” refers to foliage, not a departure, by weighing the relevance of surrounding words like “ground,” “dry,” and “forest.”</p>
</div>
<div class="readable-text intended-text" id="p204">
<p>Modern LLMs have attention mechanisms stacked over many neural network layers. This enables the LLM to attend to concepts at different levels of granularity. For example, while the first layer of attention focuses on immediate word-to-word relationships, such as “leaves” with “ground,” the next few layers connect broader phrases, treating “The Labor Day weekend wildfire” as a single entity connected to the phrase “spread rapidly.”</p>
</div>
<div class="readable-text intended-text" id="p205">
<p>Latter layers can represent the overarching theme or subject of the sentence and the broader text, connecting “The Labor Day weekend wildfire” to information about how it spread.</p>
</div>
<div class="readable-text" id="p206">
<h3 class="readable-text-h3" id="sigil_toc_id_331"><span class="num-string">13.2.3</span> From tokens to causal representation</h3>
</div>
<div class="readable-text" id="p207">
<p>The ability to talk about how attention enables the LLM to learn higher-level abstractions becomes of interest to us from the standpoint of causality. Recall figure 13.1, which first appeared in chapter 5 (as figure 5.4).<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p208">
<img alt="figure" height="366" src="../Images/CH13_F01_Ness.png" width="341"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 13.1</span> Example from chapter 5, where <em>digit</em> and <em>is-handwritten</em> are high-level causal drivers of low-level <em>X</em><em><sub>i</sub></em> pixels</h5>
</div>
<div class="readable-text" id="p209">
<p>The small squares represent pixels in an image, while the squares <em>digit</em> and <em>is-handwritten</em> represent the digit depicted in the image and whether it was handwritten, respectively. In that example (section 5.1.2), I suggested that whatever causal relations exist between individual pixels doesn’t matter to us; we’re interested in reasoning at the level of the objects depicted in the image.</p>
</div>
<div class="readable-text intended-text" id="p210">
<p>There is a similar thing going on with tokens, as shown in figure 13.2.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p211">
<img alt="figure" height="195" src="../Images/CH13_F02_Ness.png" width="901"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 13.2</span> <em>X</em><sub>1</sub> through <em>X</em><sub>12</sub> are a sequence of tokens. Whatever structure (causal or otherwise) exists between the tokens is of passing interest. We are interested in the causal relations between concepts described by the tokens.</h5>
</div>
<div class="readable-text" id="p212">
<p>In figure 13.2, as with the pixels, there is some structure at the level of the tokens. But that structure is beside the point—we’re interested in the causal relationships between the concepts that form the meaning behind the tokens.</p>
</div>
<div class="readable-text intended-text" id="p213">
<p>The question becomes, under what circumstances could attention, insofar as it can learn higher-level representations, learn a <em>causal</em> representation. For example, could an attention-based model, perhaps under some set of architectural or learning constraints, or use of interventions in training data, learn the parallel world structure and abstractions in the Labor Day counterfactual statement?</p>
</div>
<div class="readable-text intended-text" id="p214">
<p>To consider this, we can revisit LLM hallucinations in our question about the probability of necessity, and connect it to causal identification.</p>
</div>
<div class="readable-text" id="p215">
<h3 class="readable-text-h3" id="sigil_toc_id_332"><span class="num-string">13.2.4</span> Hallucination, attention, and causal identification</h3>
</div>
<div class="readable-text" id="p216">
<p>The hallucination about probability of necessity was generated with GPT-4. The same model got the Netflix question about probability of necessity correct. Indeed, the hallucinated answer would have been right if it merely stated the correct identifying assumptions. I believe future versions of GPT and similar models will likely get this question right on the first try.</p>
</div>
<div class="readable-text intended-text" id="p217">
<p>But for someone who is unfamiliar with the definition of probability of necessity, how would they know if the model were right or if it were hallucinating? First, the causal hierarchy tells us that in order to be capable of generating a right answer beyond a random guess, the query would need to be identified with level 3 information. Perhaps that information is provided by the user in the prompt. Perhaps the LLM has somehow learned level 3 representations (such a claim would require hard proof). </p>
</div>
<div class="readable-text intended-text" id="p218">
<p>If the user were providing that identifying information in the prompt, how would the user know if the model was successfully using that information to respond to the prompt? Suppose instead that the requirements for identification exist and are buried in the learned representations or data, and that they were being successfully leveraged by the model in answering the causal query, how could the user know for sure this was happening?</p>
</div>
<div class="readable-text intended-text" id="p219">
<p>We need to engineer solutions that answer these and other desiderata to build toward a causal AI future. In the next section, we’ll start on this path with a simple causal LLM.</p>
</div>
<div class="readable-text" id="p220">
<h2 class="readable-text-h2" id="sigil_toc_id_333"><span class="num-string">13.3</span> Forging your own causal LLM</h2>
</div>
<div class="readable-text" id="p221">
<p>In this section, we’ll sidestep the question of “can cutting-edge LLMs reason causally?” and move on to building a causal LLM that can reason causally. We’ll build for causality from the ground up, rather than as an afterthought.</p>
</div>
<div class="readable-text" id="p222">
<h3 class="readable-text-h3" id="sigil_toc_id_334"><span class="num-string">13.3.1</span> An LLM for script writing</h3>
</div>
<div class="readable-text" id="p223">
<p>Often our data has some implicit causal structure. When we make that structure explicit during training, the foundation model can learn better causal representations.</p>
</div>
<div class="readable-text intended-text" id="p224">
<p>To illustrate, suppose a prolific film production studio has historically insisted their writers use script-writing software that required following a three-act narrative archetype, which is common for romantic comedies: “boy meets girl, boy loses girl, boy gets girl back.” For this archetype, they have a corpus of many scripts. In causal terms, the events in act 1 cause the events of act 2, and the events of acts 1 and 2 cause the events of act 3. We can draw the DAG in figure 13.3.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p225">
<img alt="figure" height="200" src="../Images/CH13_F03_Ness.png" width="424"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 13.3</span> A causal DAG for a three-act archetype</h5>
</div>
<div class="readable-text" id="p226">
<p> The studio works with many such archetypes, and the company has many scripts that follow a given archetype template. Suppose that a set of archetypes involve a king acting a certain way in act 1, a prince acting a certain way in act 2, and these two actions having an effect on a kingdom in act 3. For example, one possible archetype is “King declares war, Prince leads army, kingdom experiences prosperity.” But there are multiple outcomes for each act:</p>
</div>
<ul>
<li class="readable-text" id="p227"> King in act 1: {king declares war; king negotiates peace; king falls ill} </li>
<li class="readable-text" id="p228"> Prince in act 2: {prince leads army; prince abdicates throne; prince marries foreigner} </li>
<li class="readable-text" id="p229"> Kingdom in act 3: {kingdom wins battle; kingdom falls into poverty; kingdom experiences prosperity}<span class="aframe-location"/> </li>
</ul>
<div class="browsable-container figure-container" id="p230">
<img alt="figure" height="207" src="../Images/CH13_F04_Ness.png" width="387"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 13.4</span> A causal DAG representing various King-Prince-Kingdom archetypes</h5>
</div>
<div class="readable-text" id="p231">
<p>Figure 13.4 shows this space of archetypes in the form of a causal DAG.</p>
</div>
<div class="readable-text intended-text" id="p232">
<p>This describes only 3 <span class="regular-symbol">×</span> 3 <span class="regular-symbol">×</span> 3 = 27 possible archetypes, but as you might expect, some archetypes are more common and some are less common. We could easily model these archetypes and the joint probability distribution by explicitly coding causal Markov kernels in pgmpy or Pyro. But that would only be a causal generative model on the archetypes. If we want a script generator, we want a causal generative model on the scripts.</p>
</div>
<div class="readable-text intended-text" id="p233">
<p>To demonstrate a proof-of-concept for this idea, we’ll work with a training dataset of short vignettes, rather than full scripts. Let’s load and examine the training data.</p>
</div>
<div class="browsable-container listing-container" id="p234">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 13.6</span> Load causal narrative data</h5>
<div class="code-area-container">
<pre class="code-area">import pandas as pd
url = ("https://raw.githubusercontent.com/altdeep/"
       "causalML/master/book/chapter%2013/"
       "king-prince-kingdom-updated.csv")
df = pd.read_csv(url)
print(df.shape[0])   <span class="aframe-location"/> #1

print(df["King"][0] + "\n")    <span class="aframe-location"/> #2
print(df["King"][1] + "\n")     #2
print(df["King"][2])     #2

print("----")
print(df["Prince"][0] + "\n")    <span class="aframe-location"/> #3
print(df["Prince"][1] + "\n")     #3
print(df["Prince"][2])    #3

print("----")
print(df["Kingdom"][0] + "\n")   <span class="aframe-location"/> #4
print(df["Kingdom"][1] + "\n")     #4
print(df["Kingdom"][2])     #4</pre>
<div class="code-annotations-overlay-container">
     #1 The data has 21,000 stories, broken up into three short vignettes.
     <br/>#2 First, the king acts.
     <br/>#3 Then the prince acts.
     <br/>#4 Finally, the kingdom experiences the consequences of the royals’ actions.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p235">
<p>This code prints the following:</p>
</div>
<div class="browsable-container listing-container" id="p236">
<div class="code-area-container">
<pre class="code-area">21000
----
King brokers a peace treaty with a rival kingdom, putting an end to years of bloody conflict

A wise king successfully negotiates peace with a rival nation

A wise king successfully negotiates peace between his kingdom and a long-time enemy
----
however, his son, the Prince, falls in love and marries a foreigner, causing political unrest

Prince falls in love with and marries a foreign princess, forging a strong alliance

but when a new threat emerges, the Prince leads the army to defend their realm
----
despite efforts, the ongoing war results in both kingdoms falling into poverty."

the alliance strengthens their forces, leading the kingdom to a victorious battle."

however, a series of misfortunes and disastrous decisions plunge their once prosperous kingdom into poverty."</pre>
</div>
</div>
<div class="readable-text" id="p237">
<p>There are 21,000 sets of three vignettes. The preceding output shows the first three sets in the dataset. </p>
</div>
<div class="readable-text" id="p238">
<h3 class="readable-text-h3" id="sigil_toc_id_335"><span class="num-string">13.3.2</span> Using pretrained models for causal Markov kernels</h3>
</div>
<div class="readable-text" id="p239">
<p>To train the causal Markov kernels for each node in our DAG, we’ll take pretrained models from the Hugging Face Transformers library, and then further train (aka “fine-tune”) the models using our vignettes. The pretraining took care of the heavy lifting in terms of learning to generate coherent natural language text. The fine-tuning will align the models toward representing our causal Markov kernels. </p>
</div>
<div class="readable-text" id="p240">
<p> First, we’ll use a GPT-2 variant to model the King’s action vignettes. As a text-completion model, it typically takes a prompt as input, but we’ll train it to generate with an empty prompt and produce vignettes according to the marginal probabilities of the King’s action texts in the training data, as in figure 13.5.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p241">
<img alt="figure" height="221" src="../Images/CH13_F05_Ness.png" width="387"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 13.5</span> GPT-2 is fine-tuned to represent the distribution of King’s action vignettes.</h5>
</div>
<div class="readable-text" id="p242">
<p> Next, we’ll use a BART model for the causal Markov kernel Prince’s action. BART is a Transformer model released in 2019 designed specifically to take an input sequence and generate a corresponding output sequence, such as with translation or summarization. Large models like GPT-4 can handle sequence-to-sequence tasks quite well, but we’ll use a version of BART with roughly 4,000-times fewer parameters than GPT-4, making it easier for you to load and train on your laptop or basic Python development environment. Given the King’s action vignette as input, it will generate a Prince’s action vignette, as illustrated in figure 13.6.</p>
</div>
<div class="readable-text intended-text" id="p243">
<p>We’ll also use a BART model to model the causal Markov kernel for the Kingdom’s fate, as shown in figure 13.7. The model will map the King’s and Prince’s actions to the Kingdom’s fate.</p>
</div>
<div class="browsable-container figure-container" id="p244">
<img alt="figure" height="221" src="../Images/CH13_F06_Ness.png" width="387"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 13.6</span> A BART sequence-to-sequence model is fine-tuned to represent the Prince’s action given the King’s action.</h5>
</div>
<div class="browsable-container figure-container" id="p245">
<img alt="figure" height="221" src="../Images/CH13_F07_Ness.png" width="387"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 13.7</span> A BART sequence-to-sequence model is also used to model the Kingdom’s fate given the King’s action and Prince’s action.</h5>
</div>
<div class="readable-text" id="p246">
<p>Jumping ahead, we’re interested in the conditional probability distribution of the Kingdom’s fate, given a certain action by the Prince. Since that will require inference of the King’s actions given the Prince, we’ll additionally train one more BART model that generates a King’s action vignette given a Prince’s action vignette, as shown in figure 13.8.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p247">
<img alt="figure" height="233" src="../Images/CH13_F08_Ness.png" width="402"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 13.8</span> A BART sequence-to-sequence model is also fine-tuned to model the Kingdom’s fate, given the King’s and Prince’s actions.</h5>
</div>
<div class="readable-text intended-text" id="p248">
<p>Let’s run the training procedure. First, we’ll set up our imports and our tokenizer. We’ll use Bart as the tokenizer for all of our models.</p>
</div>
<div class="browsable-container listing-container" id="p249">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 13.7</span> Training the causal LLM</h5>
<div class="code-area-container">
<pre class="code-area">import torch
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset
from transformers import (
    AutoModelForCausalLM, AutoModelForSeq2SeqLM,
    AutoTokenizer, DataCollatorForLanguageModeling,
    Seq2SeqTrainer, Seq2SeqTrainingArguments,
    Trainer, TrainingArguments)
url = ("https://raw.githubusercontent.com/altdeep/"
       "causalML/master/book/chapter%2013/"
       "king-prince-kingdom-updated.csv")
df = pd.read_csv(url)

tokenizer = AutoTokenizer.from_pretrained("facebook/bart-base")   <span class="aframe-location"/>  #1
tokenizer.pad_token = tokenizer.eos_token   <span class="aframe-location"/> #2
def tokenize_phrases(phrases, max_length=40): <span class="aframe-location"/> #3
    return tokenizer(
        phrases,
        truncation=True,
        padding='max_length',
        max_length=max_length
    )</pre>
<div class="code-annotations-overlay-container">
     #1 Set up the tokenizer.
     <br/>#2 The pad token is used to make all the sequences the same length to facilitate matrix operations. It is common to set it to the “end-of-sequence (EOS)” token.
     <br/>#3 The max length of the token is set to 40, as all of the vignettes are less than 40 tokens.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p250">
<p>Next we’ll create a class and a function that tokenizes the King dataset. We’ll create a custom subclass of the PyTorch <code>Dataset</code> class called <code>ModelDataset</code> that will store token encodings and their corresponding labels. When accessed by index, it returns a dictionary containing token encodings for that index and the associated label, and it provides the total number of examples via its <code>__len__</code> method.</p>
</div>
<div class="browsable-container listing-container" id="p251">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 13.8</span> Tokenizing the King vignettes</h5>
<div class="code-area-container">
<pre class="code-area">class ModelDataset(Dataset):   <span class="aframe-location"/> #1
    def __init__(self, encodings, labels):   #1
        self.encodings = encodings    #1
        self.labels = labels    #1
        #1
    def __getitem__(self, idx):   #1
        item = {    #A    #1
            key: torch.tensor(val[idx])    #1
            for key, val in self.encodings.items()    #1
        }    #A   #1
        item['labels'] = torch.tensor(self.labels[idx])    #1
        return item     #1
        #1
    def __len__(self):   #1
        return len(self.encodings.input_ids)   #1

def create_king_dataset(input_phrases):    <span class="aframe-location"/> #2
    king_phrases = input_phrases.tolist()   #2
    king_encodings = tokenize_phrases(king_phrases)    #2
    king_dataset = ModelDataset(    #2
        king_encodings,   #2
        king_encodings['input_ids']) #2
    return king_dataset    #2</pre>
<div class="code-annotations-overlay-container">
     #1 When accessed by index, ModelDataset returns a dictionary containing token encodings for that index and the associated label.
     <br/>#2 Create a ModelDataset instance for the king vignettes.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p252">
<p>Next we’ll tokenize the Prince and Kingdom vignettes. This code will also produce a validation dataset used in training sequence-to-sequence models.</p>
</div>
<div class="browsable-container listing-container" id="p253">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 13.9</span> Tokenizing the Prince and Kingdom vignettes</h5>
<div class="code-area-container">
<pre class="code-area">def create_seq2seq_datasets(input_phrases, target_phrases):
    input_phrases_list = input_phrases.tolist()
    target_phrases_list = target_phrases.tolist()
    spit = train_test_split(    <span class="aframe-location"/> #1
        input_phrases_list,     #1
        target_phrases_list,    #1
        test_size=0.1    #1
    )    #1
    train_inputs, val_inputs, train_targets, val_targets = spit     #1
    train_input_encodings = tokenize_phrases(train_inputs)   <span class="aframe-location"/> #2
    val_input_encodings = tokenize_phrases(val_inputs)    #2
    train_target_encodings = tokenize_phrases(train_targets)    #2
    val_target_encodings = tokenize_phrases(val_targets)     #2
    train_dataset = ModelDataset(
        train_input_encodings, train_target_encodings['input_ids']
    )
    val_dataset = ModelDataset(
        val_input_encodings, val_target_encodings['input_ids']
    )
    return train_dataset, val_dataset</pre>
<div class="code-annotations-overlay-container">
     #1 Split input and target phrases into training and validation sets.
     <br/>#2 Encode the training and validation sets.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p254">
<p>Next, we’ll write a training algorithm for the King model. This function initializes a GPT-2 model with the specified parameters, sets up the training arguments, and trains the model on the provided dataset, finally saving the trained model to the specified directory.</p>
</div>
<div class="browsable-container listing-container" id="p255">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 13.10</span> Training the King model</h5>
<div class="code-area-container">
<pre class="code-area">def train_king_model(output_dir, train_dataset,
                     model_name="gpt2-medium", epochs=4):
    king_model = AutoModelForCausalLM.from_pretrained(model_name)    <span class="aframe-location"/> #1
    training_args_king = TrainingArguments(     #1
      output_dir=output_dir,    #1
      per_device_train_batch_size=32,     #1
      overwrite_output_dir=True,     #1
      num_train_epochs=epochs,     #1
      save_total_limit=1,    #1
      save_steps=len(train_dataset) // 16, #1
      max_grad_norm=1.0 #1
    )    #1
    data_collator = DataCollatorForLanguageModeling(    #1
        tokenizer=tokenizer,     #1
        mlm=False)    #1
    trainer_king = Trainer(    <span class="aframe-location"/> #2
        model=king_model,     #2
        args=training_args_king,     #2
        data_collator=data_collator,    #2
        train_dataset=train_dataset,     #2
    )     #2
    trainer_king.train()  <span class="aframe-location"/> #3
    king_model.save_pretrained(output_dir)
    return king_model</pre>
<div class="code-annotations-overlay-container">
     #1 Initialize and configure model with the specified parameters.
     <br/>#2 Configure the training settings.
     <br/>#3 Train the model.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p256">
<p>Next, we’ll write a training algorithm for the sequence-to-sequence models. The function will split the provided input and target phrases into training and validation sets, tokenize them, and then create and return PyTorch <code>Dataset</code> objects for both sets using the <code>ModelDataset</code> class. The <code>train_seq2seq_model</code> function initializes a sequence-to-sequence model with the specified parameters, configures its training settings, and then trains the model using both training and validation datasets, finally returning the trained model.</p>
</div>
<div class="browsable-container listing-container" id="p257">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 13.11</span> Function for training the sequence-to-sequence models</h5>
<div class="code-area-container">
<pre class="code-area">def train_seq2seq_model(output_dir, train_dataset, val_dataset,
                        model_name="facebook/bart-base",
                        epochs=4):
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    training_args = Seq2SeqTrainingArguments(    <span class="aframe-location"/> #1
        output_dir=output_dir,     #1
        per_device_train_batch_size=16,     #1
        predict_with_generate=True,    #1
        logging_dir=f"{output_dir}/logs",     #1
        save_total_limit=1,    #1
        save_steps=len(train_dataset) // 16,     #1
        learning_rate=3e-5,     #1
        num_train_epochs=epochs,    #1
        warmup_steps=500,   #1
        weight_decay=0.01,     #1
    )    #1
    trainer = Seq2SeqTrainer(    <span class="aframe-location"/> #2
        model=model,    #2
        args=training_args,   #2
        train_dataset=train_dataset,     #2
        eval_dataset=val_dataset,    #2
    )     #2
    trainer.train()    <span class="aframe-location"/> #3
    model.save_pretrained(output_dir)
    return model</pre>
<div class="code-annotations-overlay-container">
     #1 Initialize and configure sequence-to-sequence model with the specified parameters.
     <br/>#2 Configure the training settings.
     <br/>#3 Train the model using both training and validation datasets, finally returning the trained model.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p258">
<p>Now we’ll use this function to train the models. We’ll specify some directories for saving checkpoints. </p>
</div>
<div class="readable-text print-book-callout" id="p259">
<p><span class="print-book-callout-head">Note</span>  In listing 13.14, I’ll provide code that downloads a pretrained model from Hugging Face, so if you don’t wish train the model, you can skip ahead to that step.</p>
</div>
<div class="browsable-container listing-container" id="p260">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 13.12</span> Training the King, Prince, and Kingdom models</h5>
<div class="code-area-container">
<pre class="code-area">import os

king_model_path = os.path.join(os.getcwd(), 'king_model')   <span class="aframe-location"/>  #1
prince_model_path = os.path.join(os.getcwd(), 'prince_model')     #1
kingdom_model_path = os.path.join(os.getcwd(), 'kingdom_model')     #1
prince2king_model_path = os.path.join(     #1
    os.getcwd(), 'prince2king_model')     #1

king_dataset = create_king_dataset(df["King"])    <span class="aframe-location"/> #2
king_model = train_king_model(king_model_path, king_dataset)     #2

datasets = create_seq2seq_datasets(df["King"], df["Prince"])   <span class="aframe-location"/> #3
train_dataset_prince, val_dataset_prince = datasets #3
prince_model = train_seq2seq_model(    #3
    prince_model_path,     #3
    train_dataset_prince,     #3
    val_dataset_prince, #3
    epochs=6   #3
)    #3

king_and_prince = [f"{k} {p}" for k, p in zip(df["King"], df["Prince"])]    <span class="aframe-location"/> #4
df["King and Prince"] = king_and_prince    #4
train_dataset_kingdom, val_dataset_kingdom = create_seq2seq_datasets(    #4
    df["King and Prince"], df["Kingdom"]    #4
)    #4
kingdom_model = train_seq2seq_model(    #4
    kingdom_model_path,    #4
    train_dataset_kingdom,     #4
    val_dataset_kingdom,     #4
   epochs=6    #4
)  #4</pre>
<div class="code-annotations-overlay-container">
     #1 Provide the output directories where you want to save your model.
     <br/>#2 Train the King model using Seq2Seq.
     <br/>#3 Train the Prince model using Seq2Seq. The King vignettes are used to predict the Prince vignettes.
     <br/>#4 Train the Prince model using Seq2Seq. The King vignettes are used to predict the Prince vignettes.
     <br/>#5 Train the Kingdom model using Seq2Seq. The combined King and Prince vignettes are used to predict the Kingdom vignettes.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p261">
<p>Finally, we’ll train another model for inferring the King vignette given a Prince vignette. We’ll use this in inference later.</p>
</div>
<div class="browsable-container listing-container" id="p262">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 13.13</span> Function to train the Prince to King model</h5>
<div class="code-area-container">
<pre class="code-area">p2k_data = create_seq2seq_datasets(    
    df["Prince"], df["King"])    
train_dataset_prince2king, val_dataset_prince2king = p2k_data    
prince2king_model = train_seq2seq_model(    
    prince2king_model_path,    
    train_dataset_prince2king,    
    val_dataset_prince2king,    
    epochs=6    
)</pre>
</div>
</div>
<div class="readable-text" id="p263">
<p>Running the preceding training procedure will take some time, especially if you’re not using GPU. Fortunately, there are saved versions of the trained models in the Hugging Face Hub. The following code pulls the Transformer models from the Hugging Face Hub and generates from them. It also provides a function that calculates the log probability of each generated sequence.</p>
</div>
<div class="browsable-container listing-container" id="p264">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 13.14</span> Pull Transformer models from the Hugging Face Hub and generate</h5>
<div class="code-area-container">
<pre class="code-area">import matplotlib.pyplot as plt
import pandas as pd
import torch
from sklearn.feature_extraction.text import TfidfVectorizer
from transformers import (
    AutoModelForCausalLM, AutoModelForSeq2SeqLM,
    AutoTokenizer, GPT2LMHeadModel,
    PreTrainedModel, BartForConditionalGeneration)
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

king_model = AutoModelForCausalLM.from_pretrained(    <span class="aframe-location"/> #1
    "osazuwa/causalLLM-king").to(DEVICE)    #1
prince_model = AutoModelForSeq2SeqLM.from_pretrained(    #1
    "osazuwa/causalLLM-prince").to(DEVICE)    #1
kingdom_model = AutoModelForSeq2SeqLM.from_pretrained(    #1
    "osazuwa/causalLLM-kingdom").to(DEVICE)     #1
prince2king_model = AutoModelForSeq2SeqLM.from_pretrained(     #1
    "osazuwa/causalLLM-prince2king").to(DEVICE)     #1

tokenizer = AutoTokenizer.from_pretrained("facebook/bart-base")   <span class="aframe-location"/> #2
tokenizer.pad_token = tokenizer.eos_token  #2</pre>
<div class="code-annotations-overlay-container">
     #1 Load the components of our model.
     <br/>#2 Load the components of our model.
     <br/>#3 Load the Bart-base tokenizer and set the pad token to end-of-sequence tokens.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p265">
<p>Next, we’ll write some functions to encode text to tokens, decode tokens to text, and generate text from the model given input text. </p>
</div>
<div class="browsable-container listing-container" id="p266">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 13.15</span> Helper functions for encoding, decoding, and generation</h5>
<div class="code-area-container">
<pre class="code-area">def encode(text:str, device=DEVICE) -&gt; torch.tensor:   <span class="aframe-location"/> #1
    input_ids = tokenizer.encode(text, return_tensors="pt")   #1
    input_ids = input_ids.to(device)    #1
    return input_ids   #1

def decode(text_ids: torch.tensor) -&gt; str:   <span class="aframe-location"/> #2
    output = tokenizer.decode(text_ids, skip_special_tokens=True)     #2
    return output     #2

<span class="aframe-location"/>EMPTY_TEXT = torch.tensor(tokenizer.encode("")).unsqueeze(0).to(DEVICE)   #3

def generate_from_model(model: PreTrainedModel,    <span class="aframe-location"/> #4
                        input_sequence: torch.tensor = EMPTY_TEXT,   #4
                        max_length: int = 25,    #4
                        temperature=1.0):   #4
    output = model.generate(    #4
        input_sequence,    #4
        max_length=max_length,    #4
        do_sample=True,   #4
        pad_token_id=tokenizer.pad_token_id, #4
        eos_token_id=tokenizer.pad_token_id,   #4
        temperature=temperature,   #4
        top_p=0.9,   #4
    )   #4
    return output    #4

def convert_to_text(output):
    return decode(output[0]).strip().capitalize()</pre>
<div class="code-annotations-overlay-container">
     #1 Encode text into tensor.
     <br/>#2 Decode tensor into text.
     <br/>#3 Get the encoding for empty text, for convenience.
     <br/>#4 A function for generating from models. These parameters do slightly different things for the GPT-2 and BART models, but they more or less overlap.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p267">
<p>We want to use our probabilistic ML approach, so we need a way of computing the log probabilities of generated sequences so we can use these in inference. The following function computes the log probability of a generated sequence based on related values produced by the GPT-2 and BART models.</p>
</div>
<div class="browsable-container listing-container" id="p268">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 13.16</span> Computing log probabilities of generated sequences</h5>
<div class="code-area-container">
<pre class="code-area">def compute_log_probs(model, output_sequence):
    if isinstance(model, GPT2LMHeadModel):    <span class="aframe-location"/> #1
        outputs = model(     #1
            input_ids=output_sequence,     #1
            labels=output_sequence    #1
        )     #1
        log_softmax = torch.nn.functional.log_softmax(     #1
            outputs.logits, dim=-1)     #1
        log_probs = log_softmax.gather(2, output_sequence.unsqueeze(-1))     #1
        log_probs = log_probs.squeeze(-1).sum(dim=-1)     #1
    elif isinstance(model, BartForConditionalGeneration):
        outputs = model(   <span class="aframe-location"/> #2
            input_ids=output_sequence,    #2
            labels=output_sequence)     #2
        loss = outputs.loss  #2
        log_probs = -loss * output_sequence.size(1)    #2
    else:
        raise ValueError("Unsupported model type")
    return torch.tensor(log_probs.item())</pre>
<div class="code-annotations-overlay-container">
     #1 Convert logits to logprobs for GPT-2.
     <br/>#2 Convert logits to logprobs from BART cross-entropy.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p269">
<p>Finally, we’ll put these pieces together to generate a full story from our three models.</p>
</div>
<div class="browsable-container listing-container" id="p270">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 13.17</span> Generating a full story</h5>
<div class="code-area-container">
<pre class="code-area">king_output = generate_from_model(king_model)  <span class="aframe-location"/> #1
king_statement = convert_to_text(king_output)   #1
print("Generated from king_nodel:", king_statement)    #1
log_prob_king = compute_log_probs(king_model, king_output)    #1
print("Log prob of generated king text:", log_prob_king)    #1

prince_output = generate_from_model(prince_model, king_output)    <span class="aframe-location"/> #2
prince_statement = convert_to_text(prince_output)   #2
print("Generated from prince_model:", prince_statement)    #2
log_prob_prince = compute_log_probs(prince_model, prince_output)    #2
print("Log prob of generated prince text:", log_prob_prince)     #2

king_prince_statement = king_statement + ". " + prince_statement    <span class="aframe-location"/> #3
king_prince_output = encode(king_prince_statement)    #3
kingdom_output = generate_from_model(kingdom_model, king_prince_output)     #3
kingdom_statement = convert_to_text(kingdom_output)    #3

print("Generated from kingdom model:", kingdom_statement)   #3
log_prob_kingdom = compute_log_probs(kingdom_model, kingdom_output)    #3
print("Log prob of generated kingdom text:", log_prob_kingdom)    #3

king_output_infer = generate_from_model(prince2king_model, prince_output)    <span class="aframe-location"/> #4
king_statement_infer = convert_to_text(king_output_infer)   #4
print("Generated statement from prince2king:", king_statement_infer)    #4
log_prob_prince2king = compute_log_probs(prince2king_model, prince_output)    #4
print("Log prob of generated inference text:", log_prob_prince2king)    #4</pre>
<div class="code-annotations-overlay-container">
     #1 Generate from the GPT-based model of vignettes about the King and calculate the log probabilities of the generated sequence.
     <br/>#2 Generate from the BART-based sequence-to-sequence model that generates vignettes about the Prince given vignettes about the King, and then calculate the log probability of the generated sequence.
     <br/>#3 Generate from the BART-based sequence-to-sequence model that generates vignettes about the Kingdom given vignettes about the King and the Prince, and then calculate the log probability of the generated sequence.
     <br/>#4 Another BART-based sequence-to-sequence model that maps a vignette about the Prince to a vignette about the King. We’ll use this to infer the vignette about the King from a vignette about the Prince.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p271">
<p>The output is nondeterministic, but one example of the output you’ll get is as follows:</p>
</div>
<div class="browsable-container listing-container" id="p272">
<div class="code-area-container">
<pre class="code-area">Generated statement from king_model: The king, driven by ambition, declares war on a neighboring nation to expand his kingdom's territories, declares war on. 
Log probability of generated king_model: tensor(-325.8379)
Generated statement from prince_model: The prince, disillusioned by his father's actions, abdicates the throne in protest. 
Log probability of generated prince text: tensor(-18.2486)
Generated statement from kingdom model: As the war drags on, resources are depleted, and the once-prosperous kingdom falls. 
Log probability of generated kingdom text: tensor(-38.3716)
Generated statement from prince2king: A king, driven by greed, declares war on a neighboring kingdom. 
Log probability of generated inference text: tensor(-297.3446)</pre>
</div>
</div>
<div class="readable-text" id="p273">
<p>Note that the generated output isn’t perfect—for example, the first generated statement ideally should have stopped after “…kingdom’s territories”. We could try to train it more or switch to a more powerful model, but this is pretty good for a start. </p>
</div>
<div class="readable-text intended-text" id="p274">
<p>Next, we’ll use these Transformers library models to define distributions in Pyro, and then use Pyro to build a causal generative model. First, we’ll use Pyro’s <code>TorchDistributionMixin</code> to model the causal Markov kernels with the language models. We’ll use the GPT-2 model of the King vignettes to create the causal Markov kernel of the <code>King</code> variable. </p>
</div>
<div class="readable-text intended-text" id="p275">
<p>Next, we’ll use the BART model to create the causal Markov kernel for the <code>Prince</code> variable. The <code>King</code> variable causes this variable, so the seq2seq model uses the <code>King</code> variable’s value to generate a value for this model. </p>
</div>
<div class="readable-text intended-text" id="p276">
<p>Finally, we’ll create the causal Markov kernel for the <code>Kingdom</code> variable. The <code>King</code> and <code>Prince</code> variables are causal parents, so we concatenate their generated outputs into one string, and use that string to generate the <code>Kingdom</code> output, again using a BART seq2seq model. We rely on a mixin called <code>TorchDistributionMixin</code>, which is useful for wrapping PyTorch distributions for use in Pyro.</p>
</div>
<div class="browsable-container listing-container" id="p277">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 13.18</span> Building a Torch distribution from a Transformer model</h5>
<div class="code-area-container">
<pre class="code-area">import pyro
from pyro.distributions.torch_distribution \
import TorchDistributionMixin

class TransformerModelDistribution(TorchDistributionMixin):
    def __init__(self, model: PreTrainedModel,
                 input_encoding: torch.tensor = EMPTY_TEXT,
                ):
        super().__init__()
        self.model = model
        self.input_encoding = input_encoding

    def sample(self, sample_shape=torch.Size()):    <span class="aframe-location"/> #1
        output = generate_from_model(    #1
            self.model, self.input_encoding     #1
        )    #1
        return output     #1

    def log_prob(self, value):    <span class="aframe-location"/> #2
        return compute_log_probs(self.model, value)     #2</pre>
<div class="code-annotations-overlay-container">
     #1 Use TorchDistributionMixin to turn a Transformers model into a Pyro distribution. TorchDistributionMixin is used to make PyTorch distributions compatible with Pyro’s utilities.
     <br/>#2 The log_prob method returns the log probabilities used in inference algorithms.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p278">
<p>Now we’ll use that distribution in Pyro.</p>
</div>
<div class="browsable-container listing-container" id="p279">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 13.19</span> Incorporating Transformer models into a causal model with Pyro</h5>
<div class="code-area-container">
<pre class="code-area">def causalLLM(): <span class="aframe-location"/> #1
    king = pyro.sample(    <span class="aframe-location"/> #2
        "King", TransformerModelDistribution(king_model)     #2
    )    #2
    prince = pyro.sample(    <span class="aframe-location"/> #3
        "Prince", TransformerModelDistribution(   #3
            prince_model, king)   #3
    )     #3
    king_and_prince = torch.cat([king, prince], dim=1)    <span class="aframe-location"/> #4
    kingdom = pyro.sample(    #4
        "Kingdom", TransformerModelDistribution(    #4
            kingdom_model, king_and_prince)     #4
    )    #4
    king_text = convert_to_text(king)    <span class="aframe-location"/> #5
    prince_text = convert_to_text(prince)    #5
    kingdom_text = convert_to_text(kingdom)     #5
    return king_text, prince_text, kingdom_text     #5

for _ in range(2):    <span class="aframe-location"/> #6
    king, prince, kingdom = causalLLM()    #6
    vignette = " ".join([king, prince, kingdom])     #6
    print(vignette)    #6</pre>
<div class="code-annotations-overlay-container">
     #1 Build the causal LLM.
     <br/>#2 Create the causal Markov kernel for the King variable.
     <br/>#3 Create the causal Markov kernel for the Prince variable.
     <br/>#4 Create the causal Markov kernel for the Kingdom variable.
     <br/>#5 Concatenate all the generated vignettes into one overall vignette and return the result.
     <br/>#6 Confirm our causal model generates the full vignette.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p280">
<p>The preceding code generates and prints two vignettes, such as the following:</p>
</div>
<div class="browsable-container listing-container" id="p281">
<div class="code-area-container">
<pre class="code-area">And beloved king falls gravely ill, leaving the kingdom in despair in uncertainty over the inexperienced prince to lead the kingdom. The young prince, eager to prove himself, leads the army into a costly and ill-advised war. As a result, the kingdom's resources are depleted, plunging the once-prosperous land into.
King, fueled by ambition, declares war on a neighboring realm, leaving his subjects anxious and. The prince, disillusioned by his father's actions, abdicates the throne in search of a new life. Without strong leadership, the kingdom spirals into poverty and despair.</pre>
</div>
</div>
<div class="readable-text" id="p282">
<p>We see that the generated texts are pretty good, though they seem to cut off a bit early. This, and other issues with the generations, can be addressed by tweaking the generation parameters.</p>
</div>
<div class="readable-text intended-text" id="p283">
<p>And just like that, we’ve built a causal LLM, an LLM build on a causal DAG scaffold. Let’s prove we have a causal model by comparing the observational and interventional distributions entailed by the DAG.</p>
</div>
<div class="readable-text" id="p284">
<h3 class="readable-text-h3" id="sigil_toc_id_336"><span class="num-string">13.3.3</span> Sampling from the interventional and observational distributions</h3>
</div>
<div class="readable-text" id="p285">
<p>By now, you know the distribution <em>P</em>(<em>Kingdom</em>|<em>Prince</em>=<em>x</em>) will be different from <em>P</em>(<em>Kingdom</em><sub><em>Prince</em></sub><sub>=</sub><sub><em>x</em></sub>), but let’s demonstrate the fact with this causal LLM. First, we’ll model <em>P</em>(<em>Kingdom</em>|<em>Prince</em>=<em>x</em>), where <em>x</em> is</p>
</div>
<div class="readable-text" id="p286">
<blockquote>
<div>
     His courageous Prince takes command, leading the kingdom’s army to victory in battle after battle
    </div>
</blockquote>
</div>
<div class="readable-text" id="p287">
<p>To infer <em>P</em>(<em>Kingdom</em>|<em>Prince</em>=<em>x</em>), we’ll have to infer the distribution of the latent confounder, <em>King</em>. We’ll do this using the <code>prince2king_model</code> we trained. We’ll use a probabilistic inference algorithm called “importance resampling.” We’ll start by creating a proposal function (what Pyro calls a “guide function”) that will generate samples of <em>King</em> and <em>Kingdom</em>, given <em>Prince</em>.</p>
</div>
<div class="browsable-container listing-container" id="p288">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 13.20</span> Proposal distribution for <em>P</em>(<em>Kingdom</em>|<em>Prince</em>=<em>x</em>)</h5>
<div class="code-area-container">
<pre class="code-area">import pyro.poutine as poutine
from pyro.distributions import Categorical

PRINCE_STORY = (    <span class="aframe-location"/> #1
    "His courageous Prince takes command, leading "  #1
    "the kingdom's army to victory in battle after battle")     #1
cond_model = pyro.condition(    #1
    causalLLM, {"Prince": encode(PRINCE_STORY)})   #1

def proposal_given_prince():   <span class="aframe-location"/> #2
    prince = encode(PRINCE_STORY) #2
    king = pyro.sample(  <span class="aframe-location"/> #3
        "King",     #3
        TransformerModelDistribution(prince2king_model, prince)   #3
    )    #3
    king_and_prince = torch.cat([king, prince], dim=1)   #3
    kingdom = pyro.sample(    <span class="aframe-location"/> #4
        "Kingdom",    #4
        TransformerModelDistribution(kingdom_model, king_and_prince)    #4
    )    #4
    vignette = (convert_to_text(king) +   <span class="aframe-location"/> #5
        PRINCE_STORY +  #5
        convert_to_text(kingdom))  #5
    return vignette</pre>
<div class="code-annotations-overlay-container">
     #1 We condition the model on this value of the Prince variable.
     <br/>#2 We’ll use a proposal function to generate from our target distribution P(King, Kingdom|Prince=PRINCE_STORY).
     <br/>#3 The proposal uses the prince2king_model to infer values of King given Prince=PRINCE_STORY.
     <br/>#4 Given the value of Prince, and the inferred value of King, use the king_and_prince model to sample Kingdom.
     <br/>#5 Concatenate the generated king tokens and provided prince tokens to return a generated vignette so we can inspect what is sampled.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p289">
<p>Now we’ll weigh each sample by the ratio of the probability of the sample under the conditioned model, over the probability of the sample under the proposal. Resampling the samples using these weights will generate samples from the target distribution. Pyro provides a utility for importance sampling, but because of the varying length of the generated sequences, it will be easier to implement importance sampling directly. </p>
</div>
<div class="readable-text intended-text" id="p290">
<p>First, we’ll write a function to process a sample and get its importance weight. </p>
</div>
<div class="browsable-container listing-container" id="p291">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 13.21</span> Function to draw a sample for resampling</h5>
<div class="code-area-container">
<pre class="code-area">def process_sample(model, proposal):
    sample_trace = poutine.trace(proposal).get_trace()   <span class="aframe-location"/> #1
    king_text = convert_to_text(sample_trace.nodes['King']['value'])  #1
    kingdom_text = convert_to_text( #1
        sample_trace.nodes['Kingdom']['value'])     #1
    proposal_log_prob = sample_trace.log_prob_sum()    <span class="aframe-location"/> #2
    replay = poutine.replay(model, trace=sample_trace)   <span class="aframe-location"/> #3
    model_trace = poutine.trace(replay).get_trace()     #3
    model_log_prob = model_trace.log_prob_sum()   #3
    log_importance_weight = model_log_prob - proposal_log_prob   <span class="aframe-location"/> #4
    sample = (king_text, kingdom_text, log_importance_weight)
    return sample</pre>
<div class="code-annotations-overlay-container">
     #1 Extract a sample from the proposal.
     <br/>#2 Calculate the total log probability of the sampled values of King and Kingdom.
     <br/>#3 Calculate the total log probability of the sample values of King and Kingdom under the original model.
     <br/>#4 Calculate the log importance weight.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p292">
<p>Now we’ll run the importance resampling. </p>
</div>
<div class="browsable-container listing-container" id="p293">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 13.22</span> Listing 13.22 Importance resampling of <em>P</em>(<em>Kingdom</em>|<em>Prince</em>=<em>x</em>)</h5>
<div class="code-area-container">
<pre class="code-area">def do_importance_resampling(model, proposal, num_samples):   <span class="aframe-location"/> #1
    original_samples = []
    for _ in range(num_samples):
        sample = process_sample(model, proposal)
        original_samples.append(sample)
    unique_samples = list(set(original_samples))    <span class="aframe-location"/> #2
    log_importance_weights = torch.tensor(   #2
        [sample[2] for sample in original_samples]) #2
    resampling_dist = Categorical(logits=log_importance_weights)     #2
    resampled_indices = resampling_dist.sample_n(num_samples)     #2
    samples = pd.DataFrame(   #2
        [unique_samples[i] for i in resampled_indices],      #2
        columns=["King", "Kingdom", "log_importance_weight"]    #2
    )     #2
    samples["Prince"] = PRINCE_STORY
    samples["Distribution"] = "observational"
    return samples[['King', 'Prince', 'Kingdom', 'Distribution']]

num_samples = 1000
posterior_samples = do_importance_resampling(
    cond_model, proposal_given_prince, num_samples)</pre>
<div class="code-annotations-overlay-container">
     #1 Use importance resampling as our inference procedure.
     <br/>#2 Resample using the importance weights. Pass in the log weights to the “logits” argument.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p294">
<p>Next, we’ll infer <em>P</em>(<em>Kingdom</em><sub><em>Prince</em></sub><sub>=</sub><sub><em>x</em></sub>). Given our causal model in Pyro, we can use Pyro’s do-operator to apply the intervention. We know that given the intervention on Prince, the edge from King to Prince is removed, so we don’t need to use <code>prince2king_model</code>. We can simply do ordinary forward generation from our intervention model.</p>
</div>
<div class="browsable-container listing-container" id="p295">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 13.23</span> Inferring <em>P</em>(<em>KingdomPrince</em>=<em>x</em>) using vanilla forward Monte Carlo sampling</h5>
<div class="code-area-container">
<pre class="code-area">intervention_model = pyro.do(    <span class="aframe-location"/> #1
    causalLLM, {"Prince": encode(PRINCE_STORY)})   #1
intervention_samples = pd.DataFrame(    <span class="aframe-location"/> #2
    [intervention_model() for _ in range(num_samples)],     #2
    columns=["King", "Prince", "Kingdom"]    #2
)   #2
intervention_samples["Distribution"] = "interventional"     #2
all_samples = pd.concat(   #2
    [posterior_samples, intervention_samples],    #2
    ignore_index=True     #2
)    #2</pre>
<div class="code-annotations-overlay-container">
     #1 Forward sample from the interventional distribution.
     <br/>#2 Label the samples, and combine them with the observational samples.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p296">
<p>Generating the samples will take some time. Since we’re working directly with the encoded sequence tensors in Pyro, we could leverage the potentially faster gradient-based inference algorithm. For convenience, you can access presaved samples in the book’s directory of the GitHub repo: <a href="https://github.com/altdeep/causalml">https://github.com/altdeep/causalml</a>. </p>
</div>
<div class="readable-text intended-text" id="p297">
<p>Next, let’s visualize the difference in the distributions. We need a way to visualize sampled text from the interventional and observational distributions. We can do so using TF-IDF (term frequency-inverse document frequency), a numerical statistic that reflects how important a word is to a sample within the collection of samples, emphasizing words that are unique to specific samples.</p>
</div>
<div class="browsable-container listing-container" id="p298">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 13.24</span> Get TF-IDF of generations for <em>P</em>(<em>Kingdom</em><em><sub>Prince=x</sub></em>) and <em>P</em>(<em>Kingdom</em>|<em>Prince</em>=<em>x</em>) </h5>
<div class="code-area-container">
<pre class="code-area">import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer

kingdom_samples_url = (
    "https://raw.githubusercontent.com/altdeep/causalML/"
    "master/book/chapter%2013/kingdom_samples.csv")
all_samples = pd.read_csv(kingdom_samples_url)

observational_texts = all_samples[    <span class="aframe-location"/> #1
    all_samples["Distribution"] == "observational"]["Kingdom"]   #1
interventional_texts = all_samples[all_samples[  #1
    "Distribution"] == "interventional"]["Kingdom"]   #1

    vectorizer = TfidfVectorizer(stop_words='english')    <span class="aframe-location"/> #2
X_obs = vectorizer.fit_transform(observational_texts)    #2
X_int = vectorizer.transform(interventional_texts)     #2

k = 10    <span class="aframe-location"/> #3
feature_names = vectorizer.get_feature_names_out()     #3
obs_indices = X_obs.sum(axis=0).argsort()[0, -k:][::-1]    #3
int_indices = X_int.sum(axis=0).argsort()[0, -k:][::-1]    #3
combined_indices = np.concatenate((obs_indices, int_indices))    #3
combined_indices = np.unique(combined_indices)     #3</pre>
<div class="code-annotations-overlay-container">
     #1 Extract generated Kingdom vignettes from observational and interventional distributions.
     <br/>#2 Compute the TF-IDF values for generated Kingdom vignettes in each group.
     <br/>#3 Get the top k=7 words by TF-IDF for each set.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p299">
<p>Finally, we’ll visualize the two distributions.</p>
</div>
<div class="browsable-container listing-container" id="p300">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 13.25</span> Visually contrast <em>P</em>(<em>Kingdom</em><em><sub>Prince=x</sub></em>) and <em>P</em>(<em>Kingdom</em>|<em>Prince</em>=<em>x</em>)</h5>
<div class="code-area-container">
<pre class="code-area">import matplotlib.pyplot as plt

labels = [feature_names[i] for i in combined_indices]   <span class="aframe-location"/> #1
labels, indices = np.unique(labels, return_index=True)     #1
obs_values = np.array(X_obs.sum(axis=0))[0, combined_indices]     #1
int_values = np.array(X_int.sum(axis=0))[0, combined_indices]     #1
obs_values = [obs_values[0][i] for i in indices]     #1
int_values = [int_values[0][i] for i in indices]     #1
combined = list(zip(labels, obs_values, int_values))    #1
sorted_combined = sorted(combined, key=lambda x: (-x[1], x[2]))    #1
labels, obs_values, int_values = zip(*sorted_combined)    #1

width = 0.35   <span class="aframe-location"/> #2
x = np.arange(len(labels))     #2
fig, ax = plt.subplots()    #2
rects1 = ax.bar(x - width/2, obs_values, width, #2
                label='Observational', alpha=0.7)    #2
rects2 = ax.bar(x + width/2, int_values, width, #2
                label='Interventional', alpha=0.7)    #2
ax.set_xlabel('Words')     #2
ax.set_ylabel('TF-IDF Values')     #2
ax.set_title(    #2
    'Top Words in Generated Kingdom Vignettes by TF-IDF Value')    #2
ax.set_xticks(x)     #2
ax.set_xticklabels(labels)    #2
ax.legend()    #2
fig.tight_layout()    #2
plt.xticks(rotation=45)   #2
plt.show()     #2</pre>
<div class="code-annotations-overlay-container">
     #1 Prepare data for the bar plot.
     <br/>#2 Produce the plot.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p301">
<p>This produces figure 13.9.</p>
</div>
<div class="readable-text intended-text" id="p302">
<p>Figures 13.9 shows similar TF-IDF scores for words in the observational case. This is due to the lack of variation in the observational case, since observing the Prince constrains the likely values of King. When we intervene on Prince, King can vary more, leading to more variation in the results.</p>
</div>
<div class="browsable-container figure-container" id="p303">
<img alt="figure" height="888" src="../Images/CH13_F09_Ness.png" width="1100"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 13.9</span> Visualization of the difference between samples from <em>P</em>(<em>Kingdom</em><em><sub>Prince</sub></em><sub>=</sub><em><sub>x</sub></em>) and <em>P</em>(<em>Kingdom</em>|<em>Prince</em>=<em>x</em>) using TF-IDF, where <em>x</em> is the Prince taking the army to battle. The observational values are flat because of little variation in the inferred King vignettes. The intervention enables more variation in the King vignettes and, consequently, the Kingdom vignettes.<span class="aframe-location"/></h5>
</div>
<div class="readable-text" id="p304">
<h3 class="readable-text-h3" id="sigil_toc_id_337"><span class="num-string">13.3.4</span> Closing thoughts</h3>
</div>
<div class="readable-text" id="p305">
<p>This is a toy problem with a simple DAG trained on simple data with simple LLMs. But we could extend it to more complicated DAGs and fine-tuning more advanced models. There may also be other ways to combine causal assumptions in foundation models. We’re only at the beginning of exploring this exciting space.</p>
</div>
<div class="readable-text" id="p306">
<h2 class="readable-text-h2" id="sigil_toc_id_338">Summary</h2>
</div>
<ul>
<li class="readable-text" id="p307"> Large language models (LLMs) are powerful AI models that generate text and other modalities and achieve high performance across a variety of benchmarks. </li>
<li class="readable-text" id="p308"> LLMs have proven use cases for supporting causal analysis. </li>
<li class="readable-text" id="p309"> LLMs can help build a causal DAG. Moreover, they can leverage common and expert knowledge about causal relations and mechanisms. </li>
<li class="readable-text" id="p310"> The causal frame problem is the challenge of selecting the causal variables relevant to a given problem and excluding the irrelevant. Cutting-edge LLMs emulate how humans set the causal frame, which is useful for applications such as building DAGs and root cause analysis. </li>
<li class="readable-text" id="p311"> LLMs can help us understand nuanced causal concepts and how to contextualize them within our domain of interest. </li>
<li class="readable-text" id="p312"> LLMs can help us put causal queries into formal terms. </li>
<li class="readable-text" id="p313"> LLMs are prone to hallucinations—convincing yet incorrect responses to our queries. </li>
<li class="readable-text" id="p314"> At their core, LLMs are probabilistic machine learning models that model a joint probability distribution on sequences of tokens. </li>
<li class="readable-text" id="p315"> The attention mechanism enables the LLM to learn higher-level representations that make cutting-edge LLMs so powerful. </li>
<li class="readable-text" id="p316"> Just because an LLM learns a higher-level representation doesn’t mean it learns a causal representation. Even if that did work in some special cases, it would be hard for the user to verify that it is working. </li>
<li class="readable-text" id="p317"> We can build our own causal LLM by composing fine-tuned LLMs over a causal DAG scaffold. This allows us to work with cutting-edge LLMs while admitting causal operations, such as a do-operator. </li>
<li class="readable-text" id="p318"> Use the causal hierarchy theory as your North Star in your exploration of how to combine causality with LLMs and multimodal models, as well as exploring how well these models can learn causal representations on their own. </li>
</ul>
<div class="readable-text footnote-readable-text" id="p319">
<p><a href="#footnote-source-1"><span class="footnote-definition" id="footnote-305">[1]</span></a> C. Glymour, “Android epistemology and the frame problem,” in Z.W. Pylyshyn, ed., <em>The robot’s dilemma: The frame problem in artificial intelligence</em> (Praeger, 1987), pp. 63–75. </p>
</div>
</div></body></html>