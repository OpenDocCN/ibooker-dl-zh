- en: '4 From pixels to pictures: Generating images'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 从像素到图片：生成图像
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Generative AI vision models, their model architecture, and key use cases for
    enterprises
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成式AI视觉模型，其模型架构以及企业关键应用场景
- en: Using Stable Diffusion’s GUIs and APIs for image generation and editing
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Stable Diffusion的GUI和API进行图像生成和编辑
- en: Using advanced editing techniques, such as inpainting, outpainting, and image
    variations
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用高级编辑技术，如修复、扩展和图像变体
- en: Practical image generation tips for enterprises to consider
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 企业应考虑的实际图像生成技巧
- en: Generating images represents one of the many uses of generative AI, resulting
    in unique and realistic content from a mere prompt. Enterprises have been increasingly
    adopting generative AI to develop innovative image generation and editing solutions,
    which has led to many innovative use cases—from AI-powered architecture for innovative
    designs of buildings to fashion design, avatar generation, virtual clothes try-on,
    and virtual patients for medical training, to name a few. They are accompanied
    by exciting products such as Microsoft Designer and Adobe Firefly, and they will
    be covered in this chapter.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 生成图像是生成式AI的多种用途之一，只需一个提示就能生成独特且逼真的内容。企业越来越多地采用生成式AI来开发创新的图像生成和编辑解决方案，这导致了众多创新用例的出现——从为创新建筑设计提供AI驱动的架构，到时尚设计、头像生成、虚拟试衣和用于医疗培训的虚拟病人，仅举几例。这些用例伴随着令人兴奋的产品，如Microsoft
    Designer和Adobe Firefly，它们将在本章中介绍。
- en: In the previous chapters, we talked about the fundamentals of generative AI
    and the technology that enables us to generate text, including completions and
    chats. However, in this chapter, we shift gears and explore how generative AI
    can be utilized to produce and adjust images. We will see how creating images
    is a simple process and highlight some of the complexities of getting them right.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们讨论了生成式AI的基础以及使我们能够生成文本的技术，包括补全和聊天。然而，在本章中，我们将转变方向，探讨如何利用生成式AI来生成和调整图像。我们将看到创建图像是一个简单的过程，并强调一些确保图像正确性的复杂性。
- en: Initially, this chapter focuses on comprehending the generative AI methods that
    facilitate the generation of new images and the overall workflow an enterprise
    must consider. The applications of these techniques are immense and can be particularly
    useful in the e-commerce, entertainment, and healthcare sectors. In addition,
    we will examine various generative AI products and services for image manipulation.
    Let’s dive in!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，本章重点在于理解促进新图像生成和整体工作流程的生成式AI方法，这些技术的应用范围广泛，尤其在电子商务、娱乐和医疗保健领域尤为有用。此外，我们还将探讨各种用于图像处理的生成式AI产品和服务的应用。让我们深入探讨！
- en: 4.1 Vision models
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 视觉模型
- en: 'Generative AI vision models can generate realistic new images and novel concepts
    from a prompt. Let’s start by looking at some enterprise use cases and examples
    of how these generative AI vision models can help:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI视觉模型可以从提示中生成逼真的新图像和新的概念。让我们首先看看一些企业用例和示例，了解这些生成式AI视觉模型如何帮助：
- en: '*Content creation and editing*—There are multiple use cases in different industries
    where generative AI vision models can help media and marketing professionals generate
    new themes and scenarios, remove unnecessary or unwanted things from images, or
    apply style transfer. The specific use cases vary by industry.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*内容创作和编辑*——在多个行业中，生成式AI视觉模型可以帮助媒体和营销专业人士生成新的主题和场景，从图像中移除不必要的或不希望存在的东西，或应用风格迁移。具体用例因行业而异。'
- en: '*Healthcare*—There are multiple use cases of image-generative AI in the health
    domain, from educating and training medical students or using new techniques (see
    the next item) to improving a patient’s diagnosis and prognosis by helping enhance
    and clear medical images. It also accelerates drug discovery and development by
    analyzing new novel molecules, complex molecular interactions, and their predictions,
    and optimizing formulation and synthesis.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*医疗保健*——在医疗领域，图像生成式AI有多个用例，从教育和培训医学生或使用新技术（见下一条）到通过帮助增强和清晰医疗图像来改善患者的诊断和预后。它还通过分析新的分子、复杂的分子相互作用及其预测，以及优化配方和合成来加速药物发现和开发。'
- en: '*Education*—We can create interactive visuals on the fly based on a student’s
    progress and current learning. This includes realistic and diverse scenarios,
    training simulations using data augmentation, and helping improve the teaching
    quality of both the educator and the student.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*教育*—我们可以根据学生的进度和当前学习情况即时创建交互式视觉内容。这包括现实且多样化的场景、使用数据增强的训练模拟，以及帮助提高教师和学生的教学质量。'
- en: '*R&D*—We can create a more interpretable visual representation of complex data
    structures and relationships that might not be obvious otherwise. These core elements
    help create new product designs based on trends, unique visual elements, branding,
    and layouts, and can discover subtle patterns in the data.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*研究与开发*—我们可以创建更可解释的复杂数据结构和关系的视觉表示，这些结构在其他情况下可能不明显。这些核心元素有助于基于趋势、独特的视觉元素、品牌和布局创建新产品设计，并能在数据中发现细微的模式。'
- en: '*Marketing*—Generative AI vision models generate specific visuals tailored
    to the specific individual or demographic, which can also include different sets
    of visuals for A/B testing for understanding successful marketing campaigns.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*营销*—生成式AI视觉模型生成针对特定个人或人群的特定视觉内容，这也可以包括用于A/B测试的不同视觉内容集，以了解成功的营销活动。'
- en: '*Manufacturing*—Generative AI vision models have the ability to rapidly iterate
    and visualize new materials and components, including the assembly process.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*制造业*—生成式AI视觉模型能够快速迭代和可视化新材料和组件，包括组装过程。'
- en: '*Personalization*—This horizontal use case can span different dimensions by
    allowing us to generate personalized visuals, for example, in e-commerce settings
    where a shopper can visualize objects, content, clothing, and so on to create
    highly customized and personalized avatars for gaming and social platforms. Finally,
    fashion and creative fields create new patterns, layouts, clothing, and furniture
    designs.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*个性化*—这种横向用例可以通过允许我们生成个性化视觉内容来跨越不同的维度，例如在电子商务环境中，购物者可以可视化商品、内容、服装等，为游戏和社交平台创建高度定制和个性化的头像。最后，时尚和创意领域创造了新的图案、布局、服装和家具设计。'
- en: 'Here are some real examples of how to generate and bring some of this content
    to life:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些如何生成并使部分内容生动起来的真实示例：
- en: '*Creative content*—Generative AI vision models can produce novel and diverse
    images or videos for artistic, entertainment, or marketing purposes. Some of them
    create realistic faces of people who seem real but do not exist, or they modify
    existing faces to factor in different features such as age, gender, hairstyle,
    and so forth. Figure 4.1 shows a panda bear generated using strawberries.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*创意内容*—生成式AI视觉模型可以产生新颖且多样化的图像或视频，用于艺术、娱乐或营销目的。其中一些模型创建出看似真实但实际不存在的人脸，或者修改现有的人脸以考虑不同的特征，如年龄、性别、发型等。图4.1展示了使用草莓生成的熊猫。'
- en: '![figure](../Images/CH04_F01_Bahree.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH04_F01_Bahree.png)'
- en: Figure 4.1 A strawberry panda
  id: totrans-21
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.1 草莓熊猫
- en: '*Image editing, content improvement, and style transfer*—We can use generative
    AI vision models to enhance existing images. These can address various artifacts,
    such as enhancing the resolution and quality and removing unwanted elements. We
    can also use the style and technique of one image and transpose it onto another.
    For example, figure 4.2 shows us an oil painting of Seattle’s Space Needle in
    the style of Vincent Van Gogh.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图像编辑、内容改进和风格迁移*—我们可以使用生成式AI视觉模型来增强现有图像。这些模型可以解决各种问题，如提高分辨率和质量，以及移除不需要的元素。我们还可以将一张图像的风格和技术转移到另一张图像上。例如，图4.2展示了以梵高风格绘制的西雅图太空针油画。'
- en: '![figure](../Images/CH04_F02_Bahree.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH04_F02_Bahree.png)'
- en: Figure 4.2 An oil painting of the Seattle Space Needle in the style of Vincent
    Van Gogh
  id: totrans-24
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.2 范·高风格下的西雅图太空针油画
- en: '*Synthetic data*—We can create realistic but synthetic images using generative
    AI vision models. These synthetic images can be used as training and validation
    data for other AI models. For example, the site [https://thispersondoesnotexist.com/](https://thispersondoesnotexist.com/)
    generates the faces of people who do not exist in real life. Synthetic data come
    with challenges; we will discuss them later in the book when we cover generative
    AI challenges.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*合成数据*—我们可以使用生成式AI视觉模型创建逼真但合成的图像。这些合成图像可以用作其他AI模型的训练和验证数据。例如，网站[https://thispersondoesnotexist.com/](https://thispersondoesnotexist.com/)生成现实生活中不存在的人脸。合成数据带来了挑战；我们将在本书后面讨论生成式AI挑战时再讨论这些问题。'
- en: '*Generative engineering and design*—We can generate new design options that
    include new objects and structures that can help us optimize certain criteria
    or constraints, such as functionality, performance, or aesthetics. These models
    can generate unique, novel designs for products or digital assets, reducing the
    time and resources spent on manual design. Figure 4.3 shows a chair optimized
    for various design characteristics such as material and aesthetics. These chairs
    have unique and futuristic shapes different from those in conventional chairs.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*生成工程和设计*—我们可以生成新的设计选项，包括新的物体和结构，这有助于我们优化某些标准或约束，如功能性、性能或美学。这些模型可以为产品或数字资产生成独特、新颖的设计，从而减少手动设计所需的时间和资源。图4.3显示了针对材料、美学等设计特性进行优化的椅子。这些椅子的形状独特且具有未来感，与传统的椅子不同。'
- en: '![figure](../Images/CH04_F03_Bahree.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH04_F03_Bahree.png)'
- en: Figure 4.3 A chair designed for strength, aesthetics, material, and weight
  id: totrans-28
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.3 为强度、美学、材料和重量优化的椅子
- en: 'Four main generative AI model architecture types make these use cases and examples
    possible: variational autoencoders (VAEs), generative adversarial networks (GANs),
    diffusion models, and vision transformers. Each technique has its strengths and
    weaknesses, and we outline the right approach for the scenario it can use:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 四种主要的生成式AI模型架构类型使得这些用例和示例成为可能：变分自编码器（VAEs）、生成对抗网络（GANs）、扩散模型和视觉Transformer。每种技术都有其优势和劣势，我们概述了适用于其场景的正确方法：
- en: '*Variational autoencoders*—VAEs generate realistic but simple images of animals,
    faces, and other objects. They are good for scenarios requiring data generation,
    that is, new data points similar to the original but with variations. This property
    also allows VAEs to be used for anomaly detection and recommendation systems.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*变分自编码器*—VAEs可以生成逼真但简单的动物、面孔和其他物体的图像。它们适用于需要数据生成的场景，即与原始数据相似但有所变化的新数据点。这一特性还允许VAEs用于异常检测和推荐系统。'
- en: '*Generative adversarial networks*—GANs are used for scenarios where data is
    complex and diverse and requires a high level of realism. This makes them suitable
    for high-quality images, data augmentation, and style transfers.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*生成对抗网络*—GANs用于数据复杂且多样化的场景，需要高度的真实感。这使得它们适用于高质量图像、数据增强和风格迁移。'
- en: '*Diffusion*—Diffusion-based models are used for scenarios where the data is
    high-dimensional and continuous, and we need to model complex data distribution
    with quality, with the speed of generation being unimportant. These models are
    good for generating speech and video, some of which we will touch on in the next
    chapter.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*扩散*—基于扩散的模型用于数据高维且连续的场景，我们需要以高质量、快速生成的方式对复杂数据分布进行建模。这些模型适用于生成语音和视频，其中一些将在下一章中涉及。'
- en: '*Vision transformers*—These are great when we want to generate images that
    are sequenced-based tasks, highly flexible, and adaptable to many tasks; they
    need significant computational resources.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*视觉Transformer*—当我们想要生成基于序列的任务图像时，这些模型非常出色，它们具有高度灵活性和适应性，适用于许多任务；它们需要大量的计算资源。'
- en: Let’s explore each of these architectures in more detail.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地探讨这些架构的每一个。
- en: 4.1.1 Variational autoencoders
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.1 变分自编码器
- en: VAEs are a specific generative model that has a vital role. They represent complex
    data distributions by combining aspects of deep learning, probability theory,
    and statistical mechanics.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: VAEs是一种具有关键作用的特定生成模型。它们通过结合深度学习、概率理论和统计力学的方面来表示复杂的数据分布。
- en: 'VAEs include two neural networks: an encoder and a decoder (figure 4.4). The
    encoder maps an input image into a low-dimensional latent vector (a latent space)
    that captures its essential features. Not only does it find a single point in
    the latent space, but it can find a distribution. In contrast, the decoder takes
    samples from the latency space and reconstructs the original input image, while
    adding some randomness to make it more diverse. This randomness allows us to add
    new data points, like the input data.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: VAEs包括两个神经网络：编码器和解码器（图4.4）。编码器将输入图像映射到一个低维潜在向量（潜在空间），该空间捕捉其基本特征。它不仅找到一个潜在空间中的单一点，还可以找到一个分布。相比之下，解码器从潜在空间中抽取样本并重建原始输入图像，同时添加一些随机性以使其更加多样化。这种随机性使我们能够添加新的数据点，如输入数据。
- en: 'The following two parameters define the latent state: mean and variance. As
    the name suggests, the mean is the average value of the latent state, and the
    variance is the measure difference of the latent state from the mean. The VAE
    uses these parameters to sample different latent states from a normal distribution,
    a mathematical function that describes how likely different values are to occur.
    By sampling different latent states, the VAE can generate different output data
    that is similar to the input data. Statistical mechanics allow us a framework
    to infer the probability distribution of the variables in the latent variables
    given the observer data.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下两个参数定义了潜在状态：均值和方差。正如其名所示，均值是潜在状态的平均值，方差是潜在状态与均值的差异度量。VAE 使用这些参数从正态分布中采样不同的潜在状态，这是一个描述不同值发生的可能性的数学函数。通过采样不同的潜在状态，VAE
    可以生成与输入数据相似的不同输出数据。统计力学为我们提供了一个框架，根据观察数据推断潜在变量中变量的概率分布。
- en: '![figure](../Images/CH04_F04_Bahree.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH04_F04_Bahree.png)'
- en: Figure 4.4 Variational autoencoder architecture
  id: totrans-40
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.4 变分自编码器架构
- en: Some of the key uses that VAEs allow are
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: VAEs 允许的一些关键用途包括
- en: '*Image generation*—VAEs have been used extensively for image generation to
    create unique images that share similarities with their training data, be it human-like
    faces, fashion designs, or art.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图像生成*—变分自编码器（VAEs）被广泛用于图像生成，以创建与训练数据相似的独特图像，无论是类似人类的面孔、时尚设计还是艺术作品。'
- en: '*Image reconstruction and inpainting*—By learning the underlying structure
    of the image data, VAEs can reconstruct either missing or corrupted parts of images.
    These properties of reconstructing or filling in missing aspects are tremendously
    useful in some domains, such as medical imaging, restoring old and archaeologically
    significant photographs, and so on.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图像重建和修复*—通过学习图像数据的底层结构，VAEs 可以重建图像的缺失或损坏部分。这些重建或填补缺失方面的特性在某些领域非常有用，例如医学成像、恢复古老和考古学上有重要意义的照片等。'
- en: '*Style transfer*—VAEs allow us to separate the image content from the style
    and transfer the stylistic elements from one image to another, as shown in figure
    4.2\.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*风格迁移*—VAEs 允许我们将图像内容与风格分离，并将风格元素从一个图像转移到另一个图像，如图 4.2 所示。'
- en: '*Semantic image manipulation*—This is similar to image reconstruction. Because
    of the learned latent space, VAEs can provide us with much more fine-grained control
    of the features in the generated images by tweaking specific aspects of the generated
    images, such as facial expressions, without affecting other unrelated features.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语义图像处理*—这与图像重建类似。由于学习到的潜在空间，VAEs 可以通过调整生成图像的特定方面（如面部表情）来提供对生成图像中特征的更精细的控制，而不会影响其他无关特征。'
- en: Although powerful, VAEs do have drawbacks, such as blurriness, lack of diversity,
    and difficulty modeling complex distributions. Training them can be demanding
    and unstable, leading to mode collapse. Irrespective of these challenges, the
    achievements and potential of VAEs remain at the forefront of vision AI research,
    building on the complex relationships between data, mathematics, and creativity.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管功能强大，VAEs 也有一些缺点，例如模糊、缺乏多样性和难以建模复杂分布。训练它们可能要求很高且不稳定，导致模式崩溃。尽管存在这些挑战，VAEs 在视觉人工智能研究的前沿仍取得了成就和潜力，这建立在数据、数学和创造力之间复杂关系的基础上。
- en: Note  A latent space represents complex data in a simpler and more meaningful
    way. Think of it as a map where similar items are close to each other, and different
    items are far apart. This helps us find similarities, generate new data, and understand
    data better.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：潜在空间以更简单、更有意义的方式表示复杂数据。将其想象成一个地图，其中相似的项目彼此靠近，不同的项目则相距甚远。这有助于我们找到相似之处，生成新的数据，并更好地理解数据。
- en: 4.1.2 Generative adversarial networks
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.2 生成对抗网络
- en: 'GANs [1] are among the most popular techniques for creating images with generative
    AI. They consist of two neural networks: a generator that creates new examples
    and a discriminator that tries to differentiate between real and generated examples.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: GANs [1] 是创建生成人工智能图像最受欢迎的技术之一。它们由两个神经网络组成：一个生成器，用于创建新示例，一个判别器，试图区分真实和生成的示例。
- en: The generator tries to create fake images that look like real ones from random
    noise or input data, such as text or sketches. The discriminator takes real and
    fake images and tries to distinguish between the two.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器试图从随机噪声或输入数据（如文本或草图）中创建看起来像真实图像的假图像。判别器接受真实和假图像，并试图区分两者。
- en: The two networks are trained by simultaneously competing in a game-theoretic
    manner to improve their performance over time. GANs work through a min–max game
    where the generator tries to maximize the discriminator’s mistakes, while the
    discriminator tries to minimize them.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个网络通过同时以博弈论的方式竞争来提高其性能而进行训练。GANs通过一个最小-最大博弈来工作，其中生成器试图最大化判别器的错误，而判别器则试图最小化这些错误。
- en: GANs use the prompt as an input to the generator, along with some random noise.
    The generator then produces an image that tries to match the prompt and sends
    it to the discriminator. The discriminator compares the generated image with a
    real image from the same prompt, giving a score that indicates how realistic it
    thinks the image is. The score is then used to update the weights of both networks
    using backpropagation and gradient descent. This process is repeated until the
    generator can create images that satisfy the prompt and fool the discriminator.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: GANs将提示作为输入传递给生成器，以及一些随机噪声。然后，生成器生成一个试图匹配提示的图像，并将其发送给判别器。判别器将生成的图像与来自同一提示的真实图像进行比较，给出一个分数，表示它认为图像有多逼真。然后，该分数被用来通过反向传播和梯度下降更新两个网络的权重。这个过程会重复进行，直到生成器可以创建满足提示并欺骗判别器的图像。
- en: The goal of the GAN is to make the generator produce realistic images that can
    fool the discriminator. Figure 4.5 shows what the GAN model architecture looks
    like at a high level. The latent space represents possible input for the generator,
    and the fine-tuning allows the parameters for the discriminator and the generator
    to be adjusted.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: GAN的目标是让生成器产生可以欺骗判别器的逼真图像。图4.5展示了GAN模型架构在高级别上的样子。潜在空间代表生成器的可能输入，而微调允许调整判别器和生成器的参数。
- en: '![figure](../Images/CH04_F05_Bahree.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH04_F05_Bahree.png)'
- en: Figure 4.5 GAN model architecture
  id: totrans-55
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.5 GAN模型架构
- en: GANs offer many similar use cases, such as VAEs, but they are specifically good
    for
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: GANs提供了许多与VAEs类似的使用案例，但它们在以下方面特别擅长：
- en: '*Image generation*—Creating realistic images from noise, with specific applications
    in entertaining, design, and art, allows generating high-quality images.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图像生成*—从噪声中创建逼真的图像，在娱乐、设计和艺术等特定应用中，允许生成高质量图像。'
- en: '*Style transfer*—Enabling artistic styles to transpose from one image to another;
    this is the same as in VAEs.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*风格迁移*—使艺术风格可以从一个图像转移到另一个图像；这与VAEs相同。'
- en: '*Super resolutions*—GANs can help enhance resolution, making images more detailed
    and clearer. This is very helpful in some industries, such as medical and space
    imaging.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*超分辨率*—GANs可以帮助提高分辨率，使图像更加详细和清晰。这在某些行业中非常有用，例如医疗和空间成像。'
- en: '*Data augmentation*—Similar to VAEs for creating synthetic data, GANs help
    create training data either for edge cases or where there is not enough data or
    data diversity.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据增强*—类似于VAEs用于创建合成数据，GANs帮助创建训练数据，无论是针对边缘情况还是数据不足或数据多样性不足的情况。'
- en: GANs can produce high-quality images that are indistinguishable from real ones.
    Still, they have drawbacks, such as mode collapse (i.e., the model repeatedly
    produces the same output), instability, and difficulty controlling the output.
    They also raise ethical concerns, as they can be quite easily used to create deepfakes
    that could lead to privacy invasion, potential misinformation, and misrepresentation.
    Finally, as with many other AI models, GANs can inadvertently perpetuate biases
    present in the training data in the generated output.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: GANs可以生成与真实图像难以区分的高质量图像。然而，它们也存在一些缺点，例如模式崩溃（即模型反复产生相同的输出）、不稳定性和难以控制输出。它们还引发了伦理问题，因为它们可以很容易地被用来创建可能导致隐私侵犯、潜在错误信息和误导的deepfakes。最后，与许多其他AI模型一样，GANs可能会无意中在生成的输出中延续训练数据中存在的偏差。
- en: 4.1.3 Vision transformer models
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.3 视觉Transformer模型
- en: Transformers are another model architecture that can create images. We saw the
    same architecture earlier in the context of natural language processing (NLP)
    tasks. Transformers can also operate on vision-related tasks and are called vision
    transformers (ViT) [2].
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers是另一种可以创建图像的模型架构。我们之前在自然语言处理（NLP）任务的上下文中看到了相同的架构。Transformers也可以在视觉相关任务上运行，被称为视觉Transformer（ViT）[2]。
- en: Transformers are neural networks that use attention mechanisms to process sequential
    data, such as text or speech, and they can be used to generate image prompts.
    They are also very effective for specific tasks such as image recognition and
    have outperformed previous leading model architectures.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer是一种使用注意力机制处理序列数据的神经网络，如文本或语音，并且可以用于生成图像提示。它们在特定任务（如图像识别）中也非常有效，并且已经超越了之前的领先模型架构。
- en: A ViT model’s architecture is similar to that of NLP, albeit with some differences—it
    has a larger number of self-attention layers and a global attention mechanism
    allowing the model to attend to all parts of the image simultaneously. Transformers
    calculate how much each input token is related to every other input token. This
    is called attention. The more tokens there are, the more attention calculations
    are needed. The number of attention calculations grows as the square of the number
    of tokens, that is, is quadratically.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ViT模型的架构类似于NLP，尽管有一些不同——它有更多的自注意力层和一个全局注意力机制，允许模型同时关注图像的所有部分。Transformer计算每个输入标记与每个其他输入标记的相关程度。这被称为注意力。标记越多，所需的注意力计算就越多。注意力计算的次数随着标记数量的平方增长，即二次方。
- en: For images, however, the basic unit of analysis is a pixel and not a token.
    The relationships for every pixel pair in a typical image are computationally
    prohibitive. Instead, ViT computes relationships among pixels in various small
    sections of the image (typically in 16 × 16-sized pixels), which helps reduce
    the computational cost. These 16 × 16-sized sections, along with their positional
    embeddings, are placed in a linear sequence and are the input to the transformer.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像而言，基本的分析单位是像素而不是标记。在典型图像中，每对像素之间的关系在计算上是不可行的。相反，ViT在图像的各个小部分（通常为16 × 16像素大小）中计算像素之间的关系，这有助于降低计算成本。这些16
    × 16像素大小的部分及其位置嵌入被放置在线性序列中，并作为transformer的输入。
- en: 'As shown in figure 4.6, a ViT model consists of three main sections: the left,
    middle, and right. The left section shows the input classes, such as `Class`,
    `Bird`, `Ball`, `Car`, and so forth. These are the possible labels that the model
    can assign to an image. The middle section shows the linear projection of flattened
    patches, which transform the input image into a sequence of vectors that can be
    fed to the transformer encoder. The final section is the transformer encoder.
    This comprises several multi-head attention and normalization layers and is used
    to learn the relationships between different image parts.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如图4.6所示，ViT模型由三个主要部分组成：左侧、中间和右侧。左侧部分显示了输入类别，如`类别`、`鸟`、`球`、`汽车`等。这些是模型可以分配给图像的可能标签。中间部分显示了展平补丁的线性投影，它将输入图像转换为可以输入到transformer编码器的一系列向量。最后一部分是transformer编码器。这包括多个多头注意力和归一化层，用于学习不同图像部分之间的关系。
- en: '![figure](../Images/CH04_F06_Bahree.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH04_F06_Bahree.png)'
- en: Figure 4.6 Vision transformer (ViT) architecture [2]
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.6 视觉Transformer（ViT）架构 [2]
- en: ViTs are used for various image use cases, such as segmentation, classification,
    and detection, and they are often more accurate than previous techniques. They
    also support fine-tuning, which can be used in a few-shot manner with smaller
    datasets, making them quite useful for enterprise use cases where we might not
    have much data. The ViT model aims to produce a final vector representation for
    the class token, which contains information about the whole image.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ViT被用于各种图像用例，如分割、分类和检测，并且通常比以前的技术更准确。它们还支持微调，可以在小数据集上以少量样本的方式使用，这使得它们在企业用例中非常有用，在这些用例中我们可能没有太多数据。ViT模型旨在为类别标记生成一个最终的向量表示，其中包含有关整个图像的信息。
- en: ViTs also have challenges such as high computational costs, data scarcity, and
    ethical issues. They are computationally complex both from a training and inference
    perspective and have low interpretability—both active research areas. Multimodal
    models, with ViTs such as GPT-4, hold much promise and unlock new enterprise possibilities.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ViT也面临一些挑战，如高计算成本、数据稀缺和伦理问题。它们在训练和推理方面都计算复杂，并且可解释性低——都是活跃的研究领域。具有ViT（如GPT-4）的多模态模型有很大的希望，并解锁了新的企业可能性。
- en: 4.1.4 Diffusion models
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.4 扩散模型
- en: Diffusion models are generative machine learning models that can create realistic
    data from random noise, such as images or audio. Their goal is to learn the latent
    structure of a dataset by modeling how data points diffuse through that latent
    space. The model is trained by slowly adding noise to an image and learning to
    reverse this by removing noise from the input until it resembles the desired output.
    For example, a diffusion model can generate an image of a panda by starting with
    a random image and then slowly removing noise until it looks like a panda.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型是生成式机器学习模型，可以从随机噪声（如图像或音频）中创建真实数据。它们的目标是通过模拟数据点如何通过潜在空间扩散来学习数据集的潜在结构。模型通过逐渐向图像添加噪声并学习通过从输入中去除噪声来反转这一过程进行训练，直到它类似于期望的输出。例如，扩散模型可以从一个随机图像开始，然后逐渐去除噪声，直到它看起来像一只熊猫。
- en: 'Vision diffusion models typically consist of two parts: a forward and a reverse
    diffusion process. The forward diffusion process is responsible for gradually
    adding noise to the latent representation of an image, which corrupts that latent
    space. The reverse diffusion process is just the opposite—it is responsible for
    reconstructing the original image from the corrupted latent representation.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉扩散模型通常由两部分组成：前向和反向扩散过程。前向扩散过程负责逐渐向图像的潜在表示添加噪声，从而损坏该潜在空间。反向扩散过程正好相反——它负责从损坏的潜在表示中重建原始图像。
- en: The forward diffusion process is typically implemented as a Markov chain (i.e.,
    a system with no memory of its past, and the probability of the next step depends
    on the current state). This means the corrupted latent representation at each
    step depends only on the previous step’s latent representation, which makes the
    forward diffusion process efficient and easy to train.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 前向扩散过程通常被实现为一个马尔可夫链（即一个没有记忆其过去状态的系统，下一步的概率取决于当前状态）。这意味着每个步骤的损坏潜在表示只依赖于前一步的潜在表示，这使得前向扩散过程高效且易于训练。
- en: The reverse diffusion process is typically implemented as a neural network,
    meaning the neural network learns to reverse the forward diffusion process by
    predicting the original latent representation from the corrupted one. This reverse
    diffusion process is slow, as it is a step-by-step repetition.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 反向扩散过程通常被实现为一个神经网络，这意味着神经网络通过预测从损坏的表示中恢复原始潜在表示来学习反向前向扩散过程。这个反向扩散过程很慢，因为它是一步一步重复的。
- en: 'Some of the advantages that diffusion models have are the following:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型的一些优点如下：
- en: They can produce high-quality images that match or beat GAN-generated images,
    especially for complex scenes, but they take much longer to generate.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们可以生成与GAN生成的图像相匹配或超越的高质量图像，尤其是在复杂场景中，但生成时间要长得多。
- en: They do not suffer from mode collapse, a common problem for GANs. Mode collapse
    occurs when the generator produces only a limited variety of outputs, ignoring
    some modes of data distribution.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们不会受到模式崩溃的影响，这是GANs的常见问题。模式崩溃发生在生成器只产生有限种类的输出时，忽略了数据分布的一些模式。
- en: Diffusion models can capture the full diversity of the data distribution by
    using a Markov chain process that adds noise to the input data.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩散模型可以通过使用向输入数据添加噪声的马尔可夫链过程来捕捉数据分布的全貌。
- en: Diffusion models can be combined with others, such as natural language models,
    to create text-guided generation systems.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩散模型可以与其他模型（如自然语言模型）结合，以创建文本引导的生成系统。
- en: 'Stable Diffusion is one of the most popular diffusion-based models for image
    generation. Its architecture consists of three main parts (see figure 4.7):'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定扩散是图像生成中最受欢迎的基于扩散的模型之一。其架构由三个主要部分组成（见图4.7）：
- en: The text encoder, which converts the user’s prompt into a vector representation.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本编码器，它将用户的提示转换为向量表示。
- en: A denoising autoencoder (called UNet), which is used to reconstruct an image
    from the latency space, and a scheduler algorithm, which helps reconstruct the
    original image. We call it the *image information creator*. The UNet is a denoising
    autoencoder because it learns to remove noise from the input image and produce
    a clean output image. It is a neural network that has an encoder–decoder structure.
    The encoder part reduces the resolution of an input image and extracts its features.
    On the other hand, the decoder part increases the resolution and reconstructs
    the output image.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于从延迟空间重建图像的降噪自动编码器（称为UNet），以及一个调度算法，它有助于重建原始图像。我们称之为*图像信息创建器*。UNet是一个降噪自动编码器，因为它学会了从输入图像中去除噪声并产生一个干净的输出图像。它是一个具有编码器-解码器结构的神经网络。编码器部分降低输入图像的分辨率并提取其特征。另一方面，解码器部分增加分辨率并重建输出图像。
- en: A variational autoencoder (VAE), which creates an image as close as possible
    to a normal distribution.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个变分自动编码器（VAE），它创建的图像尽可能接近正态分布。
- en: '![figure](../Images/CH04_F07_Bahree.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH04_F07_Bahree.png)'
- en: Figure 4.7 Stable Diffusion logical architecture
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.7 稳定扩散逻辑架构
- en: The choice between these models depends on the specific application, the availability
    of computing resources, training data, and nonfunctional requirements such as
    image quality, speed, and so forth. Table 4.1 lists some of the more common generative
    AI vision systems that can create images from text.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型的选择取决于具体应用、计算资源的可用性、训练数据和诸如图像质量、速度等非功能性要求。表4.1列出了可以创建文本图像的一些更常见的生成式AI视觉系统。
- en: Table 4.1 Most common AI vision tools
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表4.1 最常见的AI视觉工具
- en: '| AI vision tool | Description |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| AI视觉工具 | 描述 |'
- en: '| --- | --- |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Imagen  | Imagen is Google’s text-to-image diffusion model, which can generate
    realistic images from text descriptions. It is available in limited preview and
    has been shown to generate images indistinguishable from real photographs.  |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Imagen | Imagen是谷歌的文本到图像扩散模型，可以从文本描述生成逼真的图像。它目前处于有限预览中，并已被证明可以生成与真实照片难以区分的图像。|'
- en: '| DALL-E  | OpenAI developed a transformer language model to create diverse,
    original, realistic, and creative images and art from a prompt. It can edit images
    based on the context, such as adding, deleting, or changing specific parts. It
    has generated various images, from everyday objects to surrealistic art, from
    simple text prompts. DALL-E 3 is an improved version that can generate more realistic
    and accurate images with 4x greater resolution.  |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| DALL-E | OpenAI开发了一个转换语言模型，用于根据提示创建多样化的、原创的、逼真的和创意图像和艺术。它可以基于上下文编辑图像，例如添加、删除或更改特定部分。它已生成各种图像，从日常物体到超现实主义艺术，从简单的文本提示中。DALL-E
    3是一个改进版本，可以以4倍更高的分辨率生成更真实、更准确的图像。'
- en: '| Midjourney  | AI-based art generator that uses deep learning and neural networks
    to create artwork based on prompts and other images and videos. This is accessible
    only via a Discord server, and the results can be tailored to any aesthetics,
    from abstract to realistic, thus offering endless possibilities for creative expression.  |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Midjourney | 基于AI的艺术生成器，使用深度学习和神经网络根据提示和其他图像、视频创建艺术品。这仅可通过Discord服务器访问，结果可以根据任何美学定制，从抽象到现实，从而为创意表达提供无限可能。|'
- en: '| Adobe Firefly  | Adobe Firefly is a family of creative, generative AI diffusion
    models designed to help designers and creative professionals create images and
    text effects and edit and recolor. It is easy to use with Adobe’s other tools,
    such as Photoshop and Illustrator. Adobe has both text-to-image models and generative
    fill models.  |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| Adobe Firefly | Adobe Firefly是一系列创意生成式AI扩散模型，旨在帮助设计师和创意专业人士创建图像和文本效果，以及编辑和重新着色。它易于与Adobe的其他工具（如Photoshop和Illustrator）一起使用。Adobe拥有文本到图像模型和生成式填充模型。|'
- en: '| Stable Diffusion  | Popular models include versions of Stable Diffusion XL
    and v1.6, an image- generating model that uses diffusion models to create high-quality
    images using prompts with next-level photorealism capabilities. It can also generate
    novel images from text descriptions. The more recent v3 family of models comes
    in large and medium with 8B and 2B parameters, respectively.  |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 稳定扩散 | 流行模型包括稳定扩散XL和v1.6版本，这是一个使用扩散模型通过具有更高级别照片逼真能力的提示来创建高质量图像的图像生成模型。它还可以根据文本描述生成新颖的图像。较新的v3系列模型分别具有8B和2B参数的大中小版本。|'
- en: Many of the AI vision models listed in table 4.1 are available only to those
    who were invited to test them. This is still a new space, and most providers are
    going slowly, learning with a handful of customers before rolling these out.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Creating and manipulating images with generative AI is an exciting and challenging
    research area with many potential applications and implications. However, it raises
    ethical and social questions about the generated content’s ownership, authenticity,
    and effects. Therefore, it is important to use generative AI responsibly and ethically
    and to consider its benefits and risks to society.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.5 Multimodal models
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A multimodal model can handle different types of input data. “Modal” refers
    to the mode or type of data, and “multimodal” refers to multiple data types. These
    types include text, images, audio, video, and more. For example, GPT-4 has a multimodal
    model variant that takes both an image and an associated prompt to make predictions
    or inferences.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'Bing Chat recently enabled this multimodal feature, allowing us to use images
    and text in the prompt. For example, as shown in figure 4.8, we give the model
    two things: an image and a prompt related to the image. In this case, we show
    some produce and ask the model what we can cook with it.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F08_Bahree.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 Multimodal example using both an image and a prompt
  id: totrans-103
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In this case, the model must understand the image and the different parts (i.e.,
    ingredients in our example) and correlate to the prompt to generate an answer.
    We see the response in the shaded text, showing we can make guacamole, salsa,
    avocado toast, and so forth.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal models often use different AI techniques. While they can use different
    combinations of model architecture, in our example, GPT-4 combines different transformer
    blocks (figure 4.9).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F09_Bahree.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 Multimodal model design
  id: totrans-107
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Note  When showing transformer blocks, as in figure 4.9, the convention is
    to use Nx, referring to the transformer block repeating multiple times; in other
    words, it is stacked x number of times. In our multimodal example, this is the
    case for all three transformer blocks: the image on the left (Lx), the text on
    the right (Rx), and the combining layer (Nx).'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'Multimodal models are particularly useful in complex real-world applications
    where data comes in various forms. For example:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '*Web*—Analyzes text and images for content moderation and sentiment analysis'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*eCommerce*—Recommends products using both photos and text descriptions'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Healthcare*—Uses text data (patient medical history) and medical imaging (image
    data) for diagnosis'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Self-driving*—Integrates sensor data (radar and lidar) with visual data (cameras)
    for situational awareness and decision-making'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have seen some models, their output, and a general sense of how
    vision AI models work, let us generate images with Stable Diffusion.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Image generation with Stable Diffusion
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Stability AI, the company behind the Stable Diffusion, has advanced diffusion-based
    models with SDXL as their latest and most powerful model thus far. They offer
    multiple options for us to use:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定扩散背后的公司Stability AI，已经推出了先进的基于扩散的模型，其中SDXL是迄今为止最新且最强大的模型。他们为我们提供了多种选择：
- en: '*Self-host*—The model and associated weights have been published and are available
    via Hugging Face ([https://huggingface.co/stabilityai](https://huggingface.co/stabilityai)).
    They can be self-hosted, requiring the appropriate computing hardware, including
    GPUs.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自托管*——模型及其相关权重已发布，并通过Hugging Face（[https://huggingface.co/stabilityai](https://huggingface.co/stabilityai)）提供。它们可以自托管，需要适当的计算硬件，包括GPU。'
- en: '*DreamStudio*—This StabilityAI’s consumer application targets consumers. It
    is a simple web interface that generates images. The company also has an open
    source version called StableStudio, driven by the community. More details on DreamStudio
    can be found at [https://dreamstudio.ai](https://dreamstudio.ai).'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*DreamStudio*——这是StabilityAI面向消费者的应用程序。它是一个简单的网页界面，用于生成图像。该公司还有一个名为StableStudio的开源版本，由社区驱动。有关DreamStudio的更多详细信息，请参阅[https://dreamstudio.ai](https://dreamstudio.ai)。'
- en: '*Platform APIs*—Stability AI has a platform API ([https://platform.stability.ai](https://platform.stability.ai))
    that we will use in this book, given that most enterprises would prefer an API
    that can be managed better at scale. REST API will be used for our example here,
    as it shows the most flexibility across all platforms. Stable Diffusion also has
    a gRPC API, which is quite similar.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*平台API*——Stability AI有一个平台API（[https://platform.stability.ai](https://platform.stability.ai)），我们将在本书中使用它，因为大多数企业更倾向于一个可以更好地进行规模管理的API。在此示例中，我们将使用REST
    API，因为它在所有平台上都显示出最大的灵活性。稳定扩散还有一个gRPC API，非常相似。'
- en: 4.2.1 Dependencies
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.1 依赖项
- en: 'We will build on the packages required earlier in chapter 3 and assume that
    the following are installed: Python, development IDE, and a virtual environment
    (such as `conda`). For Stable Diffusion, we need the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第3章中早期需要的软件包的基础上构建，并假设以下都已安装：Python、开发IDE和虚拟环境（如`conda`）。对于稳定扩散，我们需要以下内容：
- en: 'A Stability AI account and associated API key; this can be acquired via the
    account page at [https://platform.stability.ai/account/keys](https://platform.stability.ai/account/keys).
    Billing details also need to be set up at the same place. We pip install the `stability-sdk`
    Python package: `pip` `install` `stability-sdk`.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个Stability AI账户及其关联的API密钥；这可以通过[https://platform.stability.ai/account/keys](https://platform.stability.ai/account/keys)的账户页面获取。同样，也需要在该处设置账单详情。我们使用pip安装`stability-sdk`
    Python软件包：`pip` `install` `stability-sdk`。
- en: 'Keep the API key confidential, and follow best practices for managing secrets.
    We will use environmental variables to store the key securely, which can be configured
    as follows:'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保密API密钥，并遵循管理秘密的最佳实践。我们将使用环境变量来安全地存储密钥，配置如下：
- en: '*Windows*—`setx` `STABILITY` `API` `KEY` `“your-openai-key”`'
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Windows*——`setx` `STABILITY` `API` `KEY` `“your-openai-key”`'
- en: '*Linux/Mac*—`export` `STABILITY` `API` `KEY=your-openai-endpoint`'
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Linux/Mac*——`export` `STABILITY` `API` `KEY=your-openai-endpoint`'
- en: '*Bash*—`echo` `export` `STABILITY_API_KEY="YOUR_KEY"` `>>` `/etc/environment
    &&` `source` `/etc/environment`'
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Bash*——`echo` `export` `STABILITY_API_KEY="YOUR_KEY"` `>>` `/etc/environment
    &&` `source` `/etc/environment`'
- en: We start by getting a list of all the models available using the engines API,
    including all the available engines (i.e., models).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用引擎API获取所有可用的模型列表，包括所有可用的引擎（即模型）。
- en: 'Listing 4.1 Stable Diffusion: Listing the models'
  id: totrans-128
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.1 稳定扩散：列出模型
- en: '[PRE0]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 REST API call for getting the models'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 获取模型的REST API调用'
- en: '#2 HTTP header for authorization'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 授权的HTTP头'
- en: '#3 Response back from the API'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 API的响应'
- en: '#4 Making the JSON more human-readable'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 使JSON更易于阅读'
- en: The output of this code is presented in the next listing. This shows us the
    engines we must use and helps in testing end to end to confirm that the API call
    works and that we can authenticate and get a response.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码的输出将在下一列表中展示。这显示了我们必须使用的引擎，并有助于测试端到端，以确认API调用是否工作，并且我们可以进行身份验证并获得响应。
- en: 'Listing 4.2 Output: Stable Diffusion model lists'
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.2 输出：稳定扩散模型列表
- en: '[PRE1]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 4.2.2 Generating an image
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.2 生成图像
- en: We use the Stable Diffusion image generation endpoint (REST API) for our image
    generation. We will use the latest model, the SDXL model, at the time of this
    publication. The corresponding engine ID for this model is `stable-diffusion-xl-1024-v1-0`,
    as shown in the previous example listing of models. This engine ID is required
    as part of the REST API path parameter and is available at https://api.stability.ai/v1/generation/{engine_id}/text-to-image.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 4.3 shows an example of using this API to generate an image. Note that
    we use v1.0 of the API for the examples in this chapter. To use the newer models,
    we only need to change the REST API path in most cases. For example, to use the
    newer models that have just been announced, Stable Diffusion 3 and currently in
    Beta, switch to the following engine ID: `https://api.stability.ai/v2beta/stable-image/generate/sd3`.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 4.3 Stable Diffusion: Image generation'
  id: totrans-140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#1 Choose the model we want to use.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Prompts used to generate the image'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Helper functions to create filenames'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '#4 API call for generating the image'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '#5 The REST API Endpoint includes the engine ID.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Parameters controlling the model generation'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Response from the API once the generation finishes'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Saves the image locally'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: The image of a “laughing panda in the clouds eating bamboo” was generated, as
    shown in figure 4.10\. It is quite a happy and lifelike panda.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F10_Bahree.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 An image generated by Stability Diffusion
  id: totrans-152
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Some of the Stability Diffusion API’s parameters are similar to those we have
    already seen. Still, some are different, given that the underlying model architecture
    differs from what is presented in table 4.2\. Because we are using the REST API,
    there are also two sets of parameters—one set is the header parameters, and the
    other is for the body.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4.2 Stable Diffusion header parameters: Image Create API'
  id: totrans-154
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Parameter | Type | Default value | Description |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
- en: '| `Accept`  | String  | Blank `(application/json)`  | The response format can
    be default (blank) JSON or set to `image/png` for PNG image.  |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
- en: '| `Organization`  | String  | Null (optional)  | A tag that allows requests
    to be scoped to an organization other than the user’s default. This parameter
    can help debug, monitor, or detect abuse.  |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
- en: '| `Stability-Client-ID`  | String  | Null (optional)  | This parameter is used
    to identify the source of requests, such as the client application or suborganization.
    It can help debug, monitor, and detect abuse.  |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
- en: '| `Stability-Client-Version`  | String  | Null (optional)  | This parameter
    identifies the version of the application or service making the requests. It can
    help debug, monitor, and detect abuse.  |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
- en: '| `Authorization`  | String  | Bearer `API_KEY`  | Key required to authenticate
    the API call  |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
- en: Table 4.3 outlines the parameters that constitute the body of the API call.
    These parameters can fine-tune the model and steer it closer to what we want to
    generate.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4.3 Stable Diffusion body parameters: Image Create API'
  id: totrans-163
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Parameter | Type | Default value | Description |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
- en: '| `height`and `width`  | Integer  | 512 (optional)  | The height and width
    of the image in pixels must be in increments of 64 and must be one of the following
    combinations: 1024 x 1024, 1152 x 896, 1216 x 832, 1344 x 768, 1536 x 640, 640
    x 1536, 768 x 1344, 832 x 1216, and 896 x 1152\. Note that some of these vary
    based on the engine used.  |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
- en: '| `text_prompts`  | String  | Null (required)  | An array of text prompts is
    used to generate the image. Two properties make up each element in this array—one
    of the prompts itself and the other the associated weight of that prompt. The
    weights should be negative for negative prompts. For example: `"text_prompts":
    [{`'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '`"text": "A dog on a mat",`'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '`"weight": 0.7`'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '`}]`'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: The text property can be up to 2,000 characters.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '| `cfg_scale`  | String  | 7 (optional)  | This can range between 0 and 35;
    it defines how strictly the diffusion process follows the prompt. Higher values
    keep the image closer to the prompt.  |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
- en: '| `clip_guidance_preset`  | String  | `None` (optional)  | Different values
    control how much CLIP guidance is used, and it controls the quality and relevance
    of the image being generated. Values are `NONE`, `FAST_BLUE`, `FAST_GREEN`, `SIMPLE`,
    `SLOW`, `SLOWER`, and `SLOWEST`.  |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
- en: '| `sampler`  | String  | Null (optional)  | This defines the sampler to use
    for the diffusion process. If this value is omitted, the API automatically selects
    an appropriate sampler for you. Values are `DDIM`, `DDPM`, `K_DPMPP_2M`, `K_DPM_2`,
    `K_EULER K_DPMPP_2S_ANCESTRAL`, `K_HEUN`, `K_DPM_2_ANCESTRAL`, `K_LMS`, `K_EULER_ANCESTRAL`.  |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
- en: '| `samples`  | Integer  | 1 (optional)  | Specifies the number of images to
    generate. Values need to range between 1 and 10\.  |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
- en: '| `seed`  | Integer  | 0 (optional)  | A random seed is a number that determines
    how the noise looks. Leave 0 for a random seed value. The possible value ranges
    between 0 and 4294967295\.  |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
- en: '| `steps`  | Integer  | 50 (optional)  | Defines the number of diffusion steps
    to run. Values range between 10 and 150\.  |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
- en: '| `style_preset`  | String  | Null (optional)  | Used to guide the image model
    toward a particular preset style. Values are `3d-model`, `analog-film`, `anime`,
    `cinematic`, `comic-book`, `digital-art`, `enhance`, `fantasy-art`, `isometric`,
    `line-art`, `low-poly`, `modeling-compound`, `neon-punk`, `origami`, `photographic`,
    `pixel-art`, and `tile-texture`. Note: This list of style presets is subject to
    change over time.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s look at some other ways we can create images.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Image generation with other providers
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we want to generate images, a few other vendors also have generative AI
    vision models; however, they don’t have a platform or API. In this section, we
    will show other platforms that allow one to create images but don’t have APIs,
    and in most cases, they need to be accessed via their GUI.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 OpenAI DALLE 3
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DALLE 3 is the newer image generation model from OpenAI that can create images
    from a prompt. It was one of the first image generation models with which most
    people could interact. DALLE stands for Discrete Autoencoder Language Latent Encoder,
    which means it employs a special type of neural network to encode images and text
    to tokens and then uses those tokens to create images. DALLE can be used both
    via an API and a GUI.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: DALLE 3 是 OpenAI 的新图像生成模型，可以从提示中创建图像。它是第一个大多数人都能够与之交互的图像生成模型之一。DALLE 代表离散自动编码器语言潜在编码器，这意味着它使用一种特殊的神经网络来编码图像和文本为标记，然后使用这些标记来创建图像。DALLE
    可以通过 API 和 GUI 两种方式使用。
- en: Given that the images generated with DALLE are similar to Stable Diffusion,
    we don’t get into the API details here. The GitHub code repository accompanying
    the book ([https://bit.ly/GenAIBook](https://bit.ly/GenAIBook)) has DALLE’s API
    and code samples.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 由于使用 DALLE 生成的图像与 Stable Diffusion 类似，我们在此不深入讲解 API 的细节。本书附带的 GitHub 代码仓库（[https://bit.ly/GenAIBook](https://bit.ly/GenAIBook)）包含了
    DALLE 的 API 和代码示例。
- en: 4.3.2 Bing image creator
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.2 Bing 图像创建器
- en: Bing has an image creator application that uses DALLE internally, but the images
    it creates are enhanced and a bit different. We only need a web browser to use
    it; an API isn’t exposed. We can generate images by going to [https://www.bing.com/create](https://www.bing.com/create)
    and entering the prompt. There aren’t many tweaks one can make other than those
    specified in the prompt itself. Figure 4.11 shows the generation of a “serene
    vacation lake house, watercolor painting with a dog.” We will use one of these
    images later to see how to edit an image.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: Bing 拥有一个使用 DALLE 内部技术的图像创建应用程序，但它创建的图像经过了增强，并且略有不同。我们只需要一个网络浏览器就可以使用它；没有暴露
    API。我们可以通过访问 [https://www.bing.com/create](https://www.bing.com/create) 并输入提示来生成图像。除了提示中指定的那些调整之外，我们无法进行很多其他调整。图
    4.11 展示了生成一幅“宁静的度假湖屋，带有狗的水彩画”的过程。我们将在稍后使用这些图像之一来展示如何编辑图像。
- en: '![figure](../Images/CH04_F11_Bahree.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH04_F11_Bahree.png)'
- en: 'Figure 4.11 Bing Create: Creating an image depicted as a watercolor painting'
  id: totrans-190
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.11 Bing Create：创建一幅水彩画风格的图像
- en: 4.3.3 Adobe Firefly
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.3 Adobe Firefly
- en: Adobe has a set of Generative AI tools, with Firefly being their family of Generative
    AI models. It is being integrated into various Adobe products, such as Photoshop,
    and is accessible via [https://firefly.adobe.com/](https://firefly.adobe.com/).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Adobe 拥有一套生成式 AI 工具，其中 Firefly 是其生成式 AI 模型系列。它正在集成到各种 Adobe 产品中，如 Photoshop，并且可以通过
    [https://firefly.adobe.com/](https://firefly.adobe.com/) 访问。
- en: 'Although there isn’t an API, the overall process and modality are the same
    as we saw earlier with OpenAI. Once we log in, we are presented with a UI where
    we enter the prompt and generate the images. Let us use one of the previous examples:
    “laughing panda in the clouds eating bamboo.” Four images are created by default
    (figure 4.12).'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然没有 API，但整个过程和模式与我们在 OpenAI 中看到的相同。一旦我们登录，就会看到一个 UI，我们可以在其中输入提示并生成图像。让我们使用一个之前的例子：“云中的笑熊猫吃竹子。”默认情况下创建了四幅图像（图
    4.12）。
- en: '![figure](../Images/CH04_F12_Bahree.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH04_F12_Bahree.png)'
- en: Figure 4.12 Adobe Firefly generative vision
  id: totrans-195
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.12 Adobe Firefly 生成视觉
- en: Note  Google recently announced its generative AI suite of APIs called Vertex
    AI; at the time of publication, the Vision APIs, which are also built on diffusion
    models, weren’t available for use.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：谷歌最近宣布了其名为 Vertex AI 的生成式 AI API 套件；在本书出版时，基于扩散模型的视觉 API 尚未开放使用。
- en: Now that we have created an image, let’s see how to edit and enhance it.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了一张图像，接下来让我们看看如何编辑和增强它。
- en: 4.4 Editing and enhancing images using Stable Diffusion
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4 使用 Stable Diffusion 编辑和增强图像
- en: In addition to generating images, Stable Diffusion allows us to edit and enhance
    images. We use Stable Diffusion web UI, one of the open source web interfaces
    for Stable Diffusion, to show how to use inpainting and enhance the images. The
    web interface is a wrapper around the model, and while it doesn’t call the API,
    it has the same properties.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 除了生成图像，Stable Diffusion 还允许我们编辑和增强图像。我们使用 Stable Diffusion 的网络 UI，这是 Stable
    Diffusion 的开源网络界面之一，来展示如何使用修复和增强图像。网络界面是模型的一个包装器，虽然它不调用 API，但它具有相同的属性。
- en: 'We start by using one of the images of a watercolor painting we generated earlier.
    In this example, we mask two areas: the dog and the different colors on the bottom
    left of the image (figure 4.13).'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用我们之前生成的一幅水彩画图像。在这个例子中，我们遮罩了两个区域：图像左下角的不同颜色和狗（图 4.13）。
- en: '![figure](../Images/CH04_F13_Bahree.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH04_F13_Bahree.png)'
- en: Figure 4.13 Inpainting sketch
  id: totrans-202
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.13 修复草图
- en: When we upload the image to Inpaint, one of the web application’s features is
    to use a CLIP model to interrogate the image and guess the prompt. Even though
    we know the prompt from the original generation, this is a different model, and
    it would be advisable to let Stable Diffusion figure out the prompt. The results
    are shown in figure 4.14.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将图像上传到 Inpaint 时，该网络应用程序的一个功能是使用 CLIP 模型来调查图像并猜测提示。尽管我们知道原始生成的提示，但这是一个不同的模型，因此建议让
    Stable Diffusion 确定提示。结果如图 4.14 所示。
- en: '![figure](../Images/CH04_F14_Bahree.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH04_F14_Bahree.png)'
- en: Figure 4.14 Guess the image prompt using a CLIP model
  id: totrans-205
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.14 使用 CLIP 模型猜测图像提示
- en: CLIP model
  id: totrans-206
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: CLIP 模型
- en: CLIP (Contrastive Language–Image Pre-training) is a neural network created by
    OpenAI that links text and images. It can comprehend and classify images to match
    natural language descriptions. This is done through a technique called *contrastive
    learning*, where the model learns from a large number of images and related text
    pairs sourced from the internet.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP（对比语言-图像预训练）是由 OpenAI 创建的一种神经网络，它将文本和图像联系起来。它能够理解和分类图像，以匹配自然语言描述。这是通过一种称为**对比学习**的技术实现的，模型从互联网上收集的大量图像和相关文本对中学习。
- en: CLIP’s unique ability to do “zero-shot” learning means it can accurately label
    images it has never encountered before based on text alone without requiring direct
    fine-tuning for that particular task. For example, CLIP can be given the names
    of visual classes and identify them in images, even if it wasn’t specifically
    trained on them.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP 的独特能力进行“零样本”学习意味着它可以根据文本准确地对它以前从未见过的图像进行标记，而无需对该特定任务进行直接微调。例如，CLIP 可以给出视觉类别的名称，并在图像中识别它们，即使它没有专门针对它们进行训练。
- en: CLIP encodes both text and images into a common representation space. It can
    estimate the most suitable text snippet for an image or vice versa. This gives
    it much flexibility and the ability to handle different kinds of visual tasks
    without requiring training data specific to each task.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP 将文本和图像编码到公共表示空间中。它可以估计最适合图像的文本片段或反之亦然。这赋予它很大的灵活性，能够处理不同类型的视觉任务，而无需针对每个任务特定的训练数据。
- en: As shown in figure 4.15, additional settings for inpainting allow for finer
    control. Some of these are the same as image generation and are equally important,
    such as the number of sampling steps and methods.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 4.15 所示，修复的附加设置允许进行更精细的控制。其中一些与图像生成相同，同样重要，例如采样步骤的数量和方法。
- en: '![figure](../Images/CH04_F15_Bahree.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH04_F15_Bahree.png)'
- en: Figure 4.15 Stable Diffusion inpainting options
  id: totrans-212
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.15 Stable Diffusion 修复选项
- en: Outpainting is an additional setting that generates and expands the image in
    our chosen direction. This option is selected via the Script dropdown on the same
    settings tab (figure 4.16).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展是一种额外的设置，它可以在我们选择的方向上生成和扩展图像。此选项通过同一设置选项卡上的脚本下拉菜单选择（图 4.16）。
- en: '![figure](../Images/CH04_F16_Bahree.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH04_F16_Bahree.png)'
- en: Figure 4.16 Outpainting settings in Stable Diffusion
  id: totrans-215
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.16 Stable Diffusion 的扩展设置
- en: We go through the iteration of inpainting by removing the areas we want using
    the mask, regenerating, and then adding the new elements. The final result of
    these iterations is shown in figure 4.17.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过使用遮罩去除我们想要删除的区域，进行再生，然后添加新元素，来完成修复的迭代过程。这些迭代的最终结果如图 4.17 所示。
- en: '![figure](../Images/CH04_F17_Bahree.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH04_F17_Bahree.png)'
- en: Figure 4.17 Final edits of inpainting using Stable Diffusion
  id: totrans-218
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.17 使用 Stable Diffusion 的修复最终编辑
- en: Note  The details on Stable Diffusion web UI, including setup, configuration,
    and deployments, are outside the scope of this book; however, it is one of the
    very popular applications that allow one to self-host across Windows, Linux, and
    MacOS. You can find more details at their GitHub repository ([https://mng.bz/znx1](https://mng.bz/znx1)).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：关于 Stable Diffusion 网络用户界面的详细信息，包括设置、配置和部署，超出了本书的范围；然而，它是一个非常流行的应用程序，允许用户在
    Windows、Linux 和 MacOS 上自行托管。更多详细信息可以在他们的 GitHub 仓库中找到（[https://mng.bz/znx1](https://mng.bz/znx1)）。
- en: 4.4.1 Generating using image-to-image API
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.1 使用图像到图像 API 生成
- en: Image-to-image is a powerful tool for generating or modifying new images that
    use existing images as a starting point and a text prompt. We can use this API
    to generate a new image but change the style and mood and add or remove aspects.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图像到图像是一个强大的工具，用于生成或修改新图像，它以现有图像作为起点和文本提示。我们可以使用这个 API 生成新图像，但可以改变风格和情绪，添加或删除某些方面。
- en: Let’s use our serene lake example from earlier and then use the image-to-image
    API to generate a new image. We build on both examples we have seen earlier—we
    use the serene lake as our input and ask the model to generate “a happy panda
    eating bamboo in the sky.”
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.4 Image-to-image generation
  id: totrans-223
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We see the generated image as shown on the left in figure 4.18 of the image-to-image
    API call; we see the panda and the bamboo and how the input image to set the scene
    and the type and aesthetic of the generated image are used. However, it doesn’t
    adhere to the cloud aspect of the prompt.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: We can tweak the parameters to make it adhere more to the prompt and less to
    the input image, as shown on the right side of figure 4.18\. An example is when
    we see a panda in the sky, eating bamboo; overall, the image aesthetics follows
    the input image.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F18_Bahree.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
- en: Figure 4.18 Stable Diffusion image-to-image generation
  id: totrans-228
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 4.4.2 Using the masking API
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Stable Diffusion also has a masking API that allows us to edit portions of
    an image programmatically. The API is very similar to the creation API, as shown
    in the example in listing 4.5\. It does have a few constraints: the mask image
    needs to be the same dimension as the original image, and a PNG, less than 4MB
    in size. The API has the same header parameters outlined earlier in the chapter
    when we discussed image generation; we will avoid duplicating that.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.5 Stable Diffusion masking API example
  id: totrans-231
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#1 Selects the inpainting model we want to use'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Image we want to edit'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Masks that we want to apply'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Masks API call'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Selects the black pixels of the image to be replaced'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Prompts for the generation'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Specifies the number of images to generate'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Determines the number of steps for each of the images'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Gets the response from the API'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '#10 Saves the edited image to disk'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.4 outlines all the API parameters. In terms of options to steer the
    model, much of it is similar to the previous image creation.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.4 Stable Diffusion masking API parameters
  id: totrans-244
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Parameter | Type | Default value | Description |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
- en: '| `init_image`  | String  | Binary (required)  | The initial image that we
    want to edit  |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
- en: '| `mask_source`  | String  | Null (required)  | Mask details that determine
    the generation areas and associated strengths. It can be one of the following:
    `MASK_IMAGE_WHITE`—Use white pixels as the mask; white pixels are modified; black
    pixels are unchanged.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '`MASK_IMAGE_BLACK`—Use black pixels as the mask; black pixels are modified;
    white pixels are unchanged'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '`INIT_IMAGE_ALPHA`—Use the alpha channel as the mask. Edit fully transparent
    pixels, and leave fully opaque pixels unchanged.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '| `mask_image`  | String  | Binary (required)  | Mask image that guides the
    model on which pixels need to be modified. This parameter is used only if the
    `mask_source` is either `MASK_IMAGE_BLACK` or `MASK_IMAGE_WHITE` `.`  |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
- en: '| `text_prompts`  | String  | Null (required)  | An array of text prompts is
    used to generate the image. Each element in this array comprises two properties—one
    of the prompt itself and the other of the associated weight. The weights should
    be negative for negative prompts. The prompts need to adhere to the following
    format: `text_prompts[index][text&#124;weight]`, with the index being unique and
    not having to be sequential.  |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
- en: '| `cfg_scale`  | String  | 7 (optional)  | Can range between 0 and 35; it defines
    how strictly the diffusion process follows the prompt. Higher values keep the
    image closer to the prompt.  |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
- en: '| `clip_guidance_preset`  | String  | None (optional)  | Different values control
    how much CLIP guidance is used and influence the quality and relevance of the
    image being generated. Possible values are `NONE`, `FAST_BLUE`, `FAST_GREEN`,
    `SIMPLE`, `SLOW`, `SLOWER` `,` and `SLOWEST`.  |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
- en: '| `sampler`  | String  | Null (optional)  | Defines the sampler to use for
    the diffusion process. If this value is omitted, the API automatically selects
    an appropriate sampler for you. Possible values are `DDIM`, `DDPM`, `K_DPMPP_2M`,
    `K_DPM_2`, `K_EULER K_DPMPP_2S_ANCESTRAL`, `K_HEUN`, `K_DPM_2_ANCESTRAL`, `K_LMS`,
    and `K_EULER_ANCESTRAL`.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '| `samples`  | Integer  | 1 (optional)  | Defines the number of images to generate.
    Values need to range between 1 and 10\.  |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
- en: '| `seed`  | Integer  | 0 (optional)  | A random seed is a number that determines
    how the noise looks. Leave 0 for a random seed value. The possible value ranges
    between 0 and 4294967295\.  |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
- en: '| `steps`  | Integer  | 50 (optional)  | Defines the number of diffusion steps
    to run. Possible values range between 10 and 150\.  |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
- en: '| `style_preset`  | String  | Null (optional)  | Used to guide the image model
    towards a particular preset style. Possible values are `3d-model`, `analog-film`,
    `anime`, `cinematic`, `comic-book`, `digital-art`, `enhance`, `fantasy-art`, `isometric`,
    `line-art`, `low-poly`, `modeling-compound`, `neon-punk`, `origami`, `photographic`,
    `pixel-art`, and `tile-texture`. Note: This list of style presets is subject to
    change over time.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.3 Resize using the upscale API
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The final Stable Diffusion API we want to cover is used to upscale an image,
    that is, generate a higher-resolution image of a given image. The default is to
    upscale the input image by a factor of two, with a maximum pixel count of 4,194,304,
    equivalent to a maximum dimension of 2,04…,048 and 4,09…,024.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: The API is straightforward, as shown in the next listing. The main thing to
    be aware of is using the right model via the `engine_id` parameter.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.6 Stable Diffusion resizing API
  id: totrans-266
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now that we have examined numerous image-generation options using both GUIs
    and APIs, let’s examine some of the best practices for enterprises.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Watermark for AI-generated images
  id: totrans-269
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Since AI-generated images are getting increasingly better, and we often cannot
    distinguish between real and AI-generated images, there is a push to watermark
    AI-generated images. There are two main ways to do this today: visible watermarks,
    like what Bing and DALLE do, and invisible watermarks, which are not visible to
    us but are embedded in the image and can be detected using special tools.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Google has gone a step further and developed a new type of watermark called
    SynthID. An invisible watermark is embedded in each image pixel, making it more
    resistant to image manipulation, such as filters, resizing, and cropping. It does
    so without degrading the image in any noticeable way and without changing the
    image size significantly.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple benefits of watermarking AI-generated images. In addition
    to indicating the origin and possibly ownership of the images, they help discourage
    unauthorized use and distribution and help prevent the spread of misinformation.
    Chapter 13 covers GenAI-related risks in more detail, including mitigation strategies
    and associated tooling.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.4 Image generation tips
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section outlines some best practices for image generation. In the context
    of enterprises, outside of some functions, such as graphic designers and artists,
    many people with different skills need help. These suggestions will help them
    get started. We will cover more details later in the book when discussing prompt
    engineering:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '*Describe in detail*—Describe the main subject you want to generate in detail.
    The visual elements we imagine or want might not match how the model interprets
    them, so adding details and hints can steer the model more toward what you want.
    Many also forget to describe the background; it is also important to add those
    details.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Vibes and art style*—Specify the style of the vibe or the art that is your
    intent; for example, we outlined a painting in the prompts earlier. The list is
    endless and, in some ways, up to your imagination, going from oil painting to
    steampunk to action photography.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Set the emotion, energy, and mood*—Add adjectives and verbs that convey the
    mood, energy, and overall emotion—for example, the generated image aims to be
    positive and high energy, or positive but low energy, and so forth.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hands and face generations*—These are problematic for many models, and while
    they are getting better, sometimes it is better to add stock or other images to
    generated images.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Structure, size, light, and viewing perspectives*—When thinking of the vibe
    and style of the target image, one also has to think of the size and structure
    of the artifacts. For example, do we expect something small and intricate or big
    and free-standing? And from what perspective are the artifacts being looked at—is
    it a closeup, a long shot, wide angle, outdoor, or in natural light? Of course,
    given that we are talking about a prompt, it can combine many of these things.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Words, logos, and characters*—The image models aren’t large language models
    and generally struggle with images wherein we expect words to be generated (e.g.,
    a pet salon with its name on the outside). It is best to add these manually when
    editing the images. Once added, we can use inpainting.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Avoid multiple characters together*—If you add many characters in the same
    prompt and generation task, it is common for the model to get confused. It might
    be better to start with smaller tasks and then use inpainting or manually edit
    these elements.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next chapter will show other things that can be generated in addition to
    text and images. We will cover audio, video, and code generators.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Vision-based generative AI models allow us to create unique and realistic content,
    all from a simple prompt. These models can generate new content, edit and enhance
    existing images, and use simple prompts.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative AI vision models have multiple use cases in which they can be used
    for creative content, image editing, synthetic data creation, and generative design.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are four primary generative AI model architectures, each with strengths
    and challenges. We explained variational autoencoders (VAEs), generative adversarial
    networks (GANs), vision transformer models (ViT), and diffusion models.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multimodal models are different generative AI models that allow us to handle
    different types of input data, including text, images, audio, and video, simultaneously.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI’s DALLE, Bing, Adobe, and Stability AI’s Stable Diffusion are some of
    the more famous and common generative AI image models used by enterprises for
    image generation and editing. Most things exposed via an API have relevant GUI
    interfaces too.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many generative AI vision models support inpainting (modifying parts within
    an image), outpainting (expanding an image beyond its original boundaries), and
    creating image variations.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diffusion models are more robust in modeling collapse and supporting various
    outputs.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, when it comes to images, we need to think about the scene, main character,
    structure, and elements such as text and faces, which are better done manually
    and edited into the image. These aspects have to be added to the prompt for the
    generation. Later in the book, we will discuss this topic as part of prompt engineering.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
