- en: '4 From pixels to pictures: Generating images'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generative AI vision models, their model architecture, and key use cases for
    enterprises
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Stable Diffusion’s GUIs and APIs for image generation and editing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using advanced editing techniques, such as inpainting, outpainting, and image
    variations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practical image generation tips for enterprises to consider
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating images represents one of the many uses of generative AI, resulting
    in unique and realistic content from a mere prompt. Enterprises have been increasingly
    adopting generative AI to develop innovative image generation and editing solutions,
    which has led to many innovative use cases—from AI-powered architecture for innovative
    designs of buildings to fashion design, avatar generation, virtual clothes try-on,
    and virtual patients for medical training, to name a few. They are accompanied
    by exciting products such as Microsoft Designer and Adobe Firefly, and they will
    be covered in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapters, we talked about the fundamentals of generative AI
    and the technology that enables us to generate text, including completions and
    chats. However, in this chapter, we shift gears and explore how generative AI
    can be utilized to produce and adjust images. We will see how creating images
    is a simple process and highlight some of the complexities of getting them right.
  prefs: []
  type: TYPE_NORMAL
- en: Initially, this chapter focuses on comprehending the generative AI methods that
    facilitate the generation of new images and the overall workflow an enterprise
    must consider. The applications of these techniques are immense and can be particularly
    useful in the e-commerce, entertainment, and healthcare sectors. In addition,
    we will examine various generative AI products and services for image manipulation.
    Let’s dive in!
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Vision models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Generative AI vision models can generate realistic new images and novel concepts
    from a prompt. Let’s start by looking at some enterprise use cases and examples
    of how these generative AI vision models can help:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Content creation and editing*—There are multiple use cases in different industries
    where generative AI vision models can help media and marketing professionals generate
    new themes and scenarios, remove unnecessary or unwanted things from images, or
    apply style transfer. The specific use cases vary by industry.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Healthcare*—There are multiple use cases of image-generative AI in the health
    domain, from educating and training medical students or using new techniques (see
    the next item) to improving a patient’s diagnosis and prognosis by helping enhance
    and clear medical images. It also accelerates drug discovery and development by
    analyzing new novel molecules, complex molecular interactions, and their predictions,
    and optimizing formulation and synthesis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Education*—We can create interactive visuals on the fly based on a student’s
    progress and current learning. This includes realistic and diverse scenarios,
    training simulations using data augmentation, and helping improve the teaching
    quality of both the educator and the student.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*R&D*—We can create a more interpretable visual representation of complex data
    structures and relationships that might not be obvious otherwise. These core elements
    help create new product designs based on trends, unique visual elements, branding,
    and layouts, and can discover subtle patterns in the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Marketing*—Generative AI vision models generate specific visuals tailored
    to the specific individual or demographic, which can also include different sets
    of visuals for A/B testing for understanding successful marketing campaigns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Manufacturing*—Generative AI vision models have the ability to rapidly iterate
    and visualize new materials and components, including the assembly process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Personalization*—This horizontal use case can span different dimensions by
    allowing us to generate personalized visuals, for example, in e-commerce settings
    where a shopper can visualize objects, content, clothing, and so on to create
    highly customized and personalized avatars for gaming and social platforms. Finally,
    fashion and creative fields create new patterns, layouts, clothing, and furniture
    designs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are some real examples of how to generate and bring some of this content
    to life:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Creative content*—Generative AI vision models can produce novel and diverse
    images or videos for artistic, entertainment, or marketing purposes. Some of them
    create realistic faces of people who seem real but do not exist, or they modify
    existing faces to factor in different features such as age, gender, hairstyle,
    and so forth. Figure 4.1 shows a panda bear generated using strawberries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F01_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 A strawberry panda
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Image editing, content improvement, and style transfer*—We can use generative
    AI vision models to enhance existing images. These can address various artifacts,
    such as enhancing the resolution and quality and removing unwanted elements. We
    can also use the style and technique of one image and transpose it onto another.
    For example, figure 4.2 shows us an oil painting of Seattle’s Space Needle in
    the style of Vincent Van Gogh.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F02_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 An oil painting of the Seattle Space Needle in the style of Vincent
    Van Gogh
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Synthetic data*—We can create realistic but synthetic images using generative
    AI vision models. These synthetic images can be used as training and validation
    data for other AI models. For example, the site [https://thispersondoesnotexist.com/](https://thispersondoesnotexist.com/)
    generates the faces of people who do not exist in real life. Synthetic data come
    with challenges; we will discuss them later in the book when we cover generative
    AI challenges.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Generative engineering and design*—We can generate new design options that
    include new objects and structures that can help us optimize certain criteria
    or constraints, such as functionality, performance, or aesthetics. These models
    can generate unique, novel designs for products or digital assets, reducing the
    time and resources spent on manual design. Figure 4.3 shows a chair optimized
    for various design characteristics such as material and aesthetics. These chairs
    have unique and futuristic shapes different from those in conventional chairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F03_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 A chair designed for strength, aesthetics, material, and weight
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Four main generative AI model architecture types make these use cases and examples
    possible: variational autoencoders (VAEs), generative adversarial networks (GANs),
    diffusion models, and vision transformers. Each technique has its strengths and
    weaknesses, and we outline the right approach for the scenario it can use:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Variational autoencoders*—VAEs generate realistic but simple images of animals,
    faces, and other objects. They are good for scenarios requiring data generation,
    that is, new data points similar to the original but with variations. This property
    also allows VAEs to be used for anomaly detection and recommendation systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Generative adversarial networks*—GANs are used for scenarios where data is
    complex and diverse and requires a high level of realism. This makes them suitable
    for high-quality images, data augmentation, and style transfers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Diffusion*—Diffusion-based models are used for scenarios where the data is
    high-dimensional and continuous, and we need to model complex data distribution
    with quality, with the speed of generation being unimportant. These models are
    good for generating speech and video, some of which we will touch on in the next
    chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Vision transformers*—These are great when we want to generate images that
    are sequenced-based tasks, highly flexible, and adaptable to many tasks; they
    need significant computational resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s explore each of these architectures in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Variational autoencoders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: VAEs are a specific generative model that has a vital role. They represent complex
    data distributions by combining aspects of deep learning, probability theory,
    and statistical mechanics.
  prefs: []
  type: TYPE_NORMAL
- en: 'VAEs include two neural networks: an encoder and a decoder (figure 4.4). The
    encoder maps an input image into a low-dimensional latent vector (a latent space)
    that captures its essential features. Not only does it find a single point in
    the latent space, but it can find a distribution. In contrast, the decoder takes
    samples from the latency space and reconstructs the original input image, while
    adding some randomness to make it more diverse. This randomness allows us to add
    new data points, like the input data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following two parameters define the latent state: mean and variance. As
    the name suggests, the mean is the average value of the latent state, and the
    variance is the measure difference of the latent state from the mean. The VAE
    uses these parameters to sample different latent states from a normal distribution,
    a mathematical function that describes how likely different values are to occur.
    By sampling different latent states, the VAE can generate different output data
    that is similar to the input data. Statistical mechanics allow us a framework
    to infer the probability distribution of the variables in the latent variables
    given the observer data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F04_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 Variational autoencoder architecture
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Some of the key uses that VAEs allow are
  prefs: []
  type: TYPE_NORMAL
- en: '*Image generation*—VAEs have been used extensively for image generation to
    create unique images that share similarities with their training data, be it human-like
    faces, fashion designs, or art.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Image reconstruction and inpainting*—By learning the underlying structure
    of the image data, VAEs can reconstruct either missing or corrupted parts of images.
    These properties of reconstructing or filling in missing aspects are tremendously
    useful in some domains, such as medical imaging, restoring old and archaeologically
    significant photographs, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Style transfer*—VAEs allow us to separate the image content from the style
    and transfer the stylistic elements from one image to another, as shown in figure
    4.2\.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Semantic image manipulation*—This is similar to image reconstruction. Because
    of the learned latent space, VAEs can provide us with much more fine-grained control
    of the features in the generated images by tweaking specific aspects of the generated
    images, such as facial expressions, without affecting other unrelated features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although powerful, VAEs do have drawbacks, such as blurriness, lack of diversity,
    and difficulty modeling complex distributions. Training them can be demanding
    and unstable, leading to mode collapse. Irrespective of these challenges, the
    achievements and potential of VAEs remain at the forefront of vision AI research,
    building on the complex relationships between data, mathematics, and creativity.
  prefs: []
  type: TYPE_NORMAL
- en: Note  A latent space represents complex data in a simpler and more meaningful
    way. Think of it as a map where similar items are close to each other, and different
    items are far apart. This helps us find similarities, generate new data, and understand
    data better.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Generative adversarial networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'GANs [1] are among the most popular techniques for creating images with generative
    AI. They consist of two neural networks: a generator that creates new examples
    and a discriminator that tries to differentiate between real and generated examples.'
  prefs: []
  type: TYPE_NORMAL
- en: The generator tries to create fake images that look like real ones from random
    noise or input data, such as text or sketches. The discriminator takes real and
    fake images and tries to distinguish between the two.
  prefs: []
  type: TYPE_NORMAL
- en: The two networks are trained by simultaneously competing in a game-theoretic
    manner to improve their performance over time. GANs work through a min–max game
    where the generator tries to maximize the discriminator’s mistakes, while the
    discriminator tries to minimize them.
  prefs: []
  type: TYPE_NORMAL
- en: GANs use the prompt as an input to the generator, along with some random noise.
    The generator then produces an image that tries to match the prompt and sends
    it to the discriminator. The discriminator compares the generated image with a
    real image from the same prompt, giving a score that indicates how realistic it
    thinks the image is. The score is then used to update the weights of both networks
    using backpropagation and gradient descent. This process is repeated until the
    generator can create images that satisfy the prompt and fool the discriminator.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of the GAN is to make the generator produce realistic images that can
    fool the discriminator. Figure 4.5 shows what the GAN model architecture looks
    like at a high level. The latent space represents possible input for the generator,
    and the fine-tuning allows the parameters for the discriminator and the generator
    to be adjusted.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F05_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 GAN model architecture
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: GANs offer many similar use cases, such as VAEs, but they are specifically good
    for
  prefs: []
  type: TYPE_NORMAL
- en: '*Image generation*—Creating realistic images from noise, with specific applications
    in entertaining, design, and art, allows generating high-quality images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Style transfer*—Enabling artistic styles to transpose from one image to another;
    this is the same as in VAEs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Super resolutions*—GANs can help enhance resolution, making images more detailed
    and clearer. This is very helpful in some industries, such as medical and space
    imaging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data augmentation*—Similar to VAEs for creating synthetic data, GANs help
    create training data either for edge cases or where there is not enough data or
    data diversity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GANs can produce high-quality images that are indistinguishable from real ones.
    Still, they have drawbacks, such as mode collapse (i.e., the model repeatedly
    produces the same output), instability, and difficulty controlling the output.
    They also raise ethical concerns, as they can be quite easily used to create deepfakes
    that could lead to privacy invasion, potential misinformation, and misrepresentation.
    Finally, as with many other AI models, GANs can inadvertently perpetuate biases
    present in the training data in the generated output.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3 Vision transformer models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Transformers are another model architecture that can create images. We saw the
    same architecture earlier in the context of natural language processing (NLP)
    tasks. Transformers can also operate on vision-related tasks and are called vision
    transformers (ViT) [2].
  prefs: []
  type: TYPE_NORMAL
- en: Transformers are neural networks that use attention mechanisms to process sequential
    data, such as text or speech, and they can be used to generate image prompts.
    They are also very effective for specific tasks such as image recognition and
    have outperformed previous leading model architectures.
  prefs: []
  type: TYPE_NORMAL
- en: A ViT model’s architecture is similar to that of NLP, albeit with some differences—it
    has a larger number of self-attention layers and a global attention mechanism
    allowing the model to attend to all parts of the image simultaneously. Transformers
    calculate how much each input token is related to every other input token. This
    is called attention. The more tokens there are, the more attention calculations
    are needed. The number of attention calculations grows as the square of the number
    of tokens, that is, is quadratically.
  prefs: []
  type: TYPE_NORMAL
- en: For images, however, the basic unit of analysis is a pixel and not a token.
    The relationships for every pixel pair in a typical image are computationally
    prohibitive. Instead, ViT computes relationships among pixels in various small
    sections of the image (typically in 16 × 16-sized pixels), which helps reduce
    the computational cost. These 16 × 16-sized sections, along with their positional
    embeddings, are placed in a linear sequence and are the input to the transformer.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in figure 4.6, a ViT model consists of three main sections: the left,
    middle, and right. The left section shows the input classes, such as `Class`,
    `Bird`, `Ball`, `Car`, and so forth. These are the possible labels that the model
    can assign to an image. The middle section shows the linear projection of flattened
    patches, which transform the input image into a sequence of vectors that can be
    fed to the transformer encoder. The final section is the transformer encoder.
    This comprises several multi-head attention and normalization layers and is used
    to learn the relationships between different image parts.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F06_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 Vision transformer (ViT) architecture [2]
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: ViTs are used for various image use cases, such as segmentation, classification,
    and detection, and they are often more accurate than previous techniques. They
    also support fine-tuning, which can be used in a few-shot manner with smaller
    datasets, making them quite useful for enterprise use cases where we might not
    have much data. The ViT model aims to produce a final vector representation for
    the class token, which contains information about the whole image.
  prefs: []
  type: TYPE_NORMAL
- en: ViTs also have challenges such as high computational costs, data scarcity, and
    ethical issues. They are computationally complex both from a training and inference
    perspective and have low interpretability—both active research areas. Multimodal
    models, with ViTs such as GPT-4, hold much promise and unlock new enterprise possibilities.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.4 Diffusion models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Diffusion models are generative machine learning models that can create realistic
    data from random noise, such as images or audio. Their goal is to learn the latent
    structure of a dataset by modeling how data points diffuse through that latent
    space. The model is trained by slowly adding noise to an image and learning to
    reverse this by removing noise from the input until it resembles the desired output.
    For example, a diffusion model can generate an image of a panda by starting with
    a random image and then slowly removing noise until it looks like a panda.
  prefs: []
  type: TYPE_NORMAL
- en: 'Vision diffusion models typically consist of two parts: a forward and a reverse
    diffusion process. The forward diffusion process is responsible for gradually
    adding noise to the latent representation of an image, which corrupts that latent
    space. The reverse diffusion process is just the opposite—it is responsible for
    reconstructing the original image from the corrupted latent representation.'
  prefs: []
  type: TYPE_NORMAL
- en: The forward diffusion process is typically implemented as a Markov chain (i.e.,
    a system with no memory of its past, and the probability of the next step depends
    on the current state). This means the corrupted latent representation at each
    step depends only on the previous step’s latent representation, which makes the
    forward diffusion process efficient and easy to train.
  prefs: []
  type: TYPE_NORMAL
- en: The reverse diffusion process is typically implemented as a neural network,
    meaning the neural network learns to reverse the forward diffusion process by
    predicting the original latent representation from the corrupted one. This reverse
    diffusion process is slow, as it is a step-by-step repetition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the advantages that diffusion models have are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: They can produce high-quality images that match or beat GAN-generated images,
    especially for complex scenes, but they take much longer to generate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They do not suffer from mode collapse, a common problem for GANs. Mode collapse
    occurs when the generator produces only a limited variety of outputs, ignoring
    some modes of data distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diffusion models can capture the full diversity of the data distribution by
    using a Markov chain process that adds noise to the input data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diffusion models can be combined with others, such as natural language models,
    to create text-guided generation systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stable Diffusion is one of the most popular diffusion-based models for image
    generation. Its architecture consists of three main parts (see figure 4.7):'
  prefs: []
  type: TYPE_NORMAL
- en: The text encoder, which converts the user’s prompt into a vector representation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A denoising autoencoder (called UNet), which is used to reconstruct an image
    from the latency space, and a scheduler algorithm, which helps reconstruct the
    original image. We call it the *image information creator*. The UNet is a denoising
    autoencoder because it learns to remove noise from the input image and produce
    a clean output image. It is a neural network that has an encoder–decoder structure.
    The encoder part reduces the resolution of an input image and extracts its features.
    On the other hand, the decoder part increases the resolution and reconstructs
    the output image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A variational autoencoder (VAE), which creates an image as close as possible
    to a normal distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F07_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 Stable Diffusion logical architecture
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The choice between these models depends on the specific application, the availability
    of computing resources, training data, and nonfunctional requirements such as
    image quality, speed, and so forth. Table 4.1 lists some of the more common generative
    AI vision systems that can create images from text.
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.1 Most common AI vision tools
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| AI vision tool | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Imagen  | Imagen is Google’s text-to-image diffusion model, which can generate
    realistic images from text descriptions. It is available in limited preview and
    has been shown to generate images indistinguishable from real photographs.  |'
  prefs: []
  type: TYPE_TB
- en: '| DALL-E  | OpenAI developed a transformer language model to create diverse,
    original, realistic, and creative images and art from a prompt. It can edit images
    based on the context, such as adding, deleting, or changing specific parts. It
    has generated various images, from everyday objects to surrealistic art, from
    simple text prompts. DALL-E 3 is an improved version that can generate more realistic
    and accurate images with 4x greater resolution.  |'
  prefs: []
  type: TYPE_TB
- en: '| Midjourney  | AI-based art generator that uses deep learning and neural networks
    to create artwork based on prompts and other images and videos. This is accessible
    only via a Discord server, and the results can be tailored to any aesthetics,
    from abstract to realistic, thus offering endless possibilities for creative expression.  |'
  prefs: []
  type: TYPE_TB
- en: '| Adobe Firefly  | Adobe Firefly is a family of creative, generative AI diffusion
    models designed to help designers and creative professionals create images and
    text effects and edit and recolor. It is easy to use with Adobe’s other tools,
    such as Photoshop and Illustrator. Adobe has both text-to-image models and generative
    fill models.  |'
  prefs: []
  type: TYPE_TB
- en: '| Stable Diffusion  | Popular models include versions of Stable Diffusion XL
    and v1.6, an image- generating model that uses diffusion models to create high-quality
    images using prompts with next-level photorealism capabilities. It can also generate
    novel images from text descriptions. The more recent v3 family of models comes
    in large and medium with 8B and 2B parameters, respectively.  |'
  prefs: []
  type: TYPE_TB
- en: Many of the AI vision models listed in table 4.1 are available only to those
    who were invited to test them. This is still a new space, and most providers are
    going slowly, learning with a handful of customers before rolling these out.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and manipulating images with generative AI is an exciting and challenging
    research area with many potential applications and implications. However, it raises
    ethical and social questions about the generated content’s ownership, authenticity,
    and effects. Therefore, it is important to use generative AI responsibly and ethically
    and to consider its benefits and risks to society.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.5 Multimodal models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A multimodal model can handle different types of input data. “Modal” refers
    to the mode or type of data, and “multimodal” refers to multiple data types. These
    types include text, images, audio, video, and more. For example, GPT-4 has a multimodal
    model variant that takes both an image and an associated prompt to make predictions
    or inferences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bing Chat recently enabled this multimodal feature, allowing us to use images
    and text in the prompt. For example, as shown in figure 4.8, we give the model
    two things: an image and a prompt related to the image. In this case, we show
    some produce and ask the model what we can cook with it.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F08_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 Multimodal example using both an image and a prompt
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In this case, the model must understand the image and the different parts (i.e.,
    ingredients in our example) and correlate to the prompt to generate an answer.
    We see the response in the shaded text, showing we can make guacamole, salsa,
    avocado toast, and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal models often use different AI techniques. While they can use different
    combinations of model architecture, in our example, GPT-4 combines different transformer
    blocks (figure 4.9).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F09_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 Multimodal model design
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Note  When showing transformer blocks, as in figure 4.9, the convention is
    to use Nx, referring to the transformer block repeating multiple times; in other
    words, it is stacked x number of times. In our multimodal example, this is the
    case for all three transformer blocks: the image on the left (Lx), the text on
    the right (Rx), and the combining layer (Nx).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multimodal models are particularly useful in complex real-world applications
    where data comes in various forms. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Web*—Analyzes text and images for content moderation and sentiment analysis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*eCommerce*—Recommends products using both photos and text descriptions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Healthcare*—Uses text data (patient medical history) and medical imaging (image
    data) for diagnosis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Self-driving*—Integrates sensor data (radar and lidar) with visual data (cameras)
    for situational awareness and decision-making'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have seen some models, their output, and a general sense of how
    vision AI models work, let us generate images with Stable Diffusion.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Image generation with Stable Diffusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Stability AI, the company behind the Stable Diffusion, has advanced diffusion-based
    models with SDXL as their latest and most powerful model thus far. They offer
    multiple options for us to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Self-host*—The model and associated weights have been published and are available
    via Hugging Face ([https://huggingface.co/stabilityai](https://huggingface.co/stabilityai)).
    They can be self-hosted, requiring the appropriate computing hardware, including
    GPUs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*DreamStudio*—This StabilityAI’s consumer application targets consumers. It
    is a simple web interface that generates images. The company also has an open
    source version called StableStudio, driven by the community. More details on DreamStudio
    can be found at [https://dreamstudio.ai](https://dreamstudio.ai).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Platform APIs*—Stability AI has a platform API ([https://platform.stability.ai](https://platform.stability.ai))
    that we will use in this book, given that most enterprises would prefer an API
    that can be managed better at scale. REST API will be used for our example here,
    as it shows the most flexibility across all platforms. Stable Diffusion also has
    a gRPC API, which is quite similar.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4.2.1 Dependencies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will build on the packages required earlier in chapter 3 and assume that
    the following are installed: Python, development IDE, and a virtual environment
    (such as `conda`). For Stable Diffusion, we need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A Stability AI account and associated API key; this can be acquired via the
    account page at [https://platform.stability.ai/account/keys](https://platform.stability.ai/account/keys).
    Billing details also need to be set up at the same place. We pip install the `stability-sdk`
    Python package: `pip` `install` `stability-sdk`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Keep the API key confidential, and follow best practices for managing secrets.
    We will use environmental variables to store the key securely, which can be configured
    as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Windows*—`setx` `STABILITY` `API` `KEY` `“your-openai-key”`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Linux/Mac*—`export` `STABILITY` `API` `KEY=your-openai-endpoint`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Bash*—`echo` `export` `STABILITY_API_KEY="YOUR_KEY"` `>>` `/etc/environment
    &&` `source` `/etc/environment`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We start by getting a list of all the models available using the engines API,
    including all the available engines (i.e., models).
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 4.1 Stable Diffusion: Listing the models'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#1 REST API call for getting the models'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 HTTP header for authorization'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Response back from the API'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Making the JSON more human-readable'
  prefs: []
  type: TYPE_NORMAL
- en: The output of this code is presented in the next listing. This shows us the
    engines we must use and helps in testing end to end to confirm that the API call
    works and that we can authenticate and get a response.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 4.2 Output: Stable Diffusion model lists'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 4.2.2 Generating an image
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We use the Stable Diffusion image generation endpoint (REST API) for our image
    generation. We will use the latest model, the SDXL model, at the time of this
    publication. The corresponding engine ID for this model is `stable-diffusion-xl-1024-v1-0`,
    as shown in the previous example listing of models. This engine ID is required
    as part of the REST API path parameter and is available at https://api.stability.ai/v1/generation/{engine_id}/text-to-image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 4.3 shows an example of using this API to generate an image. Note that
    we use v1.0 of the API for the examples in this chapter. To use the newer models,
    we only need to change the REST API path in most cases. For example, to use the
    newer models that have just been announced, Stable Diffusion 3 and currently in
    Beta, switch to the following engine ID: `https://api.stability.ai/v2beta/stable-image/generate/sd3`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 4.3 Stable Diffusion: Image generation'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Choose the model we want to use.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Prompts used to generate the image'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Helper functions to create filenames'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 API call for generating the image'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 The REST API Endpoint includes the engine ID.'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Parameters controlling the model generation'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Response from the API once the generation finishes'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Saves the image locally'
  prefs: []
  type: TYPE_NORMAL
- en: The image of a “laughing panda in the clouds eating bamboo” was generated, as
    shown in figure 4.10\. It is quite a happy and lifelike panda.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F10_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 An image generated by Stability Diffusion
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Some of the Stability Diffusion API’s parameters are similar to those we have
    already seen. Still, some are different, given that the underlying model architecture
    differs from what is presented in table 4.2\. Because we are using the REST API,
    there are also two sets of parameters—one set is the header parameters, and the
    other is for the body.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4.2 Stable Diffusion header parameters: Image Create API'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Parameter | Type | Default value | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `Accept`  | String  | Blank `(application/json)`  | The response format can
    be default (blank) JSON or set to `image/png` for PNG image.  |'
  prefs: []
  type: TYPE_TB
- en: '| `Organization`  | String  | Null (optional)  | A tag that allows requests
    to be scoped to an organization other than the user’s default. This parameter
    can help debug, monitor, or detect abuse.  |'
  prefs: []
  type: TYPE_TB
- en: '| `Stability-Client-ID`  | String  | Null (optional)  | This parameter is used
    to identify the source of requests, such as the client application or suborganization.
    It can help debug, monitor, and detect abuse.  |'
  prefs: []
  type: TYPE_TB
- en: '| `Stability-Client-Version`  | String  | Null (optional)  | This parameter
    identifies the version of the application or service making the requests. It can
    help debug, monitor, and detect abuse.  |'
  prefs: []
  type: TYPE_TB
- en: '| `Authorization`  | String  | Bearer `API_KEY`  | Key required to authenticate
    the API call  |'
  prefs: []
  type: TYPE_TB
- en: Table 4.3 outlines the parameters that constitute the body of the API call.
    These parameters can fine-tune the model and steer it closer to what we want to
    generate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4.3 Stable Diffusion body parameters: Image Create API'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Parameter | Type | Default value | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `height`and `width`  | Integer  | 512 (optional)  | The height and width
    of the image in pixels must be in increments of 64 and must be one of the following
    combinations: 1024 x 1024, 1152 x 896, 1216 x 832, 1344 x 768, 1536 x 640, 640
    x 1536, 768 x 1344, 832 x 1216, and 896 x 1152\. Note that some of these vary
    based on the engine used.  |'
  prefs: []
  type: TYPE_TB
- en: '| `text_prompts`  | String  | Null (required)  | An array of text prompts is
    used to generate the image. Two properties make up each element in this array—one
    of the prompts itself and the other the associated weight of that prompt. The
    weights should be negative for negative prompts. For example: `"text_prompts":
    [{`'
  prefs: []
  type: TYPE_NORMAL
- en: '`"text": "A dog on a mat",`'
  prefs: []
  type: TYPE_NORMAL
- en: '`"weight": 0.7`'
  prefs: []
  type: TYPE_NORMAL
- en: '`}]`'
  prefs: []
  type: TYPE_NORMAL
- en: The text property can be up to 2,000 characters.
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `cfg_scale`  | String  | 7 (optional)  | This can range between 0 and 35;
    it defines how strictly the diffusion process follows the prompt. Higher values
    keep the image closer to the prompt.  |'
  prefs: []
  type: TYPE_TB
- en: '| `clip_guidance_preset`  | String  | `None` (optional)  | Different values
    control how much CLIP guidance is used, and it controls the quality and relevance
    of the image being generated. Values are `NONE`, `FAST_BLUE`, `FAST_GREEN`, `SIMPLE`,
    `SLOW`, `SLOWER`, and `SLOWEST`.  |'
  prefs: []
  type: TYPE_TB
- en: '| `sampler`  | String  | Null (optional)  | This defines the sampler to use
    for the diffusion process. If this value is omitted, the API automatically selects
    an appropriate sampler for you. Values are `DDIM`, `DDPM`, `K_DPMPP_2M`, `K_DPM_2`,
    `K_EULER K_DPMPP_2S_ANCESTRAL`, `K_HEUN`, `K_DPM_2_ANCESTRAL`, `K_LMS`, `K_EULER_ANCESTRAL`.  |'
  prefs: []
  type: TYPE_TB
- en: '| `samples`  | Integer  | 1 (optional)  | Specifies the number of images to
    generate. Values need to range between 1 and 10\.  |'
  prefs: []
  type: TYPE_TB
- en: '| `seed`  | Integer  | 0 (optional)  | A random seed is a number that determines
    how the noise looks. Leave 0 for a random seed value. The possible value ranges
    between 0 and 4294967295\.  |'
  prefs: []
  type: TYPE_TB
- en: '| `steps`  | Integer  | 50 (optional)  | Defines the number of diffusion steps
    to run. Values range between 10 and 150\.  |'
  prefs: []
  type: TYPE_TB
- en: '| `style_preset`  | String  | Null (optional)  | Used to guide the image model
    toward a particular preset style. Values are `3d-model`, `analog-film`, `anime`,
    `cinematic`, `comic-book`, `digital-art`, `enhance`, `fantasy-art`, `isometric`,
    `line-art`, `low-poly`, `modeling-compound`, `neon-punk`, `origami`, `photographic`,
    `pixel-art`, and `tile-texture`. Note: This list of style presets is subject to
    change over time.'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s look at some other ways we can create images.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Image generation with other providers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we want to generate images, a few other vendors also have generative AI
    vision models; however, they don’t have a platform or API. In this section, we
    will show other platforms that allow one to create images but don’t have APIs,
    and in most cases, they need to be accessed via their GUI.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 OpenAI DALLE 3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DALLE 3 is the newer image generation model from OpenAI that can create images
    from a prompt. It was one of the first image generation models with which most
    people could interact. DALLE stands for Discrete Autoencoder Language Latent Encoder,
    which means it employs a special type of neural network to encode images and text
    to tokens and then uses those tokens to create images. DALLE can be used both
    via an API and a GUI.
  prefs: []
  type: TYPE_NORMAL
- en: Given that the images generated with DALLE are similar to Stable Diffusion,
    we don’t get into the API details here. The GitHub code repository accompanying
    the book ([https://bit.ly/GenAIBook](https://bit.ly/GenAIBook)) has DALLE’s API
    and code samples.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 Bing image creator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Bing has an image creator application that uses DALLE internally, but the images
    it creates are enhanced and a bit different. We only need a web browser to use
    it; an API isn’t exposed. We can generate images by going to [https://www.bing.com/create](https://www.bing.com/create)
    and entering the prompt. There aren’t many tweaks one can make other than those
    specified in the prompt itself. Figure 4.11 shows the generation of a “serene
    vacation lake house, watercolor painting with a dog.” We will use one of these
    images later to see how to edit an image.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F11_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.11 Bing Create: Creating an image depicted as a watercolor painting'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 4.3.3 Adobe Firefly
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Adobe has a set of Generative AI tools, with Firefly being their family of Generative
    AI models. It is being integrated into various Adobe products, such as Photoshop,
    and is accessible via [https://firefly.adobe.com/](https://firefly.adobe.com/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Although there isn’t an API, the overall process and modality are the same
    as we saw earlier with OpenAI. Once we log in, we are presented with a UI where
    we enter the prompt and generate the images. Let us use one of the previous examples:
    “laughing panda in the clouds eating bamboo.” Four images are created by default
    (figure 4.12).'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F12_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 Adobe Firefly generative vision
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Note  Google recently announced its generative AI suite of APIs called Vertex
    AI; at the time of publication, the Vision APIs, which are also built on diffusion
    models, weren’t available for use.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have created an image, let’s see how to edit and enhance it.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Editing and enhancing images using Stable Diffusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to generating images, Stable Diffusion allows us to edit and enhance
    images. We use Stable Diffusion web UI, one of the open source web interfaces
    for Stable Diffusion, to show how to use inpainting and enhance the images. The
    web interface is a wrapper around the model, and while it doesn’t call the API,
    it has the same properties.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by using one of the images of a watercolor painting we generated earlier.
    In this example, we mask two areas: the dog and the different colors on the bottom
    left of the image (figure 4.13).'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F13_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.13 Inpainting sketch
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: When we upload the image to Inpaint, one of the web application’s features is
    to use a CLIP model to interrogate the image and guess the prompt. Even though
    we know the prompt from the original generation, this is a different model, and
    it would be advisable to let Stable Diffusion figure out the prompt. The results
    are shown in figure 4.14.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F14_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.14 Guess the image prompt using a CLIP model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: CLIP model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: CLIP (Contrastive Language–Image Pre-training) is a neural network created by
    OpenAI that links text and images. It can comprehend and classify images to match
    natural language descriptions. This is done through a technique called *contrastive
    learning*, where the model learns from a large number of images and related text
    pairs sourced from the internet.
  prefs: []
  type: TYPE_NORMAL
- en: CLIP’s unique ability to do “zero-shot” learning means it can accurately label
    images it has never encountered before based on text alone without requiring direct
    fine-tuning for that particular task. For example, CLIP can be given the names
    of visual classes and identify them in images, even if it wasn’t specifically
    trained on them.
  prefs: []
  type: TYPE_NORMAL
- en: CLIP encodes both text and images into a common representation space. It can
    estimate the most suitable text snippet for an image or vice versa. This gives
    it much flexibility and the ability to handle different kinds of visual tasks
    without requiring training data specific to each task.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in figure 4.15, additional settings for inpainting allow for finer
    control. Some of these are the same as image generation and are equally important,
    such as the number of sampling steps and methods.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F15_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.15 Stable Diffusion inpainting options
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Outpainting is an additional setting that generates and expands the image in
    our chosen direction. This option is selected via the Script dropdown on the same
    settings tab (figure 4.16).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F16_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.16 Outpainting settings in Stable Diffusion
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We go through the iteration of inpainting by removing the areas we want using
    the mask, regenerating, and then adding the new elements. The final result of
    these iterations is shown in figure 4.17.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F17_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.17 Final edits of inpainting using Stable Diffusion
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Note  The details on Stable Diffusion web UI, including setup, configuration,
    and deployments, are outside the scope of this book; however, it is one of the
    very popular applications that allow one to self-host across Windows, Linux, and
    MacOS. You can find more details at their GitHub repository ([https://mng.bz/znx1](https://mng.bz/znx1)).
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.1 Generating using image-to-image API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Image-to-image is a powerful tool for generating or modifying new images that
    use existing images as a starting point and a text prompt. We can use this API
    to generate a new image but change the style and mood and add or remove aspects.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use our serene lake example from earlier and then use the image-to-image
    API to generate a new image. We build on both examples we have seen earlier—we
    use the serene lake as our input and ask the model to generate “a happy panda
    eating bamboo in the sky.”
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.4 Image-to-image generation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We see the generated image as shown on the left in figure 4.18 of the image-to-image
    API call; we see the panda and the bamboo and how the input image to set the scene
    and the type and aesthetic of the generated image are used. However, it doesn’t
    adhere to the cloud aspect of the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: We can tweak the parameters to make it adhere more to the prompt and less to
    the input image, as shown on the right side of figure 4.18\. An example is when
    we see a panda in the sky, eating bamboo; overall, the image aesthetics follows
    the input image.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F18_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.18 Stable Diffusion image-to-image generation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 4.4.2 Using the masking API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Stable Diffusion also has a masking API that allows us to edit portions of
    an image programmatically. The API is very similar to the creation API, as shown
    in the example in listing 4.5\. It does have a few constraints: the mask image
    needs to be the same dimension as the original image, and a PNG, less than 4MB
    in size. The API has the same header parameters outlined earlier in the chapter
    when we discussed image generation; we will avoid duplicating that.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.5 Stable Diffusion masking API example
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Selects the inpainting model we want to use'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Image we want to edit'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Masks that we want to apply'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Masks API call'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Selects the black pixels of the image to be replaced'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Prompts for the generation'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Specifies the number of images to generate'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Determines the number of steps for each of the images'
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Gets the response from the API'
  prefs: []
  type: TYPE_NORMAL
- en: '#10 Saves the edited image to disk'
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.4 outlines all the API parameters. In terms of options to steer the
    model, much of it is similar to the previous image creation.
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.4 Stable Diffusion masking API parameters
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Parameter | Type | Default value | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `init_image`  | String  | Binary (required)  | The initial image that we
    want to edit  |'
  prefs: []
  type: TYPE_TB
- en: '| `mask_source`  | String  | Null (required)  | Mask details that determine
    the generation areas and associated strengths. It can be one of the following:
    `MASK_IMAGE_WHITE`—Use white pixels as the mask; white pixels are modified; black
    pixels are unchanged.'
  prefs: []
  type: TYPE_NORMAL
- en: '`MASK_IMAGE_BLACK`—Use black pixels as the mask; black pixels are modified;
    white pixels are unchanged'
  prefs: []
  type: TYPE_NORMAL
- en: '`INIT_IMAGE_ALPHA`—Use the alpha channel as the mask. Edit fully transparent
    pixels, and leave fully opaque pixels unchanged.'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `mask_image`  | String  | Binary (required)  | Mask image that guides the
    model on which pixels need to be modified. This parameter is used only if the
    `mask_source` is either `MASK_IMAGE_BLACK` or `MASK_IMAGE_WHITE` `.`  |'
  prefs: []
  type: TYPE_TB
- en: '| `text_prompts`  | String  | Null (required)  | An array of text prompts is
    used to generate the image. Each element in this array comprises two properties—one
    of the prompt itself and the other of the associated weight. The weights should
    be negative for negative prompts. The prompts need to adhere to the following
    format: `text_prompts[index][text&#124;weight]`, with the index being unique and
    not having to be sequential.  |'
  prefs: []
  type: TYPE_TB
- en: '| `cfg_scale`  | String  | 7 (optional)  | Can range between 0 and 35; it defines
    how strictly the diffusion process follows the prompt. Higher values keep the
    image closer to the prompt.  |'
  prefs: []
  type: TYPE_TB
- en: '| `clip_guidance_preset`  | String  | None (optional)  | Different values control
    how much CLIP guidance is used and influence the quality and relevance of the
    image being generated. Possible values are `NONE`, `FAST_BLUE`, `FAST_GREEN`,
    `SIMPLE`, `SLOW`, `SLOWER` `,` and `SLOWEST`.  |'
  prefs: []
  type: TYPE_TB
- en: '| `sampler`  | String  | Null (optional)  | Defines the sampler to use for
    the diffusion process. If this value is omitted, the API automatically selects
    an appropriate sampler for you. Possible values are `DDIM`, `DDPM`, `K_DPMPP_2M`,
    `K_DPM_2`, `K_EULER K_DPMPP_2S_ANCESTRAL`, `K_HEUN`, `K_DPM_2_ANCESTRAL`, `K_LMS`,
    and `K_EULER_ANCESTRAL`.'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `samples`  | Integer  | 1 (optional)  | Defines the number of images to generate.
    Values need to range between 1 and 10\.  |'
  prefs: []
  type: TYPE_TB
- en: '| `seed`  | Integer  | 0 (optional)  | A random seed is a number that determines
    how the noise looks. Leave 0 for a random seed value. The possible value ranges
    between 0 and 4294967295\.  |'
  prefs: []
  type: TYPE_TB
- en: '| `steps`  | Integer  | 50 (optional)  | Defines the number of diffusion steps
    to run. Possible values range between 10 and 150\.  |'
  prefs: []
  type: TYPE_TB
- en: '| `style_preset`  | String  | Null (optional)  | Used to guide the image model
    towards a particular preset style. Possible values are `3d-model`, `analog-film`,
    `anime`, `cinematic`, `comic-book`, `digital-art`, `enhance`, `fantasy-art`, `isometric`,
    `line-art`, `low-poly`, `modeling-compound`, `neon-punk`, `origami`, `photographic`,
    `pixel-art`, and `tile-texture`. Note: This list of style presets is subject to
    change over time.'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.3 Resize using the upscale API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The final Stable Diffusion API we want to cover is used to upscale an image,
    that is, generate a higher-resolution image of a given image. The default is to
    upscale the input image by a factor of two, with a maximum pixel count of 4,194,304,
    equivalent to a maximum dimension of 2,04…,048 and 4,09…,024.
  prefs: []
  type: TYPE_NORMAL
- en: The API is straightforward, as shown in the next listing. The main thing to
    be aware of is using the right model via the `engine_id` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.6 Stable Diffusion resizing API
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have examined numerous image-generation options using both GUIs
    and APIs, let’s examine some of the best practices for enterprises.
  prefs: []
  type: TYPE_NORMAL
- en: Watermark for AI-generated images
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Since AI-generated images are getting increasingly better, and we often cannot
    distinguish between real and AI-generated images, there is a push to watermark
    AI-generated images. There are two main ways to do this today: visible watermarks,
    like what Bing and DALLE do, and invisible watermarks, which are not visible to
    us but are embedded in the image and can be detected using special tools.'
  prefs: []
  type: TYPE_NORMAL
- en: Google has gone a step further and developed a new type of watermark called
    SynthID. An invisible watermark is embedded in each image pixel, making it more
    resistant to image manipulation, such as filters, resizing, and cropping. It does
    so without degrading the image in any noticeable way and without changing the
    image size significantly.
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple benefits of watermarking AI-generated images. In addition
    to indicating the origin and possibly ownership of the images, they help discourage
    unauthorized use and distribution and help prevent the spread of misinformation.
    Chapter 13 covers GenAI-related risks in more detail, including mitigation strategies
    and associated tooling.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.4 Image generation tips
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section outlines some best practices for image generation. In the context
    of enterprises, outside of some functions, such as graphic designers and artists,
    many people with different skills need help. These suggestions will help them
    get started. We will cover more details later in the book when discussing prompt
    engineering:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Describe in detail*—Describe the main subject you want to generate in detail.
    The visual elements we imagine or want might not match how the model interprets
    them, so adding details and hints can steer the model more toward what you want.
    Many also forget to describe the background; it is also important to add those
    details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Vibes and art style*—Specify the style of the vibe or the art that is your
    intent; for example, we outlined a painting in the prompts earlier. The list is
    endless and, in some ways, up to your imagination, going from oil painting to
    steampunk to action photography.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Set the emotion, energy, and mood*—Add adjectives and verbs that convey the
    mood, energy, and overall emotion—for example, the generated image aims to be
    positive and high energy, or positive but low energy, and so forth.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hands and face generations*—These are problematic for many models, and while
    they are getting better, sometimes it is better to add stock or other images to
    generated images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Structure, size, light, and viewing perspectives*—When thinking of the vibe
    and style of the target image, one also has to think of the size and structure
    of the artifacts. For example, do we expect something small and intricate or big
    and free-standing? And from what perspective are the artifacts being looked at—is
    it a closeup, a long shot, wide angle, outdoor, or in natural light? Of course,
    given that we are talking about a prompt, it can combine many of these things.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Words, logos, and characters*—The image models aren’t large language models
    and generally struggle with images wherein we expect words to be generated (e.g.,
    a pet salon with its name on the outside). It is best to add these manually when
    editing the images. Once added, we can use inpainting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Avoid multiple characters together*—If you add many characters in the same
    prompt and generation task, it is common for the model to get confused. It might
    be better to start with smaller tasks and then use inpainting or manually edit
    these elements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next chapter will show other things that can be generated in addition to
    text and images. We will cover audio, video, and code generators.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Vision-based generative AI models allow us to create unique and realistic content,
    all from a simple prompt. These models can generate new content, edit and enhance
    existing images, and use simple prompts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative AI vision models have multiple use cases in which they can be used
    for creative content, image editing, synthetic data creation, and generative design.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are four primary generative AI model architectures, each with strengths
    and challenges. We explained variational autoencoders (VAEs), generative adversarial
    networks (GANs), vision transformer models (ViT), and diffusion models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multimodal models are different generative AI models that allow us to handle
    different types of input data, including text, images, audio, and video, simultaneously.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI’s DALLE, Bing, Adobe, and Stability AI’s Stable Diffusion are some of
    the more famous and common generative AI image models used by enterprises for
    image generation and editing. Most things exposed via an API have relevant GUI
    interfaces too.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many generative AI vision models support inpainting (modifying parts within
    an image), outpainting (expanding an image beyond its original boundaries), and
    creating image variations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diffusion models are more robust in modeling collapse and supporting various
    outputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, when it comes to images, we need to think about the scene, main character,
    structure, and elements such as text and faces, which are better done manually
    and edited into the image. These aspects have to be added to the prompt for the
    generation. Later in the book, we will discuss this topic as part of prompt engineering.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
