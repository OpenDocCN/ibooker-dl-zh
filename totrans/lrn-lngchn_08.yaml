- en: Chapter 8\. Patterns to Make the Most of LLMs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章\. 充分利用LLMs的模式
- en: LLMs today have some major limitations, but that doesn’t mean your dream LLM
    app is impossible to build. The experience that you design for users of your application
    needs to work around, and ideally with, the limitations.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的大型语言模型（LLMs）存在一些主要限制，但这并不意味着你梦想中的LLM应用程序无法构建。你为应用程序用户设计的体验需要围绕这些限制工作，理想情况下与之协同。
- en: '[Chapter 5](ch05.html#ch05_cognitive_architectures_with_langgraph_1736545670030774)
    touched on the key trade-off we face when building LLM apps: the trade-off between
    *agency* (the LLM’s capacity to act autonomously) and *reliability* (the degree
    to which we can trust its outputs). Intuitively, any LLM application will be more
    useful to us if it takes more actions without our involvement, but if we let agency
    go too far, the application will inevitably do things we wish it hadn’t.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[第5章](ch05.html#ch05_cognitive_architectures_with_langgraph_1736545670030774)简要讨论了我们在构建LLM应用程序时面临的关键权衡：*代理权*（LLM自主行动的能力）和*可靠性*（我们对其输出的信任程度）。直观上，如果LLM应用程序能够在我们的参与下采取更多行动，那么它对我们更有用，但如果代理权过于强大，应用程序不可避免地会做我们不愿意它做的事情。'
- en: '[Figure 8-1](#ch08_figure_1_1736545674134403) illustrates this trade-off.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8-1](#ch08_figure_1_1736545674134403)展示了这种权衡。'
- en: '![The agency-reliability trade-off](assets/lelc_0801.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![代理-可靠性权衡](assets/lelc_0801.png)'
- en: Figure 8-1\. The agency-reliability trade-off
  id: totrans-5
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-1\. 代理-可靠性权衡
- en: To borrow a concept from other fields,^([1](ch08.html#id778)) we can visualize
    the trade-off as a *frontier*—all points on the frontier’s curved line are optimal
    LLM architectures for some application, marking different choices between agency
    and reliability. (Refer to [Chapter 5](ch05.html#ch05_cognitive_architectures_with_langgraph_1736545670030774)
    for an overview of different LLM application architectures.) As an example, notice
    how the chain architecture has relatively low agency but higher reliability, whereas
    the Agent architecture has higher agency at the expense of lower reliability.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 借鉴其他领域的概念，^([1](ch08.html#id778))我们可以将这种权衡可视化为一个*前沿*——前沿曲线上的所有点都是某些应用程序的最优LLM架构，标志着在代理权和可靠性之间的不同选择。（参考[第5章](ch05.html#ch05_cognitive_architectures_with_langgraph_1736545670030774)了解不同LLM应用架构的概述。）例如，请注意链式架构具有相对较低的代理权但较高的可靠性，而代理架构则以牺牲较低的可靠性为代价实现了较高的代理权。
- en: 'Let’s briefly touch on a number of additional (but still important) objectives
    that you might want your LLM application to have. Each LLM app will be designed
    for a different mix of one or more of these objectives:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要地讨论一些额外的（但仍然重要）的目标，你可能希望你的LLM应用程序具备这些目标。每个LLM应用程序都将针对一个或多个这些目标的组合进行设计：
- en: Latency
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟
- en: Minimize time to get final answer
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化获得最终答案的时间
- en: Autonomy
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 自主性
- en: Minimize interruptions for human input
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化人类输入的干扰
- en: Variance
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 变异性
- en: Minimize variation between invocations
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化调用之间的差异
- en: This is not meant as an exhaustive list of all possible objectives, but rather
    as illustrative of the trade-offs you face when building your application. Each
    objective is somewhat at odds with all the others (for instance, the easiest path
    to higher reliability requires either higher latency or lower autonomy). Each
    objective would nullify the others if given full weight (for instance, the minimal
    latency app is the one that does nothing at all). [Figure 8-2](#ch08_figure_2_1736545674134438)
    illustrates this concept.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是所有可能目标的详尽列表，而是作为你在构建应用程序时面临权衡的示例。每个目标都与所有其他目标多少有些冲突（例如，提高可靠性的最简单途径可能需要更高的延迟或更低的自主性）。如果给予每个目标充分的权重，每个目标都会使其他目标无效（例如，最小延迟的应用程序是根本不做什么的应用程序）。[图8-2](#ch08_figure_2_1736545674134438)说明了这一概念。
- en: '![Shifting the frontier, or more agency for the same reliability](assets/lelc_0802.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![前沿的移动，或相同的可靠性下更多的代理权](assets/lelc_0802.png)'
- en: Figure 8-2\. Shifting the frontier, or more agency, for the same reliability
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-2\. 前沿的移动，或相同的可靠性下更多的代理权
- en: 'What we really want as application developers is to shift the frontier outward.
    For the same level of reliability, we’d like to achieve higher agency; and for
    the same level of agency, we’d like to achieve higher reliability. This chapter
    covers a number of techniques you can use to achieve this:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 作为应用程序开发者，我们真正想要的是将前沿向外推移。对于相同的可靠性水平，我们希望实现更高的代理权；而对于相同的代理权水平，我们希望实现更高的可靠性。本章介绍了一些你可以用来实现这一目标的技巧：
- en: Streaming/intermediate output
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 流式/中间输出
- en: Higher latency is easier to accept if there is some communication of progress/intermediate
    output throughout.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在整个过程中有一些进度/中间输出的沟通，那么更高的延迟更容易接受。
- en: Structured output
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化输出
- en: Requiring an LLM to produce output in a predefined format makes it more likely
    that it will conform to expectations.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要求LLM以预定义的格式生成输出，使其更有可能符合预期。
- en: Human in the loop
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 人工介入
- en: 'Higher-agency architectures benefit from human intervention while they’re running:
    interrupting, approving, forking, or undoing.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 高级架构在运行时受益于人工干预：中断、批准、分支或撤销。
- en: Double texting modes
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 双重文本模式
- en: The longer an LLM app takes to answer, the more likely it is that the user might
    send it new input before the previous one has finished being processed.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: LLM应用回答的时间越长，用户在之前的一个输入完成处理之前发送新输入的可能性就越大。
- en: Structured Output
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构化输出
- en: It is often crucial to have LLMs return structured output, either because a
    downstream use of that output expects a things in a specific *schema* (a definition
    of the names and types of the various fields in a piece of structured output)
    or purely to reduce variance to what would otherwise be completely free-form text
    output.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 通常来说，让LLM返回结构化输出是至关重要的，要么是因为下游使用该输出的预期需要一个特定的*模式*（结构化输出中各种字段名称和类型的定义），要么是为了纯粹减少到原本可能完全自由形式的文本输出中的方差。
- en: 'There are a few different strategies you can use for this with different LLMs:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不同的LLM，你可以使用几种不同的策略：
- en: Prompting
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: This is when you ask the LLM (very nicely) to return output in the desired format
    (for instance, JSON, XML, or CSV). Prompting’s big advantage is that it works
    to some extent with any LLM; the downside is that it acts more as a suggestion
    to the LLM and not as a guarantee that the output will come out in this format.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是当你（非常礼貌地）要求LLM以所需格式返回输出的时候（例如，JSON、XML或CSV）。提示的大优点是它在某种程度上与任何LLM都兼容；缺点是它更像是对LLM的建议，而不是保证输出将以这种格式出现。
- en: Tool calling
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 工具调用
- en: 'This is available for LLMs that have been fine-tuned to pick from a list of
    possible output schemas, and to produce something that conforms to the chosen
    one. This usually involves writing, for each of the possible output schemas: a
    name to identify it, a description to help the LLM decide when it is the appropriate
    choice, and a schema for the desired output format (usually in JSONSchema notation).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这适用于已经微调过的LLM，可以从可能的输出模式列表中进行选择，并生成符合所选模式的内容。这通常涉及为每个可能的输出模式编写：一个用于识别它的名称，一个描述以帮助LLM决定何时是合适的选项，以及一个用于所需输出格式的模式（通常使用JSONSchema表示法）。
- en: JSON mode
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: JSON模式
- en: This is a mode available in some LLMs (such as recent OpenAI models) that enforces
    the LLM to output a valid JSON document.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这是某些LLM（如最近的OpenAI模型）中可用的一种模式，它强制LLM输出有效的JSON文档。
- en: Different models may support different variants of these, with slightly different
    parameters. To make it easy to get LLMs to return structured output, LangChain
    models implement a common interface, a method called `.with_structured_output`.
    By invoking this method—and passing in a JSON schema or a Pydantic (in Python)
    or Zod (in JS) model—the model will add whatever model parameters and output parsers
    are necessary to produce and return the structured output. When a particular model
    implements more than one of the preceding strategies, you can configure which
    method to use.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的模型可能支持这些策略的不同变体，参数略有不同。为了使LLM返回结构化输出变得容易，LangChain模型实现了一个通用接口，一个名为`.with_structured_output`的方法。通过调用此方法并传入JSON模式或Pydantic（在Python中）或Zod（在JS中）模型，模型将添加生成和返回结构化输出所需的任何模型参数和输出解析器。当特定模型实现了上述策略中的多个时，您可以配置使用哪种方法。
- en: 'Let’s create a schema to use:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个要使用的模式：
- en: '*Python*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*JavaScript*'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Notice how we take care to add a description to each field. This is key because—together
    with the name of the field—this is the information the LLM will use to decide
    what part of the output should go in each field. We could also have defined a
    schema in raw JSONSchema notation, which would look like this:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们如何注意为每个字段添加描述。这是关键，因为——连同字段的名称一起——这是LLM将用于决定输出中的哪一部分应该放入每个字段的信息。我们还可以定义一个原始JSONSchema表示法的模式，其外观如下：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And now let’s get an LLM to generate output that conforms to this schema:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们让LLM生成符合此模式的输出：
- en: '*Python*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*JavaScript*'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*An example of output*'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出示例*'
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'A couple of things to notice:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 有几点需要注意：
- en: We create the instance of the model as usual, specifying the model name to use
    and other parameters.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们像往常一样创建模型的实例，指定要使用的模型名称和其他参数。
- en: Low temperature is usually a good fit for structured output, as it reduces the
    chance the LLM will produce invalid output that doesn’t conform to the schema.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低温通常适合结构化输出，因为它减少了 LLM 产生不符合模式的有效输出的可能性。
- en: Afterward, we attach the schema to the model, which returns a new object, which
    will produce output that matches the schema provided. When you pass in a Pydantic
    or Zod object for schema, this will be used for validation as well; that is, if
    the LLM produces output that doesn’t conform, a validation error will be returned
    to you instead of the failed output.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 之后，我们将模式附加到模型上，这将返回一个新的对象，该对象将生成与提供的模式相匹配的输出。当你为模式传递 Pydantic 或 Zod 对象时，这也会用于验证；也就是说，如果
    LLM 产生的输出不符合规范，将返回验证错误而不是失败的输出。
- en: Finally, we invoke the model with our (free-form) input, and receive back output
    that matches the structure we desired.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们使用我们的（自由形式的）输入调用模型，并接收与我们期望的结构相匹配的输出。
- en: This pattern of using structured output can be very useful both as a standalone
    tool and as a part of a larger application; for instance, refer back to [Chapter 5](ch05.html#ch05_cognitive_architectures_with_langgraph_1736545670030774),
    where we make use of this capability to implement the routing step of the router
    architecture.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用结构化输出的这种模式可以作为独立工具或作为更大应用程序的一部分非常有用；例如，请参阅[第 5 章](ch05.html#ch05_cognitive_architectures_with_langgraph_1736545670030774)，我们利用这种能力来实现路由架构的路由步骤。
- en: Intermediate Output
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 中间输出
- en: The more complex your LLM architecture becomes, the more likely it will take
    longer to execute. If you think back to the architecture diagrams in Chapters
    [5](ch05.html#ch05_cognitive_architectures_with_langgraph_1736545670030774) and
    [6](ch06.html#ch06_agent_architecture_1736545671750341), every time you see multiple
    steps (or nodes) connected in sequence or in a loop, that is an indication that
    the time it takes for a full invocation is increasing.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 你的 LLM 架构越复杂，它执行所需的时间就越长。如果你回想一下第 5 章[认知架构与 LangGraph](ch05.html#ch05_cognitive_architectures_with_langgraph_1736545670030774)和第
    6 章[代理架构](ch06.html#ch06_agent_architecture_1736545671750341)中的架构图，每次你看到多个步骤（或节点）按顺序或循环连接时，这表明完整调用所需的时间正在增加。
- en: This increase in latency—if not addressed—can be a blocker to user adoption
    of LLM applications, with most users expecting computer applications to produce
    some output within seconds. There are several strategies to make the higher latency
    more palatable, but they all fall under the umbrella of *streaming output*, that
    is, receiving output from the application while it is still running.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这种延迟的增加——如果未解决——可能会成为 LLM 应用程序用户采用率的障碍，因为大多数用户期望计算机应用程序在几秒钟内产生一些输出。有几种策略可以使更高的延迟更加可接受，但它们都属于
    *流式输出* 的范畴，即在应用程序仍在运行时接收输出。
- en: For this section, we’ll use the last architecture described in [“Dealing with
    Many Tools”](ch06.html#ch06_dealing_with_many_tools_1736545671750712). Refer back
    to [Chapter 6](ch06.html#ch06_agent_architecture_1736545671750341) for the full
    code snippet.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本节，我们将使用在[“处理多种工具”](ch06.html#ch06_dealing_with_many_tools_1736545671750712)中描述的最后一个架构。请参阅[第
    6 章](ch06.html#ch06_agent_architecture_1736545671750341)以获取完整的代码片段。
- en: 'To generate intermediate output with LangGraph, all you have to do is to invoke
    the graph with the `stream` method, which will yield the output of each node as
    soon as each finishes. Let’s see what that looks like:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LangGraph 生成中间输出，你只需调用 `stream` 方法来获取每个节点完成后的输出。让我们看看它是什么样子：
- en: '*Python*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '*JavaScript*'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '*The output:*'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出：*'
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Notice how each output entry is a dictionary with the name of the node that
    emitted as the key and the output of that node as the value. This gives you two
    key pieces of information:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到每个输出条目都是一个字典，其中包含发出输出的节点的名称作为键，该节点的输出作为值。这为你提供了两个关键信息：
- en: Where the application currently is; that is, if you think back to the architecture
    diagrams shown in previous chapters, where in that diagram are we currently?
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序当前所处的位置；也就是说，如果你回想一下之前章节中显示的架构图，我们在图中当前处于什么位置？
- en: Each update to the shared state of the application, which together build up
    to the final output of the graph.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序共享状态的每次更新，这些更新共同构成了图最终输出的最终输出。
- en: 'In addition, LangGraph supports more stream modes:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，LangGraph 还支持更多的流模式：
- en: '`updates`. This is the default mode, described above.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`updates`。这是默认模式，如上所述。'
- en: '`values`. This mode yields the current state of the graph every time it changes,
    that is after each set of nodes finishes executing. This can be useful when the
    way you display output to your users closely tracks the shape of the graph state.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`values`。此模式每次图状态改变时都会产生当前状态，即在每组节点执行完毕后。当您向用户显示输出的方式与图状态形状紧密相关时，这可能很有用。'
- en: '`debug`. This mode yields detailed events every time something happens in your
    graph, including:'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`debug`。此模式在您的图中发生任何事件时都会产生详细的事件，包括：'
- en: '`checkpoint` events, whenever a new checkpoint of the current state is saved
    to the database'
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`checkpoint`事件，每当将当前状态的新检查点保存到数据库时发出'
- en: '`task` events, emitted whenever a node is about to start running'
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task`事件，每当一个节点即将开始运行时发出'
- en: '`task_result` events, emitted whenever a node finishes running'
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task_result`事件，每当一个节点完成运行时发出'
- en: Finally, you can combine these modes; for instance, requesting both `updates`
    and `values` by passing a list.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，您可以组合这些模式；例如，通过传递一个列表来请求`updates`和`values`。
- en: You control the stream mode with the `stream_mode` argument to `stream()`.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过`stream()`函数的`stream_mode`参数来控制流模式。
- en: Streaming LLM Output Token-by-Token
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流式LLM输出逐词输出
- en: Sometimes you may also want to get streaming output from each LLM call inside
    your larger LLM application. This can be useful for various projects, such as
    when building an interactive chatbot, where you want each word to be displayed
    as soon as it is produced by the LLM.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，您可能还希望从您更大的LLM应用程序中的每个LLM调用中获取流式输出。这对于各种项目可能很有用，例如在构建交互式聊天机器人时，您希望每个单词都能在LLM生成后立即显示。
- en: 'You can achieve this with LangGraph as well:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以使用LangGraph实现这一点：
- en: '*Python*'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE9]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '*JavaScript*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE10]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This will emit each word (technically each token) as soon as it is received
    from the LLM. You can find more details on this pattern from [LangChain](https://oreil.ly/ExYll).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在收到LLM的每个单词（技术上称为每个标记）时立即发出。您可以从[LangChain](https://oreil.ly/ExYll)了解更多有关此模式的信息。
- en: Human-in-the-Loop Modalities
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人类在回路模式
- en: As we walk the autonomy (or agency) ladder, we find ourselves increasingly giving
    up control (or oversight) in exchange for capability (or autonomy). The shared
    state pattern used in LangGraph (see [Chapter 5](ch05.html#ch05_cognitive_architectures_with_langgraph_1736545670030774)
    for an introduction) makes it easier to observe, interrupt, and modify the application.
    This makes it possible to use many different *human-in-the-loop* modes, or ways
    for the developer/end user of an application to influence what the LLM is up to.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们攀登自主性（或代理）的阶梯时，我们发现自己在越来越多地放弃控制（或监督）以换取能力（或自主性）。LangGraph中使用的共享状态模式（参见[第5章](ch05.html#ch05_cognitive_architectures_with_langgraph_1736545670030774)以获取介绍）使得观察、中断和修改应用程序变得更加容易。这使得使用许多不同的*人类在回路*模式或开发人员/最终用户影响LLM所做的事情的方式成为可能。
- en: 'For this section, we’ll again use the last architecture described in [“Dealing
    with Many Tools”](ch06.html#ch06_dealing_with_many_tools_1736545671750712). Refer
    back to [Chapter 6](ch06.html#ch06_agent_architecture_1736545671750341) for the
    full code snippet. For all human-in-the-loop modes, we first need to attach a
    checkpointer to the graph; refer to [“Adding Memory to StateGraph”](ch04.html#ch04_adding_memory_to_stategraph_1736545668266831)
    for more details on this:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本节，我们再次使用[“处理许多工具”](ch06.html#ch06_dealing_with_many_tools_1736545671750712)中描述的最后一个架构。有关完整代码片段，请参阅[第6章](ch06.html#ch06_agent_architecture_1736545671750341)。对于所有人类在回路模式，我们首先需要将检查点器附加到图上；有关此操作的更多详细信息，请参阅[“向StateGraph添加内存”](ch04.html#ch04_adding_memory_to_stategraph_1736545668266831)：
- en: '*Python*'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '*JavaScript*'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE12]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This returns an instance of the graph that stores the state at the end of each
    step, so every invocation after the first doesn’t start from a blank slate. Any
    time the graph is called, it starts by using the checkpointer to fetch the most
    recent saved state—if any—and combines the new input with the previous state.
    And only then does it execute the first nodes. This is key to enabling human-in-the-loop
    modalities, which all rely on the graph remembering the previous state.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回一个在每一步结束时存储状态的图实例，因此第一次调用之后的每次调用都不会从空白状态开始。每次调用图时，它都会首先使用检查点器来获取最新的保存状态（如果有的话），并将新输入与先前状态结合。然后才执行第一个节点。这对于启用人类在回路模式至关重要，所有这些模式都依赖于图记住先前状态。
- en: 'The first mode, `interrupt`, is the simplest form of control—the user is looking
    at streaming output of the application as it is produced, and manually interrupts
    it when he sees fit (see [Figure 8-3](#ch08_figure_3_1736545674134460)). The state
    is saved as of the last complete step prior to the user hitting the interrupt
    button. From there the user can choose to:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种模式，`interrupt`，是最简单的控制形式——用户正在查看应用程序产生的流式输出，并在认为合适时手动中断它（参见[图8-3](#ch08_figure_3_1736545674134460)）。状态保存为用户点击中断按钮之前的最后一步完整步骤。从那里，用户可以选择：
- en: Resume from that point onward, and the computation will proceed as if it hadn’t
    been interrupted (see [“Resume”](#ch08_resume_1736545674144097)).
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从那个点开始恢复，计算将像没有中断一样继续（参见[“恢复”](#ch08_resume_1736545674144097)）。
- en: Send new input into the application (e.g., a new message in a chatbot), which
    will cancel any future steps that were pending and start dealing with the new
    input (see [“Restart”](#ch08_restart_1736545674144152)).
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将新输入发送到应用程序中（例如，聊天机器人中的新消息），这将取消任何挂起的未来步骤，并开始处理新输入（参见[“重启”](#ch08_restart_1736545674144152)）。
- en: Do nothing and nothing else will run.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不做任何事情，并且不会运行其他任何事情。
- en: '![The Interrupt pattern](assets/lelc_0803.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![中断模式](assets/lelc_0803.png)'
- en: Figure 8-3\. The `interrupt` pattern
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-3\. `interrupt`模式
- en: 'Let’s see how to do this in LangGraph:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在LangGraph中实现这一点：
- en: '*Python*'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '*JavaScript*'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE14]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This makes use of an event or signal, so that you can control interruption from
    outside of the running application. Notice in the Python code block the use of
    `aclosing`; this ensures the stream is properly closed when interrupted. Notice
    in JS the use of the `try-catch` statement, as interrupting the run will result
    in an `abort` exception being raised. Finally notice that usage of the checkpointer
    requires passing in an identifier for this thread, to distinguish this interaction
    with the graph from all others.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这利用了一个事件或信号，因此你可以从运行的应用程序外部控制中断。注意在Python代码块中使用了`aclosing`；这确保在中断时流被正确关闭。注意在JS中使用了`try-catch`语句，因为中断运行将引发`abort`异常。最后注意，使用检查点时需要传递一个标识符来区分此交互与所有其他交互。
- en: '![The Authorize patten](assets/lelc_0804.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![授权模式](assets/lelc_0804.png)'
- en: Figure 8-4\. The `authorize` pattern
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-4\. `authorize`模式
- en: 'A second control mode is `authorize`, where the user defines ahead of time
    that they want the application to hand off control to them every time a particular
    node is about to be called (see [Figure 8-4](#ch08_figure_4_1736545674134478)).
    This is usually implemented for tool confirmation—before any tool (or particular
    tools) is called, the application will pause and ask for confirmation, at which
    point the user can, again:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种控制模式是`authorize`，用户提前定义，每当特定节点即将被调用时，他们希望应用程序将控制权转交给他们（参见[图8-4](#ch08_figure_4_1736545674134478)）。这通常用于工具确认——在调用任何工具（或特定工具）之前，应用程序将暂停并请求确认，此时用户可以再次：
- en: Resume computation, accepting the tool call.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 恢复计算，接受工具调用。
- en: Send a new message to guide the bot in a different direction, in which case
    the tool will not be called.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发送一条新消息以引导机器人走向不同的方向，在这种情况下，工具将不会被调用。
- en: Do nothing.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不做任何事情。
- en: 'Here’s the code:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是代码：
- en: '*Python*'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE15]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '*JavaScript*'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE16]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This will run the graph up until it is about to enter the node called `tools`,
    thus giving you the chance to inspect the current state, and decide whether to
    proceed or not. Notice that `interrupt_before` is a list where order is not important;
    if you pass multiple node names, it will interrupt before entering each of them.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这将运行图形，直到即将进入名为`tools`的节点，从而给你机会检查当前状态，并决定是否继续。注意，`interrupt_before`是一个列表，其中顺序不重要；如果你传递多个节点名称，它将在进入每个节点之前中断。
- en: Resume
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 恢复
- en: 'To proceed from an interrupted graph—such as when using one of the previous
    two patterns—you just need to re-invoke the graph with null input (or `None` in
    Python). This is taken as a signal to continue processing the previous non-null
    input:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 从中断的图形（例如使用前两种模式之一时）继续，你只需要用空输入（或在Python中为`None`）重新调用图形。这被视为继续处理之前非空输入的信号：
- en: '*Python*'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE17]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '*JavaScript*'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE18]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Restart
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重新启动
- en: 'If instead you want an interrupted graph to start over from the first node,
    with additional new input, you just need to invoke it with new input:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要一个中断的图形从第一个节点重新开始，并带有额外的新的输入，你只需要用新的输入调用它：
- en: '*Python*'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE19]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '*JavaScript*'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE20]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This will keep the current state of the graph, merge it with the new input,
    and start again from the first node.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这将保持当前图的状态，将其与新输入合并，并从第一个节点重新开始。
- en: If you want to lose the current state, just change the `thread_id`, which will
    start a new interaction from a blank slate. Any string value is a valid `thread_id`;
    we’d recommend using UUIDs (or other unique identifiers) as thread IDs.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想丢弃当前状态，只需更改`thread_id`，这将从一个空白状态开始新的交互。任何字符串值都是有效的`thread_id`；我们建议使用UUID（或其他唯一标识符）作为线程ID。
- en: Edit state
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编辑状态
- en: Sometimes you might want to update the state of the graph before resuming; this
    is possible with the `update_state` method. You’ll usually want to first inspect
    the current state with `get_state`.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 有时您可能想在恢复之前更新图的状态；这可以通过`update_state`方法实现。您通常会首先使用`get_state`检查当前状态。
- en: 'Here’s what it looks like:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是它的样子：
- en: '*Python*'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE21]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '*JavaScript*'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE22]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This will create a new checkpoint containing your update. After this, you’re
    ready to resume the graph from this new point. See [“Resume”](#ch08_resume_1736545674144097)
    to find out how.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个包含您更新的新检查点。在此之后，您就可以从这个新点恢复图了。参见[“恢复”](#ch08_resume_1736545674144097)了解如何操作。
- en: Fork
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分叉
- en: You can also browse the history of all past states the graph has passed through,
    and any of them can be visited again, for instance, to get an alternative answer.
    This can be very useful in more creative applications, where each run through
    the graph is expected to produce different output.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以浏览图经过的所有过去状态的记录，并且可以再次访问其中任何一个，例如，以获取一个替代答案。这在更具有创造性的应用中非常有用，在这些应用中，每次通过图运行都期望产生不同的输出。
- en: 'Let’s see what it looks like:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它看起来像什么：
- en: '*Python*'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE23]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '*JavaScript*'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE24]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Notice how we collect the history into a list/array in both languages; `get_state_history`
    returns an iterator of states (to allow consuming lazily). The states returned
    from the history method are sorted with the most recent first and the oldest last.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们在两种语言中如何将历史记录收集到一个列表/数组中；`get_state_history` 返回状态迭代器（以允许懒加载）。从历史方法返回的状态按最近的时间排序，最新的状态排在最前面，最旧的状态排在最后。
- en: The true power of the human-in-the-loop controls comes from mixing them in whatever
    way suits your application.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 人类在回路控制的真实力量来自于以适合您应用程序的任何方式混合它们。
- en: Multitasking LLMs
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多任务LLM
- en: This section covers the problem of handling concurrent input for LLM applications.
    This is a particularly relevant problem given that LLMs are quite slow, much more
    so when producing long outputs or when chained in multistep architectures (like
    you can do with LangGraph). Even as LLMs become faster, dealing with concurrent
    inputs will continue to be a challenge, as latency improvements will also unlock
    the door for more and more complex use cases, in much the same way as even the
    most productive person still faces the need to prioritize competing demands on
    their time.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了处理LLM应用并发输入的问题。鉴于LLM相当慢，尤其是在生成长输出或链式多步架构（如使用LangGraph所能做到的）时，这是一个特别相关的问题。即使LLM变得更快，处理并发输入也将继续是一个挑战，因为延迟的改进也将为越来越多的复杂用例打开大门，就像即使是最有效率的人仍然需要面对优先处理时间竞争需求的需要。
- en: Let’s walk through the options.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看选项。
- en: Refuse concurrent inputs
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 拒绝并发输入
- en: Any input received while processing a previous one is rejected. This is the
    simplest strategy, but unlikely to cover all needs, as it effectively means handing
    off concurrency management to the caller.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理先前的输入时接收到的任何输入都被拒绝。这是一种最简单的策略，但可能无法满足所有需求，因为这实际上意味着将并发管理委托给调用者。
- en: Handle independently
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 独立处理
- en: Another simple option is to treat any new input as an independent invocation,
    creating a new thread (a container for remembering state) and producing output
    in that context. This has the obvious downside of needing to be shown to the user
    as two separate and unreconcilable invocations, which isn’t always possible or
    desirable. On the other hand, it has the upside of scaling to arbitrarily large
    sizes, and is something you’ll use to some extent in your application almost certainly.
    For instance, this is how you would think about the problem of getting a chatbot
    to “chat” with two different users concurrently.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个简单的选项是将任何新的输入视为独立的调用，创建一个新的线程（用于记住状态的容器）并在该上下文中生成输出。这显然有一个明显的缺点，即需要将它们展示给用户作为两个独立且不可调和的调用，这并不总是可能或理想的。另一方面，它有一个优点是可扩展到任意大的规模，您几乎肯定会在您的应用中用到这一点。例如，这就是您考虑让聊天机器人“聊天”与两个不同用户并发的问题的方式。
- en: Queue concurrent inputs
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 排队并发输入
- en: 'Any input received while processing a previous one is queued up and handled
    when the current one is finished. This strategy has some pros:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理之前的输入时接收到的任何输入都会排队，并在当前输入完成后进行处理。这种策略有一些优点：
- en: It supports receiving an arbitrary number of concurrent requests.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它支持接收任意数量的并发请求。
- en: Because we wait for current input to finish processing, it doesn’t matter if
    the new input arrives almost immediately after we start handling the current input
    or immediately before we finish; the end result will be the same, as we will finish
    processing the current input before moving on to the next.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于我们等待当前输入完成处理，所以新输入在我们开始处理当前输入后几乎立即到达或在我们完成之前到达并不重要；最终结果将是一样的，因为我们将在处理完当前输入之前继续处理。
- en: 'The strategy suffers from a few drawbacks as well:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这种策略也有一些缺点：
- en: It may take a while to process all queued inputs; in fact, the queue may grow
    unbounded if inputs are produced at a rate faster than processed.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理所有排队输入可能需要一段时间；事实上，如果输入的产生速度超过处理速度，队列可能会无限制地增长。
- en: The inputs may be stale by the time they get processed, given that they are
    queued before seeing the response to the previous one, and not altered afterwards.
    This strategy is not appropriate when new inputs depend on previous answers.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于输入在看到上一个响应之前就已经排队，并且在之后没有改变，因此在处理时输入可能已经过时。当新的输入依赖于之前的答案时，这种策略不合适。
- en: Interrupt
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 中断
- en: 'When a new input is received while another is being processed, abandon processing
    of the current one and restart the chain with the new input. This strategy can
    vary by what is kept of the interrupted run. Here are a few options:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 当接收到新的输入而另一个正在处理时，放弃当前的处理，并使用新的输入重新启动链。这种策略可以根据中断运行保留的内容而有所不同。以下是一些选项：
- en: Keep nothing. The previous input is completely forgotten, as if it had never
    been sent or processed.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保留无物。之前的输入被完全遗忘，就像它从未被发送或处理过一样。
- en: Keep the last completed step. In a checkpointing app (which stores progress
    as it moves through the computation), keep the state produced by the last completed
    step, discard any pending state updates from the currently executing step, and
    start handling the new input in that context.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保留最后一个完成的步骤。在一个检查点应用程序（在计算过程中存储进度）中，保留最后一个完成的步骤产生的状态，丢弃当前执行步骤的任何挂起状态更新，并在该上下文中处理新的输入。
- en: Keep the last completed step, as well as the current in-progress step. Attempt
    to interrupt the current step while taking care to save any incomplete updates
    to state that were being produced at the time. This is likely to not generalize
    beyond the simplest architectures.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保留最后一个完成的步骤，以及当前正在进行的步骤。在保存当时产生的任何不完整状态更新时，尝试中断当前步骤。这种方法可能不适用于最简单的架构。
- en: Wait for the current node (but not any subsequent nodes) to finish, then save
    and interrupt.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等待当前节点（但不包括任何后续节点）完成，然后保存并中断。
- en: 'This option has some pros compared to queuing concurrent inputs:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 与排队并发输入相比，这个选项有一些优点：
- en: New input is handled as soon as possible, reducing latency and the chance of
    producing stale outputs.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦接收到新的输入，就尽快处理，以减少延迟和产生过时输出的可能性。
- en: For the “keep nothing” variant, the final output doesn’t depend on when the
    new input was received.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于“保留无物”的变体，最终输出不依赖于新输入接收的时间。
- en: 'But it also has drawbacks:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 但它也有缺点：
- en: Effectively, this strategy is still limited to processing one input at a time;
    any old input is abandoned when new input is received.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实际上，这种策略仍然限制于一次处理一个输入；当接收到新的输入时，任何旧的输入都会被放弃。
- en: Keeping partial state updates for the next run requires the state to be designed
    with that in mind; if not, then your application is likely to end up in an invalid
    state. For instance, OpenAI chat models require an AI message requesting tool
    calls to be immediately followed by tool messages with the tool outputs. If your
    run is interrupted in between, you either defensively clean up the intermediate
    state or risk being unable to progress further.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为下一次运行保留部分状态更新需要考虑状态的设计；如果不这样做，那么你的应用程序可能最终会处于无效状态。例如，OpenAI聊天模型要求AI消息请求工具调用必须立即由包含工具输出的工具消息跟随。如果你的运行在中间被中断，你要么防御性地清理中间状态，要么冒着无法进一步进展的风险。
- en: The final outputs produced are very sensitive to when the new input is received;
    new input will be handled in the context of the (incomplete) progress previously
    made toward handling the previous input. This can result in brittle or unpredictable
    outcomes if you don’t design accordingly.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终输出的结果非常敏感于新输入何时接收；新输入将在处理先前输入所取得的（不完整）进展的背景下进行处理。如果您没有相应地设计，这可能会导致脆弱或不稳定的输出。
- en: Fork and merge
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分叉和合并
- en: Another option is to handle new input in parallel, forking the state of the
    thread as it is when the new input is received and merging the final states as
    inputs finish being handled. This option requires designing your state to either
    be mergeable without conflicts (e.g., using conflict-free replicated data types
    [CRDTs] or other conflict resolution algorithms) or having the user manually resolve
    conflicts before you’re able to make sense of the output or send new input in
    this thread. If either of those two requirements is met, this is likely to be
    the best option overall. This way, new input is handled in a timely manner, output
    is independent of time received, and it supports an arbitrary number of concurrent
    runs.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是并行处理新输入，当接收到新输入时，将线程的状态分叉，并在处理完最终状态后合并。这个选项需要设计您的状态，要么在没有冲突的情况下可合并（例如，使用无冲突复制数据类型[CRDTs]或其他冲突解决算法），要么在您能够理解输出或在此线程中发送新输入之前，让用户手动解决冲突。如果满足这两个要求中的任何一个，这很可能是最佳选择。这样，新输入可以及时处理，输出独立于接收时间，并且支持任意数量的并发运行。
- en: Some of these strategies are implemented in LangGraph Platform, which will be
    covered in [Chapter 9](ch09.html#ch09_deployment_launching_your_ai_application_into_pro_1736545675509604).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些策略在LangGraph平台中实现，将在[第9章](ch09.html#ch09_deployment_launching_your_ai_application_into_pro_1736545675509604)中介绍。
- en: Summary
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we returned to the main trade-off you face when building LLM
    applications: agency versus reliability. We learned that there are strategies
    to partially beat the odds and get more reliability without sacrificing agency,
    and vice versa.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们回到了构建LLM应用程序时面临的主要权衡：代理与可靠性。我们了解到，有一些策略可以在不牺牲代理的情况下部分克服不利因素并获得更高的可靠性，反之亦然。
- en: We started by covering structured outputs, which can improve the predictability
    of LLM-generated text. Next, we discussed emitting streaming/intermediate output
    from your application, which can make high latency (an inevitable side effect
    of agency currently) applications pleasant to use.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先介绍了结构化输出，这可以提高LLM生成文本的可预测性。接下来，我们讨论了从您的应用程序中发出流式/中间输出，这可以使具有高延迟（目前代理的必然副作用）的应用程序更加易于使用。
- en: We also walked through a variety of human-in-the-loop controls—that is, techniques
    to give back some oversight to the end user of your LLM application—which can
    often make the difference in making high-agency architectures reliable. Finally,
    we talked about the problem of handling concurrent input to your application,
    a particularly salient problem for LLM apps given their high latency.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还探讨了各种人机交互的控制方法——即，将一些监督权交还给您的LLM应用程序的最终用户——这通常可以在使高代理架构可靠方面起到关键作用。最后，我们讨论了处理应用程序并发输入的问题，这对于具有高延迟的LLM应用程序来说是一个特别突出的问题。
- en: In the next chapter, you’ll learn how to deploy your AI application into production.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，您将学习如何将您的AI应用程序部署到生产环境中。
- en: ^([1](ch08.html#id778-marker)) In finance, the *efficient frontier* in portfolio
    optimization; in economics, a *production-possibility frontier*; in engineering,
    the *Pareto front*.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch08.html#id778-marker)) 在金融中，投资组合优化的*有效前沿*；在经济学中，*生产可能性前沿*；在工程中，*帕累托前沿*。
