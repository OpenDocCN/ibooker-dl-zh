- en: Chapter 8\. Patterns to Make the Most of LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLMs today have some major limitations, but that doesn’t mean your dream LLM
    app is impossible to build. The experience that you design for users of your application
    needs to work around, and ideally with, the limitations.
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 5](ch05.html#ch05_cognitive_architectures_with_langgraph_1736545670030774)
    touched on the key trade-off we face when building LLM apps: the trade-off between
    *agency* (the LLM’s capacity to act autonomously) and *reliability* (the degree
    to which we can trust its outputs). Intuitively, any LLM application will be more
    useful to us if it takes more actions without our involvement, but if we let agency
    go too far, the application will inevitably do things we wish it hadn’t.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 8-1](#ch08_figure_1_1736545674134403) illustrates this trade-off.'
  prefs: []
  type: TYPE_NORMAL
- en: '![The agency-reliability trade-off](assets/lelc_0801.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1\. The agency-reliability trade-off
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To borrow a concept from other fields,^([1](ch08.html#id778)) we can visualize
    the trade-off as a *frontier*—all points on the frontier’s curved line are optimal
    LLM architectures for some application, marking different choices between agency
    and reliability. (Refer to [Chapter 5](ch05.html#ch05_cognitive_architectures_with_langgraph_1736545670030774)
    for an overview of different LLM application architectures.) As an example, notice
    how the chain architecture has relatively low agency but higher reliability, whereas
    the Agent architecture has higher agency at the expense of lower reliability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s briefly touch on a number of additional (but still important) objectives
    that you might want your LLM application to have. Each LLM app will be designed
    for a different mix of one or more of these objectives:'
  prefs: []
  type: TYPE_NORMAL
- en: Latency
  prefs: []
  type: TYPE_NORMAL
- en: Minimize time to get final answer
  prefs: []
  type: TYPE_NORMAL
- en: Autonomy
  prefs: []
  type: TYPE_NORMAL
- en: Minimize interruptions for human input
  prefs: []
  type: TYPE_NORMAL
- en: Variance
  prefs: []
  type: TYPE_NORMAL
- en: Minimize variation between invocations
  prefs: []
  type: TYPE_NORMAL
- en: This is not meant as an exhaustive list of all possible objectives, but rather
    as illustrative of the trade-offs you face when building your application. Each
    objective is somewhat at odds with all the others (for instance, the easiest path
    to higher reliability requires either higher latency or lower autonomy). Each
    objective would nullify the others if given full weight (for instance, the minimal
    latency app is the one that does nothing at all). [Figure 8-2](#ch08_figure_2_1736545674134438)
    illustrates this concept.
  prefs: []
  type: TYPE_NORMAL
- en: '![Shifting the frontier, or more agency for the same reliability](assets/lelc_0802.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. Shifting the frontier, or more agency, for the same reliability
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'What we really want as application developers is to shift the frontier outward.
    For the same level of reliability, we’d like to achieve higher agency; and for
    the same level of agency, we’d like to achieve higher reliability. This chapter
    covers a number of techniques you can use to achieve this:'
  prefs: []
  type: TYPE_NORMAL
- en: Streaming/intermediate output
  prefs: []
  type: TYPE_NORMAL
- en: Higher latency is easier to accept if there is some communication of progress/intermediate
    output throughout.
  prefs: []
  type: TYPE_NORMAL
- en: Structured output
  prefs: []
  type: TYPE_NORMAL
- en: Requiring an LLM to produce output in a predefined format makes it more likely
    that it will conform to expectations.
  prefs: []
  type: TYPE_NORMAL
- en: Human in the loop
  prefs: []
  type: TYPE_NORMAL
- en: 'Higher-agency architectures benefit from human intervention while they’re running:
    interrupting, approving, forking, or undoing.'
  prefs: []
  type: TYPE_NORMAL
- en: Double texting modes
  prefs: []
  type: TYPE_NORMAL
- en: The longer an LLM app takes to answer, the more likely it is that the user might
    send it new input before the previous one has finished being processed.
  prefs: []
  type: TYPE_NORMAL
- en: Structured Output
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is often crucial to have LLMs return structured output, either because a
    downstream use of that output expects a things in a specific *schema* (a definition
    of the names and types of the various fields in a piece of structured output)
    or purely to reduce variance to what would otherwise be completely free-form text
    output.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few different strategies you can use for this with different LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: Prompting
  prefs: []
  type: TYPE_NORMAL
- en: This is when you ask the LLM (very nicely) to return output in the desired format
    (for instance, JSON, XML, or CSV). Prompting’s big advantage is that it works
    to some extent with any LLM; the downside is that it acts more as a suggestion
    to the LLM and not as a guarantee that the output will come out in this format.
  prefs: []
  type: TYPE_NORMAL
- en: Tool calling
  prefs: []
  type: TYPE_NORMAL
- en: 'This is available for LLMs that have been fine-tuned to pick from a list of
    possible output schemas, and to produce something that conforms to the chosen
    one. This usually involves writing, for each of the possible output schemas: a
    name to identify it, a description to help the LLM decide when it is the appropriate
    choice, and a schema for the desired output format (usually in JSONSchema notation).'
  prefs: []
  type: TYPE_NORMAL
- en: JSON mode
  prefs: []
  type: TYPE_NORMAL
- en: This is a mode available in some LLMs (such as recent OpenAI models) that enforces
    the LLM to output a valid JSON document.
  prefs: []
  type: TYPE_NORMAL
- en: Different models may support different variants of these, with slightly different
    parameters. To make it easy to get LLMs to return structured output, LangChain
    models implement a common interface, a method called `.with_structured_output`.
    By invoking this method—and passing in a JSON schema or a Pydantic (in Python)
    or Zod (in JS) model—the model will add whatever model parameters and output parsers
    are necessary to produce and return the structured output. When a particular model
    implements more than one of the preceding strategies, you can configure which
    method to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a schema to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice how we take care to add a description to each field. This is key because—together
    with the name of the field—this is the information the LLM will use to decide
    what part of the output should go in each field. We could also have defined a
    schema in raw JSONSchema notation, which would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'And now let’s get an LLM to generate output that conforms to this schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '*An example of output*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'A couple of things to notice:'
  prefs: []
  type: TYPE_NORMAL
- en: We create the instance of the model as usual, specifying the model name to use
    and other parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low temperature is usually a good fit for structured output, as it reduces the
    chance the LLM will produce invalid output that doesn’t conform to the schema.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Afterward, we attach the schema to the model, which returns a new object, which
    will produce output that matches the schema provided. When you pass in a Pydantic
    or Zod object for schema, this will be used for validation as well; that is, if
    the LLM produces output that doesn’t conform, a validation error will be returned
    to you instead of the failed output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we invoke the model with our (free-form) input, and receive back output
    that matches the structure we desired.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This pattern of using structured output can be very useful both as a standalone
    tool and as a part of a larger application; for instance, refer back to [Chapter 5](ch05.html#ch05_cognitive_architectures_with_langgraph_1736545670030774),
    where we make use of this capability to implement the routing step of the router
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Intermediate Output
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The more complex your LLM architecture becomes, the more likely it will take
    longer to execute. If you think back to the architecture diagrams in Chapters
    [5](ch05.html#ch05_cognitive_architectures_with_langgraph_1736545670030774) and
    [6](ch06.html#ch06_agent_architecture_1736545671750341), every time you see multiple
    steps (or nodes) connected in sequence or in a loop, that is an indication that
    the time it takes for a full invocation is increasing.
  prefs: []
  type: TYPE_NORMAL
- en: This increase in latency—if not addressed—can be a blocker to user adoption
    of LLM applications, with most users expecting computer applications to produce
    some output within seconds. There are several strategies to make the higher latency
    more palatable, but they all fall under the umbrella of *streaming output*, that
    is, receiving output from the application while it is still running.
  prefs: []
  type: TYPE_NORMAL
- en: For this section, we’ll use the last architecture described in [“Dealing with
    Many Tools”](ch06.html#ch06_dealing_with_many_tools_1736545671750712). Refer back
    to [Chapter 6](ch06.html#ch06_agent_architecture_1736545671750341) for the full
    code snippet.
  prefs: []
  type: TYPE_NORMAL
- en: 'To generate intermediate output with LangGraph, all you have to do is to invoke
    the graph with the `stream` method, which will yield the output of each node as
    soon as each finishes. Let’s see what that looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '*The output:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice how each output entry is a dictionary with the name of the node that
    emitted as the key and the output of that node as the value. This gives you two
    key pieces of information:'
  prefs: []
  type: TYPE_NORMAL
- en: Where the application currently is; that is, if you think back to the architecture
    diagrams shown in previous chapters, where in that diagram are we currently?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each update to the shared state of the application, which together build up
    to the final output of the graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition, LangGraph supports more stream modes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`updates`. This is the default mode, described above.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`values`. This mode yields the current state of the graph every time it changes,
    that is after each set of nodes finishes executing. This can be useful when the
    way you display output to your users closely tracks the shape of the graph state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`debug`. This mode yields detailed events every time something happens in your
    graph, including:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`checkpoint` events, whenever a new checkpoint of the current state is saved
    to the database'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`task` events, emitted whenever a node is about to start running'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`task_result` events, emitted whenever a node finishes running'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, you can combine these modes; for instance, requesting both `updates`
    and `values` by passing a list.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You control the stream mode with the `stream_mode` argument to `stream()`.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming LLM Output Token-by-Token
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes you may also want to get streaming output from each LLM call inside
    your larger LLM application. This can be useful for various projects, such as
    when building an interactive chatbot, where you want each word to be displayed
    as soon as it is produced by the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can achieve this with LangGraph as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This will emit each word (technically each token) as soon as it is received
    from the LLM. You can find more details on this pattern from [LangChain](https://oreil.ly/ExYll).
  prefs: []
  type: TYPE_NORMAL
- en: Human-in-the-Loop Modalities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we walk the autonomy (or agency) ladder, we find ourselves increasingly giving
    up control (or oversight) in exchange for capability (or autonomy). The shared
    state pattern used in LangGraph (see [Chapter 5](ch05.html#ch05_cognitive_architectures_with_langgraph_1736545670030774)
    for an introduction) makes it easier to observe, interrupt, and modify the application.
    This makes it possible to use many different *human-in-the-loop* modes, or ways
    for the developer/end user of an application to influence what the LLM is up to.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this section, we’ll again use the last architecture described in [“Dealing
    with Many Tools”](ch06.html#ch06_dealing_with_many_tools_1736545671750712). Refer
    back to [Chapter 6](ch06.html#ch06_agent_architecture_1736545671750341) for the
    full code snippet. For all human-in-the-loop modes, we first need to attach a
    checkpointer to the graph; refer to [“Adding Memory to StateGraph”](ch04.html#ch04_adding_memory_to_stategraph_1736545668266831)
    for more details on this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This returns an instance of the graph that stores the state at the end of each
    step, so every invocation after the first doesn’t start from a blank slate. Any
    time the graph is called, it starts by using the checkpointer to fetch the most
    recent saved state—if any—and combines the new input with the previous state.
    And only then does it execute the first nodes. This is key to enabling human-in-the-loop
    modalities, which all rely on the graph remembering the previous state.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first mode, `interrupt`, is the simplest form of control—the user is looking
    at streaming output of the application as it is produced, and manually interrupts
    it when he sees fit (see [Figure 8-3](#ch08_figure_3_1736545674134460)). The state
    is saved as of the last complete step prior to the user hitting the interrupt
    button. From there the user can choose to:'
  prefs: []
  type: TYPE_NORMAL
- en: Resume from that point onward, and the computation will proceed as if it hadn’t
    been interrupted (see [“Resume”](#ch08_resume_1736545674144097)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Send new input into the application (e.g., a new message in a chatbot), which
    will cancel any future steps that were pending and start dealing with the new
    input (see [“Restart”](#ch08_restart_1736545674144152)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do nothing and nothing else will run.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![The Interrupt pattern](assets/lelc_0803.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-3\. The `interrupt` pattern
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s see how to do this in LangGraph:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This makes use of an event or signal, so that you can control interruption from
    outside of the running application. Notice in the Python code block the use of
    `aclosing`; this ensures the stream is properly closed when interrupted. Notice
    in JS the use of the `try-catch` statement, as interrupting the run will result
    in an `abort` exception being raised. Finally notice that usage of the checkpointer
    requires passing in an identifier for this thread, to distinguish this interaction
    with the graph from all others.
  prefs: []
  type: TYPE_NORMAL
- en: '![The Authorize patten](assets/lelc_0804.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-4\. The `authorize` pattern
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'A second control mode is `authorize`, where the user defines ahead of time
    that they want the application to hand off control to them every time a particular
    node is about to be called (see [Figure 8-4](#ch08_figure_4_1736545674134478)).
    This is usually implemented for tool confirmation—before any tool (or particular
    tools) is called, the application will pause and ask for confirmation, at which
    point the user can, again:'
  prefs: []
  type: TYPE_NORMAL
- en: Resume computation, accepting the tool call.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Send a new message to guide the bot in a different direction, in which case
    the tool will not be called.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do nothing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This will run the graph up until it is about to enter the node called `tools`,
    thus giving you the chance to inspect the current state, and decide whether to
    proceed or not. Notice that `interrupt_before` is a list where order is not important;
    if you pass multiple node names, it will interrupt before entering each of them.
  prefs: []
  type: TYPE_NORMAL
- en: Resume
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To proceed from an interrupted graph—such as when using one of the previous
    two patterns—you just need to re-invoke the graph with null input (or `None` in
    Python). This is taken as a signal to continue processing the previous non-null
    input:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Restart
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If instead you want an interrupted graph to start over from the first node,
    with additional new input, you just need to invoke it with new input:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This will keep the current state of the graph, merge it with the new input,
    and start again from the first node.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to lose the current state, just change the `thread_id`, which will
    start a new interaction from a blank slate. Any string value is a valid `thread_id`;
    we’d recommend using UUIDs (or other unique identifiers) as thread IDs.
  prefs: []
  type: TYPE_NORMAL
- en: Edit state
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sometimes you might want to update the state of the graph before resuming; this
    is possible with the `update_state` method. You’ll usually want to first inspect
    the current state with `get_state`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This will create a new checkpoint containing your update. After this, you’re
    ready to resume the graph from this new point. See [“Resume”](#ch08_resume_1736545674144097)
    to find out how.
  prefs: []
  type: TYPE_NORMAL
- en: Fork
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can also browse the history of all past states the graph has passed through,
    and any of them can be visited again, for instance, to get an alternative answer.
    This can be very useful in more creative applications, where each run through
    the graph is expected to produce different output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Notice how we collect the history into a list/array in both languages; `get_state_history`
    returns an iterator of states (to allow consuming lazily). The states returned
    from the history method are sorted with the most recent first and the oldest last.
  prefs: []
  type: TYPE_NORMAL
- en: The true power of the human-in-the-loop controls comes from mixing them in whatever
    way suits your application.
  prefs: []
  type: TYPE_NORMAL
- en: Multitasking LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section covers the problem of handling concurrent input for LLM applications.
    This is a particularly relevant problem given that LLMs are quite slow, much more
    so when producing long outputs or when chained in multistep architectures (like
    you can do with LangGraph). Even as LLMs become faster, dealing with concurrent
    inputs will continue to be a challenge, as latency improvements will also unlock
    the door for more and more complex use cases, in much the same way as even the
    most productive person still faces the need to prioritize competing demands on
    their time.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s walk through the options.
  prefs: []
  type: TYPE_NORMAL
- en: Refuse concurrent inputs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Any input received while processing a previous one is rejected. This is the
    simplest strategy, but unlikely to cover all needs, as it effectively means handing
    off concurrency management to the caller.
  prefs: []
  type: TYPE_NORMAL
- en: Handle independently
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another simple option is to treat any new input as an independent invocation,
    creating a new thread (a container for remembering state) and producing output
    in that context. This has the obvious downside of needing to be shown to the user
    as two separate and unreconcilable invocations, which isn’t always possible or
    desirable. On the other hand, it has the upside of scaling to arbitrarily large
    sizes, and is something you’ll use to some extent in your application almost certainly.
    For instance, this is how you would think about the problem of getting a chatbot
    to “chat” with two different users concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: Queue concurrent inputs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Any input received while processing a previous one is queued up and handled
    when the current one is finished. This strategy has some pros:'
  prefs: []
  type: TYPE_NORMAL
- en: It supports receiving an arbitrary number of concurrent requests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because we wait for current input to finish processing, it doesn’t matter if
    the new input arrives almost immediately after we start handling the current input
    or immediately before we finish; the end result will be the same, as we will finish
    processing the current input before moving on to the next.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The strategy suffers from a few drawbacks as well:'
  prefs: []
  type: TYPE_NORMAL
- en: It may take a while to process all queued inputs; in fact, the queue may grow
    unbounded if inputs are produced at a rate faster than processed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The inputs may be stale by the time they get processed, given that they are
    queued before seeing the response to the previous one, and not altered afterwards.
    This strategy is not appropriate when new inputs depend on previous answers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interrupt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When a new input is received while another is being processed, abandon processing
    of the current one and restart the chain with the new input. This strategy can
    vary by what is kept of the interrupted run. Here are a few options:'
  prefs: []
  type: TYPE_NORMAL
- en: Keep nothing. The previous input is completely forgotten, as if it had never
    been sent or processed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep the last completed step. In a checkpointing app (which stores progress
    as it moves through the computation), keep the state produced by the last completed
    step, discard any pending state updates from the currently executing step, and
    start handling the new input in that context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep the last completed step, as well as the current in-progress step. Attempt
    to interrupt the current step while taking care to save any incomplete updates
    to state that were being produced at the time. This is likely to not generalize
    beyond the simplest architectures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wait for the current node (but not any subsequent nodes) to finish, then save
    and interrupt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This option has some pros compared to queuing concurrent inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: New input is handled as soon as possible, reducing latency and the chance of
    producing stale outputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the “keep nothing” variant, the final output doesn’t depend on when the
    new input was received.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'But it also has drawbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: Effectively, this strategy is still limited to processing one input at a time;
    any old input is abandoned when new input is received.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keeping partial state updates for the next run requires the state to be designed
    with that in mind; if not, then your application is likely to end up in an invalid
    state. For instance, OpenAI chat models require an AI message requesting tool
    calls to be immediately followed by tool messages with the tool outputs. If your
    run is interrupted in between, you either defensively clean up the intermediate
    state or risk being unable to progress further.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final outputs produced are very sensitive to when the new input is received;
    new input will be handled in the context of the (incomplete) progress previously
    made toward handling the previous input. This can result in brittle or unpredictable
    outcomes if you don’t design accordingly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fork and merge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another option is to handle new input in parallel, forking the state of the
    thread as it is when the new input is received and merging the final states as
    inputs finish being handled. This option requires designing your state to either
    be mergeable without conflicts (e.g., using conflict-free replicated data types
    [CRDTs] or other conflict resolution algorithms) or having the user manually resolve
    conflicts before you’re able to make sense of the output or send new input in
    this thread. If either of those two requirements is met, this is likely to be
    the best option overall. This way, new input is handled in a timely manner, output
    is independent of time received, and it supports an arbitrary number of concurrent
    runs.
  prefs: []
  type: TYPE_NORMAL
- en: Some of these strategies are implemented in LangGraph Platform, which will be
    covered in [Chapter 9](ch09.html#ch09_deployment_launching_your_ai_application_into_pro_1736545675509604).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we returned to the main trade-off you face when building LLM
    applications: agency versus reliability. We learned that there are strategies
    to partially beat the odds and get more reliability without sacrificing agency,
    and vice versa.'
  prefs: []
  type: TYPE_NORMAL
- en: We started by covering structured outputs, which can improve the predictability
    of LLM-generated text. Next, we discussed emitting streaming/intermediate output
    from your application, which can make high latency (an inevitable side effect
    of agency currently) applications pleasant to use.
  prefs: []
  type: TYPE_NORMAL
- en: We also walked through a variety of human-in-the-loop controls—that is, techniques
    to give back some oversight to the end user of your LLM application—which can
    often make the difference in making high-agency architectures reliable. Finally,
    we talked about the problem of handling concurrent input to your application,
    a particularly salient problem for LLM apps given their high latency.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you’ll learn how to deploy your AI application into production.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch08.html#id778-marker)) In finance, the *efficient frontier* in portfolio
    optimization; in economics, a *production-possibility frontier*; in engineering,
    the *Pareto front*.
  prefs: []
  type: TYPE_NORMAL
