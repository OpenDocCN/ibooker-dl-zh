["```py\ndef attention(query, key, value, mask=None, dropout=None):\n    d_k = query.size(-1)\n    scores = torch.matmul(query, \n              key.transpose(-2, -1)) / math.sqrt(d_k)       ①\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)        ②\n    p_attn = nn.functional.softmax(scores, dim=-1)          ③\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return torch.matmul(p_attn, value), p_attn              ④\n```", "```py\nfrom copy import deepcopy\nclass MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        super().__init__()\n        assert d_model % h == 0\n        self.d_k = d_model // h\n        self.h = h\n        self.linears = nn.ModuleList([deepcopy(\n            nn.Linear(d_model, d_model)) for i in range(4)])\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, query, key, value, mask=None):\n        if mask is not None:\n            mask = mask.unsqueeze(1)\n        nbatches = query.size(0)  \n        query, key, value = [l(x).view(nbatches, -1, self.h,\n           self.d_k).transpose(1, 2)    \n         for l, x in zip(self.linears, (query, key, value))]    ①\n        x, self.attn = attention(\n            query, key, value, mask=mask, dropout=self.dropout) ②\n        x = x.transpose(1, 2).contiguous().view(\n            nbatches, -1, self.h * self.d_k)                    ③\n        output = self.linears[-1](x)                            ④\n        return output\n```", "```py\nclass PositionwiseFeedForward(nn.Module):\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super().__init__()\n        self.w_1 = nn.Linear(d_model, d_ff)\n        self.w_2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n    def forward(self, x):\n        h1 = self.w_1(x)\n        h2 = self.dropout(h1)\n        return self.w_2(h2) \n```", "```py\nclass EncoderLayer(nn.Module):\n    def __init__(self, size, self_attn, feed_forward, dropout):\n        super().__init__()\n        self.self_attn = self_attn\n        self.feed_forward = feed_forward\n        self.sublayer = nn.ModuleList([deepcopy(\n        SublayerConnection(size, dropout)) for i in range(2)])\n        self.size = size  \n    def forward(self, x, mask):\n        x = self.sublayer[0](\n            x, lambda x: self.self_attn(x, x, x, mask))     ①\n        output = self.sublayer[1](x, self.feed_forward)     ②\n        return output\nclass SublayerConnection(nn.Module):\n    def __init__(self, size, dropout):\n        super().__init__()\n        self.norm = LayerNorm(size)\n        self.dropout = nn.Dropout(dropout)\n    def forward(self, x, sublayer):\n        output = x + self.dropout(sublayer(self.norm(x)))   ③\n        return output  \n```", "```py\nclass LayerNorm(nn.Module):\n    def __init__(self, features, eps=1e-6):\n        super().__init__()\n        self.a_2 = nn.Parameter(torch.ones(features))\n        self.b_2 = nn.Parameter(torch.zeros(features))\n        self.eps = eps\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True) \n        std = x.std(-1, keepdim=True)\n        x_zscore = (x - mean) / torch.sqrt(std ** 2 + self.eps)\n        output = self.a_2*x_zscore+self.b_2\n        return output \n```", "```py\nfrom copy import deepcopy\nclass Encoder(nn.Module):\n    def __init__(self, layer, N):\n        super().__init__()\n        self.layers = nn.ModuleList(\n            [deepcopy(layer) for i in range(N)])\n        self.norm = LayerNorm(layer.size)\n    def forward(self, x, mask):\n        for layer in self.layers:\n            x = layer(x, mask)\n            output = self.norm(x)\n        return output\n```", "```py\nclass DecoderLayer(nn.Module):\n    def __init__(self, size, self_attn, src_attn,\n                 feed_forward, dropout):\n        super().__init__()\n        self.size = size\n        self.self_attn = self_attn\n        self.src_attn = src_attn\n        self.feed_forward = feed_forward\n        self.sublayer = nn.ModuleList([deepcopy(\n        SublayerConnection(size, dropout)) for i in range(3)])\n    def forward(self, x, memory, src_mask, tgt_mask):\n        x = self.sublayer[0](x, lambda x: \n                 self.self_attn(x, x, x, tgt_mask))             ①\n        x = self.sublayer[1](x, lambda x:\n                 self.src_attn(x, memory, memory, src_mask))    ②\n        output = self.sublayer[2](x, self.feed_forward)         ③\n        return output\n```", "```py\ntensor([[ True, False, False, False, False],\n        [ True,  True, False, False, False],\n        [ True,  True,  True, False, False],\n        [ True,  True,  True,  True, False],\n        [ True,  True,  True,  True,  True]], device='cuda:0')\n```", "```py\nclass Decoder(nn.Module):\n    def __init__(self, layer, N):\n        super().__init__()\n        self.layers = nn.ModuleList(\n            [deepcopy(layer) for i in range(N)])\n        self.norm = LayerNorm(layer.size)\n    def forward(self, x, memory, src_mask, tgt_mask):\n        for layer in self.layers:\n            x = layer(x, memory, src_mask, tgt_mask)\n        output = self.norm(x)\n        return output\n```", "```py\nclass Transformer(nn.Module):\n    def __init__(self, encoder, decoder,\n                 src_embed, tgt_embed, generator):\n        super().__init__()\n        self.encoder = encoder                                ①\n        self.decoder = decoder                                ②\n        self.src_embed = src_embed\n        self.tgt_embed = tgt_embed\n        self.generator = generator\n    def encode(self, src, src_mask):\n        return self.encoder(self.src_embed(src), src_mask)\n    def decode(self, memory, src_mask, tgt, tgt_mask):\n        return self.decoder(self.tgt_embed(tgt), \n                            memory, src_mask, tgt_mask)\n    def forward(self, src, tgt, src_mask, tgt_mask):\n        memory = self.encode(src, src_mask)                   ③\n        output = self.decode(memory, src_mask, tgt, tgt_mask) ④\n        return output\n```", "```py\nclass Generator(nn.Module):\n    def __init__(self, d_model, vocab):\n        super().__init__()\n        self.proj = nn.Linear(d_model, vocab)\n\n    def forward(self, x):\n        out = self.proj(x)\n        probs = nn.functional.log_softmax(out, dim=-1)\n        return probs  \n```", "```py\ndef create_model(src_vocab, tgt_vocab, N, d_model,\n                 d_ff, h, dropout=0.1):\n    attn=MultiHeadedAttention(h, d_model).to(DEVICE)\n    ff=PositionwiseFeedForward(d_model, d_ff, dropout).to(DEVICE)\n    pos=PositionalEncoding(d_model, dropout).to(DEVICE)\n    model = Transformer(\n        Encoder(EncoderLayer(d_model,deepcopy(attn),deepcopy(ff),\n                             dropout).to(DEVICE),N).to(DEVICE),  ①\n        Decoder(DecoderLayer(d_model,deepcopy(attn),\n             deepcopy(attn),deepcopy(ff), dropout).to(DEVICE),\n                N).to(DEVICE),                                   ②\n        nn.Sequential(Embeddings(d_model, src_vocab).to(DEVICE),\n                      deepcopy(pos)),                            ③\n        nn.Sequential(Embeddings(d_model, tgt_vocab).to(DEVICE),\n                      deepcopy(pos)),                            ④\n        Generator(d_model, tgt_vocab)).to(DEVICE)                ⑤\n    for p in model.parameters():\n        if p.dim() > 1:\n            nn.init.xavier_uniform_(p)    \n    return model.to(DEVICE)\n```"]