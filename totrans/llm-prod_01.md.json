["```py\nsentence = \"What is a bag of words and what does it do for me when \" \\\n    \"processing words?\"\nclean_text = sentence.lower().split(\" \")\nbow = {word:clean_text.count(word) for word in clean_text}\nprint(bow)\n# {'what': 2, 'is': 1, 'a': 1, 'bag': 1, 'of': 1, 'words': 1, 'and': 1, \n# 'does': 1, 'it': 1, 'do': 1, 'for': 1, 'me': 1, 'when': 1, 'processing': 1,\n# 'words?': 1}\n```", "```py\nfrom nltk.corpus.reader import PlaintextCorpusReader\nfrom nltk.util import everygrams\nfrom nltk.lm.preprocessing import (\n    pad_both_ends,\n    flatten,\n    padded_everygram_pipeline,\n)\nfrom nltk.lm import MLE\n\nmy_corpus = PlaintextCorpusReader(\"./\", \".*\\.txt\")    #1\n\nfor sent in my_corpus.sents(fileids=\"hamlet.txt\"):\n    print(sent)\n\npadded_trigrams = list(\n    pad_both_ends(my_corpus.sents(fileids=\"hamlet.txt\")[1104], n=2)\n)                                                  #2\nlist(everygrams(padded_trigrams, max_len=3))\n\nlist(\n    flatten(\n        pad_both_ends(sent, n=2)\n        for sent in my_corpus.sents(fileids=\"hamlet.txt\")\n    )\n)\n\ntrain, vocab = padded_everygram_pipeline(\n    3, my_corpus.sents(fileids=\"hamlet.txt\")\n)                                              #3\n\nlm = MLE(3)           #4\nlen(lm.vocab)           #5\n\nlm.fit(train, vocab)\nprint(lm.vocab)\nlen(lm.vocab)\n\nlm.generate(6, [\"to\", \"be\"])   #6\n```", "```py\nprint(lm.counts)\nLm.counts[[\"to\"]][\"be\"]      #1\n\nprint(lm.score(\"be\"))           #2\nprint(lm.score(\"be\", [\"to\"]))\nprint(lm.score(\"be\", [\"not\", \"to\"]))\n\nprint(lm.logscore(\"be\"))             #3\nprint(lm.logscore(\"be\", [\"to\"]))\nprint(lm.logscore(\"be\", [\"not\", \"to\"]))\n\ntest = [(\"to\", \"be\"), (\"or\", \"not\"), (\"to\", \"be\")]   #4\nprint(lm.entropy(test))\nprint(lm.perplexity(test))\n```", "```py\nfrom utils import process_utt, lookup\nfrom nltk.corpus.reader import PlaintextCorpusReader\nimport numpy as np\n\nmy_corpus = PlaintextCorpusReader(\"./\", \".*\\.txt\")\n\nsents = my_corpus.sents(fileids=\"hamlet.txt\")\n\ndef count_utts(result, utts, ys):\n    \"\"\"\n    Input:\n        result: a dictionary that is used to map each pair to its frequency\n        utts: a list of utts\n        ys: a list of the sentiment of each utt (either 0 or 1)\n    Output:\n        result: a dictionary mapping each pair to its frequency\n    \"\"\"\n\n    for y, utt in zip(ys, utts):\n        for word in process_utt(utt):\n            pair = (word, y)          #1\n\n            if pair in result:\n            result[pair] += 1     #2\n\n            else:\n            result[pair] = 1      #3\n\n    return result\n\nresult = {}\nutts = [\" \".join(sent) for sent in sents]\nys = [sent.count(\"be\") > 0 for sent in sents]\ncount_utts(result, utts, ys)\n\nfreqs = count_utts({}, utts, ys)\nlookup(freqs, \"be\", True)\nfor k, v in freqs.items():\n    if \"be\" in k:\n        print(f\"{k}:{v}\")\n\ndef train_naive_bayes(freqs, train_x, train_y):\n    \"\"\"\n    Input:\n        freqs: dictionary from (word, label) to how often the word appears\n        train_x: a list of utts\n        train_y: a list of labels correponding to the utts (0,1)\n    Output:\n        logprior: the log prior.\n        loglikelihood: the log likelihood of you Naive bayes equation.\n    \"\"\"\n    loglikelihood = {}\n    logprior = 0\n\n    vocab = set([pair[0] for pair in freqs.keys()])      #4\n    V = len(vocab)\n\n    N_pos = N_neg = 0                  #5\n    for pair in freqs.keys():\n        if pair[1] > 0:                   #6\n            N_pos += lookup(freqs, pair[0], True)     #7\n\n        else:                                         #8\n            N_neg += lookup(freqs, pair[0], False)     #9\n\n    D = len(train_y)     #10\n\n    D_pos = sum(train_y)      #11\n\n    D_neg = D - D_pos     #12\n\n    logprior = np.log(D_pos) - np.log(D_neg)      #13\n\n    for word in vocab:                        #14\n        freq_pos = lookup(freqs, word, 1)\n        freq_neg = lookup(freqs, word, 0)\n\n        p_w_pos = (freq_pos + 1) / (N_pos + V)    #15\n        p_w_neg = (freq_neg + 1) / (N_neg + V)\n\n        loglikelihood[word] = np.log(p_w_pos / p_w_neg)     #16\n\n    return logprior, loglikelihood\n\ndef naive_bayes_predict(utt, logprior, loglikelihood):\n    \"\"\"\n    Input:\n        utt: a string\n        logprior: a number\n        loglikelihood: a dictionary of words mapping to numbers\n    Output:\n        p: the sum of all the logliklihoods + logprior\n    \"\"\"\n    word_l = process_utt(utt)     #17\n\n    p = 0          #18\n\n    p += logprior      #19\n\n    for word in word_l:\n        if word in loglikelihood:       #20\n            p += loglikelihood[word]      #21\n\n    return p\n\ndef test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n    \"\"\"\n    Input:\n        test_x: A list of utts\n        test_y: the corresponding labels for the list of utts\n        logprior: the logprior\n        loglikelihood: a dictionary with the loglikelihoods for each word\n    Output:\n        accuracy: (# of utts classified correctly)/(total # of utts)\n    \"\"\"\n    accuracy = 0       #22\n\n    y_hats = []\n    for utt in test_x:\n        if naive_bayes_predict(utt, logprior, loglikelihood) > 0:    #23\n            y_hat_i = 1    #24\n        else:\n            y_hat_i = 0    #25\n\n        y_hats.append(y_hat_i)      #26\n\n    error = sum(\n        [abs(y_hat - test) for y_hat, test in zip(y_hats, test_y)]\n    ) / len(y_hats)                 #27\n\n    accuracy = 1 - error    #28\n\n    return accuracy\n\nif __name__ == \"__main__\":\n    logprior, loglikelihood = train_naive_bayes(freqs, utts, ys)\n    print(logprior)\n    print(len(loglikelihood))\n\n    my_utt = \"To be or not to be, that is the question.\"\n    p = naive_bayes_predict(my_utt, logprior, loglikelihood)\n    print(\"The expected output is\", p)\n\n    print(\n        f\"Naive Bayes accuracy = {test_naive_bayes(utts, ys, logprior, loglikelihood):0.4f}\n    )\n```", "```py\nimport re\nimport random\nfrom nltk.tokenize import word_tokenize\nfrom collections import defaultdict, deque\n\nclass MarkovChain:\n    def __init__(self):\n        self.lookup_dict = defaultdict(list)\n        self._seeded = False\n        self.__seed_me()\n\n    def __seed_me(self, rand_seed=None):\n        if self._seeded is not True:\n            try:\n                if rand_seed is not None:\n                    random.seed(rand_seed)\n                else:\n                    random.seed()\n                self._seeded = True\n            except NotImplementedError:\n                self._seeded = False\n\n    def add_document(self, str):\n        preprocessed_list = self._preprocess(str)\n        pairs = self.__generate_tuple_keys(preprocessed_list)\n        for pair in pairs:\n            self.lookup_dict[pair[0]].append(pair[1])\n\n    def _preprocess(self, str):\n        cleaned = re.sub(r\"\\W+\", \" \", str).lower()\n        tokenized = word_tokenize(cleaned)\n        return tokenized\n\n    def __generate_tuple_keys(self, data):\n        if len(data) < 1:\n            return\n\n        for i in range(len(data) - 1):\n            yield [data[i], data[i + 1]]\n\n    def generate_text(self, max_length=50):\n        context = deque()\n        output = []\n        if len(self.lookup_dict) > 0:\n            self.__seed_me(rand_seed=len(self.lookup_dict))\n            chain_head = [list(self.lookup_dict)[0]]\n            context.extend(chain_head)\n\n            while len(output) < (max_length - 1):\n                next_choices = self.lookup_dict[context[-1]]\n                if len(next_choices) > 0:\n                    next_word = random.choice(next_choices)\n                    context.append(next_word)\n                    output.append(context.popleft())\n                else:\n                    break\n            output.extend(list(context))\n        return \" \".join(output)\n\nif __name__ == \"__main__\":\n    with open(\"hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n        text = f.read()\n    HMM = MarkovChain()\n    HMM.add_document(text)\n\n    print(HMM.generate_text(max_length=25))\n```", "```py\nimport nltk\nimport numpy as np\nfrom utils import get_batches, compute_pca, get_dict\nimport re\nfrom matplotlib import pyplot\n\nwith open(\"hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n    data = f.read()                              #1\n\ndata = re.sub(r\"[,!?;-]\", \".\", data)         #2\ndata = nltk.word_tokenize(data)\ndata = [ch.lower() for ch in data if ch.isalpha() or ch == \".\"]\nprint(\"Number of tokens:\", len(data), \"\\n\", data[500:515])\n\nfdist = nltk.FreqDist(word for word in data)     #3\nprint(\"Size of vocabulary:\", len(fdist))\nprint(\"Most Frequent Tokens:\", fdist.most_common(20))\n\nword2Ind, Ind2word = get_dict(data)     #4\nV = len(word2Ind)\nprint(\"Size of vocabulary:\", V)\n\nprint(\"Index of the word 'king':\", word2Ind[\"king\"])\nprint(\"Word which has index 2743:\", Ind2word[2743])\n\ndef initialize_model(N, V, random_seed=1):     #5\n    \"\"\"\n    Inputs:\n        N: dimension of hidden vector\n        V: dimension of vocabulary\n        random_seed: seed for consistent results in tests\n    Outputs:\n        W1, W2, b1, b2: initialized weights and biases\n    \"\"\"\n    np.random.seed(random_seed)\n\n    W1 = np.random.rand(N, V)\n    W2 = np.random.rand(V, N)\n    b1 = np.random.rand(N, 1)\n    b2 = np.random.rand(V, 1)\n\n    return W1, W2, b1, b2\n\ndef softmax(z):      #6\n    \"\"\"\n    Inputs:\n        z: output scores from the hidden layer\n    Outputs:\n        yhat: prediction (estimate of y)\n    \"\"\"\n    yhat = np.exp(z) / np.sum(np.exp(z), axis=0)\n    return yhat\n\ndef forward_prop(x, W1, W2, b1, b2):      #7\n    \"\"\"\n    Inputs:\n        x: average one-hot vector for the context\n        W1,W2,b1,b2: weights and biases to be learned\n    Outputs:\n        z: output score vector\n    \"\"\"\n    h = W1 @ x + b1\n    h = np.maximum(0, h)\n    z = W2 @ h + b2\n    return z, h\n\ndef compute_cost(y, yhat, batch_size):     #8\n    logprobs = np.multiply(np.log(yhat), y) + np.multiply(\n        np.log(1 - yhat), 1 - y\n    )\n    cost = -1 / batch_size * np.sum(logprobs)\n    cost = np.squeeze(cost)\n    return cost\n\ndef back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size):    #9\n    \"\"\"\n    Inputs:\n        x:  average one hot vector for the context\n        yhat: prediction (estimate of y)\n        y:  target vector\n        h:  hidden vector (see eq. 1)\n        W1, W2, b1, b2:  weights and biases\n        batch_size: batch size\n    Outputs:\n        grad_W1, grad_W2, grad_b1, grad_b2:  gradients of weights and biases\n    \"\"\"\n    l1 = np.dot(W2.T, yhat - y)\n    l1 = np.maximum(0, l1)\n    grad_W1 = np.dot(l1, x.T) / batch_size\n    grad_W2 = np.dot(yhat - y, h.T) / batch_size\n    grad_b1 = np.sum(l1, axis=1, keepdims=True) / batch_size\n    grad_b2 = np.sum(yhat - y, axis=1, keepdims=True) / batch_size\n\n    return grad_W1, grad_W2, grad_b1, grad_b2\n\ndef gradient_descent(data, word2Ind, N, V, num_iters, alpha=0.03):     #10\n    \"\"\"\n    This is the gradient_descent function\n        Inputs:\n            data:      text\n            word2Ind:  words to Indices\n            N:         dimension of hidden vector\n            V:         dimension of vocabulary\n            num_iters: number of iterations\n        Outputs:\n            W1, W2, b1, b2:  updated matrices and biases\n\n        \"\"\"\n        W1, W2, b1, b2 = initialize_model(N, V, random_seed=8855)\n        batch_size = 128\n        iters = 0\n        C = 2\n        for x, y in get_batches(data, word2Ind, V, C, batch_size):\n            z, h = forward_prop(x, W1, W2, b1, b2)\n            yhat = softmax(z)\n            cost = compute_cost(y, yhat, batch_size)\n            if (iters + 1) % 10 == 0:\n                print(f\"iters: {iters+1} cost: {cost:.6f}\")\n            grad_W1, grad_W2, grad_b1, grad_b2 = back_prop(\n                x, yhat, y, h, W1, W2, b1, b2, batch_size\n            )\n            W1 = W1 - alpha * grad_W1\n            W2 = W2 - alpha * grad_W2\n            b1 = b1 - alpha * grad_b1\n            b2 = b2 - alpha * grad_b2\n            iters += 1\n            if iters == num_iters:\n                break\n            if iters % 100 == 0:\n                alpha *= 0.66\n\n        return W1, W2, b1, b2\n\nC = 2      #11\nN = 50\nword2Ind, Ind2word = get_dict(data)\nV = len(word2Ind)\nnum_iters = 150\nprint(\"Call gradient_descent\")\nW1, W2, b1, b2 = gradient_descent(data, word2Ind, N, V, num_iters)\n# Call gradient descent\n# Iters: 10 loss: 0.525015\n# Iters: 20 loss: 0.092373\n# Iters: 30 loss: 0.050474\n# Iters: 40 loss: 0.034724\n# Iters: 50 loss: 0.026468\n# Iters: 60 loss: 0.021385\n# Iters: 70 loss: 0.017941\n# Iters: 80 loss: 0.015453\n# Iters: 90 loss: 0.012099\n# Iters: 100 loss: 0.012099\n# Iters: 110 loss: 0.011253\n# Iters: 120 loss: 0.010551\n# Iters: 130 loss: 0.009932\n# Iters: 140 loss: 0.009382\n# Iters: 150 loss: 0.008889\n```", "```py\nwords = [          #1\n    \"King\",\n    \"Queen\",\n    \"Lord\",\n    \"Man\",\n    \"Woman\",\n    \"Prince\",\n    \"Ophelia\",\n    \"Rich\",\n    \"Happy\",\n]              \nembs = (W1.T + W2) / 2.0\nidx = [word2Ind[word] for word in words]\nX = embs[idx, :]\nprint(X.shape, idx)\n\nresult = compute_pca(X, 2)\npyplot.scatter(result[:, 0], result[:, 1])\nfor i, word in enumerate(words):\n    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\npyplot.show()\n```", "```py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MultiLayerPerceptron(nn.Module):\n    def __init__(\n        self,\n        input_size,\n        hidden_size=2,\n        output_size=3,\n        num_hidden_layers=1,\n        hidden_activation=nn.Sigmoid,\n    ):\n        \"\"\"Initialize weights.\n        Args:\n            input_size (int): size of the input\n            hidden_size (int): size of the hidden layers\n            output_size (int): size of the output\n            num_hidden_layers (int): number of hidden layers\n            hidden_activation (torch.nn.*): the activation class\n        \"\"\"\n        super(MultiLayerPerceptron, self).__init__()\n        self.module_list = nn.ModuleList()\n        interim_input_size = input_size\n        interim_output_size = hidden_size\n        torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n        for _ in range(num_hidden_layers):\n            self.module_list.append(\n                nn.Linear(interim_input_size, interim_output_size)\n            )\n            self.module_list.append(hidden_activation())\n            interim_input_size = interim_output_size\n\n        self.fc_final = nn.Linear(interim_input_size, output_size)\n\n        self.last_forward_cache = []\n\n    def forward(self, x, apply_softmax=False):\n        \"\"\"The forward pass of the MLP\n\n        Args:\n            x_in (torch.Tensor): an input data tensor.\n            x_in.shape should be (batch, input_dim)\n            apply_softmax (bool): a flag for the softmax activation\n                should be false if used with the Cross Entropy losses\n        Returns:\n            the resulting tensor. tensor.shape should be (batch, output_dim)\n        \"\"\"\n        for module in self.module_list:\n            x = module(x)\n\n        output = self.fc_final(x)\n\n        if apply_softmax:\n            output = F.softmax(output, dim=1)\n\n        return output\n```", "```py\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom gensim.models import Word2Vec\nfrom sklearn.model_selection import train_test_split\nimport nltk\nimport spacy\n\n    tokenizer = nltk.tokenize.RegexpTokenizer(\"\\w+'?\\w+|\\w+'\")\n    tokenizer.tokenize(\"This is a test\")\n    stop_words = nltk.corpus.stopwords.words(\"english\")\n    nlp = spacy.load(\"en_core_web_lg\", disable=[\"parser\", \"tagger\", \"ner\"])\n\ndataset = pd.read_csv(\"./data/twitter.csv\")     #1\n\ntext_data = list(\n    map(lambda x: tokenizer.tokenize(x.lower()), dataset[\"text\"])\n)\ntext_data = [\n    [token.lemma_ for word in text for token in nlp(word)]\n    for text in text_data\n]\nlabel_data = list(map(lambda x: x, dataset[\"feeling\"]))\nassert len(text_data) == len(\n    label_data\n), f\"{len(text_data)} does not equal {len(label_data)}\"\n\nEMBEDDING_DIM = 100\nmodel = Word2Vec(\n    text_data, vector_size=EMBEDDING_DIM, window=5, min_count=1, workers=4\n)\nword_vectors = model.wv\nprint(f\"Vocabulary Length: {len(model.wv)}\")\ndel model\n\npadding_value = len(word_vectors.index_to_key)\n    embedding_weights = torch.Tensor(word_vectors.vectors)    #2\n\nclass RNN(torch.nn.Module):\n    def __init__(\n        self,\n        input_dim,\n        embedding_dim,\n        hidden_dim,\n        output_dim,\n        embedding_weights,\n    ):\n        super().__init__()\n        self.embedding = torch.nn.Embedding.from_pretrained(\n            embedding_weights\n    )\n    self.rnn = torch.nn.RNN(embedding_dim, hidden_dim)\n    self.fc = torch.nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x, text_lengths):\n        embedded = self.embedding(x)\n        packed_embedded = torch.nn.utils.rnn.pack_padded_sequence(\n            embedded, text_lengths\n        )\n        packed_output, hidden = self.rnn(packed_embedded)\n        output, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(\n            packed_output\n        )\n        return self.fc(hidden.squeeze(0))\n\nINPUT_DIM = padding_value\nEMBEDDING_DIM = 100\nHIDDEN_DIM = 256\nOUTPUT_DIM = 1\n\nrnn_model = RNN(\n    INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, embedding_weights\n)\n\nrnn_optimizer = torch.optim.SGD(rnn_model.parameters(), lr=1e-3)\nrnn_criterion = torch.nn.BCEWithLogitsLoss()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass LSTM(torch.nn.Module):\n    def __init__(\n    self,\n    input_dim,\n    embedding_dim,\n    hidden_dim,\n    output_dim,\n    n_layers,\n    bidirectional,\n        dropout,\n        embedding_weights,\n    ):\n        super().__init__()\n        self.embedding = torch.nn.Embedding.from_pretrained(\n            embedding_weights\n        )\n        self.rnn = torch.nn.LSTM(\n            embedding_dim,\n            hidden_dim,\n            num_layers=n_layers,\n            bidirectional=bidirectional,\n            dropout=dropout,\n        )\n        self.fc = torch.nn.Linear(hidden_dim * 2, output_dim)\n        self.dropout = torch.nn.Dropout(dropout)\n\n    def forward(self, x, text_lengths):\n        embedded = self.embedding(x)\n        packed_embedded = torch.nn.utils.rnn.pack_padded_sequence(\n            embedded, text_lengths\n        )\n        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n        hidden = self.dropout(\n            torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n        )\n        return self.fc(hidden.squeeze(0))\n\nINPUT_DIM = padding_value\nEMBEDDING_DIM = 100\nHIDDEN_DIM = 256\nOUTPUT_DIM = 1\nN_LAYERS = 2\nBIDIRECTIONAL = True\nDROPOUT = 0.5\n\nlstm_model = LSTM(\n    INPUT_DIM,\n    EMBEDDING_DIM,\n    HIDDEN_DIM,\n    OUTPUT_DIM,\n    N_LAYERS,\n    BIDIRECTIONAL,\n    DROPOUT,\n    embedding_weights,\n)\n\nlstm_optimizer = torch.optim.Adam(lstm_model.parameters())\nlstm_criterion = torch.nn.BCEWithLogitsLoss()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef binary_accuracy(preds, y):\n    rounded_preds = torch.round(torch.sigmoid(preds))\n    correct = (rounded_preds == y).float()\n    acc = correct.sum() / len(correct)\n    return acc\n\ndef train(model, iterator, optimizer, criterion):\n    epoch_loss = 0\n    epoch_acc = 0\n    model.train()\n    for batch in iterator:\n        optimizer.zero_grad()\n        predictions = model(batch[\"text\"], batch[\"length\"]).squeeze(1)\n        loss = criterion(predictions, batch[\"label\"])\n        acc = binary_accuracy(predictions, batch[\"label\"])\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n\n    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n\ndef evaluate(model, iterator, criterion):\n    epoch_loss = 0\n    epoch_acc = 0\n    model.eval()\n    with torch.no_grad():\n        for batch in iterator:\n            predictions = model(batch[\"text\"], batch[\"length\"]).squeeze(1)\n            loss = criterion(predictions, batch[\"label\"])\n            acc = binary_accuracy(predictions, batch[\"label\"])\n\n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n\n    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n\nbatch_size = 2     #3\n\ndef iterator(X, y):\n    size = len(X)\n    permutation = np.random.permutation(size)\n    iterate = []\n    for i in range(0, size, batch_size):\n    indices = permutation[i : i + batch_size]\n        batch = {}\n        batch[\"text\"] = [X[i] for i in indices]\n        batch[\"label\"] = [y[i] for i in indices]\n\n        batch[\"text\"], batch[\"label\"] = zip(\n            *sorted(\n                zip(batch[\"text\"], batch[\"label\"]),\n                key=lambda x: len(x[0]),\n                reverse=True,\n            )\n        )\n        batch[\"length\"] = [len(utt) for utt in batch[\"text\"]]\n        batch[\"length\"] = torch.IntTensor(batch[\"length\"])\n        batch[\"text\"] = torch.nn.utils.rnn.pad_sequence(\n            batch[\"text\"], batch_first=True\n        ).t()\n        batch[\"label\"] = torch.Tensor(batch[\"label\"])\n\n        batch[\"label\"] = batch[\"label\"].to(device)\n        batch[\"length\"] = batch[\"length\"].to(device)\n        batch[\"text\"] = batch[\"text\"].to(device)\n\n        iterate.append(batch)\n\n    return iterate\n\nindex_utt = [\n    torch.tensor([word_vectors.key_to_index.get(word, 0) for word in text])\n    for text in text_data\n]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    index_utt, label_data, test_size=0.2\n)                                                    #4\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train, y_train, test_size=0.2\n)\n\ntrain_iterator = iterator(X_train, y_train)\nvalidate_iterator = iterator(X_val, y_val)\ntest_iterator = iterator(X_test, y_test)\n\nprint(len(train_iterator), len(validate_iterator), len(test_iterator))\n\nN_EPOCHS = 25\n\nfor model in [rnn_model, lstm_model]:\n    print(\n    \"|-----------------------------------------------------------------------------------------|\"\n    )\n    print(f\"Training with {model.__class__.__name__}\")\n    if \"RNN\" in model.__class__.__name__:\n        for epoch in range(N_EPOCHS):\n            train_loss, train_acc = train(\n                rnn_model, train_iterator, rnn_optimizer, rnn_criterion\n            )\n            valid_loss, valid_acc = evaluate(\n                rnn_model, validate_iterator, rnn_criterion\n            )\n\n            print(\n                f\"| Epoch: {epoch+1:02} | Train Loss: {train_loss: .3f} | \n                ↪ Train Acc: {train_acc*100: .2f}% | Validation Loss: \n                ↪ {valid_loss: .3f} | Validation Acc: {valid_acc*100: .2f}% |\"\n            )\n    else:\n        for epoch in range(N_EPOCHS):\n            train_loss, train_acc = train(\n                lstm_model, train_iterator, lstm_optimizer, lstm_criterion\n            )\n            valid_loss, valid_acc = evaluate(\n                lstm_model, validate_iterator, lstm_criterion\n            )\n\n            print(\n                f\"| Epoch: {epoch+1:02} | Train Loss: {train_loss: .3f} | \n                ↪ Train Acc: {train_acc*100: .2f}% | Validation Loss: \n                ↪ {valid_loss: .3f} | Validation Acc: {valid_acc*100: .2f}% |\"\n            )\n# Training on our dataset\n# | Epoch: 01 | Train Loss:  0.560 | Train Acc:  70.63% | Validation Loss:\n# 0.574 | Validation Acc:  70.88% |\n# | Epoch: 05 | Train Loss:  0.391 | Train Acc:  82.81% | Validation Loss:\n# 0.368 | Validation Acc:  83.08% |\n# | Epoch: 10 | Train Loss:  0.270 | Train Acc:  89.11% | Validation Loss:\n# 0.315 | Validation Acc:  86.22% |\n# | Epoch: 15 | Train Loss:  0.186 | Train Acc:  92.95% | Validation Loss:\n# 0.381 | Validation Acc:  87.49% |\n# | Epoch: 20 | Train Loss:  0.121 | Train Acc:  95.93% | Validation Loss:\n# 0.444 | Validation Acc:  86.29% |\n# | Epoch: 25 | Train Loss:  0.100 | Train Acc:  96.28% | Validation Loss:\n# 0.451 | Validation Acc:  86.83% |\n```", "```py\nimport numpy as np\nfrom scipy.special import softmax\n\nx = np.array([[1.0, 0.0, 1.0, 0.0],\n            [0.0, 2.0, 0.0, 2.0],\n            [1.0, 1.0, 1.0, 1.0]])     #1\n\nw_query = np.array([1,0,1],\n             [1,0,0],\n             [0,0,1],\n             [0,1,1]])          #2\nw_key = np.array([[0,0,1],\n             [1,1,0],\n             [0,1,0],\n             [1,1,0]])         #2\nw_value = np.array([[0,2,0], \n             [0,3,0],\n             [1,0,3],\n             [1,1,0]])         #2\n\nQ = np.matmul(x,w_query)     #3\nK = np.matmul(x,w_key)       #3\nV = np.matmul(x,w_value)     #3\n\nk_d = 1                                        #4\nattention_scores = (Q @ K.transpose())/k_d     #4\n\nattention_scores[0] = softmax(attention_scores[0])     #5\nattention_scores[1] = softmax(attention_scores[1])     #5\nattention_scores[2] = softmax(attention_scores[2])     #5\n\nattention1 = attention_scores[0].reshape(-1,1)     #6\nattention1 = attention_scores[0][0]*V[0]           #6\nattention2 = attention_scores[0][1]*V[1]           #6\nattention3 = attention_scores[0][2]*V[2]           #6\n\nattention_input1 = attention1 + attention2 + attention3    #7\n\nattention_head1 = np.random.random((3,64))      #8\n\nz0h1 = np.random.random((3,64))     #9\nz1h2 = np.random.random((3,64))     #9\nz2h3 = np.random.random((3,64))     #9\nz3h4 = np.random.random((3,64))     #9\nz4h5 = np.random.random((3,64))     #9\nz5h6 = np.random.random((3,64))     #9\nz6h7 = np.random.random((3,64))     #9\nz7h8 = np.random.random((3,64))     #9\n\nOutput_attention = np.hstack((z0h1,z1h2,z2h3,z3h4,z4h5,z5h6,z6h7,z7h8))    #10\n\ndef dot_product_attention(query, key, value, mask, scale=True):    #11\n    assert query.shape[-1] == key.shape[-1] == value.shape[-1], \"q,k,v have different dimensions!\"\n    if scale:\n        depth = query.shape[-1]\n    else:\n        depth = 1\n    dots = np.matmul(query, np.swapaxes(key, -1, -2)) / np.sqrt(depth)\n    if mask is not None:\n        dots = np.where(mask, dots, np.full_like(dots, -1e9))\n    logsumexp = scipy.special.logsumexp(dots, axis=-1, keepdims=True)\n    dots = np.exp(dots - logsumexp)\n    attention = np.matmul(dots, value)\n    return attention\ndef masked_dot_product_self_attention(q,k,v,scale=True):     #12\n    mask_size = q.shape[-2]\n    mask = np.tril(np.ones((1, mask_size, mask_size), dtype=np.bool_), k=0)\n    return DotProductAttention(q,k,v,mask,scale=scale)\n```", "```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nMODEL_NAME = \"bigscience/bloom\"    \n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n\nprompt = \"Hello world! This is my first time running an LLM!\"\n\ninput_tokens = tokenizer.encode(prompt, return_tensors=\"pt\", padding=True)\ngenerated_tokens = model.generate(input_tokens, max_new_tokens=20)\ngenerated_text = tokenizer.batch_decode(\n    generated_tokens, skip_special_tokens=True\n)\nprint(generated_text)\n```"]