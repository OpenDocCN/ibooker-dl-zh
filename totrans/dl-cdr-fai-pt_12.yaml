- en: Chapter 9\. Tabular Modeling Deep Dive
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。表格建模深入探讨
- en: Tabular modeling takes data in the form of a table (like a spreadsheet or CSV).
    The objective is to predict the value in one column based on the values in the
    other columns. In this chapter, we will look at not only deep learning, but also
    more general machine learning techniques like random forests, as they can give
    better results depending on your problem.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 表格建模将数据以表格形式（如电子表格或CSV文件）呈现。目标是基于其他列中的值来预测一列中的值。在本章中，我们将不仅看深度学习，还将看更一般的机器学习技术，如随机森林，因为根据您的问题，它们可能会给出更好的结果。
- en: We will look at how we should preprocess and clean the data as well as how to
    interpret the result of our models after training, but first we will see how we
    can feed columns that contain categories into a model that expects numbers by
    using embeddings.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看看我们应该如何预处理和清理数据，以及如何在训练后解释我们模型的结果，但首先我们将看看如何通过使用嵌入将包含类别的列馈送到期望数字的模型中。
- en: Categorical Embeddings
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类嵌入
- en: In tabular data, some columns may contain numerical data, like “age,” while
    others contain string values, like “sex.” The numerical data can be directly fed
    to the model (with some optional preprocessing), but the other columns need to
    be converted to numbers. Since the values in those correspond to different categories,
    we often call this type of variables *categorical variables*. The first type are
    called *continuous* *variables*.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在表格数据中，某些列可能包含数值数据，如“年龄”，而其他列包含字符串值，如“性别”。数值数据可以直接输入模型（经过一些可选的预处理），但其他列需要转换为数字。由于这些值对应不同的类别，我们通常将这种类型的变量称为*分类变量*。第一种类型被称为*连续*
    *变量*。
- en: 'Jargon: Continuous and Categorical Variables'
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 术语：连续和分类变量
- en: Continuous variables are numerical data, such as “age,” that can be directly
    fed to the model, since you can add and multiply them directly. Categorical variables
    contain a number of discrete levels, such as “movie ID,” for which addition and
    multiplication don’t have meaning (even if they’re stored as numbers).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 连续变量是数值数据，如“年龄”，可以直接输入模型，因为可以直接进行加法和乘法。分类变量包含多个离散级别，如“电影ID”，对于这些级别，加法和乘法没有意义（即使它们以数字形式存储）。
- en: At the end of 2015, the [Rossmann sales competition](https://oreil.ly/U85_1)
    ran on Kaggle. Competitors were given a wide range of information about various
    stores in Germany, and were tasked with trying to predict sales on a number of
    days. The goal was to help the company manage stock properly and be able to satisfy
    demand without holding unnecessary inventory. The official training set provided
    a lot of information about the stores. It was also permitted for competitors to
    use additional data, as long as that data was made public and available to all
    participants.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 2015年底，[Rossmann销售竞赛](https://oreil.ly/U85_1)在Kaggle上举行。参赛者获得了有关德国各个商店的各种信息，并被要求尝试预测若干天的销售额。目标是帮助公司适当管理库存，并能够满足需求而不必持有不必要的库存。官方训练集提供了大量有关商店的信息。允许参赛者使用额外的数据，只要这些数据是公开的并对所有参与者可用。
- en: 'One of the gold medalists used deep learning, in one of the earliest known
    examples of a state-of-the-art deep learning tabular model. Their method involved
    far less feature engineering, based on domain knowledge, than those of the other
    gold medalists. The paper [“Entity Embeddings of Categorical Variables”](https://oreil.ly/VmgoU)
    describes their approach. In an online-only chapter on the [book’s website](https://book.fast.ai),
    we show how to replicate it from scratch and attain the same accuracy shown in
    the paper. In the abstract of the paper, the authors (Cheng Guo and Felix Bekhahn)
    say:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一位金牌得主使用了深度学习，在已知的最先进深度学习表格模型的早期示例中。他们的方法涉及远少于其他金牌得主的基于领域知识的特征工程。论文[“分类变量的实体嵌入”](https://oreil.ly/VmgoU)描述了他们的方法。在[书籍网站](https://book.fast.ai)的在线专章中，我们展示了如何从头开始复制它，并获得论文中显示的相同准确性。在论文的摘要中，作者（Cheng
    Guo和Felix Bekhahn）说：
- en: Entity embedding not only reduces memory usage and speeds up neural networks
    compared with one-hot encoding, but more importantly by mapping similar values
    close to each other in the embedding space it reveals the intrinsic properties
    of the categorical variables…[It] is especially useful for datasets with lots
    of high cardinality features, where other methods tend to overfit…As entity embedding
    defines a distance measure for categorical variables, it can be used for visualizing
    categorical data and for data clustering.
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 实体嵌入不仅可以减少内存使用量并加快神经网络的速度，与独热编码相比，更重要的是通过将相似值映射到嵌入空间中的相邻位置，揭示了分类变量的固有属性...[它]在具有大量高基数特征的数据集中特别有用，其他方法往往容易过拟合...由于实体嵌入为分类变量定义了距离度量，因此可以用于可视化分类数据和数据聚类。
- en: We have already noticed all of these points when we built our collaborative
    filtering model. We can clearly see that these insights go far beyond just collaborative
    filtering, however.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们构建协同过滤模型时，我们已经注意到了所有这些要点。然而，我们可以清楚地看到这些见解远不止于协同过滤。
- en: The paper also points out that (as we discussed in the preceding chapter) an
    embedding layer is exactly equivalent to placing an ordinary linear layer after
    every one-hot-encoded input layer. The authors used the diagram in [Figure 9-1](#entity_emb)
    to show this equivalence. Note that “dense layer” is a term with the same meaning
    as “linear layer,” and the one-hot encoding layers represent inputs.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文还指出（正如我们在前一章中讨论的），嵌入层与在每个独热编码输入层之后放置普通线性层完全等效。作者使用[图9-1](#entity_emb)中的图表来展示这种等效性。请注意，“密集层”是与“线性层”相同含义的术语，而独热编码层代表输入。
- en: The insight is important because we already know how to train linear layers,
    so this shows that from the point of view of the architecture and our training
    algorithm, the embedding layer is just another layer. We also saw this in practice
    in the preceding chapter, when we built a collaborative filtering neural network
    that looks exactly like this diagram.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这一见解很重要，因为我们已经知道如何训练线性层，所以从架构和训练算法的角度来看，嵌入层只是另一层。我们在前一章中实践中也看到了这一点，当我们构建了一个与这个图表完全相同的协同过滤神经网络时。
- en: 'Just as we analyzed the embedding weights for movie reviews, the authors of
    the entity embeddings paper analyzed the embedding weights for their sales prediction
    model. What they found was quite amazing, and illustrates their second key insight:
    the embedding transforms the categorical variables into inputs that are both continuous
    and meaningful.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们分析了电影评论的嵌入权重一样，实体嵌入论文的作者分析了他们的销售预测模型的嵌入权重。他们发现的结果非常惊人，并展示了他们的第二个关键见解：嵌入将分类变量转换为连续且有意义的输入。
- en: '![Entity embeddings in a neural network](Images/dlcf_0901.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络中的实体嵌入](Images/dlcf_0901.png)'
- en: Figure 9-1\. Entity embeddings in a neural network (courtesy of Cheng Guo and
    Felix Berkhahn)
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-1。神经网络中的实体嵌入（由Cheng Guo和Felix Berkhahn提供）
- en: The images in [Figure 9-2](#state_emb) illustrate these ideas. They are based
    on the approaches used in the paper, along with some analysis we have added.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-2](#state_emb)中的图像说明了这些想法。它们基于论文中使用的方法，以及我们添加的一些分析。'
- en: '![State embeddings and map](Images/dlcf_0902.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![状态嵌入和地图](Images/dlcf_0902.png)'
- en: Figure 9-2\. State embeddings and map (courtesy of Cheng Guo and Felix Berkhahn)
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-2。状态嵌入和地图（由Cheng Guo和Felix Berkhahn提供）
- en: On the left is a plot of the embedding matrix for the possible values of the
    `State` category. For a categorical variable, we call the possible values of the
    variable its “levels” (or “categories” or “classes”), so here one level is “Berlin,”
    another is “Hamburg,” etc. On the right is a map of Germany. The actual physical
    locations of the German states were not part of the provided data, yet the model
    itself learned where they must be, based only on the behavior of store sales!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧是`State`类别可能值的嵌入矩阵图。对于分类变量，我们称变量的可能值为其“级别”（或“类别”或“类别”），因此这里一个级别是“柏林”，另一个是“汉堡”等。右侧是德国地图。德国各州的实际物理位置不是提供的数据的一部分，但模型本身学会了它们必须在哪里，仅基于商店销售的行为！
- en: Do you remember how we talked about *distance* between embeddings? The authors
    of the paper plotted the distance between store embeddings against the actual
    geographic distance between the stores (see [Figure 9-3](#store_emb)). They found
    that they matched very closely!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 您还记得我们谈论过嵌入之间的*距离*吗？论文的作者绘制了商店嵌入之间的距离与商店之间的实际地理距离之间的关系（参见[图9-3](#store_emb)）。他们发现它们非常接近！
- en: '![Store distances](Images/dlcf_0903.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![商店距离](Images/dlcf_0903.png)'
- en: Figure 9-3\. Store distances (courtesy of Cheng Guo and Felix Berkhahn)
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-3。存储距离（由Cheng Guo和Felix Berkhahn提供）
- en: We’ve even tried plotting the embeddings for days of the week and months of
    the year, and found that days and months that are near each other on the calendar
    ended up close as embeddings too, as shown in [Figure 9-4](#date_emb).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们甚至尝试绘制一周中的日期和一年中的月份的嵌入，发现在日历上彼此相邻的日期和月份也在嵌入中靠近，如[图9-4](#date_emb)所示。
- en: What stands out in these two examples is that we provide the model fundamentally
    categorical data about discrete entities (e.g., German states or days of the week),
    and then the model learns an embedding for these entities that defines a continuous
    notion of distance between them. Because the embedding distance was learned based
    on real patterns in the data, that distance tends to match up with our intuitions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个示例中突出的是，我们向模型提供了基本关于离散实体的分类数据（例如德国各州或一周中的日期），然后模型学习了这些实体的嵌入，定义了它们之间的连续距离概念。由于嵌入距离是基于数据中的真实模式学习的，因此该距离往往与我们的直觉相匹配。
- en: '![Date embeddings](Images/dlcf_0904.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![日期嵌入](Images/dlcf_0904.png)'
- en: Figure 9-4\. Date embeddings (courtesy of Cheng Guo and Felix Berkhahn)
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-4。日期嵌入（由Cheng Guo和Felix Berkhahn提供）
- en: In addition, it is valuable in its own right that embeddings are continuous,
    because models are better at understanding continuous variables. This is unsurprising
    considering models are built of many continuous parameter weights and continuous
    activation values, which are updated via gradient descent (a learning algorithm
    for finding the minimums of continuous functions).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，嵌入本身是有价值的，因为模型更擅长理解连续变量。这并不奇怪，因为模型由许多连续参数权重和连续激活值构成，这些值通过梯度下降（一种用于找到连续函数最小值的学习算法）进行更新。
- en: 'Another benefit is that we can combine our continuous embedding values with
    truly continuous input data in a straightforward manner: we just concatenate the
    variables and feed the concatenation into our first dense layer. In other words,
    the raw categorical data is transformed by an embedding layer before it interacts
    with the raw continuous input data. This is how fastai and Guo and Berkhahn handle
    tabular models containing continuous and categorical variables.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个好处是，我们可以将连续嵌入值与真正连续的输入数据简单地结合在一起：我们只需连接变量并将连接输入到我们的第一个密集层中。换句话说，在与原始连续输入数据交互之前，原始分类数据通过嵌入层进行转换。这就是fastai和Guo和Berkhahn处理包含连续和分类变量的表格模型的方式。
- en: An example using this concatenation approach is how Google does its recommendations
    on Google Play, as explained in the paper [“Wide & Deep Learning for Recommender
    Systems”](https://oreil.ly/wsnvQ). [Figure 9-5](#google_recsys) illustrates this.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种连接方法的一个示例是谷歌在Google Play上进行推荐的方式，正如在论文“广泛和深度学习用于推荐系统”中所解释的那样。[图9-5](#google_recsys)说明了这一点。
- en: 'Interestingly, the Google team combined both approaches we saw in the previous
    chapter: the dot product (which they call *cross product*) and neural network
    approaches.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '![The Google Play recommendation system](Images/dlcf_0905.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: Figure 9-5\. The Google Play recommendation system
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s pause for a moment. So far, the solution to all of our modeling problems
    has been to *train a deep learning model*. And indeed, that is a pretty good rule
    of thumb for complex unstructured data like images, sounds, natural language text,
    and so forth. Deep learning also works very well for collaborative filtering.
    But it is not always the best starting point for analyzing tabular data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Beyond Deep Learning
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most machine learning courses will throw dozens of algorithms at you, with a
    brief technical description of the math behind them and maybe a toy example. You’re
    left confused by the enormous range of techniques shown and have little practical
    understanding of how to apply them.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'The good news is that modern machine learning can be distilled down to a couple
    of key techniques that are widely applicable. Recent studies have shown that the
    vast majority of datasets can be best modeled with just two methods:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Ensembles of decision trees (i.e., random forests and gradient boosting machines),
    mainly for structured data (such as you might find in a database table at most
    companies)
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multilayered neural networks learned with SGD (i.e., shallow and/or deep learning),
    mainly for unstructured data (such as audio, images, and natural language)
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although deep learning is nearly always clearly superior for unstructured data,
    these two approaches tend to give quite similar results for many kinds of structured
    data. But ensembles of decision trees tend to train faster, are often easier to
    interpret, do not require special GPU hardware for inference at scale, and often
    require less hyperparameter tuning. They have also been popular for quite a lot
    longer than deep learning, so there is a more mature ecosystem of tooling and
    documentation around them.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: 'Most importantly, the critical step of interpreting a model of tabular data
    is significantly easier for decision tree ensembles. There are tools and methods
    for answering the pertinent questions, like these: Which columns in the dataset
    were the most important for your predictions? How are they related to the dependent
    variable? How do they interact with each other? And which particular features
    were most important for some particular observation?'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, ensembles of decision trees are our first approach for analyzing
    a new tabular dataset.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 'The exception to this guideline is when the dataset meets one of these conditions:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: There are some high-cardinality categorical variables that are very important
    (“cardinality” refers to the number of discrete levels representing categories,
    so a high-cardinality categorical variable is something like a zip code, which
    can take on thousands of possible levels).
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are some columns that contain data that would be best understood with
    a neural network, such as plain text data.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In practice, when we deal with datasets that meet these exceptional conditions,
    we always try both decision tree ensembles and deep learning to see which works
    best. Deep learning will likely be a useful approach in our example of collaborative
    filtering, as we have at least two high-cardinality categorical variables: the
    users and the movies. But in practice, things tend to be less cut-and-dried, and
    there will often be a mixture of high- and low-cardinality categorical variables
    and continuous variables.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Either way, it’s clear that we are going to need to add decision tree ensembles
    to our modeling toolbox!
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Up to now, we’ve used PyTorch and fastai for pretty much all of our heavy lifting.
    But these libraries are mainly designed for algorithms that do lots of matrix
    multiplication and derivatives (that is, stuff like deep learning!). Decision
    trees don’t depend on these operations at all, so PyTorch isn’t much use.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们几乎所有的繁重工作都是使用PyTorch和fastai完成的。但是这些库主要设计用于进行大量矩阵乘法和导数计算（即，类似深度学习的操作！）。决策树根本不依赖于这些操作，因此PyTorch没有太多用处。
- en: Instead, we will be largely relying on a library called *scikit-learn* (also
    known as *sklearn*). Scikit-learn is a popular library for creating machine learning
    models, using approaches that are not covered by deep learning. In addition, we’ll
    need to do some tabular data processing and querying, so we’ll want to use the
    Pandas library. Finally, we’ll also need NumPy, since that’s the main numeric
    programming library that both sklearn and Pandas rely on.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们将主要依赖一个名为*scikit-learn*（也称为*sklearn*）的库。Scikit-learn是一个流行的库，用于创建机器学习模型，使用的方法不包括深度学习。此外，我们需要进行一些表格数据处理和查询，因此我们将使用Pandas库。最后，我们还需要NumPy，因为这是sklearn和Pandas都依赖的主要数值编程库。
- en: We don’t have time to do a deep dive into all these libraries in this book,
    so we’ll just be touching on some of the main parts of each. For a far more in-depth
    discussion, we strongly suggest Wes McKinney’s [*Python for Data Analysis*](http://shop.oreilly.com/product/0636920050896.do)
    (O’Reilly). McKinney is the creator of Pandas, so you can be sure that the information
    is accurate!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有时间在本书中深入研究所有这些库，因此我们只会涉及每个库的一些主要部分。对于更深入的讨论，我们强烈建议阅读Wes McKinney的[*Python数据分析*](http://shop.oreilly.com/product/0636920050896.do)（O'Reilly）。McKinney是Pandas的创始人，因此您可以确信信息是准确的！
- en: First, let’s gather the data we will use.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们收集我们将使用的数据。
- en: The Dataset
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集
- en: 'The dataset we use in this chapter is from the Blue Book for Bulldozers Kaggle
    competition, which has the following description: “The goal of the contest is
    to predict the sale price of a particular piece of heavy equipment at auction
    based on its usage, equipment type, and configuration. The data is sourced from
    auction result postings and includes information on usage and equipment configurations.”'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的数据集来自于蓝皮书对推土机的Kaggle竞赛，该竞赛的描述如下：“比赛的目标是根据其使用情况、设备类型和配置来预测拍卖中特定重型设备的销售价格。数据来源于拍卖结果发布，并包括有关使用情况和设备配置的信息。”
- en: This is a very common type of dataset and prediction problem, similar to what
    you may see in your project or workplace. The dataset is available for download
    on Kaggle, a website that hosts data science competitions.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种非常常见的数据集类型和预测问题，类似于您在项目或工作场所中可能遇到的情况。该数据集可以在Kaggle上下载，Kaggle是一个举办数据科学竞赛的网站。
- en: Kaggle Competitions
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kaggle竞赛
- en: Kaggle is an awesome resource for aspiring data scientists or anyone looking
    to improve their machine learning skills. There is nothing like getting hands-on
    practice and receiving real-time feedback to help you improve your skills.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle是一个非常棒的资源，适合有志成为数据科学家或任何希望提高机器学习技能的人。没有什么比亲自动手实践并获得实时反馈来帮助您提高技能。
- en: 'Kaggle provides the following:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle提供了以下内容：
- en: Interesting datasets
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有趣的数据集
- en: Feedback on how you’re doing
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于您的表现的反馈
- en: A leaderboard to see what’s good, what’s possible, and what’s state-of-the-art
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排行榜可以看到什么是好的，什么是可能的，以及什么是最先进的
- en: Blog posts by winning contestants sharing useful tips and techniques
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获奖选手分享有用的技巧和技术的博客文章
- en: Until now, all our datasets have been available to download through fastai’s
    integrated dataset system. However, the dataset we will be using in this chapter
    is available only from Kaggle. Therefore, you will need to register on the site,
    then go to the [page for the competition](https://oreil.ly/B9wfd). On that page
    click Rules, and then I Understand and Accept. (Although the competition has finished,
    and you will not be entering it, you still have to agree to the rules to be allowed
    to download the data.)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所有的数据集都可以通过fastai的集成数据集系统下载。然而，在本章中我们将使用的数据集只能从Kaggle获取。因此，您需要在该网站上注册，然后转到[比赛页面](https://oreil.ly/B9wfd)。在该页面上点击规则，然后点击我理解并接受。（尽管比赛已经结束，您不会参加，但您仍然需要同意规则才能下载数据。）
- en: 'The easiest way to download Kaggle datasets is to use the Kaggle API. You can
    install this by using `pip` and running this in a notebook cell:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 下载Kaggle数据集的最简单方法是使用Kaggle API。您可以通过使用`pip`安装它，并在笔记本单元格中运行以下命令：
- en: '[PRE0]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You need an API key to use the Kaggle API; to get one, click your profile picture
    on the Kaggle website and choose My Account; then click Create New API Token.
    This will save a file called *kaggle.json* to your PC. You need to copy this key
    on your GPU server. To do so, open the file you downloaded, copy the contents,
    and paste them inside the single quotes in the following cell in the notebook
    associated with this chapter (e.g., `creds = `''`{"username":"*xxx*","key":"*xxx*"}`''``):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Kaggle API需要一个API密钥；要获取一个，点击Kaggle网站上的个人资料图片，选择我的账户；然后点击创建新的API令牌。这将在您的PC上保存一个名为*kaggle.json*的文件。您需要将此密钥复制到您的GPU服务器上。为此，请打开您下载的文件，复制内容，并将其粘贴到与本章相关的笔记本中的以下单引号内（例如，`creds
    = `'`{"username":"*xxx*","key":"*xxx*"}`'``）：
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then execute this cell (this needs to be run only once):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 然后执行此单元格（这只需要运行一次）：
- en: '[PRE2]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now you can download datasets from Kaggle! Pick a path to download the dataset
    to:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以从Kaggle下载数据集！选择一个路径来下载数据集：
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'And use the Kaggle API to download the dataset to that path and extract it:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用Kaggle API将数据集下载到该路径并解压缩：
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now that we have downloaded our dataset, let’s take a look at it!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经下载了数据集，让我们来看一下！
- en: Look at the Data
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查看数据
- en: 'Kaggle provides information about some of the fields of our dataset. The [Data
    page](https://oreil.ly/oSrBi) explains that the key fields in *train.csv* are
    as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle提供了有关我们数据集中某些字段的信息。[数据页面](https://oreil.ly/oSrBi)解释了*train.csv*中的关键字段如下：
- en: '`SalesID`'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`SalesID`'
- en: The unique identifier of the sale.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 销售的唯一标识符。
- en: '`MachineID`'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`MachineID`'
- en: The unique identifier of a machine. A machine can be sold multiple times.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 机器的唯一标识符。一台机器可以被多次出售。
- en: '`saleprice`'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`销售价格`'
- en: What the machine sold for at auction (provided only in *train.csv*).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 机器在拍卖中的售价（仅在*train.csv*中提供）。
- en: '`saledate`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`销售日期`'
- en: The date of the sale.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 销售日期。
- en: In any sort of data science work, it’s important to *look at your data directly*
    to make sure you understand the format, how it’s stored, what types of values
    it holds, etc. Even if you’ve read a description of the data, the actual data
    may not be what you expect. We’ll start by reading the training set into a Pandas
    DataFrame. Generally, it’s a good idea to also specify `low_memory=False` unless
    Pandas actually runs out of memory and returns an error. The `low_memory` parameter,
    which is `True` by default, tells Pandas to look at only a few rows of data at
    a time to figure out what type of data is in each column. This means that Pandas
    can end up using different data types for different rows, which generally leads
    to data processing errors or model training problems later.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何数据科学工作中，直接查看数据是很重要的，以确保您了解格式、存储方式、包含的值类型等。即使您已经阅读了数据的描述，实际数据可能并非您所期望的。我们将从将训练集读入Pandas
    DataFrame开始。通常，除非Pandas实际耗尽内存并返回错误，否则最好也指定`low_memory=False`。`low_memory`参数默认为`True`，告诉Pandas一次只查看几行数据，以确定每列中包含的数据类型。这意味着Pandas最终可能会为不同的行使用不同的数据类型，这通常会导致数据处理错误或模型训练问题。
- en: 'Let’s load our data and have a look at the columns:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载数据并查看列：
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: That’s a lot of columns for us to look at! Try looking through the dataset to
    get a sense of what kind of information is in each one. We’ll shortly see how
    to “zero in” on the most interesting bits.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们要查看的许多列！尝试浏览数据集，了解每个列中包含的信息类型。我们很快将看到如何“聚焦”于最有趣的部分。
- en: 'At this point, a good next step is to handle *ordinal columns*. This refers
    to columns containing strings or similar, but where those strings have a natural
    ordering. For instance, here are the levels of `ProductSize`:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，一个很好的下一步是处理*有序列的列*。这指的是包含字符串或类似内容的列，但其中这些字符串具有自然排序。例如，这里是`ProductSize`的级别：
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can tell Pandas about a suitable ordering of these levels like so:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以告诉Pandas这些级别的适当排序方式如下：
- en: '[PRE12]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The most important data column is the dependent variable—the one we want to
    predict. Recall that a model’s metric is a function that reflects how good the
    predictions are. It’s important to note what metric is being used for a project.
    Generally, selecting the metric is an important part of the project setup. In
    many cases, choosing a good metric will require more than just selecting a variable
    that already exists. It is more like a design process. You should think carefully
    about which metric, or set of metric, actually measures the notion of model quality
    that matters to you. If no variable represents that metric, you should see if
    you can build the metric from the variables that are available.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的数据列是因变量——我们想要预测的变量。请记住，模型的度量是反映预测有多好的函数。重要的是要注意项目使用的度量标准。通常，选择度量标准是项目设置的重要部分。在许多情况下，选择一个好的度量标准将需要不仅仅是选择一个已经存在的变量。这更像是一个设计过程。您应该仔细考虑哪种度量标准，或一组度量标准，实际上衡量了对您重要的模型质量概念。如果没有变量代表该度量标准，您应该看看是否可以从可用的变量构建度量标准。
- en: 'However, in this case, Kaggle tells us what metric to use: the root mean squared
    log error (RMLSE) between the actual and predicted auction prices. We need do
    only a small amount of processing to use this: we take the log of the prices,
    so that the `m_rmse` of that value will give us what we ultimately need:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在这种情况下，Kaggle告诉我们要使用的度量标准是实际和预测拍卖价格之间的平方对数误差（RMLSE）。我们只需要进行少量处理即可使用这个度量标准：我们取价格的对数，这样该值的`m_rmse`将给出我们最终需要的值：
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We are now ready to explore our first machine learning algorithm for tabular
    data: decision trees.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备探索我们的第一个用于表格数据的机器学习算法：决策树。
- en: Decision Trees
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树
- en: Decision tree ensembles, as the name suggests, rely on decision trees. So let’s
    start there! A decision tree asks a series of binary (yes or no) questions about
    the data. After each question, the data at that part of the tree is split between
    a Yes and a No branch, as shown in [Figure 9-6](#decision_tree). After one or
    more questions, either a prediction can be made on the basis of all previous answers
    or another question is required.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树集成，顾名思义，依赖于决策树。所以让我们从那里开始！决策树对数据提出一系列关于数据的二元（是或否）问题。在每个问题之后，树的那部分数据在“是”和“否”分支之间分割，如[图9-6](#decision_tree)所示。经过一个或多个问题后，可以基于所有先前答案做出预测，或者需要另一个问题。
- en: This sequence of questions is now a procedure for taking any data item, whether
    an item from the training set or a new one, and assigning that item to a group.
    Namely, after asking and answering the questions, we can say the item belongs
    to the same group as all the other training data items that yielded the same set
    of answers to the questions. But what good is this? The goal of our model is to
    predict values for items, not to assign them into groups from the training dataset.
    The value is that we can now assign a prediction value for each of these groups—for
    regression, we take the target mean of the items in the group.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这一系列问题是一个过程，用于获取任何数据项，无论是来自训练集还是新数据项，并将该项分配到一个组中。换句话说，在提出问题并回答问题之后，我们可以说该项属于与所有其他训练数据项相同的组，这些数据项对问题的答案相同。但这有什么好处呢？我们模型的目标是预测项目的值，而不是将它们分配到训练数据集中的组中。好处在于我们现在可以为这些组中的每个项目分配一个预测值——对于回归，我们取该组中项目的目标均值。
- en: '![An example of decision tree](Images/dlcf_0906.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![决策树示例](Images/dlcf_0906.png)'
- en: Figure 9-6\. An example of decision tree
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-6\. 决策树示例
- en: 'Let’s consider how we find the right questions to ask. Of course, we wouldn’t
    want to have to create all these questions ourselves—that’s what computers are
    for! The basic steps to train a decision tree can be written down very easily:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑如何找到正确的问题。当然，我们不希望自己创建所有这些问题-这就是计算机的作用！训练决策树的基本步骤可以很容易地写下来：
- en: Loop through each column of the dataset in turn.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 依次循环数据集的每一列。
- en: For each column, loop through each possible level of that column in turn.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每一列，依次循环该列的每个可能级别。
- en: Try splitting the data into two groups, based on whether they are greater than
    or less than that value (or if it is a categorical variable, based on whether
    they are equal to or not equal to that level of that categorical variable).
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试将数据分成两组，基于它们是否大于或小于该值（或者如果它是一个分类变量，则基于它们是否等于或不等于该分类变量的水平）。
- en: Find the average sale price for each of those two groups, and see how close
    that is to the actual sale price of each of the items of equipment in that group.
    Treat this as a very simple “model” in which our predictions are simply the average
    sale price of the item’s group.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到这两组中每组的平均销售价格，并查看这与该组中每个设备的实际销售价格有多接近。将这视为一个非常简单的“模型”，其中我们的预测只是该项组的平均销售价格。
- en: After looping through all of the columns and all the possible levels for each,
    pick the split point that gave the best predictions using that simple model.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在循环遍历所有列和每个可能的级别后，选择使用该简单模型给出最佳预测的分割点。
- en: We now have two groups for our data, based on this selected split. Treat each
    group as a separate dataset, and find the best split for each by going back to
    step 1 for each group.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们的数据有两组，基于这个选定的分割。将每个组视为一个单独的数据集，并通过返回到步骤1为每个组找到最佳分割。
- en: Continue this process recursively, until you have reached some stopping criterion
    for each group—for instance, stop splitting a group further when it has only 20
    items in it.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 递归地继续这个过程，直到每个组达到某个停止标准-例如，当组中只有20个项目时停止进一步分割。
- en: Although this is an easy enough algorithm to implement yourself (and it is a
    good exercise to do so), we can save some time by using the implementation built
    into sklearn.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这是一个很容易自己实现的算法（这是一个很好的练习），但我们可以节省一些时间，使用内置在sklearn中的实现。
- en: First, however, we need to do a little data preparation.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，首先，我们需要做一些数据准备。
- en: Alexis Says
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Alexis说
- en: Here’s a productive question to ponder. If you consider that the procedure for
    defining a decision tree essentially chooses one *sequence of splitting questions
    about variables*, you might ask yourself, how do we know this procedure chooses
    the *correct sequence*? The rule is to choose the splitting question that produces
    the best split (i.e., that most accurately separates the items into two distinct
    categories), and then to apply the same rule to the groups that split produces,
    and so on. This is known in computer science as a “greedy” approach. Can you imagine
    a scenario in which asking a “less powerful” splitting question would enable a
    better split down the road (or should I say down the trunk!) and lead to a better
    result overall?
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个值得思考的有益问题。如果您考虑到定义决策树的过程本质上选择了一个关于变量的*分割问题序列*，您可能会问自己，我们如何知道这个过程选择了*正确的序列*？规则是选择产生最佳分割（即最准确地将项目分为两个不同类别）的分割问题，然后将同样的规则应用于该分割产生的组，依此类推。这在计算机科学中被称为“贪婪”方法。您能想象出一个情景，其中提出一个“不那么强大”的分割问题会使未来的分割更好（或者我应该说更好地导致更好的结果）吗？
- en: Handling Dates
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理日期
- en: The first piece of data preparation we need to do is to enrich our representation
    of dates. The fundamental basis of the decision tree that we just described is
    *bisection*—dividing a group into two. We look at the ordinal variables and divide
    the dataset based on whether the variable’s value is greater (or lower) than a
    threshold, and we look at the categorical variables and divide the dataset based
    on whether the variable’s level is a particular level. So this algorithm has a
    way of dividing the dataset based on both ordinal and categorical data.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件数据准备工作是丰富我们对日期的表示。我们刚刚描述的决策树的基本基础是*二分* - 将一组分成两组。我们查看序数变量，并根据变量的值是大于（或小于）阈值来划分数据集，我们查看分类变量，并根据变量的级别是否是特定级别来划分数据集。因此，这个算法有一种根据序数和分类数据划分数据集的方法。
- en: But how does this apply to a common data type, the date? You might want to treat
    a date as an ordinal value, because it is meaningful to say that one date is greater
    than another. However, dates are a bit different from most ordinal values in that
    some dates are qualitatively different from others in a way that that is often
    relevant to the systems we are modeling.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 但是这如何适用于常见的数据类型，日期呢？您可能希望将日期视为序数值，因为说一个日期比另一个日期更大是有意义的。然而，日期与大多数序数值有所不同，因为一些日期在某种方面与其他日期有质的不同，这通常与我们建模的系统相关。
- en: To help our algorithm handle dates intelligently, we’d like our model to know
    more than whether a date is more recent or less recent than another. We might
    want our model to make decisions based on that date’s day of the week, on whether
    a day is a holiday, on what month it is in, and so forth. To do this, we replace
    every date column with a set of date metadata columns, such as holiday, day of
    week, and month. These columns provide categorical data that we suspect will be
    useful.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助我们的算法智能处理日期，我们希望我们的模型不仅知道一个日期是否比另一个日期更近或更早。我们可能希望我们的模型根据日期的星期几、某一天是否是假期、所在月份等来做决策。为此，我们用一组日期元数据列替换每个日期列，例如假期、星期几和月份。这些列提供了我们认为会有用的分类数据。
- en: 'fastai comes with a function that will do this for us—we just have to pass
    a column name that contains dates:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: fastai带有一个函数，可以为我们执行此操作-我们只需传递包含日期的列名：
- en: '[PRE16]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let’s do the same for the test set while we’re there:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在那里的同时，让我们为测试集做同样的事情：
- en: '[PRE17]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can see that there are now lots of new columns in our DataFrame:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This is a good first step, but we will need to do a bit more cleaning. For this,
    we will use fastai objects called `TabularPandas` and `TabularProc`.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Using TabularPandas and TabularProc
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A second piece of preparatory processing is to be sure we can handle strings
    and missing data. Out of the box, sklearn cannot do either. Instead we will use
    fastai’s class `TabularPandas`, which wraps a Pandas DataFrame and provides a
    few conveniences. To populate a `TabularPandas`, we will use two `TabularProc`s,
    `Categorify` and `FillMissing`. A `TabularProc` is like a regular `Transform`,
    except for the following:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: It returns the exact same object that’s passed to it, after modifying the object
    in place.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It runs the transform once, when data is first passed in, rather than lazily
    as the data is accessed.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Categorify` is a `TabularProc` that replaces a column with a numeric categorical
    column. `FillMissing` is a `TabularProc` that replaces missing values with the
    median of the column, and creates a new Boolean column that is set to `True` for
    any row where the value was missing. These two transforms are needed for nearly
    every tabular dataset you will use, so this is a good starting point for your
    data processing:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '`TabularPandas` will also handle splitting the dataset into training and validation
    sets for us. However, we need to be very careful about our validation set. We
    want to design it so that it is like the *test set* Kaggle will use to judge the
    contest.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Recall the distinction between a validation set and a test set, as discussed
    in [Chapter 1](ch01.xhtml#chapter_intro). A *validation set* is data we hold back
    from training in order to ensure that the training process does not overfit on
    the training data. A *test set* is data that is held back even more deeply, from
    us ourselves, in order to ensure that *we* don’t overfit on the validation data
    as we explore various model architectures and hyperparameters.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: We don’t get to see the test set. But we do want to define our validation data
    so that it has the same sort of relationship to the training data as the test
    set will have.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, just randomly choosing a subset of your data points will do that.
    This is not one of those cases, because it is a time series.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: If you look at the date range represented in the test set, you will discover
    that it covers a six-month period from May 2012, which is later in time than any
    date in the training set. This is a good design, because the competition sponsor
    will want to ensure that a model is able to predict the future. But it means that
    if we are going to have a useful validation set, we also want the validation set
    to be later in time than the training set. The Kaggle training data ends in April
    2012, so we will define a narrower training dataset that consists only of the
    Kaggle training data from before November 2011, and we’ll define a validation
    set consisting of data from after November 2011.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this we use `np.where`, a useful function that returns (as the first
    element of a tuple) the indices of all `True` values:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '`TabularPandas` needs to be told which columns are continuous and which are
    categorical. We can handle that automatically using the helper function `cont_cat_split`:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'A `TabularPandas` behaves a lot like a fastai `Datasets` object, including
    providing `train` and `valid` attributes:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can see that the data is still displayed as strings for categories (we show
    only a few columns here because the full table is too big to fit on a page):'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '|  | state | ProductGroup | Drive_System | Enclosure | SalePrice |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: '| 0 | Alabama | WL | #na# | EROPS w AC | 11.097410 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
- en: '| 1 | North Carolina | WL | #na# | EROPS w AC | 10.950807 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
- en: '| 2 | New York | SSL | #na# | OROPS | 9.210340 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
- en: 'However, the underlying items are all numeric:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '|  | state | ProductGroup | Drive_System | Enclosure |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 6 | 0 | 3 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
- en: '| 1 | 33 | 6 | 0 | 3 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
- en: '| 2 | 32 | 3 | 0 | 6 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
- en: 'The conversion of categorical columns to numbers is done by simply replacing
    each unique level with a number. The numbers associated with the levels are chosen
    consecutively as they are seen in a column, so there’s no particular meaning to
    the numbers in categorical columns after conversion. The exception is if you first
    convert a column to a Pandas ordered category (as we did for `ProductSize` earlier),
    in which case the ordering you chose is used. We can see the mapping by looking
    at the `classes` attribute:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Since it takes a minute or so to process the data to get to this point, we
    should save it—that way, in the future, we can continue our work from here without
    rerunning the previous steps. fastai provides a `save` method that uses Python’s
    *pickle* system to save nearly any Python object:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'To read this back later, you would type this:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Now that all this preprocessing is done, we are ready to create a decision tree.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Creating the Decision Tree
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To begin, we define our independent and dependent variables:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now that our data is all numeric, and there are no missing values, we can create
    a decision tree:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'To keep it simple, we’ve told sklearn to create just four *leaf nodes*. To
    see what it’s learned, we can display the tree:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![](Images/dlcf_09in01.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: Understanding this picture is one of the best ways to understand decision trees,
    so we will start at the top and explain each part step by step.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: The top node represents the *initial model* before any splits have been done,
    when all the data is in one group. This is the simplest possible model. It is
    the result of asking zero questions and will always predict the value to be the
    average value of the whole dataset. In this case, we can see it predicts a value
    of 10.1 for the logarithm of the sales price. It gives a mean squared error of
    0.48\. The square root of this is 0.69. (Remember that unless you see `m_rmse`,
    or a *root mean squared error*, the value you are looking at is before taking
    the square root, so it is just the average of the square of the differences.)
    We can also see that there are 404,710 auction records in this group—that is the
    total size of our training set. The final piece of information shown here is the
    decision criterion for the best split that was found, which is to split based
    on the `coupler_system` column.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Moving down and to the left, this node shows us that there were 360,847 auction
    records for equipment where `coupler_system` was less than 0.5. The average value
    of our dependent variable in this group is 10.21. Moving down and to the right
    from the initial model takes us to the records where `coupler_system` was greater
    than 0.5.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'The bottom row contains our *leaf nodes*: the nodes with no answers coming
    out of them, because there are no more questions to be answered. At the far right
    of this row is the node containing records where `coupler_system` was greater
    than 0.5\. The average value is 9.21, so we can see the decision tree algorithm
    did find a single binary decision that separated high-value from low-value auction
    results. Asking only about `coupler_system` predicts an average value of 9.21
    versus 10.1.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Returning back to the top node after the first decision point, we can see that
    a second binary decision split has been made, based on asking whether `YearMade`
    is less than or equal to 1991.5\. For the group where this is true (remember,
    this is now following two binary decisions, based on `coupler_system` and `YearMade`),
    the average value is 9.97, and there are 155,724 auction records in this group.
    For the group of auctions where this decision is false, the average value is 10.4,
    and there are 205,123 records. So again, we can see that the decision tree algorithm
    has successfully split our more expensive auction records into two more groups
    that differ in value significantly.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'We can show the same information using Terence Parr’s powerful [dtreeviz library](https://oreil.ly/e9KrM):'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![](Images/dlcf_09in02.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
- en: 'This shows a chart of the distribution of the data for each split point. We
    can clearly see that there’s a problem with our `YearMade` data: there are bulldozers
    made in the year 1000, apparently! Presumably, this is just a missing value code
    (a value that doesn’t otherwise appear in the data and that is used as a placeholder
    in cases where a value is missing). For modeling purposes, 1000 is fine, but as
    you can see, this outlier makes visualizing the values we are interested in more
    difficult. So, let’s replace it with 1950:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: That change makes the split much clearer in the tree visualization, even although
    it doesn’t change the result of the model in any significant way. This is a great
    example of how resilient decision trees are to data issues!
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![](Images/dlcf_09in03.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
- en: 'Let’s now have the decision tree algorithm build a bigger tree. Here, we are
    not passing in any stopping criteria such as `max_leaf_nodes`:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We’ll create a little function to check the root mean squared error of our
    model (`m_rmse`), since that’s how the competition was judged:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'So, our model is perfect, right? Not so fast…remember, we really need to check
    the validation set, to ensure we’re not overfitting:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Oops—it looks like we might be overfitting pretty badly. Here’s why:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We have nearly as many leaf nodes as data points! That seems a little over-enthusiastic.
    Indeed, sklearn’s default settings allow it to continue splitting nodes until
    there is only one item in each leaf node. Let’s change the stopping rule to tell
    sklearn to ensure every leaf node contains at least 25 auction records:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'That looks much better. Let’s check the number of leaves again:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Much more reasonable!
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Alexis Says
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here’s my intuition for an overfitting decision tree with more leaf nodes than
    data items. Consider the game Twenty Questions. In that game, the chooser secretly
    imagines an object (like, “our television set”), and the guesser gets to pose
    20 yes or no questions to try to guess what the object is (like “Is it bigger
    than a breadbox?”). The guesser is not trying to predict a numerical value, but
    just to identify a particular object out of the set of all imaginable objects.
    When your decision tree has more leaves than there are possible objects in your
    domain, it is essentially a well-trained guesser. It has learned the sequence
    of questions needed to identify a particular data item in the training set, and
    it is “predicting” only by describing that item’s value. This is a way of memorizing
    the training set—i.e., of overfitting.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Building a decision tree is a good way to create a model of our data. It is
    very flexible, since it can clearly handle nonlinear relationships and interactions
    between variables. But we can see there is a fundamental compromise between how
    well it generalizes (which we can achieve by creating small trees) and how accurate
    it is on the training set (which we can achieve by using large trees).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'So how do we get the best of both worlds? We’ll show you right after we handle
    an important missing detail: how to handle categorical variables.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Categorical Variables
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous chapter, when working with deep learning networks, we dealt
    with categorical variables by one-hot encoding them and feeding them to an embedding
    layer. The embedding layer helped the model to discover the meaning of the different
    levels of these variables (the levels of a categorical variable do not have an
    intrinsic meaning, unless we manually specify an ordering using Pandas). In a
    decision tree, we don’t have embedding layers—so how can these untreated categorical
    variables do anything useful in a decision tree? For instance, how could something
    like a product code be used?
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'The short answer is: it just works! Think about a situation in which one product
    code is far more expensive at auction than any other one. In that case, any binary
    split will result in that one product code being in some group, and that group
    will be more expensive than the other group. Therefore, our simple decision tree
    building algorithm will choose that split. Later, during training, the algorithm
    will be able to further split the subgroup that contains the expensive product
    code, and over time, the tree will home in on that one expensive product.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 简短的答案是：它就是有效！想象一种情况，其中一个产品代码在拍卖中比其他任何产品代码都要昂贵得多。在这种情况下，任何二元分割都将导致该产品代码位于某个组中，而该组将比其他组更昂贵。因此，我们简单的决策树构建算法将选择该分割。稍后，在训练过程中，算法将能够进一步分割包含昂贵产品代码的子组，随着时间的推移，树将聚焦于那一个昂贵的产品。
- en: It is also possible to use one-hot encoding to replace a single categorical
    variable with multiple one-hot-encoded columns, where each column represents a
    possible level of the variable. Pandas has a `get_dummies` method that does just
    that.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以使用一位编码来替换单个分类变量，其中每一列代表变量的一个可能级别。Pandas有一个`get_dummies`方法可以做到这一点。
- en: 'However, there is not really any evidence that such an approach improves the
    end result. So, we generally avoid it where possible, because it does end up making
    your dataset harder to work with. In 2019, this issue was explored in the paper
    [“Splitting on Categorical Predictors in Random Forests”](https://oreil.ly/ojzKJ)
    by Marvin Wright and Inke König:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，实际上并没有证据表明这种方法会改善最终结果。因此，我们通常会尽可能避免使用它，因为它确实会使您的数据集更难处理。在2019年，这个问题在Marvin
    Wright和Inke König的论文[“Splitting on Categorical Predictors in Random Forests”](https://oreil.ly/ojzKJ)中得到了探讨：
- en: The standard approach for nominal predictors is to consider all 2^(*k* − 1)
    − 1 2-partitions of the *k* predictor categories. However, this exponential relationship
    produces a large number of potential splits to be evaluated, increasing computational
    complexity and restricting the possible number of categories in most implementations.
    For binary classification and regression, it was shown that ordering the predictor
    categories in each split leads to exactly the same splits as the standard approach.
    This reduces computational complexity because only *k* − 1 splits have to be considered
    for a nominal predictor with *k* categories.
  id: totrans-221
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对于名义预测器，标准方法是考虑所有2^(*k* − 1) − 1个*k*预测类别的2-分区。然而，这种指数关系会产生大量需要评估的潜在分割，增加了计算复杂性并限制了大多数实现中可能的类别数量。对于二元分类和回归，已经证明按照每个分割中的预测类别进行排序会导致与标准方法完全相同的分割。这减少了计算复杂性，因为对于具有*k*个类别的名义预测器，只需要考虑*k*
    − 1个分割。
- en: 'Now that you understand how decision trees work, it’s time for that best-of-both-worlds
    solution: random forests.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您了解了决策树的工作原理，是时候尝试那种最佳的解决方案了：随机森林。
- en: Random Forests
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林
- en: 'In 1994, Berkeley professor Leo Breiman, one year after his retirement, published
    a small technical report called [“Bagging Predictors”](https://oreil.ly/6gMuG),
    which turned out to be one of the most influential ideas in modern machine learning.
    The report began:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 1994年，伯克利大学教授Leo Breiman在退休一年后发表了一份名为[“Bagging Predictors”](https://oreil.ly/6gMuG)的小型技术报告，这个报告成为现代机器学习中最有影响力的想法之一。报告开始说：
- en: Bagging predictors is a method for generating multiple versions of a predictor
    and using these to get an aggregated predictor. The aggregation averages over
    the versions…The multiple versions are formed by making bootstrap replicates of
    the learning set and using these as new learning sets. Tests…show that bagging
    can give substantial gains in accuracy. The vital element is the instability of
    the prediction method. If perturbing the learning set can cause significant changes
    in the predictor constructed, then bagging can improve accuracy.
  id: totrans-225
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Bagging预测器是一种生成预测器的多个版本并使用这些版本来获得聚合预测器的方法。聚合平均了这些版本……通过对学习集进行自助复制并将其用作新的学习集来形成多个版本。测试表明，bagging可以显著提高准确性。关键因素是预测方法的不稳定性。如果扰动学习集可以导致构建的预测器发生显著变化，那么bagging可以提高准确性。
- en: 'Here is the procedure that Breiman is proposing:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Breiman提出的程序：
- en: Randomly choose a subset of the rows of your data (i.e., “bootstrap replicates
    of your learning set”).
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择数据的子集（即“学习集的自助复制”）。
- en: Train a model using this subset.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这个子集训练模型。
- en: Save that model, and then return to step 1 a few times.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存该模型，然后返回到步骤1几次。
- en: This will give you multiple trained models. To make a prediction, predict using
    all of the models, and then take the average of each of those model’s predictions.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将为您提供多个经过训练的模型。要进行预测，请使用所有模型进行预测，然后取每个模型预测的平均值。
- en: 'This procedure is known as *bagging*. It is based on a deep and important insight:
    although each of the models trained on a subset of data will make more errors
    than a model trained on the full dataset, those errors will not be correlated
    with each other. Different models will make different errors. The average of those
    errors, therefore, is zero! So if we take the average of all of the models’ predictions,
    we should end up with a prediction that gets closer and closer to the correct
    answer, the more models we have. This is an extraordinary result—it means that
    we can improve the accuracy of nearly any kind of machine learning algorithm by
    training it multiple times, each time on a different random subset of the data,
    and averaging its predictions.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程被称为*bagging*。它基于一个深刻而重要的观点：尽管在数据子集上训练的每个模型会比在完整数据集上训练的模型产生更多错误，但这些错误不会相互关联。不同的模型会产生不同的错误。因此，这些错误的平均值为零！因此，如果我们取所有模型预测的平均值，我们应该得到一个预测，随着模型数量的增加，它会越来越接近正确答案。这是一个非凡的结果——这意味着我们可以通过多次在不同随机数据子集上训练它来改进几乎任何类型的机器学习算法的准确性，并对其预测进行平均。
- en: In 2001, Breiman went on to demonstrate that this approach to building models,
    when applied to decision tree building algorithms, was particularly powerful.
    He went even further than just randomly choosing rows for each model’s training,
    but also randomly selected from a subset of columns when choosing each split in
    each decision tree. He called this method the *random forest*. Today it is, perhaps,
    the most widely used and practically important machine learning method.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 2001年，Breiman继续展示了这种建模方法，当应用于决策树构建算法时，特别强大。他甚至比仅仅随机选择每个模型训练的行更进一步，还在每棵决策树的每个分裂点随机选择了一部分列。他将这种方法称为*随机森林*。今天，它可能是最广泛使用和实际重要的机器学习方法。
- en: In essence, a random forest is a model that averages the predictions of a large
    number of decision trees, which are generated by randomly varying various parameters
    that specify what data is used to train the tree and other tree parameters. Bagging
    is a particular approach to *ensembling*, or combining the results of multiple
    models together. To see how it works in practice, let’s get started on creating
    our own random forest!
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 实质上，随机森林是一个模型，它平均了大量决策树的预测结果，这些决策树是通过随机变化各种参数生成的，这些参数指定了用于训练树和其他树参数的数据。Bagging是一种特定的*集成*方法，或者将多个模型的结果组合在一起。为了看看它在实践中是如何工作的，让我们开始创建我们自己的随机森林！
- en: Creating a Random Forest
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建随机森林
- en: We can create a random forest just like we created a decision tree, except now
    we are also specifying parameters that indicate how many trees should be in the
    forest, how we should subset the data items (the rows), and how we should subset
    the fields (the columns).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像创建决策树一样创建随机森林，只是现在我们还指定了指示森林中应该有多少树，如何对数据项（行）进行子集化以及如何对字段（列）进行子集化的参数。
- en: 'In the following function definition, `n_estimators` defines the number of
    trees we want, `max_samples` defines how many rows to sample for training each
    tree, and `max_features` defines how many columns to sample at each split point
    (where `0.5` means “take half the total number of columns”). We can also specify
    when to stop splitting the tree nodes, effectively limiting the depth of the tree,
    by including the same `min_samples_leaf` parameter we used in the preceding section.
    Finally, we pass `n_jobs=-1` to tell sklearn to use all our CPUs to build the
    trees in parallel. By creating a little function for this, we can more quickly
    try variations in the rest of this chapter:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的函数定义中，`n_estimators`定义了我们想要的树的数量，`max_samples`定义了每棵树训练时要抽样的行数，`max_features`定义了在每个分裂点抽样的列数（其中`0.5`表示“取一半的总列数”）。我们还可以指定何时停止分裂树节点，有效地限制树的深度，通过包含我们在前一节中使用的相同`min_samples_leaf`参数。最后，我们传递`n_jobs=-1`告诉sklearn使用所有CPU并行构建树。通过创建一个小函数，我们可以更快地尝试本章其余部分的变化：
- en: '[PRE50]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Our validation RMSE is now much improved over our last result produced by the
    `DecisionTreeRegressor`, which made just one tree using all the available data:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的验证RMSE现在比我们上次使用`DecisionTreeRegressor`生成的结果要好得多，后者只使用了所有可用数据生成了一棵树：
- en: '[PRE52]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: One of the most important properties of random forests is that they aren’t very
    sensitive to the hyperparameter choices, such as `max_features`. You can set `n_estimators`
    to as high a number as you have time to train—the more trees you have, the more
    accurate the model will be. `max_samples` can often be left at its default, unless
    you have over 200,000 data points, in which case setting it to 200,000 will make
    it train faster with little impact on accuracy. `max_features=0.5` and `min_samples_leaf=4`
    both tend to work well, although sklearn’s defaults work well too.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林最重要的特性之一是它对超参数选择不太敏感，比如`max_features`。您可以将`n_estimators`设置为尽可能高的数字，以便训练更多的树，树越多，模型就越准确。`max_samples`通常可以保持默认值，除非您有超过200,000个数据点，在这种情况下，将其设置为200,000将使其在准确性上有很小影响的情况下更快地训练。`max_features=0.5`和`min_samples_leaf=4`通常效果很好，尽管sklearn的默认值也很好。
- en: The sklearn docs [show an example](https://oreil.ly/E0Och) of the effects of
    different `max_features` choices, with increasing numbers of trees. In the plot,
    the blue plot line uses the fewest features, and the green line uses the most
    (it uses all the features). As you can see in [Figure 9-7](#max_features), the
    models with the lowest error result from using a subset of features but with a
    larger number of trees.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: sklearn文档[展示了一个例子](https://oreil.ly/E0Och)，展示了不同`max_features`选择的效果，以及树的数量增加。在图中，蓝色曲线使用最少的特征，绿色曲线使用最多的特征（使用所有特征）。正如您在[图9-7](#max_features)中所看到的，使用较少特征但具有更多树的模型具有最低的错误结果。
- en: '![sklearn max_features chart](Images/dlcf_0907.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![sklearn max_features图表](Images/dlcf_0907.png)'
- en: 'Figure 9-7\. Error based on max features and number of trees (source: [*https://oreil.ly/E0Och*](https://oreil.ly/E0Och))'
  id: totrans-245
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-7\. 基于最大特征和树的数量的错误（来源：[*https://oreil.ly/E0Och*](https://oreil.ly/E0Och)）
- en: 'To see the impact of `n_estimators`, let’s get the predictions from each individual
    tree in our forest (these are in the `estimators_` attribute):'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 为了查看`n_estimators`的影响，让我们从森林中的每棵树获取预测结果（这些在`estimators_`属性中）：
- en: '[PRE54]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'As you can see, `preds.mean(0)` gives the same results as our random forest:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，`preds.mean(0)`给出了与我们的随机森林相同的结果：
- en: '[PRE55]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Let’s see what happens to the RMSE as we add more and more trees. As you can
    see, the improvement levels off quite a bit after around 30 trees:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看随着树的数量增加，RMSE会发生什么变化。如您所见，大约在30棵树后，改进水平就会显著减少：
- en: '[PRE57]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '![](Images/dlcf_09in04.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_09in04.png)'
- en: The performance on our validation set is worse than on our training set. But
    is that because we’re overfitting, or because the validation set covers a different
    time period, or a bit of both? With the existing information we’ve seen, we can’t
    tell. However, random forests have a very clever trick called *out-of-bag* (OOB)
    error that can help us with this (and more!).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在验证集上的表现比在训练集上差。但这是因为我们过拟合了，还是因为验证集涵盖了不同的时间段，或者两者都有？根据我们已经看到的信息，我们无法确定。然而，随机森林有一个非常聪明的技巧叫做*袋外*（OOB）误差，可以帮助我们解决这个问题（以及更多！）。
- en: Out-of-Bag Error
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 袋外误差
- en: Recall that in a random forest, each tree is trained on a different subset of
    the training data. The OOB error is a way of measuring prediction error in the
    training dataset by including in the calculation of a row’s error trees only where
    that row was *not* included in training. This allows us to see whether the model
    is overfitting, without needing a separate validation set.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，在随机森林中，每棵树都是在训练数据的不同子集上训练的。OOB错误是一种通过在计算行的错误时仅包括那些行*未*包含在训练中的树来测量训练数据集中的预测错误的方法。这使我们能够看到模型是否过拟合，而无需单独的验证集。
- en: Alexis Says
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Alexis说
- en: My intuition for this is that, since every tree was trained with a different
    randomly selected subset of rows, out-of-bag error is a little like imagining
    that every tree therefore also has its own validation set. That validation set
    is simply the rows that were not selected for that tree’s training.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我对此的直觉是，由于每棵树都是在不同的随机选择的行子集上训练的，因此袋外错误有点像想象每棵树因此也有自己的验证集。该验证集只是未被选中用于该树训练的行。
- en: 'This is particularly beneficial in cases where we have only a small amount
    of training data, as it allows us to see whether our model generalizes without
    removing items to create a validation set. The OOB predictions are available in
    the `oob_prediction_` attribute. Note that we compare them to the training labels,
    since this is being calculated on trees using the training set:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这在我们只有少量训练数据的情况下特别有益，因为它使我们能够看到我们的模型是否在不移除物品以创建验证集的情况下泛化。OOB预测可在`oob_prediction_`属性中找到。请注意，我们将它们与训练标签进行比较，因为这是在使用训练集的树上计算的：
- en: '[PRE58]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: We can see that our OOB error is much lower than our validation set error. This
    means that something else is causing that error, in *addition* to normal generalization
    error. We’ll discuss the reasons for this later in this chapter.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们的OOB错误远低于验证集错误。这意味着除了正常的泛化错误之外，还有其他原因导致了该错误。我们将在本章后面讨论这些原因。
- en: This is one way to interpret our model’s predictions—let’s focus on more of
    those now.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这是解释我们模型预测的一种方式——现在让我们更专注于这些。
- en: Model Interpretation
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型解释
- en: 'For tabular data, model interpretation is particularly important. For a given
    model, we are most likely to be interested in are the following:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 对于表格数据，模型解释尤为重要。对于给定的模型，我们最有兴趣的是以下内容：
- en: How confident are we in our predictions using a particular row of data?
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们对使用特定数据行进行的预测有多自信？
- en: For predicting with a particular row of data, what were the most important factors,
    and how did they influence that prediction?
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于使用特定数据行进行预测，最重要的因素是什么，它们如何影响该预测？
- en: Which columns are the strongest predictors, which can we ignore?
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪些列是最强的预测因子，哪些可以忽略？
- en: Which columns are effectively redundant with each other, for purposes of prediction?
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪些列在预测目的上实际上是多余的？
- en: How do predictions vary as we vary these columns?
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们改变这些列时，预测会如何变化？
- en: As we will see, random forests are particularly well suited to answering these
    questions. Let’s start with the first one!
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将看到的，随机森林特别适合回答这些问题。让我们从第一个问题开始！
- en: Tree Variance for Prediction Confidence
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于预测置信度的树方差
- en: We saw how the model averages the individual tree’s predictions to get an overall
    prediction—that is, an estimate of the value. But how can we know the confidence
    of the estimate? One simple way is to use the standard deviation of predictions
    across the trees, instead of just the mean. This tells us the *relative* confidence
    of predictions. In general, we would want to be more cautious of using the results
    for rows where trees give very different results (higher standard deviations),
    compared to cases where they are more consistent (lower standard deviations).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到模型如何平均每棵树的预测以获得整体预测——也就是说，一个值的估计。但是我们如何知道估计的置信度？一种简单的方法是使用树之间预测的标准差，而不仅仅是均值。这告诉我们预测的*相对*置信度。一般来说，我们会更谨慎地使用树给出非常不同结果的行的结果（更高的标准差），而不是在树更一致的情况下使用结果（更低的标准差）。
- en: 'In [“Creating a Random Forest”](#random_forest), we saw how to get predictions
    over the validation set, using a Python list comprehension to do this for each
    tree in the forest:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在["创建随机森林"](＃random_forest)中，我们看到如何使用Python列表推导来对验证集进行预测，对森林中的每棵树都这样做：
- en: '[PRE60]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Now we have a prediction for every tree and every auction in the validation
    set (40 trees and 7,988 auctions).
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对验证集中的每棵树和每个拍卖都有一个预测（40棵树和7,988个拍卖）。
- en: 'Using this, we can get the standard deviation of the predictions over all the
    trees, for each auction:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法，我们可以获得每个拍卖的所有树的预测的标准差：
- en: '[PRE63]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Here are the standard deviations for the predictions for the first five auctions—that
    is, the first five rows of the validation set:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前五个拍卖的预测的标准差——也就是验证集的前五行：
- en: '[PRE64]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: As you can see, the confidence in the predictions varies widely. For some auctions,
    there is a low standard deviation because the trees agree. For others, it’s higher,
    as the trees don’t agree. This is information that would be useful in a production
    setting; for instance, if you were using this model to decide which items to bid
    on at auction, a low-confidence prediction might cause you to look more carefully
    at an item before you made a bid.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，预测的置信度差异很大。对于一些拍卖，标准差较低，因为树是一致的。对于其他拍卖，标准差较高，因为树不一致。这是在生产环境中会有用的信息；例如，如果您使用此模型来决定在拍卖中对哪些物品进行竞标，低置信度的预测可能会导致您在竞标之前更仔细地查看物品。
- en: Feature Importance
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征重要性
- en: 'It’s not normally enough to just to know that a model can make accurate predictions—we
    also want to know *how* it’s making predictions. The *feature importances* give
    us this insight. We can get these directly from sklearn’s random forest by looking
    in the `feature_importances_` attribute. Here’s a simple function we can use to
    pop them into a DataFrame and sort them:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅知道一个模型能够做出准确的预测通常是不够的，我们还想知道*它是如何*做出预测的。*特征重要性*给了我们这种洞察力。我们可以直接从sklearn的随机森林中获取这些信息，方法是查看`feature_importances_`属性。这里有一个简单的函数，我们可以用它将它们放入一个DataFrame并对它们进行排序：
- en: '[PRE66]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The feature importances for our model show that the first few most important
    columns have much higher importance scores than the rest, with (not surprisingly)
    `YearMade` and `ProductSize` being at the top of the list:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的特征重要性显示，前几个最重要的列的重要性得分比其余的要高得多，其中（不出所料）`YearMade`和`ProductSize`位于列表的顶部：
- en: '[PRE67]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '|  | cols | imp |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '|  | cols | imp |'
- en: '| --- | --- | --- |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 69 | YearMade | 0.182890 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 69 | YearMade | 0.182890 |'
- en: '| 6 | ProductSize | 0.127268 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 6 | ProductSize | 0.127268 |'
- en: '| 30 | Coupler_System | 0.117698 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 30 | Coupler_System | 0.117698 |'
- en: '| 7 | fiProductClassDesc | 0.069939 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 7 | fiProductClassDesc | 0.069939 |'
- en: '| 66 | ModelID | 0.057263 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 66 | ModelID | 0.057263 |'
- en: '| 77 | saleElapsed | 0.050113 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 77 | saleElapsed | 0.050113 |'
- en: '| 32 | Hydraulics_Flow | 0.047091 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 32 | Hydraulics_Flow | 0.047091 |'
- en: '| 3 | fiSecondaryDesc | 0.041225 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 3 | fiSecondaryDesc | 0.041225 |'
- en: '| 31 | Grouser_Tracks | 0.031988 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 31 | Grouser_Tracks | 0.031988 |'
- en: '| 1 | fiModelDesc | 0.031838 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 1 | fiModelDesc | 0.031838 |'
- en: 'A plot of the feature importances shows the relative importances more clearly:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 特征重要性的图表显示了相对重要性更清晰：
- en: '[PRE68]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '![](Images/dlcf_09in05.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_09in05.png)'
- en: The way these importances are calculated is quite simple yet elegant. The feature
    importance algorithm loops through each tree, and then recursively explores each
    branch. At each branch, it looks to see what feature was used for that split,
    and how much the model improves as a result of that split. The improvement (weighted
    by the number of rows in that group) is added to the importance score for that
    feature. This is summed across all branches of all trees, and finally the scores
    are normalized such that they add to 1.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 这些重要性是如何计算的相当简单而优雅。特征重要性算法循环遍历每棵树，然后递归地探索每个分支。在每个分支，它查看用于该分割的特征是什么，以及模型由于该分割而改善了多少。该改善（按该组中的行数加权）被添加到该特征的重要性分数中。这些分数在所有树的所有分支中求和，最后对分数进行归一化，使它们总和为1。
- en: Removing Low-Importance Variables
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 去除低重要性变量
- en: 'It seems likely that we could use a subset of the columns by removing the variables
    of low importance and still get good results. Let’s try keeping just those with
    a feature importance greater than 0.005:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们可以通过去除低重要性的变量来使用列的子集，并且仍然能够获得良好的结果。让我们尝试只保留那些具有特征重要性大于0.005的列：
- en: '[PRE69]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'We can retrain our model using just this subset of the columns:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用列的这个子集重新训练我们的模型：
- en: '[PRE71]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'And here’s the result:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是结果：
- en: '[PRE73]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Our accuracy is about the same, but we have far fewer columns to study:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的准确率大致相同，但我们有更少的列需要研究：
- en: '[PRE75]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: We’ve found that generally the first step to improving a model is simplifying
    it—78 columns was too many for us to study them all in depth! Furthermore, in
    practice, often a simpler, more interpretable model is easier to roll out and
    maintain.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，通常改进模型的第一步是简化它——78列对我们来说太多了，我们无法深入研究它们！此外，在实践中，通常更简单、更易解释的模型更容易推出和维护。
- en: 'This also makes our feature importance plot easier to interpret. Let’s look
    at it again:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 这也使得我们的特征重要性图更容易解释。让我们再次看一下：
- en: '[PRE77]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '![](Images/dlcf_09in06.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_09in06.png)'
- en: 'One thing that makes this harder to interpret is that there seem to be some
    variables with very similar meanings: for example, `ProductGroup` and `ProductGroupDesc`.
    Let’s try to remove any redundant features.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 使这个更难解释的一点是，似乎有一些含义非常相似的变量：例如，`ProductGroup`和`ProductGroupDesc`。让我们尝试去除任何冗余特征。
- en: Removing Redundant Features
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 去除冗余特征
- en: 'Let’s start with this:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从这里开始：
- en: '[PRE78]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '![](Images/dlcf_09in07.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_09in07.png)'
- en: In this chart, the pairs of columns that are most similar are the ones that
    were merged together early, far from the “root” of the tree at the left. Unsurprisingly,
    the fields `ProductGroup` and `ProductGroupDesc` were merged quite early, as were
    `saleYear` and `saleElapsed`, and `fiModelDesc` and `fiBaseModel`. These might
    be so closely correlated they are practically synonyms for each other.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图表中，最相似的列对是在树的左侧远离“根”处早期合并在一起的。毫不奇怪，`ProductGroup`和`ProductGroupDesc`字段很早就合并了，`saleYear`和`saleElapsed`，以及`fiModelDesc`和`fiBaseModel`也是如此。它们可能是如此密切相关，以至于它们实际上是彼此的同义词。
- en: Determining Similarity
  id: totrans-329
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确定相似性
- en: The most similar pairs are found by calculating the *rank correlation*, which
    means that all the values are replaced with their *rank* (first, second, third,
    etc. within the column), and then the *correlation* is calculated. (Feel free
    to skip over this minor detail though, since it’s not going to come up again in
    the book!)
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 最相似的对是通过计算*秩相关性*来找到的，这意味着所有的值都被它们的*秩*（在列内的第一、第二、第三等）替换，然后计算*相关性*。（如果你愿意，可以跳过这个细节，因为它在本书中不会再次出现！）
- en: 'Let’s try removing some of these closely related features to see if the model
    can be simplified without impacting the accuracy. First, we create a function
    that quickly trains a random forest and returns the OOB score, by using a lower
    `max_samples` and higher `min_samples_leaf`. The OOB score is a number returned
    by sklearn that ranges between 1.0 for a perfect model and 0.0 for a random model.
    (In statistics it’s called *R*², although the details aren’t important for this
    explanation.) We don’t need it to be very accurate—we’re just going to use it
    to compare different models, based on removing some of the possibly redundant
    columns:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Here’s our baseline:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Now we try removing each of our potentially redundant variables, one at a time:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Now let’s try dropping multiple variables. We’ll drop one from each of the
    tightly aligned pairs we noticed earlier. Let’s see what that does:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Looking good! This is really not much worse than the model with all the fields.
    Let’s create DataFrames without these columns, and save them:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[PRE87]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'We can load them back later:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Now we can check our RMSE again, to confirm that the accuracy hasn’t substantially
    changed:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: By focusing on the most important variables and removing some redundant ones,
    we’ve greatly simplified our model. Now, let’s see how those variables affect
    our predictions using partial dependence plots.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: Partial Dependence
  id: totrans-351
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we’ve seen, the two most important predictors are `ProductSize` and `YearMade`.
    We’d like to understand the relationship between these predictors and sale price.
    It’s a good idea to first check the count of values per category (provided by
    the Pandas `value_counts` method), to see how common each category is:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '![](Images/dlcf_09in08.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
- en: The largest group is `#na#`, which is the label fastai applies to missing values.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s do the same thing for `YearMade`. Since this is a numeric feature, we’ll
    need to draw a histogram, which groups the year values into a few discrete bins:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '![](Images/dlcf_09in09.png)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
- en: Other than the special value 1950, which we used for coding missing year values,
    most of the data is from after 1990.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we’re ready to look at *partial dependence plots*. Partial dependence plots
    try to answer the question: if a row varied on nothing other than the feature
    in question, how would it impact the dependent variable?'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: For instance, how does `YearMade` impact sale price, all other things being
    equal? To answer this question, we can’t just take the average sale price for
    each `YearMade`. The problem with that approach is that many other things vary
    from year to year as well, such as which products are sold, how many products
    have air-conditioning, inflation, and so forth. So, merely averaging over all
    the auctions that have the same `YearMade` would also capture the effect of how
    every other field also changed along with `YearMade` and how that overall change
    affected price.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: Instead, what we do is replace every single value in the `YearMade` column with
    1950, and then calculate the predicted sale price for every auction, and take
    the average over all auctions. Then we do the same for 1951, 1952, and so forth
    until our final year of 2011\. This isolates the effect of only `YearMade` (even
    if it does so by averaging over some imagined records where we assign a `YearMade`
    value that might never actually exist alongside some other values).
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: Alexis Says
  id: totrans-363
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are philosophically minded, it is somewhat dizzying to contemplate the
    different kinds of hypotheticality that we are juggling to make this calculation.
    First, there’s the fact that *every* prediction is hypothetical, because we are
    not noting empirical data. Second, there’s the point that we’re *not* merely interested
    in asking how sale price would change if we changed `YearMade` and everything
    else along with it. Rather, we’re very specifically asking how sale price would
    change in a hypothetical world where only `YearMade` changed. Phew! It is impressive
    that we can ask such questions. I recommend Judea Pearl and Dana Mackenzie’s recent
    book on causality, *The Book of Why* (Basic Books), if you’re interested in more
    deeply exploring formalisms for analyzing these subtleties.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有哲学头脑，思考我们为了做出这个计算而搅乱的不同种类的假设性可能会让人眩晕。首先，每个预测都是假设的，因为我们没有记录实证数据。其次，我们不仅仅是想知道如果我们改变`YearMade`以及所有其他因素会如何改变销售价格。相反，我们非常明确地询问在一个假设的世界中，只有`YearMade`改变了销售价格会如何改变。哎呀！我们能够提出这样的问题令人印象深刻。如果你对更深入探讨分析这些微妙之处的形式主义感兴趣，我推荐Judea
    Pearl和Dana Mackenzie最近的关于因果关系的书籍*为什么之书*（Basic Books）。
- en: 'With these averages, we can then plot each year on the x-axis, and each prediction
    on the y-axis. This, finally, is a partial dependence plot. Let’s take a look:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些平均值，我们就可以在x轴上绘制每年，y轴上绘制每个预测。最终，这是一个偏依赖图。让我们来看一下：
- en: '[PRE93]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '![](Images/dlcf_09in10.png)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_09in10.png)'
- en: 'Looking first of all at the `YearMade` plot, and specifically at the section
    covering the years after 1990 (since, as we noted, this is where we have the most
    data), we can see a nearly linear relationship between year and price. Remember
    that our dependent variable is after taking the logarithm, so this means that
    in practice there is an exponential increase in price. This is what we would expect:
    depreciation is generally recognized as being a multiplicative factor over time,
    so for a given sale date, varying the year made ought to show an exponential relationship
    with sale price.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 首先看`YearMade`的图表，特别是涵盖1990年后的年份部分（因为，正如我们注意到的，这是我们拥有最多数据的地方），我们可以看到年份和价格之间几乎是线性关系。请记住我们的因变量是取对数后的，所以这意味着实际上价格呈指数增长。这是我们所期望的：折旧通常被认为是随时间的乘法因子，因此对于给定的销售日期，改变制造年份应该显示出与销售价格的指数关系。
- en: The `ProductSize` partial plot is a bit concerning. It shows that the final
    group, which we saw is for missing values, has the lowest price. To use this insight
    in practice, we would want to find out *why* it’s missing so often and what that
    *means*. Missing values can sometimes be useful predictors—it entirely depends
    on what causes them to be missing. Sometimes, however, they can indicate *data
    leakage*.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '`ProductSize`的部分图有点令人担忧。它显示我们看到的最终组，即缺失值，价格最低。要在实践中使用这一见解，我们需要找出*为什么*它经常缺失以及这意味着什么。缺失值有时可以是有用的预测因子-这完全取决于导致它们缺失的原因。然而，有时它们可能表明*数据泄漏*。'
- en: Data Leakage
  id: totrans-370
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据泄漏
- en: 'In the paper [“Leakage in Data Mining: Formulation, Detection, and Avoidance”](https://oreil.ly/XwvYf),
    Shachar Kaufman et al. describe leakage as follows:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 在论文[“数据挖掘中的泄漏：制定、检测和避免”](https://oreil.ly/XwvYf)中，Shachar Kaufman等人描述了泄漏如下：
- en: The introduction of information about the target of a data mining problem, which
    should not be legitimately available to mine from. A trivial example of leakage
    would be a model that uses the target itself as an input, thus concluding for
    example that “it rains on rainy days.” In practice, the introduction of this illegitimate
    information is unintentional, and facilitated by the data collection, aggregation,
    and preparation process.
  id: totrans-372
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 关于数据挖掘问题的目标的信息引入，这些信息不应该合法地从中挖掘出来。泄漏的一个微不足道的例子是一个模型将目标本身用作输入，因此得出例如“雨天下雨”的结论。实际上，引入这种非法信息是无意的，并且由数据收集、聚合和准备过程促成。
- en: 'They give as an example:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 他们举了一个例子：
- en: A real-life business intelligence project at IBM where potential customers for
    certain products were identified, among other things, based on keywords found
    on their websites. This turned out to be leakage since the website content used
    for training had been sampled at the point in time where the potential customer
    has already become a customer, and where the website contained traces of the IBM
    products purchased, such as the word “Websphere” (e.g., in a press release about
    the purchase or a specific product feature the client uses).
  id: totrans-374
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在IBM的一个实际商业智能项目中，根据其网站上发现的关键词，识别了某些产品的潜在客户。结果证明这是泄漏，因为用于训练的网站内容是在潜在客户已经成为客户的时间点进行采样的，网站包含了IBM购买的产品的痕迹，比如“Websphere”这样的词（例如，在关于购买的新闻稿或客户使用的特定产品功能中）。
- en: Data leakage is subtle and can take many forms. In particular, missing values
    often represent data leakage.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 数据泄漏是微妙的，可以采取多种形式。特别是，缺失值通常代表数据泄漏。
- en: For instance, Jeremy competed in a Kaggle competition designed to predict which
    researchers would end up receiving research grants. The information was provided
    by a university and included thousands of examples of research projects, along
    with information about the researchers involved and data on whether or not each
    grant was eventually accepted. The university hoped to be able to use the models
    developed in this competition to rank which grant applications were most likely
    to succeed, so it could prioritize its processing.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Jeremy参加了一个Kaggle竞赛，旨在预测哪些研究人员最终会获得研究资助。这些信息是由一所大学提供的，包括成千上万个研究项目的示例，以及有关涉及的研究人员和每个资助是否最终被接受的数据。大学希望能够使用在这次竞赛中开发的模型来排名哪些资助申请最有可能成功，以便优先处理。
- en: 'Jeremy used a random forest to model the data, and then used feature importance
    to find out which features were most predictive. He noticed three surprising things:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: Jeremy使用随机森林对数据进行建模，然后使用特征重要性来找出哪些特征最具预测性。他注意到了三件令人惊讶的事情：
- en: The model was able to correctly predict who would receive grants over 95% of
    the time.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apparently meaningless identifier columns were the most important predictors.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The day of week and day of year columns were also highly predictive; for instance,
    the vast majority of grant applications dated on a Sunday were accepted, and many
    accepted grant applications were dated on January 1.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the identifier columns, a partial dependence plot showed that when the information
    was missing, the application was almost always rejected. It turned out that in
    practice, the university filled out much of this information only *after* a grant
    application was accepted. Often, for applications that were not accepted, it was
    just left blank. Therefore, this information was not something that was available
    at the time that the application was received, and it would not be available for
    a predictive model—it was data leakage.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: In the same way, the final processing of successful applications was often done
    automatically as a batch at the end of the week, or the end of the year. It was
    this final processing date that ended up in the data, so again, this information,
    while predictive, was not actually available at the time that the application
    was received.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: 'This example showcases the most practical and simple approaches to identifying
    data leakage, which are to build a model and then do the following:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: Check whether the accuracy of the model is *too good to be true*.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Look for important predictors that don’t make sense in practice.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Look for partial dependence plot results that don’t make sense in practice.
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thinking back to our bear detector, this mirrors the advice that we provided
    in [Chapter 2](ch02.xhtml#chapter_production)—it is often a good idea to build
    a model first and then do your data cleaning, rather than vice versa. The model
    can help you identify potentially problematic data issues.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: It can also help you identify which factors influence specific predictions,
    with tree interpreters.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: Tree Interpreter
  id: totrans-389
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At the start of this section, we said that we wanted to be able to answer five
    questions:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: How confident are we in our predictions using a particular row of data?
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For predicting with a particular row of data, what were the most important factors,
    and how did they influence that prediction?
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which columns are the strongest predictors?
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which columns are effectively redundant with each other, for purposes of prediction?
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do predictions vary as we vary these columns?
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We’ve handled four of these already; only the second question remains. To answer
    this question, we need to use the *treeinterpreter* library. We’ll also use the
    *waterfallcharts* library to draw the chart of the results. You can install these
    by running these commands in a notebook cell:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: We have already seen how to compute feature importances across the entire random
    forest. The basic idea was to look at the contribution of each variable to improving
    the model, at each branch of every tree, and then add up all of these contributions
    per variable.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: We can do exactly the same thing, but for just a single row of data. For instance,
    let’s say we are looking at a particular item at auction. Our model might predict
    that this item will be very expensive, and we want to know why. So, we take that
    one row of data and put it through the first decision tree, looking to see what
    split is used at each point throughout the tree. For each split, we find the increase
    or decrease in the addition, compared to the parent node of the tree. We do this
    for every tree, and add up the total change in importance by split variable.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, let’s pick the first few rows of our validation set:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'We can then pass these to `treeinterpreter`:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '`prediction` is simply the prediction that the random forest makes. `bias`
    is the prediction based on taking the mean of the dependent variable (i.e., the
    *model* that is the root of every tree). `contributions` is the most interesting
    bit—it tells us the total change in prediction due to each of the independent
    variables. Therefore, the sum of `contributions` plus `bias` must equal the `prediction`,
    for each row. Let’s look at just the first row:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'The clearest way to display the contributions is with a *waterfall plot*. This
    shows how the positive and negative contributions from all the independent variables
    sum up to create the final prediction, which is the righthand column labeled “net”
    here:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '![](Images/dlcf_09in11.png)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
- en: This kind of information is most useful in production, rather than during model
    development. You can use it to provide useful information to users of your data
    product about the underlying reasoning behind the predictions.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: Now that we covered some classic machine learning techniques to solve this problem,
    let’s see how deep learning can help!
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: Extrapolation and Neural Networks
  id: totrans-412
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A problem with random forests, like all machine learning or deep learning algorithms,
    is that they don’t always generalize well to new data. We’ll see in which situations
    neural networks generalize better, but first, let’s look at the extrapolation
    problem that random forests have and how they can help identify out-of-domain
    data.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: The Extrapolation Problem
  id: totrans-414
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s consider the simple task of making predictions from 40 data points showing
    a slightly noisy linear relationship:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '![](Images/dlcf_09in12.png)'
  id: totrans-417
  prefs: []
  type: TYPE_IMG
- en: 'Although we have only a single independent variable, sklearn expects a matrix
    of independent variables, not a single vector. So we have to turn our vector into
    a matrix with one column. In other words, we have to change the *shape* from `[40]`
    to `[40,1]`. One way to do that is with the `unsqueeze` method, which adds a new
    unit axis to a tensor at the requested dimension:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '[PRE102]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'A more flexible approach is to slice an array or tensor with the special value
    `None`, which introduces an additional unit axis at that location:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '[PRE104]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'We can now create a random forest for this data. We’ll use only the first 30
    rows to train the model:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'Then we’ll test the model on the full dataset. The blue dots are the training
    data, and the red dots are the predictions:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '![](Images/dlcf_09in13.png)'
  id: totrans-428
  prefs: []
  type: TYPE_IMG
- en: We have a big problem! Our predictions outside the domain that our training
    data covered are all too low. Why do you suppose this is?
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: Remember, a random forest just averages the predictions of a number of trees.
    And a tree simply predicts the average value of the rows in a leaf. Therefore,
    a tree and a random forest can never predict values outside the range of the training
    data. This is particularly problematic for data indicating a trend over time,
    such as inflation, and you wish to make predictions for a future time. Your predictions
    will be systematically too low.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: But the problem extends beyond time variables. Random forests are not able to
    extrapolate outside the types of data they have seen, in a more general sense.
    That’s why we need to make sure our validation set does not contain out-of-domain
    data.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: Finding Out-of-Domain Data
  id: totrans-432
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes it is hard to know whether your test set is distributed in the same
    way as your training data, or, if it is different, which columns reflect that
    difference. There’s an easy way to figure this out, which is to use a random forest!
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: 'But in this case, we don’t use the random forest to predict our actual dependent
    variable. Instead, we try to predict whether a row is in the validation set or
    the training set. To see this in action, let’s combine our training and validation
    sets, create a dependent variable that represents which dataset each row comes
    from, build a random forest using that data, and get its feature importance:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  id: totrans-435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: '|  | cols | imp |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
- en: '| 5 | saleElapsed | 0.859446 |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
- en: '| 9 | SalesID | 0.119325 |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
- en: '| 13 | MachineID | 0.014259 |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
- en: '| 0 | YearMade | 0.001793 |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
- en: '| 8 | fiModelDesc | 0.001740 |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
- en: '| 11 | Enclosure | 0.000657 |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
- en: 'This shows that three columns differ significantly between the training and
    validation sets: `saleElapsed`, `SalesID`, and `MachineID`. It’s fairly obvious
    why this is the case for `saleElapsed`: it’s the number of days between the start
    of the dataset and each row, so it directly encodes the date. The difference in
    `SalesID` suggests that identifiers for auction sales might increment over time.
    `MachineID` suggests something similar might be happening for individual items
    sold in those auctions.'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s get a baseline of the original random forest model’s RMSE, and then determine
    the effect of removing each of these columns in turn:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: '[PRE109]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'It looks like we should be able to remove `SalesID` and `MachineID` without
    losing any accuracy. Let’s check:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: '[PRE111]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: Removing these variables has slightly improved the model’s accuracy; but more
    importantly, it should make it more resilient over time, and easier to maintain
    and understand. We recommend that for all datasets, you try building a model in
    which your dependent variable is `is_valid`, as we did here. It can often uncover
    subtle *domain shift* issues that you may otherwise miss.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: 'One thing that might help in our case is to simply avoid using old data. Often,
    old data shows relationships that just aren’t valid anymore. Let’s try just using
    the most recent few years of the data:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '![](Images/dlcf_09in14.png)'
  id: totrans-454
  prefs: []
  type: TYPE_IMG
- en: 'Here’s the result of training on this subset:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  id: totrans-456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: '[PRE114]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: '[PRE115]'
  id: totrans-458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: It’s a tiny bit better, which shows that you shouldn’t always use your entire
    dataset; sometimes a subset can be better.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see if using a neural network helps.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: Using a Neural Network
  id: totrans-461
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can use the same approach to build a neural network model. Let’s first replicate
    the steps we took to set up the `TabularPandas` object:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: 'We can leverage the work we did to trim unwanted columns in the random forest
    by using the same set of columns for our neural network:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 'Categorical columns are handled very differently in neural networks, compared
    to decision tree approaches. As we saw in [Chapter 8](ch08.xhtml#chapter_collab),
    in a neutral net, a great way to handle categorical variables is by using embeddings.
    To create embeddings, fastai needs to determine which columns should be treated
    as categorical variables. It does this by comparing the number of distinct levels
    in the variable to the value of the `max_card` parameter. If it’s lower, fastai
    will treat the variable as categorical. Embedding sizes larger than 10,000 should
    generally be used only after you’ve tested whether there are better ways to group
    the variable, so we’ll use 9,000 as our `max_card` value:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: 'In this case, however, there’s one variable that we absolutely do not want
    to treat as categorical: `saleElapsed`. A categorical variable cannot, by definition,
    extrapolate outside the range of values that it has seen, but we want to be able
    to predict auction sale prices in the future. Therefore, we need to make this
    a continuous variable:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: 'Let’s take a look at the cardinality of each of the categorical variables that
    we have chosen so far:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: '[PRE121]'
  id: totrans-472
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: 'The fact that there are two variables pertaining to the “model” of the equipment,
    both with similar very high cardinalities, suggests that they may contain similar,
    redundant information. Note that we would not necessarily catch this when analyzing
    redundant features, since that relies on similar variables being sorted in the
    same order (that is, they need to have similarly named levels). Having a column
    with 5,000 levels means needing 5,000 columns in our embedding matrix, which would
    be nice to avoid if possible. Let’s see what the impact of removing one of these
    model columns has on the random forest:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  id: totrans-474
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: '[PRE123]'
  id: totrans-475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: 'There’s minimal impact, so we will remove it as a predictor for our neural
    network:'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  id: totrans-477
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: 'We can create our `TabularPandas` object in the same way as when we created
    our random forest, with one very important addition: normalization. A random forest
    does not need any normalization—the tree building procedure cares only about the
    order of values in a variable, not at all about how they are scaled. But as we
    have seen, a neural network definitely does care about this. Therefore, we add
    the `Normalize` processor when we build our `TabularPandas` object:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE125]'
  id: totrans-479
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: 'Tabular models and data don’t generally require much GPU RAM, so we can use
    larger batch sizes:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  id: totrans-481
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: 'As we’ve discussed, it’s a good idea to set `y_range` for regression models,
    so let’s find the min and max of our dependent variable:'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE127]'
  id: totrans-483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: '[PRE128]'
  id: totrans-484
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: We can now create the `Learner` to create this tabular model. As usual, we use
    the application-specific learner function, to take advantage of its application-customized
    defaults. We set the loss function to MSE, since that’s what this competition
    uses.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, for tabular data fastai creates a neural network with two hidden
    layers, with 200 and 100 activations, respectively. This works quite well for
    small datasets, but here we’ve got quite a large dataset, so we increase the layer
    sizes to 500 and 250:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE129]'
  id: totrans-487
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: '[PRE130]'
  id: totrans-488
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: '[PRE131]'
  id: totrans-489
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: '[PRE132]'
  id: totrans-490
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: '![](Images/dlcf_09in15.png)'
  id: totrans-491
  prefs: []
  type: TYPE_IMG
- en: 'There’s no need to use `fine_tune`, so we’ll train with `fit_one_cycle` for
    a few epochs and see how it looks:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE133]'
  id: totrans-493
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: '| epoch | train_loss | valid_loss | time |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0.069705 | 0.062389 | 00:11 |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.056253 | 0.058489 | 00:11 |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.048385 | 0.052256 | 00:11 |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.043400 | 0.050743 | 00:11 |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0.040358 | 0.050986 | 00:11 |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
- en: 'We can use our `r_mse` function to compare the result to the random forest
    result we got earlier:'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE134]'
  id: totrans-502
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: '[PRE135]'
  id: totrans-503
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: It’s quite a bit better than the random forest (although it took longer to train,
    and it’s fussier about hyperparameter tuning).
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we move on, let’s save our model in case we want to come back to it
    again later:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE136]'
  id: totrans-506
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: Another thing that can help with generalization is to use several models and
    average their predictions—a technique, as mentioned earlier, known as *ensembling*.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: Ensembling
  id: totrans-508
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Think back to the original reasoning behind why random forests work so well:
    each tree has errors, but those errors are not correlated with each other, so
    the average of those errors should tend toward zero once there are enough trees.
    Similar reasoning could be used to consider averaging the predictions of models
    trained using different algorithms.'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, we have two very different models, trained using very different
    algorithms: a random forest and a neural network. It would be reasonable to expect
    that the kinds of errors that each one makes would be quite different. Therefore,
    we might expect that the average of their predictions would be better than either
    one’s individual predictions.'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
- en: As we saw earlier, a random forest is itself an ensemble. But we can then include
    a random forest in *another* ensemble—an ensemble of the random forest and the
    neural network! While ensembling won’t make the difference between a successful
    and an unsuccessful modeling process, it can certainly add a nice little boost
    to any models that you have built.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
- en: 'One minor issue we have to be aware of is that our PyTorch model and our sklearn
    model create data of different types: PyTorch gives us a rank-2 tensor (a column
    matrix), whereas NumPy gives us a rank-1 array (a vector). `squeeze` removes any
    unit axes from a tensor, and `to_np` converts it into a NumPy array:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE137]'
  id: totrans-513
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: 'This gives us a better result than either model achieved on its own:'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE138]'
  id: totrans-515
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: '[PRE139]'
  id: totrans-516
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: In fact, this result is better than any score shown on the Kaggle leaderboard.
    It’s not directly comparable, however, because the Kaggle leaderboard uses a separate
    dataset that we do not have access to. Kaggle does not allow us to submit to this
    old competition to find out how we would have done, but our results certainly
    look encouraging!
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
- en: Boosting
  id: totrans-518
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, our approach to ensembling has been to use *bagging*, which involves
    combining many models (each trained on a different data subset) by averaging them.
    As we saw, when this is applied to decision trees, this is called a *random forest*.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
- en: 'In another important approach to ensembling, called *boosting*, where we add
    models instead of averaging them. Here is how boosting works:'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
- en: Train a small model that underfits your dataset.
  id: totrans-521
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the predictions in the training set for this model.
  id: totrans-522
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Subtract the predictions from the targets; these are called the *residuals*
    and represent the error for each point in the training set.
  id: totrans-523
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go back to step 1, but instead of using the original targets, use the residuals
    as the targets for the training.
  id: totrans-524
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Continue doing this until you reach a stopping criterion, such as a maximum
    number of trees, or you observe your validation set error getting worse.
  id: totrans-525
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using this approach, each new tree will be attempting to fit the error of all
    of the previous trees combined. Because we are continually creating new residuals
    by subtracting the predictions of each new tree from the residuals from the previous
    tree, the residuals will get smaller and smaller.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
- en: To make predictions with an ensemble of boosted trees, we calculate the predictions
    from each tree and then add them all together. There are many models following
    this basic approach, and many names for the same models. *Gradient boosting machines*
    (GBMs) and *gradient boosted decision trees* (GBDTs) are the terms you’re most
    likely to come across, or you may see the names of specific libraries implementing
    these; at the time of writing, *XGBoost* is the most popular.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: Note that, unlike with random forests, with this approach, there is nothing
    to stop us from overfitting. Using more trees in a random forest does not lead
    to overfitting, because each tree is independent of the others. But in a boosted
    ensemble, the more trees you have, the better the training error becomes, and
    eventually you will see overfitting on the validation set.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
- en: We are not going to go into detail on how to train a gradient boosted tree ensemble
    here, because the field is moving rapidly, and any guidance we give will almost
    certainly be outdated by the time you read this. As we write this, sklearn has
    just added a `HistGradientBoostingRegressor` class that provides excellent performance.
    There are many hyperparameters to tweak for this class, and for all gradient boosted
    tree methods we have seen. Unlike random forests, gradient boosted trees are extremely
    sensitive to the choices of these hyperparameters; in practice, most people use
    a loop that tries a range of hyperparameters to find the ones that work best.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: One more technique that has gotten great results is to use embeddings learned
    by a neural net in a machine learning model.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: Combining Embeddings with Other Methods
  id: totrans-531
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The abstract of the entity embedding paper we mentioned at the start of this
    chapter states: “The embeddings obtained from the trained neural network boost
    the performance of all tested machine learning methods considerably when used
    as the input features instead.” It includes the very interesting table shown in
    [Figure 9-8](#guo_berkhahn_table).'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: '![Embeddings combined with other methods](Images/dlcf_0908.png)'
  id: totrans-533
  prefs: []
  type: TYPE_IMG
- en: Figure 9-8\. Effects of using neural network embeddings as input to other machine
    learning methods (courtesy of Cheng Guo and Felix Berkhahn)
  id: totrans-534
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is showing the mean average percent error (MAPE) compared among four modeling
    techniques, three of which we have already seen, along with *k*-nearest neighbors
    (KNN), which is a very simple baseline method. The first numeric column contains
    the results of using the methods on the data provided in the competition; the
    second column shows what happens if you first train a neural network with categorical
    embeddings, and then use those categorical embeddings instead of the raw categorical
    columns in the model. As you see, in every case, the models are dramatically improved
    by using the embeddings instead of the raw categories.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
- en: This is a really important result, because it shows that you can get much of
    the performance improvement of a neural network without having to use a neural
    network at inference time. You could just use an embedding, which is literally
    just an array lookup, along with a small decision tree ensemble.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: These embeddings need not even be necessarily learned separately for each model
    or task in an organization. Instead, once a set of embeddings are learned for
    a column for a particular task, they could be stored in a central place and reused
    across multiple models. In fact, we know from private communication with other
    practitioners at large companies that this is already happening in many places.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-538
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have discussed two approaches to tabular modeling: decision tree ensembles
    and neural networks. We’ve also mentioned two decision tree ensembles: random
    forests and gradient boosting machines. Each is effective but also requires compromises:'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
- en: '*Random forests* are the easiest to train, because they are extremely resilient
    to hyperparameter choices and require little preprocessing. They are fast to train,
    and should not overfit if you have enough trees. But they can be a little less
    accurate, especially if extrapolation is required, such as predicting future time
    periods.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Gradient boosting machines* in theory are just as fast to train as random
    forests, but in practice you will have to try lots of hyperparameters. They can
    overfit, but they are often a little more accurate than random forests.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Neural networks* take the longest time to train and require extra preprocessing,
    such as normalization; this normalization needs to be used at inference time as
    well. They can provide great results and extrapolate well, but only if you are
    careful with your hyperparameters and take care to avoid overfitting.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We suggest starting your analysis with a random forest. This will give you a
    strong baseline, and you can be confident that it’s a reasonable starting point.
    You can then use that model for feature selection and partial dependence analysis,
    to get a better understanding of your data.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
- en: From that foundation, you can try neural nets and GBMs, and if they give you
    significantly better results on your validation set in a reasonable amount of
    time, you can use them. If decision tree ensembles are working well for you, try
    adding the embeddings for the categorical variables to the data, and see if that
    helps your decision trees learn better.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
- en: Questionnaire
  id: totrans-545
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is a continuous variable?
  id: totrans-546
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a categorical variable?
  id: totrans-547
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Provide two of the words that are used for the possible values of a categorical
    variable.
  id: totrans-548
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a dense layer?
  id: totrans-549
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do entity embeddings reduce memory usage and speed up neural networks?
  id: totrans-550
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What kinds of datasets are entity embeddings especially useful for?
  id: totrans-551
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the two main families of machine learning algorithms?
  id: totrans-552
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do some categorical columns need a special ordering in their classes? How
    do you do this in Pandas?
  id: totrans-553
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summarize what a decision tree algorithm does.
  id: totrans-554
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is a date different from a regular categorical or continuous variable, and
    how can you preprocess it to allow it to be used in a model?
  id: totrans-555
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Should you pick a random validation set in the bulldozer competition? If no,
    what kind of validation set should you pick?
  id: totrans-556
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is pickle and what is it useful for?
  id: totrans-557
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How are `mse`, `samples`, and `values` calculated in the decision tree drawn
    in this chapter?
  id: totrans-558
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we deal with outliers before building a decision tree?
  id: totrans-559
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we handle categorical variables in a decision tree?
  id: totrans-560
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is bagging?
  id: totrans-561
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between `max_samples` and `max_features` when creating
    a random forest?
  id: totrans-562
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you increase `n_estimators` to a very high value, can that lead to overfitting?
    Why or why not?
  id: totrans-563
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the section “Creating a Random Forest”, after [Figure 9-7](#max_features),
    why did `preds.mean(0)` give the same result as our random forest?
  id: totrans-564
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is out-of-bag error?
  id: totrans-565
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: List the reasons that a model’s validation set error might be worse than the
    OOB error. How could you test your hypotheses?
  id: totrans-566
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Explain why random forests are well suited to answering each of the following
    questions:'
  id: totrans-567
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How confident are we in our predictions using a particular row of data?
  id: totrans-568
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For predicting with a particular row of data, what were the most important factors,
    and how did they influence that prediction?
  id: totrans-569
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Which columns are the strongest predictors?
  id: totrans-570
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How do predictions vary as we vary these columns?
  id: totrans-571
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What’s the purpose of removing unimportant variables?
  id: totrans-572
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What’s a good type of plot for showing tree interpreter results?
  id: totrans-573
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the extrapolation problem?
  id: totrans-574
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can you tell if your test or validation set is distributed in a different
    way than your training set?
  id: totrans-575
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we make `saleElapsed` a continuous variable, even though it has fewer
    than 9,000 distinct values?
  id: totrans-576
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is boosting?
  id: totrans-577
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How could we use embeddings with a random forest? Would we expect this to help?
  id: totrans-578
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why might we not always use a neural net for tabular modeling?
  id: totrans-579
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further Research
  id: totrans-580
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pick a competition on Kaggle with tabular data (current or past) and try to
    adapt the techniques seen in this chapter to get the best possible results. Compare
    your results to the private leaderboard.
  id: totrans-581
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement the decision tree algorithm in this chapter from scratch yourself,
    and try it on the dataset you used in the first exercise.
  id: totrans-582
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the embeddings from the neural net in this chapter in a random forest, and
    see if you can improve on the random forest results we saw.
  id: totrans-583
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain what each line of the source of `TabularModel` does (with the exception
    of the `BatchNorm1d` and `Dropout` layers).
  id: totrans-584
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
