- en: Chapter 6\. Summarization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章。摘要
- en: At one point or another, you’ve probably needed to summarize a document, be
    it a research article, a financial earnings report, or a thread of emails. If
    you think about it, this requires a range of abilities, such as understanding
    long passages, reasoning about the contents, and producing fluent text that incorporates
    the main topics from the original document. Moreover, accurately summarizing a
    news article is very different from summarizing a legal contract, so being able
    to do so requires a sophisticated degree of domain generalization. For these reasons,
    text summarization is a difficult task for neural language models, including transformers.
    Despite these challenges, text summarization offers the prospect for domain experts
    to significantly speed up their workflows and is used by enterprises to condense
    internal knowledge, summarize contracts, automatically generate content for social
    media releases, and more.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能在某个时候需要总结一篇文档，无论是研究文章、财务收益报告还是一系列电子邮件。如果你仔细想想，这需要一系列能力，比如理解长篇文章、推理内容，并产生流畅的文本，其中包含原始文档的主要主题。此外，准确总结新闻文章与总结法律合同大不相同，因此能够做到这一点需要一定程度的领域泛化。出于这些原因，文本摘要对于神经语言模型，包括变压器来说，是一项困难的任务。尽管存在这些挑战，文本摘要为领域专家提供了显著加快工作流程的可能性，并被企业用于压缩内部知识、总结合同、自动生成社交媒体发布的内容等。
- en: To help you understand the challenges involved, this chapter will explore how
    we can leverage pretrained transformers to summarize documents. Summarization
    is a classic sequence-to-sequence (seq2seq) task with an input text and a target
    text. As we saw in [Chapter 1](ch01.xhtml#chapter_introduction), this is where
    encoder-decoder transformers excel.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助您了解涉及的挑战，本章将探讨如何利用预训练的变压器来总结文档。摘要是一个经典的序列到序列（seq2seq）任务，有一个输入文本和一个目标文本。正如我们在[第1章](ch01.xhtml#chapter_introduction)中看到的，这正是编码器-解码器变压器擅长的地方。
- en: 'In this chapter we will build our own encoder-decoder model to condense dialogues
    between several people into a crisp summary. But before we get to that, let’s
    begin by taking a look at one of the canonical datasets for summarization: the
    CNN/DailyMail corpus.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将构建自己的编码器-解码器模型，将几个人之间的对话压缩成简洁的摘要。但在开始之前，让我们先来看看摘要的经典数据集之一：CNN/DailyMail语料库。
- en: The CNN/DailyMail Dataset
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN/DailyMail数据集
- en: 'The CNN/DailyMail dataset consists of around 300,000 pairs of news articles
    and their corresponding summaries, composed from the bullet points that CNN and
    the DailyMail attach to their articles. An important aspect of the dataset is
    that the summaries are *abstractive* and not *extractive*, which means that they
    consist of new sentences instead of simple excerpts. The dataset is available
    on the [Hub](https://oreil.ly/jcRmb); we’ll use version 3.0.0, which is a nonanonymized
    version set up for summarization. We can select versions in a similar manner as
    splits, we saw in [Chapter 4](ch04.xhtml#chapter_ner), with a `version` keyword.
    So let’s dive in and have a look at it:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: CNN/DailyMail数据集包括约30万对新闻文章及其相应摘要，由CNN和DailyMail附加到其文章的要点组成。数据集的一个重要方面是摘要是*抽象*的，而不是*提取*的，这意味着它们由新句子组成，而不是简单的摘录。数据集可在[Hub](https://oreil.ly/jcRmb)上找到；我们将使用3.0.0版本，这是为摘要设置的非匿名版本。我们可以像在[第4章](ch04.xhtml#chapter_ner)中看到的那样，使用`version`关键字选择版本。所以让我们深入研究一下：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The dataset has three columns: `article`, which contains the news articles,
    `highlights` with the summaries, and `id` to uniquely identify each article. Let’s
    look at an excerpt from an article:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集有三列：`文章`，其中包含新闻文章，`摘要`，其中包含摘要，以及`id`，用于唯一标识每篇文章。让我们来看一段文章摘录：
- en: '[PRE2]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We see that the articles can be very long compared to the target summary; in
    this particular case the difference is 17-fold. Long articles pose a challenge
    to most transformer models since the context size is usually limited to 1,000
    tokens or so, which is equivalent to a few paragraphs of text. The standard, yet
    crude way to deal with this for summarization is to simply truncate the texts
    beyond the model’s context size. Obviously there could be important information
    for the summary toward the end of the text, but for now we need to live with this
    limitation of the model architectures.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，与目标摘要相比，文章可能非常长；在这种特殊情况下，差异是17倍。长篇文章对大多数变压器模型构成挑战，因为上下文大小通常限制在1,000个标记左右，相当于几段文字。对于摘要，处理这个问题的标准但粗糙的方法是简单地截断超出模型上下文大小的文本。显然，文本末尾可能有重要信息供摘要使用，但目前我们需要接受模型架构的这种限制。
- en: Text Summarization Pipelines
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本摘要管道
- en: 'Let’s see how a few of the most popular transformer models for summarization
    perform by first looking qualitatively at the outputs for the preceding example.
    Although the model architectures we will be exploring have varying maximum input
    sizes, let’s restrict the input text to 2,000 characters to have the same input
    for all models and thus make the outputs more comparable:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先定性地查看前面示例的输出，看看一些最受欢迎的摘要变压器模型的表现如何。尽管我们将要探索的模型架构具有不同的最大输入大小，但让我们将输入文本限制为2,000个字符，以便所有模型都具有相同的输入，从而使输出更具可比性：
- en: '[PRE3]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'A convention in summarization is to separate the summary sentences by a newline.
    We could add a newline token after each full stop, but this simple heuristic would
    fail for strings like “U.S.” or “U.N.” The Natural Language Toolkit (NLTK) package
    includes a more sophisticated algorithm that can differentiate the end of a sentence
    from punctuation that occurs in abbreviations:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要中的一个惯例是用换行符分隔摘要句子。我们可以在每个句号后添加一个换行符，但这种简单的启发式方法对于像“U.S.”或“U.N.”这样的字符串将失败。自然语言工具包（NLTK）包括一个更复杂的算法，可以区分句子的结束和缩写中出现的标点符号：
- en: '[PRE4]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Warning
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: In the following sections we will load several large models. If you run out
    of memory, you can either replace the large models with smaller checkpoints (e.g.,
    “gpt”, “t5-small”) or skip this section and jump to [“Evaluating PEGASUS on the
    CNN/DailyMail Dataset”](#eval-pegasus).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将加载几个大型模型。如果内存不足，可以用较小的检查点（例如“gpt”，“t5-small”）替换大型模型，或者跳过本节，转到[“在CNN/DailyMail数据集上评估PEGASUS”](#eval-pegasus)。
- en: Summarization Baseline
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要基线
- en: 'A common baseline for summarizing news articles is to simply take the first
    three sentences of the article. With NLTK’s sentence tokenizer, we can easily
    implement such a baseline:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要新闻文章的一个常见基线是简单地取文章的前三句。使用NLTK的句子分词器，我们可以很容易地实现这样一个基线：
- en: '[PRE7]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: GPT-2
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-2
- en: 'We’ve already seen in [Chapter 5](ch05.xhtml#chapter_generation) how GPT-2
    can generate text given some prompt. One of the model’s surprising features is
    that we can also use it to generate summaries by simply appending “TL;DR” at the
    end of the input text. The expression “TL;DR” (too long; didn’t read) is often
    used on platforms like Reddit to indicate a short version of a long post. We will
    start our summarization experiment by re-creating the procedure of the original
    paper with the `pipeline()` function from ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers.^([1](ch06.xhtml#idm46238717347776)) We create a text generation
    pipeline and load the large GPT-2 model:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[第5章](ch05.xhtml#chapter_generation)中看到了GPT-2如何根据一些提示生成文本。该模型令人惊讶的一个特点是，我们还可以使用它来生成摘要，只需在输入文本末尾添加“TL;DR”。表达“TL;DR”（太长了；没看）经常在Reddit等平台上使用，表示长篇帖子的简短版本。我们将通过使用来自Transformers的`pipeline()`函数重新创建原始论文的程序开始我们的摘要实验。我们创建一个文本生成管道并加载大型GPT-2模型：
- en: '[PRE9]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here we just store the summaries of the generated text by slicing off the input
    query and keep the result in a Python dictionary for later comparison.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只需存储通过切割输入查询生成的摘要，并将结果保存在Python字典中以供以后比较。
- en: T5
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: T5
- en: 'Next let’s try the T5 transformer. As we saw in [Chapter 3](ch03.xhtml#chapter_anatomy),
    the developers of this model performed a comprehensive study of transfer learning
    in NLP and found they could create a universal transformer architecture by formulating
    all tasks as text-to-text tasks. The T5 checkpoints are trained on a mixture of
    unsupervised data (to reconstruct masked words) and supervised data for several
    tasks, including summarization. These checkpoints can thus be directly used to
    perform summarization without fine-tuning by using the same prompts used during
    pretraining. In this framework, the input format for the model to summarize a
    document is `"summarize: <ARTICLE>"`, and for translation it looks like `"translate
    English to German: <TEXT>"`. As shown in [Figure 6-1](#T5), this makes T5 extremely
    versatile and allows you to solve many tasks with a single model.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来让我们尝试T5变压器。正如我们在[第3章](ch03.xhtml#chapter_anatomy)中看到的，该模型的开发人员对NLP中的迁移学习进行了全面研究，并发现他们可以通过将所有任务制定为文本-文本任务来创建通用变压器架构。T5检查点经过混合无监督数据（用于重建掩码单词）和监督数据的训练，包括摘要等多个任务。因此，这些检查点可以直接用于执行摘要，而无需使用预训练期间使用的相同提示进行微调。在该框架中，模型摘要文档的输入格式为`"summarize:
    <ARTICLE>"`，翻译的格式看起来像`"translate English to German: <TEXT>"`。如[图6-1](#T5)所示，这使得T5非常灵活，可以使用单个模型解决许多任务。'
- en: 'We can directly load T5 for summarization with the `pipeline()` function, which
    also takes care of formatting the inputs in the text-to-text format so we don’t
    need to prepend them with `"summarize"`:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接使用`pipeline()`函数加载T5进行摘要，该函数还负责以文本-文本格式格式化输入，因此我们不需要在其前面加上`"summarize"`：
- en: '[PRE10]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![T5](Images/nlpt_0601.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![T5](Images/nlpt_0601.png)'
- en: Figure 6-1\. Diagram of T5’s text-to-text framework (courtesy of Colin Raffel);
    besides translation and summarization, the CoLA (linguistic acceptability) and
    STSB (semantic similarity) tasks are shown
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-1。T5的文本-文本框架图（由Colin Raffel提供）；除了翻译和摘要之外，还显示了CoLA（语言可接受性）和STSB（语义相似性）任务
- en: BART
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 巴特
- en: 'BART also uses an encoder-decoder architecture and is trained to reconstruct
    corrupted inputs. It combines the pretraining schemes of BERT and GPT-2.^([2](ch06.xhtml#idm46238717146864))
    We’ll use the `facebook/bart-large-ccn` checkpoint, which has been specifically
    fine-tuned on the CNN/DailyMail dataset:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: BART还使用了编码器-解码器架构，并且经过训练以重建损坏的输入。它结合了BERT和GPT-2的预训练方案。我们将使用`facebook/bart-large-ccn`检查点，该检查点已经在CNN/DailyMail数据集上进行了特定的微调：
- en: '[PRE11]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: PEGASUS
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 飞马座
- en: Like BART, PEGASUS is an encoder-decoder transformer.^([3](ch06.xhtml#idm46238717078848))
    As shown in [Figure 6-2](#pegasus), its pretraining objective is to predict masked
    sentences in multisentence texts. The authors argue that the closer the pretraining
    objective is to the downstream task, the more effective it is. With the aim of
    finding a pretraining objective that is closer to summarization than general language
    modeling, they automatically identified, in a very large corpus, sentences containing
    most of the content of their surrounding paragraphs (using summarization evaluation
    metrics as a heuristic for content overlap) and pretrained the PEGASUS model to
    reconstruct these sentences, thereby obtaining a state-of-the-art model for text
    summarization.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 像BART一样，PEGASUS是一种编码器-解码器变压器。如[图6-2](#pegasus)所示，它的预训练目标是预测多句文本中的掩码句子。作者认为，预训练目标与下游任务越接近，效果就越好。为了找到一个比通用语言建模更接近摘要的预训练目标，他们在一个非常大的语料库中自动识别了包含其周围段落大部分内容的句子（使用摘要评估指标作为内容重叠的启发式），并预训练PEGASUS模型以重建这些句子，从而获得了用于文本摘要的最先进模型。
- en: '![pegasus](Images/nlpt_0602.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![pegasus](Images/nlpt_0602.png)'
- en: Figure 6-2\. Diagram of PEGASUS architecture (courtesy of Jingqing Zhang et
    al.)
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-2。PEGASUS架构图（由Jingqing Zhang等人提供）
- en: 'This model has a special token for newlines, which is why we don’t need the
    `sent_tokenize()` function:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型具有用于换行的特殊标记，这就是为什么我们不需要`sent_tokenize()`函数的原因：
- en: '[PRE12]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Comparing Different Summaries
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较不同的摘要
- en: 'Now that we have generated summaries with four different models, let’s compare
    the results. Keep in mind that one model has not been trained on the dataset at
    all (GPT-2), one model has been fine-tuned on this task among others (T5), and
    two models have exclusively been fine-tuned on this task (BART and PEGASUS). Let’s
    have a look at the summaries these models have generated:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经用四种不同的模型生成了摘要，让我们比较一下结果。请记住，其中一个模型根本没有在数据集上进行训练（GPT-2），一个模型在这项任务以及其他任务上进行了微调（T5），而另外两个模型则专门在这项任务上进行了微调（BART和PEGASUS）。让我们看看这些模型生成的摘要：
- en: '[PRE13]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The first thing we notice by looking at the model outputs is that the summary
    generated by GPT-2 is quite different from the others. Instead of giving a summary
    of the text, it summarizes the characters. Often the GPT-2 model “hallucinates”
    or invents facts, since it was not explicitly trained to generate truthful summaries.
    For example, at the time of writing, Nesta is not the fastest man in the world,
    but sits in ninth place. Comparing the other three model summaries against the
    ground truth, we see that there is remarkable overlap, with PEGASUS’s output bearing
    the most striking resemblance.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看模型输出，我们首先注意到GPT-2生成的摘要与其他模型有很大不同。它不是给出文本的摘要，而是总结了字符。由于它并没有明确训练生成真实摘要，因此GPT-2模型经常“产生幻觉”或虚构事实。例如，在撰写时，内斯塔并不是世界上最快的人，而是排名第九。将其他三个模型的摘要与真实情况进行比较，我们发现PEGASUS的输出与之最为相似。
- en: Now that we have inspected a few models, let’s try to decide which one we would
    use in a production setting. All four models seem to provide qualitatively reasonable
    results, and we could generate a few more examples to help us decide. However,
    this is not a systematic way of determining the best model! Ideally, we would
    define a metric, measure it for all models on some benchmark dataset, and choose
    the one with the best performance. But how do you define a metric for text generation?
    The standard metrics that we’ve seen, like accuracy, recall, and precision, are
    not easy to apply to this task. For each “gold standard” summary written by a
    human, dozens of other summaries with synonyms, paraphrases, or a slightly different
    way of formulating the facts could be just as acceptable.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经检查了一些模型，让我们试着决定在生产环境中使用哪个模型。所有四个模型似乎都提供了合理的结果，我们可以生成更多的例子来帮助我们决定。然而，这并不是一个系统确定最佳模型的方法！理想情况下，我们会定义一个指标，在一些基准数据集上对所有模型进行测量，并选择性能最佳的模型。但是如何定义文本生成的指标呢？我们看到的标准指标，如准确率、召回率和精度，不容易应用于这个任务。对于人类撰写的“黄金标准”摘要，可能有数十种其他具有同义词、释义或稍微不同表达事实方式的摘要同样可以接受。
- en: In the next section we will look at some common metrics that have been developed
    for measuring the quality of generated text.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将看一些用于衡量生成文本质量的常见指标。
- en: Measuring the Quality of Generated Text
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 衡量生成文本的质量
- en: Good evaluation metrics are important, since we use them to measure the performance
    of models not only when we train them but also later, in production. If we have
    bad metrics we might be blind to model degradation, and if they are misaligned
    with the business goals we might not create any value.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 好的评估指标很重要，因为我们用它们来衡量模型的性能，不仅在训练时使用，而且在生产中也会用到。如果我们的指标不好，我们可能会对模型的退化视而不见，如果它们与业务目标不一致，我们可能无法创造任何价值。
- en: Measuring performance on a text generation task is not as easy as with standard
    classification tasks such as sentiment analysis or named entity recognition. Take
    the example of translation; given a sentence like “I love dogs!” in English and
    translating it to Spanish there can be multiple valid possibilities, like “¡Me
    encantan los perros!” or “¡Me gustan los perros!” Simply checking for an exact
    match to a reference translation is not optimal; even humans would fare badly
    on such a metric because we all write text slightly differently from each other
    (and even from ourselves, depending on the time of the day or year!). Fortunately,
    there are alternatives.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本生成任务上衡量性能并不像标准分类任务（如情感分析或命名实体识别）那样容易。以翻译为例；给定一个句子“我爱狗！”的英文翻译成西班牙语可能有多种有效的可能性，比如“¡Me
    encantan los perros!”或“¡Me gustan los perros!”简单地检查是否与参考翻译完全匹配并不是最佳选择；即使是人类在这样的指标上表现也不佳，因为我们每个人写的文本都略有不同（甚至在一天或一年的不同时间也不同！）。幸运的是，还有其他选择。
- en: Two of the most common metrics used to evaluate generated text are BLEU and
    ROUGE. Let’s take a look at how they’re defined.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 用于评估生成文本的最常见的两个指标是BLEU和ROUGE。让我们看看它们是如何定义的。
- en: BLEU
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BLEU
- en: The idea of BLEU is simple:^([4](ch06.xhtml#idm46238716919888)) instead of looking
    at how many of the tokens in the generated texts are perfectly aligned with the
    reference text tokens, we look at words or *n*-grams. BLEU is a precision-based
    metric, which means that when we compare the two texts we count the number of
    words in the generation that occur in the reference and divide it by the length
    of the reference.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: BLEU的思想很简单：^([4](ch06.xhtml#idm46238716919888))，我们不是看生成文本中有多少标记与参考文本标记完全对齐，而是看单词或*n*-grams。BLEU是一种基于精度的指标，这意味着当我们比较两个文本时，我们计算生成文本中与参考文本中出现的单词数，并将其除以参考文本的长度。
- en: 'However, there is an issue with this vanilla precision. Assume the generated
    text just repeats the same word over and over again, and this word also appears
    in the reference. If it is repeated as many times as the length of the reference
    text, then we get perfect precision! For this reason, the authors of the BLEU
    paper introduced a slight modification: a word is only counted as many times as
    it occurs in the reference. To illustrate this point, suppose we have the reference
    text “the cat is on the mat” and the generated text “the the the the the the”.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种基础精度存在一个问题。假设生成的文本只是一遍又一遍地重复相同的单词，并且这个单词也出现在参考文本中。如果它重复的次数正好等于参考文本的长度，那么我们就得到了完美的精度！因此，BLEU论文的作者引入了一个轻微的修改：一个单词只计算它在参考文本中出现的次数。为了说明这一点，假设我们有参考文本“猫在垫子上”，生成文本“猫猫猫猫猫猫”。
- en: 'From this simple example, we can calculate the precision values as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个简单的例子中，我们可以计算出精度值如下：
- en: <math alttext="p Subscript v a n i l l a Baseline equals six-sixths" display="block"><mrow><msub><mi>p</mi>
    <mrow><mi>v</mi><mi>a</mi><mi>n</mi><mi>i</mi><mi>l</mi><mi>l</mi><mi>a</mi></mrow></msub>
    <mo>=</mo> <mfrac><mn>6</mn> <mn>6</mn></mfrac></mrow></math><math alttext="p
    Subscript m o d Baseline equals two-sixths" display="block"><mrow><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi></mrow></msub> <mo>=</mo> <mfrac><mn>2</mn>
    <mn>6</mn></mfrac></mrow></math>
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="p下标vanilla基线等于六分之六" 显示="块"> <mrow> <msub> <mi> p </mi> <mrow>
    <mi> v </mi> <mi> a </mi> <mi> n </mi> <mi> i </mi> <mi> l </mi> <mi> l </mi>
    <mi> a </mi> </mrow> </msub> <mo> = </mo> <mfrac> <mn> 6 </mn> <mn> 6 </mn> </mfrac>
    </mrow> </math> <math alttext="p下标mod基线等于二分之六" 显示="块"> <mrow> <msub> <mi> p </mi>
    <mrow> <mi> m </mi> <mi> o </mi> <mi> d </mi> </mrow> </msub> <mo> = </mo> <mfrac>
    <mn> 2 </mn> <mn> 6 </mn> </mfrac> </mrow> </math>
- en: 'and we can see that the simple correction has produced a much more reasonable
    value. Now let’s extend this by not only looking at single words, but *n*-grams
    as well. Let’s assume we have one generated sentence, <math alttext="s n t"><mrow><mi>s</mi>
    <mi>n</mi> <mi>t</mi></mrow></math> , that we want to compare against a reference
    sentence, <math alttext="s n t prime"><mrow><mi>s</mi> <mi>n</mi> <msup><mi>t</mi>
    <mo>''</mo></msup></mrow></math> . We extract all possible *n*-grams of degree
    *n* and do the accounting to get the precision <math alttext="p Subscript n"><msub><mi>p</mi>
    <mi>n</mi></msub></math> :'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，简单的修正产生了一个更合理的值。现在让我们通过不仅查看单词，而且还查看*n*-克拉姆来扩展这一点。假设我们有一个生成的句子，<math alttext="snt">
    <mrow> <mi> s </mi> <mi> n </mi> <mi> t </mi> </mrow> </math>，我们想将其与一个参考句子<math
    alttext="snt prime"> <mrow> <mi> s </mi> <mi> n </mi> <msup> <mi> t </mi> <mo>
    ' </mo> </msup> </mrow> </math>进行比较。我们提取所有可能的*n*-克拉姆，并进行计算，得到精度<math alttext="p下标n">
    <msub> <mi> p </mi> <mi> n </mi> </msub> </math>：
- en: <math alttext="p Subscript n Baseline equals StartFraction sigma-summation Underscript
    n hyphen g r a m element-of s n t Endscripts upper C o u n t Subscript c l i p
    Baseline left-parenthesis n hyphen g r a m right-parenthesis Over sigma-summation
    Underscript n hyphen g r a m element-of s n t Superscript prime Baseline Endscripts
    upper C o u n t left-parenthesis n hyphen g r a m right-parenthesis EndFraction"
    display="block"><mrow><msub><mi>p</mi> <mi>n</mi></msub> <mo>=</mo> <mfrac><mrow><msub><mo>∑</mo>
    <mrow><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo>∈</mo><mi>s</mi><mi>n</mi><mi>t</mi></mrow></msub>
    <mi>C</mi><mi>o</mi><mi>u</mi><mi>n</mi><msub><mi>t</mi> <mrow><mi>c</mi><mi>l</mi><mi>i</mi><mi>p</mi></mrow></msub>
    <mrow><mo>(</mo><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo>)</mo></mrow></mrow>
    <mrow><msub><mo>∑</mo> <mrow><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo>∈</mo><mi>s</mi><mi>n</mi><msup><mi>t</mi>
    <mo>'</mo></msup></mrow></msub> <mi>C</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mrow><mo>(</mo><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 数学标记="p下标n基线等于开始分数sigma-总和下标n-克拉姆元素属于snt上标计数下标clip基线左括号n-克拉姆右括号除以sigma-总和下标n-克拉姆元素属于snt上标计数左括号n-克拉姆右括号结束分数"
    显示="块"> <mrow> <msub> <mi> p </mi> <mi> n </mi> </msub> <mo> = </mo> <mfrac> <mrow>
    <msub> <mo> ∑ </mo> <mrow> <mi> n </mi> <mtext> - </mtext> <mi> g </mi> <mi> r
    </mi> <mi> a </mi> <mi> m </mi> <mo> ∈ </mo> <mi> s </mi> <mi> n </mi> <mi> t
    </mi> </mrow> </msub> <mi> C </mi> <mi> o </mi> <mi> u </mi> <mi> n </mi> <msub>
    <mi> t </mi> <mrow> <mi> c </mi> <mi> l </mi> <mi> i </mi> <mi> p </mi> </mrow>
    </msub> <mrow> <mo> ( </mo> <mi> n </mi> <mtext> - </mtext> <mi> g </mi> <mi>
    r </mi> <mi> a </mi> <mi> m </mi> <mo> ) </mo> </mrow> </mrow> <mrow> <msub> <mo>
    ∑ </mo> <mrow> <mi> n </mi> <mtext> - </mtext> <mi> g </mi> <mi> r </mi> <mi>
    a </mi> <mi> m </mi> <mo> ∈ </mo> <mi> s </mi> <mi> n </mi> <msup> <mi> t </mi>
    <mo> ' </mo> </msup> </mrow> </msub> <mi> C </mi> <mi> o </mi> <mi> u </mi> <mi>
    n </mi> <mi> t </mi> <mrow> <mo> ( </mo> <mi> n </mi> <mtext> - </mtext> <mi>
    g </mi> <mi> r </mi> <mi> a </mi> <mi> m </mi> <mo> ) </mo> </mrow> </mrow> </mfrac>
    </mrow> </math>
- en: In order to avoid rewarding repetitive generations, the count in the numerator
    is clipped. What this means is that the occurrence count of an *n*-gram is capped
    at how many times it appears in the reference sentence. Also note that the definition
    of a sentence is not very strict in this equation, and if you had a generated
    text spanning multiple sentences you would treat it as one sentence.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免奖励重复生成，分子中的计数被剪切。这意味着*n*-克拉姆的出现次数被限制为它在参考句子中出现的次数。还要注意，这个方程中对句子的定义并不是非常严格的，如果你有一个生成的跨越多个句子的文本，你会将它视为一个句子。
- en: 'In general we have more than one sample in the test set we want to evaluate,
    so we need to slightly extend the equation by summing over all samples in the
    corpus *C*:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，我们在测试集中有多个样本要评估，因此我们需要稍微扩展方程，对语料库*C*中的所有样本进行求和：
- en: "<math alttext=\"p Subscript n Baseline equals StartFraction sigma-summation\
    \ Underscript s n t element-of upper C Endscripts sigma-summation Underscript\
    \ n hyphen g r a m element-of s n t Endscripts upper C o u n t Subscript c l i\
    \ p Baseline left-parenthesis n hyphen g r a m right-parenthesis Over sigma-summation\
    \ Underscript s n t prime element-of upper C Endscripts sigma-summation Underscript\
    \ n hyphen g r a m element-of s n t Superscript prime Baseline Endscripts upper\
    \ C o u n t left-parenthesis n hyphen g r a m right-parenthesis EndFraction\"\
    \ display=\"block\"><mrow><msub><mi>p</mi> <mi>n</mi></msub> <mo>=</mo> <mfrac><mrow><msub><mo>∑</mo>\
    \ <mrow><mi>s</mi><mi>n</mi><mi>t</mi><mo>∈</mo><mi>C</mi></mrow></msub> <msub><mo>∑</mo>\
    \ <mrow><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo>∈</mo><mi>s</mi><mi>n</mi><mi>t</mi></mrow></msub>\
    \ <mi>C</mi><mi>o</mi><mi>u</mi><mi>n</mi><msub><mi>t</mi> <mrow><mi>c</mi><mi>l</mi><mi>i</mi><mi>p</mi></mrow></msub>\
    \ <mrow><mo>(</mo><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo>)</mo></mrow></mrow>\
    \ <mrow><msub><mo>∑</mo> <mrow><mi>s</mi><mi>n</mi><mi>t</mi><mi>â</mi><mi>\x80\
    </mi><mi>\x99</mi><mo>∈</mo><mi>C</mi></mrow></msub> <msub><mo>∑</mo> <mrow><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo>∈</mo><mi>s</mi><mi>n</mi><msup><mi>t</mi>\
    \ <mo>'</mo></msup></mrow></msub> <mi>C</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mrow><mo>(</mo><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>"
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: "数学标记=\"p下标n基线等于开始分数sigma-求和下标snt属于上C结束sigma-求和下标n-hyphen-gram属于snt上C计数clip下标基线左括号n-hyphen-gram右括号除以sigma-求和下标snt\
    \ prime属于上C结束sigma-求和下标n-hyphen-gram属于snt prime计数左括号n-hyphen-gram右括号结束分数\" 显示=\"\
    块\"> <mrow> <msub> <mi> p </mi> <mi> n </mi> </msub> <mo> = </mo> <mfrac> <mrow>\
    \ <msub> <mo> ∑ </mo> <mrow> <mi> s </mi> <mi> n </mi> <mi> t </mi> <mo> ∈ </mo>\
    \ <mi> C </mi> </mrow> </msub> <msub> <mo> ∑ </mo> <mrow> <mi> n </mi> <mtext>\
    \ - </mtext> <mi> g </mi> <mi> r </mi> <mi> a </mi> <mi> m </mi> <mo> ∈ </mo>\
    \ <mi> s </mi> <mi> n </mi> <mi> t </mi> </mrow> </msub> <mi> C </mi> <mi> o </mi>\
    \ <mi> u </mi> <mi> n </mi> <msub> <mi> t </mi> <mrow> <mi> c </mi> <mi> l </mi>\
    \ <mi> i </mi> <mi> p </mi> </mrow> </msub> <mrow> <mo> ( </mo> <mi> n </mi> <mtext>\
    \ - </mtext> <mi> g </mi> <mi> r </mi> <mi> a </mi> <mi> m </mi> <mo> ) </mo>\
    \ </mrow> <mrow> <msub> <mo> ∑ </mo> <mrow> <mi> s </mi> <mi> n </mi> <mi> t </mi>\
    \ <mi>â\x80\x99</mi> <mo> ∈ </mo> <mi> C </mi> </mrow> </msub> <msub> <mo> ∑ </mo>\
    \ <mrow> <mi> n </mi> <mtext> - </mtext> <mi> g </mi> <mi> r </mi> <mi> a </mi>\
    \ <mi> m </mi> <mo> ∈ </mo> <mi> s </mi> <mi> n </mi> <msup> <mi> t </mi> <mo>\
    \ ' </mo> </msup> </mrow> </msub> <mi> C </mi> <mi> o </mi> <mi> u </mi> <mi>\
    \ n </mi> <mi> t </mi> <mrow> <mo> ( </mo> <mi> n </mi> <mtext> - </mtext> <mi>\
    \ g </mi> <mi> r </mi> <mi> a </mi> <mi> m </mi> <mo> ) </mo> </mrow> </mfrac>\
    \ </mrow> </math>"
- en: 'We’re almost there. Since we are not looking at recall, all generated sequences
    that are short but precise have a benefit compared to sentences that are longer.
    Therefore, the precision score favors short generations. To compensate for that
    the authors of BLEU introduced an additional term, the *brevity penalty*:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们快要到了。由于我们不关注召回率，所有生成的序列如果短而准确，都比长句子有益。因此，精确度得分偏爱短生成物。为了补偿这一点，BLEU的作者引入了一个额外的术语，*简洁惩罚*：
- en: <math alttext="upper B upper R equals min left-parenthesis 1 comma e Superscript
    1 minus script l Super Subscript r e f Superscript slash script l Super Subscript
    g e n Superscript Baseline right-parenthesis" display="block"><mrow><mi>B</mi>
    <mi>R</mi> <mo>=</mo> <mo movablelimits="true" form="prefix">min</mo> <mfenced
    separators="" open="(" close=")"><mn>1</mn> <mo>,</mo> <msup><mi>e</mi> <mrow><mn>1</mn><mo>-</mo><msub><mi>ℓ</mi>
    <mrow><mi>r</mi><mi>e</mi><mi>f</mi></mrow></msub> <mo>/</mo><msub><mi>ℓ</mi>
    <mrow><mi>g</mi><mi>e</mi><mi>n</mi></mrow></msub></mrow></msup></mfenced></mrow></math>
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="上B上R等于min左括号1，e上标1减脚本l Super Subscript r e f Superscript slash脚本l
    Super Subscript g e n Superscript Baseline右括号" 显示="块"> <mrow> <mi> B </mi> <mi>
    R </mi> <mo> = </mo> <mo>可移动限制="真"形式="前缀"> min </mo> <mfenced分隔符=""打开="("关闭=")">
    <mn> 1 </mn> <mo>，</mo> <msup> <mi> e </mi> <mrow> <mn> 1 </mn> <mo> - </mo> <msub>
    <mi> ℓ </mi> <mrow> <mi> r </mi> <mi> e </mi> <mi> f </mi> </mrow> </msub> <mo>
    / </mo> <msub> <mi> ℓ </mi> <mrow> <mi> g </mi> <mi> e </mi> <mi> n </mi> </mrow>
    </msub> </mrow> </msup> </mfenced> </mrow> </math>
- en: By taking the minimum, we ensure that this penalty never exceeds 1 and the exponential
    term becomes exponentially small when the length of the generated text <math alttext="l
    Subscript g e n"><msub><mi>l</mi> <mrow><mi>g</mi><mi>e</mi><mi>n</mi></mrow></msub></math>
    is smaller than the reference text <math alttext="l Subscript r e f"><msub><mi>l</mi>
    <mrow><mi>r</mi><mi>e</mi><mi>f</mi></mrow></msub></math> . At this point you
    might ask, why don’t we just use something like an *F*[1]-score to account for
    recall as well? The answer is that often in translation datasets there are multiple
    reference sentences instead of just one, so if we also measured recall we would
    incentivize translations that used all the words from all the references. Therefore,
    it’s preferable to look for high precision in the translation and make sure the
    translation and reference have a similar length.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 通过取最小值，我们确保这个惩罚永远不会超过1，当生成文本的长度<math alttext="l Subscript g e n"><msub><mi>l</mi>
    <mrow><mi>g</mi><mi>e</mi><mi>n</mi></mrow></msub></math>小于参考文本<math alttext="l
    Subscript r e f"><msub><mi>l</mi> <mrow><mi>r</mi><mi>e</mi><mi>f</mi></mrow></msub></math>时，指数项变得指数级小。此时，你可能会问，为什么我们不使用类似*F*[1]-分数来考虑召回率呢？答案是，通常在翻译数据集中，有多个参考句子而不只是一个，因此，如果我们也测量召回率，我们会鼓励使用所有参考句子中的所有单词的翻译。因此，最好是寻求翻译中的高精度，并确保翻译和参考具有类似的长度。
- en: 'Finally, we can put everything together and get the equation for the BLEU score:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以把所有东西放在一起，得到BLEU分数的方程：
- en: <math alttext="" display="block"><mrow><mtext>BLEU-</mtext><mi mathvariant="italic">N</mi>
    <mo>=</mo> <mi>B</mi> <mi>R</mi> <mo>×</mo> <msup><mfenced separators="" open="("
    close=")"><munderover><mo>∏</mo> <mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi></munderover>
    <msub><mi>p</mi> <mi>n</mi></msub></mfenced> <mrow><mn>1</mn><mo>/</mo><mi>N</mi></mrow></msup></mrow></math>
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="" 显示="块"> <mrow> <mtext> BLEU- </mtext> <mi mathvariant="italic">
    N </mi> <mo> = </mo> <mi> B </mi> <mi> R </mi> <mo> × </mo> <msup> <mfenced分隔符=""打开="("关闭=")">
    <munderover> <mo> ∏ </mo> <mrow> <mi> n </mi> <mo> = </mo> <mn> 1 </mn> </mrow>
    <mi> N </mi> </munderover> <msub> <mi> p </mi> <mi> n </mi> </msub> </mfenced>
    <mrow> <mn> 1 </mn> <mo> / </mo> <mi> N </mi> </mrow> </msup> </mrow> </math>
- en: 'The last term is the geometric mean of the modified precision up to *n*-gram
    *N*. In practice, the BLEU-4 score is often reported. However, you can probably
    already see that this metric has many limitations; for instance, it doesn’t take
    synonyms into account, and many steps in the derivation seem like ad hoc and rather
    fragile heuristics. You can find a wonderful exposition of BLEU’s flaws in Rachel
    Tatman’s blog post [“Evaluating Text Output in NLP: BLEU at Your Own Risk”](https://oreil.ly/nMXRh).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '最后一个术语是修改后的精度直到*n*-gram *N*的几何平均值。在实践中，通常报告BLEU-4分数。然而，您可能已经看到这个指标有许多局限性；例如，它不考虑同义词，并且推导过程中的许多步骤似乎是临时的和相当脆弱的启发式。您可以在Rachel
    Tatman的博客文章[“Evaluating Text Output in NLP: BLEU at Your Own Risk”](https://oreil.ly/nMXRh)中找到对BLEU缺陷的精彩阐述。'
- en: In general, the field of text generation is still looking for better evaluation
    metrics, and finding ways to overcome the limits of metrics like BLEU is an active
    area of research. Another weakness of the BLEU metric is that it expects the text
    to already be tokenized. This can lead to varying results if the exact same method
    for text tokenization is not used. The SacreBLEU metric addresses this issue by
    internalizing the tokenization step; for this reason, it is the preferred metric
    for benchmarking.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，文本生成领域仍在寻找更好的评估指标，克服BLEU等指标的局限性是一个活跃的研究领域。BLEU指标的另一个弱点是它期望文本已经被标记化。如果没有使用完全相同的文本标记化方法，这可能会导致不同的结果。SacreBLEU指标通过内部化标记化步骤来解决这个问题；因此，它是用于基准测试的首选指标。
- en: 'We’ve now worked through some theory, but what we really want to do is calculate
    the score for some generated text. Does that mean we need to implement all this
    logic in Python? Fear not, ![nlpt_pin01](Images/nlpt_pin01.png) Datasets also
    provides metrics! Loading a metric works just like loading a dataset:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经通过了一些理论，但我们真正想做的是计算一些生成文本的分数。这是否意味着我们需要在Python中实现所有这些逻辑？不用担心，![nlpt_pin01](Images/nlpt_pin01.png)
    Datasets还提供了指标！加载指标的方式与加载数据集的方式相同：
- en: '[PRE15]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The `bleu_metric` object is an instance of the `Metric` class, and works like
    an aggregator: you can add single instances with `add()` or whole batches via
    `add_batch()`. Once you have added all the samples you need to evaluate, you then
    call `compute()` and the metric is calculated. This returns a dictionary with
    several values, such as the precision for each *n*-gram, the length penalty, as
    well as the final BLEU score. Let’s look at the example from before:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`bleu_metric`对象是`Metric`类的一个实例，它像聚合器一样工作：您可以使用`add()`添加单个实例，也可以通过`add_batch()`添加整个批次。一旦添加了需要评估的所有样本，然后调用`compute()`，指标就会被计算。这将返回一个包含多个值的字典，例如每个*n*-gram的精度，长度惩罚，以及最终的BLEU分数。让我们看一下之前的例子：'
- en: '[PRE16]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '|  | Value |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | 值 |'
- en: '| --- | --- |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| score | 0.0 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 分数 | 0.0 |'
- en: '| counts | [2, 0, 0, 0] |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 计数 | [2, 0, 0, 0] |'
- en: '| totals | [6, 5, 4, 3] |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 总数 | [6, 5, 4, 3] |'
- en: '| precisions | [33.33, 0.0, 0.0, 0.0] |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 精度 | [33.33, 0.0, 0.0, 0.0] |'
- en: '| bp | 1.0 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| bp | 1.0 |'
- en: '| sys_len | 6 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 系统长度 | 6 |'
- en: '| ref_len | 6 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 参考长度 | 6 |'
- en: Note
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: The BLEU score also works if there are multiple reference translations. This
    is why `reference` is passed as a list. To make the metric smoother for zero counts
    in the *n*-grams, BLEU integrates methods to modify the precision calculation.
    One method is to add a constant to the numerator. That way, a missing *n*-gram
    does not cause the score to automatically go to zero. For the purpose of explaining
    the values, we turn it off by setting `smooth_value=0`.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有多个参考翻译，BLEU分数也适用。这就是为什么`reference`作为列表传递的原因。为了使*n*-gram中的零计数的指标更加平滑，BLEU集成了修改精度计算的方法。一种方法是向分子添加一个常数。这样，缺少的*n*-gram不会导致分数自动变为零。为了解释这些值，我们通过设置`smooth_value=0`将其关闭。
- en: 'We can see the precision of the 1-gram is indeed 2/6, whereas the precisions
    for the 2/3/4-grams are all 0\. (For more information about the individual metrics,
    like counts and bp, see the [SacreBLEU repository](https://oreil.ly/kiZPl).) This
    means the geometric mean is zero, and thus also the BLEU score. Let’s look at
    another example where the prediction is almost correct:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到1-gram的精度确实是2/6，而2/3/4-gram的精度都是0。 （有关个别指标的更多信息，如计数和bp，请参阅[SacreBLEU存储库](https://oreil.ly/kiZPl)。）这意味着几何平均值为零，因此BLEU分数也为零。让我们看另一个几乎正确的预测示例：
- en: '[PRE17]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '|  | Value |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | 值 |'
- en: '| --- | --- |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| score | 57.893007 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 分数 | 57.893007 |'
- en: '| counts | [5, 3, 2, 1] |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 计数 | [5, 3, 2, 1] |'
- en: '| totals | [5, 4, 3, 2] |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 总数 | [5, 4, 3, 2] |'
- en: '| precisions | [100.0, 75.0, 66.67, 50.0] |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 精度 | [100.0, 75.0, 66.67, 50.0] |'
- en: '| bp | 0.818731 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| bp | 0.818731 |'
- en: '| sys_len | 5 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 系统长度 | 5 |'
- en: '| ref_len | 6 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 参考长度 | 6 |'
- en: We observe that the precision scores are much better. The 1-grams in the prediction
    all match, and only in the precision scores do we see that something is off. For
    the 4-gram there are only two candidates, `["the", "cat", "is", "on"]` and `["cat",
    "is", "on", "mat"]`, where the last one does not match, hence the precision of
    0.5.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到精度分数要好得多。预测中的1-gram都匹配，只有在精度分数中我们才看到有些不对劲。对于4-gram，只有两个候选项，`["the", "cat",
    "is", "on"]`和`["cat", "is", "on", "mat"]`，后者不匹配，因此精度为0.5。
- en: The BLEU score is widely used for evaluating text, especially in machine translation,
    since precise translations are usually favored over translations that include
    all possible and appropriate words.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: BLEU分数被广泛用于评估文本，特别是在机器翻译中，因为通常更青睐精确的翻译，而不是包含所有可能和适当单词的翻译。
- en: There are other applications, such as summarization, where the situation is
    different. There, we want all the important information in the generated text,
    so we favor high recall. This is where the ROUGE score is usually used.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他应用，比如摘要，情况就不同了。在那里，我们希望生成的文本中包含所有重要信息，因此我们更青睐高召回率。这就是ROUGE分数通常被使用的地方。
- en: ROUGE
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ROUGE
- en: 'The ROUGE score was specifically developed for applications like summarization
    where high recall is more important than just precision.^([5](ch06.xhtml#idm46238716507664))
    The approach is very similar to the BLEU score in that we look at different *n*-grams
    and compare their occurrences in the generated text and the reference texts. The
    difference is that with ROUGE we check how many *n*-grams in the reference text
    also occur in the generated text. For BLEU we looked at how many *n*-grams in
    the generated text appear in the reference, so we can reuse the precision formula
    with the minor modification that we count the (unclipped) occurrence of reference
    *n*-grams in the generated text in the numerator:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ROUGE分数专门为像摘要这样的应用程序开发，其中高召回率比精确度更重要。这种方法与BLEU分数非常相似，我们观察不同的*n*-gram，并比较它们在生成文本和参考文本中的出现次数。不同之处在于，对于ROUGE，我们检查参考文本中有多少*n*-gram也出现在生成文本中。对于BLEU，我们看生成文本中有多少*n*-gram出现在参考文本中，因此我们可以重新使用精确度公式，只是在分子中计算生成文本中参考*n*-gram的（未修剪的）出现次数。
- en: "<math alttext=\"\" display=\"block\"><mrow><mtext>ROUGE-</mtext><mi mathvariant=\"\
    italic\">N</mi> <mo>=</mo> <mfrac><mrow><msub><mo>∑</mo> <mrow><mi>snt’</mi><mi>\x80\
    </mi><mi>\x99</mi><mo>∈</mo><mi>C</mi></mrow></msub> <msub><mo>∑</mo> <mrow><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo>∈</mo><mi>s</mi><mi>n</mi><msup><mi>t</mi>\
    \ <mo>'</mo></msup></mrow></msub> <mi>C</mi><mi>o</mi><mi>u</mi><mi>n</mi><msub><mi>t</mi>\
    \ <mrow><mi>m</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub> <mrow><mo>(</mo><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo>)</mo></mrow></mrow>\
    \ <mrow><msub><mo>∑</mo> <mrow><mi>snt’</mi><mi>\x80</mi><mi>\x99</mi><mo>∈</mo><mi>C</mi></mrow></msub>\
    \ <msub><mo>∑</mo> <mrow><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo>∈</mo><mi>s</mi><mi>n</mi><msup><mi>t</mi>\
    \ <mo>'</mo></msup></mrow></msub> <mi>C</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mrow><mo>(</mo><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>"
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: "<math alttext=\"\" display=\"block\"><mrow><mtext>ROUGE-</mtext><mi mathvariant=\"\
    italic\">N</mi> <mo>=</mo> <mfrac><mrow><msub><mo>∑</mo> <mrow><mi>snt’</mi><mi>\x80\
    </mi><mi>\x99</mi><mo>∈</mo><mi>C</mi></mrow></msub> <msub><mo>∑</mo> <mrow><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo>∈</mo><mi>s</mi><mi>n</mi><msup><mi>t</mi>\
    \ <mo>'</mo></msup></mrow></msub> <mi>C</mi><mi>o</mi><mi>u</mi><mi>n</mi><msub><mi>t</mi>\
    \ <mrow><mi>m</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub> <mrow><mo>(</mo><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo>)</mo></mrow></mrow>\
    \ <mrow><msub><mo>∑</mo> <mrow><mi>snt’</mi><mi>\x80</mi><mi>\x99</mi><mo>∈</mo><mi>C</mi></mrow></msub>\
    \ <msub><mo>∑</mo> <mrow><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo>∈</mo><mi>s</mi><mi>n</mi><msup><mi>t</mi>\
    \ <mo>'</mo></msup></mrow></msub> <mi>C</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mrow><mo>(</mo><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>"
- en: This was the original proposal for ROUGE. Subsequently, researchers have found
    that fully removing precision can have strong negative effects. Going back to
    the BLEU formula without the clipped counting, we can measure precision as well,
    and we can then combine both precision and recall ROUGE scores in the harmonic
    mean to get an *F*[1]-score. This score is the metric that is nowadays commonly
    reported for ROUGE.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这是ROUGE的原始提案。随后，研究人员发现完全移除精确度可能会产生强烈的负面影响。回到没有修剪计数的BLEU公式，我们可以衡量精确度，并且可以将精确度和召回的ROUGE分数结合起来，得到*F*[1]-score。这个分数是现在常用于ROUGE报告的度量标准。
- en: 'There is a separate score in ROUGE to measure the longest common substring
    (LCS), called ROUGE-L. The LCS can be calculated for any pair of strings. For
    example, the LCS for “abab” and “abc” would be “ab”, and its the length would
    be 2\. If we want to compare this value between two samples we need to somehow
    normalize it because otherwise a longer text would be at an advantage. To achieve
    this, the inventor of ROUGE came up with an <math alttext="upper F"><mi>F</mi></math>
    -score-like scheme where the LCS is normalized with the length of the reference
    and generated text, then the two normalized scores are mixed together:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ROUGE中有一个单独的分数用于衡量最长公共子串（LCS），称为ROUGE-L。LCS可以计算任意一对字符串。例如，“abab”和“abc”的LCS将是“ab”，其长度为2。如果我们想比较两个样本之间的这个值，我们需要以某种方式对其进行归一化，否则较长的文本将处于优势地位。为了实现这一点，ROUGE的发明者提出了一种类似于<math
    alttext="upper F"><mi>F</mi></math> -score的方案，其中LCS与参考文本和生成文本的长度进行了归一化，然后将两个归一化分数混合在一起：
- en: <math alttext="upper R Subscript upper L upper C upper S Baseline equals StartFraction
    upper L upper C upper S left-parenthesis upper X comma upper Y right-parenthesis
    Over m EndFraction" display="block"><mrow><msub><mi>R</mi> <mrow><mi>L</mi><mi>C</mi><mi>S</mi></mrow></msub>
    <mo>=</mo> <mfrac><mrow><mi>L</mi><mi>C</mi><mi>S</mi><mo>(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo>)</mo></mrow>
    <mi>m</mi></mfrac></mrow></math><math alttext="upper P Subscript upper L upper
    C upper S Baseline equals StartFraction upper L upper C upper S left-parenthesis
    upper X comma upper Y right-parenthesis Over n EndFraction" display="block"><mrow><msub><mi>P</mi>
    <mrow><mi>L</mi><mi>C</mi><mi>S</mi></mrow></msub> <mo>=</mo> <mfrac><mrow><mi>L</mi><mi>C</mi><mi>S</mi><mo>(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo>)</mo></mrow>
    <mi>n</mi></mfrac></mrow></math><math alttext="upper F Subscript upper L upper
    C upper S Baseline equals StartFraction left-parenthesis 1 plus beta squared right-parenthesis
    upper R Subscript upper L upper C upper S Baseline upper P Subscript upper L upper
    C upper S Baseline Over upper R Subscript upper L upper C upper S Baseline plus
    beta upper P Subscript upper L upper C upper S Baseline EndFraction comma where
    beta equals upper P Subscript upper L upper C upper S Baseline slash upper R Subscript
    upper L upper C upper S Baseline" display="block"><mrow><msub><mi>F</mi> <mrow><mi>L</mi><mi>C</mi><mi>S</mi></mrow></msub>
    <mo>=</mo> <mfrac><mrow><mrow><mo>(</mo><mn>1</mn><mo>+</mo><msup><mi>β</mi> <mn>2</mn></msup>
    <mo>)</mo></mrow><msub><mi>R</mi> <mrow><mi>L</mi><mi>C</mi><mi>S</mi></mrow></msub>
    <msub><mi>P</mi> <mrow><mi>L</mi><mi>C</mi><mi>S</mi></mrow></msub></mrow> <mrow><msub><mi>R</mi>
    <mrow><mi>L</mi><mi>C</mi><mi>S</mi></mrow></msub> <mo>+</mo><mi>β</mi><msub><mi>P</mi>
    <mrow><mi>L</mi><mi>C</mi><mi>S</mi></mrow></msub></mrow></mfrac> <mo>,</mo> <mtext>where</mtext>
    <mi>β</mi> <mo>=</mo> <msub><mi>P</mi> <mrow><mi>L</mi><mi>C</mi><mi>S</mi></mrow></msub>
    <mo>/</mo> <msub><mi>R</mi> <mrow><mi>L</mi><mi>C</mi><mi>S</mi></mrow></msub></mrow></math>
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper R Subscript upper L upper C upper S Baseline equals StartFraction
    upper L upper C upper S left-parenthesis upper X comma upper Y right-parenthesis
    Over m EndFraction" display="block"><mrow><msub><mi>R</mi> <mrow><mi>L</mi><mi>C</mi><mi>S</mi></mrow></msub>
    <mo>=</mo> <mfrac><mrow><mi>L</mi><mi>C</mi><mi>S</mi><mo>(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo>)</mo></mrow>
    <mi>m</mi></mfrac></mrow></math><math alttext="upper P Subscript upper L upper
    C upper S Baseline equals StartFraction upper L upper C upper S left-parenthesis
    upper X comma upper Y right-parenthesis Over n EndFraction" display="block"><mrow><msub><mi>P</mi>
    <mrow><mi>L</mi><mi>C</mi><mi>S</mi></mrow></msub> <mo>=</mo> <mfrac><mrow><mi>L</mi><mi>C</mi><mi>S</mi><mo>(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo>)</mo></mrow>
    <mi>n</mi></mfrac></mrow></math><math alttext="upper F Subscript upper L upper
    C upper S Baseline equals StartFraction left-parenthesis 1 plus beta squared right-parenthesis
    upper R Subscript upper L upper C upper S Baseline upper P Subscript upper L upper
    C upper S Baseline Over upper R Subscript upper L upper C upper S Baseline plus
    beta upper P Subscript upper L upper C upper S Baseline EndFraction comma where
    beta equals upper P Subscript upper L upper C upper S Baseline slash upper R Subscript
    upper L upper C upper S Baseline" display="block"><mrow><msub><mi>F</mi> <mrow><mi>L</mi><mi>C</mi><mi>S</mi></mrow></msub>
    <mo>=</mo> <mfrac><mrow><mrow><mo>(</mo><mn>1</mn><mo>+</mo><msup><mi>β</mi> <mn>2</mn></msup>
    <mo>)</mo></mrow><msub><mi>R</mi> <mrow><mi>L</mi><mi>C</mi><mi>S</mi></mrow></msub>
    <msub><mi>P</mi> <mrow><mi>L</mi><mi>C</mi><mi>S</mi></mrow></mrow> <mrow><msub><mi>R</mi>
    <mrow><mi>L</mi><mi>C</mi><mi>S</mi></mrow></msub> <mo>+</mo><mi>β</mi><msub><mi>P</mi>
    <mrow><mi>L</mi><mi>C</mi><mi>S</mi></mrow></msub></mrow></mfrac> <mo>,</mo> <mtext>where</mtext>
    <mi>β</mi> <mo>=</mo> <msub><mi>P</mi> <mrow><mi>L</mi><mi>C</mi><mi>S</mi></mrow></msub>
    <mo>/</mo> <msub><mi>R</mi> <mrow><mi>L</mi><mi>C</mi><mi>S</mi></mrow></msub></mrow></math>
- en: 'That way the LCS score is properly normalized and can be compared across samples.
    In the ![nlpt_pin01](Images/nlpt_pin01.png) Datasets implementation, two variations
    of ROUGE are calculated: one calculates the score per sentence and averages it
    for the summaries (ROUGE-L), and the other calculates it directly over the whole
    summary (ROUGE-Lsum).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，LCS分数就可以得到适当的归一化，并可以在样本之间进行比较。在![nlpt_pin01](Images/nlpt_pin01.png)数据集实现中，计算了两种ROUGE的变体：一种计算每个句子的得分并对摘要进行平均（ROUGE-L），另一种直接在整个摘要上计算（ROUGE-Lsum）。
- en: 'We can load the metric as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按以下方式加载度量标准：
- en: '[PRE18]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We already generated a set of summaries with GPT-2 and the other models, and
    now we have a metric to compare the summaries systematically. Let’s apply the
    ROUGE score to all the summaries generated by the models:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使用GPT-2和其他模型生成了一组摘要，现在我们有了一个系统比较摘要的度量标准。让我们将ROUGE分数应用于模型生成的所有摘要：
- en: '[PRE19]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '|  | rouge1 | rouge2 | rougeL | rougeLsum |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|  | rouge1 | rouge2 | rougeL | rougeLsum |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '--- | --- | --- | --- | ---'
- en: '| baseline | 0.303571 | 0.090909 | 0.214286 | 0.232143 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 基线 | 0.303571 | 0.090909 | 0.214286 | 0.232143 |'
- en: '| gpt2 | 0.187500 | 0.000000 | 0.125000 | 0.187500 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: gpt2  | 0.187500 | 0.000000 | 0.125000 | 0.187500
- en: '| t5 | 0.486486 | 0.222222 | 0.378378 | 0.486486 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: t5  | 0.486486 | 0.222222 | 0.378378 | 0.486486
- en: '| bart | 0.582278 | 0.207792 | 0.455696 | 0.506329 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: bart  | 0.582278 | 0.207792 | 0.455696 | 0.506329
- en: '| pegasus | 0.866667 | 0.655172 | 0.800000 | 0.833333 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: pegasus  | 0.866667 | 0.655172 | 0.800000 | 0.833333
- en: Note
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The ROUGE metric in the ![nlpt_pin01](Images/nlpt_pin01.png) Datasets library
    also calculates confidence intervals (by default, the 5th and 95th percentiles).
    The average value is stored in the attribute `mid` and the interval can be retrieved
    with `low` and `high`.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在![nlpt_pin01](Images/nlpt_pin01.png)数据集库中，ROUGE度量标准还计算置信区间（默认情况下为第5和第95百分位数）。平均值存储在属性`mid`中，区间可以通过`low`和`high`检索。
- en: These results are obviously not very reliable as we only looked at a single
    sample, but we can compare the quality of the summary for that one example. The
    table confirms our observation that of the models we considered, GPT-2 performs
    worst. This is not surprising since it is the only model of the group that was
    not explicitly trained to summarize. It is striking, however, that the simple
    first-three-sentence baseline doesn’t fare too poorly compared to the transformer
    models that have on the order of a billion parameters! PEGASUS and BART are the
    best models overall (higher ROUGE scores are better), but T5 is slightly better
    on ROUGE-1 and the LCS scores. These results place T5 and PEGASUS as the best
    models, but again these results should be treated with caution as we only evaluated
    the models on a single example. Looking at the results in the PEGASUS paper, we
    would expect the PEGASUS to outperform T5 on the CNN/DailyMail dataset.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这些结果并不是非常可靠，因为我们只看了一个样本，但我们可以比较一下这一个例子的摘要质量。表格证实了我们的观察，即我们考虑的模型中，GPT-2的表现最差。这并不奇怪，因为它是这个组中唯一没有明确训练用于摘要的模型。然而，令人惊讶的是，简单的前三句基线与拥有大约十亿参数的变压器模型相比，表现并不太差！PEGASUS和BART是整体上最好的模型（ROUGE分数越高越好），但T5在ROUGE-1和LCS分数上略微更好。这些结果将T5和PEGASUS列为最佳模型，但再次需要谨慎对待这些结果，因为我们只在一个例子上评估了模型。从PEGASUS论文中的结果来看，我们预期PEGASUS在CNN/DailyMail数据集上的表现将优于T5。
- en: Let’s see if we can reproduce those results with PEGASUS.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们是否可以用PEGASUS复现这些结果。
- en: Evaluating PEGASUS on the CNN/DailyMail Dataset
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估PEGASUS在CNN/DailyMail数据集上
- en: 'We now have all the pieces in place to evaluate the model properly: we have
    a dataset with a test set from CNN/DailyMail, we have a metric with ROUGE, and
    we have a summarization model. We just need to put the pieces together. Let’s
    first evaluate the performance of the three-sentence baseline:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经准备好适当地评估模型了：我们有一个来自CNN/DailyMail的测试集的数据集，我们有一个ROUGE指标，我们有一个摘要模型。我们只需要把这些部分组合起来。让我们首先评估三句基线的性能：
- en: '[PRE20]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now we’ll apply the function to a subset of the data. Since the test fraction
    of the CNN/DailyMail dataset consists of roughly 10,000 samples, generating summaries
    for all these articles takes a lot of time. Recall from [Chapter 5](ch05.xhtml#chapter_generation)
    that every generated token requires a forward pass through the model; generating
    just 100 tokens for each sample will thus require 1 million forward passes, and
    if we use beam search this number is multiplied by the number of beams. For the
    purpose of keeping the calculations relatively fast, we’ll subsample the test
    set and run the evaluation on 1,000 samples instead. This should give us a much
    more stable score estimation while completing in less than one hour on a single
    GPU for the PEGASUS model:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将把这个函数应用到数据的一个子集上。由于CNN/DailyMail数据集的测试部分大约有10,000个样本，为所有这些文章生成摘要需要很长时间。回想一下[第5章](ch05.xhtml#chapter_generation)中提到的，每个生成的标记都需要通过模型进行前向传递；为每个样本生成100个标记将需要100万次前向传递，如果我们使用波束搜索，这个数字将乘以波束的数量。为了保持计算相对快速，我们将对测试集进行子采样，然后在1,000个样本上运行评估。这应该给我们一个更加稳定的分数估计，同时在单个GPU上不到一小时内完成PEGASUS模型的评估：
- en: '[PRE21]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '|  | rouge1 | rouge2 | rougeL | rougeLsum |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|  | rouge1 | rouge2 | rougeL | rougeLsum |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| baseline | 0.396061 | 0.173995 | 0.245815 | 0.361158 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 基线 | 0.396061 | 0.173995 | 0.245815 | 0.361158 |'
- en: 'The scores are mostly worse than on the previous example, but still better
    than those achieved by GPT-2! Now let’s implement the same evaluation function
    for evaluating the PEGASUS model:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数分数比上一个例子差，但仍然比GPT-2取得的分数要好！现在让我们实现相同的评估函数来评估PEGASUS模型：
- en: '[PRE22]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Let’s unpack this evaluation code a bit. First we split the dataset into smaller
    batches that we can process simultaneously. Then for each batch we tokenize the
    input articles and feed them to the `generate()` function to produce the summaries
    using beam search. We use the same generation parameters as proposed in the paper.
    The new parameter for length penalty ensures that the model does not generate
    sequences that are too long. Finally, we decode the generated texts, replace the
    `<n>` token, and add the decoded texts with the references to the metric. At the
    end, we compute and return the ROUGE scores. Let’s now load the model again with
    the `AutoModelForSeq2SeqLM` class, used for seq2seq generation tasks, and evaluate
    it:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微解开一下这个评估代码。首先，我们将数据集分成较小的批次，然后对每个批次进行同时处理。然后，对于每个批次，我们对输入文章进行标记化，并将它们馈送到`generate()`函数中，使用波束搜索生成摘要。我们使用与论文中提出的相同的生成参数。长度惩罚的新参数确保模型不会生成太长的序列。最后，我们解码生成的文本，替换`<n>`标记，并将解码的文本与参考文献一起添加到度量中。最后，我们计算并返回ROUGE分数。现在让我们再次使用`AutoModelForSeq2SeqLM`类加载模型，用于seq2seq生成任务，并对其进行评估：
- en: '[PRE23]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '|  | rouge1 | rouge2 | rougeL | rougeLsum |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  | rouge1 | rouge2 | rougeL | rougeLsum |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| pegasus | 0.434381 | 0.210883 | 0.307195 | 0.373231 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| pegasus | 0.434381 | 0.210883 | 0.307195 | 0.373231 |'
- en: These numbers are very close to the published results. One thing to note here
    is that the loss and per-token accuracy are decoupled to some degree from the
    ROUGE scores. The loss is independent of the decoding strategy, whereas the ROUGE
    score is strongly coupled.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字非常接近已发布的结果。这里需要注意的一点是，损失和每个标记的准确性在某种程度上与ROUGE分数是分离的。损失与解码策略无关，而ROUGE分数与之强相关。
- en: Since ROUGE and BLEU correlate better with human judgment than loss or accuracy,
    we should focus on them and carefully explore and choose the decoding strategy
    when building text generation models. These metrics are far from perfect, however,
    and one should always consider human judgments as well.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 由于ROUGE和BLEU与人类判断更相关，比损失或准确性，我们应该专注于它们，并在构建文本生成模型时仔细探索和选择解码策略。然而，这些指标远非完美，人们始终应该考虑人类判断。
- en: Now that we’re equipped with an evaluation function, it’s time to train our
    own model for summarization.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个评估函数，是时候为我们自己的摘要模型进行训练了。
- en: Training a Summarization Model
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练摘要模型
- en: 'We’ve worked through a lot of details on text summarization and evaluation,
    so let’s put this to use to train a custom text summarization model! For our application,
    we’ll use the [SAMSum dataset](https://oreil.ly/n1ggq) developed by Samsung, which
    consists of a collection of dialogues along with brief summaries. In an enterprise
    setting, these dialogues might represent the interactions between a customer and
    the support center, so generating accurate summaries can help improve customer
    service and detect common patterns among customer requests. Let’s load it and
    look at an example:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经详细讨论了文本摘要和评估的许多细节，现在让我们利用这些来训练一个自定义的文本摘要模型！对于我们的应用，我们将使用三星开发的[SAMSum数据集](https://oreil.ly/n1ggq)，其中包含一系列对话以及简要摘要。在企业环境中，这些对话可能代表客户与支持中心之间的互动，因此生成准确的摘要可以帮助改善客户服务并检测客户请求中的常见模式。让我们加载它并查看一个示例：
- en: '[PRE24]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The dialogues look like what you would expect from a chat via SMS or WhatsApp,
    including emojis and placeholders for GIFs. The `dialogue` field contains the
    full text and the `summary` the summarized dialogue. Could a model that was fine-tuned
    on the CNN/DailyMail dataset deal with that? Let’s find out!
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 对话看起来就像你通过短信或WhatsApp进行聊天时所期望的那样，包括表情符号和GIF的占位符。`dialogue`字段包含完整文本，`summary`包含总结的对话。CNN/DailyMail数据集上进行了微调的模型能够处理吗？让我们找出来！
- en: Evaluating PEGASUS on SAMSum
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在SAMSum上评估PEGASUS
- en: 'First we’ll run the same summarization pipeline with PEGASUS to see what the
    output looks like. We can reuse the code we used for the CNN/DailyMail summary
    generation:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用PEGASUS运行相同的摘要流程，看看输出是什么样的。我们可以重用用于CNN/DailyMail摘要生成的代码：
- en: '[PRE26]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We can see that the model mostly tries to summarize by extracting the key sentences
    from the dialogue. This probably worked relatively well on the CNN/DailyMail dataset,
    but the summaries in SAMSum are more abstract. Let’s confirm this by running the
    full ROUGE evaluation on the test set:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，该模型主要通过提取对话中的关键句子来进行摘要。这在CNN/DailyMail数据集上可能效果相对较好，但在SAMSum中的摘要更加抽象。让我们通过在测试集上运行完整的ROUGE评估来确认这一点：
- en: '[PRE28]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '|  | rouge1 | rouge2 | rougeL | rougeLsum |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  | rouge1 | rouge2 | rougeL | rougeLsum |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| pegasus | 0.296168 | 0.087803 | 0.229604 | 0.229514 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| pegasus | 0.296168 | 0.087803 | 0.229604 | 0.229514 |'
- en: 'Well, the results aren’t great, but this is not unexpected since we’ve moved
    quite a bit away from the CNN/DailyMail data distribution. Nevertheless, setting
    up the evaluation pipeline before training has two advantages: we can directly
    measure the success of training with the metric and we have a good baseline. Fine-tuning
    the model on our dataset should result in an immediate improvement in the ROUGE
    metric, and if that is not the case we’ll know something is wrong with our training
    loop.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 结果并不理想，但这并不意外，因为我们已经远离了CNN/DailyMail数据分布。尽管如此，在训练之前设置评估流程有两个优点：我们可以直接用指标来衡量训练的成功与否，同时也有了一个很好的基准。在我们的数据集上微调模型应该会立即提高ROUGE指标，如果不是这样，我们就会知道我们的训练循环出了问题。
- en: Fine-Tuning PEGASUS
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PEGASUS微调
- en: 'Before we process the data for training, let’s have a quick look at the length
    distribution of the input and outputs:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理训练数据之前，让我们快速查看输入和输出的长度分布：
- en: '[PRE29]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '![](Images/nlpt_06in01.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_06in01.png)'
- en: We see that most dialogues are much shorter than the CNN/DailyMail articles,
    with 100–200 tokens per dialogue. Similarly, the summaries are much shorter, with
    around 20–40 tokens (the average length of a tweet).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到大多数对话比CNN/DailyMail文章要短得多，每个对话大约有100-200个标记。同样，摘要也要短得多，大约有20-40个标记（平均推文长度）。
- en: 'Let’s keep those observations in mind as we build the data collator for the
    `Trainer`. First we need to tokenize the dataset. For now, we’ll set the maximum
    lengths to 1024 and 128 for the dialogues and summaries, respectively:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建`Trainer`的数据收集器时，让我们记住这些观察结果。首先，我们需要对数据集进行标记化。目前，我们将对话和摘要的最大长度分别设置为1024和128：
- en: '[PRE30]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: A new thing in the use of the tokenization step is the `tokenizer.as_target_tokenizer()`
    context. Some models require special tokens in the decoder inputs, so it’s important
    to differentiate between the tokenization of encoder and decoder inputs. In the
    `with` statement (called a *context manager*), the tokenizer knows that it is
    tokenizing for the decoder and can process sequences accordingly.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在标记化步骤中使用的新功能是`tokenizer.as_target_tokenizer()`上下文。一些模型需要在解码器输入中使用特殊标记，因此区分编码器和解码器输入的标记化非常重要。在`with`语句（称为*上下文管理器*）中，标记器知道它正在为解码器进行标记化，并可以相应地处理序列。
- en: 'Now, we need to create the data collator. This function is called in the `Trainer`
    just before the batch is fed through the model. In most cases we can use the default
    collator, which collects all the tensors from the batch and simply stacks them.
    For the summarization task we need to not only stack the inputs but also prepare
    the targets on the decoder side. PEGASUS is an encoder-decoder transformer and
    thus has the classic seq2seq architecture. In a seq2seq setup, a common approach
    is to apply “teacher forcing” in the decoder. With this strategy, the decoder
    receives input tokens (like in decoder-only models such as GPT-2) that consists
    of the labels shifted by one in addition to the encoder output; so, when making
    the prediction for the next token the decoder gets the ground truth shifted by
    one as an input, as illustrated in the following table:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要创建数据收集器。这个函数在`Trainer`中在批处理通过模型之前调用。在大多数情况下，我们可以使用默认的收集器，它会从批处理中收集所有张量并简单地堆叠它们。对于摘要任务，我们不仅需要堆叠输入，还需要在解码器端准备目标。PEGASUS是一个编码器-解码器变换器，因此具有经典的seq2seq架构。在seq2seq设置中，一个常见的方法是在解码器中应用“教师强制”。使用这种策略，解码器接收输入标记（就像在仅解码器模型（如GPT-2）中一样），其中包括标签向后移动一个位置的内容，以及编码器输出；因此，在进行下一个标记的预测时，解码器会得到向后移动一个位置的地面真相作为输入，如下表所示：
- en: '|  | decoder_input | label |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  | 解码器输入 | 标签 |'
- en: '| --- | --- | --- |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| step |  |  |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 步骤 |  |  |'
- en: '| --- | --- | --- |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | [PAD] | Transformers |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 1 | [PAD] | Transformers |'
- en: '| 2 | [PAD, Transformers] | are |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 2 | [PAD, Transformers] | are |'
- en: '| 3 | [PAD, Transformers, are] | awesome |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 3 | [PAD, Transformers, are] | awesome |'
- en: '| 4 | [PAD, Transformers, are, awesome] | for |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 4 | [PAD, Transformers, are, awesome] | for |'
- en: '| 5 | [PAD, Transformers, are, awesome, for] | text |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 5 | [PAD, Transformers, are, awesome, for] | text |'
- en: '| 6 | [PAD, Transformers, are, awesome, for, text] | summarization |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 6 | [PAD, Transformers, are, awesome, for, text] | summarization |'
- en: We shift it by one so that the decoder only sees the previous ground truth labels
    and not the current or future ones. Shifting alone suffices since the decoder
    has masked self-attention that masks all inputs at present and in the future.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其向右移动一个，以便解码器只看到先前的地面真实标签，而不是当前或未来的标签。仅仅移位就足够了，因为解码器具有掩码自注意力，可以屏蔽当前和未来的所有输入。
- en: 'So, when we prepare our batch, we set up the decoder inputs by shifting the
    labels to the right by one. After that, we make sure the padding tokens in the
    labels are ignored by the loss function by setting them to –100\. We actually
    don’t have to do this manually, though, since the `DataCollatorForSeq2Seq` comes
    to the rescue and takes care of all these steps for us:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当我们准备批处理时，我们通过将标签向右移动一个来设置解码器输入。之后，我们确保通过将它们设置为-100来忽略标签中的填充标记，以便损失函数忽略它们。实际上，我们不必手动执行此操作，因为`DataCollatorForSeq2Seq`会帮助我们处理所有这些步骤：
- en: '[PRE31]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Then, as usual, we set up a the `TrainingArguments` for training:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，像往常一样，我们为训练设置了`TrainingArguments`：
- en: '[PRE32]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: One thing that is different from the previous settings is that new argument,
    `gradient_accumulation_steps`. Since the model is quite big, we had to set the
    batch size to 1\. However, a batch size that is too small can hurt convergence.
    To resolve that issue, we can use a nifty technique called *gradient accumulation*.
    As the name suggests, instead of calculating the gradients of the full batch all
    at once, we make smaller batches and aggregate the gradients. When we have aggregated
    enough gradients, we run the optimization step. Naturally this is a bit slower
    than doing it in one pass, but it saves us a lot of GPU memory.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前的设置不同的一件事是新的参数`gradient_accumulation_steps`。由于模型非常庞大，我们不得不将批处理大小设置为1。然而，太小的批处理大小可能会影响收敛。为了解决这个问题，我们可以使用一种称为*梯度累积*的巧妙技术。顾名思义，我们不是一次计算整个批次的梯度，而是制作更小的批次并聚合梯度。当我们聚合了足够的梯度时，我们运行优化步骤。当然，这比一次性完成要慢一些，但它节省了大量的GPU内存。
- en: 'Let’s now make sure that we are logged in to Hugging Face so we can push the
    model to the Hub after training:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们确保我们已经登录到Hugging Face，这样我们就可以在训练后将模型推送到Hub了：
- en: '[PRE33]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We have now everything we need to initialize the trainer with the model, tokenizer,
    training arguments, and data collator, as well as the training and evaluation
    sets:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好使用模型、分词器、训练参数和数据整理器以及训练和评估集来初始化训练器了：
- en: '[PRE34]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We are ready for training. After training, we can directly run the evaluation
    function on the test set to see how well the model performs:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备好进行训练。训练后，我们可以直接在测试集上运行评估函数，以查看模型的表现如何：
- en: '[PRE35]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '|  | rouge1 | rouge2 | rougeL | rougeLsum |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|  | rouge1 | rouge2 | rougeL | rougeLsum |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| pegasus | 0.427614 | 0.200571 | 0.340648 | 0.340738 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| pegasus | 0.427614 | 0.200571 | 0.340648 | 0.340738 |'
- en: 'We see that the ROUGE scores improved considerably over the model without fine-tuning,
    so even though the previous model was also trained for summarization, it was not
    well adapted for the new domain. Let’s push our model to the Hub:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到ROUGE分数比没有微调的模型有了显著的改进，因此即使先前的模型也是用于摘要，但它并没有很好地适应新的领域。让我们将模型推送到Hub：
- en: '[PRE36]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: In the next section we’ll use the model to generate a few summaries for us.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将使用模型为我们生成一些摘要。
- en: Tip
  id: totrans-192
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'You can also evaluate the generations as part of the training loop: use the
    extension of `TrainingArguments` called `Seq2Seq​Trainin⁠g​Arguments` and specify
    `predict_with_generate=True`. Pass it to the dedicated `Trainer` called `Seq2SeqTrainer`,
    which then uses the `generate()` function instead of the model’s forward pass
    to create predictions for evaluation. Give it a try!'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在训练循环中评估生成的内容：使用名为`Seq2Seq​Trainin⁠g​Arguments`的`TrainingArguments`的扩展，并指定`predict_with_generate=True`。将其传递给专用的`Trainer`称为`Seq2SeqTrainer`，然后使用`generate()`函数而不是模型的前向传递来创建评估预测。试试看！
- en: Generating Dialogue Summaries
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成对话摘要
- en: 'Looking at the losses and ROUGE scores, it seems the model is showing a significant
    improvement over the original model trained on CNN/DailyMail only. Let’s see what
    a summary generated on a sample from the test set looks like:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看损失和ROUGE分数，似乎模型在CNN/DailyMail上训练的原始模型相比显示了显著的改进。让我们看看从测试集中的样本生成的摘要是什么样子的：
- en: '[PRE37]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'That looks much more like the reference summary. It seems the model has learned
    to synthesize the dialogue into a summary without just extracting passages. Now,
    the ultimate test: how well does the model work on a custom input?'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来更像参考摘要。似乎模型已经学会将对话合成为摘要，而不仅仅是提取段落。现在，最终测试：模型在自定义输入上的表现如何？
- en: '[PRE39]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The generated summary of the custom dialogue makes sense. It summarizes well
    that all the people in the discussion want to write the book together and does
    not simply extract single sentences. For example, it synthesizes the third and
    fourth lines into a logical combination.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 定制对话的生成摘要是有意义的。它很好地总结了讨论中所有人都想一起写书，而不仅仅是提取单个句子。例如，它将第三和第四行合成了一个逻辑组合。
- en: Conclusion
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Text summarization poses some unique challenges compared to other tasks that
    can be framed as classification tasks, like sentiment analysis, named entity recognition,
    or question answering. Conventional metrics such as accuracy do not reflect the
    quality of the generated text. As we saw, the BLEU and ROUGE metrics can better
    evaluate generated texts; however, human judgment remains the best measure.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 与可以作为分类任务框架的其他任务相比，文本摘要提出了一些独特的挑战，比如情感分析、命名实体识别或问答。传统的度量标准如准确性并不能反映生成文本的质量。正如我们所看到的，BLEU和ROUGE指标可以更好地评估生成的文本；然而，人类判断仍然是最好的衡量标准。
- en: A common question when working with summarization models is how we can summarize
    documents where the texts are longer than the model’s context length. Unfortunately,
    there is no single strategy to solve this problem, and to date this is still an
    open and active research question. For example, recent work by OpenAI showed how
    to scale summarization by applying it recursively to long documents and using
    human feedback in the loop.^([6](ch06.xhtml#idm46238714384416))
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用总结模型时一个常见问题是如何总结文档，其中文本长度超过模型的上下文长度。不幸的是，目前还没有解决这个问题的单一策略，迄今为止，这仍然是一个开放和活跃的研究问题。例如，OpenAI的最新工作显示了如何通过将其递归应用于长文档并在循环中使用人类反馈来扩展总结。^([6](ch06.xhtml#idm46238714384416))
- en: In the next chapter we’ll look at question answering, which is the task of providing
    an answer to a question based on a text passage. In contrast to summarization,
    with this task there exist good strategies to deal with long or many documents,
    and we’ll show you how to scale question answering to thousands of documents.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论问答，这是根据文本段落提供问题答案的任务。与总结相比，对于这个任务，存在处理长篇或多篇文档的良好策略，我们将向您展示如何将问答扩展到数千篇文档。
- en: ^([1](ch06.xhtml#idm46238717347776-marker)) A. Radford et al., [“Language Models
    Are Unsupervised Multitask Learners”](https://openai.com/blog/better-language-models),
    OpenAI (2019).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch06.xhtml#idm46238717347776-marker)) A. Radford等人，[“语言模型是无监督多任务学习者”](https://openai.com/blog/better-language-models)，OpenAI
    (2019)。
- en: '^([2](ch06.xhtml#idm46238717146864-marker)) M. Lewis et al., [“BART: Denoising
    Sequence-to-Sequence Pre-training for Natural Language Generation, Translation,
    and Comprehension”](https://arxiv.org/abs/1910.13461), (2019).'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch06.xhtml#idm46238717146864-marker)) M. Lewis等人，[“BART:去噪序列到序列的预训练用于自然语言生成、翻译和理解”](https://arxiv.org/abs/1910.13461)，(2019)。
- en: '^([3](ch06.xhtml#idm46238717078848-marker)) J. Zhang et al., [“PEGASUS: Pre-Training
    with Extracted Gap-Sentences for Abstractive Summarization”](https://arxiv.org/abs/1912.08777),
    (2019).'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch06.xhtml#idm46238717078848-marker)) J. Zhang等人，[“PEGASUS:使用抽取的间隙句子进行抽象总结的预训练”](https://arxiv.org/abs/1912.08777)，(2019)。
- en: '^([4](ch06.xhtml#idm46238716919888-marker)) K. Papineni et al., “BLEU: A Method
    for Automatic Evaluation of Machine Translation,” *Proceedings of the 40th Annual
    Meeting of the Association for Computational Linguistics* (July 2002): 311–318,
    [*http://dx.doi.org/10.3115/1073083.1073135*](http://dx.doi.org/10.3115/1073083.1073135).'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '^([4](ch06.xhtml#idm46238716919888-marker)) K. Papineni等人，“BLEU:机器翻译自动评估的方法”，*计算语言学协会第40届年会论文集*
    (2002年7月): 311–318，[*http://dx.doi.org/10.3115/1073083.1073135*](http://dx.doi.org/10.3115/1073083.1073135)。'
- en: '^([5](ch06.xhtml#idm46238716507664-marker)) C-Y. Lin, “ROUGE: A Package for
    Automatic Evaluation of Summaries,” *Text Summarization Branches Out* (July 2004),
    [*https://aclanthology.org/W04-1013.pdf*](https://aclanthology.org/W04-1013.pdf).'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch06.xhtml#idm46238716507664-marker)) C-Y. Lin，“ROUGE:自动摘要评估包”，*文本摘要分支扩展*
    (2004年7月)，[*https://aclanthology.org/W04-1013.pdf*](https://aclanthology.org/W04-1013.pdf)。
- en: ^([6](ch06.xhtml#idm46238714384416-marker)) J. Wu et al., [“Recursively Summarizing
    Books with Human Feedback”](https://arxiv.org/abs/2109.10862), (2021).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch06.xhtml#idm46238714384416-marker)) J. Wu等人，[“使用人类反馈递归总结书籍”](https://arxiv.org/abs/2109.10862)，(2021)。
