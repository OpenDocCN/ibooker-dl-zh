- en: Chapter 6\. Summarization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At one point or another, you’ve probably needed to summarize a document, be
    it a research article, a financial earnings report, or a thread of emails. If
    you think about it, this requires a range of abilities, such as understanding
    long passages, reasoning about the contents, and producing fluent text that incorporates
    the main topics from the original document. Moreover, accurately summarizing a
    news article is very different from summarizing a legal contract, so being able
    to do so requires a sophisticated degree of domain generalization. For these reasons,
    text summarization is a difficult task for neural language models, including transformers.
    Despite these challenges, text summarization offers the prospect for domain experts
    to significantly speed up their workflows and is used by enterprises to condense
    internal knowledge, summarize contracts, automatically generate content for social
    media releases, and more.
  prefs: []
  type: TYPE_NORMAL
- en: To help you understand the challenges involved, this chapter will explore how
    we can leverage pretrained transformers to summarize documents. Summarization
    is a classic sequence-to-sequence (seq2seq) task with an input text and a target
    text. As we saw in [Chapter 1](ch01.xhtml#chapter_introduction), this is where
    encoder-decoder transformers excel.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we will build our own encoder-decoder model to condense dialogues
    between several people into a crisp summary. But before we get to that, let’s
    begin by taking a look at one of the canonical datasets for summarization: the
    CNN/DailyMail corpus.'
  prefs: []
  type: TYPE_NORMAL
- en: The CNN/DailyMail Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The CNN/DailyMail dataset consists of around 300,000 pairs of news articles
    and their corresponding summaries, composed from the bullet points that CNN and
    the DailyMail attach to their articles. An important aspect of the dataset is
    that the summaries are *abstractive* and not *extractive*, which means that they
    consist of new sentences instead of simple excerpts. The dataset is available
    on the [Hub](https://oreil.ly/jcRmb); we’ll use version 3.0.0, which is a nonanonymized
    version set up for summarization. We can select versions in a similar manner as
    splits, we saw in [Chapter 4](ch04.xhtml#chapter_ner), with a `version` keyword.
    So let’s dive in and have a look at it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The dataset has three columns: `article`, which contains the news articles,
    `highlights` with the summaries, and `id` to uniquely identify each article. Let’s
    look at an excerpt from an article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We see that the articles can be very long compared to the target summary; in
    this particular case the difference is 17-fold. Long articles pose a challenge
    to most transformer models since the context size is usually limited to 1,000
    tokens or so, which is equivalent to a few paragraphs of text. The standard, yet
    crude way to deal with this for summarization is to simply truncate the texts
    beyond the model’s context size. Obviously there could be important information
    for the summary toward the end of the text, but for now we need to live with this
    limitation of the model architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Text Summarization Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s see how a few of the most popular transformer models for summarization
    perform by first looking qualitatively at the outputs for the preceding example.
    Although the model architectures we will be exploring have varying maximum input
    sizes, let’s restrict the input text to 2,000 characters to have the same input
    for all models and thus make the outputs more comparable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'A convention in summarization is to separate the summary sentences by a newline.
    We could add a newline token after each full stop, but this simple heuristic would
    fail for strings like “U.S.” or “U.N.” The Natural Language Toolkit (NLTK) package
    includes a more sophisticated algorithm that can differentiate the end of a sentence
    from punctuation that occurs in abbreviations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the following sections we will load several large models. If you run out
    of memory, you can either replace the large models with smaller checkpoints (e.g.,
    “gpt”, “t5-small”) or skip this section and jump to [“Evaluating PEGASUS on the
    CNN/DailyMail Dataset”](#eval-pegasus).
  prefs: []
  type: TYPE_NORMAL
- en: Summarization Baseline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A common baseline for summarizing news articles is to simply take the first
    three sentences of the article. With NLTK’s sentence tokenizer, we can easily
    implement such a baseline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: GPT-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ve already seen in [Chapter 5](ch05.xhtml#chapter_generation) how GPT-2
    can generate text given some prompt. One of the model’s surprising features is
    that we can also use it to generate summaries by simply appending “TL;DR” at the
    end of the input text. The expression “TL;DR” (too long; didn’t read) is often
    used on platforms like Reddit to indicate a short version of a long post. We will
    start our summarization experiment by re-creating the procedure of the original
    paper with the `pipeline()` function from ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers.^([1](ch06.xhtml#idm46238717347776)) We create a text generation
    pipeline and load the large GPT-2 model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Here we just store the summaries of the generated text by slicing off the input
    query and keep the result in a Python dictionary for later comparison.
  prefs: []
  type: TYPE_NORMAL
- en: T5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next let’s try the T5 transformer. As we saw in [Chapter 3](ch03.xhtml#chapter_anatomy),
    the developers of this model performed a comprehensive study of transfer learning
    in NLP and found they could create a universal transformer architecture by formulating
    all tasks as text-to-text tasks. The T5 checkpoints are trained on a mixture of
    unsupervised data (to reconstruct masked words) and supervised data for several
    tasks, including summarization. These checkpoints can thus be directly used to
    perform summarization without fine-tuning by using the same prompts used during
    pretraining. In this framework, the input format for the model to summarize a
    document is `"summarize: <ARTICLE>"`, and for translation it looks like `"translate
    English to German: <TEXT>"`. As shown in [Figure 6-1](#T5), this makes T5 extremely
    versatile and allows you to solve many tasks with a single model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can directly load T5 for summarization with the `pipeline()` function, which
    also takes care of formatting the inputs in the text-to-text format so we don’t
    need to prepend them with `"summarize"`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![T5](Images/nlpt_0601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-1\. Diagram of T5’s text-to-text framework (courtesy of Colin Raffel);
    besides translation and summarization, the CoLA (linguistic acceptability) and
    STSB (semantic similarity) tasks are shown
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: BART
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'BART also uses an encoder-decoder architecture and is trained to reconstruct
    corrupted inputs. It combines the pretraining schemes of BERT and GPT-2.^([2](ch06.xhtml#idm46238717146864))
    We’ll use the `facebook/bart-large-ccn` checkpoint, which has been specifically
    fine-tuned on the CNN/DailyMail dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: PEGASUS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like BART, PEGASUS is an encoder-decoder transformer.^([3](ch06.xhtml#idm46238717078848))
    As shown in [Figure 6-2](#pegasus), its pretraining objective is to predict masked
    sentences in multisentence texts. The authors argue that the closer the pretraining
    objective is to the downstream task, the more effective it is. With the aim of
    finding a pretraining objective that is closer to summarization than general language
    modeling, they automatically identified, in a very large corpus, sentences containing
    most of the content of their surrounding paragraphs (using summarization evaluation
    metrics as a heuristic for content overlap) and pretrained the PEGASUS model to
    reconstruct these sentences, thereby obtaining a state-of-the-art model for text
    summarization.
  prefs: []
  type: TYPE_NORMAL
- en: '![pegasus](Images/nlpt_0602.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-2\. Diagram of PEGASUS architecture (courtesy of Jingqing Zhang et
    al.)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This model has a special token for newlines, which is why we don’t need the
    `sent_tokenize()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Comparing Different Summaries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have generated summaries with four different models, let’s compare
    the results. Keep in mind that one model has not been trained on the dataset at
    all (GPT-2), one model has been fine-tuned on this task among others (T5), and
    two models have exclusively been fine-tuned on this task (BART and PEGASUS). Let’s
    have a look at the summaries these models have generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The first thing we notice by looking at the model outputs is that the summary
    generated by GPT-2 is quite different from the others. Instead of giving a summary
    of the text, it summarizes the characters. Often the GPT-2 model “hallucinates”
    or invents facts, since it was not explicitly trained to generate truthful summaries.
    For example, at the time of writing, Nesta is not the fastest man in the world,
    but sits in ninth place. Comparing the other three model summaries against the
    ground truth, we see that there is remarkable overlap, with PEGASUS’s output bearing
    the most striking resemblance.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have inspected a few models, let’s try to decide which one we would
    use in a production setting. All four models seem to provide qualitatively reasonable
    results, and we could generate a few more examples to help us decide. However,
    this is not a systematic way of determining the best model! Ideally, we would
    define a metric, measure it for all models on some benchmark dataset, and choose
    the one with the best performance. But how do you define a metric for text generation?
    The standard metrics that we’ve seen, like accuracy, recall, and precision, are
    not easy to apply to this task. For each “gold standard” summary written by a
    human, dozens of other summaries with synonyms, paraphrases, or a slightly different
    way of formulating the facts could be just as acceptable.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section we will look at some common metrics that have been developed
    for measuring the quality of generated text.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring the Quality of Generated Text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Good evaluation metrics are important, since we use them to measure the performance
    of models not only when we train them but also later, in production. If we have
    bad metrics we might be blind to model degradation, and if they are misaligned
    with the business goals we might not create any value.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring performance on a text generation task is not as easy as with standard
    classification tasks such as sentiment analysis or named entity recognition. Take
    the example of translation; given a sentence like “I love dogs!” in English and
    translating it to Spanish there can be multiple valid possibilities, like “¡Me
    encantan los perros!” or “¡Me gustan los perros!” Simply checking for an exact
    match to a reference translation is not optimal; even humans would fare badly
    on such a metric because we all write text slightly differently from each other
    (and even from ourselves, depending on the time of the day or year!). Fortunately,
    there are alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: Two of the most common metrics used to evaluate generated text are BLEU and
    ROUGE. Let’s take a look at how they’re defined.
  prefs: []
  type: TYPE_NORMAL
- en: BLEU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The idea of BLEU is simple:^([4](ch06.xhtml#idm46238716919888)) instead of looking
    at how many of the tokens in the generated texts are perfectly aligned with the
    reference text tokens, we look at words or *n*-grams. BLEU is a precision-based
    metric, which means that when we compare the two texts we count the number of
    words in the generation that occur in the reference and divide it by the length
    of the reference.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there is an issue with this vanilla precision. Assume the generated
    text just repeats the same word over and over again, and this word also appears
    in the reference. If it is repeated as many times as the length of the reference
    text, then we get perfect precision! For this reason, the authors of the BLEU
    paper introduced a slight modification: a word is only counted as many times as
    it occurs in the reference. To illustrate this point, suppose we have the reference
    text “the cat is on the mat” and the generated text “the the the the the the”.'
  prefs: []
  type: TYPE_NORMAL
- en: 'From this simple example, we can calculate the precision values as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="p Subscript v a n i l l a Baseline equals six-sixths" display="block"><mrow><msub><mi>p</mi>
    <mrow><mi>v</mi><mi>a</mi><mi>n</mi><mi>i</mi><mi>l</mi><mi>l</mi><mi>a</mi></mrow></msub>
    <mo>=</mo> <mfrac><mn>6</mn> <mn>6</mn></mfrac></mrow></math><math alttext="p
    Subscript m o d Baseline equals two-sixths" display="block"><mrow><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi></mrow></msub> <mo>=</mo> <mfrac><mn>2</mn>
    <mn>6</mn></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'and we can see that the simple correction has produced a much more reasonable
    value. Now let’s extend this by not only looking at single words, but *n*-grams
    as well. Let’s assume we have one generated sentence, <math alttext="s n t"><mrow><mi>s</mi>
    <mi>n</mi> <mi>t</mi></mrow></math> , that we want to compare against a reference
    sentence, <math alttext="s n t prime"><mrow><mi>s</mi> <mi>n</mi> <msup><mi>t</mi>
    <mo>''</mo></msup></mrow></math> . We extract all possible *n*-grams of degree
    *n* and do the accounting to get the precision <math alttext="p Subscript n"><msub><mi>p</mi>
    <mi>n</mi></msub></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="p Subscript n Baseline equals StartFraction sigma-summation Underscript
    n hyphen g r a m element-of s n t Endscripts upper C o u n t Subscript c l i p
    Baseline left-parenthesis n hyphen g r a m right-parenthesis Over sigma-summation
    Underscript n hyphen g r a m element-of s n t Superscript prime Baseline Endscripts
    upper C o u n t left-parenthesis n hyphen g r a m right-parenthesis EndFraction"
    display="block"><mrow><msub><mi>p</mi> <mi>n</mi></msub> <mo>=</mo> <mfrac><mrow><msub><mo>∑</mo>
    <mrow><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo>∈</mo><mi>s</mi><mi>n</mi><mi>t</mi></mrow></msub>
    <mi>C</mi><mi>o</mi><mi>u</mi><mi>n</mi><msub><mi>t</mi> <mrow><mi>c</mi><mi>l</mi><mi>i</mi><mi>p</mi></mrow></msub>
    <mrow><mo>(</mo><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo>)</mo></mrow></mrow>
    <mrow><msub><mo>∑</mo> <mrow><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo>∈</mo><mi>s</mi><mi>n</mi><msup><mi>t</mi>
    <mo>'</mo></msup></mrow></msub> <mi>C</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mrow><mo>(</mo><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: In order to avoid rewarding repetitive generations, the count in the numerator
    is clipped. What this means is that the occurrence count of an *n*-gram is capped
    at how many times it appears in the reference sentence. Also note that the definition
    of a sentence is not very strict in this equation, and if you had a generated
    text spanning multiple sentences you would treat it as one sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general we have more than one sample in the test set we want to evaluate,
    so we need to slightly extend the equation by summing over all samples in the
    corpus *C*:'
  prefs: []
  type: TYPE_NORMAL
- en: "<math alttext=\"p Subscript n Baseline equals StartFraction sigma-summation\
    \ Underscript s n t element-of upper C Endscripts sigma-summation Underscript\
    \ n hyphen g r a m element-of s n t Endscripts upper C o u n t Subscript c l i\
    \ p Baseline left-parenthesis n hyphen g r a m right-parenthesis Over sigma-summation\
    \ Underscript s n t prime element-of upper C Endscripts sigma-summation Underscript\
    \ n hyphen g r a m element-of s n t Superscript prime Baseline Endscripts upper\
    \ C o u n t left-parenthesis n hyphen g r a m right-parenthesis EndFraction\"\
    \ display=\"block\"><mrow><msub><mi>p</mi> <mi>n</mi></msub> <mo>=</mo> <mfrac><mrow><msub><mo>∑</mo>\
    \ <mrow><mi>s</mi><mi>n</mi><mi>t</mi><mo>∈</mo><mi>C</mi></mrow></msub> <msub><mo>∑</mo>\
    \ <mrow><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo>∈</mo><mi>s</mi><mi>n</mi><mi>t</mi></mrow></msub>\
    \ <mi>C</mi><mi>o</mi><mi>u</mi><mi>n</mi><msub><mi>t</mi> <mrow><mi>c</mi><mi>l</mi><mi>i</mi><mi>p</mi></mrow></msub>\
    \ <mrow><mo>(</mo><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo>)</mo></mrow></mrow>\
    \ <mrow><msub><mo>∑</mo> <mrow><mi>s</mi><mi>n</mi><mi>t</mi><mi>â</mi><mi>\x80\
    </mi><mi>\x99</mi><mo>∈</mo><mi>C</mi></mrow></msub> <msub><mo>∑</mo> <mrow><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo>∈</mo><mi>s</mi><mi>n</mi><msup><mi>t</mi>\
    \ <mo>'</mo></msup></mrow></msub> <mi>C</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mrow><mo>(</mo><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>"
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re almost there. Since we are not looking at recall, all generated sequences
    that are short but precise have a benefit compared to sentences that are longer.
    Therefore, the precision score favors short generations. To compensate for that
    the authors of BLEU introduced an additional term, the *brevity penalty*:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper B upper R equals min left-parenthesis 1 comma e Superscript
    1 minus script l Super Subscript r e f Superscript slash script l Super Subscript
    g e n Superscript Baseline right-parenthesis" display="block"><mrow><mi>B</mi>
    <mi>R</mi> <mo>=</mo> <mo movablelimits="true" form="prefix">min</mo> <mfenced
    separators="" open="(" close=")"><mn>1</mn> <mo>,</mo> <msup><mi>e</mi> <mrow><mn>1</mn><mo>-</mo><msub><mi>ℓ</mi>
    <mrow><mi>r</mi><mi>e</mi><mi>f</mi></mrow></msub> <mo>/</mo><msub><mi>ℓ</mi>
    <mrow><mi>g</mi><mi>e</mi><mi>n</mi></mrow></msub></mrow></msup></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: By taking the minimum, we ensure that this penalty never exceeds 1 and the exponential
    term becomes exponentially small when the length of the generated text <math alttext="l
    Subscript g e n"><msub><mi>l</mi> <mrow><mi>g</mi><mi>e</mi><mi>n</mi></mrow></msub></math>
    is smaller than the reference text <math alttext="l Subscript r e f"><msub><mi>l</mi>
    <mrow><mi>r</mi><mi>e</mi><mi>f</mi></mrow></msub></math> . At this point you
    might ask, why don’t we just use something like an *F*[1]-score to account for
    recall as well? The answer is that often in translation datasets there are multiple
    reference sentences instead of just one, so if we also measured recall we would
    incentivize translations that used all the words from all the references. Therefore,
    it’s preferable to look for high precision in the translation and make sure the
    translation and reference have a similar length.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can put everything together and get the equation for the BLEU score:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="" display="block"><mrow><mtext>BLEU-</mtext><mi mathvariant="italic">N</mi>
    <mo>=</mo> <mi>B</mi> <mi>R</mi> <mo>×</mo> <msup><mfenced separators="" open="("
    close=")"><munderover><mo>∏</mo> <mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi></munderover>
    <msub><mi>p</mi> <mi>n</mi></msub></mfenced> <mrow><mn>1</mn><mo>/</mo><mi>N</mi></mrow></msup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The last term is the geometric mean of the modified precision up to *n*-gram
    *N*. In practice, the BLEU-4 score is often reported. However, you can probably
    already see that this metric has many limitations; for instance, it doesn’t take
    synonyms into account, and many steps in the derivation seem like ad hoc and rather
    fragile heuristics. You can find a wonderful exposition of BLEU’s flaws in Rachel
    Tatman’s blog post [“Evaluating Text Output in NLP: BLEU at Your Own Risk”](https://oreil.ly/nMXRh).'
  prefs: []
  type: TYPE_NORMAL
- en: In general, the field of text generation is still looking for better evaluation
    metrics, and finding ways to overcome the limits of metrics like BLEU is an active
    area of research. Another weakness of the BLEU metric is that it expects the text
    to already be tokenized. This can lead to varying results if the exact same method
    for text tokenization is not used. The SacreBLEU metric addresses this issue by
    internalizing the tokenization step; for this reason, it is the preferred metric
    for benchmarking.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve now worked through some theory, but what we really want to do is calculate
    the score for some generated text. Does that mean we need to implement all this
    logic in Python? Fear not, ![nlpt_pin01](Images/nlpt_pin01.png) Datasets also
    provides metrics! Loading a metric works just like loading a dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The `bleu_metric` object is an instance of the `Metric` class, and works like
    an aggregator: you can add single instances with `add()` or whole batches via
    `add_batch()`. Once you have added all the samples you need to evaluate, you then
    call `compute()` and the metric is calculated. This returns a dictionary with
    several values, such as the precision for each *n*-gram, the length penalty, as
    well as the final BLEU score. Let’s look at the example from before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Value |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| score | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| counts | [2, 0, 0, 0] |'
  prefs: []
  type: TYPE_TB
- en: '| totals | [6, 5, 4, 3] |'
  prefs: []
  type: TYPE_TB
- en: '| precisions | [33.33, 0.0, 0.0, 0.0] |'
  prefs: []
  type: TYPE_TB
- en: '| bp | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| sys_len | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| ref_len | 6 |'
  prefs: []
  type: TYPE_TB
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The BLEU score also works if there are multiple reference translations. This
    is why `reference` is passed as a list. To make the metric smoother for zero counts
    in the *n*-grams, BLEU integrates methods to modify the precision calculation.
    One method is to add a constant to the numerator. That way, a missing *n*-gram
    does not cause the score to automatically go to zero. For the purpose of explaining
    the values, we turn it off by setting `smooth_value=0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see the precision of the 1-gram is indeed 2/6, whereas the precisions
    for the 2/3/4-grams are all 0\. (For more information about the individual metrics,
    like counts and bp, see the [SacreBLEU repository](https://oreil.ly/kiZPl).) This
    means the geometric mean is zero, and thus also the BLEU score. Let’s look at
    another example where the prediction is almost correct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Value |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| score | 57.893007 |'
  prefs: []
  type: TYPE_TB
- en: '| counts | [5, 3, 2, 1] |'
  prefs: []
  type: TYPE_TB
- en: '| totals | [5, 4, 3, 2] |'
  prefs: []
  type: TYPE_TB
- en: '| precisions | [100.0, 75.0, 66.67, 50.0] |'
  prefs: []
  type: TYPE_TB
- en: '| bp | 0.818731 |'
  prefs: []
  type: TYPE_TB
- en: '| sys_len | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| ref_len | 6 |'
  prefs: []
  type: TYPE_TB
- en: We observe that the precision scores are much better. The 1-grams in the prediction
    all match, and only in the precision scores do we see that something is off. For
    the 4-gram there are only two candidates, `["the", "cat", "is", "on"]` and `["cat",
    "is", "on", "mat"]`, where the last one does not match, hence the precision of
    0.5.
  prefs: []
  type: TYPE_NORMAL
- en: The BLEU score is widely used for evaluating text, especially in machine translation,
    since precise translations are usually favored over translations that include
    all possible and appropriate words.
  prefs: []
  type: TYPE_NORMAL
- en: There are other applications, such as summarization, where the situation is
    different. There, we want all the important information in the generated text,
    so we favor high recall. This is where the ROUGE score is usually used.
  prefs: []
  type: TYPE_NORMAL
- en: ROUGE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The ROUGE score was specifically developed for applications like summarization
    where high recall is more important than just precision.^([5](ch06.xhtml#idm46238716507664))
    The approach is very similar to the BLEU score in that we look at different *n*-grams
    and compare their occurrences in the generated text and the reference texts. The
    difference is that with ROUGE we check how many *n*-grams in the reference text
    also occur in the generated text. For BLEU we looked at how many *n*-grams in
    the generated text appear in the reference, so we can reuse the precision formula
    with the minor modification that we count the (unclipped) occurrence of reference
    *n*-grams in the generated text in the numerator:'
  prefs: []
  type: TYPE_NORMAL
- en: "<math alttext=\"\" display=\"block\"><mrow><mtext>ROUGE-</mtext><mi mathvariant=\"\
    italic\">N</mi> <mo>=</mo> <mfrac><mrow><msub><mo>∑</mo> <mrow><mi>snt’</mi><mi>\x80\
    </mi><mi>\x99</mi><mo>∈</mo><mi>C</mi></mrow></msub> <msub><mo>∑</mo> <mrow><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo>∈</mo><mi>s</mi><mi>n</mi><msup><mi>t</mi>\
    \ <mo>'</mo></msup></mrow></msub> <mi>C</mi><mi>o</mi><mi>u</mi><mi>n</mi><msub><mi>t</mi>\
    \ <mrow><mi>m</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi></mrow></msub> <mrow><mo>(</mo><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo>)</mo></mrow></mrow>\
    \ <mrow><msub><mo>∑</mo> <mrow><mi>snt’</mi><mi>\x80</mi><mi>\x99</mi><mo>∈</mo><mi>C</mi></mrow></msub>\
    \ <msub><mo>∑</mo> <mrow><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo>∈</mo><mi>s</mi><mi>n</mi><msup><mi>t</mi>\
    \ <mo>'</mo></msup></mrow></msub> <mi>C</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mrow><mo>(</mo><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>"
  prefs: []
  type: TYPE_NORMAL
- en: This was the original proposal for ROUGE. Subsequently, researchers have found
    that fully removing precision can have strong negative effects. Going back to
    the BLEU formula without the clipped counting, we can measure precision as well,
    and we can then combine both precision and recall ROUGE scores in the harmonic
    mean to get an *F*[1]-score. This score is the metric that is nowadays commonly
    reported for ROUGE.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a separate score in ROUGE to measure the longest common substring
    (LCS), called ROUGE-L. The LCS can be calculated for any pair of strings. For
    example, the LCS for “abab” and “abc” would be “ab”, and its the length would
    be 2\. If we want to compare this value between two samples we need to somehow
    normalize it because otherwise a longer text would be at an advantage. To achieve
    this, the inventor of ROUGE came up with an <math alttext="upper F"><mi>F</mi></math>
    -score-like scheme where the LCS is normalized with the length of the reference
    and generated text, then the two normalized scores are mixed together:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper R Subscript upper L upper C upper S Baseline equals StartFraction
    upper L upper C upper S left-parenthesis upper X comma upper Y right-parenthesis
    Over m EndFraction" display="block"><mrow><msub><mi>R</mi> <mrow><mi>L</mi><mi>C</mi><mi>S</mi></mrow></msub>
    <mo>=</mo> <mfrac><mrow><mi>L</mi><mi>C</mi><mi>S</mi><mo>(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo>)</mo></mrow>
    <mi>m</mi></mfrac></mrow></math><math alttext="upper P Subscript upper L upper
    C upper S Baseline equals StartFraction upper L upper C upper S left-parenthesis
    upper X comma upper Y right-parenthesis Over n EndFraction" display="block"><mrow><msub><mi>P</mi>
    <mrow><mi>L</mi><mi>C</mi><mi>S</mi></mrow></msub> <mo>=</mo> <mfrac><mrow><mi>L</mi><mi>C</mi><mi>S</mi><mo>(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo>)</mo></mrow>
    <mi>n</mi></mfrac></mrow></math><math alttext="upper F Subscript upper L upper
    C upper S Baseline equals StartFraction left-parenthesis 1 plus beta squared right-parenthesis
    upper R Subscript upper L upper C upper S Baseline upper P Subscript upper L upper
    C upper S Baseline Over upper R Subscript upper L upper C upper S Baseline plus
    beta upper P Subscript upper L upper C upper S Baseline EndFraction comma where
    beta equals upper P Subscript upper L upper C upper S Baseline slash upper R Subscript
    upper L upper C upper S Baseline" display="block"><mrow><msub><mi>F</mi> <mrow><mi>L</mi><mi>C</mi><mi>S</mi></mrow></msub>
    <mo>=</mo> <mfrac><mrow><mrow><mo>(</mo><mn>1</mn><mo>+</mo><msup><mi>β</mi> <mn>2</mn></msup>
    <mo>)</mo></mrow><msub><mi>R</mi> <mrow><mi>L</mi><mi>C</mi><mi>S</mi></mrow></msub>
    <msub><mi>P</mi> <mrow><mi>L</mi><mi>C</mi><mi>S</mi></mrow></msub></mrow> <mrow><msub><mi>R</mi>
    <mrow><mi>L</mi><mi>C</mi><mi>S</mi></mrow></msub> <mo>+</mo><mi>β</mi><msub><mi>P</mi>
    <mrow><mi>L</mi><mi>C</mi><mi>S</mi></mrow></msub></mrow></mfrac> <mo>,</mo> <mtext>where</mtext>
    <mi>β</mi> <mo>=</mo> <msub><mi>P</mi> <mrow><mi>L</mi><mi>C</mi><mi>S</mi></mrow></msub>
    <mo>/</mo> <msub><mi>R</mi> <mrow><mi>L</mi><mi>C</mi><mi>S</mi></mrow></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'That way the LCS score is properly normalized and can be compared across samples.
    In the ![nlpt_pin01](Images/nlpt_pin01.png) Datasets implementation, two variations
    of ROUGE are calculated: one calculates the score per sentence and averages it
    for the summaries (ROUGE-L), and the other calculates it directly over the whole
    summary (ROUGE-Lsum).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can load the metric as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We already generated a set of summaries with GPT-2 and the other models, and
    now we have a metric to compare the summaries systematically. Let’s apply the
    ROUGE score to all the summaries generated by the models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '|  | rouge1 | rouge2 | rougeL | rougeLsum |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| baseline | 0.303571 | 0.090909 | 0.214286 | 0.232143 |'
  prefs: []
  type: TYPE_TB
- en: '| gpt2 | 0.187500 | 0.000000 | 0.125000 | 0.187500 |'
  prefs: []
  type: TYPE_TB
- en: '| t5 | 0.486486 | 0.222222 | 0.378378 | 0.486486 |'
  prefs: []
  type: TYPE_TB
- en: '| bart | 0.582278 | 0.207792 | 0.455696 | 0.506329 |'
  prefs: []
  type: TYPE_TB
- en: '| pegasus | 0.866667 | 0.655172 | 0.800000 | 0.833333 |'
  prefs: []
  type: TYPE_TB
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The ROUGE metric in the ![nlpt_pin01](Images/nlpt_pin01.png) Datasets library
    also calculates confidence intervals (by default, the 5th and 95th percentiles).
    The average value is stored in the attribute `mid` and the interval can be retrieved
    with `low` and `high`.
  prefs: []
  type: TYPE_NORMAL
- en: These results are obviously not very reliable as we only looked at a single
    sample, but we can compare the quality of the summary for that one example. The
    table confirms our observation that of the models we considered, GPT-2 performs
    worst. This is not surprising since it is the only model of the group that was
    not explicitly trained to summarize. It is striking, however, that the simple
    first-three-sentence baseline doesn’t fare too poorly compared to the transformer
    models that have on the order of a billion parameters! PEGASUS and BART are the
    best models overall (higher ROUGE scores are better), but T5 is slightly better
    on ROUGE-1 and the LCS scores. These results place T5 and PEGASUS as the best
    models, but again these results should be treated with caution as we only evaluated
    the models on a single example. Looking at the results in the PEGASUS paper, we
    would expect the PEGASUS to outperform T5 on the CNN/DailyMail dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see if we can reproduce those results with PEGASUS.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating PEGASUS on the CNN/DailyMail Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We now have all the pieces in place to evaluate the model properly: we have
    a dataset with a test set from CNN/DailyMail, we have a metric with ROUGE, and
    we have a summarization model. We just need to put the pieces together. Let’s
    first evaluate the performance of the three-sentence baseline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we’ll apply the function to a subset of the data. Since the test fraction
    of the CNN/DailyMail dataset consists of roughly 10,000 samples, generating summaries
    for all these articles takes a lot of time. Recall from [Chapter 5](ch05.xhtml#chapter_generation)
    that every generated token requires a forward pass through the model; generating
    just 100 tokens for each sample will thus require 1 million forward passes, and
    if we use beam search this number is multiplied by the number of beams. For the
    purpose of keeping the calculations relatively fast, we’ll subsample the test
    set and run the evaluation on 1,000 samples instead. This should give us a much
    more stable score estimation while completing in less than one hour on a single
    GPU for the PEGASUS model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '|  | rouge1 | rouge2 | rougeL | rougeLsum |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| baseline | 0.396061 | 0.173995 | 0.245815 | 0.361158 |'
  prefs: []
  type: TYPE_TB
- en: 'The scores are mostly worse than on the previous example, but still better
    than those achieved by GPT-2! Now let’s implement the same evaluation function
    for evaluating the PEGASUS model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s unpack this evaluation code a bit. First we split the dataset into smaller
    batches that we can process simultaneously. Then for each batch we tokenize the
    input articles and feed them to the `generate()` function to produce the summaries
    using beam search. We use the same generation parameters as proposed in the paper.
    The new parameter for length penalty ensures that the model does not generate
    sequences that are too long. Finally, we decode the generated texts, replace the
    `<n>` token, and add the decoded texts with the references to the metric. At the
    end, we compute and return the ROUGE scores. Let’s now load the model again with
    the `AutoModelForSeq2SeqLM` class, used for seq2seq generation tasks, and evaluate
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '|  | rouge1 | rouge2 | rougeL | rougeLsum |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| pegasus | 0.434381 | 0.210883 | 0.307195 | 0.373231 |'
  prefs: []
  type: TYPE_TB
- en: These numbers are very close to the published results. One thing to note here
    is that the loss and per-token accuracy are decoupled to some degree from the
    ROUGE scores. The loss is independent of the decoding strategy, whereas the ROUGE
    score is strongly coupled.
  prefs: []
  type: TYPE_NORMAL
- en: Since ROUGE and BLEU correlate better with human judgment than loss or accuracy,
    we should focus on them and carefully explore and choose the decoding strategy
    when building text generation models. These metrics are far from perfect, however,
    and one should always consider human judgments as well.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’re equipped with an evaluation function, it’s time to train our
    own model for summarization.
  prefs: []
  type: TYPE_NORMAL
- en: Training a Summarization Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We’ve worked through a lot of details on text summarization and evaluation,
    so let’s put this to use to train a custom text summarization model! For our application,
    we’ll use the [SAMSum dataset](https://oreil.ly/n1ggq) developed by Samsung, which
    consists of a collection of dialogues along with brief summaries. In an enterprise
    setting, these dialogues might represent the interactions between a customer and
    the support center, so generating accurate summaries can help improve customer
    service and detect common patterns among customer requests. Let’s load it and
    look at an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The dialogues look like what you would expect from a chat via SMS or WhatsApp,
    including emojis and placeholders for GIFs. The `dialogue` field contains the
    full text and the `summary` the summarized dialogue. Could a model that was fine-tuned
    on the CNN/DailyMail dataset deal with that? Let’s find out!
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating PEGASUS on SAMSum
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First we’ll run the same summarization pipeline with PEGASUS to see what the
    output looks like. We can reuse the code we used for the CNN/DailyMail summary
    generation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the model mostly tries to summarize by extracting the key sentences
    from the dialogue. This probably worked relatively well on the CNN/DailyMail dataset,
    but the summaries in SAMSum are more abstract. Let’s confirm this by running the
    full ROUGE evaluation on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '|  | rouge1 | rouge2 | rougeL | rougeLsum |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| pegasus | 0.296168 | 0.087803 | 0.229604 | 0.229514 |'
  prefs: []
  type: TYPE_TB
- en: 'Well, the results aren’t great, but this is not unexpected since we’ve moved
    quite a bit away from the CNN/DailyMail data distribution. Nevertheless, setting
    up the evaluation pipeline before training has two advantages: we can directly
    measure the success of training with the metric and we have a good baseline. Fine-tuning
    the model on our dataset should result in an immediate improvement in the ROUGE
    metric, and if that is not the case we’ll know something is wrong with our training
    loop.'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-Tuning PEGASUS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we process the data for training, let’s have a quick look at the length
    distribution of the input and outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/nlpt_06in01.png)'
  prefs: []
  type: TYPE_IMG
- en: We see that most dialogues are much shorter than the CNN/DailyMail articles,
    with 100–200 tokens per dialogue. Similarly, the summaries are much shorter, with
    around 20–40 tokens (the average length of a tweet).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s keep those observations in mind as we build the data collator for the
    `Trainer`. First we need to tokenize the dataset. For now, we’ll set the maximum
    lengths to 1024 and 128 for the dialogues and summaries, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: A new thing in the use of the tokenization step is the `tokenizer.as_target_tokenizer()`
    context. Some models require special tokens in the decoder inputs, so it’s important
    to differentiate between the tokenization of encoder and decoder inputs. In the
    `with` statement (called a *context manager*), the tokenizer knows that it is
    tokenizing for the decoder and can process sequences accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to create the data collator. This function is called in the `Trainer`
    just before the batch is fed through the model. In most cases we can use the default
    collator, which collects all the tensors from the batch and simply stacks them.
    For the summarization task we need to not only stack the inputs but also prepare
    the targets on the decoder side. PEGASUS is an encoder-decoder transformer and
    thus has the classic seq2seq architecture. In a seq2seq setup, a common approach
    is to apply “teacher forcing” in the decoder. With this strategy, the decoder
    receives input tokens (like in decoder-only models such as GPT-2) that consists
    of the labels shifted by one in addition to the encoder output; so, when making
    the prediction for the next token the decoder gets the ground truth shifted by
    one as an input, as illustrated in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | decoder_input | label |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| step |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | [PAD] | Transformers |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | [PAD, Transformers] | are |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | [PAD, Transformers, are] | awesome |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | [PAD, Transformers, are, awesome] | for |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | [PAD, Transformers, are, awesome, for] | text |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | [PAD, Transformers, are, awesome, for, text] | summarization |'
  prefs: []
  type: TYPE_TB
- en: We shift it by one so that the decoder only sees the previous ground truth labels
    and not the current or future ones. Shifting alone suffices since the decoder
    has masked self-attention that masks all inputs at present and in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, when we prepare our batch, we set up the decoder inputs by shifting the
    labels to the right by one. After that, we make sure the padding tokens in the
    labels are ignored by the loss function by setting them to –100\. We actually
    don’t have to do this manually, though, since the `DataCollatorForSeq2Seq` comes
    to the rescue and takes care of all these steps for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, as usual, we set up a the `TrainingArguments` for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: One thing that is different from the previous settings is that new argument,
    `gradient_accumulation_steps`. Since the model is quite big, we had to set the
    batch size to 1\. However, a batch size that is too small can hurt convergence.
    To resolve that issue, we can use a nifty technique called *gradient accumulation*.
    As the name suggests, instead of calculating the gradients of the full batch all
    at once, we make smaller batches and aggregate the gradients. When we have aggregated
    enough gradients, we run the optimization step. Naturally this is a bit slower
    than doing it in one pass, but it saves us a lot of GPU memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now make sure that we are logged in to Hugging Face so we can push the
    model to the Hub after training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We have now everything we need to initialize the trainer with the model, tokenizer,
    training arguments, and data collator, as well as the training and evaluation
    sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We are ready for training. After training, we can directly run the evaluation
    function on the test set to see how well the model performs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '|  | rouge1 | rouge2 | rougeL | rougeLsum |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| pegasus | 0.427614 | 0.200571 | 0.340648 | 0.340738 |'
  prefs: []
  type: TYPE_TB
- en: 'We see that the ROUGE scores improved considerably over the model without fine-tuning,
    so even though the previous model was also trained for summarization, it was not
    well adapted for the new domain. Let’s push our model to the Hub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: In the next section we’ll use the model to generate a few summaries for us.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You can also evaluate the generations as part of the training loop: use the
    extension of `TrainingArguments` called `Seq2Seq​Trainin⁠g​Arguments` and specify
    `predict_with_generate=True`. Pass it to the dedicated `Trainer` called `Seq2SeqTrainer`,
    which then uses the `generate()` function instead of the model’s forward pass
    to create predictions for evaluation. Give it a try!'
  prefs: []
  type: TYPE_NORMAL
- en: Generating Dialogue Summaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Looking at the losses and ROUGE scores, it seems the model is showing a significant
    improvement over the original model trained on CNN/DailyMail only. Let’s see what
    a summary generated on a sample from the test set looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'That looks much more like the reference summary. It seems the model has learned
    to synthesize the dialogue into a summary without just extracting passages. Now,
    the ultimate test: how well does the model work on a custom input?'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The generated summary of the custom dialogue makes sense. It summarizes well
    that all the people in the discussion want to write the book together and does
    not simply extract single sentences. For example, it synthesizes the third and
    fourth lines into a logical combination.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text summarization poses some unique challenges compared to other tasks that
    can be framed as classification tasks, like sentiment analysis, named entity recognition,
    or question answering. Conventional metrics such as accuracy do not reflect the
    quality of the generated text. As we saw, the BLEU and ROUGE metrics can better
    evaluate generated texts; however, human judgment remains the best measure.
  prefs: []
  type: TYPE_NORMAL
- en: A common question when working with summarization models is how we can summarize
    documents where the texts are longer than the model’s context length. Unfortunately,
    there is no single strategy to solve this problem, and to date this is still an
    open and active research question. For example, recent work by OpenAI showed how
    to scale summarization by applying it recursively to long documents and using
    human feedback in the loop.^([6](ch06.xhtml#idm46238714384416))
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter we’ll look at question answering, which is the task of providing
    an answer to a question based on a text passage. In contrast to summarization,
    with this task there exist good strategies to deal with long or many documents,
    and we’ll show you how to scale question answering to thousands of documents.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch06.xhtml#idm46238717347776-marker)) A. Radford et al., [“Language Models
    Are Unsupervised Multitask Learners”](https://openai.com/blog/better-language-models),
    OpenAI (2019).
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch06.xhtml#idm46238717146864-marker)) M. Lewis et al., [“BART: Denoising
    Sequence-to-Sequence Pre-training for Natural Language Generation, Translation,
    and Comprehension”](https://arxiv.org/abs/1910.13461), (2019).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([3](ch06.xhtml#idm46238717078848-marker)) J. Zhang et al., [“PEGASUS: Pre-Training
    with Extracted Gap-Sentences for Abstractive Summarization”](https://arxiv.org/abs/1912.08777),
    (2019).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([4](ch06.xhtml#idm46238716919888-marker)) K. Papineni et al., “BLEU: A Method
    for Automatic Evaluation of Machine Translation,” *Proceedings of the 40th Annual
    Meeting of the Association for Computational Linguistics* (July 2002): 311–318,
    [*http://dx.doi.org/10.3115/1073083.1073135*](http://dx.doi.org/10.3115/1073083.1073135).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([5](ch06.xhtml#idm46238716507664-marker)) C-Y. Lin, “ROUGE: A Package for
    Automatic Evaluation of Summaries,” *Text Summarization Branches Out* (July 2004),
    [*https://aclanthology.org/W04-1013.pdf*](https://aclanthology.org/W04-1013.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch06.xhtml#idm46238714384416-marker)) J. Wu et al., [“Recursively Summarizing
    Books with Human Feedback”](https://arxiv.org/abs/2109.10862), (2021).
  prefs: []
  type: TYPE_NORMAL
