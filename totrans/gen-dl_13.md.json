["```py\nbash scripts/download_kaggle_data.sh zynicide wine-reviews\n```", "```py\nThe pink elephant tried to get into the car but it was too\n```", "```py\nlayers.MultiHeadAttention(\n    num_heads = 4, ![1](Images/1.png)\n    key_dim = 128, ![2](Images/2.png)\n    value_dim = 64, ![3](Images/3.png)\n    output_shape = 256 ![4](Images/4.png)\n    )\n```", "```py\ndef causal_attention_mask(batch_size, n_dest, n_src, dtype):\n    i = tf.range(n_dest)[:, None]\n    j = tf.range(n_src)\n    m = i >= j - n_src + n_dest\n    mask = tf.cast(m, dtype)\n    mask = tf.reshape(mask, [1, n_dest, n_src])\n    mult = tf.concat(\n        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n    )\n    return tf.tile(mask, mult)\n\nnp.transpose(causal_attention_mask(1, 10, 10, dtype = tf.int32)[0])\n```", "```py\nclass TransformerBlock(layers.Layer):\n    def __init__(self, num_heads, key_dim, embed_dim, ff_dim, dropout_rate=0.1): ![1](Images/1.png)\n        super(TransformerBlock, self).__init__()\n        self.num_heads = num_heads\n        self.key_dim = key_dim\n        self.embed_dim = embed_dim\n        self.ff_dim = ff_dim\n        self.dropout_rate = dropout_rate\n        self.attn = layers.MultiHeadAttention(\n            num_heads, key_dim, output_shape = embed_dim\n        )\n        self.dropout_1 = layers.Dropout(self.dropout_rate)\n        self.ln_1 = layers.LayerNormalization(epsilon=1e-6)\n        self.ffn_1 = layers.Dense(self.ff_dim, activation=\"relu\")\n        self.ffn_2 = layers.Dense(self.embed_dim)\n        self.dropout_2 = layers.Dropout(self.dropout_rate)\n        self.ln_2 = layers.LayerNormalization(epsilon=1e-6)\n\n    def call(self, inputs):\n        input_shape = tf.shape(inputs)\n        batch_size = input_shape[0]\n        seq_len = input_shape[1]\n        causal_mask = causal_attention_mask(\n            batch_size, seq_len, seq_len, tf.bool\n        ) ![2](Images/2.png)\n        attention_output, attention_scores = self.attn(\n            inputs,\n            inputs,\n            attention_mask=causal_mask,\n            return_attention_scores=True\n        ) ![3](Images/3.png)\n        attention_output = self.dropout_1(attention_output)\n        out1 = self.ln_1(inputs + attention_output) ![4](Images/4.png)\n        ffn_1 = self.ffn_1(out1) ![5](Images/5.png)\n        ffn_2 = self.ffn_2(ffn_1)\n        ffn_output = self.dropout_2(ffn_2)\n        return (self.ln_2(out1 + ffn_output), attention_scores) ![6](Images/6.png)\n```", "```py\nclass TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super(TokenAndPositionEmbedding, self).__init__()\n        self.maxlen = maxlen\n        self.vocab_size =vocab_size\n        self.embed_dim = embed_dim\n        self.token_emb = layers.Embedding(\n            input_dim=vocab_size, output_dim=embed_dim\n        ) ![1](Images/1.png)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim) ![2](Images/2.png)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return x + positions ![3](Images/3.png)\n```", "```py\nMAX_LEN = 80\nVOCAB_SIZE = 10000\nEMBEDDING_DIM = 256\nN_HEADS = 2\nKEY_DIM = 256\nFEED_FORWARD_DIM = 256\n\ninputs = layers.Input(shape=(None,), dtype=tf.int32) ![1](Images/1.png)\nx = TokenAndPositionEmbedding(MAX_LEN, VOCAB_SIZE, EMBEDDING_DIM)(inputs) ![2](Images/2.png)\nx, attention_scores = TransformerBlock(\n    N_HEADS, KEY_DIM, EMBEDDING_DIM, FEED_FORWARD_DIM\n)(x) ![3](Images/3.png)\noutputs = layers.Dense(VOCAB_SIZE, activation = 'softmax')(x) ![4](Images/4.png)\ngpt = models.Model(inputs=inputs, outputs=[outputs, attention]) ![5](Images/5.png)\ngpt.compile(\"adam\", loss=[losses.SparseCategoricalCrossentropy(), None]) ![6](Images/6.png)\ngpt.fit(train_ds, epochs=5)\n```"]