- en: Fundamentals of machine learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习基础
- en: 原文：[https://deeplearningwithpython.io/chapters/chapter05_fundamentals-of-ml](https://deeplearningwithpython.io/chapters/chapter05_fundamentals-of-ml)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://deeplearningwithpython.io/chapters/chapter05_fundamentals-of-ml](https://deeplearningwithpython.io/chapters/chapter05_fundamentals-of-ml)
- en: 'After the three practical examples in chapter 4, you should be starting to
    feel familiar with how to approach classification and regression problems using
    neural networks, and you’ve witnessed the central problem of machine learning:
    overfitting. This chapter will formalize some of your new intuition about machine
    learning into a solid conceptual framework, highlighting the importance of accurate
    model evaluation and the balance between training and generalization.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4章的三个实际例子之后，你应该开始熟悉如何使用神经网络来处理分类和回归问题，并且你已经见证了机器学习的核心问题：过拟合。本章将把你对机器学习的新直觉正式化为一个坚实的概念框架，强调准确模型评估和训练与泛化之间平衡的重要性。
- en: 'Generalization: The goal of machine learning'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 泛化：机器学习的目标
- en: 'In the three examples presented in chapter 4 — predicting movie reviews, topic
    classification, and house-price regression — we split the data into a training
    set, a validation set, and a test set. The reason not to evaluate the models on
    the same data they were trained on quickly became evident: after just a few epochs,
    performance on never-before-seen data started diverging from performance on the
    training data, which always improves as training progresses. The models started
    to *overfit*. Overfitting happens in every machine-learning problem.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4章中提出的三个例子——预测电影评论、主题分类和房价回归中，我们将数据分为训练集、验证集和测试集。不要在训练模型的数据上评估模型的原因很快变得明显：仅仅经过几个epoch，从未见过的数据上的性能就开始与训练数据上的性能分离，而训练数据随着训练的进行总是提高。模型开始*过拟合*。过拟合发生在每个机器学习问题中。
- en: The fundamental issue in machine learning is the tension between optimization
    and generalization. *Optimization* refers to the process of adjusting a model
    to get the best performance possible on the training data (the *learning* in *machine
    learning*), whereas *generalization* refers to how well the trained model performs
    on data it has never seen before. The goal of the game is to get good generalization,
    of course, but you don’t control generalization; you can only fit the model to
    its training data. If you do that *too well*, overfitting kicks in and generalization
    suffers.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的核心问题是优化和泛化之间的张力。*优化*指的是调整模型以在训练数据上获得最佳性能的过程（在*机器学习*中的*学习*），而*泛化*指的是训练好的模型在之前未见过的数据上的表现。当然，游戏的目标是获得良好的泛化，但你无法控制泛化；你只能使模型适应其训练数据。如果你做得*太好了*，过拟合就会发生，泛化就会受到影响。
- en: But what causes overfitting? How can we achieve good generalization?
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 但是什么导致了过拟合？我们如何实现良好的泛化？
- en: Underfitting and overfitting
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 欠拟合和过拟合
- en: For all models you’ve seen in the previous chapter, performance on the held-out
    validation data initially improved as training went on and then inevitably peaked
    after a while. This pattern (illustrated in figure 5.1) is universal. You’ll see
    it with any model type and any dataset.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对于你在上一章中看到的所有模型，随着训练的进行，在保留的验证数据上的性能最初会提高，然后不可避免地会在一段时间后达到顶峰。这种模式（如图5.1所示）是普遍存在的。你会在任何模型类型和任何数据集上看到它。
- en: '![](../Images/5d6317dd0e52bc77ae547bca0f76c879.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5d6317dd0e52bc77ae547bca0f76c879.png)'
- en: '[Figure 5.1](#figure-5-1): Canonical overfitting behavior'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5.1](#figure-5-1)：典型的过拟合行为'
- en: 'At the beginning of training, optimization and generalization are correlated:
    the lower the loss on training data, the lower the loss on test data. While this
    is happening, your model is said to be *underfit*: there is still progress to
    be made; the network hasn’t yet modeled all relevant patterns in the training
    data. But after a certain number of iterations on the training data, generalization
    stops improving, and validation metrics stall and then begin to degrade: the model
    is starting to overfit. That is, it’s beginning to learn patterns that are specific
    to the training data but that are misleading or irrelevant when it comes to new
    data.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练的初期，优化和泛化是相关的：训练数据上的损失越低，测试数据上的损失也越低。在这个过程中，你的模型被认为是*欠拟合*的：还有进步的空间；网络还没有对训练数据中的所有相关模式进行建模。但经过一定次数的训练数据迭代后，泛化不再提高，验证指标停滞并开始下降：模型开始过拟合。也就是说，它开始学习特定于训练数据的模式，但这些模式在处理新数据时可能是误导性的或不相关的。
- en: Overfitting is particularly likely to occur when your data is noisy, if it involves
    uncertainty, or if it includes rare features. Let’s look at concrete examples.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据存在噪声、涉及不确定性或包含罕见特征时，过拟合尤其可能发生。让我们看看具体的例子。
- en: Noisy training data
  id: totrans-13
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 噪声训练数据
- en: In real-world datasets, it’s fairly common for some inputs to be invalid. Perhaps
    a MNIST digit could be an all-black image, for instance — or something like figure
    5.2.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的数据集中，某些输入无效的情况相当常见。例如，MNIST数字可能是一张全黑的图像，或者类似于图5.2中的某些东西。
- en: '![](../Images/048b802dd836f05362475154ee9aaa4a.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/048b802dd836f05362475154ee9aaa4a.png)'
- en: '[Figure 5.2](#figure-5-2): Some pretty weird MNIST training samples'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5.2](#figure-5-2)：一些非常奇怪的MNIST训练样本'
- en: What are these? We don’t know either. But they’re all part of the MNIST training
    set. What’s even worse, however, is having perfectly valid inputs that end up
    mislabeled, like those shown in figure 5.3.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是什么？我们也不知道。但它们都是MNIST训练集的一部分。然而，更糟糕的是，有些完全有效的输入最终被错误标注，如图5.3中所示。
- en: '![](../Images/aca83ea1f5cd193a250e1a6bfb9dec30.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aca83ea1f5cd193a250e1a6bfb9dec30.png)'
- en: '[Figure 5.3](#figure-5-3): Mislabeled MNIST training samples'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5.3](#figure-5-3)：错误标注的MNIST训练样本'
- en: If a model goes out of its way to incorporate such outliers, its generalization
    performance will degrade, as shown in figure 5.4. For instance, a 4 that looks
    very close to the mislabeled 4 in figure 5.3 may end up getting classified as
    a 9.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个模型特意包含这样的异常值，其泛化性能将下降，如图5.4所示。例如，一个看起来非常接近图5.3中错误标注的4的4，最终可能被归类为9。
- en: '![](../Images/66508074b4d77b9fb75bb21b26f0c133.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66508074b4d77b9fb75bb21b26f0c133.png)'
- en: '[Figure 5.4](#figure-5-4): Dealing with outliers: robust fit vs. overfitting'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5.4](#figure-5-4)：处理异常值：鲁棒拟合与过拟合'
- en: Ambiguous features
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模糊特征
- en: Not all data noise comes from inaccuracies — even perfectly clean and neatly
    labeled data can be noisy when the problem involves uncertainty and ambiguity
    (see figure 5.5). In classification tasks, it is often the case that some regions
    of the input feature space are associated with multiple classes at the same time.
    Let’s say you’re developing a model that takes an image of a banana and predicts
    whether the banana is unripened, ripe, or rotten. These categories have no objective
    boundaries, so the same picture might be classified as either unripened or ripe
    by different human labelers. Similarly, many problems involve randomness. You
    could use atmospheric pressure data to predict whether it will rain tomorrow,
    but the exact same measurements may be followed sometimes by rain, sometimes by
    a clear sky — with some probability.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有数据噪声都源于不准确——即使非常干净且标签清晰的 数据，当问题涉及不确定性和模糊性时也可能存在噪声（见图5.5）。在分类任务中，输入特征空间的一些区域可能同时与多个类别相关联。假设你正在开发一个模型，该模型接收香蕉的图像并预测香蕉是否未成熟、成熟或腐烂。这些类别没有客观的边界，因此同一张图片可能被不同的标注人员分别归类为未成熟或成熟。同样，许多问题涉及随机性。你可以使用大气压力数据来预测明天是否会下雨，但确切的测量结果有时会伴随着降雨，有时则是晴朗的天空——带有一定的概率。
- en: '![](../Images/2c1d27fcf1e10c783a05aa77a301b691.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2c1d27fcf1e10c783a05aa77a301b691.png)'
- en: '[Figure 5.5](#figure-5-5): Robust fit vs. overfitting giving an ambiguous area
    of the feature space'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5.5](#figure-5-5)：鲁棒拟合与过拟合在特征空间中给出的模糊区域'
- en: A model could overfit to such probabilistic data by being too confident about
    ambiguous regions of the feature space, like in figure 5.6\. A more robust fit
    would ignore individual data points and look at the bigger picture.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一个模型可能会通过过于自信地处理特征空间的模糊区域（如图5.6所示）而对这样的概率数据进行过拟合。更鲁棒的拟合将忽略个别数据点，并关注更大的图景。
- en: Rare features and spurious correlations
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 罕见特征和虚假相关性
- en: 'If you’ve only ever seen two orange tabby cats in your life, and they both
    happened to be terribly antisocial, you might infer that orange tabby cats are
    generally likely to be antisocial. That’s overfitting: if you had been exposed
    to a wider variety of cats, including more orange ones, you’d have learned that
    cat color is not well correlated with character.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你一生中只见过两只橙色虎斑猫，而且它们都极其反社会，你可能会推断橙色虎斑猫通常可能很反社会。这是过拟合：如果你接触过更多种类的猫，包括更多橙色猫，你会了解到猫的颜色与性格并不密切相关。
- en: Likewise, machine learning models trained on datasets that include rare feature
    values are highly susceptible to overfitting. In a sentiment classification task,
    if the word “cherimoya” (a fruit native to the Andes) only appears in one text
    in the training data, and this text happens to be negative in sentiment, a poorly
    regularized model might put a very high weight on this word and always classify
    new texts that mention cherimoyas as negative, whereas, objectively, there’s nothing
    negative about the cherimoya. ^([[1]](#footnote-1))
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在包含罕见特征值的训练数据集上训练的机器学习模型很容易过拟合。在一个情感分类任务中，如果单词“cherimoya”（一种安第斯山脉的本土水果）仅在训练数据中的一篇文本中出现，而这篇文本恰好是负面的情感，那么一个欠规范的模型可能会给这个单词赋予非常高的权重，并总是将提及cherimoyas的新文本分类为负面，而实际上，cherimoya并没有任何负面之处。^([[1]](#footnote-1))
- en: Importantly, a feature value doesn’t need to occur only a couple of times to
    lead to spurious correlations. Consider a word that occurs in 100 samples in your
    training data, and that’s associated with a positive sentiment 54% of the time
    and with a negative sentiment 46% of the time. That difference may well be a complete
    statistical fluke, yet your model is likely to learn to use that feature for its
    classification task. This is one of the most common sources of overfitting.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，一个特征值不需要只出现几次就会导致虚假相关性。考虑一个在训练数据中出现 100 次的单词，54% 的时间与正面情感相关，46% 的时间与负面情感相关。这种差异可能完全是完全的统计巧合，但你的模型很可能会学会使用这个特征来进行其分类任务。这是过拟合最常见的原因之一。
- en: 'Here’s a striking example. Take MNIST. Create a new training set by concatenating
    784 white noise dimensions to the existing 784 dimensions of the data — so half
    of the data is now noise. For comparison, also create an equivalent dataset by
    concatenating 784 all-zeros dimensions. Our concatenation of meaningless features
    does not at all affect the information content of the data: we’re only adding
    irrelevant data points. Human classification accuracy wouldn’t be affected by
    these transformations at all.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个引人注目的例子。以 MNIST 为例。通过将 784 个白噪声维度连接到现有数据的 784 个维度上创建一个新的训练集——因此现在一半的数据是噪声。为了比较，还创建了一个通过连接
    784 个全零维度等效的数据集。我们添加的无意义特征并没有影响数据的信含量：我们只是在添加无关的数据点。人类的分类准确率根本不会受到这些转换的影响。
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[Listing 5.1](#listing-5-1): Adding white noise channels or all-zeros channels
    to MNIST'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 5.1](#listing-5-1)：向 MNIST 添加白噪声通道或全零通道'
- en: Now, let’s train the model from chapter 2 on both of these training sets.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在这两个训练集上训练第 2 章中的模型。
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[Listing 5.2](#listing-5-2): Training the same model on MNIST data with noise
    channels or all-zero channels'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 5.2](#listing-5-2)：在 MNIST 数据上使用噪声通道或全零通道训练相同的模型'
- en: Despite the data holding the same information in both cases, the validation
    accuracy of the model trained with noise channels ends up about one percentage
    point lower — purely through the influence of spurious correlations (figure 5.6).
    The more noise channels you might add, the further accuracy would degrade.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管两种情况下的数据都包含相同的信息，但使用噪声通道训练的模型的验证准确率最终会低约一个百分点——纯粹是通过虚假相关性的影响（图 5.6）。你添加的噪声通道越多，准确率下降得越厉害。
- en: '![](../Images/ba0fabf8a77435e8b3e8434cd0f9204e.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba0fabf8a77435e8b3e8434cd0f9204e.png)'
- en: '[Figure 5.6](#figure-5-6): Effect of noise channels on validation accuracy'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5.6](#figure-5-6)：噪声通道对验证准确率的影响'
- en: Noisy features inevitably lead to overfitting. As such, in cases where you aren’t
    sure whether the features you have are informative or distracting, it’s common
    to do *feature selection* before training. Restricting the IMDB data to the top
    10,000 most common words was a crude form of feature selection, for instance.
    The typical way to do feature selection is to compute some usefulness score for
    each feature available — a measure of how informative the feature is with respect
    to the task, such as the mutual information between the feature and the labels
    — and only keep features that are above some threshold. Doing this would filter
    out the white noise channels in the preceding example.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声特征不可避免地会导致过拟合。因此，在你不确定你拥有的特征是有信息量还是分散注意力的特征时，在训练之前进行特征选择是很常见的。例如，将 IMDB 数据限制在最常见的
    10,000 个单词就是一种粗略的特征选择方法。进行特征选择的典型方法是为每个可用的特征计算一些有用性分数——这是特征相对于任务的信度度量，例如特征与标签之间的互信息——并且只保留高于某个阈值的特征。这样做将过滤掉前面例子中的白噪声通道。
- en: The nature of generalization in deep learning
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度学习中泛化的本质
- en: A remarkable fact about deep learning models is that they can be trained to
    fit anything, as long as they have enough representational power.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型的一个显著事实是，只要它们有足够的表达能力，就可以训练它们去适应任何事物。
- en: Don’t believe me? Try shuffling the order of the MNIST labels and train a model
    on that. Even though there is no relationship whatsoever between the inputs and
    the shuffled labels, the training loss goes down just fine, even with a relatively
    small model. Naturally, the validation loss does not improve at all over time,
    since there is no possibility of generalization in this setting.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你不相信吗？试着打乱MNIST标签的顺序，并在那个数据上训练一个模型。即使输入和打乱后的标签之间没有任何关系，训练损失也会相应下降，即使是一个相对较小的模型也是如此。自然地，随着时间的推移，验证损失根本不会提高，因为在这种情况下没有泛化的可能性。
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[Listing 5.3](#listing-5-3): Fitting an MNIST model with randomly shuffled
    labels'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表5.3](#listing-5-3)：使用随机打乱标签拟合MNIST模型'
- en: In fact, you don’t even need to do this with MNIST data — you could just generate
    white noise inputs and random labels. You could fit a model on that, too, as long
    as it has enough parameters. It would just end up memorizing specific inputs,
    much like a Python dictionary.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，你甚至不需要用MNIST数据来做这件事——你只需要生成白噪声输入和随机标签。只要模型有足够的参数，你也能在上面拟合一个模型。它最终会记住特定的输入，就像Python字典一样。
- en: If this is the case, then why do deep learning models generalize at all? Shouldn’t
    they just learn an ad hoc mapping between training inputs and targets, like a
    fancy `dict`? What expectation can we have that this mapping will work for new
    inputs?
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果是这样的话，那么为什么深度学习模型能够泛化呢？它们不应该只是学习训练输入和目标之间的一个临时的映射，就像一个花哨的`dict`吗？我们对此映射能够适用于新输入有什么期望？
- en: As it turns out, the nature of generalization in deep learning has rather little
    to do with deep learning models themselves and much to do with the structure of
    information in the real world. Let’s take a look at what’s really going on here.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，深度学习中泛化的本质与深度学习模型本身关系不大，而与现实世界中信息结构有很大关系。让我们看看这里真正发生了什么。
- en: The manifold hypothesis
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 流形假设
- en: 'The input to an MNIST classifier (before preprocessing) is a 28 × 28 array
    of integers between 0 and 255\. The total number of possible input values is thus
    256 to the power of 784 — much greater than the number of atoms in the universe.
    However, very few of these inputs would look like valid MNIST samples: actual
    handwritten digits only occupy a tiny *subspace* of the parent space of all possible
    28 x 28 `uint8` arrays. What’s more, this subspace isn’t just a set of points
    sprinkled at random in the parent space: it is highly structured.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST分类器的输入（在预处理之前）是一个28 × 28的整数数组，其值介于0到255之间。因此，可能的输入值的总数是256的784次方——远大于宇宙中的原子数。然而，其中只有极少数的输入看起来像有效的MNIST样本：实际的手写数字只占据了所有可能的28
    x 28 `uint8`数组父空间的一个微小的**子空间**。更重要的是，这个子空间不仅仅是在父空间中随机散布的点集：它具有高度的结构性。
- en: 'First, the subspace of valid handwritten digits is *continuous*: if you take
    a sample and modify it a little, it will still be recognizable as the same handwritten
    digit. Further, all samples in the valid subspace are *connected* by smooth paths
    that run through the subspace. This means that if you take two random MNIST digits
    A and B, there exists a sequence of “intermediate” images that morph A into B,
    such that two consecutive digits are very close to each other (see figure 5.7).
    Perhaps there will be a few ambiguous shapes close to the boundary between two
    classes, but even these shapes would still look very digit-like.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，有效手写数字的子空间是**连续的**：如果你取一个样本并稍作修改，它仍然可以被识别为相同的手写数字。进一步地，所有有效子空间中的样本都通过在子空间中运行的平滑路径**连接**在一起。这意味着如果你取两个随机的MNIST数字A和B，存在一系列“中间”图像，将A逐渐变形为B，使得两个连续的数字非常接近（见图5.7）。也许在两个类别之间的边界附近会有一些模糊的形状，但这些形状仍然非常像数字。
- en: '![](../Images/d88491a526b48c3670d1df4a3cd24fad.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![流形假设图](../Images/d88491a526b48c3670d1df4a3cd24fad.png)'
- en: '[Figure 5.7](#figure-5-7): Different MNIST digits gradually morphing into one
    another, showing that the space of handwritten digits forms a “manifold.” This
    image was generated using code from chapter 17.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5.7](#figure-5-7)：不同的MNIST数字逐渐变形为彼此，显示了手写数字空间形成“流形”。此图像使用第17章中的代码生成。'
- en: In technical terms, you would say that handwritten digits form a *manifold*
    within the space of possible 28 × 28 `uint8` arrays. That’s a big word, but the
    concept is pretty intuitive. A manifold is a lower-dimensional subspace of some
    parent space that is locally similar to a linear (Euclidean) space. For instance,
    a smooth curve in the plane is a 1D manifold within a 2D space because for every
    point of the curve, you can draw a tangent (the curve can be approximated by a
    line in every point). A smooth surface within a 3D space is a 2D manifold. And
    so on.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术角度来说，你会说手写数字在可能的28 × 28 `uint8`数组空间中形成一个*流形*。这是一个很大的词，但概念相当直观。流形是某个父空间中的低维子空间，在局部上与线性（欧几里得）空间相似。例如，平面上的光滑曲线是二维空间中的一维流形，因为对于曲线上的每一个点，你都可以画一条切线（曲线可以在每个点被近似为一条线）。三维空间中的光滑表面是一个二维流形。以此类推。
- en: More generally, the *manifold hypothesis* posits that all natural data lies
    on a low-dimensional manifold within the high-dimensional space where it is encoded.
    That’s a pretty strong statement about the structure of information in the universe.
    As far as we know, it’s accurate, and it’s the reason why deep learning works.
    It’s true for MNIST digits, as well as for human faces, tree morphology, the sounds
    of the human voice, and even natural language.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 更普遍地说，*流形假设*认为所有自然数据都位于其编码的高维空间中的低维流形上。这是一个关于宇宙中信息结构的相当强烈的陈述。据我们所知，这是准确的，也是深度学习之所以有效的原因。这适用于MNIST数字，也适用于人脸、树木形态、人类的声音，甚至自然语言。
- en: The manifold hypothesis implies
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 流形假设意味着
- en: Machine learning models only have to fit relatively simple, low-dimensional,
    highly structured subspaces within their potential input space (latent manifolds).
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习模型只需要在其潜在输入空间（潜在流形）中拟合相对简单、低维、高度结构化的子空间。
- en: Within one of these manifolds, it’s always possible to *interpolate* between
    two inputs — that is, morph one into another via a continuous path along which
    all points fall on the manifold.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这些流形中的任何一个，总是在两个输入之间进行*插值*——也就是说，通过一条所有点都位于流形上的连续路径，将一个形态转换为另一个形态。
- en: The ability to interpolate between samples is the key to understanding generalization
    in deep learning.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在样本之间进行插值的能力是理解深度学习中泛化的关键。
- en: Interpolation as a source of generalization
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 插值作为泛化的来源
- en: If you work with data points that can be interpolated, you can start making
    sense of points you’ve never seen before by relating them to other points that
    lie close on the manifold. In other words, you can make sense of the *totality*
    of the space using only a *sample* of the space. You can use interpolation to
    fill in the blanks.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你处理的是可以进行插值的数据点，你可以通过将它们与流形上靠近的其他点相关联，开始理解你以前从未见过的点。换句话说，你只需要使用空间的一个*样本*就能理解空间的*整体*。你可以使用插值来填补空白。
- en: Note that interpolation on the latent manifold is different from linear interpolation
    in the parent space, as illustrated in figure 5.8. For instance, the average of
    pixels between two MNIST digits is usually not a valid digit.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，潜在流形上的插值与父空间中的线性插值不同，如图5.8所示。例如，两个MNIST数字之间的像素平均值通常不是一个有效的数字。
- en: '![](../Images/98e9072fd0dbd5cf96bee6acb37ebbdc.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/98e9072fd0dbd5cf96bee6acb37ebbdc.png)'
- en: '[Figure 5.8](#figure-5-8): Difference between linear interpolation and interpolation
    on the latent manifold. Every point on the latent manifold of digits is a valid
    digit, but the average of two digits usually isn’t.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5.8](#figure-5-8)：线性插值与潜在流形插值的区别。数字的潜在流形上的每个点都是一个有效的数字，但两个数字的平均值通常不是。'
- en: 'Crucially, while deep learning achieves generalization via interpolation on
    a learned approximation of the data manifold, it would be a mistake to assume
    that interpolation is *all* there is to generalization. It’s the tip of the iceberg.
    Interpolation can only help you make sense of things that are very close to what
    you’ve seen before: it enables *local generalization*. But remarkably, humans
    deal with extreme novelty all the time, and they do just fine. You don’t need
    to be trained in advance on countless examples of every situation you’ll ever
    have to encounter. Every single one of your days is different from any day you’ve
    experienced before, and different from any day experienced by anyone since the
    dawn of humanity. You can switch between spending a week in NYC, a week in Shanghai,
    and a week in Bangalore without requiring thousands of lifetimes of learning and
    rehearsal for each city.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的是，虽然深度学习通过在数据流形的近似上插值来实现泛化，但认为插值就是泛化的全部是错误的。这只是冰山一角。插值只能帮助你理解非常接近你之前所见的事物：它实现了*局部泛化*。但令人惊讶的是，人类经常处理极端的新奇事物，而且他们做得很好。你不需要提前在无数种你将遇到的情境的例子上进行训练。你每一天都是不同于你之前经历过的任何一天，也不同于自人类诞生以来任何人经历过的任何一天。你可以在这三个城市中任意切换：在纽约市度过一周，在上海度过一周，在班加罗尔度过一周，而不需要为每个城市进行数千年的学习和排练。
- en: Humans are capable of *extreme generalization*, which is enabled by cognitive
    mechanisms other than interpolation — abstraction, symbolic models of the world,
    reasoning, logic, common sense, innate priors about the world — what we generally
    call *reason*, as opposed to intuition and pattern recognition. The latter are
    largely interpolative in nature, but the former isn’t. Both are essential to intelligence.
    We’ll talk more about this in chapter 19.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 人类能够进行*极端泛化*，这是由除了插值之外的认知机制所实现的——抽象、世界的符号模型、推理、逻辑、常识、关于世界的先验知识——我们通常称之为*理性*，与直觉和模式识别相对。后者在很大程度上是插值的，但前者不是。两者对智能都是必不可少的。我们将在第19章中更多地讨论这一点。
- en: Why deep learning works
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么深度学习有效
- en: Remember the crumpled paper ball metaphor from chapter 2? A sheet of paper represents
    a 2D manifold within 3D space (figure 5.9). A deep learning model is a tool for
    uncrumpling paper balls — that is, for disentangling latent manifolds.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 记得第二章中的皱巴巴的纸团隐喻吗？一张纸代表三维空间中的一个二维流形（图5.9）。深度学习模型是展开纸团的工具——也就是说，是解开潜在流形的工具。
- en: '![](../Images/a89f2451feb6a78d99e009be13583d1b.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/a89f2451feb6a78d99e009be13583d1b.png)'
- en: '[Figure 5.9](#figure-5-9): Uncrumpling a complicated manifold of data'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5.9](#figure-5-9)：解开复杂数据流形'
- en: A deep learning model is basically a very high-dimensional curve. The curve
    is smooth and continuous (with additional constraints on its structure, originating
    from model architecture priors) because it needs to be differentiable. And that
    curve is fitted to data points via gradient descent — smoothly and incrementally.
    *By construction*, deep learning is about taking a big, complex curve — a manifold
    — and incrementally adjusting its parameters until it fits some training data
    points.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型基本上是一个非常高维的曲线。这条曲线是平滑且连续的（其结构受到模型架构先验的额外约束），因为它需要可微。而且这条曲线是通过梯度下降来拟合数据点的——平滑且逐步地。*按照构造*，深度学习是关于取一个大而复杂的曲线——一个流形——并逐步调整其参数，直到它适合某些训练数据点。
- en: The curve involves enough parameters that it could fit anything. Indeed, if
    you let your model train for long enough, it will effectively end up purely memorizing
    its training data and won’t generalize at all. However, the data you’re fitting
    to isn’t made of isolated points sparsely distributed across the underlying space.
    Your data forms a highly structured, low-dimensional manifold within the input
    space — that’s the manifold hypothesis. And because fitting your model curve to
    this data happens gradually and smoothly over time, as gradient descent progresses,
    there will be an intermediate point during training at which the model roughly
    approximates the natural manifold of the data, as you can see in figure 5.10.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这条曲线包含足够的参数，可以拟合任何事物。事实上，如果你让你的模型训练足够长的时间，它实际上最终会纯粹地记住其训练数据，而不会进行任何泛化。然而，你正在拟合的数据不是由在潜在空间中稀疏分布的孤立点组成的。你的数据在输入空间中形成一个高度结构化、低维的流形——这就是流形假设。而且因为随着梯度下降的进行，拟合模型曲线到数据的过程是逐渐且平滑的，所以在训练过程中会有一个中间点，此时模型大致近似数据的自然流形，如图5.10所示。
- en: '![](../Images/6184df929f2425673e4fed427eb3b6ef.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6184df929f2425673e4fed427eb3b6ef.png)'
- en: '[Figure 5.10](#figure-5-10): Going from a random model to an overfit model
    and achieving a robust fit as an intermediate state'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5.10](#figure-5-10)：从随机模型过渡到过拟合模型，并实现作为中间状态的鲁棒拟合'
- en: Moving along the curve learned by the model at that point will come close to
    moving along the actual latent manifold of the data. As such, the model will be
    capable of making sense of never-before-seen inputs via interpolation between
    training inputs.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 沿着模型在该点学习的曲线移动将接近沿着数据的实际潜在流形移动。因此，该模型将能够通过在训练输入之间进行插值来理解从未见过的输入。
- en: 'Besides the trivial fact that they have sufficient representational power,
    there are a few properties of deep learning models that make them particularly
    well suited to learning latent manifolds:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 除了它们有足够的表示能力这一显而易见的事实之外，深度学习模型还有一些特性使它们特别适合学习潜在流形：
- en: Deep learning models implement a smooth, continuous mapping from their inputs
    to their outputs. It has to be smooth and continuous because it must be differentiable,
    by necessity (you couldn’t do gradient descent otherwise). This smoothness helps
    approximate latent manifolds, which follow the same properties.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习模型实现了从输入到输出的平滑、连续映射。它必须平滑且连续，因为它是可微分的（否则无法进行梯度下降）。这种平滑性有助于近似潜在流形，这些流形遵循相同的属性。
- en: Deep learning models tend to be structured in a way that mirrors the “shape”
    of the information in their training data (via architecture priors). This is the
    case in particular for image-processing models (see chapters 8–12) and sequence-processing
    models (see chapter 13). More generally, deep neural networks structure their
    learned representations in a hierarchical and modular way, which echoes the way
    natural data is organized.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习模型往往以反映其训练数据中“形状”的方式（通过架构先验）进行结构化。这尤其适用于图像处理模型（参见第8-12章）和序列处理模型（参见第13章）。更普遍地说，深度神经网络以分层和模块化的方式结构化其学习到的表示，这与自然数据的组织方式相呼应。
- en: Training data is paramount
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练数据至关重要
- en: While deep learning is indeed well suited to manifold learning, the power to
    generalize is more a consequence of the natural structure of your data than a
    consequence of any property of your model. You’ll only be able to generalize if
    your data forms a manifold where points can be interpolated. The more informative
    and the less noisy your features are, the better you will be able to generalize,
    since your input space will be simpler and better structured. Data curation and
    feature engineering are essential to generalization.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然深度学习确实非常适合进行流形学习，但泛化的能力更多的是数据自然结构的结果，而不是模型任何属性的结果。只有当你的数据形成一个点可以插值的流形时，你才能进行泛化。你的特征越有信息性、噪声越少，你将能够更好地进行泛化，因为你的输入空间将更简单、结构更好。数据整理和特征工程对于泛化至关重要。
- en: Further, because deep learning is curve fitting, for a model to perform well,
    *it needs to be trained on a dense sampling of its input space*. A “dense sampling”
    in this context means that the training data should densely cover the entirety
    of the input data manifold (see figure 5.11). This is especially true near decision
    boundaries. With a sufficiently dense sampling, it becomes possible to make sense
    of new inputs by interpolating between past training inputs, without having to
    use common-sense, abstract reasoning, or external knowledge about the world —
    all things that machine learning models have no access to.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于深度学习是曲线拟合，为了模型能够表现良好，*它需要在输入空间的密集采样上进行训练*。在这个上下文中，“密集采样”意味着训练数据应该密集地覆盖整个输入数据流形（参见图5.11）。这在决策边界附近尤其如此。在足够密集的采样下，通过在过去的训练输入之间进行插值，可以理解新的输入，而无需使用常识、抽象推理或关于世界的知识——所有这些都是机器学习模型无法获取的。
- en: '![](../Images/d6cbc71cfbd6e2015524613e5ce95c3a.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/d6cbc71cfbd6e2015524613e5ce95c3a.png)'
- en: '[Figure 5.11](#figure-5-11): A dense sampling of the input space is necessary
    to learn a model capable of accurate generalization.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5.11](#figure-5-11)：为了学习能够进行准确泛化的模型，对输入空间进行密集采样是必要的。'
- en: 'As such, you should always keep in mind that the best way to improve a deep
    learning model is to train it on more data or better data (of course, adding overly
    noisy or inaccurate data will harm generalization). A denser coverage of the input
    data manifold will yield a model that generalizes better. You should never expect
    a deep learning model to perform anything more than crude interpolation between
    its training samples, and thus, you should do everything you can to make interpolation
    as easy as possible. The only thing you will find in a deep learning model is
    what you put into it: the priors encoded in its architecture and the data it was
    trained on.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你应该始终牢记，提高深度学习模型的最佳方式是在更多或更好的数据上训练它（当然，添加过度嘈杂或不准确的数据会损害泛化）。对输入数据流形的更密集覆盖将产生泛化能力更强的模型。你永远不要期望深度学习模型能够执行比其在训练样本之间进行粗略插值更复杂的事情，因此，你应该尽一切可能使插值尽可能简单。你将在深度学习模型中找到的只有你放入其中的东西：其架构中编码的先验知识和其训练所使用的数据。
- en: When getting more data isn’t possible, the next-best solution is to modulate
    the quantity of information that your model is allowed to store, or to add constraints
    on the smoothness of the model curve. If a network can only afford to memorize
    a small number of patterns, or very regular patterns, the optimization process
    will force it to focus on the most prominent patterns, which have a better chance
    of generalizing well. The process of fighting overfitting this way is called *regularization*.
    We’ll review regularization techniques in depth in section 5.4.4.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当无法获取更多数据时，下一个最佳解决方案是调节模型允许存储的信息量，或者对模型曲线的平滑性添加约束。如果一个网络只能负担得起记住少量模式，或者非常规则的模式，优化过程将迫使它专注于最突出的模式，这些模式有更好的机会进行良好的泛化。通过这种方式与过拟合作斗争的过程称为*正则化*。我们将在第5.4.4节中深入探讨正则化技术。
- en: 'Before you can start tweaking your model to help it generalize better, you
    need a way to assess how your model is currently doing. In the following section,
    you’ll learn about how you can monitor generalization during model development:
    model evaluation.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在你开始调整模型以帮助其更好地泛化之前，你需要一种方法来评估你的模型目前的表现。在接下来的章节中，你将了解如何在模型开发过程中监控泛化：模型评估。
- en: Evaluating machine-learning models
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估机器学习模型
- en: You can only control what you can observe. Since your goal is to develop models
    that can successfully generalize to new data, it’s essential to be able to reliably
    measure the generalization power of your model. In this section, we’ll formally
    introduce the different ways you can evaluate machine learning models. You’ve
    already seen most of them in action in the previous chapter.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 你只能控制你能观察到的。由于你的目标是开发能够成功推广到新数据的模型，因此能够可靠地衡量你模型的泛化能力至关重要。在本节中，我们将正式介绍你可以用来评估机器学习模型的多种方式。你已经在上一章中看到了其中大部分的实际应用。
- en: Training, validation, and test sets
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练集、验证集和测试集
- en: 'Evaluating a model always boils down to splitting the available data into three
    sets: training, validation, and test. You train on the training data and evaluate
    your model on the validation data. Once your model is ready for prime time, you
    test it one final time on the test data, which is meant to be as similar as possible
    to production data. Then you can deploy the model in production.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 评估一个模型始终归结为将可用数据分成三个集合：训练集、验证集和测试集。你在训练数据上训练，并在验证数据上评估你的模型。一旦你的模型准备就绪，你最后一次在测试数据上对其进行测试，测试数据应尽可能与生产数据相似。然后你可以在生产环境中部署该模型。
- en: 'You may ask, why not have two sets: a training set and a test set? You’d train
    on the training data and evaluate on the test data. Much simpler!'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问，为什么不设置两组数据：一组训练集和一组测试集？你在训练数据上训练，在测试数据上评估。这要简单得多！
- en: 'The reason is that developing a model always involves tuning its configuration:
    for example, choosing the number of layers or the size of the layers (called the
    *hyperparameters* of the model, to distinguish them from the *parameters*, which
    are the network’s weights). You do this tuning by using as a feedback signal the
    performance of the model on the validation data. In essence, this tuning is a
    form of *learning*: a search for a good configuration in some parameter space.
    As a result, tuning the configuration of the model based on its performance on
    the validation set can quickly result in *overfitting to the validation set*,
    even though your model is never directly trained on it.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 原因是开发模型总是涉及调整其配置：例如，选择层数或层的尺寸（称为模型的*超参数*，以区分它们与*参数*，参数是网络的权重）。你通过使用作为反馈信号的模型在验证数据上的性能来进行这种调整。本质上，这种调整是一种*学习*：在某个参数空间中寻找良好配置的过程。因此，基于模型在验证集上的性能调整模型的配置可以迅速导致*过度拟合验证集*，即使你的模型从未直接在它上面训练过。
- en: Central to this phenomenon is the notion of *information leaks*. Every time
    you tune a hyperparameter of your model based on the model’s performance on the
    validation set, some information about the validation data leaks into the model.
    If you do this only once, for one parameter, then very few bits of information
    will leak, and your validation set will remain reliable to evaluate the model.
    But if you repeat this many times — running one experiment, evaluating on the
    validation set, and modifying your model as a result — then you’ll leak an increasingly
    significant amount of information about the validation set into the model.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这一现象的核心是*信息泄露*的概念。每次你根据模型在验证集上的性能调整模型的超参数时，一些关于验证数据的信息就会泄露到模型中。如果你只做一次，针对一个参数，那么泄露的信息量会非常少，你的验证集将保持可靠，用于评估模型。但如果你重复多次——运行一个实验，在验证集上评估，并根据结果修改你的模型——那么你将越来越多地泄露关于验证集的信息到模型中。
- en: 'At the end of the day, you’ll end up with a model that performs artificially
    well on the validation data because that’s what you optimized it for. You care
    about performance on completely new data, not the validation data, so you need
    to use a completely different, never-before-seen dataset to evaluate the model:
    the test dataset. Your model shouldn’t have had access to *any* information about
    the test set, even indirectly. If anything about the model has been tuned based
    on test set performance, then your measure of generalization will be flawed.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你将得到一个在验证数据上表现人工良好的模型，因为那是你优化它的目标。你关心的是在完全新的数据上的性能，而不是验证数据，所以你需要使用一个完全不同、从未见过的数据集来评估模型：测试数据集。你的模型不应该接触到*任何*关于测试集的信息，即使是间接的。如果模型的任何方面是基于测试集的性能进行调整的，那么你的泛化度量将会是有缺陷的。
- en: 'Splitting your data into training, validation, and test sets may seem straightforward,
    but there are a few advanced ways to do it that can come in handy when little
    data is available. Let’s review three classic evaluation recipes: simple hold-out
    validation, K-fold validation, and iterated K-fold validation with shuffling.
    We’ll also talk about the use of common-sense baselines to check that your training
    is going somewhere.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 将你的数据分为训练集、验证集和测试集看似简单，但有一些高级方法可以实现，当数据量较少时这些方法可能非常有用。让我们回顾三种经典的评估方法：简单的保留验证、K
    折验证和带有洗牌的迭代 K 折验证。我们还将讨论使用常识性基线来检查你的训练是否有所进展。
- en: Simple hold-out validation
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 简单的保留验证
- en: Set apart some fraction of your data as your test set. Train on the remaining
    data, and evaluate on the test set. As you saw in the previous sections, to prevent
    information leaks, you shouldn’t tune your model based on the test set, and therefore
    you should *also* reserve a validation set.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 将你数据的一部分作为测试集。在剩余的数据上训练，并在测试集上评估。正如你在前面的章节中看到的，为了防止信息泄露，你不应该根据测试集调整你的模型，因此你也应该*同样*保留一个验证集。
- en: Schematically, hold-out validation looks like figure 5.12\. The following listing
    shows a simple implementation.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 概括来说，保留验证看起来就像图 5.12 所示。以下列表展示了一个简单的实现。
- en: '![](../Images/84f0465e58c36e5d326ebdd845403f65.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84f0465e58c36e5d326ebdd845403f65.png)'
- en: '[Figure 5.12](#figure-5-12): Simple hold-out validation split'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5.12](#figure-5-12)：简单的保留验证分割'
- en: '[PRE3]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[Listing 5.4](#listing-5-4): Hold-out validation (note that labels are omitted
    for simplicity)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 5.4](#listing-5-4)：保留验证（注意为了简单起见省略了标签）'
- en: 'This is the simplest evaluation protocol, and it suffers from one flaw: if
    little data is available, then your validation and test sets may contain too few
    samples to be statistically representative of the data at hand. This is easy to
    recognize: if different random shuffling rounds of the data before splitting end
    up yielding very different measures of model performance, then you’re having this
    issue. K-fold validation and iterated K‑fold validation are two ways to address
    this, as discussed next.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最简单的评估协议，但它有一个缺点：如果数据很少，那么你的验证集和测试集可能包含的样本太少，无法在统计上代表手头的数据。这很容易识别：如果在分割数据之前的随机洗牌轮次结束后，模型性能的度量非常不同，那么你就有这个问题。K折验证和迭代K折验证是两种解决方法，如后文所述。
- en: K-fold validation
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: K折验证
- en: With this approach, you split your data into `K` partitions of equal size. For
    each partition `i`, train a model on the remaining `K - 1` partitions and evaluate
    it on partition `i`. Your final score is then the averages of the K scores obtained.
    This method is helpful when the performance of your model shows significant variance
    based on your train/test split. Like hold-out validation, this method doesn’t
    exempt you from using a distinct validation set for model calibration.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法，你将数据分成`K`个大小相等的部分。对于每个部分`i`，在剩余的`K - 1`个部分上训练一个模型，并在部分`i`上评估它。你的最终得分是K个得分的平均值。当你的模型性能基于你的训练/测试分割显示出显著的方差时，这种方法很有帮助。就像保留法验证一样，这种方法不会让你免除为模型校准使用一个独立的验证集。
- en: Schematically, K-fold cross-validation looks like figure 5.13\. Listing 5.6
    shows a simple implementation.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 概括地看，K折交叉验证看起来像图5.13。列表5.6展示了简单实现。
- en: '![](../Images/29029d5c4877975815ab98f940282a76.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/29029d5c4877975815ab98f940282a76.png)'
- en: '[Figure 5.13](#figure-5-13): Three-fold validation'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5.13](#figure-5-13)：三折验证'
- en: '[PRE4]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[Listing 5.5](#listing-5-5): K-fold cross-validation (note that labels are
    omitted for simplicity)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表5.5](#listing-5-5)：K折交叉验证（为了简单起见，省略了标签）'
- en: Iterated K-fold validation with shuffling
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 带洗牌的迭代K折验证
- en: This one is for situations in which you have relatively little data available
    and you need to evaluate your model as precisely as possible. I’ve found it to
    be extremely helpful in Kaggle competitions. It consists of applying K-fold validation
    multiple times, shuffling the data every time before splitting it `K` ways. The
    final score is the average of the scores obtained at each run of K-fold validation.
    Note that you end up training and evaluating `P * K` models (where `P` is the
    number of iterations you use), which can be very expensive.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法适用于你只有相对较少数据可用，并且需要尽可能精确地评估你的模型的情况。我发现它在Kaggle比赛中非常有帮助。它包括多次应用K折验证，每次在分割数据成`K`部分之前都进行数据洗牌。最终得分是每次K折验证运行得到的得分的平均值。请注意，你最终会训练和评估`P
    * K`个模型（其中`P`是你使用的迭代次数），这可能会非常昂贵。
- en: Beating a common-sense baseline
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 打败常识性基线
- en: Besides the different evaluation protocols you have available, one last thing
    you should know about is the use of common-sense baselines.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 除了你拥有的不同评估协议外，还有最后一件事你应该知道，那就是常识性基线的使用。
- en: Training a deep learning model is a bit like pressing a button that launches
    a rocket in a parallel world. You can’t hear it or see it. You can’t observe the
    manifold learning process — it’s happening in a space with thousands of dimensions,
    and even if you projected it to 3D, you couldn’t interpret it. The only feedback
    you have is your validation metrics — like an altitude meter on your invisible
    rocket.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个深度学习模型有点像按下在一个平行世界中发射火箭的按钮。你听不到它或看到它。你无法观察到流形学习过程——它在一个有数千维度的空间中发生，即使你将其投影到3D，你也无法解释它。你唯一能得到的反馈是你的验证指标——就像你无形火箭上的高度计。
- en: 'A particularly important point is to be able to tell whether you’re getting
    off the ground at all. What was the altitude you started at? Your model seems
    to have an accuracy of 15%, is that any good? Before you start working with a
    dataset, you should always pick a trivial baseline that you’ll try to beat. If
    you cross that threshold, you’ll know you’re doing something right: your model
    is actually using the information in the input data to make predictions that generalize
    — you can keep going. This baseline could be performance of a random classifier,
    or the performance of the simplest non-machine learning technique you can imagine.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: For instance, in the MNIST digit-classification example, a simple baseline would
    be a validation accuracy greater than 0.1 (random classifier); in the IMDB example,
    it would be a validation accuracy greater than 0.5\. In the Reuters example, it
    would be around 0.18–0.19, due to class imbalance. If you have a binary classification
    problem where 90% of samples belong to class A and 10% belong to class B, then
    a classifier that always predicts A already achieves 0.9 in validation accuracy,
    and you’ll need to do better than that.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Having a common sense baseline you can refer to is essential when you’re getting
    started on a problem no one has solved before. If you can’t beat a trivial solution,
    your model is worthless — perhaps you’re using the wrong model or perhaps the
    problem you’re tackling can’t even be approached with machine learning in the
    first place. Time to go back to the drawing board.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Things to keep in mind about model evaluation
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Keep an eye out for the following when you’re choosing an evaluation protocol:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '*Data representativeness*  — You want both your training set and test set to
    be representative of the data at hand. For instance, if you’re trying to classify
    images of digits, and you’re starting from an array of samples where the samples
    are ordered by their class, taking the first 80% of the array as your training
    set and the remaining 20% as your test set will result in your training set containing
    only classes 0–7, whereas your test set contains only classes 8–9\. This seems
    like a ridiculous mistake, but it’s surprisingly common. For this reason, you
    usually should *randomly shuffle* your data before splitting it into training
    and test sets.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The arrow of time*  — If you’re trying to predict the future given the past
    (for example, tomorrow’s weather, stock movements, and so on), you should not
    randomly shuffle your data before splitting it because doing so will create a
    *temporal leak*: your model will effectively be trained on data from the future.
    In such situations, you should always make sure all data in your test set is *posterior*
    to the data in the training set.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Redundancy in your data*  — If some data points in your data appear twice
    (fairly common with real-world data), then shuffling the data and splitting it
    into a training set and a validation set will result in redundancy between the
    training and validation sets. In effect, you’ll be testing on part of your training
    data, which is the worst thing you can do! Make sure your training set and validation
    set are disjoint.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据冗余* — 如果你的数据中某些数据点出现了两次（在现实世界数据中相当常见），那么对数据进行洗牌并将其分为训练集和验证集将导致训练集和验证集之间的冗余。实际上，你将测试你的一部分训练数据，这是最糟糕的事情！确保你的训练集和验证集是不相交的。'
- en: Having a reliable way to evaluate the performance of your model is how you’ll
    be able to monitor the tension at the heart of machine learning — between optimization
    and generalization, underfitting and overfitting.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个可靠的方式来评估你模型的性能，这样你将能够监控机器学习核心的紧张关系——在优化和泛化、欠拟合和过度拟合之间。
- en: Improving model fit
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提高模型拟合度
- en: To achieve the perfect fit, you must first overfit. Since you don’t know in
    advance where the boundary lies, you must cross it to find it. Thus, your initial
    goal as you start working on a problem is to achieve a model that shows some generalization
    power, and that is able to overfit. Once you have such a model, you’ll focus on
    refining generalization by fighting overfitting.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 要达到完美的拟合，你必须首先过度拟合。由于你事先不知道边界在哪里，你必须跨越它来找到它。因此，当你开始处理问题时，你的初始目标是实现一个显示出一些泛化能力的模型，并且能够过度拟合。一旦你有了这样的模型，你将专注于通过对抗过度拟合来细化泛化。
- en: 'There are three common problems you’ll encounter at this stage:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，你将遇到三个常见的问题：
- en: 'Training doesn’t get started: your training loss doesn’t go down over time.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练没有开始：你的训练损失并没有随时间下降。
- en: 'Training gets started just fine, but your model doesn’t meaningfully generalize:
    you can’t beat the common-sense baseline you set.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练开始得很顺利，但你的模型并没有真正地泛化：你无法超越你设定的常识性基线。
- en: Training and validation loss both go down over time, and you can beat your baseline,
    but you don’t seem to be able to overfit, which indicates you’re still underfitting.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和验证损失都随时间下降，你可以打败你的基线，但你似乎无法过度拟合，这表明你仍然欠拟合。
- en: 'Let’s see how you can address these issues to achieve the first big milestone
    of a machine learning project: getting a model that has some generalization power
    (it can beat a trivial baseline) and is able to overfit.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看你如何解决这些问题，以实现机器学习项目的第一个重大里程碑：得到一个具有一些泛化能力的模型（它可以打败一个平凡的基线）并且能够过度拟合。
- en: Tuning key gradient descent parameters
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调整关键梯度下降参数
- en: 'Sometimes, training doesn’t get started or stalls too early. Your loss is stuck.
    This is *always* something you can overcome: remember that you can fit a model
    to random data. Even if nothing about your problem makes sense, you should *still*
    be able to train something — if only by memorizing the training data.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，训练没有开始或者过早停滞。你的损失被卡住了。这*总是*可以克服的：记住你可以将模型拟合到随机数据。即使你的问题没有任何意义，你也应该*仍然*能够训练一些东西——至少通过记住训练数据。
- en: 'When this happens, it’s always a problem with the configuration of the gradient
    descent process: your choice of optimizer, the distribution of initial values
    in the weights of your model, your learning rate, or your batch size. All these
    parameters are interdependent, and as such, it is usually sufficient to tune the
    learning rate and the batch size while maintaining the rest of the parameters
    constant.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 当这种情况发生时，通常是由于梯度下降过程的配置问题：你的优化器选择、模型权重中初始值的分布、你的学习率，或者你的批量大小。所有这些参数都是相互依赖的，因此通常只需要调整学习率和批量大小，同时保持其他参数不变。
- en: 'Let’s look at a concrete example: let’s train the MNIST model from chapter
    2 with an inappropriately large learning rate, of value 1.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个具体的例子：让我们用第 2 章中不适当大的学习率（值为 1）来训练 MNIST 模型。
- en: '[PRE5]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[Listing 5.6](#listing-5-6): Training an MNIST model with an incorrectly high
    learning rate'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 5.6](#listing-5-6)：使用过高学习率的 MNIST 模型进行训练'
- en: 'The model quickly reaches a training and validation accuracy in the 20% to
    40% range, but cannot get past that. Let’s try to lower the learning rate to a
    more reasonable value of `1e-2`:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 模型很快达到训练和验证准确率在 20% 到 40% 的范围内，但无法突破这个范围。让我们尝试将学习率降低到一个更合理的值 `1e-2`：
- en: '[PRE6]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[Listing 5.7](#listing-5-7): The same model with a more appropriate learning
    rate'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 5.7](#listing-5-7)：具有更合适学习率的相同模型'
- en: The model is now able to train.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 模型现在能够进行训练了。
- en: If you find yourself in a similar situation, try
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你发现自己处于类似的情况，可以尝试
- en: Lowering or increasing the learning rate. A learning rate that is too high may
    lead to updates that vastly overshoot a proper fit, like in the previous example,
    and a learning rate that is too low may make training so slow that it appears
    to stall.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低或提高学习率。学习率过高可能导致更新远远超出适当的拟合，就像之前的例子中那样，而学习率过低可能会使训练变得非常缓慢，看起来像是停滞不前。
- en: Increasing the batch size. A batch with more samples will lead to gradients
    that are more informative and less noisy (lower variance).
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加批量大小。包含更多样本的批量将导致更具有信息量和更少噪声的梯度（方差更低）。
- en: You will, eventually, find a configuration that gets training started.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你最终会找到一个配置，使训练开始。
- en: Using better architecture priors
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用更好的架构先验
- en: 'You have a model that fits, but for some reason your validation metrics aren’t
    improving at all. They remain no better than what a random classifier would achieve:
    your model trains, but doesn’t generalize. What’s going on?'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 你有一个拟合的模型，但出于某种原因，你的验证指标根本没有任何改善。它们与随机分类器所能达到的指标一样好：你的模型在训练，但没有泛化。发生了什么？
- en: This is perhaps the worst machine learning situation you can find yourself in.
    It indicates that *something is fundamentally wrong with your approach*, and it
    may not be easy to tell what. Here are some tips.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是你可能会遇到的最糟糕的机器学习情况。这表明“你的方法存在根本性的问题”，并且可能不容易判断出是什么问题。以下是一些建议。
- en: 'First, it may be that the input data you’re using simply doesn’t contain sufficient
    information to predict your targets: the problem as formulated is not solvable.
    This is what happened earlier when we tried to fit an MNIST model where the labels
    were shuffled: the model would train just fine, but validation accuracy would
    stay stuck at 10%, because it was plainly impossible to generalize with such a
    dataset.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，可能是因为你使用的输入数据本身就不包含足够的信息来预测你的目标：按照这种表述的问题是不可解决的。这就是我们之前尝试拟合一个MNIST模型，其中标签被随机打乱时发生的情况：模型可以很好地训练，但验证准确率会一直停留在10%，因为使用这样的数据集显然无法进行泛化。
- en: 'It may also be that the kind of model you’re using is not suited for the problem
    at hand. For instance, in chapter 13, you’ll see an example of a timeseries prediction
    problem where a densely connected architecture isn’t able to beat a trivial baseline,
    whereas a more appropriate recurrent architecture does manage to generalize well.
    Using a model that makes the right assumptions about the problem is essential
    to achieve generalization: you should use the right architecture priors.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 也可能是因为你使用的模型不适合当前的问题。例如，在第13章中，你将看到一个时间序列预测问题的例子，其中密集连接的架构无法击败一个平凡的基线，而一个更合适的循环架构确实能够很好地泛化。使用对问题做出正确假设的模型对于实现泛化至关重要：你应该使用正确的架构先验。
- en: In the following chapters, you’ll learn about the best architectures to use
    for a variety of data modalities — images, text, timeseries, and so on. In general,
    you should always make sure to read up on architecture best practices for the
    kind of task you’re attacking — chances are you’re not the first person to attempt
    it.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，你将了解适用于各种数据模态的最佳架构——图像、文本、时间序列等。一般来说，你应该确保阅读针对你正在攻击的任务的架构最佳实践——很可能你不是第一个尝试这样做的人。
- en: Increasing model capacity
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提高模型容量
- en: 'If you manage to get to a model that fits, where validation metrics are going
    down, and that seems to achieve at least some level of generalization power, congratulations:
    you’re almost there. Next, you need to get your model to start overfitting.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你设法得到一个拟合的模型，其中验证指标正在下降，并且似乎达到了至少一定程度的泛化能力，恭喜你：你几乎成功了。接下来，你需要让你的模型开始过拟合。
- en: Consider the following small model — a simple logistic regression — trained
    on MNIST pixels.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下小型模型——一个简单的逻辑回归——它是在MNIST像素上训练的。
- en: '[PRE7]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[Listing 5.8](#listing-5-8): A simple logistic regression on MNIST'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表5.8](#listing-5-8)：MNIST上的简单逻辑回归'
- en: 'You get loss curves that look like this (see figure 5.14):'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 你会得到类似这样的损失曲线（见图5.14）：
- en: '[PRE8]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/74b72a738ae10806bf61978c9e5cb342.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/74b72a738ae10806bf61978c9e5cb342.png)'
- en: '[Figure 5.14](#figure-5-14): Effect of insufficient model capacity on loss
    curves'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5.14](#figure-5-14)：模型容量不足对损失曲线的影响'
- en: Validation metrics seem to stall or to improve very slowly, instead of peaking
    and reversing course. The validation loss goes to 0.26 and just stays there. You
    can fit, but you can’t clearly overfit, even after many iterations over the training
    data. You’re likely to encounter similar curves often in your career.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 验证指标似乎停滞不前或改善非常缓慢，而不是达到峰值然后逆转。验证损失下降到0.26并保持在那里。你可以拟合，但你无法清楚地过拟合，即使是在多次迭代训练数据之后。在你的职业生涯中，你很可能会经常遇到类似的曲线。
- en: 'Remember that it should always be possible to overfit. Much like the problem
    “the training loss doesn’t go down,” this is an issue that can always be solved.
    If you can’t seem to be able to overfit, it’s likely a problem with the *representational
    power* of your model: you’re going to need a bigger model, one with more *capacity*
    — that is, able to store more information. You can increase representational power
    by adding more layers, using bigger layers (layers with more parameters), or using
    kinds of layers that are more appropriate for the problem at hand (better architecture
    priors).'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，总是有可能过拟合。这与“训练损失不下降”的问题类似，这是一个可以始终解决的问题。如果你似乎无法过拟合，那么很可能是你模型的**表示能力**问题：你需要一个更大的模型，一个具有更多**容量**的模型——也就是说，能够存储更多信息。你可以通过添加更多层、使用更大的层（具有更多参数的层）或使用更适合当前问题的层（更好的架构先验）来增加表示能力。
- en: 'Let’s try training a bigger model, one with two intermediate layers with 128
    units each:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试训练一个更大的模型，一个包含两个中间层，每个层有128个单元的模型：
- en: '[PRE9]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The training curves now look exactly like they should: the model fits fast
    and starts overfitting after eight epochs (see figure 5.15):'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的训练曲线看起来完全符合预期：模型拟合速度快，在八个epoch后开始过拟合（见图5.15）：
- en: '![](../Images/ee06559a951289b0281b92145b0b33a3.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/ee06559a951289b0281b92145b0b33a3.png)'
- en: '[Figure 5.15](#figure-5-15): Validation loss for a model with appropriate capacity'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5.15](#figure-5-15)：具有适当容量的模型的验证损失'
- en: 'Note that while it is standard to work with models that are way overparameterized
    for the problem at hand, there can definitely be such a thing as *too much* memorization
    capacity. You’ll know your model is too large if it starts overfitting right away.
    Here’s what happens for an MNIST model with three intermediate layers with 2,048
    units each (see figure 5.16):'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，虽然对于当前问题来说，使用过度参数化的模型是标准的，但确实存在“过度记忆”容量的问题。如果你的模型一开始就立即开始过拟合，你就知道你的模型太大。以下是一个具有三个中间层，每个层有2,048个单元的MNIST模型的例子（见图5.16）：
- en: '[PRE10]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/8fd108e012c8f200180dba277edcbffe.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/8fd108e012c8f200180dba277edcbffe.png)'
- en: '[Figure 5.16](#figure-5-16): Effect of excessive model capacity on validation
    loss'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5.16](#figure-5-16)：过度模型容量对验证损失的影响'
- en: Improving generalization
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提高泛化能力
- en: Once your model has shown to have some generalization power and to be able to
    overfit, it’s time to switch your focus toward maximizing generalization.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你的模型显示出一定的泛化能力和过拟合的能力，是时候将你的重点转向最大化泛化。
- en: Dataset curation
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据集整理
- en: You’ve already learned that generalization in deep learning originates from
    the latent structure of your data. If your data makes it possible to smoothly
    interpolate between samples, then you will be able to train a deep learning model
    that generalizes. If your problem is overly noisy or fundamentally discrete, like,
    say, list sorting, deep learning will not help you. Deep learning is curve fitting,
    not magic.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经了解到，在深度学习中，泛化能力源于你数据的潜在结构。如果你的数据可以在样本之间平滑插值，那么你将能够训练出一个泛化的深度学习模型。如果你的问题过于嘈杂或本质上离散，比如列表排序，深度学习将无法帮助你。深度学习是曲线拟合，而不是魔法。
- en: 'As such, it is essential that you make sure that you’re working with an appropriate
    dataset. Spending more effort and money on data collection almost always yields
    a much greater return on investment than spending the same on developing a better
    model:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，确保你使用的是适当的数据库至关重要。在数据收集上投入更多努力和资金，几乎总是比在开发更好的模型上投入相同的资金带来更大的投资回报：
- en: Make sure you have enough data. Remember that you need a *dense sampling* of
    the input-cross-output space. More data will yield a better model. Sometimes,
    problems that seem impossible at first become solvable with a larger dataset.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保你有足够的数据。记住，你需要对输入-输出空间进行密集采样。更多的数据将产生更好的模型。有时，一开始看似不可能解决的问题，通过更大的数据集就能解决。
- en: Minimize labeling errors — visualize your inputs to check for anomalies, and
    proofread your labels.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小化标签错误——可视化你的输入以检查异常，并校对你的标签。
- en: Clean your data and deal with missing values (we cover this in the next chapter).
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清理你的数据并处理缺失值（我们将在下一章中介绍）。
- en: If you have many features and you aren’t sure which ones are actually useful,
    do feature selection.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你有很多特征，但你不确定哪些是有用的，那么进行特征选择。
- en: A particularly important way you can improve the generalization potential of
    your data is *feature engineering*. For most machine learning problems, *feature
    engineering* is a key ingredient for success. Let’s take a look.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过 *特征工程* 来提高数据的泛化潜力。对于大多数机器学习问题，*特征工程* 是成功的关键因素。让我们来看看。
- en: Feature engineering
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征工程
- en: '*Feature engineering* is the process of using your own knowledge about the
    data and about the machine learning algorithm at hand (in this case, a neural
    network) to make the algorithm work better by applying hardcoded (non-learned)
    transformations to the data before it goes into the model. In many cases, it isn’t
    reasonable to expect a machine learning model to be able to learn from completely
    arbitrary data. The data needs to be presented to the model in a way that will
    make the model’s job easier.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '*特征工程* 是一个过程，即利用你对数据以及当前机器学习算法（在这种情况下，是一个神经网络）的知识，通过在数据进入模型之前应用硬编码（非学习）的转换来使算法工作得更好。在许多情况下，期望机器学习模型能够从完全任意的数据中学习是不合理的。数据需要以使模型的工作更容易的方式进行呈现。'
- en: Let’s look at an intuitive example. Suppose you’re trying to develop a model
    that can take as input an image of a clock and can output the time of day (see
    figure 5.17). If you choose to use the raw pixels of the image as input data,
    then you have a difficult machine learning problem on your hands. You’ll need
    a convolutional neural network to solve it, and you’ll have to expend quite a
    bit of computational resources to train the network.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个直观的例子。假设你正在尝试开发一个模型，该模型可以接受一个钟表的图像作为输入，并输出一天中的时间（见图 5.17）。如果你选择使用图像的原始像素作为输入数据，那么你将面临一个困难的机器学习问题。你需要一个卷积神经网络来解决它，并且你需要投入相当多的计算资源来训练网络。
- en: '![](../Images/e3a49faf9d2e929985728f44a01d6afb.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e3a49faf9d2e929985728f44a01d6afb.png)'
- en: '[Figure 5.17](#figure-5-17): Feature engineering for reading the time on a
    clock'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5.17](#figure-5-17)：读取钟表时间的特征工程'
- en: 'But if you already understand the problem at a high level (you understand how
    humans read time on a clock face), then you can come up with much better input
    features for a machine learning algorithm: for instance, it’s easy to write a
    five-line Python script to follow the black pixels of the clock hands and output
    the `(x, y)` coordinates of the tip of each hand. Then a simple machine learning
    algorithm can learn to associate these coordinates with the appropriate time of
    day.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你已经从高层次上理解了问题（你理解人类是如何读取钟表面的时间的），那么你可以为机器学习算法提出更好的输入特征：例如，编写一个五行的 Python
    脚本来跟踪钟表的黑色指针并输出每个指针尖端的 `(x, y)` 坐标很容易。然后一个简单的机器学习算法可以学会将这些坐标与适当的时间联系起来。
- en: 'You can go even further: do a coordinate change and express the `(x, y)` coordinates
    as polar coordinates with regard to the center of the image. Your input will become
    the angle `theta` of each clock hand. At this point, your features are making
    the problem so easy that no machine learning is required; a simple rounding operation
    and dictionary lookup are enough to recover the approximate time of day.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 你甚至可以更进一步：进行坐标变换，将 `(x, y)` 坐标表示为以图像中心为极坐标。你的输入将变为每个钟表的指针的角度 `theta`。在这个阶段，你的特征使得问题变得如此简单，以至于不需要机器学习；简单的四舍五入操作和字典查找就足以恢复一天中的大约时间。
- en: 'That’s the essence of feature engineering: making a problem easier by expressing
    it in a simpler way. Make the latent manifold smoother, simpler, and better organized.
    It usually requires understanding the problem in depth.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是特征工程的本质：通过以更简单的方式表达问题来使问题更容易。使潜在流形更平滑、更简单、更有组织。这通常需要深入理解问题。
- en: Before deep learning, feature engineering used to be the most important part
    of the machine learning workflow because classical shallow algorithms didn’t have
    hypothesis spaces rich enough to learn useful features by themselves. The way
    you presented the data to the algorithm was absolutely critical to its success.
    For instance, before convolutional neural networks became successful on the MNIST
    digit-classification problem, solutions were typically based on hardcoded features
    such as the number of loops in a digit image, the height of each digit in an image,
    a histogram of pixel values, and so on.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习之前，特征工程曾是机器学习工作流程中最重要的一部分，因为经典浅层算法没有足够丰富的假设空间来自动学习有用的特征。你向算法呈现数据的方式对其成功至关重要。例如，在卷积神经网络在MNIST数字分类问题中取得成功之前，解决方案通常是基于硬编码的特征，如数字图像中的环数、图像中每个数字的高度、像素值直方图等。
- en: 'Fortunately, modern deep learning removes the need for most feature engineering
    because neural networks are capable of automatically extracting useful features
    from raw data. Does this mean you don’t have to worry about feature engineering
    as long as you’re using deep neural networks? No, for two reasons:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，现代深度学习消除了对大多数特征工程的需求，因为神经网络能够自动从原始数据中提取有用的特征。这意味着只要使用深度神经网络，你就不必担心特征工程吗？不，有两个原因：
- en: Good features still allow you to solve problems more elegantly while using fewer
    resources. For instance, it would be ridiculous to solve the problem of reading
    a clock face using a convolutional neural network.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 良好的特征仍然允许你在使用更少资源的情况下更优雅地解决问题。例如，使用卷积神经网络来解决读取时钟面的问题将是荒谬的。
- en: Good features let you solve a problem with far less data. The ability of deep-learning
    models to learn features on their own relies on having lots of training data available;
    if you have only a few samples, then the information value in their features becomes
    critical.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 良好的特征让你可以用更少的数据解决问题。深度学习模型能够自行学习特征的能力依赖于大量可用训练数据；如果你只有少量样本，那么它们特征中的信息价值变得至关重要。
- en: Using early stopping
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用提前停止
- en: 'In deep learning, we always use models that are vastly overparameterized: they
    have way more degrees of freedom than the minimum necessary to fit to the latent
    manifold of the data. This overparameterization is not an issue because *you never
    fully fit a deep learning model*. Such a fit wouldn’t generalize at all. You will
    always interrupt training long before you’ve reached the minimum possible training
    loss.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，我们总是使用大量过参数化的模型：它们的自由度远远超过拟合数据潜在流形所需的最小值。这种过参数化并不是问题，因为**你永远不会完全拟合一个深度学习模型**。这样的拟合根本无法泛化。你总是在达到最小可能训练损失之前就中断了训练。
- en: Finding the exact point during training where you’ve reached the most generalizable
    fit — the exact boundary between an underfit curve and an overfit curve — is one
    of the most effective things you can do to improve generalization.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中找到你达到最泛化拟合的确切点——即欠拟合曲线和过拟合曲线之间的确切边界——是你可以做的最有效的事情之一，以改善泛化能力。
- en: In the examples from the previous chapter, we would start by training our models
    for longer than needed to figure out the number of epochs that yielded the best
    validation metrics, then we would retrain a new model for exactly that number
    of epochs. This is pretty standard. However, it requires you to do redundant work,
    which can sometimes be expensive. Naturally, you could just save your model at
    the end of each epoch, then once you’ve found the best epoch, reuse the closest
    saved model you have. In Keras, it’s typical to do this with an `EarlyStopping`
    callback, which will interrupt training as soon as validation metrics have stopped
    improving, while remembering the best known model state. You’ll learn to use callbacks
    in chapter 7.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章的例子中，我们首先会训练我们的模型超过所需时间，以确定产生最佳验证指标的epoch数，然后我们会重新训练一个恰好为此数量的epoch的新模型。这是相当标准的做法。然而，这需要你做重复性的工作，有时可能会很昂贵。自然地，你可以在每个epoch结束时保存你的模型，然后一旦找到最佳epoch，就重新使用最接近的已保存模型。在Keras中，通常使用`EarlyStopping`回调来实现这一点，该回调会在验证指标停止改进时中断训练，同时记住最佳已知模型状态。你将在第7章学习如何使用回调。
- en: Regularizing your model
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 规范化你的模型
- en: '*Regularization techniques* are a set of best practices that actively impede
    the model’s ability to fit perfectly to the training data, with the goal of making
    the model perform better during validation. This is called “regularizing” the
    model because it tends to make the model simpler, more “regular,” its curve smoother,
    and more “generic” — thus less specific to the training set and better able to
    generalize by more closely approximating the latent manifold of the data. Keep
    in mind that “regularizing” a model is a process that should always be guided
    by an accurate evaluation procedure. You will only achieve generalization if you
    can measure it.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '*正则化技术*是一套最佳实践，它积极地阻碍模型完美拟合训练数据的能力，目的是使模型在验证期间表现更好。这被称为“正则化”模型，因为它往往会使模型更简单，更“规则”，其曲线更平滑，更“通用”——因此对训练集更不具体，并且能够通过更接近地逼近数据的潜在流形来更好地泛化。请记住，“正则化”模型是一个应该始终由准确的评估程序指导的过程。只有当你能够衡量它时，你才能实现泛化。'
- en: Let’s review some of the most common regularization techniques and apply them
    in practice to improve the movie classification model from chapter 4.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一些最常用的正则化技术，并在实践中应用它们来改进第4章中的电影分类模型。
- en: Reducing the network’s size
  id: totrans-202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 减小网络的大小
- en: 'You’ve already learned that a model that is too small will not overfit. The
    simplest way to mitigate overfitting is to reduce the size of the model (the number
    of learnable parameters in the model, determined by the number of layers and the
    number of units per layer). If the model has limited memorization resources, it
    won’t be able to simply memorize its training data. To minimize its loss, it will
    have to resort to learning compressed representations that have predictive power
    regarding the targets — precisely the type of representations we’re interested
    in. At the same time, keep in mind that you should use models that have enough
    parameters that they don’t underfit: your model shouldn’t be starved for memorization
    resources. There is a compromise to be found between *too much capacity* and *not
    enough capacity*.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经了解到，一个过小的模型不会过拟合。减轻过拟合的最简单方法就是减小模型的大小（模型中可学习的参数数量，由层数和每层的单元数决定）。如果模型有限的记忆资源，它将无法简单地记住其训练数据。为了最小化其损失，它将不得不求助于学习具有预测能力的压缩表示——这正是我们感兴趣的类型。同时，请记住，你应该使用具有足够参数的模型，这样它们就不会欠拟合：你的模型不应该缺乏记忆资源。在“过多容量”和“容量不足”之间需要找到一个折衷方案。
- en: Unfortunately, there is no magical formula to determine the right number of
    layers or the right size for each layer. You must evaluate an array of different
    architectures (on your validation set, not on your test set, of course) to find
    the correct model size for your data. The general workflow to find an appropriate
    model size is to start with relatively few layers and parameters and increase
    the size of the layers or add new layers until you see diminishing returns with
    regard to validation loss.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，没有神奇的公式可以确定正确的层数或每层的正确大小。你必须评估一系列不同的架构（当然是在你的验证集上，而不是测试集上），以找到适合你数据的正确模型大小。找到适当模型大小的一般工作流程是从相对较少的层和参数开始，增加层的大小或添加新层，直到你在验证损失方面看到收益递减。
- en: Let’s try this on the movie-review classification model. Here’s a condensed
    version of the model from chapter 4.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在电影评论分类模型上尝试这个。这是第4章中模型的简化版本。
- en: '[PRE11]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[Listing 5.9](#listing-5-9): Original model'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表5.9](#listing-5-9)：原始模型'
- en: Now let’s try to replace it with this smaller model.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试用这个更小的模型来替换它。
- en: '[PRE12]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[Listing 5.10](#listing-5-10): Version of the model with lower capacity'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表5.10](#listing-5-10)：容量较低的模型版本'
- en: Figure 5.18 shows a comparison of the validation losses of the original model
    and the smaller model.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.18显示了原始模型和较小模型验证损失的对比。
- en: '![](../Images/48aeb1a9a731eaaf0f1bf644071dffc1.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/48aeb1a9a731eaaf0f1bf644071dffc1.png)'
- en: '[Figure 5.18](#figure-5-18): Original model vs. smaller model on IMDb review
    classification'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5.18](#figure-5-18)：IMDb评论分类中的原始模型与较小模型的比较'
- en: As you can see, the smaller model starts overfitting later than the reference
    model (after six epochs rather than four), and its performance degrades more slowly
    once it starts overfitting.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，较小的模型比参考模型更晚开始过拟合（在六次而不是四次迭代后），一旦开始过拟合，其性能下降得更慢。
- en: Now, let’s add to our benchmark a model that has much more capacity — far more
    than the problem warrants.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将一个具有更多容量的模型添加到我们的基准中——远远超过问题所需的容量。
- en: '[PRE13]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[Listing 5.11](#listing-5-11): Version of the model with higher capacity'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 5.11](#listing-5-11)：具有更高容量的模型版本'
- en: Figure 5.19 shows how the bigger model fares compared to the reference model.
    The bigger model starts overfitting almost immediately, after just one epoch,
    and it overfits much more severely. Its validation loss is also noisier. It gets
    training loss near zero very quickly. The more capacity the model has, the more
    quickly it can model the training data (resulting in a low training loss), but
    the more susceptible it is to overfitting (resulting in a large difference between
    the training and validation loss).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.19 展示了较大模型与参考模型相比的表现。较大的模型几乎在一轮训练后就开始过拟合，并且过拟合程度更加严重。其验证损失也更加嘈杂。它迅速将训练损失降至接近零。模型容量越大，它越能快速地建模训练数据（导致训练损失较低），但它对过拟合的敏感性也越高（导致训练损失和验证损失之间差异较大）。
- en: '![](../Images/410a394fb019ca4488992ec6711798ba.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/410a394fb019ca4488992ec6711798ba.png)'
- en: '[Figure 5.19](#figure-5-19): Original model vs. much larger model on IMDB review
    classification'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5.19](#figure-5-19)：IMDB 评论分类中的原始模型与较大模型对比'
- en: Adding weight regularization
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 添加权重正则化
- en: 'You may be familiar with the principle of *Occam’s razor*: given two explanations
    for something, the explanation most likely to be correct is the simplest one —
    the one that makes fewer assumptions. This idea also applies to the models learned
    by neural networks: given some training data and a network architecture, multiple
    sets of weight values (multiple *models*) could explain the data. Simpler models
    are less likely to overfit than complex ones.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能熟悉*奥卡姆剃刀原理*：对于某种事物的两种解释，最可能正确的是最简单的一种——即做出较少假设的那一种。这种想法也适用于神经网络学习到的模型：给定一些训练数据和网络架构，多组权重值（多个*模型*）可以解释数据。简单模型比复杂模型更不容易过拟合。
- en: 'A *simple model* in this context is a model where the distribution of parameter
    values has less entropy (or a model with fewer parameters, as you saw in the previous
    section). Thus a common way to mitigate overfitting is to put constraints on the
    complexity of a model by forcing its weights to take only small values, which
    makes the distribution of weight values more *regular*. This is called *weight
    regularization*, and it’s done by adding to the loss function of the model a cost
    associated with having large weights. This cost comes in two flavors:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景下，*简单模型*是指参数值分布的熵较低（或者与上一节中看到的相比，参数更少的模型）。因此，一种常见的减轻过拟合的方法是通过强制权重只取较小的值来对模型的复杂性施加约束，这使得权重值的分布更加*规则*。这被称为*权重正则化*，它通过向模型的损失函数中添加与权重大的成本相关联的成本来实现。这种成本有两种形式：
- en: '*L1 regularization*  — The cost added is proportional to the *absolute value
    of the weight coefficients* (the *L1 norm* of the weights).'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*L1正则化* — 添加的成本与权重系数的*绝对值*（权重的*L1范数*）成比例。'
- en: '*L2 regularization* — The cost added is proportional to the *square of the
    value of the weight coefficients* (the *L2 norm* of the weights). L2 regularization
    is also called *weight decay* in the context of neural networks. Don’t let the
    different name confuse you: weight decay is mathematically the same as L2 regularization.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*L2正则化* — 添加的成本与权重系数的*平方值*（权重的*L2范数*）成比例。在神经网络背景下，L2正则化也称为*权重衰减*。不要让不同的名称混淆你：权重衰减在数学上与L2正则化相同。'
- en: In Keras, weight regularization is added by passing *weight regularizer instances*
    to layers as keyword arguments. Let’s add L2 weight regularization to the movie
    review classification model.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中，通过将*权重正则化实例*作为关键字参数传递给层来添加权重正则化。让我们将L2权重正则化添加到电影评论分类模型中。
- en: '[PRE14]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[Listing 5.12](#listing-5-12): Adding L2 weight regularization to the model'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 5.12](#listing-5-12)：将L2权重正则化添加到模型中'
- en: '`l2(0.002)` means every coefficient in the weight matrix of the layer will
    add `0.002 * weight_coefficient_value ** 2` to the total loss of the model. Note
    that because this penalty is *only added at training time*, the loss for this
    model will be much higher at training than at test time.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '`l2(0.002)` 表示层中每个权重矩阵的系数都将添加 `0.002 * weight_coefficient_value ** 2` 到模型的总体损失中。请注意，由于这种惩罚*仅在训练时添加*，因此该模型的损失在训练时比测试时高得多。'
- en: 'Figure 5.20 shows the effect of the L2 regularization penalty. As you can see,
    the model with L2 regularization has become much more resistant to overfitting
    than the reference model, even though both models have the same number of parameters:
    see figure 5.20:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.20 展示了 L2 正则化惩罚的效果。如您所见，具有 L2 正则化的模型比参考模型对过拟合的抵抗力更强，尽管两个模型具有相同数量的参数：见图 5.20：
- en: '![](../Images/a8ba8a378919e7d2c88c940518b94422.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/a8ba8a378919e7d2c88c940518b94422.png)'
- en: '[Figure 5.20](#figure-5-20): Effect of L2 weight regularization on validation
    loss'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5.20](#figure-5-20)：L2 权重正则化对验证损失的影响'
- en: As an alternative to L2 regularization, you can use one of the following Keras
    weight regularizers.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 L2 正则化的替代方案，您可以使用以下 Keras 权重正则化器之一。
- en: '[PRE15]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[Listing 5.13](#listing-5-13): Different weight regularizers available in Keras'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表 5.13](#listing-5-13)：Keras 中可用的不同权重正则化器'
- en: 'Note that weight regularization is more typically used for smaller deep learning
    models. Large deep learning models tend to be so overparameterized that imposing
    constraints on weight values does not have much effect on model capacity and generalization.
    In these cases, a different regularization technique is preferred: *dropout*.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，权重正则化通常用于较小的深度学习模型。大型深度学习模型往往过度参数化，对权重值施加约束对模型容量和泛化能力的影响不大。在这些情况下，更倾向于使用不同的正则化技术：*dropout*。
- en: Adding dropout
  id: totrans-237
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 添加 dropout
- en: '*Dropout*, developed by Geoff Hinton and his students at the University of
    Toronto, is one of the most effective and most commonly used regularization techniques
    for neural networks. Dropout, applied to a layer, consists of randomly *dropping
    out* (setting to zero) a number of output features of the layer during training.
    Let’s say a given layer would normally return a vector `[0.2, 0.5, 1.3, 0.8, 1.1]`
    for a given input sample during training. After applying dropout, this vector
    will have a few zero entries distributed at random: for example, `[0, 0.5, 1.3,
    0, 1.1]`. The *dropout rate* is the fraction of the features that are zeroed out;
    it’s usually set between 0.2 and 0.5\. At test time, no units are dropped out;
    instead, the layer’s output values are scaled down by a factor equal to the dropout
    rate, to balance for the fact that more units are active than at training time.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '*Dropout*，由多伦多大学的 Geoffrey Hinton 及其学生开发，是神经网络中最有效且最常用的正则化技术之一。Dropout 在层中的应用包括在训练期间随机
    *丢弃*（置零）一定数量的层输出特征。假设一个给定的层在训练期间对于一个给定的输入样本通常会返回一个向量 `[0.2, 0.5, 1.3, 0.8, 1.1]`。在应用
    dropout 后，这个向量将会有一些随机分布的零值：例如，`[0, 0.5, 1.3, 0, 1.1]`。*dropout 率* 是被置零的特征的比例；它通常设置在
    0.2 到 0.5 之间。在测试时，不丢弃任何单元；相反，层的输出值按 dropout 率的因子进行缩放，以平衡训练时比测试时更活跃的单元数量。'
- en: 'Consider a NumPy matrix containing the output of a layer, `layer_output`, of
    shape `(batch_size, features)`. At training time, we zero-out at random a fraction
    of the values in the matrix:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个包含层输出 `layer_output` 的 NumPy 矩阵，其形状为 `(batch_size, features)`。在训练时，我们随机将矩阵中的一部分值置零：
- en: '[PRE16]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'At test time, we scale down the output by the dropout rate. Here, we scale
    by 0.5 (because we previously dropped half the units):'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试时，我们将输出按 dropout 率进行缩放。这里，我们按 0.5 缩放（因为我们之前丢弃了一半的单元）：
- en: '[PRE17]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Note that this process can be implemented by doing both operations at training
    time and leaving the output unchanged at test time, which is often the way it’s
    implemented in practice (see figure 5.21):'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个过程可以通过在训练时执行这两个操作，并在测试时保持输出不变来实现，这在实践中通常是这样做的方式（见图 5.21）：
- en: '[PRE18]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](../Images/efa2213fd7e01bbc65bc2b5e8326ebaf.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/efa2213fd7e01bbc65bc2b5e8326ebaf.png)'
- en: '[Figure 5.21](#figure-5-21): Dropout applied to an activation matrix at training
    time, with rescaling happening during training. At test time, the activation matrix
    is unchanged.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5.21](#figure-5-21)：在训练时应用于激活矩阵的 dropout，训练过程中进行缩放。在测试时，激活矩阵保持不变。'
- en: 'This technique may seem strange and arbitrary. Why would this help reduce overfitting?
    Hinton says he was inspired by, among other things, a fraud-prevention mechanism
    used by banks:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术可能看起来很奇怪且随意。为什么这有助于减少过拟合？Hinton 表示，他受到了，包括但不限于，银行使用的欺诈预防机制的启发：
- en: I went to my bank. The tellers kept changing and I asked one of them why. He
    said he didn’t know but they got moved around a lot. I figured it must be because
    it would require cooperation between employees to successfully defraud the bank.
    This made me realize that randomly removing a different subset of neurons on each
    example would prevent conspiracies and thus reduce overfitting.
  id: totrans-248
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我去银行。出纳员一直在更换，我询问其中一位为什么。他说他不知道，但他们经常被调动。我想这肯定是因为这需要员工之间的合作才能成功欺诈银行。这让我意识到，在每一个例子中随机移除不同子集的神经元可以防止阴谋，从而减少过拟合。
- en: The core idea is that introducing noise in the output values of a layer can
    break up happenstance patterns that aren’t significant (what Hinton refers to
    as *conspiracies*), which the model will start memorizing if no noise is present.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 核心思想是在层的输出值中引入噪声可以打破不重要的偶然模式（Hinton称之为**阴谋**），如果没有噪声，模型将开始记忆这些模式。
- en: In Keras, you can introduce dropout in a model via the `Dropout` layer, which
    is applied to the output of the layer right before it. Let’s add two `Dropout`
    layers in the IMDB model to see how well they do at reducing overfitting.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中，你可以通过`Dropout`层在模型中引入dropout，该层应用于层之前的输出。让我们在IMDB模型中添加两个`Dropout`层，看看它们在减少过拟合方面做得如何。
- en: '[PRE19]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[Listing 5.14](#listing-5-14): Adding dropout to the IMDB model'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表5.14](#listing-5-14)：在IMDB模型中添加dropout'
- en: 'Figure 5.22 shows a plot of the results. This is a clear improvement over the
    reference model. It also seems to be working much better than L2 regularization
    since the lowest validation loss reached has improved:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.22显示了结果的图表。这比参考模型有明显的改进。它似乎比L2正则化工作得更好，因为最低的验证损失已经得到改善：
- en: '![](../Images/e72c698c6b306d49d152835add95f515.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e72c698c6b306d49d152835add95f515.png)'
- en: '[Figure 5.22](#figure-5-22): Effect of dropout on validation loss'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5.22](#figure-5-22)：dropout对验证损失的影响'
- en: 'To recap, these are the most common ways to maximize generalization and prevent
    overfitting in neural networks:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，这些是在神经网络中最大化泛化并防止过拟合的最常见方法：
- en: Getting more training data, or better training data
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取更多或更好的训练数据
- en: Developing better features
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发更好的特征
- en: Reducing the capacity of the model
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低模型的容量
- en: Adding weight regularization (for smaller models)
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加权重正则化（适用于较小的模型）
- en: Adding dropout
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加dropout
- en: Summary
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'The purpose of a machine learning model is to *generalize*: to perform accurately
    on never-before-seen inputs. It’s harder than it seems.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习模型的目的在于**泛化**：在从未见过的输入上执行准确。这比看起来要难。
- en: A deep neural network achieves generalization by learning a parametric model
    that can successfully *interpolate* between training samples. Such a model can
    be said to have learned the *latent manifold* of the training data. This is why
    deep learning models can only make sense of inputs that are very close to what
    they’ve seen during training.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度神经网络通过学习一个可以成功在训练样本之间进行**插值**的参数模型来实现泛化。这样的模型可以说已经学会了训练数据的**潜在流形**。这就是为什么深度学习模型只能理解在训练期间所见到的非常接近的输入。
- en: 'The fundamental problem in machine learning is *the tension between optimization
    and generalization*: to attain generalization, you must first achieve a good fit
    to the training data, but improving your model’s fit to the training data will
    inevitably start hurting generalization after a while. Every single deep learning
    best practice deals with managing this tension.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习中的基本问题是**优化与泛化之间的张力**：为了实现泛化，你必须首先对训练数据有一个良好的拟合，但提高你的模型对训练数据的拟合最终会损害泛化。每一个深度学习最佳实践都涉及管理这种张力。
- en: The ability of deep learning models to generalize comes from the fact that they
    manage to learn to approximate the *latent manifold* of their data and can thus
    make sense of new inputs via interpolation.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习模型能够泛化的能力源于它们能够学会近似其数据的**潜在流形**，因此可以通过插值理解新的输入。
- en: It’s essential to be able to accurately evaluate the generalization power of
    your model while you’re developing it. You have at your disposal an array of evaluation
    methods, from simple hold-out validation to K-fold cross-validation and iterated
    K-fold cross-validation with shuffling. Remember to always keep a completely separate
    test set for final model evaluation, since information leaks from your validation
    data to your model may have occurred.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在开发模型的过程中，能够准确评估模型的一般化能力至关重要。你可以使用一系列评估方法，从简单的保留验证到K折交叉验证和带洗牌的迭代K折交叉验证。请记住，始终为最终模型评估保留一个完全独立的测试集，因为你的验证数据可能已经泄露到模型中。
- en: When you start working on a model, your goal is first to achieve a model that
    has some generalization power and that can overfit. Best practices to do this
    include tuning your learning rate and batch size, using better architecture priors,
    increasing model capacity, or simply training longer.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你开始构建模型时，你的目标是首先实现一个具有一定一般化能力且可以过拟合的模型。实现这一目标的最佳实践包括调整学习率和批量大小、使用更好的架构先验、增加模型容量或简单地延长训练时间。
- en: As your model starts overfitting, your goal switches to improving generalization
    through *model regularization*. You can reduce your model’s capacity, add dropout
    or weight regularization, and use early stopping. And naturally, a larger or better
    dataset is always the number one way to help a model generalize.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你的模型开始过拟合时，你的目标转变为通过*模型正则化*来提高一般化能力。你可以减少模型容量，添加dropout或权重正则化，并使用提前停止。当然，更大的或更好的数据集始终是帮助模型实现一般化的首要方法。
- en: Footnotes
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 脚注
- en: Mark Twain even called it “the most delicious fruit known to men.” [[↩]](#footnote-link-1)
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 马克·吐温甚至称其为“人类已知最美味的水果。” [[↩]](#footnote-link-1)
