<html><head></head><body><div id="book-content" class="calibre2"><div id="sbo-rt-content" class="calibre3"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 6. Assembling the Prompt" class="calibre6"><div class="preface" id="ch06a_assembling_the_prompt_1728442733857948">
<h1 class="calibre5"><span class="firstname">Chapter 6. </span>Assembling the Prompt</h1>

<p class="subtitle">In the previous chapters, you gathered a wealth of content that will serve as the building blocks for your prompt. Now, it’s time to put these pieces together and craft a prompt that effectively communicates your needs. This chapter will guide you through the process of shaping your prompt by first exploring the different structures and options available to you. How you choose to organize these individual snippets will play a crucial role in the effectiveness of your final prompt.</p>

<p class="subtitle">The<a contenteditable="false" data-primary="prompt assembly" data-secondary="key to" data-type="indexterm" id="id699" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> next step involves triaging your content—deciding what to keep and what to discard so that it will fit within any size constraints you might have. This process is key to refining your prompt and ensuring it remains focused and relevant. With your content finalized, you’ll then move on to assembling your prompt, which will be your tool for eliciting relevant, coherent, and contextually accurate responses from the model. Let’s dive in.</p>

<section data-type="sect1" data-pdf-bookmark="Anatomy of the Ideal Prompt" class="calibre6"><div class="preface" id="ch06a_anatomy_of_the_ideal_prompt_1728442733858193">
<h1 class="calibre5">Anatomy of the Ideal Prompt</h1>

<p class="subtitle">Before<a contenteditable="false" data-primary="prompt assembly" data-secondary="anatomy of ideal prompts" data-type="indexterm" id="PAideal06" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> we go into the details of how to get there, let’s visualize where we want to go. Take a look at <a data-type="xref" href="#ch06a_figure_1_1728442733835135" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 6-1</a>, which gives a bird’s eye view of how your prompt <em class="hyperlink">should</em> look. We’ll go through its elements one at a time. Concise and crisp prompts are generally more effective—plus, they use less computational power and are processed more quickly. Additionally, you have a hard cut-off with the<a contenteditable="false" data-primary="context window" data-type="indexterm" id="id700" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> context window size.</p>

<p class="subtitle">As discussed in <a data-type="xref" href="ch05.html#ch05_prompt_content_1728435524680844" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Chapter 5</a>, a prompt consists of<a contenteditable="false" data-primary="prompt content" data-secondary="anatomy of ideal prompts" data-type="indexterm" id="pelements06" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> elements drawn from dynamic context and static instructions that clarify your question. There are no hard rules for the size or number of these elements. In fact, as applications evolve, a large prompt element might be broken into several smaller ones for more precise constructions. We’ve worked on projects with prompts ranging from just three lengthy elements to hundreds of one-line elements.</p>

<figure class="calibre22"><div id="ch06a_figure_1_1728442733835135" class="figure"><img src="assets/pefl_0601.png" class="calibre23"/>
<h6 class="calibre24"><span class="firstname">Figure 6-1. </span>Anatomy of a well-constructed prompt</h6>
</div></figure>

<p class="subtitle">There’s no theoretical rule that each prompt element must end with a<a contenteditable="false" data-primary="newline character" data-type="indexterm" id="id701" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> newline character. However, in practice, enforcing the rule that all elements end with newlines can simplify your string manipulation code. It can also assist with token length computation, depending on the tokenizer used (more on that next). If your prompt elements don’t easily fit this format, don’t feel compelled to force it.</p>

<p class="subtitle">Most prompts include a handful of certain elements. First is the<a contenteditable="false" data-primary="introduction (prompt element)" data-type="indexterm" id="id702" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <em class="hyperlink">introduction,</em> which helps you clarify the type of document you’re writing and sets up the model to approach the rest of the content correctly. The introduction sets the context for everything that follows. For example, if the model states, “This is about recommending a book,” it will focus on relevant aspects for book recommendations and interpret the context accordingly. The introduction also lets the model begin thinking about the problem from the start. Since the model has a fixed “thought budget” per token and can’t pause for deeper reflection, guiding its focus early on can improve its output.</p>

<p class="subtitle">Most prompts have only one introduction to set up the main question. But the principle also applies to subsections of the prompt: if there are some pieces of context where the model needs to focus on a certain aspect, it helps if you set up that aspect in the beginning.</p>

<p class="subtitle">After the introduction, you’ll see a long parade of different prompt elements. The model will try to make good use of all of them, but not equally. All LLMs are subject to two effects:</p>

<dl class="stafflist">
	<dt class="calibre13"><a href="https://browse.arxiv.org/pdf/2302.11042" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"><em class="hyperlink">In-context learning</em></a></dt>
	<dd class="calibre14">
	<p class="subtitle">The<a contenteditable="false" data-primary="in-context learning" data-type="indexterm" id="id703" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> closer a piece of information is to the end of the prompt, the more impact it has on the model.</p>
	</dd>
	<dt class="calibre13"><em class="hyperlink">The </em><a href="https://browse.arxiv.org/pdf/2307.03172.pdf" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"><em class="hyperlink">lost middle phenomenon</em></a></dt>
	<dd class="calibre14">
	<p class="subtitle">While<a contenteditable="false" data-primary="lost middle phenomenon" data-type="indexterm" id="id704" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> the model can easily recall the beginning and end of the prompt, it struggles with the information stuffed in the middle.</p>
	</dd>
</dl>

<p class="subtitle">These two dynamics create the<a contenteditable="false" data-primary="Valley of Meh" data-type="indexterm" id="id705" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <em class="hyperlink">Valley of Meh</em>, as we like to call it. The valley lies around the early middle of the prompt, and the context that is placed there is not used as effectively as the context in the beginning or the second half of the document. How deep the Valley of Meh is and its exact location depends on the model, but all models have it—as do humans!</p>

<p class="subtitle">The Valley of Meh is most problematic with large prompts, and there’s no perfect solution. You can reduce its impact by placing key, high-quality prompt elements outside the Valley of Meh and by filtering context to keep the prompt as concise as possible.</p>

<p class="subtitle">When you’ve included all the context, it’s time to remind the model of the main question. We call this the<a contenteditable="false" data-primary="refocus (prompt element)" data-type="indexterm" id="id706" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <em class="hyperlink">refocus,</em> which is necessary for longer prompts, where you’ve spent a long time adding context and you need to focus the model’s attention back to the question. Most prompt engineers use the<a contenteditable="false" data-primary="sandwich technique" data-type="indexterm" id="id707" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <em class="hyperlink">sandwich technique</em>, in which they start and end the prompt by clearly stating what they want the model to do (see <a data-type="xref" href="#ch06a_table_1_1728442733843102" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Table 6-1</a>).</p>

<table class="calibre27" id="ch06a_table_1_1728442733843102">
	<caption class="calibre28"><span class="firstname">Table 6-1. </span>Sandwiching the context between two versions of the same question for a model with the ChatML API</caption>
	<thead class="calibre29">
		<tr class="calibre30">
			<th class="calibre31">Prompt part</th>
			<th class="calibre31">Sandwich</th>
			<th class="calibre31">Prompt</th>
		</tr>
	</thead>
	<tbody class="calibre32">
		<tr class="calibre30">
			<td rowspan="3" class="calibre33">Introduction</td>
			<td class="calibre33"> </td>
			<td class="calibre33">
			<pre data-type="programlisting" class="calibre49">
[{"role": "system", "content" : "You are a helpful AI.”},</pre>
			</td>
		</tr>
		<tr class="calibre34">
			<td class="calibre33">Sandwich part 1</td>
			<td class="calibre33">
			<pre data-type="programlisting" class="calibre49">
{"role": "user", "content" : "I want to suggest to Fiona 
an idea for her next book to read.”</pre>
			</td>
		</tr>
		<tr class="calibre30">
			<td class="calibre33"> </td>
			<td class="calibre33">
			<pre data-type="programlisting" class="calibre49">
Please ask any questions you need to arrive at an informed
suggestion."}, {"role": "assistant", "content" : "Of course! 
The following information might be useful: What books did 
she read last?”},</pre>
			</td>
		</tr>
		<tr class="calibre34">
			<td rowspan="5" class="calibre33">Context</td>
			<td rowspan="5" class="calibre33"> </td>
			<td class="calibre33"><pre data-type="programlisting" class="calibre49">{"role": "user", "content" : "Harry Potter, Lioness 
Rampant, Mr Lemoncello’s Library”},</pre></td>
		</tr>
		<tr class="calibre30">
			<td class="calibre33"><pre data-type="programlisting" class="calibre49">{"role": "assistant", "content" : "What did she post on 
social media recently?”}, {"role": "user", "content" : […]</pre></td>
		</tr>
		<tr class="calibre34">
			<td class="calibre33"><code class="calibre48">[…]</code></td>
		</tr>
		<tr class="calibre30">
			<td class="calibre33"><code class="calibre47">[…]</code></td>
		</tr>
		<tr class="calibre34">
			<td class="calibre33"><code class="calibre48">[…]</code></td>
		</tr>
		<tr class="calibre30">
			<td rowspan="2" class="calibre33">Refocus + Transition</td>
			<td class="calibre33"> </td>
			<td class="calibre33">
			<pre data-type="programlisting" class="calibre49">
{"role": "assistant", "content" : "I believe this is all 
the information I need to select a single best candidate 
book suggestion.”},</pre>
			</td>
		</tr>
		<tr class="calibre34">
			<td class="calibre33">Sandwich part 2</td>
			<td class="calibre33">
			<pre data-type="programlisting" class="calibre49">
{"role": "user", "content" : "Excellent! So based on 
this, which book should I suggest to her?”}]</pre>
			</td>
		</tr>
	</tbody>
</table>

<p class="subtitle">The refocus can be as short as half a line, but it’s common to include key clarifications here. The introduction sets the stage (“I’m thinking about book suggestions for X.”), while the refocus gives clear details (“What’s the best book to recommend next, focusing on narrative prose currently available?”). If the clarification gets lengthy, you might need a brief refocus at the end, especially when discussing the output format.</p>

<p class="subtitle">The very last part of your prompt should firmly<a contenteditable="false" data-primary="transitioning" data-type="indexterm" id="id708" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> transition from explaining the problem to solving the problem—that’s the bit you want the LLM to help with, after all. It’s not helpful if it just keeps adding more (likely made-up) context to your main question.</p>

<p class="subtitle">When using a chatlike interface, this part is usually as simple as including a question mark at the end. RLHF has drilled those models to respond by solving the last question stated—or sometimes even just implied—in their input. Some commercial platforms, like OpenAI’s ChatGPT, automatically signal when the assistant should begin its response after receiving a prompt through their API. However, traditional completion models require more explicit guidance to achieve the same effect.</p>

<p class="subtitle">The most common way to transition—especially when using a completion API—is to change your perspective from problem poser to problem solver and begin writing the answer for the model. This way, the model has no choice but to present its solution. <a data-type="xref" href="#ch06a_figure_2_1728442733835180" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 6-2</a> demonstrates the difference a good transition can make in getting an answer out of the model. Note that the opening quotation mark ending the transition in column three is still part of the prompt.</p>

<figure class="calibre22"><div id="ch06a_figure_2_1728442733835180" class="figure"><img src="assets/pefl_0602.png" class="calibre23"/>
<h6 class="calibre24"><span class="firstname">Figure 6-2. </span>Three variations of transition: missing, on the left; naive, in the middle; and refined, on the right (all completions [shaded backgrounds] obtained using OpenAI’s text-davinci-002, which is a completion model, rather than a chat model)</h6>
</div></figure>

<p class="subtitle">As shown in <a data-type="xref" href="#ch06a_figure_2_1728442733835180" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 6-2</a>, you can often merge the refocus and the transition. In those cases, you write the beginning of the answer that is just restating or summarizing the problem statement. The actual answer is then supplied by the model.<a contenteditable="false" data-primary="" data-startref="PAideal06" data-type="indexterm" id="id709" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="" data-startref="pelements06" data-type="indexterm" id="id710" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/></p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="What Kind of Document?" class="calibre6"><div class="preface" id="ch06a_what_kind_of_document_1728442733858281">
<h1 class="calibre5">What Kind of Document?</h1>

<p class="subtitle">A<a contenteditable="false" data-primary="prompt assembly" data-secondary="selecting document type" data-type="indexterm" id="PAtype06" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> prompt and completion together form a document, and as the<a contenteditable="false" data-primary="Red Riding Hood principle" data-type="indexterm" id="id711" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="Little Red Riding Hood principle" data-type="indexterm" id="id712" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> Little Red Riding Hood principle from <a data-type="xref" href="ch04.html#ch04_designing_llm_applications_1728407230643376" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Chapter 4</a> suggests, it’s best to use documents similar to those in the training data so that the format of the completion will be easy to anticipate. But what type of document should you aim for? There are several useful types, each with room for personalization. Let’s explore the most common ones and when to use each.</p>

<section data-type="sect2" data-pdf-bookmark="The Advice Conversation" class="calibre6"><div class="preface" id="ch06a_the_advice_conversation_1728442733858353">
<h2 class="calibre19">The Advice Conversation</h2>

<p class="subtitle">In<a contenteditable="false" data-primary="document types" data-secondary="advice conversation" data-type="indexterm" id="DTadvice06" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="advice conversations" data-type="indexterm" id="adconver06" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> the most common archetype, your document represents a conversation between two people. One asks for some kind of help, and the other provides it. The one who asks for help represents either your application or its user, while the model will take on the role of help provider.</p>

<p class="subtitle">This approach is ideal for chat models, but even<a contenteditable="false" data-primary="completion models" data-secondary="advice conversations" data-type="indexterm" id="id713" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> completion models can benefit from it. In fact, OpenAI developed ChatML to focus on advice conversations because they believed they were the most universally useful and easiest to implement. Advice conversations have many advantages, including the following:</p>

<dl class="stafflist">
	<dt class="calibre13">Natural interaction</dt>
	<dd class="calibre14">
	<p class="subtitle">It’s<a contenteditable="false" data-primary="interactions" data-secondary="natural" data-type="indexterm" id="id714" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> easy for people to think in terms of conversations. You can pose a question directly to the model and take its continuation as the answer to simplify interactions.</p>
	</dd>
	<dt class="calibre13">Multiround interactions</dt>
	<dd class="calibre14">
	<p class="subtitle">For<a contenteditable="false" data-primary="multi-round interactions" data-type="indexterm" id="id715" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="interactions" data-secondary="multi-round" data-type="indexterm" id="id716" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> complex interactions, you can continue the prompt with new questions and answers, making it easier to manage and break down the conversation. This approach allows you to add your logic between questions and helps the model handle each query directly.</p>
	</dd>
	<dt class="calibre13">Real-world integration</dt>
	<dd class="calibre14">
	<p class="subtitle">Conversations<a contenteditable="false" data-primary="integration, real-world" data-type="indexterm" id="id717" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="real-world integration" data-type="indexterm" id="id718" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> work well for multiround processes and integrating with real-world tools and techniques, whether you’re using a chat model or a completion model with conversational documents.</p>
	</dd>
</dl>

<p class="subtitle">If you use this structure with a chat model, you’ll get the additional advantages of RLHF related to compliance with your instructions. But if you use it with a completion model instead, you can avoid any RLHF trait that’s unhelpful for your scenario<a contenteditable="false" data-primary="content policing" data-type="indexterm" id="id719" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="stylistic habits" data-type="indexterm" id="id720" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> (e.g., stylistic habits, content policing).</p>

<p class="subtitle">On the other hand, if you happen to be using a completion model, you can use a trick called<a contenteditable="false" data-primary="inception approach" data-type="indexterm" id="id721" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <em class="hyperlink">inception</em>, in which you dictate the beginning of the answer. Do you remember the 2010 movie <em class="hyperlink">Inception</em>? Same idea here—start the answer for the model and the model will think that it’s the one that came up with it, and it will generate the rest of the completion accordingly. This approach can improve model compliance and make answers easier to parse, as well as help you avoid uncertainty about whether the response will start with a general statement or get straight to the point.</p>

<p class="subtitle">When writing a prompt for a completion model, you’ll have to decide the format for your transcript. The good news is that LLMs are used to many different formats, as shown in <a data-type="xref" href="#ch06a_table_2_1728442733843133" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Table 6-2</a>, in which the same conversation is cast into different formats. Note that, typically, an application would provide more context for this question.</p>

<table id="ch06a_table_2_1728442733843133" class="calibre27">
	<caption class="calibre28"><span class="firstname">Table 6-2. </span>Asking an LLM for ideas to plan your day</caption>
	<thead class="calibre29">
		<tr class="calibre30">
			<th class="calibre31">Format</th>
			<th class="calibre31">Freeform text</th>
			<th class="calibre31">Script format</th>
			<th class="calibre31">Markerless</th>
			<th class="calibre31">Structured</th>
		</tr>
	</thead>
	<tbody class="calibre32">
		<tr class="calibre30">
			<td class="calibre33">Introduction</td>
			<td class="calibre33">I asked my husband, “What shall we do tomorrow?”</td>
			<td class="calibre33">Me: What shall we do tomorrow?</td>
			<td class="calibre33">What shall we do tomorrow?</td>
			<td class="calibre33">
			<p class="calibre44">&lt;me&gt;</p>

			<p class="calibre44">What shall we do tomorrow?</p>

			<p class="calibre44">&lt;/me&gt;</p>
			</td>
		</tr>
		<tr class="calibre34">
			<td class="calibre33">Context</td>
			<td class="calibre33">
			<p class="calibre44">and he replied, “Well, what’s the weather like?”</p>

			<p class="calibre44">I reported that “We expect a balmy 75 degrees with sunshine in the whole Boston area.”</p>
			</td>
			<td class="calibre33">
			<p class="calibre44">Husband: Well, what’s the weather like?</p>

			<p class="calibre44">Me: We expect a balmy 75 degrees with sunshine in the whole Boston area.</p>
			</td>
			<td class="calibre33">
			<p class="calibre44">Well, what’s the weather like?</p>

			<p class="calibre44">We expect a balmy 75 degrees with sunshine in the whole Boston area.</p>
			</td>
			<td class="calibre33">
			<p class="calibre44">&lt;husband&gt;</p>

			<p class="calibre44">Well, what’s the weather like?</p>

			<p class="calibre44">&lt;/husband&gt;</p>

			<p class="calibre44">&lt;me&gt;</p>

			<p class="calibre44">We expect a balmy 75 degrees with sunshine in the whole Boston area.</p>

			<p class="calibre44">&lt;/me&gt;</p>
			</td>
		</tr>
		<tr class="calibre30">
			<td class="calibre33">Refocus</td>
			<td class="calibre33">He reflected a bit about what would be a good Sunday activity for us,</td>
			<td class="calibre33">Husband (after reflection about good Sunday activities)</td>
			<td class="calibre33">(reflects about good Sunday activities)</td>
			<td class="calibre33">&lt;direction&gt; Husband reflects about good Sunday activities &lt;/direction&gt;</td>
		</tr>
		<tr class="calibre34">
			<td class="calibre33">Transition</td>
			<td class="calibre33">and he suggested,</td>
			<td class="calibre33">:</td>
			<td class="calibre33">I suggest we should</td>
			<td class="calibre33">&lt;husband&gt;</td>
		</tr>
		<tr class="calibre30">
			<td class="calibre33">Completion (text-davinci-003)</td>
			<td class="calibre33">“Let’s go on an outing to the beach, then. We could grab some coffee and sandwiches and have a picnic on the sand. We could sit on a blanket and watch the waves. Sounds nice.”</td>
			<td class="calibre33">I think it would be fun to take a bike ride, maybe in one of the nearby parks. Maybe we can bring a picnic lunch and make a day of it outside in the sunshine.</td>
			<td class="calibre33">take a picnic and a book down to the Charles River and spend the day lounging, reading, and enjoying the sunshine.</td>
			<td class="calibre33">If it’s going to be so nice, how about a hike in the Blue Hills? We can check out the reservoir, have a picnic, and watch the sunset. &lt;/husband&gt;</td>
		</tr>
	</tbody>
</table>

<p class="subtitle">While all formats are effective, each of them has unique strengths, and we’ve arranged them so that each format addresses the weaknesses of the one before it:</p>

<dl class="stafflist">
	<dt class="calibre13">Freeform text</dt>
	<dd class="calibre14">
	<p class="subtitle">This<a contenteditable="false" data-primary="freeform text format" data-type="indexterm" id="id722" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> allows you to insert various types of information between quotes, but it is challenging to assemble on the fly. It can be difficult to create a reliable system for dynamically generating prompts with many elements.</p>
	</dd>
	<dt class="calibre13">Transcript format</dt>
	<dd class="calibre14">
	<p class="subtitle">This<a contenteditable="false" data-primary="transcript format" data-type="indexterm" id="id723" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> is easy to assemble but less effective for long or formatted elements (like source code with important indentation).</p>
	</dd>
	<dt class="calibre13">Markerless format</dt>
	<dd class="calibre14">
	<p class="subtitle">This<a contenteditable="false" data-primary="markerless format" data-type="indexterm" id="id724" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> works well with formatted text and longer pieces (such as pasted emails), but it can be difficult for the model to track speakers and for the application to determine when the model’s response ends and the next input begins.</p>
	</dd>
	<dt class="calibre13">Structured format</dt>
	<dd class="calibre14">
	<p class="subtitle">This<a contenteditable="false" data-primary="structured format" data-type="indexterm" id="id725" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> clearly indicates who is speaking and when they finish. Various structures are available, and they are detailed in <a data-type="xref" href="#ch06a_the_structured_document_1728442733858464" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">“The Structured Document”</a>.</p>
	</dd>
</dl>

<p class="subtitle">In <a data-type="xref" href="ch03.html#ch03a_moving_toward_chat_1728432131625250" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Chapter 3</a>, we<a contenteditable="false" data-primary="reinforcement learning from human feedback (RLHF)" data-secondary="theatrical play metaphor" data-type="indexterm" id="id726" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="prompt engineering" data-secondary="theatrical play metaphor" data-type="indexterm" id="id727" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> introduced the notion that writing a conversational prompt is like playwriting. Except for stage directions, all parts of the text belong to one of the “roles” in the play. In conversations between an advice seeker and an assistant, you typically let the user write the speaking parts for the advice seeker and let the LLM write the speaking parts for the assistant. This needn’t be the case—there’s nothing stopping you, as a prompt engineer, from writing for the role of the assistant. This is another form of the<a contenteditable="false" data-primary="inception approach" data-type="indexterm" id="id728" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> inception approach—you speak for the assistant, and in all subsequent turns of the conversation, the assistant will act as if it actually said what you said.</p>

<div data-type="tip" class="calibre16"><h6 class="calibre17">Tip</h6>
<p class="subtitle">Writing a prompt from the assistant’s perspective helps frame context as if it’s responding to a question they asked. This approach ensures that the completion starts with the answer rather than another clarifying question.<a contenteditable="false" data-primary="" data-startref="DTadvice06" data-type="indexterm" id="id729" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="" data-startref="adconver06" data-type="indexterm" id="id730" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/></p>
</div>
</div></section>

<section data-type="sect2" data-pdf-bookmark="The Analytic Report" class="calibre6"><div class="preface" id="ch06a_the_analytic_report_1728442733858409">
<h2 class="calibre19">The Analytic Report</h2>

<p class="subtitle">Each year, millions of students are trained in<a contenteditable="false" data-primary="document types" data-secondary="analytic report" data-type="indexterm" id="DCanalytic06" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="analytic reports" data-type="indexterm" id="analytic06" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="report format" data-type="indexterm" id="repform06" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> report writing. They learn the art of crafting introductions, expositions, analyses, and conclusions, and once they graduate and enter the workforce, they churn out reports that analyze markets, weigh costs and benefits, and propose actionable conclusions. All of this is hard work, and fortunately, it serves a purpose: it provides excellent training material for LLMs. And these models are trained on vast datasets filled with reports of every kind and size.</p>

<p class="subtitle">Leveraging this abundance of reports is straightforward, especially if your task falls within domains where analytical reports are common, such as business, literature, science, or law (though it’s probably best to leave legal defense to human professionals). Reports are easy to structure because they follow a familiar format that usually starts with an introduction, leads to a conclusion, and often includes a recap. The information you’ve already gathered can be easily inserted into the discussion or background sections.</p>

<p class="subtitle">However, crafting static prompt elements like instructions requires some thought, especially if you’re making sure to keep things clear and to the point. A helpful strategy is to include a<a contenteditable="false" data-primary="scope section" data-type="indexterm" id="id731" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <em class="hyperlink">S</em><em class="hyperlink">cope</em> section that clearly defines the boundaries of the report. Instead of going back and forth in a dialogue to clarify exclusions (e.g., “Please suggest only novels, not self-help books.”), you can state up front, “This report focuses solely on novels, excluding self-help books.” LLMs tend to respect such clear boundaries more consistently in reports than in dialogues.</p>

<p class="subtitle">Reports also favor objective analysis, which lightens the cognitive load for the LLM by avoiding the need for it to simulate social interaction. That said, because analysis typically precedes the conclusion, you must ensure a clear transition when you want the model to shift into a decision-making mode. Otherwise, you might end up with a meandering response that requires extra parsing. On the flip side, this format lends itself well to chain-of-thought prompting, which is covered in more detail in <a data-type="xref" href="ch08.html#ch08_01_conversational_agency_1728429579285372" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Chapter 8</a>.</p>

<p class="subtitle">Dialogues can take many forms, depending on the context (see <a data-type="xref" href="#ch06a_table_2_1728442733843133" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Table 6-2</a>). However, for reports, we recommend that you consistently stick to one format: writing your prompts in<a contenteditable="false" data-primary="Markdown" data-secondary="benefits of for reports" data-type="indexterm" id="id732" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> Markdown. Here’s why:</p>

<ul class="stafflist">
	<li class="calibre9">
	<p class="calibre26">It’s pretty universal, and the internet is full of Markdown files, so LLMs know it well.</p>
	</li>
	<li class="calibre9">
	<p class="calibre26">Markdown is a simple, lightweight language with only a few key features. This makes it easy to write and straightforward for models to interpret the output.</p>
	</li>
	<li class="calibre9">
	<p class="calibre26">Markdown’s headings help define hierarchy, which allows you to organize prompt elements into clear sections that can be easily rearranged or omitted while maintaining structure.</p>
	</li>
	<li class="calibre9">
	<p class="calibre26">Another useful feature is that indentation doesn’t usually matter, but for technical content (e.g., source code), you can use blocks opened and closed with triple<a contenteditable="false" data-primary="triple backticks (```)" data-type="indexterm" id="id733" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="``` (backticks)" data-type="indexterm" id="id734" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="backticks (```)" data-type="indexterm" id="id735" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> backticks (```).</p>
	</li>
	<li class="calibre9">
	<p class="calibre26">If you want to display model output to the user directly, Markdown is very easy to render.</p>
	</li>
	<li class="calibre9">
	<p class="calibre26">Markdown’s hyperlink feature allows the model to include links that are easy to parse, which can help you verify sources and retrieve content programmatically.</p>
	</li>
</ul>

<p class="subtitle">Additionally, it’s common to give a<a contenteditable="false" data-primary="table of contents" data-type="indexterm" id="id736" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> table of contents at the beginning of the Markdown file, and that can be quite useful. A table of contents can serve as a useful part of the introduction for a long prompt because it helps models orient themselves just as much as it helps people. It can also be a great tool for controlling the completion, in two ways:</p>

<ol class="stafflist">
	<li class="calibre25">
	<p class="calibre26">For<a contenteditable="false" data-primary="chain-of-thought prompting" data-secondary="scratchpad approach" data-type="indexterm" id="id737" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> chain-of-thought prompting or managing overly verbose models, you can use a scratchpad approach. Adding sections like <code class="calibre15"># Ideas</code> or <code class="calibre15"># Analysis</code> before <code class="calibre15"># Conclusion</code> in the table of contents helps guide the model to a more informed conclusion while allowing you to ignore the earlier sections.</p>
	</li>
	<li class="calibre25">
	<p class="calibre26">You can easily signal when the model’s response should end by adding a section like <code class="calibre15"># Appendix</code> or <code class="calibre15"># Further Reading</code> after the conclusion. Setting <code class="calibre15"># Further Reading</code> as a stop sequence ensures the model finishes its task, conserving compute resources.</p>
	</li>
</ol>

<p class="subtitle">Both use cases for the table of contents are demonstrated in <a data-type="xref" href="#ch06a_figure_3_1728442733835211" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 6-3</a>. Note that since this is an example, the amount of context is less than what the model would need to give a proper answer to such a question. Also, LLMs aren’t oracles: with the appropriate context, a model can be a good tool for ideation, but there’s no reason its opinion should count for more than Jerry’s in Accounting.</p>

<figure class="calibre22"><div id="ch06a_figure_3_1728442733835211" class="figure"><img alt="A screenshot of a computer  Description automatically generated" src="assets/pefl_0603.png" class="calibre23"/>
<h6 class="calibre24"><span class="firstname">Figure 6-3. </span>A Markdown report using a table of contents (completion obtained using OpenAI’s text-davinci-003)<a contenteditable="false" data-primary="" data-startref="DCanalytic06" data-type="indexterm" id="id738" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="" data-startref="analytic06" data-type="indexterm" id="id739" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="" data-startref="repform06" data-type="indexterm" id="id740" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/></h6>
</div></figure>
</div></section>

<section data-type="sect2" data-pdf-bookmark="The Structured Document" class="calibre6"><div class="preface" id="ch06a_the_structured_document_1728442733858464">
<h2 class="calibre19">The Structured Document</h2>

<p class="subtitle">Structured documents<a contenteditable="false" data-primary="document types" data-secondary="structured documents" data-type="indexterm" id="DTstructured06" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="structured documents" data-type="indexterm" id="structfor06" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> follow a formal specification that allows you to make strong assumptions about the form of the completion. This makes parsing easier, including the parsing of complex outputs.</p>

<p class="subtitle">A great example of this is found in<a contenteditable="false" data-primary="Anthropic" data-secondary="Artifacts prompt" data-type="indexterm" id="Aartifactp06" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> Anthropic’s Artifacts prompt. Artifacts will come up again in the final chapter of this book, but for now, you should know that Artifacts<a contenteditable="false" data-primary="Artifacts" data-secondary="description of" data-type="indexterm" id="id741" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> are self-contained documents that the user and assistant collaborate on. Examples of Artifacts<a contenteditable="false" data-primary="Artifacts" data-secondary="examples of" data-type="indexterm" id="id742" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> are Python scripts, small React apps, mermaid diagrams, and scalable vector graphics (SVG) diagrams. Artifacts are presented in the UI as text in a pane to the right of the conversation, and in the case of React, Mermaid, and SVG, they are rendered into functioning or visual prototypes.</p>

<p class="subtitle">An abridged version of the<a contenteditable="false" data-primary="Artifacts" data-secondary="abridged Artifacts prompt" data-type="indexterm" id="Aartprompt06" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> Artifacts prompt is shown in <a data-type="xref" href="#ch06a_table_3_1728442733843157" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Table 6-3</a> (<a href="https://oreil.ly/Lwsp1" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">this prompt was extracted by @elder_plinius</a>). To make Artifacts work, the prompt uses an XML document structure that clearly delineates the pieces of the interaction. The <code class="calibre15">artifacts_info</code> prompt holds the equivalent of the system message, explaining how artifacts work. It includes an <code class="calibre15">examples</code> section with several <code class="calibre15">example</code> blocks. Each example has a <code class="calibre15">user_query</code> and an <code class="calibre15">assistant_response</code>.</p>

<p class="subtitle">Things get most interesting inside the <code class="calibre15">assistant_response</code>. First, the assistant starts its response, and then an <code class="calibre15">antThinking</code> block is injected so that the assistant can “think” about whether the user’s request should make use of an Artifact or just be handled as a normal conversation. If the decision is to use an Artifact, then the text will include an <code class="calibre15">antArtifact</code> block that contains the text of the Artifact. Notice that the <code class="calibre15">antArtifact</code> tag also includes attributes such as the title of the Artifact and the language used.</p>

<table id="ch06a_table_3_1728442733843157" class="calibre27">
	<caption class="calibre28"><span class="firstname">Table 6-3. </span>The Anthropic structured prompt used for generating Artifacts (abridged)</caption>
	<tbody class="calibre32">
		<tr class="calibre30">
			<td class="calibre33">Prompt</td>
			<td class="calibre33">
			<p class="calibre44"><code class="calibre47">The assistant can create and reference Artifacts during conversations. Artifacts are for substantial, self-contained content that users might modify or reuse, and they are displayed in a separate UI window for clarity.</code></p>

			<p class="calibre44"><code class="calibre47">Here are some examples of correct usage of Artifacts by other AI <span class="firstname">assistants</span>:</code></p>

			<pre data-type="programlisting" class="calibre49">
&lt;examples&gt;

  &lt;example&gt;
    &lt;user_query&gt;Can you help me create a Python script to calculate the
    factorial of a number?&lt;/user_query&gt;
    
    &lt;assistant_response&gt;
      Sure! Here's a Python script that calculates the factorial of a
      number:
    
      &lt;antThinking&gt;Creating a Python script to calculate factorials meets
      the criteria for a good Artifact. Therefore, I'm creating a new
      Artifact.
      &lt;/antThinking&gt;
    
      &lt;antArtifact identifier="factorial-script"
      type="application/vnd.ant.code" language="python"
      title="Simple Python factorial script"&gt;
        def factorial(n):
            if n == 0:
                return 1
            else:
                return n * factorial(n - 1)
    
      ...
    &lt;/assistant_response&gt;

  &lt;/example&gt;
    
  [...several examples omitted...]
    
&lt;/examples&gt;</pre>

			<p class="calibre44"><code class="calibre47">The assistant should always take care to not produce Artifacts that would be highly hazardous to human health or wellbeing if misused, even if is asked to produce them for seemingly benign reasons.</code></p> 
			
			<p class="calibre44"><code class="calibre47">&lt;/artifacts_info&gt;</code></p>

			<p class="calibre44"><code class="calibre47">Claude is now being connected with a human.</code></p>

			<p class="calibre44"><code class="calibre47">&lt;user_query&gt;</code></p>

			<p class="calibre44"><strong class="calibre51"><code class="calibre56">Can you help me create a Python script to factor a number into its prime factors?</code></strong></p>

			<p class="calibre44"><code class="calibre47">&lt;/user_query&gt;</code></p>

			<p class="calibre44"><code class="calibre47">&lt;assistant_response&gt;</code></p>
			</td>
		</tr>
		<tr class="calibre34">
			<td class="calibre33">Completion</td>
			<td class="calibre33">
			<p class="calibre44"><code class="calibre48">Sure! Here's a Python script that factors a number into its prime <span class="firstname">factors</span>:</code></p>

			<p class="calibre44"><code class="calibre48">&lt;antThinking&gt;This code is self-contained and is likely to be reused or modified. Therefore, I'm creating a new Artifact.&lt;/antThinking&gt;</code></p>

			<p class="calibre44"><code class="calibre48">&lt;antArtifact identifier="prime-factor-script"</code></p> 
			<p class="calibre44"><code class="calibre48">type="application/vnd.ant.code" language="python"</code></p> 
			<p class="calibre44"><code class="calibre48">title="Python script for factoring numbers into prime factors"&gt; def prime_factors(n):</code></p>

			<pre data-type="programlisting" class="calibre49">
    factors = []
    d = 2
    while n &gt; 1:
        while n % d == 0:
            factors.append(d)
            n /= d
        d += 1
    return factors
    
print(prime_factors(24)) # Output: [2, 2, 2, 3]
&lt;/assistant_response&gt;</pre>
			</td>
		</tr>
	</tbody>
</table>

<p class="subtitle">As a result of setting up this structured pattern in the prompt, it becomes much easier to parse out information from the response. In particular, <code class="calibre15">antThinking</code> is parsed out and hidden from the user, and <code class="calibre15">antArtifact</code> is extracted and placed into the Artifact pane, under the title specified in the attributes. (In <a data-type="xref" href="ch07.html#ch07_taming_the_model_1728407187651669" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Chapter 7</a>, we get into much more detail about how content can be extracted from the completions.)<a contenteditable="false" data-primary="" data-startref="Aartifactp06" data-type="indexterm" id="id743" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="" data-startref="Aartprompt06" data-type="indexterm" id="id744" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/></p>

<p class="subtitle">Like conversation transcripts, structured documents can come in many different formats. The<a contenteditable="false" data-primary="Red Riding Hood principle" data-type="indexterm" id="id745" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="Little Red Riding Hood principle" data-type="indexterm" id="id746" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> Little Red Riding Hood principle suggests that you use formats that are readily available in training data. The most suitable formats are XML and YAML. Both are common in technical documents where precision is of the essence, and both can be used in many different domains. In both cases, the whole document is hierarchically ordered into normally named elements, which can have several subelements.</p>

<p class="subtitle">In<a contenteditable="false" data-primary="XML format" data-type="indexterm" id="id747" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <a href="https://oreil.ly/NvPU4" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">XML</a> (see <a data-type="xref" href="#ch06a_table_3_1728442733843157" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Table 6-3</a>), the document consists of a series of tags that are opened and closed. The tag may have attributes and has content that may contain subtags. Choose XML if your individual elements are relatively short, and if they are multiline, indentation doesn’t matter. But you might need to be careful about<a contenteditable="false" data-primary="escape sequences" data-type="indexterm" id="id748" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> escape sequences: there are five<a contenteditable="false" data-primary="&amp;quot (&quot;)" data-type="indexterm" id="id749" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="&quot; (&amp;quot)" data-type="indexterm" id="id750" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> in XML: <code class="calibre15">&amp;quot</code> (“),<a contenteditable="false" data-primary="&amp;apos (')" data-type="indexterm" id="id751" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="' (&amp;apos)" data-type="indexterm" id="id752" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <code class="calibre15">&amp;apos</code> ('),<a contenteditable="false" data-primary="&amp;lt (&lt;)" data-type="indexterm" id="id753" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="&lt; (&amp;lt)" data-type="indexterm" id="id754" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <code class="calibre15">&amp;lt</code> (&lt;),<a contenteditable="false" data-primary="&amp;gt (&gt;)" data-type="indexterm" id="id755" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="&gt; (&amp;gt)" data-type="indexterm" id="id756" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <code class="calibre15">&amp;gt</code> (&gt;), and<a contenteditable="false" data-primary="&amp; (&amp;amp)" data-type="indexterm" id="id757" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <code class="calibre15">&amp;amp</code> (&amp;). XML also allows you to add<a contenteditable="false" data-primary="HTML-style comments" data-type="indexterm" id="id758" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> HTML-style comments as<code class="calibre15">&lt;!-- this is a comment --&gt;</code>, which can occasionally be useful for<a contenteditable="false" data-primary="editorial hints" data-type="indexterm" id="id759" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> “editorial” hints for the model.</p>

<p class="subtitle">In<a contenteditable="false" data-primary="YAML format" data-type="indexterm" id="id760" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <a href="https://yaml.org" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">YAML</a>, the document consists of a series of named fields or unnamed bullet points whose hierarchy levels are tracked by their indentation. This indentation tracking can be quite annoying because you need to get it right to be able to use standard parsers, but it’s helpful in cases where you need to be very precise about indentation, such as with code or formatted text. In particular, the syntax <code class="calibre15">fieldname: |2</code> opens a multiline text field that preserves indentation, as illustrated in <a data-type="xref" href="#ch06a_figure_4_1728442733835231" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 6-4</a>. Note that in such text fields, you don’t need to escape anything, which is nice. You’ll notice that such a text field is finished by encountering a line with smaller indentation than the text field’s “zero” indentation. Also note that the highlight boxes indicate the value of the content fields, including leading whitespace.</p>

<figure class="calibre22"><div id="ch06a_figure_4_1728442733835231" class="figure"><img src="assets/pefl_0604.png" class="calibre23"/>
<h6 class="calibre24"><span class="firstname">Figure 6-4. </span>Text fields specifying indented content in YAML</h6>
</div></figure>

<p class="subtitle">Another<a contenteditable="false" data-primary="JSON" data-secondary="as good choice for OpenAI" data-type="indexterm" id="id761" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> markup language that should feature heavily in any LLM’s training set is JSON (or its variant, JSON Lines). At one point, we would have recommended against using JSON since it is very escape heavy and less readable. However, OpenAI in particular has put a lot of effort into making its models generate JSON accurately because JSON powers their tools API. Therefore, for OpenAI at least, JSON is still a reasonably good choice.<a contenteditable="false" data-primary="" data-startref="PAtype06" data-type="indexterm" id="id762" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-startref="DTstructured06" data-type="indexterm" id="id763" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="" data-startref="structfor06" data-type="indexterm" id="id764" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/></p>
</div></section>
</div></section>

<section class="calibre6" data-type="sect1" data-pdf-bookmark="Formatting Snippets"><div class="preface" id="ch06a_formatting_snippets_1728442733858521">
<h1 class="calibre5">Formatting Snippets</h1>

<p class="subtitle">The<a contenteditable="false" data-primary="prompt assembly" data-secondary="formatting snippets" data-type="indexterm" id="PAsnippets06" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="snippets" data-secondary="formatting" data-type="indexterm" id="Sformat06" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> way you format snippet text depends a lot on your document. In an<a contenteditable="false" data-primary="advice conversations" data-type="indexterm" id="id765" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="document types" data-secondary="advice conversation" data-type="indexterm" id="id766" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> advice conversation transcript, you can format snippet information into the back-and-forth turns of the conversation. For instance, say your application retrieves this weather forecast data:</p>

<pre data-type="programlisting" class="calibre21">
weather = {
  "description": "sunny",
  "temperature": 75
}</pre>

<p class="subtitle">That information can be packaged as the advice seeker asking a clarifying question and the assistant answering with the following information:</p>

<pre data-type="programlisting" class="calibre21">
User: What's the weather like?
Assistant: It's going to be {{ weather["description"] }} with a temperature of 
{{ weather["temperature"] }} degrees.</pre>

<p class="subtitle">In an<a contenteditable="false" data-primary="document types" data-secondary="analytic report" data-type="indexterm" id="id767" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="analytic reports" data-type="indexterm" id="id768" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> analytic report, you normally want to state your knowledge in natural language. Results of API calls require you to know what the API returns, and then you can format the string into a sentence. Often, it’s useful to include the results of individual API calls as individual sections, like this:</p>

<pre data-type="programlisting" class="calibre21">
#### Weather Forecast
{{ weather["description"] }} with a temperature of {{ weather["temperature"] }} 
degrees</pre>

<p class="subtitle">Finally, if you use a structured document, your life is often easy: just serialize all relevant fields of the object you have in memory that represents your piece of knowledge:</p>

<pre data-type="programlisting" class="calibre21">
&lt;weather&gt;
&lt;description&gt;sunny&lt;/description&gt;
&lt;temperature&gt;75&lt;/temperature&gt;
&lt;/weather&gt;</pre>

<p class="subtitle">No matter what type of document you use, a useful form of communicating background context<a contenteditable="false" data-primary="background context" data-type="indexterm" id="id769" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> can be a pretty explicitly stated<a contenteditable="false" data-primary="side remarks" data-type="indexterm" id="id770" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="stated side remarks" data-type="indexterm" id="id771" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="asides" data-type="indexterm" id="id772" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> side remark (e.g., “As an aside, …”). For example, in GitHub Copilot code completions, where our document template was in the form of a source code file, we found we could usefully include code from other files using a code comment stating explicitly that some quoted snippet was included for comparison reasons, like so:</p>

<pre data-type="programlisting" class="calibre21">
// &lt;consider this snippet from ../skill.go&gt;
// type Skill interface {
//	Execute(data []byte) (refs, error)
// }
// &lt;/end snippet&gt;</pre>

<p class="subtitle">An aside provides a strong hint to the model, but without requiring it to use the side remark in a certain way or at all.</p>

<p class="subtitle">When formatting your snippets, the things to aim for are as follows:</p>

<dl class="stafflist">
	<dt class="calibre13">Modularity</dt>
	<dd class="calibre14">
	<p class="subtitle">You<a contenteditable="false" data-primary="modularity" data-type="indexterm" id="id773" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> want your snippets to be strings that can be inserted into or removed from the prompt with relative ease. Ideally, your document is like a list (a conversation with turns) or a tree (a report with hierarchical sections; a structured document), so that snippets are easier to handle as items in the list or leaves of the tree.</p>
	</dd>
	<dt class="calibre13">Naturalness</dt>
	<dd class="calibre14">
	<p class="subtitle">The<a contenteditable="false" data-primary="naturalness" data-type="indexterm" id="id774" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> snippet should feel like an organic part of your document and be formatted as such. If you’re letting the LLM complete source code, any natural language information should be formatted as a comment rather than dumped between the code lines verbatim. If your document template is a conversation or a report, then data should be interpolated into a natural text that sounds appropriate for the document (see the preceding weather examples).</p>
	</dd>
	<dt class="calibre13">Brevity</dt>
	<dd class="calibre14">
	<p class="subtitle">If<a contenteditable="false" data-primary="brevity" data-type="indexterm" id="id775" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> you can communicate relevant context with fewer tokens, great!</p>
	</dd>
	<dt class="calibre13">Inertness</dt>
	<dd class="calibre14">
	<p class="subtitle">You’d<a contenteditable="false" data-primary="inertness" data-type="indexterm" id="id776" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> like to compute the token length of a snippet only once, so the tokenization of one snippet shouldn’t affect the tokenization of the previous or next snippet.</p>
	</dd>
</dl>

<section data-type="sect2" data-pdf-bookmark="More on Inertness" class="calibre6"><div class="preface" id="ch06a_more_on_inertness_1728442733858572">
<h2 class="calibre19">More on Inertness</h2>

<p class="subtitle">The last bit, inertness, depends on your tokenizer, which might use different tokens to tokenize a<a contenteditable="false" data-primary="composite strings" data-type="indexterm" id="id777" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="strings" data-secondary="composite strings" data-type="indexterm" id="id778" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> composite string A + B than to tokenize each string individually. That can easily increase or decrease the number of tokens needed to tokenize a composite string (see <a data-type="xref" href="#ch06a_table_4_1728442733843184" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Table 6-4</a>).</p>

<p class="subtitle">Pasting strings together doesn’t mean the arrays of tokens just get concatenated. Token IDs<a contenteditable="false" data-primary="token IDs" data-type="indexterm" id="id779" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> have been obtained for OpenAI’s <a href="https://oreil.ly/Cu9Q4" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">GPT-3.5-and-later tokenizer</a>, but both examples also work for the <a href="https://oreil.ly/HyQNe" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">GPT-3-and-before tokenizer</a> used in many non-OpenAI LLMs.</p>

<table id="ch06a_table_4_1728442733843184" class="calibre27">
	<caption class="calibre28"><span class="firstname">Table 6-4. </span>Token count isn’t additive</caption>
	<thead class="calibre29">
		<tr class="calibre30">
			<th class="calibre31"> </th>
			<th class="calibre31">Example 1</th>
			<th class="calibre31">Example 2</th>
		</tr>
	</thead>
	<tbody class="calibre32">
		<tr class="calibre30">
			<td class="calibre33">Strings</td>
			<td class="calibre33">“be” + “am” ➜ “beam”</td>
			<td class="calibre33">“cat” + “tail” ➜ “cattail”</td>
		</tr>
		<tr class="calibre34">
			<td class="calibre33">Tokens</td>
			<td class="calibre33">[be] + [am] ➜ [beam]</td>
			<td class="calibre33">[cat] + [tail] ➜ [c], [att], [ail]</td>
		</tr>
		<tr class="calibre30">
			<td class="calibre33">Token ids</td>
			<td class="calibre33">1395 + 309 ➜ 54971</td>
			<td class="calibre33">4719 + 14928 ➜ 66, 1617, 607</td>
		</tr>
		<tr class="calibre34">
			<td class="calibre33">Token count</td>
			<td class="calibre33">1 + 1 ➜ 1</td>
			<td class="calibre33">1 + 1 ➜ 3</td>
		</tr>
	</tbody>
</table>

<p class="subtitle">It’s generally a good idea to separate individual prompt elements with whitespace to prevent them from merging unexpectedly. However, be aware of potential issues: GPT tokenizers<a contenteditable="false" data-primary="GPT tokenizer" data-type="indexterm" id="id780" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> often include tokens that start with a blank space but not ones that end with it. To avoid problems, prefer prompt elements that start with a space rather than ending with one. Additionally, GPT tokenizers<a contenteditable="false" data-primary="newline character" data-type="indexterm" id="id781" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> combine multiple newline characters, so it’s best to ensure that your snippets either never start or never end with a newline. Avoiding newlines at the beginning of snippets is usually easier for app developers.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Formatting Few-Shot Examples" class="calibre6"><div class="preface" id="ch06a_formatting_few_shot_examples_1728442733858620">
<h2 class="calibre19">Formatting Few-Shot Examples</h2>

<p class="subtitle">When<a contenteditable="false" data-primary="few-shot prompting" data-secondary="formatting snippets" data-type="indexterm" id="id782" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> formatting snippets for few-shot examples, you usually have a choice. One option is to designate them explicitly as examples, as shown here:</p>

<pre data-type="programlisting" class="calibre21">
In the following, when I encounter a question like "Who was the first President 
of the United States?" I will give an answer like "George Washington."</pre>

<p class="subtitle">Alternatively, you can integrate examples directly into the document as solutions to previous tasks. This approach requires careful formulation but can be highly effective. It allows the model to leverage the few-shot examples more naturally and creates a smoother prompt. This method is especially useful in ChatML or similar conversation transcript settings, where you can make the model believe it has successfully solved previous tasks in the style of the examples, thus encouraging it to continue using that successful approach.<a contenteditable="false" data-primary="" data-startref="PAsnippets06" data-type="indexterm" id="id783" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="" data-startref="Sformat06" data-type="indexterm" id="id784" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/></p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Elastic Snippets" class="calibre6"><div class="preface" id="ch06a_elastic_snippets_1728442733858686">
<h1 class="calibre5">Elastic Snippets</h1>

<p class="subtitle">When<a contenteditable="false" data-primary="prompt assembly" data-secondary="elastic snippets" data-type="indexterm" id="id785" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="snippets" data-secondary="elastic snippets" data-type="indexterm" id="id786" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="elastic snippets" data-type="indexterm" id="id787" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> you’re converting content into snippets, each piece of information usually corresponds to a single snippet. However, sometimes, one piece of content can be split into multiple snippets or be represented in various forms.</p>

<p class="subtitle">For example, consider a literary analysis task asking about the significance of a specific scene in Alex Garland’s novel <em class="hyperlink">The Beach</em>. If you ask ChatGPT about this scene, chances are that it won’t be familiar with that particular one, and any <a href="https://oreil.ly/2FPat" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">answer</a> it gives will be vague, wrong, or both. To improve the response, you need to include relevant context from the book in your prompt. You remember how to retrieve relevant book passages from <a data-type="xref" href="ch05.html#ch05_prompt_content_1728435524680844" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Chapter 5</a>, and let’s say you identify two key moments.</p>

<p class="subtitle">You can snippetize these passages in different ways, as shown in <a data-type="xref" href="#ch06a_figure_5_1728442733835249" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 6-5</a>. Ideally, you’d include the entire chapter for full context. That’s one way, but with limited prompt space and limited model attention, you probably must tighten your belt regarding context a bit. But that leaves you with different possibilities:</p>

<ul class="stafflist">
	<li class="calibre9">
	<p class="calibre26">Add two snippets with no context around them.</p>
	</li>
	<li class="calibre9">
	<p class="calibre26">Add two snippets with some context around each.</p>
	</li>
	<li class="calibre9">
	<p class="calibre26">Add one combined snippet with context linking the parts.</p>
	</li>
</ul>

<p class="subtitle">All three options have things to recommend them. The first option is short, the last option conveys most of the information (including how the snippets relate to each other), and the middle option is somewhere in between. But of course, there are even more options: you can choose a tiny bit of context, a lot of context, etc. How do you deal with such a situation?</p>

<figure class="calibre22"><div id="ch06a_figure_5_1728442733835249" class="figure"><img src="assets/pefl_0605.png" class="calibre23"/>
<h6 class="calibre24"><span class="firstname">Figure 6-5. </span>Snippetizing context into flexible snippets</h6>
</div></figure>

<p class="subtitle">There are two general approaches to such a situation where there’s<a contenteditable="false" data-primary="context" data-secondary="including variable amounts of" data-type="indexterm" id="id788" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> a variable amount of context you could include. We have used both, depending on our exact requirements:</p>

<ol class="stafflist">
	<li class="calibre25">
	<p class="calibre26">You could use what we call<a contenteditable="false" data-primary="elastic prompt elements" data-type="indexterm" id="id789" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <em class="hyperlink">elastic</em> prompt elements, which are prompt elements that have different versions, ranging from short to long. In this case, the longest version would be the whole chapter, a slightly shorter version would be one where one paragraph is replaced by<a contenteditable="false" data-primary="… (ellipsis)" data-type="indexterm" id="id790" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="ellipsis (…)" data-type="indexterm" id="id791" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> “...,” and an even shorter version would be one where two paragraphs are replaced by “...” This can go all the way down to the shortest version, where it’s just the two snippets you want to quote with no extra context and a “...” between them. Then, when you’re assembling the prompt, you don’t ask, “Do we have space to include this snippet?” Rather, you ask, “What’s the biggest version of this snippet we have space for?”</p>
	</li>
	<li class="calibre25">
	<p class="calibre26">Alternatively, you could create multiple prompt elements from the retrieved information. For example, you might have one snippet that is the first relevant text passage, another that is that passage plus some context, and another with even more context. You’ll have to remember to only actually include one of these, as they overlap. So, this approach requires a prompt assembly method that allows you to declare prompt elements as incompatible (see the next section).</p>
	</li>
</ol>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Relationships Among Prompt Elements" class="calibre6"><div class="preface" id="ch06a_relationships_among_prompt_elements_1728442733858741">
<h1 class="calibre5">Relationships Among Prompt Elements</h1>

<p class="subtitle">Prompt elements<a contenteditable="false" data-primary="prompt assembly" data-secondary="relationship among prompt elements" data-type="indexterm" id="PArelation06" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="prompt content" data-secondary="relationship among elements" data-type="indexterm" id="PErelation06" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> don’t exist in a vacuum: a prompt is an amalgam of several of them. Any algorithm that combines prompt elements has to account for three ways in which the elements relate to each other: position and ordering, importance, and dependency. You’ll need to keep these three dimensions in mind while you’re constructing the prompt elements. Let’s go through each of them.</p>

<section data-type="sect2" data-pdf-bookmark="Position" class="calibre6"><div class="preface" id="ch06a_position_1728442733858791">
<h2 class="calibre19">Position</h2>

<p class="subtitle"><em class="hyperlink">Position</em> determines<a contenteditable="false" data-primary="position (of prompt elements)" data-type="indexterm" id="id792" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> where each element should appear in the prompt. Prompt elements usually need to follow a specific order—while you might skip some, <em class="hyperlink">rearranging</em> them can make the document confusing. For example, if you’re quoting from reference documents, you should maintain the original order; don’t place the second snippet before the first. In chats or narratives, stick to the chronological order. In other situations, ensure elements are in the correct sections; for instance, a description of a book the user likes shouldn’t go in the “Books I really hate” section.</p>

<p class="subtitle">To manage these relationships, you might use an array or linked list of prompt elements, an index covering all elements, or a unique position value for each element. Often, the order reflects how you gather information (e.g., scanning a document or retrieving context section by section). In such cases, you generally only need to append new elements to the end.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Importance" class="calibre6"><div class="preface" id="ch06a_importance_1728442733858839">
<h2 class="calibre19">Importance</h2>

<p class="subtitle"><em class="hyperlink">Importance</em> determines<a contenteditable="false" data-primary="importance (of prompt elements)" data-type="indexterm" id="id793" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> how crucial it is to include a prompt element to convey relevant information to the model. Beginners often confuse position with importance, as they are frequently correlated—recent information is often more important. But there are many exceptions—for example, your introduction is often more important than most of the details in the middle (which are rightfully consigned to the Valley of Meh from <a data-type="xref" href="#ch06a_figure_1_1728442733835135" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 6-1</a>).</p>

<p class="subtitle">When evaluating the importance of each element, consider the tradeoff between including large chunks of relevant information and including many smaller, less critical elements. Decide whether to measure importance based on snippet length or an absolute scale, but choose one method and apply it consistently. Short, efficient prompt elements are often preferable to longer ones that convey the same amount of information. If you don’t account for length initially, ensure the prompt assembly engine can adjust importance based on token length later.</p>

<p class="subtitle">To assess importance, use either a numerical score or discrete priority tiers. <em class="hyperlink">Tiers</em> are a small number of levels you can quickly sort your sources into, with lower tiers being cut first, if necessary. Some elements —such as the central instructions and the description of the output format—are so vital that they must be included at all costs. These need to occupy the highest tier. Next typically come explanations in the second-highest tier and context in the third. But as you delve deeper into the subtleties and compare different context sources or different degrees of relevance, consider adding numbers for finer prioritization.</p>

<p class="subtitle">Assigning importance involves judgment and is crucial for effective prompt engineering. You also need to test and refine these importance parameters with the methods we’ll explore further in <a data-type="xref" href="ch10.html#ch10_evaluating_llm_applications_1728407085475721" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Chapter 10</a>.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Dependency" class="calibre6"><div class="preface" id="ch06a_dependency_1728442733858887">
<h2 class="calibre19">Dependency</h2>

<p class="subtitle"><em class="hyperlink">Dependency</em> is<a contenteditable="false" data-primary="dependencies (of prompt elements)" data-type="indexterm" id="id794" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> the final type of relationship among prompt elements, and it focuses on how including one element affects the inclusion of others. Dependencies can be complex, but in practice, they usually fall into two categories—requirements and incompatibilities:</p>

<dl class="stafflist">
	<dt class="calibre13">Requirements</dt>
	<dd class="calibre14">
	<p class="subtitle">These occur when one prompt element depends on another. For example, you need to establish that “Richard is the protagonist of <em class="hyperlink">The Beach</em>” before stating “He grew up in England.”</p>
	</dd>
	<dt class="calibre13">Incompatibilities</dt>
	<dd class="calibre14">
	<p class="subtitle">These<a contenteditable="false" data-primary="incompatibilities (of prompt elements)" data-type="indexterm" id="id795" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> occur when one prompt element excludes another. This often happens when the same information can be presented in different ways, such as in a summary versus a detailed explanation. If your prompt assembly engine can handle incompatibilities, you can include both versions with an exclusion note, giving the longer version when space allows and using the shorter version as a fallback.</p>
	</dd>
</dl>

<p class="subtitle">At this point in the text, you should have transformed all your pieces of content: the static ones that you prepared beforehand and the dynamic ones that you gathered as context into proper prompt elements like the ones in <a data-type="xref" href="#ch06a_figure_6_1728442733835267" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 6-6</a>. That means you’re finally ready to assemble your prompt.</p>

<figure class="calibre22"><div id="ch06a_figure_6_1728442733835267" class="figure"><img src="assets/pefl_0606.png" class="calibre23"/>
<h6 class="calibre24"><span class="firstname">Figure 6-6. </span>Prompt elements and their properties, including all the information you need to assemble your prompt<a contenteditable="false" data-primary="" data-startref="PArelation06" data-type="indexterm" id="id796" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><a contenteditable="false" data-primary="" data-startref="PErelation06" data-type="indexterm" id="id797" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/></h6>
</div></figure>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Putting It All Together" class="calibre6"><div class="preface" id="ch06a_putting_it_all_together_1728442733858937">
<h1 class="calibre5">Putting It All Together</h1>

<p class="subtitle">To<a contenteditable="false" data-primary="prompt assembly" data-secondary="creating the final prompt" data-type="indexterm" id="PAfinal06" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> create the final prompt, you need to solve an optimization problem: deciding which elements to include in the prompt to maximize its overall value.</p>

<p class="subtitle">You have two main constraints:</p>

<dl class="stafflist">
	<dt class="calibre13">Dependency structure</dt>
	<dd class="calibre14">
	<p class="subtitle">Ensure that any requirements and incompatibilities between elements are respected.</p>
	</dd>
	<dt class="calibre13">Prompt length</dt>
	<dd class="calibre14">
	<p class="subtitle">Keep<a contenteditable="false" data-primary="prompt engineering" data-secondary="prompt length" data-type="indexterm" id="id798" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> the total prompt length within a set limit, typically your context window size minus the tokens needed for the model’s response. If your context window is very large, you might use a softer token budget based on available compute and to avoid including too much irrelevant context.</p>
	</dd>
</dl>

<p class="subtitle">Once you decide which elements to include, arrange them according to their positions to form the final prompt.</p>

<p class="subtitle">This problem is similar to<a contenteditable="false" data-primary="linear programming" data-type="indexterm" id="id799" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> linear programming and 0-1<a contenteditable="false" data-primary="knapsack problems" data-type="indexterm" id="id800" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> knapsack problems, where you decide whether to include an element (though knapsack problems often don’t account for dependencies). However, there isn’t a standard tool that automatically solves this for you, so you’ll need to create your own solution. It can be a rewarding process, allowing you to customize it to your specific needs.</p>

<p class="subtitle">Consider what you need from your prompt assembly; for example, if you need a fast assembly for interactive apps or if you have specific dependency patterns to handle. In Copilot code completions, code snippets often require a specific postfix, so we handle these with custom functions that manage dependencies between lines of code.</p>

<p class="subtitle">When developing your app iteratively—starting with a basic version and then expanding—it’s useful to begin with a<a contenteditable="false" data-primary="prompt-crafting engines" data-type="indexterm" id="id801" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> minimal prompt crafter like the one shown in <a data-type="xref" href="#ch06a_figure_7_1728442733835286" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 6-7</a>. This simple tool helps you test whether your app idea has potential. With this approach, you don’t need to evaluate or prioritize snippets because the prompt crafter uses only the end portion of your content. This method works well because LLMs are trained to handle document suffixes effectively. It’s also suitable for applications where you build on a main text or for chatlike applications where recent exchanges are most relevant.</p>

<figure class="calibre22"><div id="ch06a_figure_7_1728442733835286" class="figure"><img src="assets/pefl_0607.png" class="calibre23"/>
<h6 class="calibre24"><span class="firstname">Figure 6-7. </span>The minimal prompt crafter, which orders prompt elements and keeps as many at the end as it can fit into the token budget</h6>
</div></figure>

<p class="subtitle">As your app develops, you’ll need a more advanced prompt-crafting engine. For speed, consider using a greedy algorithm as shown in <a data-type="xref" href="#ch06a_figure_8_1728442733835307" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 6-8</a> (possibly combined with some limited exploration of alternatives). There are two main types of greedy algorithms you can use, depending on how your prompt elements interact: an additive approach and a subtractive approach.</p>

<p class="subtitle">In the<a contenteditable="false" data-primary="additive greedy approach" data-type="indexterm" id="id802" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/> <em class="hyperlink">additive greedy approach</em>, you start with an empty prompt and add elements one by one. Each step involves adding the highest-value element that meets all requirements, doesn’t conflict with existing elements, and fits within the prompt length. This method is even effective if you have many more elements than fit into your prompt and you need to eliminate a lot. However, it requires few cyclical requirements and few cases of high-value elements dependent on low-value ones.</p>

<p class="subtitle">When using the additive greedy approach, you can simplify the process of finding the best element to add by sorting your elements based on their requirements and values. This way, you consider elements only once all their dependencies are satisfied.</p>

<figure class="calibre22"><div id="ch06a_figure_8_1728442733835307" class="figure"><img src="assets/pefl_0608.png" class="calibre23"/>
<h6 class="calibre24"><span class="firstname">Figure 6-8. </span>Additive greedy approach, in which the prompt crafter iteratively adds high-value elements to the prompt until the token budget is filled up, then re-sorts the elements according to position</h6>
</div></figure>

<p class="subtitle">With the <a contenteditable="false" data-primary="subtractive greedy approach" data-type="indexterm" id="id803" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/><em class="hyperlink">subtractive</em> <em class="hyperlink">greedy approach</em> shown in <a data-type="xref" href="#ch06a_figure_9_1728442733835326" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Figure 6-9</a>, you start by including all prompt elements and then gradually removing those that are less valuable or whose dependencies are no longer met. This method works well if you have a manageable number of elements and few incompatibilities. Otherwise, the process can become cumbersome. High-value elements dependent on low-value elements can also lead to suboptimal results, unless you use advanced techniques to prioritize retaining <span class="firstname">high-value</span> dependencies. Elastic snippets are normally easier to handle in a subtractive approach than in an additive approach.</p>

<figure class="calibre22"><div id="ch06a_figure_9_1728442733835326" class="figure"><img alt="A collage of different colored squares  Description automatically generated" src="assets/pefl_0609.png" class="calibre23"/>
<h6 class="calibre24"><span class="firstname">Figure 6-9. </span>The subtractive greedy approach, in which the prompt-crafting engine successively eliminates prompt elements of low value, pruning missing requirements in between</h6>
</div></figure>

<p class="subtitle">Please note, however, that all of the sketches for prompt-crafting engines presented in this chapter are meant as basic prototypes. Maybe you’ll find them sufficient for your application, but you should be willing to move beyond them in a way dictated by your specific requirements as these requirements become clear to you when you refine your application.<a contenteditable="false" data-primary="" data-startref="PAfinal06" data-type="indexterm" id="id804" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2"/></p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Conclusion" class="calibre6"><div class="preface" id="ch06a_conclusion_1728442733858984">
<h1 class="calibre5">Conclusion</h1>

<p class="subtitle">In this chapter, we’ve covered the art of crafting an effective prompt from your gathered information. We explored how to choose the right document format and examined various prototype documents that LLMs excel at completing.</p>

<p class="subtitle">You’ve also learned how to convert your information into prompt elements—snippets of text that fit seamlessly into your document while aligning in relevance, order, and dependency. Now, you’ll be able to refine these elements to create a concise and effective prompt using a custom prompt-crafting engine, inspired by the strategies covered here, with ease.</p>

<p class="subtitle">Congratulations on completing the feedforward pass from <a data-type="xref" href="ch03.html#ch03a_moving_toward_chat_1728432131625250" class="pcalibre3 calibre7 pcalibre4 pcalibre1 pcalibre2">Chapter 3</a>—you’ve successfully created a coherent prompt for the model. In the next chapter, we’ll focus on how to ensure that you receive meaningful and accurate responses.</p>
</div></section>
</div></section></div></div></body></html>