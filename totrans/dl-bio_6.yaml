- en: Chapter 6\. Learning Spatial Organization Patterns Within Cells
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章\. 在细胞内学习空间组织模式
- en: 'In this chapter, we shift focus from classifying high-level cell states—such
    as distinguishing cancerous from healthy tissue—to something more low level and
    foundational: understanding the *spatial organization inside individual cells*.
    Specifically, we’ll train a deep learning model to analyze microscopy images and
    learn where exactly in the cell different proteins are located, a task known as
    *protein localization*.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重点从分类高级细胞状态（例如区分癌组织与正常组织）转移到更基础和低级的内容：理解单个细胞内的*空间组织*。具体来说，我们将训练一个深度学习模型来分析显微镜图像，并学习蛋白质在细胞中的确切位置，这项任务被称为*蛋白质定位*。
- en: Protein localization plays a crucial role in cell biology. A protein’s position
    within the cell—for example, whether it’s in the nucleus or the mitochondria—often
    determines its function. Mislocalization of proteins is implicated in many diseases,
    even when the protein’s structure is normal (i.e., not mutated or altered). Thanks
    to modern fluorescence microscopy, we can observe a protein’s location in a cell
    directly, but the resulting images are often high dimensional, noisy, and hard
    to interpret at scale.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 蛋白质定位在细胞生物学中起着至关重要的作用。蛋白质在细胞内的位置——例如，它是否位于细胞核或线粒体中——通常决定了其功能。蛋白质的错误定位与许多疾病有关，即使蛋白质的结构是正常的（即，没有突变或改变）。多亏了现代荧光显微镜，我们可以直接观察蛋白质在细胞中的位置，但产生的图像通常是高维的、噪声的，并且难以大规模解释。
- en: 'Unlike earlier chapters, the goal here isn’t to strictly optimize a metric
    like accuracy, recall, or precision on a specific classification or regression
    task. Instead, we’ll train a model to learn a *latent representation* of protein
    localization directly from raw microscopy images. You can think of a *latent space*
    as the model’s internal map—a compressed representation where proteins with similar
    localization patterns are grouped together, even without explicit labels. This
    approach falls under *representation learning*: the goal is to uncover meaningful
    structure in the data that reflects biological patterns.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面的章节不同，这里的目的是不是严格优化特定分类或回归任务上的准确度、召回率或精确度等指标。相反，我们将训练一个模型，直接从原始显微镜图像中学习蛋白质定位的*潜在表示*。你可以将*潜在空间*视为模型的内部地图——一个压缩的表示，其中具有相似定位模式的蛋白质被分组在一起，即使没有明确的标签。这种方法属于*表示学习*：目标是揭示数据中的有意义结构，反映生物模式。
- en: Why focus on representation learning instead of just training a classifier for
    a specific task of interest? In many biological settings, and especially in protein
    localization, we don’t have clean, comprehensive labels. Instead of forcing the
    model to solve a narrow, predefined task, we want it to learn a *rich internal
    representation* of the data that captures spatial patterns and similarities between
    proteins. These representations can be reused for clustering, visualization, identifying
    unknown cellular compartments, or understanding how protein localization changes
    across cell types or conditions. This is analogous to how large language models
    learn general-purpose representations of words or sentences, which can then be
    reused for many downstream tasks.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么关注表示学习而不是仅仅为特定感兴趣的任务训练一个分类器？在许多生物环境中，尤其是在蛋白质定位中，我们没有干净、全面的标签。我们不想强迫模型解决一个狭窄、预定义的任务，而是希望它学习数据的*丰富内部表示*，捕捉蛋白质之间的空间模式和相似性。这些表示可以用于聚类、可视化、识别未知的细胞区室，或理解蛋白质定位在细胞类型或条件之间的变化。这与大型语言模型学习通用表示的单词或句子类似，这些表示可以用于许多下游任务。
- en: Our approach to modeling protein localization in this chapter is based on *cytoself*,
    a self-supervised deep learning method published in *Nature Methods* in 2022.^([1](ch06.html#id943))
    The model combines image reconstruction (rebuilding the microscopy input image)
    with a secondary task—predicting the protein’s identity—to learn a rich and interpretable
    embedding space that reflects biological localization patterns.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们建模蛋白质定位的方法基于*cytoself*，这是一种在2022年发表在*Nature Methods*上的自监督深度学习方法.^([1](ch06.html#id943))
    该模型结合了图像重建（重建显微镜输入图像）和预测蛋白质身份的辅助任务，以学习一个丰富且可解释的嵌入空间，反映生物定位模式。
- en: Unlike in previous chapters, the model’s primary output isn’t a classification
    label or regression score. Instead, for a given microscopy image of a fluorescently
    tagged protein, it produces an *embedding*—a position in the latent space—that
    captures the spatial characteristics of the protein’s localization. These embeddings
    can then be visualized, clustered, or compared to known annotations.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章不同，模型的输出主要不是一个分类标签或回归分数。相反，对于给定的一种荧光标记蛋白的显微镜图像，它产生一个*嵌入*——在潜在空间中的一个位置——该位置捕捉了蛋白定位的空间特征。然后，这些嵌入可以被可视化、聚类或与已知的注释进行比较。
- en: This is the most advanced chapter in the book. You’ll work with a large, real-world
    microscopy dataset and implement a custom vector-quantized variational autoencoder
    (VQ-VAE) from scratch. The chapter takes you deep into self-supervised learning,
    large-scale image processing, and the spatial organization of cells—and reproduces
    core results from a recent deep learning biology paper.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这是本书中最先进的一章。你将使用一个大型、真实世界的显微镜数据集，并从头开始实现自定义的向量量化变分自编码器（VQ-VAE）。这一章带你深入到自监督学习、大规模图像处理和细胞的空间组织，并重现了最近一篇深度学习生物学论文的核心结果。
- en: Warning
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: More than in any previous chapter, we strongly recommend keeping the companion
    Colab notebook open as you read. You may need to scale down the model to fit within
    memory limits, but actively running the code will solidify your understanding
    and give you room to explore.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 比任何前几章都强烈建议，在阅读时保持配套的Colab笔记本打开。你可能需要缩小模型以适应内存限制，但积极运行代码将巩固你的理解，并为你提供探索的空间。
- en: To run the full-scale model, we recommend using a powerful GPU such as an A100\.
    These are available through platforms like Google Colab Pro, Kaggle Notebooks
    (with upgrades), Google Cloud Platform (GCP), or AWS EC2.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行完整规模的模型，我们建议使用像A100这样的强大GPU。这些GPU可以通过Google Colab Pro、Kaggle Notebooks（带升级）、Google
    Cloud Platform（GCP）或AWS EC2等平台获得。
- en: Biology Primer
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生物学入门
- en: The cell was first observed in 1665 by the British scientist [Robert Hooke](https://oreil.ly/6JmDd),
    who used a microscope to describe its structure in cork tissue. Since then, microscopy
    has become one of biology’s most essential tools. Modern microscopes allow researchers
    to visualize living cells in astonishing detail—and increasingly, to capture this
    data at massive scale.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 细胞首次由英国科学家[罗伯特·胡克](https://oreil.ly/6JmDd)在1665年观察到，他使用显微镜描述了其在软木组织中的结构。从那时起，显微镜已经成为生物学最基本工具之一。现代显微镜允许研究人员以惊人的细节可视化活细胞，并且越来越多地，能够以大规模捕获这些数据。
- en: Microscopy is now central to many areas of biomedical research. For example,
    pharmaceutical companies routinely screen the effects of drug candidates by imaging
    thousands of treated cells and then use machine learning models to assess cellular
    responses in an automated way. Does a cell look alive or dead? Does its observable
    structure change in response to a particular drug compound? Do the cells divide
    more or less rapidly as a result of a treatment?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 显微镜现在在生物医学研究的许多领域都处于核心地位。例如，制药公司通常会通过成像成千上万的处理过的细胞来常规筛选药物候选物的效果，然后使用机器学习模型以自动化的方式评估细胞反应。细胞看起来是活的还是死的？它的可观察结构是否会对特定的药物化合物做出反应而发生变化？细胞在治疗后的分裂速度是加快还是减慢？
- en: 'Despite its power to capture biological detail, microscopy data can be challenging
    to analyze. Cells vary naturally in size, shape, and appearance, and the imaging
    and sample preparation process itself introduces noise and artifacts. Furthermore,
    unlike genomes, which can to some extent be compared to a universal “reference”
    genome, cells don’t come with a “baseline cell” or a standardized coordinate system,
    which can make analysis difficult. And microscopy can challenge computational
    resources: high-resolution microscopy produces large volumes of data that can
    strain both memory and compute.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管显微镜在捕捉生物细节方面具有强大的能力，但显微镜数据可能难以分析。细胞在大小、形状和外观上自然存在差异，成像和样本制备过程本身也会引入噪声和伪影。此外，与基因组不同，基因组在一定程度上可以与一个普遍的“参考”基因组进行比较，细胞没有“基准细胞”或标准化的坐标系，这可能会使分析变得困难。而且显微镜可能会挑战计算资源：高分辨率显微镜产生大量数据，可能会对内存和计算造成压力。
- en: Traditionally, microscopy image analysis relied on manually defined, hand-engineered
    features. For example, in drug screening experiments, scientists might measure
    whether a compound causes cells to shrink, swell, or change shape—signs that the
    drug is affecting cell health or behavior. To do so, researchers would extract
    properties such as cell size, shape, brightness, or texture using classical image
    processing techniques, including thresholding, edge detection, and morphological
    operations (which manipulate shapes in the image to clean up noise, fill gaps,
    or separate touching cells). These features were then fed into relatively simple
    models such as logistic regression or decision trees to predict cellular outcomes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，显微镜图像分析依赖于手动定义的手工工程特征。例如，在药物筛选实验中，科学家可能会测量化合物是否会导致细胞缩小、膨胀或变形——这些迹象表明药物正在影响细胞健康或行为。为此，研究人员会使用经典图像处理技术（包括阈值、边缘检测和形态学操作，这些操作在图像中操纵形状以清除噪声、填充间隙或分离接触的细胞）提取诸如细胞大小、形状、亮度或纹理等属性。然后，将这些特征输入相对简单的模型，如逻辑回归或决策树，以预测细胞结果。
- en: Note
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'This earlier approach to microscopy image analysis required domain expertise
    to decide which features mattered, and they often missed subtle patterns not obvious
    to the human eye. Deep learning changed this: modern convolutional neural networks
    (CNNs) can learn to extract meaningful features directly from raw pixels, capturing
    complex visual cues without relying on handcrafted rules.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这种早期的显微镜图像分析方法需要领域专业知识来决定哪些特征是重要的，而且它们经常错过人类肉眼不易察觉的微妙模式。深度学习改变了这一点：现代卷积神经网络（CNN）可以直接从原始像素中学习提取有意义的特征，捕捉复杂的视觉线索，而不依赖于手工制定的规则。
- en: These advances in image analysis have opened the door to exploring deeper biological
    questions—including how proteins are spatially organized within cells.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这些在图像分析方面的进步为探索更深入的生物学问题打开了大门——包括蛋白质在细胞内的空间组织方式。
- en: Spatial Organization Within the Cell
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 细胞内的空间组织
- en: 'Cells aren’t just sacks of molecules; they’re intricately organized. Subcellular
    compartments evolved billions of years ago and represent one of the most fundamental
    principles of biology: *specialization through spatial organization*. Instead
    of letting everything float freely in an undifferentiated soup, cells developed
    internal structures that separate and coordinate different functions.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 细胞不仅仅是分子的袋子；它们是错综复杂的。亚细胞隔室在数十亿年前进化而来，代表了生物学最基本的原则之一：通过空间组织实现**专业化**。细胞不是让所有东西都在未分化的汤中自由漂浮，而是发展了内部结构，以分隔和协调不同的功能。
- en: All three domains of life—bacteria, archaea, and eukaryotes—show signs of this
    spatial complexity. In eukaryotic cells (the kinds of cells found in humans, plants,
    fungi, and more), this organization is especially pronounced. These cells contain
    membrane-bound compartments called *organelles*, each with a specialized job,
    as shown in [Figure 6-1](#organelle-image).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 所有三个生命领域——细菌、古菌和真核生物——都显示出这种空间复杂性的迹象。在真核细胞（人类、植物、真菌等生物体内的细胞）中，这种组织结构尤为明显。这些细胞包含称为**细胞器**的膜结合隔室，每个细胞器都有其专门的功能，如图[图6-1](#organelle-image)所示。
- en: '![](assets/dlfb_0601.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0601.png)'
- en: Figure 6-1\. A visual representation of the complex organization within a eukaryotic
    cell. The different organelles compartmentalize the cell into regions where different
    specialized functions are performed (illustration from the National Institutes
    of Health).
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-1\. 欧洲细胞内复杂组织的视觉表示。不同的细胞器将细胞分隔成执行不同专门功能的区域（来自美国国家卫生研究院的插图）。
- en: 'We won’t dive into full organelle flashcards here (no need to relive high school
    biology trauma), but here’s a quick recap of the structures that will be most
    relevant in this chapter:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里不会深入探讨完整的细胞器闪卡（无需重温高中生物的痛苦经历），但这里简要回顾一下本章中将最相关的结构：
- en: Plasma membrane
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 质膜
- en: A lipid bilayer that wraps the cell, protecting its internal environment and
    controlling what enters and exits.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一层脂质双分子层包裹着细胞，保护其内部环境，并控制物质进出。
- en: Nucleus
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 核
- en: The command center of the cell. It’s enclosed in its own membrane and houses
    the DNA—the genetic blueprint.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 细胞的指挥中心。它被自己的膜所包围，并容纳着DNA——遗传蓝图。
- en: Cytoplasm
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 细胞质
- en: The gel-like substance that fills the cell. This is where most cellular activity
    happens, and it’s home to many other organelles.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 填充细胞的一种凝胶状物质。大多数细胞活动都发生在这里，许多其他细胞器也在这里。
- en: Mitochondria
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 线粒体
- en: Often called the “powerhouses” of the cell. They generate energy in the form
    of ATP. Evolutionarily, they were once free-living bacteria that formed a mutually
    beneficial partnership with early cells.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 常被称为细胞的“动力工厂”。它们以ATP的形式产生能量。从进化的角度来看，它们曾经是自由生活的细菌，与早期细胞形成了互利共生的合作关系。
- en: Endoplasmic reticulum (ER)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 内质网（ER）
- en: Comes in two flavors. The *rough ER* is studded with ribosomes (protein-making
    factories) and helps synthesize proteins. The *smooth ER* handles lipids and detoxification.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种类型。*粗面内质网*上布满了核糖体（蛋白质制造工厂）并帮助合成蛋白质。*平滑内质网*处理脂质和解毒。
- en: Golgi apparatus
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 高尔基体
- en: The cell’s shipping center. It modifies, packages, and routes proteins and lipids
    to their final destinations.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 细胞的配送中心。它修改、包装并将蛋白质和脂质运送到它们的最终目的地。
- en: With that background on cellular compartments in place, let’s turn to protein
    localization.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解了细胞区室的基础知识后，让我们转向蛋白质定位。
- en: Protein Localization
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蛋白质定位
- en: 'Every protein begins its life in the same way: it’s transcribed from a gene
    into messenger RNA (mRNA) in the nucleus, and then it is translated into a chain
    of amino acids by ribosomes in the cytoplasm. But from there, things get much
    more spatially complex.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 每种蛋白质的生命开始方式都是相同的：它从基因转录成信使RNA（mRNA）在细胞核中，然后由细胞质中的核糖体将其翻译成氨基酸链。但从那里开始，事情变得在空间上更加复杂。
- en: Once a protein is made, it doesn’t just float around randomly. It gets shipped
    to a specific destination inside the cell, guided by a sort of molecular postal
    code. Some proteins are sent to the nucleus, others to the mitochondria, the cell
    membrane, or the ER. This process is known as *protein localization*, and it’s
    critical for proper cellular function.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 蛋白质一旦合成，它不会随意漂浮。它会被运送到细胞内的一个特定目的地，由一种分子邮政编码引导。一些蛋白质被运送到细胞核，其他被运送到线粒体、细胞膜或内质网。这个过程被称为*蛋白质定位*，对于正常的细胞功能至关重要。
- en: A protein’s location is often just as important as its molecular structure.
    Even a perfectly formed protein can’t function if it’s in the wrong place. For
    example, the protein DNA polymerase needs to be inside the nucleus to carry out
    its function of replicating DNA. If it ends up in the cytoplasm, it’s effectively
    useless. On the other hand, some proteins aren’t confined to a specific location.
    For instance, the protein actin is found throughout the cell, where it helps maintain
    its shape and enable movement.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 蛋白质的位置通常与其分子结构一样重要。即使是一个结构完美的蛋白质，如果它出现在错误的位置，也无法发挥作用。例如，DNA聚合酶蛋白质需要在细胞核内才能执行其复制DNA的功能。如果它最终出现在细胞质中，它实际上就变得毫无用处。另一方面，一些蛋白质并不局限于特定的位置。例如，肌动蛋白蛋白质遍布整个细胞，它有助于维持细胞的形状并使细胞能够移动。
- en: Mislocalization—when a protein ends up in the wrong compartment—is a key driver
    of disease. For example, in amyotrophic lateral sclerosis (ALS), the protein TDP-43
    accumulates in the cytoplasm of neurons, even though its normal location is in
    the nucleus.^([2](ch06.html#id946)) The protein isn’t mutated, misfolded, or present
    in abnormal quantities—it’s simply in the wrong place. And that alone is enough
    to disrupt cellular function and trigger disease.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 蛋白质定位错误——当蛋白质最终出现在错误的空间时——是疾病的关键驱动因素。例如，在肌萎缩侧索硬化症（ALS）中，蛋白质TDP-43积累在神经元的细胞质中，尽管它的正常位置是在细胞核中。[2](ch06.html#id946)
    蛋白质并没有发生突变、错误折叠或以异常的数量存在——它只是简单地出现在了错误的位置。而仅仅这一点就足以破坏细胞功能并引发疾病。
- en: Another example is the tumor suppressor BRCA1, which normally helps repair DNA
    in the nucleus. In some breast and ovarian cancers, BRCA1 is mislocalized to the
    cytoplasm, where it can no longer perform its repair function, even though the
    protein itself may be entirely structurally normal.^([3](ch06.html#id947))
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是肿瘤抑制因子BRCA1，它通常有助于修复细胞核中的DNA。在一些乳腺癌和卵巢癌中，BRCA1被错误地定位到细胞质中，在那里它无法再执行其修复功能，尽管蛋白质本身在结构上可能完全正常。[3](ch06.html#id947)
- en: Understanding Protein Localization
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解蛋白质定位
- en: 'Once a protein is made, how does it end up in the right part of the cell? The
    answer lies in short amino acid sequences, which are like molecular zip codes
    embedded within the protein. These address tags are recognized by the cell’s transport
    machinery and guide proteins to their proper destinations: the nucleus, mitochondria,
    plasma membrane, and so on.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 蛋白质一旦合成，它是如何最终到达细胞正确部分的？答案在于蛋白质中嵌入的短氨基酸序列，这些序列就像分子邮政编码。这些地址标签被细胞的运输机械识别，并将蛋白质引导到它们正确的目的地：细胞核、线粒体、质膜等。
- en: Warning
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: The rules of this system are still only partially understood. Many proteins
    localize to multiple compartments. Some are misdirected under stress or disease
    conditions. And some just seem to defy categorization.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个系统的规则仍然只有部分被理解。许多蛋白质定位到多个细胞器中。有些在压力或疾病条件下会误定向。还有一些似乎难以归类。
- en: Even in healthy cells, we don’t yet have a complete map of where all human proteins
    go—or how those localizations change across cell types, developmental stages,
    or environmental conditions.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在健康细胞中，我们还没有一个完整的地图来显示所有人类蛋白质的去向——或者这些定位如何随细胞类型、发育阶段或环境条件的变化而变化。
- en: 'Mapping the protein localization landscape in detail is one of the big open
    challenges in cell biology. Understanding protein localization at scale could:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 详细绘制蛋白质定位景观是细胞生物学中的一个重大开放挑战。大规模理解蛋白质定位可能：
- en: Reveal new, previously unknown subcellular compartments
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 揭示新的、以前未知的亚细胞器
- en: It may be surprising, but we’re still discovering fundamental structures inside
    cells. In recent years, researchers have identified the *exclusome* (a cytoplasmic
    DNA compartment in mammalian cells), *paraspeckles* and *nuclear speckles* (membraneless
    nuclear bodies involved in RNA processing), and even new entire organelles. The
    most recent example is the *nitroplast*, a nitrogen-fixing organelle discovered
    in marine algae as recently as 2024.^([4](ch06.html#id948)) These findings show
    how much more there is to uncover about the cell’s internal architecture.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会令人惊讶，但我们仍在发现细胞内部的根本结构。近年来，研究人员已经确定了*外核区*（哺乳动物细胞中的细胞质DNA区室）、*旁核小体*和*核小体*（参与RNA处理的膜无核体），甚至发现了新的整个细胞器。最新的例子是*硝化体*，这是一种在2024年最近在海洋藻类中发现的固氮细胞器。[^([4](ch06.html#id948))]
    这些发现显示了关于细胞内部结构的更多未知领域。
- en: Help assign functions to poorly annotated proteins
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 帮助分配功能给注释不良的蛋白质
- en: If a protein consistently localizes to mitochondria, you could hypothesize that
    it plays a role in energy metabolism or apoptosis (programmed cell death), which
    are key functions of the mitochondria. For example, the cytoself model we study
    in this chapter grouped several previously uncharacterized proteins with known
    mitochondrial proteins, leading researchers to propose their involvement in oxidative
    phosphorylation—the process by which cells generate ATP within the mitochondrial
    matrix.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个蛋白质持续定位到线粒体，你可以假设它在能量代谢或细胞凋亡（程序性细胞死亡）中发挥作用，这些是线粒体的关键功能。例如，我们在本章研究的细胞自体模型将几个以前未表征的蛋白质与已知的线粒体蛋白质分组，导致研究人员提出它们可能参与氧化磷酸化——细胞在细胞器基质中生成ATP的过程。
- en: Detect early cellular changes that mark disease
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 检测标记疾病的早期细胞变化
- en: Shifts in protein localization can serve as early warning signs of various diseases.
    For example, we previously mentioned that in ALS, the protein TDP-43 moves from
    the nucleus to the cytoplasm, but remarkably, this change has been observed in
    presymptomatic individuals carrying disease-linked mutations.^([5](ch06.html#id949))
    More broadly, large-scale profiling of localization patterns could help detect
    early cellular dysfunction across a wide range of conditions.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 蛋白质定位的改变可以作为各种疾病的早期预警信号。例如，我们之前提到在肌萎缩侧索硬化症（ALS）中，蛋白质TDP-43从细胞核移动到细胞质，但令人惊讶的是，这种变化已经在携带疾病相关突变的先兆症状个体中被观察到。[^([5](ch06.html#id949))]
    更广泛地说，对定位模式的规模化分析可以帮助检测广泛条件下早期细胞功能障碍。
- en: Therapies that correct protein mislocalization
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 纠正蛋白质错位定位的疗法
- en: In diseases where a protein is physically functional but simply ends up in the
    wrong place, one therapeutic strategy is to restore its proper localization using
    engineered localization signals. For example, researchers have used nuclear localization
    signals to redirect tumor suppressors like p53 or BRCA1 back to the nucleus, where
    they can resume their normal function.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在蛋白质在物理上功能正常但最终定位错误的地方，一种治疗策略是使用工程化的定位信号恢复其正确的定位。例如，研究人员已经使用核定位信号将肿瘤抑制因子如p53或BRCA1重新定向回细胞核，在那里它们可以恢复其正常功能。
- en: Better therapeutic targeting
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的治疗靶向
- en: Another approach is to guide drugs, proteins, or nanoparticles to specific subcellular
    compartments, such as the lysosome or mitochondria, to maximize their effectiveness
    and minimize side effects. This strategy is used in emerging nanomedicine platforms.^([6](ch06.html#id950))
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是引导药物、蛋白质或纳米颗粒到特定的亚细胞器，如溶酶体或线粒体，以最大化其效果并最小化副作用。这种策略在新兴的纳米医学平台上被使用。[^([6](ch06.html#id950))]
- en: Imagine being able to say not just *what* a protein does but *where* it does
    it—and how its journey changes as a cell divides, differentiates, or begins to
    break down in disease.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你不仅能说出蛋白质做什么，还能说出它在哪里做——以及它的旅程如何随着细胞的分裂、分化或开始疾病分解而改变。
- en: Ideally, that gives you a sense of how exciting the protein localization space
    is. Now let’s dive into how machine learning can help us explore it.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，这让你对蛋白质定位空间有多么激动人心有了感觉。现在让我们深入了解机器学习如何帮助我们探索它。
- en: Machine Learning Primer
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习入门
- en: 'The model we’ll be building in this chapter—the cytoself from the Kobayashi
    paper mentioned earlier—is based on a type of neural network called a *vector-quantized
    variational autoencoder (VQ-VAE)*. If you haven’t seen this type of model before,
    don’t worry: we’ll walk through the key ideas that lead up to this model so you
    understand not just *what* we’re building but *why* it works.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将构建的模型——来自之前提到的Kobayashi论文中的细胞自我——基于一种称为*向量量化变分自动编码器（VQ-VAE）*的神经网络类型。如果你之前没有见过这种类型的模型，不要担心：我们将逐步介绍导致此模型的关键思想，以便你不仅理解我们正在构建什么，还理解它为什么有效。
- en: 'Why use a VQ-VAE? Unlike most models, which map images into a continuous feature
    space, a VQ-VAE forces the model to describe each image using a limited set of
    learned visual patterns, called a *codebook*. You can think of this like a tile
    set or a visual vocabulary, where each pattern gets reused across many inputs.
    This encourages the model to represent each protein image using discrete building
    blocks, making it easier to group proteins by similar localization and uncover
    shared visual motifs. In other words, instead of inventing new coordinates for
    every input, the model says, “This protein looks like a mix of tile #7 and tile
    #241.”'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么使用VQ-VAE？与大多数将图像映射到连续特征空间的模型不同，VQ-VAE迫使模型使用一组有限的、学习到的视觉模式来描述每张图像，这些模式被称为*代码簿*。你可以将其想象成一个瓷砖集或视觉词汇表，其中每个模式在许多输入中都被重复使用。这鼓励模型使用离散的构建块来表示每个蛋白质图像，使其更容易根据相似的定位对蛋白质进行分组，并揭示共享的视觉模式。换句话说，模型不是为每个输入发明新的坐标，而是说：“这种蛋白质看起来像是瓷砖#7和瓷砖#241的混合体。”
- en: This kind of representation is especially useful in biology, where we’re trying
    to *discover* structure in the data, not just reconstruct it. The discreteness
    also makes the model more interpretable and allows downstream tools (like clustering
    or dimensionality reduction) to work more effectively.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的表示在生物学中特别有用，因为我们试图在数据中*发现*结构，而不仅仅是重建它。这种离散性也使得模型更具可解释性，并允许下游工具（如聚类或降维）更有效地工作。
- en: Autoencoders (AEs)
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动编码器（AEs）
- en: 'Let’s start by unpacking the last and arguably most important part of the VQ-VAE
    acronym: the AE, *autoencoder*.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从VQ-VAE缩写中最后也是 arguably 最重要的一部分开始：AE，*自动编码器*。
- en: 'An autoencoder is a type of neural network trained to reconstruct its input.
    It does this in two steps:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器是一种经过训练以重建其输入的神经网络。它通过以下两个步骤来完成：
- en: An *encoder* compresses the input into a lower-dimensional representation.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个*编码器*将输入压缩成一个低维度的表示。
- en: A *decoder* then attempts to reconstruct the original input from that compressed
    version.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，一个*解码器*试图从那个压缩版本中重建原始输入。
- en: In the simplest case, both the encoder and decoder might consist of fully connected
    linear layers. The internal representation, known as the bottleneck, typically
    has fewer neurons than the input, forcing the model to compress the data, as illustrated
    in [Figure 6-2](#autoencoder-diagram) (diagram taken from “Introduction to Autoencoders”).^([7](ch06.html#id956))
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在最简单的情况下，编码器和解码器可能都由全连接线性层组成。内部表示，称为瓶颈，通常比输入具有更少的神经元，迫使模型压缩数据，如图[图6-2](#autoencoder-diagram)（来自“自动编码器简介”的图表）所示。（^([7](ch06.html#id956)）
- en: '![](assets/dlfb_0602.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0602.png)'
- en: Figure 6-2\. An autoencoder learns a compressed internal representation of its
    input by forcing it through a low-dimensional “bottleneck.” The network is trained
    to reconstruct the original input as closely as possible from this bottleneck
    representation.
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-2\. 自动编码器通过强制输入通过一个低维度的“瓶颈”来学习其输入的压缩内部表示。该网络被训练从瓶颈表示尽可能精确地重建原始输入。
- en: The bottleneck forces the model to distill the most important patterns in the
    data, while discarding irrelevant details. This is a form of dimensionality reduction,
    similar in spirit to techniques like principal component analysis (PCA). But unlike
    PCA, autoencoders can learn nonlinear transformations and scale to large, complex
    datasets, making them especially powerful for data like microscopy images.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 瓶颈层迫使模型提炼数据中最重要的模式，同时丢弃无关的细节。这是一种降维形式，与主成分分析（PCA）等技术精神相似。但与PCA不同，自动编码器可以学习非线性变换并扩展到大型、复杂的数据集，这使得它们对于像显微镜图像这样的数据特别强大。
- en: 'One important detail: in a standard autoencoder, the internal representation—the
    activations of the neurons in the bottleneck layer—are *continuous*. That means
    each neuron in the bottleneck can take on any real number (like 1.27, –3.14, etc.),
    so the representation of each input lives in a continuous space. This gives the
    model a lot of flexibility, but it can also make the latent space less structured
    and harder to interpret. Two inputs that look quite similar might map to distant
    points in the latent space, and it can be difficult to understand what each dimension
    represents.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的细节：在标准自动编码器中，内部表示——瓶颈层中神经元的激活——是*连续的*。这意味着瓶颈层中的每个神经元可以取任何实数（如1.27、-3.14等），因此每个输入的表示都存在于一个连续空间中。这给模型带来了很大的灵活性，但也可以使潜在空间的结构更少，更难以解释。两个看起来相当相似输入可能映射到潜在空间中的遥远点，理解每个维度代表什么可能很困难。
- en: 'Later, we’ll see how VQ-VAEs address this by introducing a fixed set of allowed
    *codes*. Instead of letting the encoder output arbitrary values, it’s forced to
    pick from a dictionary of discrete latent vectors—a design that makes the representation
    more structured, compressible, and interpretable. Of course, this comes with a
    trade-off: the model gives up some of the expressivity of continuous representations
    in exchange for a more constrained and interpretable latent space.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们将看到VQ-VAEs如何通过引入一组固定的允许*代码*来解决这一问题。不是让编码器输出任意值，而是迫使它从一组离散的潜在向量字典中选择——这种设计使表示更有结构、可压缩和可解释。当然，这也伴随着权衡：模型牺牲了连续表示的一些表达能力，以换取一个更受限制和可解释的潜在空间。
- en: Tip
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Cytoself uses a ResNet-style CNN as its encoder. If you’d like a refresher on
    CNNs and ResNets, see the previous chapter’s explanation of these topics.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Cytoself使用ResNet风格的CNN作为其编码器。如果您想复习CNN和ResNets，请参阅前一章对这些主题的解释。
- en: Variational Autoencoders (VAEs)
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变分自动编码器（VAEs）
- en: Now that we’ve introduced regular autoencoders, let’s look at a variation that
    adds a twist—and opens the door to powerful generative capabilities. The *V* in
    *VAE* stands for *variational*. In a *variational autoencoder*, we no longer compress
    each input (like a microscopy image) into a single point in the latent space.
    Instead, the model learns to represent each input as a *probability distribution*
    over the latent space. That’s a bit of a mouthful, so let’s break it down step-by-step.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了常规自动编码器，让我们看看一种添加了转折——并开启了强大生成能力之门的变体。在*VAE*中的*V*代表*变分*。在*变分自动编码器*中，我们不再将每个输入（如显微镜图像）压缩到潜在空间中的单个点。相反，模型学习将每个输入表示为潜在空间上的*概率分布*。这有点难以理解，所以让我们一步一步地分解它。
- en: 'In a standard autoencoder:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准自动编码器中：
- en: 'The encoder takes an input and produces a fixed set of numbers: one activation
    for each neuron in the bottleneck layer we described earlier.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器接收输入并生成一组固定的数字：每个瓶颈层中神经元的激活值。
- en: These numbers describe a single point in latent space.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些数字描述了潜在空间中的一个单一点。
- en: The decoder then uses that point to reconstruct the input.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器随后使用该点来重建输入。
- en: 'In a variational autoencoder:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在变分自动编码器中：
- en: 'The encoder outputs two numbers per latent dimension: a *mean* and a *standard
    deviation*.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器为每个潜在维度输出两个数字：一个*均值*和一个*标准差*。
- en: These define a normal distribution—a bell curve—for each coordinate in the latent
    space.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些定义了潜在空间中每个坐标的正态分布——一个钟形曲线。
- en: Instead of feeding a fixed number to the decoder, the model samples a value
    from each distribution.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 而不是向解码器提供固定数量的值，模型从每个分布中采样一个值。
- en: 'To make this concrete:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这一点更具体：
- en: In a standard autoencoder, a given input might be compressed to a single point
    in the latent space, for example, `[1.3, -0.7, 1.9]` for a bottleneck layer with
    three neurons. This exact vector is passed *directly* to the decoder to reconstruct
    the input.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在标准自动编码器中，给定的输入可能被压缩到潜在空间中的一个单独的点，例如，瓶颈层有三个神经元时，`[1.3, -0.7, 1.9]`。这个确切的向量被*直接*传递给解码器以重建输入。
- en: 'In a variational autoencoder, the encoder instead outputs two separate vectors:'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在变分自动编码器中，编码器相反地输出两个独立的向量：
- en: One for the means (e.g., `[1.3, -0.7, 1.9]`)
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于均值（例如，`[1.3, -0.7, 1.9]`）
- en: And one for the standard deviations (e.g., `[0.2, 0.5, 0.1]`)
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以及一个用于标准差（例如，`[0.2, 0.5, 0.1]`）
- en: 'This is often done using a shared hidden layer followed by two parallel linear
    layers: one predicts the means, the other the standard deviations. So, in this
    example, the encoder outputs six values in total: three means and three standard
    deviations.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常是通过一个共享的隐藏层后面跟着两个并行线性层来完成的：一个预测均值，另一个预测标准差。所以，在这个例子中，编码器总共输出六个值：三个均值和三个标准差。
- en: 'Together, these describe a 3D Gaussian distribution: not a single point, but
    a cloud of likely values. During training, the model randomly samples a vector
    from this distribution; for example:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一起，这些描述了一个3D高斯分布：不是一个单独的点，而是一团可能的值。在训练过程中，模型从这个分布中随机采样一个向量；例如：
- en: Something like `1.1` from a distribution centered at `1.3 ± 0.2`
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似于从以`1.3 ± 0.2`为中心的分布中得到的`1.1`
- en: '`-0.5` from `-0.7 ± 0.5`'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-0.5` 来自 `-0.7 ± 0.5`'
- en: '`1.7` from `1.9 ± 0.1`'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1.7` 来自 `1.9 ± 0.1`'
- en: The sampled vector—in this case, `[1.1, -0.5, 1.7]`—is what actually gets passed
    to the decoder for reconstruction.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 样本向量——在这个例子中，`[1.1, -0.5, 1.7]`——实际上是传递给解码器进行重建的。
- en: Why add randomness?
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么添加随机性？
- en: 'At first, introducing randomness might sound like unnecessary fuzziness: why
    not just stick with a fixed encoding? But this design choice has several powerful
    benefits:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，引入随机性可能听起来像是不必要的模糊：为什么不坚持使用固定的编码呢？但这个设计选择有几个强大的好处：
- en: It smooths the latent space.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 它平滑了潜在空间。
- en: Because the model samples slightly different encodings for the same input, it
    learns to decode nearby points into similar outputs. This forces the latent space
    to be smooth, continuous, and meaningful—with small movements in the space leading
    to small, realistic changes in the output.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 因为模型为相同的输入采样了略微不同的编码，它学会了将附近的点解码成相似输出。这迫使潜在空间变得平滑、连续且有意义——空间中的小移动会导致输出中的小、现实的变化。
- en: It groups similar inputs together.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 它将相似的输入分组在一起。
- en: Inputs that are alike produce similar distributions, so their samples overlap
    in the latent space. This naturally pulls similar data points closer together,
    helping the model learn structure in the dataset.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 相似的输入产生相似的分布，因此它们的样本在潜在空间中重叠。这自然会将相似的数据点拉近，帮助模型在数据集中学习结构。
- en: It prevents overfitting.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 它防止过拟合。
- en: By introducing controlled randomness during training, the model can’t just memorize
    exact input-output pairs. It has to learn patterns that hold across perturbations.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在训练过程中引入可控的随机性，模型不能只是记住精确的输入输出对。它必须学习在扰动中保持的规律。
- en: It enables generation.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 它使生成成为可能。
- en: Once trained, the model can generate new, realistic-looking outputs by simply
    sampling from the latent space, even in regions that weren’t seen during training.
    This makes VAEs useful not just for reconstruction, but for creative or exploratory
    tasks.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，模型可以通过简单地从潜在空间中采样来生成新的、看起来逼真的输出，即使在训练期间没有看到的区域。这使得VAEs不仅对重建有用，而且对创意或探索性任务也很有用。
- en: Continuous latent space
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 连续潜在空间
- en: We can visualize this difference in how latent spaces are structured in VAEs
    versus standard autoencoders using the diagram in [Figure 6-3](#latent-space-structure)
    (based on a figure by Saul Dobilas).^([8](ch06.html#id961))
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用[图6-3](#latent-space-structure)中的图表来可视化VAEs与标准自动编码器中潜在空间结构的不同（基于Saul Dobilas的图表）。（^([8](ch06.html#id961)）
- en: '![](assets/dlfb_0603.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0603.png)'
- en: Figure 6-3\. An intuitive way to think about regularized continuous latent spaces.
    In a standard autoencoder, points are mapped discretely and may not generalize
    meaningfully. In a VAE, points are sampled from smooth distributions, enabling
    meaningful interpolation and generative sampling.
  id: totrans-113
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-3\. 关于正则化连续潜在空间的一种直观思考方式。在标准自动编码器中，点被离散映射，可能无法有意义地推广。在变分自动编码器中，点是从平滑分布中采样的，这使有意义的插值和生成采样成为可能。
- en: This diagram highlights how VAEs encourage a smooth and continuous latent space,
    enabling interpolation and generation, a key distinction from regular autoencoders.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 此图突出了 VAEs 如何鼓励平滑且连续的潜在空间，从而实现插值和生成，这是与常规自编码器的一个关键区别。
- en: Note
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'You might wonder: can’t you use a regular autoencoder for grouping similar
    inputs together and for generative sampling?'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想：不能使用常规自编码器来将相似的输入分组并用于生成采样吗？
- en: In theory, yes; similar inputs often *do* end up close together in the latent
    space, and you *can* try sampling from it. But there’s no guarantee that the space
    will be smooth, continuous, or meaningful. Some regions may decode into nonsense,
    and small changes in latent space might lead to big, unpredictable jumps in the
    output.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在理论上，是的；相似的输入通常在潜在空间中会靠近，你可以尝试从中采样。但无法保证空间是平滑、连续或具有意义的。某些区域可能解码成无意义的内容，而在潜在空间中的微小变化可能会导致输出中出现大而不可预测的跳跃。
- en: VAEs solve this by explicitly *shaping the latent space*. They use probability
    distributions and regularization to encourage the model to use the space in a
    more structured and consistent way.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: VAEs 通过明确 *塑造潜在空间* 来解决这个问题。它们使用概率分布和正则化来鼓励模型以更结构化和一致的方式使用空间。
- en: 'There’s just one more concept to cover before we can understand this chapter’s
    architecture: *vector quantization*.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够理解这一章架构之前，还有一个概念需要介绍：*向量量化*。
- en: Vector-Quantized Variational Autoencoders (VQ-VAEs)
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量量化变分自编码器（VQ-VAEs）
- en: The *VQ* in *VQ-VAE* stands for *vector quantization*, a classical technique
    borrowed from signal processing and data compression. At its core, vector quantization
    means taking a continuous input, such as a floating-point vector, and snapping
    it to the nearest match from a fixed set of allowed vectors. This set is called
    a *codebook*.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *VQ-VAE* 中的 *VQ* 代表 *向量量化*，这是一种从信号处理和数据压缩中借用的经典技术。其核心是，向量量化意味着将连续的输入，例如浮点向量，捕捉到一组允许的向量中的最近匹配项。这个集合被称为
    *码本*。
- en: 'To make this concrete:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这一点具体化：
- en: 'Imagine our codebook contains just two vectors: `[2, 0.5]` and `[1, -3]`.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设我们的码本中只包含两个向量：`[2, 0.5]` 和 `[1, -3]`。
- en: Now suppose the encoder outputs `[1.8, 0.3]`.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在假设编码器输出 `[1.8, 0.3]`。
- en: Instead of passing this continuous vector to the decoder, the VQ-VAE finds the
    nearest codebook entry—in this case, `[2, 0.5]` —and *replaces* the encoder output
    with that vector.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VQ-VAE 不将这个连续向量传递给解码器，而是找到最近的码本条目——在这种情况下，`[2, 0.5]` ——并用该向量 *替换* 编码器的输出。
- en: This “snapping” process is called *quantization*. You can think of it like rounding
    a continuous input to the closest available option. The decoder never sees the
    raw encoder output, only the snapped codebook vector.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这个“捕捉”过程被称为 *量化*。你可以将其想象为将连续输入四舍五入到最近的可用选项。解码器从未看到原始编码器的输出，只看到捕捉到的码本向量。
- en: 'This makes VQ-VAEs different from variational autoencoders or standard autoencoders:
    rather than learning a continuous latent space, the model learns a *discrete vocabulary
    of embeddings* and uses that to represent everything it sees. By compressing the
    encoder output to a finite set of discrete vectors, the model encourages robustness,
    interpretability, and reusability in its representations, all of which are particularly
    helpful for downstream tasks like clustering or biological discovery.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得 VQ-VAEs 与变分自编码器或标准自编码器不同：而不是学习一个连续的潜在空间，该模型学习一个 *离散的嵌入词汇表* 并使用它来表示它所看到的一切。通过将编码器的输出压缩到一组有限的离散向量中，模型鼓励其在表示中的鲁棒性、可解释性和可重用性，这些都特别有助于下游任务，如聚类或生物发现。
- en: Where does the codebook come from?
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 码本从哪里来？
- en: The codebook in a VQ-VAE is *learned during training*. Just like the weights
    in the encoder and decoder, the vectors in the codebook start out random and are
    gradually refined through backpropagation. Over time, the vectors adapt to represent
    recurring, meaningful patterns in the data, so the model gets better at snapping
    encoder outputs to useful representations.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: VQ-VAE 中的码本是在训练过程中 *学习到的*。就像编码器和解码器中的权重一样，码本中的向量最初是随机的，并通过反向传播逐渐细化。随着时间的推移，向量适应以表示数据中的重复和有意义模式，因此模型在将编码器输出捕捉到有用的表示方面变得更好。
- en: How large should the codebook be?
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 码本应该有多大？
- en: 'There’s no one-size-fits-all answer: the optimal number of vectors depends
    on your data and goals. A *larger codebook* (e.g., 1,024+ entries) allows finer
    distinctions between inputs. A *smaller codebook* (e.g., 64–128) forces the model
    to reuse patterns more often, which can help with generalization and interpretability.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 没有一种适合所有情况的答案：最优的向量数量取决于你的数据和目标。一个*更大的代码簿*（例如，1,024+条条目）允许在输入之间进行更精细的区分。一个*较小的代码簿*（例如，64–128）迫使模型更频繁地重用模式，这有助于泛化和可解释性。
- en: In biological imaging tasks like protein localization, codebook sizes typically
    range from 128 to 512, depending on the number of proteins, the resolution of
    localization patterns, and the desired balance between expressiveness and interpretability.
    Cytoself, for instance, uses two codebooks of 2,048 entries, each made up of 64D
    vectors—one for the global representation and one for the local—giving the model
    rich capacity to represent complex spatial patterns.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在像蛋白质定位这样的生物成像任务中，代码簿的大小通常在128到512之间，这取决于蛋白质的数量、定位模式的分辨率以及表达性和可解释性之间所期望的平衡。例如，Cytoself使用两个包含2,048个条目的代码簿，每个由64D向量组成——一个用于全局表示，一个用于局部——使模型具有丰富的能力来表示复杂的空间模式。
- en: Dissecting a VQ-VAE Diagram
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解构 VQ-VAE 图表
- en: Now that we’ve covered the key building blocks—autoencoders, variational inference,
    and vector quantization—we’re ready to interpret the original VQ-VAE diagram,
    shown in [Figure 6-4](#vqvae-original).^([9](ch06.html#id975))
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了关键构建块——自动编码器、变分推理和向量量化——我们准备解释原始的VQ-VAE图表，如图[图6-4](#vqvae-original)^([9](ch06.html#id975))所示。
- en: '![](assets/dlfb_0604.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0604.png)'
- en: Figure 6-4\. Illustration of the major components in a VQ-VAE, based on van
    den Oord et al. (2017).
  id: totrans-136
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-4\. 基于 van den Oord 等人（2017）的 VQ-VAE 主要组件的示意图。
- en: 'Here’s a step-by-step breakdown of what’s happening in the above VQ-VAE diagram:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是对上述VQ-VAE图表中发生的事情的逐步分解：
- en: 1\. Input image (dog photo on the far left)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 输入图像（最左边是狗的照片）
- en: The process begins with a raw input image; here, the input is a photo of a dog.
    In our case, this would be a microscopy image showing protein localization. The
    image is passed into a CNN encoder, which extracts meaningful features and transforms
    the image into a compressed latent representation.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程从原始输入图像开始；这里，输入是一张狗的照片。在我们的案例中，这将是一张显示蛋白质定位的显微镜图像。图像被传递到一个CNN编码器，它提取有意义的特征并将图像转换成一个压缩的潜在表示。
- en: This representation is shown as the *cube labeled `z[e](x)`* (the variable `z`
    is often used to denote embeddings). It’s a 3D tensor because CNNs process images
    into pseudoimages called feature maps, which highlight specific parts of the input.
    This cube holds the distilled, continuous version of the input, and it’s what
    will be quantized next using the codebook.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这种表示形式显示为*标记为`z[e](x)`的立方体*（变量`z`通常用来表示嵌入）。它是一个3D张量，因为CNN将图像处理成伪图像，称为特征图，这些特征图突出了输入的特定部分。这个立方体包含了输入的蒸馏、连续版本，它将是接下来使用代码簿进行量化的内容。
- en: 2\. Quantization step (dot to the right of the left cube)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 量化步骤（左立方体右侧的点）
- en: Each vector in the cube `z[e](x)`—the continuous encoder output—is passed to
    the *vector quantization step*. This step compares each D-dimensional vector (D
    = 64 for cytoself) in the grid to every entry in a learned *codebook* (`e[1]`,
    `e[2]`, …, `e[k]`), shown at the top of the diagram.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 立方体`z[e](x)`中的每个向量——连续编码器的输出——都被传递到*向量量化步骤*。这一步骤将网格中的每个D维向量（D = 64对于cytoself）与学习到的*代码簿*（`e[1]`，`e[2]`，…，`e[k]`）中的每个条目进行比较，如图表顶部所示。
- en: The model then *replaces* that vector with the nearest codebook vector, snapping
    it to the closest match. This creates a new tensor called `z[q](x)`, the quantized
    version of the encoder output, where every original vector has been replaced by
    a discrete one from the codebook.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 模型然后将该向量替换为最近的代码簿向量，将其固定到最接近的匹配项。这创建了一个新的张量`z[q](x)`，它是编码器输出的量化版本，其中每个原始向量都被代码簿中的一个离散向量所替换。
- en: 3\. Decoder (cube on the right)
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 解码器（右边的立方体）
- en: Once quantized, the decoder input `z[q](x)` (right cube) is passed into the
    *decoder* (“CNN” arrow), which is another CNN. Its job is to take the snapped,
    discrete representation and *reconstruct the original image* as closely as possible.
    In this diagram, the decoder output is shown as a second image of the dog, visually
    similar to the input. For us in this chapter, this would be a reconstructed microscopy
    image.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦量化，解码器输入 `z[q](x)`（右侧立方体）被传递到**解码器**（“CNN”箭头），它也是一个 CNN。其任务是尽可能精确地重建原始图像。在这个图中，解码器输出显示为狗的第二幅图像，视觉上与输入相似。对于我们这一章来说，这将是一个重建的显微镜图像。
- en: The closer the reconstruction is to the original (i.e., the lower the reconstruction
    loss), the better the model has learned to encode meaningful information in its
    quantized representation.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 重构与原始输入越接近（即，重构损失越低），则模型在量化表示中编码有意义信息的程度就越好。
- en: 4\. Codebook embedding space (far right)
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 代码簿嵌入空间（最右侧）
- en: The panel on the far right shows a *zoomed-in view of the embedding space*,
    the set of vectors in the codebook. The central dot (with border and arrow) represents
    the encoder’s original output vector, `z[e](x)`, which sits somewhere in continuous
    space. The model snaps it to the closest codebook vector, here shown as `e[2]`.
    That snapped value becomes the quantized output `z[q](x)`.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 最右侧的图面板显示了嵌入空间的**放大视图**，即代码簿中的向量集。中心点（带有边框和箭头）代表编码器的原始输出向量 `z[e](x)`，它位于连续空间中的某个位置。模型将其映射到最近的代码簿向量，这里显示为
    `e[2]`。这个映射值成为量化输出 `z[q](x)`。
- en: 'The arrow indicates the *training signal*: during learning, the encoder is
    nudged to produce outputs that are closer to the codebook entries they get snapped
    to. This improves the efficiency of quantization over time and ensures that codebook
    entries are actually used.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 箭头表示**训练信号**：在学习过程中，编码器被微调以产生更接近它们映射到的代码簿条目的输出。这随着时间的推移提高了量化的效率，并确保代码簿条目实际上被使用。
- en: This is starting to get into VQ-VAE training strategy, so let’s now cover this
    topic explicitly.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这开始涉及到 VQ-VAE 的训练策略，因此现在让我们明确地讨论这个主题。
- en: Training a VQ-VAE
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练 VQ-VAE
- en: Now that we’ve broadly seen how a VQ-VAE is structured, let’s look at how it’s
    trained.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经大致了解了 VQ-VAE 的结构，让我们看看它是如何被训练的。
- en: 'Training a VQ-VAE involves optimizing more than just the *reconstruction loss*,
    that is, how closely the output image matches the original input. The model also
    needs to *refine the codebook vectors* over time and ensure that the encoder learns
    representations that snap cleanly to those vectors. This creates an interesting
    balance between three components:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 训练 VQ-VAE 涉及到优化不仅仅是**重构损失**，即输出图像与原始输入的匹配程度。模型还需要随着时间的推移**细化代码簿向量**，并确保编码器学习到的表示能够干净利落地映射到这些向量上。这在这三个组件之间创造了一个有趣的平衡：
- en: The quality of the encoder’s representations
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器表示的质量
- en: How well does the encoder capture the meaningful signals in the input data?
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器如何有效地捕捉输入数据中的有意义信号？
- en: The usefulness and coverage of the codebook
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 代码簿的有用性和覆盖率
- en: Does the codebook provide enough diversity to represent a wide range of inputs,
    and are all entries being used effectively?
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 代码簿是否提供了足够的多样性来表示广泛的输入，并且所有条目是否都被有效地使用？
- en: The decoder’s ability to reconstruct from quantized codes
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器从量化码中重建的能力
- en: How accurately can the decoder recover the original input using only the compressed,
    discrete representations?
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器仅使用压缩的离散表示，能有多准确地恢复原始输入？
- en: 'There are a few unique challenges in training VQ-VAEs to be aware of:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练 VQ-VAEs 时存在一些独特的挑战需要我们注意：
- en: Quantization error
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 量化误差
- en: If the nearest codebook vector is a poor match, some detail is lost during quantization,
    and the reconstruction suffers.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如果最近的代码簿向量匹配不佳，则在量化过程中会丢失一些细节，导致重构受损。
- en: The key is to make sure your codebook is large enough and your encoder is flexible
    enough to produce embeddings that land near useful entries.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 关键是确保你的代码簿足够大，并且你的编码器足够灵活，能够产生接近有用条目的嵌入。
- en: Codebook collapse
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 代码簿坍缩
- en: 'If only a few codebook vectors are used repeatedly while others are ignored,
    the model wastes capacity. Preventing this requires some extra care during training:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在训练过程中只重复使用少数代码簿向量，而忽略其他向量，则模型会浪费容量。防止这种情况需要训练过程中的额外关注：
- en: Commitment loss
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 承诺损失
- en: This loss term encourages the encoder to commit to a chosen codebook vector
    by penalizing large differences between the encoder output and its nearest codebook
    entry. It keeps the encoder from drifting too far away from the codebook.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这个损失项鼓励编码器通过惩罚编码器输出与其最近的代码簿条目之间的大差异来坚持选择一个代码簿向量。它防止编码器偏离代码簿太远。
- en: Entropy penalty
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 熵惩罚
- en: This encourages the model to use a wider range of codebook entries more evenly,
    increasing the diversity of representations and reducing collapse.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这鼓励模型更均匀地使用更广泛的代码簿条目，增加了表示的多样性并减少了崩溃。
- en: Despite these challenges, the VQ-VAE is a powerful architecture, especially
    when you want a model that produces discrete, interpretable representations while
    still learning directly from data.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些挑战，VQ-VAE 仍然是一个强大的架构，尤其是在你需要一个能够产生离散、可解释的表示同时还能直接从数据中学习的模型时。
- en: Note
  id: totrans-171
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If this feels a bit complex, you’re not alone—engaging with modern deep learning
    models can be challenging at first. But hands-on practice is the best way to build
    intuition and fluency. In the next section, you’ll start building and training
    a VQ-VAE yourself, applying everything we’ve covered so far.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这感觉有点复杂，你并不孤单——最初与现代深度学习模型打交道可能会很有挑战性。但动手实践是建立直觉和流畅性的最佳方式。在下一节中，你将开始自己构建和训练一个
    VQ-VAE，应用我们迄今为止所涵盖的所有内容。
- en: Constructing the Dataset
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建数据集
- en: All models need good data. In this section, we’ll take a closer look at how
    to prepare a cell imaging dataset to learn about protein localization.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 所有模型都需要良好的数据。在本节中，我们将更详细地探讨如何准备细胞成像数据集以了解蛋白质定位。
- en: Data Requirements
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据要求
- en: A high-quality dataset is crucial for successfully applying deep learning. Fortunately,
    there are several excellent protein localization resources available. One of the
    best is [OpenCell](https://oreil.ly/km_v9), which provides standardized, high-resolution
    images of human proteins inside cells, such as that shown in [Figure 6-5](#actb-opencell-screenshot).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 一个高质量的数据集对于成功应用深度学习至关重要。幸运的是，有几种出色的蛋白质定位资源可用。其中之一是 [OpenCell](https://oreil.ly/km_v9)，它提供了标准化、高分辨率的细胞内人类蛋白质图像，如图
    [图6-5](#actb-opencell-screenshot) 所示。
- en: Because all images were taken using a single, consistent imaging pipeline, they
    are particularly well suited for machine learning. This consistency ensures that
    the model focuses on learning biological variation in protein localization, rather
    than being distracted by irrelevant differences in imaging conditions or processing
    pipelines.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有图像都是使用单个、一致的成像流程拍摄的，因此它们特别适合机器学习。这种一致性确保模型专注于学习蛋白质定位中的生物变异，而不是被无关的成像条件或处理流程的差异所分散。
- en: Note
  id: totrans-178
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Want to explore the dataset yourself? Head to [OpenCell](https://opencell.sf.czbiohub.org),
    which has a beautifully designed interface that makes browsing the data fast and
    intuitive.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 想要自己探索数据集吗？请访问 [OpenCell](https://opencell.sf.czbiohub.org)，它拥有一个设计精美的界面，使得浏览数据变得快速直观。
- en: 'The model we’ll build in this chapter relies entirely on imaging data; it doesn’t
    use any labels or annotations during training. This is a major strength: it learns
    localization patterns directly from raw microscopy images, without any manual
    supervision. While OpenCell does provide localization annotations, we’ll only
    use them at the end to sanity-check our results. Overreliance on curated labels
    can introduce bias and limit scalability, as manual annotation is both expensive
    and subjective. This is where self-supervised learning shines—it enables models
    to uncover meaningful patterns without needing predefined labels.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将构建的模型完全依赖于成像数据；在训练过程中不使用任何标签或注释。这是一个主要优势：它直接从原始显微镜图像中学习定位模式，而不需要任何人工监督。虽然
    OpenCell 提供了定位注释，但我们只会在最后使用它们来验证我们的结果。过度依赖精心制作的标签可能会引入偏差并限制可扩展性，因为人工标注既昂贵又主观。这正是自监督学习发光的地方——它使模型能够在不需要预定义标签的情况下发现有意义的模式。
- en: '![](assets/dlfb_0605.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0605.png)'
- en: Figure 6-5\. Example entry for the protein ACTB (beta-actin) from the OpenCell
    database. Each protein has a detailed summary page that includes its identifiers
    (e.g., UniProt ID), sequence information, expression levels (how many copies of
    the gene and protein are present in the cell), and subcellular localization annotations.
    On the right, you can see a fluorescence microscopy image showing ACTB localization
    (gray) alongside nuclear staining (blue). These multichannel images, together
    with protein-specific metadata, form the basis of the dataset used in this chapter.
  id: totrans-182
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-5。OpenCell数据库中蛋白质ACTB（β-肌动蛋白）的示例条目。每个蛋白质都有一个详细的摘要页面，包括其标识符（例如，UniProt ID）、序列信息、表达水平（细胞中基因和蛋白质的拷贝数）和亚细胞定位注释。在右侧，你可以看到显示ACTB定位（灰色）和核染色（蓝色）的荧光显微镜图像。这些多通道图像与蛋白质特定的元数据一起，构成了本章所使用的数据集的基础。
- en: Sourcing the Data
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据来源
- en: Preparing images for deep learning can be a challenge in itself, especially
    when it comes to format, consistency, and scale. We saw this previously in [Chapter 5](ch05.html#detecting-skin-cancer-in-medical-images),
    where careful preprocessing was essential. Fortunately, the cytoself authors have
    released a preprocessed version of the dataset they used in their paper, so we
    don’t have to start from scratch here.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 为深度学习准备图像本身可能就是一个挑战，尤其是在格式、一致性和规模方面。我们之前在[第5章](ch05.html#detecting-skin-cancer-in-medical-images)中看到这一点，其中仔细的预处理是必不可少的。幸运的是，cytoself的作者已经发布了他们在论文中使用的数据集的预处理版本，因此我们在这里不必从头开始。
- en: 'The dataset contains 1,311 fluorescently tagged proteins, each imaged in roughly
    18 different fields of view. From each field of view, about 45 individual crops
    are extracted, each typically containing around three cells. This yields roughly
    800 image crops per protein, totaling more than 1,048,800 images in the full dataset.
    Each image includes two channels:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中包含1,311种荧光标记的蛋白质，每种蛋白质在大约18个不同的视野中进行成像。从每个视野中，提取大约45个单独的裁剪图像，每个裁剪图像通常包含大约三个细胞。这为每个蛋白质产生了大约800个图像裁剪，整个数据集总共有超过1,048,800个图像。每个图像包括两个通道：
- en: The green *protein channel*, showing the location of the fluorescently tagged
    protein of interest. This is the only channel used for training in this chapter.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绿色的**蛋白质通道**，显示感兴趣荧光标记蛋白质的位置。这是本章训练中使用的唯一通道。
- en: A *nuclear stain* using Hoechst, a blue fluorescent dye that binds strongly
    to DNA. This highlights the nuclei of the cells and provides spatial context,
    for example, helping to identify whether a protein is nuclear, cytoplasmic, or
    membrane bound.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Hoechst的**核染色**，这是一种强烈结合DNA的蓝色荧光染料。这突出了细胞的核，提供了空间背景，例如，有助于确定蛋白质是否位于核内、细胞质或膜结合。
- en: 'From the nuclear stain, a third representation is computed: the *distance-to-nucleus
    map*. This is not a separate imaging channel, but a spatial map derived from the
    Hoechst signal that gives the model more information about relative positioning:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 从核染色中，计算第三个表示：**到核距离图**。这不是一个单独的成像通道，而是从Hoechst信号派生出的空间图，为模型提供了更多关于相对位置的信息：
- en: Pixels inside the nucleus are assigned positive values, representing the shortest
    distance to the nuclear boundary.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核内的像素被分配正值，代表到核边界的最短距离。
- en: Pixels outside the nucleus are assigned negative values, also based on shortest
    distance to the nuclear edge.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核外的像素被分配负值，同样基于到核边缘的最短距离。
- en: To simplify the task, we will use only the tagged protein channel for model
    training. The other channel (Hoechst) and the derived distance map provide useful
    spatial context but are not required for this self-supervised learning setup.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化任务，我们将仅使用标记的蛋白质通道进行模型训练。其他通道（Hoechst）和派生的距离图提供了有用的空间背景，但对于这个自监督学习设置不是必需的。
- en: 'In addition to the image data, the dataset includes:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 除了图像数据外，数据集还包括：
- en: 'Protein annotations: Such as gene names and unique IDs'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蛋白质注释：例如基因名称和唯一ID
- en: 'Curated localization labels: Manually assigned organelle or compartment categories'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精心定位的标签：手动分配的细胞器或区室类别
- en: These curated labels are *not* used during training. Instead, they’re only used
    afterward to evaluate whether the model has learned biologically meaningful representations
    on its own.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这些精心挑选的标签在训练过程中**不**被使用。相反，它们仅在之后用于评估模型是否已经自行学习了具有生物学意义的表示。
- en: Warning
  id: totrans-196
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: This dataset is significantly larger than those used in previous chapters, introducing
    new challenges in terms of memory usage, data loading speed, and training efficiency.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章中使用的数据集相比，这个数据集显著更大，在内存使用、数据加载速度和训练效率方面引入了新的挑战。
- en: Make sure you have enough disk space and access to a capable GPU. For example,
    use an A100 available through Colab Pro+, or be prepared to scale down the model
    or batch size to make training feasible.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你有足够的磁盘空间，并且能够访问一个强大的GPU。例如，使用Colab Pro+提供的A100，或者准备缩小模型或批量大小以使训练可行。
- en: Getting a Glimpse of the Dataset
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概览数据集
- en: For more complex datasets, it’s common to wrap the data loading and preprocessing
    logic in a custom `Dataset` class, as we have done in earlier chapters in this
    book. This helps make the data easier to explore and use in training.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更复杂的数据集，通常会将数据加载和预处理逻辑封装在一个自定义的 `Dataset` 类中，正如我们在本书的前几章中所做的那样。这有助于使数据更容易探索并在训练中使用。
- en: 'Let’s first use the utility function `get_dataset` to start inspecting a few
    of the image frames in the dataset; see [Figure 6-6](#random-frames-plot). Sampling
    a handful of frames quickly reveals just how much variation there is in patterns
    and intensity across the dataset:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们使用实用函数 `get_dataset` 来检查数据集中的一些图像帧；参见[图6-6](#random-frames-plot)。随机抽取几个帧可以快速揭示数据集中模式和强度变化的多样性：
- en: '[PRE0]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This diversity is expected: each image corresponds to a different tagged protein,
    and many proteins aren’t restricted to a single cellular compartment. Instead,
    they localize to multiple regions of the cell, often with varying abundance. This
    adds another layer of complexity to the learning task.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这种多样性是预期的：每张图像对应不同的标记蛋白质，许多蛋白质并不局限于单个细胞器。相反，它们定位到细胞的不同区域，通常具有不同的丰度。这给学习任务增加了另一层复杂性。
- en: Interestingly, this heterogeneity isn’t just seen across different proteins.
    It also appears *within the same protein*, across different cells. Even when the
    protein and the imaging setup are held constant, localization patterns can vary
    from cell to cell.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，这种异质性不仅存在于不同的蛋白质之间，也存在于同一蛋白质的不同细胞中。即使蛋白质和成像装置保持不变，定位模式也可能从细胞到细胞有所不同。
- en: '![](assets/dlfb_0606.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0606.png)'
- en: Figure 6-6\. Plot of a random subset of frames. The protein symbols and their
    primary localization(s) are given for each frame (number indicates additional
    secondary localizations measured).
  id: totrans-206
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-6\. 随机子集帧的图。每个帧给出了蛋白质符号及其主要定位（数字表示测量的额外次级定位）。
- en: To illustrate this, we’ll load a dataset containing all image frames for a single
    protein, ACTB (beta-actin), and plot a few random samples. In [Figure 6-7](#sele-prot-random-frames-plot),
    you’ll see that even for this single protein, there is substantial variation in
    signal strength, shape, and localization simply due to the nature of biological
    variability—remember that all frames were captured under identical experimental
    conditions.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，我们将加载包含单个蛋白质ACTB（β-肌动蛋白）所有图像帧的数据集，并绘制几个随机样本。在[图6-7](#sele-prot-random-frames-plot)中，你会看到即使是这种单一蛋白质，由于生物变异性的本质，信号强度、形状和定位也存在相当大的变化——记住，所有帧都是在相同的实验条件下捕获的。
- en: '[PRE1]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](assets/dlfb_0607.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0607.png)'
- en: Figure 6-7\. Random subset of 16 image frames showing the localization of the
    protein ACTB (beta-actin), a cytoskeletal protein annotated as localizing to the
    *membrane*, *cytoskeleton*, and *cytoplasm*. Even though these frames all represent
    the same protein under identical experimental conditions, they display considerable
    variation in shape, brightness, and localization pattern, highlighting the inherent
    biological variability across cells.
  id: totrans-210
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-7\. 显示蛋白质ACTB（β-肌动蛋白）定位的16个图像帧的随机子集，该蛋白质被标注为定位于*膜*、*细胞骨架*和*细胞质*。尽管这些帧都代表在相同实验条件下相同的蛋白质，但它们在形状、亮度和定位模式上显示出相当大的变化，突显了细胞间的固有生物变异性。
- en: 'Let’s now take a closer look at just a single frame in [Figure 6-8](#sele-prot-closeup-plot),
    which will give us a better sense of what the model will be processing during
    training. The figure is deliberately shown large so that you can see the individual
    pixels clearly. Try to think of the image not just as a picture, but as a matrix
    of numbers: white pixels have values close to 255, black pixels are near 0, and
    all the shades of gray are values in between:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在更仔细地看看[图6-8](#sele-prot-closeup-plot)中的单个帧，这将让我们更好地了解模型在训练过程中将处理什么。该图故意放大，以便您可以清楚地看到单个像素。试着不要只把图像看作是一幅画，而要把它看作是一个数字矩阵：白色像素的值接近255，黑色像素接近0，所有灰度值介于两者之间：
- en: '[PRE2]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](assets/dlfb_0608.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0608.png)'
- en: 'Figure 6-8\. Close-up view of a single image frame for the protein ACTB. The
    pixelated appearance reflects the raw input that the model will see: a 2D matrix
    of intensity values. Bright regions correspond to high fluorescence signal (values
    near 255), and dark regions correspond to low signal (values near 0). This is
    the actual numerical input used to learn spatial localization patterns. Despite
    the images looking noisy or unclear to the human eye, the model must learn to
    extract consistent patterns from them.'
  id: totrans-214
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-8. 蛋白质ACTB的单个图像帧的特写视图。像素化的外观反映了模型将看到的原始输入：一个强度值的二维矩阵。明亮区域对应于高荧光信号（值接近255），而暗区域对应于低信号（值接近0）。这是用于学习空间定位模式的实际数值输入。尽管图像在人类眼中可能看起来嘈杂或不清晰，但模型必须学会从中提取一致的模式。
- en: Implementing a DatasetBuilder Class
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现DatasetBuilder类
- en: 'Here, we’ve implemented a helper class called `DatasetBuilder`, which handles
    dataset setup and caching behind the scenes:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们实现了一个名为`DatasetBuilder`的辅助类，它在幕后处理数据集设置和缓存：
- en: '[PRE3]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: During initialization, we provide the path to where the data is stored. This
    allows the `DatasetBuilder` to create linked instances of `Images` and `Labels`.
    You can also see that the `DatasetBuilder` has a single public method, `.build`,
    which returns a `dict[str, Dataset]` where every key is a dataset split and the
    value corresponds to a dataset. This method gives you the option to subset the
    dataset to a limited number of randomly selected proteins (`n_proteins`) and/or
    a maximum number of frames (`max_frames`).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化过程中，我们提供数据存储的路径。这允许`DatasetBuilder`创建`Images`和`Labels`的链接实例。您还可以看到`DatasetBuilder`有一个公共方法`.build`，它返回一个`dict[str,
    Dataset]`，其中每个键是数据集分割，值对应于数据集。此方法允许您将数据集子集化到随机选择的有限数量的蛋白质（`n_proteins`）和/或最大帧数（`max_frames`）。
- en: It also splits the dataset into different sets for the learning stage. The sets
    can be provided with the `splits` parameter where the names and fractional sizes
    can be requested. Finally, it is possible to request with the `exclusive_by` parameter
    that across splits, fields of view or proteins are never shared (with `fov_id`
    or `protein_id`, respectively). The former ensures that during learning, we do
    not inadvertently bleed information between splits, as two frames could otherwise
    capture an overlapping zone in a field of view. Not having the same proteins in
    the training and evaluation sets ensures that more general representations are
    learned, as was done in the original paper. Here we use `fov_id`, as we will restrict
    our dataset to a smaller number of proteins for faster training. Implementation-wise,
    we first randomly split the dataset by unique fields of view, and then, as we
    build the splits, we mask frames not to appear across splits. You will also have
    noticed that we are encoding the original protein IDs to consecutive integers,
    which, as you will see later, is required by the loss function during the training
    loop.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 它还将数据集分割成不同的学习阶段集合。可以通过`splits`参数提供集合，其中可以请求名称和分数大小。最后，可以通过`exclusive_by`参数请求在分割之间，视场或蛋白质永远不会共享（分别使用`fov_id`或`protein_id`）。前者确保在学习过程中，我们不会无意中在分割之间泄露信息，因为两个帧可能在其他情况下捕获视场中的重叠区域。在训练和评估集中不使用相同的蛋白质确保学习到更通用的表示，正如原始论文中所做的那样。在这里，我们使用`fov_id`，因为我们将限制数据集中的蛋白质数量以加快训练速度。在实现上，我们首先通过独特的视场随机分割数据集，然后在构建分割时，我们屏蔽帧以避免在分割中出现。您还会注意到，我们正在将原始蛋白质ID编码为连续整数，正如您稍后将会看到的，这是在训练循环中损失函数所必需的。
- en: Building a first dataset instance
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建第一个数据集实例
- en: 'We’ll now look at using the `DatasetBuilder` to create instances of `Dataset`
    splits:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将查看如何使用`DatasetBuilder`创建`Dataset`分割的实例：
- en: '[PRE4]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We now have a dataset. Having a dataset builder provides flexibility that makes
    it easy to explore, debug, and prototype with small slices of the dataset before
    scaling up to the full training set.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了数据集。拥有数据集构建器提供了灵活性，这使得在扩展到完整训练集之前，探索、调试和用数据集的小部分进行原型设计变得容易。
- en: Accessing the dataset internals
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 访问数据集内部
- en: 'Behind the scenes, the `Dataset` class contains two main components: `Images`
    and `Labels`. These handle the raw microscopy image data and the metadata annotations,
    respectively.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，`Dataset` 类包含两个主要组件：`Images` 和 `Labels`。它们分别处理原始显微镜图像数据和元数据注释。
- en: You don’t need to dive into their implementations to follow along in this chapter,
    but if you’re curious, the full source code is available. The lower-level methods
    let you load specific proteins, subsample data, or query for localization annotations—helpful
    tools for deeper biological exploration or interactive experimentation.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 你不需要深入研究它们的实现来跟随本章的内容，但如果你有兴趣，完整的源代码是可用的。底层方法让你可以加载特定的蛋白质，对数据进行子采样，或查询定位注释——这些是进行更深入生物探索或交互式实验的有用工具。
- en: If you read the earlier chapters, this modular data structure should feel familiar.
    We assemble data from different blocks, use memory mapping for large arrays, and
    wrap everything into Python classes for convenience and speed. There are of course
    a lot of improvements that you can make to allow for even faster access to larger
    datasets, and [TensorStore](https://oreil.ly/m29ho) certainly ticks a lot of boxes
    here.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你阅读了前面的章节，这种模块化数据结构应该很熟悉。我们组装来自不同块的数据，使用内存映射处理大型数组，并将所有内容包装成 Python 类以提高便利性和速度。当然，你可以进行许多改进，以允许更快地访问更大的数据集，而
    [TensorStore](https://oreil.ly/m29ho) 在这里确实勾选了很多框。
- en: Now that we’ve examined how the data is structured and loaded, let’s move on
    to training our first model.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经检查了数据的结构和加载方式，让我们继续训练我们的第一个模型。
- en: Building a Prototype Model
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建原型模型
- en: In this section, we’ll build a simplified version of the cytoself model we introduced
    earlier. The aim is to distill the core architectural ideas into a somewhat compressed
    prototype that is easy to understand, train, and modify. The code implementation
    here is adapted in part from the official [Haiku VQ-VAE repository](https://oreil.ly/KF0A0)
    and the [VQ-VAE Flax implementation](https://oreil.ly/z-ZXk) from Arnaud Aillaud.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建我们之前介绍的细胞自我模型的简化版本。目标是提取核心架构思想，形成一个易于理解、训练和修改的相对紧凑的原型。这里的代码实现部分改编自官方
    [Haiku VQ-VAE 仓库](https://oreil.ly/KF0A0) 和 Arnaud Aillaud 的 [VQ-VAE Flax 实现](https://oreil.ly/z-ZXk)。
- en: We intentionally omit some of the more complex features from the original work,
    such as split quantization, hierarchical vector quantization, and multiresolution
    training, in order to keep the code accessible and focused on the key mechanisms.
    This also makes it easier to tinker and explore your own architectural experiments.
    Check out the final section of this chapter for more information on possible extensions
    to the model.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们故意省略了原始工作中的一些更复杂的功能，例如分割量化、分层向量量化和多分辨率训练，以便使代码易于访问并专注于关键机制。这也使得对模型进行修改和探索自己的架构实验变得更加容易。关于模型可能的扩展，请参阅本章的最后一节以获取更多信息。
- en: Defining the LocalizationModel
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义 `LocalizationModel`
- en: 'The following code defines the core model we’ll use throughout this chapter:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码定义了本章我们将使用的核心模型：
- en: '[PRE5]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This model is defined with the main components we already introduced:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是用我们之前介绍的主要组件定义的：
- en: Encoder
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器
- en: Maps the input image into a latent space
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入图像映射到潜在空间
- en: Vector quantizer
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 向量化量化器
- en: Discretizes this latent space using a learned codebook
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 使用学习到的代码簿将这个潜在空间离散化
- en: Decoder
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器
- en: Reconstructs the input image from the quantized representation
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 从量化表示中重建输入图像
- en: These are all wired together in the model’s `setup` method. You’ll also notice
    an additional component, the `ClassificationHead`, which we’ll come back to later.
    For now, just know that it’s used to mitigate the previously mentioned codebook
    collapse, a failure mode where the model uses only a small fraction of the codebook
    entries, reducing the representational power of the latent space.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都在模型的 `setup` 方法中连接在一起。你还会注意到一个额外的组件，即 `ClassificationHead`，我们稍后会回到这个话题。现在，只需知道它用于缓解之前提到的代码簿崩溃问题，这是一种模型只使用代码簿中一小部分条目，从而降低潜在空间表示能力的失败模式。
- en: In the following sections, we’ll walk through each part of the model, starting
    with the `Encoder`, which is responsible for extracting latent features from the
    input microscopy images.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将逐一介绍模型的各个部分，从 `Encoder` 开始，它负责从输入显微镜图像中提取潜在特征。
- en: 'The Encoder: Processing Input Images'
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码器：处理输入图像
- en: The `Encoder` is the first part of our model and is responsible for converting
    raw input images into a continuous latent representation. Its main job is to process
    the input into a form that the `VectorQuantizer` can work with—a spatial feature
    map with rich, expressive features per pixel. This latent representation will
    later be discretized by the `VectorQuantizer`, so it needs to be of the right
    shape and dimensionality.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '`编码器` 是我们模型的第一部分，负责将原始输入图像转换为连续的潜在表示。其主要任务是处理输入，使其成为 `VectorQuantizer` 可以处理的形式——每个像素都有丰富、表达性强的空间特征图。这种潜在表示将随后被
    `VectorQuantizer` 离散化，因此它需要具有正确的形状和维度。'
- en: 'The encoder is built from three convolutional layers followed by two residual
    blocks:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器由三个卷积层和两个残差块组成：
- en: The first two conv layers downsample the image by a factor of 4 overall (each
    has a stride length of 2), reducing spatial resolution while increasing feature
    dimensionality.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前两个卷积层将图像整体下采样为4倍（每个的步长为2），降低空间分辨率同时增加特征维度性。
- en: The third conv layer preserves spatial resolution but deepens the feature map.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个卷积层保持空间分辨率，但加深了特征图。
- en: The two `ResnetBlock`s further refine the features with normalization, nonlinearity,
    and skip connections.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个 `ResnetBlock` 通过归一化、非线性跳过连接进一步细化特征。
- en: 'Together, this forms the pipeline that turns a 100 × 100 grayscale image into
    a smaller grid of `latent_dim`-dimensional feature vectors, ready for quantization:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 一起，这形成了一个管道，将 100 × 100 的灰度图像转换为 `latent_dim` 维度的特征向量的小网格，为量化做好准备：
- en: '[PRE6]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As you can see, the only parameter we are passing into the `Encoder` is `latent_dim`.
    This controls how many channels the network will output; in other words, how expressive
    each spatial location in the latent space is. In the context of our full model,
    this is set to `embedding_dim`, which ensures that the output from the encoder
    matches the dimensionality expected by the `VectorQuantizer`.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们传递给 `Encoder` 的唯一参数是 `latent_dim`。这控制着网络将输出多少通道；换句话说，潜在空间中每个空间位置的表达性如何。在我们的完整模型中，这被设置为
    `embedding_dim`，以确保编码器的输出与 `VectorQuantizer` 预期的维度相匹配。
- en: In the context of a `LocalizationModel` model instantiation, it is set to `self.embedding_dim`,
    since the `Encoder` has to encode raw input images into a form that is compatible
    with the quantization step afterwards; in other words, it needs to have a compatible
    shape. In the context of the VQ-VAE, this is directly related to the quantized
    latent space dimensions, as it will need to return encoded images that are shaped
    correctly for the quantization process.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `LocalizationModel` 模型实例化的上下文中，它被设置为 `self.embedding_dim`，因为 `Encoder` 必须将原始输入图像编码成与量化步骤兼容的形式；换句话说，它需要具有兼容的形状。在
    VQ-VAE 的上下文中，这与量化潜在空间维度直接相关，因为它需要返回正确形状的编码图像，以便进行量化过程。
- en: 'The `ResnetBlock` uses *group normalization* and the *swish* activation function,
    a choice inspired by more recent diffusion models (like *stable diffusion*). As
    a quick reminder, the key idea with residual blocks is that the output is refined
    by residual learning: instead of trying to learn a full transformation of the
    input from scratch, the network learns a correction on top of the input. This
    typically helps with gradient flow and generalization.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '`ResnetBlock` 使用 *分组归一化* 和 *swish* 激活函数，这是一个受到更近期的扩散模型（如 *stable diffusion*）启发的选择。快速提醒一下，残差块的关键思想是输出通过残差学习进行细化：而不是从头开始尝试学习输入的完整变换，网络学习在输入之上进行校正。这通常有助于梯度流和泛化。'
- en: At the end of the encoder, we are left with a spatial feature map with shape
    `[batch_size, height, width, latent_dim]`, which will then be passed into the
    `VectorQuantizer`. This completes the encoding stage of the VQ-VAE architecture.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在编码器结束时，我们得到一个形状为 `[batch_size, height, width, latent_dim]` 的空间特征图，然后将其传递到 `VectorQuantizer`。这完成了
    VQ-VAE 架构的编码阶段。
- en: Note
  id: totrans-256
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You can change up various parts of this encoder—convolution kernels, number
    of layers, activation functions, stride length, and so on. For simplicity, we
    don’t expose them as parameters to `Encoder` here, but these are all hyperparameters
    that you can try tuning to optimize performance.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以更改此编码器的各个部分——卷积核、层数、激活函数、步长等。为了简单起见，我们在这里不将这些作为参数暴露给`Encoder`，但这些都是你可以尝试调整以优化性能的超参数。
- en: 'The VectorQuantizer: Discretizing the Embeddings'
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量量化器：将嵌入离散化
- en: The `VectorQuantizer` is the heart of a VQ-VAE—it’s where we turn continuous
    embeddings into discrete codes. This step forces the model to commit to a limited
    vocabulary of learned feature vectors, improving compression and encouraging meaningful,
    reusable representations.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '`VectorQuantizer`是VQ-VAE的核心——这是我们连续嵌入转换为离散代码的地方。这一步迫使模型承诺于一个有限的学习特征向量词汇表，提高压缩并鼓励有意义的、可重用的表示。'
- en: 'Let’s break down the key components of the `VectorQuantizer` module:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解`VectorQuantizer`模块的关键组件：
- en: '[PRE7]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Following are the key parts of the `VectorQuantizer` class:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是`VectorQuantizer`类的关键部分：
- en: Codebook
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 代码簿
- en: A learnable matrix of shape `(embedding_dim, num_embeddings)`, initialized with
    `lecun_uniform`. Each column is a codebook vector—essentially, a prototype that
    the model can match against. This matrix is what defines the discrete latent space.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 一个形状为`(embedding_dim, num_embeddings)`的可学习矩阵，使用`lecun_uniform`初始化。每一列是一个代码簿向量——本质上，是一个模型可以与之匹配的原型。这个矩阵定义了离散潜在空间。
- en: Quantization
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 量化
- en: The `quantize()` function replaces each encoded input vector with the nearest
    codebook vector, based on Euclidean distance. This forces the model to express
    its understanding of each input frame using a fixed vocabulary of learned visual
    patterns.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '`quantize()`函数根据欧几里得距离将每个编码输入向量替换为最近的代码簿向量。这迫使模型使用固定的学习视觉模式词汇表来表达对每个输入帧的理解。'
- en: Losses
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 损失
- en: '`codebook_loss`'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '`codebook_loss`'
- en: Encourages codebook vectors to move toward the encoder output. This updates
    the codebook.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓励代码簿向量向编码器输出移动。这更新了代码簿。
- en: '`commitment_loss`'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '`commitment_loss`'
- en: Encourages the encoder output to commit to a chosen codebook vector rather than
    fluctuate. This updates the encoder. The two losses are combined to maintain a
    balance between encoder stability and codebook usage.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓励编码器输出承诺于所选的代码簿向量而不是波动。这更新了编码器。两个损失结合以保持编码器稳定性和代码簿使用之间的平衡。
- en: Straight-through estimator (STE)
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 直通估计器（STE）
- en: 'Quantization is nondifferentiable: you can’t backpropagate through a hard lookup
    operation. The STE solves this by copying the quantized vector for the forward
    pass, but passing gradients as if the quantization didn’t happen. It’s a standard
    trick that allows training to proceed via approximate gradients.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是非可微的：你不能通过硬查找操作进行反向传播。STE通过复制用于前向传播的量化向量来解决此问题，但传递梯度时仿佛量化从未发生。这是一个标准的技巧，允许通过近似梯度进行训练。
- en: Perplexity
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 困惑度
- en: A metric that tells us how many codebook vectors the model is using. High perplexity
    (close to `num_embeddings`) means the model is spreading its attention across
    many entries. Low perplexity means collapse—only a few vectors are being used,
    which limits the model’s capacity.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 一个指标，告诉我们模型使用了多少代码簿向量。高困惑度（接近`num_embeddings`）意味着模型在其许多条目之间分散其注意力。低困惑度意味着崩溃——只使用少数几个向量，这限制了模型的能力。
- en: 'You can see that the parameters for this module are `num_embeddings`, `embedding_dim`,
    and `commitment_cost`, which define the structure of the discrete latent space:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，此模块的参数是`num_embeddings`、`embedding_dim`和`commitment_cost`，它们定义了离散潜在空间的结构：
- en: Number of embeddings
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入数量
- en: Determines how many discrete vectors the model has to choose from. A higher
    number allows for more fine-grained and diverse representations.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 确定模型必须从多少个离散向量中选择。更高的数量允许更精细和多样化的表示。
- en: Embedding dimension (also called latent dimension)
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入维度（也称为潜在维度）
- en: Defines the richness of each vector. For example, if it’s 64, each codebook
    entry has 64 values. Higher-dimensional embeddings can capture more nuanced patterns,
    but they require more data and computation to train effectively.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了每个向量的丰富性。例如，如果它是64，则每个代码簿条目有64个值。更高维度的嵌入可以捕捉更细微的模式，但需要更多数据和计算来有效地训练。
- en: Commitment cost
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 承诺成本
- en: A weighting factor used in the loss function to penalize encoder outputs that
    deviate from their selected codebook vectors. If this value is too low, the encoder
    may ignore the codebook. If it’s too high, the encoder may be overly constrained
    and learn less expressive representations.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在损失函数中使用的加权因子，用于惩罚偏离其选定码本向量的编码器输出。如果这个值太低，编码器可能会忽略码本。如果太高，编码器可能会过度约束，学习到的表示能力会降低。
- en: 'Here is the main work the `VectorQuantizer` does during the forward pass, via
    its `__call__` method:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是`VectorQuantizer`在正向传播过程中通过其`__call__`方法执行的主要工作：
- en: '[PRE8]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This function performs four key operations:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数执行四个关键操作：
- en: 'Quantization: It quantizes the input by replacing each encoded vector with
    the nearest entry from the codebook.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 量化：它通过用码本中最接近的条目替换每个编码向量来量化输入。
- en: 'Loss computation: It calculates two loss terms: one to pull codebook entries
    toward encoder outputs (`codebook_loss`) and another to encourage the encoder
    to stay near a codebook entry (`commitment_loss`).'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失计算：它计算两个损失项：一个将码本条目拉向编码器输出（`codebook_loss`），另一个鼓励编码器保持在码本条目附近（`commitment_loss`）。
- en: 'Perplexity: It calculates the codebook perplexity to assess how well the model
    is utilizing the full range of codebook entries.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混淆度：它计算码本混淆度以评估模型如何充分利用码本条目的全部范围。
- en: 'STE: It enables backpropagation through the nondifferentiable quantization
    step.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: STE：它通过非可微量化步骤启用反向传播。
- en: 'Let’s walk through the quantization step in more detail:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地了解一下量化步骤：
- en: '[PRE9]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In `quantize`, the goal is to replace each input vector with its nearest neighbor
    in the codebook. This is done in a few steps:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在`quantize`中，目标是替换每个输入向量为其在码本中的最近邻。这是通过几个步骤完成的：
- en: Flatten the input so that we can process all spatial positions as a list of
    vectors.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 展平输入，以便我们可以将所有空间位置作为向量列表处理。
- en: Compute distances between each input vector and all codebook vectors using Euclidean
    distance.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用欧几里得距离计算每个输入向量与所有码本向量之间的距离。
- en: Find the nearest codebook vector for each input location (`argmin` over distances).
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每个输入位置找到最近的码本向量（在距离上`argmin`）。
- en: Gather the corresponding codebook entries using the indices.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用索引收集相应的码本条目。
- en: Reshape the quantized result back to the original input shape.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将量化结果重塑回原始输入形状。
- en: 'The distances are computed using `calculate_distances`, which implements the
    squared Euclidean distance between a flattened input vector `x` and codebook vector
    `y`. This is based on the identity:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 距离是通过`calculate_distances`计算的，它实现了展平输入向量`x`和码本向量`y`之间的平方欧几里得距离。这是基于以下恒等式：
- en: <mrow><msup><mrow><mo>∥</mo><mi>x</mi><mo>-</mo><mi>y</mi><mo>∥</mo></mrow>
    <mn>2</mn></msup> <mo>=</mo> <msup><mrow><mo>∥</mo><mi>x</mi><mo>∥</mo></mrow>
    <mn>2</mn></msup> <mo>-</mo> <mn>2</mn> <mrow><mo>〈</mo> <mi>x</mi> <mo lspace="0%"
    rspace="0%">,</mo> <mi>y</mi> <mo>〉</mo></mrow> <mo>+</mo> <msup><mrow><mo>∥</mo><mi>y</mi><mo>∥</mo></mrow>
    <mn>2</mn></msup></mrow>
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '`<mrow><msup><mrow><mo>∥</mo><mi>x</mi><mo>-</mo><mi>y</mi><mo>∥</mo></mrow>
    <mn>2</mn></msup> <mo>=</mo> <msup><mrow><mo>∥</mo><mi>x</mi><mo>∥</mo></mrow>
    <mn>2</mn></msup> <mo>-</mo> <mn>2</mn> <mrow><mo>〈</mo> <mi>x</mi> <mo lspace="0%"
    rspace="0%">,</mo> <mi>y</mi> <mo>〉</mo></mrow> <mo>+</mo> <msup><mrow><mo>∥</mo><mi>y</mi><mo>∥</mo></mrow>
    <mn>2</mn></msup></mrow>`'
- en: This formulation efficiently computes the distances using matrix operations.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这种公式通过矩阵运算有效地计算距离。
- en: In summary, during the forward pass, the `VectorQuantizer` finds the best-matching
    codebook vector for each encoded input, replaces it, and enables gradients to
    flow using the STE. The result is a discretized latent representation that is
    both more structured and interpretable.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在正向传播过程中，`VectorQuantizer`为每个编码输入找到最佳匹配的码本向量，将其替换，并使用STE启用梯度流动。结果是离散化的潜在表示，它既更有结构又更可解释。
- en: Calculating VQ-VAE–specific losses
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算VQ-VAE特定的损失
- en: 'With the quantized embeddings in hand, most of the hard work is done. But we
    still need to evaluate how well the original inputs are captured. In other words,
    we need to calculate the *VQ-VAE–specific* losses:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在手头有了量化嵌入之后，大部分艰苦的工作已经完成。但我们仍然需要评估原始输入被捕捉得有多好。换句话说，我们需要计算*VQ-VAE特定的*损失：
- en: '[PRE10]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'There are two components here:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两个组成部分：
- en: Codebook loss
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 码本损失
- en: This term measures how far the quantized vectors are from the original encoder
    outputs. We want this difference to be small. Ideally, the quantized version should
    be nearly identical to what the encoder originally produced. The key detail is
    that `inputs` are wrapped in `lax.stop_gradient`. This prevents gradients from
    flowing back into the encoder so that only the codebook is updated to better match
    the encoder output.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项衡量了量化向量与原始编码器输出之间的距离。我们希望这个差异很小。理想情况下，量化版本应该几乎与编码器最初产生的输出相同。关键细节是 `inputs`
    被包裹在 `lax.stop_gradient` 中。这防止了梯度流向编码器，以便只有代码簿被更新以更好地匹配编码器输出。
- en: Commitment loss
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 承诺损失
- en: This encourages the encoder to produce outputs that are close to some entry
    in the codebook. It helps avoid drifting too far from quantized values.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 这鼓励编码器产生接近代码簿中某个条目的输出。这有助于避免偏离量化值太远。
- en: Here, `quantized` is wrapped in `lax.stop_gradient`. The gradient flows only
    to the encoder and not the codebook, encouraging it to “commit” to one of the
    existing codebook vectors. The `self.commitment_cost` parameter scales this loss
    to control how strongly the encoder is pulled toward existing codebook entries.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`quantized` 被包裹在 `lax.stop_gradient` 中。梯度只流向编码器而不流向代码簿，鼓励它“承诺”使用现有的代码簿向量之一。`self.commitment_cost`
    参数将此损失缩放以控制编码器被拉向现有代码簿条目的强度。
- en: 'These two losses play complementary roles: one pulls the codebook toward the
    encoder outputs and the other pulls the encoder toward the codebook. Together,
    they ensure that both parts of the model co-adapt and stabilize over time, leading
    to a high-quality quantized latent space.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个损失起着互补的作用：一个将代码簿拉向编码器输出，另一个将编码器拉向代码簿。共同作用，确保模型的这两个部分能够共同适应并随时间稳定，从而产生高质量的量化潜在空间。
- en: Using perplexity to measure codebook use
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用困惑度来衡量代码簿的使用
- en: 'After quantization, we want to understand how well the model is using its codebook.
    Is it relying on just a few entries, or is it spreading its attention across many?
    That’s what the *perplexity* metric captures:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 量化之后，我们想了解模型使用其代码簿的效果如何。它是依赖于几个条目，还是将其注意力分散在许多条目上？这正是 *困惑度* 指标所捕捉的：
- en: '[PRE11]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let’s break this down:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解一下：
- en: The `encoding_indices` gives the index of the selected codebook entry for each
    input.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoding_indices` 提供了每个输入所选代码簿条目的索引。'
- en: We one-hot encode these indices to count how often each codebook vector is used.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将这些索引进行 one-hot 编码，以统计每个代码簿向量被使用的频率。
- en: Taking the average of this one-hot matrix gives a frequency distribution over
    the codebook entries.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将这个 one-hot 矩阵的平均值给出代码簿条目的频率分布。
- en: We then compute the entropy of this distribution, and we exponentiate it to
    get the perplexity.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们然后计算这个分布的熵，并将其指数化以获得困惑度。
- en: The result is a number between 1 and `num_embeddings`, indicating how many codebook
    entries are effectively in use.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个介于 1 和 `num_embeddings` 之间的数字，表示有多少代码簿条目实际上在使用。
- en: Using the straight-through estimator
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用直通估计器
- en: 'The final operation in the `__call__` method of the `VectorQuantizer` is the
    computation of the STE:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '`VectorQuantizer` 的 `__call__` 方法中的最终操作是计算 STE：'
- en: '[PRE12]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In a typical neural network, gradients are computed via backpropagation through
    a series of differentiable operations. However, *quantization isn’t differentiable*.
    It involves snapping continuous values to the nearest discrete codebook entry,
    and you can’t take a gradient through that. This poses a challenge for gradient-based
    optimization.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的神经网络中，梯度是通过一系列可微操作的反向传播来计算的。然而，*量化是不可微分的*。它涉及将连续值捕捉到最近的离散代码簿条目，而你不能通过那个步骤来计算梯度。这给基于梯度的优化带来了挑战。
- en: 'To solve this, we use the STE trick. It works like this:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们使用 STE 技巧。它的工作原理如下：
- en: Forward pass
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传递
- en: The output of this expression is quantized, so the decoder receives the discrete
    codebook entries.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表达式的输出是量化的，因此解码器接收离散的代码簿条目。
- en: Backward pass
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传递
- en: Because `quantized - inputs` is wrapped with `stop_gradient`, it *has no effect
    on the gradients*. During backpropagation, only `inputs` contributes to the gradient.
    In other words, the gradient of the STE output with respect to the encoder input
    is treated as if the quantization step never happened.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 `quantized - inputs` 被包裹在 `stop_gradient` 中，它 *对梯度没有影响*。在反向传播过程中，只有 `inputs`
    贡献于梯度。换句话说，STE 输出相对于编码器输入的梯度被视为量化步骤从未发生。
- en: This means that while the model behaves as if quantized during inference and
    reconstruction, the encoder receives useful gradients during training—treating
    quantization as if it were an identity function. Using the STE is essential for
    training models with discrete bottlenecks, like VQ-VAEs. It allows us to maintain
    the representational advantages of a discrete latent space, while still optimizing
    with gradient descent, the foundation of modern deep learning.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着虽然模型在推理和重建时表现得像量化，但在训练过程中，编码器接收有用的梯度——将量化视为一个恒等函数。使用 STE 对于训练具有离散瓶颈的模型（如
    VQ-VAEs）至关重要。它允许我们保持离散潜在空间的表示优势，同时仍然使用梯度下降进行优化，这是现代深度学习的基础。
- en: This covers the core of the VQ-VAE and the most complex aspect of this chapter.
    All that’s left now is the decoder—the final piece that turns the quantized latent
    codes back into a reconstructed image.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 这涵盖了 VQ-VAE 的核心以及本章最复杂的部分。现在剩下的只是解码器——将量化潜在代码转换回重建图像的最后一部分。
- en: 'Decoder: Decoding the Discretized Embeddings Back to Images'
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码器：将离散嵌入解码回图像
- en: The `Decoder` is the final stage of our VQ-VAE model. Conceptually, it mirrors
    the `Encoder`, but instead of compressing the image, it transforms the quantized
    latent representation back into the original image space. Its job is to turn the
    discrete, low-resolution feature map back into a full-resolution grayscale image.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '`Decoder` 是我们 VQ-VAE 模型的最终阶段。从概念上讲，它反映了 `Encoder`，但它不是压缩图像，而是将量化的潜在表示转换回原始图像空间。其任务是将离散、低分辨率的特征图转换回全分辨率灰度图像。'
- en: 'Here’s the code:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是代码：
- en: '[PRE13]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'And here is the structure:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是这个结构的说明：
- en: Two `ResnetBlock`s refine the latent representation.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个 `ResnetBlock` 精炼潜在表示。
- en: Two `Upsample` layers then increase the spatial resolution step-by-step.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接着两个 `Upsample` 层逐步增加空间分辨率。
- en: The final output has shape `[batch_size, height, width, 1]`, a single-channel
    image.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终输出具有形状 `[batch_size, height, width, 1]`，即单通道图像。
- en: 'The `Upsample` module works like this:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '`Upsample` 模块的工作方式如下：'
- en: It uses bilinear interpolation to resize the feature maps to a larger spatial
    size (e.g., doubling width and height).
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用双线性插值将特征图调整到更大的空间尺寸（例如，宽度和高度加倍）。
- en: Then, it applies a 3 × 3 convolution to learn a transformation of the upsampled
    features.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，它应用一个 3 × 3 卷积来学习上采样特征的变化。
- en: 'As a reminder, *bilinear interpolation* resizes images by estimating new pixel
    values based on the four nearest neighbors. It performs linear interpolation twice:
    first along one axis (e.g., left to right) and then along the other (top to bottom).
    For example, when upsampling a 2 × 2 image to 3 × 3, the new center pixel is computed
    as a weighted average of the four corner values, creating smooth transitions.
    This avoids the blocky appearance of nearest-neighbor resizing, which simply assigns
    each new pixel the value of the single closest original pixel, leading to sharp,
    jagged edges.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，**双线性插值**通过估计四个最近邻的新像素值来调整图像大小。它执行两次线性插值：首先沿着一个轴（例如，从左到右）然后沿着另一个轴（从上到下）。例如，当将
    2 × 2 的图像上采样到 3 × 3 时，新的中心像素被计算为四个角值的加权平均值，从而创建平滑的过渡。这避免了最近邻调整大小的块状外观，后者简单地将每个新像素的值分配为单个最接近的原始像素，导致边缘尖锐、锯齿状。
- en: 'ClassificationHead: A Simple but Crucial Module'
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类头：一个简单但至关重要的模块
- en: 'There is one final component in our model that we need to discuss: the `ClassificationHead`.
    This module performs a seemingly simple task—predicting protein IDs—but it turns
    out to be one of the most important parts of the architecture. The original Kobayashi
    paper discovered that this model block, which they called *FcBlock*, was actually
    crucial in guiding the model to learn general protein localization patterns.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型中需要讨论的最后一个组件是 `ClassificationHead`。这个模块执行一个看似简单的任务——预测蛋白质ID，但它实际上是该架构最重要的部分之一。Kobayashi
    的原始论文发现，这个模型块，他们称之为 *FcBlock*，实际上在引导模型学习蛋白质定位的一般模式方面至关重要。
- en: 'In essence, the `ClassificationHead` takes the quantized embeddings and tries
    to classify which protein the input microscopy image contained. This is implemented
    as a small, fully connected network (hence, “Fc”), with one or two dense layers,
    ReLU activations, and dropout:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，`ClassificationHead` 接收量化嵌入并尝试分类输入显微镜图像包含哪种蛋白质。这实现为一个小型全连接网络（因此称为“Fc”），包含一个或两个密集层、ReLU
    激活和 dropout：
- en: '[PRE14]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The parameters include:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 参数包括：
- en: '`num_classes`: The number of protein IDs in the dataset'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_classes`：数据集中蛋白质ID的数量'
- en: '`dropout_rate`: Helps regularize the model and prevent overfitting'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout_rate`: 帮助正则化模型并防止过拟合'
- en: '`layers`: Whether to use a one- or two-layer classifier'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layers`: 是否使用单层或双层分类器'
- en: 'Despite its simplicity, this block had the largest impact on performance in
    the original cytoself paper. It acts as a form of auxiliary task: the model is
    explicitly asked to predict the protein identity from its embedding; a task that
    is possible only if the embeddings encode useful spatial features.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它很简单，但这个块在原始的cytoself论文中对性能的影响最大。它作为一种辅助任务：模型被明确要求从其嵌入中预测蛋白质身份；只有当嵌入编码有用的空间特征时，这个任务才可能完成。
- en: 'This changes the role of the embeddings: instead of only trying to minimize
    reconstruction loss, the model is now encouraged to organize the latent space
    in a way that helps with protein discrimination. This helps prevent codebook collapse
    and leads to much better localization-specific features.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 这改变了嵌入的作用：模型现在不仅试图最小化重建损失，还被鼓励以有助于蛋白质区分的方式组织潜在空间。这有助于防止代码簿崩溃，并导致更好的定位特定特征。
- en: Note
  id: totrans-354
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'Interestingly, this auxiliary protein classification task is hard for the model—and
    that makes intuitive sense. Many proteins are part of the same complex and share
    the same localization, making their image frames visually indistinguishable. But
    that’s the point: by attempting this difficult task, the model is pushed to extract
    subtle, generalizable cues related to localization, even if it doesn’t achieve
    perfect classification.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，这个辅助蛋白质分类任务对模型来说很难——这从直觉上讲是合理的。许多蛋白质是同一复杂体的一部分，并且具有相同的定位，这使得它们的图像帧在视觉上难以区分。但这就是重点：通过尝试这个困难的任务，模型被推动提取与定位相关的微妙、可泛化的线索，即使它没有达到完美的分类。
- en: 'Later in the chapter, you’ll see the difference when this component is removed.
    It also demonstrates a broader lesson: *adding the right auxiliary task can transform
    a model’s ability to learn*. You can control how strongly this auxiliary task
    influences training via the `classification_weight` parameter.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后面部分，您将看到当移除此组件时的差异。这也展示了一个更广泛的教训：*添加正确的辅助任务可以改变模型的学习能力*。您可以通过`classification_weight`参数来控制这个辅助任务对训练的影响程度。
- en: We now have a model. Let’s train it.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个模型。让我们开始训练它。
- en: Setting Up Model Training
  id: totrans-358
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置模型训练
- en: We will now train the `LocalizationModel` model we’ve built. To begin, we’ll
    use a smaller number of image frames to allow for faster iteration and debugging.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将训练我们构建的`LocalizationModel`模型。首先，我们将使用较少的图像帧数量，以便进行更快的迭代和调试。
- en: 'The main training loop is defined in the `train` function. It sets up the training
    state, splits the data into batches, and iterates over the dataset for a number
    of epochs:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 主训练循环在`train`函数中定义。它设置训练状态，将数据分成批次，并在多个epoch上迭代数据集：
- en: '[PRE15]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The first thing to notice is that training proceeds in epochs, which form the
    main loop. As a reminder, an *epoch* is a full pass through the entire training
    set; every training example is seen once. Before we enter the loop for the first
    time, we initialize the training state so that we have a starting point. Then,
    the training begins with the first epoch.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要注意的是，训练是在epoch中进行的，这些epoch形成了主循环。提醒一下，一个*epoch*是对整个训练集的一次完整遍历；每个训练示例只被看到一次。在我们第一次进入循环之前，我们初始化训练状态，以便有一个起点。然后，训练从第一个epoch开始。
- en: 'Before diving into the training logic, let’s briefly look at how the dataset
    is fed into the model. A key part of this is the `Dataset.get_batches` method,
    which handles how image examples are served during training:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入训练逻辑之前，让我们简要地看看数据集是如何输入到模型中的。这其中的关键部分是`Dataset.get_batches`方法，它处理了在训练期间如何提供图像示例：
- en: '[PRE16]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: You can see that we first select either the *training* or the *test* set of
    image frames, split them into batches of a preset size, and shuffle their indices.
    The protein labels for each frame are then encoded as integers so that they can
    be used with the `optax.softmax_cross_entropy_with_integer_labels` loss function.
    Finally, each batch yields the image data, the integer-encoded protein labels,
    and the corresponding frame IDs (which can be useful for analysis or visualization).
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到我们首先选择图像帧的*训练集*或*测试集*，将它们分成预设大小的批次，并打乱它们的索引。然后，每个帧的蛋白质标签被编码为整数，以便可以使用`optax.softmax_cross_entropy_with_integer_labels`损失函数。最后，每个批次产生图像数据、整数编码的蛋白质标签和相应的帧ID（这可能对分析或可视化很有用）。
- en: 'Once the data is batched, it’s passed into two key functions: `train_step`
    for updating the model and `eval_step` for monitoring performance. Let’s take
    a closer look at `train_step`:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据被批处理，它就会被传递到两个关键函数中：`train_step`用于更新模型，`eval_step`用于监控性能。让我们更详细地看看`train_step`：
- en: '[PRE17]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Within the `train_step` function, the inner `calculate_loss` defines how the
    model’s loss is computed. This function is the core of the training step. It determines
    how well the model is performing and guides the weight updates to minimize the
    loss.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 在`train_step`函数内部，内部的`calculate_loss`定义了模型损失的计算方式。这个函数是训练步骤的核心。它决定了模型的性能如何，并指导权重更新以最小化损失。
- en: First, we call `state.apply_fn`, which runs the `__call__` method of the `LocalizationModel`
    model. This returns the reconstruction (`x_recon`), the codebook-related losses
    (`codebook_loss` and `commitment_loss`), the classification logits, and the `perplexity`
    of the quantization.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们调用`state.apply_fn`，它运行`LocalizationModel`模型的`__call__`方法。这返回了重构（`x_recon`）、与代码簿相关的损失（`codebook_loss`和`commitment_loss`）、分类的对数几率以及量化的`perplexity`。
- en: 'We then compute two additional losses:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们计算两个额外的损失：
- en: '`recon_loss`: How different the reconstructed image is from the original, using
    squared error'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`recon_loss`: 重构图像与原始图像之间的差异，使用平方误差'
- en: '`classification_loss`: How well the model predicts the protein ID from the
    image embedding, using cross-entropy'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`classification_loss`：模型使用交叉熵从图像嵌入中预测蛋白质ID的有效性'
- en: These losses are assembled into `loss_components`, and then they are combined
    into a `total_loss` that drives training. Notably, the classification loss is
    multiplied by a `classification_weight`, allowing us to control how much it contributes
    to learning. Setting it to zero *ablates* (removes) the classification task, something
    we will test later in the chapter.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 这些损失被组装成`loss_components`，然后它们被组合成一个`total_loss`，驱动训练。值得注意的是，分类损失乘以一个`classification_weight`，允许我们控制它对学习的贡献程度。将其设置为零*消除*（移除）分类任务，我们将在本章后面进行测试。
- en: 'Finally, we calculate evaluation metrics to track how training is progressing.
    These include:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们计算评估指标以跟踪训练进度。这些包括：
- en: '`perplexity`: How effectively the model is using the codebook'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`perplexity`：模型使用代码簿的有效性'
- en: '`accuracy`: How effectively the model is predicting the protein IDs'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`accuracy`：模型预测蛋白质ID的有效性'
- en: All of this is used to compute gradients via `jax.value_and_grad` and then update
    the model with `state.apply_gradients`. This design cleanly separates different
    objectives (reconstruction, quantization, classification) and lets you experiment
    with different trade-offs by adjusting loss weights.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都被用来通过`jax.value_and_grad`计算梯度，然后使用`state.apply_gradients`更新模型。这种设计清晰地分离了不同的目标（重构、量化、分类），并允许你通过调整损失权重来实验不同的权衡。
- en: After each epoch, we store the metrics collected across batches. These include
    reconstruction loss, codebook and commitment losses, classification accuracy,
    and perplexity. The metrics are averaged across batches to give a summary per
    epoch, allowing us to monitor model progress and convergence.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个epoch之后，我们存储跨批次的收集的指标。这些包括重构损失、代码簿和承诺损失、分类准确率和`perplexity`。这些指标在批次之间平均，以给出每个epoch的摘要，使我们能够监控模型进度和收敛。
- en: 'The `eval_step` is essentially the same as the `train_step`, with one key difference:
    it does not update the model weights. Instead, it runs the model in inference
    mode and is used to assess how well the current model performs on a held-out test
    set. This gives us an unbiased signal of generalization performance.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '`eval_step`基本上与`train_step`相同，但有一个关键的区别：它不会更新模型权重。相反，它以推理模式运行模型，用于评估当前模型在保留的测试集上的表现如何。这为我们提供了关于泛化性能的无偏信号。'
- en: We’ve now covered the model, dataset, and training logic. It’s finally time
    to give it a spin and see what it can learn.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经涵盖了模型、数据集和训练逻辑。现在是时候给它一个尝试，看看它能学到什么。
- en: Training with a Small Image Set
  id: totrans-381
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用小图像集进行训练
- en: 'Let’s see the model in action. We’ll start by training it on a small subset
    of the data: 50 proteins, split into 80% training data, 10% validation data, and
    10% test data, using a fixed random seed for reproducibility.'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看模型的实际效果。我们将从在数据的一个小子集上训练它开始：50个蛋白质，分为80%的训练数据、10%的验证数据和10%的测试数据，使用固定的随机种子以确保可重复性。
- en: 'We define our model architecture by setting the `embedding_dim`, `num_embeddings`,
    `commitment_cost`, `dropout_rate`, and `classification_head_layers`. Then, we
    specify the training parameters: number of `epochs`, `batch_size`, `learning_rate`,
    and `classification_weight`.'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过设置`embedding_dim`、`num_embeddings`、`commitment_cost`、`dropout_rate`和`classification_head_layers`来定义我们的模型架构。然后，我们指定训练参数：`epochs`数量、`batch_size`、`learning_rate`和`classification_weight`。
- en: '[PRE18]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now we can start training:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始训练了：
- en: '[PRE19]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: After training, we now have a `LocalizationModel` model that has learned to
    compress, quantize, and reconstruct protein localization patterns, while also
    performing auxiliary classification. But how well has it actually learned? Let’s
    find out. We’ll start by visually inspecting its reconstructions.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们现在有一个`LocalizationModel`模型，它已经学会了压缩、量化和重建蛋白质定位模式，同时执行辅助分类。但它实际上学得怎么样呢？让我们找出答案。我们将从检查其重建开始。
- en: Inspecting Image Reconstruction
  id: totrans-388
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查图像重建
- en: 'Before diving deeper, let’s do a quick sanity check: can the `LocalizationModel`
    model we just trained reconstruct the input images at all? If it has learned any
    meaningful representation of the input data, its reconstructions should at least
    vaguely resemble the original frames. We will evaluate this on the validation
    set of the dataset, that is, on frames that were never seen during training.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨之前，让我们先进行一个快速的合理性检查：我们刚刚训练的`LocalizationModel`模型能否完全重建输入图像？如果它已经学会了输入数据的任何有意义的表示，其重建应该至少大致类似于原始帧。我们将在数据集的验证集上评估这一点，即那些在训练过程中从未见过的帧。
- en: Indeed, the model captures some of the structural features present in the inputs
    (see [Figure 6-9](#recon-plot)). The reconstructions are far from perfect—blurry
    and low resolution—but that’s OK. Remember, our goal isn’t to generate photorealistic
    images, it’s to learn discrete representations that encode spatial localization
    patterns. Reconstruction is just a training objective to help guide that process.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 的确，该模型捕捉到了输入中存在的一些结构特征（参见[图6-9](#recon-plot)）。重建远非完美——模糊且分辨率低，但这是可以接受的。记住，我们的目标不是生成逼真的图像，而是学习编码空间定位模式的离散表示。重建只是帮助指导这一过程的训练目标。
- en: '[PRE20]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](assets/dlfb_0609.png)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0609.png)'
- en: Figure 6-9\. Reconstructed images of random proteins from the small-scale model.
    Each pair shows an input image (left) and its reconstruction (right). While blurry,
    the reconstructions often capture core structural features—a sign that the model
    is learning to encode localization-relevant information. Protein is indicated
    per panel (number indicates additional secondary localizations measured).
  id: totrans-393
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-9\. 来自小规模模型的随机蛋白质的重建图像。每一对显示一个输入图像（左）及其重建（右）。虽然模糊，但重建通常捕捉到核心结构特征——这是模型正在学习编码与定位相关的信息的迹象。蛋白质在每个面板中指示（数字表示测量的额外二级定位）。
- en: Examining Evaluation Metrics Over Epochs
  id: totrans-394
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查随时间变化的评估指标
- en: Let’s now look more closely at how training progressed by examining the loss
    curves (see [Figure 6-10](#losses-plot)). The left panel shows the four individual
    loss components used during training, while the right panel displays total training
    and test loss across epochs.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更仔细地观察训练的进展情况，通过检查损失曲线（参见[图6-10](#losses-plot)）。左面板显示了训练期间使用的四个单个损失组件，而右面板显示了随时间变化的总训练和测试损失。
- en: '[PRE21]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](assets/dlfb_0610.png)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0610.png)'
- en: Figure 6-10\. Training dynamics represented by individual loss components on
    the training set (left) and total training versus validation loss over epochs
    (right). All loss components decrease over time. While training loss continues
    to improve steadily, validation loss plateaus early, suggesting the model generalizes
    reasonably well but gains from further training may be limited.
  id: totrans-398
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-10\. 由训练集上的单个损失组件表示的训练动态（左）以及随时间变化的总训练与验证损失（右）。所有损失组件随时间下降。虽然训练损失持续稳步改善，但验证损失在早期就趋于平稳，表明模型泛化能力相当好，但进一步训练的收益可能有限。
- en: 'As expected, all loss components steadily decrease over time—especially the
    classification loss, which dominates the total due to its larger magnitude. This
    makes sense: distinguishing between 50 proteins based solely on their localization
    patterns is a challenging task. Remember that we can always adjust the classification
    loss’s relative weight using the `classification_weight` parameter later.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，所有损失组件随着时间的推移稳步下降——尤其是分类损失，由于其较大的幅度，它主导了总损失。这是有道理的：仅基于蛋白质的定位模式来区分50种蛋白质是一项具有挑战性的任务。记住，我们总是可以使用`classification_weight`参数来调整分类损失的相对权重。
- en: Meanwhile, validation loss closely tracks training loss throughout, with only
    a slight and stable gap between them. There’s no clear sign of overfitting, which
    is encouraging given the small dataset.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，验证损失在整个过程中紧密跟踪训练损失，两者之间只有微小的稳定差距。没有过度拟合的明显迹象，考虑到数据集很小，这是令人鼓舞的。
- en: 'Next, let’s inspect how the codebook is being used—by looking at the evolution
    of its *perplexity* over time. In VQ-VAEs, perplexity measures how many codebook
    entries are effectively being used during vector quantization. If the model relies
    on just a handful of embeddings (e.g., 10 out of 512), the perplexity will be
    low. If it spreads usage more evenly across many entries, the perplexity rises,
    approaching the total number of codebook vectors available. In [Figure 6-11](#plot-perplexity)
    we see the evolution of perplexity:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们检查代码簿是如何被使用的——通过查看其*困惑度*随时间的变化。在VQ-VAEs中，困惑度衡量在向量量化过程中有效使用多少代码簿条目。如果模型仅依赖于少数嵌入（例如，512个中的10个），则困惑度会很低。如果它在许多条目之间更均匀地分配使用，则困惑度上升，接近可用的代码簿向量的总数。在[图6-11](#plot-perplexity)中，我们看到困惑度的演变：
- en: '[PRE22]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](assets/dlfb_0611.png)'
  id: totrans-403
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0611.png)'
- en: Figure 6-11\. Perplexity increases over epochs on both the training and validation
    sets. A rising perplexity indicates that the model is using more of the available
    codebook entries, rather than collapsing onto a small subset. The close alignment
    between training and validation perplexity suggests that this richer, more diverse
    representation generalizes well beyond the training data.
  id: totrans-404
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-11。在训练集和验证集上，困惑度随epoch的推移而增加。上升的困惑度表明模型正在使用更多的可用代码簿条目，而不是缩小到一个小子集。训练和验证困惑度之间的紧密一致性表明，这种更丰富、更多样化的表示在训练数据之外很好地泛化。
- en: As you can see in [Figure 6-11](#plot-perplexity), perplexity steadily increases
    across epochs for both the training and validation sets. This is a strong signal
    that the model is learning to encode diverse spatial patterns using a broad vocabulary
    of codebook entries. Instead of relying on a narrow set of common features, it’s
    finding more nuanced ways to represent the variation in protein localization—effectively
    condensing complex microscopy images into compact, expressive codes.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在[图6-11](#plot-perplexity)中可以看到，训练集和验证集的困惑度在各个epoch中稳步上升。这是一个强烈的信号，表明模型正在学习使用广泛的代码簿条目词汇来编码多样化的空间模式。它不是依赖于一组常见的特征，而是在表示蛋白质定位的变异性方面找到了更细腻的方法——有效地将复杂的显微镜图像浓缩成紧凑、富有表现力的代码。
- en: Training a Model Without a Classification Task
  id: totrans-406
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无分类任务的模型训练
- en: Next, we’ll train the exact same model on the same data, but this time with
    the classification weight set to 0\. This simulates removing the auxiliary protein
    ID classification task, in other words, disabling the `ClassificationHead`. Note
    that we are using the same random seed to provide the closest possible comparison.
    This lets us see how much that component helps guide the model’s learning.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在相同的数据上训练完全相同的模型，但这次将分类权重设置为0。这模拟了移除辅助蛋白质ID分类任务，换句话说，禁用了`ClassificationHead`。请注意，我们使用相同的随机种子来提供尽可能接近的比较。这让我们可以看到该组件在引导模型学习方面有多大帮助。
- en: Tip
  id: totrans-408
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: As a general design principle, it’s helpful to structure your code so that model
    components can be ablated (i.e., effectively removed) by setting their weight
    to zero via a config or flag—rather than rewriting or commenting out parts of
    the architecture. This makes it much easier to test hypotheses, compare model
    variants, and run controlled experiments. It’s a simple practice that promotes
    modular, reproducible research.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一项一般的设计原则，将您的代码结构化以便通过设置配置或标志将模型组件的权重设置为零（即有效地移除它们）是有帮助的——而不是重写或注释掉架构的部分。这使得测试假设、比较模型变体和运行受控实验变得容易得多。这是一种简单的实践，有助于促进模块化和可重复的研究。
- en: 'Let’s give it a go:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试一试：
- en: '[PRE23]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This time around, the model performs much worse, and its perplexity collapses
    (down to ~30, previously ~350), as shown in [Figure 6-12](#alt-plot-perplexity).
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，模型的性能大幅下降，其困惑度崩溃（下降到约30，之前约为350），如[图6-12](#alt-plot-perplexity)所示。
- en: '[PRE24]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](assets/dlfb_0612.png)'
  id: totrans-414
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0612.png)'
- en: Figure 6-12\. Perplexity over epochs collapses without the `ClassificationHead`
    in the model architecture, suggesting that the auxiliary protein identification
    task plays a critical role in encouraging a diverse and informative representation.
  id: totrans-415
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-12。在模型架构中没有`ClassificationHead`的情况下，困惑度随epoch的推移而崩溃，这表明辅助蛋白质识别任务在鼓励多样化且信息丰富的表示中起着关键作用。
- en: Perhaps counterintuitively, this model’s reconstructed microscopy images actually
    look a bit *better* in [Figure 6-13](#alt-recon-plot).
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 可能出人意料的是，这个模型重建的显微镜图像实际上在[图6-13](#alt-recon-plot)中看起来有点*更好*。
- en: '[PRE25]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: At first glance, this might seem like an improvement, but it actually reveals
    a critical trade-off in the model’s objectives.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，这似乎是一个改进，但实际上它揭示了模型目标中的一个关键权衡。
- en: 'Without the auxiliary classification task (i.e., without the `ClassificationHead`),
    the model can focus more of its efforts on minimizing the reconstruction loss.
    This encourages it to memorize the input data as precisely as possible, often
    by collapsing to a small number of frequently used codebook entries. That’s why
    the reconstructions look sharper: the model has overfit to pixel-level detail
    rather than learning generalizable representations.'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 没有辅助分类任务（即，没有`ClassificationHead`），模型可以更多地将其精力集中在最小化重建损失上。这鼓励它尽可能精确地记住输入数据，通常是通过减少到少量频繁使用的代码簿条目。这就是为什么重建看起来更清晰：模型过度拟合到像素级细节而不是学习可泛化的表示。
- en: But VQ-VAEs aren’t just about pretty reconstructions. They’re about learning
    discrete, structured representations of the input. With the classification task
    enabled, the model is forced to organize its internal representations in a way
    that’s useful for predicting protein identity. This encourages it to capture high-level
    biological features, such as localization patterns, at the cost of slightly blurrier
    reconstructions.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 但VQ-VAEs不仅仅是关于漂亮的重建。它们是关于学习输入的离散、结构化表示。当启用分类任务时，模型被迫以对预测蛋白质身份有用的方式组织其内部表示。这鼓励它捕捉高级生物特征，如定位模式，但以略微模糊的重建为代价。
- en: 'This same observation was highlighted in the original cytoself paper: adding
    classification-like objectives improves the quality of the latent space by pushing
    the model to encode meaningful, discriminative features. In fact, cytoself showed
    that models trained only to reconstruct images were far less effective at clustering
    localizations or identifying complexes, even when their reconstructions looked
    visually fine.'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的cytoself论文中也强调了同样的观察：添加分类似的目标通过推动模型编码有意义的、可区分的特征来提高潜在空间的质量。事实上，cytoself表明，仅训练用于重建图像的模型在聚类定位或识别复合体方面远不如有效，即使它们的重建在视觉上看起来很好。
- en: '![](assets/dlfb_0613.png)'
  id: totrans-422
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0613.png)'
- en: Figure 6-13\. Reconstructed images appear slightly more faithful to the input
    when the `ClassificationHead` is removed—likely because the model can focus its
    efforts on the reconstruction task, resulting in less blurring.
  id: totrans-423
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-13. 当移除`ClassificationHead`时，重建的图像看起来更忠实于输入——这可能是由于模型可以将精力集中在重建任务上，从而减少模糊。
- en: Understanding the Model
  id: totrans-424
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解模型
- en: 'What has the model actually learned about spatial organization within cells?
    To answer this question, we need to take a closer look at the model’s *latent
    space*: the internal representation it builds from the input images. In particular,
    since our model uses a codebook, we can analyze *which* entries get used and *how
    often* for each protein. This gives us a kind of *feature spectrum* or a summary
    of how each protein maps onto the learned visual vocabulary.'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 模型实际上关于细胞内空间组织学到了什么？为了回答这个问题，我们需要更仔细地观察模型的*潜在空间*：它是从输入图像中构建的内部表示。特别是，由于我们的模型使用了一个代码簿，我们可以分析*哪些*条目被使用以及每个蛋白质*如何频繁地使用*。这为我们提供了一种*特征谱*或每个蛋白质如何映射到学习到的视觉词汇的总结。
- en: Understanding Localization Clustering
  id: totrans-426
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解定位聚类
- en: A natural question is whether the model can distinguish between different subcellular
    compartments. One way to test this is to apply the uniform manifold approximation
    and projection (UMAP) dimensionality reduction technique to the learned embeddings.
    UMAP projects high-dimensional data into two dimensions while preserving local
    structure, making it easier to visualize complex relationships.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 一个自然的问题是模型是否可以区分不同的亚细胞区室。测试这一点的 一种方法是将均匀流形近似和投影（UMAP）降维技术应用于学习到的嵌入。UMAP将高维数据投影到二维，同时保留局部结构，这使得可视化复杂关系更容易。
- en: Tip
  id: totrans-428
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'There are other dimensionality reduction techniques, such as *t-SNE* and *PCA*,
    but *UMAP* is especially well suited for this task. Unlike PCA, it can capture
    complex nonlinear relationships. UMAP is similar to t-SNE in many ways, but with
    a key advantage: it tends to preserve both local *and* global structure more effectively.
    This makes it especially useful for visualizing patterns across diverse datasets
    like this one.'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他降维技术，如*t-SNE*和*PCA*，但*UMAP*特别适合这项任务。与PCA不同，它能够捕捉复杂的非线性关系。UMAP在许多方面与t-SNE相似，但有一个关键优势：它更有效地保留了局部和全局结构。这使得它在可视化像这样多样化的数据集的图案时特别有用。
- en: UMAP has become a go-to tool in fields like proteomics and genomics—not just
    for its performance, but also because it works well out of the box, with minimal
    hyperparameter tuning.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: UMAP已成为蛋白质组学和基因组学等领域的一个首选工具——不仅因为其性能，还因为它开箱即用，超参数调整最小。
- en: If the model has learned meaningful spatial features, we’d expect image frames
    from similar cellular compartments—like the mitochondria, nucleus, or ER—to cluster
    together in the UMAP space. To visualize this, we need to extract the model’s
    internal representation of each image.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型已经学习到了有意义的空间特征，我们预计来自相似细胞区室的图像帧——如线粒体、细胞核或内质网——将在UMAP空间中聚集在一起。为了可视化这一点，我们需要提取模型对每张图像的内部表示。
- en: 'The function `get_frame_encoding_index_histogram` does this by calculating
    a codebook usage histogram for each frame:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`get_frame_encoding_index_histogram`通过为每个帧计算代码簿使用直方图来完成这项工作：
- en: '[PRE26]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'For each batch of images, we use the model to obtain the encoding indices:
    the discrete codebook entries selected for each spatial patch in each frame. These
    are returned by the `pluck_encodings` helper function. Each frame’s output is
    then flattened and passed to `np.histogram`, which counts how often each codebook
    entry is used.'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个图像批次，我们使用模型来获取编码索引：每个帧中每个空间块选择的离散代码簿条目。这些由`pluck_encodings`辅助函数返回。然后，每个帧的输出被展平并传递给`np.histogram`，该函数计算每个代码簿条目被使用的频率。
- en: The result is a histogram vector per frame—one value per codebook entry—describing
    how frequently that entry was activated. These vectors can be thought of as discrete
    fingerprints of each image, which we can then reduce to 2D with UMAP for visualization.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 每一帧的结果是一个直方图向量——每个代码簿条目一个值——描述该条目被激活的频率。这些向量可以被视为每张图像的离散指纹，然后我们可以通过UMAP将其降低到二维以进行可视化。
- en: Note
  id: totrans-436
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Why proceed in batches? Batching during the embedding extraction—just like during
    training—helps manage memory and computation. It ensures that we don’t load the
    entire dataset into memory at once. Note that the batch size used here doesn’t
    have to match the training batch size.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要分批进行？在嵌入提取过程中分批——就像在训练过程中一样——有助于管理内存和计算。这确保了我们不会一次性将整个数据集加载到内存中。请注意，这里使用的批大小不必与训练批大小匹配。
- en: 'We’ll generate projections for two versions of our model: one with the `ClassificationHead`
    enabled and one without. By plotting both side-by-side, we can observe how the
    presence of the `ClassificationHead` affects the structure of the learned embedding
    space. We continue using the validation dataset to evaluate our model. The resulting
    visualization is shown in [Figure 6-14](#projections-plot):'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为我们的模型生成两个版本的投影：一个启用了`ClassificationHead`，另一个没有。通过并排绘制这两个版本，我们可以观察`ClassificationHead`的存在如何影响学习嵌入空间的结构。我们继续使用验证数据集来评估我们的模型。结果可视化显示在[图6-14](#projections-plot)：
- en: '[PRE27]'
  id: totrans-439
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![](assets/dlfb_0614.png)'
  id: totrans-440
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0614.png)'
- en: Figure 6-14\. UMAP projections of the model with (left) and without (right)
    the `ClassificationHead`. The model on the left learns clearer, more meaningful
    structure—with frames from the same subcellular compartment (e.g., vesicles, chromatin,
    nucleolus) forming tighter clusters. Without the `ClassificationHead`, the structure
    is less distinct, and compartments are harder to separate. Around 1% of frames
    with a single predominant localization are highlighted with larger markers to
    intuitively annotate clusters.
  id: totrans-441
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-14。带有（左）和没有（右）`ClassificationHead`的模型的UMAP投影。左侧的模型学习到更清晰、更有意义的结构——来自相同亚细胞区室的帧（例如，囊泡、染色质、核仁）形成更紧密的簇。没有`ClassificationHead`，结构不太明显，区室更难分离。大约1%的具有单一主要定位的帧用较大的标记突出显示，以直观地注释簇。
- en: You can see that, once the embeddings are labeled with ground-truth localization
    categories, the model has clearly learned to distinguish different subcellular
    compartments based on raw pixel patterns. In the UMAP plot on the left (with the
    `ClassificationHead`), compartments like chromatin, mitochondria, and nucleolus
    form fairly tight, distinct clusters—indicating that the model has learned consistent
    visual features for these structures. Vesicles sometimes form a separate group
    but often show overlap with ER and cytoplasmic frames, likely reflecting their
    more diffuse and variable appearance in the raw images.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，一旦嵌入被标记为地面真实定位类别，模型已经清楚地学会了根据原始像素模式区分不同的亚细胞室。在左边的UMAP图（带有`ClassificationHead`）中，染色质、线粒体和核仁等室形成相当紧密、独特的簇——表明模型已经学会了这些结构的持续视觉特征。囊泡有时形成一个单独的组，但通常与ER和细胞质框架重叠，这很可能反映了它们在原始图像中更分散和多变的外观。
- en: Removing the `ClassificationHead` has a clear visual impact. In the righthand
    UMAP, the model still produces structured embeddings, but the clusters are less
    distinct. Localizations such as cytoplasm, ER, and vesicles are more intermixed,
    suggesting that without the auxiliary classification task, the model learns a
    weaker or more entangled representation of subcellular identity. This comparison
    highlights how architectural choices—even auxiliary objectives—can meaningfully
    shape the structure of learned representations.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 移除`ClassificationHead`有明显的视觉影响。在右边的UMAP中，模型仍然产生结构化的嵌入，但簇的区分度较低。细胞质、ER和囊泡等定位更加混合，这表明在没有辅助分类任务的情况下，模型学习到的亚细胞身份表示较弱或更纠缠。这种比较突出了架构选择——甚至辅助目标——如何有意义地塑造学习到的表示结构。
- en: Tip
  id: totrans-444
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Since we want to compare how the presence or absence of the `ClassificationHead`
    affects the learned embeddings, it’s important that both UMAP visualizations are
    directly comparable. By default, techniques like UMAP involve some randomness
    in initialization and optimization, which can lead to inconsistent visual layouts
    across runs. To address this, we use an *aligned UMAP*, a variant of UMAP that
    tracks shared data points across embeddings to ensure consistent alignment and
    make visual comparisons meaningful.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们想要比较`ClassificationHead`的存在或缺失如何影响学习到的嵌入，因此确保两个UMAP可视化直接可比较非常重要。默认情况下，像UMAP这样的技术涉及初始化和优化中的某些随机性，这可能导致运行之间的视觉布局不一致。为了解决这个问题，我们使用了一种*对齐UMAP*，这是UMAP的一种变体，它跟踪嵌入中的共享数据点以确保一致的校准并使视觉比较有意义。
- en: While these UMAP plots are useful for qualitative interpretation, it’s also
    possible to quantify clustering quality more formally; for example, by comparing
    the average distances between embeddings from the *same* localization versus those
    from *different* localizations. If the model has learned a meaningful embedding
    space, we’d expect intra-class distances (within a localization category) to be
    small, and inter-class distances to be larger. This basic intuition underpins
    several clustering quality metrics and was used by Kobayashi et al. to evaluate
    their model quantitatively. In this chapter, we’ll focus on visual exploration,
    but you can always extend the code to include such metrics if you would like to.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些UMAP图对于定性解释很有用，但也可以更正式地量化聚类质量；例如，通过比较来自*相同*定位的嵌入之间的平均距离与来自*不同*定位的嵌入之间的平均距离。如果模型已经学习到一个有意义的嵌入空间，我们预计类内距离（在定位类别内）会很小，而类间距离会更大。这种基本直觉是几个聚类质量指标的基础，并被Kobayashi等人用于定量评估他们的模型。在本章中，我们将专注于视觉探索，但如果你愿意，你总是可以扩展代码以包括此类指标。
- en: Inspecting Feature Spectrums
  id: totrans-447
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查特征谱
- en: To better understand what our model has learned, we can analyze how it uses
    its codebook across different proteins. Remember, each image is encoded using
    entries from a learned set of code vectors (the codebook). Over time, some entries
    may become specialized, for instance, consistently firing for mitochondrial proteins,
    or for nuclear-localized ones.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解我们的模型学到了什么，我们可以分析它在不同的蛋白质上如何使用其代码簿。记住，每个图像都是使用从学习到的代码向量集（代码簿）中的条目进行编码的。随着时间的推移，某些条目可能变得专业化，例如，对于线粒体蛋白或核定位蛋白的一致激活。
- en: 'A *feature spectrum* is a simple but powerful way to summarize this behavior.
    For each protein, we:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '*特征谱* 是一种简单但强大的方法来总结这种行为。对于每种蛋白质，我们：'
- en: Collect all its image frames.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集所有其图像帧。
- en: Count how often each codebook entry is used across those frames.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算每个代码簿条目在这些帧中使用的频率。
- en: Aggregate these counts into a histogram. This is the protein’s feature spectrum.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将这些计数汇总成直方图。这是蛋白质的特征光谱。
- en: Each feature spectrum essentially acts like a “fingerprint” of how the model
    encodes that protein’s spatial patterns.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 每个特征光谱本质上就像一个“指纹”，表示模型如何编码该蛋白质的空间模式。
- en: To explore how specialized the codebook has become, we compare these spectra
    across proteins. If certain codebook entries are consistently used for similar
    proteins (e.g., those localized to the ER), we expect those spectra to be correlated.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探索代码簿变得多么专业化，我们比较了这些蛋白质之间的光谱。如果某些代码簿条目在类似蛋白质（例如，那些定位于内质网上的蛋白质）中持续使用，我们预计这些光谱将相关。
- en: 'The following function computes a Pearson correlation matrix across all protein
    spectra and then applies hierarchical clustering to group similar patterns. This
    reveals which sets of codebook entries (features) tend to be co-used and may correspond
    to broader localization categories:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数计算所有蛋白质光谱之间的皮尔逊相关矩阵，然后应用层次聚类以分组相似模式。这揭示了哪些代码簿条目集（特征）倾向于一起使用，可能对应于更广泛的定位类别：
- en: '[PRE28]'
  id: totrans-456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We first compute how correlated the spectra are across all proteins using Pearson
    correlation. Then, we build a tree (dendrogram) from that correlation matrix and
    cluster it to find groups of related proteins based on how similarly they use
    the codebook. These clusters often reflect shared localization patterns or functional
    relationships.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用皮尔逊相关系数计算所有蛋白质之间的光谱相关性。然后，我们从这个相关矩阵构建一棵树（树状图），并将其聚类以根据蛋白质如何使用代码簿找到相关蛋白质的组。这些簇通常反映了共享的定位模式或功能关系。
- en: 'The resulting heatmap in [Figure 6-15](#corr-heatmap) gives us a bird’s-eye
    view of which parts of the codebook are used together and by which proteins:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-15](#corr-heatmap)中显示的结果热图为我们提供了一个俯瞰图，展示了代码簿的哪些部分被一起使用，以及哪些蛋白质使用：'
- en: '[PRE29]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '![](assets/dlfb_0615.png)'
  id: totrans-460
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0615.png)'
- en: 'Figure 6-15\. Correlation heatmap between codebook entries, based on how often
    they co-occur across proteins. Each axis shows the index of a vector quantization
    (vq) codebook entry (not all 512 indices are written out due to space constraints).
    The shading indicates the Pearson correlation between code usage patterns across
    proteins: lighter denotes strong positive correlation, and darker strong negative
    correlation. The dendrogram here clusters codebook entries into groups that represent
    shared spatial features learned by the model.'
  id: totrans-461
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-15. 基于蛋白质之间共现频率的代码簿条目之间的相关性热图。每个轴显示向量量化（vq）代码簿条目的索引（由于空间限制，并非所有512个索引都被写出）。阴影表示蛋白质之间代码使用模式的皮尔逊相关系数：较亮表示强正相关，较暗表示强负相关。这里的树状图将代码簿条目聚类成代表模型学习到的共享空间特征的组。
- en: We can now take a deep dive into what the model’s feature spectra actually represent.
    Recall that for each protein, we computed a histogram of how often each codebook
    entry is used—essentially, a fingerprint of localization patterns learned by the
    model. Then, we computed the Pearson correlation between these fingerprints across
    proteins to see which codebook entries tend to be used together.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以深入探讨模型的特征光谱实际上代表了什么。回想一下，对于每个蛋白质，我们计算了每个代码簿条目被使用的频率直方图——这实际上是模型学习到的定位模式的一个指纹。然后，我们计算了这些指纹在蛋白质之间的皮尔逊相关系数，以查看哪些代码簿条目倾向于一起使用。
- en: The resulting plot in [Figure 6-15](#corr-heatmap) shows a correlation heatmap
    between all 512 codebook entries, with the color scale ranging from –1 (strong
    anticorrelation, darker) to +1 (strong correlation, lighter). Codebook entries
    that are frequently used together—for example, to describe a specific organelle—appear
    as bright blocks.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-15](#corr-heatmap)中显示的结果图展示了所有512个代码簿条目之间的相关性热图，颜色范围从-1（强反相关，较暗）到+1（强相关，较亮）。经常一起使用的代码簿条目（例如，用于描述特定细胞器）表现为明亮的块状。'
- en: The dendrogram at the top reflects a hierarchical clustering of these correlations.
    Based on this structure, the clustering procedure has grouped the codebook entries
    into eight broader encoding clusters, each of which represents a recurring spatial
    motif captured by the model. The color bar just below the dendrogram indicates
    these groupings.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 顶部的树状图反映了这些相关性的层次聚类。基于这个结构，聚类过程将代码簿条目分成了八个更广泛的编码簇，每个簇代表模型捕获的重复空间基序。树状图下面的颜色条指示这些分组。
- en: 'Here’s what this heatmap tells us:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 这张热图告诉我们以下信息：
- en: Diagonal blocks
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 对角块
- en: The visible lighter blocks along the diagonal show groups of codebook entries
    that are highly correlated, suggesting that they work together to represent similar
    spatial features across proteins.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 对角线可见的较浅的块表示高度相关的代码簿条目组，表明它们共同工作以在蛋白质中代表相似的空间特征。
- en: Off-diagonal near-zero correlations
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 对角线附近的近零相关性
- en: The lack of strong off-diagonal structure implies that these clusters are relatively
    distinct. The model has learned specialized regions of the codebook for different
    spatial contexts.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 缺乏强烈的对角线结构意味着这些簇相对独立。模型已经为不同的空间环境学习了代码簿的专用区域。
- en: Unsupervised insight
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督洞察
- en: Crucially, this entire structure emerged without any localization labels. It
    reflects how the model has self-organized its representation space purely from
    image similarity.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的是，整个结构是在没有任何定位标签的情况下出现的。它反映了模型如何纯粹从图像相似性中自我组织其表示空间。
- en: This is a powerful window into the internal structure of the learned representations,
    revealing that even individual quantized codes fall into larger functional groups
    that track meaningful biological variation.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 这是了解学习到的表示内部结构的强大窗口，揭示了即使是单个量化代码也落入更大的功能组中，这些功能组跟踪有意义的生物变异。
- en: To take this analysis a step further, we can examine how these codebook entries
    are used across known subcellular compartments. If the model has learned meaningful
    localization features, we should see that certain codebook vectors are enriched
    for specific compartments or structures like chromatin, vesicles, or ER.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步分析，我们可以检查这些代码簿条目如何在已知的亚细胞结构中跨使用。如果模型已经学会了有意义的定位特征，我们应该看到某些代码簿向量在特定的部分或结构（如染色质、囊泡或内质网）中富集。
- en: 'This is shown in [Figure 6-16](#stacked-histograms), where we average the feature
    spectra across all proteins within each localization class. Even though the model
    was never told anything about localization labels during training, we can now
    see clear signatures that align with known biology:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 这在[图6-16](#stacked-histograms)中显示，我们平均了每个定位类别中所有蛋白质的特征谱。尽管模型在训练过程中从未被告知任何关于定位标签的事情，我们现在可以看到与已知生物学一致的清晰特征：
- en: '[PRE30]'
  id: totrans-475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Although this is our initial, relatively simple model trained on a small subset
    of proteins, we already see some evidence that it is learning meaningful biological
    structure. Each row in the figure represents the average codebook usage (feature
    spectrum) for a given localization class. For instance, `nucleolus` (first row)
    shows peaks mostly within group VIII, suggesting that only a few specific codebook
    entries are consistently used to represent that compartment. This kind of narrow,
    high-signal spectrum is typical for highly structured and easily distinguishable
    compartments.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这是一个基于蛋白质小子集的相对简单的初始模型，但我们已经看到一些证据表明它正在学习有意义的生物结构。图中的每一行代表给定定位类别的平均代码簿使用（特征谱）。例如，`核仁`（第一行）在第八组主要显示峰值，表明只有少数特定的代码簿条目被持续用来表示该部分。这种狭窄、高信号的谱是高度结构化和易于区分的部分的典型特征。
- en: 'In contrast, `mitochondria` (third row), `er` (fourth row), and especially
    `cytoplasm` (fifth row) display broader, more distributed activation across various
    groups—perhaps reflecting their more heterogeneous or variable visual features,
    or simply lacking in learned separation due to our limited dataset. Still, each
    compartment has its own distinctive fingerprint: for instance, `mitochondria`
    shows much higher peaks in group VI. Interestingly, while `nucleoplasm` is spatially
    adjacent to `nucleolus` in the cell, it exhibits a markedly different activation
    pattern—whereas `nucleoplasm` appears much more similar to `chromatin`, possibly
    due to less structural separation or shared staining characteristics.'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，`线粒体`（第三行）、`内质网`（第四行）以及尤其是`细胞质`（第五行）在各个群体中表现出更广泛、更分散的激活——这或许反映了它们更异质或可变的视觉特征，或者简单地由于我们的数据集有限而缺乏学习到的分离。然而，每个部分都有其独特的指纹：例如，`线粒体`在第六组显示出更高的峰值。有趣的是，尽管`核质`在细胞中与`核仁`空间相邻，但它表现出明显不同的激活模式——而`核质`看起来与`染色质`更为相似，这可能是由于结构分离较少或共享的染色特性。
- en: '![](assets/dlfb_0616.png)'
  id: totrans-478
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0616.png)'
- en: Figure 6-16\. Stacked histograms of codebook usage for different protein localization
    classes. Each row shows the average feature spectrum for a specific subcellular
    compartment (e.g., ER, nucleoplasm). Vertical groupings (I–VIII) reflect clusters
    of related codebook entries, as defined in the correlation heatmap shown previously.
    Despite never being trained with localization labels, the model has learned to
    associate certain codebook vectors with specific biological structures, revealing
    meaningful and interpretable signatures.
  id: totrans-479
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-16\. 不同蛋白质定位类别的代码簿使用堆叠直方图。每一行显示特定亚细胞室（例如，内质网、核质）的平均特征谱。垂直分组（I–VIII）反映了之前显示的相关代码簿条目聚类。尽管从未使用定位标签进行训练，但该模型已经学会了将某些代码簿向量与特定的生物结构相关联，揭示了有意义的可解释特征。
- en: We’ve only scratched the surface using a small dataset and a simple architecture.
    In the next section, we’ll scale up both the model and the data to see how far
    this approach can go.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只使用小数据集和简单架构刚刚触及了表面。在下一节中，我们将扩大模型和数据规模，看看这种方法能走多远。
- en: Improving the Model
  id: totrans-481
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提高模型性能
- en: We’ve seen that even a relatively simple model with a small dataset of 50 proteins
    can learn meaningful representations of protein localization. The next step is
    to scale up the dataset to push performance further.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，即使是一个相对简单的模型，使用50个蛋白质的小数据集也能学习到蛋白质定位的有意义表示。下一步是将数据集规模扩大，以进一步提高性能。
- en: Scaling Up the Data
  id: totrans-483
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩大数据规模
- en: 'We now increase the number of proteins and their corresponding imaging frames
    significantly (from 50 to 500). This presents new challenges, especially in terms
    of training time and resource management. To make this process more manageable,
    we’ll save the final training state so that it can be reloaded for further inspection
    and evaluation:'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在显著增加了蛋白质的数量及其相应的成像帧（从50增加到500）。这带来了新的挑战，尤其是在训练时间和资源管理方面。为了使这个过程更容易管理，我们将保存最终的训练状态，以便可以重新加载以进行进一步的检查和评估：
- en: '[PRE31]'
  id: totrans-485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This model takes considerably longer to train, but the payoff is a more expressive
    and well-structured representation space. [Figure 6-17](#large-stacked-histograms)
    shows the aggregated codebook usage (feature spectra) across different localization
    classes, showing clearer localization signatures across a broader range of proteins
    (this plot also includes an expanded number of location classes as rows):'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型的训练时间明显更长，但回报是更具有表现力和结构化的表示空间。[图6-17](#large-stacked-histograms)显示了不同定位类别的代码簿使用（特征谱）的汇总，显示了更广泛的蛋白质范围内的更清晰的定位特征（此图还包括作为行的扩展位置类别数量）：
- en: '[PRE32]'
  id: totrans-487
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '![](assets/dlfb_0617.png)'
  id: totrans-488
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0617.png)'
- en: Figure 6-17\. Histograms of embedding codebook usage (feature spectra) for different
    subcellular localizations. Each row shows the average distribution of codebook
    features across all image frames annotated with a given localization (e.g., chromatin,
    vesicles, nuclear membrane). Distinct localization categories exhibit unique spectral
    “signatures”—for instance, nuclear membrane and chromatin show sharp, narrow peaks.
    This representation provides a quantitative “signature” of protein localization
    and allows predictions for unannotated proteins based on their similarity to known
    spectra.
  id: totrans-489
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-17\. 不同亚细胞定位的嵌入代码簿使用（特征谱）直方图。每一行显示给定定位（例如，染色质、囊泡、核膜）的所有图像帧中代码簿特征的平均分布。不同的定位类别表现出独特的光谱“特征”——例如，核膜和染色质显示出尖锐、狭窄的峰值。这种表示提供了蛋白质定位的定量“特征”，并允许基于其与已知光谱的相似性对未注释蛋白质进行预测。
- en: Now we’re cooking; we see more clearly differentiated spectral signatures compared
    to the earlier, smaller model. Many compartments display sharp, concentrated peaks,
    indicating that the model consistently relies on a small set of highly specific
    codebook vectors for these localizations. The rows—each representing a different
    subcellular compartment—now show more distinct and characteristic patterns. For
    example, comparing the “before” ([Figure 6-16](#stacked-histograms)) and “after”
    ([Figure 6-17](#large-stacked-histograms)) spectra for `mitochondria` and `er`,
    we see that in the “after” state, the signatures are less diffuse and more sharply
    defined, suggesting that the model has learned to focus more precisely on key
    features for these compartments.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经进入佳境；与早期的小型模型相比，我们看到了更清晰区分的光谱特征。许多隔室显示出尖锐、集中的峰值，表明模型始终依赖于一小组高度特定的代码簿向量来进行这些定位。每一行——代表不同的亚细胞隔室——现在显示出更明显和独特的模式。例如，比较`mitochondria`和`er`的“之前”（[图6-16](#stacked-histograms)）和“之后”（[图6-17](#large-stacked-histograms)）的光谱，我们看到在“之后”状态下，特征更不弥散且定义更清晰，这表明模型已经学会了更精确地关注这些隔室的关键特征。
- en: We could speculate that compartments showing broader activation across several
    codebook regions, such as the `er` and `cytoplasm`, do so because of their inherently
    more variable or sprawling visual structure. These compartments tend to span large
    regions of the cell and appear with more morphological diversity. For instance,
    the ER forms a network that spans the cell, while cytoplasmic proteins can exhibit
    diffuse or context-dependent patterns. These differences likely lead the model
    to spread their representations across a wider set of features.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以推测，那些在多个代码簿区域（如`er`和`cytoplasm`）表现出更广泛激活的隔室，之所以如此，是因为它们固有的更变化多端或扩展的视觉结构。这些隔室往往跨越细胞的大片区域，并表现出更多的形态多样性。例如，内质网（ER）形成一个跨越整个细胞的网络，而细胞质蛋白可以表现出弥散或与上下文相关的模式。这些差异可能导致模型将这些表示扩展到更广泛的特征集。
- en: Tip
  id: totrans-492
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'These improvements to the spectra came purely from scaling up the dataset;
    no changes were made to the model architecture. Try exploring the model side of
    things next: increase capacity by adjusting `num_embeddings`, `embedding_dim`,
    or `classification_``head_layers`, or experiment with your own architecture changes.
    There’s a lot of room to get creative here.'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 这些光谱的改进纯粹来自于数据集的扩展；没有对模型架构进行任何更改。接下来尝试探索模型方面的事情：通过调整`num_embeddings`、`embedding_dim`或`classification_``head_layers`来增加容量，或者尝试你自己的架构更改。这里有很多空间可以发挥创意。
- en: 'Next, we can replot a UMAP projection to visualize the model’s learned representation
    space (see [Figure 6-18](#large-projections-plot)):'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以重新绘制一个UMAP投影来可视化模型学习到的表示空间（见[图6-18](#large-projections-plot)）：
- en: '[PRE33]'
  id: totrans-495
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '![](assets/dlfb_0618.png)'
  id: totrans-496
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0618.png)'
- en: Figure 6-18\. UMAP projection of embeddings produced by the larger model and
    dataset. The tight clustering of similar labels (e.g., nucleolus, ER, chromatin)
    shows that the model has learned to distinguish complex spatial patterns—without
    using labels during training.
  id: totrans-497
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-18\. 大型模型和数据集生成的嵌入的UMAP投影。相似标签（例如，核仁、ER、染色质）的紧密聚类表明，模型已经学会了在没有使用标签的情况下区分复杂的空间模式。
- en: This projection offers a compelling view into how well the larger model has
    learned to distinguish subcellular compartments. Each point represents an individual
    image frame, colored by its known localization. But remember, the model was trained
    in a self-supervised manner and never had access to these labels.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 这个投影提供了一个引人入胜的视角，展示了更大的模型如何学会区分亚细胞隔室。每个点代表一个单独的图像帧，根据其已知的定位着色。但请记住，模型是以自监督的方式进行训练的，从未接触过这些标签。
- en: First, we see more points in the plot because the dataset is larger. We also
    observe that the separation between groups is now more distinct compared to earlier
    runs. Compartments such as the `cytoplasm`, `vesicles`, and `nucleoplasm` now
    form well-defined groups and are less intermixed, suggesting that the model has
    learned highly consistent visual signatures for these localizations. In some cases—such
    as `vesicles`—the model even appears to discover distinct subcategories, which
    are cleanly separated in the embedding space. Previously diffuse or heterogeneous
    categories like `cytoplasm` now exhibit more internal structure, with clearer
    spatial separation.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们注意到图中的点更多，因为数据集更大。我们还观察到，与早期运行相比，组之间的分离现在更加明显。`细胞质`、`囊泡`和`核质`等隔室现在形成明确的组，混合较少，这表明模型已经学会了高度一致的可视签名，用于这些定位。在某些情况下——例如`囊泡`——模型甚至似乎发现了不同的子类别，这些子类别在嵌入空间中清晰分离。以前分散或异质类别如`细胞质`现在表现出更多的内部结构，具有更清晰的空间分离。
- en: Going Further
  id: totrans-500
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入了解
- en: 'This is as far as we’ll go in this chapter, but there’s much more you can explore
    from here. The original cytoself paper introduces several architectural innovations
    that improve both performance and biological insight:'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们在这个章节中要讲的内容，但你可以从这里探索更多。原始的cytoself论文介绍了几个架构创新，这些创新既提高了性能，也提高了生物洞察力：
- en: Split quantization
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 分割量化
- en: Instead of quantizing full feature vectors, cytoself divides them into smaller
    chunks and quantizes each independently. This improves codebook usage and the
    richness of the learned representations.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 与量化完整的特征向量不同，cytoself将它们分割成更小的块，并独立量化每个块。这提高了代码簿的使用效率和学习的表示的丰富性。
- en: Global and local representations
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 全局和局部表示
- en: 'The model processes each image at two spatial scales:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 模型以两个空间尺度处理每个图像：
- en: A *coarse* (4 × 4 × 576) representation that captures high-level localization
    patterns.
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种*粗略*（4 × 4 × 576）的表示，能够捕捉高级定位模式。
- en: A *fine* (25 × 25 × 64) representation that preserves detailed spatial features.
    Each is quantized using a separate codebook.
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种*精细*（25 × 25 × 64）的表示，能够保留详细的空間特征。每个表示都使用单独的代码簿进行量化。
- en: 'These additions enabled cytoself to achieve several impressive results, all
    in an unsupervised setting:'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 这些新增功能使cytoself能够在无监督设置中实现几个令人印象深刻的结果：
- en: Predicting localization of previously uncharacterized proteins
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 预测先前未表征的蛋白质的定位
- en: For instance, it correctly inferred that FAM241A localized to the endoplasmic
    reticulum—a prediction that was later confirmed through colocalization experiments
    and mass spectrometry.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，它正确地推断出FAM241A定位于内质网——这是一个后来通过共定位实验和质谱学确认的预测。
- en: Distinguishing subtle subcellular differences
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 区分微妙的亚细胞差异
- en: 'The model learned to separate visually similar compartments like lysosomes
    and endosomes. Though hard to distinguish by eye, these compartments differ in
    function: lysosomes degrade cellular material, while endosomes serve as transport
    vesicles en route to lysosomes.'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 模型学会了区分视觉上相似的隔室，如溶酶体和内体。尽管用肉眼难以区分，但这些隔室在功能上有所不同：溶酶体降解细胞物质，而内体作为运输囊泡，通往溶酶体。
- en: Discovering protein complexes
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 发现蛋白质复合物
- en: Perhaps most impressively, cytoself grouped proteins belonging to the same complexes,
    such as ribosomes or the proteasome, purely from embedding similarity. By comparing
    protein-level embeddings, the authors showed that known complex members cluster
    tightly, even without any labels or prior knowledge. In some cases, this outperformed
    previous supervised methods, and it even hinted at previously uncharacterized
    complex members.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 最令人印象深刻的是，cytoself通过嵌入相似性将属于同一复合物的蛋白质（如核糖体或蛋白酶体）分组。通过比较蛋白质级别的嵌入，作者展示了已知的复合物成员紧密聚类，即使没有任何标签或先验知识。在某些情况下，这甚至超过了之前的监督方法，甚至暗示了之前未表征的复合物成员。
- en: If you’re interested in learning more, we encourage you to read the original
    paper and explore the [OpenCell resource](https://oreil.ly/eX8XH), which hosts
    the data and interactive tools used in the study. The paper also outlines exciting
    future directions like 3D imaging, label-free microscopy, and cross-species generalization.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对了解更多感兴趣，我们鼓励你阅读原始论文并探索[OpenCell资源](https://oreil.ly/eX8XH)，该资源提供了研究中使用的数据和交互式工具。论文还概述了令人兴奋的未来方向，如3D成像、无标记显微镜和跨物种泛化。
- en: Summary
  id: totrans-516
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we built a self-supervised deep learning model to learn spatial
    organization within human cells, all without relying on any manual annotations.
    This kind of approach is especially powerful for large-scale biological imaging,
    where manual curation not only is expensive and time-consuming but also can introduce
    human bias. You tackled a real-world challenge involving complex data, biological
    nuance, and custom neural architectures, all while drawing on key ideas from across
    the book.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们构建了一个自监督深度学习模型来学习人类细胞内的空间组织，而无需依赖任何手动标注。这种方法对于大规模生物成像特别有效，因为手动编辑不仅昂贵且耗时，还可能引入人为偏见。你应对了一个涉及复杂数据、生物细微差别和定制神经网络架构的实际情况，同时借鉴了本书中的关键思想。
- en: This was also the final end-to-end project chapter of the book. If you made
    it this far, congratulations. We hope this book helped you build intuition, fluency,
    and confidence in applying deep learning to biology. Whether you’re exploring
    new datasets, designing new models, or testing the boundaries of what today’s
    models can (and can’t) do, we’re excited to see where you go next.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是本书的最终端到端项目章节。如果你已经走到这一步，恭喜你。我们希望这本书能帮助你建立将深度学习应用于生物学的直觉、流畅性和信心。无论你是在探索新的数据集、设计新的模型，还是测试今天模型能做什么以及不能做什么的边界，我们都期待看到你的下一步。
- en: Happy modeling—and keep exploring!
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 喜欢建模——继续探索！
- en: ^([1](ch06.html#id943-marker)) Kobayashi, H., Cheveralls, K. C., Leonetti, M.
    D., & Royer, L. A. (2022). Self-supervised deep learning encodes high-resolution
    features of protein subcellular localization. *Nature Methods*, 19(8), 995–1003\.
    https://doi.org/10.1038/s41592-022-01541-z
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch06.html#id943-marker)) Kobayashi, H., Cheveralls, K. C., Leonetti, M.
    D., & Royer, L. A. (2022). 自监督深度学习编码蛋白质亚细胞定位的高分辨率特征. *自然方法*, 19(8), 995–1003\.
    https://doi.org/10.1038/s41592-022-01541-z
- en: ^([2](ch06.html#id946-marker)) Suk, T. R. (2020). The Role of TDP-43 Mislocalization
    in Amyotrophic Lateral Sclerosis. *Molecular Neurode‐Generation*, 15(1), 45.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch06.html#id946-marker)) Suk, T. R. (2020). TDP-43在肌萎缩侧索硬化症中的作用. *分子神经退行性*,
    15(1), 45.
- en: ^([3](ch06.html#id947-marker)) Rodriguez, J. A., Au, W. W., & Henderson, B.
    R. (2003). Cytoplasmic mislocalization of BRCA1 caused by cancer-associated mutations
    in the BRCT domain. *Experimental Cell Research*, 293(1), 14–21\. https://doi.org/10.1016/j.yexcr.2003.09.027
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch06.html#id947-marker)) Rodriguez, J. A., Au, W. W., & Henderson, B.
    R. (2003). 由癌症相关突变引起的BRCT结构域的BRCA1细胞质错位. *实验细胞研究*, 293(1), 14–21\. https://doi.org/10.1016/j.yexcr.2003.09.027
- en: ^([4](ch06.html#id948-marker)) Coale, T. H. et al. (2024). Nitrogen-fixing organelle
    in a marine alga. *Science*, 384(6692), 217–222\. https://doi.org/10.1126/science.adk1075
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch06.html#id948-marker)) Coale, T. H. et al. (2024). 水生藻类中的固氮细胞器. *科学*,
    384(6692), 217–222\. https://doi.org/10.1126/science.adk1075
- en: ^([5](ch06.html#id949-marker)) Paré, B., et al. (2015). [Early detection of
    structural abnormalities and cytoplasmic accumulation of TDP-43 in tissue-engineered
    skins derived from ALS patients](https://doi.org/10.1186/s40478-014-0181-z). *Acta
    Neuropathologica Communications*, 3(1).
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch06.html#id949-marker)) Paré, B., et al. (2015). [ALS患者来源的组织工程皮肤中TDP-43的结构异常和细胞质积累的早期检测](https://doi.org/10.1186/s40478-014-0181-z).
    *神经病理学通讯*, 3(1).
- en: ^([6](ch06.html#id950-marker)) Ye, D., et al. (2023). Recent Advances in Nanomedicine
    Design Strategies for Targeting Subcellular Structures,. *iScience*, 28(1), 111597.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch06.html#id950-marker)) Ye, D., et al. (2023). 针对亚细胞结构的纳米医学设计策略的最新进展.
    *iScience*, 28(1), 111597.
- en: ^([7](ch06.html#id956-marker)) Jordan, J. (2018, March 19). [*Introduction to
    autoencoders*](https://oreil.ly/iLKwc). Jeremy Jordan.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch06.html#id956-marker)) Jordan, J. (2018, March 19). [*自编码器简介*](https://oreil.ly/iLKwc).
    Jeremy Jordan.
- en: '^([8](ch06.html#id961-marker)) Dobilas, S. (2025, January 22). [VAE: Variational
    Autoencoders – How to employ neural networks to generate new images](https://oreil.ly/0SB-Q).
    *Towards Data Science*.'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch06.html#id961-marker)) Dobilas, S. (2025, January 22). [VAE：变分自编码器 –
    如何使用神经网络生成新的图像](https://oreil.ly/0SB-Q). *数据科学之路*.
- en: ^([9](ch06.html#id975-marker)) Van Den Oord, Aaron, Oriol Vinyals, and Koray
    Kavukcuoglu. 2017\. [“Neural Discrete Representation Learning”](https://oreil.ly/D-Rst).
    arXiv.Org. November 2, 2017.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch06.html#id975-marker)) Van Den Oord, Aaron, Oriol Vinyals, and Koray
    Kavukcuoglu. 2017\. [“神经离散表示学习”](https://oreil.ly/D-Rst). arXiv.Org. November
    2, 2017.
