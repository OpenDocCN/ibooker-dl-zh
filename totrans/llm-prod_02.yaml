- en: '3 Large language model operations: Building a platform for LLMs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 å¤§å‹è¯­è¨€æ¨¡å‹æ“ä½œï¼šä¸ºLLMsæ„å»ºå¹³å°
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æœ¬ç« æ¶µç›–
- en: An overview of large language model operations
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¤§å‹è¯­è¨€æ¨¡å‹æ“ä½œçš„æ¦‚è¿°
- en: Deployment challenges
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: éƒ¨ç½²æŒ‘æˆ˜
- en: Large language model best practices
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¤§å‹è¯­è¨€æ¨¡å‹æœ€ä½³å®è·µ
- en: Required large language model infrastructure
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¿…éœ€çš„å¤§å‹è¯­è¨€æ¨¡å‹åŸºç¡€è®¾æ–½
- en: Before anything else, preparation is the key to success.â€”Alexander Graham Bell
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åœ¨æ‰€æœ‰äº‹æƒ…ä¹‹å‰ï¼Œå‡†å¤‡æ˜¯æˆåŠŸçš„å…³é”®ã€‚â€”â€”äºšå†å±±å¤§Â·æ ¼æ‹‰æ±‰å§†Â·è´å°”
- en: As we learned in the last chapter, when it comes to transformers and natural
    language processing (NLP), bigger is better, especially when itâ€™s linguistically
    informed. However, bigger models come with bigger challenges because of their
    size, regardless of their linguistic efficacy, thus requiring us to scale up our
    operations and infrastructure to handle these problems. In this chapter, weâ€™ll
    be looking into exactly what those challenges are, what we can do to minimize
    them, and what architecture can be set up to help solve them.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨ä¸Šä¸€ç« æ‰€å­¦ï¼Œå½“æ¶‰åŠåˆ°è½¬æ¢å™¨å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ—¶ï¼Œè¶Šå¤§è¶Šå¥½ï¼Œå°¤å…¶æ˜¯åœ¨è¯­è¨€ä¿¡æ¯æ–¹é¢ã€‚ç„¶è€Œï¼Œç”±äºæ¨¡å‹çš„å¤§å°ï¼Œæ›´å¤§çš„æ¨¡å‹å¸¦æ¥äº†æ›´å¤§çš„æŒ‘æˆ˜ï¼Œæ— è®ºå®ƒä»¬çš„è¯­è¨€æ•ˆç‡å¦‚ä½•ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦æ‰©å±•æˆ‘ä»¬çš„æ“ä½œå’ŒåŸºç¡€è®¾æ–½æ¥å¤„ç†è¿™äº›é—®é¢˜ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨è¿™äº›æŒ‘æˆ˜ç©¶ç«Ÿæ˜¯ä»€ä¹ˆï¼Œæˆ‘ä»¬å¯ä»¥åšäº›ä»€ä¹ˆæ¥æœ€å°åŒ–å®ƒä»¬ï¼Œä»¥åŠå¯ä»¥å»ºç«‹ä»€ä¹ˆæ ·çš„æ¶æ„æ¥å¸®åŠ©è§£å†³è¿™äº›é—®é¢˜ã€‚
- en: 3.1 Introduction to large language model operations
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 å¤§å‹è¯­è¨€æ¨¡å‹æ“ä½œç®€ä»‹
- en: What is large language model operations (LLMOps)? Well, since we like to focus
    on practicality over rhetoric, weâ€™re not going to dive into any fancy definitions
    that youâ€™d expect in a textbook, but letâ€™s simply say itâ€™s machine learning operations
    (MLOps) that have been scaled to handle LLMs. Let us also say scaling up is hard.
    One of the hardest tasks in software engineering. Unfortunately, too many companies
    are running rudimentary MLOps setups, and donâ€™t think for a second that they will
    be able to handle LLMs. That said, the term *LLMOps* may not be needed. It has
    yet to show through as sufficiently different from core MLOps, especially considering
    they still have the same bones. If this book were a dichotomous key, MLOps and
    LLMOps would definitely be in the same genus, and only time will tell whether
    they are the same species. Of course, by refusing to define LLMOps properly, we
    might have traded one confusion for another, so letâ€™s take a minute to describe
    MLOps.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯å¤§å‹è¯­è¨€æ¨¡å‹æ“ä½œï¼ˆLLMOpsï¼‰ï¼Ÿå—¯ï¼Œæ—¢ç„¶æˆ‘ä»¬æ›´å–œæ¬¢å…³æ³¨å®ç”¨æ€§è€Œéè¾è—»ï¼Œæˆ‘ä»¬å°±ä¸ä¼šæ·±å…¥æ¢è®¨é‚£äº›ä½ åœ¨æ•™ç§‘ä¹¦ä¸­å¯èƒ½ä¼šæœŸå¾…çš„ä»»ä½•èŠ±å“¨çš„å®šä¹‰ï¼Œä½†è®©æˆ‘ä»¬ç®€å•åœ°è¯´ï¼Œå®ƒæ˜¯æŒ‡æ‰©å±•åˆ°å¤„ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æœºå™¨å­¦ä¹ æ“ä½œï¼ˆMLOpsï¼‰ã€‚æˆ‘ä»¬è¿˜å¯ä»¥è¯´ï¼Œæ‰©å±•æ˜¯å›°éš¾çš„ã€‚è½¯ä»¶å·¥ç¨‹ä¸­æœ€å›°éš¾çš„ä»»åŠ¡ä¹‹ä¸€ã€‚ä¸å¹¸çš„æ˜¯ï¼Œå¤ªå¤šå…¬å¸æ­£åœ¨è¿è¡ŒåŸºæœ¬çš„MLOpsè®¾ç½®ï¼Œå¹¶ä¸”ä»–ä»¬ç»å¯¹æ— æ³•å¤„ç†LLMsçš„æƒ³æ³•ã€‚è¯è™½å¦‚æ­¤ï¼Œ*LLMOps*è¿™ä¸ªæœ¯è¯­å¯èƒ½å¹¶ä¸éœ€è¦ã€‚å®ƒè¿˜æ²¡æœ‰æ˜¾ç¤ºå‡ºä¸æ ¸å¿ƒMLOpsæœ‰è¶³å¤Ÿçš„åŒºåˆ«ï¼Œå°¤å…¶æ˜¯è€ƒè™‘åˆ°å®ƒä»¬ä»ç„¶æœ‰ç›¸åŒçš„ç»“æ„ã€‚å¦‚æœè¿™æœ¬ä¹¦æ˜¯ä¸€ä¸ªäºŒåˆ†æ³•çš„å…³é”®ï¼ŒMLOpså’ŒLLMOpsè‚¯å®šå±äºåŒä¸€å±ï¼Œè€Œåªæœ‰æ—¶é—´æ‰èƒ½å‘Šè¯‰æˆ‘ä»¬å®ƒä»¬æ˜¯å¦æ˜¯åŒä¸€ç‰©ç§ã€‚å½“ç„¶ï¼Œé€šè¿‡æ‹’ç»æ­£ç¡®åœ°å®šä¹‰LLMOpsï¼Œæˆ‘ä»¬å¯èƒ½å·²ç»å°†ä¸€ç§å›°æƒ‘æ¢æˆäº†å¦ä¸€ç§å›°æƒ‘ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬èŠ±ä¸€åˆ†é’Ÿæ¥æè¿°ä¸€ä¸‹MLOpsã€‚
- en: MLOps is the field and practice of reliably and efficiently deploying and maintaining
    machine learning models in production. This includesâ€”and, indeed, requiresâ€”managing
    the entire machine learning life cycle, from data acquisition and model training
    to monitoring and termination. A few principles required to master this field
    include workflow orchestration, versioning, feedback loops, continuous integration
    and continuous deployment (CI/CD), security, resource provisioning, and data governance.
    While there are often personnel who specialize in the productionizing of models,
    with titles like ML Engineers, MLOps Engineers, or ML Infrastructure Engineer,
    the field is a large-enough beast that it often abducts many other unsuspecting
    professionals to work in it who hold titles like Data Scientist or DevOps Engineerâ€”often
    against their knowledge or will, leaving them kicking and screaming, â€œItâ€™s not
    my job.â€
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: MLOpsæ˜¯å¯é ä¸”é«˜æ•ˆåœ°å°†æœºå™¨å­¦ä¹ æ¨¡å‹éƒ¨ç½²å’Œç»´æŠ¤åœ¨ç”Ÿäº§ä¸­çš„é¢†åŸŸå’Œå®è·µã€‚è¿™åŒ…æ‹¬â€”â€”å®é™…ä¸Šï¼Œè¿™æ˜¯å¿…éœ€çš„â€”â€”ç®¡ç†æ•´ä¸ªæœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸï¼Œä»æ•°æ®è·å–å’Œæ¨¡å‹è®­ç»ƒåˆ°ç›‘æ§å’Œç»ˆæ­¢ã€‚æŒæ¡è¿™ä¸ªé¢†åŸŸæ‰€éœ€çš„ä¸€äº›åŸåˆ™åŒ…æ‹¬å·¥ä½œæµç¨‹ç¼–æ’ã€ç‰ˆæœ¬æ§åˆ¶ã€åé¦ˆå¾ªç¯ã€æŒç»­é›†æˆå’ŒæŒç»­éƒ¨ç½²ï¼ˆCI/CDï¼‰ã€å®‰å…¨æ€§ã€èµ„æºåˆ†é…å’Œæ•°æ®æ²»ç†ã€‚è™½ç„¶é€šå¸¸æœ‰ä¸“é—¨ä»äº‹æ¨¡å‹ç”Ÿäº§åŒ–çš„ä¸ªäººï¼Œä»–ä»¬çš„å¤´è¡”å¯èƒ½æ˜¯MLå·¥ç¨‹å¸ˆã€MLOpså·¥ç¨‹å¸ˆæˆ–MLåŸºç¡€è®¾æ–½å·¥ç¨‹å¸ˆï¼Œä½†è¿™ä¸ªé¢†åŸŸè¶³å¤Ÿåºå¤§ï¼Œå®ƒç»å¸¸ç»‘æ¶è®¸å¤šå…¶ä»–æ„æƒ³ä¸åˆ°çš„ä¸“ä¸šäººå£«æ¥å·¥ä½œï¼Œä»–ä»¬çš„å¤´è¡”å¯èƒ½æ˜¯æ•°æ®ç§‘å­¦å®¶æˆ–DevOpså·¥ç¨‹å¸ˆâ€”â€”é€šå¸¸æ˜¯åœ¨ä»–ä»¬ä¸çŸ¥æƒ…æˆ–ä¸æ„¿æ„çš„æƒ…å†µä¸‹ï¼Œè®©ä»–ä»¬å¤§å–Šå¤§å«ï¼Œâ€œè¿™ä¸æ˜¯æˆ‘çš„å·¥ä½œã€‚â€
- en: 3.2 Operations challenges with large language models
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 å¤§å‹è¯­è¨€æ¨¡å‹çš„æ“ä½œæŒ‘æˆ˜
- en: So why have a distinction at all? If MLOps and LLMOps are so similar, is LLMOps
    just another fad opportunists throw on their resume? Not quite. In fact, itâ€™s
    quite similar to the term *Big Data*. When the term was at its peak popularity,
    people with titles like Big Data Engineer used completely different tool sets
    and developed specialized expertise necessary to handle large datasets. LLMs come
    with a set of challenges and problems you wonâ€™t find with traditional machine
    learning systems. A majority of these problems extend almost exclusively because
    they are so big. Large models are large! We hope to show you that LLMs truly earn
    their name. Letâ€™s take a look at a few of these challenges so we can appreciate
    the task ahead of us when we start talking about deploying an LLM.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¸ºä»€ä¹ˆè¿˜è¦æœ‰åŒºåˆ«å‘¢ï¼Ÿå¦‚æœMLOpså’ŒLLMOpså¦‚æ­¤ç›¸ä¼¼ï¼ŒLLMOpséš¾é“åªæ˜¯é‚£äº›æœºä¼šä¸»ä¹‰è€…ç®€å†ä¸Šçš„å¦ä¸€ä¸ªæµè¡Œè¶‹åŠ¿å—ï¼Ÿå¹¶éå¦‚æ­¤ã€‚äº‹å®ä¸Šï¼Œå®ƒä¸â€œå¤§æ•°æ®â€è¿™ä¸ªæœ¯è¯­éå¸¸ç›¸ä¼¼ã€‚å½“è¿™ä¸ªæœ¯è¯­è¾¾åˆ°é¡¶å³°æ—¶ï¼Œé‚£äº›è¢«ç§°ä¸ºå¤§æ•°æ®å·¥ç¨‹å¸ˆçš„äººä½¿ç”¨å®Œå…¨ä¸åŒçš„å·¥å…·é›†ï¼Œå¹¶å¼€å‘äº†å¤„ç†å¤§æ•°æ®æ‰€éœ€çš„ä¸“é—¨ä¸“ä¸šçŸ¥è¯†ã€‚LLMå¸¦æ¥äº†ä¸€ç³»åˆ—ä½ åœ¨ä¼ ç»Ÿæœºå™¨å­¦ä¹ ç³»ç»Ÿä¸­æ‰¾ä¸åˆ°çš„æŒ‘æˆ˜å’Œé—®é¢˜ã€‚å…¶ä¸­å¤§å¤šæ•°é—®é¢˜å‡ ä¹å®Œå…¨æ˜¯å› ä¸ºå®ƒä»¬å¦‚æ­¤ä¹‹å¤§ã€‚å¤§å‹æ¨¡å‹å°±æ˜¯å¤§ï¼æˆ‘ä»¬å¸Œæœ›å‘ä½ å±•ç¤ºLLMç¡®å®é…å¾—ä¸Šè¿™ä¸ªåå­—ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å…¶ä¸­çš„ä¸€äº›æŒ‘æˆ˜ï¼Œè¿™æ ·å½“æˆ‘ä»¬å¼€å§‹è®¨è®ºéƒ¨ç½²LLMæ—¶ï¼Œæˆ‘ä»¬å¯ä»¥æ¬£èµåˆ°æˆ‘ä»¬é¢å‰çš„ä»»åŠ¡ã€‚
- en: 3.2.1 Long download times
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.1 é•¿ä¸‹è½½æ—¶é—´
- en: 'Back in 2017, when I was still heavily involved as a data scientist, I decided
    to try my hand at reimplementing some of the most famous computer vision models
    at the time: AlexNet, VGG19, and ResNet. I figured this would be a good way to
    reinforce my understanding of the basics with some practical hands-on experience.
    Plus, I had an ulterior motive: I had just built my own rig with some NVIDIA GeForce
    1080 TI GPUs, which were state of the art at the time, and I thought this would
    be a good way to break them in. The first task was to download the ImageNet dataset.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å›åˆ°2017å¹´ï¼Œå½“æˆ‘ä»ç„¶ä½œä¸ºä¸€ä¸ªæ•°æ®ç§‘å­¦å®¶æ·±åº¦å‚ä¸æ—¶ï¼Œæˆ‘å†³å®šå°è¯•é‡æ–°å®ç°å½“æ—¶æœ€è‘—åçš„è®¡ç®—æœºè§†è§‰æ¨¡å‹ä¹‹ä¸€ï¼šAlexNetã€VGG19å’ŒResNetã€‚æˆ‘æƒ³è¿™å°†æ˜¯é€šè¿‡ä¸€äº›å®é™…åŠ¨æ‰‹ç»éªŒæ¥åŠ å¼ºæˆ‘å¯¹åŸºç¡€çŸ¥è¯†ç†è§£çš„å¥½æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘è¿˜æœ‰ä¸€ä¸ªé¢å¤–çš„åŠ¨æœºï¼šæˆ‘åˆšåˆšç”¨ä¸€äº›å½“æ—¶æœ€å…ˆè¿›çš„NVIDIA
    GeForce 1080 TI GPUæ„å»ºäº†è‡ªå·±çš„ç”µè„‘ï¼Œæˆ‘è®¤ä¸ºè¿™å°†æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æ–¹å¼æ¥ç£¨åˆå®ƒä»¬ã€‚ç¬¬ä¸€ä¸ªä»»åŠ¡æ˜¯ä¸‹è½½ImageNetæ•°æ®é›†ã€‚
- en: The ImageNet dataset was one of the largest annotated datasets available, containing
    millions of images rounding out to a file size of a whopping ~150 GB! Working
    with it was proof that you knew how to work with Big Data, which was still a trendy
    word and an invaluable skill set for a data scientist at the time. After agreeing
    to the terms and gaining access, I got my first wakeup call. Downloading it took
    an entire week.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ImageNetæ•°æ®é›†æ˜¯å½“æ—¶å¯ç”¨çš„æœ€å¤§æ ‡æ³¨æ•°æ®é›†ä¹‹ä¸€ï¼ŒåŒ…å«æ•°ç™¾ä¸‡å¼ å›¾ç‰‡ï¼Œæ€»æ–‡ä»¶å¤§å°é«˜è¾¾æƒŠäººçš„~150 GBï¼ä¸ä¹‹å·¥ä½œè¯æ˜äº†ä½ çŸ¥é“å¦‚ä½•å¤„ç†å¤§æ•°æ®ï¼Œè¿™åœ¨å½“æ—¶è¿˜æ˜¯ä¸€ä¸ªçƒ­é—¨è¯æ±‡ï¼Œä¹Ÿæ˜¯æ•°æ®ç§‘å­¦å®¶ä¸å¯æˆ–ç¼ºçš„æŠ€èƒ½é›†ã€‚åœ¨åŒæ„æ¡æ¬¾å¹¶è·å¾—è®¿é—®æƒé™åï¼Œæˆ‘æ”¶åˆ°äº†ç¬¬ä¸€ä¸ªè­¦é’Ÿã€‚ä¸‹è½½å®ƒèŠ±äº†ä¸€æ•´å‘¨æ—¶é—´ã€‚
- en: 'When my team first deployed Bloom, it took an hour and a half to download it.
    Heck, it took an hour and a half to download *The Legend of Zelda: Tears of the
    Kingdom*, and thatâ€™s only 16 GB, so we really couldnâ€™t complain.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘çš„å›¢é˜Ÿé¦–æ¬¡éƒ¨ç½²Bloomæ—¶ï¼Œä¸‹è½½å®ƒèŠ±äº†ä¸€ä¸ªåŠå°æ—¶ã€‚å¤©å“ªï¼Œä¸‹è½½ã€Šå¡å°”è¾¾ä¼ è¯´ï¼šç‹å›½ä¹‹æ³ªã€‹ä¹ŸèŠ±äº†ä¸€ä¸ªåŠå°æ—¶ï¼Œè€Œé‚£åªæ˜¯ä¸€ä¸ª16 GBçš„æ–‡ä»¶ï¼Œæ‰€ä»¥æˆ‘ä»¬çœŸçš„ä¸èƒ½æŠ±æ€¨ã€‚
- en: 'Large models are large. That canâ€™t be overstated. Youâ€™ll find throughout this
    book that that fact comes with many additional headaches and problems for the
    entire production process, and you have to be prepared for it. In comparison to
    the ImageNet dataset, the Bloom LLM model is 330 GB, more than twice the size.
    Weâ€™re guessing most readers havenâ€™t worked with either ImageNet or Bloom, so for
    comparison, *Call of Duty: Modern Warfare*, one of the largest games at the time
    of this writing, is 235 GB. *Final Fantasy 15* is only 148 GB, so you could fit
    two into the model with plenty of room to spare. Itâ€™s just hard to really comprehend
    how massive LLMs are. We went from 100 million parameters in models like BERT
    and took them to billions of parameters. If you went on a shopping spree and spent
    $20 a second (or maybe accidentally left your AWS EC2 instance on), itâ€™d take
    you half a day to spend a million dollars; it would take you two years to spend
    a billion.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å‹æ¨¡å‹å°±æ˜¯å¤§ã€‚è¿™ä¸€ç‚¹ä¸å®¹å¿½è§†ã€‚ä½ ä¼šåœ¨æ•´æœ¬ä¹¦ä¸­å‘ç°ï¼Œè¿™ä¸€äº‹å®ç»™æ•´ä¸ªç”Ÿäº§è¿‡ç¨‹å¸¦æ¥äº†è®¸å¤šé¢å¤–çš„å¤´ç—›å’Œé—®é¢˜ï¼Œä½ å¿…é¡»ä¸ºæ­¤åšå¥½å‡†å¤‡ã€‚ä¸ ImageNet æ•°æ®é›†ç›¸æ¯”ï¼ŒBloom
    LLM æ¨¡å‹ä¸º 330 GBï¼Œæ˜¯åè€…çš„ä¸¤å€å¤šã€‚æˆ‘ä»¬çŒœæµ‹å¤§å¤šæ•°è¯»è€…éƒ½æ²¡æœ‰ä½¿ç”¨è¿‡ ImageNet æˆ– Bloomï¼Œæ‰€ä»¥ä¸ºäº†æ¯”è¾ƒï¼Œ*ã€Šä½¿å‘½å¬å”¤ï¼šç°ä»£æˆ˜äº‰ã€‹*ï¼Œåœ¨æ’°å†™æœ¬æ–‡æ—¶æ˜¯æœ€å¤§çš„æ¸¸æˆä¹‹ä¸€ï¼Œå¤§å°ä¸º
    235 GBã€‚*ã€Šæœ€ç»ˆå¹»æƒ³ 15ã€‹* åªæœ‰ 148 GBï¼Œæ‰€ä»¥ä½ å¯ä»¥å°†ä¸¤ä¸ªæ¨¡å‹æ”¾å…¥å…¶ä¸­ï¼Œè¿˜æœ‰è¶³å¤Ÿçš„ç©ºé—´ã€‚çœŸæ­£ç†è§£ LLM çš„å·¨å¤§è§„æ¨¡æ˜¯å¾ˆå›°éš¾çš„ã€‚æˆ‘ä»¬ä» BERT
    ç­‰æ¨¡å‹ä¸­çš„ 1 äº¿ä¸ªå‚æ•°å¼€å§‹ï¼Œå°†å®ƒä»¬æå‡åˆ°æ•°åäº¿ä¸ªå‚æ•°ã€‚å¦‚æœä½ è¿›è¡Œäº†ä¸€åœºè´­ç‰©ç‹‚æ¬¢ï¼Œæ¯ç§’èŠ±è´¹ 20 ç¾å…ƒï¼ˆæˆ–è€…å¯èƒ½ä¸å°å¿ƒè®©ä½ çš„ AWS EC2 å®ä¾‹ä¸€ç›´è¿è¡Œï¼‰ï¼Œä½ éœ€è¦åŠå¤©æ—¶é—´æ‰èƒ½èŠ±æ‰ä¸€ç™¾ä¸‡ç¾å…ƒï¼›èŠ±æ‰åäº¿ç¾å…ƒåˆ™éœ€è¦ä¸¤å¹´ã€‚
- en: Thankfully, it doesnâ€™t take two weeks to download Bloom because unlike ImageNet,
    itâ€™s not hosted on a poorly managed university server, and it also has been sharded
    into multiple smaller files to allow downloading in parallel, but it will still
    take an uncomfortably long time. Consider a scenario where you are downloading
    the model under the best conditions. Youâ€™re equipped with a gigabit speed fiber
    internet connection, and youâ€™re magically able to dedicate the entire bandwidth
    and I/O operations of your system and the server to it. It will still take over
    5 minutes to download! Of course, thatâ€™s under the best conditions. You probably
    wonâ€™t be downloading the model under such circumstances; with modern infrastructure,
    you can expect it to take on the order of hours.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¸è¿çš„æ˜¯ï¼Œä¸‹è½½ Bloom å¹¶ä¸éœ€è¦ä¸¤å‘¨æ—¶é—´ï¼Œå› ä¸ºä¸ ImageNet ä¸åŒï¼Œå®ƒä¸æ˜¯æ‰˜ç®¡åœ¨ä¸€ä¸ªç®¡ç†ä¸å–„çš„å¤§å­¦æœåŠ¡å™¨ä¸Šï¼Œè€Œä¸”å®ƒå·²ç»è¢«åˆ†å‰²æˆå¤šä¸ªè¾ƒå°çš„æ–‡ä»¶ï¼Œä»¥ä¾¿å¹¶è¡Œä¸‹è½½ï¼Œä½†è¿™ä»ç„¶ä¼šèŠ±è´¹ä¸€ä¸ªä¸èˆ’æœçš„é•¿æ—¶é—´ã€‚è€ƒè™‘ä¸€ä¸ªåœºæ™¯ï¼Œä½ æ­£åœ¨æœ€ä½³æ¡ä»¶ä¸‹ä¸‹è½½æ¨¡å‹ã€‚ä½ æ‹¥æœ‰åƒå…†é€Ÿåº¦çš„å…‰çº¤äº’è”ç½‘è¿æ¥ï¼Œå¹¶ä¸”ç¥å¥‡åœ°èƒ½å¤Ÿå°†ä½ ç³»ç»Ÿå’ŒæœåŠ¡å™¨ä¸Šçš„å…¨éƒ¨å¸¦å®½å’Œ
    I/O æ“ä½œéƒ½åˆ†é…ç»™å®ƒã€‚å³ä½¿è¿™æ ·ï¼Œä¸‹è½½ä»ç„¶éœ€è¦è¶…è¿‡ 5 åˆ†é’Ÿï¼å½“ç„¶ï¼Œè¿™æ˜¯åœ¨æœ€ä½³æ¡ä»¶ä¸‹ã€‚ä½ å¾ˆå¯èƒ½ä¸ä¼šåœ¨å¦‚æ­¤æƒ…å†µä¸‹ä¸‹è½½æ¨¡å‹ï¼›åœ¨ç°ä»£åŸºç¡€è®¾æ–½ä¸‹ï¼Œä½ é¢„è®¡å®ƒéœ€è¦æ•°å°æ—¶ã€‚
- en: 3.2.2 Longer deploy times
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.2 æ›´é•¿çš„éƒ¨ç½²æ—¶é—´
- en: Just downloading the model is a long enough time frame to make any seasoned
    developer shake, but deployment times are going to make them keel over and call
    for medical attention. A model as big as Bloom can take 30 to 45 minutes just
    to load the model into GPU memoryâ€”at least, those are the time frames weâ€™ve experienced.
    Thatâ€™s not to mention any other steps in your deployment process that can add
    to this. Indeed, with GPU shortages, it can easily take hours just waiting for
    resources to free upâ€”more on that in a minute.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä»…ä¸‹è½½æ¨¡å‹å°±è¶³å¤Ÿè®©ä»»ä½•ç»éªŒä¸°å¯Œçš„å¼€å‘è€…æ„Ÿåˆ°ä¸å®‰ï¼Œä½†éƒ¨ç½²æ—¶é—´å°†ä½¿ä»–ä»¬å€’ä¸‹å¹¶å¯»æ±‚åŒ»ç–—æ´åŠ©ã€‚åƒ Bloom è¿™æ ·å¤§çš„æ¨¡å‹ä»…å°†æ¨¡å‹åŠ è½½åˆ° GPU å†…å­˜ä¸­å°±éœ€è¦
    30 åˆ° 45 åˆ†é’Ÿâ€”â€”è‡³å°‘ï¼Œè¿™æ˜¯æˆ‘ä»¬ç»å†çš„æ—¶é—´æ¡†æ¶ã€‚è¿™è¿˜ä¸åŒ…æ‹¬ä½ éƒ¨ç½²è¿‡ç¨‹ä¸­å¯èƒ½å¢åŠ çš„å…¶ä»–æ­¥éª¤ã€‚å®é™…ä¸Šï¼Œç”±äº GPU ç¼ºè´§ï¼Œä»…ç­‰å¾…èµ„æºé‡Šæ”¾å°±å¯èƒ½ä¼šèŠ±è´¹æ•°å°æ—¶â€”â€”å…³äºè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ç¨åå†è°ˆã€‚
- en: What does this mean for you and your team? Well, for starters, we know lots
    of teams that deploy ML products often simply download the model at run time.
    That might work for small sklearn regression models, but it isnâ€™t going to work
    for LLMs. Additionally, you can take most of what you know about deploying reliable
    systems and throw it out the window (but thankfully not too far). Most modern-day
    best practices for software engineering assume you can easily restart an application
    if anything happens, and thereâ€™s a lot of rigmarole involved to ensure your systems
    can do just that. With LLMs, it can take seconds to shut down, but potentially
    hours to redeploy, making this a semi-irreversible process. Like picking an apple
    off a tree, itâ€™s easy to pluck one off, but if you bite into it and decide itâ€™s
    too sour, you canâ€™t reattach it to the tree so it can continue to ripen. Youâ€™ll
    just have to wait awhile for another to grow.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯¹ä½ å’Œä½ çš„å›¢é˜Ÿæ„å‘³ç€ä»€ä¹ˆï¼Ÿé¦–å…ˆï¼Œæˆ‘ä»¬çŸ¥é“è®¸å¤šå›¢é˜Ÿåœ¨éƒ¨ç½²æœºå™¨å­¦ä¹ äº§å“æ—¶é€šå¸¸ä¼šåœ¨è¿è¡Œæ—¶ä¸‹è½½æ¨¡å‹ã€‚è¿™å¯èƒ½é€‚ç”¨äºå°çš„sklearnå›å½’æ¨¡å‹ï¼Œä½†å¯¹äºLLMsæ¥è¯´åˆ™ä¸è¡Œã€‚æ­¤å¤–ï¼Œä½ å¯ä»¥æŠŠå…³äºéƒ¨ç½²å¯é ç³»ç»Ÿçš„å¤§éƒ¨åˆ†çŸ¥è¯†éƒ½æ‰”åˆ°çª—å¤–ï¼ˆä½†å¹¸è¿çš„æ˜¯ï¼Œä¸æ˜¯å¤ªè¿œï¼‰ã€‚å¤§å¤šæ•°ç°ä»£è½¯ä»¶å·¥ç¨‹çš„æœ€ä½³å®è·µéƒ½å‡è®¾ä½ å¯ä»¥è½»æ¾åœ°é‡å¯åº”ç”¨ç¨‹åºï¼Œå¹¶ä¸”æœ‰å¾ˆå¤šç¹ççš„ç¨‹åºæ¥ç¡®ä¿ä½ çš„ç³»ç»Ÿå¯ä»¥åšåˆ°è¿™ä¸€ç‚¹ã€‚å¯¹äºLLMsæ¥è¯´ï¼Œå…³é—­å¯èƒ½åªéœ€è¦å‡ ç§’é’Ÿï¼Œä½†é‡æ–°éƒ¨ç½²å¯èƒ½éœ€è¦å‡ ä¸ªå°æ—¶ï¼Œè¿™ä½¿å¾—è¿™æ˜¯ä¸€ä¸ªåŠä¸å¯é€†çš„è¿‡ç¨‹ã€‚å°±åƒä»æ ‘ä¸Šæ‘˜è‹¹æœä¸€æ ·ï¼Œæ‘˜ä¸‹æ¥å¾ˆå®¹æ˜“ï¼Œä½†å¦‚æœå’¬ä¸€å£è§‰å¾—å¤ªé…¸ï¼Œå°±æ— æ³•å†æŠŠå®ƒé‡æ–°æ¥åˆ°æ ‘ä¸Šç»§ç»­æˆç†Ÿã€‚ä½ åªèƒ½ç­‰å¾…å¦ä¸€ä¸ªæˆç†Ÿã€‚
- en: While not every project requires deploying the largest models out there, you
    can expect to see deployment times measured in minutes. These longer deploy times
    make scaling down right before a surge of traffic a terrible mistake, as well
    as figuring out how to manage bursty workloads difficult. General CI/CD methodologies
    need to be adjusted since rolling updates take longer, leaving a backlog piling
    up quickly in your pipeline. Silly mistakes like typos or other bugs often take
    longer to notice and longer to correct.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å¹¶éæ¯ä¸ªé¡¹ç›®éƒ½éœ€è¦éƒ¨ç½²æœ€å¤§çš„æ¨¡å‹ï¼Œä½†ä½ å¯ä»¥é¢„æœŸéƒ¨ç½²æ—¶é—´ä»¥åˆ†é’Ÿè®¡ç®—ã€‚è¿™äº›è¾ƒé•¿çš„éƒ¨ç½²æ—¶é—´ä½¿å¾—åœ¨æµé‡æ¿€å¢å‰ç¼©å‡è§„æ¨¡æˆä¸ºä¸¥é‡çš„é”™è¯¯ï¼ŒåŒæ—¶ä¹Ÿä½¿å¾—ç®¡ç†çªå‘æ€§å·¥ä½œè´Ÿè½½å˜å¾—å›°éš¾ã€‚ç”±äºæ»šåŠ¨æ›´æ–°éœ€è¦æ›´é•¿çš„æ—¶é—´ï¼Œå› æ­¤åœ¨ä½ çš„ç®¡é“ä¸­ä¼šè¿…é€Ÿç§¯ç´¯å¤§é‡ç§¯å‹ã€‚åƒæ‹¼å†™é”™è¯¯æˆ–å…¶ä»–é”™è¯¯è¿™æ ·çš„æ„šè ¢é”™è¯¯å¾€å¾€éœ€è¦æ›´é•¿çš„æ—¶é—´æ‰èƒ½è¢«å‘ç°å’Œçº æ­£ã€‚
- en: 3.2.3 Latency
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.3 å»¶è¿Ÿ
- en: Along with increases in model size often come increases in inference latency.
    This is obvious when stated, but more parameters equate to more computations,
    and more computations mean longer inference wait times. However, this canâ€™t be
    underestimated. We know many people who downplay the latency problems because
    theyâ€™ve interacted with an LLM chatbot, and the experience felt smooth. Take a
    second look, though, and youâ€™ll notice that it is returning one word at a time,
    which is streamed to the user. It feels smooth because the answers are coming
    in faster than a human can read, but a second look helps us realize this is just
    a UX trick. LLMs are still too slow to be very useful for an autocomplete solution,
    for example, where responses have to be blazingly fast. Building it into a data
    pipeline or workflow that reads a large corpus of text and then tries to clean
    it or summarize it may also be too prohibitively slow to be useful or reliable.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å°ºå¯¸çš„å¢åŠ é€šå¸¸ä¼´éšç€æ¨ç†å»¶è¿Ÿçš„å¢åŠ ã€‚è¿™ä¸€ç‚¹è¯´å‡ºæ¥å¾ˆæ˜æ˜¾ï¼Œä½†æ›´å¤šçš„å‚æ•°æ„å‘³ç€æ›´å¤šçš„è®¡ç®—ï¼Œæ›´å¤šçš„è®¡ç®—æ„å‘³ç€æ›´é•¿çš„æ¨ç†ç­‰å¾…æ—¶é—´ã€‚ç„¶è€Œï¼Œè¿™ä¸€ç‚¹ä¸èƒ½è¢«ä½ä¼°ã€‚æˆ‘ä»¬çŸ¥é“å¾ˆå¤šäººè½»è§†å»¶è¿Ÿé—®é¢˜ï¼Œå› ä¸ºä»–ä»¬ä¸ä¸€ä¸ªLLMèŠå¤©æœºå™¨äººäº’åŠ¨è¿‡ï¼Œæ„Ÿè§‰ä½“éªŒå¾ˆæµç•…ã€‚ä½†å¦‚æœä½ ä»”ç»†è§‚å¯Ÿï¼Œä½ ä¼šå‘ç°å®ƒä¸€æ¬¡åªè¿”å›ä¸€ä¸ªå•è¯ï¼Œè¿™äº›å•è¯è¢«æµå¼ä¼ è¾“ç»™ç”¨æˆ·ã€‚ä¹‹æ‰€ä»¥æ„Ÿè§‰æµç•…ï¼Œæ˜¯å› ä¸ºç­”æ¡ˆçš„åˆ°æ¥é€Ÿåº¦è¶…è¿‡äº†äººç±»é˜…è¯»çš„é€Ÿåº¦ï¼Œä½†ä»”ç»†è§‚å¯Ÿæœ‰åŠ©äºæˆ‘ä»¬æ„è¯†åˆ°è¿™åªæ˜¯ä¸€ä¸ªUXæŠ€å·§ã€‚LLMsä»ç„¶å¤ªæ…¢ï¼Œå¯¹äºéœ€è¦å¿«é€Ÿå“åº”çš„è‡ªåŠ¨è¡¥å…¨è§£å†³æ–¹æ¡ˆç­‰ç”¨é€”æ¥è¯´ï¼Œä»ç„¶ä¸å¤ªæœ‰ç”¨ã€‚å°†å…¶æ„å»ºåˆ°è¯»å–å¤§é‡æ–‡æœ¬æ•°æ®çš„æ•°æ®ç®¡é“æˆ–å·¥ä½œæµç¨‹ä¸­ï¼Œç„¶åå°è¯•æ¸…ç†æˆ–æ€»ç»“å®ƒï¼Œä¹Ÿå¯èƒ½å› ä¸ºé€Ÿåº¦è¿‡æ…¢è€Œæ— æ³•ä½¿ç”¨æˆ–å¯é ã€‚
- en: There are also many less obvious reasons for their slowness. For starters, LLMs
    are often distributed across multiple GPUs, which adds extra communication overhead.
    As discussed later in this chapter in section 3.3.2, they are distributed in other
    ways, often even to improve latency, but any distribution adds an additional overhead
    burden. In addition, LLMsâ€™ latency is severely affected by completion length,
    meaning the more words it uses to return a response, the longer it takes. Of course,
    completion length also seems to improve accuracy. For example, using prompt engineering
    techniques like chain of thought (CoT), we ask the model to think about a problem
    in a step-by-step fashion, which has been shown to improve results for logic and
    math questions but significantly increases the response length and latency time.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ä»–ä»¬ç¼“æ…¢çš„åŸå› è¿˜æœ‰å¾ˆå¤šä¸é‚£ä¹ˆæ˜æ˜¾ã€‚é¦–å…ˆï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šå¸¸åˆ†å¸ƒåœ¨å¤šä¸ªGPUä¸Šï¼Œè¿™å¢åŠ äº†é¢å¤–çš„é€šä¿¡å¼€é”€ã€‚æ­£å¦‚æœ¬ç« 3.3.2èŠ‚åé¢æ‰€è®¨è®ºçš„ï¼Œå®ƒä»¬ä»¥å…¶ä»–æ–¹å¼åˆ†å¸ƒï¼Œæœ‰æ—¶ç”šè‡³ä¸ºäº†é™ä½å»¶è¿Ÿï¼Œä½†ä»»ä½•åˆ†å¸ƒéƒ½ä¼šå¢åŠ é¢å¤–çš„å¼€é”€è´Ÿæ‹…ã€‚æ­¤å¤–ï¼ŒLLMsçš„å»¶è¿Ÿä¸¥é‡å—å®Œæˆé•¿åº¦çš„å½±å“ï¼Œè¿™æ„å‘³ç€å®ƒä½¿ç”¨çš„å•è¯è¶Šå¤šï¼Œè¿”å›å“åº”æ‰€éœ€çš„æ—¶é—´å°±è¶Šé•¿ã€‚å½“ç„¶ï¼Œå®Œæˆé•¿åº¦ä¼¼ä¹ä¹Ÿèƒ½æé«˜å‡†ç¡®æ€§ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨åƒæ€ç»´é“¾ï¼ˆCoTï¼‰è¿™æ ·çš„æç¤ºå·¥ç¨‹æŠ€å·§ï¼Œæˆ‘ä»¬è¦æ±‚æ¨¡å‹ä»¥é€æ­¥çš„æ–¹å¼æ€è€ƒé—®é¢˜ï¼Œè¿™å·²è¢«è¯æ˜å¯ä»¥æ”¹å–„é€»è¾‘å’Œæ•°å­¦é—®é¢˜çš„ç»“æœï¼Œä½†ä¼šæ˜¾è‘—å¢åŠ å“åº”é•¿åº¦å’Œå»¶è¿Ÿæ—¶é—´ã€‚
- en: 3.2.4 Managing GPUs
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.4 ç®¡ç†GPU
- en: To help with these latency problems, we usually want to run them in GPUs. If
    we want to have any success training LLMs, weâ€™ll need GPUs for that as well, but
    this all adds additional challenges that many underestimate. Most web services
    and many ML use cases can be done solely on CPUs, but not so with LLMsâ€”partly
    because of GPUsâ€™ parallel processing capabilities offering a solution to our latency
    problems and partly because of the inherent optimization GPUs offer in the linear
    algebra, matrix multiplications, and tensor operations; thatâ€™s happening under
    the hood. For many who are stepping into the realm of LLMs, this requires utilizing
    a new resource and extra complexity. Many brazenly step into this world, acting
    like itâ€™s no big deal, but they are in for a rude awakening. Most system architectures
    and orchestrating tooling available, like Kubernetes, assume your application
    will run with CPU and memory alone. While they often support additional resources
    like GPUs, itâ€™s often an afterthought. Youâ€™ll soon find you have to rebuild containers
    from scratch and deploy new metric systems.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å¸®åŠ©è§£å†³è¿™äº›å»¶è¿Ÿé—®é¢˜ï¼Œæˆ‘ä»¬é€šå¸¸å¸Œæœ›å®ƒä»¬åœ¨GPUä¸Šè¿è¡Œã€‚å¦‚æœæˆ‘ä»¬æƒ³è¦æˆåŠŸè®­ç»ƒLLMsï¼Œæˆ‘ä»¬ä¹Ÿéœ€è¦GPUï¼Œä½†è¿™éƒ½å¢åŠ äº†è®¸å¤šä½ä¼°çš„é¢å¤–æŒ‘æˆ˜ã€‚å¤§å¤šæ•°ç½‘ç»œæœåŠ¡å’Œè®¸å¤šæœºå™¨å­¦ä¹ ç”¨ä¾‹å¯ä»¥ä»…ä½¿ç”¨CPUå®Œæˆï¼Œä½†å¯¹äºLLMsæ¥è¯´å¹¶éå¦‚æ­¤â€”â€”éƒ¨åˆ†åŸå› æ˜¯GPUçš„å¹¶è¡Œå¤„ç†èƒ½åŠ›ä¸ºæˆ‘ä»¬è§£å†³äº†å»¶è¿Ÿé—®é¢˜ï¼Œéƒ¨åˆ†åŸå› æ˜¯GPUåœ¨çº¿æ€§ä»£æ•°ã€çŸ©é˜µä¹˜æ³•å’Œå¼ é‡è¿ç®—ä¸­å›ºæœ‰çš„ä¼˜åŒ–ï¼›è¿™æ˜¯åœ¨å¹•åå‘ç”Ÿçš„ã€‚å¯¹äºè®¸å¤šåˆšåˆšè¿›å…¥LLMsé¢†åŸŸçš„äººæ¥è¯´ï¼Œè¿™éœ€è¦åˆ©ç”¨æ–°çš„èµ„æºå¹¶å¢åŠ é¢å¤–çš„å¤æ‚æ€§ã€‚è®¸å¤šäººé²è½åœ°è¿›å…¥è¿™ä¸ªä¸–ç•Œï¼Œè¡¨ç°å¾—å¥½åƒè¿™æ²¡ä»€ä¹ˆå¤§ä¸äº†çš„ï¼Œä½†ä»–ä»¬å°†é¢ä¸´ä¸€ä¸ªä»¤äººéœ‡æƒŠçš„è§‰é†’ã€‚å¤§å¤šæ•°ç³»ç»Ÿæ¶æ„å’Œå¯ç”¨çš„ç¼–æ’å·¥å…·ï¼Œå¦‚Kubernetesï¼Œéƒ½å‡è®¾åº”ç”¨ç¨‹åºå°†ä»…ä½¿ç”¨CPUå’Œå†…å­˜è¿è¡Œã€‚è™½ç„¶å®ƒä»¬é€šå¸¸æ”¯æŒé¢å¤–çš„èµ„æºï¼Œå¦‚GPUï¼Œä½†è¿™é€šå¸¸æ˜¯ä¸€ä¸ªäº‹åè€ƒè™‘ã€‚ä½ å¾ˆå¿«å°±ä¼šå‘ç°è‡ªå·±å¿…é¡»ä»å¤´å¼€å§‹é‡å»ºå®¹å™¨å¹¶éƒ¨ç½²æ–°çš„åº¦é‡ç³»ç»Ÿã€‚
- en: One aspect of managing GPUs that most companies are not prepared for is that
    they tend to be rare and limited. For the last decade, it seems that we have gone
    in and out of a global GPU shortage. They can be extremely difficult to provision
    for companies looking to stay on-premise. Weâ€™ve spent lots of time in our careers
    working with companies that chose to stay on-premise for a variety of reasons.
    One of the things they had in common is that they never had GPUs on their servers.
    When they did, they were often purposely difficult to access except for a few
    key employees.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ç®¡ç†GPUçš„ä¸€ä¸ªæ–¹é¢æ˜¯ï¼Œå¤§å¤šæ•°å…¬å¸éƒ½æ²¡æœ‰å‡†å¤‡å¥½ï¼Œé‚£å°±æ˜¯å®ƒä»¬å¾€å¾€å¾ˆç¨€ç¼ºä¸”æœ‰é™ã€‚åœ¨è¿‡å»åå¹´ä¸­ï¼Œæˆ‘ä»¬ä¼¼ä¹ä¸€ç›´åœ¨ç»å†å…¨çƒGPUçŸ­ç¼ºçš„èµ·ä¼ã€‚å¯¹äºæƒ³è¦ä¿æŒæœ¬åœ°éƒ¨ç½²çš„å…¬å¸æ¥è¯´ï¼ŒGPUçš„é…ç½®å¯èƒ½æå…¶å›°éš¾ã€‚åœ¨æˆ‘ä»¬çš„èŒä¸šç”Ÿæ¶¯ä¸­ï¼Œæˆ‘ä»¬èŠ±è´¹äº†å¤§é‡æ—¶é—´ä¸é‚£äº›å‡ºäºå„ç§åŸå› é€‰æ‹©ä¿æŒæœ¬åœ°éƒ¨ç½²çš„å…¬å¸åˆä½œã€‚ä»–ä»¬å…±åŒçš„ä¸€ç‚¹æ˜¯ï¼Œä»–ä»¬çš„æœåŠ¡å™¨ä¸Šä»æœªæœ‰è¿‡GPUã€‚å½“æœ‰GPUæ—¶ï¼Œå®ƒä»¬é€šå¸¸æ•…æ„éš¾ä»¥è®¿é—®ï¼Œåªæœ‰å°‘æ•°å…³é”®å‘˜å·¥æ‰èƒ½è®¿é—®ã€‚
- en: If you are lucky enough to be working in the cloud, a lot of these problems
    are solved, but there is no free lunch here either. Weâ€™ve both been part of teams
    that have often gone chasing their tails trying to help data scientists struggling
    to provision a new GPU workspace. Weâ€™ve run into obscure, ominous errors like
    `scale.up.error .out.of.resources`, only to discover that these esoteric readings
    indicate all the GPUs of a selected type in the entire region are being utilized,
    and none are available. CPU and memory can often be treated as infinite in a data
    center; GPU resources, however, cannot. Sometimes you canâ€™t expect them at all.
    Most data centers only support a subset of instance or GPU types, which means
    you may be forced to set up your application in a region further away from your
    user base, thus increasing latency. Of course, weâ€™re sure you can work with your
    cloud provider when looking to expand your service to a new region that doesnâ€™t
    currently support it, but you might not like what you hear based on timelines
    and cost. Ultimately, youâ€™ll run into shortage problems no matter where you choose
    to run, on-premise or in the cloud.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ è¶³å¤Ÿå¹¸è¿ï¼Œåœ¨äº‘ç«¯å·¥ä½œï¼Œè®¸å¤šè¿™äº›é—®é¢˜éƒ½å¾—åˆ°äº†è§£å†³ï¼Œä½†è¿™é‡Œä¹Ÿæ²¡æœ‰å…è´¹çš„åˆé¤ã€‚æˆ‘ä»¬ä¿©éƒ½æ›¾æ˜¯é‚£äº›ç»å¸¸è¿½é€å°¾å·´ã€è¯•å›¾å¸®åŠ©æ•°æ®ç§‘å­¦å®¶è§£å†³æ–°GPUå·¥ä½œåŒºé…ç½®é—®é¢˜çš„å›¢é˜Ÿçš„ä¸€å‘˜ã€‚æˆ‘ä»¬é‡åˆ°äº†è¯¸å¦‚`scale.up.error.out.of.resources`è¿™æ ·çš„æ™¦æ¶©ã€ä¸ç¥¥çš„é”™è¯¯ï¼Œç»“æœå‘ç°è¿™äº›ç¥ç§˜çš„è¯»æ•°è¡¨æ˜æ•´ä¸ªåŒºåŸŸæ‰€é€‰ç±»å‹çš„æ‰€æœ‰GPUéƒ½è¢«åˆ©ç”¨äº†ï¼Œæ²¡æœ‰ä¸€ä¸ªæ˜¯å¯ç”¨çš„ã€‚åœ¨æ•°æ®ä¸­å¿ƒï¼ŒCPUå’Œå†…å­˜é€šå¸¸å¯ä»¥è¢«è§†ä¸ºæ— é™çš„ï¼›ç„¶è€Œï¼ŒGPUèµ„æºå´ä¸èƒ½ã€‚æœ‰æ—¶ä½ ç”šè‡³å®Œå…¨æ— æ³•æœŸå¾…å®ƒä»¬ã€‚å¤§å¤šæ•°æ•°æ®ä¸­å¿ƒåªæ”¯æŒå®ä¾‹æˆ–GPUç±»å‹çš„ä¸€ä¸ªå­é›†ï¼Œè¿™æ„å‘³ç€ä½ å¯èƒ½è¢«è¿«åœ¨ä¸€ä¸ªç¦»ç”¨æˆ·ç¾¤æ›´è¿œçš„åœ°åŒºè®¾ç½®ä½ çš„åº”ç”¨ç¨‹åºï¼Œä»è€Œå¢åŠ å»¶è¿Ÿã€‚å½“ç„¶ï¼Œæˆ‘ä»¬ç¡®ä¿¡å½“ä½ æƒ³è¦å°†æœåŠ¡æ‰©å±•åˆ°ç›®å‰ä¸æ”¯æŒçš„æ–°åœ°åŒºæ—¶ï¼Œä½ å¯ä»¥ä¸ä½ çš„äº‘æœåŠ¡æä¾›å•†åˆä½œï¼Œä½†æ ¹æ®æ—¶é—´è¡¨å’Œæˆæœ¬ï¼Œä½ å¯èƒ½ä¸ä¼šå–œæ¬¢ä½ æ‰€å¬åˆ°çš„ã€‚æœ€ç»ˆï¼Œæ— è®ºä½ é€‰æ‹©åœ¨å“ªé‡Œè¿è¡Œï¼Œæ— è®ºæ˜¯åœ¨æœ¬åœ°è¿˜æ˜¯åœ¨äº‘ç«¯ï¼Œä½ éƒ½ä¼šé‡åˆ°çŸ­ç¼ºé—®é¢˜ã€‚
- en: 3.2.5 Peculiarities of text data
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.5 æ–‡æœ¬æ•°æ®çš„ç‰¹æ€§
- en: LLMs are the modern-day solution to NLP. NLP is one of the most fascinating
    branches of ML in general because it primarily deals with text data, which is
    primarily a qualitative measure. Every other field deals with quantitative data.
    We have figured out a way to encode our observations of the world into a direct
    translation of numerical values. For example, weâ€™ve learned how to encode heat
    into temperature scales and measure it with thermometers and thermocouples, and
    we can measure pressure with manometers and gauges and put it into pascals.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: LLMsæ˜¯ç°ä»£è§£å†³NLPé—®é¢˜çš„æ–¹æ¡ˆã€‚NLPæ˜¯æœºå™¨å­¦ä¹ ä¸­æœ€è¿·äººçš„åˆ†æ”¯ä¹‹ä¸€ï¼Œå› ä¸ºå®ƒä¸»è¦å¤„ç†æ–‡æœ¬æ•°æ®ï¼Œè¿™ä¸»è¦æ˜¯ä¸€ç§å®šæ€§åº¦é‡ã€‚å…¶ä»–æ‰€æœ‰é¢†åŸŸéƒ½å¤„ç†å®šé‡æ•°æ®ã€‚æˆ‘ä»¬å·²ç»æ‰¾åˆ°äº†ä¸€ç§æ–¹æ³•ï¼Œå°†æˆ‘ä»¬å¯¹ä¸–ç•Œçš„è§‚å¯Ÿç¼–ç æˆç›´æ¥çš„æ•°å€¼è½¬æ¢ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å·²ç»å­¦ä¼šäº†å¦‚ä½•å°†çƒ­é‡ç¼–ç åˆ°æ¸©åº¦å°ºåº¦ä¸­ï¼Œå¹¶ç”¨æ¸©åº¦è®¡å’Œçƒ­ç”µå¶æ¥æµ‹é‡å®ƒï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ç”¨å‹åŠ›è®¡å’Œå‹åŠ›è¡¨æ¥æµ‹é‡å‹åŠ›ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºå¸•æ–¯å¡ã€‚
- en: Computer vision and the practice of evaluating images are often seen as qualitative,
    but the actual encoding of images into numbers is a solved problem. Our understanding
    of light has allowed us to break images apart into pixels and assign them RGB
    values. Of course, this doesnâ€™t mean computer vision is by any means solved; thereâ€™s
    still lots of work to do to learn how to identify the different signals in the
    patterns of the data. Audio data is also often considered qualitative. How does
    one compare two songs? But we can measure sound and speech, directly measuring
    the sound waveâ€™s intensity in decibels and frequency in hertz.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—æœºè§†è§‰å’Œè¯„ä¼°å›¾åƒçš„å®è·µé€šå¸¸è¢«è§†ä¸ºå®šæ€§åˆ†æï¼Œä½†å°†å›¾åƒç¼–ç ä¸ºæ•°å­—çš„é—®é¢˜å·²ç»å¾—åˆ°äº†è§£å†³ã€‚æˆ‘ä»¬å¯¹å…‰çš„ç†è§£ä½¿æˆ‘ä»¬èƒ½å¤Ÿå°†å›¾åƒåˆ†è§£æˆåƒç´ ï¼Œå¹¶èµ‹äºˆå®ƒä»¬RGBå€¼ã€‚å½“ç„¶ï¼Œè¿™å¹¶ä¸æ„å‘³ç€è®¡ç®—æœºè§†è§‰åœ¨æŸç§ç¨‹åº¦ä¸Šå·²ç»è§£å†³äº†ï¼›ä»æœ‰å¤§é‡å·¥ä½œè¦åšï¼Œä»¥å­¦ä¹ å¦‚ä½•è¯†åˆ«æ•°æ®æ¨¡å¼ä¸­çš„ä¸åŒä¿¡å·ã€‚éŸ³é¢‘æ•°æ®ä¹Ÿå¸¸å¸¸è¢«è®¤ä¸ºå…·æœ‰å®šæ€§ã€‚ä¸€ä¸ªäººå¦‚ä½•æ¯”è¾ƒä¸¤é¦–æ­Œæ›²ï¼Ÿä½†æˆ‘ä»¬å¯ä»¥æµ‹é‡å£°éŸ³å’Œè¯­éŸ³ï¼Œç›´æ¥æµ‹é‡å£°æ³¢çš„å¼ºåº¦ï¼ˆåˆ†è´ï¼‰å’Œé¢‘ç‡ï¼ˆèµ«å…¹ï¼‰ã€‚
- en: Unlike other fields that encode our physical world into numerical data, text
    data is looking at ways to measure the ephemeral world. After all, text data is
    our best effort at encoding our thoughts, ideas, and communication patterns. While,
    yes, we have figured out ways to turn words into numbers, we havenâ€™t figured out
    a direct translation. Our best solutions to encode text and create embeddings
    are just approximations at best; in fact, we use machine learning models to do
    it! An interesting aside is that numbers are also text and a part of language.
    If we want models that are better at math, we need a more meaningful way to encode
    these numbers. Since itâ€™s all made up, when we try to encode text numbers into
    machine-readable numbers, we are creating a system attempting to reference itself
    recursively in a meaningful way. Not an easy problem to solve!
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å°†æˆ‘ä»¬çš„ç‰©ç†ä¸–ç•Œç¼–ç æˆæ•°å€¼æ•°æ®çš„å…¶ä»–é¢†åŸŸä¸åŒï¼Œæ–‡æœ¬æ•°æ®æ­£åœ¨å¯»æ‰¾è¡¡é‡çŸ­æš‚ä¸–ç•Œçš„æ–¹æ³•ã€‚æ¯•ç«Ÿï¼Œæ–‡æœ¬æ•°æ®æ˜¯æˆ‘ä»¬ç¼–ç æ€æƒ³ã€æƒ³æ³•å’Œäº¤æµæ¨¡å¼çš„æœ€ä½³å°è¯•ã€‚è™½ç„¶ï¼Œæ˜¯çš„ï¼Œæˆ‘ä»¬å·²ç»æ‰¾åˆ°äº†å°†å•è¯è½¬æ¢æˆæ•°å­—çš„æ–¹æ³•ï¼Œä½†æˆ‘ä»¬è¿˜æ²¡æœ‰æ‰¾åˆ°ç›´æ¥çš„ç¿»è¯‘ã€‚æˆ‘ä»¬ç¼–ç æ–‡æœ¬å’Œåˆ›å»ºåµŒå…¥çš„æœ€ä½³è§£å†³æ–¹æ¡ˆæœ€å¤šåªæ˜¯è¿‘ä¼¼ï¼›å®é™…ä¸Šï¼Œæˆ‘ä»¬ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹æ¥å®Œæˆè¿™é¡¹å·¥ä½œï¼ä¸€ä¸ªæœ‰è¶£çš„æ’æ›²æ˜¯ï¼Œæ•°å­—ä¹Ÿæ˜¯æ–‡æœ¬ï¼Œæ˜¯è¯­è¨€çš„ä¸€éƒ¨åˆ†ã€‚å¦‚æœæˆ‘ä»¬æƒ³è¦åœ¨æ•°å­¦æ–¹é¢åšå¾—æ›´å¥½çš„æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ç§æ›´æœ‰æ„ä¹‰çš„æ–¹å¼æ¥ç¼–ç è¿™äº›æ•°å­—ã€‚ç”±äºè¿™ä¸€åˆ‡éƒ½æ˜¯è™šæ„çš„ï¼Œå½“æˆ‘ä»¬å°è¯•å°†æ–‡æœ¬æ•°å­—ç¼–ç æˆæœºå™¨å¯è¯»çš„æ•°å­—æ—¶ï¼Œæˆ‘ä»¬æ­£åœ¨åˆ›å»ºä¸€ä¸ªè¯•å›¾ä»¥æœ‰æ„ä¹‰çš„æ–¹å¼é€’å½’å¼•ç”¨è‡ªå·±çš„ç³»ç»Ÿã€‚è¿™ä¸æ˜¯ä¸€ä¸ªå®¹æ˜“è§£å†³çš„é—®é¢˜ï¼
- en: Because of all this, LLMs (and all NLP solutions) have unique challenges. Take,
    for example, monitoring. How do you catch data drift in text data? How do you
    measure â€œcorrectnessâ€? How do you ensure the cleanliness of the data? These types
    of problems are difficult to define, let alone solve.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæ‰€æœ‰è¿™äº›ï¼ŒLLMï¼ˆä»¥åŠæ‰€æœ‰NLPè§£å†³æ–¹æ¡ˆï¼‰éƒ½æœ‰ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚ä»¥ç›‘æ§ä¸ºä¾‹ã€‚ä½ å¦‚ä½•åœ¨æ–‡æœ¬æ•°æ®ä¸­æ•æ‰æ•°æ®æ¼‚ç§»ï¼Ÿä½ å¦‚ä½•è¡¡é‡â€œæ­£ç¡®æ€§â€ï¼Ÿä½ å¦‚ä½•ç¡®ä¿æ•°æ®çš„æ¸…æ´æ€§ï¼Ÿè¿™äº›é—®é¢˜å¾ˆéš¾å®šä¹‰ï¼Œæ›´ä¸ç”¨è¯´è§£å†³äº†ã€‚
- en: 3.2.6 Token limits create bottlenecks
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.6 æ ‡è®°é™åˆ¶é€ æˆç“¶é¢ˆ
- en: 'A big challenge for those new to working with LLMs is dealing with the token
    limits. The token limit for a model is the maximum number of tokens that can be
    included as an input for a model. The larger the token limit, the more context
    we can give the model to improve its success at accomplishing the task. Everyone
    wants them to be higher, but itâ€™s not that simple. These token limits are defined
    by two problems: the memory and speed our GPUs have access to and the nature of
    memory storage in the models themselves.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºLLMæ–°ç”¨æˆ·æ¥è¯´ï¼Œä¸€ä¸ªå¾ˆå¤§çš„æŒ‘æˆ˜æ˜¯å¤„ç†æ ‡è®°é™åˆ¶ã€‚æ¨¡å‹çš„æ ‡è®°é™åˆ¶æ˜¯æŒ‡å¯ä»¥ä½œä¸ºæ¨¡å‹è¾“å…¥çš„æœ€å¤§æ ‡è®°æ•°é‡ã€‚æ ‡è®°é™åˆ¶è¶Šå¤§ï¼Œæˆ‘ä»¬å¯ä»¥æä¾›ç»™æ¨¡å‹çš„ä¸Šä¸‹æ–‡å°±è¶Šå¤šï¼Œä»è€Œæé«˜å…¶å®Œæˆä»»åŠ¡çš„æˆåŠŸç‡ã€‚æ¯ä¸ªäººéƒ½å¸Œæœ›å®ƒä»¬æ›´é«˜ï¼Œä½†è¿™å¹¶ä¸ç®€å•ã€‚è¿™äº›æ ‡è®°é™åˆ¶ç”±ä¸¤ä¸ªé—®é¢˜å®šä¹‰ï¼šæˆ‘ä»¬GPUå¯è®¿é—®çš„å†…å­˜å’Œé€Ÿåº¦ï¼Œä»¥åŠæ¨¡å‹è‡ªèº«å†…å­˜å­˜å‚¨çš„æ€§è´¨ã€‚
- en: 'The first problem seems unintuitive: Why couldnâ€™t we just increase the GPU
    memory? The answer is complex. We can, but stacking more layers in the GPU to
    take into account more gigabytes at once slows down the GPUâ€™s computational ability
    as a whole. Right now, GPU manufacturers are working on new architectures and
    ways to get around this problem. The second challenge is fascinating because increasing
    the token limits actually exacerbates the mathematical problems under the hood.
    Let me explain. Memory storage within an LLM itself isnâ€™t something we think about
    often. We call that mechanism *attention*, which we discussed in depth in section
    2.2.7\. What we didnâ€™t discuss was that attention is a quadratic solution: as
    the number of tokens increases, the number of calculations required to compute
    the attention scores between all the pairs of tokens in a sequence scales quadratically
    with the sequence length. In addition, within our gigantic context spaces, and
    since we are dealing with quadratics, weâ€™re starting to hit problems where the
    only solutions involve imaginary numbers, which can cause models to behave in
    unexpected ways. This is likely one of the reasons why LLMs hallucinate.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä¸ªé—®é¢˜çœ‹èµ·æ¥ä¸å¤ªç›´è§‚ï¼šä¸ºä»€ä¹ˆæˆ‘ä»¬ä¸èƒ½åªæ˜¯å¢åŠ GPUå†…å­˜å‘¢ï¼Ÿç­”æ¡ˆå¾ˆå¤æ‚ã€‚æˆ‘ä»¬å¯ä»¥ï¼Œä½†æ˜¯å°†æ›´å¤šå±‚å †å åœ¨GPUä¸Šä»¥ä¸€æ¬¡è€ƒè™‘æ›´å¤šçš„åƒå…†å­—èŠ‚ä¼šæ•´ä½“é™ä½GPUçš„è®¡ç®—èƒ½åŠ›ã€‚ç›®å‰ï¼ŒGPUåˆ¶é€ å•†æ­£åœ¨ç ”ç©¶æ–°çš„æ¶æ„å’Œç»•è¿‡è¿™ä¸ªé—®é¢˜çš„æ–¹æ³•ã€‚ç¬¬äºŒä¸ªæŒ‘æˆ˜éå¸¸æœ‰è¶£ï¼Œå› ä¸ºå¢åŠ æ ‡è®°é™åˆ¶å®é™…ä¸ŠåŠ å‰§äº†åº•å±‚çš„æ•°å­¦é—®é¢˜ã€‚è®©æˆ‘æ¥è§£é‡Šä¸€ä¸‹ã€‚åœ¨LLMå†…éƒ¨è¿›è¡Œå†…å­˜å­˜å‚¨å¹¶ä¸æ˜¯æˆ‘ä»¬ç»å¸¸è€ƒè™‘çš„äº‹æƒ…ã€‚æˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œæ³¨æ„åŠ›â€ï¼Œè¿™åœ¨ç¬¬2.2.7èŠ‚ä¸­æˆ‘ä»¬è¿›è¡Œäº†æ·±å…¥è®¨è®ºã€‚æˆ‘ä»¬æ²¡æœ‰è®¨è®ºçš„æ˜¯ï¼Œæ³¨æ„åŠ›æ˜¯ä¸€ä¸ªäºŒæ¬¡è§£å†³æ–¹æ¡ˆï¼šéšç€æ ‡è®°æ•°é‡çš„å¢åŠ ï¼Œè®¡ç®—åºåˆ—ä¸­æ‰€æœ‰æ ‡è®°å¯¹ä¹‹é—´çš„æ³¨æ„åŠ›åˆ†æ•°æ‰€éœ€çš„è®¡ç®—é‡ä¼šéšç€åºåˆ—é•¿åº¦çš„å¹³æ–¹è€Œå¢é•¿ã€‚æ­¤å¤–ï¼Œåœ¨æˆ‘ä»¬çš„å·¨å¤§ä¸Šä¸‹æ–‡ç©ºé—´ä¸­ï¼Œç”±äºæˆ‘ä»¬æ­£åœ¨å¤„ç†äºŒæ¬¡æ–¹ç¨‹ï¼Œæˆ‘ä»¬å¼€å§‹é‡åˆ°åªæœ‰æ¶‰åŠè™šæ•°æ‰èƒ½è§£å†³çš„éš¾é¢˜ï¼Œè¿™å¯èƒ½å¯¼è‡´æ¨¡å‹ä»¥æ„æƒ³ä¸åˆ°çš„æ–¹å¼è¡¨ç°ã€‚è¿™å¯èƒ½æ˜¯LLMäº§ç”Ÿå¹»è§‰çš„å…¶ä¸­ä¸€ä¸ªåŸå› ã€‚
- en: 'These problems have real implications and affect application designs. For example,
    when this authorâ€™s team upgraded from GPT-3 to GPT-4, the team was excited to
    have access to a higher token limit, but it soon found this led to longer inference
    times and, subsequently, a higher timeout error rate. In the real world, itâ€™s
    often better to get a less accurate response quickly than to get no response at
    all because the promise of a more accurate model often is just that: a promise.
    Of course, when deploying it locally, where you donâ€™t have to worry about response
    times, youâ€™ll likely find your hardware to be a limiting factor. For example,
    LLaMA was trained with 2,048 tokens, but youâ€™ll be lucky to take advantage of
    more than 512 of that when running with a basic consumer GPU, as you are likely
    to see out-of-memory (OOM) errors or even the model simply crashing.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›é—®é¢˜å…·æœ‰å®é™…å½±å“ï¼Œå¹¶å½±å“åº”ç”¨ç¨‹åºè®¾è®¡ã€‚ä¾‹å¦‚ï¼Œå½“ä½œè€…æ‰€åœ¨çš„å›¢é˜Ÿä»GPT-3å‡çº§åˆ°GPT-4æ—¶ï¼Œå›¢é˜Ÿå¾ˆé«˜å…´èƒ½å¤Ÿè®¿é—®æ›´é«˜çš„æ ‡è®°é™åˆ¶ï¼Œä½†å¾ˆå¿«å‘ç°è¿™å¯¼è‡´äº†æ›´é•¿çš„æ¨ç†æ—¶é—´ï¼Œéšåæ˜¯æ›´é«˜çš„è¶…æ—¶é”™è¯¯ç‡ã€‚åœ¨ç°å®ä¸–ç•Œä¸­ï¼Œå¿«é€Ÿå¾—åˆ°ä¸€ä¸ªä¸å¤ªå‡†ç¡®çš„å“åº”é€šå¸¸æ¯”å®Œå…¨å¾—ä¸åˆ°å“åº”è¦å¥½ï¼Œå› ä¸ºæ›´å‡†ç¡®æ¨¡å‹çš„æ‰¿è¯ºé€šå¸¸åªæ˜¯æ‰¿è¯ºã€‚å½“ç„¶ï¼Œå½“åœ¨æœ¬åœ°éƒ¨ç½²æ—¶ï¼Œæ‚¨ä¸å¿…æ‹…å¿ƒå“åº”æ—¶é—´ï¼Œæ‚¨å¯èƒ½ä¼šå‘ç°æ‚¨çš„ç¡¬ä»¶æ˜¯é™åˆ¶å› ç´ ã€‚ä¾‹å¦‚ï¼ŒLLaMAæ˜¯ç”¨2,048ä¸ªæ ‡è®°è®­ç»ƒçš„ï¼Œä½†æ‚¨åœ¨ä½¿ç”¨åŸºæœ¬çš„æ¶ˆè´¹çº§GPUè¿è¡Œæ—¶ï¼Œèƒ½åˆ©ç”¨çš„æœ€å¤šåªæœ‰512ä¸ªï¼Œæ‚¨å¯èƒ½ä¼šçœ‹åˆ°å†…å­˜ä¸è¶³ï¼ˆOOMï¼‰é”™è¯¯ï¼Œç”šè‡³æ¨¡å‹ç›´æ¥å´©æºƒã€‚
- en: A gotcha, which is likely to catch your team by surprise and should be pointed
    out now, is that different languages have different tokens per character. Take
    a look at table 3.1, where we compare converting the same sentence in different
    languages to tokens using OpenAIâ€™s cl100k_base Byte Pair Encoder. Just a quick
    glance reveals that LLMs typically favor the English language in this regard.
    In practice, this means that if you are building a chatbot with an LLM, your English
    users will have greater flexibility in their input space than Japanese users,
    leading to very different user experiences.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå¯èƒ½ä¼šè®©æ‚¨çš„å›¢é˜Ÿæ„Ÿåˆ°æ„å¤–çš„é—®é¢˜ï¼Œç°åœ¨åº”è¯¥æŒ‡å‡ºçš„æ˜¯ï¼Œä¸åŒè¯­è¨€çš„æ¯ä¸ªå­—ç¬¦çš„æ ‡è®°æ•°æ˜¯ä¸åŒçš„ã€‚çœ‹çœ‹è¡¨3.1ï¼Œæˆ‘ä»¬ä½¿ç”¨OpenAIçš„cl100k_baseå­—èŠ‚å¯¹ç¼–ç å™¨å°†ä¸åŒè¯­è¨€çš„ç›¸åŒå¥å­è½¬æ¢ä¸ºæ ‡è®°ã€‚åªéœ€ä¸€çœ¼å°±èƒ½çœ‹å‡ºï¼ŒLLMsé€šå¸¸åœ¨è¿™ä¸ªæ–¹é¢æ›´å€¾å‘äºè‹±è¯­ã€‚åœ¨å®è·µä¸­ï¼Œè¿™æ„å‘³ç€å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨LLMæ„å»ºèŠå¤©æœºå™¨äººï¼Œæ‚¨çš„è‹±è¯­ç”¨æˆ·çš„è¾“å…¥ç©ºé—´å°†æ¯”æ—¥è¯­ç”¨æˆ·æ›´çµæ´»ï¼Œä»è€Œå¯¼è‡´éå¸¸ä¸åŒçš„ç”¨æˆ·ä½“éªŒã€‚
- en: Table 3.1 Comparison of token counts in different languages
  id: totrans-40
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: è¡¨3.1ä¸åŒè¯­è¨€ä¸­æ ‡è®°è®¡æ•°æ¯”è¾ƒ
- en: '| Language | String | Characters | Tokens |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| è¯­è¨€ | å­—ç¬¦ä¸² | å­—ç¬¦ | æ ‡è®° |'
- en: '| --- | --- | --- | --- |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| English  | The quick brown fox jumps over the lazy dog  | 43  | 9  |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| è‹±è¯­ | The quick brown fox jumps over the lazy dog | 43 | 9 |'
- en: '| French  | Le renard brun rapide saute par-dessus le chien paresseux  | 57  |
    20  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| æ³•è¯­ | Le renard brun rapide saute par-dessus le chien paresseux | 57 | 20
    |'
- en: '| Spanish  | El rÃ¡pido zorro marrÃ³n salta sobre el perro perezoso  | 52  |
    22  |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| è¥¿ç­ç‰™è¯­ | El rÃ¡pido zorro marrÃ³n salta sobre el perro perezoso | 52 | 22 |'
- en: '| Japanese  | ç´ æ—©ã„èŒ¶è‰²ã®ã‚­ãƒ„ãƒãŒæ€ æƒ°ãªçŠ¬ã‚’é£›ã³è¶Šãˆã‚‹  | 20  | 36  |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| æ—¥è¯­ | ç´ æ—©ã„èŒ¶è‰²ã®ã‚­ãƒ„ãƒãŒæ€ æƒ°ãªçŠ¬ã‚’é£›ã³è¶Šãˆã‚‹ | 20 | 36 |'
- en: '| Chinese (simplified)  | æ•æ·çš„æ£•è‰²ç‹ç‹¸è·³è¿‡äº†æ‡’ç‹—  | 12  | 28  |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| ç®€ä½“ä¸­æ–‡ | æ•æ·çš„æ£•è‰²ç‹ç‹¸è·³è¿‡äº†æ‡’ç‹— | 12 | 28 |'
- en: If you are curious about why this is, it is due to text encodings, which are
    another peculiarity of working with text data, as discussed in the previous section.
    Consider table 3.2, where we show several different characters and their binary
    representations in UTF-8\. English characters can almost exclusively be represented
    with a single byte included in the original ASCII standard computers were originally
    built on, while most other characters require 3 or 4 bytes. Because it takes more
    memory, it also takes more token space.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨å¯¹æ­¤æ„Ÿåˆ°å¥½å¥‡ï¼Œè¿™æ˜¯å› ä¸ºæ–‡æœ¬ç¼–ç ï¼Œè¿™æ˜¯åœ¨ä¸Šä¸€èŠ‚ä¸­è®¨è®ºçš„ä¸æ–‡æœ¬æ•°æ®ä¸€èµ·å·¥ä½œçš„å¦ä¸€ä¸ªç‰¹æ€§ã€‚è€ƒè™‘è¡¨3.2ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å‡ ä¸ªä¸åŒçš„å­—ç¬¦åŠå…¶åœ¨UTF-8ä¸­çš„äºŒè¿›åˆ¶è¡¨ç¤ºã€‚è‹±è¯­å­—ç¬¦å‡ ä¹å¯ä»¥å®Œå…¨ç”¨åŸå§‹ASCIIæ ‡å‡†ä¸­çš„å•ä¸ªå­—èŠ‚è¡¨ç¤ºï¼Œè€Œå¤§å¤šæ•°å…¶ä»–å­—ç¬¦éœ€è¦3ä¸ªæˆ–4ä¸ªå­—èŠ‚ã€‚å› ä¸ºå®ƒéœ€è¦æ›´å¤šçš„å†…å­˜ï¼Œæ‰€ä»¥å®ƒä¹Ÿéœ€è¦æ›´å¤šçš„æ ‡è®°ç©ºé—´ã€‚
- en: Table 3.2 Comparison of byte lengths for different currency characters in UTF-8
  id: totrans-49
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: è¡¨3.2ä¸åŒè´§å¸å­—ç¬¦åœ¨UTF-8ä¸­çš„å­—èŠ‚é•¿åº¦æ¯”è¾ƒ
- en: '| Character | Binary UTF-8 | Hex UTF-8 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| å­—ç¬¦ | äºŒè¿›åˆ¶UTF-8 | åå…­è¿›åˆ¶UTF-8 |'
- en: '| --- | --- | --- |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| $  | 00100100  | 0x24  |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| $ | 00100100 | 0x24 |'
- en: '| Â£  | 11000010 10100011  | 0xc2 0xa3  |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| Â£ | 11000010 10100011 | 0xc2 0xa3 |'
- en: '| Â¥  | 11000010 10100101  | 0xc2 0xa5  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| Â¥ | 11000010 10100101 | 0xc2 0xa5 |'
- en: '| â‚   | 11100010 10000010 10100000  | 0xe2 0x82 0xa0  |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| â‚  | 11100010 10000010 10100000 | 0xe2 0x82 0xa0 |'
- en: '| ğŸ’°  | 11110000 10011111 10010010 10110000  | 0xf0 0x9f 0x92 0xb0  |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| ğŸ’° | 11110000 10011111 10010010 10110000 | 0xf0 0x9f 0x92 0xb0 |'
- en: Increasing the token limits has been an ongoing research question since the
    popularization of transformers, and there are some promising solutions still in
    the research phases, like recurrent memory transformers (RMT).[Â¹](#footnote-119)
    We can expect to continue to see improvements in the future, and hopefully, this
    will become naught but an annoyance.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªä»transformersçš„æ™®åŠä»¥æ¥ï¼Œå¢åŠ tokené™åˆ¶ä¸€ç›´æ˜¯ä¸€ä¸ªæŒç»­çš„ç ”ç©¶é—®é¢˜ï¼Œå¹¶ä¸”è¿˜æœ‰ä¸€äº›æœ‰å¸Œæœ›çš„è§£å†³æ–¹æ¡ˆä»å¤„äºç ”ç©¶é˜¶æ®µï¼Œä¾‹å¦‚å¾ªç¯è®°å¿†è½¬æ¢å™¨ï¼ˆRMTï¼‰ã€‚[Â¹](#footnote-119)
    æˆ‘ä»¬å¯ä»¥æœŸå¾…æœªæ¥ä¼šç»§ç»­çœ‹åˆ°æ”¹è¿›ï¼Œå¹¶ä¸”å¸Œæœ›è¿™æœ€ç»ˆåªä¼šæˆä¸ºä¸€ä¸ªå°éº»çƒ¦ã€‚
- en: 3.2.7 Hallucinations cause confusion
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.7 å¹»è§‰å¼•èµ·å›°æƒ‘
- en: 'So far, weâ€™ve been discussing some of the technical problems a team faces when
    deploying an LLM into a production environment, but nothing compares to the simple
    problem that LLMs tend to be wrong. They tend to be wrong a lot. *Hallucinations*
    is a term coined to describe occurrences when LLM models will produce correct-sounding
    results that are wrongâ€”for example, book references or hyperlinks that have the
    form and structure of what would be expected but are, nevertheless, completely
    made up. As a fun example, we asked for books on LLMs in production from the publisher,
    Manning (a book that doesnâ€™t exist yet since one author is still writing it).
    We were given the following suggestions: *Machine Learning Engineering in Production*
    by Mike Del Balso and Lucas ServeÃ©n, which could be found at [https://www.manning.com/books/machine-learning-engineering-in-production](https://www.manning.com/books/machine-learning-engineering-in-production),
    and *Deep Learning for Coders with Fastai and PyTorch* by Jeremy Howard and Sylvain
    Gugger, which could be found at [https://www.manning.com/books/deep-learning-for-coders-with-fastai-and-pytorch](https://www.manning.com/books/deep-learning-for-coders-with-fastai-and-pytorch).
    The first book is entirely made up. The second book is real; however, itâ€™s not
    published by Manning. In each case, the internet addresses are entirely made up.
    These URLs are actually very similar in format to what youâ€™d expect if you were
    browsing Manningâ€™s website, but they will return 404 errors if you visit them.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬ä¸€ç›´åœ¨è®¨è®ºå›¢é˜Ÿåœ¨å°†LLMéƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒæ—¶é¢ä¸´çš„ä¸€äº›æŠ€æœ¯é—®é¢˜ï¼Œä½†æ²¡æœ‰ä»€ä¹ˆé—®é¢˜èƒ½ä¸LLMå€¾å‘äºé”™è¯¯è¿™ä¸€ç®€å•é—®é¢˜ç›¸æå¹¶è®ºã€‚å®ƒä»¬å¾€å¾€çŠ¯å¾ˆå¤šé”™è¯¯ã€‚â€œå¹»è§‰â€æ˜¯ä¸€ä¸ªæœ¯è¯­ï¼Œç”¨æ¥æè¿°å½“LLMæ¨¡å‹ä¼šäº§ç”Ÿå¬èµ·æ¥æ­£ç¡®ä½†å®é™…ä¸Šé”™è¯¯çš„ç»“æœçš„æƒ…å†µâ€”â€”ä¾‹å¦‚ï¼Œä¹¦ç±å¼•ç”¨æˆ–å…·æœ‰é¢„æœŸå½¢å¼å’Œç»“æ„çš„è¶…é“¾æ¥ï¼Œä½†å´æ˜¯å®Œå…¨è™šæ„çš„ã€‚ä½œä¸ºä¸€ä¸ªæœ‰è¶£çš„ä¾‹å­ï¼Œæˆ‘ä»¬å‘å‡ºç‰ˆç¤¾Manningï¼ˆç”±äºä¸€ä½ä½œè€…ä»åœ¨æ’°å†™ï¼Œè¿™æœ¬ä¹¦è¿˜ä¸å­˜åœ¨ï¼‰è¯·æ±‚æœ‰å…³ç”Ÿäº§ä¸­LLMçš„ä¹¦ç±ã€‚æˆ‘ä»¬å¾—åˆ°äº†ä»¥ä¸‹å»ºè®®ï¼šMike
    Del Balsoå’ŒLucas ServeÃ©nåˆè‘—çš„ã€Šç”Ÿäº§ä¸­çš„æœºå™¨å­¦ä¹ å·¥ç¨‹ã€‹[https://www.manning.com/books/machine-learning-engineering-in-production](https://www.manning.com/books/machine-learning-engineering-in-production)ï¼Œä»¥åŠJeremy
    Howardå’ŒSylvain Guggeråˆè‘—çš„ã€Šä½¿ç”¨Fastaiå’ŒPyTorchçš„ç¼–ç è€…æ·±åº¦å­¦ä¹ ã€‹[https://www.manning.com/books/deep-learning-for-coders-with-fastai-and-pytorch](https://www.manning.com/books/deep-learning-for-coders-with-fastai-and-pytorch)ã€‚ç¬¬ä¸€æœ¬ä¹¦å®Œå…¨æ˜¯è™šæ„çš„ã€‚ç¬¬äºŒæœ¬ä¹¦æ˜¯çœŸå®çš„ï¼›ç„¶è€Œï¼Œå®ƒå¹¶éç”±Manningå‡ºç‰ˆã€‚åœ¨æ¯ç§æƒ…å†µä¸‹ï¼Œäº’è”ç½‘åœ°å€éƒ½æ˜¯è™šæ„çš„ã€‚è¿™äº›URLçš„æ ¼å¼å®é™…ä¸Šä¸ä½ æµè§ˆManningç½‘ç«™æ—¶é¢„æœŸçš„éå¸¸ç›¸ä¼¼ï¼Œä½†å¦‚æœä½ è®¿é—®å®ƒä»¬ï¼Œå°†ä¼šè¿”å›404é”™è¯¯ã€‚
- en: One of the most annoying aspects of hallucinations is that they are often surrounded
    by confident-sounding words. LLMs are terrible at expressing uncertainty, in large
    part because of the way they are trained. Consider the case â€œ2 + 2 =.â€ Would you
    prefer it to respond, â€œI think it is 4â€ or simply â€œ4â€? Most would prefer to get
    the correct â€œ4â€ back. This bias is built in, as models are often given rewards
    for being correct or at least sounding like it.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å¹»è§‰æœ€ä»¤äººçƒ¦æ¼çš„æ–¹é¢ä¹‹ä¸€æ˜¯å®ƒä»¬é€šå¸¸è¢«è‡ªä¿¡çš„è¯è¯­æ‰€åŒ…å›´ã€‚LLMåœ¨è¡¨è¾¾ä¸ç¡®å®šæ€§æ–¹é¢éå¸¸ç³Ÿç³•ï¼Œè¿™åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç”±äºå®ƒä»¬çš„è®­ç»ƒæ–¹å¼ã€‚è€ƒè™‘â€œ2 + 2 =â€çš„æƒ…å†µã€‚ä½ æ›´å¸Œæœ›å®ƒå›ç­”â€œæˆ‘è®¤ä¸ºå®ƒæ˜¯4â€è¿˜æ˜¯ç®€å•åœ°â€œ4â€ï¼Ÿå¤§å¤šæ•°äººæ›´å¸Œæœ›å¾—åˆ°æ­£ç¡®çš„â€œ4â€ã€‚è¿™ç§åè§æ˜¯å›ºæœ‰çš„ï¼Œå› ä¸ºæ¨¡å‹é€šå¸¸ä¼šå› ä¸ºæ­£ç¡®æˆ–è‡³å°‘å¬èµ·æ¥æ­£ç¡®è€Œå¾—åˆ°å¥–åŠ±ã€‚
- en: There are various explanations as to why hallucinations occur, but the most
    truthful answer is that we donâ€™t know if thereâ€™s just one cause. Itâ€™s likely a
    combination of several things; thus, there isnâ€™t a good fix for it yet. Nevertheless,
    being prepared to counter these inaccuracies and biases of the model is crucial
    to provide the best user experience for your product.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºå¹»è§‰å‘ç”Ÿçš„åŸå› ï¼Œæœ‰å„ç§è§£é‡Šï¼Œä½†æœ€çœŸå®çš„ç­”æ¡ˆæ˜¯ï¼Œæˆ‘ä»¬ä¸çŸ¥é“æ˜¯å¦åªæœ‰ä¸€ä¸ªåŸå› ã€‚å®ƒå¯èƒ½æ˜¯ç”±å‡ ä»¶äº‹æƒ…çš„ç»„åˆï¼›å› æ­¤ï¼Œç›®å‰è¿˜æ²¡æœ‰ä¸€ä¸ªå¥½çš„è§£å†³åŠæ³•ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå‡†å¤‡å¥½åº”å¯¹è¿™äº›æ¨¡å‹çš„é”™è¯¯å’Œä¸å‡†ç¡®æ€§æ˜¯è‡³å…³é‡è¦çš„ï¼Œä»¥ä¾¿ä¸ºä½ çš„äº§å“æä¾›æœ€ä½³çš„ç”¨æˆ·ä½“éªŒã€‚
- en: 3.2.8 Bias and ethical considerations
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.8 åè§å’Œä¼¦ç†è€ƒé‡
- en: Just as concerning as the model getting things wrong is when it gets things
    right in the worst possible wayâ€”for example, allowing it to encourage users to
    commit suicide,[Â²](#footnote-120) teaching users how to make a bomb,[Â³](#footnote-121)
    or participating in sexual fantasies involving children.[â´](#footnote-122) These
    are extreme examples, but prohibiting the model from answering such questions
    is undeniably vital to success.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are trained on vast amounts of text data, which is also their primary source
    of bias. Because weâ€™ve found that larger datasets are just as important as larger
    models in producing human-like results, most of these datasets have never truly
    been curated or filtered to remove harmful content, instead choosing to prioritize
    size and a larger collection. Cleaning the dataset is often seen as prohibitively
    expensive, requiring humans to go in and manually verify everything, but thereâ€™s
    a lot that could be done with simple regular expressions and other automated solutions.
    By processing these vast collections of content and learning the implicit human
    biases, these models will inadvertently perpetuate them. These biases range from
    sexism and racism to political preferences and can cause your model to inadvertently
    promote negative stereotypes and discriminatory language.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.9 Security concerns
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As with all technology, we need to be mindful of security. LLMs have been trained
    on a large corpus of text, some of which could be harmful or sensitive and shouldnâ€™t
    be exposed. So steps should be taken to protect this data from being leaked. The
    bias and ethical concerns from the last section are good examples of conversations
    you donâ€™t want your users to be having, but you could also imagine finetuning
    a model on your companyâ€™s data and potentially having secrets lost inadvertently
    if proper precautions arenâ€™t taken.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: One should be aware that LLMs are susceptible to adversarial attacks like prompt
    injections. Prompt injections are attacks done by a user to trick the LLM into
    ignoring instructions given to it and generating undesired content. For example,
    if you ask ChatGPT what its gender is, it appropriately replies that as an AI
    language model, it doesnâ€™t have a gender. However, with clever prompting, you
    may be able to bypass these protocols and get it to reveal one. While this example
    is harmless, weâ€™ve seen others successfully extract API keys and other secrets
    from an LLM, run code in nonprotected environments, steal environment variables,
    and traverse local file systems where the model is served. Thatâ€™s not to mention
    the plethora of examples of users using prompting to jailbreak or bypass protocols
    put in place for ethical considerations outlined in the previous section. An interesting
    aside to this is that LLMs are good at inventing fake secrets! Even successful
    prompt injection attacks can often fail due to LLM hallucinations, which can have
    funny consequences.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: For example, using a simple prompt to ask for Coca-Colaâ€™s secret recipe simply
    returned the boring result, â€œI cannot divulge trade secrets or any proprietary
    information.â€ Pretty lame if you ask us. With a little trickery and some prompt
    injection, we were able to get it to spill its secrets, which, of course, we decided
    to tryâ€”after confirming with a food scientist it wouldnâ€™t poison us. After following
    the instructions exactly, we were surprised to find we got a pretty tasty drink,
    but it tasted nothing like Coke. While the recipe looks legitimate, nuances like
    quantities, cook times, and even the need to strain the syrup were all off. Guess
    weâ€™ll have to hold off taking over the beverage industry until we get the real
    recipe.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œä½¿ç”¨ç®€å•çš„æç¤ºè¯·æ±‚å¯å£å¯ä¹çš„ç§˜æ–¹ä»…ä»…è¿”å›äº†æ— èŠçš„ç»“æœï¼Œâ€œæˆ‘ä¸èƒ½æ³„éœ²å•†ä¸šæœºå¯†æˆ–ä»»ä½•ä¸“æœ‰ä¿¡æ¯ã€‚â€å¦‚æœæˆ‘ä»¬é—®æ‚¨ï¼Œè¿™çœŸæ˜¯å¤ªæ— èŠäº†ã€‚é€šè¿‡ä¸€ç‚¹å°è¯¡è®¡å’Œä¸€äº›æç¤ºæ³¨å…¥ï¼Œæˆ‘ä»¬è®¾æ³•è®©å®ƒæ³„éœ²äº†ç§˜å¯†ï¼Œå½“ç„¶ï¼Œæˆ‘ä»¬åœ¨å°è¯•ä¹‹å‰ç¡®è®¤äº†ä¸€ä½é£Ÿå“ç§‘å­¦å®¶ï¼Œå®ƒä¸ä¼šè®©æˆ‘ä»¬ä¸­æ¯’ã€‚æŒ‰ç…§æŒ‡ç¤ºä¸¥æ ¼æ“ä½œåï¼Œæˆ‘ä»¬æƒŠè®¶åœ°å‘ç°æˆ‘ä»¬å¾—åˆ°äº†ä¸€æ¯ç›¸å½“ç¾å‘³çš„é¥®æ–™ï¼Œä½†å®ƒå°èµ·æ¥æ ¹æœ¬ä¸åƒå¯å£å¯ä¹ã€‚è™½ç„¶é…æ–¹çœ‹èµ·æ¥å¾ˆåˆæ³•ï¼Œä½†åƒæ•°é‡ã€çƒ¹é¥ªæ—¶é—´ï¼Œç”šè‡³éœ€è¦è¿‡æ»¤ç³–æµ†è¿™æ ·çš„ç»†å¾®å·®åˆ«éƒ½ä¸å¯¹ã€‚çœ‹æ¥æˆ‘ä»¬å¾—ç­‰åˆ°å¾—åˆ°çœŸæ­£çš„é…æ–¹åå†è€ƒè™‘æ¥ç®¡é¥®æ–™è¡Œä¸šã€‚
- en: Coca-Cola recipe hallucinated from prompt injection
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: å¯å£å¯ä¹é…æ–¹é€šè¿‡æç¤ºæ³¨å…¥äº§ç”Ÿçš„å¹»è§‰
- en: '[PRE0]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Another security concern involves pickle injections. Pickle is a library in
    Python that serializes objects and is often used to serialize ML models. It serializes
    them into a byte stream containing opcodes executed one by one as it is deserialized.
    Itâ€™s a fast and easy way to share large objects. Pickle injections corrupt this
    byte stream, often injecting malware over the wire when the model is transferred
    over an insecure network. This is especially concerning for large models that
    take a long time to download, as it makes it easier for a third party to intercept
    the transfer and inject malicious code. If this happens, the code injected can
    potentially give the attackers access to your system. This can happen when attempting
    to use the model during inference, as the harmful code will execute if it is not
    detected and properly removed. It is important to take precautions such as using
    secure networks and verifying the integrity of the model before use to prevent
    this type of attack.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªå®‰å…¨é—®é¢˜æ˜¯è…Œèœæ³¨å…¥ã€‚è…Œèœæ˜¯Pythonä¸­çš„ä¸€ä¸ªåº“ï¼Œç”¨äºåºåˆ—åŒ–å¯¹è±¡ï¼Œé€šå¸¸ç”¨äºåºåˆ—åŒ–æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚å®ƒåœ¨ååºåˆ—åŒ–æ—¶å°†å¯¹è±¡åºåˆ—åŒ–ä¸ºåŒ…å«é€ä¸ªæ‰§è¡Œçš„æŒ‡ä»¤çš„å­—èŠ‚æµã€‚è¿™æ˜¯ä¸€ç§å¿«é€Ÿä¸”ç®€å•çš„æ–¹å¼å…±äº«å¤§å‹å¯¹è±¡ã€‚è…Œèœæ³¨å…¥ä¼šç ´åè¿™ä¸ªå­—èŠ‚æµï¼Œé€šå¸¸åœ¨æ¨¡å‹é€šè¿‡ä¸å®‰å…¨çš„ç½‘ç»œä¼ è¾“æ—¶æ³¨å…¥æ¶æ„è½¯ä»¶ã€‚è¿™å¯¹äºä¸‹è½½æ—¶é—´è¾ƒé•¿çš„å¤§å‹æ¨¡å‹å°¤å…¶ä»¤äººæ‹…å¿§ï¼Œå› ä¸ºè¿™ä½¿å¾—ç¬¬ä¸‰æ–¹æ›´å®¹æ˜“æ‹¦æˆªä¼ è¾“å¹¶æ³¨å…¥æ¶æ„ä»£ç ã€‚å¦‚æœå‘ç”Ÿè¿™ç§æƒ…å†µï¼Œæ³¨å…¥çš„ä»£ç å¯èƒ½ä¼šè®©æ”»å‡»è€…è·å¾—å¯¹ç³»ç»Ÿçš„è®¿é—®æƒé™ã€‚è¿™å¯èƒ½åœ¨å°è¯•åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä½¿ç”¨æ¨¡å‹æ—¶å‘ç”Ÿï¼Œå› ä¸ºå¦‚æœæœ‰å®³ä»£ç æœªè¢«æ£€æµ‹å¹¶æ­£ç¡®ç§»é™¤ï¼Œå®ƒå°†æ‰§è¡Œã€‚å› æ­¤ï¼Œåœ¨ä½¿ç”¨æ¨¡å‹ä¹‹å‰é‡‡å–é¢„é˜²æªæ–½ï¼Œå¦‚ä½¿ç”¨å®‰å…¨ç½‘ç»œå’ŒéªŒè¯æ¨¡å‹çš„å®Œæ•´æ€§ï¼Œä»¥é˜²æ­¢æ­¤ç±»æ”»å‡»ï¼Œæ˜¯éå¸¸é‡è¦çš„ã€‚
- en: 3.2.10 Controlling costs
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.10 æ§åˆ¶æˆæœ¬
- en: Working with LLMs involves various cost-related concerns. The first, as you
    probably gathered by now, is infrastructure costs, which include high-performance
    GPUs, storage, and other hardware resources. We talked about how GPUs are harder
    to procure, which, unfortunately, means they are more expensive. Mistakes like
    leaving your service on have always had the potential to rack up the bills, but
    with GPUs in the mix, this type of mistake is even more deadly. These models also
    demand significant computational power, leading to high energy consumption during
    both training and inference. On top of all this, their longer deploy times mean
    we are often running them even during low traffic to handle bursty workloads or
    anticipated future traffic. Overall, this leads to higher operational costs.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸€èµ·å·¥ä½œæ¶‰åŠå„ç§ä¸æˆæœ¬ç›¸å…³çš„é—®é¢˜ã€‚é¦–å…ˆï¼Œä½ å¯èƒ½å·²ç»æ³¨æ„åˆ°äº†ï¼Œè¿™æ˜¯åŸºç¡€è®¾æ–½æˆæœ¬ï¼ŒåŒ…æ‹¬é«˜æ€§èƒ½GPUã€å­˜å‚¨å’Œå…¶ä»–ç¡¬ä»¶èµ„æºã€‚æˆ‘ä»¬è®¨è®ºäº†GPUé‡‡è´­éš¾åº¦è¾ƒå¤§ï¼Œä¸å¹¸çš„æ˜¯ï¼Œè¿™ä¹Ÿæ„å‘³ç€å®ƒä»¬æ›´æ˜‚è´µã€‚åƒç•™ä¸‹ä½ çš„æœåŠ¡è¿™æ ·çš„é”™è¯¯ä¸€ç›´éƒ½æœ‰ç´¯ç§¯è´¦å•çš„æ½œåŠ›ï¼Œä½†ä¸GPUæ··åˆåœ¨ä¸€èµ·ï¼Œè¿™ç§é”™è¯¯ç”šè‡³æ›´å…·ç ´åæ€§ã€‚è¿™äº›æ¨¡å‹è¿˜è¦æ±‚å¤§é‡çš„è®¡ç®—èƒ½åŠ›ï¼Œå¯¼è‡´åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­æ¶ˆè€—å¤§é‡èƒ½æºã€‚é™¤æ­¤ä¹‹å¤–ï¼Œå®ƒä»¬çš„éƒ¨ç½²æ—¶é—´æ›´é•¿ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬ç»å¸¸åœ¨ä½æµé‡æœŸé—´è¿è¡Œå®ƒä»¬ï¼Œä»¥å¤„ç†çªå‘çš„å·¥ä½œè´Ÿè½½æˆ–é¢„æœŸçš„æœªæ¥æµé‡ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™å¯¼è‡´äº†æ›´é«˜çš„è¿è¥æˆæœ¬ã€‚
- en: Additional costs include managing and storing vast amounts of data used to train
    or finetune as well as for regular maintenance, such as model updates, security
    measures, and bug fixes, which can be financially demanding. As with any technology
    used for business purposes, managing potential legal disputes and ensuring compliance
    with regulations is a concern. Lastly, investing in continuous research and development
    to improve your models and give you a competitive edge will be a factor.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: é¢å¤–çš„æˆæœ¬åŒ…æ‹¬ç®¡ç†å’Œå­˜å‚¨ç”¨äºè®­ç»ƒæˆ–å¾®è°ƒä»¥åŠå¸¸è§„ç»´æŠ¤çš„å¤§é‡æ•°æ®ï¼Œä¾‹å¦‚æ¨¡å‹æ›´æ–°ã€å®‰å…¨æªæ–½å’Œé”™è¯¯ä¿®å¤ï¼Œè¿™å¯èƒ½å¯¹è´¢åŠ¡é€ æˆå‹åŠ›ã€‚ä¸ä»»ä½•ç”¨äºå•†ä¸šç›®çš„çš„æŠ€æœ¯ä¸€æ ·ï¼Œç®¡ç†æ½œåœ¨çš„è¯‰è®¼å’Œç¡®ä¿ç¬¦åˆæ³•è§„ä¹Ÿæ˜¯ä¸€ä¸ªé—®é¢˜ã€‚æœ€åï¼ŒæŠ•èµ„äºæŒç»­çš„ç ”ç©¶å’Œå¼€å‘ä»¥æ”¹è¿›æ‚¨çš„æ¨¡å‹å¹¶ç»™æ‚¨å¸¦æ¥ç«äº‰ä¼˜åŠ¿ï¼Œå°†æ˜¯ä¸€ä¸ªå› ç´ ã€‚
- en: We talked a bit about the technical concerns regarding token limits, which are
    likely to be solved, but we didnâ€™t discuss the cost limitations, as most APIs
    charge on a token basis. This makes it more expensive to send more context and
    use better prompts. It also makes it a bit harder to predict costs since while
    you can standardize inputs, you canâ€™t standardize outputs. You can never be too
    sure how many tokens will be returned, making it difficult to govern. Just remember,
    with LLMs, it is as important as ever to implement and follow proper cost engineering
    practices to ensure costs never get away from you.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è°ˆäº†ä¸€äº›å…³äºä»¤ç‰Œé™åˆ¶çš„æŠ€æœ¯é—®é¢˜ï¼Œè¿™äº›é—®é¢˜å¯èƒ½ä¼šå¾—åˆ°è§£å†³ï¼Œä½†æˆ‘ä»¬æ²¡æœ‰è®¨è®ºæˆæœ¬é™åˆ¶ï¼Œå› ä¸ºå¤§å¤šæ•°APIæŒ‰ä»¤ç‰Œæ”¶è´¹ã€‚è¿™ä½¿å¾—å‘é€æ›´å¤šä¸Šä¸‹æ–‡å’Œä½¿ç”¨æ›´å¥½çš„æç¤ºå˜å¾—æ›´åŠ æ˜‚è´µã€‚è¿™ä¹Ÿä½¿å¾—é¢„æµ‹æˆæœ¬å˜å¾—æœ‰ç‚¹å›°éš¾ï¼Œå› ä¸ºè™½ç„¶ä½ å¯ä»¥æ ‡å‡†åŒ–è¾“å…¥ï¼Œä½†ä½ ä¸èƒ½æ ‡å‡†åŒ–è¾“å‡ºã€‚ä½ æ°¸è¿œä¸èƒ½å¤ªç¡®å®šä¼šè¿”å›å¤šå°‘ä»¤ç‰Œï¼Œè¿™ä½¿å¾—ç®¡ç†å˜å¾—å›°éš¾ã€‚è®°ä½ï¼Œå¯¹äºLLMæ¥è¯´ï¼Œå®æ–½å’Œéµå¾ªé€‚å½“çš„æˆæœ¬å·¥ç¨‹å®è·µä»¥ç¡®ä¿æˆæœ¬æ°¸è¿œä¸ä¼šå¤±æ§ï¼Œè¿™å’Œä»¥å‰ä¸€æ ·é‡è¦ã€‚
- en: 3.3 LLMOps essentials
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 LLMOpsåŸºæœ¬è¦ç´ 
- en: Now that we have a handle on the type of challenge we are grappling with, letâ€™s
    take a look at all the different LLMOps practices, tooling, and infrastructure
    to see how different components help us overcome these obstacles. First, letâ€™s
    dive into different practices, starting with compression, where we will talk about
    shrinking, trimming, and approximating to get models as small as we can. We will
    then talk about distributed computing, which is needed to make things run since
    the models are so large that they rarely fit into a single GPUâ€™s memory. After
    we are finished with that, we will venture into the infrastructure and tooling
    needed to make it all happen in the next section.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»æŒæ¡äº†æˆ‘ä»¬æ­£åœ¨åº”å¯¹çš„æŒ‘æˆ˜ç±»å‹ï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹æ‰€æœ‰ä¸åŒçš„LLMOpså®è·µã€å·¥å…·å’ŒåŸºç¡€è®¾æ–½ï¼Œçœ‹çœ‹ä¸åŒçš„ç»„ä»¶å¦‚ä½•å¸®åŠ©æˆ‘ä»¬å…‹æœè¿™äº›éšœç¢ã€‚é¦–å…ˆï¼Œè®©æˆ‘ä»¬æ·±å…¥æ¢è®¨ä¸åŒçš„å®è·µï¼Œä»å‹ç¼©å¼€å§‹ï¼Œæˆ‘ä»¬å°†è®¨è®ºç¼©å°ã€ä¿®å‰ªå’Œè¿‘ä¼¼ï¼Œä»¥ä½¿æ¨¡å‹å°½å¯èƒ½å°ã€‚ç„¶åæˆ‘ä»¬å°†è®¨è®ºåˆ†å¸ƒå¼è®¡ç®—ï¼Œè¿™æ˜¯å¿…éœ€çš„ï¼Œå› ä¸ºæ¨¡å‹å¦‚æ­¤ä¹‹å¤§ï¼Œå¾ˆå°‘èƒ½é€‚åº”å•ä¸ªGPUçš„å†…å­˜ã€‚å®Œæˆè¿™äº›åï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚ä¸­æ¢è®¨å®ç°è¿™ä¸€åˆ‡æ‰€éœ€çš„åŸºç¡€è®¾æ–½å’Œå·¥å…·ã€‚
- en: 3.3.1 Compression
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.1 å‹ç¼©
- en: As you were reading about the challenges of LLMs in the last section, you might
    have asked yourself something akin to â€œIf the biggest problems from LLMs come
    from their size, why donâ€™t we just make them smaller?â€ If you did, congratulations!
    You are a geniusâ€”compression is the practice of doing just that. Compressing models
    to as small as we can make them will improve deployment time, reduce latency,
    scale down the number of expensive GPUs needed, and, ultimately, save money. However,
    the whole point of making the models so stupefyingly gargantuan in the first place
    was because it made them better at what they do. We need to be able to shrink
    them without losing all the progress we made by making them big in the first place.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šä¸€èŠ‚ä¸­ï¼Œå½“ä½ é˜…è¯»å…³äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŒ‘æˆ˜æ—¶ï¼Œä½ å¯èƒ½è‡ªå·±é—®è¿‡ç±»ä¼¼çš„é—®é¢˜ï¼šâ€œå¦‚æœLLMçš„æœ€å¤§é—®é¢˜æ¥è‡ªå®ƒä»¬çš„å¤§å°ï¼Œä¸ºä»€ä¹ˆæˆ‘ä»¬ä¸æŠŠå®ƒä»¬åšå¾—æ›´å°ï¼Ÿâ€å¦‚æœä½ è¿™æ ·æƒ³è¿‡ï¼Œæ­å–œä½ ï¼ä½ æ˜¯ä¸ªå¤©æ‰â€”â€”å‹ç¼©å°±æ˜¯åšè¿™ä»¶äº‹çš„å®è·µã€‚å°†æ¨¡å‹å‹ç¼©åˆ°å°½å¯èƒ½å°ï¼Œå°†æé«˜éƒ¨ç½²æ—¶é—´ï¼Œå‡å°‘å»¶è¿Ÿï¼Œå‡å°‘æ‰€éœ€æ˜‚è´µGPUçš„æ•°é‡ï¼Œå¹¶æœ€ç»ˆèŠ‚çœèµ„é‡‘ã€‚ç„¶è€Œï¼Œæœ€åˆè®©æ¨¡å‹å˜å¾—å¦‚æ­¤åºå¤§æ— æ¯”çš„æ•´ä¸ªç›®çš„ï¼Œæ˜¯å› ä¸ºå®ƒä½¿å®ƒä»¬åœ¨æ‰€åšçš„äº‹æƒ…ä¸Šå˜å¾—æ›´å¥½ã€‚æˆ‘ä»¬éœ€è¦èƒ½å¤Ÿåœ¨ä¸å¤±å»æˆ‘ä»¬é€šè¿‡ä½¿å®ƒä»¬å˜å¤§è€Œå–å¾—çš„å…¨éƒ¨è¿›æ­¥çš„æƒ…å†µä¸‹ç¼©å°å®ƒä»¬ã€‚
- en: This problem is far from solved, but there are multiple ways to approach the
    problem, with different pros and cons to each method. Weâ€™ll be talking about several
    of the methods, starting with the easiest and most effective.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªé—®é¢˜è¿œæœªè§£å†³ï¼Œä½†æœ‰å¤šç§æ–¹æ³•å¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ¯ç§æ–¹æ³•éƒ½æœ‰å…¶ä¼˜ç¼ºç‚¹ã€‚æˆ‘ä»¬å°†è®¨è®ºå‡ ç§æ–¹æ³•ï¼Œä»æœ€ç®€å•å’Œæœ€æœ‰æ•ˆçš„æ–¹æ³•å¼€å§‹ã€‚
- en: Quantizing
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å®šé‡
- en: Quantizing is the process of reducing precision in preference of lowering the
    memory requirements. This tradeoff makes intuitive sense. When this author was
    in college, he was taught to always round numbers to the precision of the tooling.
    Pulling out a ruler and measuring his pencil, you wouldnâ€™t believe him if he stated
    the length was 19.025467821973739 cm. Even if he used a caliper, he couldnâ€™t verify
    a number so precisely. With our ruler, any number beyond 19.03 cm is fantasy.
    To drive the point home, one of his engineering professors once asked him him,
    â€œIf you are measuring the height of a skyscraper, do you care if there is an extra
    sheet of paper at the top?â€
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'How we represent numbers inside computers often leads us to believe we have
    better precision than we actually do. To illustrate this point, open a Python
    terminal and add 0.1 + 0.2\. If youâ€™ve never tried this before, you might be surprised
    to find it doesnâ€™t equal 0.3, but 0.30000000000000004\. We wonâ€™t go into the details
    of the math behind this phenomenon, but the question stands: Can we reduce the
    precision without making things worse? We really only need precision to the tenth
    decimal, but reducing the precision will likely get us a number like 0.304 rather
    than 0.300, thus increasing our margin of error.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, the only numbers a computer understands are 0 and 1, on or off,
    a single bit. To improve this range, we combine multiple bits and assign them
    different meanings. String 8 of them together, and you get a byte. Using the INT8
    standard, we can take that byte and encode all the integers from â€“128 to 127\.
    Weâ€™ll spare you the particulars because we assume you already know how binary
    works; suffice it to say, the more bits we have, the larger range of numbers we
    can represent, both larger and smaller. Figure 3.1 shows a few common floating
    point encodings. With 32 bits strung together, we get what we pretentiously term
    *full precision*, and that is how most numbers are stored, including the weights
    in machine learning models. Basic quantization moves us from full precision to
    half precision, shrinking models to half their size. There are two different half
    precision standards, FP16 and BF16, which differ in how many bits represent the
    range or exponent part. Since BF16 uses the same number of exponents as FP32,
    itâ€™s been found to be more effective for quantizing, and you can generally expect
    almost exactly the same level of accuracy for half the size of model. If you understood
    the paper and skyscraper analogy, it should be obvious why.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-1.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1 The bit mapping for a few common floating point encodings: 16-bit
    float or half precision (FP16), bfloat 16 (BF16), 32-bit float or single full
    precision (FP32), and NVIDIAâ€™s TensorFloat (TF32)'
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: However, thereâ€™s no reason to stop there. We can often push it down another
    byte to 8-bit formats without too much loss of accuracy. There have already even
    been successful research attempts showing selective 4-bit quantization of portions
    of LLMs is possible with only a fractional loss of accuracy. The selective application
    of quantization is a process known as dynamic quantization and is usually done
    on just the weights, leaving the activations in full precision to reduce accuracy
    loss.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: The holy grail of quantizing, though, is INT2, representing every number as
    â€“1, 0, or 1\. This currently isnâ€™t possible without completely degrading the model,
    but it would make the model up to 8 times smaller. The Bloom model would be a
    measly ~40 GB, small enough to fit on a single GPU. This is, of course, as far
    as quantizing can take us, and if we wanted to shrink further, weâ€™d need to look
    at additional methods.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: The best part of quantization is that it is easy to do. There are many frameworks
    that allow this, but in listing 3.1, we demonstrate how to use PyTorchâ€™s quantization
    library to do a simple post-training static quantization (PTQ). All you need is
    the full precision model, some example inputs, and a validation dataset to prepare
    and calibrate with. As you can see, itâ€™s only a few lines of code.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.1 Example PTQ in PyTorch
  id: totrans-90
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 A deep copy of the original model as quantization is done in place'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Gets mappings; note the use of â€œqnnpackâ€ for ARM and â€œfbgemmâ€ for x86 CPU'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Prepares'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Calibrates; youâ€™ll want to use representative (validation) data'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Quantizes'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Static PTQ is the most straightforward approach to quantizing; it is done after
    the model is trained and uniformly quantizes all the model parameters. As with
    most formulas, the most straightforward approach introduces more error. Often,
    this error is acceptable, but when itâ€™s not, we can add extra complexity to reduce
    the accuracy loss from quantization. Some methods to consider are uniform versus
    non-uniform, static versus dynamic, symmetric versus asymmetric, and applying
    it during or after training.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: To understand these methods, letâ€™s consider the case where we are quantizing
    from FP32 to INT8\. In FP32, we essentially have the full range of numbers at
    our disposal, but in INT8, we only have 256 values. We are trying to put a genie
    into a bottle, and itâ€™s no small feat. If you study the weights in your model,
    you might notice that most of the numbers are fractions between [â€“1, 1]. We could
    take advantage of this by using an 8-bit standard that represents more values
    in this region in a non-uniform way instead of the standard uniform [â€“128, 127].
    While mathematically possible, unfortunately, any such standards arenâ€™t commonplace,
    and modern-day deep learning hardware and software are not designed to take advantage
    of them. So for now, itâ€™s best to stick to uniform quantization.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: The simplest approach to shrinking the data is to normalize it, but since we
    are going from a continuous scale to a discrete scale, there are a few gotchas,
    so letâ€™s explore those. We start by taking the min and max and scale them down
    to match our new number range. We would then bucket all the other numbers based
    on where they fall. Of course, if we have really large outliers, we may find all
    our other numbers squeezed into just one or two buckets, ruining any granularity
    we once had. To prevent this, we can clip any large numbers; this is what we do
    in static quantization. However, before we clip the data, what if we choose a
    range and scale that captures the majority of our data beforehand? We need to
    be careful since if this dynamic range is too small, we will introduce more clipping
    errors; if itâ€™s too big, we will introduce more rounding errors. The goal of dynamic
    quantization is, of course, to reduce both errors.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need to consider the symmetry of the data. Generally, in normalization,
    we force the data to be normal and thus symmetric; however, we could choose to
    scale the data in a way that leaves any asymmetry it had. By doing this, we could
    potentially reduce our overall loss due to the clipping and rounding errors, but
    itâ€™s not guaranteed.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: As a last resort, if none of these other methods fail to reduce the accuracy
    loss of the model, we can use quantization-aware training (QAT). QAT is a simple
    process where we add a fake quantization step during model training. By fake,
    we mean we clip and round the data while leaving it in full precision. This allows
    the model to adjust for the error and bias introduced by quantization while itâ€™s
    training. QAT is known to produce higher accuracy compared to other methods but
    at a much higher cost in time to train.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Quantization methods
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Uniform versus non-uniform*â€”Whether we use an 8-bit standard that is uniform
    in the range it represents or non-uniform to be more precise in the -1 to 1 range.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Static versus dynamic*â€”Choosing to adjust the range or scale before clipping
    in an attempt to reduce clipping and rounding errors and reduce data loss.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Symmetric versus asymmetric*â€”Normalizing the data to be normal and force symmetry
    or choosing to keep any asymmetry and skew.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*During or after training*â€”Quantization after training is really easy to do,
    and while doing it during training is more work, it leads to reduced bias and
    better results.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantizing is a very powerful tool. It reduces the size of the model and the
    computational overhead required to run the model, thus reducing the latency and
    cost of running the model. However, the best thing about quantization is that
    it can be done after the fact, so you donâ€™t have to worry about whether your data
    scientists remembered to quantize the model during training using processes like
    QAT. This is why quantization has become so popular when working with LLMs and
    other large machine learning models. While reduced accuracy is always a concern
    with compression techniques, compared to other methods, quantization is a win-win-win.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Pruning
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Congratulations, you just trained a brand new LLM! With billions of parameters,
    all of them must be useful, right? Wrong! Unfortunately, as with most things in
    life, the modelâ€™s parameters tend to follow the Pareto principle. About 20% of
    the weights lead to 80% of the value. â€œIf thatâ€™s true,â€ you may be asking yourself,
    â€œwhy donâ€™t we just cut out all the extra fluff?â€ Great idea! Give yourself a pat
    on the back. Pruning is the process of weeding out and removing any parts of the
    model we deem unworthy.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'There are essentially two different pruning methods: *structured* and *unstructured*.
    Structured pruning is the process of finding structural components of a model
    that arenâ€™t contributing to the modelâ€™s performance and then removing themâ€”whether
    they are filters, channels, or layers in the neural network. The advantage of
    this method is that your model will be a little smaller but keep the same basic
    structure, which means we donâ€™t have to worry about losing hardware efficiencies.
    We are also guaranteed a latency improvement, as there will be fewer computations
    involved.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Unstructured pruning, on the other hand, shifts through the parameters and zeros
    out the less important ones that donâ€™t contribute much to the modelâ€™s performance.
    Unlike structured pruning, we donâ€™t actually remove any parameters; we just set
    them to zero. From this, we can imagine that a good place to start would be any
    weights or activations already close to 0\. Of course, while this effectively
    reduces the size of a model, this also means we donâ€™t cut out any computations,
    so itâ€™s common to see only minimal, if any, latency improvement. But a smaller
    model still means faster load times and fewer GPUs to run. It also gives us very
    fine-grained control over the process, allowing us to shrink a model further than
    we could with structured pruning, with less effect on performance too.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Like quantization, pruning can be done after a model is trained. However, unlike
    quantization, itâ€™s common practice to see additional finetuning needed to prevent
    too great a loss of performance. Itâ€™s becoming more common to include pruning
    steps during the model training to avoid the need to finetune later on. Since
    a more sparse model will have fewer parameters to tune, adding these pruning steps
    may help a model converge faster as well.[âµ](#footnote-123)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Youâ€™ll be surprised at how much you can shrink a model with pruning while minimally
    affecting performance. How much? In the SparseGPT[â¶](#footnote-124) paper, a method
    was developed to try to automatically one-shot the pruning process without the
    need for finetuning afterward. The authors found they could decrease a GPT-3 model
    by 50% to 60% without a problem! Depending on the model and task, they even saw
    slight improvements in a few of them. We are looking forward to seeing where pruning
    takes us in the future.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge distillation
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Knowledge distillation is probably the coolest method of compression in our
    minds. Itâ€™s a simple idea too: weâ€™ll take the large LLM and have it train a smaller
    language model to copy it. Whatâ€™s nice about this method is that the larger LLM
    provides essentially an infinite dataset for the smaller model to train on, which
    can make the training quite effective. Because the larger the dataset, the better
    the performance, weâ€™ve often seen smaller models reach almost the same level as
    their teacher counterparts in accuracy.[â·](#footnote-125)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: A smaller model trained this way is guaranteed to both be smaller and improve
    latency. The downside is that it will require us to train a completely new model,
    which is a pretty significant upfront cost to pay. Any future improvements to
    the teacher model will require being passed down to the student model, which can
    lead to complex training cycles and version structure. Itâ€™s definitely a lot more
    work compared to some of the other compression methods.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: The hardest part about knowledge distillation, though, is that we donâ€™t really
    have good recipes for them yet. Tough questions like â€œHow small can the student
    model be?â€ will have to be solved through trial and error. Thereâ€™s still a lot
    to learn and research to be done here.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: However, there has been some exciting work in this field via Stanfordâ€™s Alpaca.[â¸](#footnote-126)
    Instead of training a student model from scratch, they chose to finetune the open
    source LLaMA 7B parameter model using OpenAIâ€™s GPT3.5â€™s 175B parameter model as
    a teacher via knowledge distillation. Itâ€™s a simple idea, but it paid off big,
    as they were able to get great results from their evaluation. The biggest surprise
    was the cost, as they only spent $500 on API costs to get the training data from
    the teacher model and $100 worth of GPU training time to finetune the student
    model. Granted, if you did this for a commercial application, youâ€™d be violating
    OpenAIâ€™s terms of service, so itâ€™s best to stick to using your own or open source
    models as the teacher.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Low-rank approximation
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Low-rank approximation, also known as low-rank factorization, low-rank decomposition,
    or matrix factorization, among other terms (too many namesâ€”we blame the mathematicians),
    uses linear algebra math tricks to simplify large matrices or tensors to find
    a lower-dimensional representation. There are several techniques to do this. Singular
    value decomposition (SVD), Tucker decomposition (TD), and canonical polyadic decomposition
    (CPD) are the most common ones you run into.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: In figure 3.2, we show the general idea behind the SVD method. Essentially,
    we are going to take a very large matrix, A, and break it up into three smaller
    matrices, U, S, and V. While U and V are there to ensure we keep the same dimensions
    and relative strengths of the original matrix, S allows us to apply a direction
    and bias. The smaller S is, the more we end up compressing and reducing the total
    number of parameters, but the less accurate the approximation becomes.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-2.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2 Example of SVD, a low-rank approximation. A is a large matrix with
    dimensions N and M. We can approximate it with three smaller matrices: U with
    dimensions M and P, S a square matrix with dimension P, and V with dimensions
    N and P (here we show the transpose). Usually, both P<<M and P<<N are true.'
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To solidify this concept, it may help to see a concrete example. In the next
    listing, we show a simple example of SVD at work compressing a â€¦ matrix. For this,
    we only need the basic libraries SciPy and NumPy, which are imported on lines
    1 and 2\. In line 3, we define the matrix, and then in line 9, we apply SVD to
    it.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.2 Example of SVD low-rank approximation
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The generated text is
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Taking a moment to inspect U, Sigma, and the transpose of V, we see a â€¦ matrix,
    a â€¦ matrix, and a â€¦ matrix, respectively. All in all, we now only need 9 parameters
    versus the original 16, shrinking the memory footprint almost by half.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we multiply the matrices back together to get an approximation of the
    original matrix. In this case, the approximation isnâ€™t all that great, but we
    can still see that the general order and magnitudes match the original matrix:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The generated text is
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Unfortunately, we are not aware of anyone actually using this to compress models
    in production, most likely due to the poor accuracy of the approximation. What
    they are using it forâ€”and this is importantâ€”is adaptation and finetuning, which
    is where low-rank adaptation (LoRA)[â¹](#footnote-127) comes in. Adaptation is
    the process of finetuning a generic or base model to do a specific task. LoRA
    applies SVD low-rank approximation to the attention weights or, rather, to inject
    update matrices that run parallel to the attention weights, allowing us to finetune
    a much smaller model. LoRA has become very popular because it makes it a breeze
    to take an LLM, shrink the trainable layers to a tiny fraction of the original
    model, and then allow anyone to train it on commodity hardware. You can get started
    with LoRA using the PEFT library from Hugging Face, where you can check out several
    LoRA tutorials.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE â€ƒFor the extra curious, parameter-efficient finetuning (PEFT) is a class
    of methods aimed at finetuning models in a computationally efficient way. The
    PEFT library seeks to put them all in one easy-to-access place; you can get started
    here: [https://huggingface.co/docs/peft](https://huggingface.co/docs/peft).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Mixture of experts
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Mixture of experts (MoE) is a technique where we replace the feed-forward layers
    in a transformer with MoE layers instead. Feed-forward layers are notorious for
    being parameter-dense and computationally intensive, so replacing them with something
    better can often have a large effect. MoEs are a group of sparsely activated models.
    They differ from ensemble techniques in that typically only one or a few expert
    models will be run, rather than combining results from all models. The sparsity
    is often induced by a gate mechanism that learns which experts to use and/or a
    router mechanism that determines which experts should even be consulted. In figure
    3.3, we demonstrate the MoE architecture with potentially N experts, as well as
    show where it goes inside a decoder stack.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-3.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 Example mixture of an expert's model with both a gate and router
    to control flow. The MoE model is used to replace the FFN layers in a transformer;
    here, we show it replacing the FFN in a decoder.
  id: totrans-139
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Depending on how many experts you have, the MoE layer could potentially have
    more parameters than the FFN, leading to a larger model, but in practice, this
    isnâ€™t the case, since engineers and researchers are aiming to create a smaller
    model. What we are guaranteed to see, though, is a faster computation path and
    improved inference times. However, what really makes MoE stand out is when itâ€™s
    combined with quantization. One study[^(10)](#footnote-128) between Microsoft
    and NVIDIA showed that 2-bit quantization was reachable with only a minimal effect
    on accuracy using MoE!
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Of course, since this is a pretty big change to the modelâ€™s structure, finetuning
    will be required afterward. You should also be aware that MoE layers often reduce
    a modelâ€™s generalizability, so itâ€™s best when used on models designed for a specific
    task. There are several libraries that implement MoE layers, but we recommend
    checking out DeepSpeed.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: NOTE â€ƒDeepSpeed is a library that optimizes many of the hard parts for large-scale
    deep learning models like LLMs and is particularly useful when training. Check
    out their MoE tutorial at [https://www.deepspeed.ai/tutorials/mixture-of-experts/](https://www.deepspeed.ai/tutorials/mixture-of-experts/).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2 Distributed computing
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Distributed computing is a technique used in deep learning to parallelize and
    speed up large, complex neural networks by dividing the workload across multiple
    devices or nodes in a cluster. This approach significantly reduces training and
    inference times by enabling concurrent computation, data parallelism, and model
    parallelism. With the ever-growing size of datasets and complexity of models,
    distributed computing has become crucial for deep learning workflows, ensuring
    efficient resource utilization and enabling researchers to iterate on their models
    effectively. Distributed computing is one of the core practices that separate
    deep learning from machine learning, and with LLMs, we have to pull out every
    trick in the book. Letâ€™s look at different parallel processing practices to take
    full advantage of distributed computing.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Data parallelism
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Data parallelism is what most people think about when they think about running
    processes in parallel; itâ€™s also the easiest to do. The practice involves splitting
    up the data and running them through multiple copies of the model or pipeline.
    For most frameworks, this is easy to set up; for example, in PyTorch, you can
    use the `DistributedDataParallel` method. Thereâ€™s just one catch for most of these
    setups: your model has to be able to fit onto one GPU. This is where a tool like
    Ray.io comes in.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Ray.io, or Ray, is an open source project designed for distributed computing,
    specifically aimed at parallel and cluster computing. Itâ€™s a flexible and user-friendly
    tool that simplifies distributed programming and helps developers easily execute
    concurrent tasks in parallel. Ray is primarily built for machine learning and
    other high-performance applications but can be utilized in other applications.
    In listing 3.3, we give a simple example of using Ray to distribute a task. The
    beauty of Ray is the simplicityâ€”all we need to do to make our code run in parallel
    is add a decorator. It sure beats the complexity of multithreading or asynchronization
    setups.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.3 Example Ray parallelization task
  id: totrans-148
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 Starts Ray'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Î”efines a regular Python function'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Turns the function into a Ray task'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Executes the slow function without Ray (takes 10 seconds)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Executes the slow function with Ray (takes 1 second)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Ray uses the concepts of tasks and actors to manage distributed computing. Tasks
    are functions, whereas actors are stateful objects that can be invoked and run
    concurrently. When you execute tasks using Ray, it distributes tasks across the
    available resources (e.g., multicore CPUs or multiple nodes in a cluster). For
    LLMs, we would need to set up a Ray cluster in a cloud environment, as this would
    allow each pipeline to run on a node with as many GPUs as needed, greatly simplifying
    the infrastructure set up to run LLMs in parallel.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE â€ƒLearn more about Ray clusters here: [https://mng.bz/eVJP](https://mng.bz/eVJP).'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple alternatives out there, but Ray has been gaining a lot of
    traction and becoming more popular as more and more machine learning workflows
    require distributed training. Teams have had great success with it. By utilizing
    Ray, developers can ensure better performance and more efficient utilization of
    resources in distributed workflows.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Tensor parallelism
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Tensor parallelism takes advantage of matrix multiplication properties to split
    up the activations across multiple processors, running the data through, and then
    combining them on the other side of the processors. Figure 3.4 demonstrates how
    this process works for a matrix, which can be parallelized in two separate ways
    that give us the same result. Imagine that Y is a really big matrix that canâ€™t
    fit on a single processor or, more likely, a bottleneck in our data flow that
    takes too much time to run all the calculations. In either case, we could split
    Y by columns or rows, run the calculations, and then combine the results. In this
    example, we are dealing with matrices, but in reality, we often deal with tensors
    with more than two dimensions. However, the same mathematical principles that
    make this work still apply.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-4.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 Tensor parallelism example showing that you can break up tensors
    by different dimensions and get the same end result. Here, we compare column and
    row parallelism of a matrix.
  id: totrans-161
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Choosing which dimension to parallelize is a bit of an art, but there are a
    few things to remember to help make this decision easier. First, how many columns
    or rows do you have? In general, you want to pick a dimension that has more than
    the number of processors you have, or you will end up stopping short. Generally,
    this isnâ€™t a problem, but with tools like Ray, discussed in the last section,
    parallelizing in a cluster and spinning up loads of processes is a breeze. Second,
    different dimensions have different multiplicity costs. For example, column parallelism
    requires us to send the entire dataset to each process but with the benefit of
    concatenating them together at the end, which is fast and easy. Row parallelism,
    however, allows us to break up the dataset into chunks but requires us to add
    the results, a more expensive operation than concatenating. You can see that one
    operation is more I/O bound, while the other is more computation bound. Ultimately,
    the best dimension will be dataset dependent and hardware limited. It will require
    experimentation to optimize this fully, but a good default is to just choose the
    largest dimension.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Tensor parallelism allows us to split up the heavy computation layers like MLP
    and attention layers onto different devices, but it doesnâ€™t help us with normalization
    or dropout layers that donâ€™t utilize tensors. To get better overall performance
    of our pipeline, we can add sequence parallelism that targets these blocks.[^(11)](#footnote-129)
    Sequence parallelism is a process that partitions activations along the sequence
    dimension, preventing redundant storage, and can be mixed with tensor parallelism
    to achieve significant memory savings with minimal additional computational overhead.
    In combination, they reduce the memory needed to store activations in transformer
    models. In fact, they nearly eliminate activation recomputation and save activation
    memory up to five times.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.5 shows how combining tensor parallelism, which allows us to distribute
    the computationally heavy layers, and sequence parallelism, which does the same
    for the memory limiting layers, allows us to fully parallelize the entire transformer
    model. Together, they allow for extremely efficient use of resources.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-5.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 Combining tensor parallelism that focuses on computational heavy
    layers with sequence parallelism to reduce memory overhead to create a fully parallel
    process for the entire transformer
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Pipeline parallelism
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: So far, we can run lots of data and speed up any bottlenecks, but none of that
    matters because our model is too big; we canâ€™t fit it into a single GPUâ€™s memory
    to even get it to run. Thatâ€™s where pipeline parallelism comes in; itâ€™s the process
    of splitting up a model vertically and putting each part onto a different GPU.
    This creates a pipeline, as input data will go to the first GPU, process, then
    transfer to the next GPU, and so on until itâ€™s run through the entire model. While
    other parallelism techniques improve our processing power and speed up inference,
    pipeline parallelism is required to get it to run. However, it comes with some
    major downsides, mainly device utilization.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: To understand where this downside comes from and how to mitigate it, letâ€™s first
    consider the naive approach to this, where we simply run all the data at once
    through the model. We find that this leaves a giant â€œbubbleâ€ of underutilization.
    Since the model is broken up, we have to process everything sequentially through
    the devices. This means that while one GPU is processing, the others are sitting
    idle. In figure 3.6, we can see this naive approach and a large bubble of inactivity
    as the GPUs sit idle. We also see a better way to take advantage of each device.
    We do this by sending the data in small batches. A smaller batch allows the first
    GPU to pass on what it was working on quicker and move on to another batch. This
    allows the next device to get started earlier and reduces the size of the bubble.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-6.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 The bubble problem. When data runs through a broken-up model, the
    GPUs holding the model weights are underutilized while they wait for their counterparts
    to process the data. A simple way to reduce this bubble is to use microbatching.
  id: totrans-171
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We can actually calculate the size of the bubble quite easily with the following
    formula:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Idle Percentage = 1 â€“ m / (m + n â€“ 1)
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: where m is the number of microbatches and n is the depth of the pipeline or
    number of GPUs. So for our naive example case of four GPUs and one large batch,
    we see the devices sitting idle 75% of the time! GPUs are quite expensive to allow
    to sit idle three quarters of the time. Letâ€™s see what that looks like using the
    microbatch strategy. With a microbatch of 4, it cuts this almost in half, down
    to just 43% of the time. We can glean from this formula that the more GPUs we
    have, the higher the idle times, but the more microbatches, the better the utilization.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, we often can neither reduce the number of GPUs nor make the
    microbatches as large as we want. There are limits. For GPUs, we have to use as
    many as it takes to fit the model into memory. However, try to use a few larger
    GPUs, as this will lead to more optimal utilization than using many smaller GPUs.
    Reducing the bubble in pipeline parallelism is another reason why compression
    is so important. For microbatching, the first limit is obvious: since the microbatch
    is a fraction of our batch size, we are limited by how big that is. The second
    is that each microbatch increases the memory demand for cached activations in
    a linear relationship. One way to counter this higher memory demand is a method
    called PipeDream.[^(12)](#footnote-130) There are different configurations and
    approaches, but the basic idea is the same. In this method, we start working on
    the backward pass as soon as weâ€™ve finished the forward pass of any of the microbatches.
    This allows us to fully complete a training cycle and release the cache for that
    microbatch.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 3D parallelism
  id: totrans-176
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For LLMs, we are going to want to take advantage of all three parallelism practices,
    as they can all be run together. This is known as 3D parallelism, which combines
    data, tensor, and pipeline parallelism (DP + TP + PP) together. Since each technique
    and, thus, dimension will require at least two GPUs to run 3D parallelism, weâ€™ll
    need at least eight GPUs to get started. How we configure these GPUs will be important
    to get the most efficiency out of this process. Because TP has the largest communication
    overhead, we want to ensure these GPUs are close together, preferably on the same
    node and machine. PP has the least communication volume of the three, so breaking
    up the model across nodes is the least expensive here.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: By running the three together, we see some interesting interactions and synergies
    between them. Since TP splits the model to work well within a deviceâ€™s memory,
    we see that PP can perform well even with small batch sizes due to the reduced
    effective batch size enabled by TP. This combination also improves the communication
    between DP nodes at different pipeline stages, allowing DP to work effectively
    too. The communication bandwidth between nodes is proportional to the number of
    pipeline stages. Consequently, DP can scale well even with smaller batch sizes.
    Overall, we see that when running in combination, we get better performance than
    when we run them individually.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know some tricks of the trade, itâ€™s just as important to have the
    right tools to do the job.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 LLM operations infrastructure
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are finally going to start talking about the infrastructure needed to make
    this all work. This likely comes as a surprise, as we know that some readers would
    have expected this section at the beginning of chapter 1\. Why wait till the end
    of chapter 3? In the many times weâ€™ve interviewed machine learning engineers,
    we have often asked this open-ended question: â€œWhat can you tell me about MLOps?â€
    An easy softball question to get the conversation going. Most junior candidates
    would immediately start jumping into the tooling and infrastructure. It makes
    sense; there are so many different tools available. Thatâ€™s not to mention the
    fact that whenever you see posts or blogs describing MLOps, thereâ€™s a pretty little
    diagram showing the infrastructure. While all of that is important, itâ€™s useful
    to recognize what a more senior candidate jumps intoâ€”the machine learning life
    cycle.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: For many, the nuance is lost, but the infrastructure is the *how*, and the life
    cycle is the *why*. Most companies can get by with just bare-bones infrastructure.
    Weâ€™ve seen our share of scrappy systems that exist entirely on one data scientistâ€™s
    laptop, and they work surprisingly wellâ€”especially in the era of scikit-learn
    everything!
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, a rickshaw machine learning platform doesnâ€™t cut it in the world
    of LLMs. Since we still live in a world where the standard storage capacity of
    a MacBook Pro laptop is 256 GB, just storing the model locally can already be
    a problem. Companies that invest in a sturdier infrastructure are better prepared
    for the world of LLMs.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: In figure 3.7, we see an example MLOps infrastructure designed with LLMs in
    mind. While most infrastructure diagrams simplify the structure to make everything
    look clean, the raw truth is that thereâ€™s a bit more complexity to the entire
    system. Of course, a lot of this complexity would disappear if we could get data
    scientists to work inside scripts instead of ad hoc workstationsâ€”usually with
    a Jupyter Notebook interface.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-7.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 A high level view of an MLOps infrastructure with LLMs in mind. This
    diagram attempts to cover the full picture and the complexity of the many tools
    involved to make ML models work in production.
  id: totrans-186
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Taking a closer look at figure 3.7, you can see several tools on the outskirts
    that squarely land in DataOps or even just DevOpsâ€”data stores, orchestrators,
    pipelines, streaming integrations, and container registries. These are tools you
    are likely already using for just about any data-intensive application and arenâ€™t
    necessarily focused on MLOps. Toward the center, we have more traditional MLOps
    toolsâ€”experiment trackers, model registry, feature store, and ad hoc data science
    workstations. For LLMs, we really only introduce one new tool to the stack: a
    vector database. Whatâ€™s not pictured is the monitoring system because it intertwines
    with every piece. This all culminates into what we are working toward in this
    bookâ€”a deployment service where we can confidently deploy and run LLMs in production.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure by discipline
  id: totrans-188
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The following list defines infrastructure by the specific discipline:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '*DevOps*â€”In charge of procuring the environmental resources: experimental (dev,
    staging) and production. This includes hardware, clusters, and networking to make
    it all work. Also in charge of basic infrastructure systems like Github/Gitlab,
    artifact registries, container registries, application or transactional databases
    like Postgres or MySQL, caching systems, and CI/CD pipelines. This list is by
    no means comprehensive.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*DataOps*â€”In charge of data, in motion and at rest. It includes centralized
    or decentralized data stores like data warehouses, data lakes, and data meshes,
    as well as data pipelines, either in batch systems or in streaming systems with
    tools like Kafka and Flink. It also includes orchestrators like Airflow, Prefect,
    and Mage. DataOps is built on top of DevOps. For example, weâ€™ve seen many CI/CD
    pipelines being used for data pipeline work until eventually graduating to systems
    like Apache Spark or DBT.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*MLOps*â€”In charge of the machine learning life cycle, from the creation of
    models to deprecation. This includes data science workstations like JupyterHub,
    experiment trackers, and a model registry. It includes specialty databases like
    feature stores and vector databases, as well as a deployment service to tie everything
    together and actually serve results. It is built on top of both DataOps and DevOps.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Letâ€™s go through each piece of the infrastructure puzzle and discuss features
    you should consider when thinking about LLMs in particular. While we will be discussing
    specialized tooling for each piece, weâ€™ll note that there are also MLOps as a
    service platform, like Dataiku, Amazonâ€™s SageMaker, Azure Machine Learning, and
    Googleâ€™s VertexAI. These platforms attempt to complete the whole puzzle; how well
    they do that is another question. However, they are often a great shortcut, and
    you should be aware of them. Well, thatâ€™s enough dillydallying; letâ€™s dive in
    already!
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.1 Data infrastructure
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While not the focus of this book, itâ€™s important to note that MLOps is built
    on top of a data operations infrastructure, which itself is built on top of DevOps.
    Key features of the DataOps ecosystem include a data store, an orchestrator, and
    pipelines. Additional features usually required include a container registry and
    a streaming integration service.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Data stores are the foundation of DataOps and come in many forms, from a simple
    database to large data warehouses and from even larger data lakes to an intricate
    data mesh. This is where your data is stored, and a lot of work goes into managing,
    governing, and securing the data store. The orchestrator is the cornerstone of
    DataOps, as itâ€™s a tool that manages and automates both simple and complex multistep
    workflows and tasks, ensuring they run across multiple resources and services
    in a system. The most commonly talked about are Airflow, Prefect, and Mage. Lastly,
    pipelines are the pillars. They hold everything else up and are where we run our
    jobs. Initially built to simply move, clean, and define data, these same systems
    are used to run machine learning training jobs on a schedule and do batch inference
    and loads of other work needed to ensure MLOps runs smoothly.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: A container registry is a keystone of DevOps and, subsequently, DataOps and
    MLOps. Running all our pipelines and services in containers is necessary to ensure
    consistency. Streaming services are a much bigger beast than what we may let on
    in this chapter, and if you know, you know. Thankfully, for most text-related
    tasks, real-time processing isnâ€™t a major concern. Even for tasks like real-time
    captioning or translation, we can often get by with some sort of pseudoâ€“real-time
    processing strategy that doesnâ€™t degrade the user experience depending on the
    task.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.2 Experiment trackers
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Experiment trackers are central to MLOps. Experiment trackers do the fundamental
    job of keeping track and recording tests and results. As the famous Adam Savage
    quote from *Myth Busters* states, â€œRemember, kids, the only difference between
    screwing around and science is writing it down.â€ Without it, your organization
    is likely missing the â€œscienceâ€ in data science, which is honestly quite embarrassing.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Even if your data scientists are keen to manually track and record results in
    notebooks, it might as well be thrown in the garbage if itâ€™s not easy for others
    to view and search for. This is the real purpose of experiment trackersâ€”to ensure
    knowledge is easily shared and made available. Eventually, a model will make it
    to production, and that model is going to have problems. Sure, you can always
    just train a new model, but unless the team is able to go back and investigate
    what went wrong the first time, you are likely to repeat the same mistakes over
    and over.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: There are many experiment trackers out there; the most popular by far is MLFlow,
    which is open source. It was started by the team at Databricks, which also offers
    an easy hosting solution. Some paid alternatives worth checking out include CometML
    and Weights & Biases.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Experiment trackers nowadays come with so many bells and whistles. Most open
    source and paid solutions will certainly have what you need when looking to scale
    up your needs for LLMOps. However, ensuring you take advantage of these tools
    correctly might require a few small tweaks. For example, the default assumption
    is usually that you are training a model from scratch, but often when working
    with LLMs, you will be finetuning models instead. In this case, itâ€™s important
    to note the checkpoint of the model you started from. If possible, even linking
    back to the original training experiment. This will allow future scientists to
    dig deeper into their test results, find original training data, and discover
    paths forward to eliminate bias.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Another feature to look out for is evaluation metric tooling. We will go into
    greater depth in chapter 4, but evaluation metrics are difficult for language
    models. There are often multiple metrics you care about, and none of them are
    simple, like complexity ratings or similarity scores. While experiment tracker
    vendors try to be agnostic and unopinionated about evaluation metrics, they should
    at least make it easy to compare models and their metrics to help us decide which
    one is better. Since LLMs have become so popular, some have made it easy to evaluate
    the more common metrics like ROUGE for text summarization.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: You will also find that many experiment-tracking vendors have started to add
    tools specifically for LLMs. Some features you might consider looking for include
    direct Hugging Face support, LangChain support, prompt engineering toolkits, finetuning
    frameworks, and foundation model shops. The space is developing quickly, and no
    one tool has all the same features right now, but these feature sets will likely
    converge.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.3 Model registry
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The model registry is probably the simplest tool of an MLOps infrastructure.
    The main objective is one thatâ€™s easy to solve; we just need a place to store
    the models. Weâ€™ve seen many successful teams get by simply by putting their models
    in an object store or shared filesystem and calling it good. That said, there
    are a couple bells and whistles you should look for when choosing one.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: The first is whether the model registry tracks metadata about the model. Most
    of what you care about is going to be in the experiment tracker, so you can usually
    get away with simply ensuring you can link the two. In fact, most model registries
    are built into experiment tracking systems because of this. However, a problem
    with these systems happens when the company decides to use an open source model
    or even buy one. Is it easy to upload a model and tag it with relevant information?
    The answer is usually no.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Next, you want to make sure you can version your models. At some point, a model
    will reach a point where itâ€™s no longer useful and will need to be replaced. Versioning
    your models will simplify this process. It also makes running production experiments
    like A/B testing or shadow tests easier.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, if we are promoting and demoting models, we need to be concerned with
    access. Models tend to be valuable intellectual property for many companies, so
    ensuring only the right users have access to the models is important. But itâ€™s
    also important to ensure that only the team that understands the modelsâ€”what they
    do and why they were trainedâ€”is in charge of promoting and demoting the models.
    The last thing we want is to delete a model in production or worse.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'For LLMs, there are some important caveats you should be aware of: mainly,
    when choosing a model registry, be aware of any limit sizes. Several model registries
    restrict model sizes to 10 GB or smaller. Thatâ€™s just not going to cut it. We
    could speculate on the many reasons for this, but none of them are worthy of note.
    Speaking of limit sizes, if you are going to be running your model registry on
    an on-premise storage system like Ceph, make sure it has lots of space. You can
    buy multiple terabytes of storage for a couple of hundred dollars for your on-premise
    servers, but even a couple of terabytes fills up quickly when your LLM is over
    300 GB. Donâ€™t forget: you are likely to be keeping multiple checkpoints and versions
    during training and finetuning, as well as duplicates for reliability purposes.
    Storage is still one of the cheapest aspects of running LLMs, though, so thereâ€™s
    no reason to skimp here and cause headaches down the road.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'This brings me to a good point: a lot of optimization could still be made,
    allowing for better space-saving approaches to storing LLMs and their derivatives,
    especially since most of these models will be very similar overall. Weâ€™ll likely
    see storage solutions to solve just this problem in the future.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.4 Feature stores
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Feature stores solve many important problems and answer questions like, Who
    owns this feature? How was it defined? Who has access to it? Which models are
    using it? How do we serve this feature in production? Essentially, they solve
    the â€œsingle source of truthâ€ problem. Creating a centralized store allows teams
    to shop for the highest quality, most well-maintained, thoroughly managed data.
    Feature stores solve the problems of collaboration, documentation, and versioning
    of data.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'If youâ€™ve ever thought, â€œA feature store is just a database, right?â€, you are
    probably thinking about the wrong type of storeâ€”we are referencing a place to
    shop, not a place of storage. Donâ€™t worry: this confusion is normal, as weâ€™ve
    heard this sentiment a lot and have had similar thoughts ourselves. The truth
    is that modern-day feature stores are more virtual than a physical database, which
    means they are built on top of whatever data store you are already using. For
    example, Googleâ€™s Vertex AI feature store is just BigQuery, and weâ€™ve seen a lot
    of confusion from data teams wondering, â€œWhy donâ€™t we just query BigQuery?â€ Loading
    the data into a feature store feels like an unnecessary extra step, but think
    about shopping at an IKEA store. No one goes directly to the warehouse where all
    the furniture is in boxes. That would be a frustrating shopping experience. The
    features store is the showroom that allows others in your company to easily peruse,
    experience, and use the data.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Often, we see people reach for a feature store to solve a technical problem
    like low latency access for online feature serving. A huge win for feature stores
    is solving the training-serving skew. Some features are just easier to do in SQL
    after the fact, like calculating the average number of requests for the last 30
    seconds. This can lead to naive data pipelines being built for training but causing
    massive headaches when going to production because getting this type of feature
    in real time can be anything but easy. Feature store abstractions help minimize
    this burden. Related to this are feature store point-in-time retrievals, which
    are table stakes when talking feature stores. Point-in-time retrievals ensure
    that, given a specific time, a query will always return the same result. This
    is important because features like averages over â€œthe last 30 secondsâ€ are constantly
    changing, so this allows us to version the data (without the extra burden of a
    bloated versioning system), as well as ensure our models will give accurate and
    predictable responses.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: As far as options, Feast is a popular open source feature store. Featureform
    and Hopsworks are also open source. All three offer paid hosting options. For
    LLMs, weâ€™ve heard the sentiment that feature stores arenâ€™t as critical as other
    parts of the MLOps infrastructure. After all, the model is so large that it should
    incorporate all the features needed inside it, so you donâ€™t need to query for
    additional context. Just give the model the userâ€™s query, and let the model do
    its thing. However, this approach is still a bit naive, and we havenâ€™t quite gotten
    to a point where LLMs are completely self-sufficient. To avoid hallucinations
    and improve factual correctness, it is often best to give the model some context.
    We do this by feeding it embeddings of our documents that we want it to know very
    well, and a feature store is a great place to put these embeddings.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.5 Vector databases
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you are familiar with the general MLOps infrastructure, most of this section
    has been review for you. Weâ€™ve only had to make slight adjustments highlighting
    important scaling concerns to make a system work for LLMs. Vector databases, however,
    are new to the scene and have been developed to be a tailored solution for working
    with LLMs and language models in general, but you can also use them with other
    datasets like images or tabular data, which are easy enough to transform into
    a vector. Vector databases are specialized databases that store vectors along
    with some metadata around the vector, which makes them great for storing embeddings.
    Now, while that last sentence is true, it is a bit misleading because the power
    of vector databases isnâ€™t in their storage but in the way that they search through
    the data.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'Traditional databases, using b-tree indexing to find IDs or text-based search
    using reverse indexes, all have the same common flaw: you have to know what you
    are looking for. If you donâ€™t have the ID or you donâ€™t know the keywords, itâ€™s
    impossible to find the right row or document. Vector databases, however, take
    advantage of the vector space, meaning you donâ€™t need to know exactly what you
    are looking for; you just need to know something similar, which you can then use
    to find the nearest neighbors using similarity searches based on Euclidean distance,
    cosine similarity, dot product similarity, or what have you. For example, using
    a vector database makes solving the reverse image search problem a breeze.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, some readers may be confused. First, we told you to put your
    embeddings into a feature store, and now weâ€™re telling you to put them into a
    Vector DB. Which one is it? Well, thatâ€™s the beauty of it: you can do both at
    the same time. If it didnâ€™t make sense before, we hope it makes sense now. A feature
    store is not a database; it is just an abstraction. You can use a feature store
    built on top of a vector DB, and it will solve many of your problems. Vector DBs
    can be difficult to maintain when you have multiple data sources, are experimenting
    with different embedding models, or otherwise have frequent data updates. Managing
    this complexity can be a real pain, but a feature store can handily solve this
    problem. Using them in combination will ensure a more accurate and up-to-date
    search index.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Vector databases have only been around for a couple of years at the time of
    writing, and their popularity is still relatively new, as they have grown hand
    in hand with LLMs. Itâ€™s easy to understand why since they provide a fast and efficient
    way to retrieve vector data, making it simple to provide LLMs with needed context
    to improve their accuracy.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'That said, itâ€™s a relatively new field, and there are lots of competitors in
    this space right now. Itâ€™s a bit too early to know who the winners and losers
    are. Not wanting to date this book too much, let me at least suggest two options
    to start: Pinecone and Milvus. Pinecone is one of the first vector databases as
    a product and has a thriving community with lots of documentation. Itâ€™s packed
    with features and has proven itself to scale. Pinecone is a fully managed infrastructure
    offering that has a free tier for beginners to learn. If you are a fan of open
    source, however, then youâ€™ll want to check out Milvus. Milvus is feature rich
    and has a great community. Zilliz, the company behind Milvus, offers a fully managed
    offering, but itâ€™s also available to deploy in your own clusters. If you already
    have a bit of infrastructure experience, itâ€™s relatively easy and straightforward
    to do.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: There are lots of alternatives out there right now, and itâ€™s likely worth a
    bit of investigation before picking one. The two things youâ€™ll probably care most
    about are price and scalability, as the two often go hand in hand. After that,
    itâ€™s valuable to pay attention to search features, such as support for different
    similarity measures like cosine similarities, dot product, and Euclidean distance,
    as well as indexing features like Hierarchical Navigable Small World (HNSW) and
    locality-sensitive hashing (LSH). Being able to customize your search parameters
    and index settings is important for any database, as you can customize the workload
    for your dataset and workflow, allowing you to optimize query latency and search
    result accuracy.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Itâ€™s also important to note that with the rise of vector databases, we are quickly
    seeing many database incumbents like Redis and Elastic offering vector search
    capabilities. For now, most of these tend to offer the most straightforward feature
    sets, but they are hard to ignore if you are already using these tool sets, as
    they can provide quick wins to help you get started quickly.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Vector databases are powerful tools that can help you train or finetune LLMs,
    as well as improve the accuracy and results of your LLM queries.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.6 Monitoring system
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A monitoring system is crucial to the success of any ML system, LLMs included.
    Unlike other software applications, ML models are known to fail silentlyâ€”that
    is, continue to operate but start to give poor results. This is often due to data
    drift, a common example being a recommendation system that gives worse results
    over time because sellers start to game the system by giving fake reviews to get
    better recommendation results. A monitoring system allows us to catch poorly performing
    models and make adjustments or simply retrain them.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Despite their importance, monitoring systems are often the last piece of the
    puzzle added. This is often purposeful, as putting resources into figuring out
    how to monitor models doesnâ€™t help if you donâ€™t have any models to monitor. However,
    donâ€™t make the mistake of putting it off too long. Many companies have been burned
    by a model that went rogue with no one knowing about it, often costing them dearly.
    Itâ€™s also important to realize you donâ€™t have to wait to get a model into production
    to start monitoring your data. There are plenty of ways to introduce a monitoring
    system into the training and data pipelines to improve data governance and compliance.
    Regardless, you can usually tell the maturity of a data science organization by
    its monitoring system.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: There are lots of great monitoring toolings out there; some great open source
    options include whylogs and Evidently AI. We are also fans of Great Expectations
    but have found it rather slow outside of batch jobs. There are also many more
    paid options out there. Typically, for ML monitoring workloads, youâ€™ll want to
    monitor everything youâ€™d normally record in other software applications; this
    includes resource metrics like memory and CPU utilization, performance metrics
    like latency and queries per second, and operational metrics like status codes
    and error rates. In addition, youâ€™ll need ways to monitor data drift going in
    and out of the model. Youâ€™ll want to pay attention to things like missing values,
    uniqueness, and standard deviation shifts. In many instances, youâ€™ll want to be
    able to segment your data while monitoringâ€”for example, for A/B testing or monitoring
    by region. Some metrics useful to monitor in ML systems include model accuracy,
    precision, recall, and F1 scores. These are difficult since you wonâ€™t know the
    correct answer at inference time, so itâ€™s often helpful to set up some sort of
    auditing system. Of course, auditing will be easier if your LLM is designed to
    be a Q&A bot rather than if your LLM is built to help writers be more creative.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: This hints at a whole set of new challenges for your monitoring systems, even
    more than what we see with other ML systems. With LLMs, we are dealing with text
    data, which is hard to quantify, as discussed earlier in this chapter. For instance,
    consider the features you look at to monitor for data drift, because language
    is known to drift a lot! One feature we suggest is unique tokens. These will alert
    you when new slang words or terms are created; however, they still donâ€™t help
    when words switch meaning, for example, when â€œwickedâ€ means â€œcool.â€ We would also
    recommend monitoring the embeddings; however, youâ€™ll likely find this to either
    add a lot of noise and false alarms or, at the very least, be difficult to decipher
    and dig into when problems do occur. The systems that work the best often involve
    a lot of handcrafted rules and features to monitor, but these can be error-prone
    and time-consuming to create.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring text-based systems is far from a solved problem, mostly stemming
    from the difficulties in understanding text data to begin with. This begs the
    question of what the best methods are to use language models to monitor themselves
    since they are our current best solution to codifying language. Unfortunately,
    weâ€™re not aware of anyone researching this, but we imagine itâ€™s only a matter
    of time.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.7 GPU-enabled workstations
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GPU-enabled workstations and remote workstations in general are often considered
    a nice-to-have or luxury by many teams, but when working with LLMs, that mindset
    has to change. When troubleshooting a problem or just developing a model in general,
    a data scientist isnâ€™t able to spin up the model in a notebook on their laptop
    anymore. The easiest way to solve this is to simply provide remote workstations
    with GPU resources. There are plenty of cloud solutions for this, but if your
    company is working mainly on-premise, it may be a bit more difficult to provide
    but necessary nonetheless.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are GPU memory-intensive models. Consequently, there are some numbers every
    engineer should know when it comes to working in the field. The first is how many
    GPUs to have. The NVIDIA Tesla T4 and V100 are the two most common GPUs youâ€™ll
    find in a datacenter, but they only have 16 GB of memory. They are workhorses,
    though, and cost-effective, so if we can compress our model to run on these, all
    the better. After these, youâ€™ll find a range of GPUs like NVIDIA A10G, NVIDIA
    Quadro series, and NVIDIA RTX series that offer GPU memories in the ranges of
    24, 32, and 48 GB. All of these are fine upgrades; youâ€™ll just have to figure
    out which ones are offered by your cloud provider and available to you. This brings
    us to the NVIDIA A100, which is likely going to be your GPU of choice when working
    with LLMs. Thankfully, they are relatively common and offer two different models
    providing 40 or 80 GB. A big problem with these is that they are constantly in
    high demand by everyone right now. You should also be aware of the NVIDIA H100,
    which offers 80 GB like the A100\. The H100 NVL is promised to support up to 188
    GB and has been designed with LLMs in mind. Another new GPU you should be aware
    of is the NVIDIA L4 Tensor Core GPU, which has 24 GB and is positioned to take
    over as a new workhorse along with the T4 and V100, at least as far as AI workloads
    are concerned.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'LLMs come in all different sizes, and itâ€™s useful to have a horse sense for
    what these numbers mean. For example, the LLaMA model has 7B, 13B, 33B, and 65B
    parameter variants. If you arenâ€™t sure off the top of your head which GPU you
    need to run which model, hereâ€™s a shortcut: multiply the number of billions of
    parameters by two, and thatâ€™s how much GPU memory you need. The reason is that
    most models at inference will default to run at half precision, FP16 of BF16,
    which means we need at least 2 bytes for every parameter. For example, 7 billion
    Ã— 2 bytes = 14 GB. Youâ€™ll need a little extra as well for the embedding model,
    which will be about another gigabyte, and more for the actual tokens you are running
    through the model. One token is about 1 MB, so 512 tokens will require 512 MB.
    This isnâ€™t a big deal until you consider running larger batch sizes to improve
    performance. For 16 batches of this size, youâ€™ll need an extra 8 GB of space.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Of course, so far, weâ€™ve only been talking about inference; for training, youâ€™ll
    need a lot more space. While training, youâ€™ll always want to do this in full precision,
    and youâ€™ll need extra room for the optimizer tensors and gradients. In general,
    to account for this, youâ€™ll need about 16 bytes for every parameter. So to train
    a 7B parameter model, youâ€™ll want 112 GB of memory.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.8 Deployment service
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Everything weâ€™ve been working toward is collected and finally put to good use
    here. In fact, if you took away every other service and were left with just a
    deployment service, youâ€™d still have a working MLOps system. A deployment service
    provides an easy way to integrate with all the previous systems we talked about
    and to configure and define the needed resources to get our model running in production.
    It will often provide boilerplate code to serve the model behind a REST and gRPC
    API or directly inside a batch or streaming pipeline.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Some tools to help create this service include NVIDIA Triton Inference Service,
    MLServer, Seldon, and BentoML. These services provide a standard API interface,
    typically the KServe V2 Inference Protocol. This protocol provides a unified and
    extensible way to deploy, manage, and serve machine learning models across different
    platforms and frameworks. It defines a common interface to interact with models,
    including gRPC and HTTP/RESTful APIs. It standardizes concepts like input/output
    tensor data encoding, predict and explain methods, model health checks, and metadata
    retrieval. It also allows seamless integration with languages and frameworks,
    including TensorFlow, PyTorch, ONNX, Scikit Learn, and XGBoost.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Of course, there are times when flexibility and customization provide enough
    value to step away from the automated path these other frameworks provide, in
    which case itâ€™s best to reach for a tool like FastAPI. Your deployment service
    should still provide as much automation and boilerplate code here as possible
    to make the process as smooth as possible. It should be mentioned that most of
    the previously mentioned frameworks do offer custom methods, but your mileage
    may vary.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a model is more than just building the interface. Your deployment
    service will also provide a bridge to close the gap between the MLOps infrastructure
    and general DevOps infrastructure. The connection to whatever CI/CD tooling and
    build and shipping pipelines your company has set up so you can ensure appropriate
    tests and deployment strategies like health checks and rollbacks can easily be
    monitored and done. This is often very platform- and thus company-specific. It
    must also provide the needed configurations to talk to Kubernetes or whatever
    other container orchestrator you may be using to acquire the needed resources
    like CPU, memory, accelerators, autoscalers, proxies, etc. It also applies the
    needed environment variables and secret management tools to ensure everything
    runs.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: All in all, this service ensures you can easily deploy a model into production.
    For LLMs, the main concern is often just being sure the platform and clusters
    are set up with enough resources to provision what will ultimately be configured.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Weâ€™ve discussed a lot so far in this chapter, starting with what makes LLMs
    so much harder than traditional ML, which is hard enough as it is. First, we learned
    that their size canâ€™t be underestimated, but then we also discovered many peculiarities
    about them, from token limits to hallucinationsâ€”not to mention that they are expensive.
    Fortunately, despite being difficult, they arenâ€™t impossible. We discussed compression
    techniques and distributed computing, which are crucial to master. We then explored
    the infrastructure needed to make LLMs work. While most of it was likely familiar,
    we came to realize that LLMs put a different level of pressure on each tool, and
    often, we need to be ready for a larger scale than what we could get away with
    for deploying other ML models.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs are difficult to work with mostly because they are big. This results in
    a longer time to download, load into memory, and deploy, forcing us to use expensive
    resources.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs are also hard because they deal with natural language and all its complexities,
    including hallucinations, bias, ethics, and security.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regardless of whether you build or buy, LLMs are expensive, and managing costs
    and risks associated with them will be crucial to the success of any project utilizing
    them.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compressing models to be as small as we can makes them easier to work with;
    quantization, pruning, and knowledge distillation are particularly useful for
    this.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantization is popular because it is easy and can be done after training without
    any finetuning.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low-rank approximation is an effective way to shrink a model and has been used
    heavily for adaptation, thanks to LoRA.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are three core directions we use to parallelize LLM workflows: data,
    tensor, and pipeline. DP helps us increase throughput, TP helps us increase speed,
    and PP makes it all possible to run in the first place.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining the parallelism methods, we get 3D parallelism (data + tensor + pipeline),
    where we find that the techniques synergize, covering each otherâ€™s weaknesses
    and helping us get more utilization.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The infrastructure for LLMOps is similar to MLOps, but donâ€™t let that fool you,
    since there are many caveats where â€œgood enoughâ€ no longer works.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many tools have begun to offer new features specifically for LLM support.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector databases, in particular, are interesting as a new piece of the infrastructure
    puzzle needed for LLMs that allow quick search and retrievals of embeddings.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1]](#footnote-source-1) A. Bulatov, Y. Kuratov, and M. S. Burtsev, â€œScaling
    transformer to 1M tokens and beyond with RMT,â€ April 2023, [https://arxiv.org/abs/2304.11062](https://arxiv.org/abs/2304.11062).'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[[2]](#footnote-source-2) R. Daws, â€œMedical chatbot using OpenAIâ€™s GPT-3 told
    a fake patient to kill themselves,â€ *AI News*, October 28, 2020, [https://mng.bz/qO6z](https://mng.bz/qO6z).'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[[3]](#footnote-source-3) T. Kington, â€œChatGPT bot tricked into giving bomb-making
    instructions, say developers,â€ *The Times*, December 17, 2022, [https://mng.bz/7d64](https://mng.bz/7d64).'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[[4]](#footnote-source-4) K. Quach, â€œAI game bans players for NSFW stories
    it generated itself,â€ The Register, October 8, 2021, [https://www.theregister.com/2021/10/08/ai_game_abuse/](https://www.theregister.com/2021/10/08/ai_game_abuse/).'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[[5]](#footnote-source-5) T. Hoefler, D. Alistarh, T. Ben-Nun, N. Dryden, and
    A. Peste, â€œSparsity in deep learning: Pruning and growth for efficient inference
    and training in neural networks,â€ January 2021, [https://arxiv.org/abs/2102.00554](https://arxiv.org/abs/2102.00554).'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '[[6]](#footnote-source-6) E. Frantar and D. Alistarh, â€œSparseGPT: Massive Language
    models can be accurately pruned in one-shot,â€ January 2023, [https://arxiv.org/abs/2301.00774](https://arxiv.org/abs/2301.00774).'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[[7]](#footnote-source-7) V. Sanh, L. Debut, J. Chaumond, and T. Wolf, â€œDistilBERT,
    a distilled version of BERT: smaller, faster, cheaper and lighter,â€ October 2019,
    [https://arxiv.org/abs/1910.01108](https://arxiv.org/abs/1910.01108).'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[[8]](#footnote-source-8) R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li,
    C. Guestrin, P Liang, and T. B. Hashimoto, â€œAlpaca: A strong, replicable instruction-following
    model,â€ CRFM, 2023, [https://crfm.stanford.edu/2023/03/13/alpaca.xhtml](https://crfm.stanford.edu/2023/03/13/alpaca.xhtml).'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[[9]](#footnote-source-9) E. J. Hu et al., â€œLoRA: Low-rank adaptation of large
    language models.,â€ June 2021, [https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685).'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '[[10]](#footnote-source-10) R. Henry and Y. J. Kim, â€œAccelerating large language
    models via low-bit quantization,â€ March 2023, [https://mng.bz/maD0](https://mng.bz/maD0).'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[[11]](#footnote-source-11) V. Korthikanti et al., â€œReducing activation recomputation
    in large transformer models,â€ May 2022, [https://arxiv.org/abs/2205.05198](https://arxiv.org/abs/2205.05198).'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '[[12]](#footnote-source-12) A. Harlap et al., â€œPipeDream: Fast and efficient
    pipeline parallel DNN training,â€ June 8, 2018, [https://arxiv.org/abs/1806.03377](https://arxiv.org/abs/1806.03377).'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
