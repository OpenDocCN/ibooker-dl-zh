- en: '3 Large language model operations: Building a platform for LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An overview of large language model operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment challenges
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large language model best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Required large language model infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before anything else, preparation is the key to success.‚ÄîAlexander Graham Bell
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As we learned in the last chapter, when it comes to transformers and natural
    language processing (NLP), bigger is better, especially when it‚Äôs linguistically
    informed. However, bigger models come with bigger challenges because of their
    size, regardless of their linguistic efficacy, thus requiring us to scale up our
    operations and infrastructure to handle these problems. In this chapter, we‚Äôll
    be looking into exactly what those challenges are, what we can do to minimize
    them, and what architecture can be set up to help solve them.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Introduction to large language model operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What is large language model operations (LLMOps)? Well, since we like to focus
    on practicality over rhetoric, we‚Äôre not going to dive into any fancy definitions
    that you‚Äôd expect in a textbook, but let‚Äôs simply say it‚Äôs machine learning operations
    (MLOps) that have been scaled to handle LLMs. Let us also say scaling up is hard.
    One of the hardest tasks in software engineering. Unfortunately, too many companies
    are running rudimentary MLOps setups, and don‚Äôt think for a second that they will
    be able to handle LLMs. That said, the term *LLMOps* may not be needed. It has
    yet to show through as sufficiently different from core MLOps, especially considering
    they still have the same bones. If this book were a dichotomous key, MLOps and
    LLMOps would definitely be in the same genus, and only time will tell whether
    they are the same species. Of course, by refusing to define LLMOps properly, we
    might have traded one confusion for another, so let‚Äôs take a minute to describe
    MLOps.
  prefs: []
  type: TYPE_NORMAL
- en: MLOps is the field and practice of reliably and efficiently deploying and maintaining
    machine learning models in production. This includes‚Äîand, indeed, requires‚Äîmanaging
    the entire machine learning life cycle, from data acquisition and model training
    to monitoring and termination. A few principles required to master this field
    include workflow orchestration, versioning, feedback loops, continuous integration
    and continuous deployment (CI/CD), security, resource provisioning, and data governance.
    While there are often personnel who specialize in the productionizing of models,
    with titles like ML Engineers, MLOps Engineers, or ML Infrastructure Engineer,
    the field is a large-enough beast that it often abducts many other unsuspecting
    professionals to work in it who hold titles like Data Scientist or DevOps Engineer‚Äîoften
    against their knowledge or will, leaving them kicking and screaming, ‚ÄúIt‚Äôs not
    my job.‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Operations challenges with large language models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So why have a distinction at all? If MLOps and LLMOps are so similar, is LLMOps
    just another fad opportunists throw on their resume? Not quite. In fact, it‚Äôs
    quite similar to the term *Big Data*. When the term was at its peak popularity,
    people with titles like Big Data Engineer used completely different tool sets
    and developed specialized expertise necessary to handle large datasets. LLMs come
    with a set of challenges and problems you won‚Äôt find with traditional machine
    learning systems. A majority of these problems extend almost exclusively because
    they are so big. Large models are large! We hope to show you that LLMs truly earn
    their name. Let‚Äôs take a look at a few of these challenges so we can appreciate
    the task ahead of us when we start talking about deploying an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Long download times
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Back in 2017, when I was still heavily involved as a data scientist, I decided
    to try my hand at reimplementing some of the most famous computer vision models
    at the time: AlexNet, VGG19, and ResNet. I figured this would be a good way to
    reinforce my understanding of the basics with some practical hands-on experience.
    Plus, I had an ulterior motive: I had just built my own rig with some NVIDIA GeForce
    1080 TI GPUs, which were state of the art at the time, and I thought this would
    be a good way to break them in. The first task was to download the ImageNet dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: The ImageNet dataset was one of the largest annotated datasets available, containing
    millions of images rounding out to a file size of a whopping ~150 GB! Working
    with it was proof that you knew how to work with Big Data, which was still a trendy
    word and an invaluable skill set for a data scientist at the time. After agreeing
    to the terms and gaining access, I got my first wakeup call. Downloading it took
    an entire week.
  prefs: []
  type: TYPE_NORMAL
- en: 'When my team first deployed Bloom, it took an hour and a half to download it.
    Heck, it took an hour and a half to download *The Legend of Zelda: Tears of the
    Kingdom*, and that‚Äôs only 16 GB, so we really couldn‚Äôt complain.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Large models are large. That can‚Äôt be overstated. You‚Äôll find throughout this
    book that that fact comes with many additional headaches and problems for the
    entire production process, and you have to be prepared for it. In comparison to
    the ImageNet dataset, the Bloom LLM model is 330 GB, more than twice the size.
    We‚Äôre guessing most readers haven‚Äôt worked with either ImageNet or Bloom, so for
    comparison, *Call of Duty: Modern Warfare*, one of the largest games at the time
    of this writing, is 235 GB. *Final Fantasy 15* is only 148 GB, so you could fit
    two into the model with plenty of room to spare. It‚Äôs just hard to really comprehend
    how massive LLMs are. We went from 100 million parameters in models like BERT
    and took them to billions of parameters. If you went on a shopping spree and spent
    $20 a second (or maybe accidentally left your AWS EC2 instance on), it‚Äôd take
    you half a day to spend a million dollars; it would take you two years to spend
    a billion.'
  prefs: []
  type: TYPE_NORMAL
- en: Thankfully, it doesn‚Äôt take two weeks to download Bloom because unlike ImageNet,
    it‚Äôs not hosted on a poorly managed university server, and it also has been sharded
    into multiple smaller files to allow downloading in parallel, but it will still
    take an uncomfortably long time. Consider a scenario where you are downloading
    the model under the best conditions. You‚Äôre equipped with a gigabit speed fiber
    internet connection, and you‚Äôre magically able to dedicate the entire bandwidth
    and I/O operations of your system and the server to it. It will still take over
    5 minutes to download! Of course, that‚Äôs under the best conditions. You probably
    won‚Äôt be downloading the model under such circumstances; with modern infrastructure,
    you can expect it to take on the order of hours.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Longer deploy times
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Just downloading the model is a long enough time frame to make any seasoned
    developer shake, but deployment times are going to make them keel over and call
    for medical attention. A model as big as Bloom can take 30 to 45 minutes just
    to load the model into GPU memory‚Äîat least, those are the time frames we‚Äôve experienced.
    That‚Äôs not to mention any other steps in your deployment process that can add
    to this. Indeed, with GPU shortages, it can easily take hours just waiting for
    resources to free up‚Äîmore on that in a minute.
  prefs: []
  type: TYPE_NORMAL
- en: What does this mean for you and your team? Well, for starters, we know lots
    of teams that deploy ML products often simply download the model at run time.
    That might work for small sklearn regression models, but it isn‚Äôt going to work
    for LLMs. Additionally, you can take most of what you know about deploying reliable
    systems and throw it out the window (but thankfully not too far). Most modern-day
    best practices for software engineering assume you can easily restart an application
    if anything happens, and there‚Äôs a lot of rigmarole involved to ensure your systems
    can do just that. With LLMs, it can take seconds to shut down, but potentially
    hours to redeploy, making this a semi-irreversible process. Like picking an apple
    off a tree, it‚Äôs easy to pluck one off, but if you bite into it and decide it‚Äôs
    too sour, you can‚Äôt reattach it to the tree so it can continue to ripen. You‚Äôll
    just have to wait awhile for another to grow.
  prefs: []
  type: TYPE_NORMAL
- en: While not every project requires deploying the largest models out there, you
    can expect to see deployment times measured in minutes. These longer deploy times
    make scaling down right before a surge of traffic a terrible mistake, as well
    as figuring out how to manage bursty workloads difficult. General CI/CD methodologies
    need to be adjusted since rolling updates take longer, leaving a backlog piling
    up quickly in your pipeline. Silly mistakes like typos or other bugs often take
    longer to notice and longer to correct.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3 Latency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Along with increases in model size often come increases in inference latency.
    This is obvious when stated, but more parameters equate to more computations,
    and more computations mean longer inference wait times. However, this can‚Äôt be
    underestimated. We know many people who downplay the latency problems because
    they‚Äôve interacted with an LLM chatbot, and the experience felt smooth. Take a
    second look, though, and you‚Äôll notice that it is returning one word at a time,
    which is streamed to the user. It feels smooth because the answers are coming
    in faster than a human can read, but a second look helps us realize this is just
    a UX trick. LLMs are still too slow to be very useful for an autocomplete solution,
    for example, where responses have to be blazingly fast. Building it into a data
    pipeline or workflow that reads a large corpus of text and then tries to clean
    it or summarize it may also be too prohibitively slow to be useful or reliable.
  prefs: []
  type: TYPE_NORMAL
- en: There are also many less obvious reasons for their slowness. For starters, LLMs
    are often distributed across multiple GPUs, which adds extra communication overhead.
    As discussed later in this chapter in section 3.3.2, they are distributed in other
    ways, often even to improve latency, but any distribution adds an additional overhead
    burden. In addition, LLMs‚Äô latency is severely affected by completion length,
    meaning the more words it uses to return a response, the longer it takes. Of course,
    completion length also seems to improve accuracy. For example, using prompt engineering
    techniques like chain of thought (CoT), we ask the model to think about a problem
    in a step-by-step fashion, which has been shown to improve results for logic and
    math questions but significantly increases the response length and latency time.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.4 Managing GPUs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To help with these latency problems, we usually want to run them in GPUs. If
    we want to have any success training LLMs, we‚Äôll need GPUs for that as well, but
    this all adds additional challenges that many underestimate. Most web services
    and many ML use cases can be done solely on CPUs, but not so with LLMs‚Äîpartly
    because of GPUs‚Äô parallel processing capabilities offering a solution to our latency
    problems and partly because of the inherent optimization GPUs offer in the linear
    algebra, matrix multiplications, and tensor operations; that‚Äôs happening under
    the hood. For many who are stepping into the realm of LLMs, this requires utilizing
    a new resource and extra complexity. Many brazenly step into this world, acting
    like it‚Äôs no big deal, but they are in for a rude awakening. Most system architectures
    and orchestrating tooling available, like Kubernetes, assume your application
    will run with CPU and memory alone. While they often support additional resources
    like GPUs, it‚Äôs often an afterthought. You‚Äôll soon find you have to rebuild containers
    from scratch and deploy new metric systems.
  prefs: []
  type: TYPE_NORMAL
- en: One aspect of managing GPUs that most companies are not prepared for is that
    they tend to be rare and limited. For the last decade, it seems that we have gone
    in and out of a global GPU shortage. They can be extremely difficult to provision
    for companies looking to stay on-premise. We‚Äôve spent lots of time in our careers
    working with companies that chose to stay on-premise for a variety of reasons.
    One of the things they had in common is that they never had GPUs on their servers.
    When they did, they were often purposely difficult to access except for a few
    key employees.
  prefs: []
  type: TYPE_NORMAL
- en: If you are lucky enough to be working in the cloud, a lot of these problems
    are solved, but there is no free lunch here either. We‚Äôve both been part of teams
    that have often gone chasing their tails trying to help data scientists struggling
    to provision a new GPU workspace. We‚Äôve run into obscure, ominous errors like
    `scale.up.error .out.of.resources`, only to discover that these esoteric readings
    indicate all the GPUs of a selected type in the entire region are being utilized,
    and none are available. CPU and memory can often be treated as infinite in a data
    center; GPU resources, however, cannot. Sometimes you can‚Äôt expect them at all.
    Most data centers only support a subset of instance or GPU types, which means
    you may be forced to set up your application in a region further away from your
    user base, thus increasing latency. Of course, we‚Äôre sure you can work with your
    cloud provider when looking to expand your service to a new region that doesn‚Äôt
    currently support it, but you might not like what you hear based on timelines
    and cost. Ultimately, you‚Äôll run into shortage problems no matter where you choose
    to run, on-premise or in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.5 Peculiarities of text data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLMs are the modern-day solution to NLP. NLP is one of the most fascinating
    branches of ML in general because it primarily deals with text data, which is
    primarily a qualitative measure. Every other field deals with quantitative data.
    We have figured out a way to encode our observations of the world into a direct
    translation of numerical values. For example, we‚Äôve learned how to encode heat
    into temperature scales and measure it with thermometers and thermocouples, and
    we can measure pressure with manometers and gauges and put it into pascals.
  prefs: []
  type: TYPE_NORMAL
- en: Computer vision and the practice of evaluating images are often seen as qualitative,
    but the actual encoding of images into numbers is a solved problem. Our understanding
    of light has allowed us to break images apart into pixels and assign them RGB
    values. Of course, this doesn‚Äôt mean computer vision is by any means solved; there‚Äôs
    still lots of work to do to learn how to identify the different signals in the
    patterns of the data. Audio data is also often considered qualitative. How does
    one compare two songs? But we can measure sound and speech, directly measuring
    the sound wave‚Äôs intensity in decibels and frequency in hertz.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike other fields that encode our physical world into numerical data, text
    data is looking at ways to measure the ephemeral world. After all, text data is
    our best effort at encoding our thoughts, ideas, and communication patterns. While,
    yes, we have figured out ways to turn words into numbers, we haven‚Äôt figured out
    a direct translation. Our best solutions to encode text and create embeddings
    are just approximations at best; in fact, we use machine learning models to do
    it! An interesting aside is that numbers are also text and a part of language.
    If we want models that are better at math, we need a more meaningful way to encode
    these numbers. Since it‚Äôs all made up, when we try to encode text numbers into
    machine-readable numbers, we are creating a system attempting to reference itself
    recursively in a meaningful way. Not an easy problem to solve!
  prefs: []
  type: TYPE_NORMAL
- en: Because of all this, LLMs (and all NLP solutions) have unique challenges. Take,
    for example, monitoring. How do you catch data drift in text data? How do you
    measure ‚Äúcorrectness‚Äù? How do you ensure the cleanliness of the data? These types
    of problems are difficult to define, let alone solve.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.6 Token limits create bottlenecks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A big challenge for those new to working with LLMs is dealing with the token
    limits. The token limit for a model is the maximum number of tokens that can be
    included as an input for a model. The larger the token limit, the more context
    we can give the model to improve its success at accomplishing the task. Everyone
    wants them to be higher, but it‚Äôs not that simple. These token limits are defined
    by two problems: the memory and speed our GPUs have access to and the nature of
    memory storage in the models themselves.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first problem seems unintuitive: Why couldn‚Äôt we just increase the GPU
    memory? The answer is complex. We can, but stacking more layers in the GPU to
    take into account more gigabytes at once slows down the GPU‚Äôs computational ability
    as a whole. Right now, GPU manufacturers are working on new architectures and
    ways to get around this problem. The second challenge is fascinating because increasing
    the token limits actually exacerbates the mathematical problems under the hood.
    Let me explain. Memory storage within an LLM itself isn‚Äôt something we think about
    often. We call that mechanism *attention*, which we discussed in depth in section
    2.2.7\. What we didn‚Äôt discuss was that attention is a quadratic solution: as
    the number of tokens increases, the number of calculations required to compute
    the attention scores between all the pairs of tokens in a sequence scales quadratically
    with the sequence length. In addition, within our gigantic context spaces, and
    since we are dealing with quadratics, we‚Äôre starting to hit problems where the
    only solutions involve imaginary numbers, which can cause models to behave in
    unexpected ways. This is likely one of the reasons why LLMs hallucinate.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These problems have real implications and affect application designs. For example,
    when this author‚Äôs team upgraded from GPT-3 to GPT-4, the team was excited to
    have access to a higher token limit, but it soon found this led to longer inference
    times and, subsequently, a higher timeout error rate. In the real world, it‚Äôs
    often better to get a less accurate response quickly than to get no response at
    all because the promise of a more accurate model often is just that: a promise.
    Of course, when deploying it locally, where you don‚Äôt have to worry about response
    times, you‚Äôll likely find your hardware to be a limiting factor. For example,
    LLaMA was trained with 2,048 tokens, but you‚Äôll be lucky to take advantage of
    more than 512 of that when running with a basic consumer GPU, as you are likely
    to see out-of-memory (OOM) errors or even the model simply crashing.'
  prefs: []
  type: TYPE_NORMAL
- en: A gotcha, which is likely to catch your team by surprise and should be pointed
    out now, is that different languages have different tokens per character. Take
    a look at table 3.1, where we compare converting the same sentence in different
    languages to tokens using OpenAI‚Äôs cl100k_base Byte Pair Encoder. Just a quick
    glance reveals that LLMs typically favor the English language in this regard.
    In practice, this means that if you are building a chatbot with an LLM, your English
    users will have greater flexibility in their input space than Japanese users,
    leading to very different user experiences.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.1 Comparison of token counts in different languages
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Language | String | Characters | Tokens |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| English  | The quick brown fox jumps over the lazy dog  | 43  | 9  |'
  prefs: []
  type: TYPE_TB
- en: '| French  | Le renard brun rapide saute par-dessus le chien paresseux  | 57  |
    20  |'
  prefs: []
  type: TYPE_TB
- en: '| Spanish  | El r√°pido zorro marr√≥n salta sobre el perro perezoso  | 52  |
    22  |'
  prefs: []
  type: TYPE_TB
- en: '| Japanese  | Á¥†Êó©„ÅÑËå∂Ëâ≤„ÅÆ„Ç≠„ÉÑ„Éç„ÅåÊÄ†ÊÉ∞„Å™Áä¨„ÇíÈ£õ„Å≥Ë∂ä„Åà„Çã  | 20  | 36  |'
  prefs: []
  type: TYPE_TB
- en: '| Chinese (simplified)  | ÊïèÊç∑ÁöÑÊ£ïËâ≤ÁãêÁã∏Ë∑≥Ëøá‰∫ÜÊáíÁãó  | 12  | 28  |'
  prefs: []
  type: TYPE_TB
- en: If you are curious about why this is, it is due to text encodings, which are
    another peculiarity of working with text data, as discussed in the previous section.
    Consider table 3.2, where we show several different characters and their binary
    representations in UTF-8\. English characters can almost exclusively be represented
    with a single byte included in the original ASCII standard computers were originally
    built on, while most other characters require 3 or 4 bytes. Because it takes more
    memory, it also takes more token space.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.2 Comparison of byte lengths for different currency characters in UTF-8
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Character | Binary UTF-8 | Hex UTF-8 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $  | 00100100  | 0x24  |'
  prefs: []
  type: TYPE_TB
- en: '| ¬£  | 11000010 10100011  | 0xc2 0xa3  |'
  prefs: []
  type: TYPE_TB
- en: '| ¬•  | 11000010 10100101  | 0xc2 0xa5  |'
  prefs: []
  type: TYPE_TB
- en: '| ‚Ç†  | 11100010 10000010 10100000  | 0xe2 0x82 0xa0  |'
  prefs: []
  type: TYPE_TB
- en: '| üí∞  | 11110000 10011111 10010010 10110000  | 0xf0 0x9f 0x92 0xb0  |'
  prefs: []
  type: TYPE_TB
- en: Increasing the token limits has been an ongoing research question since the
    popularization of transformers, and there are some promising solutions still in
    the research phases, like recurrent memory transformers (RMT).[¬π](#footnote-119)
    We can expect to continue to see improvements in the future, and hopefully, this
    will become naught but an annoyance.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.7 Hallucinations cause confusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'So far, we‚Äôve been discussing some of the technical problems a team faces when
    deploying an LLM into a production environment, but nothing compares to the simple
    problem that LLMs tend to be wrong. They tend to be wrong a lot. *Hallucinations*
    is a term coined to describe occurrences when LLM models will produce correct-sounding
    results that are wrong‚Äîfor example, book references or hyperlinks that have the
    form and structure of what would be expected but are, nevertheless, completely
    made up. As a fun example, we asked for books on LLMs in production from the publisher,
    Manning (a book that doesn‚Äôt exist yet since one author is still writing it).
    We were given the following suggestions: *Machine Learning Engineering in Production*
    by Mike Del Balso and Lucas Serve√©n, which could be found at [https://www.manning.com/books/machine-learning-engineering-in-production](https://www.manning.com/books/machine-learning-engineering-in-production),
    and *Deep Learning for Coders with Fastai and PyTorch* by Jeremy Howard and Sylvain
    Gugger, which could be found at [https://www.manning.com/books/deep-learning-for-coders-with-fastai-and-pytorch](https://www.manning.com/books/deep-learning-for-coders-with-fastai-and-pytorch).
    The first book is entirely made up. The second book is real; however, it‚Äôs not
    published by Manning. In each case, the internet addresses are entirely made up.
    These URLs are actually very similar in format to what you‚Äôd expect if you were
    browsing Manning‚Äôs website, but they will return 404 errors if you visit them.'
  prefs: []
  type: TYPE_NORMAL
- en: One of the most annoying aspects of hallucinations is that they are often surrounded
    by confident-sounding words. LLMs are terrible at expressing uncertainty, in large
    part because of the way they are trained. Consider the case ‚Äú2 + 2 =.‚Äù Would you
    prefer it to respond, ‚ÄúI think it is 4‚Äù or simply ‚Äú4‚Äù? Most would prefer to get
    the correct ‚Äú4‚Äù back. This bias is built in, as models are often given rewards
    for being correct or at least sounding like it.
  prefs: []
  type: TYPE_NORMAL
- en: There are various explanations as to why hallucinations occur, but the most
    truthful answer is that we don‚Äôt know if there‚Äôs just one cause. It‚Äôs likely a
    combination of several things; thus, there isn‚Äôt a good fix for it yet. Nevertheless,
    being prepared to counter these inaccuracies and biases of the model is crucial
    to provide the best user experience for your product.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.8 Bias and ethical considerations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Just as concerning as the model getting things wrong is when it gets things
    right in the worst possible way‚Äîfor example, allowing it to encourage users to
    commit suicide,[¬≤](#footnote-120) teaching users how to make a bomb,[¬≥](#footnote-121)
    or participating in sexual fantasies involving children.[‚Å¥](#footnote-122) These
    are extreme examples, but prohibiting the model from answering such questions
    is undeniably vital to success.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are trained on vast amounts of text data, which is also their primary source
    of bias. Because we‚Äôve found that larger datasets are just as important as larger
    models in producing human-like results, most of these datasets have never truly
    been curated or filtered to remove harmful content, instead choosing to prioritize
    size and a larger collection. Cleaning the dataset is often seen as prohibitively
    expensive, requiring humans to go in and manually verify everything, but there‚Äôs
    a lot that could be done with simple regular expressions and other automated solutions.
    By processing these vast collections of content and learning the implicit human
    biases, these models will inadvertently perpetuate them. These biases range from
    sexism and racism to political preferences and can cause your model to inadvertently
    promote negative stereotypes and discriminatory language.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.9 Security concerns
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As with all technology, we need to be mindful of security. LLMs have been trained
    on a large corpus of text, some of which could be harmful or sensitive and shouldn‚Äôt
    be exposed. So steps should be taken to protect this data from being leaked. The
    bias and ethical concerns from the last section are good examples of conversations
    you don‚Äôt want your users to be having, but you could also imagine finetuning
    a model on your company‚Äôs data and potentially having secrets lost inadvertently
    if proper precautions aren‚Äôt taken.
  prefs: []
  type: TYPE_NORMAL
- en: One should be aware that LLMs are susceptible to adversarial attacks like prompt
    injections. Prompt injections are attacks done by a user to trick the LLM into
    ignoring instructions given to it and generating undesired content. For example,
    if you ask ChatGPT what its gender is, it appropriately replies that as an AI
    language model, it doesn‚Äôt have a gender. However, with clever prompting, you
    may be able to bypass these protocols and get it to reveal one. While this example
    is harmless, we‚Äôve seen others successfully extract API keys and other secrets
    from an LLM, run code in nonprotected environments, steal environment variables,
    and traverse local file systems where the model is served. That‚Äôs not to mention
    the plethora of examples of users using prompting to jailbreak or bypass protocols
    put in place for ethical considerations outlined in the previous section. An interesting
    aside to this is that LLMs are good at inventing fake secrets! Even successful
    prompt injection attacks can often fail due to LLM hallucinations, which can have
    funny consequences.
  prefs: []
  type: TYPE_NORMAL
- en: For example, using a simple prompt to ask for Coca-Cola‚Äôs secret recipe simply
    returned the boring result, ‚ÄúI cannot divulge trade secrets or any proprietary
    information.‚Äù Pretty lame if you ask us. With a little trickery and some prompt
    injection, we were able to get it to spill its secrets, which, of course, we decided
    to try‚Äîafter confirming with a food scientist it wouldn‚Äôt poison us. After following
    the instructions exactly, we were surprised to find we got a pretty tasty drink,
    but it tasted nothing like Coke. While the recipe looks legitimate, nuances like
    quantities, cook times, and even the need to strain the syrup were all off. Guess
    we‚Äôll have to hold off taking over the beverage industry until we get the real
    recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Coca-Cola recipe hallucinated from prompt injection
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Another security concern involves pickle injections. Pickle is a library in
    Python that serializes objects and is often used to serialize ML models. It serializes
    them into a byte stream containing opcodes executed one by one as it is deserialized.
    It‚Äôs a fast and easy way to share large objects. Pickle injections corrupt this
    byte stream, often injecting malware over the wire when the model is transferred
    over an insecure network. This is especially concerning for large models that
    take a long time to download, as it makes it easier for a third party to intercept
    the transfer and inject malicious code. If this happens, the code injected can
    potentially give the attackers access to your system. This can happen when attempting
    to use the model during inference, as the harmful code will execute if it is not
    detected and properly removed. It is important to take precautions such as using
    secure networks and verifying the integrity of the model before use to prevent
    this type of attack.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.10 Controlling costs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Working with LLMs involves various cost-related concerns. The first, as you
    probably gathered by now, is infrastructure costs, which include high-performance
    GPUs, storage, and other hardware resources. We talked about how GPUs are harder
    to procure, which, unfortunately, means they are more expensive. Mistakes like
    leaving your service on have always had the potential to rack up the bills, but
    with GPUs in the mix, this type of mistake is even more deadly. These models also
    demand significant computational power, leading to high energy consumption during
    both training and inference. On top of all this, their longer deploy times mean
    we are often running them even during low traffic to handle bursty workloads or
    anticipated future traffic. Overall, this leads to higher operational costs.
  prefs: []
  type: TYPE_NORMAL
- en: Additional costs include managing and storing vast amounts of data used to train
    or finetune as well as for regular maintenance, such as model updates, security
    measures, and bug fixes, which can be financially demanding. As with any technology
    used for business purposes, managing potential legal disputes and ensuring compliance
    with regulations is a concern. Lastly, investing in continuous research and development
    to improve your models and give you a competitive edge will be a factor.
  prefs: []
  type: TYPE_NORMAL
- en: We talked a bit about the technical concerns regarding token limits, which are
    likely to be solved, but we didn‚Äôt discuss the cost limitations, as most APIs
    charge on a token basis. This makes it more expensive to send more context and
    use better prompts. It also makes it a bit harder to predict costs since while
    you can standardize inputs, you can‚Äôt standardize outputs. You can never be too
    sure how many tokens will be returned, making it difficult to govern. Just remember,
    with LLMs, it is as important as ever to implement and follow proper cost engineering
    practices to ensure costs never get away from you.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 LLMOps essentials
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have a handle on the type of challenge we are grappling with, let‚Äôs
    take a look at all the different LLMOps practices, tooling, and infrastructure
    to see how different components help us overcome these obstacles. First, let‚Äôs
    dive into different practices, starting with compression, where we will talk about
    shrinking, trimming, and approximating to get models as small as we can. We will
    then talk about distributed computing, which is needed to make things run since
    the models are so large that they rarely fit into a single GPU‚Äôs memory. After
    we are finished with that, we will venture into the infrastructure and tooling
    needed to make it all happen in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 Compression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you were reading about the challenges of LLMs in the last section, you might
    have asked yourself something akin to ‚ÄúIf the biggest problems from LLMs come
    from their size, why don‚Äôt we just make them smaller?‚Äù If you did, congratulations!
    You are a genius‚Äîcompression is the practice of doing just that. Compressing models
    to as small as we can make them will improve deployment time, reduce latency,
    scale down the number of expensive GPUs needed, and, ultimately, save money. However,
    the whole point of making the models so stupefyingly gargantuan in the first place
    was because it made them better at what they do. We need to be able to shrink
    them without losing all the progress we made by making them big in the first place.
  prefs: []
  type: TYPE_NORMAL
- en: This problem is far from solved, but there are multiple ways to approach the
    problem, with different pros and cons to each method. We‚Äôll be talking about several
    of the methods, starting with the easiest and most effective.
  prefs: []
  type: TYPE_NORMAL
- en: Quantizing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Quantizing is the process of reducing precision in preference of lowering the
    memory requirements. This tradeoff makes intuitive sense. When this author was
    in college, he was taught to always round numbers to the precision of the tooling.
    Pulling out a ruler and measuring his pencil, you wouldn‚Äôt believe him if he stated
    the length was 19.025467821973739 cm. Even if he used a caliper, he couldn‚Äôt verify
    a number so precisely. With our ruler, any number beyond 19.03 cm is fantasy.
    To drive the point home, one of his engineering professors once asked him him,
    ‚ÄúIf you are measuring the height of a skyscraper, do you care if there is an extra
    sheet of paper at the top?‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: 'How we represent numbers inside computers often leads us to believe we have
    better precision than we actually do. To illustrate this point, open a Python
    terminal and add 0.1 + 0.2\. If you‚Äôve never tried this before, you might be surprised
    to find it doesn‚Äôt equal 0.3, but 0.30000000000000004\. We won‚Äôt go into the details
    of the math behind this phenomenon, but the question stands: Can we reduce the
    precision without making things worse? We really only need precision to the tenth
    decimal, but reducing the precision will likely get us a number like 0.304 rather
    than 0.300, thus increasing our margin of error.'
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, the only numbers a computer understands are 0 and 1, on or off,
    a single bit. To improve this range, we combine multiple bits and assign them
    different meanings. String 8 of them together, and you get a byte. Using the INT8
    standard, we can take that byte and encode all the integers from ‚Äì128 to 127\.
    We‚Äôll spare you the particulars because we assume you already know how binary
    works; suffice it to say, the more bits we have, the larger range of numbers we
    can represent, both larger and smaller. Figure 3.1 shows a few common floating
    point encodings. With 32 bits strung together, we get what we pretentiously term
    *full precision*, and that is how most numbers are stored, including the weights
    in machine learning models. Basic quantization moves us from full precision to
    half precision, shrinking models to half their size. There are two different half
    precision standards, FP16 and BF16, which differ in how many bits represent the
    range or exponent part. Since BF16 uses the same number of exponents as FP32,
    it‚Äôs been found to be more effective for quantizing, and you can generally expect
    almost exactly the same level of accuracy for half the size of model. If you understood
    the paper and skyscraper analogy, it should be obvious why.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1 The bit mapping for a few common floating point encodings: 16-bit
    float or half precision (FP16), bfloat 16 (BF16), 32-bit float or single full
    precision (FP32), and NVIDIA‚Äôs TensorFloat (TF32)'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: However, there‚Äôs no reason to stop there. We can often push it down another
    byte to 8-bit formats without too much loss of accuracy. There have already even
    been successful research attempts showing selective 4-bit quantization of portions
    of LLMs is possible with only a fractional loss of accuracy. The selective application
    of quantization is a process known as dynamic quantization and is usually done
    on just the weights, leaving the activations in full precision to reduce accuracy
    loss.
  prefs: []
  type: TYPE_NORMAL
- en: The holy grail of quantizing, though, is INT2, representing every number as
    ‚Äì1, 0, or 1\. This currently isn‚Äôt possible without completely degrading the model,
    but it would make the model up to 8 times smaller. The Bloom model would be a
    measly ~40 GB, small enough to fit on a single GPU. This is, of course, as far
    as quantizing can take us, and if we wanted to shrink further, we‚Äôd need to look
    at additional methods.
  prefs: []
  type: TYPE_NORMAL
- en: The best part of quantization is that it is easy to do. There are many frameworks
    that allow this, but in listing 3.1, we demonstrate how to use PyTorch‚Äôs quantization
    library to do a simple post-training static quantization (PTQ). All you need is
    the full precision model, some example inputs, and a validation dataset to prepare
    and calibrate with. As you can see, it‚Äôs only a few lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.1 Example PTQ in PyTorch
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#1 A deep copy of the original model as quantization is done in place'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Gets mappings; note the use of ‚Äúqnnpack‚Äù for ARM and ‚Äúfbgemm‚Äù for x86 CPU'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Prepares'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Calibrates; you‚Äôll want to use representative (validation) data'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Quantizes'
  prefs: []
  type: TYPE_NORMAL
- en: Static PTQ is the most straightforward approach to quantizing; it is done after
    the model is trained and uniformly quantizes all the model parameters. As with
    most formulas, the most straightforward approach introduces more error. Often,
    this error is acceptable, but when it‚Äôs not, we can add extra complexity to reduce
    the accuracy loss from quantization. Some methods to consider are uniform versus
    non-uniform, static versus dynamic, symmetric versus asymmetric, and applying
    it during or after training.
  prefs: []
  type: TYPE_NORMAL
- en: To understand these methods, let‚Äôs consider the case where we are quantizing
    from FP32 to INT8\. In FP32, we essentially have the full range of numbers at
    our disposal, but in INT8, we only have 256 values. We are trying to put a genie
    into a bottle, and it‚Äôs no small feat. If you study the weights in your model,
    you might notice that most of the numbers are fractions between [‚Äì1, 1]. We could
    take advantage of this by using an 8-bit standard that represents more values
    in this region in a non-uniform way instead of the standard uniform [‚Äì128, 127].
    While mathematically possible, unfortunately, any such standards aren‚Äôt commonplace,
    and modern-day deep learning hardware and software are not designed to take advantage
    of them. So for now, it‚Äôs best to stick to uniform quantization.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest approach to shrinking the data is to normalize it, but since we
    are going from a continuous scale to a discrete scale, there are a few gotchas,
    so let‚Äôs explore those. We start by taking the min and max and scale them down
    to match our new number range. We would then bucket all the other numbers based
    on where they fall. Of course, if we have really large outliers, we may find all
    our other numbers squeezed into just one or two buckets, ruining any granularity
    we once had. To prevent this, we can clip any large numbers; this is what we do
    in static quantization. However, before we clip the data, what if we choose a
    range and scale that captures the majority of our data beforehand? We need to
    be careful since if this dynamic range is too small, we will introduce more clipping
    errors; if it‚Äôs too big, we will introduce more rounding errors. The goal of dynamic
    quantization is, of course, to reduce both errors.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need to consider the symmetry of the data. Generally, in normalization,
    we force the data to be normal and thus symmetric; however, we could choose to
    scale the data in a way that leaves any asymmetry it had. By doing this, we could
    potentially reduce our overall loss due to the clipping and rounding errors, but
    it‚Äôs not guaranteed.
  prefs: []
  type: TYPE_NORMAL
- en: As a last resort, if none of these other methods fail to reduce the accuracy
    loss of the model, we can use quantization-aware training (QAT). QAT is a simple
    process where we add a fake quantization step during model training. By fake,
    we mean we clip and round the data while leaving it in full precision. This allows
    the model to adjust for the error and bias introduced by quantization while it‚Äôs
    training. QAT is known to produce higher accuracy compared to other methods but
    at a much higher cost in time to train.
  prefs: []
  type: TYPE_NORMAL
- en: Quantization methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Uniform versus non-uniform*‚ÄîWhether we use an 8-bit standard that is uniform
    in the range it represents or non-uniform to be more precise in the -1 to 1 range.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Static versus dynamic*‚ÄîChoosing to adjust the range or scale before clipping
    in an attempt to reduce clipping and rounding errors and reduce data loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Symmetric versus asymmetric*‚ÄîNormalizing the data to be normal and force symmetry
    or choosing to keep any asymmetry and skew.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*During or after training*‚ÄîQuantization after training is really easy to do,
    and while doing it during training is more work, it leads to reduced bias and
    better results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantizing is a very powerful tool. It reduces the size of the model and the
    computational overhead required to run the model, thus reducing the latency and
    cost of running the model. However, the best thing about quantization is that
    it can be done after the fact, so you don‚Äôt have to worry about whether your data
    scientists remembered to quantize the model during training using processes like
    QAT. This is why quantization has become so popular when working with LLMs and
    other large machine learning models. While reduced accuracy is always a concern
    with compression techniques, compared to other methods, quantization is a win-win-win.
  prefs: []
  type: TYPE_NORMAL
- en: Pruning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Congratulations, you just trained a brand new LLM! With billions of parameters,
    all of them must be useful, right? Wrong! Unfortunately, as with most things in
    life, the model‚Äôs parameters tend to follow the Pareto principle. About 20% of
    the weights lead to 80% of the value. ‚ÄúIf that‚Äôs true,‚Äù you may be asking yourself,
    ‚Äúwhy don‚Äôt we just cut out all the extra fluff?‚Äù Great idea! Give yourself a pat
    on the back. Pruning is the process of weeding out and removing any parts of the
    model we deem unworthy.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are essentially two different pruning methods: *structured* and *unstructured*.
    Structured pruning is the process of finding structural components of a model
    that aren‚Äôt contributing to the model‚Äôs performance and then removing them‚Äîwhether
    they are filters, channels, or layers in the neural network. The advantage of
    this method is that your model will be a little smaller but keep the same basic
    structure, which means we don‚Äôt have to worry about losing hardware efficiencies.
    We are also guaranteed a latency improvement, as there will be fewer computations
    involved.'
  prefs: []
  type: TYPE_NORMAL
- en: Unstructured pruning, on the other hand, shifts through the parameters and zeros
    out the less important ones that don‚Äôt contribute much to the model‚Äôs performance.
    Unlike structured pruning, we don‚Äôt actually remove any parameters; we just set
    them to zero. From this, we can imagine that a good place to start would be any
    weights or activations already close to 0\. Of course, while this effectively
    reduces the size of a model, this also means we don‚Äôt cut out any computations,
    so it‚Äôs common to see only minimal, if any, latency improvement. But a smaller
    model still means faster load times and fewer GPUs to run. It also gives us very
    fine-grained control over the process, allowing us to shrink a model further than
    we could with structured pruning, with less effect on performance too.
  prefs: []
  type: TYPE_NORMAL
- en: Like quantization, pruning can be done after a model is trained. However, unlike
    quantization, it‚Äôs common practice to see additional finetuning needed to prevent
    too great a loss of performance. It‚Äôs becoming more common to include pruning
    steps during the model training to avoid the need to finetune later on. Since
    a more sparse model will have fewer parameters to tune, adding these pruning steps
    may help a model converge faster as well.[‚Åµ](#footnote-123)
  prefs: []
  type: TYPE_NORMAL
- en: You‚Äôll be surprised at how much you can shrink a model with pruning while minimally
    affecting performance. How much? In the SparseGPT[‚Å∂](#footnote-124) paper, a method
    was developed to try to automatically one-shot the pruning process without the
    need for finetuning afterward. The authors found they could decrease a GPT-3 model
    by 50% to 60% without a problem! Depending on the model and task, they even saw
    slight improvements in a few of them. We are looking forward to seeing where pruning
    takes us in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge distillation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Knowledge distillation is probably the coolest method of compression in our
    minds. It‚Äôs a simple idea too: we‚Äôll take the large LLM and have it train a smaller
    language model to copy it. What‚Äôs nice about this method is that the larger LLM
    provides essentially an infinite dataset for the smaller model to train on, which
    can make the training quite effective. Because the larger the dataset, the better
    the performance, we‚Äôve often seen smaller models reach almost the same level as
    their teacher counterparts in accuracy.[‚Å∑](#footnote-125)'
  prefs: []
  type: TYPE_NORMAL
- en: A smaller model trained this way is guaranteed to both be smaller and improve
    latency. The downside is that it will require us to train a completely new model,
    which is a pretty significant upfront cost to pay. Any future improvements to
    the teacher model will require being passed down to the student model, which can
    lead to complex training cycles and version structure. It‚Äôs definitely a lot more
    work compared to some of the other compression methods.
  prefs: []
  type: TYPE_NORMAL
- en: The hardest part about knowledge distillation, though, is that we don‚Äôt really
    have good recipes for them yet. Tough questions like ‚ÄúHow small can the student
    model be?‚Äù will have to be solved through trial and error. There‚Äôs still a lot
    to learn and research to be done here.
  prefs: []
  type: TYPE_NORMAL
- en: However, there has been some exciting work in this field via Stanford‚Äôs Alpaca.[‚Å∏](#footnote-126)
    Instead of training a student model from scratch, they chose to finetune the open
    source LLaMA 7B parameter model using OpenAI‚Äôs GPT3.5‚Äôs 175B parameter model as
    a teacher via knowledge distillation. It‚Äôs a simple idea, but it paid off big,
    as they were able to get great results from their evaluation. The biggest surprise
    was the cost, as they only spent $500 on API costs to get the training data from
    the teacher model and $100 worth of GPU training time to finetune the student
    model. Granted, if you did this for a commercial application, you‚Äôd be violating
    OpenAI‚Äôs terms of service, so it‚Äôs best to stick to using your own or open source
    models as the teacher.
  prefs: []
  type: TYPE_NORMAL
- en: Low-rank approximation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Low-rank approximation, also known as low-rank factorization, low-rank decomposition,
    or matrix factorization, among other terms (too many names‚Äîwe blame the mathematicians),
    uses linear algebra math tricks to simplify large matrices or tensors to find
    a lower-dimensional representation. There are several techniques to do this. Singular
    value decomposition (SVD), Tucker decomposition (TD), and canonical polyadic decomposition
    (CPD) are the most common ones you run into.
  prefs: []
  type: TYPE_NORMAL
- en: In figure 3.2, we show the general idea behind the SVD method. Essentially,
    we are going to take a very large matrix, A, and break it up into three smaller
    matrices, U, S, and V. While U and V are there to ensure we keep the same dimensions
    and relative strengths of the original matrix, S allows us to apply a direction
    and bias. The smaller S is, the more we end up compressing and reducing the total
    number of parameters, but the less accurate the approximation becomes.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2 Example of SVD, a low-rank approximation. A is a large matrix with
    dimensions N and M. We can approximate it with three smaller matrices: U with
    dimensions M and P, S a square matrix with dimension P, and V with dimensions
    N and P (here we show the transpose). Usually, both P<<M and P<<N are true.'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To solidify this concept, it may help to see a concrete example. In the next
    listing, we show a simple example of SVD at work compressing a ‚Ä¶ matrix. For this,
    we only need the basic libraries SciPy and NumPy, which are imported on lines
    1 and 2\. In line 3, we define the matrix, and then in line 9, we apply SVD to
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.2 Example of SVD low-rank approximation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The generated text is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Taking a moment to inspect U, Sigma, and the transpose of V, we see a ‚Ä¶ matrix,
    a ‚Ä¶ matrix, and a ‚Ä¶ matrix, respectively. All in all, we now only need 9 parameters
    versus the original 16, shrinking the memory footprint almost by half.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we multiply the matrices back together to get an approximation of the
    original matrix. In this case, the approximation isn‚Äôt all that great, but we
    can still see that the general order and magnitudes match the original matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The generated text is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Unfortunately, we are not aware of anyone actually using this to compress models
    in production, most likely due to the poor accuracy of the approximation. What
    they are using it for‚Äîand this is important‚Äîis adaptation and finetuning, which
    is where low-rank adaptation (LoRA)[‚Åπ](#footnote-127) comes in. Adaptation is
    the process of finetuning a generic or base model to do a specific task. LoRA
    applies SVD low-rank approximation to the attention weights or, rather, to inject
    update matrices that run parallel to the attention weights, allowing us to finetune
    a much smaller model. LoRA has become very popular because it makes it a breeze
    to take an LLM, shrink the trainable layers to a tiny fraction of the original
    model, and then allow anyone to train it on commodity hardware. You can get started
    with LoRA using the PEFT library from Hugging Face, where you can check out several
    LoRA tutorials.
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE ‚ÄÉFor the extra curious, parameter-efficient finetuning (PEFT) is a class
    of methods aimed at finetuning models in a computationally efficient way. The
    PEFT library seeks to put them all in one easy-to-access place; you can get started
    here: [https://huggingface.co/docs/peft](https://huggingface.co/docs/peft).'
  prefs: []
  type: TYPE_NORMAL
- en: Mixture of experts
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Mixture of experts (MoE) is a technique where we replace the feed-forward layers
    in a transformer with MoE layers instead. Feed-forward layers are notorious for
    being parameter-dense and computationally intensive, so replacing them with something
    better can often have a large effect. MoEs are a group of sparsely activated models.
    They differ from ensemble techniques in that typically only one or a few expert
    models will be run, rather than combining results from all models. The sparsity
    is often induced by a gate mechanism that learns which experts to use and/or a
    router mechanism that determines which experts should even be consulted. In figure
    3.3, we demonstrate the MoE architecture with potentially N experts, as well as
    show where it goes inside a decoder stack.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 Example mixture of an expert's model with both a gate and router
    to control flow. The MoE model is used to replace the FFN layers in a transformer;
    here, we show it replacing the FFN in a decoder.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Depending on how many experts you have, the MoE layer could potentially have
    more parameters than the FFN, leading to a larger model, but in practice, this
    isn‚Äôt the case, since engineers and researchers are aiming to create a smaller
    model. What we are guaranteed to see, though, is a faster computation path and
    improved inference times. However, what really makes MoE stand out is when it‚Äôs
    combined with quantization. One study[^(10)](#footnote-128) between Microsoft
    and NVIDIA showed that 2-bit quantization was reachable with only a minimal effect
    on accuracy using MoE!
  prefs: []
  type: TYPE_NORMAL
- en: Of course, since this is a pretty big change to the model‚Äôs structure, finetuning
    will be required afterward. You should also be aware that MoE layers often reduce
    a model‚Äôs generalizability, so it‚Äôs best when used on models designed for a specific
    task. There are several libraries that implement MoE layers, but we recommend
    checking out DeepSpeed.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE ‚ÄÉDeepSpeed is a library that optimizes many of the hard parts for large-scale
    deep learning models like LLMs and is particularly useful when training. Check
    out their MoE tutorial at [https://www.deepspeed.ai/tutorials/mixture-of-experts/](https://www.deepspeed.ai/tutorials/mixture-of-experts/).
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2 Distributed computing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Distributed computing is a technique used in deep learning to parallelize and
    speed up large, complex neural networks by dividing the workload across multiple
    devices or nodes in a cluster. This approach significantly reduces training and
    inference times by enabling concurrent computation, data parallelism, and model
    parallelism. With the ever-growing size of datasets and complexity of models,
    distributed computing has become crucial for deep learning workflows, ensuring
    efficient resource utilization and enabling researchers to iterate on their models
    effectively. Distributed computing is one of the core practices that separate
    deep learning from machine learning, and with LLMs, we have to pull out every
    trick in the book. Let‚Äôs look at different parallel processing practices to take
    full advantage of distributed computing.
  prefs: []
  type: TYPE_NORMAL
- en: Data parallelism
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Data parallelism is what most people think about when they think about running
    processes in parallel; it‚Äôs also the easiest to do. The practice involves splitting
    up the data and running them through multiple copies of the model or pipeline.
    For most frameworks, this is easy to set up; for example, in PyTorch, you can
    use the `DistributedDataParallel` method. There‚Äôs just one catch for most of these
    setups: your model has to be able to fit onto one GPU. This is where a tool like
    Ray.io comes in.'
  prefs: []
  type: TYPE_NORMAL
- en: Ray.io, or Ray, is an open source project designed for distributed computing,
    specifically aimed at parallel and cluster computing. It‚Äôs a flexible and user-friendly
    tool that simplifies distributed programming and helps developers easily execute
    concurrent tasks in parallel. Ray is primarily built for machine learning and
    other high-performance applications but can be utilized in other applications.
    In listing 3.3, we give a simple example of using Ray to distribute a task. The
    beauty of Ray is the simplicity‚Äîall we need to do to make our code run in parallel
    is add a decorator. It sure beats the complexity of multithreading or asynchronization
    setups.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.3 Example Ray parallelization task
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Starts Ray'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Œîefines a regular Python function'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Turns the function into a Ray task'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Executes the slow function without Ray (takes 10 seconds)'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Executes the slow function with Ray (takes 1 second)'
  prefs: []
  type: TYPE_NORMAL
- en: Ray uses the concepts of tasks and actors to manage distributed computing. Tasks
    are functions, whereas actors are stateful objects that can be invoked and run
    concurrently. When you execute tasks using Ray, it distributes tasks across the
    available resources (e.g., multicore CPUs or multiple nodes in a cluster). For
    LLMs, we would need to set up a Ray cluster in a cloud environment, as this would
    allow each pipeline to run on a node with as many GPUs as needed, greatly simplifying
    the infrastructure set up to run LLMs in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE ‚ÄÉLearn more about Ray clusters here: [https://mng.bz/eVJP](https://mng.bz/eVJP).'
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple alternatives out there, but Ray has been gaining a lot of
    traction and becoming more popular as more and more machine learning workflows
    require distributed training. Teams have had great success with it. By utilizing
    Ray, developers can ensure better performance and more efficient utilization of
    resources in distributed workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Tensor parallelism
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Tensor parallelism takes advantage of matrix multiplication properties to split
    up the activations across multiple processors, running the data through, and then
    combining them on the other side of the processors. Figure 3.4 demonstrates how
    this process works for a matrix, which can be parallelized in two separate ways
    that give us the same result. Imagine that Y is a really big matrix that can‚Äôt
    fit on a single processor or, more likely, a bottleneck in our data flow that
    takes too much time to run all the calculations. In either case, we could split
    Y by columns or rows, run the calculations, and then combine the results. In this
    example, we are dealing with matrices, but in reality, we often deal with tensors
    with more than two dimensions. However, the same mathematical principles that
    make this work still apply.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 Tensor parallelism example showing that you can break up tensors
    by different dimensions and get the same end result. Here, we compare column and
    row parallelism of a matrix.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Choosing which dimension to parallelize is a bit of an art, but there are a
    few things to remember to help make this decision easier. First, how many columns
    or rows do you have? In general, you want to pick a dimension that has more than
    the number of processors you have, or you will end up stopping short. Generally,
    this isn‚Äôt a problem, but with tools like Ray, discussed in the last section,
    parallelizing in a cluster and spinning up loads of processes is a breeze. Second,
    different dimensions have different multiplicity costs. For example, column parallelism
    requires us to send the entire dataset to each process but with the benefit of
    concatenating them together at the end, which is fast and easy. Row parallelism,
    however, allows us to break up the dataset into chunks but requires us to add
    the results, a more expensive operation than concatenating. You can see that one
    operation is more I/O bound, while the other is more computation bound. Ultimately,
    the best dimension will be dataset dependent and hardware limited. It will require
    experimentation to optimize this fully, but a good default is to just choose the
    largest dimension.
  prefs: []
  type: TYPE_NORMAL
- en: Tensor parallelism allows us to split up the heavy computation layers like MLP
    and attention layers onto different devices, but it doesn‚Äôt help us with normalization
    or dropout layers that don‚Äôt utilize tensors. To get better overall performance
    of our pipeline, we can add sequence parallelism that targets these blocks.[^(11)](#footnote-129)
    Sequence parallelism is a process that partitions activations along the sequence
    dimension, preventing redundant storage, and can be mixed with tensor parallelism
    to achieve significant memory savings with minimal additional computational overhead.
    In combination, they reduce the memory needed to store activations in transformer
    models. In fact, they nearly eliminate activation recomputation and save activation
    memory up to five times.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.5 shows how combining tensor parallelism, which allows us to distribute
    the computationally heavy layers, and sequence parallelism, which does the same
    for the memory limiting layers, allows us to fully parallelize the entire transformer
    model. Together, they allow for extremely efficient use of resources.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 Combining tensor parallelism that focuses on computational heavy
    layers with sequence parallelism to reduce memory overhead to create a fully parallel
    process for the entire transformer
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Pipeline parallelism
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: So far, we can run lots of data and speed up any bottlenecks, but none of that
    matters because our model is too big; we can‚Äôt fit it into a single GPU‚Äôs memory
    to even get it to run. That‚Äôs where pipeline parallelism comes in; it‚Äôs the process
    of splitting up a model vertically and putting each part onto a different GPU.
    This creates a pipeline, as input data will go to the first GPU, process, then
    transfer to the next GPU, and so on until it‚Äôs run through the entire model. While
    other parallelism techniques improve our processing power and speed up inference,
    pipeline parallelism is required to get it to run. However, it comes with some
    major downsides, mainly device utilization.
  prefs: []
  type: TYPE_NORMAL
- en: To understand where this downside comes from and how to mitigate it, let‚Äôs first
    consider the naive approach to this, where we simply run all the data at once
    through the model. We find that this leaves a giant ‚Äúbubble‚Äù of underutilization.
    Since the model is broken up, we have to process everything sequentially through
    the devices. This means that while one GPU is processing, the others are sitting
    idle. In figure 3.6, we can see this naive approach and a large bubble of inactivity
    as the GPUs sit idle. We also see a better way to take advantage of each device.
    We do this by sending the data in small batches. A smaller batch allows the first
    GPU to pass on what it was working on quicker and move on to another batch. This
    allows the next device to get started earlier and reduces the size of the bubble.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 The bubble problem. When data runs through a broken-up model, the
    GPUs holding the model weights are underutilized while they wait for their counterparts
    to process the data. A simple way to reduce this bubble is to use microbatching.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We can actually calculate the size of the bubble quite easily with the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: Idle Percentage = 1 ‚Äì m / (m + n ‚Äì 1)
  prefs: []
  type: TYPE_NORMAL
- en: where m is the number of microbatches and n is the depth of the pipeline or
    number of GPUs. So for our naive example case of four GPUs and one large batch,
    we see the devices sitting idle 75% of the time! GPUs are quite expensive to allow
    to sit idle three quarters of the time. Let‚Äôs see what that looks like using the
    microbatch strategy. With a microbatch of 4, it cuts this almost in half, down
    to just 43% of the time. We can glean from this formula that the more GPUs we
    have, the higher the idle times, but the more microbatches, the better the utilization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, we often can neither reduce the number of GPUs nor make the
    microbatches as large as we want. There are limits. For GPUs, we have to use as
    many as it takes to fit the model into memory. However, try to use a few larger
    GPUs, as this will lead to more optimal utilization than using many smaller GPUs.
    Reducing the bubble in pipeline parallelism is another reason why compression
    is so important. For microbatching, the first limit is obvious: since the microbatch
    is a fraction of our batch size, we are limited by how big that is. The second
    is that each microbatch increases the memory demand for cached activations in
    a linear relationship. One way to counter this higher memory demand is a method
    called PipeDream.[^(12)](#footnote-130) There are different configurations and
    approaches, but the basic idea is the same. In this method, we start working on
    the backward pass as soon as we‚Äôve finished the forward pass of any of the microbatches.
    This allows us to fully complete a training cycle and release the cache for that
    microbatch.'
  prefs: []
  type: TYPE_NORMAL
- en: 3D parallelism
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For LLMs, we are going to want to take advantage of all three parallelism practices,
    as they can all be run together. This is known as 3D parallelism, which combines
    data, tensor, and pipeline parallelism (DP + TP + PP) together. Since each technique
    and, thus, dimension will require at least two GPUs to run 3D parallelism, we‚Äôll
    need at least eight GPUs to get started. How we configure these GPUs will be important
    to get the most efficiency out of this process. Because TP has the largest communication
    overhead, we want to ensure these GPUs are close together, preferably on the same
    node and machine. PP has the least communication volume of the three, so breaking
    up the model across nodes is the least expensive here.
  prefs: []
  type: TYPE_NORMAL
- en: By running the three together, we see some interesting interactions and synergies
    between them. Since TP splits the model to work well within a device‚Äôs memory,
    we see that PP can perform well even with small batch sizes due to the reduced
    effective batch size enabled by TP. This combination also improves the communication
    between DP nodes at different pipeline stages, allowing DP to work effectively
    too. The communication bandwidth between nodes is proportional to the number of
    pipeline stages. Consequently, DP can scale well even with smaller batch sizes.
    Overall, we see that when running in combination, we get better performance than
    when we run them individually.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know some tricks of the trade, it‚Äôs just as important to have the
    right tools to do the job.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 LLM operations infrastructure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are finally going to start talking about the infrastructure needed to make
    this all work. This likely comes as a surprise, as we know that some readers would
    have expected this section at the beginning of chapter 1\. Why wait till the end
    of chapter 3? In the many times we‚Äôve interviewed machine learning engineers,
    we have often asked this open-ended question: ‚ÄúWhat can you tell me about MLOps?‚Äù
    An easy softball question to get the conversation going. Most junior candidates
    would immediately start jumping into the tooling and infrastructure. It makes
    sense; there are so many different tools available. That‚Äôs not to mention the
    fact that whenever you see posts or blogs describing MLOps, there‚Äôs a pretty little
    diagram showing the infrastructure. While all of that is important, it‚Äôs useful
    to recognize what a more senior candidate jumps into‚Äîthe machine learning life
    cycle.'
  prefs: []
  type: TYPE_NORMAL
- en: For many, the nuance is lost, but the infrastructure is the *how*, and the life
    cycle is the *why*. Most companies can get by with just bare-bones infrastructure.
    We‚Äôve seen our share of scrappy systems that exist entirely on one data scientist‚Äôs
    laptop, and they work surprisingly well‚Äîespecially in the era of scikit-learn
    everything!
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, a rickshaw machine learning platform doesn‚Äôt cut it in the world
    of LLMs. Since we still live in a world where the standard storage capacity of
    a MacBook Pro laptop is 256 GB, just storing the model locally can already be
    a problem. Companies that invest in a sturdier infrastructure are better prepared
    for the world of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: In figure 3.7, we see an example MLOps infrastructure designed with LLMs in
    mind. While most infrastructure diagrams simplify the structure to make everything
    look clean, the raw truth is that there‚Äôs a bit more complexity to the entire
    system. Of course, a lot of this complexity would disappear if we could get data
    scientists to work inside scripts instead of ad hoc workstations‚Äîusually with
    a Jupyter Notebook interface.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 A high level view of an MLOps infrastructure with LLMs in mind. This
    diagram attempts to cover the full picture and the complexity of the many tools
    involved to make ML models work in production.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Taking a closer look at figure 3.7, you can see several tools on the outskirts
    that squarely land in DataOps or even just DevOps‚Äîdata stores, orchestrators,
    pipelines, streaming integrations, and container registries. These are tools you
    are likely already using for just about any data-intensive application and aren‚Äôt
    necessarily focused on MLOps. Toward the center, we have more traditional MLOps
    tools‚Äîexperiment trackers, model registry, feature store, and ad hoc data science
    workstations. For LLMs, we really only introduce one new tool to the stack: a
    vector database. What‚Äôs not pictured is the monitoring system because it intertwines
    with every piece. This all culminates into what we are working toward in this
    book‚Äîa deployment service where we can confidently deploy and run LLMs in production.'
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure by discipline
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The following list defines infrastructure by the specific discipline:'
  prefs: []
  type: TYPE_NORMAL
- en: '*DevOps*‚ÄîIn charge of procuring the environmental resources: experimental (dev,
    staging) and production. This includes hardware, clusters, and networking to make
    it all work. Also in charge of basic infrastructure systems like Github/Gitlab,
    artifact registries, container registries, application or transactional databases
    like Postgres or MySQL, caching systems, and CI/CD pipelines. This list is by
    no means comprehensive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*DataOps*‚ÄîIn charge of data, in motion and at rest. It includes centralized
    or decentralized data stores like data warehouses, data lakes, and data meshes,
    as well as data pipelines, either in batch systems or in streaming systems with
    tools like Kafka and Flink. It also includes orchestrators like Airflow, Prefect,
    and Mage. DataOps is built on top of DevOps. For example, we‚Äôve seen many CI/CD
    pipelines being used for data pipeline work until eventually graduating to systems
    like Apache Spark or DBT.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*MLOps*‚ÄîIn charge of the machine learning life cycle, from the creation of
    models to deprecation. This includes data science workstations like JupyterHub,
    experiment trackers, and a model registry. It includes specialty databases like
    feature stores and vector databases, as well as a deployment service to tie everything
    together and actually serve results. It is built on top of both DataOps and DevOps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let‚Äôs go through each piece of the infrastructure puzzle and discuss features
    you should consider when thinking about LLMs in particular. While we will be discussing
    specialized tooling for each piece, we‚Äôll note that there are also MLOps as a
    service platform, like Dataiku, Amazon‚Äôs SageMaker, Azure Machine Learning, and
    Google‚Äôs VertexAI. These platforms attempt to complete the whole puzzle; how well
    they do that is another question. However, they are often a great shortcut, and
    you should be aware of them. Well, that‚Äôs enough dillydallying; let‚Äôs dive in
    already!
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.1 Data infrastructure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While not the focus of this book, it‚Äôs important to note that MLOps is built
    on top of a data operations infrastructure, which itself is built on top of DevOps.
    Key features of the DataOps ecosystem include a data store, an orchestrator, and
    pipelines. Additional features usually required include a container registry and
    a streaming integration service.
  prefs: []
  type: TYPE_NORMAL
- en: Data stores are the foundation of DataOps and come in many forms, from a simple
    database to large data warehouses and from even larger data lakes to an intricate
    data mesh. This is where your data is stored, and a lot of work goes into managing,
    governing, and securing the data store. The orchestrator is the cornerstone of
    DataOps, as it‚Äôs a tool that manages and automates both simple and complex multistep
    workflows and tasks, ensuring they run across multiple resources and services
    in a system. The most commonly talked about are Airflow, Prefect, and Mage. Lastly,
    pipelines are the pillars. They hold everything else up and are where we run our
    jobs. Initially built to simply move, clean, and define data, these same systems
    are used to run machine learning training jobs on a schedule and do batch inference
    and loads of other work needed to ensure MLOps runs smoothly.
  prefs: []
  type: TYPE_NORMAL
- en: A container registry is a keystone of DevOps and, subsequently, DataOps and
    MLOps. Running all our pipelines and services in containers is necessary to ensure
    consistency. Streaming services are a much bigger beast than what we may let on
    in this chapter, and if you know, you know. Thankfully, for most text-related
    tasks, real-time processing isn‚Äôt a major concern. Even for tasks like real-time
    captioning or translation, we can often get by with some sort of pseudo‚Äìreal-time
    processing strategy that doesn‚Äôt degrade the user experience depending on the
    task.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.2 Experiment trackers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Experiment trackers are central to MLOps. Experiment trackers do the fundamental
    job of keeping track and recording tests and results. As the famous Adam Savage
    quote from *Myth Busters* states, ‚ÄúRemember, kids, the only difference between
    screwing around and science is writing it down.‚Äù Without it, your organization
    is likely missing the ‚Äúscience‚Äù in data science, which is honestly quite embarrassing.
  prefs: []
  type: TYPE_NORMAL
- en: Even if your data scientists are keen to manually track and record results in
    notebooks, it might as well be thrown in the garbage if it‚Äôs not easy for others
    to view and search for. This is the real purpose of experiment trackers‚Äîto ensure
    knowledge is easily shared and made available. Eventually, a model will make it
    to production, and that model is going to have problems. Sure, you can always
    just train a new model, but unless the team is able to go back and investigate
    what went wrong the first time, you are likely to repeat the same mistakes over
    and over.
  prefs: []
  type: TYPE_NORMAL
- en: There are many experiment trackers out there; the most popular by far is MLFlow,
    which is open source. It was started by the team at Databricks, which also offers
    an easy hosting solution. Some paid alternatives worth checking out include CometML
    and Weights & Biases.
  prefs: []
  type: TYPE_NORMAL
- en: Experiment trackers nowadays come with so many bells and whistles. Most open
    source and paid solutions will certainly have what you need when looking to scale
    up your needs for LLMOps. However, ensuring you take advantage of these tools
    correctly might require a few small tweaks. For example, the default assumption
    is usually that you are training a model from scratch, but often when working
    with LLMs, you will be finetuning models instead. In this case, it‚Äôs important
    to note the checkpoint of the model you started from. If possible, even linking
    back to the original training experiment. This will allow future scientists to
    dig deeper into their test results, find original training data, and discover
    paths forward to eliminate bias.
  prefs: []
  type: TYPE_NORMAL
- en: Another feature to look out for is evaluation metric tooling. We will go into
    greater depth in chapter 4, but evaluation metrics are difficult for language
    models. There are often multiple metrics you care about, and none of them are
    simple, like complexity ratings or similarity scores. While experiment tracker
    vendors try to be agnostic and unopinionated about evaluation metrics, they should
    at least make it easy to compare models and their metrics to help us decide which
    one is better. Since LLMs have become so popular, some have made it easy to evaluate
    the more common metrics like ROUGE for text summarization.
  prefs: []
  type: TYPE_NORMAL
- en: You will also find that many experiment-tracking vendors have started to add
    tools specifically for LLMs. Some features you might consider looking for include
    direct Hugging Face support, LangChain support, prompt engineering toolkits, finetuning
    frameworks, and foundation model shops. The space is developing quickly, and no
    one tool has all the same features right now, but these feature sets will likely
    converge.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.3 Model registry
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The model registry is probably the simplest tool of an MLOps infrastructure.
    The main objective is one that‚Äôs easy to solve; we just need a place to store
    the models. We‚Äôve seen many successful teams get by simply by putting their models
    in an object store or shared filesystem and calling it good. That said, there
    are a couple bells and whistles you should look for when choosing one.
  prefs: []
  type: TYPE_NORMAL
- en: The first is whether the model registry tracks metadata about the model. Most
    of what you care about is going to be in the experiment tracker, so you can usually
    get away with simply ensuring you can link the two. In fact, most model registries
    are built into experiment tracking systems because of this. However, a problem
    with these systems happens when the company decides to use an open source model
    or even buy one. Is it easy to upload a model and tag it with relevant information?
    The answer is usually no.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you want to make sure you can version your models. At some point, a model
    will reach a point where it‚Äôs no longer useful and will need to be replaced. Versioning
    your models will simplify this process. It also makes running production experiments
    like A/B testing or shadow tests easier.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, if we are promoting and demoting models, we need to be concerned with
    access. Models tend to be valuable intellectual property for many companies, so
    ensuring only the right users have access to the models is important. But it‚Äôs
    also important to ensure that only the team that understands the models‚Äîwhat they
    do and why they were trained‚Äîis in charge of promoting and demoting the models.
    The last thing we want is to delete a model in production or worse.
  prefs: []
  type: TYPE_NORMAL
- en: 'For LLMs, there are some important caveats you should be aware of: mainly,
    when choosing a model registry, be aware of any limit sizes. Several model registries
    restrict model sizes to 10 GB or smaller. That‚Äôs just not going to cut it. We
    could speculate on the many reasons for this, but none of them are worthy of note.
    Speaking of limit sizes, if you are going to be running your model registry on
    an on-premise storage system like Ceph, make sure it has lots of space. You can
    buy multiple terabytes of storage for a couple of hundred dollars for your on-premise
    servers, but even a couple of terabytes fills up quickly when your LLM is over
    300 GB. Don‚Äôt forget: you are likely to be keeping multiple checkpoints and versions
    during training and finetuning, as well as duplicates for reliability purposes.
    Storage is still one of the cheapest aspects of running LLMs, though, so there‚Äôs
    no reason to skimp here and cause headaches down the road.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This brings me to a good point: a lot of optimization could still be made,
    allowing for better space-saving approaches to storing LLMs and their derivatives,
    especially since most of these models will be very similar overall. We‚Äôll likely
    see storage solutions to solve just this problem in the future.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.4 Feature stores
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Feature stores solve many important problems and answer questions like, Who
    owns this feature? How was it defined? Who has access to it? Which models are
    using it? How do we serve this feature in production? Essentially, they solve
    the ‚Äúsingle source of truth‚Äù problem. Creating a centralized store allows teams
    to shop for the highest quality, most well-maintained, thoroughly managed data.
    Feature stores solve the problems of collaboration, documentation, and versioning
    of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you‚Äôve ever thought, ‚ÄúA feature store is just a database, right?‚Äù, you are
    probably thinking about the wrong type of store‚Äîwe are referencing a place to
    shop, not a place of storage. Don‚Äôt worry: this confusion is normal, as we‚Äôve
    heard this sentiment a lot and have had similar thoughts ourselves. The truth
    is that modern-day feature stores are more virtual than a physical database, which
    means they are built on top of whatever data store you are already using. For
    example, Google‚Äôs Vertex AI feature store is just BigQuery, and we‚Äôve seen a lot
    of confusion from data teams wondering, ‚ÄúWhy don‚Äôt we just query BigQuery?‚Äù Loading
    the data into a feature store feels like an unnecessary extra step, but think
    about shopping at an IKEA store. No one goes directly to the warehouse where all
    the furniture is in boxes. That would be a frustrating shopping experience. The
    features store is the showroom that allows others in your company to easily peruse,
    experience, and use the data.'
  prefs: []
  type: TYPE_NORMAL
- en: Often, we see people reach for a feature store to solve a technical problem
    like low latency access for online feature serving. A huge win for feature stores
    is solving the training-serving skew. Some features are just easier to do in SQL
    after the fact, like calculating the average number of requests for the last 30
    seconds. This can lead to naive data pipelines being built for training but causing
    massive headaches when going to production because getting this type of feature
    in real time can be anything but easy. Feature store abstractions help minimize
    this burden. Related to this are feature store point-in-time retrievals, which
    are table stakes when talking feature stores. Point-in-time retrievals ensure
    that, given a specific time, a query will always return the same result. This
    is important because features like averages over ‚Äúthe last 30 seconds‚Äù are constantly
    changing, so this allows us to version the data (without the extra burden of a
    bloated versioning system), as well as ensure our models will give accurate and
    predictable responses.
  prefs: []
  type: TYPE_NORMAL
- en: As far as options, Feast is a popular open source feature store. Featureform
    and Hopsworks are also open source. All three offer paid hosting options. For
    LLMs, we‚Äôve heard the sentiment that feature stores aren‚Äôt as critical as other
    parts of the MLOps infrastructure. After all, the model is so large that it should
    incorporate all the features needed inside it, so you don‚Äôt need to query for
    additional context. Just give the model the user‚Äôs query, and let the model do
    its thing. However, this approach is still a bit naive, and we haven‚Äôt quite gotten
    to a point where LLMs are completely self-sufficient. To avoid hallucinations
    and improve factual correctness, it is often best to give the model some context.
    We do this by feeding it embeddings of our documents that we want it to know very
    well, and a feature store is a great place to put these embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.5 Vector databases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you are familiar with the general MLOps infrastructure, most of this section
    has been review for you. We‚Äôve only had to make slight adjustments highlighting
    important scaling concerns to make a system work for LLMs. Vector databases, however,
    are new to the scene and have been developed to be a tailored solution for working
    with LLMs and language models in general, but you can also use them with other
    datasets like images or tabular data, which are easy enough to transform into
    a vector. Vector databases are specialized databases that store vectors along
    with some metadata around the vector, which makes them great for storing embeddings.
    Now, while that last sentence is true, it is a bit misleading because the power
    of vector databases isn‚Äôt in their storage but in the way that they search through
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Traditional databases, using b-tree indexing to find IDs or text-based search
    using reverse indexes, all have the same common flaw: you have to know what you
    are looking for. If you don‚Äôt have the ID or you don‚Äôt know the keywords, it‚Äôs
    impossible to find the right row or document. Vector databases, however, take
    advantage of the vector space, meaning you don‚Äôt need to know exactly what you
    are looking for; you just need to know something similar, which you can then use
    to find the nearest neighbors using similarity searches based on Euclidean distance,
    cosine similarity, dot product similarity, or what have you. For example, using
    a vector database makes solving the reverse image search problem a breeze.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, some readers may be confused. First, we told you to put your
    embeddings into a feature store, and now we‚Äôre telling you to put them into a
    Vector DB. Which one is it? Well, that‚Äôs the beauty of it: you can do both at
    the same time. If it didn‚Äôt make sense before, we hope it makes sense now. A feature
    store is not a database; it is just an abstraction. You can use a feature store
    built on top of a vector DB, and it will solve many of your problems. Vector DBs
    can be difficult to maintain when you have multiple data sources, are experimenting
    with different embedding models, or otherwise have frequent data updates. Managing
    this complexity can be a real pain, but a feature store can handily solve this
    problem. Using them in combination will ensure a more accurate and up-to-date
    search index.'
  prefs: []
  type: TYPE_NORMAL
- en: Vector databases have only been around for a couple of years at the time of
    writing, and their popularity is still relatively new, as they have grown hand
    in hand with LLMs. It‚Äôs easy to understand why since they provide a fast and efficient
    way to retrieve vector data, making it simple to provide LLMs with needed context
    to improve their accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'That said, it‚Äôs a relatively new field, and there are lots of competitors in
    this space right now. It‚Äôs a bit too early to know who the winners and losers
    are. Not wanting to date this book too much, let me at least suggest two options
    to start: Pinecone and Milvus. Pinecone is one of the first vector databases as
    a product and has a thriving community with lots of documentation. It‚Äôs packed
    with features and has proven itself to scale. Pinecone is a fully managed infrastructure
    offering that has a free tier for beginners to learn. If you are a fan of open
    source, however, then you‚Äôll want to check out Milvus. Milvus is feature rich
    and has a great community. Zilliz, the company behind Milvus, offers a fully managed
    offering, but it‚Äôs also available to deploy in your own clusters. If you already
    have a bit of infrastructure experience, it‚Äôs relatively easy and straightforward
    to do.'
  prefs: []
  type: TYPE_NORMAL
- en: There are lots of alternatives out there right now, and it‚Äôs likely worth a
    bit of investigation before picking one. The two things you‚Äôll probably care most
    about are price and scalability, as the two often go hand in hand. After that,
    it‚Äôs valuable to pay attention to search features, such as support for different
    similarity measures like cosine similarities, dot product, and Euclidean distance,
    as well as indexing features like Hierarchical Navigable Small World (HNSW) and
    locality-sensitive hashing (LSH). Being able to customize your search parameters
    and index settings is important for any database, as you can customize the workload
    for your dataset and workflow, allowing you to optimize query latency and search
    result accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: It‚Äôs also important to note that with the rise of vector databases, we are quickly
    seeing many database incumbents like Redis and Elastic offering vector search
    capabilities. For now, most of these tend to offer the most straightforward feature
    sets, but they are hard to ignore if you are already using these tool sets, as
    they can provide quick wins to help you get started quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Vector databases are powerful tools that can help you train or finetune LLMs,
    as well as improve the accuracy and results of your LLM queries.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.6 Monitoring system
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A monitoring system is crucial to the success of any ML system, LLMs included.
    Unlike other software applications, ML models are known to fail silently‚Äîthat
    is, continue to operate but start to give poor results. This is often due to data
    drift, a common example being a recommendation system that gives worse results
    over time because sellers start to game the system by giving fake reviews to get
    better recommendation results. A monitoring system allows us to catch poorly performing
    models and make adjustments or simply retrain them.
  prefs: []
  type: TYPE_NORMAL
- en: Despite their importance, monitoring systems are often the last piece of the
    puzzle added. This is often purposeful, as putting resources into figuring out
    how to monitor models doesn‚Äôt help if you don‚Äôt have any models to monitor. However,
    don‚Äôt make the mistake of putting it off too long. Many companies have been burned
    by a model that went rogue with no one knowing about it, often costing them dearly.
    It‚Äôs also important to realize you don‚Äôt have to wait to get a model into production
    to start monitoring your data. There are plenty of ways to introduce a monitoring
    system into the training and data pipelines to improve data governance and compliance.
    Regardless, you can usually tell the maturity of a data science organization by
    its monitoring system.
  prefs: []
  type: TYPE_NORMAL
- en: There are lots of great monitoring toolings out there; some great open source
    options include whylogs and Evidently AI. We are also fans of Great Expectations
    but have found it rather slow outside of batch jobs. There are also many more
    paid options out there. Typically, for ML monitoring workloads, you‚Äôll want to
    monitor everything you‚Äôd normally record in other software applications; this
    includes resource metrics like memory and CPU utilization, performance metrics
    like latency and queries per second, and operational metrics like status codes
    and error rates. In addition, you‚Äôll need ways to monitor data drift going in
    and out of the model. You‚Äôll want to pay attention to things like missing values,
    uniqueness, and standard deviation shifts. In many instances, you‚Äôll want to be
    able to segment your data while monitoring‚Äîfor example, for A/B testing or monitoring
    by region. Some metrics useful to monitor in ML systems include model accuracy,
    precision, recall, and F1 scores. These are difficult since you won‚Äôt know the
    correct answer at inference time, so it‚Äôs often helpful to set up some sort of
    auditing system. Of course, auditing will be easier if your LLM is designed to
    be a Q&A bot rather than if your LLM is built to help writers be more creative.
  prefs: []
  type: TYPE_NORMAL
- en: This hints at a whole set of new challenges for your monitoring systems, even
    more than what we see with other ML systems. With LLMs, we are dealing with text
    data, which is hard to quantify, as discussed earlier in this chapter. For instance,
    consider the features you look at to monitor for data drift, because language
    is known to drift a lot! One feature we suggest is unique tokens. These will alert
    you when new slang words or terms are created; however, they still don‚Äôt help
    when words switch meaning, for example, when ‚Äúwicked‚Äù means ‚Äúcool.‚Äù We would also
    recommend monitoring the embeddings; however, you‚Äôll likely find this to either
    add a lot of noise and false alarms or, at the very least, be difficult to decipher
    and dig into when problems do occur. The systems that work the best often involve
    a lot of handcrafted rules and features to monitor, but these can be error-prone
    and time-consuming to create.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring text-based systems is far from a solved problem, mostly stemming
    from the difficulties in understanding text data to begin with. This begs the
    question of what the best methods are to use language models to monitor themselves
    since they are our current best solution to codifying language. Unfortunately,
    we‚Äôre not aware of anyone researching this, but we imagine it‚Äôs only a matter
    of time.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.7 GPU-enabled workstations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GPU-enabled workstations and remote workstations in general are often considered
    a nice-to-have or luxury by many teams, but when working with LLMs, that mindset
    has to change. When troubleshooting a problem or just developing a model in general,
    a data scientist isn‚Äôt able to spin up the model in a notebook on their laptop
    anymore. The easiest way to solve this is to simply provide remote workstations
    with GPU resources. There are plenty of cloud solutions for this, but if your
    company is working mainly on-premise, it may be a bit more difficult to provide
    but necessary nonetheless.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are GPU memory-intensive models. Consequently, there are some numbers every
    engineer should know when it comes to working in the field. The first is how many
    GPUs to have. The NVIDIA Tesla T4 and V100 are the two most common GPUs you‚Äôll
    find in a datacenter, but they only have 16 GB of memory. They are workhorses,
    though, and cost-effective, so if we can compress our model to run on these, all
    the better. After these, you‚Äôll find a range of GPUs like NVIDIA A10G, NVIDIA
    Quadro series, and NVIDIA RTX series that offer GPU memories in the ranges of
    24, 32, and 48 GB. All of these are fine upgrades; you‚Äôll just have to figure
    out which ones are offered by your cloud provider and available to you. This brings
    us to the NVIDIA A100, which is likely going to be your GPU of choice when working
    with LLMs. Thankfully, they are relatively common and offer two different models
    providing 40 or 80 GB. A big problem with these is that they are constantly in
    high demand by everyone right now. You should also be aware of the NVIDIA H100,
    which offers 80 GB like the A100\. The H100 NVL is promised to support up to 188
    GB and has been designed with LLMs in mind. Another new GPU you should be aware
    of is the NVIDIA L4 Tensor Core GPU, which has 24 GB and is positioned to take
    over as a new workhorse along with the T4 and V100, at least as far as AI workloads
    are concerned.
  prefs: []
  type: TYPE_NORMAL
- en: 'LLMs come in all different sizes, and it‚Äôs useful to have a horse sense for
    what these numbers mean. For example, the LLaMA model has 7B, 13B, 33B, and 65B
    parameter variants. If you aren‚Äôt sure off the top of your head which GPU you
    need to run which model, here‚Äôs a shortcut: multiply the number of billions of
    parameters by two, and that‚Äôs how much GPU memory you need. The reason is that
    most models at inference will default to run at half precision, FP16 of BF16,
    which means we need at least 2 bytes for every parameter. For example, 7 billion
    √ó 2 bytes = 14 GB. You‚Äôll need a little extra as well for the embedding model,
    which will be about another gigabyte, and more for the actual tokens you are running
    through the model. One token is about 1 MB, so 512 tokens will require 512 MB.
    This isn‚Äôt a big deal until you consider running larger batch sizes to improve
    performance. For 16 batches of this size, you‚Äôll need an extra 8 GB of space.'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, so far, we‚Äôve only been talking about inference; for training, you‚Äôll
    need a lot more space. While training, you‚Äôll always want to do this in full precision,
    and you‚Äôll need extra room for the optimizer tensors and gradients. In general,
    to account for this, you‚Äôll need about 16 bytes for every parameter. So to train
    a 7B parameter model, you‚Äôll want 112 GB of memory.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.8 Deployment service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Everything we‚Äôve been working toward is collected and finally put to good use
    here. In fact, if you took away every other service and were left with just a
    deployment service, you‚Äôd still have a working MLOps system. A deployment service
    provides an easy way to integrate with all the previous systems we talked about
    and to configure and define the needed resources to get our model running in production.
    It will often provide boilerplate code to serve the model behind a REST and gRPC
    API or directly inside a batch or streaming pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Some tools to help create this service include NVIDIA Triton Inference Service,
    MLServer, Seldon, and BentoML. These services provide a standard API interface,
    typically the KServe V2 Inference Protocol. This protocol provides a unified and
    extensible way to deploy, manage, and serve machine learning models across different
    platforms and frameworks. It defines a common interface to interact with models,
    including gRPC and HTTP/RESTful APIs. It standardizes concepts like input/output
    tensor data encoding, predict and explain methods, model health checks, and metadata
    retrieval. It also allows seamless integration with languages and frameworks,
    including TensorFlow, PyTorch, ONNX, Scikit Learn, and XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, there are times when flexibility and customization provide enough
    value to step away from the automated path these other frameworks provide, in
    which case it‚Äôs best to reach for a tool like FastAPI. Your deployment service
    should still provide as much automation and boilerplate code here as possible
    to make the process as smooth as possible. It should be mentioned that most of
    the previously mentioned frameworks do offer custom methods, but your mileage
    may vary.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a model is more than just building the interface. Your deployment
    service will also provide a bridge to close the gap between the MLOps infrastructure
    and general DevOps infrastructure. The connection to whatever CI/CD tooling and
    build and shipping pipelines your company has set up so you can ensure appropriate
    tests and deployment strategies like health checks and rollbacks can easily be
    monitored and done. This is often very platform- and thus company-specific. It
    must also provide the needed configurations to talk to Kubernetes or whatever
    other container orchestrator you may be using to acquire the needed resources
    like CPU, memory, accelerators, autoscalers, proxies, etc. It also applies the
    needed environment variables and secret management tools to ensure everything
    runs.
  prefs: []
  type: TYPE_NORMAL
- en: All in all, this service ensures you can easily deploy a model into production.
    For LLMs, the main concern is often just being sure the platform and clusters
    are set up with enough resources to provision what will ultimately be configured.
  prefs: []
  type: TYPE_NORMAL
- en: We‚Äôve discussed a lot so far in this chapter, starting with what makes LLMs
    so much harder than traditional ML, which is hard enough as it is. First, we learned
    that their size can‚Äôt be underestimated, but then we also discovered many peculiarities
    about them, from token limits to hallucinations‚Äînot to mention that they are expensive.
    Fortunately, despite being difficult, they aren‚Äôt impossible. We discussed compression
    techniques and distributed computing, which are crucial to master. We then explored
    the infrastructure needed to make LLMs work. While most of it was likely familiar,
    we came to realize that LLMs put a different level of pressure on each tool, and
    often, we need to be ready for a larger scale than what we could get away with
    for deploying other ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs are difficult to work with mostly because they are big. This results in
    a longer time to download, load into memory, and deploy, forcing us to use expensive
    resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs are also hard because they deal with natural language and all its complexities,
    including hallucinations, bias, ethics, and security.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regardless of whether you build or buy, LLMs are expensive, and managing costs
    and risks associated with them will be crucial to the success of any project utilizing
    them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compressing models to be as small as we can makes them easier to work with;
    quantization, pruning, and knowledge distillation are particularly useful for
    this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantization is popular because it is easy and can be done after training without
    any finetuning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low-rank approximation is an effective way to shrink a model and has been used
    heavily for adaptation, thanks to LoRA.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are three core directions we use to parallelize LLM workflows: data,
    tensor, and pipeline. DP helps us increase throughput, TP helps us increase speed,
    and PP makes it all possible to run in the first place.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining the parallelism methods, we get 3D parallelism (data + tensor + pipeline),
    where we find that the techniques synergize, covering each other‚Äôs weaknesses
    and helping us get more utilization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The infrastructure for LLMOps is similar to MLOps, but don‚Äôt let that fool you,
    since there are many caveats where ‚Äúgood enough‚Äù no longer works.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many tools have begun to offer new features specifically for LLM support.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector databases, in particular, are interesting as a new piece of the infrastructure
    puzzle needed for LLMs that allow quick search and retrievals of embeddings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1]](#footnote-source-1) A. Bulatov, Y. Kuratov, and M. S. Burtsev, ‚ÄúScaling
    transformer to 1M tokens and beyond with RMT,‚Äù April 2023, [https://arxiv.org/abs/2304.11062](https://arxiv.org/abs/2304.11062).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[2]](#footnote-source-2) R. Daws, ‚ÄúMedical chatbot using OpenAI‚Äôs GPT-3 told
    a fake patient to kill themselves,‚Äù *AI News*, October 28, 2020, [https://mng.bz/qO6z](https://mng.bz/qO6z).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[3]](#footnote-source-3) T. Kington, ‚ÄúChatGPT bot tricked into giving bomb-making
    instructions, say developers,‚Äù *The Times*, December 17, 2022, [https://mng.bz/7d64](https://mng.bz/7d64).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[4]](#footnote-source-4) K. Quach, ‚ÄúAI game bans players for NSFW stories
    it generated itself,‚Äù The Register, October 8, 2021, [https://www.theregister.com/2021/10/08/ai_game_abuse/](https://www.theregister.com/2021/10/08/ai_game_abuse/).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[5]](#footnote-source-5) T. Hoefler, D. Alistarh, T. Ben-Nun, N. Dryden, and
    A. Peste, ‚ÄúSparsity in deep learning: Pruning and growth for efficient inference
    and training in neural networks,‚Äù January 2021, [https://arxiv.org/abs/2102.00554](https://arxiv.org/abs/2102.00554).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[6]](#footnote-source-6) E. Frantar and D. Alistarh, ‚ÄúSparseGPT: Massive Language
    models can be accurately pruned in one-shot,‚Äù January 2023, [https://arxiv.org/abs/2301.00774](https://arxiv.org/abs/2301.00774).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[7]](#footnote-source-7) V. Sanh, L. Debut, J. Chaumond, and T. Wolf, ‚ÄúDistilBERT,
    a distilled version of BERT: smaller, faster, cheaper and lighter,‚Äù October 2019,
    [https://arxiv.org/abs/1910.01108](https://arxiv.org/abs/1910.01108).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[8]](#footnote-source-8) R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li,
    C. Guestrin, P Liang, and T. B. Hashimoto, ‚ÄúAlpaca: A strong, replicable instruction-following
    model,‚Äù CRFM, 2023, [https://crfm.stanford.edu/2023/03/13/alpaca.xhtml](https://crfm.stanford.edu/2023/03/13/alpaca.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[9]](#footnote-source-9) E. J. Hu et al., ‚ÄúLoRA: Low-rank adaptation of large
    language models.,‚Äù June 2021, [https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[10]](#footnote-source-10) R. Henry and Y. J. Kim, ‚ÄúAccelerating large language
    models via low-bit quantization,‚Äù March 2023, [https://mng.bz/maD0](https://mng.bz/maD0).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[11]](#footnote-source-11) V. Korthikanti et al., ‚ÄúReducing activation recomputation
    in large transformer models,‚Äù May 2022, [https://arxiv.org/abs/2205.05198](https://arxiv.org/abs/2205.05198).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[12]](#footnote-source-12) A. Harlap et al., ‚ÄúPipeDream: Fast and efficient
    pipeline parallel DNN training,‚Äù June 8, 2018, [https://arxiv.org/abs/1806.03377](https://arxiv.org/abs/1806.03377).'
  prefs: []
  type: TYPE_NORMAL
