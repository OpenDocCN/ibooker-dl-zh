<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 6. Fine-Tuning"><div class="chapter" id="llm-fine-tuning">
<h1><span class="label">Chapter 6. </span>Fine-Tuning</h1>


<p>In the previous chapter, we discussed the various factors<a data-type="indexterm" data-primary="fine-tuning models" id="xi_finetuningmodels6558"/> that need to be taken into account while choosing the right LLM for your specific needs, including pointers on how to evaluate LLMs to be able to make an informed choice. Next, let us utilize these LLMs to solve our tasks.</p>

<p>In this chapter, we will explore the process of adapting an LLM to solve your task of interest, using fine-tuning. We will go through a full example of fine-tuning, covering all the important decisions one needs to make. We will also discuss the art and science of creating fine-tuning datasets.</p>






<section data-type="sect1" data-pdf-bookmark="The Need for Fine-Tuning"><div class="sect1" id="id244">
<h1>The Need for Fine-Tuning</h1>

<p>Why do we need to fine-tune LLMs? Why doesn’t a pre-trained LLM with few-shot prompts suffice for our needs? Let us look at a couple of examples to drive the point home:</p>
<dl>
<dt>Use Case 1</dt>
<dd>
<p>Consider you are working on the rather whimsical task of detecting all sentences written in the past tense within a body of text and transforming them to future tense. To solve this task, you might provide a few examples of past tense sentences and input-output pairs representing past tense and their corresponding future tense sentences. However, the LLM doesn’t seem to be able to tackle this task to your satisfaction, making mistakes in both the identification and transformation steps. In response, you elaborate on your instructions, adding grammar rules and exceptions in the English language into your prompt. You notice an increase in performance. But with each new rule added, your prompt balloons, slowly turning into a grammar mini-book.</p>

<p>As we saw in <a data-type="xref" href="ch05.html#chapter_utilizing_llms">Chapter 5</a>, the LLM can adhere to only a finite set of instructions in the prompt, and its effective context window is much smaller than the advertised context window<a data-type="indexterm" data-primary="context window" data-secondary="and need for fine-tuning" data-secondary-sortas="need for fine-tuning" id="id1009"/>. We have hit an impasse.</p>
</dd>
<dt>Use Case 2</dt>
<dd>
<p>Consider a task that deals with answering questions from content in financial text. LLMs are not financial experts and have difficulty dealing with financial jargon. To address this, you add the definitions of key financial terms in the prompt. While you notice a small improvement in performance, it is not long before you realize you need to stuff the entire curriculum of the CPA exam into your measly context window to achieve the desired gains.</p>

<p>This is where fine-tuning comes in. By providing a dataset of input-output pairs, such that the model learns the input-output mapping by updating its weights, you can accomplish tasks that cannot be performed by in-context learning alone. For both the tasks mentioned above, fine-tuning the model massively improves performance.</p>
</dd>
</dl>

<p>When should fine-tuning not be used? If your primary goal is to impart new or updated facts or knowledge to the language model, this is better served with techniques like RAG, which we will explore in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch10.html#ch10">10</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch12.html#ch12">12</a>. Fine-tuning is best suited for situations where you need the model to learn a particular input-output mapping, be familiarized to a new textual domain, or exhibit more complex capabilities and behavior.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Recall from <a data-type="xref" href="ch05.html#chapter_utilizing_llms">Chapter 5</a> that updating a language model’s parameters can cause the base model capabilities to regress! Fine-tuning a model on one task can inadvertently cause the base model to perform worse on other tasks. Handle with care.</p>
</div>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Fine-Tuning: A Full Example"><div class="sect1" id="id92">
<h1>Fine-Tuning: A Full Example</h1>

<p>Let’s walk through a practical fine-tuning example<a data-type="indexterm" data-primary="fine-tuning models" data-secondary="example" id="xi_finetuningmodelsexample63051"/> from start to finish. We would like to train a <em>political promises detector</em>, which can be used to identify promises made by representatives of the ruling party in campaign speeches or parliamentary proceedings. We define a political promise as something that is tangible, specific, and an action that the government has the agency to make.</p>

<p>An example of such a sentence is: “We will build 10,000 kilometres of subway lines in the next ten years.”</p>

<p>However, not all future tense or forward-looking statements are promises. The following sentences are not promises, per our definition:</p>
<ul class="simplelist">
  <li>“We expect the Japanese to increase tariffs next year.” (expectation, and not something the government can control)</li>

<li>“We will work toward making Canada a better place.” (no specifics provided)</li>

<li>“AI will cause the loss of a million jobs next year.” (prediction, not promise)</li></ul>

<p>Our base LLM, Llama2-7B, finds it difficult to accurately identify such promises in an in-context learning setup. Therefore, we will fine-tune it for this specific task. We can then use the resulting model to detect political promises, and then match those promises against structured datasets or budgetary text to track whether these promises have been fulfilled over a period of time.</p>

<p>To this end, I have constructed a synthetic fine-tuning dataset containing examples of both promises and mere statements. Later in this chapter, we will go through the process of creating such a dataset.</p>

<p>Fortunately, fine-tuning today is easier due to the existence of several libraries that streamline the fine-tuning process. The most important of these libraries<a data-type="indexterm" data-primary="Hugging Face" data-secondary="fine-tuning libraries" id="id1010"/><a data-type="indexterm" data-primary="Transformers library" id="id1011"/><a data-type="indexterm" data-primary="PEFT (parameter-efficient fine-tuning)" id="id1012"/><a data-type="indexterm" data-primary="TRL library" id="id1013"/><a data-type="indexterm" data-primary="bitsandbytes library" id="id1014"/><a data-type="indexterm" data-primary="Accelerate" id="id1015"/><a data-type="indexterm" data-primary="Hugging Face" data-secondary="Accelerate" id="id1016"/><a data-type="indexterm" data-primary="parameter-efficient fine-tuning (PEFT)" id="id1017"/><a data-type="indexterm" data-primary="fine-tuning models" data-secondary="parameter efficient" id="id1018"/> are <a href="https://oreil.ly/BTi76">Transformers</a>, <a href="https://oreil.ly/W8oLi">Accelerate</a>, <a href="https://oreil.ly/QbQoq">PEFT</a>, <a href="https://oreil.ly/Ya9Xj">TRL</a>, and <a href="https://oreil.ly/ruVEX">bitsandbytes</a>. The first four are from Hugging Face. You have encountered many of these libraries in prior chapters already. Being familiar with the inner workings of these libraries is a very useful skill.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Given that these libraries are relatively new and are part of a fast-moving field, they frequently undergo substantial updates. I recommend keeping in touch with major updates of these libraries, as they continue to introduce enhancements that will simplify your workflow.</p>
</div>

<p>Let’s begin by loading the dataset. The custom dataset can be downloaded from this book’s <a href="https://oreil.ly/llm-playbooks">GitHub repo</a>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">datasets</code> <code class="kn">import</code> <code class="n">load_dataset</code>
<code class="n">tune_data</code> <code class="o">=</code> <code class="n">load_dataset</code><code class="p">(</code><code class="s2">"csv"</code><code class="p">,</code> <code class="n">data_files</code><code class="o">=</code><code class="s1">'/path/to/finetune_data.csv'</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>I highly recommend using the <a href="https://oreil.ly/3LX5X"><em>datasets</em> library</a> for loading your training and fine-tuning datasets, as it is an excellent abstraction for efficiently loading large datasets, abstracting away memory management details.</p>
</div>

<p>Next, let us set some relevant hyperparameters in the <code>Transformers</code> library through the <code>TrainingArguments</code> class:</p>

<pre data-type="programlisting" data-code-language="python" class="less_space pagebreak-before"><code class="c1"># Make sure you have installed the correct version</code>
<code class="err">!</code><code class="n">pip</code> <code class="n">install</code> <code class="n">transformers</code><code class="o">==</code><code class="mf">4.35.0</code>

<code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">TrainingArguments</code></pre>

<p>There are more than a hundred arguments available; we will go through the important ones. The arguments relate to the learning algorithms used, memory and space optimizations, quantization, regularization, and distributed training. Let’s explore these in detail.</p>








<section data-type="sect2" data-pdf-bookmark="Learning Algorithms Parameters"><div class="sect2" id="id93">
<h2>Learning Algorithms Parameters</h2>

<p>Let’s explore optimization algorithms used for training the network and learn how to choose the <a data-type="indexterm" data-primary="fine-tuning models" data-secondary="learning algorithms parameters" id="xi_finetuningmodelslearningalgorithmsparameters68590"/><a data-type="indexterm" data-primary="learning algorithms parameters" id="xi_learningalgorithmsparameters68590"/><a data-type="indexterm" data-primary="parameters" data-secondary="learning algorithms" id="xi_parameterslearningalgorithms68590"/>right one for our purposes.</p>










<section data-type="sect3" data-pdf-bookmark="Optimizers"><div class="sect3" id="id94">
<h3>Optimizers</h3>

<p>AdamW<a data-type="indexterm" data-primary="AdamW" id="id1019"/><a data-type="indexterm" data-primary="Adafactor" id="id1020"/> and Adafactor are currently the most used optimizers. Other popular optimization algorithms include stochastic gradient descent (SGD)<a data-type="indexterm" data-primary="stochastic gradient descent (SGD)" id="id1021"/><a data-type="indexterm" data-primary="SGD (stochastic gradient descent)" id="id1022"/>, RMSProp, Adagrad, Lion, and their variants. For more background on optimization algorithms, refer to Florian June’s <a href="https://oreil.ly/VTiDa">blog post</a>.</p>

<p>Adafactor and SGD use four bytes of memory per parameter, while AdamW uses eight bytes per parameter. This means that a 7B model undergoing full fine-tuning with the AdamW optimizer requires 7 * 8 = ~56GB of memory to store the optimizer states alone. Even more memory is needed to store the parameters, gradients, and the forward activations.</p>

<p>More recently<a data-type="indexterm" data-primary="quantization" data-secondary="optimization with" id="id1023"/><a data-type="indexterm" data-primary="optimization and optimizers" data-secondary="quantization" id="id1024"/>, <a href="https://oreil.ly/4Z14D">8-bit optimizers</a> have been introduced that perform quantization of the optimizer state. A 7B model undergoing full fine-tuning with the AdamW 8-bit version requires only ~14GB of memory for the optimizer state.</p>

<p>These 8-bit optimizers are available through the bitsnbytes<a data-type="indexterm" data-primary="bitsandbytes library" id="id1025"/> library and are also supported by Hugging Face. For using the 8-bit AdamW version, you can set in the <code>TrainingArguments</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">optim</code> <code class="o">=</code> <code class="s1">'adamw_bnb_8bit'</code></pre>

<p>For all the optimizer options directly available through Hugging Face, refer to the <a href="https://oreil.ly/7kdSO">OptimizerNames class</a>.</p>
<div data-type="tip"><h6>Tip</h6>
<p>In his benchmarking experiments, Stas Bekman<a data-type="indexterm" data-primary="Bekman, Stas" id="id1026"/> <a href="https://oreil.ly/0_0lt">shows</a> that surprisingly, the 8-bit AdamW optimizer is actually faster than the standard AdamW optimizer. His experiments also show that Adafactor is slightly slower than AdamW overall.</p>
</div>

<p>The default optimizer provided in the Hugging Face <code>TrainingArguments</code> class is AdamW. For most cases, the default optimizer works just fine. However, if it doesn’t, you can try Adafactor and Lion<a data-type="indexterm" data-primary="Lion optimizer" id="id1027"/>. For reinforcement learning, SGD seems to work well.</p>

<p>If you are especially memory constrained, 8-bit AdamW is a compelling choice. If available, the paged version of these optimizers will further mitigate your memory requirements.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1028">
<h1>Paged Optimizers</h1>
<p>Using AdamW as an optimizer requires eight bytes of memory per parameter, which is a significant drag on memory requirements. This affects the maximum sequence length that can be supported. This is where paged optimizers<a data-type="indexterm" data-primary="paged optimizers" id="id1029"/><a data-type="indexterm" data-primary="optimization and optimizers" data-secondary="paged optimizers" id="id1030"/> can come in handy. In cases where the GPU runs out of memory during fine-tuning, paged optimizers automatically transfer memory pages to CPU RAM, then transfer them back to GPU memory when it is needed.</p>

<p>In Hugging Face<a data-type="indexterm" data-primary="Hugging Face" data-secondary="paged optimizers" id="id1031"/>, paged variants are available for AdamW and Lion, and can be accessed using optimizer names <em>paged_adamw_32bit</em>, <em>paged_adamw_8bit</em>, <em>paged_lion</em>, and <em>paged_lion_8bit</em>, respectively.</p>
</div></aside>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Learning rates"><div class="sect3" id="id245">
<h3>Learning rates</h3>

<p>For each optimizer, certain learning rates<a data-type="indexterm" data-primary="learning rate" data-secondary="optimization" id="id1032"/><a data-type="indexterm" data-primary="optimization and optimizers" data-secondary="learning rate" id="id1033"/> have been shown to be very effective. A recommended learning rate for AdamW is 1e-4 with a weight decay of 0.01. Weight decay is a regularization technique that helps reduce overfitting. Similarly, the default values for minor optimizer parameters like <em>adam_beta1</em>, <em>adam_beta2</em>, and <em>adam_epsilon</em> are good enough and need not be changed.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1034">
<h1>Exercise</h1>
<p>Learning rate rules for fine-tuning models might differ from those used for training neural networks from scratch. Read the paper <a href="https://oreil.ly/T1xNB">“Rethinking Learning Rate Tuning in the Era of Large Language Models”</a> by Jin et al., which provides a good survey of the collective wisdom on learning rates developed by the LLM research community.</p>

<p>Additionally, play with automated learning rate optimization tools like PyTorch Lightning’s <a href="https://oreil.ly/_zdA9">LearningRateFinder</a>.</p>
</div></aside>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Learning schedules"><div class="sect3" id="id95">
<h3>Learning schedules</h3>

<p>Toward the end of the training process<a data-type="indexterm" data-primary="learning rate" data-secondary="schedule and schedulers" id="xi_learningratescheduleandschedulers613239"/>, it is a good idea to lower the learning rate because you do not want to overshoot when you are so close to convergence. In a similar vein, you would like to prevent your model from learning too much from the first few batches of examples. In either case, we would like to be able to automatically adjust the learning rate as training progresses. To facilitate this, we can use a learning schedule.</p>

<p>Hugging Face<a data-type="indexterm" data-primary="Hugging Face" data-secondary="learning schedulers" id="id1035"/> supports several different types of learning schedulers. Here are a few important ones:</p>
<dl>
<dt>Constant</dt>
<dd>
<p>This is the vanilla training schedule where the learning rate remains constant throughout the course of the training.</p>
</dd>
<dt>Constant with warmup</dt>
<dd>
<p>In this setting, the learning rate starts from zero and is increased linearly toward the specified learning rate during a warmup phase. After the warmup phase is completed, the learning rate remains constant.</p>

<p><a data-type="xref" href="#constant-lr">Figure 6-1</a> shows how the learning rate changes over time while using the constant with warmup scheduler.</p>

<figure><div id="constant-lr" class="figure">
<img src="assets/dllm_0601.png" alt="constant-lr" width="600" height="291"/>
<h6><span class="label">Figure 6-1. </span>Learning rate with a constant schedule with warmup</h6>
</div></figure>
</dd>
<dt>Cosine</dt>
<dd>
<p>In this setting, called <em>cosine annealing</em>, the learning rate has a warmup phase after which it slowly declines to zero, as per the cosine function.</p>

<p><a data-type="xref" href="#cosine-warmup">Figure 6-2</a> shows how the learning rate changes over time while using the cosine scheduler.</p>

<figure><div id="cosine-warmup" class="figure">
<img src="assets/dllm_0602.png" alt="cosine-warmup" width="600" height="293"/>
<h6><span class="label">Figure 6-2. </span>Learning rate with a cosine schedule</h6>
</div></figure>
</dd>
<dt>Cosine with restarts</dt>
<dd>
<p>In this setting, called <em>cosine annealing with warm restart</em>, after a warmup phase, the learning rate decreases to zero following the cosine function, but undergoes several hard restarts, where the learning rate shoots back to the specified learning rate after it reaches zero. For more details on why this is effective, check out Loshcilov and Hutter’s <a href="https://oreil.ly/Q4c3o">paper</a> that introduced this concept.</p>

<p><a data-type="xref" href="#cosine-restart">Figure 6-3</a> shows how the learning rate changes across time while using the cosine with restarts scheduler.</p>

<figure><div id="cosine-restart" class="figure">
<img src="assets/dllm_0603.png" alt="cosine-restart" width="600" height="293"/>
<h6><span class="label">Figure 6-3. </span>Learning rate with a cosine with restarts schedule</h6>
</div></figure>
</dd>
<dt>Linear</dt>
<dd>
<p>This is very similar to the cosine setting, except that the learning rate decreases to zero linearly instead of following the cosine function.</p>

<p><a data-type="xref" href="#linear-lr">Figure 6-4</a> shows how the learning rate changes over time while using the linear scheduler.</p>

<figure><div id="linear-lr" class="figure">
<img src="assets/dllm_0604.png" alt="linear-lr" width="600" height="293"/>
<h6><span class="label">Figure 6-4. </span>Learning rate with a linear scheduler</h6>
</div></figure>
</dd>
</dl>

<p>If you are using AdamW, schedulers with a warmup phase are even more important to prevent getting trapped in a bad minima. Empirically, it has been found that cosine annealing outperforms linear decay<a data-type="indexterm" data-startref="xi_finetuningmodelslearningalgorithmsparameters68590" id="id1036"/><a data-type="indexterm" data-startref="xi_learningalgorithmsparameters68590" id="id1037"/><a data-type="indexterm" data-startref="xi_parameterslearningalgorithms68590" id="id1038"/>.</p>

<p>For our political promises detector fine-tuning, let’s use the paged variant of AdamW, a learning rate of 3e-4, a weight decay of 0.01, and the cosine learning schedule<a data-type="indexterm" data-startref="xi_learningratescheduleandschedulers613239" id="id1039"/>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">optim</code> <code class="o">=</code> <code class="s2">"paged_adamw_32bit"</code>
<code class="n">learning_rate</code> <code class="o">=</code> <code class="mf">3e-4</code>
<code class="n">weight_decay</code> <code class="o">=</code> <code class="mf">0.01</code>
<code class="n">lr_scheduler_type</code> <code class="o">=</code> <code class="s1">'cosine'</code>
<code class="n">warmup_ratio</code> <code class="o">=</code> <code class="mf">0.03</code>  <code class="c1">#The proportion of training steps to be used as warmup</code></pre>
</div></section>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Memory Optimization Parameters"><div class="sect2" id="id246">
<h2>Memory Optimization Parameters</h2>

<p>After we have set the parameters<a data-type="indexterm" data-primary="fine-tuning models" data-secondary="memory optimization parameters" id="id1040"/><a data-type="indexterm" data-primary="memory management" data-secondary="fine-tuning parameters" id="id1041"/><a data-type="indexterm" data-primary="parameters" data-secondary="memory optimization" id="id1042"/> related to the optimizers, let’s explore memory and compute optimization parameters. Two prevalent techniques in this area include gradient checkpointing and gradient accumulation.</p>










<section data-type="sect3" data-pdf-bookmark="Gradient checkpointing"><div class="sect3" id="id96">
<h3>Gradient checkpointing</h3>

<p>Gradient checkpointing<a data-type="indexterm" data-primary="gradient checkpointing" id="id1043"/> helps save memory at the cost of more compute. During the forward pass of the backpropagation algorithm<a data-type="indexterm" data-primary="backpropagation algorithm" data-secondary="gradient checkpointing" id="id1044"/>, activations are computed and saved in memory so that they can be used in the backward pass. What if we did not save all of the activations? The missing activations could be recalculated on the fly during the backward pass. This does cost us more compute, but we could save a lot of memory. We could even train models where a batch size of only one does not fit in our GPU memory. For more technical details on gradient checkpointing, check out Yaroslav<a data-type="indexterm" data-primary="Bulatov, Yaroslav" id="id1045"/> Bulatov’s <a href="https://oreil.ly/i-R4I">blog</a>.</p>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Gradient accumulation"><div class="sect3" id="id365">
<h3>Gradient accumulation</h3>

<p>Let’s say we have a desired batch size but we do not have the required memory to support that batch size. We can simulate the desired batch size using a technique called gradient accumulation. In this technique, the gradient updates are not done at every batch, but are accumulated over several batches and then summed or averaged.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Gradient accumulation can make training slower, since there are fewer updates being made. Gradient accumulation does not reduce the computation required.</p>
</div>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Quantization"><div class="sect3" id="id247">
<h3>Quantization</h3>

<p>A very effective form of saving memory is through quantization<a data-type="indexterm" data-primary="quantization" data-secondary="optimization with" id="id1046"/><a data-type="indexterm" data-primary="memory management" data-secondary="optimization" id="id1047"/><a data-type="indexterm" data-primary="optimization and optimizers" data-secondary="quantization" id="id1048"/>, as introduced in <a data-type="xref" href="ch05.html#chapter_utilizing_llms">Chapter 5</a>. We will go through quantization techniques in more detail in <a data-type="xref" href="ch09.html#ch09">Chapter 9</a>. For our use case, we will use bf16 as it represents a sound tradeoff between memory savings and performance.</p>

<p>For our political promises detector fine-tuning, we’ll set the following parameters for memory optimization, given that we are trying to train it on a relatively memory constrained 16 GB RAM GPU:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">gradient_accumulation_steps</code> <code class="o">=</code> <code class="mi">4</code>
<code class="n">bf16</code> <code class="o">=</code> <code class="kc">True</code>
<code class="n">gradient_checkpointing</code> <code class="o">=</code> <code class="kc">True</code></pre>
</div></section>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Regularization Parameters"><div class="sect2" id="id97">
<h2>Regularization Parameters</h2>

<p>Next, let’s look at various techniques available for tackling model overfitting<a data-type="indexterm" data-primary="fine-tuning models" data-secondary="regularization parameters" id="id1049"/><a data-type="indexterm" data-primary="regularization parameters" id="id1050"/><a data-type="indexterm" data-primary="overfitting of model" id="xi_overfittingofmodel622080"/><a data-type="indexterm" data-primary="optimization and optimizers" data-secondary="to avoid overfitting" data-secondary-sortas="avoid overfitting" id="xi_optimizationandoptimizerstoavoidoverfitting622080"/><a data-type="indexterm" data-primary="parameters" data-secondary="regularization" id="id1051"/>.</p>










<section data-type="sect3" data-pdf-bookmark="Label smoothing"><div class="sect3" id="id98">
<h3>Label smoothing</h3>

<p>Label smoothing<a data-type="indexterm" data-primary="label smoothing" id="id1052"/><a data-type="indexterm" data-primary="calibration, model" id="id1053"/> is a technique that not only helps with combatting overfitting but also aids in model calibration.</p>

<p>Calibration is an underappreciated topic in deep learning. A model is said to be well-calibrated if there is a correlation between its output probability<a data-type="indexterm" data-primary="output probability" id="id1054"/> values and task 
<span class="keep-together">accuracy</span>.</p>

<p>For example, consider a task that classifies a sentence as being abusive or not. If the model is well-calibrated, then among all examples for which the model produces an output probability of 0.9, 90% of them would be expected to be correctly classified. Similarly, for an output probability of 0.6, there should be a lower (~60%) likelihood of the classification being correct. Simply put, the output probability should accurately reflect the confidence in the classification decision.</p>

<p>A model being well-calibrated implies that it is not overconfident. This helps us in nuanced handling of examples that have low output probabilities (using a bigger model to handle those examples, for instance).</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Larger models are less calibrated compared to models like BERT, according to a study by <a href="https://oreil.ly/ij7mS">Li et al.</a> Larger models tend to be more confident in general about their predictions. The inability to calculate reasonably accurate uncertainty estimates for large language models could be an argument to use smaller ones instead!</p>
</div>

<p>One of the techniques for calibrating models is label smoothing. The usual training process involves training against hard target labels (0 or 1 for a binary classification task). When using cross-entropy as the loss function, this amounts to pushing the model logits closer to 0 or 1, thus making the model highly confident. Label smoothing involves using a regularization term that is subtracted or divided from the hard target label.</p>

<p>Label smoothing is especially useful when the input dataset is noisy, i.e., contains some inaccurate labels. Regularization prevents the model from learning too much from incorrect examples.</p>

<p>For the political promises detector, we will use label smoothing, given that some examples could be subjective or open to interpretation.</p>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Noise Embeddings"><div class="sect3" id="id99">
<h3>Noise Embeddings</h3>

<p>The datasets<a data-type="indexterm" data-primary="fine-tuning models" data-secondary="embeddings" id="id1055"/><a data-type="indexterm" data-primary="noise embeddings" id="id1056"/><a data-type="indexterm" data-primary="embeddings" data-secondary="noise embeddings" id="id1057"/><a data-type="indexterm" data-primary="embeddings" data-secondary="fine-tuning" id="id1058"/> we use for fine-tuning typically consist of a small number of examples 
<span class="keep-together">(&lt; 50,000)</span>. We would like our model to not overfit to the stylistic characteristics of the dataset, like the formatting, wording, and length of the text. One way to address this is by adding noise to the input embeddings.</p>

<p><a href="https://oreil.ly/ouESL">Jain et al.</a> observe that adding noise embeddings reduces the tendency of the model to overfit to wording and formatting of the fine-tuning datasets. An interesting side effect of noise embeddings is that the models generate longer, verbose texts. By measuring token diversity of the outputs, they confirmed that the longer texts actually include more information and are not just repetitive.</p>

<p>Hugging Face<a data-type="indexterm" data-primary="Hugging Face" data-secondary="NEFTune" id="id1059"/><a data-type="indexterm" data-primary="NEFTune" id="id1060"/> supports <a href="https://oreil.ly/dSaem">Noisy Embedding Instruction Fine-Tuning (NEFTune)</a>, a noise addition technique. In NEFTune, a noise vector is added to each embedding vector. The elements in the noise vector are generated by sampling independent and identically distributed (iid) from [-1,1]. The resulting vector is scaled using a scaling factor before being added to the embedding vector.</p>

<p>Noise embeddings have been empirically found to be very effective in reducing overfitting. Therefore, we will use it for our political promises detector fine-tuning. Note that the noise embeddings are added only during training and not during inference.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>The impact of noise embeddings is not yet well understood. Improvements in the fine-tuning task could come at the cost of other model capabilities. Make sure you test the model for 
<span class="keep-together">regressions</span>!</p>
</div>

<p>For our political promises detector fine-tuning task, let’s activate both label smoothing and noise embeddings:</p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># Label 0 will be transformed to label_smoothing_factor/num_labels</code>
<code class="c1"># Label 1 will be transformed to 1 - label_smoothing_factor +</code>
<code class="c1">#label_smoothing_factor/num_labels</code>

<code class="n">label_smoothing_factor</code> <code class="o">=</code> <code class="mf">0.1</code>
<code class="n">neftune_noise_alpha</code> <code class="o">=</code> <code class="mi">5</code></pre>
</div></section>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Batch Size"><div class="sect2" id="id100">
<h2>Batch Size</h2>

<p>Along with the learning rate<a data-type="indexterm" data-primary="fine-tuning models" data-secondary="batch size" id="xi_finetuningmodelsbatchsize627229"/><a data-type="indexterm" data-primary="batch size, fine-tuning LLM" id="xi_batchsizefinetuningLLM627229"/>, the batch size is one of the most important hyperparameters we need to set. A larger batch size means training will proceed faster. However, larger batch sizes also require more memory. Larger batch sizes can also lead the model to land in a sharp local minima, which can be a sign of overfitting. Therefore, there are trade offs involving memory, compute, and performance.</p>

<p>For the political promises detector, we will use a batch size of 8, given our memory limitations. Of course during inference, the maximum possible batch size is the ideal one. Note that it is recommended that the batch size be always a number that is a power of two, to reduce GPU I/O overhead.</p>

<p>The <code>TrainingArguments</code> class by Hugging Face supports <em>auto_find_batch_size</em>, which when set, selects the maximum possible batch size supported by the memory. To use this feature, you need to install the <code>accelerate</code> library:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">per_device_train_batch_size</code> <code class="o">=</code> <code class="mi">8</code>
<code class="n">per_device_eval_batch_size</code> <code class="o">=</code> <code class="mi">8</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>You can reduce your maximum sequence length to support a larger batch size.</p>
</div>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1061">
<h1>The Relationship Between Learning Rate and Batch Size</h1>
<p>The relationship between learning rate<a data-type="indexterm" data-primary="learning rate" data-secondary="and batch size" data-secondary-sortas="batch size" id="id1062"/> and batch size is extremely complex and depends on several external factors including the model architecture.</p>

<p>A high learning rate requires fewer steps, thus helping you finish training faster, but at the risk of overshooting the minima, leading to lack of convergence. Conversely, a low learning rate requires more steps and takes longer to converge, but you might end up in a narrow suboptimal minima. The narrow local minima likely means that you are overfitting. We would like to converge to a flatter minima instead, which can be accomplished by increasing the learning rate.</p>

<p>A smaller batch size will mean greater variance between examples in each batch, thus potentially leading the model toward a flatter minima. Thus, a relatively high learning rate and a relatively low batch size theoretically could help with more effective convergence. However, theoretical insights might not always be true in practice.</p>
</div></aside>

<p>Finally, let’s set some miscellaneous parameters:</p>
<dl>
<dt><code>max_grad_norm</code></dt>
<dd>
<p>This is used for gradient clipping<a data-type="indexterm" data-primary="gradient clipping" id="id1063"/>, which is a solution for the exploding gradients issue that is sometimes encountered during training. The <code>max_grad_norm</code> value is the threshold for gradient clipping. If the L2 gradient norm is above the threshold, then it will be rescaled to <code>max_grad_norm</code>. For more details on gradient clipping, see <a href="https://oreil.ly/gH7L7">“Understanding Gradient Clipping (and How It Can Fix Exploding Gradients Problem)”</a>.</p>
</dd>
<dt><code>group_by_length</code></dt>
<dd>
<p>This is used to group examples that have similar lengths in the same batch, so that the padding tokens can be optimized.</p>
</dd>
<dt><code>max_train_epochs</code></dt>
<dd>
<p>Number of passes over the training dataset. This is usually set to less than five to prevent overfitting<a data-type="indexterm" data-startref="xi_finetuningmodelsbatchsize627229" id="id1064"/><a data-type="indexterm" data-startref="xi_batchsizefinetuningLLM627229" id="id1065"/><a data-type="indexterm" data-startref="xi_overfittingofmodel622080" id="id1066"/><a data-type="indexterm" data-startref="xi_optimizationandoptimizerstoavoidoverfitting622080" id="id1067"/>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">max_grad_norm</code><code class="o">=</code><code class="mi">2</code>
<code class="n">group_by_length</code><code class="o">=</code><code class="kc">True</code>
<code class="n">max_train_epochs</code><code class="o">=</code><code class="mi">3</code></pre>
</dd>
</dl>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Parameter-Efficient Fine-Tuning"><div class="sect2" id="id101">
<h2>Parameter-Efficient Fine-Tuning</h2>

<p>After filling in the <code>TrainingArguments</code>, let’s next fill in parameters<a data-type="indexterm" data-primary="fine-tuning models" data-secondary="parameter efficient" id="id1068"/><a data-type="indexterm" data-primary="PEFT (parameter-efficient fine-tuning)" id="id1069"/><a data-type="indexterm" data-primary="parameter-efficient fine-tuning (PEFT)" id="id1070"/> of the PEFT library.</p>

<p>The PEFT library by Hugging Face is an impressive facilitator of parameter-efficient fine-tuning. This refers to a set of fine-tuning techniques that update only a small proportion of parameters in the model while keeping the performance closer to what it would have been if all the parameters were updated.</p>

<p>In this example, we will use low-rank adaptation (LoRA)<a data-type="indexterm" data-primary="low-rank adaptation (LoRA)" id="id1071"/> as the fine-tuning technique. Here are some hyperparameters to consider:</p>
<dl>
<dt><code>r</code></dt>
<dd>
<p>The attention dimension of LoRA.</p>
</dd>
<dt><code>lora_alpha</code></dt>
<dd>
<p>The alpha parameter in the LoRA technique.</p>
</dd>
<dt><code>lora_dropout</code></dt>
<dd>
<p>The dropout probability used in the layers being tuned. This helps reduce overfitting.</p>
</dd>
<dt><code>layers_to_transform</code></dt>
<dd>
<p>This specifies the layers for which the LoRA transformation is to be applied.</p>
</dd>
</dl>

<p>Here are some recommended default values:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">r</code> <code class="o">=</code> <code class="mi">64</code>
<code class="n">lora_alpha</code> <code class="o">=</code> <code class="mi">8</code>
<code class="n">lora_dropout</code> <code class="o">=</code> <code class="mf">0.1</code></pre>

<p>For  more background on LoRA, refer to Ogban Ugot’s <a href="https://oreil.ly/_l91y">blog post</a>.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Working with Reduced Precision"><div class="sect2" id="id102">
<h2>Working with Reduced Precision</h2>

<p>The bitsandbytes library<a data-type="indexterm" data-primary="bitsandbytes library" id="id1072"/>, built by Tim Dettmers<a data-type="indexterm" data-primary="Dettmers, Tim" id="id1073"/>, facilitates working with reduced precision formats<a data-type="indexterm" data-primary="fine-tuning models" data-secondary="reduced precision formats" id="xi_finetuningmodelsreducedprecisionformats6341102"/><a data-type="indexterm" data-primary="reduced precision formats" id="xi_reducedprecisionformats6341102"/>, which we introduced in <a data-type="xref" href="ch05.html#chapter_utilizing_llms">Chapter 5</a>. In this example, we will work with the FP4 format. Note that you need the bitsandbytes version to be &gt;= 0.39.0.</p>

<p>Hugging Face has integrated bitsandbytes support into its ecosystem. The <code>BitsAndBytesConfig</code> class allows us to set the parameters. Here are some relevant ones:</p>
<dl>
<dt><code>load_in_8bit/load_in_4bit</code></dt>
<dd>
<p>This is used to specify if we want to load the model in 4-bit mode or 8-bit mode.</p>
</dd>
<dt><code>llm_int8_threshold</code></dt>
<dd>
<p>We need to specify a threshold of values beyond which fp16 will be used. This is because int8 quantization works well only for values lesser than 5–6.</p>
</dd>
<dt><code>llm_int8_skip_modules</code></dt>
<dd>
<p>This is used to specify the exceptions for which we do not want int8 quantization.</p>
</dd>
<dt><code>llm_int8_enable_fp32_cpu_offload</code></dt>
<dd>
<p>If we want parts of the model to be run in int8 on GPU and the rest in FP32 on CPU, this parameter facilitates it. This is used in cases where the model is too large to fit on our GPU.</p>
</dd>
<dt><code>bnb_4bit_compute_dtype</code></dt>
<dd>
<p>This sets the computational type, regardless of the input type.</p>
</dd>
<dt><code>bnb_4bit_quant_type</code></dt>
<dd>
<p>The options here are FP4 or NF4. This is used to set the quantization type in the 4-bit layers.</p>
</dd>
</dl>

<p>Here are some recommended default values:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">use_4bit</code> <code class="o">=</code> <code class="kc">True</code>
<code class="n">bnb_4bit_compute_dtype</code> <code class="o">=</code> <code class="s1">'float16'</code>
<code class="n">bnb_4bit_quant_type</code> <code class="o">=</code> <code class="s1">'nf4'</code>
<code class="n">use_nested_quant</code> <code class="o">=</code> <code class="kc">False</code></pre>

<p>Finally, we use the Transformer Reinforcement Learning (TRL) library that, in addition to reinforcement learning, provides support for supervised fine-tuning.</p>

<p>Here are some recommended default values<a data-type="indexterm" data-startref="xi_finetuningmodelsreducedprecisionformats6341102" id="id1074"/><a data-type="indexterm" data-startref="xi_reducedprecisionformats6341102" id="id1075"/>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">max_seq_length</code> <code class="o">=</code> <code class="mi">128</code>
<code class="c1"># Packing is used to place multiple instructions in the same input sequence</code>

<code class="n">packing</code> <code class="o">=</code> <code class="kc">True</code></pre>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Putting It All Together"><div class="sect2" id="id103">
<h2>Putting It All Together</h2>

<p>Now that we have set up all the requisite parameters, here is the full code for the fine-tuning process:</p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># Ensure that the specified versions of these libraries are installed.</code>
<code class="err">!</code><code class="n">pip</code> <code class="n">install</code> <code class="n">transformers</code><code class="o">==</code><code class="mf">4.35.0</code> <code class="n">accelerate</code><code class="o">==</code><code class="mf">0.24.0</code> <code class="n">peft</code><code class="o">==</code><code class="mf">0.6.0</code>
<code class="n">bitsandbytes</code><code class="o">==</code><code class="mf">0.41.0</code>  <code class="n">trl</code><code class="o">==</code><code class="mf">0.7.4</code>

<code class="kn">from</code> <code class="nn">datasets</code> <code class="kn">import</code> <code class="n">load_dataset</code>
<code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">TrainingArguments</code><code class="p">,</code> <code class="n">BitsAndBytesConfig</code>
<code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">LlamaForCausalLM</code><code class="p">,</code> <code class="n">LlamaTokenizer</code>
<code class="kn">from</code> <code class="nn">peft</code> <code class="kn">import</code> <code class="n">PeftModel</code><code class="p">,</code> <code class="n">LoraConfig</code>
<code class="kn">from</code> <code class="nn">trl</code> <code class="kn">import</code> <code class="n">SFTTrainer</code>

<code class="n">train_params</code> <code class="o">=</code> <code class="n">TrainingArguments</code><code class="p">(</code>
    <code class="n">optim</code> <code class="o">=</code> <code class="s2">"paged_adamw_32bit"</code><code class="p">,</code>
    <code class="n">learning_rate</code> <code class="o">=</code> <code class="mf">3e-4</code><code class="p">,</code>
    <code class="n">weight_decay</code> <code class="o">=</code> <code class="mf">0.01</code><code class="p">,</code>
    <code class="n">lr_scheduler_type</code> <code class="o">=</code> <code class="s1">'cosine'</code><code class="p">,</code>
    <code class="n">warmup_ratio</code> <code class="o">=</code> <code class="mf">0.03</code><code class="p">,</code>
    <code class="n">gradient_accumulation_steps</code> <code class="o">=</code> <code class="mi">4</code><code class="p">,</code>
    <code class="n">bf16</code> <code class="o">=</code> <code class="kc">True</code><code class="p">,</code>
    <code class="n">gradient_checkpointing</code> <code class="o">=</code> <code class="kc">True</code><code class="p">,</code>
    <code class="n">label_smoothing_factor</code> <code class="o">=</code> <code class="mf">0.1</code><code class="p">,</code>
    <code class="n">neftune_noise_alpha</code> <code class="o">=</code> <code class="mi">5</code><code class="p">,</code>
    <code class="n">per_device_train_batch_size</code> <code class="o">=</code> <code class="mi">8</code><code class="p">,</code>
    <code class="n">per_device_eval_batch_size</code> <code class="o">=</code> <code class="mi">8</code><code class="p">,</code>
    <code class="n">max_grad_norm</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code>
    <code class="n">group_by_length</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
    <code class="n">max_train_epochs</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code>
    <code class="n">output_dir</code> <code class="o">=</code> <code class="s1">'/model_outputs'</code><code class="p">,</code>
    <code class="n">save_steps</code> <code class="o">=</code> <code class="mi">50</code><code class="p">,</code>
    <code class="n">logging_steps</code> <code class="o">=</code> <code class="mi">10</code>
    <code class="p">)</code>


<code class="n">quantize_params</code> <code class="o">=</code> <code class="n">BitsAndBytesConfig</code> <code class="p">(</code>

    <code class="n">use_4bit</code> <code class="o">=</code> <code class="kc">True</code><code class="p">,</code>
    <code class="n">bnb_4bit_compute_dtype</code> <code class="o">=</code> <code class="s1">'float16'</code><code class="p">,</code>
    <code class="n">bnb_4bit_quant_type</code> <code class="o">=</code> <code class="s1">'nf4'</code><code class="p">,</code>
    <code class="n">use_nested_quant</code> <code class="o">=</code> <code class="kc">False</code>
    <code class="p">)</code>


<code class="n">lora_params</code> <code class="o">=</code> <code class="n">LoraConfig</code> <code class="p">(</code>
    <code class="n">r</code> <code class="o">=</code> <code class="mi">64</code><code class="p">,</code>
    <code class="n">lora_alpha</code> <code class="o">=</code> <code class="mi">8</code><code class="p">,</code>
    <code class="n">lora_dropout</code> <code class="o">=</code> <code class="mf">0.1</code>
    <code class="p">)</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">LlamaForCausalLM</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code>
    <code class="n">pretrained_model_name_or_path</code> <code class="o">=</code> <code class="s1">'meta-llama/Llama-2-7b'</code><code class="p">,</code>
    <code class="n">quantization_config</code><code class="o">=</code><code class="n">quantize_params</code><code class="p">,</code>
    <code class="n">device_map</code><code class="o">=</code><code class="s1">'auto'</code>
    <code class="p">)</code>

<code class="n">tokenizer</code> <code class="o">=</code> <code class="n">LlamaTokenizer</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s1">'meta-llama/Llama-2-7b'</code><code class="p">)</code>

<code class="n">tune_data</code> <code class="o">=</code> <code class="n">load_dataset</code><code class="p">(</code><code class="s2">"csv"</code><code class="p">,</code> <code class="n">data_files</code><code class="o">=</code><code class="s1">'/path/to/finetune_data.csv'</code><code class="p">)</code>

<code class="n">sft</code> <code class="o">=</code> <code class="n">SFTTrainer</code> <code class="p">(</code>
    <code class="n">model</code> <code class="o">=</code> <code class="n">model</code><code class="p">,</code>
    <code class="n">args</code> <code class="o">=</code> <code class="n">train_params</code><code class="p">,</code>
    <code class="n">train_dataset</code> <code class="o">=</code> <code class="n">tune_data</code><code class="p">,</code>
    <code class="n">tokenizer</code> <code class="o">=</code> <code class="n">tokenizer</code>
    <code class="n">peft_config</code> <code class="o">=</code> <code class="n">lora_params</code><code class="p">,</code>
    <code class="n">max_seq_length</code> <code class="o">=</code> <code class="mi">128</code><code class="p">,</code>
    <code class="n">dataset_text_field</code> <code class="o">=</code> <code class="s1">'text'</code><code class="p">,</code>
    <code class="n">packing</code> <code class="o">=</code> <code class="kc">True</code>
    <code class="p">)</code>

<code class="n">sft</code><code class="o">.</code><code class="n">train</code><code class="p">()</code>
<code class="n">sft</code><code class="o">.</code><code class="n">model</code><code class="o">.</code><code class="n">save_pretrained</code><code class="p">(</code><code class="s1">'/path/to/llama-2-it.csv'</code><code class="p">)</code></pre>

<p>The relationship between the hyperparameters is very complex, and you might find surprising results. It will take several iterations before you hit the sweet spot. However, do not spend too much time squeezing out the last bit of performance from your fine-tuning, as that time is better spent developing better training data. In the next section, we will learn how to create effective training datasets<a data-type="indexterm" data-startref="xi_finetuningmodelsexample63051" id="id1076"/>.</p>

<p>The exact memory<a data-type="indexterm" data-primary="memory management" data-secondary="needed for fine-tuning" id="id1077"/> you need to fine-tune an LLM depends on several factors: the optimizer used, whether gradient accumulation and gradient checkpointing are activated, the type of quantization used, etc.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1078">
<h1>Exercise</h1>
<p>Ablation studies are an important part of machine learning experimentation. This refers to studying the impact of a single component by removing the component and rerunning the experiment. For our fine-tuning example, let’s study the impact of noise embeddings on the final performance. Run five fine-tuning runs with noise embeddings activated, and five without, keeping all other hyperparameters constant. Perform error analysis on the test set and understand how noise embeddings impact the performance of the model. Are they a net positive?</p>
</div></aside>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Fine-Tuning Datasets"><div class="sect1" id="id104">
<h1>Fine-Tuning Datasets</h1>

<p>In our fine-tuning example<a data-type="indexterm" data-primary="fine-tuning models" data-secondary="dataset instruction-tuning" id="xi_finetuningmodelsdatasetinstructiontuning646927"/><a data-type="indexterm" data-primary="datasets" data-secondary="fine-tuning" id="xi_datasetsfinetuning646927"/>, we directly loaded a preconstructed dataset, focusing primarily on the fine-tuning process. Now, let’s shift our attention to the dataset, to understand the various techniques for creating datasets.</p>

<p>First, let’s look into the dataset we used in our fine-tuning example:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">datasets</code> <code class="kn">import</code> <code class="n">load_dataset</code>
<code class="n">tune_data</code> <code class="o">=</code> <code class="n">load_dataset</code><code class="p">(</code><code class="s2">"csv"</code><code class="p">,</code> <code class="n">data_files</code><code class="o">=</code><code class="s1">'/path/to/finetune_data.csv'</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">tune_data</code><code class="p">[:</code><code class="mi">2</code><code class="p">])</code></pre>

<p>Output:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">Input</code><code class="p">:</code> <code class="n">We</code> <code class="n">will</code> <code class="n">support</code> <code class="n">women</code> <code class="ow">and</code> <code class="n">children</code> <code class="ow">and</code> <code class="n">give</code> <code class="n">every</code> <code class="n">child</code> <code class="n">the</code> <code class="n">best</code>
<code class="n">possible</code> <code class="n">start</code> <code class="k">with</code> <code class="err">$</code><code class="mi">10</code> <code class="n">a</code> <code class="n">day</code> <code class="n">child</code> <code class="n">care</code><code class="o">.</code>
<code class="n">Identify</code> <code class="k">if</code> <code class="n">the</code> <code class="n">above</code> <code class="n">sentence</code> <code class="n">represents</code> <code class="n">a</code> <code class="n">political</code> <code class="n">promise</code><code class="o">.</code> <code class="n">A</code> <code class="n">political</code>
<code class="n">promise</code> <code class="ow">is</code> <code class="n">a</code> <code class="n">promise</code> <code class="n">that</code> <code class="ow">is</code> <code class="n">tangible</code><code class="p">,</code> <code class="n">specific</code><code class="p">,</code> <code class="ow">and</code> <code class="n">an</code> <code class="n">action</code> <code class="n">that</code> <code class="n">the</code>
<code class="n">government</code> <code class="n">has</code> <code class="n">the</code> <code class="n">agency</code> <code class="n">to</code> <code class="n">make</code><code class="o">.</code> <code class="n">Reply</code> <code class="s1">'True'</code> <code class="k">if</code> <code class="n">the</code> <code class="n">sentence</code> <code class="n">represents</code> <code class="n">a</code>
<code class="n">political</code> <code class="n">promise</code><code class="p">,</code> <code class="s1">'False'</code> <code class="k">if</code> <code class="ow">not</code><code class="o">.</code>
<code class="n">Output</code><code class="p">:</code> <code class="kc">True</code>
<code class="n">Input</code><code class="p">:</code> <code class="n">It</code> <code class="ow">is</code> <code class="n">time</code> <code class="k">for</code> <code class="n">leadership</code> <code class="n">that</code> <code class="n">never</code> <code class="n">seeks</code> <code class="n">to</code> <code class="n">divide</code> <code class="n">Canadians</code><code class="p">,</code> <code class="n">but</code>
<code class="n">takes</code> <code class="n">every</code> <code class="n">single</code> <code class="n">opportunity</code> <code class="n">to</code> <code class="n">bring</code> <code class="n">us</code> <code class="n">together</code><code class="p">,</code> <code class="n">including</code> <code class="ow">in</code> <code class="n">Parliament</code><code class="o">.</code>
<code class="n">Identify</code> <code class="k">if</code> <code class="n">the</code> <code class="n">above</code> <code class="n">sentence</code> <code class="n">represents</code> <code class="n">a</code> <code class="n">political</code> <code class="n">promise</code><code class="o">.</code> <code class="n">A</code> <code class="n">political</code>
<code class="n">promise</code> <code class="ow">is</code> <code class="n">a</code> <code class="n">promise</code> <code class="n">that</code> <code class="ow">is</code> <code class="n">tangible</code><code class="p">,</code> <code class="n">specific</code><code class="p">,</code> <code class="ow">and</code> <code class="n">an</code> <code class="n">action</code> <code class="n">that</code> <code class="n">the</code>
<code class="n">government</code> <code class="n">has</code> <code class="n">the</code> <code class="n">agency</code> <code class="n">to</code> <code class="n">make</code><code class="o">.</code> <code class="n">Reply</code> <code class="s1">'True'</code> <code class="k">if</code> <code class="n">the</code> <code class="n">sentence</code> <code class="n">represents</code> <code class="n">a</code>
<code class="n">political</code> <code class="n">promise</code><code class="p">,</code> <code class="s1">'False'</code> <code class="k">if</code> <code class="ow">not</code><code class="o">.</code>
<code class="n">Output</code><code class="p">:</code> <code class="kc">False</code></pre>

<p>As we can see, this is not a traditional dataset with just (input, output) pairs but one that also contains the task description in natural language. A typical example in this type of fine-tuning dataset consists of :</p>

<ul>
<li>
<p>The instruction, which describes the task and specifies the desired output format. Optionally, the instruction contains positive and/or negative examples of the task. It can also contain constraints and exceptions to be followed.</p>
</li>
<li>
<p>An optional input, which in our example is the sentence or paragraph for the model to evaluate.</p>
</li>
<li>
<p>The output, which is the correct answer to the task in the format specified in the instruction.</p>
</li>
</ul>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Fine-tuning datasets can be either multi-task or single-task. Multi-task datasets<a data-type="indexterm" data-primary="multi-task datasets" id="id1079"/> are used for instruction-tuning. In general, instruction-tuning can be treated as an intermediate step before single-task fine-tuning. For example, you can take a T5 language model, instruction-tune it with FLAN to create FLAN-T5, and then further fine-tune it with your task-specific dataset. This approach is <a href="https://oreil.ly/e-MVh">shown</a> to yield better results than directly fine-tuning on T5 alone.</p>
</div>

<p>Later in this chapter, we will learn how to create task-specific datasets<a data-type="indexterm" data-primary="task-specific datasets" id="id1080"/>. First, let’s look at how we can create instruction-tuning datasets.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1081">
<h1>Why Do We Need Instruction-Tuning?</h1>
<p>As seen in <a data-type="xref" href="ch04.html#chapter_transformer-architecture">Chapter 4</a>, the learning objectives of LLMs are typically either next-token prediction or denoising tasks. These objectives do not correspond to real-world user tasks. Thus there is a mismatch in how LLMs are trained and how they are used. To bridge this gap, we employ instruction-tuning<a data-type="indexterm" data-primary="datasets" data-secondary="instruction-tuned" id="id1082"/><a data-type="indexterm" data-primary="instruction-tuned models" data-secondary="importance of" id="id1083"/>.</p>

<p class="pagebreak-before">Instruction-tuning allows for more controllable behavior from LLMs. The instructions in these datasets are similar to instructions provided by humans in real-world scenarios. Instruction-tuning also enables the model to learn the output format and thus generate more structured output.</p>
</div></aside>

<p>There are plenty of instruction-tuned LLMs available, both open source and proprietary. Why do we want to instruction-tune the LLM ourselves? Public datasets are too general, lack diversity, and are primarily geared to general usage. Leveraging your domain expertise and knowledge of intended use cases to construct the dataset can be highly effective. In fact, at my company, which specializes in the financial domain, this technique delivered the single largest boost in performance.</p>

<p>Approaches to creating instruction-tuning datasets include:</p>

<ul>
<li>
<p>Utilizing publicly available instruction-tuning datasets</p>
</li>
<li>
<p>Transforming traditional fine-tuning datasets into instruction-tuning datasets</p>
</li>
<li>
<p>Starting with manually crafted seed examples, followed by optionally augmenting the dataset by utilizing an LLM to generate similar examples</p>
</li>
</ul>

<p>Next, let’s examine these methods more closely.</p>








<section data-type="sect2" data-pdf-bookmark="Utilizing Publicly Available Instruction-Tuning Datasets"><div class="sect2" id="id105">
<h2>Utilizing Publicly Available Instruction-Tuning Datasets</h2>

<p>If your use case is sufficiently general or popular, you may be able to use publicly available datasets<a data-type="indexterm" data-primary="publicly available datasets for instruction-tuning" id="xi_publiclyavailabledatasetsforinstructiontuning6550104"/><a data-type="indexterm" data-primary="instruction-tuned models" data-secondary="publicly available" id="xi_instructiontunedmodelspubliclyavailable6550104"/><a data-type="indexterm" data-primary="datasets" data-secondary="publicly available" id="xi_datasetspubliclyavailable6550104"/> for instruction-tuning. The following table lists some popular instruction-tuning datasets, along with information on their creators, sizes, and creation process.</p>
<table id="popular-it-datasets">
<caption><span class="label">Table 6-1. </span>Popular instruction-tuning datasets</caption>
<thead>
<tr>
<th>Name</th>
<th>Size</th>
<th>Created by</th>
<th>Created using</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>OIG</p></td>
<td><p>43M</p></td>
<td><p><a href="https://www.ontocord.ai/">Ontocord</a></p></td>
<td><p>Rule-based<br/></p></td>
</tr>
<tr>
<td><p>FLAN</p></td>
<td><p>4.4M</p></td>
<td><p>Google</p></td>
<td><p>Templates</p></td>
</tr>
<tr>
<td><p>P3 (Public Pool of Prompts)</p></td>
<td><p>12M</p></td>
<td><p>Big Science</p></td>
<td><p>Templates</p></td>
</tr>
<tr>
<td><p>Natural Instruction</p></td>
<td><p>193K</p></td>
<td><p>Allen AI</p></td>
<td><p>Templates</p></td>
</tr>
<tr>
<td><p>Unnatural Instructions</p></td>
<td><p>240K</p></td>
<td><p><a href="https://github.com/orhonovich/unnatural-instructions">Honovich et al.</a>, Meta</p></td>
<td><p>LLMs</p></td>
</tr>
<tr>
<td><p>LIMA (Less Is More for Alignment)</p></td>
<td><p>1K</p></td>
<td><p><a href="https://arxiv.org/abs/2305.11206">Zhou et al.</a>, Meta</p></td>
<td><p>Templates</p></td>
</tr>
<tr>
<td><p>Self-Instruct</p></td>
<td><p>52K</p></td>
<td><p><a href="https://github.com/yizhongw/self-instruct">Wang et al.</a></p></td>
<td><p>LLMs</p></td>
</tr>
<tr>
<td><p>Evol-Instruct</p></td>
<td><p>52K</p></td>
<td><p><a href="https://arxiv.org/abs/2304.12244">Xu et al.</a></p></td>
<td><p>LLMs</p></td>
</tr>
<tr>
<td><p>InstructWild v2</p></td>
<td><p>110K</p></td>
<td><p><a href="https://github.com/XueFuzhao/InstructionWild">Ni et al.</a></p></td>
<td><p>LLMs</p></td>
</tr>
<tr>
<td><p>Alpaca</p></td>
<td><p>52K</p></td>
<td><p>Stanford</p></td>
<td><p>LLMs</p></td>
</tr>
<tr>
<td><p>Guanaco</p></td>
<td><p>534K</p></td>
<td><p><a href="https://arxiv.org/abs/2305.14314">Dettmers et al.</a></p></td>
<td><p>LLMs</p></td>
</tr>
<tr>
<td><p>Vicuna</p></td>
<td><p>70K</p></td>
<td><p>LMSYS</p></td>
<td><p>Human conversations</p></td>
</tr>
<tr>
<td><p>OpenAssistant</p></td>
<td><p>161K</p></td>
<td><p>Open Assistant</p></td>
<td><p>Human conversations</p></td>
</tr>
</tbody>
</table>

<p>Let’s go through fine-tuned language net (FLAN), one of the most popular instruction-tuning datasets in detail. Understanding how it was constructed will provide you with roadmaps to create your own instruction-tuning datasets. Most publicly available instruction-tuning datasets are meant to augment an LLM that will be used for open-ended tasks, as opposed to domain-specific use cases.</p>

<p>FLAN is actually a collection of several datasets<a data-type="indexterm" data-primary="FLAN (Fine-tuned Language Net)" id="xi_FLANFinetunedLanguageNet657950"/>. The <a href="https://oreil.ly/SrXV-">FLAN collection</a>, published in 2022, is composed of five components:</p>

<ul>
<li>
<p>FLAN 2021</p>
</li>
<li>
<p>T0</p>
</li>
<li>
<p>Super-natural Instructions</p>
</li>
<li>
<p>Chain-of-Thought</p>
</li>
<li>
<p>Dialog</p>
</li>
</ul>

<p>The original FLAN 2021 datasets were one of the pioneering instruction-tuning datasets, which were used to train FLAN-T5. The FLAN 2021 datasets were constructed by taking existing academic NLP datasets and converting them to the instruction format using instruction templates. The templates were manually constructed, with ten templates created for each task. The templates are available <a href="https://oreil.ly/DNKCv">here</a>.</p>

<p>Here is how a template list for a task looks, as drawn from the <a href="https://oreil.ly/DNKCv">templates.py</a> file in the FLAN GitHub repo. Our example task is text summarization on the CNN/DailyMail news dataset:</p>

<pre data-type="programlisting" data-code-language="python"><code class="s2">"cnn_dailymail"</code><code class="p">:</code> <code class="p">[</code>
  <code class="p">(</code><code class="s2">"Write highlights for this article:</code><code class="se">\n\n</code><code class="si">{text}</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="si">{highlights}</code><code class="s2">"</code><code class="p">),</code>
  <code class="p">(</code><code class="s2">"Write some highlights for the following article:</code><code class="se">\n\n</code><code class="si">{text}</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="si">{highlights}</code><code class="s2">"</code><code class="p">),</code>
  <code class="p">(</code><code class="s2">"</code><code class="si">{text}</code><code class="se">\n\n</code><code class="s2">Write highlights for this article."</code><code class="p">,</code> <code class="s2">"</code><code class="si">{highlights}</code><code class="s2">"</code><code class="p">),</code>
  <code class="p">(</code><code class="s2">"</code><code class="si">{text}</code><code class="se">\n\n</code><code class="s2">What are highlight points for this article?"</code><code class="p">,</code> <code class="s2">"</code><code class="si">{highlights}</code><code class="s2">"</code><code class="p">),</code>
  <code class="p">(</code><code class="s2">"</code><code class="si">{text}</code><code class="se">\n</code><code class="s2">Summarize the highlights of this article."</code><code class="p">,</code> <code class="s2">"</code><code class="si">{highlights}</code><code class="s2">"</code><code class="p">),</code>
  <code class="p">(</code><code class="s2">"</code><code class="si">{text}</code><code class="se">\n</code><code class="s2">What are the important parts of this article?"</code><code class="p">,</code> <code class="s2">"</code><code class="si">{highlights}</code><code class="s2">"</code><code class="p">),</code>
  <code class="p">(</code><code class="s2">"</code><code class="si">{text}</code><code class="se">\n</code><code class="s2">Here is a summary of the highlights for this article:"</code><code class="p">,</code>
    <code class="s2">"</code><code class="si">{highlights}</code><code class="s2">"</code><code class="p">),</code>
  <code class="p">(</code><code class="s2">"Write an article using the following points:</code><code class="se">\n\n</code><code class="si">{highlights}</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="si">{text}</code><code class="s2">"</code><code class="p">),</code>
  <code class="p">(</code><code class="s2">"Use the following highlights to write an article:</code><code class="se">\n\n</code><code class="si">{highlights}</code><code class="s2">"</code><code class="p">,</code>
    <code class="s2">"</code><code class="si">{text}</code><code class="s2">"</code><code class="p">),</code>
  <code class="p">(</code><code class="s2">"</code><code class="si">{highlights}</code><code class="se">\n\n</code><code class="s2">Write an article based on these highlights."</code><code class="p">,</code> <code class="s2">"</code><code class="si">{text}</code><code class="s2">"</code><code class="p">),</code>
<code class="p">],</code></pre>

<p>Note that the last three instructions represent an inverted version of the task, where given a summary, the model is encouraged to write the entire article. This has been done to increase the diversity of the instructions at scale.</p>

<p>Rather than painstakingly constructing these templates by hand, can we automate their generation using LLMs? Yes, this is possible. We can leverage LLMs to generate more diverse templates. When I asked my favorite LLM to generate similar instructions to a news summarization task template provided in the prompt, it came up with:</p>

<pre data-type="programlisting" data-code-language="python"><code class="s2">"cnn_dailymail"</code><code class="p">:</code> <code class="p">[</code>
  <code class="p">(</code><code class="s2">"Distill the essence of this article:</code><code class="se">\n\n</code><code class="si">{text}</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="si">{highlights}</code><code class="s2">"</code><code class="p">),</code>
  <code class="p">(</code><code class="s2">"Give a quick rundown of this article's key points:</code><code class="se">\n\n</code><code class="si">{text}</code><code class="s2">"</code><code class="p">,</code>
    <code class="s2">"</code><code class="si">{highlights}</code><code class="s2">"</code><code class="p">),</code>
  <code class="p">(</code><code class="s2">"Summarize the main elements of this text:</code><code class="se">\n\n</code><code class="si">{text}</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="si">{highlights}</code><code class="s2">"</code><code class="p">),</code>
  <code class="p">(</code><code class="s2">"Highlight the primary takeaways from the following:</code><code class="se">\n\n</code><code class="si">{text}</code><code class="s2">"</code><code class="p">,</code>
    <code class="s2">"</code><code class="si">{highlights}</code><code class="s2">"</code><code class="p">),</code>
  <code class="p">(</code><code class="s2">"Extract and summarize the top points of this article:</code><code class="se">\n\n</code><code class="si">{text}</code><code class="s2">"</code><code class="p">,</code>
    <code class="s2">"</code><code class="si">{highlights}</code><code class="s2">"</code><code class="p">),</code>
  <code class="p">(</code><code class="s2">"Condense this article into its most important aspects:</code><code class="se">\n\n</code><code class="si">{text}</code><code class="s2">"</code><code class="p">,</code>
    <code class="s2">"</code><code class="si">{highlights}</code><code class="s2">"</code><code class="p">),</code>
  <code class="p">(</code><code class="s2">"What are the key insights of this article?</code><code class="se">\n\n</code><code class="si">{text}</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="si">{highlights}</code><code class="s2">"</code><code class="p">),</code>
      <code class="p">],</code></pre>

<p>As you can see, the generated templates reflect various ways of expressing the summarization task.</p>

<p>For classification tasks, it is recommended to append the instruction with an <em>Options</em> clause. This introduces the LLM to the output space and can thus concentrate the probability mass over the defined label space. Without this guidance, the LLM would distribute its probability across several different tokens that express the same concept, for example there are several different ways of expressing the <em>True</em> label in a binary classification task.
An example prompt is: “Identify the tone of this text. OPTIONS: happy, sad, neutral.”</p>

<p>Constructing these prompts manually can be a tedious exercise. The <a href="https://oreil.ly/WIyOq"><em>promptsource</em> tool</a> enables<a data-type="indexterm" data-primary="promptsource tool" id="id1084"/> you to create, access, and apply prompts through a graphical user interface tool or through the promptsource Python library. Here is an example from the Public Pool of Prompts (P3) collection<a data-type="indexterm" data-primary="Public Pool of Prompts (P3) collection" id="id1085"/><a data-type="indexterm" data-primary="P3 (Public Pool of Prompts) collection" id="id1086"/> for the paraphrasing task, constructed by Big Science, which is available through the promptsource tool. P3 prompts consist of an Input template, a Target template, and an Answer Choices 
<span class="keep-together">template</span>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">Input</code> <code class="n">Template</code><code class="p">:</code>
<code class="n">I</code> <code class="n">want</code> <code class="n">to</code> <code class="n">know</code> <code class="n">whether</code> <code class="n">the</code> <code class="n">following</code> <code class="n">two</code> <code class="n">sentences</code> <code class="n">mean</code> <code class="n">the</code> <code class="n">same</code> <code class="n">thing</code><code class="o">.</code>
<code class="p">{{</code><code class="n">sentence1</code><code class="p">}}</code>
<code class="p">{{</code><code class="n">sentence2</code><code class="p">}}</code>
<code class="n">Do</code> <code class="n">they</code><code class="err">?</code>

<code class="n">Target</code> <code class="n">Template</code><code class="p">:</code>
<code class="p">{{</code> <code class="n">answer_choices</code><code class="p">[</code><code class="n">label</code><code class="p">]</code> <code class="p">}}</code>

<code class="n">Answer</code> <code class="n">Choices</code> <code class="n">Template</code><code class="p">:</code>
<code class="n">no</code> <code class="o">|||</code> <code class="n">yes</code></pre>

<p>Another key component of the FLAN collection is the<a data-type="indexterm" data-primary="Super-NaturalInstructions dataset" id="id1087"/> <a href="https://oreil.ly/D_rv_">Super-NaturalInstructions dataset</a>. This dataset contains very rich descriptions of instructions that contain not just task definitions, but also positive and negative examples, constraints, and things to watch out for. The answers are enriched with explanations on why the answer was chosen. The effectiveness of adding explanations to the answer is not yet determined.</p>

<p>Here is an example of such a task from the Super-NaturalInstructions dataset:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">Definition</code>
<code class="n">In</code> <code class="n">this</code> <code class="n">task</code><code class="p">,</code> <code class="n">we</code> <code class="n">ask</code> <code class="n">you</code> <code class="n">convert</code> <code class="n">a</code> <code class="n">data</code> <code class="n">table</code> <code class="n">of</code> <code class="n">restaurant</code> <code class="n">descriptions</code> <code class="n">into</code>
<code class="n">fluent</code> <code class="n">natural</code><code class="o">-</code><code class="n">sounding</code> <code class="n">English</code> <code class="n">sentences</code><code class="o">.</code>
<code class="n">The</code> <code class="nb">input</code> <code class="ow">is</code> <code class="n">a</code> <code class="n">string</code> <code class="n">of</code> <code class="n">key</code><code class="o">-</code><code class="n">value</code> <code class="n">pairs</code><code class="p">;</code> <code class="n">the</code> <code class="n">output</code> <code class="n">should</code> <code class="n">be</code> <code class="n">a</code> <code class="n">natural</code> <code class="ow">and</code>
<code class="n">grammatical</code> <code class="n">English</code> <code class="n">sentence</code> <code class="n">containing</code> <code class="nb">all</code> <code class="n">the</code> <code class="n">information</code> <code class="kn">from</code> <code class="nn">the</code> <code class="nb">input</code><code class="o">.</code>

<code class="n">Positive</code> <code class="n">Example</code>

<code class="n">Input</code><code class="p">:</code> <code class="n">name</code><code class="p">[</code><code class="n">Aromi</code><code class="p">],</code> <code class="n">eatType</code><code class="p">[</code><code class="n">restaurant</code><code class="p">],</code> <code class="n">food</code><code class="p">[</code><code class="n">English</code><code class="p">],</code> <code class="n">area</code><code class="p">[</code><code class="n">city</code> <code class="n">centre</code><code class="p">]</code>

<code class="n">Output</code><code class="p">:</code> <code class="n">Aromi</code> <code class="ow">is</code> <code class="n">an</code> <code class="n">English</code> <code class="n">restaurant</code> <code class="ow">in</code> <code class="n">the</code> <code class="n">city</code> <code class="n">centre</code><code class="o">.</code>
<code class="n">Explanation</code><code class="p">:</code> <code class="n">The</code> <code class="n">output</code> <code class="n">sentence</code> <code class="n">faithfully</code> <code class="n">converts</code> <code class="n">the</code> <code class="n">data</code> <code class="ow">in</code> <code class="n">the</code> <code class="nb">input</code>
<code class="n">into</code> <code class="n">a</code> <code class="n">natural</code><code class="o">-</code><code class="n">sounding</code> <code class="n">sentence</code><code class="o">.</code>

<code class="n">Negative</code> <code class="n">Example</code>
<code class="n">Input</code><code class="p">:</code> <code class="n">name</code><code class="p">[</code><code class="n">Blue</code> <code class="n">Spice</code><code class="p">],</code> <code class="n">eatType</code><code class="p">[</code><code class="n">coffee</code> <code class="n">shop</code><code class="p">],</code> <code class="n">priceRange</code><code class="p">[</code><code class="n">more</code> <code class="n">than</code> <code class="mi">00</code><code class="n">a330</code><code class="p">],</code>
<code class="n">customer</code> <code class="n">rating</code><code class="p">[</code><code class="mi">5</code> <code class="n">out</code> <code class="n">of</code> <code class="mi">5</code><code class="p">],</code> <code class="err">˘</code>
<code class="n">area</code><code class="p">[</code><code class="n">riverside</code><code class="p">],</code> <code class="n">familyFriendly</code><code class="p">[</code><code class="n">yes</code><code class="p">],</code> <code class="n">near</code><code class="p">[</code><code class="n">Avalon</code><code class="p">]</code>
<code class="n">Output</code><code class="p">:</code> <code class="n">Blue</code> <code class="n">Spice</code> <code class="ow">is</code> <code class="n">a</code> <code class="n">Colombian</code> <code class="n">coffee</code> <code class="n">shop</code> <code class="n">located</code> <code class="n">by</code> <code class="n">the</code> <code class="n">riverside</code><code class="p">,</code> <code class="n">near</code>
<code class="n">Avalon</code> <code class="ow">in</code> <code class="n">Boston</code><code class="o">.</code> <code class="n">Its</code> <code class="n">prices</code> <code class="n">are</code> <code class="n">over</code>
<code class="mi">00</code><code class="n">a330</code><code class="o">.</code> <code class="n">Its</code> <code class="n">customer</code> <code class="n">ratings</code> <code class="n">are</code> <code class="mi">5</code> <code class="n">out</code> <code class="n">of</code> <code class="mf">5.</code> <code class="err">˘</code>

<code class="n">Explanation</code><code class="p">:</code> <code class="n">While</code> <code class="n">the</code> <code class="n">output</code> <code class="n">contains</code> <code class="n">most</code> <code class="n">of</code> <code class="n">the</code> <code class="n">information</code> <code class="kn">from</code> <code class="nn">the</code> <code class="nb">input</code><code class="p">,</code>
<code class="n">it</code> <code class="n">hallucinates</code> <code class="n">by</code> <code class="n">adding</code> <code class="n">ungrounded</code>
<code class="n">information</code> <code class="n">such</code> <code class="k">as</code> <code class="s2">"Colombian"</code> <code class="ow">and</code> <code class="s2">"Boston"</code><code class="o">.</code>

<code class="n">Instance</code> <code class="n">Input</code><code class="p">:</code> <code class="n">name</code><code class="p">[</code><code class="n">The</code> <code class="n">Mill</code><code class="p">],</code> <code class="n">eatType</code><code class="p">[</code><code class="n">restaurant</code><code class="p">],</code> <code class="n">area</code><code class="p">[</code><code class="n">riverside</code><code class="p">],</code> <code class="n">near</code><code class="p">[</code><code class="n">The</code>
<code class="n">Rice</code> <code class="n">Boat</code><code class="p">]</code>

<code class="n">Valid</code> <code class="n">Output</code><code class="p">:</code> <code class="p">[</code><code class="s2">"A restaurant called The Mill, can be found near the riverside</code>
<code class="nb">next</code> <code class="n">to</code> <code class="n">The</code> <code class="n">Rice</code> <code class="n">Boat</code><code class="o">.</code><code class="s2">"]</code></pre>

<p>Let’s now look at datasets that are constructed with the help of LLMs<a data-type="indexterm" data-startref="xi_publiclyavailabledatasetsforinstructiontuning6550104" id="id1088"/><a data-type="indexterm" data-startref="xi_instructiontunedmodelspubliclyavailable6550104" id="id1089"/><a data-type="indexterm" data-startref="xi_FLANFinetunedLanguageNet657950" id="id1090"/><a data-type="indexterm" data-startref="xi_datasetspubliclyavailable6550104" id="id1091"/>.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="LLM-Generated Instruction-Tuning Datasets"><div class="sect2" id="id106">
<h2>LLM-Generated Instruction-Tuning Datasets</h2>

<p>As seen earlier, hand-constructing these datasets<a data-type="indexterm" data-primary="instruction-tuned models" data-secondary="LLM-generated tuning" id="xi_instructiontunedmodelsLLMgeneratedtuning670550"/><a data-type="indexterm" data-primary="datasets" data-secondary="LLM-generated" id="xi_datasetsLLMgenerated670550"/><a data-type="indexterm" data-primary="generative AI" data-secondary="LLM-generated datasets" id="xi_generativeAILLMgenerateddatasets670550"/> can be painstaking, and paraphrasing/synthetic data generation is where LLMs shine. Therefore, we can leverage LLMs to generate our instruction-tuning datasets.
The <a href="https://oreil.ly/HVBfK">Self-Instruct</a> and <a href="https://oreil.ly/1wV_G">Unnatural Instructions papers</a> are the first attempts in this regard. Both start from a seed set of high-quality hand-generated examples, and then in a few-shot setting, ask the LLM to generate similar examples with more diverse linguistic expressions.</p>

<p>Given an instruction, a combination of input-first and output-first is shown to be beneficial for generating input-output pairs. Typically, you would generate input-output pairs using an input-first approach, where the LLM is asked to generate an input instance for the given instruction and subsequently asked to generate the output label for that input. However, this approach might lead to label imbalance as shown in <a href="https://oreil.ly/hYFYH">Wang et al.</a>, with certain labels being overrepresented. Therefore, it is a good approach to mix output-first generation, where you ask the LLM to generate the output label first and then ask it to generate an input text that satisfies the label.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>It is against OpenAI’s policies<a data-type="indexterm" data-primary="OpenAI" data-secondary="anti-competitive policy" id="id1092"/> to use its outputs to generate data that can be used to train a competing model. While there are several public instruction-tuning datasets that have been synthetically generated using GPT-4, they are technically violating OpenAI’s terms of service. I recommend using open source LLMs<a data-type="indexterm" data-primary="open source LLMs" id="id1093"/> for synthetic data generation instead.</p>
</div>

<p>Simply asking an LLM to generate similar examples to your seed set may not give you the desired results. You want a diverse but relevant set of examples, and it is easy for your LLM to drift into territory that ends up generating spurious examples outside of your desired distribution.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>How large should your instruction-tuning dataset be? The <a href="https://oreil.ly/z0BWh">“LIMA: Less Is More for Alignment” paper</a> shows that you need only a few thousand high-quality examples to effectively fine-tune a model.</p>
</div>

<p>Xu et al. propose <a href="https://oreil.ly/9nw3G">Evol-Instruct</a>, a structured<a data-type="indexterm" data-primary="Evol-Instruct" id="id1094"/> way to generate these synthetic instructions by making controlled edits to the seed examples. The process consists of three steps:</p>
<ol>
<li>
<p>Instruction evolution: The seed examples are evolved using in-depth and in-breadth strategies. In-depth evolution increases the complexity and difficulty of the original instruction through five types of prompts:</p>

<ul>
<li>
<p>Adding constraints</p>
</li>
<li>
<p>Increasing reasoning steps</p>
</li>
<li>
<p>Asking deeper questions</p>
</li>
<li>
<p>Asking more specific questions</p>
</li>
<li>
<p>Increasing the complexity of the input</p>

<p>In-breadth evolution increases topic coverage by generating a completely new instruction from the same domain as the original instruction.</p>
</li>
</ul>
</li>
<li>
<p>Response generation: The response for the evolved instruction is generated, either using humans or LLMs.</p>
</li>
<li>
<p>Candidate filtering: Candidate instances that do not meet quality criteria are filtered out. You could use either heuristics or LLMs for candidate filtering.</p>
</li>

</ol>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Why not pre-train<a data-type="indexterm" data-primary="pre-training of data" data-secondary="instruction-tuning dataset issue" id="id1095"/> on instruction-tuning datasets? If instruction-tuning is a necessary step after pre-training a model, why don’t we just pre-train the model using an instruction-tuning dataset? It is indeed possible, but these datasets are hard to construct at scale without incurring a significant drop in quality.</p>

<p>We need not wait until someone releases a massive dataset to reap the benefits of instruction-tuning during the pre-training phase. It has been <a href="https://oreil.ly/tfO4a">shown</a> that mixing instruction-tuning data during pre-training is beneficial<a data-type="indexterm" data-startref="xi_finetuningmodelsdatasetinstructiontuning646927" id="id1096"/><a data-type="indexterm" data-startref="xi_datasetsfinetuning646927" id="id1097"/><a data-type="indexterm" data-startref="xi_instructiontunedmodelsLLMgeneratedtuning670550" id="id1098"/><a data-type="indexterm" data-startref="xi_datasetsLLMgenerated670550" id="id1099"/><a data-type="indexterm" data-startref="xi_generativeAILLMgenerateddatasets670550" id="id1100"/>.</p>
</div>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1101">
<h1>Exercise</h1>
<p>Take all of the Canadian parliamentary proceedings data and convert it into an instruction-tuning dataset. This task sounds daunting, but luckily we have libraries that facilitate this process. One such library is called <a href="https://oreil.ly/8wJ_o">Bonito</a>, which comes with a model for conditional task generation. This library takes unstructured text and converts it into instruction tuning format. Several types of tasks are supported, including summarization, sentiment, and question generation.</p>

<p>Use this library to create an instruction-tuning dataset from the parliamentary proceedings data. What is the quality of the resulting dataset? How can you further improve the diversity of the dataset?</p>
</div></aside>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="id107">
<h1>Summary</h1>

<p>In this chapter, we underscored the inevitability of needing to fine-tune models to solve more complex tasks<a data-type="indexterm" data-startref="xi_finetuningmodels6558" id="id1102"/>. We performed a deep dive of the fine-tuning process and highlighted the tradeoffs involved in selecting hyperparameters. We also showed the uncanny effectiveness of instruction-tuning along with pointers on how to create your own instruction-tuning datasets.</p>

<p>In the next chapter, we will discuss more advanced techniques for updating an LLM’s parameters, including continual pre-training, parameter efficient fine-tuning, and model merging.</p>
</div></section>
</div></section></div>
</div>
</body></html>