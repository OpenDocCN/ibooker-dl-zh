["```py\nfrom pathlib import Path\nimport pandas as pd\nfrom ch06 import (\n    download_and_unzip_spam_data,\n    create_balanced_dataset,\n    random_split\n)\n\nurl = \\ \n\"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\nzip_path = \"sms_spam_collection.zip\"\nextracted_path = \"sms_spam_collection\"\ndata_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n\ndownload_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n\ndf = pd.read_csv(\n    data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"]\n)\nbalanced_df = create_balanced_dataset(df)\nbalanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n\ntrain_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\ntrain_df.to_csv(\"train.csv\", index=None)\nvalidation_df.to_csv(\"validation.csv\", index=None)\ntest_df.to_csv(\"test.csv\", index=None)\n```", "```py\nimport torch\nfrom torch.utils.data import Dataset\nimport tiktoken\nfrom chapter06 import SpamDataset\n\ntokenizer = tiktoken.get_encoding(\"gpt2\")\ntrain_dataset = SpamDataset(\"train.csv\", max_length=None, \n    tokenizer=tokenizer\n)\nval_dataset = SpamDataset(\"validation.csv\", \n    max_length=train_dataset.max_length, tokenizer=tokenizer\n)\ntest_dataset = SpamDataset(\n    \"test.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer\n)\n```", "```py\nfrom torch.utils.data import DataLoader\n\nnum_workers = 0\nbatch_size = 8\n\ntorch.manual_seed(123)\n\ntrain_loader = DataLoader(\n    dataset=train_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=num_workers,\n    drop_last=True,\n)\n\nval_loader = DataLoader(\n    dataset=val_dataset,\n    batch_size=batch_size,\n    num_workers=num_workers,\n    drop_last=False,\n)\n\ntest_loader = DataLoader(\n    dataset=test_dataset,\n    batch_size=batch_size,\n    num_workers=num_workers,\n    drop_last=False,\n)\n```", "```py\nprint(\"Train loader:\")\nfor input_batch, target_batch in train_loader:\n    pass\n\nprint(\"Input batch dimensions:\", input_batch.shape)\nprint(\"Label batch dimensions\", target_batch.shape)\n```", "```py\nTrain loader:\nInput batch dimensions: torch.Size([8, 120])\nLabel batch dimensions torch.Size([8])\n```", "```py\nprint(f\"{len(train_loader)} training batches\")\nprint(f\"{len(val_loader)} validation batches\")\nprint(f\"{len(test_loader)} test batches\")\n```", "```py\n130 training batches\n19 validation batches\n38 test batches\n```", "```py\nfrom gpt_download import download_and_load_gpt2\nfrom chapter04 import GPTModel\nfrom chapter05 import load_weights_into_gpt\n\nCHOOSE_MODEL = \"gpt2-small (124M)\"\nINPUT_PROMPT = \"Every effort moves\"\n\nBASE_CONFIG = {\n    \"vocab_size\": 50257,         #1\n    \"context_length\": 1024,      #2\n    \"drop_rate\": 0.0,            #3\n    \"qkv_bias\": True             #4\n}\n\nmodel_configs = {\n    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n}\n\nBASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n\nmodel_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\nsettings, params = download_and_load_gpt2(\n    model_size=model_size, models_dir=\"gpt2\"\n)\n\nmodel = GPTModel(BASE_CONFIG)\nload_weights_into_gpt(model, params)\nmodel.eval()\n```", "```py\nfrom chapter04 import generate_text_simple\nfrom chapter05 import text_to_token_ids, token_ids_to_text\n\ntext_1 = \"Every effort moves you\"\n\ntoken_ids = generate_text_simple(\n    model=model,\n    idx=text_to_token_ids(text_1, tokenizer),\n    max_new_tokens=15,\n    context_size=BASE_CONFIG[\"context_length\"]\n)\n\nprint(token_ids_to_text(token_ids, tokenizer))\n```", "```py\nEvery effort moves you forward.\nThe first step is to understand the importance of your work\n```", "```py\ntorch.manual_seed(123)\nnum_classes = 2\nmodel.out_head = torch.nn.Linear(in_features=768, out_features=num_classes)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n```", "```py\nfrom chapter06 import calc_accuracy_loader\n\ntorch.manual_seed(123)\ntrain_accuracy = calc_accuracy_loader(\n    train_loader, model, device, num_batches=10\n)\nval_accuracy = calc_accuracy_loader(\n    val_loader, model, device, num_batches=10\n)\ntest_accuracy = calc_accuracy_loader(\n    test_loader, model, device, num_batches=10\n)\n\nprint(f\"Training accuracy: {train_accuracy*100:.2f}%\")\nprint(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\nprint(f\"Test accuracy: {test_accuracy*100:.2f}%\")\n```", "```py\nTraining accuracy: 46.25%\nValidation accuracy: 45.00%\nTest accuracy: 48.75%\n```", "```py\nimport math\n\nclass LoRALayer(torch.nn.Module):\n    def __init__(self, in_dim, out_dim, rank, alpha):\n        super().__init__()\n        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\n        torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))    #1\n        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n        self.alpha = alpha\n\n    def forward(self, x):\n        x = self.alpha * (x @ self.A @ self.B)\n        return x\n```", "```py\nclass LinearWithLoRA(torch.nn.Module):\n    def __init__(self, linear, rank, alpha):\n        super().__init__()\n        self.linear = linear\n        self.lora = LoRALayer(\n            linear.in_features, linear.out_features, rank, alpha\n        )\n\n    def forward(self, x):\n        return self.linear(x) + self.lora(x)\n```", "```py\ndef replace_linear_with_lora(model, rank, alpha):\n    for name, module in model.named_children():\n        if isinstance(module, torch.nn.Linear):     #1\n            setattr(model, name, LinearWithLoRA(module, rank, alpha))\n        else:    #2\n            replace_linear_with_lora(module, rank, alpha)\n```", "```py\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total trainable parameters before: {total_params:,}\")\n\nfor param in model.parameters():\n    param.requires_grad = False\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total trainable parameters after: {total_params:,}\")\n```", "```py\nTotal trainable parameters before: 124,441,346\nTotal trainable parameters after: 0\n```", "```py\nreplace_linear_with_lora(model, rank=16, alpha=16)\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total trainable LoRA parameters: {total_params:,}\")\n```", "```py\nTotal trainable LoRA parameters: 2,666,528\n```", "```py\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nprint(model)\n```", "```py\nGPTModel(\n  (tok_emb): Embedding(50257, 768)\n  (pos_emb): Embedding(1024, 768)\n  (drop_emb): Dropout(p=0.0, inplace=False)\n  (trf_blocks): Sequential(\n    ...\n    (11): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): LinearWithLoRA(\n          (linear): Linear(in_features=768, out_features=768, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_key): LinearWithLoRA(\n          (linear): Linear(in_features=768, out_features=768, bias=True)\n          (lora): LoRALayer()\n        )\n        (W_value): LinearWithLoRA(\n          (linear): Linear(in_features=768, out_features=768, bias=True)\n          (lora): LoRALayer()\n        )\n        (out_proj): LinearWithLoRA(\n          (linear): Linear(in_features=768, out_features=768, bias=True)\n          (lora): LoRALayer()\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): LinearWithLoRA(\n            (linear): Linear(in_features=768, out_features=3072, bias=True)\n            (lora): LoRALayer()\n          )\n          (1): GELU()\n          (2): LinearWithLoRA(\n            (linear): Linear(in_features=3072, out_features=768, bias=True)\n            (lora): LoRALayer()\n          )\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_resid): Dropout(p=0.0, inplace=False)\n    )\n  )\n  (final_norm): LayerNorm()\n  (out_head): LinearWithLoRA(\n    (linear): Linear(in_features=768, out_features=2, bias=True)\n    (lora): LoRALayer()\n  )\n)\n```", "```py\ntorch.manual_seed(123)\n\ntrain_accuracy = calc_accuracy_loader(\n    train_loader, model, device, num_batches=10\n)\nval_accuracy = calc_accuracy_loader(\n    val_loader, model, device, num_batches=10\n)\ntest_accuracy = calc_accuracy_loader(\n    test_loader, model, device, num_batches=10\n)\n\nprint(f\"Training accuracy: {train_accuracy*100:.2f}%\")\nprint(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\nprint(f\"Test accuracy: {test_accuracy*100:.2f}%\")\n```", "```py\nTraining accuracy: 46.25%\nValidation accuracy: 45.00%\nTest accuracy: 48.75%\n```", "```py\nimport time\nfrom chapter06 import train_classifier_simple\n\nstart_time = time.time()\ntorch.manual_seed(123)\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n\nnum_epochs = 5\ntrain_losses, val_losses, train_accs, val_accs, examples_seen = \\\n    train_classifier_simple(\n        model, train_loader, val_loader, optimizer, device,\n        num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n        tokenizer=tokenizer\n    )\n\nend_time = time.time()\nexecution_time_minutes = (end_time - start_time) / 60\nprint(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n```", "```py\nEp 1 (Step 000000): Train loss 3.820, Val loss 3.462 \nEp 1 (Step 000050): Train loss 0.396, Val loss 0.364 \nEp 1 (Step 000100): Train loss 0.111, Val loss 0.229 \nTraining accuracy: 97.50% | Validation accuracy: 95.00% \nEp 2 (Step 000150): Train loss 0.135, Val loss 0.073 \nEp 2 (Step 000200): Train loss 0.008, Val loss 0.052 \nEp 2 (Step 000250): Train loss 0.021, Val loss 0.179 \nTraining accuracy: 97.50% | Validation accuracy: 97.50%\nEp 3 (Step 000300): Train loss 0.096, Val loss 0.080 \nEp 3 (Step 000350): Train loss 0.010, Val loss 0.116 \nTraining accuracy: 97.50% | Validation accuracy: 95.00% \nEp 4 (Step 000400): Train loss 0.003, Val loss 0.151 \nEp 4 (Step 000450): Train loss 0.008, Val loss 0.077 \nEp 4 (Step 000500): Train loss 0.001, Val loss 0.147 \nTraining accuracy: 100.00% | Validation accuracy: 97.50%\nEp 5 (Step 000550): Train loss 0.007, Val loss 0.094 \nEp 5 (Step 000600): Train loss 0.000, Val loss 0.056 \nTraining accuracy: 100.00% | Validation accuracy: 97.50%\n\nTraining completed in 12.10 minutes.\n```", "```py\nfrom chapter06 import plot_values\n\nepochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\nexamples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n\nplot_values(\n    epochs_tensor, examples_seen_tensor, \n    train_losses, val_losses, label=\"loss\"\n)\n```", "```py\ntrain_accuracy = calc_accuracy_loader(train_loader, model, device)\nval_accuracy = calc_accuracy_loader(val_loader, model, device)\ntest_accuracy = calc_accuracy_loader(test_loader, model, device)\n\nprint(f\"Training accuracy: {train_accuracy*100:.2f}%\")\nprint(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\nprint(f\"Test accuracy: {test_accuracy*100:.2f}%\")\n```", "```py\nTraining accuracy: 100.00%\nValidation accuracy: 96.64%\nTest accuracy: 98.00%\n```"]