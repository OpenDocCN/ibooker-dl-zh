["```py\npython3 -m venv chapter12env\n```", "```py\nsource chapter12env/bin/activate\n```", "```py\npip install torchserve torch-model-archiver torch-workflow-archiver\n```", "```py\nmkdir model_store\n```", "```py\n# config.properties\ninference_address=http://0.0.0.0:8080\nmanagement_address=http://0.0.0.0:8081\nmetrics_address=http://0.0.0.0:8082\nnumber_of_netty_threads=32\njob_queue_size=1000\nmodel_store=model_store\ndefault_response_timeout=120\ndefault_workers_per_model=1\nlog_level=DEBUG\n```", "```py\nimport torch\nimport torch.nn as nn\n\nclass SimpleLinearModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(1, 1)\n\n    def forward(self, x):\n        return self.linear(x)\n```", "```py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom linear import SimpleLinearModel\n\ndef train_model():\n    model = SimpleLinearModel()\n    optimizer = optim.SGD(model.parameters(), lr=0.01)\n    criterion = nn.MSELoss()\n\n    xs = torch.tensor([[–1.0], [0.0], [1.0], [2.0], [3.0], [4.0]], \n                      dtype=torch.float32)\n    ys = torch.tensor([[–3.0], [–1.0], [1.0], [3.0], [5.0], [7.0]], \n                      dtype=torch.float32)\n\n    for _ in range(500):\n        optimizer.zero_grad()\n        outputs = model(xs)\n        loss = criterion(outputs, ys)\n        loss.backward()\n        optimizer.step()\n\n    return model\n# Save model\nmodel = train_model()\ntorch.save(model.state_dict(), \"model.pth\")\n```", "```py\nfrom ts.torch_handler.base_handler import BaseHandler\n```", "```py\nclass ModelHandler(BaseHandler):\n    def __init__(self):\n        super().__init__()\n        self.initialized = False\n        logger.info(\"ModelHandler initialized\")\n```", "```py\ndef initialize(self, ctx):\n    self.manifest = ctx.manifest\n    properties = ctx.system_properties\n    model_dir = properties.get(\"model_dir\")\n\n    # Load model\n    serialized_file = \"model.pth\"\n    model_pt_path = os.path.join(model_dir, serialized_file)\n    self.device = torch.device(\"cuda:\" + str(properties.get(\"gpu_id\")) \n                  if torch.cuda.is_available() else \"cpu\")\n\n    # Initialize model\n    self.model = SimpleLinearModel()\n    state_dict = torch.load(model_pt_path, weights_only=True)\n    self.model.load_state_dict(state_dict)\n    self.model.to(self.device)\n    self.model.eval()\n\n    self.initialized = True\n    return self\n```", "```py\ndef preprocess(self, data):\n    # Get the value directly without decoding\n    value = float(data[0].get(\"body\"))\n    tensor = torch.tensor([value], dtype=torch.float32).view(1, 1)\n    return tensor.to(self.device)\n```", "```py\ndef inference(self, data):\n    \"\"\"Run inference on the preprocessed data\"\"\"\n    with torch.no_grad():\n        results = self.model(data)\n    return results\n```", "```py\ndef postprocess(self, inference_output):\n    \"\"\"Return inference result\"\"\"\n    return inference_output.tolist()\n```", "```py\ntorch-model-archiver \n  --model-name simple_linear\n  --version 1.0\n  --serialized-file model.pth\n  --handler model_handler.py\n  --model-file models/linear.py\n  --export-path model-store\n  --force\n  --extra-files models/linear.py,models/__init__.py\n```", "```py\ntorchserve\n  --start\n  --model-store model-store\n  --ts-config config/config.properties\n  --disable-token-auth\n  --models simple_linear=model_store/simple_linear.mar\n```", "```py\ncurl -X POST http://127.0.0.1:8080/predictions/simple_linear -H \n                       \"Content-Type: text/plain\" -d \"5.0\"\n```", "```py\n[\n  8.997674942016602\n]\n```", "```py\ncurl http://localhost:8081/models\n```", "```py\n{\n  \"models\": [\n    {\n      \"modelName\": \"simple_linear\",\n      \"modelUrl\": \"model_store/simple_linear.mar\"\n    }\n  ]\n}\n\n```", "```py\ncurl http://localhost:8081/models/simple_linear\n```", "```py\n[\n  {\n    \"modelName\": \"simple_linear\",\n    \"modelVersion\": \"1.0\",\n    \"modelUrl\": \"model_store/simple_linear.mar\",\n    \"runtime\": \"python\",\n    \"minWorkers\": 1,\n    \"maxWorkers\": 1,\n    \"batchSize\": 1,\n    \"maxBatchDelay\": 100,\n    \"responseTimeout\": 120,\n    \"startupTimeout\": 120,\n    \"maxRetryTimeoutInSec\": 300,\n    \"clientTimeoutInMills\": 0,\n    \"parallelType\": \"\",\n    \"parallelLevel\": 0,\n    \"deviceType\": \"gpu\",\n    \"continuousBatching\": false,\n    \"useJobTicket\": false,\n    \"useVenv\": false,\n    \"stateful\": false,\n    \"sequenceMaxIdleMSec\": 0,\n    \"sequenceTimeoutMSec\": 0,\n    \"maxNumSequence\": 0,\n    \"maxSequenceJobQueueSize\": 0,\n    \"loadedAtStartup\": true,\n    \"workers\": [\n      {\n        \"id\": \"9000\",\n        \"startTime\": \"2024-11-16T08:26:59.394Z\",\n        \"status\": \"READY\",\n        \"memoryUsage\": 0,\n        \"pid\": 20395,\n        \"gpu\": true,\n        \"gpuUsage\": \"failed to obtained gpu usage\"\n      }\n    ],\n    \"jobQueueStatus\": {\n      \"remainingCapacity\": 1000,\n      \"pendingRequests\": 0\n    }\n  }\n]\n```", "```py\npip install flask\n```", "```py\napp = Flask(__name__)\n\n```", "```py\nfrom flask import Flask, request, jsonify\nimport torch\nfrom model_def import SimpleLinearModel\n\napp = Flask(__name__)\n\n# Load the trained model\nmodel = SimpleLinearModel()\nmodel.load_state_dict(torch.load(\"model.pth\"))\nmodel.eval()\n\n@app.route(\"/predict\", methods=[\"POST\"])\ndef predict():\n    value = float(request.form.get('value', 0))\n    input_tensor = torch.tensor([[value]], dtype=torch.float32)\n    with torch.no_grad():\n        prediction = model(input_tensor)\n\n    return jsonify({\n        \"input\": value,\n        \"prediction\": prediction.item()\n    })\n\nif __name__ == \"__main__\":\n    app.run(port=5001)\n```", "```py\ncurl -X POST -d \"value=5\" http://localhost:5001/predict\n```", "```py\n{\"input\":5.0,\"prediction\":8.993191719055176}\n```"]