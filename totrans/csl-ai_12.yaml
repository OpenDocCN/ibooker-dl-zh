- en: 10 Identification and the causal hierarchy
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10 识别和因果层次结构
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Motivating examples for identification
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别的激励示例
- en: Using y0 for identification and deriving estimands
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用y0进行识别和推导估计值
- en: How to derive counterfactual graphs in y0
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在y0中推导出反事实图
- en: Deriving SWIGs for graph-based counterfactual identification
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推导基于图的反事实识别的SWIGs
- en: The practice of advancing machine learning often relies on a blind confidence
    that more data and the right architecture can solve any task. For tasks with causal
    elements, *causal identification* can make that less of a matter of faith and
    more of a science. It can tell us when more data won’t help, and what types of
    inductive biases are needed for the algorithm to work.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 提高机器学习实践通常依赖于一种盲目的信心，即更多的数据和正确的架构可以解决任何任务。对于具有因果因素的任务，*因果识别*可以使这不再是信仰的问题，而是科学的问题。它可以告诉我们更多数据不会有所帮助，以及算法要正常工作需要哪些类型的归纳偏差。
- en: Causal identification is the task of determining when we can make a causal inference
    from purely observational data or a counterfactual inference from observational
    or experimental data. In statistics and data science, it is the theory that allows
    us to distill causation from correlation and estimate causal effects in the presence
    of confounders. But causal identification has applications in AI. For example,
    suppose a deep learning algorithm achieves high performance on a particular causal
    reasoning benchmark. The ideas behind causal identification tell us that certain
    causal inductive biases must be baked into the model architecture, training data,
    training procedure, hyperparameters (e.g., prompts), and/or benchmark data. By
    tracking down that causal information, we can make sure the algorithm can consistently
    achieve that benchmark performance in new scenarios.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 因果识别是从纯粹观察数据或从观察或实验数据中做出反事实推断的任务。在统计学和数据科学中，它是允许我们从相关性中提炼因果关系并估计存在混杂因素时的因果效应的理论。但因果识别在人工智能领域也有应用。例如，假设一个深度学习算法在特定的因果推理基准上取得了高性能。因果识别背后的思想告诉我们，某些因果归纳偏差必须嵌入到模型架构、训练数据、训练过程、超参数（例如，提示）和/或基准数据中。通过追踪这些因果信息，我们可以确保算法在新场景中能够持续达到该基准性能。
- en: Identification is a theory-heavy part of causal inference. Fortunately, we can
    rely on libraries to do the theoretical heavy lifting for us and focus on skill-building
    with these libraries. In this chapter, we’ll focus on a library called y0 (pronounced
    why-not), which implements algorithms for identification using graphs. By the
    end of the chapter, we’ll have demystified causal identification and you’ll know
    how to apply y0’s identification algorithms.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 识别是因果推理中理论性很强的部分。幸运的是，我们可以依赖库来为我们做理论上的繁重工作，并专注于使用这些库进行技能培养。在本章中，我们将重点关注一个名为y0（发音为“why-not”）的库，该库使用图实现识别算法。到本章结束时，我们将揭开因果识别的神秘面纱，你将知道如何应用y0的识别算法。
- en: 10.1 The causal hierarchy
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1 因果层次结构
- en: The *causal hierarchy*, also known as *Pearl’s hierarchy* or the *ladder of
    causation*, is a three-level hierarchy over the types of causal questions we ask,
    models we build, data we acquire, and causal inferences we make.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*因果层次结构*，也称为*Pearl的层次结构*或*因果阶梯*，是我们询问的因果问题类型、构建的模型、获取的数据以及做出的因果推断的三层层次结构。'
- en: 'The causal hierarchy consists of three levels:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 因果层次结构由三个层次组成：
- en: Association
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关联
- en: Intervention
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 介入
- en: Counterfactual
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反事实
- en: When we do a statistical or causal analysis, we are reasoning at one of these
    three levels. When we know at what level we are reasoning, we can determine what
    kind of assumptions and data we need to rely on to do that reasoning correctly.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们进行统计或因果分析时，我们正在这些三个层次中的一个进行推理。当我们知道我们在哪个层次进行推理时，我们可以确定我们需要依赖哪些假设和数据来进行正确的推理。
- en: 10.1.1 Where questions and queries fall on the hierarchy
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.1 问题和查询在层次结构中的位置
- en: The questions we ask of our causal model, and the causal queries we formalize
    from those questions, fall at different levels of the hierarchy. First, level
    1 (the association level) is concerned with “What is…?” questions. Let’s illustrate
    with the online gaming example, shown again in figure 10.1.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对因果模型提出的问题，以及从这些问题中形式化的因果查询，位于层次结构的不同层次。首先，第1层（关联层）关注“是什么...？”的问题。让我们以在线游戏示例来说明，如图10.1所示。
- en: '![figure](../Images/CH10_F01_Ness.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH10_F01_Ness.png)'
- en: Figure 10.1 The DAG for the online gaming example
  id: totrans-19
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.1 在线游戏示例的DAG
- en: An example level 1 question and associated query is
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一个第1级问题及其相关查询的例子是
- en: “What are in-game purchase amounts for players highly engaged in side-quests?”
    *P* ( *I*| *E*=“high”)
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “对于高度参与支线任务的玩家，他们的游戏内购买金额是多少？” *P* ( *I*| *E*=“high”)
- en: Reasoning at this level aims to describe, model, or detect dependence between
    variables. At this level, we’re not reasoning about any causal relationships between
    the variables.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个级别上的推理旨在描述、建模或检测变量之间的依赖关系。在这个级别上，我们不是在推理变量之间的任何因果关系。
- en: Questions at level 2 (the intervention level) involve non-counterfactual hypothetical
    conditions, such as
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 第2级（干预级）的问题涉及非反事实的假设条件，例如
- en: “What would in-game purchases be for a player if side-quest engagement were
    high?” *P* ( *I*  [*E*] [=“high”])
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “如果支线任务参与度高，玩家的游戏内购买会是什么？” *P* ( *I*  [*E*] [=“high”])
- en: At level 2, we formalize such questions with the ideal intervention. Note that
    any query derived from a level 2 query is also a level 2 query, such as ATEs,
    (e.g., *E*(*I*[*E*][=“high”] – *I*[*E*][=“low”])) and CATEs.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2级，我们使用理想干预来形式化这些问题。请注意，从第2级查询派生出的任何查询也是第2级查询，例如ATEs（例如，*E*(*I*[*E*][=“high”]
    – *I*[*E*][=“low”]))和CATEs。
- en: 'Finally, counterfactual questions and queries fall at level 3 (the counterfactual
    level):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，反事实问题和查询位于第3级（反事实级）：
- en: “Given this player had low side-quest engagement and low purchases, what would
    their level of purchases have been if they were more engaged?” *P*( *I* [*E*]
    [=“high”]| *E*=“low”, *I*=“low”)
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “假设这个玩家有低支线任务参与度和低购买，如果他们更投入，他们的购买水平会是什么？” *P*( *I* [*E*] [=“high”]| *E*=“low”,
    *I*=“low”）
- en: As with level 2 queries, any query we derive from a level 3 query also falls
    at level 3\. For example, a causal attribution query designed to answer “Why did
    this player have low purchases” would be a level 3 query if it were a function
    of level 3 queries like the probabilities of causation described in section 8.3.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 与第2级查询一样，我们从第3级查询派生出的任何查询也位于第3级。例如，一个旨在回答“为什么这个玩家的购买量低”的因果归因查询，如果它是第3级查询（如第8.3节中描述的因果概率）的函数，则它将是一个第3级查询。
- en: In identification, we work directly with queries. The y0 library in Python gives
    us a domain specific language for representing queries. The following code implements
    the query *P*(*I*[*E*][=][*e*]).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在识别中，我们直接与查询打交道。Python 中的 y0 库为我们提供了一个用于表示查询的特定领域语言。以下代码实现了查询 *P*(*I*[*E*][=][*e*])。
- en: Listing 10.1 Creating a query in y0
  id: totrans-30
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.1 在 y0 中创建查询
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 “P” is for probability distributions, and “Variable” is for defining. variables.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 “P”代表概率分布，“Variable”用于定义变量。'
- en: '#2 Define variables G (guild membership), E (side-quest engagement), and I
    (in-game purchases).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 定义变量 G（公会成员资格）、E（支线任务参与度）和 I（游戏内购买）。'
- en: '#3 Define the distributional query P(I [E]).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 定义分布查询 P(I [E])。'
- en: '#4 If running in a notebook environment, this will show a rendered image of
    P(I [E]).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 如果在笔记本环境中运行，这将显示 P(I [E]) 的渲染图像。'
- en: Setting up your environment
  id: totrans-36
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 设置你的环境
- en: In this chapter, I rely on version 0.2.0 of the y0 library. As it is a relatively
    new library, the library’s API is in development and recent versions will deviate
    slightly from what is shown here. Check out the library’s tutorials for recent
    developments.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我依赖于 y0 库的 0.2.0 版本。由于这是一个相对较新的库，库的 API 正在开发中，最新版本将与此处所示略有不同。查看库的教程以获取最新发展。
- en: Again, we rely on Graphviz and some custom utilities for plotting DAGs. The
    Graphviz installation depends on your environment. I am using Ubuntu 22.04 and
    install Graphviz via libgraphviz-dev. Then I install Python libraries graphviz
    version 0.20.3, and PyGraphviz version 1.13\. The Graphviz code is for plotting
    only, so if you get stuck, you could forgo plotting for the rest of the code.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我们依赖于 Graphviz 和一些自定义工具来绘制 DAG。Graphviz 的安装取决于你的环境。我使用 Ubuntu 22.04，通过
    libgraphviz-dev 安装 Graphviz。然后我安装 Python 库 graphviz 版本 0.20.3 和 PyGraphviz 版本
    1.13。Graphviz 代码仅用于绘图，所以如果你遇到困难，你可以跳过绘图而继续编写其余的代码。
- en: The `query` object is an object of the class `Probability`. The class’s `__repr__`
    method (which tells Python what to return in the terminal when you call it directly)
    is implemented such that when we evaluate the object in the last line of the preceding
    code in a Jupyter notebook, it will display rendered LaTeX (a typesetting/markup
    language with a focus on math notation), as in figure 10.2.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`query` 对象是 `Probability` 类的一个对象。该类的 `__repr__` 方法（它告诉 Python 当直接调用它时返回什么）实现为，当我们在一个
    Jupyter 笔记本中的上一行代码的最后评估该对象时，它将显示渲染的 LaTeX（一种专注于数学符号的排版/标记语言），如图 10.2 所示。'
- en: '![figure](../Images/CH10_F02_Ness.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH10_F02_Ness.png)'
- en: Figure 10.2 The rendered math image returned when you evaluate the `query` object
    in listing 10.1
  id: totrans-41
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 10.2 当你在列表 10.1 中评估 `query` 对象时返回的渲染数学图像
- en: The causal hierarchy applies to models and data as well.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 因果层次结构适用于模型和数据。
- en: 10.1.2 Where models and assumptions fall on the hierarchy
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.2 模型和假设在层次结构中的位置
- en: A “model” is a set of assumptions about the data generating process (DGP). Those
    assumptions live at various levels of the hierarchy.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: “模型”是一组关于数据生成过程（DGP）的假设。这些假设存在于层次结构的各个层级。
- en: Level 1 assumptions
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Level 1 假设
- en: Models at the associational level have statistical but non-causal assumptions.
    For example, suppose we’re interested in *P*(*I*|*E*=*e*), for either value (“low”,
    “high”) that *e* might take. We might fit a linear model to regress in-game purchases
    *I* against side-quest engagement *E*. Or we might train a neural network that
    maps *E* to *I*. These are two statistical models with different parameterizations.
    In other words, they differ in non-causal, statistical assumptions placed on *P*(*I*|*E*).
    Once we add causal assumptions, we move to a higher level of the hierarchy.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在关联层面的模型具有统计但非因果的假设。例如，假设我们感兴趣的是 *P*(*I*|*E*=*e*)，对于 *e* 可能采取的任何值（“低”，“高”）。我们可能拟合一个线性模型来回归游戏内购买
    *I* 对侧任务参与 *E*。或者我们可能训练一个将 *E* 映射到 *I* 的神经网络。这些都是两个参数化不同的统计模型。换句话说，它们在非因果、统计假设上对
    *P*(*I*|*E*) 的放置上有所不同。一旦我们添加因果假设，我们就进入了层次结构的高层。
- en: Level 2 assumptions
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Level 2 假设
- en: Assumptions that we can represent with a causal DAG are level 2 (interventional)
    assumptions. An example of a level 2 model would be a causal graphical model (aka,
    a causal Bayesian network)—a probabilistic model trained on a causal DAG. A causal
    DAG by itself is a level 2 set of assumptions; assumptions about what causes what.
    Generally, assumptions that let you deduce the consequences of an intervention
    are level 2 assumptions.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用因果有向图（DAG）表示的假设是第二层（介入）假设。一个第二层模型的例子是因果图模型（也称为因果贝叶斯网络）——一个在因果有向图上训练的概率模型。一个因果有向图本身就是一个第二层假设集；关于什么导致什么的假设。一般来说，允许你推断干预措施后果的假设是第二层假设。
- en: Level 3 Assumptions
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Level 3 假设
- en: The canonical example of a level 3 model is a structural causal model. But more
    generally, assumptions about mechanism—*how* variables affect one another—are
    level 3 (counterfactual) assumptions.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Level 3 模型的典型例子是结构因果模型。但更普遍地说，关于机制——*变量如何相互影响*——的假设是第三层（反事实）假设。
- en: One way to think about this is that any causal assumption you cannot represent
    in the structure of the DAG is, by process of elimination, a level 3 assumption.
    For example, suppose your DAG has the edge *X*→*Y*. Further, you believe the causal
    relationship between *X* and *Y* is naturally linear. You can’t “see” linearity
    on the DAG structure, so linearity is a level 3 assumption.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点的一种方式是，任何你无法在DAG结构中表示的因果假设，通过排除法，就是一个第三层假设。例如，假设你的DAG有边 *X*→*Y*。进一步地，你相信
    *X* 和 *Y* 之间的因果关系是自然线性的。你无法在DAG结构上“看到”线性，因此线性是一个第三层假设。
- en: 10.1.3 Where data falls on the hierarchy
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.3 数据在层次结构中的位置
- en: Recall the differences between observational data and interventional data. Observational
    data is passively observed; as a result, it captures statistical associations
    resulting from dependence between variables in the DGP.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾观察数据和介入数据之间的差异。观察数据是被被动观察的；因此，它捕捉了DGP中变量之间依赖关系产生的统计关联。
- en: Level 1 data
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Level 1 数据
- en: In our online gaming example, the level 1 data was logged examples of side-quest
    engagement and in-game purchases pulled by a database query. Observational data
    lives at level 1 of the causal hierarchy.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的在线游戏示例中，第一层数据是数据库查询拉取的侧任务参与和游戏内购买日志示例。观察数据位于因果层次结构的第一层。
- en: Level 2 data
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Level 2 数据
- en: Interventional data is generated as the result of applying an intervention,
    such as data collected from a randomized experiment. In the gaming example, this
    was the data created because of an A/B test that randomly assigned players to
    different groups where they are coerced into different fixed side-quest engagement
    levels. Intervention data lives at level 2 of the hierarchy.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 介入数据是在应用介入措施的结果下生成的，例如从随机实验中收集的数据。在游戏示例中，这是由于A/B测试随机分配玩家到不同的组，他们被强制参与不同固定侧任务参与水平所创建的数据。介入数据位于层次结构的第二层。
- en: Level 3 data
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Level 3 数据
- en: Counterfactual data, which lives at level 3 of the hierarchy, is the odd case.
    Counterfactual data would contain data from across possible worlds. In most domains,
    we only have data from one world—one *potential outcome* for each unit of observation
    in the data.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 反事实数据，位于层次结构的第 3 级，是一个特殊情况。反事实数据将包含来自可能世界的跨世界的数据。在大多数领域，我们只有一个世界的数据——数据中每个观察单位的*潜在结果*。
- en: However, there are special cases where counterfactual data exists. For example,
    cloud service providers use complex but deterministic policies for allocating
    resources in the cloud, given various constraints. For one example with a given
    allocation outcome in the log, we could generate a counterfactual outcome for
    that example by applying a different allocation policy to that example. Similarly,
    given data produced by simulation software, we could generate counterfactual data
    by changing the simulation to reflect a *hypothetical condition* and then rerunning
    it with the same initial conditions as the original data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，存在一些特殊情况，其中存在反事实数据。例如，云服务提供商在给定各种约束的情况下，使用复杂但确定性的策略在云中分配资源。以日志中给定的一个分配结果为例，我们可以通过对该示例应用不同的分配策略来生成该示例的反事实结果。同样，给定由仿真软件产生的数据，我们可以通过改变仿真以反映一个*假设条件*并使用与原始数据相同的初始条件重新运行它来生成反事实数据。
- en: 10.1.4 The causal hierarchy theorem
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.4 因果层次定理
- en: 'The causal hierarchy offers us a key insight from something called the *causal
    hierarchy theorem*. That insight is this: “You cannot answer a level *k* question
    without level *k* assumptions.” For example, if you want a causal effect, you
    need a DAG or some other level 2 (or level 3) assumptions. If you want to answer
    a counterfactual question, you need level 3 assumptions. And even the most cutting-edge
    of deep learning models can’t answer level *k* questions reliably unless they
    encode a representation of level *k* assumptions.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 因果层次为我们提供了来自称为*因果层次定理*的东西的关键见解。这个见解是这样的：“没有第 *k* 级别的假设，你不能回答第 *k* 级别的问题。”例如，如果你想得到因果效应，你需要
    DAG 或其他第 2 级（或第 3 级）假设。如果你想回答反事实问题，你需要第 3 级假设。即使是最前沿的深度学习模型，如果不能编码第 *k* 级别的假设，也无法可靠地回答第
    *k* 级别的问题。
- en: More formally, the causal hierarchy theorem establishes that the three layers
    of the causal hierarchy are, in mathematical jargon, “almost always separate.”
    Roughly speaking, “separate” means that data from a lower level of the hierarchy
    is insufficient to infer a query from a higher level of the hierarchy. And “almost
    always” means this statement is true except in cases so rare that we can dismiss
    them as practically unimportant.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，因果层次定理确立了因果层次的三层在数学术语中是“几乎总是分开的”。粗略地说，“分开”意味着来自层次较低级别的数据不足以从较高级别的层次推断查询。而“几乎总是”意味着这个陈述是正确的，除非在极其罕见的情况下，我们可以将其视为实际上并不重要。
- en: Aside from this insight, the causal hierarchy makes understanding identification—perhaps
    the hardest topic in all causal inference—much easier, as we’ll see in the rest
    of the chapter.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这个见解之外，因果层次使得理解识别——可能是所有因果推理中最难的主题——变得容易得多，正如我们在本章的其余部分将看到的那样。
- en: 10.2 Identification and the causal inference workflow
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2 识别和因果推理流程
- en: In this section, we’ll look at the workflow for posing and answering causal
    questions and the role that identification plays in that workflow. We’ll use the
    online gaming DAG introduced in chapter 7 as an example. Let’s start by building
    the DAG with y0.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨提出和回答因果问题的流程以及识别在该流程中的作用。我们将使用第 7 章中引入的在线游戏 DAG 作为例子。让我们首先使用 y0 构建DAG。
- en: Listing 10.2 Building the online gaming DAG in y0
  id: totrans-67
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.2 在 y0 中构建在线游戏 DAG
- en: '[PRE1]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 Install Graphviz for DAG visualization. Download some helper functions for
    identification and visualization that convert some y0 abstractions into abstractions
    we’re familiar with.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 安装 Graphviz 以进行 DAG 可视化。下载一些识别和可视化的辅助函数，将一些 y0 抽象转换为我们所熟悉的抽象。'
- en: '#2 Inspect the downloaded code before executing as a matter of good security
    practice. Then uncomment the last line and execute.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 在执行之前检查下载的代码，这是一项良好的安全实践。然后取消注释最后一行并执行。'
- en: '#3 y0 works with a custom graph class called NxMixedGraph. To avoid confusion,
    we’ll call it a Y0Graph and use it to implement DAGs.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 y0 与一个名为 NxMixedGraph 的自定义图类一起工作。为了避免混淆，我们将称之为 Y0Graph 并用它来实现 DAG。'
- en: '#4 Build the graph.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 构建图形。'
- en: '#5 Draw the graph with a Graphviz helper function.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 使用 Graphviz 辅助函数绘制图形。'
- en: This produces the graph in figure 10.3.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了图 10.3 中的图形。
- en: '![figure](../Images/CH10_F03_Ness.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH10_F03_Ness.png)'
- en: Figure 10.3 Drawing the online gaming graph with y0
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 10.3 绘制带有 y0 的在线游戏图
- en: Our goal in chapter 7 was to use our model of *P*(*G*, *E*, *I*) to simulate
    from *P*(*I*[*E*][=“high”]) using the intervention operator. In sections 7.1.6
    and 7.2.6, we did this simulation and saw empirical evidence that it works for
    this online game example. Identification means showing that it works in general,
    based on your model and assumptions. Formally, we want to be sure that level 1
    distribution *P*(*G*, *E*, *I*), or data from that distribution, combined with
    our DAG,
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 7 章中，我们的目标是使用我们的 *P*(*G*, *E*, *I*) 模型通过干预操作从 *P*(*I*[*E*][=“high”]) 中进行模拟。在第
    7.1.6 和 7.2.6 节中，我们进行了这种模拟，并看到了对于这个在线游戏示例的实证证据，证明它是有效的。识别意味着基于你的模型和假设，证明它在一般情况下是有效的。正式来说，我们想确保第一层分布
    *P*(*G*, *E*, *I*)，或者来自该分布的数据，结合我们的 DAG，
- en: is enough to simulate from level 2 distribution *P*(*I*[*E*][=“high”]). Identification
    with y0 confirms that this is indeed possible.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 就足以从第二层分布 *P*(*I*[*E*][=“high”]) 中进行模拟。y0 的识别确认这确实可能。
- en: Listing 10.3 Checking identification of *P*(*I**[E]*[=“high”])from *P*(*G*,
    *E*,*I*)
  id: totrans-79
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.3 检查 *P*(*I**[E]*[=“high”]) 从 *P*(*G*, *E*,*I*) 的识别
- en: '[PRE2]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#1 Make a lowercase “e” to represent an intervention value.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 用小写“e”表示干预值。'
- en: '#2 Check identifiability given the DAG, a distribution, and a target query.
    Y0 represents ideal interventions with @, so we write P(I [E=e] as P(I @ e).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 检查给定 DAG、分布和目标查询的可识别性。Y0 代表理想的干预措施，所以我们写作 P(I [E=e] 作为 P(I @ e)。'
- en: This will return `True`, but what if we didn’t have any observations of guild
    membership *G*? We can use y0 to test if we have identification for *P*(*I*[*E*][=“high”])
    from *P*(*E*, *I*). In other words, test if it is possible to infer *P*(*I*[*E*][=“high”])
    from observations of *E* and *I* only.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回`True`，但如果我们没有任何关于行会成员 *G* 的观察呢？我们可以使用 y0 来测试我们是否从 *P*(*E*, *I*) 中获得了对 *P*(*I*[*E*][=“high”])
    的识别。换句话说，测试是否可以从对 *E* 和 *I* 的观察中推断出 *P*(*I*[*E*][=“high”])。
- en: Listing 10.4 Checking identification of *P*(*I**[E]*[=“high”])from *P*(*E*,*I*)
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.4 检查 *P*(*I**[E]*[=“high”]) 从 *P*(*E*,*I*) 的识别
- en: '[PRE3]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This will return `False`, because we don’t have identification for *P*(*I*[*E*][=][*e*])
    from the DAG and *P*(*E*, *I*) given our graphical assumptions.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回`False`，因为我们没有从 DAG 和基于我们的图形假设的 *P*(*E*, *I*) 中对 *P*(*I*[*E*][=][*e*]) 进行识别。
- en: Lack of identification and misguided probabilistic ML
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 识别不足和误导性的概率机器学习
- en: Y0 shows us that *P*(*I**[E]*[=]*[e]*) is not identified from *P*(*E*, *I*)
    given our online game DAG. Consider the implications of this result from the perspective
    of probabilistic machine learning (ML). As experts in probabilistic ML, given
    *G* is unmeasured, we might be inclined to train a latent variable model on *P*(*E*,
    *I*) where *G* is the latent variable. Once we’ve learned that model, we could
    implement the intervention with graph surgery setting *E*=*e*, and then sampling
    *I* from the transformed model.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Y0 显示，根据我们的在线游戏 DAG，从 *P*(*E*, *I*) 中无法识别 *P*(*I**[E]*[=]*[e]*)。从概率机器学习 (ML)
    的角度来看，这个结果有什么含义。作为概率机器学习的专家，如果 *G* 是未测量的，我们可能会倾向于在 *P*(*E*, *I*) 上训练一个潜在变量模型，其中
    *G* 是潜在变量。一旦我们学习了该模型，我们就可以通过图手术设置 *E*=*e* 来实施干预，然后从转换后的模型中采样 *I*。
- en: This algorithm would *run*; it would generate samples. But the lack of identification
    result from y0 proves that, given only the assumptions in our DAG, we could not
    consider these to be valid samples from *P*(*I**[E]*[=]*[e]*). And training on
    more data wouldn’t help. The only way this could work is if there were additional
    causal assumptions constraining inference beyond the assumptions encoded by the
    DAG.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法将*运行*；它会生成样本。但 y0 的识别不足结果证明，仅基于我们 DAG 中的假设，我们不能将这些样本视为来自 *P*(*I**[E]*[=]*[e]*)
    的有效样本。并且更多的数据训练也不会有帮助。这个算法能够工作的唯一方式是，如果存在额外的因果假设，这些假设限制了推断，超出了 DAG 编码的假设。
- en: Given this introduction, let’s define identification.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个介绍的基础上，让我们定义识别。
- en: 10.2.1 Defining identification
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.1 定义识别
- en: Suppose I were to randomly choose a pair of numbers, *X* and *Y*, and add them
    together to get *Z*. Then, I tell you what *Z* was and ask you to infer the values
    of *X* and *Y*. Could you do it? Not without more information. So, what if I gave
    you millions of examples of feature *Z* and label {*X*, *Y*}. Could you train
    a deep learning model to predict label {*X*, *Y*} from input feature *Z*? Again,
    no, at least not without strong assumptions on the possible values of {*X*, *Y*}.
    What if, instead of millions, I gave you billions of examples? No; more data would
    not help. In statistics, we would say the prediction target {*X*, *Y*} is not
    *identified*.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我随机选择一对数字，*X* 和 *Y*，并将它们相加得到 *Z*。然后，我告诉你 *Z* 的值，并要求你推断 *X* 和 *Y* 的值。你能做到吗？没有更多信息的话，不能。那么，如果我给你数百万个特征
    *Z* 和标签 {*X*，*Y*} 的例子呢？你能训练一个深度学习模型从输入特征 *Z* 预测标签 {*X*，*Y*} 吗？再次，不能，至少在没有对可能值有强假设的情况下不能。如果我只给你十亿个例子呢？不；更多的数据不会有帮助。在统计学中，我们会说预测目标
    {*X*，*Y*} 是没有*识别*的。
- en: In other words, you want to infer something, and you have an algorithm (e.g.,
    a deep net) that takes in data and produces an answer. That answer will usually
    be a bit different than the true value because of statistical variation in the
    input data. If your inference objective is identified, then the more data you
    input to the algorithm, the more that variance will shrink and your algorithm’s
    answer will converge to the true answer. If your inference objective is not identified,
    then more data will not reduce your algorithm’s errors.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，你想要推断某事，并且你有一个算法（例如，深度网络）可以接收数据并生成答案。由于输入数据的统计变化，这个答案通常会与真实值略有不同。如果你的推断目标是确定的，那么你输入到算法中的数据越多，这种差异就会越小，你的算法答案就会收敛到真实答案。如果你的推断目标没有确定，那么更多的数据不会减少你的算法错误。
- en: '*Causal identification* is just statistical identification across levels of
    the causal hierarchy. A causal query is identified when your causal assumptions
    enable you to infer that query using data from a lower level on the hierarchy.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*因果识别*只是因果层次结构各层之间的统计识别。当你的因果假设使你能够使用层次结构较低级别的数据来推断该查询时，因果查询就被识别了。'
- en: 10.2.2 The causal inference workflow
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.2 因果推断工作流程
- en: Now that we have defined identification, we can define a full workflow for causal
    inference. Figure 10.4 shows the full workflow.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了识别，我们可以定义因果推断的完整工作流程。图10.4显示了完整的工作流程。
- en: '![figure](../Images/CH10_F04_Ness.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH10_F04_Ness.png)'
- en: Figure 10.4 The causal inference workflow. The identification step is an essential
    step in the workflow.
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.4 因果推断工作流程。识别步骤是工作流程中的关键步骤。
- en: Identification is a key step in the workflow. Let’s walk through each of the
    steps.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 识别是工作流程中的关键步骤。让我们逐一走过每个步骤。
- en: 'Step 1: Pose your query'
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第一步：提出你的查询
- en: First, we pose our causal question as a query. For example, given our question
    “What would in-game purchases be for a player if side-quest engagement was high?”
    our query is *P*(*I*[*E*][=“high”]).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将我们的因果问题作为一个查询提出。例如，给定我们的问题“如果副任务参与度很高，玩家在游戏中的购买会是什么？”我们的查询是 *P*(*I*[*E*][=“high”])。
- en: '![figure](../Images/CH10_F05_Ness.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH10_F05_Ness.png)'
- en: 'Figure 10.5 Step 2: Build the model to capture your causal assumptions relative
    to your query. For the query *P*(*I**[E]*[=“high”]), this is our online gaming
    DAG.'
  id: totrans-103
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.5 第二步：构建模型以捕捉相对于你的查询的因果假设。对于查询 *P*(*I**[E]*[=“high”])，这是我们在线游戏的DAG。
- en: 'Step 2: Build your model'
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第二步：构建你的模型
- en: Next, build a causal model that captures your basic causal assumptions. Our
    model will be the online game causal DAG, shown again in figure 10.5.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，构建一个因果模型，该模型捕捉你的基本因果假设。我们的模型将是在线游戏的因果DAG，如图10.5所示。
- en: Your model’s assumptions should at least match the level of your query in the
    causal hierarchy. For example, the query *P*(*I*[*E*][=“high”]) is a level 2 query,
    so we need at least some level 2 assumptions. The causal DAG is a level 2 causal
    model, so in our analysis, the DAG provides the necessary level 2 assumptions.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 你的模型假设至少应该与你在因果层次结构中的查询级别相匹配。例如，查询 *P*(*I*[*E*][=“high”]) 是一个2级查询，因此我们需要至少一些2级假设。因果DAG是一个2级因果模型，因此在我们的分析中，DAG提供了必要的2级假设。
- en: 'Step 3: Check identification'
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第三步：检查识别
- en: Evaluate whether you have identification for your query, given your model assumptions
    and your available data. If you don’t have identification, you must either observe
    additional variables in your data or change your assumptions. For example, we
    could modify our online gaming DAG (changing level 2 assumptions). Or simply stop
    and conclude you can’t answer the question given your data and knowledge about
    the problem, and devote your attention elsewhere.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 评估你是否在你的查询中具有识别，基于你的模型假设和可用数据。如果你没有识别，你必须要么在你的数据中观察额外的变量，要么改变你的假设。例如，我们可以修改我们的在线游戏DAG（改变二级假设）。或者简单地停止并得出结论，根据你的数据和关于问题的知识，你不能回答这个问题，并将你的注意力转移到其他地方。
- en: 'Step 4: Estimate your query'
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第4步：估计你的查询
- en: Once you know you have identification for your query, you can run statistical
    inference on, or “estimate,” your query. There are a variety of estimation methods
    and algorithms, from Bayesian inference to linear regression to propensity scores
    to double machine learning. We’ll review some estimation methods in the next chapter.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你知道你的查询具有识别，你就可以对你的查询进行统计推断，或“估计”。有各种各样的估计方法和算法，从贝叶斯推断到线性回归、倾向得分到双重机器学习。我们将在下一章中回顾一些估计方法。
- en: 'Step 5: Refute your causal inference'
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第5步：反驳你的因果推断
- en: Refutation is a final step where we conduct sensitivity analysis to evaluate
    how sensitive our results from step 4 are to violations of our assumptions, including
    the assumptions that enabled identification. We’ll see examples of this in chapter
    11\.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 反驳是最终一步，我们在这里进行敏感性分析，以评估第4步的结果对假设违反的敏感性，包括使识别成为可能的假设。我们将在第11章中看到这方面的例子。
- en: 10.2.3 Separating identification and estimation
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.3 分离识别和估计
- en: 'In many texts, identification and estimation are combined in one step by matching
    the estimators and practical scenarios where those estimators will work. In this
    book, we’ll highlight the separation of identification and estimation for several
    reasons:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多文本中，识别和估计在一步中结合，通过匹配估计量和那些估计量将有效的工作场景。在这本书中，我们将强调识别和估计的分离，原因有以下几点：
- en: The separation lets us shunt all the causal considerations into the identification
    step. This helps us be explicit about what causal assumptions we are relying on
    for estimation to work and builds intuition for when our analysis might fail.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分离使我们能够将所有因果考虑因素都转移到识别步骤。这有助于我们明确我们依赖哪些因果假设来进行估计，并为我们的分析可能失败的情况建立直觉。
- en: The estimation step thus simplifies to purely statistical questions, where we
    consider the usual statistical trade-offs (bias vs. variance, uncertainty quantification,
    how well it scales, etc.).
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，估计步骤简化为纯粹统计问题，其中我们考虑通常的统计权衡（偏差与方差、不确定性量化、扩展性等）。
- en: The separation also allows us to handle estimation with the automatic differentiation
    capabilities that power cutting-edge deep learning libraries without worrying
    whether these learning procedures will get the causality wrong.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种分离也使我们能够利用自动微分能力来处理估计，这些能力是前沿深度学习库的核心，而无需担心这些学习过程是否会错误地得出因果关系。
- en: 'Next, we’ll dive into the most common identification strategy: backdoor adjustment.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入探讨最常见的识别策略：后门调整。
- en: 10.3 Identification with backdoor adjustment
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.3 使用后门调整进行识别
- en: 'Suppose we want to determine the causal effect of engagement on in-game purchases,
    i.e., *E*(*I*[*E*][=][“high”] – *I*[*E*][=][“low”]). We can derive this expectation
    from the query *E*(*I*[*E*][=][*e*]=*i*), so we focus on *P*(*I*[*E*][=][*e*]=*i*).
    We can use the online gaming DAG to prove the following is true:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要确定参与对游戏内购买的影响，即*E*(*I*[*E*][=][“high”] – *I*[*E*][=][“low”])。我们可以从这个查询*E*(*I*[*E*][=][*e*]=*i*)中推导出这个期望，所以我们关注*P*(*I*[*E*][=][*e*]=*i*)。我们可以使用在线游戏DAG来证明以下是真的：
- en: We’ll see how to derive this equation in the next section. The right side of
    this equation is a level 1 quantity called an *estimand* that we can derive from
    the joint distribution *P*(*I*, *E*, *G*).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节中看到如何推导出这个方程。该方程的右侧是一个称为*估计量*的一级量，我们可以从联合分布*P*(*I*, *E*, *G*)中推导出来。
- en: '![figure](../Images/ness-ch10-eqs-0x.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch10-eqs-0x.png)'
- en: Queries, estimands, and estimators
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 查询、估计量和估计者
- en: In statistics, the *estimand* is the thing the statistical algorithm (the *estimator*)
    estimates. The task of identification is finding (identifying) an estimand for
    your query. In terms of the causal hierarchy, causal identification is about finding
    a lower-level estimand for a higher-level query.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计学中，*估计量*是统计算法（*估计器*）估计的东西。识别的任务是找到（识别）查询的估计量。从因果层次结构的角度来看，因果识别是关于找到一个较低级别的估计量来满足较高级别的查询。
- en: In the online gaming backdoor identification example, *P*(*I*[E=“high”]=i) is
    a level 2 query, and ∑*[g]**P*(*I*=*i*|*E*=“high”, *G*=*g*)*P*(*G*=*g*) is the
    level 1 estimand called the *backdoor adjustment estimand*. Backdoor adjustment
    is an operation we apply to *P*(*E*, *I*, *G*), where we sum out (or integrate
    out in the continuous case) the common cause *G*. In some cases, we’ll see we
    don’t need to know the estimand explicitly, only that it exists.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在在线游戏后门识别示例中，*P*(*I*[E=“high”]=i) 是一个二级查询，而 ∑*[g]**P*(*I*=*i*|*E*=“high”, *G*=*g*)*P*(*G*=*g*)
    是一级估计量，称为*后门调整估计量*。后门调整是我们应用于 *P*(*E*, *I*, *G*) 的一个操作，其中我们消去（或在连续情况下积分出）共同原因
    *G*。在某些情况下，我们会看到我们不需要明确知道估计量，只需要知道它存在。
- en: We passed our DAG and the intervention-level query *P*(*I*[*E*][=][“high”])
    to y0, and it told us it identified an estimand, an operation applied to *P*(*E*,
    *I*, *G*) that is equivalent to *P*(*I*[*E*][=][“high”]). Let’s have y0 display
    that estimand.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的 DAG 和干预级别的查询 *P*(*I*[*E*][=][“high”]) 传递给 y0，它告诉我们它识别了一个估计量，这是一个应用于 *P*(*E*,
    *I*, *G*) 的操作，与 *P*(*I*[*E*][=][“high”]) 等价。让我们让 y0 显示这个估计量。
- en: Listing 10.5 Deriving the estimand to get *P*(*I**[E]*[=“high”]) from *P*(*E*,
    *I*, *G*)
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.5 从 *P*(*E*, *I*, *G*) 推导出估计量以获得 *P*(*I**[E]*[=“high”])
- en: '[PRE4]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This returns the expression in figure 10.6.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回了图 10.6 中的表达式。
- en: '![figure](../Images/ness-ch10-eqs-1x.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch10-eqs-1x.png)'
- en: Figure 10.6 Output of y0’s identify function
  id: totrans-131
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 10.6 y0 识别函数的输出
- en: In our notation, this is ∑[*g*]*P*(*I*=*i*|*E*=“high”, *G*=*g*) ∑*[ε]*[,]*[i]**P*(*E*=*ε*,
    *G*=*g*, *I*=*i*), which simplifies to ∑[*g*]*P*(*I*=*i*|*E*=“high”, *G*=*g*)
    *P*(*G*=*g*). This is the *backdoor adjustment estimand*. We’ll see at a high
    level how y0 derives this estimand. But first, let’s look a bit more closely at
    this estimand.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的记号中，这表示为 ∑[*g*]*P*(*I*=*i*|*E*=“high”, *G*=*g*) ∑*[ε]*[,]*[i]**P*(*E*=*ε*,
    *G*=*g*, *I*=*i*), 这可以简化为 ∑[*g*]*P*(*I*=*i*|*E*=“high”, *G*=*g*) *P*(*G*=*g*).
    这就是*后门调整估计量*。我们将从高层次看到 y0 如何推导出这个估计量。但首先，让我们更仔细地看看这个估计量。
- en: 10.3.1 The backdoor adjustment formula
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.1 后门调整公式
- en: 'In general terms, suppose *X* is a cause of *Y*, and we are interested in the
    intervention-level query *P*(*Y*[*X*][=][*x*]). In that case, the *backdoor adjustment
    estimand* is ∑*[g]**P*(*X*=*x*, *Z*=*z*) *P*(*Z*=*z*). The *backdoor adjustment
    formula* equates the causal query *P*(*X**[X]*[=]*[x]*) with its estimand:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，假设 *X* 是 *Y* 的原因，并且我们对干预级别的查询 *P*(*Y*[*X*][=][*x*]) 感兴趣。在这种情况下，*后门调整估计量*是
    ∑*[g]**P*(*X*=*x*, *Z*=*z*) *P*(*Z*=*z*)。*后门调整公式*将因果查询 *P*(*X**[X]*[=]*[x]*) 与其估计量等同起来：
- en: '![figure](../Images/ness-ch10-eqs-12x.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch10-eqs-12x.png)'
- en: Here, *Z* is a set of variables called the adjustment set. The summation is
    shorthand for summation and integration—you sum over discrete variables in the
    adjustment set and integrate over continuous variables. The adjustment set is
    defined as fa set of variables that satisfies the *backdoor criterion*—(1) the
    set collectively *d*-separates all *backdoor paths* from *X* to *Y*, and (2) it
    contains no descendants of *X*.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*Z* 是一组被称为调整集的变量。求和是求和和积分的简写——你在调整集中的离散变量上求和，在连续变量上积分。调整集被定义为满足*后门准则*的一组变量——（1）该集共同*分离*了从
    *X* 到 *Y* 的所有*后门路径*，并且（2）它不包含 *X* 的后代。
- en: To understand why we want to d-separate backdoor paths between *X* and *Y*,
    consider again our DAG for our online gaming example in figure 10.7.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解为什么我们要分离 *X* 和 *Y* 之间的后门路径，再次考虑我们的在线游戏示例图 10.7 中的 DAG。
- en: '![figure](../Images/CH10_F07_Ness.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH10_F07_Ness.png)'
- en: Figure 10.7 The online gaming DAG
  id: totrans-139
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 10.7 在线游戏 DAG
- en: What is the difference between *P*(*I*|*E*=“high”) and *P*(*I*[*E*][=“high”])?
    Consider the two paths between *E* and *I* in figure 10.8\. In the case of *P*(*I*|*E*=“high”),
    observing *E*=“high” gives us information about *I* by way of its direct causal
    impact on *I*, i.e., through path *E*→*I*. But observing *E*=“high” also gives
    us information about *G*, and subsequently about *I* through the *backdoor path*
    *E*←*G*→*I*. A *backdoor path* between two variables is a *d*-connected path between
    a common cause. In the case of *P*(*I**[E]*[=“high”]), we only want the impact
    on *I* through the direct path *E*→*I*.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*I*|*E*=“high”)和*P*(*I*[*E*][=“high”])之间的区别是什么？考虑图10.8中*E*和*I*之间的两条路径。在*P*(*I*|*E*=“high”)的情况下，观察到*E*=“high”通过其对*I*的直接因果影响为我们提供了关于*I*的信息，即通过路径*E*→*I*。但观察到*E*=“high”也为我们提供了关于*G*的信息，随后通过后门路径*E*←*G*→*I*关于*I*的信息。两个变量之间的后门路径是共同原因之间的*d*-连接路径。在*P*(*I**[E]*[=“high”])的情况下，我们只想通过直接路径*E*→*I*对*I*产生影响。'
- en: '![figure](../Images/CH10_F08_Ness.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH10_F08_Ness.png)'
- en: Figure 10.8 *E*←*G*→*I* is a backdoor path where *G* is a “confounder” that
    is a common cause of *E* and *I*. We are interested in the statistical signal
    flowing along the causal path from *E* to *I*, but that signal is “confounded”
    by the noncausal noise from additional statistical information through *G* on
    the backdoor path *E*←*G*→*I*.
  id: totrans-142
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.8 *E*←*G*→*I*是一个后门路径，其中*G*是*E*和*I*的共同原因的“混杂因素”。我们感兴趣的是沿着从*E*到*I*的因果路径流动的统计信号，但这个信号被后门路径*E*←*G*→*I*上的额外统计信息的非因果噪声所“混杂”。
- en: We call *G* a *confounder*, because the statistical “signal” flowing along the
    causal path from *E* to *I* is “confounded” by the noncausal “noise” from additional
    statistical information through *G* on the *backdoor path* *E*←*G*→*I*. To address
    this problem, we seek to d-separate this backdoor path by blocking on *G*.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称*G*为*混杂因素*，因为沿着从*E*到*I*的因果路径流动的统计“信号”被来自*G*的额外统计信息的非因果“噪声”所“混杂”，这些信息通过后门路径*E*←*G*→*I*传递。为了解决这个问题，我们试图通过在*G*上阻断来d-分离这条后门路径。
- en: 'We want to identify a backdoor estimand for the query *P*(*I*[*E*][=“high”]).
    So we substitute *I* for *Y*, and *E* for *X* in the backdoor adjustment formula.
    *G* blocks the backdoor path *E* *G* *I*, so the set *G* becomes our adjustment
    set:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要识别查询*P*(*I*[*E*][=“high”])的后门估计量。因此，我们在后门调整公式中将*I*替换为*Y*，将*E*替换为*X*。*G*阻断后门路径*E*
    *G* *I*，因此集合*G*成为我们的调整集：
- en: '*P*(*I*[*E*][=][*e*]=*i*) = ∑[*g*]*P*(*I*=*i*|*E*=*e*, *G*=*g*)*P*(*G*=*g*)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*I*[*E*][=][*e*]=*i*) = ∑[*g*]*P*(*I*=*i*|*E*=*e*, *G*=*g*)*P*(*G*=*g*)'
- en: The backdoor adjustment formula d-separates the backdoor paths by summing out/integrating
    over, or in other words, “adjusting for” the backdoor statistical signal, leaving
    only the signal derived from the direct causal relationship.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 后门调整公式通过求和/积分或换句话说，“调整”后门统计信号来d-分离后门路径，只留下来自直接因果关系的信号。
- en: NOTE  Some texts refer to the G-formula instead of backdoor adjustment formula.
    The backdoor adjustment formula is just the G-formula where the adjustment set
    is defined in terms of the backdoor criterion.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：一些文献中提到的是G公式而不是后门调整公式。后门调整公式就是G公式，其中调整集是以后门标准来定义的。
- en: While an adjustment set can include non-confounders, in practice, excluding
    all but a minimal set of backdoor-blocking confounders cuts down on complexity
    and statistical variation. We dive into the statistical considerations of backdoor
    adjustment in chapter 11.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然调整集可以包括非混杂因素，但在实践中，排除除最小化后门阻断混杂因素之外的所有因素可以减少复杂性和统计变异。我们在第11章深入探讨了后门调整的统计考虑因素。
- en: 10.3.2 Demystifying the back door
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.2 揭秘后门
- en: So where does the backdoor adjustment estimand come from? Let’s consider our
    online gaming example again. The query is *P*(*I*[*E*][=][*e*]) where *e* is “high”
    or “low.” In counterfactual terms, let’s consider two possible worlds, one with
    our original DAG, and one where we apply the intervention to side-quest engagement
    (*E*). Let’s view the parallel world graph in figure 10.9.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，后门调整估计量从何而来？让我们再次考虑我们的在线游戏示例。查询是*P*(*I*[*E*][=][*e*])，其中*e*是“高”或“低”。在反事实的术语中，让我们考虑两个可能的世界，一个是我们原始的DAG，另一个是我们对副任务参与度(*E*)进行干预。让我们查看图10.9中的平行世界图。
- en: '![figure](../Images/CH10_F09_Ness.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH10_F09_Ness.png)'
- en: 'Figure 10.9 We have two parallel worlds: world A where *E* is not intervened
    upon, and world B where *E* is intervened upon.'
  id: totrans-152
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.9 我们有两个平行世界：世界A中*E*没有受到干预，世界B中*E*受到了干预。
- en: If you squint hard enough at this graph, you’ll notice that it implies that
    *E* is conditionally independent from *I*[*E*][=][*e*] given *G*. We’ll use some
    d-separation–based reasoning to see this. Remember that, in general, we can’t
    use d-separation to reason across worlds on a parallel world graph because the
    d-separation rules don’t account for nodes that are equivalent across worlds (like
    *G*). But we’ll use a trick where we reason about conditional independence between
    *E* and *I*[*E*][=][*e*] by looking at a d-connected path from *E* to *G* in world
    A, and then extend that d-connected path *from* the equivalent *G* in world B
    to *I*[*E*][=][*e*].
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你足够用力地眯着眼睛看这张图，你会注意到它暗示了在给定 *G* 的条件下，*E* 是与 *I*[*E*][=][*e*] 条件独立的。我们将使用一些基于
    d-separation 的推理来观察这一点。记住，在一般情况下，我们不能在平行世界图上使用 d-separation 来进行跨世界的推理，因为 d-separation
    规则没有考虑到在各个世界之间等效的节点（如 *G*）。但我们将使用一个技巧，通过观察世界 A 中从 *E* 到 *G* 的 d-connected 路径，然后扩展这个
    d-connected 路径 *从* 世界 B 中等效的 *G* 到 *I*[*E*][=][*e*]。
- en: First, consider that paths from *E* in world A to world B have to cross one
    of two bridges between worlds, *N*[*G*] and *N*[*I*]. But the two paths to *N*[*I*]
    (*E* → *I* ← *N*[*I*], *E* ← *G* → *I* ← *N*[*I*]) are both d-separated due to
    the collider on *I*.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，考虑从世界 A 中的 *E* 到世界 B 的路径必须穿过两个世界之间的桥梁之一，*N*[*G*] 和 *N*[*I*]。但是，由于 *I* 上的碰撞器，到
    *N*[*I*] 的两条路径（*E* → *I* ← *N*[*I*]，*E* ← *G* → *I* ← *N*[*I*]）都是 d-separated
    的。
- en: So we have one d-connected path to world B (*E* ← *G* ← *N*[*G*]). Now suppose
    we look at *G* in world B; from world B’s *G*, it is one step to *I*[*E*][=][*e*].
    But we know that, by the law of consistency, the value of *G* in both worlds must
    be the same; both *G*s are the same deterministic function of *N*[*G*], and neither
    *G* is affected by an intervention. So, for convenience, we’ll collapse the two
    *G*s into one node in the parallel world graph (figure 10.10). Looking now at
    the path *E* ← *G* → *I*[*E*][=][*e*], we can see this path is d-separated by
    *G*. Hence, we can conclude *E* ⊥ *I*[*E*][=][*e*] | *G*.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有一条到世界 B 的 d-connected 路径（*E* ← *G* ← *N*[*G*]）。现在假设我们观察世界 B 中的 *G*；从世界
    B 的 *G* 出发，到 *I*[*E*][=][*e*] 只是一步之遥。但我们知道，根据一致性法则，两个世界中的 *G* 的值必须相同；两个 *G* 都是
    *N*[*G*] 的相同确定性函数，并且两个 *G* 都不受干预的影响。因此，为了方便起见，我们将两个 *G* 在平行世界图中折叠成一个节点（图 10.10）。现在观察路径
    *E* ← *G* → *I*[*E*][=][*e*]，我们可以看到这条路径被 *G* d-separated。因此，我们可以得出结论 *E* ⊥ *I*[*E*][=][*e*]
    | *G*。
- en: '![figure](../Images/CH10_F10_Ness.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH10_F10_Ness.png)'
- en: Figure 10.10 Collapsing *G* across worlds reveals *G* d-separates *E* and *I**[E]*[=]*[e]*.
  id: totrans-157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 10.10 在各个世界之间折叠 *G* 揭示了 *G* d-separates *E* 和 *I**[E]*[=]*[e*]。
- en: In causal inference jargon, this simplification is called *ignorability*. *Ignorability*
    means the causal variable *E* and the counterfactual potential outcomes like *I*[*E*][=][*e*]
    are conditionally independent given confounders. Ignorability is a common assumption
    made in causal inference. We can use this ignorability assumption in deriving
    the backdoor estimand.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在因果推理术语中，这种简化被称为 *不可知性*。*不可知性*意味着因果变量 *E* 和像 *I*[*E*][=][*e*] 这样的反事实潜在结果在混杂因素给定的情况下是条件独立的。不可知性是因果推理中常见的假设。我们可以在推导后门估计量时使用这个不可知性假设。
- en: 'Before we start, let’s recall a key definitional fact of conditional independence:
    if two variables *U* and *V* are conditionally independent given *Z*, then *P*(*U*|*Z*=*z*,
    *V*=*v*) = *P*(*U*|*Z*=*z*). Flipping that around, *P*(*U*|*Z*=*z*) = *P*(*U*|*Z*=*z*,
    *V*=*v*). In other words, *P*(*U*|*Z*=*z*) = *P*(*U*|*Z*=*z*, *V*=“apples”) =
    *P*(*U*|*Z*=*z*, *V*=“oranges”); it doesn’t matter what value *V* takes because,
    since *Z* rendered it independent from *U*, its value has no bearing on *U*. Introducing
    *V* and giving it whatever value we want is the trick that makes the derivation
    work. Also, recall the *law of total probability* says that we can marginalize
    a variable out of a joint distribution by summing (or integrating) over that variable,
    as in *P*(*U*=*u*) = ∑[*v*]*P*(*U*=*u*, *V*=*v*). The same is true when the joint
    distribution is subject to intervention, as in *P*(*U**[W]*[=]*[w]*=*u*) = ∑[*v*]*P*(*U**[W]*[=]*[w]*=*u*,
    *V**[W]*[=]*[w]*=*v*).'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，让我们回顾一个关于条件独立的关键定义事实：如果两个变量 *U* 和 *V* 在给定 *Z* 的条件下是条件独立的，那么 *P*(*U*|*Z*=*z*,
    *V*=*v*) = *P*(*U*|*Z*=*z*)。反过来，*P*(*U*|*Z*=*z*) = *P*(*U*|*Z*=*z*, *V*=*v*)。换句话说，*P*(*U*|*Z*=*z*)
    = *P*(*U*|*Z*=*z*, *V*=“apples”) = *P*(*U*|*Z*=*z*, *V*=“oranges”)；*V* 的值无关紧要，因为
    *Z* 使其与 *U* 独立，其值对 *U* 没有影响。引入 *V* 并赋予它我们想要的任何值是使推导工作起来的技巧。此外，回顾一下全概率定律说，我们可以通过对该变量求和（或积分）来从联合分布中消去一个变量，例如
    *P*(*U*=*u*) = ∑[*v*]*P*(*U*=*u*, *V*=*v*)。当联合分布受到干预时，也是如此，例如 *P*(*U**[W]*[=]*[w]*=*u*)
    = ∑[*v*]*P*(*U**[W]*[=]*[w]*=*u*, *V**[W]*[=]*[w]*=*v*)。
- en: Now let’s start with the causal query *P*(*I*[*E*][=][*e*]) and see how to equate
    it with the backdoor estimand ∑[*g*] *P*(*I*|*E*=*e*, *G*=*g*)*P*(*G*=*g*).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们从因果查询 *P*(*I*[*E*][=][*e*]) 开始，看看它是如何与后门估计量 ∑[*g*] *P*(*I*|*E*=*e*, *G*=*g*)*P*(*G*=*g*)
    相等的。
- en: For some value of in-game purchases *i*, *P*(*I*[*E*][=][*e*]=*i*) = ∑[*g*]
    *P*(*I*[*E*][=][*e*]=*i*, *G*[*E*][=][*e*]=*g*) by the law of total probability.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于游戏内购买的某个值 *i*，根据全概率定律，*P*(*I*[*E*][=][*e*]=*i*) = ∑[*g*] *P*(*I*[*E*][=][*e*]=*i*,
    *G*[*E*][=][*e*]=*g*)。
- en: ∑[*g*] *P*(*I*[*E*][=][*e*]=*i*, *G*[*E*][=][*e*]=*g*) = ∑[*g*] *P*(*I*[*E*][=][*e*]=*i*,
    *G*=*g*), because we know from our original DAG that *G* is not affected by the
    intervention on *E*.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ∑[*g*] *P*(*I*[*E*][=][*e*]=*i*, *G*[*E*][=][*e*]=*g*) = ∑[*g*] *P*(*I*[*E*][=][*e*]=*i*,
    *G*=*g*)，因为我们知道从我们的原始 DAG 中，*G* 不受对 *E* 的干预的影响。
- en: 'Next we use the chain rule to factorize *P*(*I*[*E*][=][*e*]=*i*, *G*=*g*):
    ∑[*g*] *P*(*I*[*E*][=][*e*]=*i*, *G*=*g*) = ∑[*g*] *P*(*I*[*E*][=][*e*]=*i*| *G*=*g*)*P*(*G*=*g*).'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用链式法则对 *P*(*I*[*E*][=][*e*]=*i*, *G*=*g*) 进行因式分解：∑[*g*] *P*(*I*[*E*][=][*e*]=*i*,
    *G*=*g*) = ∑[*g*] *P*(*I*[*E*][=][*e*]=*i*| *G*=*g*)*P*(*G*=*g*)。
- en: Now we come to the trick—*P*(*I*[*E*][=][*e*]=*i*|*G*=*g*) = *P*(*I*[*E*][=][*e*]=*i*|*E*=*e*,
    *G*=*g*) for any value of *e*, because once we condition on *G*=*g*, *E*=*e* and
    *I*[*E*][=][*e*] are independent. So in our derivation, we can replace *P*(*I*[*E*][=][*e*]=*i*|*G*=*g*)
    with *P*(*I*[*E*][=][*e*]=*i*|*E*=*e*, *G*=*g*).
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们来到了一个技巧——对于任何 *e* 的值，*P*(*I*[*E*][=][*e*]=*i*|*G*=*g*) = *P*(*I*[*E*][=][*e*]=*i*|*E*=*e*,
    *G*=*g*)，因为一旦我们条件化 *G*=*g*，*E*=*e* 和 *I*[*E*][=][*e*] 就是独立的。因此，在我们的推导中，我们可以用 *P*(*I*[*E*][=][*e*]=*i*|*E*=*e*,
    *G*=*g*) 替换 *P*(*I*[*E*][=][*e*]=*i*|*G*=*g*)。
- en: 'Once we condition that *E*=*e*, we can use the law of consistency to drop the
    subscript: ∑[*g*] *P*(*I*[*E*][=][*e*]=*i*|*E*=*e*, *G*=*g*)*P*(*G*=*g*) = ∑[*g*]
    *P*(*I*=*i*|*E*=*e*, *G*=*g*)*P*(*G*=*g*).'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们条件化 *E*=*e*，我们就可以使用一致性定律取消下标：∑[*g*] *P*(*I*[*E*][=][*e*]=*i*|*E*=*e*, *G*=*g*)*P*(*G*=*g*)
    = ∑[*g*] *P*(*I*=*i*|*E*=*e*, *G*=*g*)*P*(*G*=*g*)。
- en: Let’s explain steps 4 and 5\. Our ignorability result shows that *I*[*E*][=][*e*]
    and *E* are conditionally independent given *G*. So in step 4 we apply the independence
    trick that lets us introduce *E*. Further, we set the value of *E* to be *e* so
    it matches the subscript [*E*][=][*e*]. This allows us to apply the law of consistency
    from chapter 8 and drop the subscript [*E*][=][*e*].
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解释步骤 4 和 5。我们的不可知性结果表明，在给定 *G* 的条件下，*I*[*E*][=][*e*] 和 *E* 是条件独立的。因此，在第 4
    步中，我们应用了独立性技巧，使我们能够引入 *E*。进一步地，我们将 *E* 的值设为 *e*，以便与下标 [*E*][=][*e*] 匹配。这使我们能够应用第
    8 章中的一致性定律，并取消下标 [*E*][=][*e*]。
- en: Voila, we’ve identified a backdoor estimand, an estimand from level 1 of the
    causal hierarchy, for a level 2 causal query *P*(*I*[*E*][=][*e*]) using level
    2 assumptions encoded in a DAG. Causal identification is just coming up with derivations
    like this. Much, if not most, of traditional causal inference research boils down
    to doing this kind of math, or writing algorithms that do it for you.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 哇，我们已经确定了一个后门估计量，这是一个来自因果层次结构第 1 层的估计量，用于第 2 层因果查询 *P*(*I*[*E*][=][*e*])，它使用在
    DAG 中编码的第 2 层假设。因果识别只是提出这样的推导。如果不是大部分，那么传统因果推理研究的大部分都归结为做这种数学运算，或者编写为你做这种运算的算法。
- en: Next, we’ll look at the do-calculus, which provides simple graph-based rules
    for identification that we can use in identification algorithms.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨do-calculus，它提供了一套基于图的简单规则，用于识别，我们可以在识别算法中使用这些规则。
- en: 10.4 Graphical identification with the do-calculus
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.4 使用do-calculus进行图形识别
- en: '*Graphical identification* (sometimes called *nonparametric identification*)
    refers to identification techniques that rely on reasoning over the DAG. One of
    the most well-known approaches to graphical identification is the *do-calculus*,
    a set of three rules used for identification with causal graphs. The rules use
    graph surgery and d-separation to determine cases when you can replace causal
    terms like *I*[*E*][=][*e*] with non-causal terms like *I*|*E*=*e*. Starting with
    a query on a higher level of the causal hierarchy, we can apply these rules in
    sequence to derive a lower-level estimand.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '*图形识别*（有时称为*非参数识别*）指的是依赖于对DAG进行推理的识别技术。图形识别最著名的途径之一是*do-calculus*，这是一套用于因果图识别的三条规则。这些规则使用图手术和d-separation来确定何时可以用非因果术语如*I*[*E*][=][*e*]替换因果术语。从因果层次结构较高级别的查询开始，我们可以按顺序应用这些规则来推导出较低级别的估计量。'
- en: 10.4.1 Demystifying the do-calculus
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.1 揭秘do-calculus
- en: 'Recall high school geometry, where you saw if-then statements like this:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下高中几何，你看到了这样的if-then语句：
- en: '*If* the shape is a square, *then* all the sides are equal.'
  id: totrans-173
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*如果*形状是正方形，*那么*所有边都相等。'
- en: When you were trying to solve a geometry problem, you used facts like this in
    the steps of your solution.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 当你试图解决一个几何问题时，你会在你的解决方案步骤中使用这样的事实。
- en: 'Similarly, the do-calculus consists of three rules (if-then statements) of
    the following form:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，do-calculus由三个规则（if-then语句）组成，形式如下：
- en: '*If* certain variables are d-separated after applying graph surgery to the
    DAG, *then* probability query *A* equals probability query *B*.'
  id: totrans-176
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*如果*某些变量在应用图手术后的DAG中d-separation，*那么*概率查询*A*等于概率查询*B*。'
- en: The rules of the do-calculus are not intuitive
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: do-calculus的规则并不直观
- en: The three rules of the do-calculus are not intuitive upon reading them, just
    as geometric rules like cos²*x* + sin²*x* = 1 were not intuitive when you first
    saw them in high school. But like those geometric rules, we derive the rules of
    the do-calculus from simpler familiar concepts, namely d-separation, ideal interventions,
    and the rules of probability. And like the rules of geometry, we can use the rules
    of the do-calculus to prove that a causal query from one level of the hierarchy
    is equivalent to one from another level.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: do-calculus的三个规则在阅读时并不直观，就像你在高中第一次看到像cos²x + sin²x = 1这样的几何规则时并不直观一样。但就像那些几何规则一样，我们是从更简单、更熟悉的概念中推导出do-calculus的规则，即d-separation、理想干预和概率规则。而且就像几何规则一样，我们可以使用do-calculus的规则来证明从层次结构的一个级别发出的因果查询与另一个级别的查询是等价的。
- en: Practically speaking, we can rely either on software libraries that implement
    the do-calculus in graphical identification algorithms (like y0) or simply hard-code
    well-known identification results like the backdoor adjustment estimand. To take
    away some of the mystery, I’ll introduce the rules and show how they can derive
    the backdoor estimand. The goal here is not to memorize these rules, but rather
    to see how they work in a derivation of the backdoor estimand that contrasts with
    the derivation in the previous section.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们可以依赖实现图形识别算法中的do-calculus的软件库（如y0）或简单地硬编码已知的识别结果，如后门调整估计量。为了消除一些神秘感，我将介绍这些规则并展示它们如何推导出后门估计量。这里的目的是不是记住这些规则，而是要看看它们在推导后门估计量时的作用，这与上一节的推导形成对比。
- en: In defining these rules, we’ll focus on the target distribution *Y* under an
    intervention on *X*. We want to generalize to all DAGs, so we’ll name two other
    nodes, *Z* and *W*. *Z* and *W* will allow us to cover cases where we have another
    potential intervention target *Z* and any node *W* we’d like to condition upon.
    Further, while I’ll often refer to individual variables, keep in mind that the
    rules apply when *X*, *Y*, *Z*, and *W* are sets of variables.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义这些规则时，我们将关注在*X*上的干预下的目标分布*Y*。我们希望推广到所有DAG，所以我们将命名另外两个节点，*Z*和*W*。*Z*和*W*将使我们能够涵盖我们还有另一个潜在干预目标*Z*和任何我们想要条件化的节点*W*的情况。此外，虽然我经常提到单个变量，但请记住，当*X*、*Y*、*Z*和*W*是变量集时，这些规则适用。
- en: 'Rule 1: Insertion or removal of observations'
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 规则1：观察值的插入或删除
- en: If *Y* and *Z* are d-separated in your DAG by *X* and *W* after the incoming
    edges to *X* are removed . . .
  id: totrans-182
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果 *Y* 和 *Z* 在你的DAG中通过 *X* 和 *W* 分隔，在移除 *X* 的进入边之后……
- en: '*Then* *P* ( *Y* [*X*] [=] [*x*]=  *y* | *Z*  =  *z*, *W*  =  *w*) = *P*( *Y*
    [*X*] [=] [*x*] =  *y* | *W*  =  *w*).'
  id: totrans-183
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*那么* *P*（ *Y* [*X*][=][*x*]= *y* | *Z* = *z*，*W* = *w*）= *P*（ *Y* [*X*][=][*x*]
    = *y* | *W* = *w*）。'
- en: This is called “insertion or removal” because we can remove *Z*=*z* from *P*(*Y*[*X*][=][*x*]=*y*|*Z*=*z*,
    *W*=*w*) to get *P*(*Y*[*X*][=][*x*]=*y* | *W*=*w*) and vice versa.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为“插入或移除”，因为我们可以从 *P*（ *Y* [*X*][=][*x*]=*y*|*Z*=*z*，*W*=*w*）中移除 *Z*=*z* 来得到
    *P*（ *Y* [*X*][=][*x*]=*y* | *W*=*w*），反之亦然。
- en: 'Rule 2: Exchange of an intervention for an observation'
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 规则2：交换干预和观察
- en: '*If* *Y* and *Z* are d-separated in your DAG by *X* and *W* after incoming
    edges in *X* and outgoing edges from *Z* have been removed . . .'
  id: totrans-186
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*如果* *Y* 和 *Z* 在你的DAG中通过 *X* 和 *W* 分隔，在 *X* 的进入边和 *Z* 的出去边被移除之后……'
- en: '*then P* ( *Y* [*X*] [=] [*x*] [,] [*Z*] [=] [*z*]= *y* | *W* =  *w*) = *P*(
    *Y* [*X*] [=] [*x*] =  *y* | *Z*  =   *z*, *W*  =   *w*).'
  id: totrans-187
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*然后 P*（ *Y* [*X*] [=] [*x*] [,] [*Z*] [=] [*z*]= *y* | *W* = *w*）= *P*（ *Y*
    [*X*] [=] [*x*] = *y* | *Z* = *z*，*W* = *w*）。'
- en: Here we can either *exchange* the intervention [*Z*][=][*z*] in *P*(*Y*[*X*][=][*x*][,]
    [*Z*][=][*z*]=*y* | *W*=*w*) for conditioning on the observation *Z*=*z* to get
    *P*(*Y*[*X*][=][*x*]=*y* | *Z*=*z*, *W*=*w*), or vice versa.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以在 *P*（ *Y* [*X*][=][*x*][,] [*Z*][=][*z*]=*y* | *W*=*w*）中 *交换* 干预 [*Z*][=][*z*]
    以观察 *Z*=*z* 为条件，得到 *P*（ *Y* [*X*][=][*x*]=*y* | *Z*=*z*，*W*=*w*），反之亦然。
- en: 'Rule 3: Insertion or removal of interventions'
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 规则3：插入或移除干预
- en: For rule 3, we are going to define *Z* as a set of nodes, and *Z*(*W*) as the
    subset of *Z* that are not ancestors of *W*.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 对于规则3，我们将定义 *Z* 为一组节点，*Z*(*W*)为不是 *W* 的祖先的 *Z* 的子集。
- en: If *Y* and *Z* are d-separated in your DAG by *X* and *W* after you remove all
    incoming edges to *X* and *Z*( *W*) . . .
  id: totrans-191
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你移除了 *X* 和 *Z*（ *W*）的所有进入边后，*Y* 和 *Z* 在你的DAG中通过 *X* 和 *W* 分隔……
- en: '*then* *P*( *Y* [*X*] [=] [*x*] [,] [*Z*] [=] [*z*] =  *y* | *W* =  *w*) =
    *P*( *Y* [*X*] [=] [*x*] =  *y* | *W* =  *w*).'
  id: totrans-192
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*然后* *P*（ *Y* [*X*][=][*x*] [,] [*Z*][=][*z*] = *y* | *W* = *w*）= *P*（ *Y*
    [*X*][=][*x*] = *y* | *W* = *w*）。'
- en: This rule allows you to insert [*Z*][=][*z*] into *P*(*Y*[*X*][=][*x*]=*y* |
    *W*=*w*) to get *P*(*Y*[*X*][=][*x*][,] [*Z*][=][*z*]=*y* | *W*=*w*) or remove
    [*Z*][=][*z*] from *P*(*Y*[*X*][=][*x*][,] [*Z*][=][*z*]=*y* | *W*=*w*) to get
    *P*(*Y*[*X*][=][*x*]=*y* | *W*=*w*).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这条规则允许你将 [*Z*][=][*z*] 插入到 *P*（ *Y* [*X*][=][*x*]=*y* | *W*=*w*）中，得到 *P*（ *Y*
    [*X*][=][*x*][,] [*Z*][=][*z*]=*y* | *W*=*w*），或者从 *P*（ *Y* [*X*][=][*x*][,] [*Z*][=][*z*]=*y*
    | *W*=*w*）中移除 [*Z*][=][*z*]，得到 *P*（ *Y* [*X*][=][*x*]=*y* | *W*=*w*）。
- en: 10.4.2 Using the do-calculus for backdoor identification
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.2 使用do-calculus进行后门识别
- en: 'Now we’ll use the do-calculus to provide an alternative derivation of the backdoor
    estimand that differs from our “ignorability”-based definition. Again, I include
    this derivation to demystify the application of the do-calculus. Don’t worry if
    you don’t completely follow each step:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用do-calculus来提供对后门估计量的另一种推导，这种推导与基于“不可知性”的定义不同。同样，我包括这个推导是为了阐明do-calculus的应用。如果你没有完全理解每一步，请不要担心：
- en: '*P*(*I*[*E*][=][*e*]=*i*) = ∑[*g*] *P*(*I*[*E*][=][*e*]=*i*, *G*[*E*][=][*e*]=*g*)
    by the law of total probability.'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*P*（*I*[*E*][=][*e*]=*i*）= ∑[*g*] *P*（*I*[*E*][=][*e*]=*i*，*G*[*E*][=][*e*]=*g*）根据全概率定律。'
- en: ∑[*g*] *P*(*I*[*E*][=][*e*]=*i*, *G*[*E*][=][*e*]=*g*) = ∑[*g*] *P*(*I*[*E*][=][*e*][,]
    [*G*][=][*g*]=*i*)*P*(*G*[*E*][=][*e*][,] [*I*][=][*i*]=*g*) by way of *c-component
    factorization*.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ∑[*g*] *P*（*I*[*E*][=][*e*]=*i*，*G*[*E*][=][*e*]=*g*）= ∑[*g*] *P*（*I*[*E*][=][*e*][,]
    [*G*][=][*g*]=*i*）*P*（*G*[*E*][=][*e*][,] [*I*][=][*i*]=*g*）通过c-component分解。
- en: '*P*(I[*E*][=][*e*][,] [*G*][=][*g*]=i) = *P*(*I*=*i*|*E*=*e*, *G*=*g*) by rule
    2 of the do-calculus.'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*P*（I[*E*][=][*e*][,] [*G*][=][*g*]=i）= *P*（*I*=*i*|*E*=*e*，*G*=*g*）根据do-calculus的第2条规则。'
- en: '*P*(G[*E*][=][*e*][,] [*I*][=][*i*]=g) = *P*(*G*=*g*) by rule 3 of the do-calculus.'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*P*（G[*E*][=][*e*][,] [*I*][=][*i*]=g）= *P*（*G*=*g*）根据do-calculus的第3条规则。'
- en: Therefore, *P*(I[*E*][=][*e*]=i)= ∑[*g*] *P*(*I*=*i*|*E*=*e*, *G*=*g*) *P*(*G*=*g*)
    by plugging 3 and 4 into 2\.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，*P*（I[*E*][=][*e*]=i）= ∑[*g*] *P*（*I*=*i*|*E*=*e*，*G*=*g*）*P*（*G*=*g*）通过将3和4代入2。
- en: The do-calculus rules are applied in steps 3 and 4\.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: do-calculus规则在步骤3和4中应用。
- en: Note  Step 2 uses a factorization rule called *c-component factorization*. A
    c-component (confounded component) is a set of nodes in a DAG where each pair
    of observable nodes is connected by a path with edges that always point toward,
    never away from, the observable nodes (these are the “orphaned cousins” mentioned
    in chapter 4). The joint probability of the observed variables can be factorized
    into c-components, and this fact enabled step 2\. Factorizing over c-components
    is common in identification algorithms. See the references in the chapter notes
    at [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：步骤2使用了一种称为*c-component factorization*的分解规则。c-component（混淆组件）是DAG中一组节点，其中每个可观察节点的成对节点通过一条路径相连，该路径的边始终指向可观察节点，而不是远离它们（这些是第4章中提到的“孤儿表亲”）。观察变量的联合概率可以被分解为c-components，这一事实使得步骤2成为可能。在识别算法中，对c-components进行分解是常见的。请参阅章节注释中的参考文献[https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)。
- en: This do-calculus-based derivation is far less intuitive than our “ignorability”-based
    derivation. There are two advantages we get in exchange for that of intuition.
    First, the do-calculus is *complete*, meaning that if a query has an identifiable
    estimand using graphical assumptions alone, it can be derived using the do-calculus.
    Second, we have algorithms that leverage the do-calculus to automate graphical
    identification.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这种基于do-calculus的推导比我们基于“不可知性”的推导要直观得多。我们为此直觉交换了两个优势。首先，do-calculus是*完整的*，这意味着如果一个查询仅使用图形假设就有可识别的估计量，那么它可以使用do-calculus推导出来。其次，我们有利用do-calculus来自动化图形识别的算法。
- en: 10.5 Graphical identification algorithms
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.5 图形识别算法
- en: Graphical identification algorithms, often called *ID algorithms*, automate
    the application of graph-based identification systems like the do-calculus. When
    we used y0 to check for identification of *P*(*I*[*E*][=][*e*]) and to derive
    the backdoor estimand, it was using its implementation of graphical identification
    algorithms. In this section, we’ll see how we can use these algorithms to identify
    another useful estimand called the *front-door estimand*.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图形识别算法，通常称为*ID算法*，自动化了基于图的应用识别系统，如do-calculus。当我们使用y0检查*P*(*I*[*E*][=][*e*])的识别并推导后门估计量时，它正在使用其图形识别算法的实现。在本节中，我们将看到如何使用这些算法来识别另一个有用的估计量，称为*前门估计量*。
- en: '10.5.1 Case study: The front-door estimand'
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.5.1 案例研究：前门估计量
- en: In our online gaming example, suppose we were not able to observe guild membership.
    Then we would not have backdoor identification of *P*(*I*[*E*][=][*e*]). However,
    suppose we had a *mediator* between side-quest engagement (*E*) and in-game purchases
    (*I*)—a node on the graph between *E* and *I*. Specifically, our mediator represents
    *won items* (*W*), as seen in figure 10.11\.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的在线游戏示例中，假设我们无法观察到公会成员资格。那么我们就无法进行*P*(*I*[*E*][=][*e*])的后门识别。然而，假设我们在侧任务参与(*E*)和游戏内购买(*I*)之间有一个*中介*——一个在*E*和*I*之间的图节点。具体来说，我们的中介代表*赢得的物品*(*W*)，如图10.11所示。
- en: '![figure](../Images/CH10_F11_Ness.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH10_F11_Ness.png)'
- en: Figure 10.11 Side-quest engagement leads to winning virtual items like this
    magic bow. Won items drive more in-game purchases, such as magic arrows for the
    magic bow, so we introduce a mediator “won items” on the causal path between side-quest
    engagement and in-game purchases.
  id: totrans-209
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.11 侧任务参与导致赢得像这种魔法弓这样的虚拟物品。赢得的物品会推动更多游戏内购买，例如魔法弓的魔法箭，因此我们在侧任务参与和游戏内购买之间的因果路径上引入了一个中介“赢得的物品”。
- en: The idea of won items is as follows. When a player successfully completes a
    side-quest, they win a virtual item. The more side-quests they finish, the more
    items they earn. Those *won* virtual items and *purchased* virtual items can complement
    one another. For example, winning a magic bow motivates purchases of magical arrows.
    Thus, the amount of won items a player has influences the amount of virtual items
    they purchase.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 赢得的物品的想法如下。当玩家成功完成一个侧任务时，他们会赢得一个虚拟物品。他们完成的侧任务越多，他们获得的物品就越多。那些*赢得的*虚拟物品和*购买的*虚拟物品可以相互补充。例如，赢得一个魔法弓会促使购买魔法箭。因此，玩家拥有的赢得物品的数量会影响他们购买的虚拟物品的数量。
- en: Given this graph, we can use y0’s implementation of graphical identification
    algorithms to derive the front-door estimand.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 给定这个图，我们可以使用y0的图形识别算法实现来推导前门估计量。
- en: Listing 10.6 Deriving the front-door estimand in y0
  id: totrans-212
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.6 在y0中推导前门估计量
- en: '[PRE5]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#1 Build a new graph with the mediator variable.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 使用中介变量构建一个新的图。'
- en: '#2 Still the same query as in listing 10.5, P(I_{E=e})'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 仍然是列表10.5中的相同查询，P(I_{E=e})'
- en: '#3 But now we observe I, E, and W'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 但现在我们观察到I，E和W'
- en: '#4 Finally, we check if the query is identified given the DAG and observational
    distribution.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 最后，我们检查在DAG和观测分布的条件下查询是否被识别。'
- en: This code will return the output in figure 10.12.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将返回图10.12的输出。
- en: '![figure](../Images/ness-ch10-eqs-3x.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/ness-ch10-eqs-3x.png)'
- en: Figure 10.12 Y0 renders a math figure as output of identification.
  id: totrans-220
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.12 Y0将识别的数学图作为输出。
- en: 'Rearranging the output, and in our notation, this is the result:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 重新排列输出，按照我们的记号，这是结果：
- en: '![figure](../Images/ness-ch10-eqs-4x.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/ness-ch10-eqs-4x.png)'
- en: 'Simplifying as before, we get the front-door estimand:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述简化，我们得到前门估计量：
- en: '![figure](../Images/ness-ch10-eqs-5x.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/ness-ch10-eqs-5x.png)'
- en: Note that there is an outer summation over *W* and an inner summation over all
    values of *E* (with each value of *E* denoted as *ε*, distinct from the intervention
    value *e*).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，有一个关于*W*的外层求和和一个关于所有*E*值的内层求和（每个*E*值用*ε*表示，与干预值*e*不同）。
- en: 10.5.2 Demystifying the front door
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.5.2 揭示前门之谜
- en: Like the backdoor estimand, the do-calculus derivation of the front-door estimand
    involves repeated substitutions using rules 2 and 3\. The rough intuition behind
    the front-door estimand is that the statistical association between side-quest
    engagement and in-game purchases comes from both the direct causal path and the
    path through the backdoor confounder guild membership (*G*). The front-door estimand
    uses the mediator to determine how much of that association is due to the direct
    causal path; the mediator acts as a gauge of the flow of statistical information
    through that direct causal path.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 与后门估计量类似，前门估计量的do-calculus推导涉及到使用规则2和3的重复替换。前门估计量的粗略直觉是，侧面任务参与和游戏内购买的统计关联来自直接的因果路径以及通过后门混杂因素公会成员资格（*G*）的路径。前门估计量使用中介变量来确定这种关联中有多少是由于直接的因果路径；中介变量充当通过该直接因果路径的统计信息流的测量器。
- en: A key benefit of the estimand is that it does not require observing a set of
    confounders that block all possible backdoor paths. Avoiding backdoor adjustment
    is useful when you have many confounders, are unable to adjust due to latent confounders,
    or are concerned that there might be some unknown confounders.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 估计量的一个关键好处是，它不需要观察一组阻止所有可能后门路径的混杂因素。当你有许多混杂因素、无法调整由于潜在混杂因素，或者担心可能存在一些未知的混杂因素时，避免后门调整是有用的。
- en: Next, we’ll examine how to identify counterfactuals.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨如何识别反事实。
- en: 10.6 General counterfactual identification
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.6 通用反事实识别
- en: The causal DAG is a level 2 modeling assumption. The causal hierarchy theorem
    tells us that the graph in general is not sufficient to identify level 3 counterfactual
    queries. For counterfactual identification from level 1 or level 2 distributions,
    you need level 3 assumptions. In simple terms, a level 3 assumption is any causal
    assumption that you can’t represent with a simple causal DAG.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 因果DAG是一个2级建模假设。因果层次定理告诉我们，在一般情况下，图本身不足以识别3级反事实查询。要从1级或2级分布中进行反事实识别，你需要3级假设。简单来说，3级假设是任何你不能用简单的因果DAG表示的因果假设。
- en: In chapter 9, I introduced the general algorithm for counterfactual inference.
    The algorithm requires a structural causal model (SCM), which is a level 3 model;
    it encapsulates level 3 assumptions. With an SCM, the algorithm can infer *all*
    counterfactual queries that can be defined on its underlying variables. The cost
    of this ability is that the SCM must encapsulate *all* the assumptions needed
    to answer all those queries. Many of these assumptions cannot be validated with
    level 1 or level 2 data.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在第9章中，我介绍了反事实推理的通用算法。该算法需要一个结构因果模型（SCM），这是一个3级模型；它封装了3级假设。有了SCM，算法可以推断出在其基础变量上可以定义的所有反事实查询。这种能力的代价是，SCM必须封装回答所有这些查询所需的所有假设。其中许多假设不能用1级或2级数据进行验证。
- en: The more assumptions you make, the more vulnerable your inferences are to violations
    of these assumptions. For this reason, we seek identification techniques that
    target specific counterfactual queries (rather than every counterfactual query)
    with the minimal set of level 3 assumptions possible.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 你做出的假设越多，你的推断就越容易受到这些假设违反的影响。因此，我们寻求识别技术，这些技术针对特定的反事实查询（而不是每个反事实查询）使用尽可能少的3级假设。
- en: 10.6.1 The problem with the general algorithm for counterfactual inference
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.6.1 通用反事实推理算法的问题
- en: We can see the problem with the general algorithm for counterfactual inference
    when we apply it to two similar SCMs. Let’s suppose there is a ground-truth SCM
    that differs from the SCM you are using to run the algorithm. Suppose both SCMs
    have the exact same underlying DAG and the same statistical fit on observational
    and experimental data; in other words, the SCMs provide the same inferences for
    all level 1 and level 2 queries. Your SCM could still produce different (inaccurate)
    counterfactual inferences relative to the ground-truth SCM.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将通用反事实推理算法应用于两个类似的SCM时，我们可以看到问题。假设有一个与您用于运行算法的SCM不同的真实SCM。假设这两个SCM具有完全相同的底层DAG和相同的观察数据和实验数据的统计拟合；换句话说，SCM为所有一级和二级查询提供相同的推理。您的SCM仍然可能相对于真实SCM产生不同的（不准确的）反事实推理。
- en: 'To see why, recall the stick-breaking example from chapter 6\. I posed two
    similar but different SCMs. This was the first:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解原因，回顾第6章中的木棍断裂示例。我提出了两个相似但不同的SCM。这是第一个：
- en: '![figure](../Images/ness-ch10-eqs-10x.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch10-eqs-10x.png)'
- en: 'And this was the second:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这是第二个：
- en: '![figure](../Images/ness-ch10-eqs-11x.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/ness-ch10-eqs-11x.png)'
- en: Figure 10.13 visualizes sampling a single value from these models.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.13可视化了对这些模型进行单值采样。
- en: '![figure](../Images/CH10_F13_Ness.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH10_F13_Ness.png)'
- en: Figure 10.13 Two different SCMs encode the exact same observational and interventional
    distributions, but given the same exogenous variable value, you can get two different
    values of the corresponding endogenous variable in each model.
  id: totrans-242
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.13两个不同的SCM编码了完全相同的观察和干预分布，但给定相同的外生变量值，在每个模型中，您可以得到对应内生变量的两个不同值。
- en: Figure 10.13 shows how, given a value of *n*[*y*] = .15, the sticks break at
    the .15 meters point, but the first stick will break in region 2, returning a
    value of 2, while the second stick will break in region 3, returning a 3\. They
    produce different outcomes given the same random input because they differ in
    a level 3 assumption, i.e., *how* they process the input.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.13展示了当给定*n*[y] = .15的值时，木棍在.15米处断裂，但第一根木棍将在区域2断裂，返回值为2，而第二根木棍将在区域3断裂，返回值为3。由于它们在第三级假设上的不同，即它们如何处理输入，它们在相同的随机输入下产生不同的结果。
- en: For this reason, when we go in the opposite direction and apply the abduction
    step in the general counterfactual inference algorithm, we can get different results
    across these models. For a given value of the endogenous variable, we can get
    different posterior distributions on the exogenous variable.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当我们朝相反方向进行，并在通用反事实推理算法中应用归纳步骤时，我们可以得到这些模型的不同结果。对于内生变量的给定值，我们可以得到外生变量的不同后验分布。
- en: Figure 10.14 illustrates how the two models, for an observed outcome of 3, would
    produce different inferences on *N*[*y*]. For the first SCM, a value of *y*=3
    means *P*(*N*[*y*]|*Y*=3) is a continuous uniform distribution on the range (*p*[*x*][1]
    + *p*[*x*][2]) to 1, and for the second SCM, it is a continuous uniform distribution
    on the range 0 to *p*[*x*][3]. These different distributions of *P*(*N*[*y*]|*Y*=3)
    would lead to different results from the counterfactual inference algorithm. Now
    suppose SCM 2 is right and SCM 1 is wrong. If we choose SCM 1, our counterfactual
    inferences will be inaccurate.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.14说明了这两个模型在观察到结果为3时，如何对*N*[y]产生不同的推理。对于第一个SCM，*y*=3的值意味着*P*(N*[y]|*Y*=3)是在(*p*[x][1]
    + *p*[x][2])到1范围内的连续均匀分布，而对于第二个SCM，它是在0到*p*[x][3]范围内的连续均匀分布。这些不同的*P*(N*[y]|*Y*=3)分布会导致反事实推理算法产生不同的结果。现在假设SCM
    2是正确的，而SCM 1是错误的。如果我们选择SCM 1，我们的反事实推理将是不准确的。
- en: '![figure](../Images/CH10_F14_Ness.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH10_F14_Ness.png)'
- en: Figure 10.14 The two SCMs, despite encoding the same set of observational and
    interventional distributions, would produce two different posteriors of *N**[y]*
    given *Y*=3 in the abduction step. Therefore, they encode different counterfactual
    distributions and can produce different counterfactual inferences.
  id: totrans-247
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.14尽管两个SCM编码了相同的观察和干预分布集，但在归纳步骤中，它们会对*Y*=3给定*N*[y]产生两个不同的后验。因此，它们编码了不同的反事实分布，可以产生不同的反事实推理。
- en: The general case is even harder; there can be many SCMs entailing the same level
    1 and 2 assumptions but have different level 3 assumptions. You might learn one
    of those SCMs by, for example, using a deep neural network-based approach to learn
    a deep SCM from level 1 and level 2 data. But the deep SCM might not be the *right*
    SCM with respect to the counterfactual inferences you want to make.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 一般情况更难；可能有多个SCM包含相同的第1级和第2级假设，但具有不同的第3级假设。例如，您可能通过使用基于深度神经网络的方法从第1级和第2级数据中学习深度SCM来学习这些SCM之一。但是，深度SCM可能不是您想要进行反事实推断的*正确*SCM。
- en: The general algorithm for counterfactual inference is ideal if you are confident
    in the ground-truth SCM. But in cases where you aren’t, you can look toward counterfactual
    identification, where you specify a *minimal* set of level 3 assumptions that
    enable you to identify a target counterfactual query.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对基础真实SCM有信心，反事实推断的一般算法是理想的。但在您不确定的情况下，您可以转向反事实识别，在那里您指定一个*最小*的第三级假设集合，使您能够识别一个目标反事实查询。
- en: '10.6.2 Example: Monotonicity and the probabilities of causation'
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.6.2 示例：单调性和因果概率
- en: 'Monotonicity is an example of a powerful level 3 assumption. Monotonicity is
    the simple assumption that the relationship between a cause *X* and an outcome
    *Y* is monotonic: *E*(*Y*|*X*=*x*) either never increases or never decreases as
    *x* increases. Note that linearity is a special case of monotonicity.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 单调性是强大第三级假设的一个例子。单调性是一个简单的假设，即原因 *X* 和结果 *Y* 之间的关系是单调的：*E*(*Y*|*X*=*x*) 当 *x*
    增加时要么从不增加要么从不减少。请注意，线性是单调性的一个特例。
- en: An intuitive example of monotonicity and non-monotonicity is in the dosage of
    medicine. In a monotonic dose-response relationship, taking more of the medicine
    either helps or does nothing. In a non-monotonic dose-response relationship, taking
    the medicine might help at a normal dose, but taking an overdose might cause the
    problem to get worse. Monotinicity helps identification by eliminating counterfactual
    possibilities; if the dose-response relationship is monotonic, when you imagine
    what would have happened if you took a stronger dose, you can eliminate the possiblity
    that you would have gotten worse.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 单调性和非单调性的直观例子在药物剂量中。在单调剂量反应关系中，服用更多的药物要么有帮助要么没有帮助。在非单调剂量反应关系中，服用药物可能在正常剂量下有帮助，但过量服用可能会使问题变得更糟。单调性通过消除反事实可能性来帮助识别；如果剂量反应关系是单调的，当你想象如果你服用更强的剂量会发生什么时，你可以消除你变得更糟的可能性。
- en: 'Recall the probabilities of causation we saw in chapter 8:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 回想第8章中我们看到的因果概率：
- en: 'Probability of necessity (PN): *P*(*Y*[*X*][=][0]=0|*X*=1, *Y*=1)'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必要概率（PN）：*P*(*Y*[*X*][=][0]=0|*X*=1, *Y*=1)
- en: 'Probability of sufficiency (PS): *P*(*Y*[*X*][=1]=1|*X*=0, *Y*=0)'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 足够概率（PS）：*P*(*Y*[*X*][=1]=1|*X*=0, *Y*=0)
- en: 'Probability of necessity and sufficiency (PNS): *P*(*Y*[*X*][=1]=1, *Y*[*X*][=0]=0)'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必要性和充分性概率（PNS）：*P*(*Y*[*X*][=1]=1, *Y*[*X*][=0]=0)
- en: Given monotonicity, we can identify the following level 2 estimands for the
    probabilities of causation.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在单调性的假设下，我们可以识别以下关于因果概率的第二级估计量。
- en: PN = (*P*(*Y*=1) – *P*(*Y*[*X*][=0]=1))/*P*(*X*=1, *Y*=1)
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PN = (*P*(*Y*=1) – *P*(*Y*[*X*][=0]=1))/*P*(*X*=1, *Y*=1)
- en: PS = (*P*(*Y*[*X*][=1]=1) – *P*(*Y*=1))/*P*(*X*=0, *Y*=0)
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PS = (*P*(*Y*[*X*][=1]=1) – *P*(*Y*=1))/*P*(*X*=0, *Y*=0)
- en: PNS = *P*(*Y*[*X*][=1]=1) – *P*(*Y*[*X*][=0]=1)
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PNS = *P*(*Y*[*X*][=1]=1) – *P*(*Y*[*X*][=0]=1)
- en: We can estimate these level 2 estimands from level 2 data, such as a randomized
    experiment. And, of course, if we only have observational data, we can use backdoor
    or front-door adjustment or another identification strategy to infer *P*(*Y*[*X*][=0]=1)
    and *P*(*Y*[*X*][=1]=1) from that data.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从第二级数据中估计这些第二级估计量，例如一个随机实验。当然，如果我们只有观察数据，我们可以使用后门或前门调整或另一种识别策略，从那些数据中推断出
    *P*(*Y*[*X*][=0]=1) 和 *P*(*Y*[*X*][=1]=1)。
- en: 'We could derive these estimands by hand again, but instead, let’s think about
    the monotonicity enabled this identification by eliminating counterfactual possibilities.
    To see this, consider our uplift modeling question in chapter 8\. There, *X* was
    whether we sent a promotion, and *Y* was whether the customer remained a paying
    subscriber (*Y*=1) or “churned” (unsubscribed; *Y*=0). We segmented the subscribers
    as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再次手动推导这些估计量，但相反，让我们考虑通过消除反事实可能性来使这种识别单调性得以实现的识别。为了看到这一点，考虑第8章中的提升建模问题。在那里，*X*
    是我们是否发送了促销，而 *Y* 是客户是否继续作为付费订阅者（*Y*=1）或“流失”（取消订阅；*Y*=0）。我们按以下方式对订阅者进行细分：
- en: '*Persuadables*—Subscribers whose chance of remaining increases when you send
    a promotion'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可说服者*—当你发送促销时，留下可能性增加的订阅者'
- en: '*Sure things*—Subscribers who have a high chance of remaining regardless of
    whether you send a promotion'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*确定事件*—无论是否发送促销，都有很大可能性留下的订阅者'
- en: '*Lost causes*—Subscribers who have a low chance of remaining regardless of
    whether you send a promotion'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*无望的原因*—无论是否发送促销，留下可能性很低的订阅者'
- en: '*Sleeping dogs*: Subscribers whose chances of remaining *go down* when you
    send a promotion'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*沉睡的狗*：当你发送促销时，留下可能性下降的订阅者'
- en: If you assume monotonicity, you are assuming that sending the promotion either
    does nothing or increases the chances of remaining. It assumes there are no users
    who will respond poorly to the promotion. In other words, assuming monotonicity
    means you assume there are no sleeping dogs.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你假设单调性，你是在假设发送促销要么不起作用，要么会增加留下的可能性。它假设没有用户会对促销做出不良反应。换句话说，假设单调性意味着你假设没有沉睡的狗。
- en: 'Now let’s consider how this narrows things down. Suppose you have the following
    question:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑这如何缩小范围。假设你有一个以下问题：
- en: I failed to send a promotion to a customer and they churned. Would they have
    remained had I sent the promotion? *P* ( *Y* [*X*] [=1] = 1| *X*  = 0, *Y*  = 0)
  id: totrans-269
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我未能向一位客户发送促销，他们流失了。如果我发送了促销，他们会留下吗？*P*（*Y*[*X*][=1]=1|*X* = 0, *Y* = 0）
- en: This counterfactual query is the probability of sufficiency. We want to know
    if sending the promotion would have increased the chances of their remaining.
    Thinking through the question,
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这个反事实查询是充分性的概率。我们想知道发送促销是否会增加他们留下的可能性。思考这个问题，
- en: If the customer was a persuadable, sending the promotion would have increased
    their chances of remaining.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果客户是可说服的，发送促销会增加他们留下的可能性。
- en: If the customer was a lost cause, sending the promotion would have had no effect.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果客户是无望的原因，发送促销将不会有任何效果。
- en: If the customer was a sleeping dog, sending the promotion would have made them
    *even less* likely to remain.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果客户是沉睡的狗，发送促销会使他们留下可能性更低。
- en: It’s hard to determine if we should have sent the promotion if being a persuadable
    and being a sleeping dog were both possible for this customer, in one case the
    promotion would have helped and in the other it would have made churning even
    more certain. But if we assume monotonicity, we eliminate the possibility that
    they were a sleeping dog, and can conclude sending the promotion would have helped
    or, at least, not have hurt their chances of staying.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这位客户既是可说服的，又是沉睡的狗，那么很难确定我们是否应该发送促销，在一个情况下促销会有所帮助，而在另一个情况下它会使客户流失更加确定。但如果我们假设单调性，我们就排除了他们是沉睡的狗的可能性，并可以得出结论，发送促销会有所帮助，或者至少不会损害他们留下的机会。
- en: Bayesian modeling and counterfactual identification
  id: totrans-275
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 贝叶斯建模和反事实识别
- en: Although the graphical identification algorithms will work with some counterfactual
    queries, we don’t have general algorithms for counterfactual identification. But
    given our focus on the tools of probabilistic ML, we can look to Bayesian modeling
    for a path forward.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管图形识别算法可以处理一些反事实查询，但我们没有反事实识别的通用算法。但鉴于我们关注概率机器学习工具，我们可以转向贝叶斯建模作为前进的道路。
- en: Identification is fundamentally about uncertainty. For example, in the counterfactual
    case, a lack of identification means that even with infinite level 1 and level
    2 data, you can’t be certain about the true value of the level 3 query. From a
    Bayesian perspective, we can use probability to handle that uncertainty.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 识别本质上关于不确定性。例如，在反事实情况下，缺乏识别意味着即使有无限的一级和二级数据，你也不能确定三级查询的真实值。从贝叶斯的角度来看，我们可以使用概率来处理这种不确定性。
- en: 'Suppose you have a set of causal assumptions, including non-graphical assumptions,
    and some level 1 and 2 data. You can take the following Bayesian approach to test
    whether your assumptions and data are sufficient to identify your counterfactual
    query:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一组因果假设，包括非图形假设，以及一些一级和二级数据。你可以采取以下贝叶斯方法来测试你的假设和数据是否足以识别你的反事实查询：
- en: Specify a set of SCMs that are diverse yet all consistent with your causal assumptions.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定一组既多样化又与你的因果假设一致的SCM（结构因果模型）。
- en: Place a prior distribution over this set, such that more plausible models get
    higher prior probability values.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个集合上放置一个先验分布，使得更合理的模型获得更高的先验概率值。
- en: Obtain a posterior distribution on the SCMs given observational (level 1) and
    interventional (level 2) data.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在给定观测（第1级）和干预（第2级）数据的情况下，获得SCM的后验分布。
- en: Sample SCMs from the posterior distribution, and for each sample SCM, you apply
    the general algorithm for counterfactual inference for a specific counterfactual
    query.
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从后验分布中抽取SCM样本，并对每个样本SCM，你应用针对特定反事实查询的通用反事实推理算法。
- en: The result would constitute the posterior distribution over this counterfactual
    inference. If your causal assumptions and your data are enough to identify the
    counterfactual query, the posterior on the counterfactual inference will converge
    to the true value as the size of your data increases. (Successful convergence
    assumes typical “regularity” conditions for Bayesian estimation. Results will
    depend on the quality of the prior.) But even if it doesn’t converge to the true
    value, your assumptions might still enable convergence to a ballpark region around
    the true value that is small enough to be useful (this is called partial identification,
    as described in section 10.9).
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将构成关于这一反事实推理的后验分布。如果你的因果假设和你的数据足以识别反事实查询，随着你数据的增加，反事实推理的后验将收敛到真实值。（成功的收敛假设了贝叶斯估计的典型“规律性”条件。结果将取决于先验的质量。）但即使它没有收敛到真实值，你的假设仍然可能使收敛到一个足够小以有用的真实值附近区域（这被称为部分识别，如第10.9节所述）。
- en: The Pyro library, and its causality-focused extension ChiRho, facilitate combining
    Bayesian and causal ideas in this way.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: Pyro库及其因果性扩展ChiRho，使得以这种方式结合贝叶斯和因果思想成为可能。
- en: There are generalizations of monotonicity from binary actions (like sending
    or not sending a promotion) to multiple actions as in a decision or reinforcement
    learning problem, see the course notes at [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    for references.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 从二元动作（如发送或不发送促销）到多动作（如决策或强化学习问题）的单调性推广是通用的，有关参考文献请参阅[https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)。
- en: 10.7 Graphical counterfactual identification
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.7 图形反事实识别
- en: A conventional causal DAG only encodes level 2 assumptions, but there are graphical
    techniques for reasoning about counterfactuals. Graphical counterfactual inference
    only works in special cases, but these cases are quite practical. Further, working
    with graphs enables us to automate identification with algorithms. To illustrate
    graphical counterfactual identification, we’ll introduce a new case study.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 一个传统的因果DAG仅编码第2级假设，但存在推理反事实的图形技术。图形反事实推理仅在特殊情况下有效，但这些情况非常实用。此外，使用图形使我们能够通过算法自动化识别。为了说明图形反事实识别，我们将引入一个新的案例研究。
- en: 'When you open Netflix, you see the Netflix dashboard, which shows several forms
    of recommended content. Two of these are “Top Picks For You,” which is a personalized
    selection of shows and movies that Netflix’s algorithms predict you will enjoy
    based on your past viewing behavior and ratings, and “Because You Watched,” which
    recommends content based on things you watched recently. The model of this system
    includes the following variables:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 当你打开Netflix时，你会看到Netflix仪表板，它显示了多种推荐内容形式。其中两种是“为你精选”，这是Netflix算法根据你的过去观看行为和评分预测你将喜欢的个性化节目和电影选择，以及“因为你看了”，它根据你最近观看的内容推荐内容。该系统的模型包括以下变量：
- en: '*T*—A variable for the recommendation policy that selects a subscriber’s “Top
    Picks for You” content. For simplicity, we’ll consider a policy, “+*t*”, that
    is currently in production. We’ll use “–*t*”, meaning “not *t*”, to represent
    alternative policies.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*T*—一个用于推荐策略的变量，该策略选择订阅者的“为你精选”内容。为了简单起见，我们将考虑一个当前在生产中的策略，“+*t*”。我们将使用“–*t*”，表示“非*t*”，来表示替代策略。'
- en: '*B*—A variable for the recommendation policy that selects a subscriber’s “Because
    You Watched” content. Again, we’ll simplify this to a binary variable with policy
    “+*b*”, representing the policy in production, and all alternative policies “–*b*”,
    as in “not *b*.”'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*B*—一个用于推荐策略的变量，该策略选择订阅者的“因为你看了”内容。同样，我们将简化为具有策略“+*b*”的二进制变量，代表生产中的策略，以及所有替代策略“–*b*”，如“非*b*”。'
- en: '*V*—The amount of engagement that a subscriber has with the content recommended
    by “Because You Watched.”'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*V*—订阅者与“因为你看了”推荐内容互动的参与度。'
- en: '*W*—The amount of engagement that a subscriber has with the content recommended
    by “Top Picks for You.”'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*W*—订阅者与“为你精选”推荐内容互动的参与度。'
- en: '*A*—Attrition, meaning whether a subscriber eventually leaves Netflix.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*A*—流失，意味着订阅者最终是否离开Netflix。'
- en: '*C*—Subscriber context, meaning the type of subscriber (location, demographics,
    preferences, etc.) we are dealing with.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*C*—订阅者背景，意味着我们正在处理的订阅者类型（位置、人口统计、偏好等）。'
- en: Recommendation algorithms always take the profile of the subscriber into account,
    along with the viewership history, so subscriber profile *C* is a cause of both
    recommendation policy variables *T* and *B*.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐算法始终考虑订阅者的概况以及观看历史，因此订阅者概况 *C* 是推荐政策变量 *T* 和 *B* 的原因。
- en: In this section, we’ll use y0 to analyze this problem at various levels of the
    hierarchy. We’ll start by visualizing the graph.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用y0在层次结构的各个级别分析这个问题。我们将首先可视化图形。
- en: Listing 10.7 Plot the recommendation DAG
  id: totrans-297
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.7 绘制推荐DAG
- en: '[PRE6]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 Define variables for the model.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 为模型定义变量。'
- en: '#2 Create the graph.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 创建图。'
- en: '#3 Plot the graph.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 绘制图。'
- en: '![figure](../Images/CH10_F15_Ness.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH10_F15_Ness.png)'
- en: Figure 10.15 Causal DAG for the recommendation algorithm problem
  id: totrans-303
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.15 推荐算法问题的因果DAG
- en: This generates the DAG in figure 10.15\.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 这生成了图10.15中的DAG。
- en: As a preliminary investigation, you might look at the average treatment effect
    (ATE, a level 2 query) of the “Top Picks for You” content on attrition *E*(*A*[*T*][=+][*t*]
    – *A*[*T*][=–][*t*]). Given that attrition *A* has a binary outcome, we can write
    this as *P*(*A*[*T*][=+][*t*]=+*a*) – *P*(*A*[*T*][=–][*t*]=+*a*). Focusing on
    *P*(*A*[*T*][=–][*t*]=+*a*), we know right away that we can identify this via
    both the (level 2) backdoor and the front door. So let’s move on to an interesting
    (level 3) counterfactual query called *effect of treatment on the treated* (ETT).
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 作为初步调查，您可能需要查看“为您精选”内容对流失率 *E*(*A*[*T*][=+][*t*] – *A*[*T*][=–][*t*]) 的平均处理效果（ATE，二级查询）。鉴于流失率
    *A* 具有二元结果，我们可以将其表示为 *P*(*A*[*T*][=+][*t*]=+*a*) – *P*(*A*[*T*][=–][*t*]=+*a*)。专注于
    *P*(*A*[*T*][=–][*t*]=+*a*)，我们立即知道我们可以通过（二级）后门和前门来识别这一点。因此，让我们继续探讨一个有趣的（三级）反事实查询，称为*治疗对接受治疗者的效果*（ETT）。
- en: 10.7.1 Effect of treatment on the treated
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.7.1 治疗对接受治疗者的效果
- en: Recall that you get the ATE directly (without needing to identify and estimate
    a level 1 estimand) from a randomized experiment. Suppose you ran such an experiment
    on a cohort of users, and it showed a favorable ATE, such as that +*t* has a favorable
    impact on *W* and *A* relative to –*t*. So your team deploys the policy.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，您可以直接从随机实验中获得ATE（无需识别和估计一级估计量）。假设您在一组用户上运行了这样的实验，并显示了一个有利的ATE，例如，+*t*相对于-*t*对*W*和*A*有积极的影响。因此，您的团队部署了该政策。
- en: Suppose the +*t* policy works best with users who have watched a lot of movies
    and thus have more viewing data. For this reason, when the policy is deployed
    to production, such users are more likely to get assigned the policy. But since
    they are so highly engaged, they are unlikely to leave, regardless of whether
    they are assigned the +*t* or –*t* policy. We could have a situation where the
    +*t* policy looks effective in an experiment where people are assigned policies
    randomly, regardless of their level of engagement, but in production the assignment
    is biased to highly engaged people who are indifferent to the policy.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 假设+*t*政策对观看大量电影并因此拥有更多观看数据的用户效果最好。因此，当政策部署到生产环境中时，这类用户更有可能被分配该政策。但由于他们高度参与，无论他们被分配+*t*或-*t*政策，他们都不太可能离开。我们可能会出现这样的情况：在随机分配政策且不考虑参与程度的实验中，+*t*政策看起来有效，但在生产中分配是有偏见的，偏向于高度参与但对政策漠不关心的用户。
- en: The level 3 query that addresses this is a counterfactual version of the ATE
    called effect of treatment on the treated (ETT, or sometimes ATT, as in *average
    treatment effect on the treated*). We write this as counterfactual query *E*(*A*[*T*][=+][*t*]
    – *A*[*T*][=–][*t*]|*T*=+*t*), as in “for people who saw policy +*t*, how much
    more attrition do they have relative to what they would have if they had seen
    –*t*?”Decomposing for binary *A* as we did with the ATE, we can write this as
    *P*(*A*[*T*][=+][*t*]=+*a*|*T*=+*t*) – *P*(*A*[*T*][=–][*t*]=+*a*|*T*=+*t*). *P*(*A*[*T*][=+][*t*]=+*a*|*T*=+*t*)
    simplifies to *P*(*A*=+*a*|*T*=+*t*) by the law of consistency. So we can focus
    on the second term, *P*(*A*[*T*][=–][*t*]=+*a*|*T*=+*t*).
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: In this special case of binary *A*, we can identify the ETT using graphical
    identification (for non-binary *A*, more level 3 assumptions are needed). To do
    graphical identification for counterfactuals, we can use graphical identification
    algorithms with counterfactual graphs.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: 10.7.2 Identification over the counterfactual graph
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Y0 can derive an estimand for ETT using a graphical identification algorithm
    called “IDC*” (pronounced I-D-C-star).
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Graph ID algorithms, ID, IDC, ID*, IDC*, in y0
  id: totrans-313
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Some of the core graphical identification algorithms implemented in y0 are ID,
    ID*, IDC, and IDC*. ID identifies interventional (level 2) queries from a DAG
    and observational (level 1) data. ID* identifies counterfactual (level 3) queries
    from observational and experimental (level 1 and level 2) data. IDC and IDC* extend
    ID and ID* to work on queries that condition on evidence, such as ETT.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: The algorithms use the structure of the causal graph to recursively simplify
    the identification problem by removing irrelevant variables and decomposing the
    graph into c-component subgraphs. They apply the rules of do-calculus to reduce
    intervention terms, block confounding backdoor paths, and factorize the query
    into simpler subqueries. If no further simplification is possible due to the graph's
    structure, the algorithms return a 'non-identifiable' result.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: This chapter's code relies on Y0’s implementations of these algorithms, though
    Y0 implements other graphical identification algorithms as well.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.8 Identifying ETT with a graphical identification algorithm
  id: totrans-317
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '#1 Hypothetical outcome A [T=–t] = +a'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Factual condition T = +t'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 'This will produce a rather verbose level 2 estimand. We can then apply level
    2 graphical identification algorithms to get a level 1 estimand, which will simplify
    to the following:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch10-eqs-6x.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
- en: I’ll show a simple derivation in the next section.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: For now, the intuition is that we are applying graphical identification algorithms
    over something called a counterfactual graph. Up until now, our graph of choice
    for counterfactual reasoning was the parallel world graph. Indeed, we can have
    y0 make a parallel world graph for us.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.9 Plotting the parallel world graph with y0
  id: totrans-325
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '#1 The make_parallel_worlds_graph method takes an input DAG and sets of interventions.
    It constructs a new world for each set.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 make_parallel_worlds_graph方法接受一个输入DAG和干预集。它为每个集构建一个新的世界。'
- en: '#2 The helper function visualizes the graph in a familiar way.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 辅助函数以熟悉的方式可视化图表。'
- en: This graph differs slightly from the ones I’ve drawn because the algorithm applies
    the subscript for an intervention to every node in the world where the intervention
    occurred; the subscript indexes all the variables in a world. It’s up to us to
    reason that *C* from one world and *C*[+][*t*] from another must have the same
    outcomes, since *C*[+][*t*] is not affected by its world’s intervention do(*T*=+*t*).
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图与我绘制的图略有不同，因为算法将干预的下标应用于发生干预的世界的每个节点；下标索引世界中的所有变量。我们必须推理出从一个世界的*C*和从另一个世界的*C*[+][*t*]必须具有相同的结果，因为*C*[+][*t*]不受其世界干预do(*T*=+*t*)的影响。
- en: Now recall that the problem with the parallel world graph is that d-separation
    won’t work with it. For example, in figure 10.16, d-separation suggests that *C*
    and *C*[+][*t*] are conditionally independent given their common exogenous parent
    *N*[*C*], but we just articulated that *C* and C[+][*t*] must be the same; if
    C has a value, *C*[+][*t*] must have the same value, so they are perfectly dependent.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 现在回想一下，并行世界图的问题在于d分离无法与之一起工作。例如，在图10.16中，d分离表明在它们的共同外生父节点*N*[C*]的条件下，*C*和*C*[+][*t*]是条件独立的，但我们刚刚阐述*C*和C[+][*t*]必须是相同的；如果C有一个值，*C*[+][*t*]必须具有相同的值，因此它们是完全相关的。
- en: '![figure](../Images/CH10_F16_Ness.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH10_F16_Ness.png)'
- en: Figure 10.16 A parallel world graph drawn by y0 (and Graphviz). In this version
    of the parallel world graph, the subscripts indicate a world. For example, +*t*
    indicates the world where the intervention do(*T*=+*t*) is applied. To prevent
    confusion, the exogenous variables use superscripts instead of subscripts to indicate
    their child endogenous variables (e.g., *N**^C*is the parent of *C* (and *C*[+t]).
  id: totrans-332
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.16 由y0（和Graphviz）绘制的并行世界图。在这个并行世界图的版本中，下标表示一个世界。例如，+*t*表示应用干预do(*T*=+*t*)的世界。为了避免混淆，外生变量使用上标而不是下标来表示它们的子内生变量（例如，*N**^C*是*C*（和*C*[+t]）的父节点）。
- en: We can remedy this with the *counterfactual graph*. A counterfactual graph is
    created by using a parallel world graph and the counterfactual query to understand
    which nodes across worlds in the parallel world graph are equivalent, and then
    collapsing equivalent nodes into one. The resulting graph contains nodes across
    parallel worlds that are relevant to the events in the query. Unlike parallel
    world graphs, you can use d-separation to reason about counterfactual graphs.
    We can use y0 to create a counterfactual graph for events *A*[*T*][=–][*t*]=+*a*
    and *T*=+*t*.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用*反事实图*来解决这个问题。反事实图是通过使用并行世界图和反事实查询来理解并行世界图中不同世界的节点哪些是等价的，然后将等价节点合并为一个来创建的。结果图包含与查询中事件相关的并行世界中的节点。与并行世界图不同，你可以使用d分离来推理反事实图。我们可以使用y0为事件*A*[*T*][=–][*t*]=+*a*和*T*=+*t*创建一个反事实图。
- en: Listing 10.10 Listing 10.10 Counterfactual graph events *A**[T]*[=][–]*[t]*=+*a*
    and *T*=+*t*
  id: totrans-334
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.10 列表10.10 反事实图事件 *A**[T]*[=][–]*[t]*=+*a* 和 *T*=+*t*
- en: '[PRE9]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 Counterfactual graphs work with event outcomes in the query. For P(A [T=–t]=+a|T=+t),
    we want events A [T=–t] =+a and T=+t.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 反事实图与查询中的事件结果一起工作。对于P(A [T=–t]=+a|T=+t)，我们希望事件A [T=–t] =+a和T=+t。'
- en: This creates the counterfactual graph in figure 10.17.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 这就创建了图10.17中的反事实图。
- en: '![figure](../Images/CH10_F17_Ness.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH10_F17_Ness.png)'
- en: Figure 10.17 Counterfactual graph for events, produced by y0 (and Graphviz).
    *T**[–t]*corresponds to the intervention do(*T*=–*t*).
  id: totrans-339
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.17 由y0（和Graphviz）生成的事件反事实图。*T**[–t]*对应于干预do(*T*=–*t*)。
- en: At a high level, graphical identification algorithms in y0 do counterfactual
    identification by working with counterfactual graphs in lieu of conventional DAGs.
    First, it finds a level 2 estimand for a level 3 query. From there, you can use
    experimental data to answer the level 2 terms in the estimand, or you can attempt
    to further derive them to level 1 estimands from the level 2 terms.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，y0中的图形识别算法通过使用反事实图而不是传统的DAG来进行反事实识别。首先，它找到一个3级查询的2级估计量。从那里，你可以使用实验数据来回答估计量中的2级术语，或者你可以尝试从2级术语进一步推导出1级估计量。
- en: Graphs alone won’t work when you condition on outcome!
  id: totrans-341
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 当你对结果进行条件限制时，仅使用图表是不够的！
- en: Suppose that instead of the ETT term *P*(*A**[T]*[=–]*[t]*=+*a*|*T*=+*t*), you
    were interested in *P*(*A**[T]*[=–]*[t]* =+*a*|*T*=+*t*, *A*=+*a*), answering
    the question “Given a subscriber exposed to policy +*t* and later unsubscribed,
    would they still have unsubscribed had they not been exposed to that policy?”
    Or you could be interested in *E*(*A**[T]*[=–]*[t]* – *A**[T]*[=+]*[t]*|*T*=+*t*,
    *A*=+*a*) sometimes called *counterfactual regret*, which captures the amount
    the policy +*t* contributed to an unsubscribed individual’s decision to unsubscribe.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你感兴趣的不是 ETT 项 *P*(*A**[T]*[=–]*[t]*=+*a*|*T*=+*t*)，而是 *P*(*A**[T]*[=–]*[t]*
    =+*a*|*T*=+*t*, *A*=+*a*)，即回答问题：“给定一个接触到策略 +*t* 并后来取消订阅的订阅者，如果他们没有接触到该策略，他们还会取消订阅吗？”或者你可能对
    *E*(*A**[T]*[=–]*[t]* – *A**[T]*[=+]*[t]*|*T*=+*t*, *A*=+*a*) 感兴趣，这有时被称为 *反事实遗憾*，它捕捉了策略
    +*t* 对取消订阅的个人决定取消订阅的贡献量。
- en: '*P*(*A**[T]*[=–]*[t]*=+*a*|*T*=+*t*, *A*=+*a*) is an example of a query where
    the hypothetical outcomes and factual conditions are in conflict. In this case,
    the factual conditions contain an outcome for *A*, and the hypothetical condition
    contains an interventional outcome for *A*. The graphical counterfactual identification
    techniques mentioned in this section will not work for this type of query. Identification
    in this case requires additional level 3 assumptions.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*A**[T]*[=–]*[t]*=+*a*|*T*=+*t*, *A*=+*a*) 是一个假设结果和事实条件冲突的查询示例。在这种情况下，事实条件包含
    *A* 的一个结果，而假设条件包含 *A* 的干预结果。本节中提到的图形反事实识别技术不适用于此类查询。在这种情况下，识别需要额外的第三级假设。'
- en: This is unfortunate, because this type of counterfactual is precisely the kind
    of “how might things have turned out differently?” counterfactual questions that
    are the most interesting, and the most central to how humans reason and make decisions.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 这很不幸，因为这种反事实正是那种最有趣、最核心的“事情可能如何不同？”反事实问题，这些问题最有趣，也是人类推理和做决策的核心。
- en: 'We can also use graphical identification for more advanced queries. For example,
    suppose you want to isolate how *T* affects *A* from how *B* affects *A*. You
    want to focus on users where *B* was –*b*. You find the data from a past experiment
    where “Because you watched . . .” policy *B* was randomized. You take that data
    and zoom in on participants in the experiment who were assigned –*b*. The outcome
    of interest in that experiment was *V*, the amount of engagement with the content
    recommended in the “Because you Watched” box. So you have the outcomes of *V*[*B*][=–][*b*]
    for those subscribers of interest. With this new data, you expand your query from
    *P*(*A*[*T*][=–][*t*]=+*a*|*T*=+*t*) to *P*(*A*[*T*][=–][*t*]=+*a*|*T*=+*t*, *B*=–*b*,
    *V*[*B*][=–][*b*]=*v*), including *V*[*B*][=–][*b*]=*v* because it is helpful
    in predicting attrition. Now you have three parallel worlds to reason over: the
    actual world, the world with do(*T*=+*t*), and the world with do(*B*=–*b*).'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用图形识别来进行更高级的查询。例如，假设你想隔离 *T* 对 *A* 的影响与 *B* 对 *A* 的影响。你想要关注 *B* 为 –*b*
    的用户。你找到过去实验中的数据，其中“因为你观看了……”策略 *B* 是随机化的。你取那些数据，并聚焦于实验中被分配 –*b* 的参与者。该实验中感兴趣的结果是
    *V*，即“因为你观看了”框中推荐内容的参与量。因此，你有了对那些感兴趣订阅者的 *V*[*B*][=–][*b*] 的结果。有了这些新数据，你将查询从 *P*(*A*[*T*][=–][*t*]=+*a*|*T*=+*t*)
    扩展到 *P*(*A*[*T*][=–][*t*]=+*a*|*T*=+*t*, *B*=–*b*, *V*[*B*][=–][*b*]=*v*)，包括 *V*[*B*][=–][*b*]=*v*，因为它有助于预测流失。现在你有三个并行世界来推理：实际世界、do(*T*=+*t*)
    的世界和 do(*B*=–*b*) 的世界。
- en: Listing 10.11 Create a parallel world graph for do(*T*=+*t*) and do(*B*=–*b*)
  id: totrans-346
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.11 为 do(*T*=+*t*) 和 do(*B*=–*b*) 创建一个并行世界图
- en: '[PRE10]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#1 The second argument enumerates the hypothetical conditions.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 第二个参数列举了假设条件。'
- en: This code creates this three-world parallel world graph seen in figure 10.18.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码创建了图 10.18 中所示的三世界并行世界图。
- en: '![figure](../Images/CH10_F18_Ness.png)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH10_F18_Ness.png)'
- en: Figure 10.18 A parallel world graph with the actual world *T*=+*t* and hypothetical
    worlds do(*T*=–*t*) and do(*B*=–*b*). The dashed lines are edges from exogenous
    variables (dark gray).
  id: totrans-351
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 10.18 一个具有实际世界 *T*=+*t* 和假设世界 do(*T*=–*t*) 以及 do(*B*=–*b*) 的并行世界图。虚线是来自外生变量的边（深灰色）。
- en: Notably, the query *P*(*A*[*T*][=–][*t*]=+*a*|*T*=+*t*, *B*=–*b*, *V*[*B*][=–][*b*]=*v*)
    collapses the parallel world graph to the same counterfactual graph as *P*(*A*[*T*][=–][*t*]=+*a*|*T*=+*t*).
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，查询 *P*(*A*[*T*][=–][*t*]=+*a*|*T*=+*t*, *B*=–*b*, *V*[*B*][=–][*b*]=*v*)
    将并行世界图折叠成与 *P*(*A*[*T*][=–][*t*]=+*a*|*T*=+*t*) 相同的反事实图。
- en: Listing 10.12 Counterfactual graph for expanded expression
  id: totrans-353
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.12 扩展表达式的反事实图
- en: '[PRE11]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This gives us the counterfactual graph in figure 10.19, which is the same as
    the graph in figure 10.17.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F19_Ness.png)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
- en: Figure 10.19 The counterfactual graph for *P*(*A**[T]*[=–]*[t]*=+*a*|*T*=+*t*,
    *B*=–*b*, *V**[B]*[=–]*[b]*=*v*) is the same as for *P*(*A**[T]*[=–]*[t]*=+*a*|*T*=+*t*).
  id: totrans-357
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Next, we’ll look at another graph-based approach called single-world intervention
    graphs.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: 10.7.3 Counterfactual identification with single-world intervention graphs
  id: totrans-359
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Single-world intervention graphs (SWIGs) provide an alternative to counterfactual
    identification with counterfactual graphs. Like a counterfactual graph, we construct
    a SWIG using the original causal DAG and the causal query. We’ll use the Netflix
    recommendation example to construct a SWIG for the interventions do(*T*=–*t*)
    and do (*B*=–*b*). Let’s construct a SWIG from a causal DAG.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: Node-spitting operation
  id: totrans-361
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We have the intervention that targets do(*T*=+*t*), and we can implement it
    with a special kind of graph surgery called a *node-splitting operation*. We split
    a new node off the intervention target *T**, as i*n figure 10.20\. *T* still represents
    the same variable as in the original graph, but the new node represents a constant,
    the intervention value +*t*. *T* keeps its parents (in this case *C*) but loses
    its children (in this case *W*) to the new node.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: Subscript inheritance
  id: totrans-363
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Next, every node downstream of the new node inherits the new node’s values as
    a subscript. For example, in figure 10.21, *W* and *A* are downstream of the intervention,
    so the subscript *[T]*[= ][–]*[t]* is appended to these nodes, so they become
    *W*[*T*][=][-][*t*] and *A*[*T*][=][-][*t*].
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F20_Ness.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
- en: Figure 10.20 The intervention operator for a SWIG is the node-splitting operation.
  id: totrans-366
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F21_Ness.png)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
- en: Figure 10.21 Every node downstream of the intervention gets the intervention
    subscript.
  id: totrans-368
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Repeat for each intervention
  id: totrans-369
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We repeat this process for each intervention. In figure 10.22, we apply do(*B*=–*b*),
    and split *B* and we convert *V* to *V*[*B*][=–][*b*] and *A*[*T*][=-][*t*] to
    *A*[*T*][=–][*t*][,][*B*][=–][*b*].
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F22_Ness.png)'
  id: totrans-371
  prefs: []
  type: TYPE_IMG
- en: Figure 10.22 A node takes the subscript of all its upstream interventions.
  id: totrans-372
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Like the counterfactual graph, the SWIG contains counterfactual variables and
    admits d-separation. With these properties, we can do identification.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: 10.7.4 Identification with SWIGs
  id: totrans-374
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose we are interested in ETT and want to identify *P*(*A*[*T*][=][–][*t*]=+*a*|*T*=+*t*).
    We derive the SWIG in figure 10.23.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F23_Ness.png)'
  id: totrans-376
  prefs: []
  type: TYPE_IMG
- en: Figure 10.23 We can use the SWIG to derive ETT using the ignorability trick.
  id: totrans-377
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'With this graph, we can identify *P*(*A*[*T*][=–][*t*]=+*a*|*T*=+*t*) using
    the ignorability trick I introduced in section 10.4:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(*A*[*T*][=–][*t*]=+*a*|*T*=+*t*) = ∑[*c*] *P*(*A*[*T*][=–][*t*]=+*a*, *C*[*T*][=–][*t*]=*c*|*T*=+*t*)
    by the law of total probability.'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ∑[*c*] *P*(*A*[*T*][=][–][*t*]=+*a*, *C*[*T*][=–][*t*]=*c*|*T*=+*t*) = ∑[*c*]
    *P*(*A*[*T*][=–][*t*]=+*a*, *C* =*c*|*T*=+*t*), since *C* is not affected by interventions
    on *T*.
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ∑[*c*] *P*(*A*[*T*][=–][*t*]=+*a*, *C*=*c*|*T*=+*t*) factorizes into ∑[*c*]
    *P*(*A*[*T*][=–][*t*] =+*a*|*C*=*c*, *T*=+*t*) *P*(*C*=*c* | *T*=+*t*) by the
    chain rule of probability.
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*P*(*A*[*T*][=–][*t*]=+*a*|*C*=*c*, *T*=+*t*) = *P*(*A*[*T*][=–][*t*]=+*a*|*C*=*c*,
    *T*=–*t*), again by the ignorability trick.'
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: And as before, *P*(*A*[*T*][=–][*t*]=+*a*|*C*=*c*, *T*=–*t*) = *P*(*A* =+*a*|*C*=*c*,
    *T*=–*t*) by the law of consistency. Thus, *P*(*A*[*T*][=–][*t*]=+*a*|*T*=+*t*)
    = ∑[*c*] *P*(*A* =+*a*|*C*=*c*, *T*=–*t*) *P*(*C*=*c* | *T*=+*t*)
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The magic happens in the ignorability trick in step 4, where *C*’s d-separation
    of *A*[*T*][=–][*t*] and *T* lets us change *T*=+*t* to *T*=–*t*. Notice that
    the same d-separation exists in the counterfactual graph we derived for *P*(*A*[*T*][=–][*t*]=+*a*|*T*=+*t*),
    shown in figure 10.17\. The difference is that deriving the SWIG is easy, while
    deriving the counterfactual graph is nuanced, and one generally uses an algorithm
    like `make_counterfactual_grap` in y0.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: 10.7.5 The single-world assumption
  id: totrans-385
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The node-splitting operation relies on a new level 3 assumption. If you are
    going to node-split a variable *X*, then you are assuming it is possible to know
    what value *X* would naturally take without the intervention and that it would
    be possible for you intervene before it realized that value. Imagine in our Netflix
    example that, given a subscriber had profile *C*=*c*, the recommendation algorithm
    was about to assign the subscriber a policy +*t* for recommending “Top picks for
    you,” but before that policy went into effect, you intervened and artificially
    changed it to –*t*. It’s possible that the way you forced the policy to be –*t*
    had some side effects that changed the recommendation system in some fundamental
    way, such that in this new system, *T* would not have been *+t*, in the first
    place. With the single-world assumption, you assume you can know T’s natural value
    would have been *+t*, and that your intervention wouldn’t change the system in
    a way that would affect *T* taking that natural value. You are implicitly making
    this assumption when you reason with SWIGs.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: That assumption allows you to avoid the need to create additional worlds to
    reason over. You can condition on outcome *T*=+*t* and intervene do(*T*=–*t*)
    in a “single world.” You can also run experiments, where you apply the intervention
    do(*T*=–*t*) and test if *T* (where you know T’s “natural values”) is conditionally
    independent of *A*(*T*=–*t*) given *C*. This reduces the number of counterfactual
    queries you can answer, but proponents of SWIGs suggest this is a strength, because
    it limits you to counterfactuals that can be validated by experiments.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: Contrasting counterfactual graphs and SWIGs
  id: totrans-388
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Counterfactual graphs and SWIGs are similar in function, but they are distinctly
    different artifacts.*'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: '*Counterfactual graphs*—The counterfactual graph works by collapsing equivalent
    parallel world graph nodes over possible worlds. They only contain nodes relevant
    to the specific query. They are defined for binary events like {*T*=+*t*} and
    {*T*=–*t*}—this works well even with continuous variables, because counterfactual
    language typically compares one *hypothetical condition* to one *factual condition*
    (e.g., “We invested 1 million; what if we had invested {2/more/half/…}?”).'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Single-world intervention graphs (SWIGs) **—*The SWIG works by applying a
    node-splitting type of graph surgery. Unlike counterfactual graphs, they work
    with general variables (rather than just binary events) and are not query-specific
    (all original nodes are present). However, they rely on a single-world assumption—that
    it is possible to know with certainty what value a variable would have taken had
    it not been intervened upon.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The primary use case for both graphs is identification. Neither counterfactual
    graphs nor SWIGs enable identification from level 1 or 2 data of counterfactual
    queries such as *P*(*A**[T]*[=–]*[t]*=+*a*|*T*=+*t*, *A*=+*a*) where the same
    variable appears in the hypothetical outcome and the factual condition. But you
    can still *derive* the counterfactual graph for such queries; this is not true
    for SWIGs. That is useful if you want to reason about independence across worlds
    in cases of queries such as *P*(*A**[T]*[=–]*[t]*=+*a*|*T*=+*t*, *A*=+*a*).
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: 10.8 Identification and probabilistic inference
  id: totrans-393
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve seen that a core part of the identification task is deriving an estimand.
    How does that estimand mesh with a probabilistic machine learning approach?
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: Consider, for example, our online game model, where ETT = *E*(*I*[*E*][=“high”]
    – *I*[*E*][=“low”]|*E*=“high”) = *E*(*I*[*E*][=“high”]|*E*=“high”) – *E*(*I*[*E*][=“low”]|*E*=“high”).
    We need to identify *P*(*I*[*E*][=“high”]|*E*=“high”) and *P*(*I*[*E*][=“low”]|*E*=“high”).
    Recall that *P*(*I*[*E*][=“high”]|*E*=“high”) simplifies to the level 1 query
    *P*(*I*|*E*=“high”) by the law of consistency, so the challenge lies with identifying
    the counterfactual distribution *P*(*I*[*E*][=“low”]|*E*=“high”).
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: Using a probabilistic machine learning approach with Pyro, we know we can infer
    *P*(*I*|*E*=“high”) by using `pyro.condition` to condition on *E*=“high” and then
    running inference. The question is how we’ll infer the counterfactual distribution
    *P*(*I*[*E*][=“low”]|*E*=“high”).
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous section, we saw that we can identify this query with a SWIG
    (assuming the single-world assumption holds). We used the SWIG to derive the following
    estimand for *P*(*I*[*E*][=0]=*i*|*E*=“high”):'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch10-eqs-7x.png)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
- en: 'But what do we do with this estimand with respect to building a model in Pyro?
    We could construct two Pyro models, one for *P*(*G*|*E*) and one for *P*(*I*|*G*,
    *E*), infer *P*(*I*=*i*| *G*=*g*, *E*=“low”) and *P*(*G*=*g* | *E*=“high”) and
    then do the summation. But this is inelegant relative to our regular approach
    to probabilistic inference with a causal generative model:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: Implement the full causal generative model.
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train its parameters on data.
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the intervention operator to simulate an intervention.
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run an inference algorithm.
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this approach, we build one causal model—we don’t build separate models for
    the estimand's components *P*(*G*|*E*) and *P*(*I*|*G*, *E*). Nonetheless, our
    regular approach to probabilistic inference with a causal generative model does
    work if we have identification, given the causal assumptions we implement in step
    1 and the data we train on in step 2\. We don’t even need to know the estimand
    explicitly; it is enough to know it exists—in other words, that the query is identified
    (e.g., by using Y0’s `check_ identifiable` function). With identification, steps
    2–4 collectively become just another estimator for that estimand.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate, let’s consider how we’d sample from *P*(*I*[*E*][=“low”]|*E*=“high”)
    using a Pyro model of our online gaming example. For simplicity, let’s replace
    *E*=“high” and *E*=“low” with *E*=1 and *E*=0 respectively. We know *P*(*I*[*E*][=0]|*E*=1)
    is identified given our causal DAG and the single-word assumption. Fortunately,
    Pyro’s (and ChiRho’s) `do` intervention operator implements the SWIG’s node-splitting
    operation by default (if you used `pyro.render_model` to visualize an intervention
    and didn’t get what you expected, this is why). For ordinary interventional queries
    on a causal DAG, there is no difference between this and the ordinary graph surgery
    approach to interventions. But when we want to condition on *E*=1 and intervene
    to set *E*=0, Pyro will accommodate us. We’ll use this approach to sample from
    *P*(*I*[*E*][=0]|*E*=1). As a sanity check, we’ll also sample from the plain vanilla
    intervention distribution *P*(*I*[*E*][=0]) and contrast those samples with samples
    from *P*(*I*[*E*][=0]|*E*=1).
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: Setting up your environment
  id: totrans-406
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As a change in pace, I’ll illustrate this example using NumPyro instead of Pyro,
    though the code will work in Pyro with small tweaks. We’ll use NumPyro version
    0.15.0\. We’ll also use an inference library meant to complement NumPyro and Pyro
    called Funsor, version 0.4.5\. We’ll also use Matplotlib for plotting.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s build the model.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: NumPyro vs. Pyro
  id: totrans-409
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Pyro extends PyTorch, while NumPyro extends NumPy and automatic differentiation
    with JAX. The user interfaces are quite similar. If you are less comfortable with
    PyTorch abstractions and debugging PyTorch errors, or you prefer MCMC-based inference
    with the Bayesian programming patterns one uses in Stan or PyMC, then you might
    prefer NumPyro.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.13 Generating from *P*(*I**[E]*[=][0]) vs. *P*(*I**[E]*[=][0]|*E*=1)
    in Pyro
  id: totrans-411
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '#1 A version of the online gaming model. The weights are estimates from the
    data (learning procedure not shown here).'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll apply the intervention and run inference to sample from *P*(*I*[*E*][=][0]).
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.14 Apply intervention do(*E*=0) and infer from *P*(*I**[E]*[=][0])
  id: totrans-415
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '#1 Apply the do operator to the model.'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Apply inference to sample from P(I [E=0]).'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: We’ll contrast these samples from *P*(*I*[*E*][=][0]) with samples we’ll draw
    from *P*(*I**[E]*[=][0]|*E*=1). To infer *P*(*I**[E]*[=][0]|*E*=1), we’ll condition
    `intervention_model` on the factual condition *E*=1\. Then we’ll run inference
    again on this conditioned-upon intervened-upon model.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.15 Condition intervention model and infer *P*(*I**[E]*[=][0]|*E*=1)
  id: totrans-420
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#1 Now apply the condition operator to sample from P(I [E=0]|E=1).'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Apply inference to sample from P(I [E=0]|E=1).'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that Pyro’s `do`and `condition`subroutines mutually compose; i.e., for
    a model with a variable *X*, `do(condition(model,` `{"X": 1.}),` `{"X":` `0.})`
    is equivalent to `condition(do(model,` `{"X":` `0.}),` `{"X":` `1.})`.'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we’ll plot samples from *P*(*I**[E]*[=0]) and *P*(*I**[E]*[=0]|*E*=1)
    and evaluate the difference in these distributions.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.16 Plot samples from *P*(*I**[E]*[=][0]) and *P*(*I**[E]*[=][0]|*E*=1)
  id: totrans-426
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '#1 Plot a histogram of samples from P(I [E=0]).'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Plot a histogram of samples from P(I [E=0]|E=1).'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: This code generates the histograms in figure 10.24.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F24_Ness.png)'
  id: totrans-431
  prefs: []
  type: TYPE_IMG
- en: Figure 10.24 Histograms of samples from *P*(*I**[E]*[=0]) and *P*(*I**[E]*[=0]|*E*=1)
    generated in Pyro
  id: totrans-432
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In this example, the parameters were given. In chapter 11, where we’ll look
    at estimation, we’ll seamlessly combine this query inference with Bayesian parameter
    inference from data.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: 10.9 Partial identification
  id: totrans-434
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll close this chapter with a quick note on partial identification. Sometimes
    a query is not identified, given your assumptions, but it may be *partially identifiable*.
    Partial identifiability means you can identify estimands for an upper and lower
    bound of your query. Partial identification is highly relevant to causal AI because
    machine learning algorithms often rely on finding and optimizing bounds on objective
    functions. Let’s walk through a few examples.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: MCMC vs. SVI
  id: totrans-436
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Here we used Markov chain Monte Carlo (MCMC), but both Pyro and NumPyro have
    abstractions for stochastic variational inference (SVI). In this example, the
    parameters (`p_member`, `p_engaged`, etc.) of the model are specified. We could
    also make the parameters unknown variables with Bayesian priors and do the inference
    on these causal queries *P*(*I**[E]*[=0]) and *P*(*I**[E]*[=0]|*E*=1); in this
    case, we’d be doing Bayesian inference of these queries.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: But for this we’d need *N* IID samples from an observational distribution where
    we had graphical identification (*P*(*G*, *E*, *W*, *I*), *P*(*G*, *E*, *I*),
    or *P*(*E*, *W*, *I*)). In the case of *P*(*G*, *E*, *W*, *I*), where all the
    variables in the DAG are observed, the number of unknown variables is just the
    number of parameters. But in the latter two cases, of *P*(*G*, *E*, *I*) or *P*(*E*,
    *W*, *I*), where there is a latent *G*or *W* of size *N*, the number of unknowns
    grows with *N*. In this case, SVI will scale better with large *N*. We’ll see
    an example in chapter 11.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: Suppose in our online gaming example you ran an experiment where you randomly
    assigned players to a treatment or control group. Players in the treatment group
    are exposed to a policy that encourages more side-quest engagement. You reason
    that since you can’t actually force players to engage in side-quests, it’s better
    to have this randomized treatment/control variable as a parent of our side-quest
    engagement variable, as seen in the DAG in figure 10.25.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F25_Ness.png)'
  id: totrans-440
  prefs: []
  type: TYPE_IMG
- en: Figure 10.25 We don’t have identification for the ATE of *E* on *I* because
    *G* and *W* are unobserved. But we have partial identification given variable
    *A*, representing gamers’ assignments in a randomized experiment.
  id: totrans-441
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For this new variable *A*, let *A*=1 refer to the treatment group and *A*=0
    refer to the control group. We have this new variable *A*, and the average treatment
    effect of the policy on in-game purchases *E*(*I*[*A*][=1] – *I*[*A*][=0]) is
    an interesting query. But suppose we’re still ultimately interested in knowing
    the average treatment effect of side-quest engagement *itself* on purchases, i.e.,
    *E*(*I*[*E*][=“high”] – *I*[*E*][=“low”]).
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: If guild membership (*G*) were observed, we’d have identification through backdoor
    adjustment. If won items (*W*) were observed, we could use front-door adjustment.
    But suppose that in this scenario you observe neither. In this case, observing
    the side-quest group assignment variable would give you partial identification.
    Suppose that the in-game purchases variable was a binary 1 for “high” and 0 for
    “low” instead of a continuous value. Then the bounds on *E*(*I*[*E*][=“high”]
    – *I*[*E*][=“low”]) are
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch10-eqs-8x.png)'
  id: totrans-444
  prefs: []
  type: TYPE_IMG
- en: These bounds can be the next best thing to having full identification, especially
    if the bounds are tight. Alternatively, perhaps it is enough to know that the
    lower bound on the ATE for side-quest engagement is significantly greater than
    0.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, general bounds exist for common counterfactual queries, such as
    probabilities of causation. For example, suppose you wanted to know if high side-quest
    engagement was a necessary and sufficient condition of high in-game purchases.
    You can construct the following bounds on the probability of necessity and sufficiency
    (PNS):'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch10-eqs-9x.png)'
  id: totrans-447
  prefs: []
  type: TYPE_IMG
- en: These bounds consist of level 2 quantities like *P*(*I*[*E*][=e]=*i*), and you
    can go on to identify level 1 estimands if possible given your assumptions.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: Remember that partial identification bounds are highly specific to your causal
    assumptions (like the DAG) and the parameterization of the variables; for example,
    the preceding examples are specific to binary variables. See the chapter notes
    at [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    for links to papers that derived these bounds as well as bounds for other practical
    sets of assumptions.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-450
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The importance of causal identification has increased in the AI era as we seek
    to understand the causal inductive bias in deep learning architectures.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Libraries like y0 implement strategies for algorithmic identification.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The causal hierarchy is a three-tiered structure that categorizes the causal
    questions we pose, the models we develop, and the causal inferences we draw. These
    levels are association, intervention, and counterfactual.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Association-level reasoning addresses “what is” questions and models that answer
    these questions with basic statistical assumptions.
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interventional or counterfactual queries fall on their corresponding level of
    the hierarchy.
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Observational data falls on the associational level, and experimental data falls
    on the interventional level of the hierarchy. Counterfactual data arises in situations
    where the modeler can control a deterministic data generating process (DGP).
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Causal identification is the procedure of discerning when causal inferences
    can be drawn from experimental or observational data. It is done by determining
    if data at a lower level of the hierarchy can be used to infer a query at a higher
    level of the hierarchy.
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An example of a causal identification result is the backdoor formula, which
    equates intervention level query *P*(*Y*[*X*][=][*x*]) to association level quantity
    ∑[*z*]*P*(*Y*|*X*=*x*, *Z*=z)*P*(*Z*=z), where *Z* is a set of common causes.
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The causal hierarchy theorem shows how lower-level data is insufficient to infer
    a distribution at a higher level without higher-level modeling assumptions.
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The do-calculus has three rules that can be used for graph-based identification.
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A counterfactual graph is a DAG that includes variables across counterfactual
    worlds on one graph. Unlike the parallel world graph, it admits d-separation.
    We derive the counterfactual graph from the parallel world graph and the target
    query.
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graphical identification algorithms automate identification with graphs using
    rules such as the do-calculus.
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nonparametric identification is identification with non-graphical assumptions,
    such as assumptions about the functional relationships between variables in the
    model.
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ignorability assumption is that the causal variable and the potential outcomes
    are conditionally independent given confounders.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Effect of treatment on the treated (ETT) evaluates the effect of a cause on
    the subset of the population that was exposed to the cause.
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Single world intervention graphs (SWIGs) provide an intuitive alternative to
    counterfactual identification with do-calculus and counterfactual graphs. They
    are constructed by applying a node-splitting operation to the original causal
    DAG. SWIGs use a “single-world” assumption, which assumes it’s possible to know
    a variable’s natural value while also intervening on it before it realizes that
    value without any side-effects that would affect that natural value.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SWIGs work with variables and a narrow set of counterfactuals under the single-world
    assumption, while counterfactual graphs can accommodate queries that cannot be
    graphically identified.
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pyro implements the SWIG’s node-splitting model of intervention, which enables
    probabilistic inference of SWIG-identified quantities.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inference of causal queries using a causal graphical model and probabilistic
    inference algorithms is possible as long as the query is identified, given the
    model’s assumptions and training data.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Partial identification means you can at least identify estimands for bounds
    on a target query. This can be quite useful if you lack full identification, especially
    since machine learning often works by optimizing bounds on objective functions.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
