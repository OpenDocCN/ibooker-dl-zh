- en: 10 Identification and the causal hierarchy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Motivating examples for identification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using y0 for identification and deriving estimands
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to derive counterfactual graphs in y0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deriving SWIGs for graph-based counterfactual identification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The practice of advancing machine learning often relies on a blind confidence
    that more data and the right architecture can solve any task. For tasks with causal
    elements, *causal identification* can make that less of a matter of faith and
    more of a science. It can tell us when more data won’t help, and what types of
    inductive biases are needed for the algorithm to work.
  prefs: []
  type: TYPE_NORMAL
- en: Causal identification is the task of determining when we can make a causal inference
    from purely observational data or a counterfactual inference from observational
    or experimental data. In statistics and data science, it is the theory that allows
    us to distill causation from correlation and estimate causal effects in the presence
    of confounders. But causal identification has applications in AI. For example,
    suppose a deep learning algorithm achieves high performance on a particular causal
    reasoning benchmark. The ideas behind causal identification tell us that certain
    causal inductive biases must be baked into the model architecture, training data,
    training procedure, hyperparameters (e.g., prompts), and/or benchmark data. By
    tracking down that causal information, we can make sure the algorithm can consistently
    achieve that benchmark performance in new scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Identification is a theory-heavy part of causal inference. Fortunately, we can
    rely on libraries to do the theoretical heavy lifting for us and focus on skill-building
    with these libraries. In this chapter, we’ll focus on a library called y0 (pronounced
    why-not), which implements algorithms for identification using graphs. By the
    end of the chapter, we’ll have demystified causal identification and you’ll know
    how to apply y0’s identification algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1 The causal hierarchy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *causal hierarchy*, also known as *Pearl’s hierarchy* or the *ladder of
    causation*, is a three-level hierarchy over the types of causal questions we ask,
    models we build, data we acquire, and causal inferences we make.
  prefs: []
  type: TYPE_NORMAL
- en: 'The causal hierarchy consists of three levels:'
  prefs: []
  type: TYPE_NORMAL
- en: Association
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Intervention
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Counterfactual
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When we do a statistical or causal analysis, we are reasoning at one of these
    three levels. When we know at what level we are reasoning, we can determine what
    kind of assumptions and data we need to rely on to do that reasoning correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.1 Where questions and queries fall on the hierarchy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The questions we ask of our causal model, and the causal queries we formalize
    from those questions, fall at different levels of the hierarchy. First, level
    1 (the association level) is concerned with “What is…?” questions. Let’s illustrate
    with the online gaming example, shown again in figure 10.1.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F01_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 The DAG for the online gaming example
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: An example level 1 question and associated query is
  prefs: []
  type: TYPE_NORMAL
- en: “What are in-game purchase amounts for players highly engaged in side-quests?”
    *P* ( *I*| *E*=“high”)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Reasoning at this level aims to describe, model, or detect dependence between
    variables. At this level, we’re not reasoning about any causal relationships between
    the variables.
  prefs: []
  type: TYPE_NORMAL
- en: Questions at level 2 (the intervention level) involve non-counterfactual hypothetical
    conditions, such as
  prefs: []
  type: TYPE_NORMAL
- en: “What would in-game purchases be for a player if side-quest engagement were
    high?” *P* ( *I*  [*E*] [=“high”])
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: At level 2, we formalize such questions with the ideal intervention. Note that
    any query derived from a level 2 query is also a level 2 query, such as ATEs,
    (e.g., *E*(*I*[*E*][=“high”] – *I*[*E*][=“low”])) and CATEs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, counterfactual questions and queries fall at level 3 (the counterfactual
    level):'
  prefs: []
  type: TYPE_NORMAL
- en: “Given this player had low side-quest engagement and low purchases, what would
    their level of purchases have been if they were more engaged?” *P*( *I* [*E*]
    [=“high”]| *E*=“low”, *I*=“low”)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As with level 2 queries, any query we derive from a level 3 query also falls
    at level 3\. For example, a causal attribution query designed to answer “Why did
    this player have low purchases” would be a level 3 query if it were a function
    of level 3 queries like the probabilities of causation described in section 8.3.
  prefs: []
  type: TYPE_NORMAL
- en: In identification, we work directly with queries. The y0 library in Python gives
    us a domain specific language for representing queries. The following code implements
    the query *P*(*I*[*E*][=][*e*]).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.1 Creating a query in y0
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#1 “P” is for probability distributions, and “Variable” is for defining. variables.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Define variables G (guild membership), E (side-quest engagement), and I
    (in-game purchases).'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Define the distributional query P(I [E]).'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 If running in a notebook environment, this will show a rendered image of
    P(I [E]).'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up your environment
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In this chapter, I rely on version 0.2.0 of the y0 library. As it is a relatively
    new library, the library’s API is in development and recent versions will deviate
    slightly from what is shown here. Check out the library’s tutorials for recent
    developments.
  prefs: []
  type: TYPE_NORMAL
- en: Again, we rely on Graphviz and some custom utilities for plotting DAGs. The
    Graphviz installation depends on your environment. I am using Ubuntu 22.04 and
    install Graphviz via libgraphviz-dev. Then I install Python libraries graphviz
    version 0.20.3, and PyGraphviz version 1.13\. The Graphviz code is for plotting
    only, so if you get stuck, you could forgo plotting for the rest of the code.
  prefs: []
  type: TYPE_NORMAL
- en: The `query` object is an object of the class `Probability`. The class’s `__repr__`
    method (which tells Python what to return in the terminal when you call it directly)
    is implemented such that when we evaluate the object in the last line of the preceding
    code in a Jupyter notebook, it will display rendered LaTeX (a typesetting/markup
    language with a focus on math notation), as in figure 10.2.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F02_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 The rendered math image returned when you evaluate the `query` object
    in listing 10.1
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The causal hierarchy applies to models and data as well.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.2 Where models and assumptions fall on the hierarchy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A “model” is a set of assumptions about the data generating process (DGP). Those
    assumptions live at various levels of the hierarchy.
  prefs: []
  type: TYPE_NORMAL
- en: Level 1 assumptions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Models at the associational level have statistical but non-causal assumptions.
    For example, suppose we’re interested in *P*(*I*|*E*=*e*), for either value (“low”,
    “high”) that *e* might take. We might fit a linear model to regress in-game purchases
    *I* against side-quest engagement *E*. Or we might train a neural network that
    maps *E* to *I*. These are two statistical models with different parameterizations.
    In other words, they differ in non-causal, statistical assumptions placed on *P*(*I*|*E*).
    Once we add causal assumptions, we move to a higher level of the hierarchy.
  prefs: []
  type: TYPE_NORMAL
- en: Level 2 assumptions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Assumptions that we can represent with a causal DAG are level 2 (interventional)
    assumptions. An example of a level 2 model would be a causal graphical model (aka,
    a causal Bayesian network)—a probabilistic model trained on a causal DAG. A causal
    DAG by itself is a level 2 set of assumptions; assumptions about what causes what.
    Generally, assumptions that let you deduce the consequences of an intervention
    are level 2 assumptions.
  prefs: []
  type: TYPE_NORMAL
- en: Level 3 Assumptions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The canonical example of a level 3 model is a structural causal model. But more
    generally, assumptions about mechanism—*how* variables affect one another—are
    level 3 (counterfactual) assumptions.
  prefs: []
  type: TYPE_NORMAL
- en: One way to think about this is that any causal assumption you cannot represent
    in the structure of the DAG is, by process of elimination, a level 3 assumption.
    For example, suppose your DAG has the edge *X*→*Y*. Further, you believe the causal
    relationship between *X* and *Y* is naturally linear. You can’t “see” linearity
    on the DAG structure, so linearity is a level 3 assumption.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.3 Where data falls on the hierarchy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recall the differences between observational data and interventional data. Observational
    data is passively observed; as a result, it captures statistical associations
    resulting from dependence between variables in the DGP.
  prefs: []
  type: TYPE_NORMAL
- en: Level 1 data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In our online gaming example, the level 1 data was logged examples of side-quest
    engagement and in-game purchases pulled by a database query. Observational data
    lives at level 1 of the causal hierarchy.
  prefs: []
  type: TYPE_NORMAL
- en: Level 2 data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Interventional data is generated as the result of applying an intervention,
    such as data collected from a randomized experiment. In the gaming example, this
    was the data created because of an A/B test that randomly assigned players to
    different groups where they are coerced into different fixed side-quest engagement
    levels. Intervention data lives at level 2 of the hierarchy.
  prefs: []
  type: TYPE_NORMAL
- en: Level 3 data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Counterfactual data, which lives at level 3 of the hierarchy, is the odd case.
    Counterfactual data would contain data from across possible worlds. In most domains,
    we only have data from one world—one *potential outcome* for each unit of observation
    in the data.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are special cases where counterfactual data exists. For example,
    cloud service providers use complex but deterministic policies for allocating
    resources in the cloud, given various constraints. For one example with a given
    allocation outcome in the log, we could generate a counterfactual outcome for
    that example by applying a different allocation policy to that example. Similarly,
    given data produced by simulation software, we could generate counterfactual data
    by changing the simulation to reflect a *hypothetical condition* and then rerunning
    it with the same initial conditions as the original data.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.4 The causal hierarchy theorem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The causal hierarchy offers us a key insight from something called the *causal
    hierarchy theorem*. That insight is this: “You cannot answer a level *k* question
    without level *k* assumptions.” For example, if you want a causal effect, you
    need a DAG or some other level 2 (or level 3) assumptions. If you want to answer
    a counterfactual question, you need level 3 assumptions. And even the most cutting-edge
    of deep learning models can’t answer level *k* questions reliably unless they
    encode a representation of level *k* assumptions.'
  prefs: []
  type: TYPE_NORMAL
- en: More formally, the causal hierarchy theorem establishes that the three layers
    of the causal hierarchy are, in mathematical jargon, “almost always separate.”
    Roughly speaking, “separate” means that data from a lower level of the hierarchy
    is insufficient to infer a query from a higher level of the hierarchy. And “almost
    always” means this statement is true except in cases so rare that we can dismiss
    them as practically unimportant.
  prefs: []
  type: TYPE_NORMAL
- en: Aside from this insight, the causal hierarchy makes understanding identification—perhaps
    the hardest topic in all causal inference—much easier, as we’ll see in the rest
    of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2 Identification and the causal inference workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we’ll look at the workflow for posing and answering causal
    questions and the role that identification plays in that workflow. We’ll use the
    online gaming DAG introduced in chapter 7 as an example. Let’s start by building
    the DAG with y0.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.2 Building the online gaming DAG in y0
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Install Graphviz for DAG visualization. Download some helper functions for
    identification and visualization that convert some y0 abstractions into abstractions
    we’re familiar with.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Inspect the downloaded code before executing as a matter of good security
    practice. Then uncomment the last line and execute.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 y0 works with a custom graph class called NxMixedGraph. To avoid confusion,
    we’ll call it a Y0Graph and use it to implement DAGs.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Build the graph.'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Draw the graph with a Graphviz helper function.'
  prefs: []
  type: TYPE_NORMAL
- en: This produces the graph in figure 10.3.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F03_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 Drawing the online gaming graph with y0
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Our goal in chapter 7 was to use our model of *P*(*G*, *E*, *I*) to simulate
    from *P*(*I*[*E*][=“high”]) using the intervention operator. In sections 7.1.6
    and 7.2.6, we did this simulation and saw empirical evidence that it works for
    this online game example. Identification means showing that it works in general,
    based on your model and assumptions. Formally, we want to be sure that level 1
    distribution *P*(*G*, *E*, *I*), or data from that distribution, combined with
    our DAG,
  prefs: []
  type: TYPE_NORMAL
- en: is enough to simulate from level 2 distribution *P*(*I*[*E*][=“high”]). Identification
    with y0 confirms that this is indeed possible.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.3 Checking identification of *P*(*I**[E]*[=“high”])from *P*(*G*,
    *E*,*I*)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Make a lowercase “e” to represent an intervention value.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Check identifiability given the DAG, a distribution, and a target query.
    Y0 represents ideal interventions with @, so we write P(I [E=e] as P(I @ e).'
  prefs: []
  type: TYPE_NORMAL
- en: This will return `True`, but what if we didn’t have any observations of guild
    membership *G*? We can use y0 to test if we have identification for *P*(*I*[*E*][=“high”])
    from *P*(*E*, *I*). In other words, test if it is possible to infer *P*(*I*[*E*][=“high”])
    from observations of *E* and *I* only.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.4 Checking identification of *P*(*I**[E]*[=“high”])from *P*(*E*,*I*)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This will return `False`, because we don’t have identification for *P*(*I*[*E*][=][*e*])
    from the DAG and *P*(*E*, *I*) given our graphical assumptions.
  prefs: []
  type: TYPE_NORMAL
- en: Lack of identification and misguided probabilistic ML
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Y0 shows us that *P*(*I**[E]*[=]*[e]*) is not identified from *P*(*E*, *I*)
    given our online game DAG. Consider the implications of this result from the perspective
    of probabilistic machine learning (ML). As experts in probabilistic ML, given
    *G* is unmeasured, we might be inclined to train a latent variable model on *P*(*E*,
    *I*) where *G* is the latent variable. Once we’ve learned that model, we could
    implement the intervention with graph surgery setting *E*=*e*, and then sampling
    *I* from the transformed model.
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm would *run*; it would generate samples. But the lack of identification
    result from y0 proves that, given only the assumptions in our DAG, we could not
    consider these to be valid samples from *P*(*I**[E]*[=]*[e]*). And training on
    more data wouldn’t help. The only way this could work is if there were additional
    causal assumptions constraining inference beyond the assumptions encoded by the
    DAG.
  prefs: []
  type: TYPE_NORMAL
- en: Given this introduction, let’s define identification.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.1 Defining identification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose I were to randomly choose a pair of numbers, *X* and *Y*, and add them
    together to get *Z*. Then, I tell you what *Z* was and ask you to infer the values
    of *X* and *Y*. Could you do it? Not without more information. So, what if I gave
    you millions of examples of feature *Z* and label {*X*, *Y*}. Could you train
    a deep learning model to predict label {*X*, *Y*} from input feature *Z*? Again,
    no, at least not without strong assumptions on the possible values of {*X*, *Y*}.
    What if, instead of millions, I gave you billions of examples? No; more data would
    not help. In statistics, we would say the prediction target {*X*, *Y*} is not
    *identified*.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, you want to infer something, and you have an algorithm (e.g.,
    a deep net) that takes in data and produces an answer. That answer will usually
    be a bit different than the true value because of statistical variation in the
    input data. If your inference objective is identified, then the more data you
    input to the algorithm, the more that variance will shrink and your algorithm’s
    answer will converge to the true answer. If your inference objective is not identified,
    then more data will not reduce your algorithm’s errors.
  prefs: []
  type: TYPE_NORMAL
- en: '*Causal identification* is just statistical identification across levels of
    the causal hierarchy. A causal query is identified when your causal assumptions
    enable you to infer that query using data from a lower level on the hierarchy.'
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.2 The causal inference workflow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have defined identification, we can define a full workflow for causal
    inference. Figure 10.4 shows the full workflow.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F04_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 The causal inference workflow. The identification step is an essential
    step in the workflow.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Identification is a key step in the workflow. Let’s walk through each of the
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Pose your query'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: First, we pose our causal question as a query. For example, given our question
    “What would in-game purchases be for a player if side-quest engagement was high?”
    our query is *P*(*I*[*E*][=“high”]).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F05_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.5 Step 2: Build the model to capture your causal assumptions relative
    to your query. For the query *P*(*I**[E]*[=“high”]), this is our online gaming
    DAG.'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Step 2: Build your model'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Next, build a causal model that captures your basic causal assumptions. Our
    model will be the online game causal DAG, shown again in figure 10.5.
  prefs: []
  type: TYPE_NORMAL
- en: Your model’s assumptions should at least match the level of your query in the
    causal hierarchy. For example, the query *P*(*I*[*E*][=“high”]) is a level 2 query,
    so we need at least some level 2 assumptions. The causal DAG is a level 2 causal
    model, so in our analysis, the DAG provides the necessary level 2 assumptions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Check identification'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Evaluate whether you have identification for your query, given your model assumptions
    and your available data. If you don’t have identification, you must either observe
    additional variables in your data or change your assumptions. For example, we
    could modify our online gaming DAG (changing level 2 assumptions). Or simply stop
    and conclude you can’t answer the question given your data and knowledge about
    the problem, and devote your attention elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Estimate your query'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Once you know you have identification for your query, you can run statistical
    inference on, or “estimate,” your query. There are a variety of estimation methods
    and algorithms, from Bayesian inference to linear regression to propensity scores
    to double machine learning. We’ll review some estimation methods in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 5: Refute your causal inference'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Refutation is a final step where we conduct sensitivity analysis to evaluate
    how sensitive our results from step 4 are to violations of our assumptions, including
    the assumptions that enabled identification. We’ll see examples of this in chapter
    11\.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.3 Separating identification and estimation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In many texts, identification and estimation are combined in one step by matching
    the estimators and practical scenarios where those estimators will work. In this
    book, we’ll highlight the separation of identification and estimation for several
    reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: The separation lets us shunt all the causal considerations into the identification
    step. This helps us be explicit about what causal assumptions we are relying on
    for estimation to work and builds intuition for when our analysis might fail.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The estimation step thus simplifies to purely statistical questions, where we
    consider the usual statistical trade-offs (bias vs. variance, uncertainty quantification,
    how well it scales, etc.).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The separation also allows us to handle estimation with the automatic differentiation
    capabilities that power cutting-edge deep learning libraries without worrying
    whether these learning procedures will get the causality wrong.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we’ll dive into the most common identification strategy: backdoor adjustment.'
  prefs: []
  type: TYPE_NORMAL
- en: 10.3 Identification with backdoor adjustment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Suppose we want to determine the causal effect of engagement on in-game purchases,
    i.e., *E*(*I*[*E*][=][“high”] – *I*[*E*][=][“low”]). We can derive this expectation
    from the query *E*(*I*[*E*][=][*e*]=*i*), so we focus on *P*(*I*[*E*][=][*e*]=*i*).
    We can use the online gaming DAG to prove the following is true:'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll see how to derive this equation in the next section. The right side of
    this equation is a level 1 quantity called an *estimand* that we can derive from
    the joint distribution *P*(*I*, *E*, *G*).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch10-eqs-0x.png)'
  prefs: []
  type: TYPE_IMG
- en: Queries, estimands, and estimators
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In statistics, the *estimand* is the thing the statistical algorithm (the *estimator*)
    estimates. The task of identification is finding (identifying) an estimand for
    your query. In terms of the causal hierarchy, causal identification is about finding
    a lower-level estimand for a higher-level query.
  prefs: []
  type: TYPE_NORMAL
- en: In the online gaming backdoor identification example, *P*(*I*[E=“high”]=i) is
    a level 2 query, and ∑*[g]**P*(*I*=*i*|*E*=“high”, *G*=*g*)*P*(*G*=*g*) is the
    level 1 estimand called the *backdoor adjustment estimand*. Backdoor adjustment
    is an operation we apply to *P*(*E*, *I*, *G*), where we sum out (or integrate
    out in the continuous case) the common cause *G*. In some cases, we’ll see we
    don’t need to know the estimand explicitly, only that it exists.
  prefs: []
  type: TYPE_NORMAL
- en: We passed our DAG and the intervention-level query *P*(*I*[*E*][=][“high”])
    to y0, and it told us it identified an estimand, an operation applied to *P*(*E*,
    *I*, *G*) that is equivalent to *P*(*I*[*E*][=][“high”]). Let’s have y0 display
    that estimand.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.5 Deriving the estimand to get *P*(*I**[E]*[=“high”]) from *P*(*E*,
    *I*, *G*)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This returns the expression in figure 10.6.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch10-eqs-1x.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 Output of y0’s identify function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In our notation, this is ∑[*g*]*P*(*I*=*i*|*E*=“high”, *G*=*g*) ∑*[ε]*[,]*[i]**P*(*E*=*ε*,
    *G*=*g*, *I*=*i*), which simplifies to ∑[*g*]*P*(*I*=*i*|*E*=“high”, *G*=*g*)
    *P*(*G*=*g*). This is the *backdoor adjustment estimand*. We’ll see at a high
    level how y0 derives this estimand. But first, let’s look a bit more closely at
    this estimand.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.1 The backdoor adjustment formula
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In general terms, suppose *X* is a cause of *Y*, and we are interested in the
    intervention-level query *P*(*Y*[*X*][=][*x*]). In that case, the *backdoor adjustment
    estimand* is ∑*[g]**P*(*X*=*x*, *Z*=*z*) *P*(*Z*=*z*). The *backdoor adjustment
    formula* equates the causal query *P*(*X**[X]*[=]*[x]*) with its estimand:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch10-eqs-12x.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *Z* is a set of variables called the adjustment set. The summation is
    shorthand for summation and integration—you sum over discrete variables in the
    adjustment set and integrate over continuous variables. The adjustment set is
    defined as fa set of variables that satisfies the *backdoor criterion*—(1) the
    set collectively *d*-separates all *backdoor paths* from *X* to *Y*, and (2) it
    contains no descendants of *X*.
  prefs: []
  type: TYPE_NORMAL
- en: To understand why we want to d-separate backdoor paths between *X* and *Y*,
    consider again our DAG for our online gaming example in figure 10.7.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F07_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 The online gaming DAG
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: What is the difference between *P*(*I*|*E*=“high”) and *P*(*I*[*E*][=“high”])?
    Consider the two paths between *E* and *I* in figure 10.8\. In the case of *P*(*I*|*E*=“high”),
    observing *E*=“high” gives us information about *I* by way of its direct causal
    impact on *I*, i.e., through path *E*→*I*. But observing *E*=“high” also gives
    us information about *G*, and subsequently about *I* through the *backdoor path*
    *E*←*G*→*I*. A *backdoor path* between two variables is a *d*-connected path between
    a common cause. In the case of *P*(*I**[E]*[=“high”]), we only want the impact
    on *I* through the direct path *E*→*I*.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F08_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.8 *E*←*G*→*I* is a backdoor path where *G* is a “confounder” that
    is a common cause of *E* and *I*. We are interested in the statistical signal
    flowing along the causal path from *E* to *I*, but that signal is “confounded”
    by the noncausal noise from additional statistical information through *G* on
    the backdoor path *E*←*G*→*I*.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We call *G* a *confounder*, because the statistical “signal” flowing along the
    causal path from *E* to *I* is “confounded” by the noncausal “noise” from additional
    statistical information through *G* on the *backdoor path* *E*←*G*→*I*. To address
    this problem, we seek to d-separate this backdoor path by blocking on *G*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We want to identify a backdoor estimand for the query *P*(*I*[*E*][=“high”]).
    So we substitute *I* for *Y*, and *E* for *X* in the backdoor adjustment formula.
    *G* blocks the backdoor path *E* *G* *I*, so the set *G* becomes our adjustment
    set:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(*I*[*E*][=][*e*]=*i*) = ∑[*g*]*P*(*I*=*i*|*E*=*e*, *G*=*g*)*P*(*G*=*g*)'
  prefs: []
  type: TYPE_NORMAL
- en: The backdoor adjustment formula d-separates the backdoor paths by summing out/integrating
    over, or in other words, “adjusting for” the backdoor statistical signal, leaving
    only the signal derived from the direct causal relationship.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  Some texts refer to the G-formula instead of backdoor adjustment formula.
    The backdoor adjustment formula is just the G-formula where the adjustment set
    is defined in terms of the backdoor criterion.
  prefs: []
  type: TYPE_NORMAL
- en: While an adjustment set can include non-confounders, in practice, excluding
    all but a minimal set of backdoor-blocking confounders cuts down on complexity
    and statistical variation. We dive into the statistical considerations of backdoor
    adjustment in chapter 11.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.2 Demystifying the back door
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So where does the backdoor adjustment estimand come from? Let’s consider our
    online gaming example again. The query is *P*(*I*[*E*][=][*e*]) where *e* is “high”
    or “low.” In counterfactual terms, let’s consider two possible worlds, one with
    our original DAG, and one where we apply the intervention to side-quest engagement
    (*E*). Let’s view the parallel world graph in figure 10.9.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F09_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.9 We have two parallel worlds: world A where *E* is not intervened
    upon, and world B where *E* is intervened upon.'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If you squint hard enough at this graph, you’ll notice that it implies that
    *E* is conditionally independent from *I*[*E*][=][*e*] given *G*. We’ll use some
    d-separation–based reasoning to see this. Remember that, in general, we can’t
    use d-separation to reason across worlds on a parallel world graph because the
    d-separation rules don’t account for nodes that are equivalent across worlds (like
    *G*). But we’ll use a trick where we reason about conditional independence between
    *E* and *I*[*E*][=][*e*] by looking at a d-connected path from *E* to *G* in world
    A, and then extend that d-connected path *from* the equivalent *G* in world B
    to *I*[*E*][=][*e*].
  prefs: []
  type: TYPE_NORMAL
- en: First, consider that paths from *E* in world A to world B have to cross one
    of two bridges between worlds, *N*[*G*] and *N*[*I*]. But the two paths to *N*[*I*]
    (*E* → *I* ← *N*[*I*], *E* ← *G* → *I* ← *N*[*I*]) are both d-separated due to
    the collider on *I*.
  prefs: []
  type: TYPE_NORMAL
- en: So we have one d-connected path to world B (*E* ← *G* ← *N*[*G*]). Now suppose
    we look at *G* in world B; from world B’s *G*, it is one step to *I*[*E*][=][*e*].
    But we know that, by the law of consistency, the value of *G* in both worlds must
    be the same; both *G*s are the same deterministic function of *N*[*G*], and neither
    *G* is affected by an intervention. So, for convenience, we’ll collapse the two
    *G*s into one node in the parallel world graph (figure 10.10). Looking now at
    the path *E* ← *G* → *I*[*E*][=][*e*], we can see this path is d-separated by
    *G*. Hence, we can conclude *E* ⊥ *I*[*E*][=][*e*] | *G*.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F10_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.10 Collapsing *G* across worlds reveals *G* d-separates *E* and *I**[E]*[=]*[e]*.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In causal inference jargon, this simplification is called *ignorability*. *Ignorability*
    means the causal variable *E* and the counterfactual potential outcomes like *I*[*E*][=][*e*]
    are conditionally independent given confounders. Ignorability is a common assumption
    made in causal inference. We can use this ignorability assumption in deriving
    the backdoor estimand.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we start, let’s recall a key definitional fact of conditional independence:
    if two variables *U* and *V* are conditionally independent given *Z*, then *P*(*U*|*Z*=*z*,
    *V*=*v*) = *P*(*U*|*Z*=*z*). Flipping that around, *P*(*U*|*Z*=*z*) = *P*(*U*|*Z*=*z*,
    *V*=*v*). In other words, *P*(*U*|*Z*=*z*) = *P*(*U*|*Z*=*z*, *V*=“apples”) =
    *P*(*U*|*Z*=*z*, *V*=“oranges”); it doesn’t matter what value *V* takes because,
    since *Z* rendered it independent from *U*, its value has no bearing on *U*. Introducing
    *V* and giving it whatever value we want is the trick that makes the derivation
    work. Also, recall the *law of total probability* says that we can marginalize
    a variable out of a joint distribution by summing (or integrating) over that variable,
    as in *P*(*U*=*u*) = ∑[*v*]*P*(*U*=*u*, *V*=*v*). The same is true when the joint
    distribution is subject to intervention, as in *P*(*U**[W]*[=]*[w]*=*u*) = ∑[*v*]*P*(*U**[W]*[=]*[w]*=*u*,
    *V**[W]*[=]*[w]*=*v*).'
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s start with the causal query *P*(*I*[*E*][=][*e*]) and see how to equate
    it with the backdoor estimand ∑[*g*] *P*(*I*|*E*=*e*, *G*=*g*)*P*(*G*=*g*).
  prefs: []
  type: TYPE_NORMAL
- en: For some value of in-game purchases *i*, *P*(*I*[*E*][=][*e*]=*i*) = ∑[*g*]
    *P*(*I*[*E*][=][*e*]=*i*, *G*[*E*][=][*e*]=*g*) by the law of total probability.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ∑[*g*] *P*(*I*[*E*][=][*e*]=*i*, *G*[*E*][=][*e*]=*g*) = ∑[*g*] *P*(*I*[*E*][=][*e*]=*i*,
    *G*=*g*), because we know from our original DAG that *G* is not affected by the
    intervention on *E*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next we use the chain rule to factorize *P*(*I*[*E*][=][*e*]=*i*, *G*=*g*):
    ∑[*g*] *P*(*I*[*E*][=][*e*]=*i*, *G*=*g*) = ∑[*g*] *P*(*I*[*E*][=][*e*]=*i*| *G*=*g*)*P*(*G*=*g*).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now we come to the trick—*P*(*I*[*E*][=][*e*]=*i*|*G*=*g*) = *P*(*I*[*E*][=][*e*]=*i*|*E*=*e*,
    *G*=*g*) for any value of *e*, because once we condition on *G*=*g*, *E*=*e* and
    *I*[*E*][=][*e*] are independent. So in our derivation, we can replace *P*(*I*[*E*][=][*e*]=*i*|*G*=*g*)
    with *P*(*I*[*E*][=][*e*]=*i*|*E*=*e*, *G*=*g*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once we condition that *E*=*e*, we can use the law of consistency to drop the
    subscript: ∑[*g*] *P*(*I*[*E*][=][*e*]=*i*|*E*=*e*, *G*=*g*)*P*(*G*=*g*) = ∑[*g*]
    *P*(*I*=*i*|*E*=*e*, *G*=*g*)*P*(*G*=*g*).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s explain steps 4 and 5\. Our ignorability result shows that *I*[*E*][=][*e*]
    and *E* are conditionally independent given *G*. So in step 4 we apply the independence
    trick that lets us introduce *E*. Further, we set the value of *E* to be *e* so
    it matches the subscript [*E*][=][*e*]. This allows us to apply the law of consistency
    from chapter 8 and drop the subscript [*E*][=][*e*].
  prefs: []
  type: TYPE_NORMAL
- en: Voila, we’ve identified a backdoor estimand, an estimand from level 1 of the
    causal hierarchy, for a level 2 causal query *P*(*I*[*E*][=][*e*]) using level
    2 assumptions encoded in a DAG. Causal identification is just coming up with derivations
    like this. Much, if not most, of traditional causal inference research boils down
    to doing this kind of math, or writing algorithms that do it for you.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll look at the do-calculus, which provides simple graph-based rules
    for identification that we can use in identification algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 10.4 Graphical identification with the do-calculus
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Graphical identification* (sometimes called *nonparametric identification*)
    refers to identification techniques that rely on reasoning over the DAG. One of
    the most well-known approaches to graphical identification is the *do-calculus*,
    a set of three rules used for identification with causal graphs. The rules use
    graph surgery and d-separation to determine cases when you can replace causal
    terms like *I*[*E*][=][*e*] with non-causal terms like *I*|*E*=*e*. Starting with
    a query on a higher level of the causal hierarchy, we can apply these rules in
    sequence to derive a lower-level estimand.'
  prefs: []
  type: TYPE_NORMAL
- en: 10.4.1 Demystifying the do-calculus
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Recall high school geometry, where you saw if-then statements like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*If* the shape is a square, *then* all the sides are equal.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: When you were trying to solve a geometry problem, you used facts like this in
    the steps of your solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, the do-calculus consists of three rules (if-then statements) of
    the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '*If* certain variables are d-separated after applying graph surgery to the
    DAG, *then* probability query *A* equals probability query *B*.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The rules of the do-calculus are not intuitive
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The three rules of the do-calculus are not intuitive upon reading them, just
    as geometric rules like cos²*x* + sin²*x* = 1 were not intuitive when you first
    saw them in high school. But like those geometric rules, we derive the rules of
    the do-calculus from simpler familiar concepts, namely d-separation, ideal interventions,
    and the rules of probability. And like the rules of geometry, we can use the rules
    of the do-calculus to prove that a causal query from one level of the hierarchy
    is equivalent to one from another level.
  prefs: []
  type: TYPE_NORMAL
- en: Practically speaking, we can rely either on software libraries that implement
    the do-calculus in graphical identification algorithms (like y0) or simply hard-code
    well-known identification results like the backdoor adjustment estimand. To take
    away some of the mystery, I’ll introduce the rules and show how they can derive
    the backdoor estimand. The goal here is not to memorize these rules, but rather
    to see how they work in a derivation of the backdoor estimand that contrasts with
    the derivation in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: In defining these rules, we’ll focus on the target distribution *Y* under an
    intervention on *X*. We want to generalize to all DAGs, so we’ll name two other
    nodes, *Z* and *W*. *Z* and *W* will allow us to cover cases where we have another
    potential intervention target *Z* and any node *W* we’d like to condition upon.
    Further, while I’ll often refer to individual variables, keep in mind that the
    rules apply when *X*, *Y*, *Z*, and *W* are sets of variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rule 1: Insertion or removal of observations'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If *Y* and *Z* are d-separated in your DAG by *X* and *W* after the incoming
    edges to *X* are removed . . .
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Then* *P* ( *Y* [*X*] [=] [*x*]=  *y* | *Z*  =  *z*, *W*  =  *w*) = *P*( *Y*
    [*X*] [=] [*x*] =  *y* | *W*  =  *w*).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is called “insertion or removal” because we can remove *Z*=*z* from *P*(*Y*[*X*][=][*x*]=*y*|*Z*=*z*,
    *W*=*w*) to get *P*(*Y*[*X*][=][*x*]=*y* | *W*=*w*) and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rule 2: Exchange of an intervention for an observation'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*If* *Y* and *Z* are d-separated in your DAG by *X* and *W* after incoming
    edges in *X* and outgoing edges from *Z* have been removed . . .'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*then P* ( *Y* [*X*] [=] [*x*] [,] [*Z*] [=] [*z*]= *y* | *W* =  *w*) = *P*(
    *Y* [*X*] [=] [*x*] =  *y* | *Z*  =   *z*, *W*  =   *w*).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Here we can either *exchange* the intervention [*Z*][=][*z*] in *P*(*Y*[*X*][=][*x*][,]
    [*Z*][=][*z*]=*y* | *W*=*w*) for conditioning on the observation *Z*=*z* to get
    *P*(*Y*[*X*][=][*x*]=*y* | *Z*=*z*, *W*=*w*), or vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rule 3: Insertion or removal of interventions'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For rule 3, we are going to define *Z* as a set of nodes, and *Z*(*W*) as the
    subset of *Z* that are not ancestors of *W*.
  prefs: []
  type: TYPE_NORMAL
- en: If *Y* and *Z* are d-separated in your DAG by *X* and *W* after you remove all
    incoming edges to *X* and *Z*( *W*) . . .
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*then* *P*( *Y* [*X*] [=] [*x*] [,] [*Z*] [=] [*z*] =  *y* | *W* =  *w*) =
    *P*( *Y* [*X*] [=] [*x*] =  *y* | *W* =  *w*).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This rule allows you to insert [*Z*][=][*z*] into *P*(*Y*[*X*][=][*x*]=*y* |
    *W*=*w*) to get *P*(*Y*[*X*][=][*x*][,] [*Z*][=][*z*]=*y* | *W*=*w*) or remove
    [*Z*][=][*z*] from *P*(*Y*[*X*][=][*x*][,] [*Z*][=][*z*]=*y* | *W*=*w*) to get
    *P*(*Y*[*X*][=][*x*]=*y* | *W*=*w*).
  prefs: []
  type: TYPE_NORMAL
- en: 10.4.2 Using the do-calculus for backdoor identification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now we’ll use the do-calculus to provide an alternative derivation of the backdoor
    estimand that differs from our “ignorability”-based definition. Again, I include
    this derivation to demystify the application of the do-calculus. Don’t worry if
    you don’t completely follow each step:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(*I*[*E*][=][*e*]=*i*) = ∑[*g*] *P*(*I*[*E*][=][*e*]=*i*, *G*[*E*][=][*e*]=*g*)
    by the law of total probability.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ∑[*g*] *P*(*I*[*E*][=][*e*]=*i*, *G*[*E*][=][*e*]=*g*) = ∑[*g*] *P*(*I*[*E*][=][*e*][,]
    [*G*][=][*g*]=*i*)*P*(*G*[*E*][=][*e*][,] [*I*][=][*i*]=*g*) by way of *c-component
    factorization*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*P*(I[*E*][=][*e*][,] [*G*][=][*g*]=i) = *P*(*I*=*i*|*E*=*e*, *G*=*g*) by rule
    2 of the do-calculus.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*P*(G[*E*][=][*e*][,] [*I*][=][*i*]=g) = *P*(*G*=*g*) by rule 3 of the do-calculus.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Therefore, *P*(I[*E*][=][*e*]=i)= ∑[*g*] *P*(*I*=*i*|*E*=*e*, *G*=*g*) *P*(*G*=*g*)
    by plugging 3 and 4 into 2\.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The do-calculus rules are applied in steps 3 and 4\.
  prefs: []
  type: TYPE_NORMAL
- en: Note  Step 2 uses a factorization rule called *c-component factorization*. A
    c-component (confounded component) is a set of nodes in a DAG where each pair
    of observable nodes is connected by a path with edges that always point toward,
    never away from, the observable nodes (these are the “orphaned cousins” mentioned
    in chapter 4). The joint probability of the observed variables can be factorized
    into c-components, and this fact enabled step 2\. Factorizing over c-components
    is common in identification algorithms. See the references in the chapter notes
    at [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook).
  prefs: []
  type: TYPE_NORMAL
- en: This do-calculus-based derivation is far less intuitive than our “ignorability”-based
    derivation. There are two advantages we get in exchange for that of intuition.
    First, the do-calculus is *complete*, meaning that if a query has an identifiable
    estimand using graphical assumptions alone, it can be derived using the do-calculus.
    Second, we have algorithms that leverage the do-calculus to automate graphical
    identification.
  prefs: []
  type: TYPE_NORMAL
- en: 10.5 Graphical identification algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Graphical identification algorithms, often called *ID algorithms*, automate
    the application of graph-based identification systems like the do-calculus. When
    we used y0 to check for identification of *P*(*I*[*E*][=][*e*]) and to derive
    the backdoor estimand, it was using its implementation of graphical identification
    algorithms. In this section, we’ll see how we can use these algorithms to identify
    another useful estimand called the *front-door estimand*.
  prefs: []
  type: TYPE_NORMAL
- en: '10.5.1 Case study: The front-door estimand'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our online gaming example, suppose we were not able to observe guild membership.
    Then we would not have backdoor identification of *P*(*I*[*E*][=][*e*]). However,
    suppose we had a *mediator* between side-quest engagement (*E*) and in-game purchases
    (*I*)—a node on the graph between *E* and *I*. Specifically, our mediator represents
    *won items* (*W*), as seen in figure 10.11\.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F11_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.11 Side-quest engagement leads to winning virtual items like this
    magic bow. Won items drive more in-game purchases, such as magic arrows for the
    magic bow, so we introduce a mediator “won items” on the causal path between side-quest
    engagement and in-game purchases.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The idea of won items is as follows. When a player successfully completes a
    side-quest, they win a virtual item. The more side-quests they finish, the more
    items they earn. Those *won* virtual items and *purchased* virtual items can complement
    one another. For example, winning a magic bow motivates purchases of magical arrows.
    Thus, the amount of won items a player has influences the amount of virtual items
    they purchase.
  prefs: []
  type: TYPE_NORMAL
- en: Given this graph, we can use y0’s implementation of graphical identification
    algorithms to derive the front-door estimand.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.6 Deriving the front-door estimand in y0
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Build a new graph with the mediator variable.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Still the same query as in listing 10.5, P(I_{E=e})'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 But now we observe I, E, and W'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Finally, we check if the query is identified given the DAG and observational
    distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: This code will return the output in figure 10.12.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch10-eqs-3x.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.12 Y0 renders a math figure as output of identification.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Rearranging the output, and in our notation, this is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch10-eqs-4x.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Simplifying as before, we get the front-door estimand:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch10-eqs-5x.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that there is an outer summation over *W* and an inner summation over all
    values of *E* (with each value of *E* denoted as *ε*, distinct from the intervention
    value *e*).
  prefs: []
  type: TYPE_NORMAL
- en: 10.5.2 Demystifying the front door
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Like the backdoor estimand, the do-calculus derivation of the front-door estimand
    involves repeated substitutions using rules 2 and 3\. The rough intuition behind
    the front-door estimand is that the statistical association between side-quest
    engagement and in-game purchases comes from both the direct causal path and the
    path through the backdoor confounder guild membership (*G*). The front-door estimand
    uses the mediator to determine how much of that association is due to the direct
    causal path; the mediator acts as a gauge of the flow of statistical information
    through that direct causal path.
  prefs: []
  type: TYPE_NORMAL
- en: A key benefit of the estimand is that it does not require observing a set of
    confounders that block all possible backdoor paths. Avoiding backdoor adjustment
    is useful when you have many confounders, are unable to adjust due to latent confounders,
    or are concerned that there might be some unknown confounders.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll examine how to identify counterfactuals.
  prefs: []
  type: TYPE_NORMAL
- en: 10.6 General counterfactual identification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The causal DAG is a level 2 modeling assumption. The causal hierarchy theorem
    tells us that the graph in general is not sufficient to identify level 3 counterfactual
    queries. For counterfactual identification from level 1 or level 2 distributions,
    you need level 3 assumptions. In simple terms, a level 3 assumption is any causal
    assumption that you can’t represent with a simple causal DAG.
  prefs: []
  type: TYPE_NORMAL
- en: In chapter 9, I introduced the general algorithm for counterfactual inference.
    The algorithm requires a structural causal model (SCM), which is a level 3 model;
    it encapsulates level 3 assumptions. With an SCM, the algorithm can infer *all*
    counterfactual queries that can be defined on its underlying variables. The cost
    of this ability is that the SCM must encapsulate *all* the assumptions needed
    to answer all those queries. Many of these assumptions cannot be validated with
    level 1 or level 2 data.
  prefs: []
  type: TYPE_NORMAL
- en: The more assumptions you make, the more vulnerable your inferences are to violations
    of these assumptions. For this reason, we seek identification techniques that
    target specific counterfactual queries (rather than every counterfactual query)
    with the minimal set of level 3 assumptions possible.
  prefs: []
  type: TYPE_NORMAL
- en: 10.6.1 The problem with the general algorithm for counterfactual inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can see the problem with the general algorithm for counterfactual inference
    when we apply it to two similar SCMs. Let’s suppose there is a ground-truth SCM
    that differs from the SCM you are using to run the algorithm. Suppose both SCMs
    have the exact same underlying DAG and the same statistical fit on observational
    and experimental data; in other words, the SCMs provide the same inferences for
    all level 1 and level 2 queries. Your SCM could still produce different (inaccurate)
    counterfactual inferences relative to the ground-truth SCM.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see why, recall the stick-breaking example from chapter 6\. I posed two
    similar but different SCMs. This was the first:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch10-eqs-10x.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And this was the second:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch10-eqs-11x.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.13 visualizes sampling a single value from these models.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F13_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.13 Two different SCMs encode the exact same observational and interventional
    distributions, but given the same exogenous variable value, you can get two different
    values of the corresponding endogenous variable in each model.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Figure 10.13 shows how, given a value of *n*[*y*] = .15, the sticks break at
    the .15 meters point, but the first stick will break in region 2, returning a
    value of 2, while the second stick will break in region 3, returning a 3\. They
    produce different outcomes given the same random input because they differ in
    a level 3 assumption, i.e., *how* they process the input.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, when we go in the opposite direction and apply the abduction
    step in the general counterfactual inference algorithm, we can get different results
    across these models. For a given value of the endogenous variable, we can get
    different posterior distributions on the exogenous variable.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.14 illustrates how the two models, for an observed outcome of 3, would
    produce different inferences on *N*[*y*]. For the first SCM, a value of *y*=3
    means *P*(*N*[*y*]|*Y*=3) is a continuous uniform distribution on the range (*p*[*x*][1]
    + *p*[*x*][2]) to 1, and for the second SCM, it is a continuous uniform distribution
    on the range 0 to *p*[*x*][3]. These different distributions of *P*(*N*[*y*]|*Y*=3)
    would lead to different results from the counterfactual inference algorithm. Now
    suppose SCM 2 is right and SCM 1 is wrong. If we choose SCM 1, our counterfactual
    inferences will be inaccurate.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F14_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.14 The two SCMs, despite encoding the same set of observational and
    interventional distributions, would produce two different posteriors of *N**[y]*
    given *Y*=3 in the abduction step. Therefore, they encode different counterfactual
    distributions and can produce different counterfactual inferences.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The general case is even harder; there can be many SCMs entailing the same level
    1 and 2 assumptions but have different level 3 assumptions. You might learn one
    of those SCMs by, for example, using a deep neural network-based approach to learn
    a deep SCM from level 1 and level 2 data. But the deep SCM might not be the *right*
    SCM with respect to the counterfactual inferences you want to make.
  prefs: []
  type: TYPE_NORMAL
- en: The general algorithm for counterfactual inference is ideal if you are confident
    in the ground-truth SCM. But in cases where you aren’t, you can look toward counterfactual
    identification, where you specify a *minimal* set of level 3 assumptions that
    enable you to identify a target counterfactual query.
  prefs: []
  type: TYPE_NORMAL
- en: '10.6.2 Example: Monotonicity and the probabilities of causation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Monotonicity is an example of a powerful level 3 assumption. Monotonicity is
    the simple assumption that the relationship between a cause *X* and an outcome
    *Y* is monotonic: *E*(*Y*|*X*=*x*) either never increases or never decreases as
    *x* increases. Note that linearity is a special case of monotonicity.'
  prefs: []
  type: TYPE_NORMAL
- en: An intuitive example of monotonicity and non-monotonicity is in the dosage of
    medicine. In a monotonic dose-response relationship, taking more of the medicine
    either helps or does nothing. In a non-monotonic dose-response relationship, taking
    the medicine might help at a normal dose, but taking an overdose might cause the
    problem to get worse. Monotinicity helps identification by eliminating counterfactual
    possibilities; if the dose-response relationship is monotonic, when you imagine
    what would have happened if you took a stronger dose, you can eliminate the possiblity
    that you would have gotten worse.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall the probabilities of causation we saw in chapter 8:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Probability of necessity (PN): *P*(*Y*[*X*][=][0]=0|*X*=1, *Y*=1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Probability of sufficiency (PS): *P*(*Y*[*X*][=1]=1|*X*=0, *Y*=0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Probability of necessity and sufficiency (PNS): *P*(*Y*[*X*][=1]=1, *Y*[*X*][=0]=0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given monotonicity, we can identify the following level 2 estimands for the
    probabilities of causation.
  prefs: []
  type: TYPE_NORMAL
- en: PN = (*P*(*Y*=1) – *P*(*Y*[*X*][=0]=1))/*P*(*X*=1, *Y*=1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PS = (*P*(*Y*[*X*][=1]=1) – *P*(*Y*=1))/*P*(*X*=0, *Y*=0)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PNS = *P*(*Y*[*X*][=1]=1) – *P*(*Y*[*X*][=0]=1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can estimate these level 2 estimands from level 2 data, such as a randomized
    experiment. And, of course, if we only have observational data, we can use backdoor
    or front-door adjustment or another identification strategy to infer *P*(*Y*[*X*][=0]=1)
    and *P*(*Y*[*X*][=1]=1) from that data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We could derive these estimands by hand again, but instead, let’s think about
    the monotonicity enabled this identification by eliminating counterfactual possibilities.
    To see this, consider our uplift modeling question in chapter 8\. There, *X* was
    whether we sent a promotion, and *Y* was whether the customer remained a paying
    subscriber (*Y*=1) or “churned” (unsubscribed; *Y*=0). We segmented the subscribers
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Persuadables*—Subscribers whose chance of remaining increases when you send
    a promotion'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Sure things*—Subscribers who have a high chance of remaining regardless of
    whether you send a promotion'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Lost causes*—Subscribers who have a low chance of remaining regardless of
    whether you send a promotion'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Sleeping dogs*: Subscribers whose chances of remaining *go down* when you
    send a promotion'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you assume monotonicity, you are assuming that sending the promotion either
    does nothing or increases the chances of remaining. It assumes there are no users
    who will respond poorly to the promotion. In other words, assuming monotonicity
    means you assume there are no sleeping dogs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s consider how this narrows things down. Suppose you have the following
    question:'
  prefs: []
  type: TYPE_NORMAL
- en: I failed to send a promotion to a customer and they churned. Would they have
    remained had I sent the promotion? *P* ( *Y* [*X*] [=1] = 1| *X*  = 0, *Y*  = 0)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This counterfactual query is the probability of sufficiency. We want to know
    if sending the promotion would have increased the chances of their remaining.
    Thinking through the question,
  prefs: []
  type: TYPE_NORMAL
- en: If the customer was a persuadable, sending the promotion would have increased
    their chances of remaining.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the customer was a lost cause, sending the promotion would have had no effect.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the customer was a sleeping dog, sending the promotion would have made them
    *even less* likely to remain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s hard to determine if we should have sent the promotion if being a persuadable
    and being a sleeping dog were both possible for this customer, in one case the
    promotion would have helped and in the other it would have made churning even
    more certain. But if we assume monotonicity, we eliminate the possibility that
    they were a sleeping dog, and can conclude sending the promotion would have helped
    or, at least, not have hurt their chances of staying.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian modeling and counterfactual identification
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Although the graphical identification algorithms will work with some counterfactual
    queries, we don’t have general algorithms for counterfactual identification. But
    given our focus on the tools of probabilistic ML, we can look to Bayesian modeling
    for a path forward.
  prefs: []
  type: TYPE_NORMAL
- en: Identification is fundamentally about uncertainty. For example, in the counterfactual
    case, a lack of identification means that even with infinite level 1 and level
    2 data, you can’t be certain about the true value of the level 3 query. From a
    Bayesian perspective, we can use probability to handle that uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose you have a set of causal assumptions, including non-graphical assumptions,
    and some level 1 and 2 data. You can take the following Bayesian approach to test
    whether your assumptions and data are sufficient to identify your counterfactual
    query:'
  prefs: []
  type: TYPE_NORMAL
- en: Specify a set of SCMs that are diverse yet all consistent with your causal assumptions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Place a prior distribution over this set, such that more plausible models get
    higher prior probability values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Obtain a posterior distribution on the SCMs given observational (level 1) and
    interventional (level 2) data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample SCMs from the posterior distribution, and for each sample SCM, you apply
    the general algorithm for counterfactual inference for a specific counterfactual
    query.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The result would constitute the posterior distribution over this counterfactual
    inference. If your causal assumptions and your data are enough to identify the
    counterfactual query, the posterior on the counterfactual inference will converge
    to the true value as the size of your data increases. (Successful convergence
    assumes typical “regularity” conditions for Bayesian estimation. Results will
    depend on the quality of the prior.) But even if it doesn’t converge to the true
    value, your assumptions might still enable convergence to a ballpark region around
    the true value that is small enough to be useful (this is called partial identification,
    as described in section 10.9).
  prefs: []
  type: TYPE_NORMAL
- en: The Pyro library, and its causality-focused extension ChiRho, facilitate combining
    Bayesian and causal ideas in this way.
  prefs: []
  type: TYPE_NORMAL
- en: There are generalizations of monotonicity from binary actions (like sending
    or not sending a promotion) to multiple actions as in a decision or reinforcement
    learning problem, see the course notes at [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    for references.
  prefs: []
  type: TYPE_NORMAL
- en: 10.7 Graphical counterfactual identification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A conventional causal DAG only encodes level 2 assumptions, but there are graphical
    techniques for reasoning about counterfactuals. Graphical counterfactual inference
    only works in special cases, but these cases are quite practical. Further, working
    with graphs enables us to automate identification with algorithms. To illustrate
    graphical counterfactual identification, we’ll introduce a new case study.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you open Netflix, you see the Netflix dashboard, which shows several forms
    of recommended content. Two of these are “Top Picks For You,” which is a personalized
    selection of shows and movies that Netflix’s algorithms predict you will enjoy
    based on your past viewing behavior and ratings, and “Because You Watched,” which
    recommends content based on things you watched recently. The model of this system
    includes the following variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '*T*—A variable for the recommendation policy that selects a subscriber’s “Top
    Picks for You” content. For simplicity, we’ll consider a policy, “+*t*”, that
    is currently in production. We’ll use “–*t*”, meaning “not *t*”, to represent
    alternative policies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*B*—A variable for the recommendation policy that selects a subscriber’s “Because
    You Watched” content. Again, we’ll simplify this to a binary variable with policy
    “+*b*”, representing the policy in production, and all alternative policies “–*b*”,
    as in “not *b*.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*V*—The amount of engagement that a subscriber has with the content recommended
    by “Because You Watched.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*W*—The amount of engagement that a subscriber has with the content recommended
    by “Top Picks for You.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A*—Attrition, meaning whether a subscriber eventually leaves Netflix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*C*—Subscriber context, meaning the type of subscriber (location, demographics,
    preferences, etc.) we are dealing with.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommendation algorithms always take the profile of the subscriber into account,
    along with the viewership history, so subscriber profile *C* is a cause of both
    recommendation policy variables *T* and *B*.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll use y0 to analyze this problem at various levels of the
    hierarchy. We’ll start by visualizing the graph.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.7 Plot the recommendation DAG
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Define variables for the model.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Create the graph.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Plot the graph.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F15_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.15 Causal DAG for the recommendation algorithm problem
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This generates the DAG in figure 10.15\.
  prefs: []
  type: TYPE_NORMAL
- en: As a preliminary investigation, you might look at the average treatment effect
    (ATE, a level 2 query) of the “Top Picks for You” content on attrition *E*(*A*[*T*][=+][*t*]
    – *A*[*T*][=–][*t*]). Given that attrition *A* has a binary outcome, we can write
    this as *P*(*A*[*T*][=+][*t*]=+*a*) – *P*(*A*[*T*][=–][*t*]=+*a*). Focusing on
    *P*(*A*[*T*][=–][*t*]=+*a*), we know right away that we can identify this via
    both the (level 2) backdoor and the front door. So let’s move on to an interesting
    (level 3) counterfactual query called *effect of treatment on the treated* (ETT).
  prefs: []
  type: TYPE_NORMAL
- en: 10.7.1 Effect of treatment on the treated
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recall that you get the ATE directly (without needing to identify and estimate
    a level 1 estimand) from a randomized experiment. Suppose you ran such an experiment
    on a cohort of users, and it showed a favorable ATE, such as that +*t* has a favorable
    impact on *W* and *A* relative to –*t*. So your team deploys the policy.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose the +*t* policy works best with users who have watched a lot of movies
    and thus have more viewing data. For this reason, when the policy is deployed
    to production, such users are more likely to get assigned the policy. But since
    they are so highly engaged, they are unlikely to leave, regardless of whether
    they are assigned the +*t* or –*t* policy. We could have a situation where the
    +*t* policy looks effective in an experiment where people are assigned policies
    randomly, regardless of their level of engagement, but in production the assignment
    is biased to highly engaged people who are indifferent to the policy.
  prefs: []
  type: TYPE_NORMAL
- en: The level 3 query that addresses this is a counterfactual version of the ATE
    called effect of treatment on the treated (ETT, or sometimes ATT, as in *average
    treatment effect on the treated*). We write this as counterfactual query *E*(*A*[*T*][=+][*t*]
    – *A*[*T*][=–][*t*]|*T*=+*t*), as in “for people who saw policy +*t*, how much
    more attrition do they have relative to what they would have if they had seen
    –*t*?”Decomposing for binary *A* as we did with the ATE, we can write this as
    *P*(*A*[*T*][=+][*t*]=+*a*|*T*=+*t*) – *P*(*A*[*T*][=–][*t*]=+*a*|*T*=+*t*). *P*(*A*[*T*][=+][*t*]=+*a*|*T*=+*t*)
    simplifies to *P*(*A*=+*a*|*T*=+*t*) by the law of consistency. So we can focus
    on the second term, *P*(*A*[*T*][=–][*t*]=+*a*|*T*=+*t*).
  prefs: []
  type: TYPE_NORMAL
- en: In this special case of binary *A*, we can identify the ETT using graphical
    identification (for non-binary *A*, more level 3 assumptions are needed). To do
    graphical identification for counterfactuals, we can use graphical identification
    algorithms with counterfactual graphs.
  prefs: []
  type: TYPE_NORMAL
- en: 10.7.2 Identification over the counterfactual graph
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Y0 can derive an estimand for ETT using a graphical identification algorithm
    called “IDC*” (pronounced I-D-C-star).
  prefs: []
  type: TYPE_NORMAL
- en: Graph ID algorithms, ID, IDC, ID*, IDC*, in y0
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Some of the core graphical identification algorithms implemented in y0 are ID,
    ID*, IDC, and IDC*. ID identifies interventional (level 2) queries from a DAG
    and observational (level 1) data. ID* identifies counterfactual (level 3) queries
    from observational and experimental (level 1 and level 2) data. IDC and IDC* extend
    ID and ID* to work on queries that condition on evidence, such as ETT.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithms use the structure of the causal graph to recursively simplify
    the identification problem by removing irrelevant variables and decomposing the
    graph into c-component subgraphs. They apply the rules of do-calculus to reduce
    intervention terms, block confounding backdoor paths, and factorize the query
    into simpler subqueries. If no further simplification is possible due to the graph's
    structure, the algorithms return a 'non-identifiable' result.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter's code relies on Y0’s implementations of these algorithms, though
    Y0 implements other graphical identification algorithms as well.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.8 Identifying ETT with a graphical identification algorithm
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Hypothetical outcome A [T=–t] = +a'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Factual condition T = +t'
  prefs: []
  type: TYPE_NORMAL
- en: 'This will produce a rather verbose level 2 estimand. We can then apply level
    2 graphical identification algorithms to get a level 1 estimand, which will simplify
    to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch10-eqs-6x.png)'
  prefs: []
  type: TYPE_IMG
- en: I’ll show a simple derivation in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: For now, the intuition is that we are applying graphical identification algorithms
    over something called a counterfactual graph. Up until now, our graph of choice
    for counterfactual reasoning was the parallel world graph. Indeed, we can have
    y0 make a parallel world graph for us.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.9 Plotting the parallel world graph with y0
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The make_parallel_worlds_graph method takes an input DAG and sets of interventions.
    It constructs a new world for each set.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The helper function visualizes the graph in a familiar way.'
  prefs: []
  type: TYPE_NORMAL
- en: This graph differs slightly from the ones I’ve drawn because the algorithm applies
    the subscript for an intervention to every node in the world where the intervention
    occurred; the subscript indexes all the variables in a world. It’s up to us to
    reason that *C* from one world and *C*[+][*t*] from another must have the same
    outcomes, since *C*[+][*t*] is not affected by its world’s intervention do(*T*=+*t*).
  prefs: []
  type: TYPE_NORMAL
- en: Now recall that the problem with the parallel world graph is that d-separation
    won’t work with it. For example, in figure 10.16, d-separation suggests that *C*
    and *C*[+][*t*] are conditionally independent given their common exogenous parent
    *N*[*C*], but we just articulated that *C* and C[+][*t*] must be the same; if
    C has a value, *C*[+][*t*] must have the same value, so they are perfectly dependent.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F16_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.16 A parallel world graph drawn by y0 (and Graphviz). In this version
    of the parallel world graph, the subscripts indicate a world. For example, +*t*
    indicates the world where the intervention do(*T*=+*t*) is applied. To prevent
    confusion, the exogenous variables use superscripts instead of subscripts to indicate
    their child endogenous variables (e.g., *N**^C*is the parent of *C* (and *C*[+t]).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We can remedy this with the *counterfactual graph*. A counterfactual graph is
    created by using a parallel world graph and the counterfactual query to understand
    which nodes across worlds in the parallel world graph are equivalent, and then
    collapsing equivalent nodes into one. The resulting graph contains nodes across
    parallel worlds that are relevant to the events in the query. Unlike parallel
    world graphs, you can use d-separation to reason about counterfactual graphs.
    We can use y0 to create a counterfactual graph for events *A*[*T*][=–][*t*]=+*a*
    and *T*=+*t*.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.10 Listing 10.10 Counterfactual graph events *A**[T]*[=][–]*[t]*=+*a*
    and *T*=+*t*
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Counterfactual graphs work with event outcomes in the query. For P(A [T=–t]=+a|T=+t),
    we want events A [T=–t] =+a and T=+t.'
  prefs: []
  type: TYPE_NORMAL
- en: This creates the counterfactual graph in figure 10.17.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F17_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.17 Counterfactual graph for events, produced by y0 (and Graphviz).
    *T**[–t]*corresponds to the intervention do(*T*=–*t*).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: At a high level, graphical identification algorithms in y0 do counterfactual
    identification by working with counterfactual graphs in lieu of conventional DAGs.
    First, it finds a level 2 estimand for a level 3 query. From there, you can use
    experimental data to answer the level 2 terms in the estimand, or you can attempt
    to further derive them to level 1 estimands from the level 2 terms.
  prefs: []
  type: TYPE_NORMAL
- en: Graphs alone won’t work when you condition on outcome!
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Suppose that instead of the ETT term *P*(*A**[T]*[=–]*[t]*=+*a*|*T*=+*t*), you
    were interested in *P*(*A**[T]*[=–]*[t]* =+*a*|*T*=+*t*, *A*=+*a*), answering
    the question “Given a subscriber exposed to policy +*t* and later unsubscribed,
    would they still have unsubscribed had they not been exposed to that policy?”
    Or you could be interested in *E*(*A**[T]*[=–]*[t]* – *A**[T]*[=+]*[t]*|*T*=+*t*,
    *A*=+*a*) sometimes called *counterfactual regret*, which captures the amount
    the policy +*t* contributed to an unsubscribed individual’s decision to unsubscribe.
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(*A**[T]*[=–]*[t]*=+*a*|*T*=+*t*, *A*=+*a*) is an example of a query where
    the hypothetical outcomes and factual conditions are in conflict. In this case,
    the factual conditions contain an outcome for *A*, and the hypothetical condition
    contains an interventional outcome for *A*. The graphical counterfactual identification
    techniques mentioned in this section will not work for this type of query. Identification
    in this case requires additional level 3 assumptions.'
  prefs: []
  type: TYPE_NORMAL
- en: This is unfortunate, because this type of counterfactual is precisely the kind
    of “how might things have turned out differently?” counterfactual questions that
    are the most interesting, and the most central to how humans reason and make decisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also use graphical identification for more advanced queries. For example,
    suppose you want to isolate how *T* affects *A* from how *B* affects *A*. You
    want to focus on users where *B* was –*b*. You find the data from a past experiment
    where “Because you watched . . .” policy *B* was randomized. You take that data
    and zoom in on participants in the experiment who were assigned –*b*. The outcome
    of interest in that experiment was *V*, the amount of engagement with the content
    recommended in the “Because you Watched” box. So you have the outcomes of *V*[*B*][=–][*b*]
    for those subscribers of interest. With this new data, you expand your query from
    *P*(*A*[*T*][=–][*t*]=+*a*|*T*=+*t*) to *P*(*A*[*T*][=–][*t*]=+*a*|*T*=+*t*, *B*=–*b*,
    *V*[*B*][=–][*b*]=*v*), including *V*[*B*][=–][*b*]=*v* because it is helpful
    in predicting attrition. Now you have three parallel worlds to reason over: the
    actual world, the world with do(*T*=+*t*), and the world with do(*B*=–*b*).'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.11 Create a parallel world graph for do(*T*=+*t*) and do(*B*=–*b*)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The second argument enumerates the hypothetical conditions.'
  prefs: []
  type: TYPE_NORMAL
- en: This code creates this three-world parallel world graph seen in figure 10.18.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F18_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.18 A parallel world graph with the actual world *T*=+*t* and hypothetical
    worlds do(*T*=–*t*) and do(*B*=–*b*). The dashed lines are edges from exogenous
    variables (dark gray).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Notably, the query *P*(*A*[*T*][=–][*t*]=+*a*|*T*=+*t*, *B*=–*b*, *V*[*B*][=–][*b*]=*v*)
    collapses the parallel world graph to the same counterfactual graph as *P*(*A*[*T*][=–][*t*]=+*a*|*T*=+*t*).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.12 Counterfactual graph for expanded expression
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This gives us the counterfactual graph in figure 10.19, which is the same as
    the graph in figure 10.17.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F19_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.19 The counterfactual graph for *P*(*A**[T]*[=–]*[t]*=+*a*|*T*=+*t*,
    *B*=–*b*, *V**[B]*[=–]*[b]*=*v*) is the same as for *P*(*A**[T]*[=–]*[t]*=+*a*|*T*=+*t*).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Next, we’ll look at another graph-based approach called single-world intervention
    graphs.
  prefs: []
  type: TYPE_NORMAL
- en: 10.7.3 Counterfactual identification with single-world intervention graphs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Single-world intervention graphs (SWIGs) provide an alternative to counterfactual
    identification with counterfactual graphs. Like a counterfactual graph, we construct
    a SWIG using the original causal DAG and the causal query. We’ll use the Netflix
    recommendation example to construct a SWIG for the interventions do(*T*=–*t*)
    and do (*B*=–*b*). Let’s construct a SWIG from a causal DAG.
  prefs: []
  type: TYPE_NORMAL
- en: Node-spitting operation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We have the intervention that targets do(*T*=+*t*), and we can implement it
    with a special kind of graph surgery called a *node-splitting operation*. We split
    a new node off the intervention target *T**, as i*n figure 10.20\. *T* still represents
    the same variable as in the original graph, but the new node represents a constant,
    the intervention value +*t*. *T* keeps its parents (in this case *C*) but loses
    its children (in this case *W*) to the new node.
  prefs: []
  type: TYPE_NORMAL
- en: Subscript inheritance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Next, every node downstream of the new node inherits the new node’s values as
    a subscript. For example, in figure 10.21, *W* and *A* are downstream of the intervention,
    so the subscript *[T]*[= ][–]*[t]* is appended to these nodes, so they become
    *W*[*T*][=][-][*t*] and *A*[*T*][=][-][*t*].
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F20_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.20 The intervention operator for a SWIG is the node-splitting operation.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F21_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.21 Every node downstream of the intervention gets the intervention
    subscript.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Repeat for each intervention
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We repeat this process for each intervention. In figure 10.22, we apply do(*B*=–*b*),
    and split *B* and we convert *V* to *V*[*B*][=–][*b*] and *A*[*T*][=-][*t*] to
    *A*[*T*][=–][*t*][,][*B*][=–][*b*].
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F22_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.22 A node takes the subscript of all its upstream interventions.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Like the counterfactual graph, the SWIG contains counterfactual variables and
    admits d-separation. With these properties, we can do identification.
  prefs: []
  type: TYPE_NORMAL
- en: 10.7.4 Identification with SWIGs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose we are interested in ETT and want to identify *P*(*A*[*T*][=][–][*t*]=+*a*|*T*=+*t*).
    We derive the SWIG in figure 10.23.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F23_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.23 We can use the SWIG to derive ETT using the ignorability trick.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'With this graph, we can identify *P*(*A*[*T*][=–][*t*]=+*a*|*T*=+*t*) using
    the ignorability trick I introduced in section 10.4:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(*A*[*T*][=–][*t*]=+*a*|*T*=+*t*) = ∑[*c*] *P*(*A*[*T*][=–][*t*]=+*a*, *C*[*T*][=–][*t*]=*c*|*T*=+*t*)
    by the law of total probability.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ∑[*c*] *P*(*A*[*T*][=][–][*t*]=+*a*, *C*[*T*][=–][*t*]=*c*|*T*=+*t*) = ∑[*c*]
    *P*(*A*[*T*][=–][*t*]=+*a*, *C* =*c*|*T*=+*t*), since *C* is not affected by interventions
    on *T*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ∑[*c*] *P*(*A*[*T*][=–][*t*]=+*a*, *C*=*c*|*T*=+*t*) factorizes into ∑[*c*]
    *P*(*A*[*T*][=–][*t*] =+*a*|*C*=*c*, *T*=+*t*) *P*(*C*=*c* | *T*=+*t*) by the
    chain rule of probability.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*P*(*A*[*T*][=–][*t*]=+*a*|*C*=*c*, *T*=+*t*) = *P*(*A*[*T*][=–][*t*]=+*a*|*C*=*c*,
    *T*=–*t*), again by the ignorability trick.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: And as before, *P*(*A*[*T*][=–][*t*]=+*a*|*C*=*c*, *T*=–*t*) = *P*(*A* =+*a*|*C*=*c*,
    *T*=–*t*) by the law of consistency. Thus, *P*(*A*[*T*][=–][*t*]=+*a*|*T*=+*t*)
    = ∑[*c*] *P*(*A* =+*a*|*C*=*c*, *T*=–*t*) *P*(*C*=*c* | *T*=+*t*)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The magic happens in the ignorability trick in step 4, where *C*’s d-separation
    of *A*[*T*][=–][*t*] and *T* lets us change *T*=+*t* to *T*=–*t*. Notice that
    the same d-separation exists in the counterfactual graph we derived for *P*(*A*[*T*][=–][*t*]=+*a*|*T*=+*t*),
    shown in figure 10.17\. The difference is that deriving the SWIG is easy, while
    deriving the counterfactual graph is nuanced, and one generally uses an algorithm
    like `make_counterfactual_grap` in y0.
  prefs: []
  type: TYPE_NORMAL
- en: 10.7.5 The single-world assumption
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The node-splitting operation relies on a new level 3 assumption. If you are
    going to node-split a variable *X*, then you are assuming it is possible to know
    what value *X* would naturally take without the intervention and that it would
    be possible for you intervene before it realized that value. Imagine in our Netflix
    example that, given a subscriber had profile *C*=*c*, the recommendation algorithm
    was about to assign the subscriber a policy +*t* for recommending “Top picks for
    you,” but before that policy went into effect, you intervened and artificially
    changed it to –*t*. It’s possible that the way you forced the policy to be –*t*
    had some side effects that changed the recommendation system in some fundamental
    way, such that in this new system, *T* would not have been *+t*, in the first
    place. With the single-world assumption, you assume you can know T’s natural value
    would have been *+t*, and that your intervention wouldn’t change the system in
    a way that would affect *T* taking that natural value. You are implicitly making
    this assumption when you reason with SWIGs.
  prefs: []
  type: TYPE_NORMAL
- en: That assumption allows you to avoid the need to create additional worlds to
    reason over. You can condition on outcome *T*=+*t* and intervene do(*T*=–*t*)
    in a “single world.” You can also run experiments, where you apply the intervention
    do(*T*=–*t*) and test if *T* (where you know T’s “natural values”) is conditionally
    independent of *A*(*T*=–*t*) given *C*. This reduces the number of counterfactual
    queries you can answer, but proponents of SWIGs suggest this is a strength, because
    it limits you to counterfactuals that can be validated by experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Contrasting counterfactual graphs and SWIGs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Counterfactual graphs and SWIGs are similar in function, but they are distinctly
    different artifacts.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Counterfactual graphs*—The counterfactual graph works by collapsing equivalent
    parallel world graph nodes over possible worlds. They only contain nodes relevant
    to the specific query. They are defined for binary events like {*T*=+*t*} and
    {*T*=–*t*}—this works well even with continuous variables, because counterfactual
    language typically compares one *hypothetical condition* to one *factual condition*
    (e.g., “We invested 1 million; what if we had invested {2/more/half/…}?”).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Single-world intervention graphs (SWIGs) **—*The SWIG works by applying a
    node-splitting type of graph surgery. Unlike counterfactual graphs, they work
    with general variables (rather than just binary events) and are not query-specific
    (all original nodes are present). However, they rely on a single-world assumption—that
    it is possible to know with certainty what value a variable would have taken had
    it not been intervened upon.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The primary use case for both graphs is identification. Neither counterfactual
    graphs nor SWIGs enable identification from level 1 or 2 data of counterfactual
    queries such as *P*(*A**[T]*[=–]*[t]*=+*a*|*T*=+*t*, *A*=+*a*) where the same
    variable appears in the hypothetical outcome and the factual condition. But you
    can still *derive* the counterfactual graph for such queries; this is not true
    for SWIGs. That is useful if you want to reason about independence across worlds
    in cases of queries such as *P*(*A**[T]*[=–]*[t]*=+*a*|*T*=+*t*, *A*=+*a*).
  prefs: []
  type: TYPE_NORMAL
- en: 10.8 Identification and probabilistic inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve seen that a core part of the identification task is deriving an estimand.
    How does that estimand mesh with a probabilistic machine learning approach?
  prefs: []
  type: TYPE_NORMAL
- en: Consider, for example, our online game model, where ETT = *E*(*I*[*E*][=“high”]
    – *I*[*E*][=“low”]|*E*=“high”) = *E*(*I*[*E*][=“high”]|*E*=“high”) – *E*(*I*[*E*][=“low”]|*E*=“high”).
    We need to identify *P*(*I*[*E*][=“high”]|*E*=“high”) and *P*(*I*[*E*][=“low”]|*E*=“high”).
    Recall that *P*(*I*[*E*][=“high”]|*E*=“high”) simplifies to the level 1 query
    *P*(*I*|*E*=“high”) by the law of consistency, so the challenge lies with identifying
    the counterfactual distribution *P*(*I*[*E*][=“low”]|*E*=“high”).
  prefs: []
  type: TYPE_NORMAL
- en: Using a probabilistic machine learning approach with Pyro, we know we can infer
    *P*(*I*|*E*=“high”) by using `pyro.condition` to condition on *E*=“high” and then
    running inference. The question is how we’ll infer the counterfactual distribution
    *P*(*I*[*E*][=“low”]|*E*=“high”).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous section, we saw that we can identify this query with a SWIG
    (assuming the single-world assumption holds). We used the SWIG to derive the following
    estimand for *P*(*I*[*E*][=0]=*i*|*E*=“high”):'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch10-eqs-7x.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But what do we do with this estimand with respect to building a model in Pyro?
    We could construct two Pyro models, one for *P*(*G*|*E*) and one for *P*(*I*|*G*,
    *E*), infer *P*(*I*=*i*| *G*=*g*, *E*=“low”) and *P*(*G*=*g* | *E*=“high”) and
    then do the summation. But this is inelegant relative to our regular approach
    to probabilistic inference with a causal generative model:'
  prefs: []
  type: TYPE_NORMAL
- en: Implement the full causal generative model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train its parameters on data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the intervention operator to simulate an intervention.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run an inference algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this approach, we build one causal model—we don’t build separate models for
    the estimand's components *P*(*G*|*E*) and *P*(*I*|*G*, *E*). Nonetheless, our
    regular approach to probabilistic inference with a causal generative model does
    work if we have identification, given the causal assumptions we implement in step
    1 and the data we train on in step 2\. We don’t even need to know the estimand
    explicitly; it is enough to know it exists—in other words, that the query is identified
    (e.g., by using Y0’s `check_ identifiable` function). With identification, steps
    2–4 collectively become just another estimator for that estimand.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate, let’s consider how we’d sample from *P*(*I*[*E*][=“low”]|*E*=“high”)
    using a Pyro model of our online gaming example. For simplicity, let’s replace
    *E*=“high” and *E*=“low” with *E*=1 and *E*=0 respectively. We know *P*(*I*[*E*][=0]|*E*=1)
    is identified given our causal DAG and the single-word assumption. Fortunately,
    Pyro’s (and ChiRho’s) `do` intervention operator implements the SWIG’s node-splitting
    operation by default (if you used `pyro.render_model` to visualize an intervention
    and didn’t get what you expected, this is why). For ordinary interventional queries
    on a causal DAG, there is no difference between this and the ordinary graph surgery
    approach to interventions. But when we want to condition on *E*=1 and intervene
    to set *E*=0, Pyro will accommodate us. We’ll use this approach to sample from
    *P*(*I*[*E*][=0]|*E*=1). As a sanity check, we’ll also sample from the plain vanilla
    intervention distribution *P*(*I*[*E*][=0]) and contrast those samples with samples
    from *P*(*I*[*E*][=0]|*E*=1).
  prefs: []
  type: TYPE_NORMAL
- en: Setting up your environment
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As a change in pace, I’ll illustrate this example using NumPyro instead of Pyro,
    though the code will work in Pyro with small tweaks. We’ll use NumPyro version
    0.15.0\. We’ll also use an inference library meant to complement NumPyro and Pyro
    called Funsor, version 0.4.5\. We’ll also use Matplotlib for plotting.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s build the model.
  prefs: []
  type: TYPE_NORMAL
- en: NumPyro vs. Pyro
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Pyro extends PyTorch, while NumPyro extends NumPy and automatic differentiation
    with JAX. The user interfaces are quite similar. If you are less comfortable with
    PyTorch abstractions and debugging PyTorch errors, or you prefer MCMC-based inference
    with the Bayesian programming patterns one uses in Stan or PyMC, then you might
    prefer NumPyro.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.13 Generating from *P*(*I**[E]*[=][0]) vs. *P*(*I**[E]*[=][0]|*E*=1)
    in Pyro
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '#1 A version of the online gaming model. The weights are estimates from the
    data (learning procedure not shown here).'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll apply the intervention and run inference to sample from *P*(*I*[*E*][=][0]).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.14 Apply intervention do(*E*=0) and infer from *P*(*I**[E]*[=][0])
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Apply the do operator to the model.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Apply inference to sample from P(I [E=0]).'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll contrast these samples from *P*(*I*[*E*][=][0]) with samples we’ll draw
    from *P*(*I**[E]*[=][0]|*E*=1). To infer *P*(*I**[E]*[=][0]|*E*=1), we’ll condition
    `intervention_model` on the factual condition *E*=1\. Then we’ll run inference
    again on this conditioned-upon intervened-upon model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.15 Condition intervention model and infer *P*(*I**[E]*[=][0]|*E*=1)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Now apply the condition operator to sample from P(I [E=0]|E=1).'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Apply inference to sample from P(I [E=0]|E=1).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that Pyro’s `do`and `condition`subroutines mutually compose; i.e., for
    a model with a variable *X*, `do(condition(model,` `{"X": 1.}),` `{"X":` `0.})`
    is equivalent to `condition(do(model,` `{"X":` `0.}),` `{"X":` `1.})`.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we’ll plot samples from *P*(*I**[E]*[=0]) and *P*(*I**[E]*[=0]|*E*=1)
    and evaluate the difference in these distributions.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.16 Plot samples from *P*(*I**[E]*[=][0]) and *P*(*I**[E]*[=][0]|*E*=1)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Plot a histogram of samples from P(I [E=0]).'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Plot a histogram of samples from P(I [E=0]|E=1).'
  prefs: []
  type: TYPE_NORMAL
- en: This code generates the histograms in figure 10.24.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F24_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.24 Histograms of samples from *P*(*I**[E]*[=0]) and *P*(*I**[E]*[=0]|*E*=1)
    generated in Pyro
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In this example, the parameters were given. In chapter 11, where we’ll look
    at estimation, we’ll seamlessly combine this query inference with Bayesian parameter
    inference from data.
  prefs: []
  type: TYPE_NORMAL
- en: 10.9 Partial identification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll close this chapter with a quick note on partial identification. Sometimes
    a query is not identified, given your assumptions, but it may be *partially identifiable*.
    Partial identifiability means you can identify estimands for an upper and lower
    bound of your query. Partial identification is highly relevant to causal AI because
    machine learning algorithms often rely on finding and optimizing bounds on objective
    functions. Let’s walk through a few examples.
  prefs: []
  type: TYPE_NORMAL
- en: MCMC vs. SVI
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Here we used Markov chain Monte Carlo (MCMC), but both Pyro and NumPyro have
    abstractions for stochastic variational inference (SVI). In this example, the
    parameters (`p_member`, `p_engaged`, etc.) of the model are specified. We could
    also make the parameters unknown variables with Bayesian priors and do the inference
    on these causal queries *P*(*I**[E]*[=0]) and *P*(*I**[E]*[=0]|*E*=1); in this
    case, we’d be doing Bayesian inference of these queries.
  prefs: []
  type: TYPE_NORMAL
- en: But for this we’d need *N* IID samples from an observational distribution where
    we had graphical identification (*P*(*G*, *E*, *W*, *I*), *P*(*G*, *E*, *I*),
    or *P*(*E*, *W*, *I*)). In the case of *P*(*G*, *E*, *W*, *I*), where all the
    variables in the DAG are observed, the number of unknown variables is just the
    number of parameters. But in the latter two cases, of *P*(*G*, *E*, *I*) or *P*(*E*,
    *W*, *I*), where there is a latent *G*or *W* of size *N*, the number of unknowns
    grows with *N*. In this case, SVI will scale better with large *N*. We’ll see
    an example in chapter 11.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose in our online gaming example you ran an experiment where you randomly
    assigned players to a treatment or control group. Players in the treatment group
    are exposed to a policy that encourages more side-quest engagement. You reason
    that since you can’t actually force players to engage in side-quests, it’s better
    to have this randomized treatment/control variable as a parent of our side-quest
    engagement variable, as seen in the DAG in figure 10.25.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH10_F25_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.25 We don’t have identification for the ATE of *E* on *I* because
    *G* and *W* are unobserved. But we have partial identification given variable
    *A*, representing gamers’ assignments in a randomized experiment.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For this new variable *A*, let *A*=1 refer to the treatment group and *A*=0
    refer to the control group. We have this new variable *A*, and the average treatment
    effect of the policy on in-game purchases *E*(*I*[*A*][=1] – *I*[*A*][=0]) is
    an interesting query. But suppose we’re still ultimately interested in knowing
    the average treatment effect of side-quest engagement *itself* on purchases, i.e.,
    *E*(*I*[*E*][=“high”] – *I*[*E*][=“low”]).
  prefs: []
  type: TYPE_NORMAL
- en: If guild membership (*G*) were observed, we’d have identification through backdoor
    adjustment. If won items (*W*) were observed, we could use front-door adjustment.
    But suppose that in this scenario you observe neither. In this case, observing
    the side-quest group assignment variable would give you partial identification.
    Suppose that the in-game purchases variable was a binary 1 for “high” and 0 for
    “low” instead of a continuous value. Then the bounds on *E*(*I*[*E*][=“high”]
    – *I*[*E*][=“low”]) are
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch10-eqs-8x.png)'
  prefs: []
  type: TYPE_IMG
- en: These bounds can be the next best thing to having full identification, especially
    if the bounds are tight. Alternatively, perhaps it is enough to know that the
    lower bound on the ATE for side-quest engagement is significantly greater than
    0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, general bounds exist for common counterfactual queries, such as
    probabilities of causation. For example, suppose you wanted to know if high side-quest
    engagement was a necessary and sufficient condition of high in-game purchases.
    You can construct the following bounds on the probability of necessity and sufficiency
    (PNS):'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/ness-ch10-eqs-9x.png)'
  prefs: []
  type: TYPE_IMG
- en: These bounds consist of level 2 quantities like *P*(*I*[*E*][=e]=*i*), and you
    can go on to identify level 1 estimands if possible given your assumptions.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that partial identification bounds are highly specific to your causal
    assumptions (like the DAG) and the parameterization of the variables; for example,
    the preceding examples are specific to binary variables. See the chapter notes
    at [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    for links to papers that derived these bounds as well as bounds for other practical
    sets of assumptions.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The importance of causal identification has increased in the AI era as we seek
    to understand the causal inductive bias in deep learning architectures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Libraries like y0 implement strategies for algorithmic identification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The causal hierarchy is a three-tiered structure that categorizes the causal
    questions we pose, the models we develop, and the causal inferences we draw. These
    levels are association, intervention, and counterfactual.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Association-level reasoning addresses “what is” questions and models that answer
    these questions with basic statistical assumptions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interventional or counterfactual queries fall on their corresponding level of
    the hierarchy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Observational data falls on the associational level, and experimental data falls
    on the interventional level of the hierarchy. Counterfactual data arises in situations
    where the modeler can control a deterministic data generating process (DGP).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Causal identification is the procedure of discerning when causal inferences
    can be drawn from experimental or observational data. It is done by determining
    if data at a lower level of the hierarchy can be used to infer a query at a higher
    level of the hierarchy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An example of a causal identification result is the backdoor formula, which
    equates intervention level query *P*(*Y*[*X*][=][*x*]) to association level quantity
    ∑[*z*]*P*(*Y*|*X*=*x*, *Z*=z)*P*(*Z*=z), where *Z* is a set of common causes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The causal hierarchy theorem shows how lower-level data is insufficient to infer
    a distribution at a higher level without higher-level modeling assumptions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The do-calculus has three rules that can be used for graph-based identification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A counterfactual graph is a DAG that includes variables across counterfactual
    worlds on one graph. Unlike the parallel world graph, it admits d-separation.
    We derive the counterfactual graph from the parallel world graph and the target
    query.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graphical identification algorithms automate identification with graphs using
    rules such as the do-calculus.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nonparametric identification is identification with non-graphical assumptions,
    such as assumptions about the functional relationships between variables in the
    model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ignorability assumption is that the causal variable and the potential outcomes
    are conditionally independent given confounders.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Effect of treatment on the treated (ETT) evaluates the effect of a cause on
    the subset of the population that was exposed to the cause.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Single world intervention graphs (SWIGs) provide an intuitive alternative to
    counterfactual identification with do-calculus and counterfactual graphs. They
    are constructed by applying a node-splitting operation to the original causal
    DAG. SWIGs use a “single-world” assumption, which assumes it’s possible to know
    a variable’s natural value while also intervening on it before it realizes that
    value without any side-effects that would affect that natural value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SWIGs work with variables and a narrow set of counterfactuals under the single-world
    assumption, while counterfactual graphs can accommodate queries that cannot be
    graphically identified.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pyro implements the SWIG’s node-splitting model of intervention, which enables
    probabilistic inference of SWIG-identified quantities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inference of causal queries using a causal graphical model and probabilistic
    inference algorithms is possible as long as the query is identified, given the
    model’s assumptions and training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Partial identification means you can at least identify estimands for bounds
    on a target query. This can be quite useful if you lack full identification, especially
    since machine learning often works by optimizing bounds on objective functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
