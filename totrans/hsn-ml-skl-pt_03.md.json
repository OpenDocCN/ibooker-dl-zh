["```py\nfrom pathlib import Path\nimport pandas as pd\nimport tarfile\nimport urllib.request\n\ndef load_housing_data():\n    tarball_path = Path(\"datasets/housing.tgz\")\n    if not tarball_path.is_file():\n        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n        urllib.request.urlretrieve(url, tarball_path)\n        with tarfile.open(tarball_path) as housing_tarball:\n            housing_tarball.extractall(path=\"datasets\", filter=\"data\")\n    return pd.read_csv(Path(\"datasets/housing/housing.csv\"))\n\nhousing_full = load_housing_data()\n```", "```py\n>>> housing_full.info() `<class 'pandas.core.frame.DataFrame'>`\n`RangeIndex: 20640 entries, 0 to 20639`\n`Data columns (total 10 columns):`\n `#   Column              Non-Null Count  Dtype`\n`---  ------              --------------  -----`\n `0   longitude           20640 non-null  float64`\n `1   latitude            20640 non-null  float64`\n `2   housing_median_age  20640 non-null  float64`\n `3   total_rooms         20640 non-null  float64`\n `4   total_bedrooms      20433 non-null  float64`\n `5   population          20640 non-null  float64`\n `6   households          20640 non-null  float64`\n `7   median_income       20640 non-null  float64`\n `8   median_house_value  20640 non-null  float64`\n `9   ocean_proximity     20640 non-null  object`\n`dtypes: float64(9), object(1)`\n`memory usage: 1.6+ MB`\n```", "```py`` ###### Note    In this book, when a code example contains a mix of code and outputs, as is the case here, it is formatted like in the Python interpreter for better readability: the code lines are prefixed with `>>>` (or `...` for indented blocks), and the outputs have no prefix.    There are 20,640 instances in the dataset, which means that it is fairly small by machine learning standards, but it’s perfect to get started. You notice that the `total_bedrooms` attribute has only 20,433 non-null values, meaning that 207 districts are missing this feature. You will need to take care of this later.    All attributes are numerical, except for `ocean_proximity`. Its type is `object`, so it could hold any kind of Python object. But since you loaded this data from a CSV file, you know that it must be a text attribute. When you looked at the top five rows, you probably noticed that the values in the `ocean_proximity` column were repetitive, which means that it is probably a categorical attribute. You can find out what categories exist and how many districts belong to each category by using the `value_counts()` method:    ```", "```py   ```", "```py import matplotlib.pyplot as plt  housing_full.hist(bins=50, figsize=(12, 8)) plt.show() ```", "```py` ```", "```py```", "``` ```", "````` ## Create a Test Set    Before you look at the data any further, you need to create a test set, put it aside, and never look at it. It may seem strange to voluntarily set aside part of the data at this stage. After all, you have only taken a quick glance at the data, and surely you should learn a whole lot more about it before you decide what algorithms to use, right? This is true, but your brain is an amazing pattern detection system, which also means that it is highly prone to overfitting: if you look at the test set, you may stumble upon some seemingly interesting pattern in the test data that leads you to select a particular kind of machine learning model. When you estimate the generalization error using the test set, your estimate will be too optimistic, and you will launch a system that will not perform as well as expected. This is called *data snooping* bias.    Creating a test set is theoretically simple; pick some instances randomly, typically 20% of the dataset (or less if your dataset is very large), and set them aside:    ```py import numpy as np  def shuffle_and_split_data(data, test_ratio, rng):     shuffled_indices = rng.permutation(len(data))     test_set_size = int(len(data) * test_ratio)     test_indices = shuffled_indices[:test_set_size]     train_indices = shuffled_indices[test_set_size:]     return data.iloc[train_indices], data.iloc[test_indices] ```    You can then use this function like this:    ```py >>> rng = np.random.default_rng()  # default random number generator `>>>` `train_set``,` `test_set` `=` `shuffle_and_split_data``(``housing_full``,` `0.2``,` `rng``)` ```` `>>>` `len``(``train_set``)` ```py `16512` `>>>` `len``(``test_set``)` `` `4128` `` ``` ```py` ```   ```py```` ```py``` `````", "```py from zlib import crc32  def is_id_in_test_set(identifier, test_ratio):     return crc32(np.int64(identifier)) < test_ratio * 2**32  def split_data_with_id_hash(data, test_ratio, id_column):     ids = data[id_column]     in_test_set = ids.apply(lambda id_: is_id_in_test_set(id_, test_ratio))     return data.loc[~in_test_set], data.loc[in_test_set] ```", "```py housing_with_id = housing_full.reset_index()  # adds an `index` column train_set, test_set = split_data_with_id_hash(housing_with_id, 0.2, \"index\") ```", "```py housing_with_id[\"id\"] = (housing_full[\"longitude\"] * 1000                          + housing_full[\"latitude\"]) train_set, test_set = split_data_with_id_hash(housing_with_id, 0.2, \"id\") ```", "```py from sklearn.model_selection import train_test_split  train_set, test_set = train_test_split(housing_full, test_size=0.2,                                        random_state=42) ```", "```py housing_full[\"income_cat\"] = pd.cut(housing_full[\"median_income\"],                                     bins=[0., 1.5, 3.0, 4.5, 6., np.inf],                                     labels=[1, 2, 3, 4, 5]) ```", "```py cat_counts = housing_full[\"income_cat\"].value_counts().sort_index() cat_counts.plot.bar(rot=0, grid=True) plt.xlabel(\"Income category\") plt.ylabel(\"Number of districts\") plt.show() ```", "```py from sklearn.model_selection import StratifiedShuffleSplit  splitter = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42) strat_splits = [] for train_index, test_index in splitter.split(housing_full,                                               housing_full[\"income_cat\"]):     strat_train_set_n = housing_full.iloc[train_index]     strat_test_set_n = housing_full.iloc[test_index]     strat_splits.append([strat_train_set_n, strat_test_set_n]) ```", "```py strat_train_set, strat_test_set = strat_splits[0] ```", "```py strat_train_set, strat_test_set = train_test_split(     housing_full, test_size=0.2, stratify=housing_full[\"income_cat\"],     random_state=42) ```", "```py >>> strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set) `income_cat` `3    0.350533` `2    0.318798` `4    0.176357` `5    0.114341` `1    0.039971` `Name: count, dtype: float64` ```", "```py`With similar code you can measure the income category proportions in the full dataset. [Figure 2-10](#compare_sampling_errors_screenshot) compares the income category proportions in the overall dataset, in the test set generated with stratified sampling, and in a test set generated using purely random sampling. As you can see, the test set generated using stratified sampling has income category proportions almost identical to those in the full dataset, whereas the test set generated using purely random sampling is skewed.  ![Table comparing income category proportions in the overall dataset, stratified sampling, and random sampling, highlighting lower errors in stratified sampling.](assets/hmls_0210.png)  ###### Figure 2-10\\. Sampling bias comparison of stratified versus purely random sampling    You won’t use the `income_cat` column again, so you might as well drop it, reverting the data back to its original state:    ```", "```py    We spent quite a bit of time on test set generation for a good reason: this is an often neglected but critical part of a machine learning project. Moreover, many of these ideas will be useful later when we discuss cross-validation. Now it’s time to move on to the next stage: exploring the data.```", "```py`` ```", "```py ```", "```py` ```", "```py`` ```", "```py```", "``````py``````", "```````py```````", "``````py``````", "```````py`````` ```py```````", "```````py```````", "``` housing = strat_train_set.copy() ```", "``` housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True) plt.show() ```", "``` housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True, alpha=0.2) plt.show() ```", "``` housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True,              s=housing[\"population\"] / 100, label=\"population\",              c=\"median_house_value\", cmap=\"jet\", colorbar=True,              legend=True, sharex=False, figsize=(10, 7)) plt.show() ```", "``` corr_matrix = housing.corr(numeric_only=True) ```", "``` >>> corr_matrix[\"median_house_value\"].sort_values(ascending=False) `median_house_value    1.000000` `median_income         0.688380` `total_rooms           0.137455` `housing_median_age    0.102175` `households            0.071426` `total_bedrooms        0.054635` `population           -0.020153` `longitude            -0.050859` `latitude             -0.139584` `Name: median_house_value, dtype: float64` ```", "````The correlation coefficient ranges from –1 to 1\\. When it is close to 1, it means that there is a strong positive correlation; for example, the median house value tends to go up when the median income goes up. When the coefficient is close to –1, it means that there is a strong negative correlation; you can see a small negative correlation between the latitude and the median house value (i.e., prices have a slight tendency to go down when you go north). Finally, coefficients close to 0 mean that there is no linear correlation.    Another way to check for correlation between attributes is to use the Pandas `scatter_matrix()` function, which plots every numerical attribute against every other numerical attribute. Since there are now 9 numerical attributes, you would get 9² = 81 plots, which would not fit on a page—so you decide to focus on a few promising attributes that seem most correlated with the median housing value ([Figure 2-14](#scatter_matrix_plot)):    ```py from pandas.plotting import scatter_matrix  attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",               \"housing_median_age\"] scatter_matrix(housing[attributes], figsize=(12, 8)) plt.show() ```  ![Scatter matrix displaying pairwise comparisons of median house value, median income, total rooms, and housing median age, with histograms on the diagonals.](assets/hmls_0214.png)  ###### Figure 2-14\\. This scatter matrix plots every numerical attribute against every other numerical attribute, plus a histogram of each numerical attribute’s values on the main diagonal (top left to bottom right)    The main diagonal would be full of straight lines if Pandas plotted each variable against itself, which would not be very useful. So instead, the Pandas displays a histogram of each attribute (other options are available; see the Pandas documentation for more details).    Looking at the correlation scatterplots, it seems like the most promising attribute to predict the median house value is the median income, so you zoom in on that scatterplot ([Figure 2-15](#income_vs_house_value_scatterplot)):    ```py housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",              alpha=0.1, grid=True) plt.show() ```  ![Scatter plot showing the correlation between median income and median house value, highlighting an upward trend and visible price caps at specific values.](assets/hmls_0215.png)  ###### Figure 2-15\\. Median income versus median house value    This plot reveals a few things. First, the correlation is indeed quite strong; you can clearly see the upward trend, although the data is noisy. Second, the price cap you noticed earlier is clearly visible as a horizontal line at $500,000\\. But the plot also reveals other less obvious straight lines: a horizontal line around $450,000, another around $350,000, perhaps one around $280,000, and a few more below that. You may want to try removing the corresponding districts to prevent your algorithms from learning to reproduce these data quirks.    Note that the correlation coefficient only measures linear correlations (“as *x* goes up, *y* generally goes up/down”). It may completely miss out on nonlinear relationships (e.g., “as *x* approaches 0, *y* generally goes up”). [Figure 2-16](#correlation_coefficient_plots) shows a variety of datasets along with their correlation coefficient. Note how all the plots of the bottom row have a correlation coefficient equal to 0, despite the fact that their axes are clearly *not* independent: these are examples of nonlinear relationships. Also, the second row shows examples where the correlation coefficient is equal to 1 or –1; notice that this has nothing to do with the slope. For example, your height in inches has a correlation coefficient of 1 with your height in feet or in nanometers.  ![Scatter plots of various datasets showing how correlation coefficients can indicate strong linear relationships in some cases but fail to capture nonlinear relationships, with coefficients ranging from -1 to 1.](assets/hmls_0216.png)  ###### Figure 2-16\\. Standard correlation coefficient of various datasets (source: Wikipedia; public domain image)```py`  ````", "```py housing[\"rooms_per_house\"] = housing[\"total_rooms\"] / housing[\"households\"] housing[\"bedrooms_ratio\"] = housing[\"total_bedrooms\"] / housing[\"total_rooms\"] housing[\"people_per_house\"] = housing[\"population\"] / housing[\"households\"] ```", "```py >>> corr_matrix = housing.corr(numeric_only=True) `>>>` `corr_matrix``[``\"median_house_value\"``]``.``sort_values``(``ascending``=``False``)` `` `median_house_value    1.000000` `median_income         0.688380` `rooms_per_house       0.143663` `total_rooms           0.137455` `housing_median_age    0.102175` `households            0.071426` `total_bedrooms        0.054635` `population           -0.020153` `people_per_house     -0.038224` `longitude            -0.050859` `latitude             -0.139584` `bedrooms_ratio       -0.256397` `Name: median_house_value, dtype: float64` `` ```", "```py ``Hey, not bad! The new `bedrooms_ratio` attribute is much more correlated with the median house value than the total number of rooms or bedrooms. It’s a strong negative correlation, so it looks like houses with a lower bedroom/room ratio tend to be more expensive. The number of rooms per household is also more informative than the total number of rooms in a district—obviously the larger the houses, the more expensive they are.    ###### Warning    When creating new combined features, make sure they are not too linearly correlated with existing features: *collinearity* can cause issues with some models, such as linear regression. In particular, avoid simple weighted sums of existing features.    This round of exploration does not have to be absolutely thorough; the point is to start off on the right foot and quickly gain insights that will help you get a first reasonably good prototype. But this is an iterative process: once you get a prototype up and running, you can analyze its output to gain more insights and come back to this exploration step.`` ```", "```py`  ```", "```py```", "````py````", "```py```", "```py```", "````py````", "```py```", "``` ```", "```````py```````", "````` # Prepare the Data for Machine Learning Algorithms    It’s time to prepare the data for your machine learning algorithms. Instead of doing this manually, you should write functions for this purpose, for several good reasons:    *   This will allow you to reproduce these transformations easily on any dataset (e.g., the next time you get a fresh dataset).           *   You will gradually build a library of transformation functions that you can reuse in future projects.           *   You can use these functions in your live system to transform the new data before feeding it to your algorithms.           *   This will make it possible for you to easily try various transformations and see which combination of transformations works best.              But first, revert to a clean training set (by copying `strat_train_set` once again). You should also separate the predictors and the labels, since you don’t necessarily want to apply the same transformations to the predictors and the target values (note that `drop()` creates a copy of the data and does not affect `strat_train_set`):    ```py housing = strat_train_set.drop(\"median_house_value\", axis=1) housing_labels = strat_train_set[\"median_house_value\"].copy() ```    ## Clean the Data    Most machine learning algorithms cannot work with missing features, so you’ll need to take care of these. For example, you noticed earlier that the `total_bedrooms` attribute has some missing values. You have three options to fix this:    1.  Get rid of the corresponding districts.           2.  Get rid of the whole attribute.           3.  Set the missing values to some value (zero, the mean, the median, etc.). This is called *imputation*.              You can accomplish these easily using the Pandas DataFrame’s `dropna()`, `drop()`, and `fillna()` methods:    ```py housing.dropna(subset=[\"total_bedrooms\"], inplace=True)  # option 1  housing.drop(\"total_bedrooms\", axis=1, inplace=True)  # option 2  median = housing[\"total_bedrooms\"].median()  # option 3 housing[\"total_bedrooms\"] = housing[\"total_bedrooms\"].fillna(median) ```    You decide to go for option 3 since it is the least destructive, but instead of the preceding code, you will use a handy Scikit-Learn class: `SimpleImputer`. The benefit is that it will store the median value of each feature: this will make it possible to impute missing values not only on the training set, but also on the validation set, the test set, and any new data fed to the model. To use it, first you need to create a `SimpleImputer` instance, specifying that you want to replace each attribute’s missing values with the median of that attribute:    ```py from sklearn.impute import SimpleImputer  imputer = SimpleImputer(strategy=\"median\") ```    Since the median can only be computed on numerical attributes, you then need to create a copy of the data with only the numerical attributes (this will exclude the text attribute `ocean_proximity`):    ```py housing_num = housing.select_dtypes(include=[np.number]) ```    Now you can fit the `imputer` instance to the training data using the `fit()` method:    ```py imputer.fit(housing_num) ```    The `imputer` has simply computed the median of each attribute and stored the result in its `statistics_` instance variable. Only the `total_bedrooms` attribute had missing values, but you cannot be sure that there won’t be any missing values in new data after the system goes live, so it is safer to apply the `imputer` to all the numerical attributes:    ```py >>> imputer.statistics_ `array([-118.51 , 34.26 , 29\\. , 2125\\. , 434\\. , 1167\\. , 408\\. , 3.5385])` `>>>` `housing_num``.``median``()``.``values` `` `array([-118.51 , 34.26 , 29\\. , 2125\\. , 434\\. , 1167\\. , 408\\. , 3.5385])` `` ```   ```py`` ```` Now you can use this “trained” `imputer` to transform the training set by replacing missing values with the learned medians:    ```py X = imputer.transform(housing_num) ```    Missing values can also be replaced with the mean value (`strategy=\"mean\"`), or with the most frequent value (`strategy=\"most_frequent\"`), or with a constant value (`strategy=\"constant\", fill_value=`…​). The last two strategies support non-numerical data.    ###### Tip    There are also more powerful imputers available in the `sklearn.​impute` package (both for numerical features only):    *   `KNNImputer` replaces each missing value with the mean of the *k*-nearest neighbors’ values for that feature. The distance is based on all the available features.           *   `IterativeImputer` trains a regression model per feature to predict the missing values based on all the other available features. It then trains the model again on the updated data, and repeats the process several times, improving the models and the replacement values at each iteration.              Scikit-Learn transformers output NumPy arrays (or sometimes SciPy sparse matrices) even when they are fed Pandas DataFrames as input.⁠^([11](ch02.html#id1107)) So, the output of `imputer.transform(housing_num)` is a NumPy array: `X` has neither column names nor index. Luckily, it’s not too hard to wrap `X` in a DataFrame and recover the column names and index from `housing_num`:    ```py housing_tr = pd.DataFrame(X, columns=housing_num.columns,                           index=housing_num.index) ``` ```py` `````", "```py```", "````py````", "```py```", "```py```", "````py````", "```py```", "``` >>> housing_cat = housing[[\"ocean_proximity\"]] `>>>` `housing_cat``.``head``(``8``)` `` `ocean_proximity` `13096        NEAR BAY` `14973       <1H OCEAN` `3785           INLAND` `14689          INLAND` `20507      NEAR OCEAN` `1286           INLAND` `18078       <1H OCEAN` `4396         NEAR BAY` `` ```", "``````py``````", "`````` ```py``````", "```````py` It’s not arbitrary text: there are a limited number of possible values, each of which represents a category. So this attribute is a categorical attribute. Most machine learning algorithms prefer to work with numbers, so let’s convert these categories from text to numbers. For this, we can use Scikit-Learn’s `OrdinalEncoder` class:    ``` from sklearn.preprocessing import OrdinalEncoder  ordinal_encoder = OrdinalEncoder() housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat) ```py    Here’s what the first few encoded values in `housing_cat_encoded` look like:    ``` >>> housing_cat_encoded[:8] `array([[3.],`  `[0.],`  `[1.],`  `[1.],`  `[4.],`  `[1.],`  `[0.],`  `[3.]])` ```py   ``````py```````", "```` You can get the list of categories using the `categories_` instance variable. It is a list containing a 1D array of categories for each categorical attribute (in this case, a list containing a single array since there is just one categorical attribute):    ```py >>> ordinal_encoder.categories_ `[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],`  `dtype=object)]` ```   ```py````", "```py```", "``` One issue with this representation is that ML algorithms will assume that two nearby values are more similar than two distant values. This may be fine in some cases (e.g., for ordered categories such as “bad”, “average”, “good”, and “excellent”), but it is obviously not the case for the `ocean_proximity` column (for example, categories 0 and 4 are clearly more similar than categories 0 and 1). To fix this issue, a common solution is to create one binary attribute per category: one attribute equal to 1 when the category is `\"<1H OCEAN\"` (and 0 otherwise), another attribute equal to 1 when the category is `\"INLAND\"` (and 0 otherwise), and so on. This is called *one-hot encoding*, because only one attribute will be equal to 1 (hot), while the others will be 0 (cold). The new attributes are sometimes called *dummy* attributes. Scikit-Learn provides a `OneHotEncoder` class to convert categorical values into one-hot vectors:    ```", "```    By default, the output of a `OneHotEncoder` is a SciPy *sparse matrix*, instead of a NumPy array:    ```", "```   ```", "```````py````` A sparse matrix is a very efficient representation for matrices that contain mostly zeros. Indeed, internally it only stores the nonzero values and their positions. When a categorical attribute has hundreds or thousands of categories, one-hot encoding it results in a very large matrix full of 0s except for a single 1 per row. In this case, a sparse matrix is exactly what you need: it will save plenty of memory and speed up computations. You can use a sparse matrix mostly like a normal 2D array,⁠^([12](ch02.html#id1118)) but if you want to convert it to a (dense) NumPy array, just call the `toarray()` method:    ```py >>> housing_cat_1hot.toarray() `array([[0., 0., 0., 1., 0.],`  `[1., 0., 0., 0., 0.],`  `[0., 1., 0., 0., 0.],`  `...,`  `[0., 0., 0., 0., 1.],`  `[1., 0., 0., 0., 0.],`  `[0., 0., 0., 0., 1.]], shape=(16512, 5))` ```   ```py```````", "```` Alternatively, you can set `sparse_output=False` when creating the `OneHotEncoder`, in which case the `transform()` method will return a regular (dense) NumPy array directly:    ```py cat_encoder = OneHotEncoder(sparse_output=False) housing_cat_1hot = cat_encoder.fit_transform(housing_cat)  # now a dense array ```    As with the `OrdinalEncoder`, you can get the list of categories using the encoder’s `categories_` instance variable:    ```py >>> cat_encoder.categories_ `[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],`  `dtype=object)]` ```   ```py````", "```py```", "``` >>> df_test = pd.DataFrame({\"ocean_proximity\": [\"INLAND\", \"NEAR BAY\"]}) `>>>` `pd``.``get_dummies``(``df_test``)` `` `ocean_proximity_INLAND  ocean_proximity_NEAR      BAY` `0                       True                      False` `1                       False                     True` `` ```", "``````py``````", "```py```", "````py` It looks nice and simple, so why not use it instead of `OneHotEncoder`? Well, the advantage of `OneHotEncoder` is that it remembers which categories it was trained on. This is very important because once your model is in production, it should be fed exactly the same features as during training: no more, no less. Look what our trained `cat_encoder` outputs when we make it transform the same `df_test` (using `transform()`, not `fit_transform()`):    ``` >>> cat_encoder.transform(df_test) `array([[0., 1., 0., 0., 0.],`  `[0., 0., 0., 1., 0.]])` ```py   ````", "```` See the difference? `get_dummies()` saw only two categories, so it output two columns, whereas `OneHotEncoder` output one column per learned category, in the right order. Moreover, if you feed `get_dummies()` a DataFrame containing an unknown category (e.g., `\"<2H OCEAN\"`), it will happily generate a column for it:    ```py >>> df_test_unknown = pd.DataFrame({\"ocean_proximity\": [\"<2H OCEAN\", \"ISLAND\"]}) `>>>` `pd``.``get_dummies``(``df_test_unknown``)` `` `ocean_proximity_<2H OCEAN  ocean_proximity_ISLAND` `0                       True                   False` `1                      False                    True` `` ```   ```py````", "```py```", "```py >>> cat_encoder.handle_unknown = \"ignore\" `>>>` `cat_encoder``.``transform``(``df_test_unknown``)` `` `array([[0., 0., 0., 0., 0.],`  `[0., 0., 1., 0., 0.]])` `` ```", "```py```", "```py```", "``` >>> cat_encoder.feature_names_in_ `array(['ocean_proximity'], dtype=object)` `>>>` `cat_encoder``.``get_feature_names_out``()` ```", "```` `... `                         `columns``=``cat_encoder``.``get_feature_names_out``(),` ```py `... `                         `index``=``df_test_unknown``.``index``)` `` `...` `` ``` ```py` ````", "```py   ```", "```py ```", "```py ```", "```py` ```", "```py`` ```", "```py```", "``````py```` ```py``````", "``````py``````", "```py```", "````py``` ````", "```````py` ``````py```````", "``````py``````", "``` ```", "```py```", "````py````", "```py```", "````py````", "```py` ```", "```py```", "````py````", "```py```", "````py` ````", "```` ```py````", "```py from sklearn.preprocessing import MinMaxScaler  min_max_scaler = MinMaxScaler(feature_range=(-1, 1)) housing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num) ```", "```py from sklearn.preprocessing import StandardScaler  std_scaler = StandardScaler() housing_num_std_scaled = std_scaler.fit_transform(housing_num) ```", "```py from sklearn.metrics.pairwise import rbf_kernel  age_simil_35 = rbf_kernel(housing[[\"housing_median_age\"]], [[35]], gamma=0.1) ```", "```py from sklearn.linear_model import LinearRegression  target_scaler = StandardScaler() scaled_labels = target_scaler.fit_transform(housing_labels.to_frame())  model = LinearRegression() model.fit(housing[[\"median_income\"]], scaled_labels) some_new_data = housing[[\"median_income\"]].iloc[:5]  # pretend this is new data  scaled_predictions = model.predict(some_new_data) predictions = target_scaler.inverse_transform(scaled_predictions) ```", "```py from sklearn.compose import TransformedTargetRegressor  model = TransformedTargetRegressor(LinearRegression(),                                    transformer=StandardScaler()) model.fit(housing[[\"median_income\"]], housing_labels) predictions = model.predict(some_new_data) ```", "```py from sklearn.preprocessing import FunctionTransformer  log_transformer = FunctionTransformer(np.log, inverse_func=np.exp) log_pop = log_transformer.transform(housing[[\"population\"]]) ```", "```py rbf_transformer = FunctionTransformer(rbf_kernel,                                       kw_args=dict(Y=[[35.]], gamma=0.1)) age_simil_35 = rbf_transformer.transform(housing[[\"housing_median_age\"]]) ```", "```py sf_coords = 37.7749, -122.41 sf_transformer = FunctionTransformer(rbf_kernel,                                      kw_args=dict(Y=[sf_coords], gamma=0.1)) sf_simil = sf_transformer.transform(housing[[\"latitude\", \"longitude\"]]) ```", "```py >>> ratio_transformer = FunctionTransformer(lambda X: X[:, [0]] / X[:, [1]]) `>>>` `ratio_transformer``.``transform``(``np``.``array``([[``1.``,` `2.``],` `[``3.``,` `4.``]]))` `` `array([[0.5 ],`  `[0.75]])` `` ```", "```py`` ```", "```py from sklearn.base import BaseEstimator, TransformerMixin from sklearn.utils.validation import check_array, check_is_fitted  class StandardScalerClone(BaseEstimator, TransformerMixin):     def __init__(self, with_mean=True):  # no *args or **kwargs!         self.with_mean = with_mean      def fit(self, X, y=None):  # y is required even though we don't use it         X = check_array(X)  # checks that X is an array with finite float values         self.mean_ = X.mean(axis=0)         self.scale_ = X.std(axis=0)         self.n_features_in_ = X.shape[1]  # every estimator stores this in fit()         return self  # always return self!      def transform(self, X):         check_is_fitted(self)  # looks for learned attributes (with trailing _)         X = check_array(X)         assert self.n_features_in_ == X.shape[1]         if self.with_mean:             X = X - self.mean_         return X / self.scale_ ```", "```py from sklearn.cluster import KMeans  class ClusterSimilarity(BaseEstimator, TransformerMixin):     def __init__(self, n_clusters=10, gamma=1.0, random_state=None):         self.n_clusters = n_clusters         self.gamma = gamma         self.random_state = random_state      def fit(self, X, y=None, sample_weight=None):         self.kmeans_ = KMeans(self.n_clusters, random_state=self.random_state)         self.kmeans_.fit(X, sample_weight=sample_weight)         return self  # always return self!      def transform(self, X):         return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)      def get_feature_names_out(self, names=None):         return [f\"Cluster {i} similarity\" for i in range(self.n_clusters)] ```", "```py cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42) similarities = cluster_simil.fit_transform(housing[[\"latitude\", \"longitude\"]]) ```", "```py >>> similarities[:3].round(2) `array([[0.46, 0\\.  , 0.08, 0\\.  , 0\\.  , 0\\.  , 0\\.  , 0.98, 0\\.  , 0\\.  ],`  `[0\\.  , 0.96, 0\\.  , 0.03, 0.04, 0\\.  , 0\\.  , 0\\.  , 0.11, 0.35],`  `[0.34, 0\\.  , 0.45, 0\\.  , 0\\.  , 0\\.  , 0.01, 0.73, 0\\.  , 0\\.  ]])` ```", "```py` ```", "```py```", "```py```", "```py```", "``` from sklearn.pipeline import Pipeline  num_pipeline = Pipeline([     (\"impute\", SimpleImputer(strategy=\"median\")),     (\"standardize\", StandardScaler()), ]) ```", "``` from sklearn.pipeline import make_pipeline  num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler()) ```", "``` >>> housing_num_prepared = num_pipeline.fit_transform(housing_num) `>>>` `housing_num_prepared``[:``2``]``.``round``(``2``)` `` `array([[-1.42,  1.01,  1.86,  0.31,  1.37,  0.14,  1.39, -0.94],`  `[ 0.6 , -0.7 ,  0.91, -0.31, -0.44, -0.69, -0.37,  1.17]])` `` ```", "````` ```py` As you saw earlier, if you want to recover a nice DataFrame, you can use the pipeline’s `get_feature_names_out()` method:    ``` df_housing_num_prepared = pd.DataFrame(     housing_num_prepared, columns=num_pipeline.get_feature_names_out(),     index=housing_num.index) ```py    Pipelines support indexing; for example, `pipeline[1]` returns the second estimator in the pipeline, and `pipeline[:-1]` returns a `Pipeline` object containing all but the last estimator. You can also access the estimators via the `steps` attribute, which is a list of name/estimator pairs, or via the `named_steps` dictionary attribute, which maps the names to the estimators. For example, `num_pipeline[\"simpleimputer\"]` returns the estimator named `\"simpleimputer\"`.    So far, we have handled the categorical columns and the numerical columns separately. It would be more convenient to have a single transformer capable of handling all columns, applying the appropriate transformations to each column. For this, you can use a `ColumnTransformer`. For example, the following `ColumnTransformer` will apply `num_pipeline` (the one we just defined) to the numerical attributes, and `cat_pipeline` to the categorical attribute:    ``` from sklearn.compose import ColumnTransformer  num_attribs = [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\",                \"total_bedrooms\", \"population\", \"households\", \"median_income\"] cat_attribs = [\"ocean_proximity\"]  cat_pipeline = make_pipeline(     SimpleImputer(strategy=\"most_frequent\"),     OneHotEncoder(handle_unknown=\"ignore\"))  preprocessing = ColumnTransformer([     (\"num\", num_pipeline, num_attribs),     (\"cat\", cat_pipeline, cat_attribs), ]) ```py    First we import the `ColumnTransformer` class, then we define the list of numerical and categorical column names and construct a simple pipeline for categorical attributes. Lastly, we construct a `ColumnTransformer`. Its constructor requires a list of triplets (3-tuples), each containing a name (which must be unique and not contain double underscores), a transformer, and a list of names (or indices) of columns that the transformer should be applied to.    ###### Tip    Instead of using a transformer, you can specify the string `\"drop\"` if you want the columns to be dropped, or you can specify `\"passthrough\"` if you want the columns to be left untouched. By default, the remaining columns (i.e., the ones that were not listed) will be dropped, but you can set the `remainder` hyperparameter to any transformer (or to `\"passthrough\"`) if you want these columns to be handled differently.    Since listing all the column names is not very convenient, Scikit-Learn provides a `make_column_selector` class that you can use to automatically select all the features of a given type, such as numerical or categorical. You can pass a selector to the `ColumnTransformer` instead of column names or indices. Moreover, if you don’t care about naming the transformers, you can use `make_column_transformer()`, which chooses the names for you, just like `make_pipeline()` does. For example, the following code creates the same `ColumnTransformer` as earlier, except the transformers are automatically named `\"pipeline-1\"` and `\"pipeline-2\"` instead of `\"num\"` and `\"cat\"`:    ``` from sklearn.compose import make_column_selector, make_column_transformer  preprocessing = make_column_transformer(     (num_pipeline, make_column_selector(dtype_include=np.number)),     (cat_pipeline, make_column_selector(dtype_include=object)), ) ```py    Now we’re ready to apply this `ColumnTransformer` to the housing data:    ``` housing_prepared = preprocessing.fit_transform(housing) ```py    Great! We have a preprocessing pipeline that takes the entire training dataset and applies each transformer to the appropriate columns, then concatenates the transformed columns horizontally (transformers must never change the number of rows). Once again this returns a NumPy array, but you can get the column names using `preprocessing.get_feature_names_out()` and wrap the data in a nice DataFrame as we did before.    ###### Note    The `OneHotEncoder` returns a sparse matrix and the `num_pipeline` returns a dense matrix. When there is such a mix of sparse and dense matrices, the `ColumnTransformer` estimates the density of the final matrix (i.e., the ratio of nonzero cells), and it returns a sparse matrix if the density is lower than a given threshold (by default, `sparse_threshold=0.3`). In this example, it returns a dense matrix.    Your project is going really well and you’re almost ready to train some models! You now want to create a single pipeline that will perform all the transformations you’ve experimented with up to now. Let’s recap what the pipeline will do and why:    *   Missing values in numerical features will be imputed by replacing them with the median, as most ML algorithms don’t expect missing values. In categorical features, missing values will be replaced by the most frequent category.           *   The categorical feature will be one-hot encoded, as most ML algorithms only accept numerical inputs.           *   A few ratio features will be computed and added: `bedrooms_ratio`, `rooms_​per_house`, and `people_per_house`. Hopefully these will better correlate with the median house value, and thereby help the ML models.           *   A few cluster similarity features will also be added. These will likely be more useful to the model than latitude and longitude.           *   Features with a long tail will be replaced by their logarithm, as most models prefer features with roughly uniform or Gaussian distributions.           *   All numerical features will be standardized, as most ML algorithms prefer when all features have roughly the same scale.              The code that builds the pipeline to do all of this should look familiar to you by now:    ``` def column_ratio(X):     return X[:, [0]] / X[:, [1]]  def ratio_name(function_transformer, feature_names_in):     return [\"ratio\"]  # feature names out  def ratio_pipeline():     return make_pipeline(         SimpleImputer(strategy=\"median\"),         FunctionTransformer(column_ratio, feature_names_out=ratio_name),         StandardScaler())  log_pipeline = make_pipeline(     SimpleImputer(strategy=\"median\"),     FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),     StandardScaler()) cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42) default_num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"),                                      StandardScaler()) preprocessing = ColumnTransformer([         (\"bedrooms\", ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]),         (\"rooms_per_house\", ratio_pipeline(), [\"total_rooms\", \"households\"]),         (\"people_per_house\", ratio_pipeline(), [\"population\", \"households\"]),         (\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\",                                \"households\", \"median_income\"]),         (\"geo\", cluster_simil, [\"latitude\", \"longitude\"]),         (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)),     ],     remainder=default_num_pipeline)  # one column remaining: housing_median_age ```py    If you run this `ColumnTransformer`, it performs all the transformations and outputs a NumPy array with 24 features:    ``` >>> housing_prepared = preprocessing.fit_transform(housing) `>>>` `housing_prepared``.``shape` ```py `(16512, 24)` `>>>` `preprocessing``.``get_feature_names_out``()` `` `array(['bedrooms__ratio', 'rooms_per_house__ratio',`  `'people_per_house__ratio', 'log__total_bedrooms',`  `'log__total_rooms', 'log__population', 'log__households',`  `'log__median_income', 'geo__Cluster 0 similarity', [...],`  `'geo__Cluster 9 similarity', 'cat__ocean_proximity_<1H OCEAN',`  `'cat__ocean_proximity_INLAND', 'cat__ocean_proximity_ISLAND',`  `'cat__ocean_proximity_NEAR BAY', 'cat__ocean_proximity_NEAR OCEAN',`  `'remainder__housing_median_age'], dtype=object)` `` ``` ```py ```` ```py`` `````", "``````py` ``````", "``````py``` ``````", "```` ```py````", "```py` ```", "```py```", "````py````", "```py ```", "```py```", "````py````", "```py`  ```", "```py```", "````py``` ````", "```````py`` ``````py```````", "``` from sklearn.linear_model import LinearRegression  lin_reg = make_pipeline(preprocessing, LinearRegression()) lin_reg.fit(housing, housing_labels) ```", "``` >>> housing_predictions = lin_reg.predict(housing) `>>>` `housing_predictions``[:``5``]``.``round``(``-``2``)`  `# -2 = rounded to the nearest hundred` ```", "``` ```", "``````py``` ``````", "``````py` Well, it works, but not always: the first prediction is way off (by over $200,000!), while the other predictions are better: two are off by about 25%, and two are off by less than 10%. Remember that you chose to use the RMSE as your performance measure, so you want to measure this regression model’s RMSE on the whole training set using Scikit-Learn’s `root_mean_squared_error()` function:    ``` >>> from sklearn.metrics import root_mean_squared_error `>>>` `lin_rmse` `=` `root_mean_squared_error``(``housing_labels``,` `housing_predictions``)` ```py `>>>` `lin_rmse` `` `68972.88910758484` `` ``` ```py   ``````", "````` ```py` ###### Tip    We’re not using the `score()` method here because it returns the *R² coefficient of determination* instead of the RMSE. This coefficient represents the ratio of the variance in the data that the model can explain: the closer to 1 (which is the max value), the better. If the model simply predicts the mean all the time, it does not explain any part of the variance, so the model’s R² score is 0\\. And if the model does even worse than that, then its R² score can be negative, and indeed arbitrarily low.    This is better than nothing, but clearly not a great score: the `median_housing_values` of most districts range between $120,000 and $265,000, so a typical prediction error of $68,973 is really not very satisfying. This is an example of a model underfitting the training data. When this happens it can mean that the features do not provide enough information to make good predictions, or that the model is not powerful enough. As we saw in the previous chapter, the main ways to fix underfitting are to select a more powerful model, to feed the training algorithm with better features, or to reduce the constraints on the model. This model is not regularized, which rules out the last option. You could try to add more features, but first you want to try a more complex model to see how it does.    You decide to try a `DecisionTreeRegressor`, as this is a fairly powerful model capable of finding complex nonlinear relationships in the data (decision trees are presented in more detail in [Chapter 5](ch05.html#trees_chapter)):    ``` from sklearn.tree import DecisionTreeRegressor  tree_reg = make_pipeline(preprocessing, DecisionTreeRegressor(random_state=42)) tree_reg.fit(housing, housing_labels) ```py    Now that the model is trained, you evaluate it on the training set:    ``` >>> housing_predictions = tree_reg.predict(housing) `>>>` `tree_rmse` `=` `root_mean_squared_error``(``housing_labels``,` `housing_predictions``)` ```py `>>>` `tree_rmse` `` `0.0` `` ``` ```py   ``` `` `Wait, what!? No error at all? Could this model really be absolutely perfect? Of course, it is much more likely that the model has badly overfit the data. How can you be sure? As you saw earlier, you don’t want to touch the test set until you are ready to launch a model you are confident about, so you need to use part of the training set for training and part of it for model validation.` `` ```py ```` ```py`` `````", "``````py` ``````", "``````py```  ``````", "``````py ````` ## Better Evaluation Using Cross-Validation    One way to evaluate the decision tree model would be to use the `train_​test_split()` function to split the training set into a smaller training set and a validation set, then train your models against the smaller training set and evaluate them against the validation set. It’s a bit of effort, but nothing too difficult, and it would work fairly well.    A great alternative is to use Scikit-Learn’s *k-fold cross-validation* feature. You split the training set into *k* nonoverlapping subsets called *folds*, then you train and evaluate your model *k* times, picking a different fold for evaluation every time (i.e., the validation fold) and using the other *k* – 1 folds for training. This process produces *k* evaluation scores (see [Figure 2-20](#k_fold_cross_validation_diagram)).  ![Diagram illustrating _k_-fold cross-validation with _k_ = 10, displaying different validation folds and corresponding evaluation scores for each split.](assets/hmls_0220.png)  ###### Figure 2-20\\. *k*-fold cross-validation, with *k* = 10    Scikit-Learn provides a convenient `cross_val_score()` function that does just that, and it returns an array containing the *k* evaluation scores. For example, let’s use it to evaluate our tree regressor, using *k* = 10:    ```py from sklearn.model_selection import cross_val_score  tree_rmses = -cross_val_score(tree_reg, housing, housing_labels,                               scoring=\"neg_root_mean_squared_error\", cv=10) ```    ###### Warning    Scikit-Learn’s cross-validation features expect a utility function (greater is better) rather than a cost function (lower is better), so the scoring function is actually the opposite of the RMSE. It’s a negative value, so you need to switch the sign of the output to get the RMSE scores.    Let’s look at the results:    ```py >>> pd.Series(tree_rmses).describe() `count       10.000000` `mean     66573.734600` `std       1103.402323` `min      64607.896046` `25%      66204.731788` `50%      66388.272499` `75%      66826.257468` `max      68532.210664` `dtype: float64` ```   ```py` Now the decision tree doesn’t look as good as it did earlier. In fact, it seems to perform almost as poorly as the linear regression model! Notice that cross-validation allows you to get not only an estimate of the performance of your model, but also a measure of how precise this estimate is (i.e., its standard deviation). The decision tree has an RMSE of about 66,574, with a standard deviation of about 1,103\\. You would not have this information if you just used one validation set. But cross-validation comes at the cost of training the model several times, so it is not always feasible.    If you compute the same metric for the linear regression model, you will find that the mean RMSE is 70,003 and the standard deviation is 4,182\\. So the decision tree model seems to perform very slightly better than the linear model, but the difference is minimal due to severe overfitting. We know there’s an overfitting problem because the training error is low (actually zero) while the validation error is high.    Let’s try one last model now: the `RandomForestRegressor`. As you will see in [Chapter 6](ch06.html#ensembles_chapter), random forests work by training many decision trees on random subsets of the features, then averaging out their predictions. Such models composed of many other models are called *ensembles*: if the underlying models are very diverse, then their errors will not be very correlated, and therefore averaging out the predictions will smooth out the errors, reduce overfitting, and improve the overall performance. The code is much the same as earlier:    ``` from sklearn.ensemble import RandomForestRegressor  forest_reg = make_pipeline(preprocessing,                            RandomForestRegressor(random_state=42)) forest_rmses = -cross_val_score(forest_reg, housing, housing_labels,                                 scoring=\"neg_root_mean_squared_error\", cv=10) ```py    Let’s look at the scores:    ``` >>> pd.Series(forest_rmses).describe() `count       10.000000` `mean     47038.092799` `std       1021.491757` `min      45495.976649` `25%      46510.418013` `50%      47118.719249` `75%      47480.519175` `max      49140.832210` `dtype: float64` ```py   ``Wow, this is much better: random forests really look very promising for this task! However, if you train a `RandomForestRegressor` and measure the RMSE on the training set, you will find roughly 17,551: that’s much lower, meaning that there’s still quite a lot of overfitting going on. Possible solutions are to simplify the model, constrain it (i.e., regularize it), or get a lot more training data. Before you dive much deeper into random forests, however, you should try out many other models from various categories of machine learning algorithms (e.g., several support vector machines with different kernels, and possibly a neural network), without spending too much time tweaking the hyperparameters. The goal is to shortlist a few (two to five) promising models.`` ```` ```py`` ``````", "``````py`  ``````", "```````py ``````py`````` ```py```````", "``` from sklearn.model_selection import GridSearchCV  full_pipeline = Pipeline([     (\"preprocessing\", preprocessing),     (\"random_forest\", RandomForestRegressor(random_state=42)), ]) param_grid = [     {'preprocessing__geo__n_clusters': [5, 8, 10],      'random_forest__max_features': [4, 6, 8]},     {'preprocessing__geo__n_clusters': [10, 15],      'random_forest__max_features': [6, 8, 10]}, ] grid_search = GridSearchCV(full_pipeline, param_grid, cv=3,                            scoring='neg_root_mean_squared_error') grid_search.fit(housing, housing_labels) ```", "``` >>> grid_search.best_params_ `{'preprocessing__geo__n_clusters': 15, 'random_forest__max_features': 6}` ```", "````` In this example, the best model is obtained by setting `n_clusters` to 15 and setting `max_features` to 6.    ###### Tip    Since 15 is the maximum value that was evaluated for `n_clusters`, you should probably try searching again with higher values; the score may continue to improve.    You can access the best estimator using `grid_search.best_estimator_`. If `Grid​SearchCV` is initialized with `refit=True` (which is the default), then once it finds the best estimator using cross-validation, it retrains it on the whole training set. This is usually a good idea, since feeding it more data will likely improve its performance.    The evaluation scores are available using `grid_search.cv_results_`. This is a dictionary, but if you wrap it in a DataFrame you get a nice list of all the test scores for each combination of hyperparameters and for each cross-validation split, as well as the mean test score across all splits:    ```py >>> cv_res = pd.DataFrame(grid_search.cv_results_) `>>>` `cv_res``.``sort_values``(``by``=``\"mean_test_score\"``,` `ascending``=``False``,` `inplace``=``True``)` ```` `>>>` `[``...``]`  `# change column names to fit on this page, and show rmse = -score` ```py `>>>` `cv_res``.``head``()`  `# note: the 1st column is the row ID` `` `n_clusters  max_features  split0  split1  split2  mean_test_rmse` `12          15             6   42725   43708   44335           43590` `13          15             8   43486   43820   44900           44069` `6           10             4   43798   44036   44961           44265` `9           10             6   43710   44163   44967           44280` `7           10             6   43710   44163   44967           44280` `` ``` ```py` ```   ```py `` `The mean test RMSE score for the best model is 43,590, which is better than the score you got earlier using the default hyperparameter values (which was 47,038). Congratulations, you have successfully fine-tuned your best model!` `` ``` ```py``  `````", "```` ```py````", "```py```", "```py from sklearn.model_selection import RandomizedSearchCV from scipy.stats import randint  param_distribs = {'preprocessing__geo__n_clusters': randint(low=3, high=50),                   'random_forest__max_features': randint(low=2, high=20)}  rnd_search = RandomizedSearchCV(     full_pipeline, param_distributions=param_distribs, n_iter=10, cv=3,     scoring='neg_root_mean_squared_error', random_state=42)  rnd_search.fit(housing, housing_labels) ```", "```py >>> final_model = rnd_search.best_estimator_  # includes preprocessing `>>>` `feature_importances` `=` `final_model``[``\"random_forest\"``]``.``feature_importances_` ```", "```py ```", "```py```", "```py```", "````` Let’s sort these importance scores in descending order and display them next to their corresponding attribute names:    ```py >>> sorted(zip(feature_importances, `... `           `final_model``[``\"preprocessing\"``]``.``get_feature_names_out``()),` ```` `... `       `reverse``=``True``)` ```py `...` `` `[(np.float64(0.18599734460509476), 'log__median_income'),`  `(np.float64(0.07338850855844489), 'cat__ocean_proximity_INLAND'),`  `(np.float64(0.06556941990883976), 'bedrooms__ratio'),`  `(np.float64(0.053648710076725316), 'rooms_per_house__ratio'),`  `(np.float64(0.04598870861894749), 'people_per_house__ratio'),`  `(np.float64(0.04175269214442519), 'geo__Cluster 30 similarity'),`  `(np.float64(0.025976797232869678), 'geo__Cluster 25 similarity'),`  `(np.float64(0.023595895886342255), 'geo__Cluster 36 similarity'),`  `[...]`  `(np.float64(0.0004325970342247361), 'cat__ocean_proximity_NEAR BAY'),`  `(np.float64(3.0190221102670295e-05), 'cat__ocean_proximity_ISLAND')]` `` ``` ```py` ```   ```py` ``` ``With this information, you may want to try dropping some of the less useful features (e.g., apparently only one `ocean_proximity` category is really useful, so you could try dropping the others).    ###### Tip    The `sklearn.feature_selection.SelectFromModel` transformer can automatically drop the least useful features for you: when you fit it, it trains a model (typically a random forest), looks at its `feature_importances_` attribute, and selects the most useful features. Then when you call `transform()`, it drops the other features.    You should also look at the specific errors that your system makes, then try to understand why it makes them and what could fix the problem: adding extra features or getting rid of uninformative ones, cleaning up outliers, etc.    Now is also a good time to check *model fairness*: it should not only work well on average, but also on various categories of districts, whether they’re rural or urban, rich or poor, northern or southern, minority or not, etc. This requires a detailed *bias analysis*: creating subsets of your validation set for each category, and analyzing your model’s performance on them. That’s a lot of work, but it’s important: if your model performs poorly on a whole category of districts, then it should probably not be deployed until the issue is resolved, or at least it should not be used to make predictions for that category, as it may do more harm than good.`` ```py ```` ```py`` `````", "``````py`  ``````", "````` ```py`## Evaluate Your System on the Test Set    After tweaking your models for a while, you eventually have a system that performs sufficiently well. You are ready to evaluate the final model on the test set. There is nothing special about this process; just get the predictors and the labels from your test set and run your `final_model` to transform the data and make predictions, then evaluate these predictions:    ``` X_test = strat_test_set.drop(\"median_house_value\", axis=1) y_test = strat_test_set[\"median_house_value\"].copy()  final_predictions = final_model.predict(X_test)  final_rmse = root_mean_squared_error(y_test, final_predictions) print(final_rmse)  # prints 41445.533268606625 ```py    In some cases, such a point estimate of the generalization error will not be quite enough to convince you to launch: what if it is just 0.1% better than the model currently in production? You might want to have an idea of how precise this estimate is. For this, you can compute a 95% *confidence interval* for the generalization error using `scipy.stats.bootstrap()`. You get a fairly large interval from 39,521 to 43,702, and your previous point estimate of 41,445 is roughly in the middle of it:    ``` from scipy import stats  def rmse(squared_errors):     return np.sqrt(np.mean(squared_errors))  confidence = 0.95 squared_errors = (final_predictions - y_test) ** 2 boot_result = stats.bootstrap([squared_errors], rmse,                               confidence_level=confidence, random_state=42) rmse_lower, rmse_upper = boot_result.confidence_interval ```py    If you do a lot of hyperparameter tuning, the performance will usually be slightly worse than what you measured using cross-validation. That’s because your system ends up fine-tuned to perform well on the validation data and will likely not perform as well on unknown datasets. That’s not the case in this example since the test RMSE is lower than the validation RMSE, but when it happens you must resist the temptation to tweak the hyperparameters to make the numbers look good on the test set; the improvements would be unlikely to generalize to new data.    Now comes the project prelaunch phase. Presenting your solution effectively is what sets great data scientists apart from good ones. You should create concise reports (Markdown, PDFs, slides), visualize key insights (e.g., using Matplotlib or other tools such as SeaBorn or Tableau), and tailor your message to the audience: technical for peers, high-level for stakeholders. Provide impactful and easy-to-remember statements (e.g., “the median income is the number one predictor of housing prices”). Highlight what you have learned, what worked and what did not, what assumptions were made, and what your system’s limitations are.    Your results should be reproducible (as much as possible): make the code accessible to your team (e.g., via GitHub), add a structured *README* file to guide a technical person through the installation steps. Provide clear notebooks (e.g., Jupyter) with code, explanations, and results, writing clean, well-commented code. Define a *requirements.txt* or *environment.yml* file containing all the required libraries along with their precise versions (or create a Docker image). Set seeds for random generators, and remove any other source of variability.    In this California housing example, the final performance of the system is not much better than the experts’ price estimates, which were often off by 30%, but it may still be a good idea to launch it, especially if this frees up some time for the experts so they can work on more interesting and productive tasks.```` ```py`` `````", "``````py`` ``````", "``` ```", "```py```", "```py```", "````` ```py`# Launch, Monitor, and Maintain Your System    Perfect, you got approval to launch! You now need to get your solution ready for production (e.g., polish the code, write documentation and tests, and so on). Then you can deploy your model to your production environment. The most basic way to do this is just to save the best model you trained, transfer the file to your production environment, and load it. To save the model, you can use the `joblib` library like this:    ``` import joblib  joblib.dump(final_model, \"my_california_housing_model.pkl\") ```py    ###### Tip    It’s often a good idea to save every model you experiment with so that you can come back easily to any model you want. You may also save the cross-validation scores and perhaps the actual predictions on the validation set. This will allow you to easily compare scores across model types, and compare the types of errors they make.    Once your model is transferred to production, you can load it and use it. For this you must first import any custom classes and functions the model relies on (which means transferring the code to production), then load the model using `joblib` and use it to make predictions:    ``` import joblib [...]  # import KMeans, BaseEstimator, TransformerMixin, rbf_kernel, etc.  def column_ratio(X): [...] def ratio_name(function_transformer, feature_names_in): [...] class ClusterSimilarity(BaseEstimator, TransformerMixin): [...]  final_model_reloaded = joblib.load(\"my_california_housing_model.pkl\")  new_data = [...]  # some new districts to make predictions for predictions = final_model_reloaded.predict(new_data) ```py    For example, perhaps the model will be used within a website: the user will type in some data about a new district and click the Estimate Price button. This will send a query containing the data to the web server, which will forward it to your web application, and finally your code will simply call the model’s `predict()` method (you want to load the model upon server startup, rather than every time the model is used). Alternatively, you can wrap the model within a dedicated web service that your web application can query through a REST API⁠^([14](ch02.html#id1268)) (see [Figure 2-21](#webservice_model_diagram)). This makes it easier to upgrade your model to new versions without interrupting the main application. It also simplifies scaling, since you can start as many web services as needed and load-balance the requests coming from your web application across these web services. Moreover, it allows your web application to use any programming language, not just Python.  ![Diagram showing a user interacting with a web app, which sends inputs to a web service hosting a model and receives predictions in return.](assets/hmls_0221.png)  ###### Figure 2-21\\. A model deployed as a web service and used by a web application    Another popular strategy is to deploy your model to the cloud, for example on Google’s Vertex AI (formerly Google Cloud AI Platform and Google Cloud ML Engine): just save your model using `joblib` and upload it to Google Cloud Storage (GCS), then go to Vertex AI and create a new model version, pointing it to the GCS file. That’s it! This gives you a simple web service that takes care of load balancing and scaling for you. It takes JSON requests containing the input data (e.g., of a district) and returns JSON responses containing the predictions. You can then use this web service in your website (or whatever production environment you are using).    But deployment is not the end of the story. You also need to write monitoring code to check your system’s live performance at regular intervals and trigger alerts when it drops. It may drop very quickly, for example if a component breaks in your infrastructure, but be aware that it could also decay very slowly, which can easily go unnoticed for a long time. This is quite common because of data drift: if the model was trained with last year’s data, it may not be adapted to today’s data.    So, you need to monitor your model’s live performance. But how do you do that? Well, it depends. In some cases, the model’s performance can be inferred from downstream metrics. For example, if your model is part of a recommender system and it suggests products that the users may be interested in, then it’s easy to monitor the number of recommended products sold each day. If this number drops (compared to nonrecommended products), then the prime suspect is the model. This may be because the data pipeline is broken, or perhaps the model needs to be retrained on fresh data (as we will discuss shortly).    However, you may also need human analysis to assess the model’s performance. For example, suppose you trained an image classification model (we’ll look at these in [Chapter 3](ch03.html#classification_chapter)) to detect various product defects on a production line. How can you get an alert if the model’s performance drops, before thousands of defective products get shipped to your clients? One solution is to send to human raters a sample of all the pictures that the model classified (especially pictures that the model wasn’t so sure about). Depending on the task, the raters may need to be experts, or they could be nonspecialists, such as workers on a crowdsourcing platform (e.g., Amazon Mechanical Turk). In some applications they could even be the users themselves, responding, for example, via surveys or repurposed captchas.⁠^([15](ch02.html#id1274))    Either way, you need to put in place a monitoring system (with or without human raters to evaluate the live model), as well as all the relevant processes to define what to do in case of failures and how to prepare for them. Unfortunately, this can be a lot of work. In fact, it is often much more work than building and training a model.    If the data keeps evolving, you will need to update your datasets and retrain your model regularly. You should probably automate the whole process as much as possible. Here are a few things you can automate:    *   Collect fresh data regularly and label it (e.g., using human raters).           *   Write a script to train the model and fine-tune the hyperparameters automatically. This script could run automatically, for example every day or every week, depending on your needs.           *   Write another script that will evaluate both the new model and the previous model on the updated test set, and deploy the model to production if the performance has not decreased (if it did, make sure you investigate why). The script should probably test the performance of your model on various subsets of the test set, such as poor or rich districts, rural or urban districts, etc.              You should also make sure you evaluate the model’s input data quality. Sometimes performance will degrade slightly because of a poor-quality signal (e.g., a malfunctioning sensor sending random values, or another team’s output becoming stale), but it may take a while before your system’s performance degrades enough to trigger an alert. If you monitor your model’s inputs, you may catch this earlier. For example, you could trigger an alert if more and more inputs are missing a feature, or the mean or standard deviation drifts too far from the training set, or a categorical feature starts containing new categories.    Finally, make sure you keep backups of every model you create and have the process and tools in place to roll back to a previous model quickly, in case the new model starts failing badly for some reason. Having backups also makes it possible to easily compare new models with previous ones. Similarly, you should keep backups of every version of your datasets so that you can roll back to a previous dataset if the new one ever gets corrupted (e.g., if the fresh data that gets added to it turns out to be full of outliers). Having backups of your datasets also allows you to evaluate any model against any previous dataset.    As you can see, machine learning involves quite a lot of infrastructure. This is a very broad topic called *ML Operations* (MLOps), which deserves its own book. So don’t be surprised if your first ML project takes a lot of effort and time to build and deploy to production. Fortunately, once all the infrastructure is in place, going from idea to production will be much faster.    # Try It Out!    Hopefully this chapter gave you a good idea of what a machine learning project looks like as well as showing you some of the tools you can use to train a great system. As you can see, much of the work is in the data preparation step: building monitoring tools, setting up human evaluation pipelines, and automating regular model training. The machine learning algorithms are important, of course, but it is probably preferable to be comfortable with the overall process and know three or four algorithms well rather than to spend all your time exploring advanced algorithms.    So, if you have not already done so, now is a good time to pick up a laptop, select a dataset that you are interested in, and try to go through the whole process from A to Z. A good place to start is on a competition website such as [Kaggle](https://kaggle.com): you will have a dataset to play with, a clear goal, and people to share the experience with. Have fun!    # Exercises    The following exercises are based on this chapter’s housing dataset:    1.  Try a support vector machine regressor (`sklearn.svm.SVR`) with various hyperparameters, such as `kernel=\"linear\"` (with various values for the `C` hyperparameter) or `kernel=\"rbf\"` (with various values for the `C` and `gamma` hyperparameters). Note that support vector machines don’t scale well to large datasets, so you should probably train your model on just the first 5,000 instances of the training set and use only 3-fold cross-validation, or else it will take hours. Don’t worry about what the hyperparameters mean for now; these are explained in the online chapter on SVMs at [*https://homl.info/*](https://homl.info/). How does the best `SVR` predictor perform?           2.  Try replacing the `GridSearchCV` with a `RandomizedSearchCV`.           3.  Try adding a `SelectFromModel` transformer in the preparation pipeline to select only the most important attributes.           4.  Try creating a custom transformer that trains a *k*-nearest neighbors regressor (`sklearn.neighbors.KNeighborsRegressor`) in its `fit()` method, and outputs the model’s predictions in its `transform()` method. The KNN regressor should use only the latitude and longitude as input and predict the median income. Next, add this new transformer to the preprocessing pipeline. This will add a feature representing the smoothed median income over the nearby districts.           5.  Automatically explore some preparation options using `RandomizedSearchCV`.           6.  Try to implement the `StandardScalerClone` class again from scratch, then add support for the `inverse_transform()` method: executing `scaler.​inverse_transform(scaler.fit_transform(X))` should return an array very close to `X`. Then add support for feature names: set `feature_names_in_` in the `fit()` method if the input is a DataFrame. This attribute should be a NumPy array of column names. Lastly, implement the `get_feature_names_out()` method: it should have one optional `input_features=None` argument. If passed, the method should check that its length matches `n_features_in_`, and it should match `feature_names_in_` if it is defined; then `input_features` should be returned. If `input_features` is `None`, then the method should either return `feature_names_in_` if it is defined or `np.array([\"x0\", \"x1\", ...])` with length `n_features_in_` otherwise.           7.  Tackle a regression task of your choice by following the process you learned in this chapter. For example, you can try tackling the [Vehicle dataset](https://homl.info/usedcars), where the goal is to predict the selling price of a used car, based on its age, the number of kilometers it has driven, its make and model, and more. Another good dataset to try is the [Bike Sharing dataset](https://homl.info/bikes): the objective is to predict the number of bikes rented within a period of time (column `cnt`), based on the day of the week, the time, and the weather conditions.              Solutions to these exercises are available at the end of this chapter’s notebook, at [*https://homl.info/colab-p*](https://homl.info/colab-p).    ^([1](ch02.html#id990-marker)) The original dataset appeared in R. Kelley Pace and Ronald Barry, “Sparse Spatial Autoregressions”, *Statistics & Probability Letters* 33, no. 3 (1997): 291–297.    ^([2](ch02.html#id994-marker)) A piece of information fed to a machine learning system is often called a *signal*, in reference to Claude Shannon’s information theory, which he developed at Bell Labs to improve telecommunications. His theory: you want a high signal-to-noise ratio.    ^([3](ch02.html#id1005-marker)) Recall that the transpose operator flips a column vector into a row vector (and vice versa).    ^([4](ch02.html#id1032-marker)) You might also need to check legal constraints, such as private fields that should never be copied to unsafe data stores.    ^([5](ch02.html#id1038-marker)) The standard deviation is generally denoted *σ* (the Greek letter sigma), and it is the square root of the *variance*, which is the average of the squared deviation from the mean. When a feature has a bell-shaped *normal distribution* (also called a *Gaussian distribution*), which is very common, the “68-95-99.7” rule applies: about 68% of the values fall within 1*σ* of the mean, 95% within 2*σ*, and 99.7% within 3*σ*.    ^([6](ch02.html#id1054-marker)) You will often see people set the random seed to 42\\. This number has no special property, other than being the Answer to the Ultimate Question of Life, the Universe, and Everything.    ^([7](ch02.html#id1055-marker)) The location information is actually quite coarse, and as a result many districts will have the exact same ID, so they will end up in the same set (test or train). This introduces some unfortunate sampling bias.    ^([8](ch02.html#id1068-marker)) If you are reading this in grayscale, grab a red pen and scribble over most of the coastline from the Bay Area down to San Diego (as you might expect). You can add a patch of yellow around Sacramento as well.    ^([9](ch02.html#id1102-marker)) For more details on the design principles, see Lars Buitinck et al., “API Design for Machine Learning Software: Experiences from the Scikit-Learn Project”, arXiv preprint arXiv:1309.0238 (2013).    ^([10](ch02.html#id1103-marker)) Some predictors also provide methods to measure the confidence of their predictions.    ^([11](ch02.html#id1107-marker)) If you run `sklearn.set_config(transform_output=\"pandas\")`, all transformers will output Pandas DataFrames when they receive a DataFrame as input: Pandas in, Pandas out.    ^([12](ch02.html#id1118-marker)) See SciPy’s documentation for more details.    ^([13](ch02.html#id1177-marker)) With duck typing, an object’s methods and behavior are what matters, not its type: “if it looks like a duck and quacks like a duck, it must be a duck”.    ^([14](ch02.html#id1268-marker)) In a nutshell, a REST (or RESTful) API is an HTTP-based API that follows some conventions, such as using standard HTTP verbs to read, update, create, or delete resources (GET, POST, PUT, and DELETE) and using JSON for the inputs and outputs.    ^([15](ch02.html#id1274-marker)) A captcha is a test to ensure a user is not a robot. These tests have often been used as a cheap way to label training data.```` ```py`` `````", "``````py````` ```py``````", "``````py``````", "``````py``````", "``````py``````", "``````py``````", "``` ```", "```py```", "````py````", "```py`` ```", "```py```", "````py````", "```py```", "``````py``````", "```````py```` ```py```````", "```````py```````", "``````py``````", "```````py`````` ```py```````", "```````py```````"]