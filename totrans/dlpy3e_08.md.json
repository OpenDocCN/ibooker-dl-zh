["```py\nimport keras\nfrom keras import layers\n\ninputs = keras.Input(shape=(28, 28, 1))\nx = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\nx = layers.MaxPooling2D(pool_size=2)(x)\nx = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\nx = layers.MaxPooling2D(pool_size=2)(x)\nx = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\nx = layers.GlobalAveragePooling2D()(x)\noutputs = layers.Dense(10, activation=\"softmax\")(x)\nmodel = keras.Model(inputs=inputs, outputs=outputs) \n```", "```py\n>>> model.summary()\nModel: \"functional\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer (InputLayer)          │ (None, 28, 28, 1)        │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ conv2d (Conv2D)                   │ (None, 26, 26, 64)       │           640 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ max_pooling2d (MaxPooling2D)      │ (None, 13, 13, 64)       │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ conv2d_1 (Conv2D)                 │ (None, 11, 11, 128)      │        73,856 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ max_pooling2d_1 (MaxPooling2D)    │ (None, 5, 5, 128)        │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ conv2d_2 (Conv2D)                 │ (None, 3, 3, 256)        │       295,168 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ global_average_pooling2d          │ (None, 256)              │             0 │\n│ (GlobalAveragePooling2D)          │                          │               │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense (Dense)                     │ (None, 10)               │         2,570 │\n└───────────────────────────────────┴──────────────────────────┴───────────────┘\n Total params: 372,234 (1.42 MB)\n Trainable params: 372,234 (1.42 MB)\n Non-trainable params: 0 (0.00 B)\n```", "```py\nfrom keras.datasets import mnist\n\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\ntrain_images = train_images.reshape((60000, 28, 28, 1))\ntrain_images = train_images.astype(\"float32\") / 255\ntest_images = test_images.reshape((10000, 28, 28, 1))\ntest_images = test_images.astype(\"float32\") / 255\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nmodel.fit(train_images, train_labels, epochs=5, batch_size=64) \n```", "```py\n>>> test_loss, test_acc = model.evaluate(test_images, test_labels)\n>>> print(f\"Test accuracy: {test_acc:.3f}\")\nTest accuracy: 0.991\n```", "```py\ninputs = keras.Input(shape=(28, 28, 1))\nx = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\nx = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\nx = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\nx = layers.GlobalAveragePooling2D()(x)\noutputs = layers.Dense(10, activation=\"softmax\")(x)\nmodel_no_max_pool = keras.Model(inputs=inputs, outputs=outputs) \n```", "```py\n>>> model_no_max_pool.summary()\nModel: \"functional_1\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_1 (InputLayer)        │ (None, 28, 28, 1)        │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ conv2d_3 (Conv2D)                 │ (None, 26, 26, 64)       │           640 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ conv2d_4 (Conv2D)                 │ (None, 24, 24, 128)      │        73,856 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ conv2d_5 (Conv2D)                 │ (None, 22, 22, 256)      │       295,168 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ global_average_pooling2d_1        │ (None, 256)              │             0 │\n│ (GlobalAveragePooling2D)          │                          │               │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_1 (Dense)                   │ (None, 10)               │         2,570 │\n└───────────────────────────────────┴──────────────────────────┴───────────────┘\n Total params: 372,234 (1.42 MB)\n Trainable params: 372,234 (1.42 MB)\n Non-trainable params: 0 (0.00 B)\n```", "```py\ndogs_vs_cats_small/\n...train/\n# Contains 1,000 cat images\n......cat/\n# Contains 1,000 dog images\n......dog/\n...validation/\n# Contains 500 cat images\n......cat/\n# Contains 500 dog images\n......dog/\n...test/\n# Contains 1,000 cat images\n......cat/\n# Contains 1,000 dog images\n......dog/ \n```", "```py\nimport os, shutil, pathlib\n\n# Path to the directory where the original dataset was uncompressed\noriginal_dir = pathlib.Path(\"train\")\n# Directory where we will store our smaller dataset\nnew_base_dir = pathlib.Path(\"dogs_vs_cats_small\")\n\n# Utility function to copy cat (respectively, dog) images from index\n# `start_index` to index `end_index` to the subdirectory\n# `new_base_dir/{subset_name}/cat` (respectively, dog). \"subset_name\"\n# will be either \"train,\" \"validation,\" or \"test.\"\ndef make_subset(subset_name, start_index, end_index):\n    for category in (\"cat\", \"dog\"):\n        dir = new_base_dir / subset_name / category\n        os.makedirs(dir)\n        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n        for fname in fnames:\n            shutil.copyfile(src=original_dir / fname, dst=dir / fname)\n\n# Creates the training subset with the first 1,000 images of each\n# category\nmake_subset(\"train\", start_index=0, end_index=1000)\n# Creates the validation subset with the next 500 images of each\n# category\nmake_subset(\"validation\", start_index=1000, end_index=1500)\n# Creates the test subset with the next 1,000 images of each category\nmake_subset(\"test\", start_index=1500, end_index=2500) \n```", "```py\nimport keras\nfrom keras import layers\n\n# The model expects RGB images of size 180 x 180.\ninputs = keras.Input(shape=(180, 180, 3))\n# Rescales inputs to the [0, 1] range by dividing them by 255\nx = layers.Rescaling(1.0 / 255)(inputs)\nx = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\nx = layers.MaxPooling2D(pool_size=2)(x)\nx = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\nx = layers.MaxPooling2D(pool_size=2)(x)\nx = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\nx = layers.MaxPooling2D(pool_size=2)(x)\nx = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\nx = layers.MaxPooling2D(pool_size=2)(x)\nx = layers.Conv2D(filters=512, kernel_size=3, activation=\"relu\")(x)\n# Flattens the 3D activations with shape (height, width, 512) into 1D\n# activations with shape (512,) by averaging them over spatial\n# dimensions\nx = layers.GlobalAveragePooling2D()(x)\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = keras.Model(inputs=inputs, outputs=outputs) \n```", "```py\n>>> model.summary()\nModel: \"functional_2\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_2 (InputLayer)        │ (None, 180, 180, 3)      │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ rescaling (Rescaling)             │ (None, 180, 180, 3)      │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ conv2d_6 (Conv2D)                 │ (None, 178, 178, 32)     │           896 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ max_pooling2d_2 (MaxPooling2D)    │ (None, 89, 89, 32)       │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ conv2d_7 (Conv2D)                 │ (None, 87, 87, 64)       │        18,496 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ max_pooling2d_3 (MaxPooling2D)    │ (None, 43, 43, 64)       │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ conv2d_8 (Conv2D)                 │ (None, 41, 41, 128)      │        73,856 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ max_pooling2d_4 (MaxPooling2D)    │ (None, 20, 20, 128)      │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ conv2d_9 (Conv2D)                 │ (None, 18, 18, 256)      │       295,168 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ max_pooling2d_5 (MaxPooling2D)    │ (None, 9, 9, 256)        │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ conv2d_10 (Conv2D)                │ (None, 7, 7, 512)        │     1,180,160 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ global_average_pooling2d_2        │ (None, 512)              │             0 │\n│ (GlobalAveragePooling2D)          │                          │               │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_2 (Dense)                   │ (None, 1)                │           513 │\n└───────────────────────────────────┴──────────────────────────┴───────────────┘\n Total params: 1,569,089 (5.99 MB)\n Trainable params: 1,569,089 (5.99 MB)\n Non-trainable params: 0 (0.00 B)\n```", "```py\nmodel.compile(\n    loss=\"binary_crossentropy\",\n    optimizer=\"adam\",\n    metrics=[\"accuracy\"],\n) \n```", "```py\nfrom keras.utils import image_dataset_from_directory\n\nbatch_size = 64\nimage_size = (180, 180)\ntrain_dataset = image_dataset_from_directory(\n    new_base_dir / \"train\", image_size=image_size, batch_size=batch_size\n)\nvalidation_dataset = image_dataset_from_directory(\n    new_base_dir / \"validation\", image_size=image_size, batch_size=batch_size\n)\ntest_dataset = image_dataset_from_directory(\n    new_base_dir / \"test\", image_size=image_size, batch_size=batch_size\n) \n```", "```py\nimport numpy as np\nimport tensorflow as tf\n\nrandom_numbers = np.random.normal(size=(1000, 16))\n# The from_tensor_slices() class method can be used to create a Dataset\n# from a NumPy array or a tuple or dict of NumPy arrays.\ndataset = tf.data.Dataset.from_tensor_slices(random_numbers) \n```", "```py\n>>> for i, element in enumerate(dataset):\n>>>     print(element.shape)\n>>>     if i >= 2:\n>>>         break\n(16,)\n(16,)\n(16,)\n```", "```py\n>>> batched_dataset = dataset.batch(32)\n>>> for i, element in enumerate(batched_dataset):\n>>>     print(element.shape)\n>>>     if i >= 2:\n>>>         break\n(32, 16)\n(32, 16)\n(32, 16)\n```", "```py\n>>> reshaped_dataset = dataset.map(\n...     lambda x: tf.reshape(x, (4, 4)),\n...     num_parallel_calls=8)\n>>> for i, element in enumerate(reshaped_dataset):\n...     print(element.shape)\n...     if i >= 2:\n...         break\n(4, 4)\n(4, 4)\n(4, 4)\n```", "```py\n>>> for data_batch, labels_batch in train_dataset:\n>>>     print(\"data batch shape:\", data_batch.shape)\n>>>     print(\"labels batch shape:\", labels_batch.shape)\n>>>     break\ndata batch shape: (32, 180, 180, 3)\nlabels batch shape: (32,)\n```", "```py\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\n        filepath=\"convnet_from_scratch.keras\",\n        save_best_only=True,\n        monitor=\"val_loss\",\n    )\n]\nhistory = model.fit(\n    train_dataset,\n    epochs=50,\n    validation_data=validation_dataset,\n    callbacks=callbacks,\n) \n```", "```py\nimport matplotlib.pyplot as plt\n\naccuracy = history.history[\"accuracy\"]\nval_accuracy = history.history[\"val_accuracy\"]\nloss = history.history[\"loss\"]\nval_loss = history.history[\"val_loss\"]\nepochs = range(1, len(accuracy) + 1)\n\nplt.plot(epochs, accuracy, \"r--\", label=\"Training accuracy\")\nplt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\nplt.title(\"Training and validation accuracy\")\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, \"r--\", label=\"Training loss\")\nplt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\nplt.title(\"Training and validation loss\")\nplt.legend()\nplt.show() \n```", "```py\ntest_model = keras.models.load_model(\"convnet_from_scratch.keras\")\ntest_loss, test_acc = test_model.evaluate(test_dataset)\nprint(f\"Test accuracy: {test_acc:.3f}\") \n```", "```py\n# Defines the transformations to apply as a list\ndata_augmentation_layers = [\n    layers.RandomFlip(\"horizontal\"),\n    layers.RandomRotation(0.1),\n    layers.RandomZoom(0.2),\n]\n\n# Creates a function that applies them sequentially\ndef data_augmentation(images, targets):\n    for layer in data_augmentation_layers:\n        images = layer(images)\n    return images, targets\n\n# Maps this function into the dataset\naugmented_train_dataset = train_dataset.map(\n    data_augmentation, num_parallel_calls=8\n)\n# Enables prefetching of batches on GPU memory; important for best\n# performance\naugmented_train_dataset = augmented_train_dataset.prefetch(tf.data.AUTOTUNE) \n```", "```py\nplt.figure(figsize=(10, 10))\n# You can use take(N) to only sample N batches from the dataset. This\n# is equivalent to inserting a break in the loop after the Nth batch.\nfor image_batch, _ in train_dataset.take(1):\n    image = image_batch[0]\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        augmented_image, _ = data_augmentation(image, None)\n        augmented_image = keras.ops.convert_to_numpy(augmented_image)\n        # Displays the first image in the output batch. For each of the\n        # nine iterations, this is a different augmentation of the same\n        # image.\n        plt.imshow(augmented_image.astype(\"uint8\"))\n        plt.axis(\"off\") \n```", "```py\ninputs = keras.Input(shape=(180, 180, 3))\nx = layers.Rescaling(1.0 / 255)(inputs)\nx = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\nx = layers.MaxPooling2D(pool_size=2)(x)\nx = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\nx = layers.MaxPooling2D(pool_size=2)(x)\nx = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\nx = layers.MaxPooling2D(pool_size=2)(x)\nx = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\nx = layers.MaxPooling2D(pool_size=2)(x)\nx = layers.Conv2D(filters=512, kernel_size=3, activation=\"relu\")(x)\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dropout(0.25)(x)\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = keras.Model(inputs=inputs, outputs=outputs)\n\nmodel.compile(\n    loss=\"binary_crossentropy\",\n    optimizer=\"adam\",\n    metrics=[\"accuracy\"],\n) \n```", "```py\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\n        filepath=\"convnet_from_scratch_with_augmentation.keras\",\n        save_best_only=True,\n        monitor=\"val_loss\",\n    )\n]\nhistory = model.fit(\n    augmented_train_dataset,\n    # Since we expect the model to overfit slower, we train for more\n    # epochs.\n    epochs=100,\n    validation_data=validation_dataset,\n    callbacks=callbacks,\n) \n```", "```py\ntest_model = keras.models.load_model(\n    \"convnet_from_scratch_with_augmentation.keras\"\n)\ntest_loss, test_acc = test_model.evaluate(test_dataset)\nprint(f\"Test accuracy: {test_acc:.3f}\") \n```", "```py\nimport keras_hub\n\nconv_base = keras_hub.models.Backbone.from_preset(\"xception_41_imagenet\") \n```", "```py\npreprocessor = keras_hub.layers.ImageConverter.from_preset(\n    \"xception_41_imagenet\",\n    image_size=(180, 180),\n) \n```", "```py\ndef get_features_and_labels(dataset):\n    all_features = []\n    all_labels = []\n    for images, labels in dataset:\n        preprocessed_images = preprocessor(images)\n        features = conv_base.predict(preprocessed_images, verbose=0)\n        all_features.append(features)\n        all_labels.append(labels)\n    return np.concatenate(all_features), np.concatenate(all_labels)\n\ntrain_features, train_labels = get_features_and_labels(train_dataset)\nval_features, val_labels = get_features_and_labels(validation_dataset)\ntest_features, test_labels = get_features_and_labels(test_dataset) \n```", "```py\n>>> train_features.shape\n(2000, 6, 6, 2048)\n```", "```py\ninputs = keras.Input(shape=(6, 6, 2048))\n# Averages spatial dimensions to flatten the feature map\nx = layers.GlobalAveragePooling2D()(inputs)\nx = layers.Dense(256, activation=\"relu\")(x)\nx = layers.Dropout(0.25)(x)\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = keras.Model(inputs, outputs)\nmodel.compile(\n    loss=\"binary_crossentropy\",\n    optimizer=\"adam\",\n    metrics=[\"accuracy\"],\n)\n\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\n        filepath=\"feature_extraction.keras\",\n        save_best_only=True,\n        monitor=\"val_loss\",\n    )\n]\nhistory = model.fit(\n    train_features,\n    train_labels,\n    epochs=10,\n    validation_data=(val_features, val_labels),\n    callbacks=callbacks,\n) \n```", "```py\nimport matplotlib.pyplot as plt\n\nacc = history.history[\"accuracy\"]\nval_acc = history.history[\"val_accuracy\"]\nloss = history.history[\"loss\"]\nval_loss = history.history[\"val_loss\"]\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, \"r--\", label=\"Training accuracy\")\nplt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\nplt.title(\"Training and validation accuracy\")\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, \"r--\", label=\"Training loss\")\nplt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\nplt.title(\"Training and validation loss\")\nplt.legend()\nplt.show() \n```", "```py\ntest_model = keras.models.load_model(\"feature_extraction.keras\")\ntest_loss, test_acc = test_model.evaluate(test_features, test_labels)\nprint(f\"Test accuracy: {test_acc:.3f}\") \n```", "```py\nimport keras_hub\n\nconv_base = keras_hub.models.Backbone.from_preset(\n    \"xception_41_imagenet\",\n    trainable=False,\n) \n```", "```py\n>>> conv_base.trainable = True\n>>> # The number of trainable weights before freezing the conv base\n>>> len(conv_base.trainable_weights)\n154\n>>> conv_base.trainable = False\n>>> # The number of trainable weights after freezing the conv base\n>>> len(conv_base.trainable_weights)\n0\n```", "```py\ninputs = keras.Input(shape=(180, 180, 3))\nx = preprocessor(inputs)\nx = conv_base(x)\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dense(256)(x)\nx = layers.Dropout(0.25)(x)\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = keras.Model(inputs, outputs)\nmodel.compile(\n    loss=\"binary_crossentropy\",\n    optimizer=\"adam\",\n    metrics=[\"accuracy\"],\n) \n```", "```py\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\n        filepath=\"feature_extraction_with_data_augmentation.keras\",\n        save_best_only=True,\n        monitor=\"val_loss\",\n    )\n]\nhistory = model.fit(\n    augmented_train_dataset,\n    epochs=30,\n    validation_data=validation_dataset,\n    callbacks=callbacks,\n) \n```", "```py\ntest_model = keras.models.load_model(\n    \"feature_extraction_with_data_augmentation.keras\"\n)\ntest_loss, test_acc = test_model.evaluate(test_dataset)\nprint(f\"Test accuracy: {test_acc:.3f}\") \n```", "```py\nmodel.compile(\n    loss=\"binary_crossentropy\",\n    optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n    metrics=[\"accuracy\"],\n)\n\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\n        filepath=\"fine_tuning.keras\",\n        save_best_only=True,\n        monitor=\"val_loss\",\n    )\n]\nhistory = model.fit(\n    augmented_train_dataset,\n    epochs=30,\n    validation_data=validation_dataset,\n    callbacks=callbacks,\n) \n```", "```py\nmodel = keras.models.load_model(\"fine_tuning.keras\")\ntest_loss, test_acc = model.evaluate(test_dataset)\nprint(f\"Test accuracy: {test_acc:.3f}\") \n```"]