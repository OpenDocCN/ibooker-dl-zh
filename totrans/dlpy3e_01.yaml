- en: What is deep learning?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://deeplearningwithpython.io/chapters/chapter01_what-is-deep-learning](https://deeplearningwithpython.io/chapters/chapter01_what-is-deep-learning)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Over the past decade, artificial intelligence (AI) has been a subject of intense
    media hype. Machine learning, deep learning, and AI come up in countless articles,
    often outside of technology-minded publications. We’re promised a future of intelligent
    chatbots, self-driving cars, and virtual assistants — a future sometimes painted
    in a grim light and other times as utopian, where human jobs will be scarce and
    most economic activity will be handled by robots or AI agents. For a practitioner
    of machine learning, it’s important to be able to recognize the signal amid the
    noise, so that you can tell world-changing developments from overhyped press releases.
    Our future is at stake, and it’s one in which you have an active role to play:
    after reading this book, you’ll be one of those who can develop these AI systems.
    So let’s tackle these questions: What has deep learning achieved so far? How significant
    is it? Where are we headed next? Should you believe the hype?'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Artificial intelligence, machine learning, and deep learning
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we need to define clearly what we’re talking about when we mention AI.
    What are artificial intelligence, machine learning, and deep learning (figure
    1.1)? How do they relate to each other?
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/921f303080c9876c7d564bf1f84f7d03.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
- en: '[Figure 1.1](#figure-1-1): Artificial intelligence, machine learning, and deep
    learning'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Artificial intelligence
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Artificial intelligence was born in the 1950s, when a handful of pioneers from
    the nascent field of computer science started asking whether computers could be
    made to “think” — a question whose ramifications we’re still exploring today.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 'While many of the underlying ideas had been brewing in the years and even decades
    prior, “artificial intelligence” finally crystallized as a field of research in
    1956, when John McCarthy, then a young Assistant Professor of Mathematics at Dartmouth
    College, organized a summer workshop under the following proposal:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: The study is to proceed on the basis of the conjecture that every aspect of
    learning or any other feature of intelligence can in principle be so precisely
    described that a machine can be made to simulate it. An attempt will be made to
    find how to make machines use language, form abstractions and concepts, solve
    kinds of problems now reserved for humans, and improve themselves. We think that
    a significant advance can be made in one or more of these problems if a carefully
    selected group of scientists work on it together for a summer.
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: At the end of the summer, the workshop concluded without having fully solved
    the riddle it set out to investigate. Nevertheless, it was attended by many people
    who would move on to become pioneers in the field, and it set in motion an intellectual
    revolution that is still ongoing to this day.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Concisely, AI can be described as *the effort to automate intellectual tasks
    normally performed by humans*. As such, AI is a general field that encompasses
    machine learning and deep learning, but that also includes many more approaches
    that may not involve any learning. Consider that until the 1980s, most AI textbooks
    didn’t mention “learning” at all! Early chess programs, for instance, only involved
    hardcoded rules crafted by programmers and didn’t qualify as machine learning.
    In fact, for a fairly long time, most experts believed that human-level artificial
    intelligence could be achieved by having programmers handcraft a sufficiently
    large set of explicit rules for manipulating knowledge stored in explicit databases.
    This approach is known as *symbolic AI*. It was the dominant paradigm in AI from
    the 1950s to the late 1980s, and it reached its peak popularity during the *expert
    systems* boom of the 1980s.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'Although symbolic AI proved suitable to solve well-defined, logical problems,
    such as playing chess, it turned out to be intractable to figure out explicit
    rules for solving more complex, fuzzy problems, such as image classification,
    speech recognition, or natural language translation. A new approach arose to take
    symbolic AI’s place: *machine learning*.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Victorian England, Lady Ada Lovelace was a friend and collaborator of Charles
    Babbage, the inventor of the Analytical Engine: the first-known general-purpose
    mechanical computer. Although visionary and far ahead of its time, the Analytical
    Engine wasn’t meant as a general-purpose computer when it was designed in the
    1830s and 1840s, because the concept of general-purpose computation was yet to
    be invented. It was merely meant as a way to use mechanical operations to automate
    certain computations from the field of mathematical analysis — hence, the name
    Analytical Engine. As such, it was the intellectual descendant of earlier attempts
    at encoding mathematical operations in gear form, such as the Pascaline, or Leibniz’s
    step reckoner, a refined version of the Pascaline. Designed by Blaise Pascal in
    1642 (at age 19!), the Pascaline was the world’s first mechanical calculator —
    it could add, subtract, multiply, or even divide digits.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'In 1843, Ada Lovelace remarked on the invention of the Analytical Engine:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: The Analytical Engine has no pretensions whatever to originate anything. It
    can do whatever we know how to order it to perform… Its province is to assist
    us in making available what we’re already acquainted with.
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Even with 182 years of historical perspective, Lady Lovelace’s observation remains
    arresting. Could a general-purpose computer “originate” anything, or would it
    always be bound to dully execute processes we humans fully understand? Could it
    ever be capable of any original thought? Could it learn from experience? Could
    it show creativity?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Her remark was later quoted by AI pioneer Alan Turing as “Lady Lovelace’s objection”
    in his landmark 1950 paper “Computing Machinery and Intelligence,” ^([[1]](#footnote-1))
    which introduced the *Turing test*^([[2]](#footnote-2)) as well as key concepts
    that would come to shape AI. Turing was of the opinion — highly provocative at
    the time — that computers could, in principle, be made to emulate all aspects
    of human intelligence.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 她的评论后来被人工智能先驱艾伦·图灵在1950年的里程碑式论文《计算机机械与智能》中引用为“洛夫莱斯异议”，该论文介绍了**图灵测试**以及将塑造人工智能的关键概念。图灵当时持有——极具挑衅性——的观点，即计算机原则上可以模仿人类智能的所有方面。
- en: 'The usual way to make a computer do useful work is to have a human programmer
    write down rules — a computer program — to be followed to turn input data into
    appropriate answers, just like Lady Lovelace writing down step-by-step instructions
    for the Analytical Engine to perform. Machine learning turns this around: the
    machine looks at the input data and the corresponding answers, and figures out
    what the rules should be (figure 1.2).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通常让计算机做有用的工作的方式是让人类程序员写下规则——一个计算机程序——以将输入数据转换为适当的答案，就像洛夫莱斯女士为分析机编写逐步指令一样。机器学习则相反：机器查看输入数据和相应的答案，并找出应该遵循的规则（见图1.2）。
- en: '![](../Images/74e9799f9c19724503aa9019479fa0c9.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/74e9799f9c19724503aa9019479fa0c9.png)'
- en: '[Figure 1.2](#figure-1-2): Machine learning: a new programming paradigm'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.2](#figure-1-2)：机器学习：一种新的编程范式'
- en: A machine learning system is *trained* rather than explicitly programmed. It’s
    presented with many examples relevant to a task, and it finds statistical structure
    in these examples that eventually allows the system to come up with rules for
    automating the task. For instance, if you wished to automate the task of tagging
    your vacation pictures, you could present a machine learning system with many
    examples of pictures already tagged by humans, and the system would learn statistical
    rules for associating specific pictures to specific tags like “landscape” or “food.”
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统是通过训练而不是明确编程的。它被提供了许多与任务相关的示例，并从这些示例中找到统计结构，最终使系统能够制定自动化任务的规则。例如，如果你希望自动化标记假期照片的任务，你可以向机器学习系统提供许多已经被人类标记的图片示例，系统就会学习将特定图片与特定标签（如“风景”或“食物”）关联的统计规则。
- en: Although machine learning only started to flourish in the 1990s, it has quickly
    become the most popular and most successful subfield of AI, a trend driven by
    the availability of faster hardware and larger datasets. Machine learning is related
    to mathematical statistics, but it differs from statistics in several important
    ways — in the same sense that medicine is related to chemistry but cannot be reduced
    to chemistry, as medicine deals with its own distinct systems with their own distinct
    properties. Unlike statistics, machine learning tends to deal with large, complex
    datasets (such as a dataset of millions of images, each consisting of tens of
    thousands of pixels) for which classical statistical analysis such as Bayesian
    analysis would be impractical. As a result, machine learning, and especially deep
    learning, exhibits comparatively little mathematical theory — maybe too little
    — and is fundamentally an engineering discipline. Unlike theoretical physics or
    mathematics, machine learning is a very hands-on field driven by empirical findings
    and deeply reliant on advances in software and hardware.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管机器学习直到20世纪90年代才开始蓬勃发展，但它迅速成为人工智能最受欢迎且最成功的子领域，这一趋势是由更快的硬件和更大的数据集的可用性所驱动的。机器学习与数学统计学相关，但在几个重要方面与统计学有所不同——就像医学与化学相关，但不能简化为化学一样，因为医学处理的是其自身独特的系统及其独特的属性。与统计学不同，机器学习倾向于处理大型、复杂的数据集（例如包含数百万个图像的数据集，每个图像由数万个像素组成），对于这些数据集，传统的统计分析（如贝叶斯分析）将是不切实际的。因此，机器学习，尤其是深度学习，在数学理论上相对较少——可能太少——它本质上是一门工程学科。与理论物理学或数学不同，机器学习是一个非常注重实践、由经验发现驱动且深度依赖软件和硬件进步的领域。
- en: Learning rules and representations from data
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从数据中学习规则和表示
- en: 'To define *deep learning* and understand the difference between deep learning
    and other machine learning approaches, first we need some idea of what machine
    learning algorithms do. We just stated that machine learning discovers rules to
    execute a data processing task, given examples of what’s expected. So, to do machine
    learning, we need three things:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '*Input data points*  — For instance, if the task is speech recognition, these
    data points could be sound files of people speaking. If the task is image tagging,
    they could be pictures.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Examples of the expected output*  — In a speech-recognition task, these could
    be human-generated transcripts of sound files. In an image task, expected outputs
    could be tags such as “dog,” “cat,” and so on.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A way to measure whether the algorithm is doing a good job*  — This is necessary
    to determine the distance between the algorithm’s current output and its expected
    output. The measurement is used as a feedback signal to adjust the way the algorithm
    works. This adjustment step is what we call *learning*.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A machine learning model transforms its input data into meaningful outputs,
    a process that is “learned” from exposure to known examples of inputs and outputs.
    Therefore, the central problem in machine learning and deep learning is to *meaningfully
    transform data*: in other words, to learn useful *representations* of the input
    data at hand — representations that get us closer to the expected output.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we go any further, what’s a representation? At its core, it’s a different
    way to look at data to represent or encode data. For instance, a color image can
    be encoded in the RGB format (red-green-blue) or in the HSV format (hue-saturation-value):
    these are two different representations of the same data. Some tasks that may
    be difficult with one representation can become easy with another. For example,
    the task “Select all red pixels in the image” is simpler in the RGB format, whereas
    “Make the image less saturated” is simpler in the HSV format. Machine learning
    models are all about finding appropriate representations for their input data
    — transformations of the data that make it more amenable to the task at hand.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Let’s make this concrete. Consider an x-axis, a y-axis, and some points represented
    by their coordinates in the (x, y) system, as shown in figure 1.3.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ca21dc65164cbdc981ffb88e1d6b807c.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
- en: '[Figure 1.3](#figure-1-3): Some sample data'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we have a few white points and a few black points. Let’s say
    we want to develop an algorithm that can take a point’s (x, y) coordinates and
    output whether that point is likely black or white. In this case,
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: The inputs are the coordinates of our points.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The expected outputs are the colors of our points.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A way to measure whether our algorithm is doing a good job could be, for instance,
    the percentage of points that are being correctly classified.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What we need here is a new representation of our data that cleanly separates
    the white points from the black points. One transformation we could use, among
    many other possibilities, would be a coordinate change, illustrated in figure
    1.4.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们需要一种新的数据表示方法，能够清晰地分离出白点和黑点。在众多可能性中，我们可以使用的一种变换就是坐标变换，如图 1.4 所示。
- en: '![](../Images/b76cb9dc33b7b113a7e534cffb01629d.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/b76cb9dc33b7b113a7e534cffb01629d.png)'
- en: '[Figure 1.4](#figure-1-4): Coordinate change'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 1.4](#figure-1-4)：坐标变换'
- en: 'In this new coordinate system, the coordinates of our points can be said to
    be a new representation of our data. And it’s a good one! With this representation,
    the black/white classification problem can be expressed as a simple rule: “Black
    points are such that x > 0,” or “White points are such that x < 0.” This new representation,
    combined with this simple rule, neatly solves the classification problem.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个新的坐标系中，我们点的坐标可以说是一种新的数据表示。而且这是一个很好的表示！使用这种表示，黑/白分类问题可以表达为一个简单的规则：“黑点满足 x
    > 0，”或者“白点满足 x < 0。”这种新的表示，结合这个简单的规则，巧妙地解决了分类问题。
- en: 'In this case, we defined the coordinate change by hand: we used our human intelligence
    to come up with our own appropriate representation of the data. This is fine for
    such an extremely simple problem, but could you do the same if the task were to
    classify images of handwritten digits? Could you write down explicit, computer-executable
    image transformations that would illuminate the difference between a 6 and an
    8, between a 1 and a 7, across all kinds of different handwritings?'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们手动定义了坐标变换：我们利用人类智慧来提出我们自己的数据适当表示。对于如此简单的问题来说，这是可以的，但如果你要分类手写数字的图像，你能做到同样的事情吗？你能写下明确的、计算机可执行的图像变换，以阐明
    6 和 8、1 和 7 之间的区别，以及各种不同手写的区别吗？
- en: This is possible to an extent. Rules based on representations of digits such
    as “counting the number of closed loops” or vertical and horizontal pixel histograms
    can do a decent job at telling apart handwritten digits. But finding such useful
    representations by hand is hard work, and as you can imagine the resulting rule-based
    system would be brittle and a nightmare to maintain. Every time you would come
    across a new example of handwriting that would break your carefully thought-out
    rules, you would have to add new data transformations and new rules, while taking
    into account their interaction with every previous rule.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这在某种程度上是可能的。基于数字表示的规则，例如“计算封闭环的数量”或垂直和水平像素直方图，可以在区分手写数字方面做得相当不错。但手动找到这样的有用表示是件辛苦的工作，而且正如你可以想象的那样，由此产生的基于规则的系统将是脆弱的，维护起来将是一场噩梦。每次你遇到一个新的手写例子，它打破了你的精心设计的规则，你都必须添加新的数据变换和新的规则，同时考虑到它们与每个先前规则之间的相互作用。
- en: You’re probably thinking, if this process is so painful, could we automate it?
    What if we tried systematically searching for different sets of automatically
    generated representations of the data and rules based on them, identifying good
    ones using the percentage of digits being correctly classified in some development
    dataset as feedback? We would then be doing machine learning. *Learning*, in the
    context of machine learning, describes an automatic search process for data transformations
    that produce useful representations of some data, guided by some feedback signal
    — representations that are amenable to simpler rules solving the task at hand.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能正在想，如果这个过程如此痛苦，我们能否自动化它？如果我们尝试系统地搜索不同集合的自动生成数据表示及其规则，并使用某些开发数据集中正确分类数字的百分比作为反馈来识别好的表示，我们会做什么？那么我们就会进行机器学习。在机器学习的背景下，“学习”描述了一个自动搜索过程，用于寻找产生某些数据有用表示的数据变换，这个过程由某些反馈信号引导——这些表示可以适用于更简单的规则来解决手头的任务。*学习*，在机器学习的上下文中，描述了一个自动搜索过程，用于寻找产生有用表示的数据变换，这个过程由某些反馈信号引导——这些表示可以适用于更简单的规则来解决手头的任务。
- en: These transformations can be coordinate changes (like in our 2D coordinates
    classification example) or a histogram of pixels and counting loops (like in our
    digits classification example), but they could also be linear projections, translations,
    and nonlinear operations (such as “Select all points such that x > 0”), and so
    on. Machine learning algorithms aren’t usually creative in finding these transformations;
    they’re merely searching through a predefined set of operations, called a *hypothesis
    space*. For instance, the space of all possible coordinate changes would be our
    hypothesis space in the 2D coordinates classification example.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这些变换可以是坐标变化（就像我们在二维坐标分类示例中那样）或像素直方图和计数环（就像我们在数字分类示例中那样），但它们也可以是线性投影、平移和非线性操作（例如“选择所有x
    > 0的点”），等等。机器学习算法在寻找这些变换方面通常并不具有创造性；它们只是在预定义的操作集合中搜索，称为*假设空间*。例如，所有可能的坐标变化空间将是我们在二维坐标分类示例中的假设空间。
- en: 'So that’s what machine learning is, concisely: searching for useful representations
    and rules over some input data, within a predefined space of possibilities, using
    guidance from a feedback signal. This simple idea allows us to solve a remarkably
    broad range of intellectual tasks, from autonomous driving to natural language
    question-answering.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这就是机器学习的简要概述：在预定义的可能空间内，通过反馈信号的引导，搜索一些输入数据的有用表示和规则。这个简单的想法使我们能够解决从自动驾驶到自然语言问答的广泛智力任务。
- en: Now that you understand what we mean by *learning*, let’s take a look at what
    makes *deep learning* special.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经理解了我们所说的*学习*是什么意思，让我们来看看是什么让*深度学习*变得特殊。
- en: The “deep” in “deep learning”
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: “深度学习”中的“深度”
- en: Deep learning is a specific subfield of machine learning; it’s a new take on
    learning representations from data, which emphasizes learning successive layers
    of increasingly meaningful representations. The “deep” in “deep learning” isn’t
    a reference to any kind of deeper understanding achieved by the approach; rather,
    it stands for this idea of successive layers of representations. How many layers
    contribute to a model of the data is called the *depth* of the model. Other appropriate
    names for the field could have been *layered representations learning* or *hierarchical
    representations learning*. Modern deep learning often involves tens or even hundreds
    of successive layers of representations, and they’re all learned automatically
    from exposure to training data. Meanwhile, other approaches to machine learning
    tend to focus on learning only one or two layers of representations of the data
    (say, taking a pixel histogram and then applying a classification rule); hence,
    they’re sometimes called *shallow learning*.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是机器学习的一个特定子领域；它是对从数据中学习表示的新方法，强调学习越来越有意义的表示的连续层。在“深度学习”中的“深度”并不是指通过这种方法实现的任何更深入的理解；相反，它代表了这个连续层表示的想法。对数据模型贡献多少层被称为模型的*深度*。该领域的其他适当名称可以是*分层表示学习*或*层次表示学习*。现代深度学习通常涉及数十甚至数百个连续的表示层，它们都是通过接触训练数据自动学习的。同时，其他机器学习方法倾向于只学习数据的一层或两层表示（例如，获取像素直方图然后应用分类规则）；因此，它们有时被称为*浅层学习*。
- en: In deep learning, these layered representations are learned via models called
    *neural networks*, structured in literal layers stacked on top of each other.
    The term *neural network* is a reference to neurobiology, but although some of
    the central concepts in deep learning were developed in part by drawing inspiration
    from our understanding of the brain (in particular, the visual cortex), deep learning
    models are not models of the brain. There’s no evidence that the brain implements
    anything like the learning mechanisms used in modern deep learning models. You
    may come across pop science articles proclaiming that deep learning works like
    the brain or is modeled after the brain, but that isn’t the case. It would be
    confusing and counterproductive for newcomers to the field to think of deep learning
    as being in any way related to neurobiology; you don’t need that shroud of “just
    like our minds” mystique and mystery, and you may as well forget anything you
    may have read about hypothetical links between deep learning and biology. For
    our purposes, deep learning is a mathematical framework for learning representations
    from data.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: What do the representations learned by a deep learning algorithm look like?
    Let’s examine how a network several layers deep (see figure 1.5) transforms an
    image of a digit to recognize what digit it is.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/80771b3e14f09b5c2b106f322b07f35f.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: '[Figure 1.5](#figure-1-5): A deep neural network for digit classification'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in figure 1.6, the network transforms the digit image into representations
    that are increasingly different from the original image and increasingly informative
    about the final result. You can think of a deep network as a multistage *information-distillation*
    process, where information goes through successive filters and comes out increasingly
    *purified* (that is, useful with regard to some task).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aa0afeb7f51dcd33dd1409e3b258100e.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: '[Figure 1.6](#figure-1-6): Deep representations learned by a digit-classification
    model'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 'So that’s what deep learning is, technically: a multistage way to learn data
    representations. It’s a simple idea, but, as it turns out, very simple mechanisms,
    sufficiently scaled, can end up looking like magic.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how deep learning works, in three figures
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At this point, you know that machine learning is about mapping inputs (such
    as images) to targets (such as the label “cat”), which is done by observing many
    examples of inputs and targets. You also know that deep neural networks do this
    input-to-target mapping via a deep sequence of simple data transformations (layers)
    and that these data transformations are learned by exposure to examples. Now let’s
    look at how this learning happens, concretely.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'The specification of what a layer does to its input data is stored in the layer’s
    *weights*, which in essence are a bunch of numbers. In technical terms, we’d say
    that the transformation implemented by a layer is *parameterized* by its weights
    (see figure 1.7). (Weights are also sometimes called the parameters of a layer.)
    In this context, *learning* means finding a set of values for the weights of all
    layers in a network, such that the network will correctly map example inputs to
    their associated targets. But here’s the thing: a deep neural network can contain
    tens of millions of parameters. Finding the correct value for all of them may
    seem like a daunting task, especially given that modifying the value of one parameter
    will affect the behavior of all the others!'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 层对输入数据的处理规范存储在层的**权重**中，本质上是一系列数字。从技术角度讲，我们可以说层的实现是通过其权重**参数化**的（见图1.7）。（权重有时也被称为层的参数。）在这个背景下，**学习**意味着找到网络中所有层权重的值，使得网络能够正确地将示例输入映射到其相关目标。但问题是：深度神经网络可能包含数百万个参数。找到所有这些参数的正确值可能是一项艰巨的任务，尤其是考虑到修改一个参数的值将影响所有其他参数的行为！
- en: '![](../Images/bcfc87fd7b2f678a24aa223d1a16ff8d.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/bcfc87fd7b2f678a24aa223d1a16ff8d.png)'
- en: '[Figure 1.7](#figure-1-7): A neural network is parameterized by its weights.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.7](#figure-1-7)：神经网络通过其权重进行参数化。'
- en: To control something, first you need to be able to observe it. To control the
    output of a neural network, you need to be able to measure how far this output
    is from what you expected. This is the job of the *loss function* of the network,
    also sometimes called the *objective function* or *cost function*. The loss function
    takes the predictions of the network and the true target (what you wanted the
    network to output) and computes a distance score, capturing how well the network
    has done on this specific example (see figure 1.8).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 要控制某物，首先你需要能够观察它。要控制神经网络的输出，你需要能够测量这个输出与预期之间的差距。这是网络**损失函数**的工作，有时也称为**目标函数**或**成本函数**。损失函数接受网络的预测和真实目标（你希望网络输出的内容）并计算一个距离分数，捕捉网络在这个特定示例上的表现（见图1.8）。
- en: '![](../Images/6b7df3d893ae72bc9656b4cc90909641.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6b7df3d893ae72bc9656b4cc90909641.png)'
- en: '[Figure 1.8](#figure-1-8): A loss function measures the quality of the network’s
    output.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.8](#figure-1-8)：损失函数衡量网络输出的质量。'
- en: 'The fundamental trick in deep learning is to use this score as a feedback signal
    to adjust the value of the weights a little, in a direction that will lower the
    loss score for the current example (see figure 1.9). This adjustment is the job
    of the *optimizer*, which implements what’s called the *Backpropagation* algorithm:
    the central algorithm in deep learning. The next chapter explains in more detail
    how backpropagation works.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的基本技巧是使用这个分数作为反馈信号来稍微调整权重的值，使其在当前示例中降低损失分数（见图1.9）。这个调整是**优化器**的工作，它实现了所谓的**反向传播**算法：深度学习的核心算法。下一章将更详细地解释反向传播是如何工作的。
- en: '![](../Images/2cb83f777a3616c258905f9e94de13ad.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2cb83f777a3616c258905f9e94de13ad.png)'
- en: '[Figure 1.9](#figure-1-9): The loss score is used as a feedback signal to adjust
    the weights.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.9](#figure-1-9)：损失分数被用作反馈信号来调整权重。'
- en: 'Initially, the weights of the network are assigned random values, so the network
    merely implements a series of random transformations. Naturally, its output is
    far from what it should ideally be, and the loss score is accordingly very high.
    But with every example the network processes, the weights are adjusted a little
    in the correct direction, and the loss score decreases. This is the *training
    loop*, which, repeated a sufficient number of times (typically tens of passes
    over thousands of examples), yields weight values that minimize the loss function.
    A network with a minimal loss is one for which the outputs are as close as they
    can be to the targets: a trained network. Once again, it’s a simple mechanism
    that, once scaled, ends up looking like magic.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 初始时，网络的权重被分配随机值，因此网络仅仅实现了一系列随机变换。自然地，其输出远远不是理想的，相应的损失分数也极高。但随着网络处理每个示例，权重都会在正确的方向上稍微调整，损失分数随之降低。这就是**训练循环**，重复足够多次（通常是数千个示例的数十次遍历），可以得到最小化损失函数的权重值。损失最小的网络是输出尽可能接近目标的网络：一个训练好的网络。再次强调，这是一个简单的机制，一旦扩展，最终看起来就像魔法。
- en: What makes deep learning different
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习与众不同的地方
- en: Is there anything special about deep neural networks that makes them the “right”
    approach for companies to invest in and for researchers to flock to? Will we still
    be using deep neural networks in 20 years?
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络有什么特别之处，使得它们成为公司投资和研究人员蜂拥而至的“正确”方法？我们还会在20年后继续使用深度神经网络吗？
- en: 'Deep learning has several properties that justify its status as an AI revolution,
    and it’s here to stay. We may not be using neural networks many decades from now,
    but whatever we use will directly inherit from modern deep learning and its core
    concepts. These important properties can be broadly sorted into three categories:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习有几个特性使其成为AI革命的正当理由，并且它将长期存在。我们可能在未来几十年内不会使用神经网络，但我们将使用的东西将直接继承自现代深度学习和其核心概念。这些重要特性可以大致分为三类：
- en: '*Simplicity*  — Deep learning makes problem solving much easier, because it
    automates what used to be the most crucial step in a machine learning workflow:
    feature engineering. Previous machine learning techniques — shallow learning —
    only involved transforming the input data into one or two successive representation
    spaces, which wasn’t expressive enough for most problems. As such, humans had
    to go to great lengths to make the initial input data more amenable to processing
    by these methods: they had to manually engineer good representations for their
    data. This is called *feature engineering*. Deep learning, on the other hand,
    completely automates this step: with deep learning, you learn all features in
    one pass rather than having to engineer them yourself. This has greatly simplified
    machine learning workflows, often replacing sophisticated multistage pipelines
    with a single, simple, end-to-end deep learning model.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*简单性* — 深度学习使问题解决变得更加容易，因为它自动化了机器学习工作流程中最关键的一步：特征工程。先前的机器学习技术——浅层学习——仅涉及将输入数据转换为一个或两个连续的表示空间，这对于大多数问题来说表达性不足。因此，人类不得不付出巨大的努力来使初始输入数据更适合这些方法：他们必须手动为他们的数据设计良好的表示。这被称为*特征工程*。另一方面，深度学习完全自动化了这一步骤：使用深度学习，你可以在一次遍历中学习所有特征，而无需自己设计。这极大地简化了机器学习工作流程，通常用单个简单、端到端的深度学习模型取代了复杂的多个阶段管道。'
- en: '*Scalability*  — Deep learning is highly amenable to parallelization on GPUs
    or more specialized machine learning hardware, so it can take full advantage of
    Moore’s law. In addition, deep learning models are trained by iterating over small
    batches of data, allowing them to be trained on datasets of arbitrary size. (The
    only bottleneck is the amount of parallel computational power available, which,
    thanks to Moore’s law, is a fast-moving barrier.)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可扩展性* — 深度学习非常适合在GPU或更专业的机器学习硬件上进行并行化，因此它可以充分利用摩尔定律。此外，深度学习模型通过迭代处理小批量数据来训练，这使得它们可以在任意大小的数据集上训练。（唯一的瓶颈是可用的并行计算能力，但由于摩尔定律，这是一个快速移动的障碍。）'
- en: '*Versatility and reusability*  — Unlike many prior machine learning approaches,
    deep learning models can be trained on additional data without restarting from
    scratch, making them viable for continuous online learning — an important property
    for very large production models. Furthermore, trained deep learning models are
    repurposable and thus reusable: this is the big idea behind “foundation models”
    — large models trained on humongous amounts of data, which can be used across
    many new tasks with little retraining, or even none at all.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通用性和可重用性* — 与许多先前的机器学习方法不同，深度学习模型可以在不从头开始的情况下使用额外数据进行训练，这使得它们适用于持续在线学习——这对于非常大的生产模型来说是一个重要的特性。此外，训练好的深度学习模型可以重新部署，因此可以重复使用：这就是“基础模型”背后的重大理念——在大量数据上训练的大型模型，可以用于许多新的任务，只需少量重新训练，甚至无需重新训练。'
- en: The age of generative AI
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成式AI的时代
- en: Perhaps the most well-known example of deep learning today is the recent wave
    of generative AI applications — chatbot assistants like ChatGPT, Gemini, and Claude,
    as well as image generation services like Midjourney. These applications have
    captured the public imagination with their ability to produce informative or even
    creative content in response to simple prompts, blurring the lines between human
    and machine creativity.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习今天最著名的例子可能是最近一波生成式AI应用——如ChatGPT、Gemini和Claude等聊天机器人助手，以及Midjourney等图像生成服务。这些应用通过其能够对简单提示产生信息或甚至创造性的内容的能力，吸引了公众的想象力，模糊了人类与机器创造力的界限。
- en: Generative AI is powered by very large “foundation models” that learn to *reconstruct*
    the text and image content fed into them — reconstruct a sharp image from a noisy
    version, predict the next word in a sentence, and so on. This means that the *targets*
    from figure 1.8 are taken from the input itself. This is referred to as *self-supervised
    learning*, and it enables those models to use vast amounts of unlabeled data.
    Doing away with the manual data annotations that bottlenecked previous brands
    of machine learning has unlocked a level of scale never seen before — some of
    these foundation models have hundreds of billions of parameters and are trained
    on over 1 petabyte of data, at the cost of tens of millions of dollars.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI由非常庞大的“基础模型”驱动，这些模型学会*重建*输入给它们的文本和图像内容——从噪声版本重建清晰图像，预测句子中的下一个单词，等等。这意味着图1.8中的*目标*是从输入本身提取的。这被称为*自监督学习*，它使得这些模型能够使用大量未标记的数据。摆脱了以前机器学习瓶颈的手动数据标注，解锁了前所未有的规模——这些基础模型中的一些拥有数百亿个参数，并在超过1PB的数据上进行训练，成本高达数百万美元。
- en: These foundation models operate as a kind of fuzzy database of human knowledge,
    making them amenable to a very wide range of applications without needing special-purpose
    programming or retraining. Because they’ve already memorized so much, they can
    solve new problems merely via *prompting* — querying the knowledge representations
    they’ve learned and returning the output most likely to be associated with your
    prompt.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基础模型作为人类知识的一种模糊数据库运行，使得它们适用于非常广泛的应用，而无需专门的编程或重新训练。因为它们已经记住了很多，它们可以通过*提示*来解决新问题——查询它们学习到的知识表示，并返回与提示最可能相关联的输出。
- en: Generative AI only rose to mainstream awareness in 2022, but it has a long history
    — the earliest experiments with text generation date back to the 1990s. The first
    edition of this book, released in 2017, already had a hefty chapter titled “Generative
    AI” that explored the text generation and image generation techniques of the time,
    while promising the then-outlandish notion that, “soon,” much of the cultural
    content we consume would be created with the help of AI.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI直到2022年才进入主流意识，但它有着悠久的历史——最早的文本生成实验可以追溯到20世纪90年代。本书的第一版于2017年发布，其中已经有一个名为“生成式AI”的章节，探讨了当时的文本生成和图像生成技术，并承诺“很快”，我们消费的大部分文化内容都将借助AI创造。
- en: What deep learning has achieved so far
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习至今所取得的成就
- en: Over the past decade, deep learning has achieved nothing short of a technological
    revolution, starting with remarkable results on perceptual tasks from 2013 to
    2017, then making fast progress on natural language processing tasks from 2017
    to 2022, and culminating with a wave of transformative generative AI applications
    from 2022 to now.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年中，深度学习取得了不亚于技术革命的成就，从2013年到2017年，在感知任务上取得了显著成果，然后从2017年到2022年在自然语言处理任务上快速进步，最终从2022年到如今，涌现出一波变革性的生成式AI应用。
- en: 'Deep learning has enabled major breakthroughs, all in extremely challenging
    problems that had long eluded machines:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在极其具有挑战性的问题上实现了重大突破，这些问题长期以来一直困扰着机器：
- en: Fluent and highly versatile chatbots such as ChatGPT and Gemini
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流畅且高度通用的聊天机器人，如ChatGPT和Gemini
- en: Programming assistants like GitHub Copilot
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如GitHub Copilot之类的编程助手
- en: Photorealistic image generation
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真实感图像生成
- en: Human-level image classification
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人类水平的图像分类
- en: Human-level speech transcription
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人类水平的语音转录
- en: Human-level handwriting transcription and printed text transcription
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人类水平的手写转录和印刷文本转录
- en: Dramatically improved machine translation
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显著改进的机器翻译
- en: Dramatically improved text-to-speech conversion
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显著改进的文本到语音转换
- en: Human-level autonomous driving, already deployed to the public in Phoenix, San
    Francisco, Los Angeles, and Austin as of 2025
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人类水平的自动驾驶，截至2025年已在凤凰城、旧金山、洛杉矶和奥斯汀向公众部署
- en: Improved recommender systems, as used by YouTube, Netflix, or Spotify
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改进的推荐系统，如YouTube、Netflix或Spotify所使用
- en: Superhuman Go, Chess, and Poker playing
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超人类水平的围棋、象棋和国际象棋游戏
- en: We’re still exploring the full extent of what deep learning can do. We’ve started
    applying it with great success to a wide variety of problems that were thought
    to be impossible to solve just a few years ago — automatically transcribing the
    tens of thousands of ancient manuscripts held in the Vatican Secret Archive, detecting
    and classifying plant diseases in fields using a simple smartphone, assisting
    oncologists or radiologists with interpreting medical imaging data, predicting
    natural disasters such as floods, hurricanes, and even earthquakes. With every
    milestone, we’re getting closer to an age where deep learning assists us in every
    activity and every field of human endeavor — science, medicine, manufacturing,
    energy, transportation, software development, agriculture, and even artistic creation.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍在探索深度学习的全部潜力。我们已经成功地将它应用于各种问题，这些问题几年前还被认为是不可能解决的——自动转录梵蒂冈秘密档案中持有的数万份古代手稿，使用简单的智能手机在田间检测和分类植物疾病，协助肿瘤学家或放射科医生解读医学影像数据，预测自然灾害，如洪水、飓风甚至地震。随着每一个里程碑的达成，我们正越来越接近一个深度学习帮助我们从事每一个活动和每一个人类努力领域的时代——科学、医学、制造业、能源、交通、软件开发、农业甚至艺术创作。
- en: Beware of the short-term hype
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 警惕短期炒作
- en: 'This seemingly unstoppable string of successes has led to a wave of intense
    hype, some of which is somewhat grounded, but most of which is just fancy fairy
    tales. In early 2023, soon after the release of GPT-4 by OpenAI, many pundits
    were claiming that “no one needed to work anymore” and that mass unemployment
    would be coming within a year, or that economic productivity would soon shoot
    up by 10× to 100×. Of course, two years later, none of this has come to pass —
    the unemployment rate in the US remains low, while productivity metrics are far
    from the promised explosion. Don’t misunderstand: the impact of AI — in particular,
    generative AI — is already considerable, and it is growing remarkably fast. As
    of mid-2025, generative AI was generating tens of billions of dollars in revenue
    per year, which is extremely impressive for an industry that did not exist two
    years prior! But it doesn’t yet make much of a dent in the overall economy and
    pales in comparison to the absolutely unbridled promises we were inundated with
    at its onset.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这看似不可阻挡的成功连锁反应引发了一波强烈的炒作热潮，其中一些多少有些根据，但大部分只是华丽的童话。在2023年初，OpenAI发布GPT-4之后不久，许多专家声称“没有人需要再工作了”，或者大规模失业将在一年内到来，或者经济生产力将很快增长10倍到100倍。当然，两年后，这一切都没有发生——美国的失业率仍然很低，而生产力指标远未达到预期的爆炸性增长。不要误解：AI——特别是生成式AI——的影响已经相当可观，并且增长速度非常快。截至2025年中，生成式AI每年产生数十亿美元的收益，这对于两年前还不存在的行业来说是非常令人印象深刻的！但这还没有在整体经济中造成太大的影响，与我们在其初期所受到的绝对不受约束的承诺相比相形见绌。
- en: While discussions about unemployment and 100× productivity gains triggered by
    AI are already stirring anxieties, there’s an even more sensational side to the
    AI hype. This side proclaims the imminent arrival of human-level general intelligence
    (AGI), or even “superintelligence” far surpassing human capabilities. These claims
    are fueling fears beyond economic disruption — the human species itself might
    be in danger of being replaced by our digital creations.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然关于由AI引发的失业和100倍生产力增长的讨论已经引起了焦虑，但AI炒作的另一个更引人注目的方面是，它宣称人类水平的一般智能（AGI）或甚至超越人类能力的“超级智能”即将到来。这些说法正在引发超越经济破坏的恐惧——人类物种本身可能面临被我们的数字创造物所取代的危险。
- en: It might be tempting for those new to the field to assume that it is the practical
    successes of generative AI that caused the belief in near-term AGI, but that is
    actually backward. The claims of near-term AGI came first, and they significantly
    contributed to the rise of generative AI. As early as 2013, there were fears among
    tech elites that AGI might be coming within a few years. Back then, the idea was
    that DeepMind, a London AI research startup acquired by Google, was on track to
    achieve it. This belief was the impetus behind the founding of OpenAI in 2015,
    which initially aimed to be an open source counterweight to DeepMind. OpenAI played
    a critical role in kick-starting generative AI, so in a peculiar twist, it was
    the belief in near-term AGI that fueled the ascent of generative AI, not the other
    way around. In 2016, OpenAI’s recruiting pitch was that it would achieve AGI by
    2020! To be fair, though, only a minority of people in the tech industry believed
    in such an optimistic timeline back then. By early 2023, however, a significant
    fraction of engineers in the San Francisco Bay Area seemed convinced that AGI
    would be coming within the following couple of years.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: It’s crucial to approach such claims with a healthy dose of skepticism. Despite
    its name, today’s “artificial intelligence” is more accurately described as “cognitive
    automation” — the encoding and operationalization of human skills and knowledge.
    AI excels at solving problems with narrowly defined requirements or those where
    ample precise examples are available. It’s about enhancing the capabilities of
    computers, not about replicating human minds.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 'To be clear, cognitive automation is incredibly useful. But intelligence —
    cognitive autonomy — is a different creature altogether. Think of it this way:
    AI is like a cartoon character, while intelligence is like a living being. A cartoon,
    no matter how realistic, can only act out the scenes it was drawn for. A living
    being, on the other hand, can adapt to the unexpected.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: “If the cartoon is drawn with sufficient realism and covers sufficiently many
    scenes, what’s the difference?” you may ask. If a large language model can output
    a sufficiently human-sounding answer when asked a question, does it matter if
    it possesses true cognitive autonomy? The key difference is adaptability. Intelligence
    is the ability to face the unknown, adapt to it, and learn from it. Automation,
    even at its best, can only handle situations it’s been trained on or programmed
    for. That’s why creating robust automation is so challenging — it requires accounting
    for every possible scenario.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: So don’t worry about AI suddenly becoming self-aware and taking over humanity.
    Today’s technology simply isn’t headed in that direction. Even with significant
    advancements, AI will remain a sophisticated tool, not a sentient being. It’s
    like expecting a better clock to lead to time travel — they’re just different
    things altogether.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Summer can turn to winter
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The danger of inflated short-term expectations is that when technology inevitably
    falls short, research investment could dry up, slowing progress for a long time.
    This has happened before. Twice in the past, AI went through a cycle of intense
    optimism followed by disappointment and skepticism, with a dearth of funding as
    a result. It started with symbolic AI in the 1960s. In those early days, projections
    about AI were flying high. One of the best-known pioneers and proponents of the
    symbolic AI approach was Marvin Minsky, who claimed in 1967, “Within a generation
    . . . the problem of creating ‘artificial intelligence’ will substantially be
    solved.” Three years later, in 1970, he made a more precisely quantified prediction:
    “In from three to eight years we will have a machine with the general intelligence
    of an average human being.” In 2025, such an achievement still appears to be far
    in the future — so far that we have no way to predict how long it will take —
    but in the 1960s and early 1970s, several experts believed it to be right around
    the corner (as do many people today). A few years later, as these high expectations
    failed to materialize, researchers and government funds turned away from the field,
    marking the start of the first *AI winter* (a reference to a nuclear winter, because
    this was shortly after the height of the Cold War).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 过度膨胀的短期期望的危险在于，当技术不可避免地未能达到预期时，研究投资可能会枯竭，长时间地减缓进步。这种情况以前发生过。在过去，人工智能经历了两次高度乐观随后是失望和怀疑的周期，结果是资金短缺。这一切始于20世纪60年代的符号人工智能。在那些早期日子里，对人工智能的预测很高。符号人工智能方法最著名的先驱和倡导者之一是马文·明斯基，他在1967年声称：“在一代人之内……创造‘人工智能’的问题将基本得到解决。”三年后，在1970年，他做出了一个更精确的预测：“在三到八年之内，我们将拥有一个具有普通人类一般智能的机器。”到2025年，这样的成就似乎仍然遥不可及——如此遥远以至于我们无法预测它需要多长时间——但在20世纪60年代和早期70年代，一些专家认为它就在眼前（正如今天许多人所认为的那样）。几年后，随着这些高期望未能实现，研究人员和政府资金开始远离这个领域，标志着第一次**人工智能寒冬**（这个术语是参照核冬天而来的，因为这是冷战高潮之后不久）的开始。
- en: It wouldn’t be the last one. In the 1980s, a new take on symbolic AI, *expert
    systems*, started gathering steam among large companies. A few initial success
    stories triggered a wave of investment, with corporations around the world starting
    their own in-house AI departments to develop expert systems. Around 1985, companies
    were spending over $1 billion each year on the technology; but by the early 1990s,
    these systems had proven expensive to maintain, difficult to scale, and limited
    in scope, and interest died down. Thus began the second AI winter. We may be currently
    witnessing the third cycle of AI hype and disappointment — and we’re still in
    the phase of intense optimism.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这不会是最后一次。在20世纪80年代，一种新的符号人工智能观点，即**专家系统**，开始在大型公司中积聚势头。一些初步的成功故事引发了一波投资热潮，全球各地的公司开始建立自己的内部人工智能部门来开发专家系统。到1985年左右，公司每年在技术上的支出超过10亿美元；但到20世纪90年代初，这些系统已被证明维护成本高昂、难以扩展且应用范围有限，兴趣逐渐消退。于是开始了第二次人工智能寒冬。我们现在可能正在见证第三次人工智能炒作和失望的周期——我们仍然处于极度乐观的阶段。
- en: My current view is that we’re unlikely to see a full-scale retreat away from
    AI research like we saw in the 1990s. If there is a winter, it should be very
    mild. AI has already demonstrated its world-changing value. However, it seems
    inevitable that some air will need to be let out of the 2023–2024 AI bubble. Currently,
    AI investment, primarily in data centers and GPUs, surpasses $100 billion annually,
    while revenue generation lags significantly, closer to $10 billion. AI is currently
    being judged by executives and investors not by what it has accomplished, but
    by what we are told it might soon become able to do — much of which will durably
    stay out of reach of existing technologies. Something will have to give. But what
    will happen precisely as the AI bubble deflates is still up in the air.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我目前的观点是，我们不太可能看到像20世纪90年代那样全面退出人工智能研究的情况。如果真的有寒冬，它应该非常温和。人工智能已经证明了其改变世界的价值。然而，似乎不可避免的是，2023-2024年的人工智能泡沫需要释放一些空气。目前，人工智能投资，主要在数据中心和GPU上，每年超过1000亿美元，而收入生成却远远落后，接近100亿美元。目前，人工智能正在被高管和投资者根据我们被告知它可能很快能够做到的事情来评判——其中许多将长期超出现有技术的范围。总会有所让步。但人工智能泡沫缩水时会发生什么仍然是个未知数。
- en: The promise of AI
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人工智能的承诺
- en: Although we may have unrealistic short-term expectations for AI, the long-term
    picture is looking bright. We’re only getting started in applying deep learning
    to many important problems for which it could prove transformative, from medical
    diagnoses to digital assistants.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可能对人工智能的短期期望不切实际，但长期前景看起来光明。我们只是在将深度学习应用于许多可能证明具有变革性的重要问题，例如医疗诊断和数字助手，才刚刚开始。
- en: 'In 2017, in this very book, I wrote:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在2017年，在这本同一本书中，我写道：
- en: 'Right now, it may seem hard to believe that AI could have a large impact on
    our world, because it isn’t yet widely deployed — much as, back in 1995, it would
    have been difficult to believe in the future impact of the internet. Back then,
    most people didn’t see how the internet was relevant to them and how it was going
    to change their lives. The same is true for deep learning and AI today. But make
    no mistake: AI is coming. In a not-so-distant future, AI will be your assistant,
    even your friend; it will answer your questions, help educate your kids, and watch
    over your health. It will deliver your groceries to your door and drive you from
    point A to point B. It will be your interface to an increasingly complex and information-intensive
    world. And, even more important, AI will help humanity as a whole move forward,
    by assisting human scientists in new breakthrough discoveries across all scientific
    fields, from genomics to mathematics.'
  id: totrans-112
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 目前，可能很难相信人工智能会对我们的世界产生重大影响，因为它尚未得到广泛部署——就像1995年，人们很难相信互联网未来的影响一样。当时，大多数人看不到互联网与他们有何关联，以及它将如何改变他们的生活。对于深度学习和人工智能来说，今天也是如此。但不要误解：人工智能正在到来。在不那么遥远的未来，人工智能将成为你的助手，甚至你的朋友；它将回答你的问题，帮助你教育孩子，并关注你的健康。它将把你的杂货送到门口，并把你从A点带到B点。它将成为你通往日益复杂和信息密集世界的接口。更重要的是，人工智能将通过协助人类科学家在所有科学领域的新突破性发现，帮助整个人类社会前进，从基因组学到数学。
- en: 'Fast-forwarding to 2025, most of these things have either come true or are
    on the verge of coming true — and this is just the beginning:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 快进到2025年，这些事情中的大多数要么已经实现，要么即将实现——而这只是开始：
- en: Tens of millions of people are using AI chatbots like ChatGPT, Gemini, and Claude
    as assistants on a daily basis. In fact, question-answering and “educating your
    kids” (homework assistance) have turned out to be the top applications of these
    chatbots! For many people, AI is already the go-to interface to the world’s information.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数以千万计的人正在每天使用像ChatGPT、Gemini和Claude这样的AI聊天机器人作为助手。事实上，问答和“教育你的孩子”（家庭作业辅助）已经成为这些聊天机器人的主要应用！对许多人来说，人工智能已经成为通往世界信息的首选接口。
- en: Hundreds of thousands of people interact with AI “friends” in applications such
    as Character.ai.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数十万人在与AI“朋友”互动，例如在Character.ai等应用程序中。
- en: Fully autonomous driving is already deployed at scale in cities like Phoenix,
    San Francisco, Los Angeles, and Austin.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在凤凰城、旧金山、洛杉矶和奥斯汀等城市，完全自动驾驶已经大规模部署。
- en: AI is making major strides toward helping accelerate science. The AlphaFold
    model from DeepMind is helping biologists predict protein structures with unprecedented
    accuracy. Renowned mathematician Terence Tao believes that by around 2026, AI
    could become a reliable co-author in mathematical research and other fields when
    used appropriately.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工智能正在迈出重大步伐，帮助加速科学发展。DeepMind的AlphaFold模型正在帮助生物学家以前所未有的准确性预测蛋白质结构。著名数学家田纳西·陶（Terence
    Tao）认为，到2026年左右，如果使用得当，人工智能可能成为数学研究和其他领域的可靠合著者。
- en: The AI revolution, once a distant vision, is now rapidly unfolding before our
    eyes. On the way, we may face a few setbacks — in much the same way the internet
    industry was overhyped in 1998–1999 and suffered from a crash that dried up investment
    throughout the early 2000s. But we’ll get there eventually. AI will end up being
    applied to nearly every process that makes up our society and our daily lives,
    much like the internet is today.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能革命，曾经是一个遥远的愿景，现在正迅速在我们眼前展开。在这个过程中，我们可能会遇到一些挫折——就像1998-1999年互联网行业过度炒作，并在2000年代初遭受了投资枯竭的崩溃一样。但最终我们会到达那里。人工智能最终将应用于构成我们社会和日常生活的几乎每一个过程，就像今天的互联网一样。
- en: Don’t believe the short-term hype, but do believe in the long-term vision. It
    may take a while for AI to be deployed to its true potential — a potential the
    full extent of which no one has yet dared to dream — but AI is coming, and it
    will transform our world in a fantastic way.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 不要相信短期炒作，但要对长期愿景保持信心。AI发挥其真正潜力可能需要一段时间——这种潜力的全貌至今无人敢梦想——但AI正在到来，并以惊人的方式改变我们的世界。
- en: Footnotes
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 脚注
- en: 'A. M. Turing, “Computing Machinery and Intelligence,” Mind 59, no. 236 (1950):
    433-460. [[↩]](#footnote-link-1)'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A. M. 图灵，“计算机与智能”，《心灵》59卷，第236期（1950年）：433-460。 [[↩]](#footnote-link-1)]
- en: Although the Turing test has sometimes been interpreted as a literal test —
    a goal the field of AI should set out to reach — Turing merely meant it as a conceptual
    device in a philosophical discussion about the nature of cognition. [[↩]](#footnote-link-2)
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尽管图灵测试有时被解释为一种字面意义上的测试——人工智能领域应努力实现的目标——图灵只是将其视为关于认知本质的哲学讨论中的一个概念性工具。 [[↩]](#footnote-link-2)
