- en: Chapter 3\. Variational Autoencoders
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章. 变分自动编码器
- en: In 2013, Diederik P. Kingma and Max Welling published a paper that laid the
    foundations for a type of neural network known as a *variational autoencoder*
    (VAE).^([1](ch03.xhtml#idm45387024580992)) This is now one of the most fundamental
    and well-known deep learning architectures for generative modeling and an excellent
    place to start our journey into generative deep learning.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 2013年，Diederik P. Kingma和Max Welling发表了一篇论文，奠定了一种称为*变分自动编码器*（VAE）的神经网络类型的基础。^([1](ch03.xhtml#idm45387024580992))
    这现在是最基本和最知名的深度学习架构之一，用于生成建模，也是我们进入生成式深度学习旅程的绝佳起点。
- en: In this chapter, we shall start by building a standard autoencoder and then
    see how we can extend this framework to develop a variational autoencoder. Along
    the way, we will pick apart both types of models, to understand how they work
    at a granular level. By the end of the chapter you should have a complete understanding
    of how to build and manipulate autoencoder-based models and, in particular, how
    to build a variational autoencoder from scratch to generate images based on your
    own dataset.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先构建一个标准的自动编码器，然后看看如何扩展这个框架以开发一个变分自动编码器。在这个过程中，我们将分析这两种模型，以了解它们在细粒度级别上的工作原理。通过本章的结束，您应该完全了解如何构建和操作基于自动编码器的模型，特别是如何从头开始构建一个变分自动编码器，以根据您自己的数据集生成图像。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Let’s start with a simple story that will help to explain the fundamental problem
    that an autoencoder is trying to solve.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个简单的故事开始，这将有助于解释自动编码器试图解决的基本问题。
- en: Let’s now explore how this story relates to building autoencoders.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们探讨这个故事如何与构建自动编码器相关。
- en: Autoencoders
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动编码器
- en: A diagram of the process described by the story is shown in [Figure 3-2](#encoder_decoder_story).
    You play the part of the *encoder*, moving each item of clothing to a location
    in the wardrobe. This process is called *encoding*. Brian plays the part of the
    *decoder*, taking a location in the wardrobe and attempting to re-create the item.
    This process is called *decoding*.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 故事描述的过程的图表显示在[图3-2](#encoder_decoder_story)中。您扮演*编码器*的角色，将每件服装移动到衣柜中的一个位置。这个过程称为*编码*。Brian扮演*解码器*的角色，接受衣柜中的一个位置，并尝试重新创建该项目。这个过程称为*解码*。
- en: '![](Images/gdl2_0302.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0302.png)'
- en: Figure 3-2\. Items of clothing in the infinite wardrobe—each black dot represents
    an item of clothing
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-2. 无限衣柜中的服装项目-每个黑点代表一个服装项目
- en: Each location in the wardrobe is represented by two numbers (i.e., a 2D vector).
    For example, the trousers in [Figure 3-2](#encoder_decoder_story) are encoded
    to the point [6.3, –0.9]. This vector is also known as an *embedding* because
    the encoder attempts to embed as much information into it as possible, so that
    the decoder can produce an accurate reconstruction.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 衣柜中的每个位置由两个数字表示（即一个2D向量）。例如，[图3-2](#encoder_decoder_story)中的裤子被编码为点[6.3，-0.9]。这个向量也被称为*嵌入*，因为编码器试图将尽可能多的信息嵌入其中，以便解码器可以产生准确的重建。
- en: An *autoencoder* is simply a neural network that is trained to perform the task
    of encoding and decoding an item, such that the output from this process is as
    close to the original item as possible. Crucially, it can be used as a generative
    model, because we can decode any point in the 2D space that we want (in particular,
    those that are not embeddings of original items) to produce a novel item of clothing.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*自动编码器*只是一个经过训练的神经网络，用于执行编码和解码项目的任务，使得这个过程的输出尽可能接近原始项目。关键是，它可以用作生成模型，因为我们可以解码2D空间中的任何点（特别是那些不是原始项目的嵌入）以生成新的服装项目。'
- en: Let’s now see how we can build an autoencoder using Keras and apply it to a
    real dataset!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何使用Keras构建自动编码器并将其应用于真实数据集！
- en: Running the Code for This Example
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行此示例的代码
- en: The code for this example can be found in the Jupyter notebook located at *notebooks/03_vae/01_autoencoder/autoencoder.ipynb*
    in the book repository.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例的代码可以在位于书籍存储库中的Jupyter笔记本*notebooks/03_vae/01_autoencoder/autoencoder.ipynb*中找到。
- en: The Fashion-MNIST Dataset
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Fashion-MNIST数据集
- en: For this example, we’ll be using the [Fashion-MNIST dataset](https://oreil.ly/DS4-4)—a
    collection of grayscale images of clothing items, each of size 28 × 28 pixels.
    Some example images from the dataset are shown in [Figure 3-3](#fashion_mnist).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用[Fashion-MNIST数据集](https://oreil.ly/DS4-4)-一个由28×28像素大小的服装项目的灰度图像组成的集合。数据集中的一些示例图像显示在[图3-3](#fashion_mnist)中。
- en: '![](Images/gdl2_0303.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0303.png)'
- en: Figure 3-3\. Examples of images from the Fashion-MNIST dataset
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-3. Fashion-MNIST数据集中的图像示例
- en: The dataset comes prepackaged with TensorFlow, so it can be downloaded as shown
    in [Example 3-1](#fashion-mnist-ex).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集已经预先打包到TensorFlow中，因此可以按照[示例3-1](#fashion-mnist-ex)中所示进行下载。
- en: Example 3-1\. Loading the Fashion-MNIST dataset
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-1. 加载Fashion-MNIST数据集
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: These are 28 × 28 grayscale images (pixel values between 0 and 255) out of the
    box, which we need to preprocess to ensure that the pixel values are scaled between
    0 and 1\. We will also pad each image to 32 × 32 for easier manipulation of the
    tensor shape as it passes through the network, as shown in [Example 3-2](#fashion-preprocessing-ex).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是28×28的灰度图像（像素值在0到255之间），我们需要对其进行预处理，以确保像素值在0到1之间。我们还将每个图像填充到32×32，以便更容易地处理通过网络的张量形状，如[示例3-2](#fashion-preprocessing-ex)中所示。
- en: Example 3-2\. Preprocessing the data
  id: totrans-23
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-2. 数据预处理
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, we need to understand the overall structure of an autoencoder, so that
    we can code it up using TensorFlow and Keras.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要了解自动编码器的整体结构，以便我们可以使用TensorFlow和Keras对其进行编码。
- en: The Autoencoder Architecture
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动编码器架构
- en: 'An *autoencoder* is a neural network made up of two parts:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*自动编码器*是由两部分组成的神经网络：'
- en: An *encoder* network that compresses high-dimensional input data such as an
    image into a lower-dimensional embedding vector
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*编码器*网络，将高维输入数据（如图像）压缩成较低维度的嵌入向量
- en: A *decoder* network that decompresses a given embedding vector back to the original
    domain (e.g., back to an image)
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*解码器*网络，将给定的嵌入向量解压缩回原始域（例如，回到图像）
- en: A diagram of the network architecture is shown in [Figure 3-4](#autoencoder).
    An input image is encoded to a latent embedding vector <math alttext="z"><mi>z</mi></math>
    , which is then decoded back to the original pixel space.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 网络架构图显示在[图3-4](#autoencoder)中。输入图像被编码为潜在嵌入向量<math alttext="z"><mi>z</mi></math>，然后解码回原始像素空间。
- en: '![](Images/gdl2_0304.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0304.png)'
- en: Figure 3-4\. Autoencoder architecture diagram
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-4。自动编码器架构图
- en: The autoencoder is trained to reconstruct an image, after it has passed through
    the encoder and back out through the decoder. This may seem strange at first—why
    would you want to reconstruct a set of images that you already have available
    to you? However, as we shall see, it is the embedding space (also called the *latent
    space*) that is the interesting part of the autoencoder, as sampling from this
    space will allow us to generate new images.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器经过编码器和解码器后被训练重建图像。这一开始可能看起来很奇怪——为什么要重建一组已经可用的图像？然而，正如我们将看到的，自动编码器中有趣的部分是嵌入空间（也称为*潜在空间*），因为从这个空间中取样将允许我们生成新的图像。
- en: Let’s first define what we mean by an embedding. The embedding ( <math alttext="z"><mi>z</mi></math>
    ) is a compression of the original image into a lower-dimensional latent space.
    The idea is that by choosing any point in the latent space, we can generate novel
    images by passing this point through the decoder, since the decoder has learned
    how to convert points in the latent space into viable images.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 首先定义一下我们所说的嵌入。嵌入（<math alttext="z"><mi>z</mi></math>）是原始图像压缩到较低维度潜在空间中。这个想法是通过选择潜在空间中的任意点，我们可以通过解码器生成新颖的图像，因为解码器已经学会如何将潜在空间中的点转换为可行的图像。
- en: In our example, we will embed images into a two-dimensional latent space. This
    will help us to visualize the latent space, since we can easily plot points in
    2D. In practice, the latent space of an autoencoder will usually have more than
    two dimensions in order to have more freedom to capture greater nuance in the
    images.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们将图像嵌入到一个二维潜在空间中。这将帮助我们可视化潜在空间，因为我们可以轻松在2D中绘制点。实际上，自动编码器的潜在空间通常会有超过两个维度，以便更自由地捕获图像中更多的细微差别。
- en: Autoencoders as Denoising Models
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动编码器作为去噪模型
- en: Autoencoders can be used to clean noisy images, since the encoder learns that
    it is not useful to capture the position of the random noise inside the latent
    space in order to reconstruct the original. For tasks such as this, a 2D latent
    space is probably too small to encode sufficient relevant information from the
    input. However, as we shall see, increasing the dimensionality of the latent space
    quickly leads to problems if we want to use the autoencoder as a generative model.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器可以用于清理嘈杂的图像，因为编码器学习到捕获潜在空间中随机噪声的位置并不有用，以便重建原始图像。对于这样的任务，一个二维潜在空间可能太小，无法从输入中编码足够的相关信息。然而，正如我们将看到的，如果我们想将自动编码器用作生成模型，增加潜在空间的维度会很快导致问题。
- en: Let’s now see how to build the encoder and decoder.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何构建编码器和解码器。
- en: The Encoder
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码器
- en: In an autoencoder, the encoder’s job is to take the input image and map it to
    an embedding vector in the latent space. The architecture of the encoder we will
    be building is shown in [Table 3-1](#encoder_diagram).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动编码器中，编码器的任务是将输入图像映射到潜在空间中的嵌入向量。我们将构建的编码器的架构显示在[表3-1](#encoder_diagram)中。
- en: Table 3-1\. Model summary of the encoder
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 表3-1。编码器的模型摘要
- en: '| Layer (type) | Output shape | Param # |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 层（类型）| 输出形状 | 参数 # |'
- en: '| --- | --- | --- |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| InputLayer | (None, 32, 32, 1) | 0 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| InputLayer |（None，32，32，1）| 0 |'
- en: '| Conv2D | (None, 16, 16, 32) | 320 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| Conv2D |（None，16，16，32）| 320 |'
- en: '| Conv2D | (None, 8, 8, 64) | 18,496 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| Conv2D |（None，8，8，64）| 18,496 |'
- en: '| Conv2D | (None, 4, 4, 128) | 73,856 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| Conv2D |（None，4，4，128）| 73,856 |'
- en: '| Flatten | (None, 2048) | 0 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| Flatten |（None，2048）| 0 |'
- en: '| Dense | (None, 2) | 4,098 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| Dense |（None，2）| 4,098 |'
- en: '| Total params | 96,770 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 总参数 | 96,770 |'
- en: '| Trainable params | 96,770 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 可训练参数 | 96,770 |'
- en: '| Non-trainable params | 0 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 不可训练参数 | 0 |'
- en: To achieve this, we first create an `Input` layer for the image and pass this
    through three `Conv2D` layers in sequence, each capturing increasingly high-level
    features. We use a stride of 2 to halve the size of the output of each layer,
    while increasing the number of channels. The last convolutional layer is flattened
    and connected to a `Dense` layer of size 2, which represents our two-dimensional
    latent space.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们首先为图像创建一个“输入”层，并依次通过三个“Conv2D”层，每个层捕获越来越高级的特征。我们使用步幅为2，以减半每个层的输出大小，同时增加通道数。最后一个卷积层被展平，并连接到大小为2的“Dense”层，代表我们的二维潜在空间。
- en: '[Example 3-3](#the-encoder-ex) shows how to build this in Keras.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例3-3](#the-encoder-ex)展示了如何在Keras中构建这个模型。'
- en: Example 3-3\. The encoder
  id: totrans-55
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-3。编码器
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[![1](Images/1.png)](#co_variational_autoencoders_CO1-1)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_variational_autoencoders_CO1-1)'
- en: Define the `Input` layer of the encoder (the image).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 定义编码器（图像）的“输入”层。
- en: '[![2](Images/2.png)](#co_variational_autoencoders_CO1-2)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_variational_autoencoders_CO1-2)'
- en: Stack `Conv2D` layers sequentially on top of each other.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序堆叠“Conv2D”层。
- en: '[![3](Images/3.png)](#co_variational_autoencoders_CO1-3)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_variational_autoencoders_CO1-3)'
- en: Flatten the last convolutional layer to a vector.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 将最后一个卷积层展平为一个向量。
- en: '[![4](Images/4.png)](#co_variational_autoencoders_CO1-4)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_variational_autoencoders_CO1-4)'
- en: Connect this vector to the 2D embeddings with a `Dense` layer.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个向量连接到2D嵌入中的“Dense”层。
- en: '[![5](Images/5.png)](#co_variational_autoencoders_CO1-5)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_variational_autoencoders_CO1-5)'
- en: The Keras `Model` that defines the encoder—a model that takes an input image
    and encodes it into a 2D embedding.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 定义编码器的Keras“Model”——一个将输入图像并将其编码为2D嵌入的模型。
- en: Tip
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: I strongly encourage you to experiment with the number of convolutional layers
    and filters to understand how the architecture affects the overall number of model
    parameters, model performance, and model runtime.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈建议您尝试不同数量的卷积层和滤波器，以了解架构如何影响模型参数的总数、模型性能和模型运行时间。
- en: The Decoder
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码器
- en: The decoder is a mirror image of the encoder—instead of convolutional layers,
    we use *convolutional transpose* layers, as shown in [Table 3-2](#decoder_diagram).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器是编码器的镜像——我们使用*卷积转置*层，而不是卷积层，如[表 3-2](#decoder_diagram)所示。
- en: Table 3-2\. Model summary of the decoder
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3-2. 解码器的模型摘要
- en: '| Layer (type) | Output shape | Param # |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 层（类型） | 输出形状 | 参数 # |'
- en: '| --- | --- | --- |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| InputLayer | (None, 2) | 0 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| InputLayer | (None, 2) | 0 |'
- en: '| Dense | (None, 2048) | 6,144 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Dense | (None, 2048) | 6,144 |'
- en: '| Reshape | (None, 4, 4, 128) | 0 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 重塑 | (None, 4, 4, 128) | 0 |'
- en: '| Conv2DTranspose | (None, 8, 8, 128) | 147,584 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| Conv2DTranspose | (None, 8, 8, 128) | 147,584 |'
- en: '| Conv2DTranspose | (None, 16, 16, 64) | 73,792 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| Conv2DTranspose | (None, 16, 16, 64) | 73,792 |'
- en: '| Conv2DTranspose | (None, 32, 32, 32) | 18,464 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| Conv2DTranspose | (None, 32, 32, 32) | 18,464 |'
- en: '| Conv2D | (None, 32, 32, 1) | 289 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| Conv2D | (None, 32, 32, 1) | 289 |'
- en: '| Total params | 246,273 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 总参数 | 246,273 |'
- en: '| Trainable params | 246,273 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 可训练参数 | 246,273 |'
- en: '| Non-trainable params | 0 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 不可训练参数 | 0 |'
- en: '[Example 3-4](#the-decoder-ex) shows how we build the decoder in Keras.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 3-4](#the-decoder-ex) 展示了我们如何在Keras中构建解码器。'
- en: Example 3-4\. The decoder
  id: totrans-85
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-4. 解码器
- en: '[PRE3]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[![1](Images/1.png)](#co_variational_autoencoders_CO2-1)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_variational_autoencoders_CO2-1)'
- en: Define the `Input` layer of the decoder (the embedding).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 定义解码器（嵌入）的`Input`层。
- en: '[![2](Images/2.png)](#co_variational_autoencoders_CO2-2)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_variational_autoencoders_CO2-2)'
- en: Connect the input to a `Dense` layer.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入连接到`Dense`层。
- en: '[![3](Images/3.png)](#co_variational_autoencoders_CO2-3)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_variational_autoencoders_CO2-3)'
- en: '`Reshape` this vector into a tensor that can be fed as input into the first
    `Conv2DTranspose` layer.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个向量重塑成一个张量，可以作为输入传递到第一个`Conv2DTranspose`层。
- en: '[![4](Images/4.png)](#co_variational_autoencoders_CO2-4)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_variational_autoencoders_CO2-4)'
- en: Stack `Conv2DTranspose` layers on top of each other.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 将`Conv2DTranspose`层堆叠在一起。
- en: '[![5](Images/5.png)](#co_variational_autoencoders_CO2-5)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_variational_autoencoders_CO2-5)'
- en: The Keras `Model` that defines the decoder—a model that takes an embedding in
    the latent space and decodes it into the original image domain.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 定义解码器的Keras `Model`——一个模型，将潜在空间中的嵌入解码为原始图像域。
- en: Joining the Encoder to the Decoder
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将编码器连接到解码器
- en: To train the encoder and decoder simultaneously, we need to define a model that
    will represent the flow of an image through the encoder and back out through the
    decoder. Luckily, Keras makes it extremely easy to do this, as you can see in
    [Example 3-5](#full-autoencoder-ex). Notice the way in which we specify that the
    output from the autoencoder is simply the output from the encoder after it has
    been passed through the decoder.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了同时训练编码器和解码器，我们需要定义一个模型，表示图像通过编码器流动并通过解码器返回。幸运的是，Keras使这变得非常容易，如[示例 3-5](#full-autoencoder-ex)所示。请注意我们如何指定自编码器的输出只是经过解码器传递后的编码器的输出的方式。
- en: Example 3-5\. The full autoencoder
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-5. 完整自编码器
- en: '[PRE4]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](Images/1.png)](#co_variational_autoencoders_CO3-1)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_variational_autoencoders_CO3-1)'
- en: The Keras `Model` that defines the full autoencoder—a model that takes an image
    and passes it through the encoder and back out through the decoder to generate
    a reconstruction of the original image.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 定义完整自编码器的Keras `Model`——一个模型，将图像通过编码器传递并通过解码器返回，生成原始图像的重建。
- en: Now that we’ve defined our model, we just need to compile it with a loss function
    and optimizer, as shown in [Example 3-6](#the-compilation-ex). The loss function
    is usually chosen to be either the root mean squared error (RMSE) or binary cross-entropy
    between the individual pixels of the original image and the reconstruction.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了我们的模型，我们只需要使用损失函数和优化器对其进行编译，如[示例 3-6](#the-compilation-ex)所示。损失函数通常选择为原始图像的每个像素之间的均方根误差（RMSE）或二进制交叉熵。
- en: Example 3-6\. Compiling the autoencoder
  id: totrans-104
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-6. 编译自编码器
- en: '[PRE5]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We can now train the autoencoder by passing in the input images as both the
    input and output, as shown in [Example 3-7](#training-the-autoencoder-ex).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过将输入图像作为输入和输出来训练自编码器，如[示例 3-7](#training-the-autoencoder-ex)所示。
- en: Example 3-7\. Training the autoencoder
  id: totrans-107
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-7. 训练自编码器
- en: '[PRE6]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now that our autoencoder is trained, the first thing we need to check is that
    it is able to accurately reconstruct the input images.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的自编码器已经训练好了，我们需要检查的第一件事是它是否能够准确重建输入图像。
- en: Reconstructing Images
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重建图像
- en: We can test the ability to reconstruct images by passing images from the test
    set through the autoencoder and comparing the output to the original images. The
    code for this is shown in [Example 3-8](#reconstruction-ex).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将测试集中的图像通过自编码器并将输出与原始图像进行比较来测试重建图像的能力。这个代码在[示例 3-8](#reconstruction-ex)中展示。
- en: Example 3-8\. Reconstructing images using the autoencoder
  id: totrans-112
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-8. 使用自编码器重建图像
- en: '[PRE7]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In [Figure 3-6](#reconstructed_clothing) you can see some examples of original
    images (top row), the 2D vectors after encoding, and the reconstructed items after
    decoding (bottom row).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 3-6](#reconstructed_clothing)中，您可以看到一些原始图像的示例（顶部行），编码后的2D向量，以及解码后的重建物品（底部行）。
- en: '![](Images/gdl2_0306.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0306.png)'
- en: Figure 3-6\. Examples of encoding and decoding items of clothing
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-6. 服装项目的编码和解码示例
- en: Notice how the reconstruction isn’t perfect—there are still some details of
    the original images that aren’t captured by the decoding process, such as logos.
    This is because by reducing each image to just two numbers, we naturally lose
    some information.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 注意重建并不完美——解码过程中仍然有一些原始图像的细节没有被捕捉到，比如标志。这是因为将每个图像减少到只有两个数字，自然会丢失一些信息。
- en: Let’s now investigate how the encoder is representing images in the latent space.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来研究编码器如何在潜在空间中表示图像。
- en: Visualizing the Latent Space
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化潜在空间
- en: We can visualize how images are embedded into the latent space by passing the
    test set through the encoder and plotting the resulting embeddings, as shown in
    [Example 3-9](#embedding-ex).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将测试集通过编码器并绘制结果嵌入来可视化图像如何嵌入到潜在空间中，如[示例3-9](#embedding-ex)所示。
- en: Example 3-9\. Embedding images using the encoder
  id: totrans-121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-9。使用编码器嵌入图像
- en: '[PRE8]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The resulting plot is the scatter plot shown in [Figure 3-2](#encoder_decoder_story)—each
    black point represents an image that has been embedded into the latent space.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 结果绘图是[图3-2](#encoder_decoder_story)中显示的散点图-每个黑点代表一个被嵌入到潜在空间中的图像。
- en: In order to better understand how this latent space is structured, we can make
    use of the labels that come with the Fashion-MNIST dataset, describing the type
    of item in each image. There are 10 groups altogether, shown in [Table 3-3](#fashion-mnist-table).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这个潜在空间的结构，我们可以利用Fashion-MNIST数据集中附带的标签，描述每个图像中的物品类型。总共有10组，如[表3-3](#fashion-mnist-table)所示。
- en: Table 3-3\. The Fashion-MNIST labels
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 表3-3。时尚MNIST标签
- en: '| ID | Clothing label |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| ID | 服装标签 |'
- en: '| --- | --- |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 0 | T-shirt/top |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 0 | T恤/上衣 |'
- en: '| 1 | Trouser |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 裤子 |'
- en: '| 2 | Pullover |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 套头衫 |'
- en: '| 3 | Dress |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 连衣裙 |'
- en: '| 4 | Coat |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 外套 |'
- en: '| 5 | Sandal |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 凉鞋 |'
- en: '| 6 | Shirt |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 衬衫 |'
- en: '| 7 | Sneaker |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 运动鞋 |'
- en: '| 8 | Bag |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 包 |'
- en: '| 9 | Ankle boot |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 短靴 |'
- en: We can color each point based on the label of the corresponding image to produce
    the plot in [Figure 3-7](#ae_plot_colour). Now the structure becomes very clear!
    Even though the clothing labels were never shown to the model during training,
    the autoencoder has naturally grouped items that look alike into the same parts
    of the latent space. For example, the dark blue cloud of points in the bottom-right
    corner of the latent space are all different images of trousers and the red cloud
    of points toward the center are all ankle boots.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据相应图像的标签对每个点进行着色，以生成[图3-7](#ae_plot_colour)中的绘图。现在结构变得非常清晰！尽管在训练期间模型从未展示过服装标签，但自动编码器自然地将外观相似的项目分组到潜在空间的相同部分。例如，潜在空间右下角的深蓝色点云都是不同的裤子图像，中心附近的红色点云都是短靴。
- en: '![](Images/gdl2_0307.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0307.png)'
- en: Figure 3-7\. Plot of the latent space, colored by clothing label
  id: totrans-140
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-7。潜在空间的绘图，按服装标签着色
- en: Generating New Images
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成新图像
- en: We can generate novel images by sampling some points in the latent space and
    using the decoder to convert these back into pixel space, as shown in [Example 3-10](#generation-ex).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在潜在空间中抽样一些点并使用解码器将其转换回像素空间来生成新图像，如[示例3-10](#generation-ex)所示。
- en: Example 3-10\. Generating novel images using the decoder
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-10。使用解码器生成新图像
- en: '[PRE9]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Some examples of generated images are shown in [Figure 3-8](#autoencoder_exhibits),
    alongside their embeddings in the latent space.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 一些生成的图像示例显示在[图3-8](#autoencoder_exhibits)中，以及它们在潜在空间中的嵌入。
- en: '![](Images/gdl2_0308.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0308.png)'
- en: Figure 3-8\. Generated items of clothing
  id: totrans-147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-8。生成的服装项目
- en: Each blue dot maps to one of the images shown on the right of the diagram, with
    the embedding vector shown underneath. Notice how some of the generated items
    are more realistic than others. Why is this?
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 每个蓝点映射到图表右侧显示的图像之一，下面显示嵌入向量。注意一些生成的项目比其他项目更真实。为什么呢？
- en: 'To answer this, let’s first make a few observations about the overall distribution
    of points in the latent space, referring back to [Figure 3-7](#ae_plot_colour):'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这个问题，让我们首先观察一下潜在空间中点的整体分布，参考[图3-7](#ae_plot_colour)：
- en: Some clothing items are represented over a very small area and others over a
    much larger area.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些服装项目在一个非常小的区域内表示，而其他服装项目在一个更大的区域内表示。
- en: The distribution is not symmetrical about the point (0, 0), or bounded. For
    example, there are far more points with positive y-axis values than negative,
    and some points even extend to a y-axis value > 8.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布关于点(0, 0)不对称，也不是有界的。例如，具有正y轴值的点比负值更多，甚至有些点甚至延伸到y轴值> 8。
- en: There are large gaps between colors containing few points.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 颜色之间有很大的间隙，包含很少的点。
- en: These observations actually make sampling from the latent space quite challenging.
    If we overlay the latent space with images of decoded points on a grid, as shown
    in [Figure 3-9](#ae_overlay), we can begin to understand why the decoder may not
    always generate images to a satisfactory standard.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这些观察实际上使得从潜在空间中抽样变得非常具有挑战性。如果我们将解码点的图像叠加在网格上的潜在空间上，如[图3-9](#ae_overlay)所示，我们可以开始理解为什么解码器可能不总是生成令人满意的图像。
- en: '![](Images/gdl2_0309.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0309.png)'
- en: Figure 3-9\. A grid of decoded embeddings, overlaid with the embeddings from
    the original images in the dataset, colored by item type
  id: totrans-155
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-9。解码嵌入的网格，与数据集中原始图像的嵌入叠加，按项目类型着色
- en: Firstly, we can see that if we pick points uniformly in a bounded space that
    we define, we’re more likely to sample something that decodes to look like a bag
    (ID 8) than an ankle boot (ID 9) because the part of the latent space carved out
    for bags (orange) is larger than the ankle boot area (red).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以看到，如果我们在我们定义的有界空间中均匀选择点，我们更有可能抽样出看起来像包（ID 8）而不是短靴（ID 9）的东西，因为为包（橙色）划定的潜在空间部分比短靴区域（红色）更大。
- en: Secondly, it is not obvious how we should go about choosing a *random* point
    in the latent space, since the distribution of these points is undefined. Technically,
    we would be justified in choosing any point in the 2D plane! It’s not even guaranteed
    that points will be centered around (0, 0). This makes sampling from our latent
    space problematic.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们应该如何选择潜在空间中的*随机*点并不明显，因为这些点的分布是未定义的。从技术上讲，我们可以选择2D平面上的任意点！甚至不能保证点会围绕(0,
    0)中心。这使得从我们的潜在空间中抽样成为问题。
- en: Lastly, we can see holes in the latent space where none of the original images
    are encoded. For example, there are large white spaces at the edges of the domain—the
    autoencoder has no reason to ensure that points here are decoded to recognizable
    clothing items as very few images in the training set are encoded here.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以看到潜在空间中存在一些空洞，原始图像没有被编码。例如，在域的边缘有大片白色空间——自动编码器没有理由确保这些点被解码为可识别的服装项目，因为训练集中很少有图像被编码到这里。
- en: Even points that are central may not be decoded into well-formed images. This
    is because the autoencoder is not forced to ensure that the space is continuous.
    For example, even though the point (–1, –1) might be decoded to give a satisfactory
    image of a sandal, there is no mechanism in place to ensure that the point (–1.1,
    –1.1) also produces a satisfactory image of a sandal.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 即使中心点也可能无法解码为形式良好的图像。这是因为自动编码器没有被迫确保空间是连续的。例如，即使点（-1，-1）可能被解码为一个令人满意的凉鞋图像，但没有机制来确保点（-1.1，-1.1）也产生一个令人满意的凉鞋图像。
- en: In two dimensions this issue is subtle; the autoencoder only has a small number
    of dimensions to work with, so naturally it has to squash clothing groups together,
    resulting in the space between clothing groups being relatively small. However,
    as we start to use more dimensions in the latent space to generate more complex
    images such as faces, this problem becomes even more apparent. If we give the
    autoencoder free rein over how it uses the latent space to encode images, there
    will be huge gaps between groups of similar points with no incentive for the spaces
    in between to generate well-formed images.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在二维中，这个问题是微妙的；自动编码器只有少量维度可用，因此自然地必须将服装组合在一起，导致服装组之间的空间相对较小。然而，当我们开始在潜在空间中使用更多维度来生成更复杂的图像，如面孔时，这个问题变得更加明显。如果我们让自动编码器自由使用潜在空间来编码图像，那么相似点之间将会有巨大的间隙，而没有动机使之间的空间生成形式良好的图像。
- en: In order to solve these three problems, we need to convert our autoencoder into
    a *variational autoencoder*.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这三个问题，我们需要将我们的自动编码器转换为*变分自动编码器*。
- en: Variational Autoencoders
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变分自动编码器
- en: To explain, let’s revisit the infinite wardrobe and make a few changes…​
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释，让我们重新审视无限衣柜并做一些改变...​
- en: Let’s now try to understand what we need to do to our autoencoder model to convert
    it into a variational autoencoder and thus make it a more sophisticated generative
    model.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试理解如何将我们的自动编码器模型转换为变分自动编码器，从而使其成为一个更复杂的生成模型。
- en: The two parts that we need to change are the encoder and the loss function.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要更改的两个部分是编码器和损失函数。
- en: The Encoder
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码器
- en: In an autoencoder, each image is mapped directly to one point in the latent
    space. In a variational autoencoder, each image is instead mapped to a multivariate
    normal distribution around a point in the latent space, as shown in [Figure 3-10](#vae_encoder_normal).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动编码器中，每个图像直接映射到潜在空间中的一个点。在变分自动编码器中，每个图像实际上被映射到潜在空间中某一点周围的多变量正态分布，如[图3-10](#vae_encoder_normal)所示。
- en: '![](Images/gdl2_0310.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0310.png)'
- en: Figure 3-10\. The difference between the encoders in an autoencoder and a variational
    autoencoder
  id: totrans-169
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-10。自动编码器和变分自动编码器中编码器的区别
- en: The encoder only needs to map each input to a mean vector and a variance vector
    and does not need to worry about covariance between dimensions. Variational autoencoders
    assume that there is no correlation between dimensions in the latent space.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器只需要将每个输入映射到一个均值向量和一个方差向量，不需要担心潜在空间维度之间的协方差。变分自动编码器假设潜在空间中的维度之间没有相关性。
- en: Variance values are always positive, so we actually choose to map to the *logarithm*
    of the variance, as this can take any real number in the range ( <math alttext="negative
    normal infinity"><mrow><mo>-</mo> <mi>∞</mi></mrow></math> , <math alttext="normal
    infinity"><mi>∞</mi></math> ). This way we can use a neural network as the encoder
    to perform the mapping from the input image to the mean and log variance vectors.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 方差值始终为正，因此我们实际上选择将其映射到*方差的对数*，因为这可以取任何实数范围内的值（ <math alttext="负无穷"><mrow><mo>-</mo>
    <mi>∞</mi></mrow></math> , <math alttext="正无穷"><mi>∞</mi></math> ）。这样我们可以使用神经网络作为编码器，将输入图像映射到均值和对数方差向量。
- en: 'To summarize, the encoder will take each input image and encode it to two vectors
    that together define a multivariate normal distribution in the latent space:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，编码器将每个输入图像编码为两个向量，这两个向量一起定义了潜在空间中的多变量正态分布：
- en: '`z_mean`'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`z_mean`'
- en: The mean point of the distribution
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 分布的均值点
- en: '`z_log_var`'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`z_log_var`'
- en: The logarithm of the variance of each dimension
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 每个维度的方差的对数
- en: 'We can sample a point `z` from the distribution defined by these values using
    the following equation:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下方程从这些值定义的分布中采样一个点`z`：
- en: '[PRE10]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'where:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '[PRE11]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Tip
  id: totrans-181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'The derivation of the relationship between `z_sigma` ( <math alttext="sigma"><mi>σ</mi></math>
    ) and `z_log_var` ( <math alttext="log left-parenthesis sigma squared right-parenthesis"><mrow><mo
    form="prefix">log</mo> <mo>(</mo> <msup><mi>σ</mi> <mn>2</mn></msup> <mo>)</mo></mrow></math>
    ) is as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`z_sigma`（ <math alttext="sigma"><mi>σ</mi></math> ）和`z_log_var`（ <math alttext="对数左括号sigma平方右括号"><mrow><mo
    form="prefix">log</mo> <mo>(</mo> <msup><mi>σ</mi> <mn>2</mn></msup> <mo>)</mo></mrow></math>
    ）之间的关系推导如下：'
- en: <math alttext="sigma equals exp left-parenthesis log left-parenthesis sigma
    right-parenthesis right-parenthesis equals exp left-parenthesis 2 log left-parenthesis
    sigma right-parenthesis slash 2 right-parenthesis equals exp left-parenthesis
    log left-parenthesis sigma squared right-parenthesis slash 2 right-parenthesis"
    display="block"><mrow><mi>σ</mi> <mo>=</mo> <mo form="prefix">exp</mo> <mrow><mo>(</mo>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>σ</mi> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>=</mo> <mo form="prefix">exp</mo> <mrow><mo>(</mo> <mn>2</mn> <mo form="prefix">log</mo>
    <mrow><mo>(</mo> <mi>σ</mi> <mo>)</mo></mrow> <mo>/</mo> <mn>2</mn> <mo>)</mo></mrow>
    <mo>=</mo> <mo form="prefix">exp</mo> <mrow><mo>(</mo> <mo form="prefix">log</mo>
    <mrow><mo>(</mo> <msup><mi>σ</mi> <mn>2</mn></msup> <mo>)</mo></mrow> <mo>/</mo>
    <mn>2</mn> <mo>)</mo></mrow></mrow></math>
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="sigma equals exp left-parenthesis log left-parenthesis sigma
    right-parenthesis right-parenthesis equals exp left-parenthesis 2 log left-parenthesis
    sigma right-parenthesis slash 2 right-parenthesis equals exp left-parenthesis
    log left-parenthesis sigma squared right-parenthesis slash 2 right-parenthesis"
    display="block"><mrow><mi>σ</mi> <mo>=</mo> <mo form="prefix">exp</mo> <mrow><mo>(</mo>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>σ</mi> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>=</mo> <mo form="prefix">exp</mo> <mrow><mo>(</mo> <mn>2</mn> <mo form="prefix">log</mo>
    <mrow><mo>(</mo> <mi>σ</mi> <mo>)</mo></mrow> <mo>/</mo> <mn>2</mn> <mo>)</mo></mrow>
    <mo>=</mo> <mo form="prefix">exp</mo> <mrow><mo>(</mo> <mo form="prefix">log</mo>
    <mrow><mo>(</mo> <msup><mi>σ</mi> <mn>2</mn></msup> <mo>)</mo></mrow> <mo>/</mo>
    <mn>2</mn> <mo>)</mo></mrow></mrow></math>
- en: The decoder of a variational autoencoder is identical to the decoder of a plain
    autoencoder, giving the overall architecture shown in [Figure 3-12](#vae_architecture).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自动编码器的解码器与普通自动编码器的解码器相同，给出了[图3-12](#vae_architecture)所示的整体架构。
- en: '![](Images/gdl2_0312.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0312.png)'
- en: Figure 3-12\. VAE architecture diagram
  id: totrans-186
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-12。VAE架构图
- en: Why does this small change to the encoder help?
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么对编码器进行这种小改变有帮助？
- en: Previously, we saw that there was no requirement for the latent space to be
    continuous—even if the point (–2, 2) decodes to a well-formed image of a sandal,
    there’s no requirement for (–2.1, 2.1) to look similar. Now, since we are sampling
    a random point from an area around `z_mean`, the decoder must ensure that all
    points in the same neighborhood produce very similar images when decoded, so that
    the reconstruction loss remains small. This is a very nice property that ensures
    that even when we choose a point in the latent space that has never been seen
    by the decoder, it is likely to decode to an image that is well formed.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 先前，我们看到潜在空间不需要连续——即使点（-2, 2）解码为一个良好形成的凉鞋图像，也没有要求（-2.1, 2.1）看起来相似。现在，由于我们从`z_mean`周围的区域对随机点进行采样，解码器必须确保同一邻域中的所有点在解码时产生非常相似的图像，以便重构损失保持较小。这是一个非常好的特性，确保即使我们选择一个解码器从未见过的潜在空间中的点，它也可能解码为一个良好形成的图像。
- en: Building the VAE encoder
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建VAE编码器
- en: Let’s now see how we build this new version of the encoder in Keras.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何在Keras中构建这个编码器的新版本。
- en: Running the Code for This Example
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行此示例的代码
- en: The code for this example can be found in the Jupyter notebook located at *notebooks/03_vae/02_vae_fashion/vae_fashion.ipynb*
    in the book repository.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例的代码可以在位于书籍存储库中的*notebooks/03_vae/02_vae_fashion/vae_fashion.ipynb*的Jupyter笔记本中找到。
- en: The code has been adapted from the excellent [VAE tutorial](https://oreil.ly/A7yqJ)
    created by Francois Chollet, available on the Keras website.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码已经改编自由Francois Chollet创建的优秀[VAE教程](https://oreil.ly/A7yqJ)，可在Keras网站上找到。
- en: First, we need to create a new type of `Sampling` layer that will allow us to
    sample from the distribution defined by `z_mean` and `z_log_var`, as shown in
    [Example 3-11](#variational-autoencoder-sampling-ex).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要创建一个新类型的`Sampling`层，这将允许我们从由`z_mean`和`z_log_var`定义的分布中进行采样，如[示例3-11](#variational-autoencoder-sampling-ex)所示。
- en: Example 3-11\. The `Sampling` layer
  id: totrans-195
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-11. `Sampling`层
- en: '[PRE12]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](Images/1.png)](#co_variational_autoencoders_CO4-1)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_variational_autoencoders_CO4-1)'
- en: We create a new layer by subclassing the Keras base `Layer` class (see the “Subclassing
    the Layer Class” sidebar).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过对Keras基础`Layer`类进行子类化来创建一个新层（请参阅“子类化Layer类”侧边栏）。
- en: '[![2](Images/2.png)](#co_variational_autoencoders_CO4-2)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_variational_autoencoders_CO4-2)'
- en: We use the reparameterization trick (see “The Reparameterization Trick” sidebar)
    to build a sample from the normal distribution parameterized by `z_mean` and `z_log_var`.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用重参数化技巧（请参阅“重参数化技巧”侧边栏）来构建由`z_mean`和`z_log_var`参数化的正态分布的样本。
- en: The complete code for the encoder, including the new `Sampling` layer, is shown
    in [Example 3-12](#variational-autoencoder-encoder-ex).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 包括新的`Sampling`层在内的编码器的完整代码显示在[示例3-12](#variational-autoencoder-encoder-ex)中。
- en: Example 3-12\. The encoder
  id: totrans-202
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-12. 编码器
- en: '[PRE13]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[![1](Images/1.png)](#co_variational_autoencoders_CO5-1)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_variational_autoencoders_CO5-1)'
- en: Instead of connecting the `Flatten` layer directly to the 2D latent space, we
    connect it to layers `z_mean` and `z_log_var`.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`Flatten`层直接连接到2D潜在空间，而不是直接连接到`z_mean`和`z_log_var`层。
- en: '[![2](Images/2.png)](#co_variational_autoencoders_CO5-2)'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_variational_autoencoders_CO5-2)'
- en: The `Sampling` layer samples a point `z` in the latent space from the normal
    distribution defined by the parameters `z_mean` and `z_log_var`.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '`Sampling`层从由参数`z_mean`和`z_log_var`定义的正态分布中对潜在空间中的点`z`进行采样。'
- en: '[![3](Images/3.png)](#co_variational_autoencoders_CO5-3)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_variational_autoencoders_CO5-3)'
- en: The Keras `Model` that defines the encoder—a model that takes an input image
    and outputs `z_mean`, `z_log_var`, and a sampled point `z` from the normal distribution
    defined by these parameters.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 定义编码器的Keras `Model`——一个接受输入图像并输出`z_mean`、`z_log_var`和由这些参数定义的正态分布中的采样点`z`的模型。
- en: A summary of the encoder is shown in [Table 3-4](#vae_clothing_encoder).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器的摘要显示在[表3-4](#vae_clothing_encoder)中。
- en: Table 3-4\. Model summary of the VAE encoder
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 表3-4. VAE编码器的模型摘要
- en: '| Layer (type) | Output shape | Param # | Connected to |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| Layer (type) | 输出形状 | 参数 # | 连接到 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| InputLayer (input) | (None, 32, 32, 1) | 0 | [] |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| InputLayer (input) | (None, 32, 32, 1) | 0 | [] |'
- en: '| Conv2D (conv2d_1) | (None, 16, 16, 32) | 320 | [input] |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| Conv2D (conv2d_1) | (None, 16, 16, 32) | 320 | [input] |'
- en: '| Conv2D (conv2d_2) | (None, 8, 8, 64) | 18,496 | [conv2d_1] |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| Conv2D (conv2d_2) | (None, 8, 8, 64) | 18,496 | [conv2d_1] |'
- en: '| Conv2D (conv2d_3) | (None, 4, 4, 128) | 73,856 | [conv2d_2] |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| Conv2D (conv2d_3) | (None, 4, 4, 128) | 73,856 | [conv2d_2] |'
- en: '| Flatten (flatten) | (None, 2048) | 0 | [conv2d_3] |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| Flatten (flatten) | (None, 2048) | 0 | [conv2d_3] |'
- en: '| Dense (z_mean) | (None, 2) | 4,098 | [flatten] |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| Dense (z_mean) | (None, 2) | 4,098 | [flatten] |'
- en: '| Dense (z_log_var) | (None, 2) | 4,098 | [flatten] |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| Dense (z_log_var) | (None, 2) | 4,098 | [flatten] |'
- en: '| Sampling (z) | (None, 2) | 0 | [z_mean, z_log_var] |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| Sampling (z) | (None, 2) | 0 | [z_mean, z_log_var] |'
- en: '| Total params | 100,868 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 总参数 | 100,868 |'
- en: '| Trainable params | 100,868 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 可训练参数 | 100,868 |'
- en: '| Non-trainable params | 0 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 不可训练参数 | 0 |'
- en: The only other part of the original autoencoder that we need to change is the
    loss function.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要更改的原始自动编码器的唯一其他部分是损失函数。
- en: The Loss Function
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数
- en: 'Previously, our loss function only consisted of the *reconstruction loss* between
    images and their attempted copies after being passed through the encoder and decoder.
    The reconstruction loss also appears in a variational autoencoder, but we now
    require one extra component: the *Kullback–Leibler (KL) divergence* term.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 先前，我们的损失函数仅包括图像与通过编码器和解码器传递后的尝试副本之间的*重构损失*。重构损失也出现在变分自动编码器中，但现在我们需要一个额外的组件：*Kullback-Leibler（KL）散度*项。
- en: 'KL divergence is a way of measuring how much one probability distribution differs
    from another. In a VAE, we want to measure how much our normal distribution with
    parameters `z_mean` and `z_log_var` differs from a standard normal distribution.
    In this special case, it can be shown that the KL divergence has the following
    closed form:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: KL散度是衡量一个概率分布与另一个之间差异的一种方式。在VAE中，我们想要衡量我们的具有参数`z_mean`和`z_log_var`的正态分布与标准正态分布之间的差异。在这种特殊情况下，可以证明KL散度具有以下封闭形式：
- en: '[PRE14]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'or in mathematical notation:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 或者用数学符号表示：
- en: <math alttext="upper D Subscript upper K upper L Baseline left-bracket upper
    N left-parenthesis mu comma sigma parallel-to upper N left-parenthesis 0 comma
    1 right-parenthesis right-bracket equals minus one-half sigma-summation left-parenthesis
    1 plus l o g left-parenthesis sigma squared right-parenthesis minus mu squared
    minus sigma squared right-parenthesis" display="block"><mstyle scriptlevel="0"
    displaystyle="true"><mrow><msub><mi>D</mi> <mrow><mi>K</mi><mi>L</mi></mrow></msub>
    <mrow><mo>[</mo> <mi>N</mi> <mrow><mo>(</mo> <mi>μ</mi> <mo>,</mo> <mi>σ</mi>
    <mo>∥</mo> <mi>N</mi> <mrow><mo>(</mo> <mn>0</mn> <mo>,</mo> <mn>1</mn> <mo>)</mo></mrow>
    <mo>]</mo></mrow> <mo>=</mo> <mo>-</mo></mrow> <mfrac><mn>1</mn> <mn>2</mn></mfrac>
    <mo>∑</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>+</mo> <mi>l</mi> <mi>o</mi> <mi>g</mi>
    <mrow><mo>(</mo> <msup><mi>σ</mi> <mn>2</mn></msup> <mo>)</mo></mrow> <mo>-</mo>
    <msup><mi>μ</mi> <mn>2</mn></msup> <mo>-</mo> <msup><mi>σ</mi> <mn>2</mn></msup>
    <mo>)</mo></mrow></mrow></mstyle></math>
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper D Subscript upper K upper L Baseline left-bracket upper
    N left-parenthesis mu comma sigma parallel-to upper N left-parenthesis 0 comma
    1 right-parenthesis right-bracket equals minus one-half sigma-summation left-parenthesis
    1 plus l o g left-parenthesis sigma squared right-parenthesis minus mu squared
    minus sigma squared right-parenthesis" display="block"><mstyle scriptlevel="0"
    displaystyle="true"><mrow><msub><mi>D</mi> <mrow><mi>K</mi><mi>L</mi></mrow></msub>
    <mrow><mo>[</mo> <mi>N</mi> <mrow><mo>(</mo> <mi>μ</mi> <mo>,</mo> <mi>σ</mi>
    <mo>∥</mo> <mi>N</mi> <mrow><mo>(</mo> <mn>0</mn> <mo>,</mo> <mn>1</mn> <mo>)</mo></mrow>
    <mo>]</mo></mrow> <mo>=</mo> <mo>-</mo></mrow> <mfrac><mn>1</mn> <mn>2</mn></mfrac>
    <mo>∑</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>+</mo> <mi>l</mi> <mi>o</mi> <mi>g</mi>
    <mrow><mo>(</mo> <msup><mi>σ</mi> <mn>2</mn></msup> <mo>)</mo></mrow> <mo>-</mo>
    <msup><mi>μ</mi> <mn>2</mn></msup> <mo>-</mo> <msup><mi>σ</mi> <mn>2</mn></msup>
    <mo>)</mo></mrow></mrow></mstyle></math>
- en: The sum is taken over all the dimensions in the latent space. `kl_loss` is minimized
    to 0 when `z_mean = 0` and `z_log_var = 0` for all dimensions. As these two terms
    start to differ from 0, `kl_loss` increases.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 总和取自潜在空间中的所有维度。当所有维度上的`z_mean = 0`和`z_log_var = 0`时，`kl_loss`被最小化为0。当这两个项开始与0不同，`kl_loss`增加。
- en: In summary, the KL divergence term penalizes the network for encoding observations
    to `z_mean` and `z_log_var` variables that differ significantly from the parameters
    of a standard normal distribution, namely `z_mean = 0` and `z_log_var = 0`.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，KL散度项惩罚网络将观测编码为与标准正态分布的参数明显不同的`z_mean`和`z_log_var`变量，即`z_mean = 0`和`z_log_var
    = 0`。
- en: Why does this addition to the loss function help?
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么将这个损失函数的添加有助于什么？
- en: Firstly, we now have a well-defined distribution that we can use for choosing
    points in the latent space—the standard normal distribution. Secondly, since this
    term tries to force all encoded distributions toward the standard normal distribution,
    there is less chance that large gaps will form between point clusters. Instead,
    the encoder will try to use the space around the origin symmetrically and efficiently.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们现在有一个明确定义的分布，可以用于选择潜在空间中的点——标准正态分布。其次，由于这个项试图将所有编码分布推向标准正态分布，因此大型间隙形成的机会较小。相反，编码器将尝试对称且高效地使用原点周围的空间。
- en: In the original VAE paper, the loss function for a VAE was simply the addition
    of the reconstruction loss and the KL divergence loss term. A variant on this
    (the <math alttext="beta"><mi>β</mi></math> -VAE) includes a factor that weights
    the KL divergence to ensure that it is well balanced with the reconstruction loss.
    If we weight the reconstruction loss too heavily, the KL loss will not have the
    desired regulatory effect and we will see the same problems that we experienced
    with the plain autoencoder. If the KL divergence term is weighted too heavily,
    the KL divergence loss will dominate and the reconstructed images will be poor.
    This weighting term is one of the parameters to tune when you’re training your
    VAE.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始VAE论文中，VAE的损失函数简单地是重建损失和KL散度损失项的加法。这个变体（β-VAE）包括一个因子，用于对KL散度进行加权，以确保它与重建损失平衡良好。如果我们过分权重重建损失，KL损失将不会产生所需的调节效果，我们将看到与普通自动编码器相同的问题。如果KL散度项权重过大，KL散度损失将占主导地位，重建图像将很差。在训练VAE时，这个权重项是需要调整的参数之一。
- en: Training the Variational Autoencoder
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练变分自动编码器
- en: '[Example 3-13](#kl-divergence-ex) shows how we build the overall VAE model
    as a subclass of the abstract Keras `Model` class. This allows us to include the
    calculation of the KL divergence term of the loss function in a custom `train_step`
    method.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例3-13](#kl-divergence-ex)展示了我们如何将整体VAE模型构建为抽象Keras `Model`类的子类。这使我们能够在自定义的`train_step`方法中包含损失函数的KL散度项的计算。'
- en: Example 3-13\. Training the VAE
  id: totrans-239
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-13。训练VAE
- en: '[PRE15]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[![1](Images/1.png)](#co_variational_autoencoders_CO6-1)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_variational_autoencoders_CO6-1)'
- en: This function describes what we would like returned what we call the VAE on
    a particular input image.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数描述了我们希望在特定输入图像上返回的内容，我们称之为VAE。
- en: '[![2](Images/2.png)](#co_variational_autoencoders_CO6-2)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_variational_autoencoders_CO6-2)'
- en: This function describes one training step of the VAE, including the calculation
    of the loss function.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数描述了VAE的一个训练步骤，包括损失函数的计算。
- en: '[![3](Images/3.png)](#co_variational_autoencoders_CO6-3)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_variational_autoencoders_CO6-3)'
- en: A beta value of 500 is used in the reconstruction loss.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 重建损失中使用了一个beta值为500。
- en: '[![4](Images/4.png)](#co_variational_autoencoders_CO6-4)'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_variational_autoencoders_CO6-4)'
- en: The total loss is the sum of the reconstruction loss and the KL divergence loss.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 总损失是重建损失和KL散度损失的总和。
- en: Gradient Tape
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度带
- en: TensorFlow’s *Gradient Tape* is a mechanism that allows the computation of gradients
    of operations executed during a forward pass of a model. To use it, you need to
    wrap the code that performs the operations you want to differentiate in a `tf.GradientTape()`
    context. Once you have recorded the operations, you can compute the gradient of
    the loss function with respect to some variables by calling `tape.gradient()`.
    The gradients can then be used to update the variables with the optimizer.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow的*梯度带*是一种机制，允许在模型前向传递期间计算操作的梯度。要使用它，您需要将执行您想要区分的操作的代码包装在`tf.GradientTape()`上下文中。一旦记录了操作，您可以通过调用`tape.gradient()`计算损失函数相对于某些变量的梯度。然后可以使用这些梯度来更新变量与优化器。
- en: This mechanism is useful for calculating the gradient of custom loss functions
    (as we have done here) and also for creating custom training loops, as we shall
    see in [Chapter 4](ch04.xhtml#chapter_gan).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这个机制对于计算自定义损失函数的梯度（就像我们在这里所做的那样）以及创建自定义训练循环非常有用，正如我们将在[第4章](ch04.xhtml#chapter_gan)中看到的。
- en: Analysis of the Variational Autoencoder
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变分自动编码器的分析
- en: Now that we have trained our VAE, we can use the encoder to encode the images
    in the test set and plot the `z_mean` values in the latent space. We can also
    sample from a standard normal distribution to generate points in the latent space
    and use the decoder to decode these points back into pixel space to see how the
    VAE performs.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练了我们的VAE，我们可以使用编码器对测试集中的图像进行编码，并在潜在空间中绘制`z_mean`值。我们还可以从标准正态分布中进行采样，生成潜在空间中的点，并使用解码器将这些点解码回像素空间，以查看VAE的性能如何。
- en: '[Figure 3-13](#vae_wardrobe) shows the structure of the new latent space, alongside
    some sampled points and their decoded images. We can immediately see several changes
    in how the latent space is organized.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-13](#vae_wardrobe)展示了新潜在空间的结构，以及一些采样点和它们的解码图像。我们可以立即看到潜在空间的组织方式发生了几处变化。'
- en: '![](Images/gdl2_0313.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0313.png)'
- en: 'Figure 3-13\. The new latent space: the black dots show the `z_mean` value
    of each encoded image, while blue dots show some sampled points in the latent
    space (with their decoded images on the right)'
  id: totrans-256
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-13。新的潜在空间：黑点显示每个编码图像的`z_mean`值，而蓝点显示潜在空间中的一些采样点（其解码图像显示在右侧）
- en: Firstly, the KL divergence loss term ensures that the `z_mean` and `z_log_var`
    values of the encoded images never stray too far from a standard normal distribution.
    Secondly, there are not so many poorly formed images as the latent space is now
    much more continuous, due to fact that the encoder is now stochastic, rather than
    deterministic.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，KL散度损失项确保编码图像的`z_mean`和`z_log_var`值永远不会偏离标准正态分布太远。其次，由于编码器现在是随机的而不是确定性的，因此现在的潜在空间更加连续，因此没有那么多形状不佳的图像。
- en: Finally, by coloring points in the latent space by clothing type ([Figure 3-14](#vae_wardrobe_colour)),
    we can see that there is no preferential treatment of any one type. The righthand
    plot shows the space transformed into *p*-values—we can see that each color is
    approximately equally represented. Again, it’s important to remember that the
    labels were not used at all during training; the VAE has learned the various forms
    of clothing by itself in order to help minimize reconstruction loss.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过按服装类型对潜在空间中的点进行着色（[图3-14](#vae_wardrobe_colour)），我们可以看到没有任何一种类型受到优待。右侧的图显示了空间转换为*p*值——我们可以看到每种颜色大致上都有相同的表示。再次强调，重要的是要记住在训练过程中根本没有使用标签；VAE已经自己学会了各种服装形式，以帮助最小化重构损失。
- en: '![](Images/gdl2_0314.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0314.png)'
- en: Figure 3-14\. The latent space of the VAE colored by clothing type
  id: totrans-260
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-14。VAE的潜在空间按服装类型着色
- en: Exploring the Latent Space
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索潜在空间
- en: So far, all of our work on autoencoders and variational autoencoders has been
    limited to a latent space with two dimensions. This has helped us to visualize
    the inner workings of a VAE on the page and understand why the small tweaks that
    we made to the architecture of the autoencoder helped transform it into a more
    powerful class of network that can be used for generative modeling.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们对自动编码器和变分自动编码器的所有工作都局限于具有两个维度的潜在空间。这帮助我们在页面上可视化VAE的内部工作原理，并理解我们对自动编码器架构所做的小调整是如何将其转变为一种更强大的网络类别，可用于生成建模。
- en: Let’s now turn our attention to a more complex dataset and see the amazing things
    that variational autoencoders can achieve when we increase the dimensionality
    of the latent space.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将注意力转向更复杂的数据集，并看看当我们增加潜在空间的维度时，变分自动编码器可以实现的惊人成就。
- en: Running the Code for This Example
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行此示例的代码
- en: The code for this example can be found in the Jupyter notebook located at *notebooks/03_vae/03_faces/vae_faces.ipynb*
    in the book repository.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例的代码可以在书籍存储库中的Jupyter笔记本中找到，位置为*notebooks/03_vae/03_faces/vae_faces.ipynb*。
- en: The CelebA Dataset
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CelebA数据集
- en: We shall be using the [CelebFaces Attributes (CelebA) dataset](https://oreil.ly/tEUnh)
    to train our next variational autoencoder. This is a collection of over 200,000
    color images of celebrity faces, each annotated with various labels (e.g., *wearing
    hat*, *smiling*, etc.). A few examples are shown in [Figure 3-15](#celeba_sample).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用[CelebFaces Attributes (CelebA)数据集](https://oreil.ly/tEUnh)来训练我们的下一个变分自动编码器。这是一个包含超过200,000张名人面孔彩色图像的集合，每张图像都带有各种标签（例如，*戴帽子*，*微笑*等）。一些示例显示在[图3-15](#celeba_sample)中。
- en: '![](Images/gdl2_0315.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0315.png)'
- en: 'Figure 3-15\. Some examples from the CelebA dataset (source: [Liu et al., 2015](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html))^([3](ch03.xhtml#idm45387022470176))'
  id: totrans-269
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-15。CelebA数据集的一些示例（来源：[Liu等，2015](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html))^([3](ch03.xhtml#idm45387022470176))
- en: Of course, we don’t need the labels to train the VAE, but these will be useful
    later when we start exploring how these features are captured in the multidimensional
    latent space. Once our VAE is trained, we can sample from the latent space to
    generate new examples of celebrity faces.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始探索这些特征如何在多维潜在空间中被捕获时，我们当然不需要标签来训练VAE，但这些标签以后会很有用。一旦我们的VAE训练完成，我们就可以从潜在空间中进行采样，生成名人面孔的新示例。
- en: The CelebA dataset is also available through Kaggle, so you can download the
    dataset by running the Kaggle dataset downloader script in the book repository,
    as shown in [Example 3-14](#downloading-celeba-dataset). This will save the images
    and accompanying metadata locally to the */data* folder.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: CelebA数据集也可以通过Kaggle获得，因此您可以通过在书籍存储库中运行Kaggle数据集下载脚本来下载数据集，如[示例3-14](#downloading-celeba-dataset)所示。这将把图像和相关元数据保存到*/data*文件夹中。
- en: Example 3-14\. Downloading the CelebA dataset
  id: totrans-272
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-14。下载CelebA数据集
- en: '[PRE16]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '`We use the Keras function `image_dataset_from_directory` to create a TensorFlow
    Dataset pointed at the directory where the images are stored, as shown in [Example 3-15](#preprocessing-face-data).
    This allows us to read batches of images into memory only when required (e.g.,
    during training), so that we can work with large datasets and not worry about
    having to fit the entire dataset into memory. It also resizes the images to 64
    × 64, interpolating between pixel values.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Keras函数`image_dataset_from_directory`来创建一个指向存储图像的目录的TensorFlow数据集，如[示例3-15](#preprocessing-face-data)所示。这使我们能够在需要时（例如在训练期间）将图像批量读入内存，以便我们可以处理大型数据集，而不必担心将整个数据集装入内存。它还将图像调整大小为64×64，在像素值之间进行插值。
- en: Example 3-15\. Preprocessing the CelebA dataset
  id: totrans-275
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-15。处理CelebA数据集
- en: '[PRE17]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The original data is scaled in the range [0, 255] to denote the pixel intensity,
    which we rescale to the range [0, 1] as shown in [Example 3-16](#preprocessing-celeba-data).
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据在范围[0, 255]内进行缩放以表示像素强度，我们将其重新缩放到范围[0, 1]，如[示例3-16](#preprocessing-celeba-data)所示。
- en: Example 3-16\. Preprocessing the CelebA dataset
  id: totrans-278
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-16。处理CelebA数据集
- en: '[PRE18]`  `## Training the Variational Autoencoder'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '## 训练变分自动编码器'
- en: 'The network architecture for the faces model is similar to the Fashion-MNIST
    example, with a few slight differences:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 面部模型的网络架构与Fashion-MNIST示例类似，有一些细微差异：
- en: Our data now has three input channels (RGB) instead of one (grayscale). This
    means we need to change the number of channels in the final convolutional transpose
    layer of the decoder to 3.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的数据现在有三个输入通道（RGB），而不是一个（灰度）。这意味着我们需要将解码器的最后一个卷积转置层中的通道数更改为3。
- en: We shall be using a latent space with 200 dimensions instead of 2\. Since faces
    are much more complex than the Fashion-MNIST images, we increase the dimensionality
    of the latent space so that the network can encode a satisfactory amount of detail
    from the images.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将使用一个具有200维而不是2维的潜在空间。由于面孔比时尚MNIST图像复杂得多，我们增加了潜在空间的维度，以便网络可以从图像中编码出令人满意的细节量。
- en: There are batch normalization layers after each convolutional layer to stabilize
    training. Even though each batch takes a longer time to run, the number of batches
    required to reach the same loss is greatly reduced.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个卷积层后都有批量归一化层以稳定训练。尽管每个批次运行时间较长，但达到相同损失所需的批次数量大大减少。
- en: We increase the <math alttext="beta"><mi>β</mi></math> factor for the KL divergence
    to 2,000\. This is a parameter that requires tuning; for this dataset and architecture
    this value was found to generate good results.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将KL散度的β因子增加到2,000。这是一个需要调整的参数；对于这个数据集和架构，这个值被发现可以产生良好的结果。
- en: The full architectures of the encoder and decoder are shown in Tables [3-5](#vae_faces_encoder)
    and [3-6](#vae_faces_decoder), respectively.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '编码器和解码器的完整架构分别显示在表[3-5](#vae_faces_encoder)和表[3-6](#vae_faces_decoder)中。 '
- en: Table 3-5\. Model summary of the VAE faces encoder
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 表3-5。VAE面部编码器的模型摘要
- en: '| Layer (type) | Output shape | Param # | Connected to |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 层（类型） | 输出形状 | 参数 # | 连接到 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| InputLayer (input) | (None, 32, 32, 3) | 0 | [] |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 输入层（input） | (None, 32, 32, 3) | 0 | [] |'
- en: '| Conv2D (conv2d_1) | (None, 16, 16, 128) | 3,584 | [input] |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 卷积层（conv2d_1） | (None, 16, 16, 128) | 3,584 | [input] |'
- en: '| BatchNormalization (bn_1) | (None, 16, 16, 128) | 512 | [conv2d_1] |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 批量归一化（bn_1） | (None, 16, 16, 128) | 512 | [conv2d_1] |'
- en: '| LeakyReLU (lr_1) | (None, 16, 16, 128) | 0 | [bn_1] |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| LeakyReLU（lr_1） | (None, 16, 16, 128) | 0 | [bn_1] |'
- en: '| Conv2D (conv2d_2) | (None, 8, 8, 128) | 147,584 | [lr_1] |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 卷积层（conv2d_2） | (None, 8, 8, 128) | 147,584 | [lr_1] |'
- en: '| BatchNormalization (bn_2) | (None, 8, 8, 128) | 512 | [conv2d_2] |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 批量归一化（bn_2） | (None, 8, 8, 128) | 512 | [conv2d_2] |'
- en: '| LeakyReLU (lr_2) | (None, 8, 8, 128) | 0 | [bn_2] |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| LeakyReLU（lr_2） | (None, 8, 8, 128) | 0 | [bn_2] |'
- en: '| Conv2D (conv2d_3) | (None, 4, 4, 128) | 147,584 | [lr_2] |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 卷积层（conv2d_3） | (None, 4, 4, 128) | 147,584 | [lr_2] |'
- en: '| BatchNormalization (bn_3) | (None, 4, 4, 128) | 512 | [conv2d_3] |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 批量归一化（bn_3） | (None, 4, 4, 128) | 512 | [conv2d_3] |'
- en: '| LeakyReLU (lr_3) | (None, 4, 4, 128) | 0 | [bn_3] |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| LeakyReLU（lr_3） | (None, 4, 4, 128) | 0 | [bn_3] |'
- en: '| Conv2D (conv2d_4) | (None, 2, 2, 128) | 147,584 | [lr_3] |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 卷积层（conv2d_4） | (None, 2, 2, 128) | 147,584 | [lr_3] |'
- en: '| BatchNormalization (bn_4) | (None, 2, 2, 128) | 512 | [conv2d_4] |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 批量归一化（bn_4） | (None, 2, 2, 128) | 512 | [conv2d_4] |'
- en: '| LeakyReLU (lr_4) | (None, 2, 2, 128) | 0 | [bn_4] |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| LeakyReLU（lr_4） | (None, 2, 2, 128) | 0 | [bn_4] |'
- en: '| Flatten (flatten) | (None, 512) | 0 | [lr_4] |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 展平（flatten） | (None, 512) | 0 | [lr_4] |'
- en: '| Dense (z_mean) | (None, 200) | 102,600 | [flatten] |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 密集层（z_mean） | (None, 200) | 102,600 | [flatten] |'
- en: '| Dense (z_log_var) | (None, 200) | 102,600 | [flatten] |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 密集层（z_log_var） | (None, 200) | 102,600 | [flatten] |'
- en: '| Sampling (z) | (None, 200) | 0 | [z_mean, z_log_var] |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 采样（z） | (None, 200) | 0 | [z_mean, z_log_var] |'
- en: '| Total params | 653,584 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 总参数 | 653,584 |'
- en: '| Trainable params | 652,560 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 可训练参数 | 652,560 |'
- en: '| Non-trainable params | 1,024 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 不可训练参数 | 1,024 |'
- en: Table 3-6\. Model summary of the VAE faces decoder
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 表3-6。VAE面部解码器的模型摘要
- en: '| Layer (type) | Output shape | Param # |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 层（类型） | 输出形状 | 参数 # |'
- en: '| --- | --- | --- |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| InputLayer | (None, 200) | 0 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 输入层 | (None, 200) | 0 |'
- en: '| Dense | (None, 512) | 102,912 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 密集层 | (None, 512) | 102,912 |'
- en: '| BatchNormalization | (None, 512) | 2,048 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 批量归一化 | (None, 512) | 2,048 |'
- en: '| LeakyReLU | (None, 512) | 0 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| LeakyReLU | (None, 512) | 0 |'
- en: '| Reshape | (None, 2, 2, 128) | 0 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 重塑 | (None, 2, 2, 128) | 0 |'
- en: '| Conv2DTranspose | (None, 4, 4, 128) | 147,584 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 转置卷积层 | (None, 4, 4, 128) | 147,584 |'
- en: '| BatchNormalization | (None, 4, 4, 128) | 512 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 批量归一化 | (None, 4, 4, 128) | 512 |'
- en: '| LeakyReLU | (None, 4, 4, 128) | 0 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| LeakyReLU | (None, 4, 4, 128) | 0 |'
- en: '| Conv2DTranspose | (None, 8, 8, 128) | 147,584 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 转置卷积层 | (None, 8, 8, 128) | 147,584 |'
- en: '| BatchNormalization | (None, 8, 8, 128) | 512 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 批量归一化 | (None, 8, 8, 128) | 512 |'
- en: '| LeakyReLU | (None, 8, 8, 128) | 0 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| LeakyReLU | (None, 8, 8, 128) | 0 |'
- en: '| Conv2DTranspose | (None, 16, 16, 128) | 147,584 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 转置卷积层 | (None, 16, 16, 128) | 147,584 |'
- en: '| BatchNormalization | (None, 16, 16, 128) | 512 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 批量归一化 | (None, 16, 16, 128) | 512 |'
- en: '| LeakyReLU | (None, 16, 16, 128) | 0 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| LeakyReLU | (None, 16, 16, 128) | 0 |'
- en: '| Conv2DTranspose | (None, 32, 32, 128) | 147,584 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 转置卷积层 | (None, 32, 32, 128) | 147,584 |'
- en: '| BatchNormalization | (None, 32, 32, 128) | 512 |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 批量归一化 | (None, 32, 32, 128) | 512 |'
- en: '| LeakyReLU | (None, 32, 32, 128) | 0 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| LeakyReLU | (None, 32, 32, 128) | 0 |'
- en: '| Conv2DTranspose | (None, 32, 32, 3) | 3,459 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 转置卷积层 | (None, 32, 32, 3) | 3,459 |'
- en: '| Total params | 700,803 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 总参数 | 700,803 |'
- en: '| Trainable params | 698,755 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| 可训练参数 | 698,755 |'
- en: '| Non-trainable params | 2,048 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| 不可训练参数 | 2,048 |'
- en: After around five epochs of training, our VAE should be able to produce novel
    images of celebrity faces!
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练大约五个时期后，我们的VAE应该能够生成名人面孔的新颖图像！
- en: Analysis of the Variational Autoencoder
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变分自动编码器的分析
- en: First, let’s take a look at a sample of reconstructed faces. The top row in
    [Figure 3-16](#vae_faces_reconstruction) shows the original images and the bottom
    row shows the reconstructions once they have passed through the encoder and decoder.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看一下重建面孔的样本。[图3-16](#vae_faces_reconstruction)中的顶行显示原始图像，底行显示它们通过编码器和解码器后的重建图像。
- en: '![](Images/gdl2_0316.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0316.png)'
- en: Figure 3-16\. Reconstructed faces, after passing through the encoder and decoder
  id: totrans-337
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-16。通过编码器和解码器传递后重建的面孔
- en: We can see that the VAE has successfully captured the key features of each face—the
    angle of the head, the hairstyle, the expression, etc. Some of the fine detail
    is missing, but it is important to remember that the aim of building variational
    autoencoders isn’t to achieve perfect reconstruction loss. Our end goal is to
    sample from the latent space in order to generate new faces.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: For this to be possible we must check that the distribution of points in the
    latent space approximately resembles a multivariate standard normal distribution.
    If we see any dimensions that are significantly different from a standard normal
    distribution, we should probably reduce the reconstruction loss factor, since
    the KL divergence term isn’t having enough effect.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: The first 50 dimensions in our latent space are shown in [Figure 3-17](#vae_faces_normal_distributions).
    There aren’t any distributions that stand out as being significantly different
    from the standard normal, so we can move on to generating some faces!
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0317.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
- en: Figure 3-17\. Distributions of points for the first 50 dimensions in the latent
    space
  id: totrans-342
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Generating New Faces
  id: totrans-343
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To generate new faces, we can use the code in [Example 3-17](#new-faces-latent-space-ex).
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-17\. Generating new faces from the latent space
  id: totrans-345
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[![1](Images/1.png)](#co_variational_autoencoders_CO7-1)'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: Sample 30 points from a standard multivariate normal distribution with 200 dimensions.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_variational_autoencoders_CO7-2)'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: Decode the sampled points.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_variational_autoencoders_CO7-3)'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: Plot the images!
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: The output is shown in [Figure 3-18](#vae_faces_generated).
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0318.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
- en: Figure 3-18\. New generated faces
  id: totrans-355
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Amazingly, the VAE is able to take the set of points that we sampled from a
    standard normal distribution and convert each into a convincing image of a person’s
    face. This is our first glimpse of the true power of generative models!
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s see if we can start to use the latent space to perform some interesting
    operations on generated images.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: Latent Space Arithmetic
  id: totrans-358
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One benefit of mapping images into a lower-dimensional latent space is that
    we can perform arithmetic on vectors in this latent space that has a visual analogue
    when decoded back into the original image domain.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose we want to take an image of somebody who looks sad and
    give them a smile. To do this we first need to find a vector in the latent space
    that points in the direction of increasing smile. Adding this vector to the encoding
    of the original image in the latent space will give us a new point which, when
    decoded, should give us a more smiley version of the original image.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: So how can we find the *smile* vector? Each image in the CelebA dataset is labeled
    with attributes, one of which is `Smiling`. If we take the average position of
    encoded images in the latent space with the attribute `Smiling` and subtract the
    average position of encoded images that do not have the attribute `Smiling`, we
    will obtain the vector that points in the direction of `Smiling`, which is exactly
    what we need.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: 'Conceptually, we are performing the following vector arithmetic in the latent
    space, where `alpha` is a factor that determines how much of the feature vector
    is added or subtracted:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Let’s see this in action. [Figure 3-19](#vae_adding_features) shows several
    images that have been encoded into the latent space. We then add or subtract multiples
    of a certain vector (e.g., `Smiling`, `Black_Hair`, `Eyeglasses`, `Young`, `Male`,
    `Blond_Hair`) to obtain different versions of the image, with only the relevant
    feature changed.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0319.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
- en: Figure 3-19\. Adding and subtracting features to and from faces
  id: totrans-366
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is remarkable that even though we are moving the point a significantly large
    distance in the latent space, the core image remains approximately the same, except
    for the one feature that we want to manipulate. This demonstrates the power of
    variational autoencoders for capturing and adjusting high-level features in images.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊奇的是，即使我们在潜在空间中移动点的距离相当大，核心图像仍然大致相同，除了我们想要操作的一个特征。这展示了变分自动编码器在捕捉和调整图像中的高级特征方面的能力。
- en: Morphing Between Faces
  id: totrans-368
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在面孔之间变形
- en: We can use a similar idea to morph between two faces. Imagine two points in
    the latent space, A and B, that represent two images. If you started at point
    A and walked toward point B in a straight line, decoding each point on the line
    as you went, you would see a gradual transition from the starting face to the
    end face.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用类似的想法在两个面孔之间变形。想象一下潜在空间中表示两个图像的两个点A和B。如果您从点A开始沿直线走向点B，解码沿途的每个点，您将看到从起始面孔到结束面孔的逐渐过渡。
- en: 'Mathematically, we are traversing a straight line, which can be described by
    the following equation:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上，我们正在遍历一条直线，可以用以下方程描述：
- en: '[PRE21]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Here, `alpha` is a number between 0 and 1 that determines how far along the
    line we are, away from point A.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`alpha`是一个介于0和1之间的数字，确定我们离点A有多远。
- en: '[Figure 3-20](#vae_morphing_faces) shows this process in action. We take two
    images, encode them into the latent space, and then decode points along the straight
    line between them at regular intervals.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-20](#vae_morphing_faces)展示了这一过程的实际操作。我们取两个图像，将它们编码到潜在空间中，然后在它们之间的直线上以固定间隔解码点。'
- en: '![](Images/gdl2_0320.png)'
  id: totrans-374
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gdl2_0320.png)'
- en: Figure 3-20\. Morphing between two faces
  id: totrans-375
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-20。在两个面孔之间变形
- en: It is worth noting the smoothness of the transition—even where there are multiple
    features to change simultaneously (e.g., removal of glasses, hair color, gender),
    the VAE manages to achieve this fluidly, showing that the latent space of the
    VAE is truly a continuous space that can be traversed and explored to generate
    a multitude of different human faces.`  `# Summary
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是过渡的平滑性——即使同时有多个要同时更改的特征（例如，去除眼镜、头发颜色、性别），VAE也能够流畅地实现这一点，显示出VAE的潜在空间确实是一个可以遍历和探索以生成多种不同人脸的连续空间。`  `#
    摘要
- en: In this chapter we have seen how variational autoencoders are a powerful tool
    in the generative modeling toolbox. We started by exploring how plain autoencoders
    can be used to map high-dimensional images into a low-dimensional latent space,
    so that high-level features can be extracted from the individually uninformative
    pixels. However, we quickly found that there were some drawbacks to using plain
    autoencoders as a generative model—sampling from the learned latent space was
    problematic, for example.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到变分自动编码器是生成建模工具箱中的一个强大工具。我们首先探讨了如何使用普通自动编码器将高维图像映射到低维潜在空间，以便从单独无信息的像素中提取高级特征。然而，我们很快发现使用普通自动编码器作为生成模型存在一些缺点——例如，从学习的潜在空间中进行采样是有问题的。
- en: Variational autoencoders solve these problems by introducing randomness into
    the model and constraining how points in the latent space are distributed. We
    saw that with a few minor adjustments, we can transform our autoencoder into a
    variational autoencoder, thus giving it the power to be a true generative model.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自动编码器通过在模型中引入随机性并约束潜在空间中的点如何分布来解决这些问题。我们看到，通过一些微小的调整，我们可以将我们的自动编码器转变为变分自动编码器，从而赋予它成为真正生成模型的能力。
- en: Finally, we applied our new technique to the problem of face generation and
    saw how we can simply decode points from a standard normal distribution to generate
    new faces. Moreover, by performing vector arithmetic within the latent space,
    we can achieve some amazing effects, such as face morphing and feature manipulation.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将我们的新技术应用于面部生成问题，并看到我们如何简单地解码标准正态分布中的点以生成新的面孔。此外，通过在潜在空间内执行向量算术，我们可以实现一些惊人的效果，如面部变形和特征操作。
- en: 'In the next chapter, we shall explore a different kind of model that remains
    a popular choice for generative image modeling: the generative adversarial network.'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探索一种不同类型的模型，这种模型仍然是生成图像建模的一种流行选择：生成对抗网络。
- en: ^([1](ch03.xhtml#idm45387024580992-marker)) Diederik P. Kingma and Max Welling,
    “Auto-Encoding Variational Bayes,” December 20, 2013, [*https://arxiv.org/abs/1312.6114*](https://arxiv.org/abs/1312.6114).
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch03.xhtml#idm45387024580992-marker)) Diederik P. Kingma和Max Welling，“自动编码变分贝叶斯”，2013年12月20日，[*https://arxiv.org/abs/1312.6114*](https://arxiv.org/abs/1312.6114)。
- en: ^([2](ch03.xhtml#idm45387024036912-marker)) Vincent Dumoulin and Francesco Visin,
    “A Guide to Convolution Arithmetic for Deep Learning,” January 12, 2018, [*https://arxiv.org/abs/1603.07285*](https://arxiv.org/abs/1603.07285).
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch03.xhtml#idm45387024036912-marker)) Vincent Dumoulin和Francesco Visin，“深度学习卷积算术指南”，2018年1月12日，[*https://arxiv.org/abs/1603.07285*](https://arxiv.org/abs/1603.07285)。
- en: ^([3](ch03.xhtml#idm45387022470176-marker)) Ziwei Liu et al., “Large-Scale CelebFaces
    Attributes (CelebA) Dataset,” 2015, [*http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html*](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html).`
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch03.xhtml#idm45387022470176-marker)) Ziwei Liu等，“大规模CelebFaces属性（CelebA）数据集”，2015年，[*http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html*](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)。
