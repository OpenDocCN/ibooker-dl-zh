- en: Chapter 3\. Variational Autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 2013, Diederik P. Kingma and Max Welling published a paper that laid the
    foundations for a type of neural network known as a *variational autoencoder*
    (VAE).^([1](ch03.xhtml#idm45387024580992)) This is now one of the most fundamental
    and well-known deep learning architectures for generative modeling and an excellent
    place to start our journey into generative deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we shall start by building a standard autoencoder and then
    see how we can extend this framework to develop a variational autoencoder. Along
    the way, we will pick apart both types of models, to understand how they work
    at a granular level. By the end of the chapter you should have a complete understanding
    of how to build and manipulate autoencoder-based models and, in particular, how
    to build a variational autoencoder from scratch to generate images based on your
    own dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s start with a simple story that will help to explain the fundamental problem
    that an autoencoder is trying to solve.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now explore how this story relates to building autoencoders.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A diagram of the process described by the story is shown in [Figure 3-2](#encoder_decoder_story).
    You play the part of the *encoder*, moving each item of clothing to a location
    in the wardrobe. This process is called *encoding*. Brian plays the part of the
    *decoder*, taking a location in the wardrobe and attempting to re-create the item.
    This process is called *decoding*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. Items of clothing in the infinite wardrobe—each black dot represents
    an item of clothing
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Each location in the wardrobe is represented by two numbers (i.e., a 2D vector).
    For example, the trousers in [Figure 3-2](#encoder_decoder_story) are encoded
    to the point [6.3, –0.9]. This vector is also known as an *embedding* because
    the encoder attempts to embed as much information into it as possible, so that
    the decoder can produce an accurate reconstruction.
  prefs: []
  type: TYPE_NORMAL
- en: An *autoencoder* is simply a neural network that is trained to perform the task
    of encoding and decoding an item, such that the output from this process is as
    close to the original item as possible. Crucially, it can be used as a generative
    model, because we can decode any point in the 2D space that we want (in particular,
    those that are not embeddings of original items) to produce a novel item of clothing.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now see how we can build an autoencoder using Keras and apply it to a
    real dataset!
  prefs: []
  type: TYPE_NORMAL
- en: Running the Code for This Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for this example can be found in the Jupyter notebook located at *notebooks/03_vae/01_autoencoder/autoencoder.ipynb*
    in the book repository.
  prefs: []
  type: TYPE_NORMAL
- en: The Fashion-MNIST Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this example, we’ll be using the [Fashion-MNIST dataset](https://oreil.ly/DS4-4)—a
    collection of grayscale images of clothing items, each of size 28 × 28 pixels.
    Some example images from the dataset are shown in [Figure 3-3](#fashion_mnist).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. Examples of images from the Fashion-MNIST dataset
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The dataset comes prepackaged with TensorFlow, so it can be downloaded as shown
    in [Example 3-1](#fashion-mnist-ex).
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-1\. Loading the Fashion-MNIST dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: These are 28 × 28 grayscale images (pixel values between 0 and 255) out of the
    box, which we need to preprocess to ensure that the pixel values are scaled between
    0 and 1\. We will also pad each image to 32 × 32 for easier manipulation of the
    tensor shape as it passes through the network, as shown in [Example 3-2](#fashion-preprocessing-ex).
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-2\. Preprocessing the data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Next, we need to understand the overall structure of an autoencoder, so that
    we can code it up using TensorFlow and Keras.
  prefs: []
  type: TYPE_NORMAL
- en: The Autoencoder Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An *autoencoder* is a neural network made up of two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: An *encoder* network that compresses high-dimensional input data such as an
    image into a lower-dimensional embedding vector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A *decoder* network that decompresses a given embedding vector back to the original
    domain (e.g., back to an image)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A diagram of the network architecture is shown in [Figure 3-4](#autoencoder).
    An input image is encoded to a latent embedding vector <math alttext="z"><mi>z</mi></math>
    , which is then decoded back to the original pixel space.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0304.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-4\. Autoencoder architecture diagram
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The autoencoder is trained to reconstruct an image, after it has passed through
    the encoder and back out through the decoder. This may seem strange at first—why
    would you want to reconstruct a set of images that you already have available
    to you? However, as we shall see, it is the embedding space (also called the *latent
    space*) that is the interesting part of the autoencoder, as sampling from this
    space will allow us to generate new images.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first define what we mean by an embedding. The embedding ( <math alttext="z"><mi>z</mi></math>
    ) is a compression of the original image into a lower-dimensional latent space.
    The idea is that by choosing any point in the latent space, we can generate novel
    images by passing this point through the decoder, since the decoder has learned
    how to convert points in the latent space into viable images.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, we will embed images into a two-dimensional latent space. This
    will help us to visualize the latent space, since we can easily plot points in
    2D. In practice, the latent space of an autoencoder will usually have more than
    two dimensions in order to have more freedom to capture greater nuance in the
    images.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders as Denoising Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Autoencoders can be used to clean noisy images, since the encoder learns that
    it is not useful to capture the position of the random noise inside the latent
    space in order to reconstruct the original. For tasks such as this, a 2D latent
    space is probably too small to encode sufficient relevant information from the
    input. However, as we shall see, increasing the dimensionality of the latent space
    quickly leads to problems if we want to use the autoencoder as a generative model.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now see how to build the encoder and decoder.
  prefs: []
  type: TYPE_NORMAL
- en: The Encoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In an autoencoder, the encoder’s job is to take the input image and map it to
    an embedding vector in the latent space. The architecture of the encoder we will
    be building is shown in [Table 3-1](#encoder_diagram).
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-1\. Model summary of the encoder
  prefs: []
  type: TYPE_NORMAL
- en: '| Layer (type) | Output shape | Param # |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| InputLayer | (None, 32, 32, 1) | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Conv2D | (None, 16, 16, 32) | 320 |'
  prefs: []
  type: TYPE_TB
- en: '| Conv2D | (None, 8, 8, 64) | 18,496 |'
  prefs: []
  type: TYPE_TB
- en: '| Conv2D | (None, 4, 4, 128) | 73,856 |'
  prefs: []
  type: TYPE_TB
- en: '| Flatten | (None, 2048) | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Dense | (None, 2) | 4,098 |'
  prefs: []
  type: TYPE_TB
- en: '| Total params | 96,770 |'
  prefs: []
  type: TYPE_TB
- en: '| Trainable params | 96,770 |'
  prefs: []
  type: TYPE_TB
- en: '| Non-trainable params | 0 |'
  prefs: []
  type: TYPE_TB
- en: To achieve this, we first create an `Input` layer for the image and pass this
    through three `Conv2D` layers in sequence, each capturing increasingly high-level
    features. We use a stride of 2 to halve the size of the output of each layer,
    while increasing the number of channels. The last convolutional layer is flattened
    and connected to a `Dense` layer of size 2, which represents our two-dimensional
    latent space.
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 3-3](#the-encoder-ex) shows how to build this in Keras.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-3\. The encoder
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_variational_autoencoders_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Define the `Input` layer of the encoder (the image).
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_variational_autoencoders_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Stack `Conv2D` layers sequentially on top of each other.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_variational_autoencoders_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Flatten the last convolutional layer to a vector.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_variational_autoencoders_CO1-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Connect this vector to the 2D embeddings with a `Dense` layer.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_variational_autoencoders_CO1-5)'
  prefs: []
  type: TYPE_NORMAL
- en: The Keras `Model` that defines the encoder—a model that takes an input image
    and encodes it into a 2D embedding.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: I strongly encourage you to experiment with the number of convolutional layers
    and filters to understand how the architecture affects the overall number of model
    parameters, model performance, and model runtime.
  prefs: []
  type: TYPE_NORMAL
- en: The Decoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The decoder is a mirror image of the encoder—instead of convolutional layers,
    we use *convolutional transpose* layers, as shown in [Table 3-2](#decoder_diagram).
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-2\. Model summary of the decoder
  prefs: []
  type: TYPE_NORMAL
- en: '| Layer (type) | Output shape | Param # |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| InputLayer | (None, 2) | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Dense | (None, 2048) | 6,144 |'
  prefs: []
  type: TYPE_TB
- en: '| Reshape | (None, 4, 4, 128) | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Conv2DTranspose | (None, 8, 8, 128) | 147,584 |'
  prefs: []
  type: TYPE_TB
- en: '| Conv2DTranspose | (None, 16, 16, 64) | 73,792 |'
  prefs: []
  type: TYPE_TB
- en: '| Conv2DTranspose | (None, 32, 32, 32) | 18,464 |'
  prefs: []
  type: TYPE_TB
- en: '| Conv2D | (None, 32, 32, 1) | 289 |'
  prefs: []
  type: TYPE_TB
- en: '| Total params | 246,273 |'
  prefs: []
  type: TYPE_TB
- en: '| Trainable params | 246,273 |'
  prefs: []
  type: TYPE_TB
- en: '| Non-trainable params | 0 |'
  prefs: []
  type: TYPE_TB
- en: '[Example 3-4](#the-decoder-ex) shows how we build the decoder in Keras.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-4\. The decoder
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_variational_autoencoders_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Define the `Input` layer of the decoder (the embedding).
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_variational_autoencoders_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Connect the input to a `Dense` layer.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_variational_autoencoders_CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: '`Reshape` this vector into a tensor that can be fed as input into the first
    `Conv2DTranspose` layer.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_variational_autoencoders_CO2-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Stack `Conv2DTranspose` layers on top of each other.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_variational_autoencoders_CO2-5)'
  prefs: []
  type: TYPE_NORMAL
- en: The Keras `Model` that defines the decoder—a model that takes an embedding in
    the latent space and decodes it into the original image domain.
  prefs: []
  type: TYPE_NORMAL
- en: Joining the Encoder to the Decoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To train the encoder and decoder simultaneously, we need to define a model that
    will represent the flow of an image through the encoder and back out through the
    decoder. Luckily, Keras makes it extremely easy to do this, as you can see in
    [Example 3-5](#full-autoencoder-ex). Notice the way in which we specify that the
    output from the autoencoder is simply the output from the encoder after it has
    been passed through the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-5\. The full autoencoder
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_variational_autoencoders_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The Keras `Model` that defines the full autoencoder—a model that takes an image
    and passes it through the encoder and back out through the decoder to generate
    a reconstruction of the original image.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve defined our model, we just need to compile it with a loss function
    and optimizer, as shown in [Example 3-6](#the-compilation-ex). The loss function
    is usually chosen to be either the root mean squared error (RMSE) or binary cross-entropy
    between the individual pixels of the original image and the reconstruction.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-6\. Compiling the autoencoder
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We can now train the autoencoder by passing in the input images as both the
    input and output, as shown in [Example 3-7](#training-the-autoencoder-ex).
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-7\. Training the autoencoder
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now that our autoencoder is trained, the first thing we need to check is that
    it is able to accurately reconstruct the input images.
  prefs: []
  type: TYPE_NORMAL
- en: Reconstructing Images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can test the ability to reconstruct images by passing images from the test
    set through the autoencoder and comparing the output to the original images. The
    code for this is shown in [Example 3-8](#reconstruction-ex).
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-8\. Reconstructing images using the autoencoder
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In [Figure 3-6](#reconstructed_clothing) you can see some examples of original
    images (top row), the 2D vectors after encoding, and the reconstructed items after
    decoding (bottom row).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0306.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-6\. Examples of encoding and decoding items of clothing
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice how the reconstruction isn’t perfect—there are still some details of
    the original images that aren’t captured by the decoding process, such as logos.
    This is because by reducing each image to just two numbers, we naturally lose
    some information.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now investigate how the encoder is representing images in the latent space.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the Latent Space
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can visualize how images are embedded into the latent space by passing the
    test set through the encoder and plotting the resulting embeddings, as shown in
    [Example 3-9](#embedding-ex).
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-9\. Embedding images using the encoder
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The resulting plot is the scatter plot shown in [Figure 3-2](#encoder_decoder_story)—each
    black point represents an image that has been embedded into the latent space.
  prefs: []
  type: TYPE_NORMAL
- en: In order to better understand how this latent space is structured, we can make
    use of the labels that come with the Fashion-MNIST dataset, describing the type
    of item in each image. There are 10 groups altogether, shown in [Table 3-3](#fashion-mnist-table).
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-3\. The Fashion-MNIST labels
  prefs: []
  type: TYPE_NORMAL
- en: '| ID | Clothing label |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | T-shirt/top |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Trouser |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Pullover |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Dress |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Coat |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | Sandal |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | Shirt |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | Sneaker |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | Bag |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | Ankle boot |'
  prefs: []
  type: TYPE_TB
- en: We can color each point based on the label of the corresponding image to produce
    the plot in [Figure 3-7](#ae_plot_colour). Now the structure becomes very clear!
    Even though the clothing labels were never shown to the model during training,
    the autoencoder has naturally grouped items that look alike into the same parts
    of the latent space. For example, the dark blue cloud of points in the bottom-right
    corner of the latent space are all different images of trousers and the red cloud
    of points toward the center are all ankle boots.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0307.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-7\. Plot of the latent space, colored by clothing label
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Generating New Images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can generate novel images by sampling some points in the latent space and
    using the decoder to convert these back into pixel space, as shown in [Example 3-10](#generation-ex).
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-10\. Generating novel images using the decoder
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Some examples of generated images are shown in [Figure 3-8](#autoencoder_exhibits),
    alongside their embeddings in the latent space.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0308.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-8\. Generated items of clothing
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Each blue dot maps to one of the images shown on the right of the diagram, with
    the embedding vector shown underneath. Notice how some of the generated items
    are more realistic than others. Why is this?
  prefs: []
  type: TYPE_NORMAL
- en: 'To answer this, let’s first make a few observations about the overall distribution
    of points in the latent space, referring back to [Figure 3-7](#ae_plot_colour):'
  prefs: []
  type: TYPE_NORMAL
- en: Some clothing items are represented over a very small area and others over a
    much larger area.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The distribution is not symmetrical about the point (0, 0), or bounded. For
    example, there are far more points with positive y-axis values than negative,
    and some points even extend to a y-axis value > 8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are large gaps between colors containing few points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These observations actually make sampling from the latent space quite challenging.
    If we overlay the latent space with images of decoded points on a grid, as shown
    in [Figure 3-9](#ae_overlay), we can begin to understand why the decoder may not
    always generate images to a satisfactory standard.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0309.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-9\. A grid of decoded embeddings, overlaid with the embeddings from
    the original images in the dataset, colored by item type
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Firstly, we can see that if we pick points uniformly in a bounded space that
    we define, we’re more likely to sample something that decodes to look like a bag
    (ID 8) than an ankle boot (ID 9) because the part of the latent space carved out
    for bags (orange) is larger than the ankle boot area (red).
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, it is not obvious how we should go about choosing a *random* point
    in the latent space, since the distribution of these points is undefined. Technically,
    we would be justified in choosing any point in the 2D plane! It’s not even guaranteed
    that points will be centered around (0, 0). This makes sampling from our latent
    space problematic.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we can see holes in the latent space where none of the original images
    are encoded. For example, there are large white spaces at the edges of the domain—the
    autoencoder has no reason to ensure that points here are decoded to recognizable
    clothing items as very few images in the training set are encoded here.
  prefs: []
  type: TYPE_NORMAL
- en: Even points that are central may not be decoded into well-formed images. This
    is because the autoencoder is not forced to ensure that the space is continuous.
    For example, even though the point (–1, –1) might be decoded to give a satisfactory
    image of a sandal, there is no mechanism in place to ensure that the point (–1.1,
    –1.1) also produces a satisfactory image of a sandal.
  prefs: []
  type: TYPE_NORMAL
- en: In two dimensions this issue is subtle; the autoencoder only has a small number
    of dimensions to work with, so naturally it has to squash clothing groups together,
    resulting in the space between clothing groups being relatively small. However,
    as we start to use more dimensions in the latent space to generate more complex
    images such as faces, this problem becomes even more apparent. If we give the
    autoencoder free rein over how it uses the latent space to encode images, there
    will be huge gaps between groups of similar points with no incentive for the spaces
    in between to generate well-formed images.
  prefs: []
  type: TYPE_NORMAL
- en: In order to solve these three problems, we need to convert our autoencoder into
    a *variational autoencoder*.
  prefs: []
  type: TYPE_NORMAL
- en: Variational Autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To explain, let’s revisit the infinite wardrobe and make a few changes…​
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now try to understand what we need to do to our autoencoder model to convert
    it into a variational autoencoder and thus make it a more sophisticated generative
    model.
  prefs: []
  type: TYPE_NORMAL
- en: The two parts that we need to change are the encoder and the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: The Encoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In an autoencoder, each image is mapped directly to one point in the latent
    space. In a variational autoencoder, each image is instead mapped to a multivariate
    normal distribution around a point in the latent space, as shown in [Figure 3-10](#vae_encoder_normal).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0310.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-10\. The difference between the encoders in an autoencoder and a variational
    autoencoder
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The encoder only needs to map each input to a mean vector and a variance vector
    and does not need to worry about covariance between dimensions. Variational autoencoders
    assume that there is no correlation between dimensions in the latent space.
  prefs: []
  type: TYPE_NORMAL
- en: Variance values are always positive, so we actually choose to map to the *logarithm*
    of the variance, as this can take any real number in the range ( <math alttext="negative
    normal infinity"><mrow><mo>-</mo> <mi>∞</mi></mrow></math> , <math alttext="normal
    infinity"><mi>∞</mi></math> ). This way we can use a neural network as the encoder
    to perform the mapping from the input image to the mean and log variance vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, the encoder will take each input image and encode it to two vectors
    that together define a multivariate normal distribution in the latent space:'
  prefs: []
  type: TYPE_NORMAL
- en: '`z_mean`'
  prefs: []
  type: TYPE_NORMAL
- en: The mean point of the distribution
  prefs: []
  type: TYPE_NORMAL
- en: '`z_log_var`'
  prefs: []
  type: TYPE_NORMAL
- en: The logarithm of the variance of each dimension
  prefs: []
  type: TYPE_NORMAL
- en: 'We can sample a point `z` from the distribution defined by these values using
    the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The derivation of the relationship between `z_sigma` ( <math alttext="sigma"><mi>σ</mi></math>
    ) and `z_log_var` ( <math alttext="log left-parenthesis sigma squared right-parenthesis"><mrow><mo
    form="prefix">log</mo> <mo>(</mo> <msup><mi>σ</mi> <mn>2</mn></msup> <mo>)</mo></mrow></math>
    ) is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="sigma equals exp left-parenthesis log left-parenthesis sigma
    right-parenthesis right-parenthesis equals exp left-parenthesis 2 log left-parenthesis
    sigma right-parenthesis slash 2 right-parenthesis equals exp left-parenthesis
    log left-parenthesis sigma squared right-parenthesis slash 2 right-parenthesis"
    display="block"><mrow><mi>σ</mi> <mo>=</mo> <mo form="prefix">exp</mo> <mrow><mo>(</mo>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>σ</mi> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>=</mo> <mo form="prefix">exp</mo> <mrow><mo>(</mo> <mn>2</mn> <mo form="prefix">log</mo>
    <mrow><mo>(</mo> <mi>σ</mi> <mo>)</mo></mrow> <mo>/</mo> <mn>2</mn> <mo>)</mo></mrow>
    <mo>=</mo> <mo form="prefix">exp</mo> <mrow><mo>(</mo> <mo form="prefix">log</mo>
    <mrow><mo>(</mo> <msup><mi>σ</mi> <mn>2</mn></msup> <mo>)</mo></mrow> <mo>/</mo>
    <mn>2</mn> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The decoder of a variational autoencoder is identical to the decoder of a plain
    autoencoder, giving the overall architecture shown in [Figure 3-12](#vae_architecture).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0312.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-12\. VAE architecture diagram
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Why does this small change to the encoder help?
  prefs: []
  type: TYPE_NORMAL
- en: Previously, we saw that there was no requirement for the latent space to be
    continuous—even if the point (–2, 2) decodes to a well-formed image of a sandal,
    there’s no requirement for (–2.1, 2.1) to look similar. Now, since we are sampling
    a random point from an area around `z_mean`, the decoder must ensure that all
    points in the same neighborhood produce very similar images when decoded, so that
    the reconstruction loss remains small. This is a very nice property that ensures
    that even when we choose a point in the latent space that has never been seen
    by the decoder, it is likely to decode to an image that is well formed.
  prefs: []
  type: TYPE_NORMAL
- en: Building the VAE encoder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s now see how we build this new version of the encoder in Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Running the Code for This Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for this example can be found in the Jupyter notebook located at *notebooks/03_vae/02_vae_fashion/vae_fashion.ipynb*
    in the book repository.
  prefs: []
  type: TYPE_NORMAL
- en: The code has been adapted from the excellent [VAE tutorial](https://oreil.ly/A7yqJ)
    created by Francois Chollet, available on the Keras website.
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to create a new type of `Sampling` layer that will allow us to
    sample from the distribution defined by `z_mean` and `z_log_var`, as shown in
    [Example 3-11](#variational-autoencoder-sampling-ex).
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-11\. The `Sampling` layer
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_variational_autoencoders_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: We create a new layer by subclassing the Keras base `Layer` class (see the “Subclassing
    the Layer Class” sidebar).
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_variational_autoencoders_CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: We use the reparameterization trick (see “The Reparameterization Trick” sidebar)
    to build a sample from the normal distribution parameterized by `z_mean` and `z_log_var`.
  prefs: []
  type: TYPE_NORMAL
- en: The complete code for the encoder, including the new `Sampling` layer, is shown
    in [Example 3-12](#variational-autoencoder-encoder-ex).
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-12\. The encoder
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_variational_autoencoders_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of connecting the `Flatten` layer directly to the 2D latent space, we
    connect it to layers `z_mean` and `z_log_var`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_variational_autoencoders_CO5-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The `Sampling` layer samples a point `z` in the latent space from the normal
    distribution defined by the parameters `z_mean` and `z_log_var`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_variational_autoencoders_CO5-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The Keras `Model` that defines the encoder—a model that takes an input image
    and outputs `z_mean`, `z_log_var`, and a sampled point `z` from the normal distribution
    defined by these parameters.
  prefs: []
  type: TYPE_NORMAL
- en: A summary of the encoder is shown in [Table 3-4](#vae_clothing_encoder).
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-4\. Model summary of the VAE encoder
  prefs: []
  type: TYPE_NORMAL
- en: '| Layer (type) | Output shape | Param # | Connected to |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| InputLayer (input) | (None, 32, 32, 1) | 0 | [] |'
  prefs: []
  type: TYPE_TB
- en: '| Conv2D (conv2d_1) | (None, 16, 16, 32) | 320 | [input] |'
  prefs: []
  type: TYPE_TB
- en: '| Conv2D (conv2d_2) | (None, 8, 8, 64) | 18,496 | [conv2d_1] |'
  prefs: []
  type: TYPE_TB
- en: '| Conv2D (conv2d_3) | (None, 4, 4, 128) | 73,856 | [conv2d_2] |'
  prefs: []
  type: TYPE_TB
- en: '| Flatten (flatten) | (None, 2048) | 0 | [conv2d_3] |'
  prefs: []
  type: TYPE_TB
- en: '| Dense (z_mean) | (None, 2) | 4,098 | [flatten] |'
  prefs: []
  type: TYPE_TB
- en: '| Dense (z_log_var) | (None, 2) | 4,098 | [flatten] |'
  prefs: []
  type: TYPE_TB
- en: '| Sampling (z) | (None, 2) | 0 | [z_mean, z_log_var] |'
  prefs: []
  type: TYPE_TB
- en: '| Total params | 100,868 |'
  prefs: []
  type: TYPE_TB
- en: '| Trainable params | 100,868 |'
  prefs: []
  type: TYPE_TB
- en: '| Non-trainable params | 0 |'
  prefs: []
  type: TYPE_TB
- en: The only other part of the original autoencoder that we need to change is the
    loss function.
  prefs: []
  type: TYPE_NORMAL
- en: The Loss Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Previously, our loss function only consisted of the *reconstruction loss* between
    images and their attempted copies after being passed through the encoder and decoder.
    The reconstruction loss also appears in a variational autoencoder, but we now
    require one extra component: the *Kullback–Leibler (KL) divergence* term.'
  prefs: []
  type: TYPE_NORMAL
- en: 'KL divergence is a way of measuring how much one probability distribution differs
    from another. In a VAE, we want to measure how much our normal distribution with
    parameters `z_mean` and `z_log_var` differs from a standard normal distribution.
    In this special case, it can be shown that the KL divergence has the following
    closed form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'or in mathematical notation:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper D Subscript upper K upper L Baseline left-bracket upper
    N left-parenthesis mu comma sigma parallel-to upper N left-parenthesis 0 comma
    1 right-parenthesis right-bracket equals minus one-half sigma-summation left-parenthesis
    1 plus l o g left-parenthesis sigma squared right-parenthesis minus mu squared
    minus sigma squared right-parenthesis" display="block"><mstyle scriptlevel="0"
    displaystyle="true"><mrow><msub><mi>D</mi> <mrow><mi>K</mi><mi>L</mi></mrow></msub>
    <mrow><mo>[</mo> <mi>N</mi> <mrow><mo>(</mo> <mi>μ</mi> <mo>,</mo> <mi>σ</mi>
    <mo>∥</mo> <mi>N</mi> <mrow><mo>(</mo> <mn>0</mn> <mo>,</mo> <mn>1</mn> <mo>)</mo></mrow>
    <mo>]</mo></mrow> <mo>=</mo> <mo>-</mo></mrow> <mfrac><mn>1</mn> <mn>2</mn></mfrac>
    <mo>∑</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>+</mo> <mi>l</mi> <mi>o</mi> <mi>g</mi>
    <mrow><mo>(</mo> <msup><mi>σ</mi> <mn>2</mn></msup> <mo>)</mo></mrow> <mo>-</mo>
    <msup><mi>μ</mi> <mn>2</mn></msup> <mo>-</mo> <msup><mi>σ</mi> <mn>2</mn></msup>
    <mo>)</mo></mrow></mrow></mstyle></math>
  prefs: []
  type: TYPE_NORMAL
- en: The sum is taken over all the dimensions in the latent space. `kl_loss` is minimized
    to 0 when `z_mean = 0` and `z_log_var = 0` for all dimensions. As these two terms
    start to differ from 0, `kl_loss` increases.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, the KL divergence term penalizes the network for encoding observations
    to `z_mean` and `z_log_var` variables that differ significantly from the parameters
    of a standard normal distribution, namely `z_mean = 0` and `z_log_var = 0`.
  prefs: []
  type: TYPE_NORMAL
- en: Why does this addition to the loss function help?
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, we now have a well-defined distribution that we can use for choosing
    points in the latent space—the standard normal distribution. Secondly, since this
    term tries to force all encoded distributions toward the standard normal distribution,
    there is less chance that large gaps will form between point clusters. Instead,
    the encoder will try to use the space around the origin symmetrically and efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: In the original VAE paper, the loss function for a VAE was simply the addition
    of the reconstruction loss and the KL divergence loss term. A variant on this
    (the <math alttext="beta"><mi>β</mi></math> -VAE) includes a factor that weights
    the KL divergence to ensure that it is well balanced with the reconstruction loss.
    If we weight the reconstruction loss too heavily, the KL loss will not have the
    desired regulatory effect and we will see the same problems that we experienced
    with the plain autoencoder. If the KL divergence term is weighted too heavily,
    the KL divergence loss will dominate and the reconstructed images will be poor.
    This weighting term is one of the parameters to tune when you’re training your
    VAE.
  prefs: []
  type: TYPE_NORMAL
- en: Training the Variational Autoencoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Example 3-13](#kl-divergence-ex) shows how we build the overall VAE model
    as a subclass of the abstract Keras `Model` class. This allows us to include the
    calculation of the KL divergence term of the loss function in a custom `train_step`
    method.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-13\. Training the VAE
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_variational_autoencoders_CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This function describes what we would like returned what we call the VAE on
    a particular input image.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_variational_autoencoders_CO6-2)'
  prefs: []
  type: TYPE_NORMAL
- en: This function describes one training step of the VAE, including the calculation
    of the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_variational_autoencoders_CO6-3)'
  prefs: []
  type: TYPE_NORMAL
- en: A beta value of 500 is used in the reconstruction loss.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_variational_autoencoders_CO6-4)'
  prefs: []
  type: TYPE_NORMAL
- en: The total loss is the sum of the reconstruction loss and the KL divergence loss.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Tape
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow’s *Gradient Tape* is a mechanism that allows the computation of gradients
    of operations executed during a forward pass of a model. To use it, you need to
    wrap the code that performs the operations you want to differentiate in a `tf.GradientTape()`
    context. Once you have recorded the operations, you can compute the gradient of
    the loss function with respect to some variables by calling `tape.gradient()`.
    The gradients can then be used to update the variables with the optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: This mechanism is useful for calculating the gradient of custom loss functions
    (as we have done here) and also for creating custom training loops, as we shall
    see in [Chapter 4](ch04.xhtml#chapter_gan).
  prefs: []
  type: TYPE_NORMAL
- en: Analysis of the Variational Autoencoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have trained our VAE, we can use the encoder to encode the images
    in the test set and plot the `z_mean` values in the latent space. We can also
    sample from a standard normal distribution to generate points in the latent space
    and use the decoder to decode these points back into pixel space to see how the
    VAE performs.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-13](#vae_wardrobe) shows the structure of the new latent space, alongside
    some sampled points and their decoded images. We can immediately see several changes
    in how the latent space is organized.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0313.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-13\. The new latent space: the black dots show the `z_mean` value
    of each encoded image, while blue dots show some sampled points in the latent
    space (with their decoded images on the right)'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Firstly, the KL divergence loss term ensures that the `z_mean` and `z_log_var`
    values of the encoded images never stray too far from a standard normal distribution.
    Secondly, there are not so many poorly formed images as the latent space is now
    much more continuous, due to fact that the encoder is now stochastic, rather than
    deterministic.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, by coloring points in the latent space by clothing type ([Figure 3-14](#vae_wardrobe_colour)),
    we can see that there is no preferential treatment of any one type. The righthand
    plot shows the space transformed into *p*-values—we can see that each color is
    approximately equally represented. Again, it’s important to remember that the
    labels were not used at all during training; the VAE has learned the various forms
    of clothing by itself in order to help minimize reconstruction loss.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0314.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-14\. The latent space of the VAE colored by clothing type
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Exploring the Latent Space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, all of our work on autoencoders and variational autoencoders has been
    limited to a latent space with two dimensions. This has helped us to visualize
    the inner workings of a VAE on the page and understand why the small tweaks that
    we made to the architecture of the autoencoder helped transform it into a more
    powerful class of network that can be used for generative modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now turn our attention to a more complex dataset and see the amazing things
    that variational autoencoders can achieve when we increase the dimensionality
    of the latent space.
  prefs: []
  type: TYPE_NORMAL
- en: Running the Code for This Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for this example can be found in the Jupyter notebook located at *notebooks/03_vae/03_faces/vae_faces.ipynb*
    in the book repository.
  prefs: []
  type: TYPE_NORMAL
- en: The CelebA Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We shall be using the [CelebFaces Attributes (CelebA) dataset](https://oreil.ly/tEUnh)
    to train our next variational autoencoder. This is a collection of over 200,000
    color images of celebrity faces, each annotated with various labels (e.g., *wearing
    hat*, *smiling*, etc.). A few examples are shown in [Figure 3-15](#celeba_sample).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0315.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-15\. Some examples from the CelebA dataset (source: [Liu et al., 2015](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html))^([3](ch03.xhtml#idm45387022470176))'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Of course, we don’t need the labels to train the VAE, but these will be useful
    later when we start exploring how these features are captured in the multidimensional
    latent space. Once our VAE is trained, we can sample from the latent space to
    generate new examples of celebrity faces.
  prefs: []
  type: TYPE_NORMAL
- en: The CelebA dataset is also available through Kaggle, so you can download the
    dataset by running the Kaggle dataset downloader script in the book repository,
    as shown in [Example 3-14](#downloading-celeba-dataset). This will save the images
    and accompanying metadata locally to the */data* folder.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-14\. Downloading the CelebA dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '`We use the Keras function `image_dataset_from_directory` to create a TensorFlow
    Dataset pointed at the directory where the images are stored, as shown in [Example 3-15](#preprocessing-face-data).
    This allows us to read batches of images into memory only when required (e.g.,
    during training), so that we can work with large datasets and not worry about
    having to fit the entire dataset into memory. It also resizes the images to 64
    × 64, interpolating between pixel values.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-15\. Preprocessing the CelebA dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The original data is scaled in the range [0, 255] to denote the pixel intensity,
    which we rescale to the range [0, 1] as shown in [Example 3-16](#preprocessing-celeba-data).
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-16\. Preprocessing the CelebA dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]`  `## Training the Variational Autoencoder'
  prefs: []
  type: TYPE_NORMAL
- en: 'The network architecture for the faces model is similar to the Fashion-MNIST
    example, with a few slight differences:'
  prefs: []
  type: TYPE_NORMAL
- en: Our data now has three input channels (RGB) instead of one (grayscale). This
    means we need to change the number of channels in the final convolutional transpose
    layer of the decoder to 3.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We shall be using a latent space with 200 dimensions instead of 2\. Since faces
    are much more complex than the Fashion-MNIST images, we increase the dimensionality
    of the latent space so that the network can encode a satisfactory amount of detail
    from the images.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are batch normalization layers after each convolutional layer to stabilize
    training. Even though each batch takes a longer time to run, the number of batches
    required to reach the same loss is greatly reduced.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We increase the <math alttext="beta"><mi>β</mi></math> factor for the KL divergence
    to 2,000\. This is a parameter that requires tuning; for this dataset and architecture
    this value was found to generate good results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The full architectures of the encoder and decoder are shown in Tables [3-5](#vae_faces_encoder)
    and [3-6](#vae_faces_decoder), respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-5\. Model summary of the VAE faces encoder
  prefs: []
  type: TYPE_NORMAL
- en: '| Layer (type) | Output shape | Param # | Connected to |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| InputLayer (input) | (None, 32, 32, 3) | 0 | [] |'
  prefs: []
  type: TYPE_TB
- en: '| Conv2D (conv2d_1) | (None, 16, 16, 128) | 3,584 | [input] |'
  prefs: []
  type: TYPE_TB
- en: '| BatchNormalization (bn_1) | (None, 16, 16, 128) | 512 | [conv2d_1] |'
  prefs: []
  type: TYPE_TB
- en: '| LeakyReLU (lr_1) | (None, 16, 16, 128) | 0 | [bn_1] |'
  prefs: []
  type: TYPE_TB
- en: '| Conv2D (conv2d_2) | (None, 8, 8, 128) | 147,584 | [lr_1] |'
  prefs: []
  type: TYPE_TB
- en: '| BatchNormalization (bn_2) | (None, 8, 8, 128) | 512 | [conv2d_2] |'
  prefs: []
  type: TYPE_TB
- en: '| LeakyReLU (lr_2) | (None, 8, 8, 128) | 0 | [bn_2] |'
  prefs: []
  type: TYPE_TB
- en: '| Conv2D (conv2d_3) | (None, 4, 4, 128) | 147,584 | [lr_2] |'
  prefs: []
  type: TYPE_TB
- en: '| BatchNormalization (bn_3) | (None, 4, 4, 128) | 512 | [conv2d_3] |'
  prefs: []
  type: TYPE_TB
- en: '| LeakyReLU (lr_3) | (None, 4, 4, 128) | 0 | [bn_3] |'
  prefs: []
  type: TYPE_TB
- en: '| Conv2D (conv2d_4) | (None, 2, 2, 128) | 147,584 | [lr_3] |'
  prefs: []
  type: TYPE_TB
- en: '| BatchNormalization (bn_4) | (None, 2, 2, 128) | 512 | [conv2d_4] |'
  prefs: []
  type: TYPE_TB
- en: '| LeakyReLU (lr_4) | (None, 2, 2, 128) | 0 | [bn_4] |'
  prefs: []
  type: TYPE_TB
- en: '| Flatten (flatten) | (None, 512) | 0 | [lr_4] |'
  prefs: []
  type: TYPE_TB
- en: '| Dense (z_mean) | (None, 200) | 102,600 | [flatten] |'
  prefs: []
  type: TYPE_TB
- en: '| Dense (z_log_var) | (None, 200) | 102,600 | [flatten] |'
  prefs: []
  type: TYPE_TB
- en: '| Sampling (z) | (None, 200) | 0 | [z_mean, z_log_var] |'
  prefs: []
  type: TYPE_TB
- en: '| Total params | 653,584 |'
  prefs: []
  type: TYPE_TB
- en: '| Trainable params | 652,560 |'
  prefs: []
  type: TYPE_TB
- en: '| Non-trainable params | 1,024 |'
  prefs: []
  type: TYPE_TB
- en: Table 3-6\. Model summary of the VAE faces decoder
  prefs: []
  type: TYPE_NORMAL
- en: '| Layer (type) | Output shape | Param # |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| InputLayer | (None, 200) | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Dense | (None, 512) | 102,912 |'
  prefs: []
  type: TYPE_TB
- en: '| BatchNormalization | (None, 512) | 2,048 |'
  prefs: []
  type: TYPE_TB
- en: '| LeakyReLU | (None, 512) | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Reshape | (None, 2, 2, 128) | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Conv2DTranspose | (None, 4, 4, 128) | 147,584 |'
  prefs: []
  type: TYPE_TB
- en: '| BatchNormalization | (None, 4, 4, 128) | 512 |'
  prefs: []
  type: TYPE_TB
- en: '| LeakyReLU | (None, 4, 4, 128) | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Conv2DTranspose | (None, 8, 8, 128) | 147,584 |'
  prefs: []
  type: TYPE_TB
- en: '| BatchNormalization | (None, 8, 8, 128) | 512 |'
  prefs: []
  type: TYPE_TB
- en: '| LeakyReLU | (None, 8, 8, 128) | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Conv2DTranspose | (None, 16, 16, 128) | 147,584 |'
  prefs: []
  type: TYPE_TB
- en: '| BatchNormalization | (None, 16, 16, 128) | 512 |'
  prefs: []
  type: TYPE_TB
- en: '| LeakyReLU | (None, 16, 16, 128) | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Conv2DTranspose | (None, 32, 32, 128) | 147,584 |'
  prefs: []
  type: TYPE_TB
- en: '| BatchNormalization | (None, 32, 32, 128) | 512 |'
  prefs: []
  type: TYPE_TB
- en: '| LeakyReLU | (None, 32, 32, 128) | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Conv2DTranspose | (None, 32, 32, 3) | 3,459 |'
  prefs: []
  type: TYPE_TB
- en: '| Total params | 700,803 |'
  prefs: []
  type: TYPE_TB
- en: '| Trainable params | 698,755 |'
  prefs: []
  type: TYPE_TB
- en: '| Non-trainable params | 2,048 |'
  prefs: []
  type: TYPE_TB
- en: After around five epochs of training, our VAE should be able to produce novel
    images of celebrity faces!
  prefs: []
  type: TYPE_NORMAL
- en: Analysis of the Variational Autoencoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, let’s take a look at a sample of reconstructed faces. The top row in
    [Figure 3-16](#vae_faces_reconstruction) shows the original images and the bottom
    row shows the reconstructions once they have passed through the encoder and decoder.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0316.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-16\. Reconstructed faces, after passing through the encoder and decoder
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can see that the VAE has successfully captured the key features of each face—the
    angle of the head, the hairstyle, the expression, etc. Some of the fine detail
    is missing, but it is important to remember that the aim of building variational
    autoencoders isn’t to achieve perfect reconstruction loss. Our end goal is to
    sample from the latent space in order to generate new faces.
  prefs: []
  type: TYPE_NORMAL
- en: For this to be possible we must check that the distribution of points in the
    latent space approximately resembles a multivariate standard normal distribution.
    If we see any dimensions that are significantly different from a standard normal
    distribution, we should probably reduce the reconstruction loss factor, since
    the KL divergence term isn’t having enough effect.
  prefs: []
  type: TYPE_NORMAL
- en: The first 50 dimensions in our latent space are shown in [Figure 3-17](#vae_faces_normal_distributions).
    There aren’t any distributions that stand out as being significantly different
    from the standard normal, so we can move on to generating some faces!
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0317.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-17\. Distributions of points for the first 50 dimensions in the latent
    space
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Generating New Faces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To generate new faces, we can use the code in [Example 3-17](#new-faces-latent-space-ex).
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-17\. Generating new faces from the latent space
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_variational_autoencoders_CO7-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Sample 30 points from a standard multivariate normal distribution with 200 dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_variational_autoencoders_CO7-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Decode the sampled points.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_variational_autoencoders_CO7-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Plot the images!
  prefs: []
  type: TYPE_NORMAL
- en: The output is shown in [Figure 3-18](#vae_faces_generated).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0318.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-18\. New generated faces
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Amazingly, the VAE is able to take the set of points that we sampled from a
    standard normal distribution and convert each into a convincing image of a person’s
    face. This is our first glimpse of the true power of generative models!
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s see if we can start to use the latent space to perform some interesting
    operations on generated images.
  prefs: []
  type: TYPE_NORMAL
- en: Latent Space Arithmetic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One benefit of mapping images into a lower-dimensional latent space is that
    we can perform arithmetic on vectors in this latent space that has a visual analogue
    when decoded back into the original image domain.
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose we want to take an image of somebody who looks sad and
    give them a smile. To do this we first need to find a vector in the latent space
    that points in the direction of increasing smile. Adding this vector to the encoding
    of the original image in the latent space will give us a new point which, when
    decoded, should give us a more smiley version of the original image.
  prefs: []
  type: TYPE_NORMAL
- en: So how can we find the *smile* vector? Each image in the CelebA dataset is labeled
    with attributes, one of which is `Smiling`. If we take the average position of
    encoded images in the latent space with the attribute `Smiling` and subtract the
    average position of encoded images that do not have the attribute `Smiling`, we
    will obtain the vector that points in the direction of `Smiling`, which is exactly
    what we need.
  prefs: []
  type: TYPE_NORMAL
- en: 'Conceptually, we are performing the following vector arithmetic in the latent
    space, where `alpha` is a factor that determines how much of the feature vector
    is added or subtracted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Let’s see this in action. [Figure 3-19](#vae_adding_features) shows several
    images that have been encoded into the latent space. We then add or subtract multiples
    of a certain vector (e.g., `Smiling`, `Black_Hair`, `Eyeglasses`, `Young`, `Male`,
    `Blond_Hair`) to obtain different versions of the image, with only the relevant
    feature changed.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0319.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-19\. Adding and subtracting features to and from faces
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is remarkable that even though we are moving the point a significantly large
    distance in the latent space, the core image remains approximately the same, except
    for the one feature that we want to manipulate. This demonstrates the power of
    variational autoencoders for capturing and adjusting high-level features in images.
  prefs: []
  type: TYPE_NORMAL
- en: Morphing Between Faces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can use a similar idea to morph between two faces. Imagine two points in
    the latent space, A and B, that represent two images. If you started at point
    A and walked toward point B in a straight line, decoding each point on the line
    as you went, you would see a gradual transition from the starting face to the
    end face.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, we are traversing a straight line, which can be described by
    the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Here, `alpha` is a number between 0 and 1 that determines how far along the
    line we are, away from point A.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-20](#vae_morphing_faces) shows this process in action. We take two
    images, encode them into the latent space, and then decode points along the straight
    line between them at regular intervals.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0320.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-20\. Morphing between two faces
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is worth noting the smoothness of the transition—even where there are multiple
    features to change simultaneously (e.g., removal of glasses, hair color, gender),
    the VAE manages to achieve this fluidly, showing that the latent space of the
    VAE is truly a continuous space that can be traversed and explored to generate
    a multitude of different human faces.`  `# Summary
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we have seen how variational autoencoders are a powerful tool
    in the generative modeling toolbox. We started by exploring how plain autoencoders
    can be used to map high-dimensional images into a low-dimensional latent space,
    so that high-level features can be extracted from the individually uninformative
    pixels. However, we quickly found that there were some drawbacks to using plain
    autoencoders as a generative model—sampling from the learned latent space was
    problematic, for example.
  prefs: []
  type: TYPE_NORMAL
- en: Variational autoencoders solve these problems by introducing randomness into
    the model and constraining how points in the latent space are distributed. We
    saw that with a few minor adjustments, we can transform our autoencoder into a
    variational autoencoder, thus giving it the power to be a true generative model.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we applied our new technique to the problem of face generation and
    saw how we can simply decode points from a standard normal distribution to generate
    new faces. Moreover, by performing vector arithmetic within the latent space,
    we can achieve some amazing effects, such as face morphing and feature manipulation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we shall explore a different kind of model that remains
    a popular choice for generative image modeling: the generative adversarial network.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch03.xhtml#idm45387024580992-marker)) Diederik P. Kingma and Max Welling,
    “Auto-Encoding Variational Bayes,” December 20, 2013, [*https://arxiv.org/abs/1312.6114*](https://arxiv.org/abs/1312.6114).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch03.xhtml#idm45387024036912-marker)) Vincent Dumoulin and Francesco Visin,
    “A Guide to Convolution Arithmetic for Deep Learning,” January 12, 2018, [*https://arxiv.org/abs/1603.07285*](https://arxiv.org/abs/1603.07285).
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch03.xhtml#idm45387022470176-marker)) Ziwei Liu et al., “Large-Scale CelebFaces
    Attributes (CelebA) Dataset,” 2015, [*http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html*](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html).`
  prefs: []
  type: TYPE_NORMAL
