- en: Chapter 3\. Implementing Cloud Native Generative AI with Azure OpenAI Service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will focus on the implementation of generative AI architectures
    with Microsoft Azure and Azure OpenAI models, always aiming to present all available
    options, and minimize the required development, integration, and usage cost, while
    accelerating the operationalization. For that purpose, I’ve included a series
    of best practices and typical architectures that will allow you to choose the
    best building blocks for your specific scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: We will include the most relevant Azure OpenAI implementation approaches, based
    on existing features and repositories that will continue evolving, improving,
    and including new functionalities. I’ve included URLs to the original documentation
    because they are continuously updated with new features, so these links will allow
    you to explore any details you need. Most of them rely on official accelerators
    from GitHub repositories, and projects that you will be able to follow and/or
    fork. But before getting into the details, let’s explore some fundamental topics
    that will help you understand the full extent of what a generative AI with Azure
    OpenAI Service means.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the Knowledge Scope of Azure OpenAI Service–Enabled Apps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative AI applications on Microsoft Azure are not only for regular ChatGPT-type
    applications. They are advanced architectures that rely on diverse technology
    pieces, including the core infrastructure (servers, [GPUs](https://oreil.ly/y5mXm),
    etc.) required to run generative AI models, and that allow adopters to create
    conversational applications and search engines, develop and integrate new AI copilots
    into their applications, customize customer attention, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'From an Azure OpenAI point of view, we are talking about a managed service
    that includes advanced functionalities that will allow you to implement *different
    levels of knowledge*, depending on the desired scope of your applications, and
    based on default capabilities and specific adjustment and customization techniques.
    By levels of knowledge, we mean something that goes beyond the initial scope of
    the LLM and its massive dataset (e.g., adding new information for an internal
    company application, based on its own data). Some of the options to adjust that
    knowledge include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Baseline LLM
  prefs: []
  type: TYPE_NORMAL
- en: Azure OpenAI’s language models are trained on enormous datasets containing billions
    of words. These datasets are carefully curated to include a wide range of topics,
    genres, and writing styles. The size and diversity of the training data helps
    the models develop a broad understanding of human language. The specific details
    of the training data have not been disclosed, but it includes text data from a
    variety of sources, including books, articles, websites, and other publicly available
    written material. Additionally, the training process (RLHF) includes human reviewers
    who help annotate and curate the data, flagging and addressing potential biases
    or problematic content. Feedback loops with reviewers are established to continuously
    improve and refine the models. One of the key advantages of the enterprise-grade
    Azure OpenAI service is that [your data is only yours](https://oreil.ly/qA5Ok)
    and is not used by anyone to retrain models. The end-to-end process is explained
    in [OpenAI’s public paper](https://oreil.ly/2uFNS) titled “Training language models
    to follow instructions with human feedback,” and their official GPT model card,
    shown in [Figure 3-1](#fig_1_the_chatgpt_training_process_source).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aoas_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-1\. The ChatGPT training process (source: adapted from an image by
    [OpenAI](https://oreil.ly/9Lt-2); [Creative Commons 4.0 license](https://oreil.ly/YGKJ5))'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Additional knowledge (grounding)
  prefs: []
  type: TYPE_NORMAL
- en: 'You can provide the LLMs with some additional context or knowledge, making
    them specific to the activity scope of the developed system. This could go from
    setting the topic of discussion for a chatbot to specifying URLs that are related
    to the topics we want to include. There are different ways to implement this grounding:'
  prefs: []
  type: TYPE_NORMAL
- en: Fine tuning
  prefs: []
  type: TYPE_NORMAL
- en: Using small knowledge bases or private data to retrain the LLM with new additional
    information. Available via Azure OpenAI Service, it’s a good option to adjust
    the knowledge scope of the LLM, but a less cost-efficient option (as we will explore
    in [Chapter 5](ch05.html#operationalizing_generative_ai_implementations) when
    we calculate the cost of the Azure OpenAI–enabled implementations). In reality,
    there are very few use cases that require fine-tuning, because it updates the
    weights of the models but does not necessarily make the model more factual with
    respect to the data it was fine-tuned for. Most use cases can be achieved through
    retrieval-augmented generation (RAG).
  prefs: []
  type: TYPE_NORMAL
- en: RAG, embeddings-based retrieval
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on [Microsoft’s definition](https://oreil.ly/QlN0L), embeddings are:'
  prefs: []
  type: TYPE_NORMAL
- en: representations or encodings of tokens, such as sentences, paragraphs, or documents,
    in a high-dimensional vector space, where each dimension corresponds to a learned
    feature or attribute of the language. Embeddings are the way that the model captures
    and stores the meaning and the relationships of the language, and the way that
    the model compares and contrasts different tokens or units of language. Embeddings
    are the bridge between the discrete and the continuous, and between the symbolic
    and the numeric, aspects of language for the model.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: RAG, index-based retrieval
  prefs: []
  type: TYPE_NORMAL
- en: 'The ability to index existing files so we can locate them when interacting
    with the LLM engine. Microsoft [defines indexes](https://oreil.ly/iJSKP) as:'
  prefs: []
  type: TYPE_NORMAL
- en: crawlers that extract searchable content from data sources and populate a search
    index using field-to-field mappings between source data and a search index. This
    approach is sometimes referred to as a “pull model” because the search service
    pulls data in without you having to write any code that adds data to an index.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: RAG, hybrid search
  prefs: []
  type: TYPE_NORMAL
- en: As the result of combining grounding techniques, [hybrid search](https://oreil.ly/c2W8A)
    leverages both embedding-based retrieval in combination with index-based retrieval
    to unlock some of the most powerful techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Other grounding techniques
  prefs: []
  type: TYPE_NORMAL
- en: Other techniques include contextualization (providing information about topics
    and/or specific URLs to define a reduced knowledge scope) and live internet results
    to complement the LLM information and include external sources.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in [Figure 3-2](#fig_2_knowledge_scope_for_generative_ai), all
    these elements contribute to the creation of an extended knowledge domain from
    regular LLMs, based on internet and private data. The rest of this chapter will
    focus on different techniques to implement them with Azure OpenAI and other Microsoft
    services.
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing, any generative AI architecture or approach will depend on the knowledge
    domains and levels we require for the end solutions. If our application can rely
    on (just) the LLM, which already contains a massive amount of information, then
    we can implement the model with no additional building blocks. On the other hand,
    if we need to add specific information from other sources (including PDFs, text
    documents, websites, databases, etc.), then we will leverage the so-called fine-tuning
    and grounding techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now explore the available interfaces and tools for you to create new applications
    with Azure OpenAI Service. You will understand the key building blocks before
    moving into a step-by-step guide of the most relevant implementation approaches.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aoas_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. Knowledge scope for generative AI
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Generative AI Modeling with Azure OpenAI Service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the key adoption factors that encourages people to use Azure OpenAI is
    the availability of different visual and code-based interfaces that you can leverage
    while using the service. In this section we will explore them as well as how to
    use these interfaces depending on your generative AI implementation approach.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Initially, Microsoft released Azure OpenAI Service with “Gated General Availability,”
    meaning that any organization willing to use the service had to *complete a detailed
    application form* to explain the potential use cases and guarantee good usage
    of the platform. Microsoft’s goal was to validate that any application enabled
    by Azure OpenAI was always aligned with their [responsible AI approach](https://oreil.ly/QsuYY)
    and the [intended use](https://oreil.ly/7ojQP) of the platform. If you are getting
    started with Azure OpenAI Service, [check first if you still need to apply for
    access](https://oreil.ly/MDBhf) and prepare the required information for the [application
    form](https://oreil.ly/dp14y).
  prefs: []
  type: TYPE_NORMAL
- en: Azure OpenAI Service Building Blocks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before diving into the “how to,” let’s explore the available building blocks
    for any Azure OpenAI practitioner to prepare and deploy new solutions. Essentially,
    there are two primary components for the Azure OpenAI Service: the *visual interfaces*
    that allow users to test, customize, and deploy their generative AI models and
    the *development interfaces* that enable the exploitation and integration of those
    advanced capabilities with any application.'
  prefs: []
  type: TYPE_NORMAL
- en: Both elements are complementary and great assets for any kind of adopter, as
    they require a relatively low level of AI knowledge to make them work. For example,
    *citizen users* (hybrid technical-business profiles that are not very technical,
    but that understand the principles of generative AI and have some knowledge of
    prompt engineering, can use the visual playground, or leverage Microsoft Copilot
    Studio) and *regular developers* are great candidates for the development platforms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Visual interfaces: Azure OpenAI Studio and Playground'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As with any other [Azure AI service](https://oreil.ly/TDoRH), Azure OpenAI includes
    the notion of a “Studio” (i.e., [Azure OpenAI Studio)](https://oreil.ly/LWQO1)
    that makes the interaction with generative AI models very simple, by providing
    an intuitive UI that facilitates service deployments and leverages existing Azure
    OpenAI APIs without any code required from the user perspective.
  prefs: []
  type: TYPE_NORMAL
- en: Azure OpenAI Studio includes access to [all available models](https://oreil.ly/BI5Ue)
    (by type and geographic region), predefined prompting scenarios and examples,
    and several applications called *playgrounds*. The Azure OpenAI Playgrounds are
    different apps within Azure OpenAI Service, which include (as you can see in [Figure 3-3](#fig_3_azure_openai_studio))
    a customizable ChatGPT type of instance (*Chat*), other GPT language models for
    nonchat scenarios (*Completion*), a playground to connect AI models with your
    data (*Bring your own data*), and one for image generation applications with OpenAI’s
    DALL·E models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aoas_0303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. Azure OpenAI Studio
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You can access each playground (and their related management features) from
    the left panel of the studio, or visit them directly by following the URLs included
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chat playground](https://oreil.ly/FRU9K)'
  prefs: []
  type: TYPE_NORMAL
- en: This includes both the conversational Chat playground with the [features and
    settings](https://oreil.ly/K_fjL) required to create a private ChatGPT implementation,
    and the bring your own data (represented as one of the playgrounds in Azure OpenAI
    Studio) functionality that I will explain later in this section. The Chat playground
    (shown in [Figure 3-4](#fig_4_azure_openai_studio_chat_playground)) leverages
    the [Chat Completion API](https://oreil.ly/ZJOLp).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aoas_0304.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-4\. Azure OpenAI Studio: Chat playground'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As indicated in [Figure 3-4](#fig_4_azure_openai_studio_chat_playground), the
    main tiles and features of the Chat playground comprise the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Assistant setup
  prefs: []
  type: TYPE_NORMAL
- en: 'This area is located on the left side of the screen and allows users to configure
    the chatbot’s behavior. Users can choose from templates or create their own custom
    system messages. This section helps users define how the chatbot should act and
    respond to user queries:'
  prefs: []
  type: TYPE_NORMAL
- en: System message
  prefs: []
  type: TYPE_NORMAL
- en: A type of [meta-prompt](https://oreil.ly/OmKQO) (i.e., a prompt that sets the
    by-default context of the discussion) to guide the AI system’s behavior. It can
    be used to introduce the system, set expectations, provide feedback, or handle
    errors. One important thing to remember is that even if there is no token limit
    for this message, it will be included with every API call, so it counts against
    the overall [token limit/context length](https://oreil.ly/BI5Ue) of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs: []
  type: TYPE_NORMAL
- en: 'This area is located at the bottom-left corner of the screen. You can add examples
    to the bot intelligence, so it learns the proper way to answer specific questions.
    It’s a good option when we don’t need to fully retrain a model, for example, when
    you need to add a couple of topics from your company’s knowledge base and you
    want to define the best way to answer. From the official description: “Add examples
    to show the chat what responses you want. It will try to mimic any responses you
    add here so make sure they match the rules you laid out in the system message.”'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Chat session
  prefs: []
  type: TYPE_NORMAL
- en: This area is located in the middle of the screen and serves as the main interaction
    point between you and the chatbot. You can type your queries here and the chatbot
    will respond accordingly. The chat session allows you to test the chatbot’s performance
    and make adjustments to the assistant setup as needed, as well as import and export
    bot configurations, or get the result as a [JavaScript Object Notation (JSON)
    file](https://oreil.ly/LZJH4).
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Deploy to
  prefs: []
  type: TYPE_NORMAL
- en: This option allows you to deploy your chatbot to a specific platform or environment.
    Azure OpenAI Studio allows direct deployments to both [Azure Web Apps](https://oreil.ly/TtlXr)
    and [Microsoft Copilot Studio](https://oreil.ly/YV0SN). We will explore these
    deployment options later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Configuration
  prefs: []
  type: TYPE_NORMAL
- en: 'This area is located in the top-right corner of the screen. It provides options
    for you to access deployment and session settings. Users can also clear the chat
    history and manage parameters related to the chatbot’s deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: Deployment
  prefs: []
  type: TYPE_NORMAL
- en: 'To handle session-level configurations, such as the Azure OpenAI deployment
    resource you want to use (e.g., you may have several for different geographic
    regions), as well as the memory of the session, which will impact how many interactions
    the system can remember when getting new questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Deployment instance
  prefs: []
  type: TYPE_NORMAL
- en: You will select one option, from the resources you have [previously deployed](https://oreil.ly/-4D4f)
    (if you haven’t, you will need to create one before using Azure OpenAI Studio),
    based on the geography and model needs you may have.
  prefs: []
  type: TYPE_NORMAL
- en: Past messages included and current token count
  prefs: []
  type: TYPE_NORMAL
- en: Session-level parameters you may want to adjust for the specific test you do
    via the Chat playground. These parameters will be gone when you finish the playground
    session, except if you deploy an application (we will see the deployment options
    in a couple of sections).
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: 'This right panel includes all technical settings that will allow you to configure
    the expected output message, including the level of creativity versus determinism
    of the answer:'
  prefs: []
  type: TYPE_NORMAL
- en: Max response
  prefs: []
  type: TYPE_NORMAL
- en: This parameter helps you set a limit on the number of tokens per model response.
    The max response is measured in the number of tokens, and it is shared between
    the question (including system message, examples, message history, and prompt/user
    query) and the model’s response.
  prefs: []
  type: TYPE_NORMAL
- en: Temperature
  prefs: []
  type: TYPE_NORMAL
- en: This parameter and the Top-p parameter are direct alternatives to control the
    AI model’s randomness. Lowering the temperature means that the model will produce
    more repetitive and deterministic responses. Increasing the temperature will result
    in more unexpected or creative responses. Try adjusting temperature or Top-p,
    but not both.
  prefs: []
  type: TYPE_NORMAL
- en: '[Assistants playground](https://oreil.ly/S4KFy)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Released in 2024](https://oreil.ly/MdxvG), the Assistants playground is visually
    similar to the Chat playground, but it includes:'
  prefs: []
  type: TYPE_NORMAL
- en: The ability to handle conversation threads, by using the “thread ID” parameter
    that converts the chat discussion into a stateful application that keeps context
    and memory. You can see the details in Azure OpenAI’s [Assistants API specification](https://oreil.ly/ErRd6).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other functionalities such as the API call log, the [Code Interpreter](https://oreil.ly/3jSFV),
    and [function calling](https://oreil.ly/2R7Pz).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep in mind that this is a relatively new option, but the [official documentation](https://oreil.ly/HH4hH)
    includes the detailed steps for creation and management of assistant files. Keep
    an eye on and bookmark this URL to follow any news and technical resources.
  prefs: []
  type: TYPE_NORMAL
- en: '[Completions playground](https://oreil.ly/zJYtL)'
  prefs: []
  type: TYPE_NORMAL
- en: As we reviewed in [Chapter 1](ch01.html#introduction_to_generative_ai_and_azure_openai_ser),
    the completion skill is (along with chat and embeddings models) one of the core
    concepts for NLP and modern LLMs. Completion focuses on unitary interactions for
    all kinds of text-based requests (with no need for memory between interactions,
    as you may need for chat-based applications in which the model keeps the discussion
    context). It leverages the [Completions API](https://oreil.ly/Uczv9). As shown
    in [Figure 3-5](#fig_5_azure_openai_studio_completions_playground), the Completions
    playground allows you to type a prompt, or choose from a series of examples. It
    also includes the same kind of setting parameters that we reviewed in the Chat
    playground.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aoas_0305.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-5\. Azure OpenAI Studio: Completions playground'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can generate an answer (completion), and even regenerate it to obtain a
    totally new output. If you choose one of the examples from the drop-down menu,
    you will see an automatic prompt appear and the corresponding completion, highlighted
    as in [Figure 3-6](#fig_6_azure_openai_studio_completions_playground_examp).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aoas_0306.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-6\. Azure OpenAI Studio: Completions playground (example)'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Summarizing, you may use chat for multistep scenarios where you need to maintain
    a sequence of interactions with the AI model, while completions can be used for
    specific unitary cases. As you will see later, these two playgrounds are just
    visual interfaces that consume existing Azure OpenAI [completion](https://oreil.ly/Uczv9)
    and [chat](https://oreil.ly/ZJOLp) APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Bring your own data playground
  prefs: []
  type: TYPE_NORMAL
- en: Even if Azure OpenAI Studio shows this feature as a separate playground, it
    is technically part of the Chat playground. To access this functionally, you can
    either use the Chat playground’s Assistant setup and select the “Add your data”
    tab or go directly to the “Bring your own data” tile of the Studio ([Figure 3-7](#fig_7_azure_openai_studio_bring_your_own_data)).
    For both cases, the result will be the same.
  prefs: []
  type: TYPE_NORMAL
- en: Once you reach this point, the sequence of steps is pretty simple. As you can
    see in [Figure 3-8](#fig_8_azure_openai_studio_bring_your_own_data_source_de),
    the system will allow you to select your own sources of data, to combine their
    knowledge with the baseline LLM. That knowledge can come from PDF files, text-based
    documents, slides, web files, etc. In this case, besides the Azure OpenAI resource
    previously deployed, the bring your own data functionality will leverage other
    resources such as Azure Data Lake Gen2/Azure Storage, to save the files, and Azure
    Cognitive Search, to index the files. Azure Cognitive Search offers a vector search
    functionality based on the [Embeddings API](https://oreil.ly/imKOS)) that I will
    explain by the end of the chapter. Finally, you can always check the [official
    documentation](https://oreil.ly/z_iRM) to follow the latest updates for this Azure
    OpenAI feature, as it is a quickly evolving one due to the continuous incorporation
    of new functionalities.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aoas_0307.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-7\. Azure OpenAI Studio: Bring your own data'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![](assets/aoas_0308.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-8\. Azure OpenAI Studio: Bring your own data source details'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[DALL·E playground](https://oreil.ly/r4h7Y)'
  prefs: []
  type: TYPE_NORMAL
- en: The last playground tile provides direct access to the generative AI DALL·E
    models (versions 2 and 3) from OpenAI. This is a text-to-image model that allows
    you to create new images from just text-based descriptions. Imagine describing
    a place or a scene and getting a visual representation in the form of images that
    are freshly created on demand. This means they didn’t exist previously and that
    you can integrate this capability into your solutions and combine it with the
    rest of the language. The DALL·E playground (shown in [Figure 3-9](#fig_9_azure_openai_studio_dall_e_playground))
    leverages the [Image Generation API](https://oreil.ly/bm-7a).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aoas_0309.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-9\. Azure OpenAI Studio: DALL·E playground'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As shown in [Figure 3-9](#fig_9_azure_openai_studio_dall_e_playground), relevant
    aspects of the playground include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Playground
  prefs: []
  type: TYPE_NORMAL
- en: The DALL·E playground is visually simple—a prompt field and the results (image)
    below. It’s similar to the structure of the [Bing Create application](https://oreil.ly/YwDy-),
    but with the option to deploy the DALL·E model for your own development.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Settings
  prefs: []
  type: TYPE_NORMAL
- en: The settings panel offers you the option to choose the number of images you
    want to generate and the image size.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Album
  prefs: []
  type: TYPE_NORMAL
- en: The album section showcases all past image experiments, offering you the option
    to review previously created images, generate new ones, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the different playgrounds, you can also explore the left-side *Management*
    panel shown in [Figure 3-10](#fig_10_azure_openai_studio_management_panels), which
    include options such as deployments, models, data files, quotas, and content filters.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aoas_0310.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-10\. Azure OpenAI Studio: Management panels'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s explore the most important features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Deployments](https://oreil.ly/PGocU)'
  prefs: []
  type: TYPE_NORMAL
- en: Allows you to deploy any specific model instance [available in the geographic
    region](https://oreil.ly/XZnCX) of your Azure OpenAI resource and to visualize
    those that you previously deployed ([Figure 3-11](#fig_11_azure_openai_studio_deployments)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aoas_0311.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-11\. Azure OpenAI Studio: deployments'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Content filters](https://oreil.ly/Bpsud)'
  prefs: []
  type: TYPE_NORMAL
- en: For responsible AI moderation. Each filter from those in [Figure 3-12](#fig_12_azure_openai_studio_content_filters)
    (e.g., hate, sexual, self-harm, and violence topics for both prompts and completions,
    with different levels of filtering) can be applied to the deployments, and those
    deployments will include the content filter for each chat or completion implementation.
    We will explore this feature in [Chapter 4](ch04.html#additional_cloud_and_ai_capabilities),
    as part of the responsible AI measures for generative AI implementations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aoas_0312.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-12\. Azure OpenAI Studio: content filters'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Models](https://oreil.ly/oC3Hj)'
  prefs: []
  type: TYPE_NORMAL
- en: This option shows the [available Azure OpenAI models](https://oreil.ly/XZnCX),
    related to the specific geographic region of the chosen deployment.
  prefs: []
  type: TYPE_NORMAL
- en: '[Data files](https://oreil.ly/2TZwW)'
  prefs: []
  type: TYPE_NORMAL
- en: This file management feature allows you to [prepare the dataset for fine-tuned
    implementations](https://oreil.ly/FDMr1). We will explore more about fine-tuning
    later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '[Quotas](https://oreil.ly/ONn5Q)'
  prefs: []
  type: TYPE_NORMAL
- en: The quota panel shows the [usage quotas](https://oreil.ly/bEN4D) related to
    different models and geographic regions. It also helps you [request a quota increase](https://oreil.ly/iiysu)
    if you need more. Alternatively, and I will explain this in [Chapter 6](ch06.html#elaborating_generative_ai_business_cases)
    as part of the pricing and estimation exercise, you have an option to hire dedicated
    capacity, by leveraging the so-called [provisioned throughput units (PTU) for
    Azure OpenAI](https://oreil.ly/KCC6K), which are reserved instances with performance
    and service availability benefits.
  prefs: []
  type: TYPE_NORMAL
- en: We will explore some of these functionalities later in this chapter and in [Chapter 4](ch04.html#additional_cloud_and_ai_capabilities),
    as they will all be relevant, depending on the type of Azure OpenAI implementation
    you plan to utilize. Now, let’s see what you can do to deploy these models via
    Azure OpenAI Studio.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deployment interfaces: Web apps and Microsoft Copilot agents'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As mentioned in this chapter, the Chat playground includes some easy-to-use
    deployment options. They are not available for the rest of the playgrounds, but
    they can simplify the preliminary deployment of Azure OpenAI models for internal
    testing and use purposes, without any coding required. These no-code deployments
    can incorporate the specific knowledge from the bring your own data functionality.
    There are two possibilities:'
  prefs: []
  type: TYPE_NORMAL
- en: Web apps with [Azure App Service](https://oreil.ly/moBFz)
  prefs: []
  type: TYPE_NORMAL
- en: The first available deployment option, which you can use with or without the
    “bring your own data” feature activated. As we discussed in [Chapter 2](ch02.html#designing_cloud_native_architectures_for_generativ),
    App Service is the Azure option to deploy native web apps; it allows integrations
    with both external and internal systems and web development with a variety of
    programming languages. From Azure OpenAI Studio and its Chat playground, you can
    simply “Deploy to” and then configure your deployment (see [Figure 3-13](#fig_13_azure_openai_studio_web_app_deployment)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aoas_0313.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-13\. Azure OpenAI Studio: web app deployment'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As shown in [Figure 3-13](#fig_13_azure_openai_studio_web_app_deployment),
    configuration options include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the web app
  prefs: []
  type: TYPE_NORMAL
- en: You can create a new App Service resource directly from this feature (in that
    case, you will need to define the “app name” that will be part of your web app
    URL), or choose an existing one if you have previously deployed via [Azure portal’s
    App Service panel](https://oreil.ly/dPLy2).
  prefs: []
  type: TYPE_NORMAL
- en: Pricing plan
  prefs: []
  type: TYPE_NORMAL
- en: To select the preferred [pricing tier](https://oreil.ly/IdDXQ) for the web app.
  prefs: []
  type: TYPE_NORMAL
- en: Chat history
  prefs: []
  type: TYPE_NORMAL
- en: A functionality that allows the web app users to recover their [previous interactions](https://oreil.ly/-yyQg)
    with chat. It relies on [Cosmos DB (Azure’s NoSQL database)](https://oreil.ly/-yyQg),
    which obviously adds cost to the existing Azure OpenAI and App Service resources.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have selected all these options, you can click on Deploy. You will
    need to wait around 10 minutes for all the resources to be deployed, then you
    will be able to launch your web app from the studio, or by typing the URL *https://<appname>.azurewebsites.net**.*The
    look and feel will be something like the interface you see in [Figure 3-14](#fig_14_azure_openai_studio_web_app_interface).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aoas_0314.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-14\. Azure OpenAI Studio: web app interface'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The UI of the new app will contain a regular chatbot setup, with options to
    share and check previous discussions on the top-right side of the window. You
    can also [customize the visual aspect of the application](https://oreil.ly/BVUkG)
    by using the [official source code](https://oreil.ly/MeBin), and deploy it programmatically,
    with Azure App Service and using your preferred programming language, instead
    of leveraging Azure OpenAI Studio.
  prefs: []
  type: TYPE_NORMAL
- en: Bots with [Microsoft Copilot Studio (formerly Power Virtual Agents [PVAs])](https://oreil.ly/YV0SN)
  prefs: []
  type: TYPE_NORMAL
- en: This option is available for Chat playground implementations that include the
    “bring your own data” feature. That means that if you don’t add extended knowledge
    from PDFs or other documents, the Chat playground won’t include Microsoft Copilot
    Studio/PVA as a deployment option in the top-right corner in [Figure 3-15](#fig_15_azure_openai_studio_copilot_deployment).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aoas_0315.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-15\. Azure OpenAI Studio: Copilot deployment'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: How to handle PVAs is outside of the scope of this book, but you can explore
    the [detailed instructions from the official documentation](https://oreil.ly/Qi9J3)
    that show how to use PVAs with Azure OpenAI for the *generative answers* feature.
    This option is available for only certain geographic regions, so you will need
    to validate if your deployments with Azure OpenAI models show the PVA deployment
    option in the Chat playground. If this is not the case, you may want to deploy
    new models in other regions.
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing, these visual interfaces can help you leverage Azure OpenAI models
    in a simple manner. They provide an intuitive way to launch the Azure OpenAI APIs
    in just a few clicks. However, you will need code-based tools to implement the
    other advanced architectures you will see later in this chapter. Let’s now explore
    those APIs and other development kits so you can leverage everything that Azure
    OpenAI Service has to offer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Development interfaces: APIs and SDKs'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to all the previously explored interfaces, one of the key enablers
    for integrating Azure OpenAI with existing or new applications is the ability
    to consume the preconfigured models as regular endpoints. From a development point
    of view, we can call those models by using the APIs and related software development
    kits (SDKs) and pass any input and configuration parameters within the code. This
    section covers the main pieces you need to know—the *Azure OpenAI Service REST
    APIs*, including the [official API reference documentation](https://oreil.ly/qH3FL),
    with specific details for chat, completions, embeddings, and other deployments.
    There is also an [official repo](https://oreil.ly/mbA1v) with the full specifications.
    There are general APIs that will help you with the configuration and deployment
    of Azure OpenAI services, while the service APIs help you consume the models to
    bring the AI capabilities to your generative AI applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main APIs you need to know and their high-level call details are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[General management APIs](https://oreil.ly/xkqqk)'
  prefs: []
  type: TYPE_NORMAL
- en: For Azure AI service account management (including Azure OpenAI), with tasks
    such as account creation, deletion, listing, etc.
  prefs: []
  type: TYPE_NORMAL
- en: '[APIs for model-related information](https://oreil.ly/Y7VMR)'
  prefs: []
  type: TYPE_NORMAL
- en: To obtain the list of available Azure OpenAI models and information about their
    specific capabilities and the model lifecycle (including potential deprecation
    details).
  prefs: []
  type: TYPE_NORMAL
- en: '[Completions](https://oreil.ly/Uczv9)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The required API for nonchat language scenarios. This and other APIs are versioned
    by using the “YYYY-MM-DD” date structure for `api-version`, and you will need
    to copy the resource name and deployment-ID from the Azure OpenAI model you previously
    deployed (remember the step-by-step process from the Azure portal, in [Chapter 2](ch02.html#designing_cloud_native_architectures_for_generativ)).
    To create a completion resource, the POST operation is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The request and response dynamic follows this structure, with the prompt parameter
    the input for the model to generate a specific completion, and a series of [optional
    parameters](https://oreil.ly/Uczv9) such as `max_tokens` (the limit of tokens
    for the expected answer) or the number `n` of expected completions/answers.
  prefs: []
  type: TYPE_NORMAL
- en: '*Request*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '*Response*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2][PRE3][PRE4]``py[PRE5]`py` `"model"``:` `"gpt-35-turbo-instruct"``,`
    [PRE6] `{` [PRE7]`` `"text"``:` `", is eating burgers with a milkshake"``,` [PRE8]`
    `"index"``:` `0``,` [PRE9] `"logprobs"``:` `null``,` [PRE10]`py [PRE11]py`` [PRE12]py[PRE13][PRE14][PRE15][PRE16]py[PRE17]py[PRE18]``py[PRE19]py[PRE20]
    { `"messages"``:` `[` [PRE21][PRE22]` `{` [PRE23][PRE24] `"role"``:` `"system"``,`
    [PRE25][PRE26]``py[PRE27]`py` `{` [PRE28] `"content"``:` `"Who wrote ''Romeo and
    Juliet''?"` [PRE29]`` `},` [PRE30]` `{` [PRE31] `"role"``:` `"assistant"``,` [PRE32]`py
    [PRE33]py`` [PRE34]py[PRE35][PRE36][PRE37][PRE38]py[PRE39]py` [PRE40]`py`` [PRE41]py[PRE42]``
    [PRE43] {"prompt": "<prompt text>", "completion": "<ideal generated text>"} `{``"prompt"``:`
    `"<prompt text>"``,` `"completion"``:` `"<ideal generated text>"``}` [PRE44]`
    [PRE45][PRE46] `` `### Embedding-based grounding    As you now know from earlier
    chapters, embeddings are mathematical representations of text-based information
    as vectors in a vector space, and an alternative and/or complement to the traditional
    index-based approach. These embeddings are stored and managed as mathematical
    vectors that represent distances between topics. This means if we are looking
    for information about animals and we have a vectorized knowledge base that includes
    animal-related topics, we can recover the Top-k answers (i.e., the most relevant
    “K” number of pieces of information).    You can use the Azure OpenAI embeddings
    API to generate vector representations of text that capture the semantic meaning
    and similarity of the text. Some possible use cases for embeddings are document
    search, text classification, clustering, or text similarity.    The end-to-end
    process to create and use an embedding-based system is aligned with what you have
    seen thus far in this chapter. From an Azure OpenAI perspective, the steps are
    as follows:    1.  *Select the knowledge base* that contains the information that
    will complement the baseline LLM knowledge domain. This may include PDF, DOC,
    PPT, TXT, and other file formats. In Azure, you may store that information via
    Azure Blob Storage or Azure Data Lake Gen2\. Keep in mind that if your files are
    similar to any general information that may be available on the internet (for
    example, public descriptions of industry concepts), you probably don’t need to
    ground them. However, if you have very specific files with information on how
    to answer questions, or perform internal tasks, those may be good candidates for
    embeddings generation.           2.  *Choose and deploy your database/vector store*.
    By the end of this chapter, you will see all available options for implementation
    in Azure with Azure OpenAI–generated embeddings.           3.  *Prepare the input
    dataset*. This includes two different steps:               1.  *Extract the information*
    from your documents. For example, you can use Azure Document Intelligence/Form
    Recognizer to extract text from your PDFs with the OCR feature. You can also use
    other non-Azure tools.                       2.  *Split the information*. For
    this to work, it is important to keep in mind the [embeddings model token limit](https://oreil.ly/SQSGw)
    (e.g., 8K for Ada model version 2) to prepare the input without exceeding the
    limit (you can use [OpenAI’s tokenizer tool](https://oreil.ly/DDQHG) to understand
    the extent of what 8K means in terms of document length). This means you will
    need to make one API call for each of the limited-size blocks you have prepared
    before, or leverage [chunking techniques](https://oreil.ly/3DHfa) to split and
    handle larger documents.                   4.  *Leverage the Azure OpenAI [embeddings
    models](https://oreil.ly/gvAHr)*. Use the API operations you saw earlier in this
    chapter and get the mathematical vectors from the API response. *Store the vectors*
    within the chosen vector store.           5.  Any time you want to find information
    from your knowledge base, or if you want to leverage it from any chat or search
    application, you will need to *generate the embeddings of the question itself*,
    then perform the search against the vector search. Keep in mind that you will
    need to leverage the same model (e.g., Ada version 2) for both your knowledge
    base and the question. You can send the result of the search, with the Top-k results,
    to the chat or search application, directly or by including it as content for
    the answer of the completion.              This process is similar for other embeddings
    and conversation models (for example, those that are available via Azure AI Studio’s
    model catalog and Hugging Face), and the high-level architecture includes the
    elements you can see in [Figure 3-21](#fig_21_embedding_based_grounding_architecture):
    basically, the baseline Azure OpenAI model gets complemented with the internal
    knowledge base that contains PDFs, Word docs, etc. Instead of retraining/fine-tuning
    the model, we just combine it with that knowledge base so it can find similarities
    between the users’ questions and the information contained within the data sources.  ![](assets/aoas_0321.png)  ######
    Figure 3-21\. Embedding-based grounding architecture    You can find more information
    and code examples on [how to create embeddings](https://oreil.ly/8Duc8) from the
    official Microsoft documentation (in addition to the API definitions we covered
    earlier in this chapter).    Additionally, there is [one official Microsoft accelerator](https://oreil.ly/iG5UU)
    for this type of implementation that you can leverage during the development phase.
    There are several deployment and storage options. Feel free to explore the code
    to see the API call details.    ### Document indexing/retrieval-based grounding    The
    document indexing/retrieval-based grounding approach is an alternative to the
    embedding-based approach. In this case, we will not generate mathematical vectors.
    Instead, we will generate indexes of specific documents, so Azure OpenAI Service
    can find the information from those sources and include it as part of its answers.
    For that purpose, we will also use Azure Cognitive Search, which is a service
    that allows you to index, understand, and retrieve relevant data from a knowledge
    base or a collection of documents.    The combination of both services enables
    powerful chatbot applications that can communicate with users in natural language
    and provide intuitive and personalized interactions, based on specific data from
    the organization. Much like the embedding-based approach, there is an official
    [Microsoft accelerator](https://oreil.ly/JNWAz) available for you to deploy your
    first proof of concept, in addition to a second one called [GPT-RAG](https://oreil.ly/Q5NK9)
    from the Microsoft Argentina team, with some additional functionalities for bigger
    implementations. You can explore both to see updated details and implementation
    approaches with Azure OpenAI and Azure Cognitive Search. You can also see the
    high-level architecture of the key building blocks in [Figure 3-22](#fig_22_retrieval_based_grounding_architecture).  ![](assets/aoas_0322.png)  ######
    Figure 3-22\. Retrieval-based grounding architecture    The main difference when
    compared to the embedding-based approach is that instead of generating embeddings
    for both the knowledge base and the user question, you will just perform a search
    against the Azure AI Search engine (or any equivalent, as we will explore in [Chapter 4](ch04.html#additional_cloud_and_ai_capabilities)
    for vector databases).    You may see this option as something a bit simpler than
    the embeddings approach, and a better fit for applications where you need to find
    the source of information (and even provide a link to the original document as
    part of the answer); embeddings can potentially handle bigger datasets and deliver
    better performance. However, it really depends on the specific dataset and its
    knowledge scope and file format as well as the envisioned use case, so my recommendation
    is for you to try both options and evaluate the one that delivers best results
    from a user perspective.    ### Hybrid search–based grounding    There are newer
    implementation approaches based on [hybrid search techniques](https://oreil.ly/mwZPy).
    Concretely, hybrid search combines vector embeddings and doc retrieval capabilities.
    The [hybrid search feature](https://oreil.ly/c2W8A) from Azure AI Search offers
    that combination, plus a [reranking technique](https://oreil.ly/S7b8p) that produces
    the final result, with better performance than the previously mentioned grounding
    techniques. Now, let’s explore some additional grounding options that can add
    more knowledge scope to your generative AI applications.    ### Other grounding
    techniques    We have explored several fine-tuning and grounding techniques, mainly
    based on text information from different sources. But what happens if you want
    to leverage other kinds of data? Or if the required information can be found only
    via live internet results? Here are some other grounding techniques you may want
    to explore:    LLM + web results      This approach relies on the [Bing Web Search
    API](https://oreil.ly/qud-9) to extend the knowledge scope of Azure OpenAI Service
    models. As you may know, all LLMs are based on training datasets that go up to
    a specific date (e.g., initial Azure OpenAI models were updated with data up to
    2021). If you need updated information, you can use the Bing Web Search API to
    find web pages, images, videos, news, etc., or use it to create a custom search
    instance that filters web results based on the criteria. The result from the API
    can then be used by Azure OpenAI to return an answer based on that information.      LLM
    + tabular data and/or databases      Similar to other sources, tabular data (e.g.,
    Excel and CSV files) and regular SQL-type databases (e.g., SQL Server, Azure SQL,
    PostgreSQL) can be good grounding sources. You can develop what the industry calls
    Database Copilots to allow end users to query information without any complex
    SQL syntax, just natural language–based prompts. Or you can leverage it for [other
    data exploration](https://oreil.ly/s3snz) topics, such as exploratory data analysis
    or root-case analysis.      Just as with the other previous grounding options,
    there is an [official Microsoft accelerator](https://oreil.ly/eFneC) that combines
    these grounding techniques, with specific code samples and updated implementations.    At
    the end of the day, each implementation approach (baseline, fine-tuned, or grounding
    based) serves a different purpose, but the next section is a summarized guide
    for you to understand the pros and cons of each one, so you can make the most
    informed decision and create your generative AI applications with Azure OpenAI
    with the best balance of performance, cost, and technical complexity.` `` [PRE47]
    `` `## Approach Comparison and Final Recommendation    There is not a single right
    answer to the question, “Which approach should I use for my generative AI implementation?”
    It really depends on the use case, type and volume of available data, existing
    IT architectures, available budget and resources, etc. Again, there is no right
    answer, and the choice relies for now on experimentation and performance testing.    [Table 3-1](#table-3-1)
    shows the general pros and cons of the implementation approaches.      Table 3-1\.
    Comparison of implementation approaches with Azure OpenAI Service   |  | Approach
    | Pros | Cons | | --- | --- | --- | --- | | 1 | Basic ChatGPT-type instance (vanilla,
    private) |   *   Relatively simple and quick to deploy *   Good option for internal
    (employee) use cases *   Available via Azure OpenAI’s visual playground *   Option
    to define the topic scope based on URLs, by leveraging the system message   |   *   Lack
    of updated data *   Very limited for client-side applications *   Higher risk
    of model hallucination   | | 2 | Examples with one-shot/few-shot learning |   *   Easy
    to implement *   Good option to adapt system behavior based on specific pieces
    of knowledge from your company *   Available via Azure OpenAI’s visual playground   |   *   Lack
    of updated data *   Very limited for client-side applications *   Higher risk
    of model hallucination   | | 3 | Fine-tuning |   *   Good to fine-tune an existing
    model with specific company data *   Leverages mature product features   |   *   Complex
    to prepare input data for both fine-tuning and few-shot learning *   Increased
    cost for fine-tuned models   | | 4 | Embedding-based grounding (vectors with Azure
    AI Search) |   *   Great for customization without requiring fine-tuning *   Good
    fit for large amounts of data *   Easy use of embeddings APIs   |   *   Requires
    preparation of the input data based on token limits *   Need to scan files via
    OCR to extract content first *   Initial embeddings generation cost for custom
    data (depending on the data scope)   | | 5 | Retrieval-based grounding (indexing
    with Azure AI Search, no embeddings) |   *   Good option for information retrieval
    from existing files *   Indexing allows for citing sources (good for explainability)
    *   Option to use the “add your own data” option from the Playground, for small
    implementations   |   *   Potentially less performant than embeddings for large
    amounts of private data (to be confirmed during your preliminary experimentation)   |
    | 6 | Hybrid search |   *   More performant thanks to the combination of indexing,
    embeddings, and reranking of model results *   Relatively feasible via Azure OpenAI
    Playground   |   *   Complex, but for Azure OpenAI, no more than the regular embedding-based
    RAG   | | 7 | Other grounding techniques (Bing Search, databases, etc.) |   *   Great
    to add live results to the LLM, and to explore internal sources such as databases
    and tabular files *   Updated results with no need to retrain or adjust the model   |   *   A
    bit more complex (requires orchestration engines such as LangChain or Semantic
    Kernel) *   Less documentation available for this kind of implementation   |    These
    implementation approaches have different advantages and levels of complexity.
    One of the key aspects is the ability to evaluate how well they perform, and how
    good these Azure OpenAI models are for specific questions and tasks. Let’s explore
    all of this in the next section.    ## AI Performance Evaluation Methods    One
    of the key stages of any generative AI project is model performance evaluation.
    However, it is not a simple task to evaluate the performance of LLM-enabled systems,
    and it is not fully standardized yet. That said, you can start evaluating metrics
    with Azure OpenAI and Azure AI Studio, as you will see in [Chapter 5](ch05.html#operationalizing_generative_ai_implementations)
    with LLMOps and prompt flow for evals.    Here is a selection of the most important
    metrics for generative AI evaluation:    Groundedness      Groundedness refers
    to how well a generative AI’s responses are based on the information given or
    available in the input. This is a good metric to analyze how AI sticks to the
    facts, in order to avoid hallucinations. You can explore the new [Groundedness
    Detection feature](https://oreil.ly/Lk4ZI) from the AI Content Safety Studio.      Similarity      This
    metric measures how much a GPT output resembles that of a human one. This is useful
    for human validation of the results from Azure OpenAI models.      Relevance      It
    measures how connected an AI’s output is to the input given. It’s like checking
    if someone’s answer in a conversation is related to the question you asked.      Classification
    accuracy      A metric for classification tasks, between 0 and 1, that measures
    the output of the AI model compared to a ground truth.      Levenshtein distance      This
    measures how many changes, such as adding, deleting, or changing pieces, you would
    need to make to get from the AI’s output to the expected output.      Coherence      This
    checks if the AI’s output makes sense and follows a logical order, like checking
    if a story has a beginning, middle, and end, and doesn’t jump around randomly.      Fluency      This
    measures how smoothly the AI’s output reads, by checking if a written paragraph
    is easy to read and understand, from a linguistics and grammar point of view.      F1
    score      This is a balance between the words in the model answer and the ground
    truth.      Other metrics      Other metrics from traditional NLP.      From an
    Azure perspective, you can explore the available metrics for evaluation via [Azure
    AI Studio](https://oreil.ly/q2S7r) and [Azure Databricks with MLFlow](https://oreil.ly/3kONX).
    Here are several ongoing initiatives from some of the main industry actors (including
    Microsoft and OpenAI), but you can expect more news and tools in the upcoming
    months and years:    *   Microsoft’s [LLM evaluation framework](https://oreil.ly/H6gB8)           *   Microsoft’s
    [evaluation flows (Azure AI Studio)](https://oreil.ly/4NrIz)           *   Microsoft’s
    documentation for [LLM metrics monitoring](https://oreil.ly/VtjxD)           *   OpenAI’s
    [Evals project](https://oreil.ly/NgdLZ)              Additionally, there are other
    families of metrics that you can use to measure and analyze performance:    Positive/negative
    review of answers      A manual way to both track performance and potentially
    reeducate the model with weighted reconfigurations (e.g., few-shot learning with
    the good answers). You could enable this by using a positive/negative sign in
    the UI and by adding a binary numeric value at the database level if you decide
    to store the questions and answers for review purposes (e.g., ID, question, answer,
    review) in a JSON file stored via Cosmos DB. For this purpose, my recommendation
    is to create a set of test questions, and to involve subject-matter experts during
    the creation of that set and during the evaluation of the system.      Traditional
    product analytics metrics      For example, session time, amount of re-questioning
    to get the best answer, overall product rating, etc. This would require tools
    such as [Microsoft Clarity](https://oreil.ly/2RHm1), Pendo, Amplitude, Mixpanel,
    etc., connected to the cloud native app (e.g., iOS, Android, web, etc.). Alternatively,
    there are cloud native features such as [Azure App Insights](https://oreil.ly/HXkzL)
    that can be deployed as part of the generative AI app monitoring system. Additionally,
    these tools can be leveraged to track performance for A/B testing experiments
    (for example, if we launch two different versions of the AI model with different
    user sets).` `` [PRE48][PRE49][PRE50]``py[PRE51]py[PRE52]py`  [PRE53]'
  prefs: []
  type: TYPE_NORMAL
