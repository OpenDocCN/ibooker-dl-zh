- en: Chapter 3\. Implementing Cloud Native Generative AI with Azure OpenAI Service
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 3 章：使用 Azure OpenAI 服务实现云原生生成式 AI
- en: This chapter will focus on the implementation of generative AI architectures
    with Microsoft Azure and Azure OpenAI models, always aiming to present all available
    options, and minimize the required development, integration, and usage cost, while
    accelerating the operationalization. For that purpose, I’ve included a series
    of best practices and typical architectures that will allow you to choose the
    best building blocks for your specific scenarios.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将专注于使用 Microsoft Azure 和 Azure OpenAI 模型实现生成式 AI 架构，始终旨在展示所有可用选项，并最小化所需的开发、集成和使用成本，同时加速运营化。为此，我包含了一系列最佳实践和典型架构，这将允许您为您的特定场景选择最佳构建块。
- en: We will include the most relevant Azure OpenAI implementation approaches, based
    on existing features and repositories that will continue evolving, improving,
    and including new functionalities. I’ve included URLs to the original documentation
    because they are continuously updated with new features, so these links will allow
    you to explore any details you need. Most of them rely on official accelerators
    from GitHub repositories, and projects that you will be able to follow and/or
    fork. But before getting into the details, let’s explore some fundamental topics
    that will help you understand the full extent of what a generative AI with Azure
    OpenAI Service means.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将包括最相关的 Azure OpenAI 实现方法，基于现有功能库和将不断进化、改进并包含新功能的存储库。我已包含原始文档的 URL，因为它们会持续更新以包含新功能，所以这些链接将允许您探索您需要的任何细节。其中大部分依赖于来自
    GitHub 存储库的官方加速器，以及您可以跟踪和/或分叉的项目。但在深入细节之前，让我们探索一些基本主题，这将帮助您理解 Azure OpenAI 服务中的生成式
    AI 的全部含义。
- en: Defining the Knowledge Scope of Azure OpenAI Service–Enabled Apps
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义 Azure OpenAI 服务启用应用的知识范围
- en: Generative AI applications on Microsoft Azure are not only for regular ChatGPT-type
    applications. They are advanced architectures that rely on diverse technology
    pieces, including the core infrastructure (servers, [GPUs](https://oreil.ly/y5mXm),
    etc.) required to run generative AI models, and that allow adopters to create
    conversational applications and search engines, develop and integrate new AI copilots
    into their applications, customize customer attention, etc.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Microsoft Azure 上的生成式 AI 应用不仅限于常规的 ChatGPT 类型应用。它们是依赖于多种技术组件的先进架构，包括运行生成式
    AI 模型所需的核心基础设施（服务器、[GPU](https://oreil.ly/y5mXm) 等），并允许采用者创建对话应用和搜索引擎，开发并将新的 AI
    协作者集成到他们的应用中，定制客户关注点等。
- en: 'From an Azure OpenAI point of view, we are talking about a managed service
    that includes advanced functionalities that will allow you to implement *different
    levels of knowledge*, depending on the desired scope of your applications, and
    based on default capabilities and specific adjustment and customization techniques.
    By levels of knowledge, we mean something that goes beyond the initial scope of
    the LLM and its massive dataset (e.g., adding new information for an internal
    company application, based on its own data). Some of the options to adjust that
    knowledge include the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Azure OpenAI 的角度来看，我们谈论的是一个包含高级功能的管理服务，它将允许您根据应用所需的功能范围实现不同级别的知识，基于默认功能和特定的调整和定制技术。通过知识级别，我们指的是超越
    LLM 及其海量数据集初始范围的东西（例如，基于自身数据为内部公司应用添加新信息）。调整该知识的某些选项包括以下内容：
- en: Baseline LLM
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 基线 LLM
- en: Azure OpenAI’s language models are trained on enormous datasets containing billions
    of words. These datasets are carefully curated to include a wide range of topics,
    genres, and writing styles. The size and diversity of the training data helps
    the models develop a broad understanding of human language. The specific details
    of the training data have not been disclosed, but it includes text data from a
    variety of sources, including books, articles, websites, and other publicly available
    written material. Additionally, the training process (RLHF) includes human reviewers
    who help annotate and curate the data, flagging and addressing potential biases
    or problematic content. Feedback loops with reviewers are established to continuously
    improve and refine the models. One of the key advantages of the enterprise-grade
    Azure OpenAI service is that [your data is only yours](https://oreil.ly/qA5Ok)
    and is not used by anyone to retrain models. The end-to-end process is explained
    in [OpenAI’s public paper](https://oreil.ly/2uFNS) titled “Training language models
    to follow instructions with human feedback,” and their official GPT model card,
    shown in [Figure 3-1](#fig_1_the_chatgpt_training_process_source).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Azure OpenAI的语言模型是在包含数十亿单词的巨大数据集上训练的。这些数据集经过精心策划，包括广泛的主题、流派和写作风格。训练数据的大小和多样性有助于模型发展对人类语言的广泛理解。训练数据的具体细节尚未公开，但它包括来自各种来源的文本数据，包括书籍、文章、网站和其他公开可用的书面材料。此外，训练过程（RLHF）包括人类审稿人，他们帮助标注和策划数据，标记和解决潜在的偏见或问题内容。与审稿人的反馈循环被建立起来，以持续改进和细化模型。企业级Azure
    OpenAI服务的一个关键优势是[您的数据仅属于您](https://oreil.ly/qA5Ok)且不会被任何人用于重新训练模型。端到端过程在[OpenAI的公开论文](https://oreil.ly/2uFNS)中解释，标题为“通过人类反馈训练语言模型以遵循指令”，以及他们官方的GPT模型卡，如图[图3-1](#fig_1_the_chatgpt_training_process_source)所示。
- en: '![](assets/aoas_0301.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aoas_0301.png)'
- en: 'Figure 3-1\. The ChatGPT training process (source: adapted from an image by
    [OpenAI](https://oreil.ly/9Lt-2); [Creative Commons 4.0 license](https://oreil.ly/YGKJ5))'
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-1. ChatGPT训练过程（来源：改编自[OpenAI](https://oreil.ly/9Lt-2)的一张图片；[Creative Commons
    4.0许可](https://oreil.ly/YGKJ5))
- en: Additional knowledge (grounding)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 额外知识（基础化）
- en: 'You can provide the LLMs with some additional context or knowledge, making
    them specific to the activity scope of the developed system. This could go from
    setting the topic of discussion for a chatbot to specifying URLs that are related
    to the topics we want to include. There are different ways to implement this grounding:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以为LLMs提供一些额外的上下文或知识，使它们特定于开发系统的活动范围。这可以从为聊天机器人设定讨论主题到指定与我们想要包含的主题相关的URL。实现这种基础化的方法有多种：
- en: Fine tuning
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 微调
- en: Using small knowledge bases or private data to retrain the LLM with new additional
    information. Available via Azure OpenAI Service, it’s a good option to adjust
    the knowledge scope of the LLM, but a less cost-efficient option (as we will explore
    in [Chapter 5](ch05.html#operationalizing_generative_ai_implementations) when
    we calculate the cost of the Azure OpenAI–enabled implementations). In reality,
    there are very few use cases that require fine-tuning, because it updates the
    weights of the models but does not necessarily make the model more factual with
    respect to the data it was fine-tuned for. Most use cases can be achieved through
    retrieval-augmented generation (RAG).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 使用小型知识库或私有数据，用新的额外信息重新训练LLM。通过Azure OpenAI服务提供，这是一个调整LLM知识范围的好选择，但成本效益较低（我们将在[第5章](ch05.html#operationalizing_generative_ai_implementations)中探讨，当我们计算Azure
    OpenAI启用实现的成本时）。实际上，需要微调的使用案例非常少，因为它更新了模型的权重，但并不一定使模型在微调的数据上更符合事实。大多数使用案例可以通过检索增强生成（RAG）来实现。
- en: RAG, embeddings-based retrieval
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: RAG，基于嵌入的检索
- en: 'Based on [Microsoft’s definition](https://oreil.ly/QlN0L), embeddings are:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[Microsoft的定义](https://oreil.ly/QlN0L)，嵌入是：
- en: representations or encodings of tokens, such as sentences, paragraphs, or documents,
    in a high-dimensional vector space, where each dimension corresponds to a learned
    feature or attribute of the language. Embeddings are the way that the model captures
    and stores the meaning and the relationships of the language, and the way that
    the model compares and contrasts different tokens or units of language. Embeddings
    are the bridge between the discrete and the continuous, and between the symbolic
    and the numeric, aspects of language for the model.
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 将标记（如句子、段落或文档）表示或编码为高维向量空间中的表示，其中每个维度对应于语言学习到的特征或属性。嵌入是模型捕捉和存储语言意义及其关系的方式，以及模型比较和对比不同标记或语言单位的方式。嵌入是模型在语言的离散和连续、符号和数值方面之间的桥梁。
- en: RAG, index-based retrieval
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: RAG，基于索引的检索
- en: 'The ability to index existing files so we can locate them when interacting
    with the LLM engine. Microsoft [defines indexes](https://oreil.ly/iJSKP) as:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 能够索引现有文件，以便我们在与 LLM 引擎交互时可以找到它们。Microsoft [定义索引](https://oreil.ly/iJSKP)为：
- en: crawlers that extract searchable content from data sources and populate a search
    index using field-to-field mappings between source data and a search index. This
    approach is sometimes referred to as a “pull model” because the search service
    pulls data in without you having to write any code that adds data to an index.
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 从数据源中提取可搜索内容并使用源数据与搜索索引之间的字段映射来填充搜索索引的爬虫。这种方法有时被称为“拉模型”，因为搜索服务在不需要你编写任何将数据添加到索引的代码的情况下拉取数据。
- en: RAG, hybrid search
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: RAG，混合搜索
- en: As the result of combining grounding techniques, [hybrid search](https://oreil.ly/c2W8A)
    leverages both embedding-based retrieval in combination with index-based retrieval
    to unlock some of the most powerful techniques.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 作为结合基础技术的结果，[混合搜索](https://oreil.ly/c2W8A)结合了基于嵌入的检索和基于索引的检索，以解锁一些最强大的技术。
- en: Other grounding techniques
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 其他基础技术
- en: Other techniques include contextualization (providing information about topics
    and/or specific URLs to define a reduced knowledge scope) and live internet results
    to complement the LLM information and include external sources.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 其他技术包括上下文化（提供有关主题和/或特定 URL 的信息以定义一个减少的知识范围）和实时互联网结果，以补充 LLM 信息并包含外部来源。
- en: As you can see in [Figure 3-2](#fig_2_knowledge_scope_for_generative_ai), all
    these elements contribute to the creation of an extended knowledge domain from
    regular LLMs, based on internet and private data. The rest of this chapter will
    focus on different techniques to implement them with Azure OpenAI and other Microsoft
    services.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在[图 3-2](#fig_2_knowledge_scope_for_generative_ai)中看到的，所有这些元素都为从常规 LLM 基于互联网和私有数据创建扩展知识领域做出了贡献。本章的其余部分将专注于使用
    Azure OpenAI 和其他 Microsoft 服务实现这些技术的不同方法。
- en: Summarizing, any generative AI architecture or approach will depend on the knowledge
    domains and levels we require for the end solutions. If our application can rely
    on (just) the LLM, which already contains a massive amount of information, then
    we can implement the model with no additional building blocks. On the other hand,
    if we need to add specific information from other sources (including PDFs, text
    documents, websites, databases, etc.), then we will leverage the so-called fine-tuning
    and grounding techniques.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，任何生成式人工智能架构或方法都将取决于我们为最终解决方案所需的知识领域和水平。如果我们的应用程序可以依赖于（仅）LLM，它已经包含大量信息，那么我们可以不添加任何额外的构建块来实施模型。另一方面，如果我们需要从其他来源添加特定信息（包括
    PDF、文本文档、网站、数据库等），那么我们将利用所谓的微调和基础技术。
- en: Let’s now explore the available interfaces and tools for you to create new applications
    with Azure OpenAI Service. You will understand the key building blocks before
    moving into a step-by-step guide of the most relevant implementation approaches.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来探索可用于使用 Azure OpenAI 服务创建新应用的可用接口和工具。在进入最相关实现方法的逐步指南之前，你将了解关键构建块。
- en: '![](assets/aoas_0302.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aoas_0302.png)'
- en: Figure 3-2\. Knowledge scope for generative AI
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-2\. 生成式人工智能的知识范围
- en: Generative AI Modeling with Azure OpenAI Service
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Azure OpenAI 服务进行生成式人工智能建模
- en: One of the key adoption factors that encourages people to use Azure OpenAI is
    the availability of different visual and code-based interfaces that you can leverage
    while using the service. In this section we will explore them as well as how to
    use these interfaces depending on your generative AI implementation approach.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓励人们使用 Azure OpenAI 的关键采用因素之一是服务提供的不同视觉和基于代码的界面，您可以在使用服务时利用这些界面。在本节中，我们将探讨这些界面以及根据您的生成式
    AI 实施方法如何使用这些界面。
- en: Warning
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Initially, Microsoft released Azure OpenAI Service with “Gated General Availability,”
    meaning that any organization willing to use the service had to *complete a detailed
    application form* to explain the potential use cases and guarantee good usage
    of the platform. Microsoft’s goal was to validate that any application enabled
    by Azure OpenAI was always aligned with their [responsible AI approach](https://oreil.ly/QsuYY)
    and the [intended use](https://oreil.ly/7ojQP) of the platform. If you are getting
    started with Azure OpenAI Service, [check first if you still need to apply for
    access](https://oreil.ly/MDBhf) and prepare the required information for the [application
    form](https://oreil.ly/dp14y).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，微软以“有限通用可用性”的形式发布了 Azure OpenAI 服务，这意味着任何希望使用该服务的组织都必须 *填写一份详细的申请表* 来解释潜在的使用案例并保证平台的使用良好。微软的目标是验证任何由
    Azure OpenAI 启用的应用程序始终与其 [负责任的 AI 方法](https://oreil.ly/QsuYY) 和平台的 [预期用途](https://oreil.ly/7ojQP)
    保持一致。如果您刚开始使用 Azure OpenAI 服务，请先 [检查您是否还需要申请访问权限](https://oreil.ly/MDBhf)，并准备申请表所需的必要信息。
- en: Azure OpenAI Service Building Blocks
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Azure OpenAI 服务构建块
- en: 'Before diving into the “how to,” let’s explore the available building blocks
    for any Azure OpenAI practitioner to prepare and deploy new solutions. Essentially,
    there are two primary components for the Azure OpenAI Service: the *visual interfaces*
    that allow users to test, customize, and deploy their generative AI models and
    the *development interfaces* that enable the exploitation and integration of those
    advanced capabilities with any application.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨“如何做”之前，让我们先探索任何 Azure OpenAI 实践者可用的构建块，以便准备和部署新的解决方案。本质上，Azure OpenAI
    服务有两个主要组件：允许用户测试、定制和部署其生成式 AI 模型的 *视觉界面* 以及使高级功能能够与任何应用程序利用和集成的 *开发界面*。
- en: Both elements are complementary and great assets for any kind of adopter, as
    they require a relatively low level of AI knowledge to make them work. For example,
    *citizen users* (hybrid technical-business profiles that are not very technical,
    but that understand the principles of generative AI and have some knowledge of
    prompt engineering, can use the visual playground, or leverage Microsoft Copilot
    Studio) and *regular developers* are great candidates for the development platforms.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个元素都是任何采用者的互补和宝贵资产，因为它们只需要相对较低水平的 AI 知识就能使其工作。例如，*公民用户*（混合技术-商业背景，技术性不强，但理解生成式
    AI 的原理并具备一些提示工程知识，可以使用视觉游乐场，或利用 Microsoft Copilot Studio）和 *常规开发者* 都是开发平台的理想人选。
- en: 'Visual interfaces: Azure OpenAI Studio and Playground'
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 视觉界面：Azure OpenAI Studio 和 Playground
- en: As with any other [Azure AI service](https://oreil.ly/TDoRH), Azure OpenAI includes
    the notion of a “Studio” (i.e., [Azure OpenAI Studio)](https://oreil.ly/LWQO1)
    that makes the interaction with generative AI models very simple, by providing
    an intuitive UI that facilitates service deployments and leverages existing Azure
    OpenAI APIs without any code required from the user perspective.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何其他 [Azure AI 服务](https://oreil.ly/TDoRH) 一样，Azure OpenAI 包含了“工作室”的概念（即 [Azure
    OpenAI Studio](https://oreil.ly/LWQO1)），通过提供直观的用户界面，使与生成式 AI 模型的交互变得非常简单，用户无需编写任何代码即可利用现有的
    Azure OpenAI API 进行服务部署。
- en: Azure OpenAI Studio includes access to [all available models](https://oreil.ly/BI5Ue)
    (by type and geographic region), predefined prompting scenarios and examples,
    and several applications called *playgrounds*. The Azure OpenAI Playgrounds are
    different apps within Azure OpenAI Service, which include (as you can see in [Figure 3-3](#fig_3_azure_openai_studio))
    a customizable ChatGPT type of instance (*Chat*), other GPT language models for
    nonchat scenarios (*Completion*), a playground to connect AI models with your
    data (*Bring your own data*), and one for image generation applications with OpenAI’s
    DALL·E models.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Azure OpenAI Studio 包含了对所有可用模型（按类型和地理区域）的访问权限[所有可用模型](https://oreil.ly/BI5Ue)，预定义的提示场景和示例，以及几个称为
    *playgrounds* 的应用程序。Azure OpenAI Playgrounds 是 Azure OpenAI 服务内的不同应用程序，包括（如 [图
    3-3](#fig_3_azure_openai_studio) 所示）一个可定制的 ChatGPT 类型实例（*Chat*），用于非聊天场景的其他 GPT
    语言模型（*Completion*），一个用于将 AI 模型与您的数据连接的 playground（*Bring your own data*），以及一个用于图像生成应用程序的
    OpenAI 的 DALL·E 模型。
- en: '![](assets/aoas_0303.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aoas_0303.png)'
- en: Figure 3-3\. Azure OpenAI Studio
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-3. Azure OpenAI Studio
- en: 'You can access each playground (and their related management features) from
    the left panel of the studio, or visit them directly by following the URLs included
    here:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从工作室的左侧面板访问每个 playground（及其相关管理功能），或通过以下链接直接访问它们：
- en: '[Chat playground](https://oreil.ly/FRU9K)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[Chat playground](https://oreil.ly/FRU9K)'
- en: This includes both the conversational Chat playground with the [features and
    settings](https://oreil.ly/K_fjL) required to create a private ChatGPT implementation,
    and the bring your own data (represented as one of the playgrounds in Azure OpenAI
    Studio) functionality that I will explain later in this section. The Chat playground
    (shown in [Figure 3-4](#fig_4_azure_openai_studio_chat_playground)) leverages
    the [Chat Completion API](https://oreil.ly/ZJOLp).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括具有创建私有 ChatGPT 实现所需的 [功能和设置](https://oreil.ly/K_fjL) 的对话式 Chat playground，以及将在本节后面解释的将您的数据带入（在
    Azure OpenAI Studio 中表示为其中一个 playground）的功能。Chat playground（如 [图 3-4](#fig_4_azure_openai_studio_chat_playground)
    所示）利用了 [Chat Completion API](https://oreil.ly/ZJOLp)。
- en: '![](assets/aoas_0304.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aoas_0304.png)'
- en: 'Figure 3-4\. Azure OpenAI Studio: Chat playground'
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-4. Azure OpenAI Studio：Chat playground
- en: 'As indicated in [Figure 3-4](#fig_4_azure_openai_studio_chat_playground), the
    main tiles and features of the Chat playground comprise the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [图 3-4](#fig_4_azure_openai_studio_chat_playground) 所示，Chat playground 的主要板块和功能包括以下内容：
- en: 1\. Assistant setup
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 辅助设置
- en: 'This area is located on the left side of the screen and allows users to configure
    the chatbot’s behavior. Users can choose from templates or create their own custom
    system messages. This section helps users define how the chatbot should act and
    respond to user queries:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 此区域位于屏幕左侧，允许用户配置聊天机器人的行为。用户可以从模板中选择或创建自己的自定义系统消息。本节帮助用户定义聊天机器人应该如何行动并响应用户查询：
- en: System message
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 系统消息
- en: A type of [meta-prompt](https://oreil.ly/OmKQO) (i.e., a prompt that sets the
    by-default context of the discussion) to guide the AI system’s behavior. It can
    be used to introduce the system, set expectations, provide feedback, or handle
    errors. One important thing to remember is that even if there is no token limit
    for this message, it will be included with every API call, so it counts against
    the overall [token limit/context length](https://oreil.ly/BI5Ue) of the model.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一种 [meta-prompt](https://oreil.ly/OmKQO) 类型（即设置讨论默认上下文的提示）来指导 AI 系统的行为。它可以用来介绍系统、设定期望、提供反馈或处理错误。需要记住的一个重要事项是，即使此消息没有令牌限制，它也将包含在每个
    API 调用中，因此它将计入模型的总体 [令牌限制/上下文长度](https://oreil.ly/BI5Ue)。
- en: Examples
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: 'This area is located at the bottom-left corner of the screen. You can add examples
    to the bot intelligence, so it learns the proper way to answer specific questions.
    It’s a good option when we don’t need to fully retrain a model, for example, when
    you need to add a couple of topics from your company’s knowledge base and you
    want to define the best way to answer. From the official description: “Add examples
    to show the chat what responses you want. It will try to mimic any responses you
    add here so make sure they match the rules you laid out in the system message.”'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 此区域位于屏幕左下角。您可以为机器人智能添加示例，以便它学习正确回答特定问题的方法。当我们不需要完全重新训练模型时，这是一个很好的选择，例如，当您需要从公司的知识库中添加几个主题，并希望定义最佳回答方式时。根据官方描述：“添加示例以显示聊天您想要的响应。它将尝试模仿您在此处添加的任何响应，因此请确保它们与系统消息中设定的规则相匹配。”
- en: 2\. Chat session
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 聊天会话
- en: This area is located in the middle of the screen and serves as the main interaction
    point between you and the chatbot. You can type your queries here and the chatbot
    will respond accordingly. The chat session allows you to test the chatbot’s performance
    and make adjustments to the assistant setup as needed, as well as import and export
    bot configurations, or get the result as a [JavaScript Object Notation (JSON)
    file](https://oreil.ly/LZJH4).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 此区域位于屏幕中间，是您与聊天机器人之间的主要交互点。您可以在其中输入您的查询，聊天机器人将相应地做出回应。聊天会话允许您测试聊天机器人的性能，并根据需要调整助手设置，以及导入和导出机器人配置，或以[JavaScript
    Object Notation (JSON)文件](https://oreil.ly/LZJH4)的形式获取结果。
- en: 3\. Deploy to
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 部署到
- en: This option allows you to deploy your chatbot to a specific platform or environment.
    Azure OpenAI Studio allows direct deployments to both [Azure Web Apps](https://oreil.ly/TtlXr)
    and [Microsoft Copilot Studio](https://oreil.ly/YV0SN). We will explore these
    deployment options later in this chapter.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 此选项允许您将聊天机器人部署到特定的平台或环境。Azure OpenAI Studio允许直接部署到[Azure Web Apps](https://oreil.ly/TtlXr)和[Microsoft
    Copilot Studio](https://oreil.ly/YV0SN)。我们将在本章的后面部分探讨这些部署选项。
- en: 4\. Configuration
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 配置
- en: 'This area is located in the top-right corner of the screen. It provides options
    for you to access deployment and session settings. Users can also clear the chat
    history and manage parameters related to the chatbot’s deployment:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 此区域位于屏幕的右上角。它提供了您访问部署和会话设置的选项。用户还可以清除聊天历史并管理与聊天机器人部署相关的参数：
- en: Deployment
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 部署
- en: 'To handle session-level configurations, such as the Azure OpenAI deployment
    resource you want to use (e.g., you may have several for different geographic
    regions), as well as the memory of the session, which will impact how many interactions
    the system can remember when getting new questions:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理会话级别的配置，例如您想要使用的Azure OpenAI部署资源（例如，您可能为不同的地理区域有几个），以及会话的内存，这将影响系统在获取新问题时可以记住多少交互：
- en: Deployment instance
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 部署实例
- en: You will select one option, from the resources you have [previously deployed](https://oreil.ly/-4D4f)
    (if you haven’t, you will need to create one before using Azure OpenAI Studio),
    based on the geography and model needs you may have.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 您将根据您可能需要的地理和模型需求，从您之前部署的资源[（如果您还没有，您在使用Azure OpenAI Studio之前需要创建一个）]中选择一个选项。
- en: Past messages included and current token count
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 包含的过去消息和当前令牌计数
- en: Session-level parameters you may want to adjust for the specific test you do
    via the Chat playground. These parameters will be gone when you finish the playground
    session, except if you deploy an application (we will see the deployment options
    in a couple of sections).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能想要调整的特定测试的会话级别参数，您将通过聊天游乐场进行这些测试。完成游乐场会话后，这些参数将消失，除非您部署了一个应用程序（我们将在接下来的几个部分中看到部署选项）。
- en: Parameters
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: 'This right panel includes all technical settings that will allow you to configure
    the expected output message, including the level of creativity versus determinism
    of the answer:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 此右侧面板包括所有技术设置，这些设置将允许您配置预期的输出消息，包括答案的创造性与确定性水平：
- en: Max response
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 最大响应
- en: This parameter helps you set a limit on the number of tokens per model response.
    The max response is measured in the number of tokens, and it is shared between
    the question (including system message, examples, message history, and prompt/user
    query) and the model’s response.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 此参数帮助您设置每个模型响应的令牌数量上限。最大响应以令牌数量衡量，并且它由问题（包括系统消息、示例、消息历史和提示/用户查询）和模型的响应共享。
- en: Temperature
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 温度
- en: This parameter and the Top-p parameter are direct alternatives to control the
    AI model’s randomness. Lowering the temperature means that the model will produce
    more repetitive and deterministic responses. Increasing the temperature will result
    in more unexpected or creative responses. Try adjusting temperature or Top-p,
    but not both.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 此参数和Top-p参数是直接替代品，用于控制AI模型的随机性。降低温度意味着模型将产生更多重复和确定的响应。提高温度将导致更多意外或创造性的响应。尝试调整温度或Top-p，但不要同时调整。
- en: '[Assistants playground](https://oreil.ly/S4KFy)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[助手游乐场](https://oreil.ly/S4KFy)'
- en: '[Released in 2024](https://oreil.ly/MdxvG), the Assistants playground is visually
    similar to the Chat playground, but it includes:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[发布于2024年](https://oreil.ly/MdxvG)，助手游乐场在视觉上与聊天游乐场相似，但它包括：'
- en: The ability to handle conversation threads, by using the “thread ID” parameter
    that converts the chat discussion into a stateful application that keeps context
    and memory. You can see the details in Azure OpenAI’s [Assistants API specification](https://oreil.ly/ErRd6).
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用“线程ID”参数处理对话线程的能力，将聊天讨论转换为具有上下文和记忆的状态化应用程序。您可以在Azure OpenAI的[助手API规范](https://oreil.ly/ErRd6)中查看详细信息。
- en: Other functionalities such as the API call log, the [Code Interpreter](https://oreil.ly/3jSFV),
    and [function calling](https://oreil.ly/2R7Pz).
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他功能，如API调用日志、[代码解释器](https://oreil.ly/3jSFV)和[函数调用](https://oreil.ly/2R7Pz)。
- en: Keep in mind that this is a relatively new option, but the [official documentation](https://oreil.ly/HH4hH)
    includes the detailed steps for creation and management of assistant files. Keep
    an eye on and bookmark this URL to follow any news and technical resources.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这是一个相对较新的选项，但[官方文档](https://oreil.ly/HH4hH)包括了助手文件创建和管理的详细步骤。请关注并收藏此URL，以跟踪任何新闻和技术资源。
- en: '[Completions playground](https://oreil.ly/zJYtL)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[完成游乐场](https://oreil.ly/zJYtL)'
- en: As we reviewed in [Chapter 1](ch01.html#introduction_to_generative_ai_and_azure_openai_ser),
    the completion skill is (along with chat and embeddings models) one of the core
    concepts for NLP and modern LLMs. Completion focuses on unitary interactions for
    all kinds of text-based requests (with no need for memory between interactions,
    as you may need for chat-based applications in which the model keeps the discussion
    context). It leverages the [Completions API](https://oreil.ly/Uczv9). As shown
    in [Figure 3-5](#fig_5_azure_openai_studio_completions_playground), the Completions
    playground allows you to type a prompt, or choose from a series of examples. It
    also includes the same kind of setting parameters that we reviewed in the Chat
    playground.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第1章](ch01.html#introduction_to_generative_ai_and_azure_openai_ser)中回顾的那样，完成技能（与聊天和嵌入模型一起）是NLP和现代LLM的核心概念之一。完成技能专注于所有基于文本请求的单元交互（无需在交互之间保持记忆，如聊天应用程序中可能需要的，在这些应用程序中，模型会保持讨论上下文）。它利用了[完成API](https://oreil.ly/Uczv9)。如图3-5所示，完成游乐场允许您输入提示，或从一系列示例中选择。它还包括我们在聊天游乐场中回顾过的相同类型的设置参数。
- en: '![](assets/aoas_0305.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aoas_0305.png)'
- en: 'Figure 3-5\. Azure OpenAI Studio: Completions playground'
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-5\. Azure OpenAI Studio：完成游乐场
- en: You can generate an answer (completion), and even regenerate it to obtain a
    totally new output. If you choose one of the examples from the drop-down menu,
    you will see an automatic prompt appear and the corresponding completion, highlighted
    as in [Figure 3-6](#fig_6_azure_openai_studio_completions_playground_examp).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以生成一个答案（完成），甚至可以重新生成以获得全新的输出。如果您从下拉菜单中选择一个示例，您将看到一个自动提示出现，并显示相应的完成，如图3-6所示。
- en: '![](assets/aoas_0306.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aoas_0306.png)'
- en: 'Figure 3-6\. Azure OpenAI Studio: Completions playground (example)'
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-6\. Azure OpenAI Studio：完成游乐场（示例）
- en: Summarizing, you may use chat for multistep scenarios where you need to maintain
    a sequence of interactions with the AI model, while completions can be used for
    specific unitary cases. As you will see later, these two playgrounds are just
    visual interfaces that consume existing Azure OpenAI [completion](https://oreil.ly/Uczv9)
    and [chat](https://oreil.ly/ZJOLp) APIs.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，您可以使用聊天来处理需要与AI模型保持一系列交互的多步场景，而完成技能则可用于特定的单元案例。您稍后将会看到，这两个游乐场只是消耗现有Azure
    OpenAI[完成](https://oreil.ly/Uczv9)和[聊天](https://oreil.ly/ZJOLp)API的视觉界面。
- en: Bring your own data playground
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 带自数据游乐场
- en: Even if Azure OpenAI Studio shows this feature as a separate playground, it
    is technically part of the Chat playground. To access this functionally, you can
    either use the Chat playground’s Assistant setup and select the “Add your data”
    tab or go directly to the “Bring your own data” tile of the Studio ([Figure 3-7](#fig_7_azure_openai_studio_bring_your_own_data)).
    For both cases, the result will be the same.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 即使 Azure OpenAI Studio 将此功能显示为独立的游乐场，但从技术上讲，它仍然是聊天游乐场的一部分。要访问此功能，您可以使用聊天游乐场的助手设置并选择“添加您的数据”标签，或者直接前往工作室的“自带数据”磁贴（[图3-7](#fig_7_azure_openai_studio_bring_your_own_data)）。在两种情况下，结果都将相同。
- en: Once you reach this point, the sequence of steps is pretty simple. As you can
    see in [Figure 3-8](#fig_8_azure_openai_studio_bring_your_own_data_source_de),
    the system will allow you to select your own sources of data, to combine their
    knowledge with the baseline LLM. That knowledge can come from PDF files, text-based
    documents, slides, web files, etc. In this case, besides the Azure OpenAI resource
    previously deployed, the bring your own data functionality will leverage other
    resources such as Azure Data Lake Gen2/Azure Storage, to save the files, and Azure
    Cognitive Search, to index the files. Azure Cognitive Search offers a vector search
    functionality based on the [Embeddings API](https://oreil.ly/imKOS)) that I will
    explain by the end of the chapter. Finally, you can always check the [official
    documentation](https://oreil.ly/z_iRM) to follow the latest updates for this Azure
    OpenAI feature, as it is a quickly evolving one due to the continuous incorporation
    of new functionalities.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦达到这个阶段，步骤序列相当简单。如图3-8[图3-8](#fig_8_azure_openai_studio_bring_your_own_data_source_de)所示，系统将允许你选择自己的数据源，将它们的知识与基线LLM相结合。这些知识可以来自PDF文件、基于文本的文档、幻灯片、网页文件等。在这种情况下，除了之前部署的Azure
    OpenAI资源外，自带数据功能将利用其他资源，如Azure Data Lake Gen2/Azure Storage来保存文件，以及Azure Cognitive
    Search来索引文件。Azure Cognitive Search提供基于[嵌入API](https://oreil.ly/imKOS)的向量搜索功能，我将在本章末尾解释。最后，你可以始终检查[官方文档](https://oreil.ly/z_iRM)，以了解此Azure
    OpenAI功能的最新更新，因为它是一个快速发展的功能，因为持续整合新的功能。
- en: '![](assets/aoas_0307.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aoas_0307.png)'
- en: 'Figure 3-7\. Azure OpenAI Studio: Bring your own data'
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-7\. Azure OpenAI Studio：自带数据
- en: '![](assets/aoas_0308.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aoas_0308.png)'
- en: 'Figure 3-8\. Azure OpenAI Studio: Bring your own data source details'
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-8\. Azure OpenAI Studio：自带数据源详情
- en: '[DALL·E playground](https://oreil.ly/r4h7Y)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[DALL·E游乐场](https://oreil.ly/r4h7Y)'
- en: The last playground tile provides direct access to the generative AI DALL·E
    models (versions 2 and 3) from OpenAI. This is a text-to-image model that allows
    you to create new images from just text-based descriptions. Imagine describing
    a place or a scene and getting a visual representation in the form of images that
    are freshly created on demand. This means they didn’t exist previously and that
    you can integrate this capability into your solutions and combine it with the
    rest of the language. The DALL·E playground (shown in [Figure 3-9](#fig_9_azure_openai_studio_dall_e_playground))
    leverages the [Image Generation API](https://oreil.ly/bm-7a).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个游乐场瓷砖提供了直接访问OpenAI的生成式AI DALL·E模型（版本2和3）的途径。这是一个文本到图像的模型，允许你仅通过文本描述来创建新的图像。想象一下描述一个地方或场景，并得到以图像形式呈现的视觉表示，这些图像是按需新鲜创建的。这意味着它们之前并不存在，你可以将这种能力集成到你的解决方案中，并与语言的其他部分相结合。DALL·E游乐场（如图3-9[图3-9](#fig_9_azure_openai_studio_dall_e_playground)所示）利用了[图像生成API](https://oreil.ly/bm-7a)。
- en: '![](assets/aoas_0309.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aoas_0309.png)'
- en: 'Figure 3-9\. Azure OpenAI Studio: DALL·E playground'
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-9\. Azure OpenAI Studio：DALL·E游乐场
- en: 'As shown in [Figure 3-9](#fig_9_azure_openai_studio_dall_e_playground), relevant
    aspects of the playground include the following:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如图3-9[图3-9](#fig_9_azure_openai_studio_dall_e_playground)所示，游乐场的相关方面包括以下内容：
- en: 1\. Playground
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 游乐场
- en: The DALL·E playground is visually simple—a prompt field and the results (image)
    below. It’s similar to the structure of the [Bing Create application](https://oreil.ly/YwDy-),
    but with the option to deploy the DALL·E model for your own development.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: DALL·E游乐场在视觉上很简单——一个提示字段和下面的结果（图像）。它与[Bing Create应用](https://oreil.ly/YwDy-)的结构相似，但提供了部署DALL·E模型以供你自己的开发使用的选项。
- en: 2\. Settings
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 设置
- en: The settings panel offers you the option to choose the number of images you
    want to generate and the image size.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 设置面板为你提供了选择你想要生成的图像数量和图像大小的选项。
- en: 3\. Album
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 相册
- en: The album section showcases all past image experiments, offering you the option
    to review previously created images, generate new ones, etc.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 相册部分展示了所有过去图像实验，为你提供了回顾之前创建的图像、生成新的图像等选项。
- en: Besides the different playgrounds, you can also explore the left-side *Management*
    panel shown in [Figure 3-10](#fig_10_azure_openai_studio_management_panels), which
    include options such as deployments, models, data files, quotas, and content filters.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 除了不同的游乐场，你还可以探索显示在[图3-10](#fig_10_azure_openai_studio_management_panels)左侧的*管理*面板，其中包括部署、模型、数据文件、配额和内容过滤器等选项。
- en: '![](assets/aoas_0310.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aoas_0310.png)'
- en: 'Figure 3-10\. Azure OpenAI Studio: Management panels'
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-10\. Azure OpenAI Studio：管理面板
- en: 'Let’s explore the most important features:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索最重要的功能：
- en: '[Deployments](https://oreil.ly/PGocU)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[部署](https://oreil.ly/PGocU)'
- en: Allows you to deploy any specific model instance [available in the geographic
    region](https://oreil.ly/XZnCX) of your Azure OpenAI resource and to visualize
    those that you previously deployed ([Figure 3-11](#fig_11_azure_openai_studio_deployments)).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 允许您部署Azure OpenAI资源地理区域中[可用的任何特定模型实例](https://oreil.ly/XZnCX)，并可视化您之前部署的模型（[图3-11](#fig_11_azure_openai_studio_deployments)）。
- en: '![](assets/aoas_0311.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aoas_0311.png)'
- en: 'Figure 3-11\. Azure OpenAI Studio: deployments'
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-11. Azure OpenAI Studio：部署
- en: '[Content filters](https://oreil.ly/Bpsud)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[内容过滤器](https://oreil.ly/Bpsud)'
- en: For responsible AI moderation. Each filter from those in [Figure 3-12](#fig_12_azure_openai_studio_content_filters)
    (e.g., hate, sexual, self-harm, and violence topics for both prompts and completions,
    with different levels of filtering) can be applied to the deployments, and those
    deployments will include the content filter for each chat or completion implementation.
    We will explore this feature in [Chapter 4](ch04.html#additional_cloud_and_ai_capabilities),
    as part of the responsible AI measures for generative AI implementations.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了负责任的AI监管。图3-12中所示的所有过滤器（例如，对于提示和完成的仇恨、性、自残和暴力主题，具有不同的过滤级别）都可以应用于部署，并且这些部署将包括每个聊天或完成实现的
    内容过滤器。我们将在[第4章](ch04.html#additional_cloud_and_ai_capabilities)中探讨此功能，作为生成式AI实施的责任AI措施的一部分。
- en: '![](assets/aoas_0312.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aoas_0312.png)'
- en: 'Figure 3-12\. Azure OpenAI Studio: content filters'
  id: totrans-113
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-12. Azure OpenAI Studio：内容过滤器
- en: '[Models](https://oreil.ly/oC3Hj)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[模型](https://oreil.ly/oC3Hj)'
- en: This option shows the [available Azure OpenAI models](https://oreil.ly/XZnCX),
    related to the specific geographic region of the chosen deployment.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 此选项显示与所选部署的特定地理区域相关的[可用的Azure OpenAI模型](https://oreil.ly/XZnCX)。
- en: '[Data files](https://oreil.ly/2TZwW)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[数据文件](https://oreil.ly/2TZwW)'
- en: This file management feature allows you to [prepare the dataset for fine-tuned
    implementations](https://oreil.ly/FDMr1). We will explore more about fine-tuning
    later in this chapter.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 此文件管理功能允许您[为微调实现准备数据集](https://oreil.ly/FDMr1)。我们将在本章后面部分探讨更多关于微调的内容。
- en: '[Quotas](https://oreil.ly/ONn5Q)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[配额](https://oreil.ly/ONn5Q)'
- en: The quota panel shows the [usage quotas](https://oreil.ly/bEN4D) related to
    different models and geographic regions. It also helps you [request a quota increase](https://oreil.ly/iiysu)
    if you need more. Alternatively, and I will explain this in [Chapter 6](ch06.html#elaborating_generative_ai_business_cases)
    as part of the pricing and estimation exercise, you have an option to hire dedicated
    capacity, by leveraging the so-called [provisioned throughput units (PTU) for
    Azure OpenAI](https://oreil.ly/KCC6K), which are reserved instances with performance
    and service availability benefits.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 配额面板显示了与不同模型和地理区域相关的[使用配额](https://oreil.ly/bEN4D)。它还帮助您[请求配额增加](https://oreil.ly/iiysu)，如果您需要更多的话。或者，我将在[第6章](ch06.html#elaborating_generative_ai_business_cases)中解释这一点，作为定价和估算练习的一部分，您可以选择通过利用所谓的[为Azure
    OpenAI预留的吞吐量单位（PTU）](https://oreil.ly/KCC6K)来雇佣专用容量，这些是具有性能和服务可用性优势的预留实例。
- en: We will explore some of these functionalities later in this chapter and in [Chapter 4](ch04.html#additional_cloud_and_ai_capabilities),
    as they will all be relevant, depending on the type of Azure OpenAI implementation
    you plan to utilize. Now, let’s see what you can do to deploy these models via
    Azure OpenAI Studio.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章和[第4章](ch04.html#additional_cloud_and_ai_capabilities)中探讨这些功能的一些内容，因为它们都将根据您计划利用的Azure
    OpenAI实现类型而变得相关。现在，让我们看看您可以通过Azure OpenAI Studio部署这些模型能做什么。
- en: 'Deployment interfaces: Web apps and Microsoft Copilot agents'
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署接口：Web应用和Microsoft Copilot代理
- en: 'As mentioned in this chapter, the Chat playground includes some easy-to-use
    deployment options. They are not available for the rest of the playgrounds, but
    they can simplify the preliminary deployment of Azure OpenAI models for internal
    testing and use purposes, without any coding required. These no-code deployments
    can incorporate the specific knowledge from the bring your own data functionality.
    There are two possibilities:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章所述，聊天游乐场包括一些易于使用的部署选项。这些选项在其他游乐场中不可用，但它们可以简化Azure OpenAI模型的初步部署，用于内部测试和使用，无需编写任何代码。这些无代码部署可以结合来自“自带数据”功能的具体知识。有两种可能性：
- en: Web apps with [Azure App Service](https://oreil.ly/moBFz)
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 带有[Azure App Service](https://oreil.ly/moBFz)的Web应用
- en: The first available deployment option, which you can use with or without the
    “bring your own data” feature activated. As we discussed in [Chapter 2](ch02.html#designing_cloud_native_architectures_for_generativ),
    App Service is the Azure option to deploy native web apps; it allows integrations
    with both external and internal systems and web development with a variety of
    programming languages. From Azure OpenAI Studio and its Chat playground, you can
    simply “Deploy to” and then configure your deployment (see [Figure 3-13](#fig_13_azure_openai_studio_web_app_deployment)).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个可用的部署选项，你可以使用或不用激活“自带数据”功能。正如我们在[第2章](ch02.html#designing_cloud_native_architectures_for_generativ)中讨论的那样，App
    Service是Azure用于部署原生Web应用的选项；它允许与外部和内部系统以及使用各种编程语言的Web开发集成。从Azure OpenAI Studio及其Chat
    playground，你可以简单地“部署到”，然后配置你的部署（见[图3-13](#fig_13_azure_openai_studio_web_app_deployment)）。
- en: '![](assets/aoas_0313.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aoas_0313.png)'
- en: 'Figure 3-13\. Azure OpenAI Studio: web app deployment'
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-13\. Azure OpenAI Studio：Web应用部署
- en: 'As shown in [Figure 3-13](#fig_13_azure_openai_studio_web_app_deployment),
    configuration options include the following:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图3-13](#fig_13_azure_openai_studio_web_app_deployment)所示，配置选项包括以下内容：
- en: Choosing the web app
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 选择Web应用
- en: You can create a new App Service resource directly from this feature (in that
    case, you will need to define the “app name” that will be part of your web app
    URL), or choose an existing one if you have previously deployed via [Azure portal’s
    App Service panel](https://oreil.ly/dPLy2).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以直接从该功能创建一个新的App Service资源（在这种情况下，你需要定义将成为你的Web应用URL一部分的“应用名称”），或者如果你之前通过[Azure门户的App
    Service面板](https://oreil.ly/dPLy2)部署过，可以选择现有的一个。
- en: Pricing plan
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 定价计划
- en: To select the preferred [pricing tier](https://oreil.ly/IdDXQ) for the web app.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 选择Web应用的[定价层](https://oreil.ly/IdDXQ)。
- en: Chat history
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天历史记录
- en: A functionality that allows the web app users to recover their [previous interactions](https://oreil.ly/-yyQg)
    with chat. It relies on [Cosmos DB (Azure’s NoSQL database)](https://oreil.ly/-yyQg),
    which obviously adds cost to the existing Azure OpenAI and App Service resources.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 一种允许Web应用用户恢复他们与聊天[之前的交互](https://oreil.ly/-yyQg)的功能。它依赖于[Cosmos DB（Azure的NoSQL数据库）](https://oreil.ly/-yyQg)，这显然会增加现有的Azure
    OpenAI和App Service资源的成本。
- en: Once you have selected all these options, you can click on Deploy. You will
    need to wait around 10 minutes for all the resources to be deployed, then you
    will be able to launch your web app from the studio, or by typing the URL *https://<appname>.azurewebsites.net**.*The
    look and feel will be something like the interface you see in [Figure 3-14](#fig_14_azure_openai_studio_web_app_interface).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你选择了所有这些选项，你就可以点击部署。你需要等待大约10分钟，直到所有资源部署完成，然后你将能够从工作室或通过输入URL *https://<appname>.azurewebsites.net**.*
    启动你的Web应用。外观和感觉将类似于你在[图3-14](#fig_14_azure_openai_studio_web_app_interface)中看到的界面。
- en: '![](assets/aoas_0314.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aoas_0314.png)'
- en: 'Figure 3-14\. Azure OpenAI Studio: web app interface'
  id: totrans-136
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-14\. Azure OpenAI Studio：Web应用界面
- en: The UI of the new app will contain a regular chatbot setup, with options to
    share and check previous discussions on the top-right side of the window. You
    can also [customize the visual aspect of the application](https://oreil.ly/BVUkG)
    by using the [official source code](https://oreil.ly/MeBin), and deploy it programmatically,
    with Azure App Service and using your preferred programming language, instead
    of leveraging Azure OpenAI Studio.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 新应用的UI将包含常规聊天机器人设置，并在窗口右上角提供分享和检查之前讨论的选项。你还可以通过使用[官方源代码](https://oreil.ly/MeBin)来[自定义应用程序的视觉方面](https://oreil.ly/BVUkG)，并通过Azure
    App Service和你的首选编程语言以编程方式部署它，而不是利用Azure OpenAI Studio。
- en: Bots with [Microsoft Copilot Studio (formerly Power Virtual Agents [PVAs])](https://oreil.ly/YV0SN)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 带有[Microsoft Copilot Studio（以前称为Power Virtual Agents [PVAs]）](https://oreil.ly/YV0SN)的机器人
- en: This option is available for Chat playground implementations that include the
    “bring your own data” feature. That means that if you don’t add extended knowledge
    from PDFs or other documents, the Chat playground won’t include Microsoft Copilot
    Studio/PVA as a deployment option in the top-right corner in [Figure 3-15](#fig_15_azure_openai_studio_copilot_deployment).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 此选项适用于包含“自带数据”功能的Chat playground实现。这意味着，如果你没有从PDF或其他文档中添加扩展知识，Chat playground不会在[图3-15](#fig_15_azure_openai_studio_copilot_deployment)右上角将Microsoft
    Copilot Studio/PVA作为部署选项。
- en: '![](assets/aoas_0315.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aoas_0315.png)'
- en: 'Figure 3-15\. Azure OpenAI Studio: Copilot deployment'
  id: totrans-141
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-15\. Azure OpenAI Studio：Copilot部署
- en: How to handle PVAs is outside of the scope of this book, but you can explore
    the [detailed instructions from the official documentation](https://oreil.ly/Qi9J3)
    that show how to use PVAs with Azure OpenAI for the *generative answers* feature.
    This option is available for only certain geographic regions, so you will need
    to validate if your deployments with Azure OpenAI models show the PVA deployment
    option in the Chat playground. If this is not the case, you may want to deploy
    new models in other regions.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如何处理 PVA（预测变量分析）超出了本书的范围，但您可以探索[官方文档中的详细说明](https://oreil.ly/Qi9J3)，其中展示了如何使用
    PVA 与 Azure OpenAI 的*生成答案*功能。此选项仅适用于某些地理区域，因此您需要验证您的 Azure OpenAI 模型部署是否在 Chat
    playground 中显示 PVA 部署选项。如果不是这种情况，您可能需要在其他地区部署新的模型。
- en: Summarizing, these visual interfaces can help you leverage Azure OpenAI models
    in a simple manner. They provide an intuitive way to launch the Azure OpenAI APIs
    in just a few clicks. However, you will need code-based tools to implement the
    other advanced architectures you will see later in this chapter. Let’s now explore
    those APIs and other development kits so you can leverage everything that Azure
    OpenAI Service has to offer.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，这些可视化界面可以帮助您以简单的方式利用 Azure OpenAI 模型。它们提供了一种直观的方式，只需几点击即可启动 Azure OpenAI
    API。然而，您需要基于代码的工具来实现本章后面将看到的其他高级架构。现在让我们探索这些 API 和其他开发工具包，以便您可以利用 Azure OpenAI
    服务提供的一切。
- en: 'Development interfaces: APIs and SDKs'
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开发接口：API 和 SDK
- en: In addition to all the previously explored interfaces, one of the key enablers
    for integrating Azure OpenAI with existing or new applications is the ability
    to consume the preconfigured models as regular endpoints. From a development point
    of view, we can call those models by using the APIs and related software development
    kits (SDKs) and pass any input and configuration parameters within the code. This
    section covers the main pieces you need to know—the *Azure OpenAI Service REST
    APIs*, including the [official API reference documentation](https://oreil.ly/qH3FL),
    with specific details for chat, completions, embeddings, and other deployments.
    There is also an [official repo](https://oreil.ly/mbA1v) with the full specifications.
    There are general APIs that will help you with the configuration and deployment
    of Azure OpenAI services, while the service APIs help you consume the models to
    bring the AI capabilities to your generative AI applications.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 除了之前探索的所有接口之外，将 Azure OpenAI 与现有或新应用程序集成的一个关键推动因素是能够将预配置的模型作为常规端点进行消费。从开发的角度来看，我们可以通过使用
    API 和相关的软件开发工具包（SDK）来调用这些模型，并在代码中传递任何输入和配置参数。本节涵盖了您需要了解的主要部分——*Azure OpenAI 服务
    REST API*，包括[官方 API 参考文档](https://oreil.ly/qH3FL)，其中包含聊天、完成、嵌入和其他部署的详细信息。还有一个[官方仓库](https://oreil.ly/mbA1v)，包含完整的规范。有一些通用的
    API 可以帮助您配置和部署 Azure OpenAI 服务，而服务 API 则帮助您消费模型，将 AI 功能带入您的生成式 AI 应用程序。
- en: 'The main APIs you need to know and their high-level call details are as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要了解的主要 API 及其高级调用细节如下：
- en: '[General management APIs](https://oreil.ly/xkqqk)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[通用管理 API](https://oreil.ly/xkqqk)'
- en: For Azure AI service account management (including Azure OpenAI), with tasks
    such as account creation, deletion, listing, etc.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Azure AI 服务账户管理（包括 Azure OpenAI），包括账户创建、删除、列出等任务。
- en: '[APIs for model-related information](https://oreil.ly/Y7VMR)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[模型相关信息 API](https://oreil.ly/Y7VMR)'
- en: To obtain the list of available Azure OpenAI models and information about their
    specific capabilities and the model lifecycle (including potential deprecation
    details).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 获取可用的 Azure OpenAI 模型列表以及它们的具体功能和模型生命周期（包括潜在的弃用细节）。
- en: '[Completions](https://oreil.ly/Uczv9)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[完成](https://oreil.ly/Uczv9)'
- en: 'The required API for nonchat language scenarios. This and other APIs are versioned
    by using the “YYYY-MM-DD” date structure for `api-version`, and you will need
    to copy the resource name and deployment-ID from the Azure OpenAI model you previously
    deployed (remember the step-by-step process from the Azure portal, in [Chapter 2](ch02.html#designing_cloud_native_architectures_for_generativ)).
    To create a completion resource, the POST operation is:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 非聊天语言场景所需的 API。这些和其他 API 都通过使用“YYYY-MM-DD”日期结构来对`api-version`进行版本控制，并且您需要从之前部署的
    Azure OpenAI 模型中复制资源名称和部署-ID（记住 Azure 门户中的逐步过程，见[第 2 章](ch02.html#designing_cloud_native_architectures_for_generativ)）。要创建完成资源，POST
    操作如下：
- en: '[PRE0]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The request and response dynamic follows this structure, with the prompt parameter
    the input for the model to generate a specific completion, and a series of [optional
    parameters](https://oreil.ly/Uczv9) such as `max_tokens` (the limit of tokens
    for the expected answer) or the number `n` of expected completions/answers.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 请求和响应动态遵循此结构，其中提示参数是模型生成特定完成内容的输入，以及一系列可选参数[可选参数](https://oreil.ly/Uczv9)，例如`max_tokens`（预期答案的令牌限制）或预期完成/答案的数量`n`。
- en: '*Request*:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '*请求*:'
- en: '[PRE1]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Response*:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*响应*:'
- en: '[PRE2][PRE3][PRE4]``py[PRE5]`py` `"model"``:` `"gpt-35-turbo-instruct"``,`
    [PRE6] `{` [PRE7]`` `"text"``:` `", is eating burgers with a milkshake"``,` [PRE8]`
    `"index"``:` `0``,` [PRE9] `"logprobs"``:` `null``,` [PRE10]`py [PRE11]py`` [PRE12]py[PRE13][PRE14][PRE15][PRE16]py[PRE17]py[PRE18]``py[PRE19]py[PRE20]
    { `"messages"``:` `[` [PRE21][PRE22]` `{` [PRE23][PRE24] `"role"``:` `"system"``,`
    [PRE25][PRE26]``py[PRE27]`py` `{` [PRE28] `"content"``:` `"Who wrote ''Romeo and
    Juliet''?"` [PRE29]`` `},` [PRE30]` `{` [PRE31] `"role"``:` `"assistant"``,` [PRE32]`py
    [PRE33]py`` [PRE34]py[PRE35][PRE36][PRE37][PRE38]py[PRE39]py` [PRE40]`py`` [PRE41]py[PRE42]``
    [PRE43] {"prompt": "<prompt text>", "completion": "<ideal generated text>"} `{``"prompt"``:`
    `"<prompt text>"``,` `"completion"``:` `"<ideal generated text>"``}` [PRE44]`
    [PRE45][PRE46] `` `### Embedding-based grounding    As you now know from earlier
    chapters, embeddings are mathematical representations of text-based information
    as vectors in a vector space, and an alternative and/or complement to the traditional
    index-based approach. These embeddings are stored and managed as mathematical
    vectors that represent distances between topics. This means if we are looking
    for information about animals and we have a vectorized knowledge base that includes
    animal-related topics, we can recover the Top-k answers (i.e., the most relevant
    “K” number of pieces of information).    You can use the Azure OpenAI embeddings
    API to generate vector representations of text that capture the semantic meaning
    and similarity of the text. Some possible use cases for embeddings are document
    search, text classification, clustering, or text similarity.    The end-to-end
    process to create and use an embedding-based system is aligned with what you have
    seen thus far in this chapter. From an Azure OpenAI perspective, the steps are
    as follows:    1.  *Select the knowledge base* that contains the information that
    will complement the baseline LLM knowledge domain. This may include PDF, DOC,
    PPT, TXT, and other file formats. In Azure, you may store that information via
    Azure Blob Storage or Azure Data Lake Gen2\. Keep in mind that if your files are
    similar to any general information that may be available on the internet (for
    example, public descriptions of industry concepts), you probably don’t need to
    ground them. However, if you have very specific files with information on how
    to answer questions, or perform internal tasks, those may be good candidates for
    embeddings generation.           2.  *Choose and deploy your database/vector store*.
    By the end of this chapter, you will see all available options for implementation
    in Azure with Azure OpenAI–generated embeddings.           3.  *Prepare the input
    dataset*. This includes two different steps:               1.  *Extract the information*
    from your documents. For example, you can use Azure Document Intelligence/Form
    Recognizer to extract text from your PDFs with the OCR feature. You can also use
    other non-Azure tools.                       2.  *Split the information*. For
    this to work, it is important to keep in mind the [embeddings model token limit](https://oreil.ly/SQSGw)
    (e.g., 8K for Ada model version 2) to prepare the input without exceeding the
    limit (you can use [OpenAI’s tokenizer tool](https://oreil.ly/DDQHG) to understand
    the extent of what 8K means in terms of document length). This means you will
    need to make one API call for each of the limited-size blocks you have prepared
    before, or leverage [chunking techniques](https://oreil.ly/3DHfa) to split and
    handle larger documents.                   4.  *Leverage the Azure OpenAI [embeddings
    models](https://oreil.ly/gvAHr)*. Use the API operations you saw earlier in this
    chapter and get the mathematical vectors from the API response. *Store the vectors*
    within the chosen vector store.           5.  Any time you want to find information
    from your knowledge base, or if you want to leverage it from any chat or search
    application, you will need to *generate the embeddings of the question itself*,
    then perform the search against the vector search. Keep in mind that you will
    need to leverage the same model (e.g., Ada version 2) for both your knowledge
    base and the question. You can send the result of the search, with the Top-k results,
    to the chat or search application, directly or by including it as content for
    the answer of the completion.              This process is similar for other embeddings
    and conversation models (for example, those that are available via Azure AI Studio’s
    model catalog and Hugging Face), and the high-level architecture includes the
    elements you can see in [Figure 3-21](#fig_21_embedding_based_grounding_architecture):
    basically, the baseline Azure OpenAI model gets complemented with the internal
    knowledge base that contains PDFs, Word docs, etc. Instead of retraining/fine-tuning
    the model, we just combine it with that knowledge base so it can find similarities
    between the users’ questions and the information contained within the data sources.  ![](assets/aoas_0321.png)  ######
    Figure 3-21\. Embedding-based grounding architecture    You can find more information
    and code examples on [how to create embeddings](https://oreil.ly/8Duc8) from the
    official Microsoft documentation (in addition to the API definitions we covered
    earlier in this chapter).    Additionally, there is [one official Microsoft accelerator](https://oreil.ly/iG5UU)
    for this type of implementation that you can leverage during the development phase.
    There are several deployment and storage options. Feel free to explore the code
    to see the API call details.    ### Document indexing/retrieval-based grounding    The
    document indexing/retrieval-based grounding approach is an alternative to the
    embedding-based approach. In this case, we will not generate mathematical vectors.
    Instead, we will generate indexes of specific documents, so Azure OpenAI Service
    can find the information from those sources and include it as part of its answers.
    For that purpose, we will also use Azure Cognitive Search, which is a service
    that allows you to index, understand, and retrieve relevant data from a knowledge
    base or a collection of documents.    The combination of both services enables
    powerful chatbot applications that can communicate with users in natural language
    and provide intuitive and personalized interactions, based on specific data from
    the organization. Much like the embedding-based approach, there is an official
    [Microsoft accelerator](https://oreil.ly/JNWAz) available for you to deploy your
    first proof of concept, in addition to a second one called [GPT-RAG](https://oreil.ly/Q5NK9)
    from the Microsoft Argentina team, with some additional functionalities for bigger
    implementations. You can explore both to see updated details and implementation
    approaches with Azure OpenAI and Azure Cognitive Search. You can also see the
    high-level architecture of the key building blocks in [Figure 3-22](#fig_22_retrieval_based_grounding_architecture).  ![](assets/aoas_0322.png)  ######
    Figure 3-22\. Retrieval-based grounding architecture    The main difference when
    compared to the embedding-based approach is that instead of generating embeddings
    for both the knowledge base and the user question, you will just perform a search
    against the Azure AI Search engine (or any equivalent, as we will explore in [Chapter 4](ch04.html#additional_cloud_and_ai_capabilities)
    for vector databases).    You may see this option as something a bit simpler than
    the embeddings approach, and a better fit for applications where you need to find
    the source of information (and even provide a link to the original document as
    part of the answer); embeddings can potentially handle bigger datasets and deliver
    better performance. However, it really depends on the specific dataset and its
    knowledge scope and file format as well as the envisioned use case, so my recommendation
    is for you to try both options and evaluate the one that delivers best results
    from a user perspective.    ### Hybrid search–based grounding    There are newer
    implementation approaches based on [hybrid search techniques](https://oreil.ly/mwZPy).
    Concretely, hybrid search combines vector embeddings and doc retrieval capabilities.
    The [hybrid search feature](https://oreil.ly/c2W8A) from Azure AI Search offers
    that combination, plus a [reranking technique](https://oreil.ly/S7b8p) that produces
    the final result, with better performance than the previously mentioned grounding
    techniques. Now, let’s explore some additional grounding options that can add
    more knowledge scope to your generative AI applications.    ### Other grounding
    techniques    We have explored several fine-tuning and grounding techniques, mainly
    based on text information from different sources. But what happens if you want
    to leverage other kinds of data? Or if the required information can be found only
    via live internet results? Here are some other grounding techniques you may want
    to explore:    LLM + web results      This approach relies on the [Bing Web Search
    API](https://oreil.ly/qud-9) to extend the knowledge scope of Azure OpenAI Service
    models. As you may know, all LLMs are based on training datasets that go up to
    a specific date (e.g., initial Azure OpenAI models were updated with data up to
    2021). If you need updated information, you can use the Bing Web Search API to
    find web pages, images, videos, news, etc., or use it to create a custom search
    instance that filters web results based on the criteria. The result from the API
    can then be used by Azure OpenAI to return an answer based on that information.      LLM
    + tabular data and/or databases      Similar to other sources, tabular data (e.g.,
    Excel and CSV files) and regular SQL-type databases (e.g., SQL Server, Azure SQL,
    PostgreSQL) can be good grounding sources. You can develop what the industry calls
    Database Copilots to allow end users to query information without any complex
    SQL syntax, just natural language–based prompts. Or you can leverage it for [other
    data exploration](https://oreil.ly/s3snz) topics, such as exploratory data analysis
    or root-case analysis.      Just as with the other previous grounding options,
    there is an [official Microsoft accelerator](https://oreil.ly/eFneC) that combines
    these grounding techniques, with specific code samples and updated implementations.    At
    the end of the day, each implementation approach (baseline, fine-tuned, or grounding
    based) serves a different purpose, but the next section is a summarized guide
    for you to understand the pros and cons of each one, so you can make the most
    informed decision and create your generative AI applications with Azure OpenAI
    with the best balance of performance, cost, and technical complexity.` `` [PRE47]
    `` `## Approach Comparison and Final Recommendation    There is not a single right
    answer to the question, “Which approach should I use for my generative AI implementation?”
    It really depends on the use case, type and volume of available data, existing
    IT architectures, available budget and resources, etc. Again, there is no right
    answer, and the choice relies for now on experimentation and performance testing.    [Table 3-1](#table-3-1)
    shows the general pros and cons of the implementation approaches.      Table 3-1\.
    Comparison of implementation approaches with Azure OpenAI Service   |  | Approach
    | Pros | Cons | | --- | --- | --- | --- | | 1 | Basic ChatGPT-type instance (vanilla,
    private) |   *   Relatively simple and quick to deploy *   Good option for internal
    (employee) use cases *   Available via Azure OpenAI’s visual playground *   Option
    to define the topic scope based on URLs, by leveraging the system message   |   *   Lack
    of updated data *   Very limited for client-side applications *   Higher risk
    of model hallucination   | | 2 | Examples with one-shot/few-shot learning |   *   Easy
    to implement *   Good option to adapt system behavior based on specific pieces
    of knowledge from your company *   Available via Azure OpenAI’s visual playground   |   *   Lack
    of updated data *   Very limited for client-side applications *   Higher risk
    of model hallucination   | | 3 | Fine-tuning |   *   Good to fine-tune an existing
    model with specific company data *   Leverages mature product features   |   *   Complex
    to prepare input data for both fine-tuning and few-shot learning *   Increased
    cost for fine-tuned models   | | 4 | Embedding-based grounding (vectors with Azure
    AI Search) |   *   Great for customization without requiring fine-tuning *   Good
    fit for large amounts of data *   Easy use of embeddings APIs   |   *   Requires
    preparation of the input data based on token limits *   Need to scan files via
    OCR to extract content first *   Initial embeddings generation cost for custom
    data (depending on the data scope)   | | 5 | Retrieval-based grounding (indexing
    with Azure AI Search, no embeddings) |   *   Good option for information retrieval
    from existing files *   Indexing allows for citing sources (good for explainability)
    *   Option to use the “add your own data” option from the Playground, for small
    implementations   |   *   Potentially less performant than embeddings for large
    amounts of private data (to be confirmed during your preliminary experimentation)   |
    | 6 | Hybrid search |   *   More performant thanks to the combination of indexing,
    embeddings, and reranking of model results *   Relatively feasible via Azure OpenAI
    Playground   |   *   Complex, but for Azure OpenAI, no more than the regular embedding-based
    RAG   | | 7 | Other grounding techniques (Bing Search, databases, etc.) |   *   Great
    to add live results to the LLM, and to explore internal sources such as databases
    and tabular files *   Updated results with no need to retrain or adjust the model   |   *   A
    bit more complex (requires orchestration engines such as LangChain or Semantic
    Kernel) *   Less documentation available for this kind of implementation   |    These
    implementation approaches have different advantages and levels of complexity.
    One of the key aspects is the ability to evaluate how well they perform, and how
    good these Azure OpenAI models are for specific questions and tasks. Let’s explore
    all of this in the next section.    ## AI Performance Evaluation Methods    One
    of the key stages of any generative AI project is model performance evaluation.
    However, it is not a simple task to evaluate the performance of LLM-enabled systems,
    and it is not fully standardized yet. That said, you can start evaluating metrics
    with Azure OpenAI and Azure AI Studio, as you will see in [Chapter 5](ch05.html#operationalizing_generative_ai_implementations)
    with LLMOps and prompt flow for evals.    Here is a selection of the most important
    metrics for generative AI evaluation:    Groundedness      Groundedness refers
    to how well a generative AI’s responses are based on the information given or
    available in the input. This is a good metric to analyze how AI sticks to the
    facts, in order to avoid hallucinations. You can explore the new [Groundedness
    Detection feature](https://oreil.ly/Lk4ZI) from the AI Content Safety Studio.      Similarity      This
    metric measures how much a GPT output resembles that of a human one. This is useful
    for human validation of the results from Azure OpenAI models.      Relevance      It
    measures how connected an AI’s output is to the input given. It’s like checking
    if someone’s answer in a conversation is related to the question you asked.      Classification
    accuracy      A metric for classification tasks, between 0 and 1, that measures
    the output of the AI model compared to a ground truth.      Levenshtein distance      This
    measures how many changes, such as adding, deleting, or changing pieces, you would
    need to make to get from the AI’s output to the expected output.      Coherence      This
    checks if the AI’s output makes sense and follows a logical order, like checking
    if a story has a beginning, middle, and end, and doesn’t jump around randomly.      Fluency      This
    measures how smoothly the AI’s output reads, by checking if a written paragraph
    is easy to read and understand, from a linguistics and grammar point of view.      F1
    score      This is a balance between the words in the model answer and the ground
    truth.      Other metrics      Other metrics from traditional NLP.      From an
    Azure perspective, you can explore the available metrics for evaluation via [Azure
    AI Studio](https://oreil.ly/q2S7r) and [Azure Databricks with MLFlow](https://oreil.ly/3kONX).
    Here are several ongoing initiatives from some of the main industry actors (including
    Microsoft and OpenAI), but you can expect more news and tools in the upcoming
    months and years:    *   Microsoft’s [LLM evaluation framework](https://oreil.ly/H6gB8)           *   Microsoft’s
    [evaluation flows (Azure AI Studio)](https://oreil.ly/4NrIz)           *   Microsoft’s
    documentation for [LLM metrics monitoring](https://oreil.ly/VtjxD)           *   OpenAI’s
    [Evals project](https://oreil.ly/NgdLZ)              Additionally, there are other
    families of metrics that you can use to measure and analyze performance:    Positive/negative
    review of answers      A manual way to both track performance and potentially
    reeducate the model with weighted reconfigurations (e.g., few-shot learning with
    the good answers). You could enable this by using a positive/negative sign in
    the UI and by adding a binary numeric value at the database level if you decide
    to store the questions and answers for review purposes (e.g., ID, question, answer,
    review) in a JSON file stored via Cosmos DB. For this purpose, my recommendation
    is to create a set of test questions, and to involve subject-matter experts during
    the creation of that set and during the evaluation of the system.      Traditional
    product analytics metrics      For example, session time, amount of re-questioning
    to get the best answer, overall product rating, etc. This would require tools
    such as [Microsoft Clarity](https://oreil.ly/2RHm1), Pendo, Amplitude, Mixpanel,
    etc., connected to the cloud native app (e.g., iOS, Android, web, etc.). Alternatively,
    there are cloud native features such as [Azure App Insights](https://oreil.ly/HXkzL)
    that can be deployed as part of the generative AI app monitoring system. Additionally,
    these tools can be leveraged to track performance for A/B testing experiments
    (for example, if we launch two different versions of the AI model with different
    user sets).` `` [PRE48][PRE49][PRE50]``py[PRE51]py[PRE52]py`  [PRE53]'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
