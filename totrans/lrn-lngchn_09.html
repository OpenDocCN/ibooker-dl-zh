<html><head></head><body><section data-pdf-bookmark="Chapter 9. Deployment: Launching Your AI Application into Production" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch09_deployment_launching_your_ai_application_into_pro_1736545675509604">
<h1><span class="label">Chapter 9. </span>Deployment: Launching Your <span class="keep-together">AI Application into Production</span></h1>

<p>So far, we’ve explored the key concepts, ideas, and tools to help you build the core functionality of your AI application. You’ve learned how to utilize LangChain and LangGraph to generate LLM outputs, index and retrieve data, and enable memory and agency.</p>

<p>But your application is limited to your local environment, so external users can’t access its features yet.</p>

<p>In this chapter, you’ll learn the best practices for deploying your AI application into production. We’ll also explore various tools to debug, collaborate, test, and monitor your LLM applications.</p>

<p>Let’s get started.</p>

<section data-pdf-bookmark="Prerequisites" data-type="sect1"><div class="sect1" id="ch09_prerequisites_1736545675509817">
<h1>Prerequisites</h1>

<p class="fix_tracking">In<a contenteditable="false" data-primary="deployment" data-secondary="prerequisites" data-type="indexterm" id="Dpre09"/> order to effectively deploy your AI application, you need to utilize various services to host your application, store and retrieve data, and monitor your application. In the deployment example in this chapter, we will incorporate the following services:</p>

<dl>
	<dt>Vector store</dt>
	<dd>
	<p>Supabase</p>
	</dd>
	
	
	<dt>Monitoring and debugging</dt>
	<dd>
	<p>LangSmith</p>
	</dd>
	<dt>Backend API</dt>
	<dd>
	<p>LangGraph Platform</p>
	</dd>
</dl>

<p>We will dive deeper into each of these components and services and see how to adapt them for your use case. But first, let’s install necessary dependencies and set up the environment variables.</p>

<p>If you’d like to follow the example, fork this <a href="https://oreil.ly/brqVm">LangChain template</a> to your GitHub account. This repository contains the full logic of a retrieval agent-based AI <span class="keep-together">application.</span></p>

<section data-pdf-bookmark="Install Dependencies" data-type="sect2"><div class="sect2" id="ch09_install_dependencies_1736545675509897">
<h2>Install Dependencies</h2>

<p>First, follow<a contenteditable="false" data-primary="dependencies, installing" data-type="indexterm" id="id839"/> the instructions in the <a href="https://oreil.ly/N5eqe"><em>README.md</em> file</a> to install the project <span class="keep-together">dependencies.</span></p>

<p>If you’re not using the template, you can install the dependencies individually from the respective <em>pyproject.toml</em> or <em>package.json</em> files.</p>

<p>Second, create a <em>.env</em> file and store the following variables:</p>

<pre class="pre" data-type="programlisting">
OPENAI_API_KEY=
SUPABASE_URL=
SUPABASE_SERVICE_ROLE_KEY=

# for tracing
LANGCHAIN_TRACING_V2=true
LANGCHAIN_ENDPOINT="https://api.smith.langchain.com"
LANGCHAIN_API_KEY=</pre>

<p>Next, we’ll walk through the process of retrieving the values for each of these <span class="keep-together">variables.</span></p>
</div></section>

<section data-pdf-bookmark="Large Language Model" data-type="sect2"><div class="sect2" id="ch09_large_language_model_1736545675509961">

<h2>Large Language Model</h2>

<p>The<a contenteditable="false" data-primary="large language models (LLMs)" data-secondary="deployment example" data-type="indexterm" id="id840"/> LLM is responsible for generating the output based on a given query. LangChain provides access to popular LLM providers, including OpenAI, Anthropic, Google, and Cohere.</p>

<p>In this deployment example, we’ll utilize OpenAI by<a contenteditable="false" data-primary="API keys, retrieving" data-type="indexterm" id="id841"/> retrieving the <a href="https://oreil.ly/MIpY5">API keys</a>, as shown in <a data-type="xref" href="#ch09_figure_1_1736545675498459">Figure 9-1</a>. Once you’ve retrieved your API keys, input the value as <code>OPENAI_API_KEY</code> in your <em>.env</em> file.</p>

<figure><div class="figure" id="ch09_figure_1_1736545675498459">
<img alt="A screenshot of a computer  Description automatically generated" src="assets/lelc_0901.png"/>

<h6><span class="label">Figure 9-1. </span>OpenAI API keys dashboard
</h6>
</div></figure>


</div></section>

<section data-pdf-bookmark="Vector Store" data-type="sect2"><div class="sect2" id="ch09_vector_store_1736545675510022">
<h2>Vector Store</h2>

<p>As<a contenteditable="false" data-primary="vector stores" data-secondary="deployment example" data-type="indexterm" id="VSdeploy09"/> discussed in previous chapters, a vector store is a special database responsible for storing and managing vector representations of your data—in other words, embeddings. A vector store enables similarity search and context retrieval to help the LLM generate accurate answers based on the user’s query.</p>

<p>For our deployment, we’ll use<a contenteditable="false" data-primary="Supabase" data-type="indexterm" id="supabase09"/> Supabase—a PostgreSQL database—as the vector store. Supabase utilizes the <code>pgvector</code> extension to store embeddings and query vectors for similarity search.</p>

<p>If you haven’t yet done it, create a <a href="https://oreil.ly/CXDsx">Supabase account</a>. Once you’ve created an account, click “New project” on the dashboard page. Follow the steps and save the database password after creating it, as shown in <a data-type="xref" href="#ch09_figure_2_1736545675498492">Figure 9-2</a>.</p>

<figure><div class="figure" id="ch09_figure_2_1736545675498492"><img alt="A screenshot of a computer  Description automatically generated" src="assets/lelc_0902.png"/>
<h6><span class="label">Figure 9-2. </span>Supabase project creation dashboard</h6>
</div></figure>

<p>Once your Supabase project is created, navigate to the Project Settings tab and select API under Configuration. Under this new tab, you will see Project URL and Project API keys.</p>

<p>In your <em>.env</em> file, copy and paste the Project URL as the value to <code>SUPABASE_URL</code> and the <code>service_role</code> secret API key as the value to <code>SUPABASE_SERVICE_ROLE_KEY</code>.</p>

<p>Navigate to the SQL editor in the Supabase menu and run the following SQL scripts. First, let’s enable <code>pgvector</code>:</p>

<pre data-type="programlisting">
## Enable the pgvector extension to work with embedding vectors
create extension vector;</pre>

<p>Now create a table called <code>documents</code> to store vectors of your data:</p>

<pre data-type="programlisting">
## Create a table to store your documents

create table documents (
  id bigserial primary key,
  content text, -- corresponds to Document.pageContent
  metadata jsonb, -- corresponds to Document.metadata
  embedding vector(1536) -- 1536 works for OpenAI embeddings, change if needed
);</pre>

<p>You should now see the <code>documents</code> table in the Supabase database.</p>

<p>Now you can create a script to generate the embeddings of your data, store them, and query from the database. Open the Supabase SQL editor again and run the following script:</p>

<pre data-type="programlisting">
## Create a function to search for documents
create function match_documents (
  query_embedding vector(1536),
  match_count int DEFAULT null,
  filter jsonb DEFAULT '{}'
) returns table (
  id bigint,
  content text,
  metadata jsonb,
  embedding jsonb,
  similarity float
)
language plpgsql
as $$
#variable_conflict use_column
begin
  return query
  select
    id,
    content,
    metadata,
    (embedding::text)::jsonb as embedding,
    1 - (documents.embedding &lt;=&gt; query_embedding) as similarity
  from documents
  where metadata @&gt; filter
  order by documents.embedding &lt;=&gt; query_embedding
  limit match_count;
end;
$$;</pre>

<p>The <code>match_documents</code> database function takes a <code>query_embedding</code> vector and compares it to embeddings in the <code>documents</code> table using cosine similarity. It calculates a similarity score for each document (1 - (<code>documents.embedding</code> &lt;=&gt; <span class="keep-together"><code>query_embedding</code></span>)), then returns the most similar matches. The results are:</p>
<ol>
	<li>Filtered first by the metadata criteria specified in the filter argument (using JSON containment @&gt;).</li>
	<li>Ordered by similarity score (highest first).</li>
	<li>Limited to the number of matches specified in <code>match_count</code>.</li>
</ol>

<p>Once the vector similarity function is generated, you can use Supabase as a vector store by importing the class and providing the necessary parameters. Here’s an example of how it works:</p>

<p><em>Python</em></p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">os</code>

<code class="kn">from</code> <code class="nn">langchain_community.vectorstores</code> <code class="kn">import</code> <code class="n">SupabaseVectorStore</code>
<code class="kn">from</code> <code class="nn">langchain_openai</code> <code class="kn">import</code> <code class="n">OpenAIEmbeddings</code>
<code class="kn">from</code> <code class="nn">supabase.client</code> <code class="kn">import</code> <code class="n">Client</code><code class="p">,</code> <code class="n">create_client</code>

<code class="n">supabase_url</code> <code class="o">=</code> <code class="n">os</code><code class="o">.</code><code class="n">environ</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"SUPABASE_URL"</code><code class="p">)</code>
<code class="n">supabase_key</code> <code class="o">=</code> <code class="n">os</code><code class="o">.</code><code class="n">environ</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"SUPABASE_SERVICE_ROLE_KEY"</code><code class="p">)</code>
<code class="n">supabase</code><code class="p">:</code> <code class="n">Client</code> <code class="o">=</code> <code class="n">create_client</code><code class="p">(</code><code class="n">supabase_url</code><code class="p">,</code> <code class="n">supabase_key</code><code class="p">)</code>

<code class="n">embeddings</code> <code class="o">=</code> <code class="n">OpenAIEmbeddings</code><code class="p">()</code>


<code class="c1">## Assuming you've already generated embeddings of your data</code>

<code class="n">vector_store</code> <code class="o">=</code> <code class="n">SupabaseVectorStore</code><code class="p">(</code>
    <code class="n">embedding</code><code class="o">=</code><code class="n">embeddings</code><code class="p">,</code>
    <code class="n">client</code><code class="o">=</code><code class="n">supabase</code><code class="p">,</code>
    <code class="n">table_name</code><code class="o">=</code><code class="s2">"documents"</code><code class="p">,</code>
    <code class="n">query_name</code><code class="o">=</code><code class="s2">"match_documents"</code><code class="p">,</code>
<code class="p">)</code>

<code class="c1">## Test that similarity search is working</code>

<code class="n">query</code> <code class="o">=</code> <code class="s2">"What is this document about?"</code>
<code class="n">matched_docs</code> <code class="o">=</code> <code class="n">vector_store</code><code class="o">.</code><code class="n">similarity_search</code><code class="p">(</code><code class="n">query</code><code class="p">)</code>

<code class="nb">print</code><code class="p">(</code><code class="n">matched_docs</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">page_content</code><code class="p">)</code></pre>

<p><em>JavaScript</em></p>

<pre data-code-language="javascript" data-type="programlisting">
<code class="kr">import</code> <code class="p">{</code>
  <code class="nx">SupabaseVectorStore</code>
<code class="p">}</code> <code class="nx">from</code> <code class="s2">"@langchain/community/vectorstores/supabase"</code><code class="p">;</code>
<code class="kr">import</code> <code class="p">{</code> <code class="nx">OpenAIEmbeddings</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"@langchain/openai"</code><code class="p">;</code>

<code class="kr">import</code> <code class="p">{</code> <code class="nx">createClient</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"@supabase/supabase-js"</code><code class="p">;</code>

<code class="kr">const</code> <code class="nx">embeddings</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">OpenAIEmbeddings</code><code class="p">();</code>

<code class="kr">const</code> <code class="nx">supabaseClient</code> <code class="o">=</code> <code class="nx">createClient</code><code class="p">(</code>
  <code class="nx">process</code><code class="p">.</code><code class="nx">env</code><code class="p">.</code><code class="nx">SUPABASE_URL</code><code class="p">,</code>
  <code class="nx">process</code><code class="p">.</code><code class="nx">env</code><code class="p">.</code><code class="nx">SUPABASE_SERVICE_ROLE_KEY</code>
<code class="p">);</code>

<code class="kr">const</code> <code class="nx">vectorStore</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">SupabaseVectorStore</code><code class="p">(</code><code class="nx">embeddings</code><code class="p">,</code> <code class="p">{</code>
  <code class="nx">client</code><code class="o">:</code> <code class="nx">supabaseClient</code><code class="p">,</code>
  <code class="nx">tableName</code><code class="o">:</code> <code class="s2">"documents"</code><code class="p">,</code>
  <code class="nx">queryName</code><code class="o">:</code> <code class="s2">"match_documents"</code><code class="p">,</code>
<code class="p">});</code>

<code class="c1">// Example documents structure of your data</code>

<code class="kr">const</code> <code class="nx">document1</code><code class="o">:</code> <code class="nx">Document</code> <code class="o">=</code> <code class="p">{</code>
  <code class="nx">pageContent</code><code class="o">:</code> <code class="s2">"The powerhouse of the cell is the mitochondria"</code><code class="p">,</code>
  <code class="nx">metadata</code><code class="o">:</code> <code class="p">{</code> <code class="nx">source</code><code class="o">:</code> <code class="s2">"https://example.com"</code> <code class="p">},</code>
<code class="p">};</code>

<code class="kr">const</code> <code class="nx">document2</code><code class="o">:</code> <code class="nx">Document</code> <code class="o">=</code> <code class="p">{</code>
  <code class="nx">pageContent</code><code class="o">:</code> <code class="s2">"Buildings are made out of brick"</code><code class="p">,</code>
  <code class="nx">metadata</code><code class="o">:</code> <code class="p">{</code> <code class="nx">source</code><code class="o">:</code> <code class="s2">"https://example.com"</code> <code class="p">},</code>
<code class="p">};</code>

<code class="kr">const</code> <code class="nx">documents</code> <code class="o">=</code> <code class="p">[</code><code class="nx">document1</code><code class="p">,</code> <code class="nx">document2</code><code class="p">]</code>

<code class="c1">//Embed and store the data in the database</code>

<code class="nx">await</code> <code class="nx">vectorStore</code><code class="p">.</code><code class="nx">addDocuments</code><code class="p">(</code><code class="nx">documents</code><code class="p">,</code> <code class="p">{</code> <code class="nx">ids</code><code class="o">:</code> <code class="p">[</code><code class="s2">"1"</code><code class="p">,</code> <code class="s2">"2"</code><code class="p">]</code> <code class="p">});</code>

<code class="c1">// Query the Vector Store</code>

<code class="kr">const</code> <code class="nx">filter</code> <code class="o">=</code> <code class="p">{</code> <code class="nx">source</code><code class="o">:</code> <code class="s2">"https://example.com"</code> <code class="p">};</code>

<code class="kr">const</code> <code class="nx">similaritySearchResults</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">vectorStore</code><code class="p">.</code><code class="nx">similaritySearch</code><code class="p">(</code>
  <code class="s2">"biology"</code><code class="p">,</code>
  <code class="mi">2</code><code class="p">,</code>
  <code class="nx">filter</code>
<code class="p">);</code>

<code class="k">for</code> <code class="p">(</code><code class="kr">const</code> <code class="nx">doc</code> <code class="k">of</code> <code class="nx">similaritySearchResults</code><code class="p">)</code> <code class="p">{</code>
  <code class="nx">console</code><code class="p">.</code><code class="nx">log</code><code class="p">(</code><code class="sb">`* </code><code class="si">${</code><code class="nx">doc</code><code class="p">.</code><code class="nx">pageContent</code><code class="si">}</code><code class="sb"> [</code><code class="si">${</code><code class="nx">JSON</code><code class="p">.</code><code class="nx">stringify</code><code class="p">(</code><code class="nx">doc</code><code class="p">.</code><code class="nx">metadata</code><code class="p">,</code> <code class="kc">null</code><code class="p">)</code><code class="si">}</code><code class="sb">]`</code><code class="p">);</code>
<code class="p">}</code>
</pre>

<p>
<em>The output:</em>
</p>
<pre>
The powerhouse of the cell is the mitochondria [{"source":"https://example.com"}]
</pre>
<p>You can review the full logic of the Supabase vector store implementation in the Github LangChain template mentioned previously.</p>
</div></section>


<section data-pdf-bookmark="Backend API" data-type="sect2"><div class="sect2" id="ch09_backend_api_1736545675510199">
<h2>Backend API</h2>

<p>As<a contenteditable="false" data-primary="LangGraph" data-secondary="control flow illustration" data-type="indexterm" id="id842"/> discussed in previous chapters, LangGraph is a low-level open source framework used to build complex agentic systems powered by LLMs. LangGraph enables fine-grained control over the flow and state of your application, built-in persistence, and advanced human-in-the-loop and memory features. <a data-type="xref" href="#ch09_figure_3_1736545675498514">Figure 9-3</a> illustrates <span class="keep-together">LangGraph’s</span> control flow.</p>

<figure><div class="figure" id="ch09_figure_3_1736545675498514"><img alt="A diagram of a software flowchart  Description automatically generated" src="assets/lelc_0903.png"/>
<h6><span class="label">Figure 9-3. </span>Example of LangGraph API control flow</h6>
</div></figure>

<p>To<a contenteditable="false" data-primary="LangGraph Platform API" data-secondary="deployment using" data-type="indexterm" id="id843"/> deploy an AI application that utilizes LangGraph, we will use LangGraph Platform. LangGraph Platform is a managed service for deploying and hosting LangGraph agents at scale.</p>

<p>As your agentic use case gains traction, uneven task distribution among agents can overload the system, leading to downtime. LangGraph Platform manages horizontally scaling task queues, servers, and a robust Postgres checkpointer to handle many concurrent users and efficiently store large states and threads. This ensures fault-tolerant scalability.</p>

<p>LangGraph Platform is designed to support real-world interaction patterns. In addition to streaming and human-in-the-loop features, LangGraph Platform enables the following:</p>

<ul>
	<li>
	<p>Double texting<strong> </strong>to handle new user inputs on ongoing graph threads</p>
	</li>
	<li>
	<p>Asynchronous background jobs for long-running tasks</p>
	</li>
	<li>
	<p>Cron jobs for running common tasks on a schedule</p>
	</li>
</ul>

<p>LangGraph Platform also provides an integrated solution for collaborating on, deploying, and monitoring agentic AI applications. It includes <a href="https://oreil.ly/2Now-">LangGraph Studio</a>—a visual playground for debugging, editing, and testing agents. LangGraph Studio also enables you to share your LangGraph agent with team members for collaborative feedback and rapid iteration, as <a data-type="xref" href="#ch09_figure_4_1736545675498542">Figure 9-4</a> shows.</p>

<figure><div class="figure" id="ch09_figure_4_1736545675498542"><img alt="Screens screenshot of a computer  Description automatically generated" src="assets/lelc_0904.png"/>
<h6><span class="label">Figure 9-4. </span>Snapshot of LangGraph Studio UI</h6>
</div></figure>

<p>Additionally, LangGraph Platform simplifies agentic deployment by enabling one-click submissions.</p>
</div></section>

<section data-pdf-bookmark="Create a LangSmith Account" data-type="sect2"><div class="sect2" id="ch09_create_a_langsmith_account_1736545675510258">
<h2>Create a LangSmith Account</h2>

<p>LangSmith<a contenteditable="false" data-primary="LangSmith" data-secondary="account creation" data-type="indexterm" id="id844"/> is an all-in-one developer platform that enables you to debug, collaborate, test, and monitor your LLM applications. LangGraph Platform is seamlessly integrated with LangSmith and is accessible from within the LangSmith UI.</p>

<p>To deploy your application on LangGraph Platform, you need to create a <a href="https://oreil.ly/2WVCn">LangSmith account</a>. Once you’re logged in to the dashboard, navigate to the Settings page, then scroll to the API Keys section and click Create API Key. You should see a UI similar to <a data-type="xref" href="#ch09_figure_5_1736545675498579">Figure 9-5</a>.</p>

<figure><div class="figure" id="ch09_figure_5_1736545675498579"><img alt="A screenshot of a application  Description automatically generated" src="assets/lelc_0905.png"/>
<h6><span class="label">Figure 9-5. </span>Create LangSmith API Key UI</h6>
</div></figure>

<p>Copy the API Key value as your <code>LANGCHAIN_API_KEY</code> in your <em>.env</em> file.</p>

<p>Navigate to “Usage and billing” and set up your billing details. Then click the “Plans and Billings” tab and the “Upgrade to Plus” button to get instructions on transitioning to a LangSmith Plus plan, which will enable LangGraph Platform usage.<a contenteditable="false" data-primary="" data-startref="Dpre09" data-type="indexterm" id="id845"/> If you’d prefer to use a free self-hosted deployment, you can follow the <a href="https://oreil.ly/TBgSQ">instructions here</a>. Please note that this option requires management of the infrastructure, including setting up and maintaining required databases and Redis instances.</p>
</div></section>
</div></section>

<section data-pdf-bookmark="Understanding the LangGraph Platform API" data-type="sect1"><div class="sect1" id="ch09_understanding_the_langgraph_cloud_api_1736545675510326">
<h1>Understanding the LangGraph Platform API</h1>

<p>Before deploying your AI application on LangGraph Platform, it’s important to understand how each component of the LangGraph API works. These components can generally be split into data models and features.</p>

<section data-pdf-bookmark="Data Models" data-type="sect2"><div class="sect2" id="ch09_data_models_1736545675510393">
<h2>Data Models</h2>

<p>The<a contenteditable="false" data-primary="LangGraph Platform API" data-secondary="data models" data-type="indexterm" id="id846"/><a contenteditable="false" data-primary="data models (LangGraph Platform)" data-type="indexterm" id="id847"/> LangGraph Platform API consists of a few core data models:</p>

<ul>
	<li>
	<p>Assistants</p>
	</li>
	<li>
	<p>Threads</p>
	</li>
	<li>
	<p>Runs</p>
	</li>
	<li>
	<p>Cron jobs</p>
	</li>
</ul>

<section data-pdf-bookmark="Assistants" data-type="sect3"><div class="sect3" id="ch09_assistants_1736545675510449">
<h3>Assistants</h3>

<p>An<a contenteditable="false" data-primary="assistants (LangGraph Platform)" data-type="indexterm" id="id848"/> <em>assistant</em> is a configured instance of a <code>CompiledGraph</code>. It abstracts the cognitive architecture of the graph and contains instance-specific configuration and metadata. Multiple assistants can reference the same graph but can contain different configuration and metadata—which may differentiate the behavior of the assistants. An assistant (that is, the graph) is invoked as part of a run.</p>

<p>The LangGraph Platform API provides several endpoints for creating and managing assistants.</p>
</div></section>

<section data-pdf-bookmark="Threads" data-type="sect3"><div class="sect3" id="ch09_threads_1736545675510504">
<h3>Threads</h3>

<p>A <em>thread</em> contains<a contenteditable="false" data-primary="threads (LangGraph Platform)" data-type="indexterm" id="id849"/> the accumulated state of a group of runs. If a run is executed on a thread, then the state of the underlying graph of the assistant will be persisted to the thread. A thread’s current and historical state can be retrieved. To persist state, a thread must be created prior to executing a run. The state of a thread at a particular point in time is called a <em>checkpoint</em>.</p>

<p>The LangGraph Platform API provides several endpoints for creating and managing threads and thread state.</p>
</div></section>

<section data-pdf-bookmark="Runs" data-type="sect3"><div class="sect3" id="ch09_runs_1736545675510561">
<h3>Runs</h3>

<p>A <em>run</em> is<a contenteditable="false" data-primary="runs (LangGraph Platform)" data-type="indexterm" id="id850"/> an invocation of an assistant. Each run may have its own input, configuration, and metadata—which may affect the execution and output of the underlying graph. A run can optionally be executed on a thread.</p>

<p>The LangGraph Platform API provides several endpoints for creating and managing runs.</p>
</div></section>

<section class="pagebreak-before" data-pdf-bookmark="Cron jobs" data-type="sect3"><div class="sect3" id="ch09_cron_jobs_1736545675510616">
<h3 class="less_space">Cron jobs</h3>

<p>LangGraph Platform<a contenteditable="false" data-primary="cron jobs (LangGraph Platform)" data-type="indexterm" id="id851"/> supports <em>cron jobs</em>, which enable graphs to be run on a user-defined schedule. The user specifies a schedule, an assistant, and an input. Then LangGraph Platform creates a new thread with the specified assistant and sends the specified input to that thread.</p>
</div></section>
</div></section>

<section data-pdf-bookmark="Features" data-type="sect2"><div class="sect2" id="ch09_features_1736545675510676">
<h2>Features</h2>

<p>The<a contenteditable="false" data-primary="LangGraph Platform API" data-secondary="features" data-type="indexterm" id="LGCAfeatures09"/> LangGraph Platform API also offers several features to support complex agent architectures, including the following:</p>

<ul>
	<li>
	<p>Streaming</p>
	</li>
	<li>
	<p>Human-in-the-loop</p>
	</li>
	<li>
	<p>Double texting</p>
	</li>
	<li>
	<p>Stateless runs</p>
	</li>
	<li>
	<p>Webhooks</p>
	</li>
</ul>

<section data-pdf-bookmark="Streaming" data-type="sect3"><div class="sect3" id="ch09_streaming_1736545675510735">
<h3>Streaming</h3>

<p>Streaming<a contenteditable="false" data-primary="streaming (LangGraph Platform)" data-type="indexterm" id="id852"/> is critical for ensuring that LLM applications feel responsive to end users. When creating a streaming run, the streaming mode determines what data is streamed back to the API client. The LangGraph Platform API supports five streaming modes:</p>

<dl>
	<dt>Values</dt>
	<dd>
	<p>Stream the full state of the graph after each super-step is executed.</p>
	</dd>
	<dt>Messages</dt>
	<dd>
	<p>Stream complete messages (at the end of node execution) as well as tokens for any messages generated inside a node. This mode is primarily meant for powering chat applications. This is only an option if your graph contains a <code>messages</code> key.</p>
	</dd>
	<dt>Updates</dt>
	<dd>
	<p>Stream updates to the state of the graph after each node is executed.</p>
	</dd>
	<dt>Events</dt>
	<dd>
	<p>Stream all events (including the state of the graph) that occur during graph execution. This can be used to do token-by-token streaming for LLMs.</p>
	</dd>
	<dt>Debug</dt>
	<dd>
	<p>Stream debug events throughout graph execution.</p>
	</dd>
</dl>
</div></section>

<section data-pdf-bookmark="Human-in-the-loop" data-type="sect3"><div class="sect3" id="ch09_human_in_the_loop_1736545675510794">
<h3>Human-in-the-loop</h3>

<p class="fix_tracking">If<a contenteditable="false" data-primary="human-in-the-loop modalities" data-secondary="LangGraph Platform API" data-type="indexterm" id="id853"/> left to run autonomously, a complex agent can take unintended actions, leading to catastrophic application outcomes. To prevent this, human intervention is recommended, especially at checkpoints where application logic involves invoking certain tools or accessing specific documents. LangGraph Platform enables you to insert this human-in-the-loop behavior to ensure your graph doesn’t have undesired outcomes.</p>
</div></section>

<section data-pdf-bookmark="Double texting" data-type="sect3"><div class="sect3" id="ch09_double_texting_1736545675510851">
<h3>Double texting</h3>

<p class="fix_tracking">Graph execution<a contenteditable="false" data-primary="double texting (LangGraph Platform)" data-type="indexterm" id="id854"/> may take longer than expected, and often users may send one message and then, before the graph has finished running, send a second message. This is known as <em>double texting</em>. For example, a user might notice a typo in their original request and edit the prompt and resend it. In such scenarios, it’s important to prevent your graphs from behaving in unexpected ways and ensure a smooth user experience. LangGraph Platform provides four different solutions to handle double texting:</p>

<dl>
	<dt>Reject</dt>
	<dd>
	<p>This rejects any follow-up runs and does not allow double texting.</p>
	</dd>
	<dt>Enqueue</dt>
	<dd>
	<p>This option continues the first run until it completes the whole run, then sends the new input as a separate run.</p>
	</dd>
	<dt>Interrupt</dt>
	<dd>
	<p>This option interrupts the current execution but saves all the work done up until that point. It then inserts the user input and continues from there. If you enable this option, your graph should be able to handle weird edge cases that may arise.</p>
	</dd>
	<dt>Rollback</dt>
	<dd>
	<p>This option rolls back all work done up until that point. It then sends the user input in—as if it just followed the original run input.</p>
	</dd>
</dl>
</div></section>

<section data-pdf-bookmark="Stateless runs" data-type="sect3"><div class="sect3" id="ch09_stateless_runs_1736545675510907">
<h3>Stateless runs</h3>

<p>All<a contenteditable="false" data-primary="stateless runs (LangGraph Platform)" data-type="indexterm" id="id855"/> runs use the built-in checkpointer to store checkpoints for runs. However, it can often be useful to just kick off a run without worrying about explicitly creating a thread and keeping those checkpointers around. <em>Stateless</em> runs allow you to do this by exposing an endpoint that does these things:</p>

<ul>
	<li>
	<p>Takes in user input</p>
	</li>
	<li>
	<p>Creates a thread</p>
	</li>
	<li>
	<p>Runs the agent, but skips all checkpointing steps</p>
	</li>
	<li>
	<p>Cleans up the thread afterwards</p>
	</li>
</ul>

<p>Stateless runs are retried while keeping memory intact. However, in the case of stateless background runs, if the task worker dies halfway, the entire run will be retried from scratch.</p>
</div></section>

<section data-pdf-bookmark="Webhooks" data-type="sect3"><div class="sect3" id="ch09_webhooks_1736545675510960">
<h3>Webhooks</h3>

<p>LangGraph Platform also supports completion<a contenteditable="false" data-primary="webhooks (LangGraph Platform)" data-type="indexterm" id="id856"/> <em>webhooks</em>. A webhook URL is provided, which notifies your application whenever a run completes.<a contenteditable="false" data-primary="" data-startref="LGCAfeatures09" data-type="indexterm" id="id857"/></p>
</div></section>
</div></section>
</div></section>

<section data-pdf-bookmark="Deploying Your AI Application on LangGraph Platform" data-type="sect1"><div class="sect1" id="ch09_deploying_your_ai_application_on_langgraph_cloud_1736545675511031">
<h1>Deploying Your AI Application on LangGraph Platform</h1>

<p>At this point, you have created accounts for the recommended services, filled in your <em>.env</em> file with values of all necessary environment variables, and completed the core logic for your AI application. Next, we will take the necessary steps to effectively deploy your application.</p>

<section data-pdf-bookmark="Create a LangGraph API Config" data-type="sect2"><div class="sect2" id="ch09_create_a_langgraph_api_config_1736545675511101">
<h2>Create a LangGraph API Config</h2>

<p>Prior<a contenteditable="false" data-primary="deployment" data-secondary="LangGraph API configuration file" data-type="indexterm" id="id858"/><a contenteditable="false" data-primary="LangGraph" data-secondary="configuration file" data-type="indexterm" id="id859"/> to deployment, you need to configure your application with a <a href="https://oreil.ly/aVDhd">LangGraph API configuration file called <em>langgraph.json</em></a>. Here’s an example of what the file looks like in a Python repository:</p>

<p><em>Python</em></p>

<pre data-code-language="python" data-type="programlisting">
<code class="p">{</code>
    <code class="s2">"dependencies"</code><code class="p">:</code> <code class="p">[</code><code class="s2">"./my_agent"</code><code class="p">],</code>
    <code class="s2">"graphs"</code><code class="p">:</code> <code class="p">{</code>
        <code class="s2">"agent"</code><code class="p">:</code> <code class="s2">"./my_agent/agent.py:graph"</code>
    <code class="p">},</code>
    <code class="s2">"env"</code><code class="p">:</code> <code class="s2">".env"</code>
<code class="p">}</code></pre>

<p>And here’s an example repository structure:</p>

<pre data-type="programlisting">
my-app/
├── my_agent # all project code lies within here
│   ├── utils # utilities for your graph
│   │   ├── __init__.py
│   │   ├── tools.py # tools for your graph
│   │   ├── nodes.py # node functions for you graph
│   │   └── state.py # state definition of your graph
│   ├── requirements.txt # package dependencies
│   ├── __init__.py
│   └── agent.py # code for constructing your graph
├── .env # environment variables
└── langgraph.json # configuration file for LangGraph</pre>

<p>Note that the <em>langgraph.json</em> file is placed on the same level or higher than the files that contain compiled graphs and associated dependencies.</p>

<p>In addition, the dependencies are specified in a <em>requirements.txt</em> file. But they can also be specified in <em>pyproject.toml</em>, <em>setup.py</em>, or <em>package.json</em> files.</p>

<p>Here’s what each of the properties mean:</p>

<dl>
	<dt>Dependencies</dt>
	<dd>
	<p>Array of dependencies for LangGraph Platform API server</p>
	</dd>
	<dt>Graphs</dt>
	<dd>
	<p>Mapping from graph ID to path where the compiled graph or a function that makes a graph is defined</p>
	</dd>
	<dt>Env</dt>
	<dd>
	<p>Path to your <em>.env</em> file or a mapping from environment variable to its value (you can learn more about configurations for the <code>langgraph.json</code> file <a href="https://oreil.ly/bPA0W">here</a>)</p>
	</dd>
</dl>
</div></section>

<section data-pdf-bookmark="Test Your LangGraph App Locally" data-type="sect2"><div class="sect2" id="ch09_test_your_langgraph_app_locally_1736545675511163">
<h2>Test Your LangGraph App Locally</h2>

<p>Testing<a contenteditable="false" data-primary="deployment" data-secondary="testing LangGraph app locally" data-type="indexterm" id="Dtestlocal09"/><a contenteditable="false" data-primary="testing" data-secondary="predeployment" data-type="indexterm" id="Tpredeploy09"/> your application locally ensures that there are no errors or dependency conflicts prior to deployment. To do this, we will utilize the LangGraph CLI, which includes commands to run a local development server with hot reloading and debugging capabilities.</p>

<p>For Python, install the Python <code>langgraph-cli</code> package (note: this requires Python 3.11 or higher):</p>

<pre data-type="programlisting">
pip install -U "langgraph-cli[inmem]"</pre>

<p>Or for JavaScript, install the package as follows:</p>
<pre data-type="programlisting">
npm i @langchain/langgraph-cli
</pre>

<p>Once the CLI is installed, run the following command to start the API:</p>

<pre data-type="programlisting">
langgraph dev</pre>

<p>This will start up the LangGraph API server locally. If this runs successfully, you should see something like this:</p>

<pre data-type="programlisting">
Ready!
API: http://localhost:2024
Docs: http://localhost:2024/docs</pre>

<p>The LangGraph Platform API reference is available with each deployment at the <em>/docs</em> URL path (<em>http://localhost:2024/docs</em>).</p>

<p class="pagebreak-before less_space">The easiest way to interact with your local API server is to use the auto-launched LangGraph Studio UI. Alternatively, you can interact with the local API server using cURL, as seen in this example:</p>

<pre data-type="programlisting">
curl --request POST \
    --url http://localhost:8123/runs/stream \
    --header 'Content-Type: application/json' \
    --data '{
    "assistant_id": "agent",
    "input": {
        "messages": [
            {
                "role": "user",
                "content": "How are you?"
            }
        ]
    },
    "metadata": {},
    "config": {
        "configurable": {}
    },
    "multitask_strategy": "reject",
    "stream_mode": [
        "values"
    ]
}'</pre>

<p>If you receive a valid response, your application is functioning well. Next, we can interact with the server using the LangGraph SDK.</p>

<p>Here’s an example both initializing the SDK client and invoking the graph:</p>

<p><em>Python</em></p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">langgraph_sdk</code> <code class="kn">import</code> <code class="n">get_client</code>

<code class="c1"># only pass the url argument to get_client() if you changed the default port </code>
<code class="c1"># when calling langgraph up</code>
<code class="n">client</code> <code class="o">=</code> <code class="n">get_client</code><code class="p">()</code>
<code class="c1"># Using the graph deployed with the name "agent"</code>
<code class="n">assistant_id</code> <code class="o">=</code> <code class="s2">"agent"</code>
<code class="n">thread</code> <code class="o">=</code> <code class="k">await</code> <code class="n">client</code><code class="o">.</code><code class="n">threads</code><code class="o">.</code><code class="n">create</code><code class="p">()</code>


<code class="nb">input</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"messages"</code><code class="p">:</code> <code class="p">[{</code><code class="s2">"role"</code><code class="p">:</code> <code class="s2">"user"</code><code class="p">,</code> <code class="s2">"content"</code><code class="p">:</code> <code class="s2">"what's the weather in sf"</code><code class="p">}]}</code>
<code class="k">async</code> <code class="k">for</code> <code class="n">chunk</code> <code class="ow">in</code> <code class="n">client</code><code class="o">.</code><code class="n">runs</code><code class="o">.</code><code class="n">stream</code><code class="p">(</code>
    <code class="n">thread</code><code class="p">[</code><code class="s2">"thread_id"</code><code class="p">],</code>
    <code class="n">assistant_id</code><code class="p">,</code>
    <code class="nb">input</code><code class="o">=</code><code class="nb">input</code><code class="p">,</code>
    <code class="n">stream_mode</code><code class="o">=</code><code class="s2">"updates"</code><code class="p">,</code>
<code class="p">):</code>
    <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Receiving new event of type: </code><code class="si">{</code><code class="n">chunk</code><code class="o">.</code><code class="n">event</code><code class="si">}</code><code class="s2">..."</code><code class="p">)</code>
    <code class="nb">print</code><code class="p">(</code><code class="n">chunk</code><code class="o">.</code><code class="n">data</code><code class="p">)</code>
    <code class="nb">print</code><code class="p">(</code><code class="s2">"</code><code class="se">\n\n</code><code class="s2">"</code><code class="p">)</code></pre>

<p><em>JavaScript</em></p>

<pre data-code-language="javascript" data-type="programlisting">
<code class="kr">import</code> <code class="p">{</code> <code class="nx">Client</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"@langchain/langgraph-sdk"</code><code class="p">;</code>

<code class="c1">// only set the apiUrl if you changed the default port when calling langgraph up</code>
<code class="kr">const</code> <code class="nx">client</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">Client</code><code class="p">();</code>
<code class="c1">// Using the graph deployed with the name "agent"</code>
<code class="kr">const</code> <code class="nx">assistantId</code> <code class="o">=</code> <code class="s2">"agent"</code><code class="p">;</code>
<code class="kr">const</code> <code class="nx">thread</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">client</code><code class="p">.</code><code class="nx">threads</code><code class="p">.</code><code class="nx">create</code><code class="p">();</code>

<code class="kr">const</code> <code class="nx">input</code> <code class="o">=</code> <code class="p">{</code>
  <code class="nx">messages</code><code class="o">:</code> <code class="p">[{</code> <code class="s2">"role"</code><code class="o">:</code> <code class="s2">"user"</code><code class="p">,</code> <code class="s2">"content"</code><code class="o">:</code> <code class="s2">"what's the weather in sf"</code><code class="p">}]</code>
<code class="p">}</code>

<code class="kr">const</code> <code class="nx">streamResponse</code> <code class="o">=</code> <code class="nx">client</code><code class="p">.</code><code class="nx">runs</code><code class="p">.</code><code class="nx">stream</code><code class="p">(</code>
  <code class="nx">thread</code><code class="p">[</code><code class="s2">"thread_id"</code><code class="p">],</code>
  <code class="nx">assistantId</code><code class="p">,</code>
  <code class="p">{</code>
    <code class="nx">input</code><code class="o">:</code> <code class="nx">input</code><code class="p">,</code>
    <code class="nx">streamMode</code><code class="o">:</code> <code class="s2">"updates"</code><code class="p">,</code>
  <code class="p">}</code>
<code class="p">);</code>
<code class="k">for</code> <code class="nx">await</code> <code class="p">(</code><code class="kr">const</code> <code class="nx">chunk</code> <code class="k">of</code> <code class="nx">streamResponse</code><code class="p">)</code> <code class="p">{</code>
  <code class="nx">console</code><code class="p">.</code><code class="nx">log</code><code class="p">(</code><code class="sb">`Receiving new event of type: </code><code class="si">${</code><code class="nx">chunk</code><code class="p">.</code><code class="nx">event</code><code class="si">}</code><code class="sb">...`</code><code class="p">);</code>
  <code class="nx">console</code><code class="p">.</code><code class="nx">log</code><code class="p">(</code><code class="nx">chunk</code><code class="p">.</code><code class="nx">data</code><code class="p">);</code>
  <code class="nx">console</code><code class="p">.</code><code class="nx">log</code><code class="p">(</code><code class="s2">"\n\n"</code><code class="p">);</code>
<code class="p">}</code></pre>

<p>If your LangGraph application is working correctly, you should see your graph output displayed in the console.<a contenteditable="false" data-primary="" data-startref="Dtestlocal09" data-type="indexterm" id="id860"/><a contenteditable="false" data-primary="" data-startref="Tpredeploy09" data-type="indexterm" id="id861"/></p>
</div></section>

<section data-pdf-bookmark="Deploy from the LangSmith UI" data-type="sect2"><div class="sect2" id="ch09_deploy_from_the_langsmith_ui_1736545675511226">
<h2>Deploy from the LangSmith UI</h2>

<p>At<a contenteditable="false" data-primary="deployment" data-secondary="from LangSmith UI" data-secondary-sortas="LangSmith UI" data-type="indexterm" id="Dsmith09"/><a contenteditable="false" data-primary="LangSmith" data-secondary="deployment from LangSmith UI" data-type="indexterm" id="LSdeployui09"/> this point, you should have completed all prerequisite steps and your LangGraph API should be working locally. Your next step is to navigate to your LangSmith dashboard panel and click the Deployments tab. You should see a UI similar to <a data-type="xref" href="#ch09_figure_6_1736545675498598">Figure 9-6</a>.</p>

<figure><div class="figure" id="ch09_figure_6_1736545675498598"><img alt="A screenshot of a computer  Description automatically generated" src="assets/lelc_0906.png"/>
<h6><span class="label">Figure 9-6. </span>LangGraph Platform deployment UI page</h6>
</div></figure>

<p>Next, click the New Deployment button in the top right corner of the page.</p>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>If you don’t see a page with the New Deployment button, it’s likely that you haven’t yet upgraded to a LangSmith Plus plan according to the instructions in the “Usage and billing” setting.</p>
</div>

<p>You should now see a page of three form fields to complete.</p>

<section data-pdf-bookmark="Deployment details" data-type="sect3"><div class="sect3" id="ch09_deployment_details_1736545675511286">
<h3>Deployment details</h3>

<ol>
	<li>
	<p>Select “Import with GitHub” and follow the GitHub OAuth workflow to install and authorize LangChain’s hosted-langserve GitHub app to access the selected repositories. After installation is complete, return to the Create New Deployment panel and select the GitHub repository to deploy from the drop-down menu.</p>
	</li>
	<li>
	<p>Specify a name for the deployment and the full path to the LangGraph API config file, including the filename. For example, if the file <em>langgraph.json</em> is in the root of the repository, simply specify<em> </em><em>langgraph.json</em>.</p>
	</li>
	<li>
	<p>Specify the desired <code>git</code> reference (branch name) of your repository to deploy.</p>
	</li>
</ol>
</div></section>

<section data-pdf-bookmark="Development type" data-type="sect3"><div class="sect3" id="ch09_development_type_1736545675511342">
<h3>Development type</h3>

<p>Select Production from the dropdown. This will enable a production deployment that can serve up to 500 requests/second and is provisioned with highly available storage and automatic backups.</p>
</div></section>

<section data-pdf-bookmark="Environment variables" data-type="sect3"><div class="sect3" id="ch09_environment_variables_1736545675511398">
<h3>Environment variables</h3>

<p>Provide the properties and values<a contenteditable="false" data-primary="environment variables" data-type="indexterm" id="id862"/> in your <em>.env</em> here. For sensitive values, like your <code>OPENAI_API_KEY</code>, make sure to tick the Secret box before inputting the value.</p>

<p>Once you’ve completed the fields, click the button to submit the deployment and wait for a few seconds for the build to complete. You should see a new revision associated with the deployment.</p>

<p>Since LangGraph Platform is integrated within LangSmith, you can gain deeper visibility into your app and track and monitor usage, errors, performance, and costs in production too. <a data-type="xref" href="#ch09_figure_7_1736545675498618">Figure 9-7</a> shows a visual Trace Count summary chart showing successful, pending, and error traces over a given time period. You can also view all monitoring info for your server by clicking the “All charts” button.</p>

<figure><div class="figure" id="ch09_figure_7_1736545675498618"><img alt="A screenshot of a computer  Description automatically generated" src="assets/lelc_0907.png"/>
<h6><span class="label">Figure 9-7. </span>Deployment revisions and trace count on dashboard</h6>
</div></figure>

<p>To view the build and deployment logs, select the desired revision from the Revisions tab, then choose the Deploy tab to view the full deployment logs history. You can also adjust the date and time range.</p>

<p>To create a new deployment, click the New Revision button in the navigation bar. Fill out the necessary fields, including the LangGraph API config file path, git reference, and environment variables, as done previously.</p>

<p>Finally, you<a contenteditable="false" data-primary="" data-startref="Dsmith09" data-type="indexterm" id="id863"/><a contenteditable="false" data-primary="" data-startref="LSdeployui09" data-type="indexterm" id="id864"/> can access the API documentation by clicking the API docs link, which should display a similar page to the UI shown in <a data-type="xref" href="#ch09_figure_8_1736545675498636">Figure 9-8</a>.</p>

<figure><div class="figure" id="ch09_figure_8_1736545675498636"><img alt="A screenshot of a computer  Description automatically generated" src="assets/lelc_0908.png"/>
<h6><span class="label">Figure 9-8. </span>LangGraph API documentation</h6>
</div></figure>
</div></section>
</div></section>

<section data-pdf-bookmark="Launch LangGraph Studio" data-type="sect2"><div class="sect2" id="ch09_launch_langgraph_studio_1736545675511457">
<h2>Launch LangGraph Studio</h2>

<p>LangGraph Studio<a contenteditable="false" data-primary="LangGraph Studio" data-type="indexterm" id="lgstudio09"/><a contenteditable="false" data-primary="deployment" data-secondary="LangGraph Studio" data-type="indexterm" id="Dstudio09"/> provides a specialized agent IDE for visualizing, interacting with, and debugging complex agentic applications. It enables developers to modify an agent result (or the logic underlying a specific node) halfway through the agent’s trajectory. This creates an iterative process by letting you interact with and manipulate the state at that point in time.</p>

<p>Once you’ve deployed your AI application, click the LangGraph Studio button at the top righthand corner of the deployment dashboard, as you can see in <a data-type="xref" href="#ch09_figure_9_1736545675498656">Figure 9-9</a>.</p>

<figure><div class="figure" id="ch09_figure_9_1736545675498656"><img alt="A screenshot of a computer  Description automatically generated" src="assets/lelc_0909.png"/>
<h6><span class="label">Figure 9-9. </span>LangGraph deployment UI</h6>
</div></figure>

<p>After clicking the button, you should see the LangGraph Studio UI (for example, see <a data-type="xref" href="#ch09_figure_10_1736545675498675">Figure 9-10</a>).</p>

<figure><div class="figure" id="ch09_figure_10_1736545675498675"><img alt="A screenshot of a computer  Description automatically generated" src="assets/lelc_0910.png"/>
<h6><span class="label">Figure 9-10. </span>LangGraph Studio UI</h6>
</div></figure>

<p>To invoke a graph and start a new run, follow these steps:</p>

<ol>
	<li>
	<p>Select a graph from the drop-down menu in the top left corner of the lefthand pane. The graph in <a data-type="xref" href="#ch09_figure_10_1736545675498675">Figure 9-10</a> is called <em>agent</em>.</p>
	</li>
	<li>
	<p>In the Input section, click the “+ Message” icon and input a <em>human</em> message, but the input will vary depending on your application state definitions.</p>
	</li>
	<li>
	<p>Click Submit to invoke the selected graph.</p>
	</li>
	<li>
	<p>View the output of the invocation in the right-hand pane.</p>
	</li>
</ol>

<p>The output of your invoked graph should look like <a data-type="xref" href="#ch09_figure_11_1736545675498694">Figure 9-11</a>.</p>

<figure><div class="figure" id="ch09_figure_11_1736545675498694"><img alt="A screenshot of a computer  Description automatically generated" src="assets/lelc_0911.png"/>
<h6><span class="label">Figure 9-11. </span>LangGraph Studio invocation output</h6>
</div></figure>

<p>In addition to invocation, LangGraph Studio enables you to change run configurations, create and edit threads, interrupt your graphs, edit graph code, and enable human-in-the-loop intervention. You can read the <a href="https://oreil.ly/xUU37">full guide</a> to learn more.</p>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>LangGraph Studio is also available as a desktop application (for Apple silicon), which enables you to test your AI application locally.</p>
</div>

<p>If you’ve followed the installation guide in the GitHub template and successfully deployed your AI application, it’s now live for production use. But before you share to external users or use the backend API in existing applications, it’s important to be aware of key security considerations.<a contenteditable="false" data-primary="" data-startref="Dstudio09" data-type="indexterm" id="id865"/><a contenteditable="false" data-primary="" data-startref="lgstudio09" data-type="indexterm" id="id866"/></p>
</div></section>
</div></section>

<section class="pagebreak-before" data-pdf-bookmark="Security" data-type="sect1"><div class="sect1" id="ch09_security_1736545675511516">
<h1 class="less_space">Security</h1>

<p>Although AI applications<a contenteditable="false" data-primary="deployment" data-secondary="security considerations" data-type="indexterm" id="id867"/><a contenteditable="false" data-primary="LLM applications" data-secondary="security considerations" data-type="indexterm" id="id868"/> are powerful, they are vulnerable to several security risks that may lead to data corruption or loss, unauthorized access to confidential information, and compromised performance. These risks may carry adverse legal, reputational, and financial consequences.</p>

<p>To<a contenteditable="false" data-primary="security best practices" data-type="indexterm" id="id869"/><a contenteditable="false" data-primary="mitigation strategies" data-type="indexterm" id="id870"/> mitigate these risks, it’s recommended to follow general application security best practices, including the following:</p>

<dl>
	<dt>Limit permissions</dt>
	<dd>
	<p>Scope permissions<a contenteditable="false" data-primary="permissions, limiting" data-type="indexterm" id="id871"/> specific to the application’s need. Granting broad or excessive permissions can introduce significant security vulnerabilities. To avoid such vulnerabilities, consider using read-only credentials, disallowing access to sensitive resources, and using sandboxing techniques (such as running inside a container).</p>
	</dd>
	<dt>Anticipate potential misuse</dt>
	<dd>
	<p>Always<a contenteditable="false" data-primary="misuse, anticipating potential" data-type="indexterm" id="id872"/> assume that any system access or credentials may be used in any way allowed by the permissions they are assigned. For example, if a pair of database credentials allows deleting data, it’s safest to assume that any LLM able to use those credentials may in fact delete data.</p>
	</dd>
	<dt>Defense in depth</dt>
	<dd>
	<p>It’s<a contenteditable="false" data-primary="defense in depth" data-type="indexterm" id="id873"/> often best to combine multiple layered security approaches rather than rely on any single layer of defense to ensure security. For example, use both read-only permissions and<a contenteditable="false" data-primary="sandboxing" data-type="indexterm" id="id874"/> sandboxing to ensure that LLMs are only able to access data that is explicitly meant for them to use.</p>
	</dd>
</dl>

<p>Here are three example scenarios implementing these mitigation strategies:</p>

<dl>
	<dt>File access</dt>
	<dd>
	<p>A<a contenteditable="false" data-primary="file access, protecting" data-type="indexterm" id="id875"/> user may ask an agent with access to the file system to delete files that should not be deleted or read the content of files that contain<a contenteditable="false" data-primary="sensitive data, protecting" data-type="indexterm" id="id876"/> sensitive information. To mitigate this risk, limit the agent to only use a specific directory and only allow it to read or write files that are safe to read or write. Consider further sandboxing the agent by running it in a container.</p>
	</dd>
	<dt>API access</dt>
	<dd>
	<p>A<a contenteditable="false" data-primary="API access, protecting" data-type="indexterm" id="id877"/> user may ask an agent with write access to an external API to write malicious data to the API or delete data from that API. To mitigate, give the agent read-only API keys, or limit it to only use endpoints that are already resistant to such misuse.</p>
	</dd>
	<dt>Database access</dt>
	<dd>
	<p>A<a contenteditable="false" data-primary="database access, protecting" data-type="indexterm" id="id878"/> user may ask an agent with access to a database to drop a table or mutate the schema. To mitigate, scope the credentials to only the tables that the agent needs to access and consider issuing read-only credentials.</p>
	</dd>
</dl>

<p>In addition to the preceding security measures, you can take further steps to mitigate abuse of your AI application. Due to the dependency of external LLM API providers (such as OpenAI), there is a direct cost associated with running your application. To prevent abuse of your API and exponential costs, you can implement the following:</p>

<dl>
	<dt>Account creation verification</dt>
	<dd>
	<p>This typically includes a form of<a contenteditable="false" data-primary="authentication" data-type="indexterm" id="id879"/> authentication login, such as email or phone number verification.</p>
	</dd>
	<dt>Rate limiting</dt>
	<dd>
	<p>Implement<a contenteditable="false" data-primary="rate limiting" data-type="indexterm" id="id880"/> a rate-limiting mechanism in the middleware of the application to prevent users from making too many requests in a short period of time. This should check the number of requests a user has made in the last X minutes and “timeout” or “ban” the user if the abuse is severe.</p>
	</dd>
	<dt>Implement prompt injection guardrails</dt>
	<dd>
	<p><em>Prompt injection</em> occurs<a contenteditable="false" data-primary="prompt injection guardrails" data-type="indexterm" id="id881"/> when a malicious user injects a prompt in an attempt to trick the LLM to act in unintended ways. This usually includes extracting confidential data or generating unrelated outputs. To mitigate this, you should ensure the LLM has proper permission scoping and that the application’s prompts are specific and strict to the desired outcomes.</p>
	</dd>
</dl>
</div></section>

<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="ch09_summary_1736545675511580">
<h1>Summary</h1>

<p>Throughout this chapter, you’ve learned the best practices for deploying your AI application and enabling users to interact with it. We explored recommended services to handle various key components of the application in production, including the LLM, vector store, and backend API.</p>

<p>We also discussed using LangGraph Platform as a managed service for deploying and hosting LangGraph agents at scale—in conjunction with LangGraph Studio—to visualize, interact with, and debug your application.</p>

<p>Finally, we briefly explored various security best practices to mitigate data breach risks often associated with AI applications.</p>

<p>In <a data-type="xref" href="ch10.html#ch10_testing_evaluation_monitoring_and_continuous_im_1736545678108525">Chapter 10</a>, you’ll learn how to effectively evaluate, monitor, benchmark, and improve the performance of your AI application.</p>
</div></section>
</div></section></body></html>