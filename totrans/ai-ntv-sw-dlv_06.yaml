- en: Chapter 6\. Chaos Engineering and Service Reliability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Complex modern systems are inherently vulnerable. Even seemingly minor disruptions,
    or a single weak link, can cause issues that spiral to have catastrophic consequences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this scenario: a prominent e-commerce platform suffers a significant
    outage during a peak sales event, comparable to Black Friday. As traffic builds,
    the platform’s checkout service grinds to a crawl, eventually culminating in complete
    failure. Thousands of customers are left unable to finalize purchases, resulting
    in not only immediate lost revenue but also damaged reputation and eroded trust
    and brand loyalty. Postincident analysis reveals the root cause to be network
    latency between the checkout service and a critical pricing data cache. As the
    cache response slowed under the strain of high traffic, the system’s retry mechanism
    became overwhelmed, leading to a cascade of failed requests that ultimately overloaded
    the database.'
  prefs: []
  type: TYPE_NORMAL
- en: Scenarios like these and the rising cost of failures have led to the emergence
    of service reliability as a discipline and the practice of chaos engineering (sometimes
    called failure or fault injection testing). The goal of chaos engineering is to
    provide an understanding of how systems behave when stressed in an unusual (chaotic)
    way. The increased popularity of these practices has been fueled by the development
    of new tools, technologies, and practices.
  prefs: []
  type: TYPE_NORMAL
- en: The term chaos engineering can be traced back to Netflix in 2010\. The company
    was transitioning its infrastructure to the cloud, which introduced new complexity,
    with hundreds of microservices interacting in unpredictable ways. To test the
    resilience of their systems, Netflix engineers developed Chaos Monkey, a tool
    designed to randomly terminate VM instances in their production environment. This
    simulated real-world failures, forcing engineers to build systems that could gracefully
    handle unexpected disruptions.
  prefs: []
  type: TYPE_NORMAL
- en: The use of the word chaos and the image of a monkey set loose to randomly terminate
    software in a production environment does conjure mayhem. Given these preconceptions,
    introducing chaos engineering into an organization may be met with resistance.
    More than one boss has wondered, “Don’t we have enough chaos around here?”
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll counter those notions by understanding modern chaos engineering
    as a rigorous approach to implementing experiments. As a methodology, we use this
    *controlled* disruption to test the resilience of our systems. In addition to
    testing our current state, chaos engineering gives us a powerful methodology to
    systematically improve resilience.
  prefs: []
  type: TYPE_NORMAL
- en: The experiments we conduct give us a deeper understanding of our software’s
    behavior under stress. This knowledge enables us to design targeted improvements.
    We then test to validate their effectiveness in meeting our targets.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll also cover how to use service-level objectives (SLOs) to set our resiliency
    targets. We’ll look at using error budgets to allow for an acceptable level of
    failure within that target. We’ll see how chaos engineering works with these mechanisms
    by helping us validate whether our system can operate within its error budget
    and still meet our targets even when facing unexpected disruptions.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we’ll also move beyond static chaos experiments to understand
    a more modern and dynamic approach that involves integrating chaos engineering
    into our CI/CD pipelines, allowing us to continuously assess and improve system
    resilience as part of our regular development workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, we will explore how advanced chaos engineering tools
    leverage AI/ML-driven insights to recommend and guide the execution of these experiments,
    leading to more efficient and effective resilience testing while reducing risk.
    We will also see how agentic AI, GenAI, and MCP address critical scalability and
    precision challenges in chaos engineering by automating experiment design, enabling
    dynamic risk detection, and providing intelligent remediation. These technologies
    transform chaos engineering from a reactive practice into a proactive, self-optimizing
    system resilience strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started with Chaos Engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While many chaos engineering experiments employ randomness (e.g., selecting
    a random server or service to take down), the practice of chaos engineering is
    as methodical as lab science. In this section, we’ll dive into the core tenets
    of chaos engineering and look at best practices to reduce the risk of causing
    service disruptions when conducting experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Principles of Chaos Engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Netflix has defined a set of core principles that provide a useful framework
    for exploring how your systems behave under stress. A structured approach ensures
    that your chaos experiments are not just disruptive events but structured investigations
    that generate valuable data that you can use to drive improvements to your system’s
    resilience. These principles are:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining a “steady state” that characterizes normal system behavior
  prefs: []
  type: TYPE_NORMAL
- en: Observability is key here. You must have the metrics you need to understand
    the normal range of values that indicate your system is healthy and performing
    as expected. This could include request latency, error rates, throughput, or application-specific
    metrics. Be sure to account for external factors that might influence your metrics,
    such as time of day, day of week, or the presence of a marketing campaign that
    could spike traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Turning expectation into a hypothesis
  prefs: []
  type: TYPE_NORMAL
- en: Based on your understanding of the system’s architecture and dependencies, formulate
    a hypothesis about how it *should* behave when a specific failure is introduced.
    Frame your hypothesis in a way that can be objectively tested using your chosen
    metrics. For example, “If we simulate a 20% increase in traffic, the average response
    time should remain below three seconds, and the error rate should not exceed 0.5%.”
  prefs: []
  type: TYPE_NORMAL
- en: Executing the experiment by simulating real-world events
  prefs: []
  type: TYPE_NORMAL
- en: Use chaos engineering tools to automate the injection of failures. Simulate
    a server crashing or becoming unavailable, an outage of a critical third-party
    service, or a sudden surge in user requests.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the results against the hypothesis
  prefs: []
  type: TYPE_NORMAL
- en: Compare the system’s behavior during the experiment to your established baseline
    and your hypothesized outcome. Did the metrics stay within acceptable ranges?
    Did the system recover as expected? Were any unexpected side effects observed?If
    the system deviates from the expected behavior, investigate the root cause. Based
    on the experiment’s outcome, refine your hypotheses and adjust your system design
    or operational procedures to enhance resilience.
  prefs: []
  type: TYPE_NORMAL
- en: Starting Small and Scaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Simulating failures to intentionally take down systems does, of course, incur
    risk. We knowingly incur risk in this controlled way to validate the hypothesis
    we’ve defined. An important strategy for reducing risk is to start with small
    experiments.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate starting small and scaling experiments, let’s walk through an
    example focused on testing a checkout service in an e-commerce system. This service
    is a critical microservice that processes user purchases. The expected outcome
    is simple: a customer adds items to their cart, proceeds to checkout, and completes
    the payment. The customer expects a smooth, fast, and reliable experience.'
  prefs: []
  type: TYPE_NORMAL
- en: Behind the scenes, this straightforward operation relies on a series of complex
    processes. The checkout service depends on multiple APIs and external services
    to function properly, including inventory systems, payment gateways, and caching
    layers (like Redis) to quickly retrieve important data such as product prices,
    discounts, and availability.The checkout service fetches pricing data from a cache
    for quick access. If the cache is slow or fails, the checkout service should still
    provide the right information by failing over to another cache instance or even
    to a database as a backup, though it may be slower.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: GenAI can transform chaos engineering from manual hypothesis testing into an
    adaptive, self-optimizing resilience validation system. This approach proves particularly
    valuable in critical e-commerce workflows like checkout services, where balancing
    risk mitigation with realistic failure simulation is paramount.
  prefs: []
  type: TYPE_NORMAL
- en: 'Developers typically configure retry logic, timeouts, and circuit breakers
    to handle network issues or failures. Let’s look at each:'
  prefs: []
  type: TYPE_NORMAL
- en: Retry logic
  prefs: []
  type: TYPE_NORMAL
- en: This ensures that if a request to the cache fails or experiences network issues,
    the system will automatically try again a few times before giving up. This helps
    handle transient failures. The system might, for example, retry up to three times
    with a delay of 100 ms between each retry.
  prefs: []
  type: TYPE_NORMAL
- en: Timeouts
  prefs: []
  type: TYPE_NORMAL
- en: Timeout settings define how long the service should wait for a response before
    deciding that the attempt has failed. This prevents the service from hanging indefinitely
    if the cache is slow or unresponsive. A system might be configured to time out
    after 200 ms for each request to the cache.
  prefs: []
  type: TYPE_NORMAL
- en: Circuit breakers
  prefs: []
  type: TYPE_NORMAL
- en: A circuit breaker prevents further attempts to call a failing service after
    a certain number of failed attempts. If the cache continues to fail or is too
    slow, the circuit breaker “trips” and routes traffic to a fallback system (e.g.,
    another cache or a database). The circuit breaker can automatically reset after
    a set period to test if the original service has recovered. For example, a circuit
    breaker might be configured to trip after five consecutive retries fail.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start testing the checkout service by introducing small latencies to ensure
    the retry logic and timeout settings are functioning before scaling up to introduce
    more severe issues that will ultimately trigger the circuit breaker. If all goes
    well, we expect the system to fail over to an alternative data source. These are
    our steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Conduct a simple latency experiment'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We start with a test of our retry logic. We want to ensure the system is resilient
    if network issues arise, such as high latency or a temporary loss of connectivity.
    Our steady state is a responsive service that responds within an acceptable time
    limit.
  prefs: []
  type: TYPE_NORMAL
- en: Our hypothesis is that if the network experiences significant latency when trying
    to reach the cache, the system should use its retry logic and timeout settings
    to handle the issue gracefully, eventually tripping the circuit breaker to prevent
    further degradation of service.
  prefs: []
  type: TYPE_NORMAL
- en: We start small by injecting a small amount of network latency (e.g., 200 ms)
    between the checkout service and the cache.
  prefs: []
  type: TYPE_NORMAL
- en: We observe whether the retry logic kicks in and whether the service handles
    the delay within the acceptable time limit without user impact. We continue to
    monitor whether the system continues functioning as expected, pulling from the
    cache after the latency delay.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Test resilience against a more significant network issue'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once we’ve tested our retry logic with a small latency, we can increase the
    scope and intensity of the experiment to simulate a more significant network issue.
    This tests our timeouts. We increase the network latency (e.g., from 500 ms to
    1 second) to see how the service behaves under heavier load or network congestion.
    We test how the retry logic handles the extended delay. Does the service retry
    the call to the cache, and does it respect the timeout setting? If so, we increase
    the severity of the issue by causing the cache API to completely fail after a
    set number of retries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Validate that the circuit breaker fails over to an alternative'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We next set experiment conditions to render the cache inaccessible. After retrying,
    the circuit breaker mechanism should be triggered. When the circuit breaker is
    tripped, the checkout service fails over to an alternative data source, such as
    another cache instance in a different data center (in this case, our Postgres
    database). While the Postgres database might be slower than the cache, the goal
    is to keep the service operational, albeit with slightly degraded performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'AI agents can make this process even simpler by dynamically adjusting failure
    injection parameters using reinforcement learning. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: Start with 200 ms delays, then autonomously scale to 500 ms to 1 second based
    on real-time performance telemetry.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Limit experiment impact to 0.5% of transactions initially, expanding only after
    validating safety mechanisms.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Optimize trip thresholds (e.g., five failures to four) through historical success
    pattern analysis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can further scale the experiment to test the resilience of the failover
    by introducing similar network issues between the checkout service and the Postgres
    database to see how the system continues to adapt under increased failure conditions.
    By following this process, we gradually increase the complexity of the experiment
    to validate the system’s resilience mechanisms, without jumping into major disruptions
    immediately.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that the initial settings for resilience mechanisms are
    often based on educated guesses rather than precise data. This is another reason
    that testing through chaos engineering experiments is so crucial.
  prefs: []
  type: TYPE_NORMAL
- en: Injecting network latency is just one condition we can scale in a chaos experiment.
    We’ll discuss other conditions later in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Starting in Production-Like Environments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another important best practice to minimize risk in chaos engineering is to
    test experiments in pre-production environments before moving to production. This
    allows us to experiment safely without impacting real users. We can rapidly iterate,
    adjust parameters, and observe results free from production constraints. Once
    we confirm system resilience in these settings, we promote our experiment to the
    next environment, eventually reaching production. Each promotion carries risk,
    so we proceed with caution. Configuration drift between environments can lead
    to discrepancies in experiment results. Maintaining the “start small and scale”
    approach when moving between environments is crucial in case we encounter issues.
    Thoroughly vetting our experiments in pre-production ensures that our experiments
    are well-designed and insightful without unintended consequences.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging Modern Tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We looked extensively at an example of testing network latency in a chaos experiment.
    There are many other types of conditions that are important to test. Modern tools
    (such as Harness Chaos Engineering, Chaos Monkey, and LitmusChaos) can help here
    by offering extensive catalogs of predefined experiments. Modern tools will typically
    offer chaos engineering experiments across categories and common failure patterns,
    including:'
  prefs: []
  type: TYPE_NORMAL
- en: Resource exhaustion
  prefs: []
  type: TYPE_NORMAL
- en: CPU exhaustion
  prefs: []
  type: TYPE_NORMAL
- en: Force high CPU utilization to simulate a process consuming excessive processing
    power.
  prefs: []
  type: TYPE_NORMAL
- en: Memory exhaustion
  prefs: []
  type: TYPE_NORMAL
- en: Consume all available memory to test how your application handles memory pressure
    and potential out-of-memory errors.
  prefs: []
  type: TYPE_NORMAL
- en: Disk I/O exhaustion
  prefs: []
  type: TYPE_NORMAL
- en: Generate heavy disk read/write operations to simulate storage bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: Network bandwidth exhaustion
  prefs: []
  type: TYPE_NORMAL
- en: Saturate network bandwidth to test how your application performs under network
    congestion.
  prefs: []
  type: TYPE_NORMAL
- en: Network disruption
  prefs: []
  type: TYPE_NORMAL
- en: Network latency
  prefs: []
  type: TYPE_NORMAL
- en: Introduce delays in network communication between services or with external
    dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Packet loss
  prefs: []
  type: TYPE_NORMAL
- en: Simulate the loss of network packets to test how your application handles unreliable
    connections.
  prefs: []
  type: TYPE_NORMAL
- en: Network partition
  prefs: []
  type: TYPE_NORMAL
- en: Isolate parts of your network to simulate connectivity issues between services
    or availability zone outages.
  prefs: []
  type: TYPE_NORMAL
- en: DNS failures
  prefs: []
  type: TYPE_NORMAL
- en: Simulate DNS resolution problems to test how your application handles DNS outages
    or incorrect responses.
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure failure
  prefs: []
  type: TYPE_NORMAL
- en: Node failure
  prefs: []
  type: TYPE_NORMAL
- en: Terminate or shut down VMs or containers to simulate hardware failures.
  prefs: []
  type: TYPE_NORMAL
- en: Pod failure (Kubernetes)
  prefs: []
  type: TYPE_NORMAL
- en: Kill or evict pods to test the self-healing capabilities of your Kubernetes
    deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Availability zone outage
  prefs: []
  type: TYPE_NORMAL
- en: Simulate the failure of an entire availability zone to test your disaster recovery
    plan and multiregion deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Inference layer attacks
  prefs: []
  type: TYPE_NORMAL
- en: Simulate GPU memory exhaustion during ML model serving.
  prefs: []
  type: TYPE_NORMAL
- en: Application-level faults
  prefs: []
  type: TYPE_NORMAL
- en: Service failure
  prefs: []
  type: TYPE_NORMAL
- en: Stop or crash specific services within your application to test fault tolerance
    and service degradation.
  prefs: []
  type: TYPE_NORMAL
- en: Function failure
  prefs: []
  type: TYPE_NORMAL
- en: Introduce errors or exceptions within specific functions or methods to test
    error handling and recovery mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: Data corruption
  prefs: []
  type: TYPE_NORMAL
- en: Corrupt data in a database or storage system to test your data integrity and
    recovery processes.
  prefs: []
  type: TYPE_NORMAL
- en: State management
  prefs: []
  type: TYPE_NORMAL
- en: Time travel
  prefs: []
  type: TYPE_NORMAL
- en: Manipulate the system clock to simulate time shifts, testing how your application
    handles time-sensitive operations or scheduled tasks.
  prefs: []
  type: TYPE_NORMAL
- en: State injection
  prefs: []
  type: TYPE_NORMAL
- en: Inject specific data or states into your application to test its behavior under
    unusual conditions. Use GenAI to create plausible corrupt data entries matching
    schema constraints.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic scenario generation using AI
  prefs: []
  type: TYPE_NORMAL
- en: Architecture modeling
  prefs: []
  type: TYPE_NORMAL
- en: Use AI to analyze service dependencies (e.g., Redis cache → payment gateway
    → database) to create failure chains mirroring production environments.
  prefs: []
  type: TYPE_NORMAL
- en: Generative adversarial networks
  prefs: []
  type: TYPE_NORMAL
- en: Create novel failure modes by pitting AI models against each other to discover
    unexplored vulnerability combinations.
  prefs: []
  type: TYPE_NORMAL
- en: The more types of experiments we try, the more we can learn about weaknesses
    in our system and how we can strengthen our resiliency.
  prefs: []
  type: TYPE_NORMAL
- en: Newer tools can go beyond offering catalogs to analyze your system architecture
    to suggest targeted experiments that expose potential weaknesses specific to your
    setup. For example, for software built with a microservices architecture, a chaos
    engineering tool might analyze network traffic and dependencies to identify critical
    services and suggest experiments that target these specifically. A modern tool
    might also recommend injecting latency or errors into API calls between services
    to test resilience to communication disruptions.
  prefs: []
  type: TYPE_NORMAL
- en: For applications deployed with Kubernetes, the tool could analyze your Kubernetes
    deployments and suggest experiments that target specific pods, deployments, or
    namespaces to test replica scaling, resource limits, and health checks. Tools
    like Red Hat’s Krkn use AI to profile Kubernetes pods to prioritize network-intensive
    services for partition tests. In the case of multiregion deployments, a modern
    tool might analyze your multiregion setup and suggest experiments that simulate
    regional failures or network partitions to test your disaster recovery plan and
    the ability of your application to failover to another region.
  prefs: []
  type: TYPE_NORMAL
- en: Learning from Others
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Paying close attention to industry-wide incidents, particularly those affecting
    companies with similar tech stacks, is crucial for proactive risk mitigation.
    For instance, an OpenAI outage on December 11, 2024, serves as a stark reminder
    of how seemingly minor deployments can have cascading consequences.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, a new telemetry service overwhelmed the company’s Kubernetes
    control plane, triggering DNS failures that brought down its API, ChatGPT, and
    Sora platforms for hours. The impact was widespread and long-lasting: for several
    hours, developers and users couldn’t access the services they rely on. Engineers
    identified the root cause within minutes but faced a major hurdle—without access
    to the Kubernetes control plane, rolling back or fixing the deployment was extremely
    challenging.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a few targeted chaos engineering experiments to see how these
    cascading failures might have been prevented.
  prefs: []
  type: TYPE_NORMAL
- en: 'Experiment 1: Control plane overload simulation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, we design an experiment to test our Kubernetes API server resilience.
    In this experiment, we would intentionally flood the Kubernetes API server with
    a high volume of read/write operations to mimic what the new telemetry service
    did in production. By running this test on a staging environment with a production-like
    scale, we could have spotted the exact threshold where the API server starts to
    fail. This early detection would inform better load limiting, improved alerting,
    and possibly a safer phased rollout strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Experiment 2: DNS failure testing'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This experiment would involve introducing latency or failures in the DNS resolution
    process—specifically targeting the components responsible for service discovery.
    Running this experiment helps confirm that essential services can continue functioning
    even if DNS is disrupted. We will discover if our caches, fallback mechanisms,
    or alternative routing strategies are sufficient. If not, we would know to invest
    in those areas before a real outage hits.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 3: Break-glass access drills'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This last experiment (or drill) involves simulating a situation where engineers
    are locked out of the Kubernetes API under heavy load. By practicing emergency
    access methods—like having dedicated out-of-band channels or specialized tooling—teams
    can quickly revert or disable problematic deployments when the standard control
    plane is inaccessible. If this drill had been done beforehand, teams would have
    known exactly how to remove the faulty telemetry service within minutes, minimizing
    downtime.
  prefs: []
  type: TYPE_NORMAL
- en: Service-Level Objectives and Service Resiliency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We see how chaos engineering helps us uncover weaknesses and build more resilient
    systems. But how do we define “resilient”? How do we measure and track whether
    our systems are meeting our reliability goals? This is where SLOs and service-level
    indicators (SLIs) come in. Together, these provide the framework for defining
    and measuring the reliability of our services, giving us a clear target to aim
    for and a way to track our progress.
  prefs: []
  type: TYPE_NORMAL
- en: SLOs are the targets we set for the reliability of our services. SLIs are the
    specific metrics we use to measure whether we’re meeting those targets. SLOs are
    typically expressed as a percentage of time or number of requests that must meet
    the defined SLI criteria. For example, *99.9% of requests should have a latency
    of under 200 milliseconds*. SLIs are the specific, measurable metrics that reflect
    the performance of your service from a user’s perspective. They quantify aspects
    like availability, latency, error rate, throughput, and other relevant factors.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, SLIs are *what* you measure, and SLOs are the *targets* you set
    for those measurements.
  prefs: []
  type: TYPE_NORMAL
- en: Establishing Reliability Targets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When establishing reliability targets, it’s essential to align them with the
    overarching business needs. Monitoring and observability solutions provide many
    SLI metrics, but it is important to prioritize those that accurately reflect how
    your customers experience your applications. The goal is not to track every individual
    service, but to focus on those services that are critical to the customer experience.
  prefs: []
  type: TYPE_NORMAL
- en: 'Common SLIs include “the four golden signals”:'
  prefs: []
  type: TYPE_NORMAL
- en: Request latency
  prefs: []
  type: TYPE_NORMAL
- en: The time taken to process a user request
  prefs: []
  type: TYPE_NORMAL
- en: Throughput
  prefs: []
  type: TYPE_NORMAL
- en: The number of requests processed per second
  prefs: []
  type: TYPE_NORMAL
- en: Error rate
  prefs: []
  type: TYPE_NORMAL
- en: The percentage of failed requests
  prefs: []
  type: TYPE_NORMAL
- en: Saturation
  prefs: []
  type: TYPE_NORMAL
- en: The utilization percentage of the system
  prefs: []
  type: TYPE_NORMAL
- en: Consider carefully how to implement each of these within your system. For instance,
    when measuring latency (response time), you can choose to track all transactions
    or focus on a subset of the most crucial ones, such as login, payment submission,
    or adding items to a shopping cart. Again, select a metric that provides a meaningful
    representation of your customers’ experience.
  prefs: []
  type: TYPE_NORMAL
- en: Shared Ownership of System Reliability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [Chapter 1](ch01.html#chapter_1_the_road_to_ai_native_devops_1749354009875299),
    we introduced DevOps as practices that combine software development (Dev) and
    IT operations (Ops) concerns. Nowhere are shared ownership and collaboration more
    important than in ensuring system reliability. SLOs are a great example of this
    shared responsibility. Development, operations, and reliability teams should work
    together to define SLOs. The collaboration establishes an understanding of acceptable
    system performance and creates a common goal for everyone to work toward. SLOs
    then act as a guide for making decisions that balance the need for rapid development
    (velocity) with the need for stable and reliable systems.
  prefs: []
  type: TYPE_NORMAL
- en: With this shared understanding, developers can prioritize features that maintain
    reliability, knowing how their work impacts overall system performance. At the
    same time, operations teams gain the context they need to effectively support
    the application. If an SLO is breached, it triggers activities that encourage
    engineering teams to stabilize the service before releasing new features. This
    helps prevent a cycle of instability and ensures that reliability remains a top
    priority.
  prefs: []
  type: TYPE_NORMAL
- en: A collaborative approach to designing, prioritizing, and conducting the chaos
    engineering experiments themselves brings teams together. All teams benefit from
    the insights gained from these experiments and from working together to address
    when failures are found.
  prefs: []
  type: TYPE_NORMAL
- en: Modern tools facilitate this collaborative approach to system reliability. Monitoring
    platforms, incident management systems, and communication tools give a shared
    visibility into system performance and potential issues. Real-time data and automated
    alerts empower both Dev and Ops teams to respond quickly to incidents. More importantly,
    these tools foster a culture of proactive problem-solving (such as data-driven
    prioritization, real-time collaboration triage, etc.), where teams can identify
    and address potential issues before they impact users.
  prefs: []
  type: TYPE_NORMAL
- en: Error Budgets and Their Role in Reliability and Innovation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve learned how chaos engineering helps us proactively find system weaknesses,
    and how SLOs and SLIs provide a clear framework for defining our reliability goals
    and measuring whether our systems are meeting those targets. [Error budgets](https://oreil.ly/K0JqJ)
    enter to provide a safety net.
  prefs: []
  type: TYPE_NORMAL
- en: Error budgets represent the maximum amount of unreliability or downtime that
    a service can have while still meeting its SLOs. By tolerating minor hiccups in
    the pursuit of rapid innovation, error budgets acknowledge that perfection is
    unattainable, and instead help us achieve an acceptable level of reliability that
    balances these two competing priorities.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at how this works by returning to our e-commerce example. Imagine
    we’ve set an SLO of 99.9% for website logins taking less than 300 ms. Over a one-week
    period, this translates to a maximum allowable SLO violation time of 10.08 minutes.
    This is our error budget. How does that impact us? In the event that the error
    budget burns down to zero, we will stop or slow down deployments of new software
    and focus on stabilizing the system while our error budget replenishes. Not only
    does the state of our error budget impact our deployment priorities, but it also
    factors into chaos testing priorities.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring to Inform Chaos Testing Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Keeping a close eye on your SLIs does more than just alert you to immediate
    problems—it reveals potential weaknesses in your system. For example, if you notice
    your system constantly pushing against latency limits, draining your error budget,
    your system might be struggling to keep up in high-traffic scenarios. This suggests
    a good area to focus your chaos experiments on. By simulating those high-traffic,
    high-latency situations, you can see how your system holds up under pressure and
    make sure it can still meet its SLOs during peak usage.
  prefs: []
  type: TYPE_NORMAL
- en: With modern tools, you can automate this by automatically triggering chaos tests
    based on these patterns, so you can continuously test and improve your system’s
    resilience without lifting a finger. Modern platforms can correlate SLI trends
    with chaos test recommendations using AI, thus increasing test coverage significantly.
  prefs: []
  type: TYPE_NORMAL
- en: Strategic Use of Error Budgets for Chaos Testing Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Error budgets are not merely a safety net for occasional failures; they are
    a tool for managing risk. Using our e-commerce website example, we think of the
    10.08-minute error budget as a resource to be spent wisely. In this section we’ll
    look at how to proactively use this budget to conduct chaos experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Prioritizing chaos experiments in alignment with your available error budget
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Effective chaos engineering requires consideration of your available error budget.
    When your error budget is healthy, your runway is long. You have more freedom
    to conduct aggressive experiments, simulating large-scale failures or pushing
    critical system components to their limits. This might involve testing failover
    mechanisms, injecting network latency, or even simulating the complete outage
    of a core service.
  prefs: []
  type: TYPE_NORMAL
- en: As your error budget dwindles, it’s essential to shift focus toward smaller-scale
    experiments that carry less risk of significant disruption. These might involve
    testing individual components in isolation, simulating minor network issues, or
    validating the resilience of recent changes. Prioritizing experiments in this
    way ensures that you can continue to learn and improve without jeopardizing overall
    system stability.
  prefs: []
  type: TYPE_NORMAL
- en: Modern automation tools can help. By analyzing your error budget in real time,
    these tools can recommend appropriate experiments based on your available “room
    for failure.” This allows you to maintain a balance between proactive testing
    and service reliability, ensuring that your chaos engineering efforts are both
    insightful and responsible.
  prefs: []
  type: TYPE_NORMAL
- en: Protect your error budget by utilizing AI-augmented dry runs and simulations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Simulating first is another strategy to minimize the risk of unintended consequences
    during chaos experiments. This is especially important when up against a dwindling
    error budget. The practice of AI-augmented “dry running” chaos experiments involves
    simulating experiments in a controlled environment, using system models or replicas,
    to assess their potential impact before executing them in production, and using
    AI remediation agents to roll back experiments if anomaly detection thresholds
    breach predefined limits. By identifying potential issues and refining experiment
    parameters beforehand, teams can reduce the likelihood of causing significant
    disruptions that could drain your error budget and cause significant disruptions.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating Chaos Engineering and SLOs into CI/CD Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reliability issues are primarily driven by change, changes to our applications,
    or changes to your infrastructure. Google DevOps Research and Assessment (DORA)
    defines the change failure rate (CFR) metric, which gives us another view of the
    challenge. The CFR describes how often our changes, such as new code deployments
    or infrastructure updates, introduce problems in production that require hotfixes
    or rollbacks. The DORA 2024 State of DevOps report indicates that 80% of surveyed
    teams have average CFRs of 20% of their releases. In fact, 25% of teams have CFRs
    averaging an alarming 40% of releases.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we must consider the time and cost to remediate each change failure.
    The failed deployment recovery time metric (replacing the similar mean time to
    recovery [MTTR] metric) focuses on how quickly an organization can recover from
    failures. This gives us a sense of the challenges teams face on this front. While
    many teams are able to remediate in less than a day, 25% require a week to a month
    to replace defective software.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout earlier chapters, we’ve looked at strategies to prevent defects from
    getting to production. We test at every stage in our delivery pipelines, executing
    tests of every type. We take care in managing our environments. We guard against
    configuration drift with practices like GitOps, combined with IaC. And we conduct
    chaos engineering testing in pre-production and production environments to help
    us find weaknesses in our systems. Yet, despite our best efforts, occasional defects
    that require fast remediation are inevitable. This is where continuous resilience
    comes in.
  prefs: []
  type: TYPE_NORMAL
- en: Just as continuous integration and continuous delivery are about using automation
    to build, test, and deploy our code, continuous resilience is about automating
    our resiliency practices by adding chaos engineering experiments to our CI/CD
    pipelines. Doing so means we are not just testing functionality, but actively
    and constantly evaluating how changes stand to impact the stability of our systems.
    Using AI agents for DevOps, chaos experiments can be intelligently integrated
    into CI/CD pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: In [“Scaling Your Chaos Engineering Practices”](#chapter_6_scaling_your_chaos_engineering_practices_1749354010917388)
    we’ll explore how to scale a chaos engineering practice by incorporating it into
    our delivery pipeline with the help of modern tools. We’ll look at how to prioritize
    the experiments to add to your pipeline, and best practices for securing and governing
    chaos experiments in your pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling Your Chaos Engineering Practices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Organizations start their chaos engineering journey in different ways. Often
    a single team or two will adopt an open source tool and introduce experiments
    in a small pocket of an organization. An organization may host periodic chaos
    engineering “game days.” There are all-hands-on-deck, planned events where teams
    deliberately inject a series of failures into systems to practice incident response
    and identify weaknesses in a controlled environment. These are typically infrequent
    and responses are reactive to issues that are discovered.
  prefs: []
  type: TYPE_NORMAL
- en: The trick to implementing continuous resiliency at scale, across an organization,
    can be a matter of choosing the right tooling. While both open source and proprietary
    solutions offer valuable capabilities, organizations should carefully evaluate
    their requirements. Some enterprise environments may need specific features like
    advanced security controls, comprehensive audit trails, and RBAC—features that
    may vary in availability and maturity across open source solutions.
  prefs: []
  type: TYPE_NORMAL
- en: This challenge was acutely felt by a leading fintech company processing over
    a billion daily payment transactions. Faced with increasing transaction failures
    during peak demand, it sought a solution to improve the reliability of its complex
    platform supporting 20+ financial products.
  prefs: []
  type: TYPE_NORMAL
- en: The company’s choice of a modern chaos engineering tool was instrumental in
    overcoming the obstacles of scaling its chaos engineering practices. The tool
    it chose (in this case, Harness Chaos Engineering) included an extensive library
    of prebuilt experiments that simplified the work of automating and orchestrating
    numerous chaos experiments. In addition, comprehensive analysis and reporting
    capabilities gave the company quick insights into the resiliency of its systems.
  prefs: []
  type: TYPE_NORMAL
- en: The company began by focusing on a single critical service that handled nine
    million daily payment requests. It pinpointed fault-tolerant targets within the
    intricate infrastructure, laying the groundwork for a controlled rollout of resilience
    testing. By prioritizing the automation of chaos experiments within delivery pipelines
    and production environments, it addressed the root causes of transaction failures
    and built a foundation for continuous resilience.
  prefs: []
  type: TYPE_NORMAL
- en: 'Through its automated resilience testing platform, the company was able to
    expand the breadth of its testing to uncover vulnerabilities in service recovery,
    optimize application design patterns, and fine-tune configurations. The results
    were significant: a 16× reduction in failed transactions, MTTR reduced to 10 minutes,
    and a 10× improvement in customer satisfaction. Without modern tooling that offers
    security, templates and automation, and orchestration, it would have been impossible
    to roll out chaos engineering across the organization and achieve these results
    in the short time it took.'
  prefs: []
  type: TYPE_NORMAL
- en: Adding Chaos Engineering Experiments and SLOs to Your CI/CD Pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To solidify your resilience strategy, integrate SLOs as reliability guardrails
    within your CI/CD pipeline. Think of SLOs as the brakes on a race car—essential
    for maintaining control while pushing for maximum speed. Development teams, much
    like race car drivers, strive for rapid deployments, but without robust SLOs in
    place, they risk crashing their systems with unchecked changes. By monitoring
    key metrics, you can automatically block deployments that breach these thresholds
    or exhaust their error budget. This approach can accelerate your development velocity
    without sacrificing stability.
  prefs: []
  type: TYPE_NORMAL
- en: 'When adding chaos engineering experiments to your CI/CD pipeline, keep in mind
    two measures to guide your progress: resilience scores and resilience coverage.
    Resilience scores are simply how well your services perform against the experiments
    you apply in QA and production. Resilience coverage, similar to code coverage,
    assesses how many more experiments are needed to comprehensively evaluate system
    resilience, guiding the creation of a practical number of tests. Together, these
    metrics provide a holistic view of resilience, enabling all teams to contribute
    to and measure progress toward continuous resilience goals.'
  prefs: []
  type: TYPE_NORMAL
- en: Start by adding experiments that test against known resilience conditions, ensuring
    your resilience score remains stable with each new deployment. Slowly increase
    your resilience coverage by adding experiments to test new conditions. If increasing
    the resilience coverage means that the resilience scores are reduced, determine
    if the failed chaos experiment warrants stopping the pipeline or if action can
    be taken in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: Next, add experiments that address changes to the platform on which the target
    deployments run. For example, when upgrading underlying platforms like Kubernetes,
    incorporate chaos experiments into your CI/CD pipeline to proactively identify
    potential weaknesses and compatibility issues. This helps prevent latent issues
    from impacting applications in the future and ensures a smooth transition during
    platform updates. By catching these issues early, you can avoid costly incidents
    and maintain continuous resilience.
  prefs: []
  type: TYPE_NORMAL
- en: Add experiments to the pipeline that validate the deployments against previous
    production incidents and alerts as incidents occur. Lastly, add experiments that
    validate the deployments against configuration changes to the target infrastructure.
    This is another scenario where the resilience tests that passed earlier will start
    failing because the target environment changed through a higher or lower configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you invest in creating and fine-tuning your experiments, treat them like
    any other piece of software: version them, test them, and manage their lifecycle
    in a repository. This ensures your chaos engineering practice remains effective
    and adapts to changes in your systems. Centralized repositories facilitate collaboration
    and the sharing of these experiments, promoting consistency and best practices
    across teams.'
  prefs: []
  type: TYPE_NORMAL
- en: Security and Governance for Chaos Engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Clearly, chaos engineering is a powerful approach, but careless experimentation
    has the potential to cause serious harm to both system resilience and trust in
    your chaos engineering program. By integrating it with your security and governance
    frameworks, you can define the guardrails you need to ensure experiments are conducted
    responsibly.
  prefs: []
  type: TYPE_NORMAL
- en: Just like tech debt, resilience debt can accumulate in your production services.
    Every alert, incident, hotfix, or workaround—like simply throwing more resources
    at a problem—contributes to this debt. Instead of addressing the root cause, these
    quick fixes often mask underlying issues and create a false sense of stability.
  prefs: []
  type: TYPE_NORMAL
- en: Modern chaos engineering tools can help you establish and enforce policies to
    combat this. For example, we could set a policy that mandates a corresponding
    chaos experiment for every production incident related to component misbehavior,
    network issues, API failures, or unexpected load. This experiment, integrated
    into your CI/CD pipeline, would need to be validated within a specific timeframe,
    say, within 60 days of the incident. Such a policy would not only enforce a discipline
    of addressing resilience debt but also encourage developers and QA teams to prioritize
    fixing production code over pushing new features that might further exacerbate
    the problem.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to using policies to manage resilience debt, you can use security
    governance policies to prevent unauthorized experiments, restrict access to critical
    systems, and limit testing by environment, time window, personnel, or even fault
    type. By automating oversight and integrating these policies into your CI/CD pipelines,
    you can increase resilience coverage while reducing risk.
  prefs: []
  type: TYPE_NORMAL
- en: The Future of AI-Native Chaos Engineering in Service Reliability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The future of chaos engineering promises even greater sophistication and integration
    within service reliability practices. Tools such as Harness Chaos Engineering
    and Chaos Monkey will not only automate experiments but also leverage AI/ML to
    predict their impact, analyze system behavior under stress, and recommend optimal
    mitigation strategies. This intelligent automation will minimize risk, allowing
    teams to conduct more complex experiments with greater confidence and efficiency.
    Advancements in observability and tracing will provide deeper insights into system
    dynamics, enabling more precise identification of vulnerabilities and faster recovery
    from disruptions.
  prefs: []
  type: TYPE_NORMAL
- en: As systems grow increasingly complex, with distributed architectures and microservices
    becoming ever-present, chaos engineering will play a crucial role in ensuring
    their resilience. Even large language model–based multiagentic systems can be
    [enhanced using chaos engineering](https://oreil.ly/g7tjd). By combining chaos
    testing with AI-powered analysis and automated remediation (for example, [ChaosEater](https://oreil.ly/IKlJW)),
    we will be able to address potential failures faster and with greater precision,
    minimizing downtime and maintaining high levels of service reliability.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored chaos engineering as a methodical approach to building
    and validating system resilience. We learned to design and execute experiments
    responsibly, using SLOs and error budgets to balance innovation and stability.
    By integrating chaos engineering into CI/CD pipelines and leveraging modern tooling,
    organizations can proactively identify weaknesses, learn from failures, and continuously
    improve resilience. Ultimately, chaos engineering empowers us to create more robust
    systems that meet the demands of today’s complex digital world. With these principles
    in place, the next step is to apply them seamlessly as part of your deployment
    process. Let’s explore how to ensure stability during production rollouts.
  prefs: []
  type: TYPE_NORMAL
