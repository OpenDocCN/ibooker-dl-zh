- en: Chapter 6\. Chaos Engineering and Service Reliability
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章. 混沌工程与服务可靠性
- en: Complex modern systems are inherently vulnerable. Even seemingly minor disruptions,
    or a single weak link, can cause issues that spiral to have catastrophic consequences.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂的现代系统天生具有脆弱性。即使是看似微小的中断，或者一个单一的薄弱环节，也可能导致问题螺旋式地升级，产生灾难性的后果。
- en: 'Consider this scenario: a prominent e-commerce platform suffers a significant
    outage during a peak sales event, comparable to Black Friday. As traffic builds,
    the platform’s checkout service grinds to a crawl, eventually culminating in complete
    failure. Thousands of customers are left unable to finalize purchases, resulting
    in not only immediate lost revenue but also damaged reputation and eroded trust
    and brand loyalty. Postincident analysis reveals the root cause to be network
    latency between the checkout service and a critical pricing data cache. As the
    cache response slowed under the strain of high traffic, the system’s retry mechanism
    became overwhelmed, leading to a cascade of failed requests that ultimately overloaded
    the database.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下场景：一个著名的电子商务平台在高峰销售活动期间（类似于黑色星期五）遭受了重大中断。随着流量的增加，平台的结账服务变得缓慢，最终导致完全失败。数千名客户无法完成购买，这不仅导致了即时的收入损失，还损害了声誉，削弱了信任和品牌忠诚度。事后分析发现，根本原因是结账服务与关键定价数据缓存之间的网络延迟。随着缓存响应在高流量压力下变慢，系统的重试机制变得不堪重负，导致一系列失败的请求最终使数据库过载。
- en: Scenarios like these and the rising cost of failures have led to the emergence
    of service reliability as a discipline and the practice of chaos engineering (sometimes
    called failure or fault injection testing). The goal of chaos engineering is to
    provide an understanding of how systems behave when stressed in an unusual (chaotic)
    way. The increased popularity of these practices has been fueled by the development
    of new tools, technologies, and practices.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的场景以及故障成本的上升导致了服务可靠性作为一个学科的出现，以及混沌工程（有时称为故障或故障注入测试）的实践。混沌工程的目标是提供对系统在异常（混沌）压力下如何行为的理解。这些实践日益普及，得益于新工具、技术和实践的发展。
- en: The term chaos engineering can be traced back to Netflix in 2010\. The company
    was transitioning its infrastructure to the cloud, which introduced new complexity,
    with hundreds of microservices interacting in unpredictable ways. To test the
    resilience of their systems, Netflix engineers developed Chaos Monkey, a tool
    designed to randomly terminate VM instances in their production environment. This
    simulated real-world failures, forcing engineers to build systems that could gracefully
    handle unexpected disruptions.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 混沌工程的术语可以追溯到2010年的Netflix。该公司正在将其基础设施过渡到云端，这引入了新的复杂性，数百个微服务以不可预测的方式相互交互。为了测试他们系统的弹性，Netflix工程师开发了Chaos
    Monkey，这是一个旨在随机终止其生产环境中虚拟机实例的工具。这模拟了现实世界的故障，迫使工程师构建能够优雅地处理意外中断的系统。
- en: The use of the word chaos and the image of a monkey set loose to randomly terminate
    software in a production environment does conjure mayhem. Given these preconceptions,
    introducing chaos engineering into an organization may be met with resistance.
    More than one boss has wondered, “Don’t we have enough chaos around here?”
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 使用“混沌”一词以及将猴子释放到生产环境中随机终止软件的形象确实会引发混乱。鉴于这些先入为主的观念，将混沌工程引入一个组织可能会遇到阻力。不止一位老板曾想过：“我们这里不是已经够混乱了吗？”
- en: In this chapter, we’ll counter those notions by understanding modern chaos engineering
    as a rigorous approach to implementing experiments. As a methodology, we use this
    *controlled* disruption to test the resilience of our systems. In addition to
    testing our current state, chaos engineering gives us a powerful methodology to
    systematically improve resilience.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过理解现代混沌工程作为一种严格的实验实施方法来反驳那些观念。作为一种方法，我们利用这种*可控*的破坏来测试我们系统的弹性。除了测试我们的当前状态外，混沌工程还给我们提供了一种强大的方法，以系统地提高弹性。
- en: The experiments we conduct give us a deeper understanding of our software’s
    behavior under stress. This knowledge enables us to design targeted improvements.
    We then test to validate their effectiveness in meeting our targets.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行的实验使我们更深入地理解了软件在压力下的行为。这些知识使我们能够设计有针对性的改进。然后我们测试以验证它们在满足我们的目标方面的有效性。
- en: We’ll also cover how to use service-level objectives (SLOs) to set our resiliency
    targets. We’ll look at using error budgets to allow for an acceptable level of
    failure within that target. We’ll see how chaos engineering works with these mechanisms
    by helping us validate whether our system can operate within its error budget
    and still meet our targets even when facing unexpected disruptions.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将介绍如何使用服务级别目标（SLOs）来设定我们的弹性目标。我们将探讨使用错误预算来允许在目标范围内接受一定程度的失败。我们将通过帮助验证我们的系统是否可以在其错误预算内运行并仍然达到目标，即使面临意外的中断，来了解混沌工程如何与这些机制协同工作。
- en: In this chapter we’ll also move beyond static chaos experiments to understand
    a more modern and dynamic approach that involves integrating chaos engineering
    into our CI/CD pipelines, allowing us to continuously assess and improve system
    resilience as part of our regular development workflows.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们还将超越静态的混沌实验，了解一种更现代、更动态的方法，该方法涉及将混沌工程集成到我们的 CI/CD 管道中，使我们能够作为常规开发工作流程的一部分，持续评估和改进系统弹性。
- en: Throughout this chapter, we will explore how advanced chaos engineering tools
    leverage AI/ML-driven insights to recommend and guide the execution of these experiments,
    leading to more efficient and effective resilience testing while reducing risk.
    We will also see how agentic AI, GenAI, and MCP address critical scalability and
    precision challenges in chaos engineering by automating experiment design, enabling
    dynamic risk detection, and providing intelligent remediation. These technologies
    transform chaos engineering from a reactive practice into a proactive, self-optimizing
    system resilience strategy.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨高级混沌工程工具如何利用 AI/ML 驱动的洞察力来推荐和指导这些实验的执行，从而实现更高效、更有效的弹性测试，同时降低风险。我们还将看到代理
    AI、通用 AI 和 MCP 如何通过自动化实验设计、实现动态风险检测和提供智能修复来解决混沌工程中的关键可扩展性和精度挑战。这些技术将混沌工程从一种被动实践转变为一种主动、自我优化的系统弹性策略。
- en: Getting Started with Chaos Engineering
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用混沌工程
- en: While many chaos engineering experiments employ randomness (e.g., selecting
    a random server or service to take down), the practice of chaos engineering is
    as methodical as lab science. In this section, we’ll dive into the core tenets
    of chaos engineering and look at best practices to reduce the risk of causing
    service disruptions when conducting experiments.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然许多混沌工程实验采用随机性（例如，随机选择一个服务器或服务进行关闭），但混沌工程的实践与实验室科学一样严谨。在本节中，我们将深入研究混沌工程的核心原则，并探讨在开展实验时降低造成服务中断风险的最佳实践。
- en: Principles of Chaos Engineering
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混沌工程原理
- en: 'Netflix has defined a set of core principles that provide a useful framework
    for exploring how your systems behave under stress. A structured approach ensures
    that your chaos experiments are not just disruptive events but structured investigations
    that generate valuable data that you can use to drive improvements to your system’s
    resilience. These principles are:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Netflix 已经定义了一套核心原则，为探索你的系统在压力下的行为提供了一个有用的框架。一种结构化的方法确保你的混沌实验不仅是不规则事件，而且是结构化的调查，可以生成有价值的数据，你可以使用这些数据来推动系统弹性的改进。这些原则包括：
- en: Defining a “steady state” that characterizes normal system behavior
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个表征正常系统行为的“稳态”
- en: Observability is key here. You must have the metrics you need to understand
    the normal range of values that indicate your system is healthy and performing
    as expected. This could include request latency, error rates, throughput, or application-specific
    metrics. Be sure to account for external factors that might influence your metrics,
    such as time of day, day of week, or the presence of a marketing campaign that
    could spike traffic.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 可观察性在这里是关键。你必须拥有你需要的指标来理解指示你的系统健康且按预期运行的正常值范围。这可能包括请求延迟、错误率、吞吐量或特定于应用程序的指标。务必考虑可能影响你的指标的外部因素，例如一天中的时间、一周中的哪一天，或者可能引发流量激增的市场营销活动的存在。
- en: Turning expectation into a hypothesis
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 将期望转化为假设
- en: Based on your understanding of the system’s architecture and dependencies, formulate
    a hypothesis about how it *should* behave when a specific failure is introduced.
    Frame your hypothesis in a way that can be objectively tested using your chosen
    metrics. For example, “If we simulate a 20% increase in traffic, the average response
    time should remain below three seconds, and the error rate should not exceed 0.5%.”
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您对系统架构和依赖关系的理解，提出一个假设，即当引入特定故障时，系统应该如何行为。以您选择的标准来客观测试的方式构建您的假设。例如，“如果我们模拟20%的流量增加，平均响应时间应该保持在三秒以下，错误率不应超过0.5%。”
- en: Executing the experiment by simulating real-world events
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 通过模拟现实世界事件来执行实验
- en: Use chaos engineering tools to automate the injection of failures. Simulate
    a server crashing or becoming unavailable, an outage of a critical third-party
    service, or a sudden surge in user requests.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 使用混沌工程工具来自动化故障注入。模拟服务器崩溃或不可用、关键第三方服务的中断或用户请求的突然激增。
- en: Evaluating the results against the hypothesis
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 将结果与假设进行评估
- en: Compare the system’s behavior during the experiment to your established baseline
    and your hypothesized outcome. Did the metrics stay within acceptable ranges?
    Did the system recover as expected? Were any unexpected side effects observed?If
    the system deviates from the expected behavior, investigate the root cause. Based
    on the experiment’s outcome, refine your hypotheses and adjust your system design
    or operational procedures to enhance resilience.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 将实验期间系统的行为与您建立的基线和假设的结果进行比较。指标是否保持在可接受的范围内？系统是否按预期恢复？是否观察到任何意外的副作用？如果系统偏离了预期的行为，调查其根本原因。根据实验的结果，完善您的假设，并调整系统设计或操作程序以提高弹性。
- en: Starting Small and Scaling
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从小规模开始并扩展
- en: Simulating failures to intentionally take down systems does, of course, incur
    risk. We knowingly incur risk in this controlled way to validate the hypothesis
    we’ve defined. An important strategy for reducing risk is to start with small
    experiments.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 故意模拟故障以关闭系统当然会带来风险。我们明知故犯地以这种方式承担风险，以验证我们定义的假设。降低风险的一个重要策略是从小规模的实验开始。
- en: 'To illustrate starting small and scaling experiments, let’s walk through an
    example focused on testing a checkout service in an e-commerce system. This service
    is a critical microservice that processes user purchases. The expected outcome
    is simple: a customer adds items to their cart, proceeds to checkout, and completes
    the payment. The customer expects a smooth, fast, and reliable experience.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明从小规模开始并扩展实验，让我们通过一个专注于测试电子商务系统中结账服务的示例来进行分析。这个服务是一个关键的微服务，负责处理用户购买。预期的结果是简单的：客户将商品添加到购物车，进入结账流程，并完成支付。客户期望获得流畅、快速和可靠的体验。
- en: Behind the scenes, this straightforward operation relies on a series of complex
    processes. The checkout service depends on multiple APIs and external services
    to function properly, including inventory systems, payment gateways, and caching
    layers (like Redis) to quickly retrieve important data such as product prices,
    discounts, and availability.The checkout service fetches pricing data from a cache
    for quick access. If the cache is slow or fails, the checkout service should still
    provide the right information by failing over to another cache instance or even
    to a database as a backup, though it may be slower.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，这个简单的操作依赖于一系列复杂的过程。结账服务依赖于多个API和外部服务才能正常运行，包括库存系统、支付网关和缓存层（如Redis）以快速检索重要数据，例如产品价格、折扣和可用性。结账服务从缓存中获取定价数据以快速访问。如果缓存速度慢或失败，结账服务应通过切换到另一个缓存实例或甚至到数据库作为备份来提供正确信息，尽管这可能会慢一些。
- en: Note
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注
- en: GenAI can transform chaos engineering from manual hypothesis testing into an
    adaptive, self-optimizing resilience validation system. This approach proves particularly
    valuable in critical e-commerce workflows like checkout services, where balancing
    risk mitigation with realistic failure simulation is paramount.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: GenAI可以将混沌工程从手动假设测试转变为自适应、自我优化的弹性验证系统。这种方法在关键电子商务工作流程（如结账服务）中尤其有价值，在这些工作流程中，平衡风险缓解与现实故障模拟至关重要。
- en: 'Developers typically configure retry logic, timeouts, and circuit breakers
    to handle network issues or failures. Let’s look at each:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 开发人员通常配置重试逻辑、超时和断路器来处理网络问题或故障。让我们逐一看看：
- en: Retry logic
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 重试逻辑
- en: This ensures that if a request to the cache fails or experiences network issues,
    the system will automatically try again a few times before giving up. This helps
    handle transient failures. The system might, for example, retry up to three times
    with a delay of 100 ms between each retry.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这确保了如果对缓存的请求失败或遇到网络问题，系统会在放弃之前自动尝试几次。这有助于处理暂时性的故障。例如，系统可能会在每次重试之间延迟100毫秒，最多重试三次。
- en: Timeouts
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 超时
- en: Timeout settings define how long the service should wait for a response before
    deciding that the attempt has failed. This prevents the service from hanging indefinitely
    if the cache is slow or unresponsive. A system might be configured to time out
    after 200 ms for each request to the cache.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 超时设置定义了服务在决定尝试失败之前应该等待响应多长时间。这防止了服务在缓存缓慢或无响应时无限期地挂起。系统可能被配置为每个缓存请求在200毫秒后超时。
- en: Circuit breakers
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 断路器
- en: A circuit breaker prevents further attempts to call a failing service after
    a certain number of failed attempts. If the cache continues to fail or is too
    slow, the circuit breaker “trips” and routes traffic to a fallback system (e.g.,
    another cache or a database). The circuit breaker can automatically reset after
    a set period to test if the original service has recovered. For example, a circuit
    breaker might be configured to trip after five consecutive retries fail.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 断路器在经过一定数量的失败尝试后阻止进一步尝试调用失败的服务。如果缓存继续失败或太慢，断路器“跳闸”，并将流量路由到备用系统（例如，另一个缓存或数据库）。断路器可以在设定的时间后自动重置，以测试原始服务是否已恢复。例如，断路器可能被配置为在连续五次重试失败后跳闸。
- en: We’ll start testing the checkout service by introducing small latencies to ensure
    the retry logic and timeout settings are functioning before scaling up to introduce
    more severe issues that will ultimately trigger the circuit breaker. If all goes
    well, we expect the system to fail over to an alternative data source. These are
    our steps.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过引入小的延迟来开始测试结账服务，以确保在扩展到引入更严重的问题（这将最终触发断路器）之前，重试逻辑和超时设置正在正常工作。如果一切顺利，我们预计系统将切换到备用数据源。这些是我们的步骤。
- en: 'Step 1: Conduct a simple latency experiment'
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第1步：进行简单的延迟实验
- en: We start with a test of our retry logic. We want to ensure the system is resilient
    if network issues arise, such as high latency or a temporary loss of connectivity.
    Our steady state is a responsive service that responds within an acceptable time
    limit.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从测试我们的重试逻辑开始。我们希望确保如果出现网络问题，例如高延迟或暂时性的连接丢失，系统是具有弹性的。我们的稳定状态是响应性服务，在可接受的时间限制内响应。
- en: Our hypothesis is that if the network experiences significant latency when trying
    to reach the cache, the system should use its retry logic and timeout settings
    to handle the issue gracefully, eventually tripping the circuit breaker to prevent
    further degradation of service.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的假设是，如果在尝试访问缓存时网络出现显著的延迟，系统应该使用其重试逻辑和超时设置来优雅地处理问题，最终触发断路器以防止服务进一步退化。
- en: We start small by injecting a small amount of network latency (e.g., 200 ms)
    between the checkout service and the cache.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过在结账服务和缓存之间注入少量的网络延迟（例如，200毫秒）来从小处开始。
- en: We observe whether the retry logic kicks in and whether the service handles
    the delay within the acceptable time limit without user impact. We continue to
    monitor whether the system continues functioning as expected, pulling from the
    cache after the latency delay.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察重试逻辑是否启动，以及服务是否在可接受的时间限制内处理延迟，而不会对用户产生影响。我们继续监控系统是否继续按预期工作，在延迟延迟后从缓存中获取数据。
- en: 'Step 2: Test resilience against a more significant network issue'
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第2步：测试对更严重的网络问题的弹性
- en: Once we’ve tested our retry logic with a small latency, we can increase the
    scope and intensity of the experiment to simulate a more significant network issue.
    This tests our timeouts. We increase the network latency (e.g., from 500 ms to
    1 second) to see how the service behaves under heavier load or network congestion.
    We test how the retry logic handles the extended delay. Does the service retry
    the call to the cache, and does it respect the timeout setting? If so, we increase
    the severity of the issue by causing the cache API to completely fail after a
    set number of retries.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们使用小延迟测试了我们的重试逻辑，我们就可以增加实验的范围和强度，以模拟更重大的网络问题。这测试了我们的超时。我们增加网络延迟（例如，从500毫秒到1秒），以查看服务在更重负载或网络拥塞下的表现。我们测试重试逻辑如何处理延长的延迟。服务是否重试对缓存的调用，并且是否尊重超时设置？如果是这样，我们通过在设置的重试次数后使缓存API完全失败来增加问题的严重性。
- en: 'Step 3: Validate that the circuit breaker fails over to an alternative'
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第3步：验证断路器是否切换到备用
- en: We next set experiment conditions to render the cache inaccessible. After retrying,
    the circuit breaker mechanism should be triggered. When the circuit breaker is
    tripped, the checkout service fails over to an alternative data source, such as
    another cache instance in a different data center (in this case, our Postgres
    database). While the Postgres database might be slower than the cache, the goal
    is to keep the service operational, albeit with slightly degraded performance.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来设置实验条件以使缓存不可访问。在重试后，断路器机制应该被触发。当断路器跳闸时，检查服务将切换到备用数据源，例如位于不同数据中心（在这种情况下，我们的Postgres数据库）的另一个缓存实例。虽然Postgres数据库可能比缓存慢，但目标是保持服务运行，尽管性能略有下降。
- en: 'AI agents can make this process even simpler by dynamically adjusting failure
    injection parameters using reinforcement learning. For example:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用强化学习动态调整故障注入参数，AI代理可以使此过程更加简单。例如：
- en: Start with 200 ms delays, then autonomously scale to 500 ms to 1 second based
    on real-time performance telemetry.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从200毫秒的延迟开始，然后根据实时性能遥测自动扩展到500毫秒到1秒。
- en: Limit experiment impact to 0.5% of transactions initially, expanding only after
    validating safety mechanisms.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最初将实验影响限制在交易量的0.5%，仅在验证安全机制后扩大。
- en: Optimize trip thresholds (e.g., five failures to four) through historical success
    pattern analysis.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过历史成功模式分析优化跳闸阈值（例如，五次失败转为四次）。
- en: You can further scale the experiment to test the resilience of the failover
    by introducing similar network issues between the checkout service and the Postgres
    database to see how the system continues to adapt under increased failure conditions.
    By following this process, we gradually increase the complexity of the experiment
    to validate the system’s resilience mechanisms, without jumping into major disruptions
    immediately.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过引入类似网络问题来进一步扩展实验，以测试故障转移的弹性，这些网络问题存在于检查服务和Postgres数据库之间，以查看系统在增加的故障条件下的适应情况。通过遵循此过程，我们逐渐增加实验的复杂性，以验证系统的弹性机制，而无需立即跳入重大中断。
- en: It’s important to note that the initial settings for resilience mechanisms are
    often based on educated guesses rather than precise data. This is another reason
    that testing through chaos engineering experiments is so crucial.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，弹性机制的初始设置通常基于有根据的猜测而不是精确数据。这也是通过混沌工程实验进行测试如此关键的原因之一。
- en: Injecting network latency is just one condition we can scale in a chaos experiment.
    We’ll discuss other conditions later in this section.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在混沌实验中，注入网络延迟只是我们可以扩展的一个条件。我们将在本节稍后讨论其他条件。
- en: Starting in Production-Like Environments
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从类似生产环境开始
- en: Another important best practice to minimize risk in chaos engineering is to
    test experiments in pre-production environments before moving to production. This
    allows us to experiment safely without impacting real users. We can rapidly iterate,
    adjust parameters, and observe results free from production constraints. Once
    we confirm system resilience in these settings, we promote our experiment to the
    next environment, eventually reaching production. Each promotion carries risk,
    so we proceed with caution. Configuration drift between environments can lead
    to discrepancies in experiment results. Maintaining the “start small and scale”
    approach when moving between environments is crucial in case we encounter issues.
    Thoroughly vetting our experiments in pre-production ensures that our experiments
    are well-designed and insightful without unintended consequences.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的最佳实践是在将实验移至生产之前在预生产环境中测试实验，以最小化混沌工程中的风险。这允许我们在不影响真实用户的情况下安全地进行实验。我们可以快速迭代、调整参数并观察结果，不受生产约束的限制。一旦在这些设置中确认系统弹性，我们就将实验提升到下一个环境，最终达到生产环境。每次提升都伴随着风险，因此我们谨慎行事。环境之间的配置漂移可能导致实验结果的不一致。在环境之间移动时保持“从小规模开始并扩展”的方法在遇到问题时至关重要。在预生产中对我们的实验进行彻底审查确保我们的实验设计良好且富有洞察力，没有意外的后果。
- en: Leveraging Modern Tools
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用现代工具
- en: 'We looked extensively at an example of testing network latency in a chaos experiment.
    There are many other types of conditions that are important to test. Modern tools
    (such as Harness Chaos Engineering, Chaos Monkey, and LitmusChaos) can help here
    by offering extensive catalogs of predefined experiments. Modern tools will typically
    offer chaos engineering experiments across categories and common failure patterns,
    including:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在混沌实验中广泛研究了测试网络延迟的示例。还有许多其他类型的条件是重要的测试对象。现代工具（如 Harness 混沌工程、Chaos Monkey
    和 LitmusChaos）通过提供广泛的预定义实验目录来帮助这里。现代工具通常会提供跨类别和常见故障模式的混沌工程实验，包括：
- en: Resource exhaustion
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 资源耗尽
- en: CPU exhaustion
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: CPU 资源耗尽
- en: Force high CPU utilization to simulate a process consuming excessive processing
    power.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 强制高 CPU 利用率以模拟进程消耗过多的处理能力。
- en: Memory exhaustion
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 内存耗尽
- en: Consume all available memory to test how your application handles memory pressure
    and potential out-of-memory errors.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 消耗所有可用内存以测试您的应用程序如何处理内存压力和潜在的内存不足错误。
- en: Disk I/O exhaustion
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 磁盘 I/O 资源耗尽
- en: Generate heavy disk read/write operations to simulate storage bottlenecks.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 生成大量的磁盘读写操作以模拟存储瓶颈。
- en: Network bandwidth exhaustion
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 网络带宽耗尽
- en: Saturate network bandwidth to test how your application performs under network
    congestion.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 满足网络带宽以测试您的应用程序在网络拥塞下的性能。
- en: Network disruption
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 网络中断
- en: Network latency
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 网络延迟
- en: Introduce delays in network communication between services or with external
    dependencies.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在服务或外部依赖之间引入网络通信延迟。
- en: Packet loss
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 数据包丢失
- en: Simulate the loss of network packets to test how your application handles unreliable
    connections.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟网络数据包丢失以测试您的应用程序如何处理不可靠的连接。
- en: Network partition
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 网络分区
- en: Isolate parts of your network to simulate connectivity issues between services
    or availability zone outages.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 将您的网络部分隔离以模拟服务或可用区故障之间的连接问题。
- en: DNS failures
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: DNS 故障
- en: Simulate DNS resolution problems to test how your application handles DNS outages
    or incorrect responses.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟 DNS 解析问题以测试您的应用程序如何处理 DNS 崩溃或错误的响应。
- en: Infrastructure failure
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 基础设施故障
- en: Node failure
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 节点故障
- en: Terminate or shut down VMs or containers to simulate hardware failures.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 终止或关闭虚拟机或容器以模拟硬件故障。
- en: Pod failure (Kubernetes)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 故障（Kubernetes）
- en: Kill or evict pods to test the self-healing capabilities of your Kubernetes
    deployments.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 终止或驱逐 pods 以测试您的 Kubernetes 部署的自我修复能力。
- en: Availability zone outage
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 可用区故障
- en: Simulate the failure of an entire availability zone to test your disaster recovery
    plan and multiregion deployments.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟整个可用区故障以测试您的灾难恢复计划和跨区域部署。
- en: Inference layer attacks
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 推理层攻击
- en: Simulate GPU memory exhaustion during ML model serving.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习模型服务期间模拟 GPU 内存耗尽。
- en: Application-level faults
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 应用层故障
- en: Service failure
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 服务故障
- en: Stop or crash specific services within your application to test fault tolerance
    and service degradation.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 停止或崩溃应用程序中的特定服务以测试容错性和服务降级。
- en: Function failure
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 函数故障
- en: Introduce errors or exceptions within specific functions or methods to test
    error handling and recovery mechanisms.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在特定函数或方法中引入错误或异常以测试错误处理和恢复机制。
- en: Data corruption
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 数据损坏
- en: Corrupt data in a database or storage system to test your data integrity and
    recovery processes.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据库或存储系统中损坏数据，以测试您的数据完整性和恢复流程。
- en: State management
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 状态管理
- en: Time travel
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 时间旅行
- en: Manipulate the system clock to simulate time shifts, testing how your application
    handles time-sensitive operations or scheduled tasks.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统时钟以模拟时间变化，测试您的应用程序如何处理时间敏感的操作或计划任务。
- en: State injection
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 状态注入
- en: Inject specific data or states into your application to test its behavior under
    unusual conditions. Use GenAI to create plausible corrupt data entries matching
    schema constraints.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 将特定数据或状态注入您的应用程序以测试其在异常条件下的行为。使用生成式AI创建符合模式约束的合理损坏数据条目。
- en: Dynamic scenario generation using AI
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 使用AI动态生成场景
- en: Architecture modeling
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 架构建模
- en: Use AI to analyze service dependencies (e.g., Redis cache → payment gateway
    → database) to create failure chains mirroring production environments.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 使用AI分析服务依赖关系（例如，Redis缓存→支付网关→数据库），以创建与生产环境相似的故障链。
- en: Generative adversarial networks
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: Create novel failure modes by pitting AI models against each other to discover
    unexplored vulnerability combinations.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将AI模型相互对抗，创建新的故障模式，以发现未探索的漏洞组合。
- en: The more types of experiments we try, the more we can learn about weaknesses
    in our system and how we can strengthen our resiliency.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试的实验类型越多，我们就能更多地了解我们系统中的弱点以及我们如何加强我们的弹性。
- en: Newer tools can go beyond offering catalogs to analyze your system architecture
    to suggest targeted experiments that expose potential weaknesses specific to your
    setup. For example, for software built with a microservices architecture, a chaos
    engineering tool might analyze network traffic and dependencies to identify critical
    services and suggest experiments that target these specifically. A modern tool
    might also recommend injecting latency or errors into API calls between services
    to test resilience to communication disruptions.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 新的工具不仅能提供目录，还能分析您的系统架构，并提出针对性的实验，以揭示您特定设置中的潜在弱点。例如，对于使用微服务架构构建的软件，混沌工程工具可能会分析网络流量和依赖关系，以识别关键服务并建议针对这些服务的特定实验。现代工具还可能建议在服务之间的API调用中注入延迟或错误，以测试对通信中断的弹性。
- en: For applications deployed with Kubernetes, the tool could analyze your Kubernetes
    deployments and suggest experiments that target specific pods, deployments, or
    namespaces to test replica scaling, resource limits, and health checks. Tools
    like Red Hat’s Krkn use AI to profile Kubernetes pods to prioritize network-intensive
    services for partition tests. In the case of multiregion deployments, a modern
    tool might analyze your multiregion setup and suggest experiments that simulate
    regional failures or network partitions to test your disaster recovery plan and
    the ability of your application to failover to another region.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用Kubernetes部署的应用程序，该工具可以分析您的Kubernetes部署，并提出针对特定Pod、部署或命名空间的实验建议，以测试副本扩展、资源限制和健康检查。像Red
    Hat的Krkn这样的工具使用AI来分析Kubernetes Pod，以优先考虑网络密集型服务进行分区测试。在多区域部署的情况下，现代工具可能会分析您的多区域设置，并提出模拟区域故障或网络分区的实验建议，以测试您的灾难恢复计划以及您的应用程序切换到另一个区域的能力。
- en: Learning from Others
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向他人学习
- en: Paying close attention to industry-wide incidents, particularly those affecting
    companies with similar tech stacks, is crucial for proactive risk mitigation.
    For instance, an OpenAI outage on December 11, 2024, serves as a stark reminder
    of how seemingly minor deployments can have cascading consequences.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 密切关注行业范围内的事件，尤其是那些影响具有相似技术堆栈的公司的事件，对于主动风险缓解至关重要。例如，2024年12月11日OpenAI的故障，是一个鲜明的提醒，表明看似微小的部署可能会产生连锁反应。
- en: 'In this case, a new telemetry service overwhelmed the company’s Kubernetes
    control plane, triggering DNS failures that brought down its API, ChatGPT, and
    Sora platforms for hours. The impact was widespread and long-lasting: for several
    hours, developers and users couldn’t access the services they rely on. Engineers
    identified the root cause within minutes but faced a major hurdle—without access
    to the Kubernetes control plane, rolling back or fixing the deployment was extremely
    challenging.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，一家公司的新遥测服务压倒了其Kubernetes控制平面，引发了DNS故障，导致其API、ChatGPT和Sora平台瘫痪数小时。影响广泛且持久：数小时内，开发者和用户无法访问他们依赖的服务。工程师几分钟内就确定了根本原因，但面临一个主要障碍——没有访问Kubernetes控制平面，回滚或修复部署极为困难。
- en: Let’s look at a few targeted chaos engineering experiments to see how these
    cascading failures might have been prevented.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看几个有针对性的混沌工程实验，看看这些级联故障是如何可能被预防的。
- en: 'Experiment 1: Control plane overload simulation'
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实验1：控制平面过载模拟
- en: First, we design an experiment to test our Kubernetes API server resilience.
    In this experiment, we would intentionally flood the Kubernetes API server with
    a high volume of read/write operations to mimic what the new telemetry service
    did in production. By running this test on a staging environment with a production-like
    scale, we could have spotted the exact threshold where the API server starts to
    fail. This early detection would inform better load limiting, improved alerting,
    and possibly a safer phased rollout strategy.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们设计一个实验来测试我们的Kubernetes API服务器的弹性。在这个实验中，我们会故意向Kubernetes API服务器发送大量读写操作，以模拟新遥测服务在生产环境中所做的操作。通过在一个具有类似生产规模的预演环境中运行这个测试，我们可以发现API服务器开始失败的确切阈值。这种早期检测将有助于更好的负载限制、改进的警报以及可能更安全的分阶段发布策略。
- en: 'Experiment 2: DNS failure testing'
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实验2：DNS故障测试
- en: This experiment would involve introducing latency or failures in the DNS resolution
    process—specifically targeting the components responsible for service discovery.
    Running this experiment helps confirm that essential services can continue functioning
    even if DNS is disrupted. We will discover if our caches, fallback mechanisms,
    or alternative routing strategies are sufficient. If not, we would know to invest
    in those areas before a real outage hits.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实验将涉及在DNS解析过程中引入延迟或故障——特别是针对负责服务发现的组件。运行这个实验有助于确认即使DNS被中断，基本服务也能继续运行。我们将发现我们的缓存、回退机制或替代路由策略是否足够。如果不充分，我们就会知道在真正的事故发生之前，需要在这些领域进行投资。
- en: 'Example 3: Break-glass access drills'
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例3：破窗访问演习
- en: This last experiment (or drill) involves simulating a situation where engineers
    are locked out of the Kubernetes API under heavy load. By practicing emergency
    access methods—like having dedicated out-of-band channels or specialized tooling—teams
    can quickly revert or disable problematic deployments when the standard control
    plane is inaccessible. If this drill had been done beforehand, teams would have
    known exactly how to remove the faulty telemetry service within minutes, minimizing
    downtime.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 最后这个实验（或演习）涉及模拟工程师在重负载下被锁出Kubernetes API的情况。通过练习紧急访问方法——比如拥有专门的备用通道或专用工具——团队可以在标准控制平面不可访问时快速回滚或禁用有问题的部署。如果这个演习事先进行过，团队就会知道如何在几分钟内确切地移除有缺陷的遥测服务，从而最小化停机时间。
- en: Service-Level Objectives and Service Resiliency
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务级别目标和服务的弹性
- en: We see how chaos engineering helps us uncover weaknesses and build more resilient
    systems. But how do we define “resilient”? How do we measure and track whether
    our systems are meeting our reliability goals? This is where SLOs and service-level
    indicators (SLIs) come in. Together, these provide the framework for defining
    and measuring the reliability of our services, giving us a clear target to aim
    for and a way to track our progress.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到混沌工程如何帮助我们揭示弱点并构建更具有弹性的系统。但“弹性”是如何定义的呢？我们如何衡量和跟踪我们的系统是否达到了我们的可靠性目标？这正是SLOs（服务级别目标）和服务级别指标（SLIs）发挥作用的地方。它们共同为定义和衡量我们服务的可靠性提供了框架，为我们提供了一个清晰的目标，以及跟踪我们进步的方法。
- en: SLOs are the targets we set for the reliability of our services. SLIs are the
    specific metrics we use to measure whether we’re meeting those targets. SLOs are
    typically expressed as a percentage of time or number of requests that must meet
    the defined SLI criteria. For example, *99.9% of requests should have a latency
    of under 200 milliseconds*. SLIs are the specific, measurable metrics that reflect
    the performance of your service from a user’s perspective. They quantify aspects
    like availability, latency, error rate, throughput, and other relevant factors.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: SLOs是我们为服务的可靠性设定的目标。SLIs是我们用来衡量是否达到这些目标的特定指标。SLOs通常以百分比时间或必须满足定义的SLI标准的请求数量来表示。例如，*99.9%的请求应该有低于200毫秒的延迟*。SLIs是反映您服务性能的特定、可衡量的指标，从用户的角度来看。它们量化了可用性、延迟、错误率、吞吐量和其他相关因素。
- en: In essence, SLIs are *what* you measure, and SLOs are the *targets* you set
    for those measurements.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，SLIs是你要衡量的内容，而SLOs是为这些测量设定的目标。
- en: Establishing Reliability Targets
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建立可靠性目标
- en: When establishing reliability targets, it’s essential to align them with the
    overarching business needs. Monitoring and observability solutions provide many
    SLI metrics, but it is important to prioritize those that accurately reflect how
    your customers experience your applications. The goal is not to track every individual
    service, but to focus on those services that are critical to the customer experience.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在制定可靠性目标时，必须确保它们与整体业务需求相一致。监控和可观察性解决方案提供了许多 SLI 指标，但重要的是要优先考虑那些准确反映客户体验的指标。目标是跟踪每个单独的服务，而是关注那些对客户体验至关重要的服务。
- en: 'Common SLIs include “the four golden signals”:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的 SLI 包括“四个黄金信号”：
- en: Request latency
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 请求延迟
- en: The time taken to process a user request
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 处理用户请求所需的时间
- en: Throughput
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 吞吐量
- en: The number of requests processed per second
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 每秒处理的请求数量
- en: Error rate
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 错误率
- en: The percentage of failed requests
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 失败请求的百分比
- en: Saturation
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 饱和
- en: The utilization percentage of the system
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 系统的利用率百分比
- en: Consider carefully how to implement each of these within your system. For instance,
    when measuring latency (response time), you can choose to track all transactions
    or focus on a subset of the most crucial ones, such as login, payment submission,
    or adding items to a shopping cart. Again, select a metric that provides a meaningful
    representation of your customers’ experience.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细考虑如何在您的系统中实施这些指标。例如，在测量延迟（响应时间）时，您可以选择跟踪所有事务或专注于最重要的子集，例如登录、支付提交或添加购物车中的项目。再次强调，选择一个能够提供客户体验有意义表现的指标。
- en: Shared Ownership of System Reliability
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 系统可靠性的共享所有权
- en: In [Chapter 1](ch01.html#chapter_1_the_road_to_ai_native_devops_1749354009875299),
    we introduced DevOps as practices that combine software development (Dev) and
    IT operations (Ops) concerns. Nowhere are shared ownership and collaboration more
    important than in ensuring system reliability. SLOs are a great example of this
    shared responsibility. Development, operations, and reliability teams should work
    together to define SLOs. The collaboration establishes an understanding of acceptable
    system performance and creates a common goal for everyone to work toward. SLOs
    then act as a guide for making decisions that balance the need for rapid development
    (velocity) with the need for stable and reliable systems.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 1 章](ch01.html#chapter_1_the_road_to_ai_native_devops_1749354009875299)中，我们介绍了
    DevOps 作为结合软件开发（Dev）和 IT 运营（Ops）关注的实践。在确保系统可靠性方面，共享所有权和协作尤为重要。SLO（服务等级目标）是这种共享责任的绝佳例子。开发、运营和可靠性团队应共同努力定义
    SLO。这种协作建立了对可接受系统性能的理解，并为每个人设定了一个共同的目标。SLO 然后作为指南，在平衡快速开发（速度）和稳定可靠系统需求之间做出决策。
- en: With this shared understanding, developers can prioritize features that maintain
    reliability, knowing how their work impacts overall system performance. At the
    same time, operations teams gain the context they need to effectively support
    the application. If an SLO is breached, it triggers activities that encourage
    engineering teams to stabilize the service before releasing new features. This
    helps prevent a cycle of instability and ensures that reliability remains a top
    priority.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种共享理解，开发者可以优先考虑维护可靠性的功能，了解他们的工作如何影响整体系统性能。同时，运营团队获得了有效支持应用程序所需的环境。如果 SLO
    被违反，它将触发活动，鼓励工程团队在发布新功能之前稳定服务。这有助于防止不稳定循环，并确保可靠性始终是首要任务。
- en: A collaborative approach to designing, prioritizing, and conducting the chaos
    engineering experiments themselves brings teams together. All teams benefit from
    the insights gained from these experiments and from working together to address
    when failures are found.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 一种协作方法，用于设计、优先排序和执行混沌工程实验本身，将团队聚集在一起。所有团队都从这些实验中获得见解，并从一起工作时解决故障中受益。
- en: Modern tools facilitate this collaborative approach to system reliability. Monitoring
    platforms, incident management systems, and communication tools give a shared
    visibility into system performance and potential issues. Real-time data and automated
    alerts empower both Dev and Ops teams to respond quickly to incidents. More importantly,
    these tools foster a culture of proactive problem-solving (such as data-driven
    prioritization, real-time collaboration triage, etc.), where teams can identify
    and address potential issues before they impact users.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现代工具促进了这种系统可靠性的协作方法。监控平台、事件管理系统和通信工具提供了对系统性能和潜在问题的共享可见性。实时数据和自动警报使开发和运维团队能够快速响应事件。更重要的是，这些工具培养了一种主动解决问题的文化（如数据驱动的优先级排序、实时协作分类等），团队可以在问题影响用户之前识别并解决潜在问题。
- en: Error Budgets and Their Role in Reliability and Innovation
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 错误预算及其在可靠性和创新中的作用
- en: We’ve learned how chaos engineering helps us proactively find system weaknesses,
    and how SLOs and SLIs provide a clear framework for defining our reliability goals
    and measuring whether our systems are meeting those targets. [Error budgets](https://oreil.ly/K0JqJ)
    enter to provide a safety net.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解到混沌工程如何帮助我们主动发现系统弱点，以及SLOs和SLIs如何为我们定义可靠性目标并提供一个清晰的框架，以衡量我们的系统是否达到这些目标。[错误预算](https://oreil.ly/K0JqJ)提供了安全网。
- en: Error budgets represent the maximum amount of unreliability or downtime that
    a service can have while still meeting its SLOs. By tolerating minor hiccups in
    the pursuit of rapid innovation, error budgets acknowledge that perfection is
    unattainable, and instead help us achieve an acceptable level of reliability that
    balances these two competing priorities.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 错误预算代表了服务在满足其SLOs的同时可以容忍的最大不可靠性或停机时间。通过容忍快速创新过程中的小故障，错误预算承认完美是无法实现的，并帮助我们实现一个可接受的可靠性水平，平衡这两个相互竞争的优先级。
- en: Let’s look at how this works by returning to our e-commerce example. Imagine
    we’ve set an SLO of 99.9% for website logins taking less than 300 ms. Over a one-week
    period, this translates to a maximum allowable SLO violation time of 10.08 minutes.
    This is our error budget. How does that impact us? In the event that the error
    budget burns down to zero, we will stop or slow down deployments of new software
    and focus on stabilizing the system while our error budget replenishes. Not only
    does the state of our error budget impact our deployment priorities, but it also
    factors into chaos testing priorities.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过回到我们的电子商务示例来看看这是如何工作的。想象一下，我们为小于300毫秒的网站登录设置了99.9%的服务目标（SLO）。在一周内，这相当于10.08分钟的允许的最大SLO违规时间。这是我们错误预算。这会对我们产生什么影响？如果错误预算耗尽到零，我们将停止或减缓新软件的部署，并专注于稳定系统，同时我们的错误预算得到补充。不仅我们的错误预算状态影响我们的部署优先级，它还影响到混沌测试的优先级。
- en: Monitoring to Inform Chaos Testing Experiments
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控以提供混沌测试实验的信息
- en: Keeping a close eye on your SLIs does more than just alert you to immediate
    problems—it reveals potential weaknesses in your system. For example, if you notice
    your system constantly pushing against latency limits, draining your error budget,
    your system might be struggling to keep up in high-traffic scenarios. This suggests
    a good area to focus your chaos experiments on. By simulating those high-traffic,
    high-latency situations, you can see how your system holds up under pressure and
    make sure it can still meet its SLOs during peak usage.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 密切关注你的服务级别指标（SLIs）不仅能让你及时发现即时问题——它还能揭示你系统中潜在的不稳定因素。例如，如果你注意到你的系统不断逼近延迟限制，耗尽错误预算，那么你的系统可能在高流量场景下难以维持。这表明，这是一个很好的领域，可以集中进行混沌实验。通过模拟这些高流量、高延迟的情况，你可以看到你的系统在压力下的表现，并确保在高峰使用期间仍能满足其服务目标（SLOs）。
- en: With modern tools, you can automate this by automatically triggering chaos tests
    based on these patterns, so you can continuously test and improve your system’s
    resilience without lifting a finger. Modern platforms can correlate SLI trends
    with chaos test recommendations using AI, thus increasing test coverage significantly.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 使用现代工具，你可以通过自动触发基于这些模式的混沌测试来自动化这个过程，这样你就可以在不费吹灰之力的情况下持续测试和改进你系统的弹性。现代平台可以使用人工智能将SLI趋势与混沌测试建议相关联，从而显著增加测试覆盖率。
- en: Strategic Use of Error Budgets for Chaos Testing Experiments
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混沌测试实验中错误预算的战略使用
- en: Error budgets are not merely a safety net for occasional failures; they are
    a tool for managing risk. Using our e-commerce website example, we think of the
    10.08-minute error budget as a resource to be spent wisely. In this section we’ll
    look at how to proactively use this budget to conduct chaos experiments.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 错误预算不仅仅是偶尔失败的保障网；它们是管理风险的工具。以我们的电子商务网站为例，我们认为10.08分钟的错误预算是一种需要明智使用的资源。在本节中，我们将探讨如何积极使用这笔预算来开展混沌实验。
- en: Prioritizing chaos experiments in alignment with your available error budget
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 根据可用的错误预算优先考虑混沌实验
- en: Effective chaos engineering requires consideration of your available error budget.
    When your error budget is healthy, your runway is long. You have more freedom
    to conduct aggressive experiments, simulating large-scale failures or pushing
    critical system components to their limits. This might involve testing failover
    mechanisms, injecting network latency, or even simulating the complete outage
    of a core service.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的混沌工程需要考虑你的可用错误预算。当你的错误预算健康时，你的运行时间就长。你有更多的自由进行激进的实验，模拟大规模故障或推动关键系统组件达到极限。这可能包括测试故障转移机制、注入网络延迟，甚至模拟核心服务的完全中断。
- en: As your error budget dwindles, it’s essential to shift focus toward smaller-scale
    experiments that carry less risk of significant disruption. These might involve
    testing individual components in isolation, simulating minor network issues, or
    validating the resilience of recent changes. Prioritizing experiments in this
    way ensures that you can continue to learn and improve without jeopardizing overall
    system stability.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的错误预算减少时，转向关注规模较小、风险较低的实验至关重要。这些实验可能包括单独测试单个组件、模拟轻微的网络问题或验证最近更改的弹性。以这种方式优先考虑实验确保你可以在不危及整体系统稳定性的情况下继续学习和改进。
- en: Modern automation tools can help. By analyzing your error budget in real time,
    these tools can recommend appropriate experiments based on your available “room
    for failure.” This allows you to maintain a balance between proactive testing
    and service reliability, ensuring that your chaos engineering efforts are both
    insightful and responsible.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现代自动化工具可以帮助。通过实时分析你的错误预算，这些工具可以根据你的可用“失败空间”推荐适当的实验。这允许你在主动测试和服务可靠性之间保持平衡，确保你的混沌工程工作既具有洞察力又负责任。
- en: Protect your error budget by utilizing AI-augmented dry runs and simulations
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过利用增强人工智能的干跑和模拟来保护你的错误预算
- en: Simulating first is another strategy to minimize the risk of unintended consequences
    during chaos experiments. This is especially important when up against a dwindling
    error budget. The practice of AI-augmented “dry running” chaos experiments involves
    simulating experiments in a controlled environment, using system models or replicas,
    to assess their potential impact before executing them in production, and using
    AI remediation agents to roll back experiments if anomaly detection thresholds
    breach predefined limits. By identifying potential issues and refining experiment
    parameters beforehand, teams can reduce the likelihood of causing significant
    disruptions that could drain your error budget and cause significant disruptions.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟先行是另一种策略，以最小化混沌实验中意外后果的风险。这在面对日益减少的错误预算时尤为重要。使用人工智能增强的“干跑”混沌实验实践涉及在一个受控环境中模拟实验，使用系统模型或副本来评估它们在执行生产前的潜在影响，并使用人工智能修复代理在异常检测阈值超过预定义限制时回滚实验。通过事先识别潜在问题和细化实验参数，团队可以降低造成重大中断的可能性，这些中断可能会耗尽你的错误预算并造成重大破坏。
- en: Integrating Chaos Engineering and SLOs into CI/CD Pipelines
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将混沌工程和SLOs集成到CI/CD管道中
- en: Reliability issues are primarily driven by change, changes to our applications,
    or changes to your infrastructure. Google DevOps Research and Assessment (DORA)
    defines the change failure rate (CFR) metric, which gives us another view of the
    challenge. The CFR describes how often our changes, such as new code deployments
    or infrastructure updates, introduce problems in production that require hotfixes
    or rollbacks. The DORA 2024 State of DevOps report indicates that 80% of surveyed
    teams have average CFRs of 20% of their releases. In fact, 25% of teams have CFRs
    averaging an alarming 40% of releases.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 可靠性问题主要是由变化引起的，无论是我们应用程序的变化还是您基础设施的变化。Google DevOps Research and Assessment
    (DORA) 定义了变更失败率（CFR）指标，这为我们提供了对挑战的另一种视角。CFR 描述了我们的变更，如新代码部署或基础设施更新，在生产中引入问题，需要热修复或回滚的频率。DORA
    2024 年 DevOps 状态报告表明，80% 的受访团队的平均 CFR 为其发布版本的 20%。实际上，25% 的团队的平均 CFR 高达令人担忧的 40%。
- en: In addition, we must consider the time and cost to remediate each change failure.
    The failed deployment recovery time metric (replacing the similar mean time to
    recovery [MTTR] metric) focuses on how quickly an organization can recover from
    failures. This gives us a sense of the challenges teams face on this front. While
    many teams are able to remediate in less than a day, 25% require a week to a month
    to replace defective software.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们必须考虑修复每个变更失败所需的时间和成本。失败的部署恢复时间指标（取代类似的平均恢复时间 [MTTR] 指标）关注的是组织从故障中恢复的速度。这让我们对团队在这一方面的挑战有了感觉。虽然许多团队能够在一天内修复，但仍有
    25% 的团队需要一周到一个月的时间来替换有缺陷的软件。
- en: Throughout earlier chapters, we’ve looked at strategies to prevent defects from
    getting to production. We test at every stage in our delivery pipelines, executing
    tests of every type. We take care in managing our environments. We guard against
    configuration drift with practices like GitOps, combined with IaC. And we conduct
    chaos engineering testing in pre-production and production environments to help
    us find weaknesses in our systems. Yet, despite our best efforts, occasional defects
    that require fast remediation are inevitable. This is where continuous resilience
    comes in.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们探讨了防止缺陷进入生产的策略。我们在交付管道的每个阶段进行测试，执行各种类型的测试。我们小心地管理我们的环境。我们使用 GitOps
    等实践来防止配置漂移，并结合 IaC。我们在预生产和生产环境中进行混沌工程测试，以帮助我们找到系统中的弱点。尽管我们尽了最大努力，但偶尔需要快速修复的缺陷仍然是不可避免的。这就是持续弹性的作用所在。
- en: Just as continuous integration and continuous delivery are about using automation
    to build, test, and deploy our code, continuous resilience is about automating
    our resiliency practices by adding chaos engineering experiments to our CI/CD
    pipelines. Doing so means we are not just testing functionality, but actively
    and constantly evaluating how changes stand to impact the stability of our systems.
    Using AI agents for DevOps, chaos experiments can be intelligently integrated
    into CI/CD pipelines.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 正如持续集成和持续交付是关于使用自动化来构建、测试和部署我们的代码一样，持续弹性是通过将混沌工程实验添加到我们的 CI/CD 管道中来自动化我们的弹性实践。这样做意味着我们不仅测试功能，而且积极且持续地评估变化对我们系统稳定性的影响。使用
    AI 代理进行 DevOps，混沌实验可以智能地集成到 CI/CD 管道中。
- en: In [“Scaling Your Chaos Engineering Practices”](#chapter_6_scaling_your_chaos_engineering_practices_1749354010917388)
    we’ll explore how to scale a chaos engineering practice by incorporating it into
    our delivery pipeline with the help of modern tools. We’ll look at how to prioritize
    the experiments to add to your pipeline, and best practices for securing and governing
    chaos experiments in your pipeline.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在[“扩展您的混沌工程实践”](#chapter_6_scaling_your_chaos_engineering_practices_1749354010917388)中，我们将探讨如何通过将混沌工程实践纳入我们的交付管道并借助现代工具来扩展它。我们将探讨如何优先考虑添加到管道中的实验，以及确保您的管道中混沌实验安全和管理的最佳实践。
- en: Scaling Your Chaos Engineering Practices
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展您的混沌工程实践
- en: Organizations start their chaos engineering journey in different ways. Often
    a single team or two will adopt an open source tool and introduce experiments
    in a small pocket of an organization. An organization may host periodic chaos
    engineering “game days.” There are all-hands-on-deck, planned events where teams
    deliberately inject a series of failures into systems to practice incident response
    and identify weaknesses in a controlled environment. These are typically infrequent
    and responses are reactive to issues that are discovered.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 组织开始混沌工程之旅的方式各不相同。通常，一个或两个团队会采用开源工具，并在组织的一个小范围内引入实验。一个组织可能会定期举办混沌工程“游戏日”。这些是全员参与、计划中的活动，团队会故意向系统中注入一系列故障以练习事件响应并识别受控环境中的弱点。这些活动通常不频繁，且对发现的问题的反应是反应性的。
- en: The trick to implementing continuous resiliency at scale, across an organization,
    can be a matter of choosing the right tooling. While both open source and proprietary
    solutions offer valuable capabilities, organizations should carefully evaluate
    their requirements. Some enterprise environments may need specific features like
    advanced security controls, comprehensive audit trails, and RBAC—features that
    may vary in availability and maturity across open source solutions.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在组织规模上实施持续弹性，选择合适的工具可能是一个关键问题。虽然开源和专有解决方案都提供了有价值的特性，但组织应仔细评估其需求。一些企业环境可能需要特定的功能，如高级安全控制、全面的审计跟踪和RBAC——这些功能在开源解决方案中的可用性和成熟度可能有所不同。
- en: This challenge was acutely felt by a leading fintech company processing over
    a billion daily payment transactions. Faced with increasing transaction failures
    during peak demand, it sought a solution to improve the reliability of its complex
    platform supporting 20+ financial products.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这项挑战在一个每天处理超过十亿笔支付交易的主要金融科技公司中尤为明显。面对高峰需求期间交易失败的不断增加，它寻求一种解决方案来提高其支持20多个金融产品的复杂平台的可靠性。
- en: The company’s choice of a modern chaos engineering tool was instrumental in
    overcoming the obstacles of scaling its chaos engineering practices. The tool
    it chose (in this case, Harness Chaos Engineering) included an extensive library
    of prebuilt experiments that simplified the work of automating and orchestrating
    numerous chaos experiments. In addition, comprehensive analysis and reporting
    capabilities gave the company quick insights into the resiliency of its systems.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 公司选择现代混沌工程工具对于克服其混沌工程实践扩展的障碍至关重要。它选择的工具（在本例中为Harness混沌工程）包含了一个庞大的预构建实验库，简化了自动化和编排多个混沌实验的工作。此外，全面的分析和报告能力使公司能够快速了解其系统的弹性。
- en: The company began by focusing on a single critical service that handled nine
    million daily payment requests. It pinpointed fault-tolerant targets within the
    intricate infrastructure, laying the groundwork for a controlled rollout of resilience
    testing. By prioritizing the automation of chaos experiments within delivery pipelines
    and production environments, it addressed the root causes of transaction failures
    and built a foundation for continuous resilience.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 公司开始时专注于一个关键服务，该服务每天处理九百万笔支付请求。它确定了复杂基础设施中的容错目标，为弹性测试的受控推出奠定了基础。通过优先考虑在交付管道和生产环境中自动化混沌实验，它解决了交易失败的根本原因，并为持续弹性建立了基础。
- en: 'Through its automated resilience testing platform, the company was able to
    expand the breadth of its testing to uncover vulnerabilities in service recovery,
    optimize application design patterns, and fine-tune configurations. The results
    were significant: a 16× reduction in failed transactions, MTTR reduced to 10 minutes,
    and a 10× improvement in customer satisfaction. Without modern tooling that offers
    security, templates and automation, and orchestration, it would have been impossible
    to roll out chaos engineering across the organization and achieve these results
    in the short time it took.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 通过其自动化的弹性测试平台，公司能够扩大其测试范围，以发现服务恢复中的漏洞，优化应用程序设计模式，并微调配置。结果是显著的：失败交易减少了16倍，MTTR减少到10分钟，客户满意度提高了10倍。如果没有提供安全、模板、自动化和编排的现代工具，就不可能在短时间内将混沌工程推广到整个组织并取得这些成果。
- en: Adding Chaos Engineering Experiments and SLOs to Your CI/CD Pipeline
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将混沌工程实验和SLO添加到您的CI/CD管道中
- en: To solidify your resilience strategy, integrate SLOs as reliability guardrails
    within your CI/CD pipeline. Think of SLOs as the brakes on a race car—essential
    for maintaining control while pushing for maximum speed. Development teams, much
    like race car drivers, strive for rapid deployments, but without robust SLOs in
    place, they risk crashing their systems with unchecked changes. By monitoring
    key metrics, you can automatically block deployments that breach these thresholds
    or exhaust their error budget. This approach can accelerate your development velocity
    without sacrificing stability.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为了巩固你的弹性策略，将SLOs（服务水平目标）作为可靠性护栏整合到你的CI/CD（持续集成/持续部署）管道中。将SLOs比作赛车上的刹车——在追求最大速度的同时，保持控制至关重要。开发团队，就像赛车手一样，追求快速部署，但如果没有稳健的SLOs，他们可能会因为未经检查的更改而导致系统崩溃。通过监控关键指标，你可以自动阻止超过这些阈值或耗尽错误预算的部署。这种方法可以在不牺牲稳定性的情况下加速你的开发速度。
- en: 'When adding chaos engineering experiments to your CI/CD pipeline, keep in mind
    two measures to guide your progress: resilience scores and resilience coverage.
    Resilience scores are simply how well your services perform against the experiments
    you apply in QA and production. Resilience coverage, similar to code coverage,
    assesses how many more experiments are needed to comprehensively evaluate system
    resilience, guiding the creation of a practical number of tests. Together, these
    metrics provide a holistic view of resilience, enabling all teams to contribute
    to and measure progress toward continuous resilience goals.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 当将混沌工程实验添加到你的CI/CD管道时，请记住两个指导你进步的指标：弹性评分和弹性覆盖率。弹性评分简单地衡量你的服务在QA和生产中应用实验时的表现。弹性覆盖率，类似于代码覆盖率，评估还需要多少实验才能全面评估系统弹性，指导创建实际数量的测试。这两个指标共同提供了一个全面的弹性视图，使所有团队都能为持续弹性目标做出贡献并衡量进展。
- en: Start by adding experiments that test against known resilience conditions, ensuring
    your resilience score remains stable with each new deployment. Slowly increase
    your resilience coverage by adding experiments to test new conditions. If increasing
    the resilience coverage means that the resilience scores are reduced, determine
    if the failed chaos experiment warrants stopping the pipeline or if action can
    be taken in parallel.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，添加测试已知弹性条件的实验，确保每次新部署后你的弹性评分保持稳定。通过添加测试新条件的实验，逐步增加你的弹性覆盖率。如果增加弹性覆盖率意味着弹性评分降低，确定失败的混沌实验是否需要停止管道，或者是否可以并行采取行动。
- en: Next, add experiments that address changes to the platform on which the target
    deployments run. For example, when upgrading underlying platforms like Kubernetes,
    incorporate chaos experiments into your CI/CD pipeline to proactively identify
    potential weaknesses and compatibility issues. This helps prevent latent issues
    from impacting applications in the future and ensures a smooth transition during
    platform updates. By catching these issues early, you can avoid costly incidents
    and maintain continuous resilience.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，添加针对目标部署平台变更的实验。例如，当升级底层平台如Kubernetes时，将混沌实验纳入你的CI/CD管道，以主动识别潜在弱点和兼容性问题。这有助于防止潜在问题在未来影响应用程序，并确保平台更新期间的平稳过渡。通过早期发现这些问题，你可以避免昂贵的意外事件并保持持续的弹性。
- en: Add experiments to the pipeline that validate the deployments against previous
    production incidents and alerts as incidents occur. Lastly, add experiments that
    validate the deployments against configuration changes to the target infrastructure.
    This is another scenario where the resilience tests that passed earlier will start
    failing because the target environment changed through a higher or lower configuration.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 向管道中添加实验，以验证部署对先前生产事件和警报的响应。最后，添加实验以验证部署对目标基础设施配置更改的响应。这也是一个场景，其中之前通过弹性测试的情况将开始失败，因为目标环境通过更高的或更低的配置发生了变化。
- en: 'As you invest in creating and fine-tuning your experiments, treat them like
    any other piece of software: version them, test them, and manage their lifecycle
    in a repository. This ensures your chaos engineering practice remains effective
    and adapts to changes in your systems. Centralized repositories facilitate collaboration
    and the sharing of these experiments, promoting consistency and best practices
    across teams.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 当你投资于创建和微调你的实验时，就像对待任何其他软件一样对待它们：对它们进行版本控制，测试它们，并在存储库中管理它们的生命周期。这确保了你的混沌工程实践保持有效，并适应你系统中的变化。集中式存储库促进了协作和这些实验的共享，促进了团队间的一致性和最佳实践。
- en: Security and Governance for Chaos Engineering
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混沌工程的安全和治理
- en: Clearly, chaos engineering is a powerful approach, but careless experimentation
    has the potential to cause serious harm to both system resilience and trust in
    your chaos engineering program. By integrating it with your security and governance
    frameworks, you can define the guardrails you need to ensure experiments are conducted
    responsibly.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，混沌工程是一种强大的方法，但粗心实验可能会对系统的弹性和你对混沌工程计划的信任造成严重伤害。通过将其与你的安全和治理框架集成，你可以定义所需的护栏，以确保实验负责任地进行。
- en: Just like tech debt, resilience debt can accumulate in your production services.
    Every alert, incident, hotfix, or workaround—like simply throwing more resources
    at a problem—contributes to this debt. Instead of addressing the root cause, these
    quick fixes often mask underlying issues and create a false sense of stability.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 就像技术债务一样，弹性债务也可能积累在你的生产服务中。每一次警报、事件、热修复或临时解决方案——比如简单地向问题投入更多资源——都会增加这种债务。这些快速修复措施通常掩盖了潜在问题，并创造了一种虚假的稳定感。
- en: Modern chaos engineering tools can help you establish and enforce policies to
    combat this. For example, we could set a policy that mandates a corresponding
    chaos experiment for every production incident related to component misbehavior,
    network issues, API failures, or unexpected load. This experiment, integrated
    into your CI/CD pipeline, would need to be validated within a specific timeframe,
    say, within 60 days of the incident. Such a policy would not only enforce a discipline
    of addressing resilience debt but also encourage developers and QA teams to prioritize
    fixing production code over pushing new features that might further exacerbate
    the problem.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现代混沌工程工具可以帮助你建立和执行政策来对抗这种问题。例如，我们可以设定一项政策，要求对与组件行为不当、网络问题、API故障或意外负载相关的每个生产事件进行相应的混沌实验。这个实验，集成到你的CI/CD管道中，需要在特定时间内得到验证，比如说，在事件发生后的60天内。这样的政策不仅会强制执行解决弹性债务的纪律，还会鼓励开发者和QA团队优先修复生产代码，而不是推动可能进一步加剧问题的新的功能。
- en: In addition to using policies to manage resilience debt, you can use security
    governance policies to prevent unauthorized experiments, restrict access to critical
    systems, and limit testing by environment, time window, personnel, or even fault
    type. By automating oversight and integrating these policies into your CI/CD pipelines,
    you can increase resilience coverage while reducing risk.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用政策来管理弹性债务外，你还可以使用安全治理政策来防止未经授权的实验，限制对关键系统的访问，并限制按环境、时间窗口、人员或甚至故障类型进行测试。通过自动化监督并将这些政策集成到你的CI/CD管道中，你可以增加弹性覆盖率同时降低风险。
- en: The Future of AI-Native Chaos Engineering in Service Reliability
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务可靠性中AI原生混沌工程的未来
- en: The future of chaos engineering promises even greater sophistication and integration
    within service reliability practices. Tools such as Harness Chaos Engineering
    and Chaos Monkey will not only automate experiments but also leverage AI/ML to
    predict their impact, analyze system behavior under stress, and recommend optimal
    mitigation strategies. This intelligent automation will minimize risk, allowing
    teams to conduct more complex experiments with greater confidence and efficiency.
    Advancements in observability and tracing will provide deeper insights into system
    dynamics, enabling more precise identification of vulnerabilities and faster recovery
    from disruptions.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 混沌工程的未来在服务可靠性实践中承诺了更高的复杂性和集成度。像Harness混沌工程和Chaos Monkey这样的工具不仅会自动化实验，还会利用AI/ML来预测它们的影响，分析系统在压力下的行为，并推荐最佳的缓解策略。这种智能自动化将最小化风险，使团队能够更有信心和效率地开展更复杂的实验。可观察性和跟踪技术的进步将提供对系统动态的更深入洞察，使更精确地识别漏洞和更快地从中断中恢复成为可能。
- en: As systems grow increasingly complex, with distributed architectures and microservices
    becoming ever-present, chaos engineering will play a crucial role in ensuring
    their resilience. Even large language model–based multiagentic systems can be
    [enhanced using chaos engineering](https://oreil.ly/g7tjd). By combining chaos
    testing with AI-powered analysis and automated remediation (for example, [ChaosEater](https://oreil.ly/IKlJW)),
    we will be able to address potential failures faster and with greater precision,
    minimizing downtime and maintaining high levels of service reliability.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 随着系统变得越来越复杂，分布式架构和微服务变得无处不在，混沌工程将在确保其弹性方面发挥关键作用。即使是基于大型语言模型的多智能体系统也可以通过混沌工程[得到增强](https://oreil.ly/g7tjd)。通过将混沌测试与人工智能分析（例如，[ChaosEater](https://oreil.ly/IKlJW)）和自动修复相结合，我们将能够更快、更精确地解决潜在故障，最小化停机时间并保持高水平的服务可靠性。
- en: Summary
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored chaos engineering as a methodical approach to building
    and validating system resilience. We learned to design and execute experiments
    responsibly, using SLOs and error budgets to balance innovation and stability.
    By integrating chaos engineering into CI/CD pipelines and leveraging modern tooling,
    organizations can proactively identify weaknesses, learn from failures, and continuously
    improve resilience. Ultimately, chaos engineering empowers us to create more robust
    systems that meet the demands of today’s complex digital world. With these principles
    in place, the next step is to apply them seamlessly as part of your deployment
    process. Let’s explore how to ensure stability during production rollouts.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了混沌工程作为一种构建和验证系统弹性的系统方法。我们学习了如何负责任地设计和执行实验，利用服务等级目标（SLOs）和错误预算来平衡创新与稳定性。通过将混沌工程集成到持续集成/持续部署（CI/CD）管道中并利用现代工具，组织可以主动识别弱点，从失败中学习，并持续提高弹性。最终，混沌工程使我们能够创建更健壮的系统，以满足当今复杂数字世界的要求。有了这些原则，下一步就是将这些原则无缝地应用于您的部署流程中。让我们探讨如何在生产部署过程中确保稳定性。
