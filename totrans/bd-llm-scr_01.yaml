- en: 2 Working with text data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 处理文本数据
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Preparing text for large language model training
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为大型语言模型训练准备文本
- en: Splitting text into word and subword tokens
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文本分割成单词和子词标记
- en: Byte pair encoding as a more advanced way of tokenizing text
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字节对编码作为更高级的文本标记方法
- en: Sampling training examples with a sliding window approach
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用滑动窗口方法采样训练示例
- en: Converting tokens into vectors that feed into a large language model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将标记转换为输入大型语言模型的向量
- en: So far, we’ve covered the general structure of large language models (LLMs)
    and learned that they are pretrained on vast amounts of text. Specifically, our
    focus was on decoder-only LLMs based on the transformer architecture, which underlies
    the models used in ChatGPT and other popular GPT-like LLMs.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了大型语言模型（LLMs）的一般结构，并了解到它们是在大量文本上预训练的。具体来说，我们的重点是基于transformer架构的仅解码器LLMs，这是ChatGPT和其他流行GPT-like
    LLMs所使用的模型的基础。
- en: During the pretraining stage, LLMs process text, one word at a time. Training
    LLMs with millions to billions of parameters using a next-word prediction task
    yields models with impressive capabilities. These models can then be further finetuned
    to follow general instructions or perform specific target tasks. But before we
    can implement and train LLMs, we need to prepare the training dataset, as illustrated
    in figure 2.1.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练阶段，LLMs逐个单词处理文本。使用数百万到数十亿参数的下一个单词预测任务训练LLMs，可以得到具有令人印象深刻能力的模型。然后，这些模型可以进一步微调以遵循一般指令或执行特定目标任务。但在我们能够实现和训练LLMs之前，我们需要准备训练数据集，如图2.1所示。
- en: '![figure](../Images/2-1.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-1.png)'
- en: 'Figure 2.1 The three main stages of coding an LLM. This chapter focuses on
    step 1 of stage 1: implementing the data sample pipeline.'
  id: totrans-10
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.1 编码LLM的三个主要阶段。本章重点介绍第一阶段的第一步：实现数据采样管道。
- en: You’ll learn how to prepare input text for training LLMs. This involves splitting
    text into individual word and subword tokens, which can then be encoded into vector
    representations for the LLM. You’ll also learn about advanced tokenization schemes
    like byte pair encoding, which is utilized in popular LLMs like GPT. Lastly, we’ll
    implement a sampling and data-loading strategy to produce the input-output pairs
    necessary for training LLMs.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你将学习如何为训练LLMs准备输入文本。这涉及到将文本分割成单个单词和子词标记，然后可以将这些标记编码成LLM的向量表示。你还将了解诸如字节对编码等高级标记方案，这些方案在GPT等流行LLMs中得到应用。最后，我们将实现一个采样和数据加载策略，以生成训练LLMs所需的输入-输出对。
- en: 2.1 Understanding word embeddings
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 理解词嵌入
- en: Deep neural network models, including LLMs, cannot process raw text directly.
    Since text is categorical, it isn’t compatible with the mathematical operations
    used to implement and train neural networks. Therefore, we need a way to represent
    words as continuous-valued vectors.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络模型，包括LLMs，不能直接处理原始文本。由于文本是分类的，它不兼容用于实现和训练神经网络所使用的数学运算。因此，我们需要一种方法将单词表示为连续值的向量。
- en: Note  Readers unfamiliar with vectors and tensors in a computational context
    can learn more in appendix A, section A.2.2.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：不熟悉计算环境中向量和张量的读者可以在附录A的第A.2.2节中了解更多信息。
- en: The concept of converting data into a vector format is often referred to as
    *embedding*. Using a specific neural network layer or another pretrained neural
    network model, we can embed different data types—for example, video, audio, and
    text, as illustrated in figure 2.2\. However, it’s important to note that different
    data formats require distinct embedding models. For example, an embedding model
    designed for text would not be suitable for embedding audio or video data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据转换为向量格式的概念通常被称为*嵌入*。使用特定的神经网络层或另一个预训练的神经网络模型，我们可以嵌入不同类型的数据——例如，视频、音频和文本，如图2.2所示。然而，需要注意的是，不同的数据格式需要不同的嵌入模型。例如，为文本设计的嵌入模型不适合嵌入音频或视频数据。
- en: '![figure](../Images/2-2.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-2.png)'
- en: Figure 2.2 Deep learning models cannot process data formats like video, audio,
    and text in their raw form. Thus, we use an embedding model to transform this
    raw data into a dense vector representation that deep learning architectures can
    easily understand and process. Specifically, this figure illustrates the process
    of converting raw data into a three-dimensional numerical vector.
  id: totrans-17
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.2 深度学习模型不能以原始形式处理视频、音频和文本等数据格式。因此，我们使用嵌入模型将原始数据转换为深度学习架构可以轻松理解和处理的高密度向量表示。具体来说，此图说明了将原始数据转换为三维数值向量的过程。
- en: At its core, an embedding is a mapping from discrete objects, such as words,
    images, or even entire documents, to points in a continuous vector space—the primary
    purpose of embeddings is to convert nonnumeric data into a format that neural
    networks can process.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本质上，嵌入是将离散对象（如单词、图像甚至整个文档）映射到连续向量空间中的点的映射——嵌入的主要目的是将非数值数据转换为神经网络可以处理的形式。
- en: While word embeddings are the most common form of text embedding, there are
    also embeddings for sentences, paragraphs, or whole documents. Sentence or paragraph
    embeddings are popular choices for *retrieval-augmented generation.* Retrieval-augmented
    generation combines generation (like producing text) with retrieval (like searching
    an external knowledge base) to pull relevant information when generating text,
    which is a technique that is beyond the scope of this book. Since our goal is
    to train GPT-like LLMs, which learn to generate text one word at a time, we will
    focus on word embeddings.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然词嵌入是文本嵌入最常见的形式，但也有句子、段落或整个文档的嵌入。句子或段落嵌入是*检索增强生成*的流行选择。检索增强生成结合了生成（如产生文本）和检索（如搜索外部知识库）来在生成文本时提取相关信息，这是一种超出本书范围的技术。由于我们的目标是训练类似于GPT的LLMs，这些LLMs一次学习生成一个单词，因此我们将专注于词嵌入。
- en: Several algorithms and frameworks have been developed to generate word embeddings.
    One of the earlier and most popular examples is the *Word2Vec* approach. Word2Vec
    trained neural network architecture to generate word embeddings by predicting
    the context of a word given the target word or vice versa. The main idea behind
    Word2Vec is that words that appear in similar contexts tend to have similar meanings.
    Consequently, when projected into two-dimensional word embeddings for visualization
    purposes, similar terms are clustered together, as shown in figure 2.3.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 已经开发出多种算法和框架来生成词嵌入。其中一个较早且最受欢迎的例子是*Word2Vec*方法。Word2Vec通过预测目标词或反之的上下文来训练神经网络架构以生成词嵌入。Word2Vec背后的主要思想是，在相似上下文中出现的单词往往具有相似的含义。因此，当为了可视化目的投影到二维词嵌入时，相似术语通常会聚集在一起，如图2.3所示。
- en: '![figure](../Images/2-3.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-3.png)'
- en: Figure 2.3 If word embeddings are two-dimensional, we can plot them in a two-dimensional
    scatterplot for visualization purposes as shown here. When using word embedding
    techniques, such as Word2Vec, words corresponding to similar concepts often appear
    close to each other in the embedding space. For instance, different types of birds
    appear closer to each other in the embedding space than in countries and cities.
  id: totrans-22
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.3 如果词嵌入是二维的，我们可以将其绘制在二维散点图中进行可视化，如图所示。当使用Word2Vec等词嵌入技术时，对应于相似概念的单词在嵌入空间中通常彼此靠近。例如，不同类型的鸟在嵌入空间中的距离比在国家和城市中的距离更近。
- en: Word embeddings can have varying dimensions, from one to thousands. A higher
    dimensionality might capture more nuanced relationships but at the cost of computational
    efficiency.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入可以有不同维度，从一到数千。更高的维度可能能够捕捉更细微的关系，但会以计算效率为代价。
- en: While we can use pretrained models such as Word2Vec to generate embeddings for
    machine learning models, LLMs commonly produce their own embeddings that are part
    of the input layer and are updated during training. The advantage of optimizing
    the embeddings as part of the LLM training instead of using Word2Vec is that the
    embeddings are optimized to the specific task and data at hand. We will implement
    such embedding layers later in this chapter. (LLMs can also create contextualized
    output embeddings, as we discuss in chapter 3.)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用预训练模型如Word2Vec为机器学习模型生成嵌入时，LLMs通常会产生自己的嵌入，这些嵌入是输入层的一部分，并在训练过程中更新。将嵌入作为LLM训练的一部分进行优化，而不是使用Word2Vec的优势在于，嵌入被优化以适应特定任务和现有数据。我们将在本章后面实现这样的嵌入层。（LLMs还可以创建上下文化的输出嵌入，如我们在第3章中讨论的。）
- en: Unfortunately, high-dimensional embeddings present a challenge for visualization
    because our sensory perception and common graphical representations are inherently
    limited to three dimensions or fewer, which is why figure 2.3 shows two-dimensional
    embeddings in a two-dimensional scatterplot. However, when working with LLMs,
    we typically use embeddings with a much higher dimensionality. For both GPT-2
    and GPT-3, the embedding size (often referred to as the dimensionality of the
    model’s hidden states) varies based on the specific model variant and size. It
    is a tradeoff between performance and efficiency. The smallest GPT-2 models (117M
    and 125M parameters) use an embedding size of 768 dimensions to provide concrete
    examples. The largest GPT-3 model (175B parameters) uses an embedding size of
    12,288 dimensions.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，高维嵌入对可视化构成了挑战，因为我们的感官感知和常见的图形表示本质上限于三维或更少，这就是为什么图2.3展示了二维嵌入在二维散点图中的原因。然而，当与LLM（大型语言模型）一起工作时，我们通常使用更高维度的嵌入。对于GPT-2和GPT-3，嵌入大小（通常被称为模型隐藏状态的维度）根据具体的模型变体和大小而变化。这是性能和效率之间的权衡。最小的GPT-2模型（117M和125M参数）使用768维的嵌入大小来提供具体示例。最大的GPT-3模型（175B参数）使用12,288维的嵌入大小。
- en: Next, we will walk through the required steps for preparing the embeddings used
    by an LLM, which include splitting text into words, converting words into tokens,
    and turning tokens into embedding vectors.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍为LLM准备嵌入所需的步骤，包括将文本分割成单词、将单词转换为标记以及将标记转换为嵌入向量。
- en: 2.2 Tokenizing text
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 文本分词
- en: Let’s discuss how we split input text into individual tokens, a required preprocessing
    step for creating embeddings for an LLM. These tokens are either individual words
    or special characters, including punctuation characters, as shown in figure 2.4.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论如何将输入文本分割成单个标记，这是为LLM创建嵌入所需的预处理步骤。这些标记可以是单个单词或特殊字符，包括标点符号，如图2.4所示。
- en: '![figure](../Images/2-4.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-4.png)'
- en: Figure 2.4 A view of the text processing steps in the context of an LLM. Here,
    we split an input text into individual tokens, which are either words or special
    characters, such as punctuation characters.
  id: totrans-30
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.4 在LLM的上下文中查看文本处理步骤。在这里，我们将输入文本分割成单个标记，这些标记可以是单词或特殊字符，例如标点符号。
- en: The text we will tokenize for LLM training is “The Verdict,” a short story by
    Edith Wharton, which has been released into the public domain and is thus permitted
    to be used for LLM training tasks. The text is available on Wikisource at [https://en.wikisource.org/wiki/The_Verdict](https://en.wikisource.org/wiki/The_Verdict),
    and you can copy and paste it into a text file, which I copied into a text file
    `"the-verdict.txt".`
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为LLM训练进行分词的文本是“Verdict”，这是伊迪丝·华顿的短篇小说，已经进入公有领域，因此可以用于LLM训练任务。该文本可在维基源[https://en.wikisource.org/wiki/The_Verdict](https://en.wikisource.org/wiki/The_Verdict)上找到，您可以将它复制并粘贴到文本文件中，我将它复制到了名为“the-verdict.txt”的文本文件中。
- en: 'Alternatively, you can find this `"the-verdict.txt"` file in this book’s GitHub
    repository at [https://mng.bz/Adng](https://mng.bz/Adng). You can download the
    file with the following Python code:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以在本书的GitHub仓库[https://mng.bz/Adng](https://mng.bz/Adng)中找到这个“the-verdict.txt”文件。您可以使用以下Python代码下载该文件：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next, we can load the `the-verdict.txt` file using Python’s standard file reading
    utilities.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用Python的标准文件读取实用工具加载`the-verdict.txt`文件。
- en: Listing 2.1 Reading in a short story as text sample into Python
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.1 将短篇小说作为文本样本读入Python
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The print command prints the total number of characters followed by the first
    99 characters of this file for illustration purposes:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 打印命令打印了该文件的总字符数，以及为了说明目的的前99个字符：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Our goal is to tokenize this 20,479-character short story into individual words
    and special characters that we can then turn into embeddings for LLM training.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是将这个20,479个字符的短篇小说分解成单个单词和特殊字符，然后我们可以将其转换为LLM训练的嵌入。
- en: Note  It’s common to process millions of articles and hundreds of thousands
    of books—many gigabytes of text—when working with LLMs. However, for educational
    purposes, it’s sufficient to work with smaller text samples like a single book
    to illustrate the main ideas behind the text processing steps and to make it possible
    to run it in a reasonable time on consumer hardware.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：当与LLM一起工作时，通常需要处理数百万篇文章和数十万本书——许多GB的文本。然而，出于教育目的，使用较小的文本样本（如一本书）就足够了，以说明文本处理步骤背后的主要思想，并使其能够在消费级硬件上合理时间内运行。
- en: How can we best split this text to obtain a list of tokens? For this, we go
    on a small excursion and use Python’s regular expression library `re` for illustration
    purposes. (You don’t have to learn or memorize any regular expression syntax since
    we will later transition to a prebuilt tokenizer.)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何最好地分割这段文本以获得一个标记列表？为此，我们进行了一次小旅行，并使用Python的正则表达式库`re`进行说明。（您不必学习或记住任何正则表达式语法，因为我们稍后将过渡到预构建的分词器。）
- en: 'Using some simple example text, we can use the `re.split` command with the
    following syntax to split a text on whitespace characters:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一些简单的示例文本，我们可以使用以下语法的`re.split`命令来在空白字符上分割文本：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The result is a list of individual words, whitespaces, and punctuation characters:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个包含单个单词、空白字符和标点符号的列表：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This simple tokenization scheme mostly works for separating the example text
    into individual words; however, some words are still connected to punctuation
    characters that we want to have as separate list entries. We also refrain from
    making all text lowercase because capitalization helps LLMs distinguish between
    proper nouns and common nouns, understand sentence structure, and learn to generate
    text with proper capitalization.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的分词方案主要适用于将示例文本分割成单个单词；然而，一些单词仍然与我们要作为单独列表条目保留的标点符号相连。我们也不将所有文本转换为小写，因为大小写有助于大型语言模型区分专有名词和普通名词，理解句子结构，并学会生成正确大小写的文本。
- en: 'Let’s modify the regular expression splits on whitespaces (`\s`), commas, and
    periods (`[,.]`):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们修改空白字符（`\s`）、逗号和句号（`[,.]`）上的正则表达式分割：
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can see that the words and punctuation characters are now separate list
    entries just as we wanted:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，单词和标点符号现在作为我们想要的单独列表条目分开：
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'A small remaining problem is that the list still includes whitespace characters.
    Optionally, we can remove these redundant characters safely as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一个小问题仍然存在，列表中仍然包括空白字符。可选地，我们可以安全地移除这些冗余字符，如下所示：
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The resulting whitespace-free output looks like as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的无空白输出如下所示：
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note  When developing a simple tokenizer, whether we should encode whitespaces
    as separate characters or just remove them depends on our application and its
    requirements. Removing whitespaces reduces the memory and computing requirements.
    However, keeping whitespaces can be useful if we train models that are sensitive
    to the exact structure of the text (for example, Python code, which is sensitive
    to indentation and spacing). Here, we remove whitespaces for simplicity and brevity
    of the tokenized outputs. Later, we will switch to a tokenization scheme that
    includes whitespaces.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：当开发一个简单的分词器时，我们是否应该将空白字符编码为单独的字符或者只是简单地移除它们，这取决于我们的应用及其需求。移除空白字符可以减少内存和计算需求。然而，如果我们训练对文本精确结构敏感的模型（例如，对缩进和间距敏感的Python代码），保留空白字符可能是有用的。在这里，我们为了简单和分词输出的简洁性移除了空白字符。稍后，我们将切换到一个包括空白字符的分词方案。
- en: 'The tokenization scheme we devised here works well on the simple sample text.
    Let’s modify it a bit further so that it can also handle other types of punctuation,
    such as question marks, quotation marks, and the double-dashes we have seen earlier
    in the first 100 characters of Edith Wharton’s short story, along with additional
    special characters:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里设计的分词方案在简单的样本文本上运行良好。让我们进一步修改它，使其也能处理其他类型的标点符号，例如问号、引号以及我们在爱迪丝·华顿短篇小说前100个字符中看到的破折号，以及额外的特殊字符：
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The resulting output is:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 结果输出如下：
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As we can see based on the results summarized in figure 2.5, our tokenization
    scheme can now handle the various special characters in the text successfully.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 根据图2.5中总结的结果，我们可以看到，我们的分词方案现在可以成功处理文本中的各种特殊字符。
- en: '![figure](../Images/2-5.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-5.png)'
- en: Figure 2.5 The tokenization scheme we implemented so far splits text into individual
    words and punctuation characters. In this specific example, the sample text gets
    split into 10 individual tokens.
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.5 我们迄今为止实现的分词方案将文本分割成单个单词和标点符号。在这个特定示例中，样本文本被分割成10个单独的标记。
- en: 'Now that we have a basic tokenizer working, let’s apply it to Edith Wharton’s
    entire short story:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个基本的分词器正在工作，让我们将其应用于爱迪丝·华顿的整个短篇小说：
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This print statement outputs `4690`, which is the number of tokens in this
    text (without whitespaces). Let’s print the first 30 tokens for a quick visual
    check:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这个打印语句输出`4690`，这是文本中的标记数量（不含空白字符）。让我们打印前30个标记以进行快速视觉检查：
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The resulting output shows that our tokenizer appears to be handling the text
    well since all words and special characters are neatly separated:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 结果输出显示，我们的标记化器似乎很好地处理了文本，因为所有单词和特殊字符都被整洁地分隔开：
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 2.3 Converting tokens into token IDs
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 将标记转换为标记ID
- en: Next, let’s convert these tokens from a Python string to an integer representation
    to produce the token IDs. This conversion is an intermediate step before converting
    the token IDs into embedding vectors.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们将这些标记从Python字符串转换为整数表示，以生成标记ID。这种转换是在将标记ID转换为嵌入向量之前的中间步骤。
- en: To map the previously generated tokens into token IDs, we have to build a vocabulary
    first. This vocabulary defines how we map each unique word and special character
    to a unique integer, as shown in figure 2.6.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将之前生成的标记映射到标记ID，我们首先必须构建一个词汇表。这个词汇表定义了如何将每个唯一的单词和特殊字符映射到一个唯一的整数，如图2.6所示。
- en: '![figure](../Images/2-6.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-6.png)'
- en: Figure 2.6 We build a vocabulary by tokenizing the entire text in a training
    dataset into individual tokens. These individual tokens are then sorted alphabetically,
    and duplicate tokens are removed. The unique tokens are then aggregated into a
    vocabulary that defines a mapping from each unique token to a unique integer value.
    The depicted vocabulary is purposely small and contains no punctuation or special
    characters for simplicity.
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.6 我们通过将训练数据集中的整个文本标记化成单个标记来构建词汇表。然后，这些单个标记按字母顺序排序，并删除重复的标记。然后，唯一的标记被汇总到一个词汇表中，该词汇表定义了从每个唯一标记到唯一整数值的映射。为了简化，图中展示的词汇表故意很小，且不包含标点符号或特殊字符。
- en: 'Now that we have tokenized Edith Wharton’s short story and assigned it to a
    Python variable called `preprocessed`, let’s create a list of all unique tokens
    and sort them alphabetically to determine the vocabulary size:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将伊迪丝·华顿的短篇小说进行了标记化，并将其分配给一个名为`preprocessed`的Python变量，让我们创建一个包含所有唯一标记的列表，并按字母顺序排序，以确定词汇表的大小：
- en: '[PRE14]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: After determining that the vocabulary size is 1,130 via this code, we create
    the vocabulary and print its first 51 entries for illustration purposes.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这段代码确定词汇表大小为1,130后，我们创建了词汇表，并打印出其前51个条目以供说明。
- en: Listing 2.2 Creating a vocabulary
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.2 创建词汇表
- en: '[PRE15]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The output is
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下
- en: '[PRE16]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As we can see, the dictionary contains individual tokens associated with unique
    integer labels. Our next goal is to apply this vocabulary to convert new text
    into token IDs (figure 2.7).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，这个字典包含与唯一整数标签关联的单个标记。我们的下一个目标是应用这个词汇表将新文本转换为标记ID（图2.7）。
- en: '![figure](../Images/2-7.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-7.png)'
- en: Figure 2.7 Starting with a new text sample, we tokenize the text and use the
    vocabulary to convert the text tokens into token IDs. The vocabulary is built
    from the entire training set and can be applied to the training set itself and
    any new text samples. The depicted vocabulary contains no punctuation or special
    characters for simplicity.
  id: totrans-83
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.7 从一个新的文本样本开始，我们对文本进行标记化，并使用词汇表将文本标记转换为标记ID。这个词汇表是由整个训练集构建的，可以应用于训练集本身和任何新的文本样本。为了简化，图中展示的词汇表不包含标点符号或特殊字符。
- en: When we want to convert the outputs of an LLM from numbers back into text, we
    need a way to turn token IDs into text. For this, we can create an inverse version
    of the vocabulary that maps token IDs back to the corresponding text tokens.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要将一个语言模型（LLM）的输出从数字转换回文本时，我们需要一种方法将标记ID转换成文本。为此，我们可以创建一个词汇表的逆版本，它将标记ID映射回相应的文本标记。
- en: Let’s implement a complete tokenizer class in Python with an `encode` method
    that splits text into tokens and carries out the string-to-integer mapping to
    produce token IDs via the vocabulary. In addition, we’ll implement a `decode`
    method that carries out the reverse integer-to-string mapping to convert the token
    IDs back into text. The following listing shows the code for this tokenizer implementation.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在Python中实现一个完整的标记化器类，它包含一个`encode`方法，该方法将文本分割成标记，并通过词汇表执行字符串到整数的映射，以生成标记ID。此外，我们还将实现一个`decode`方法，它执行反向的整数到字符串映射，将标记ID转换回文本。以下列表展示了这个标记化器实现的代码。
- en: Listing 2.3 Implementing a simple text tokenizer
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.3 实现简单的文本标记化器
- en: '[PRE17]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '#1 Stores the vocabulary as a class attribute for access in the encode and
    decode methods'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将词汇表作为类属性存储，以便在`encode`和`decode`方法中访问'
- en: '#2 Creates an inverse vocabulary that maps token IDs back to the original text
    tokens'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 创建一个逆词汇表，将标记ID映射回原始文本标记'
- en: '#3 Processes input text into token IDs'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 将输入文本处理成标记ID'
- en: '#4 Converts token IDs back into text'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 将标记ID转换回文本'
- en: '#5 Removes spaces before the specified punctuation'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 删除指定标点符号前的空格'
- en: Using the `SimpleTokenizerV1` Python class, we can now instantiate new tokenizer
    objects via an existing vocabulary, which we can then use to encode and decode
    text, as illustrated in figure 2.8.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`SimpleTokenizerV1` Python类，我们现在可以通过现有的词汇表实例化新的令牌化器对象，然后我们可以使用它来编码和解码文本，如图2.8所示。
- en: '![figure](../Images/2-8.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-8.png)'
- en: 'Figure 2.8 Tokenizer implementations share two common methods: an encode method
    and a decode method. The encode method takes in the sample text, splits it into
    individual tokens, and converts the tokens into token IDs via the vocabulary.
    The decode method takes in token IDs, converts them back into text tokens, and
    concatenates the text tokens into natural text.'
  id: totrans-95
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.8 令牌化器实现共享两种常见方法：一种编码方法和一种解码方法。编码方法接收样本文本，将其分割成单个标记，并通过词汇表将这些标记转换为标记ID。解码方法接收标记ID，将它们转换回文本标记，并将文本标记连接成自然文本。
- en: 'Let’s instantiate a new tokenizer object from the `SimpleTokenizerV1` class
    and tokenize a passage from Edith Wharton’s short story to try it out in practice:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从`SimpleTokenizerV1`类中实例化一个新的令牌化器对象，并从伊迪丝·华顿的短篇小说中标记一段文本来实际尝试：
- en: '[PRE18]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The preceding code prints the following token IDs:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码打印了以下标记ID：
- en: '[PRE19]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, let’s see whether we can turn these token IDs back into text using the
    decode method:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看我们是否可以使用解码方法将这些标记ID转换回文本：
- en: '[PRE20]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This outputs:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出：
- en: '[PRE21]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Based on this output, we can see that the decode method successfully converted
    the token IDs back into the original text.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个输出，我们可以看到解码方法成功地将标记ID转换回原始文本。
- en: 'So far, so good. We implemented a tokenizer capable of tokenizing and detokenizing
    text based on a snippet from the training set. Let’s now apply it to a new text
    sample not contained in the training set:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切顺利。我们实现了一个令牌化器，能够根据训练集的片段对文本进行标记化和反标记化。现在，让我们将其应用于训练集中未包含的新文本样本：
- en: '[PRE22]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Executing this code will result in the following error:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码将导致以下错误：
- en: '[PRE23]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The problem is that the word “Hello” was not used in the “The Verdict” short
    story. Hence, it is not contained in the vocabulary. This highlights the need
    to consider large and diverse training sets to extend the vocabulary when working
    on LLMs.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于单词“Hello”没有在“The Verdict”短故事中使用。因此，它不包含在词汇表中。这突出了在处理大型语言模型（LLMs）时考虑大型和多样化的训练集以扩展词汇表的需求。
- en: Next, we will test the tokenizer further on text that contains unknown words
    and discuss additional special tokens that can be used to provide further context
    for an LLM during training.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将进一步测试令牌化器在包含未知单词的文本上的表现，并讨论在训练LLM期间可以使用的其他特殊标记，以提供更多上下文。
- en: 2.4 Adding special context tokens
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 添加特殊上下文标记
- en: We need to modify the tokenizer to handle unknown words. We also need to address
    the usage and addition of special context tokens that can enhance a model’s understanding
    of context or other relevant information in the text. These special tokens can
    include markers for unknown words and document boundaries, for example. In particular,
    we will modify the vocabulary and tokenizer, `SimpleTokenizerV2`, to support two
    new tokens, `<|unk|>` and `<|endoftext|>`, as illustrated in figure 2.9.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要修改令牌化器以处理未知单词。我们还需要解决特殊上下文标记的使用和添加，这些标记可以增强模型对文本中上下文或其他相关信息的理解。这些特殊标记可以包括未知单词和文档边界的标记，例如。特别是，我们将修改词汇表和令牌化器`SimpleTokenizerV2`，以支持两个新标记`<|unk|>`和`<|endoftext|>`，如图2.9所示。
- en: '![figure](../Images/2-9.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-9.png)'
- en: Figure 2.9 We add special tokens to a vocabulary to deal with certain contexts.
    For instance, we add an `<|unk|>` token to represent new and unknown words that
    were not part of the training data and thus not part of the existing vocabulary.
    Furthermore, we add an `<|endoftext|>` token that we can use to separate two unrelated
    text sources.
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.9 我们向词汇表中添加特殊标记以处理某些上下文。例如，我们添加一个`<|unk|>`标记来表示新词和未知词，这些词不是训练数据的一部分，因此也不属于现有词汇表的一部分。此外，我们添加一个`<|endoftext|>`标记，我们可以用它来分隔两个不相关的文本源。
- en: We can modify the tokenizer to use an `<|unk|>` token if it encounters a word
    that is not part of the vocabulary. Furthermore, we add a token between unrelated
    texts. For example, when training GPT-like LLMs on multiple independent documents
    or books, it is common to insert a token before each document or book that follows
    a previous text source, as illustrated in figure 2.10\. This helps the LLM understand
    that although these text sources are concatenated for training, they are, in fact,
    unrelated.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以修改分词器，使其在遇到不在词汇表中的单词时使用`<|unk|>`标记。此外，我们在无关文本之间添加一个标记。例如，当在多个独立文档或书籍上训练类似GPT的LLM时，通常在跟随前一个文本源之后的每个文档或书籍之前插入一个标记，如图2.10所示。这有助于LLM理解，尽管这些文本源在训练时被拼接在一起，但它们实际上是无关的。
- en: '![figure](../Images/2-10.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/2-10.png)'
- en: Figure 2.10 When working with multiple independent text source, we add `<|endoftext|>`
    tokens between these texts. These `<|endoftext|>` tokens act as markers, signaling
    the start or end of a particular segment, allowing for more effective processing
    and understanding by the LLM.
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.10 当处理多个独立文本源时，我们在这些文本之间添加`<|endoftext|>`标记。这些`<|endoftext|>`标记作为标记，指示特定段落的开始或结束，使LLM能够更有效地处理和理解。
- en: 'Let’s now modify the vocabulary to include these two special tokens, `<unk>`
    and `<|endoftext|>`, by adding them to our list of all unique words:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们修改词汇表以包括这两个特殊标记`<unk>`和`<|endoftext|>`，通过将它们添加到我们所有独特单词的列表中：
- en: '[PRE24]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Based on the output of this print statement, the new vocabulary size is 1,132
    (the previous vocabulary size was 1,130).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个打印语句的输出，新的词汇表大小是1,132（之前的词汇表大小是1,130）。
- en: 'As an additional quick check, let’s print the last five entries of the updated
    vocabulary:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 作为额外的快速检查，让我们打印更新后的词汇表的最后五个条目：
- en: '[PRE25]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The code prints
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 代码打印
- en: '[PRE26]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Based on the code output, we can confirm that the two new special tokens were
    indeed successfully incorporated into the vocabulary. Next, we adjust the tokenizer
    from code listing 2.3 accordingly as shown in the following listing.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 根据代码输出，我们可以确认这两个新的特殊标记确实成功融入了词汇表。接下来，我们根据以下列表相应地调整分词器。
- en: Listing 2.4 A simple text tokenizer that handles unknown words
  id: totrans-126
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.4 一个简单的文本分词器，可以处理未知单词
- en: '[PRE27]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '#1 Replaces unknown words by <|unk|> tokens'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 用<|unk|>标记替换未知单词'
- en: '#2 Replaces spaces before the specified punctuations'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 替换指定标点符号前的空格'
- en: Compared to the `SimpleTokenizerV1` we implemented in listing 2.3, the new `SimpleTokenizerV2`
    replaces unknown words with `<|unk|>` tokens.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在列表2.3中实现的`SimpleTokenizerV1`相比，新的`SimpleTokenizerV2`将未知单词替换为`<|unk|>`标记。
- en: 'Let’s now try this new tokenizer out in practice. For this, we will use a simple
    text sample that we concatenate from two independent and unrelated sentences:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来实际尝试这个新的分词器。为此，我们将使用一个简单的文本样本，它是从两个独立且无关的句子中拼接而成的：
- en: '[PRE28]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The output is
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是
- en: '[PRE29]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, let’s tokenize the sample text using the `SimpleTokenizerV2` on the vocab
    we previously created in listing 2.2:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们使用之前在列表2.2中创建的词汇表，使用`SimpleTokenizerV2`对样本文本进行分词：
- en: '[PRE30]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This prints the following token IDs:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印以下标记ID：
- en: '[PRE31]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We can see that the list of token IDs contains `1130` for the `<|endoftext|>`
    separator token as well as two `1131` tokens, which are used for unknown words.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，标记ID列表中包含`1130`，这是`<|endoftext|>`分隔符标记的标记ID，以及两个`1131`标记，这些标记用于未知单词。
- en: 'Let’s detokenize the text for a quick sanity check:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行去标记化以进行快速检查：
- en: '[PRE32]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The output is
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是
- en: '[PRE33]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Based on comparing this detokenized text with the original input text, we know
    that the training dataset, Edith Wharton’s short story “The Verdict,” does not
    contain the words “Hello” and “palace.”
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较这个去标记化的文本与原始输入文本，我们知道训练数据集，伊迪丝·华顿的短篇小说《判决》，不包含单词“Hello”和“palace”。
- en: 'Depending on the LLM, some researchers also consider additional special tokens
    such as the following:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 根据LLM的不同，一些研究人员还考虑了以下额外的特殊标记：
- en: '`[BOS]` *(beginning of sequence) *—This token marks the start of a text. It
    signifies to the LLM where a piece of content begins.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[BOS]` *(序列开始)* —这个标记标记了文本的开始。它向LLM（大型语言模型）指示内容从哪里开始。'
- en: '`[EOS]` *(end of sequence) *—This token is positioned at the end of a text
    and is especially useful when concatenating multiple unrelated texts, similar
    to `<|endoftext|>`. For instance, when combining two different Wikipedia articles
    or books, the `[EOS]` token indicates where one ends and the next begins.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[EOS]` *(序列结束)* —此标记位于文本的末尾，当连接多个不相关的文本时特别有用，类似于 `<|endoftext|>`。例如，当合并两个不同的维基百科文章或书籍时，`[EOS]`
    标记指示一个结束，下一个开始。'
- en: '`[PAD]` *(padding) *—When training LLMs with batch sizes larger than one, the
    batch might contain texts of varying lengths. To ensure all texts have the same
    length, the shorter texts are extended or “padded” using the `[PAD]` token, up
    to the length of the longest text in the batch.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[PAD]` *(填充)* —当使用大于一个的批量大小训练大型语言模型 (LLM) 时，批量中可能包含不同长度的文本。为了确保所有文本具有相同的长度，较短的文本将使用
    `[PAD]` 标记进行扩展或“填充”，直到与批量中最长文本的长度相同。'
- en: The tokenizer used for GPT models does not need any of these tokens; it only
    uses an `<|endoftext|>` token for simplicity. `<|endoftext|>` is analogous to
    the `[EOS]` token. `<|endoftext|>` is also used for padding. However, as we’ll
    explore in subsequent chapters, when training on batched inputs, we typically
    use a mask, meaning we don’t attend to padded tokens. Thus, the specific token
    chosen for padding becomes inconsequential.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 用于 GPT 模型的分词器不需要这些标记中的任何一个；它仅为了简洁起见使用一个 `<|endoftext|>` 标记。`<|endoftext|>` 与
    `[EOS]` 标记类似。`<|endoftext|>` 也用于填充。然而，正如我们将在后续章节中探讨的，在批量输入上进行训练时，我们通常使用一个掩码，这意味着我们不关注填充标记。因此，用于填充的具体标记变得无关紧要。
- en: Moreover, the tokenizer used for GPT models also doesn’t use an `<|unk|>` token
    for out-of-vocabulary words. Instead, GPT models use a *byte pair encoding* tokenizer,
    which breaks words down into subword units, which we will discuss next.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，用于 GPT 模型的分词器也不使用 `<|unk|>` 标记来处理词汇表外的单词。相反，GPT 模型使用一种称为 *字节对编码* 的分词器，它将单词分解成子词单元，我们将在下一节中讨论这一点。
- en: 2.5 Byte pair encoding
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 字节对编码
- en: Let’s look at a more sophisticated tokenization scheme based on a concept called
    byte pair encoding (BPE). The BPE tokenizer was used to train LLMs such as GPT-2,
    GPT-3, and the original model used in ChatGPT.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一种基于称为字节对编码 (BPE) 的概念的更复杂的分词方案。BPE 分词器被用于训练 GPT-2、GPT-3 以及 ChatGPT 中使用的原始模型等大型语言模型
    (LLM)。
- en: 'Since implementing BPE can be relatively complicated, we will use an existing
    Python open source library called *tiktoken* ([https://github.com/openai/tiktoken](https://github.com/openai/tiktoken)),
    which implements the BPE algorithm very efficiently based on source code in Rust.
    Similar to other Python libraries, we can install the tiktoken library via Python’s
    `pip` installer from the terminal:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 由于实现 BPE 可能相对复杂，我们将使用一个现有的 Python 开源库，称为 *tiktoken* ([https://github.com/openai/tiktoken](https://github.com/openai/tiktoken))，它基于
    Rust 中的源代码非常高效地实现了 BPE 算法。类似于其他 Python 库，我们可以通过终端中的 Python 的 `pip` 安装程序安装 tiktoken
    库：
- en: '[PRE34]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The code we will use is based on tiktoken 0.7.0\. You can use the following
    code to check the version you currently have installed:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的代码基于 tiktoken 0.7.0。您可以使用以下代码来检查您当前安装的版本：
- en: '[PRE35]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Once installed, we can instantiate the BPE tokenizer from tiktoken as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，我们可以如下实例化从 tiktoken 的 BPE 分词器：
- en: '[PRE36]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The usage of this tokenizer is similar to the `SimpleTokenizerV2` we implemented
    previously via an `encode` method:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 此分词器的使用方式与我们之前通过 `encode` 方法实现的 `SimpleTokenizerV2` 类似：
- en: '[PRE37]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The code prints the following token IDs:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 代码打印以下标记 ID：
- en: '[PRE38]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We can then convert the token IDs back into text using the decode method, similar
    to our `SimpleTokenizerV2`:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用解码方法将标记 ID 转换回文本，类似于我们的 `SimpleTokenizerV2`：
- en: '[PRE39]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The code prints
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 代码打印
- en: '[PRE40]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We can make two noteworthy observations based on the token IDs and decoded text.
    First, the `<|endoftext|>` token is assigned a relatively large token ID, namely,
    `50256`. In fact, the BPE tokenizer, which was used to train models such as GPT-2,
    GPT-3, and the original model used in ChatGPT, has a total vocabulary size of
    50,257, with `<|endoftext|>` being assigned the largest token ID.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据标记 ID 和解码文本做出两个值得注意的观察。首先，`<|endoftext|>` 标记被分配了一个相对较大的标记 ID，即 `50256`。实际上，用于训练
    GPT-2、GPT-3 以及 ChatGPT 中使用的原始模型的 BPE 分词器，其总词汇量为 50,257，其中 `<|endoftext|>` 被分配了最大的标记
    ID。
- en: Second, the BPE tokenizer encodes and decodes unknown words, such as `someunknownPlace`,
    correctly. The BPE tokenizer can handle any unknown word. How does it achieve
    this without using `<|unk|>` tokens?
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，BPE 分词器能够正确地编码和解码未知单词，例如 `someunknownPlace`。BPE 分词器可以处理任何未知单词。它是如何在不使用 `<|unk|>`
    标记的情况下实现这一点的呢？
- en: The algorithm underlying BPE breaks down words that aren’t in its predefined
    vocabulary into smaller subword units or even individual characters, enabling
    it to handle out-of-vocabulary words. So, thanks to the BPE algorithm, if the
    tokenizer encounters an unfamiliar word during tokenization, it can represent
    it as a sequence of subword tokens or characters, as illustrated in figure 2.11.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: BPE背后的算法将不在其预定义词汇表中的单词分解成更小的子词单元或甚至单个字符，使其能够处理词汇表外的单词。因此，多亏了BPE算法，如果分词器在分词过程中遇到不熟悉的单词，它可以将其表示为一系列子词标记或字符，如图2.11所示。
- en: '![figure](../Images/2-11.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-11.png)'
- en: Figure 2.11 BPE tokenizers break down unknown words into subwords and individual
    characters. This way, a BPE tokenizer can parse any word and doesn’t need to replace
    unknown words with special tokens, such as `<|unk|>`.
  id: totrans-171
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.11 BPE分词器将未知单词分解成子词和单个字符。这样，BPE分词器可以解析任何单词，不需要用特殊标记（如`<|unk|>`）替换未知单词。
- en: The ability to break down unknown words into individual characters ensures that
    the tokenizer and, consequently, the LLM that is trained with it can process any
    text, even if it contains words that were not present in its training data.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 将未知单词分解成单个字符的能力确保了分词器以及随之训练的LLM可以处理任何文本，即使其中包含训练数据中未出现的单词。
- en: Exercise 2.1 Byte pair encoding of unknown words
  id: totrans-173
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习2.1 未知单词的字节对编码
- en: Try the BPE tokenizer from the tiktoken library on the unknown words “Akwirw
    ier” and print the individual token IDs. Then, call the `decode` function on each
    of the resulting integers in this list to reproduce the mapping shown in figure
    2.11\. Lastly, call the decode method on the token IDs to check whether it can
    reconstruct the original input, “Akwirw ier.”
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试使用tiktoken库中的BPE分词器对未知单词“Akwirw ier”进行分词，并打印出单个标记ID。然后，对列表中的每个结果整数调用`decode`函数以重现图2.11所示的映射。最后，对标记ID调用decode方法以检查它是否可以重建原始输入，“Akwirw
    ier”。
- en: A detailed discussion and implementation of BPE is out of the scope of this
    book, but in short, it builds its vocabulary by iteratively merging frequent characters
    into subwords and frequent subwords into words. For example, BPE starts with adding
    all individual single characters to its vocabulary (“a,” “b,” etc.). In the next
    stage, it merges character combinations that frequently occur together into subwords.
    For example, “d” and “e” may be merged into the subword “de,” which is common
    in many English words like “define,” “depend,” “made,” and “hidden.” The merges
    are determined by a frequency cutoff.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: BPE（字节对编码）的详细讨论和实现超出了本书的范围，但简而言之，它是通过迭代地将频繁出现的字符合并成子词，再将频繁出现的子词合并成单词来构建其词汇表。例如，BPE首先将所有单个字符添加到其词汇表中（例如，“a”，“b”等）。在下一阶段，它将经常一起出现的字符组合合并成子词。例如，“d”和“e”可能合并成子词“de”，这在许多英语单词中很常见，如“define”，“depend”，“made”和“hidden”。合并是由一个频率阈值决定的。
- en: 2.6 Data sampling with a sliding window
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.6 使用滑动窗口进行数据采样
- en: The next step in creating the embeddings for the LLM is to generate the input–target
    pairs required for training an LLM. What do these input–target pairs look like?
    As we already learned, LLMs are pretrained by predicting the next word in a text,
    as depicted in figure 2.12.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 创建LLM嵌入的下一步是生成用于训练LLM的输入-目标对。这些输入-目标对看起来是什么样子？正如我们之前所学的，LLM是通过预测文本中的下一个单词进行预训练的，如图2.12所示。
- en: '![figure](../Images/2-12.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-12.png)'
- en: Figure 2.12 Given a text sample, extract input blocks as subsamples that serve
    as input to the LLM, and the LLM’s prediction task during training is to predict
    the next word that follows the input block. During training, we mask out all words
    that are past the target. Note that the text shown in this figure must undergo
    tokenization before the LLM can process it; however, this figure omits the tokenization
    step for clarity.
  id: totrans-179
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.12 给定一个文本样本，提取输入块作为LLM的输入子样本，LLM在训练期间的预测任务是预测输入块之后的下一个单词。在训练过程中，我们屏蔽掉所有目标之后的单词。请注意，此图中的文本必须在LLM处理之前进行分词；然而，为了清晰起见，此图省略了分词步骤。
- en: 'Let’s implement a data loader that fetches the input–target pairs in figure
    2.12 from the training dataset using a sliding window approach. To get started,
    we will tokenize the whole “The Verdict” short story using the BPE tokenizer:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现一个数据加载器，使用滑动窗口方法从训练数据集中提取图2.12中的输入-目标对。为了开始，我们将使用BPE分词器对整个“The Verdict”短篇小说进行分词：
- en: '[PRE41]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Executing this code will return `5145`, the total number of tokens in the training
    set, after applying the BPE tokenizer.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码将在应用 BPE 标记化器后返回 `5145`，这是训练集中的总标记数。
- en: 'Next, we remove the first 50 tokens from the dataset for demonstration purposes,
    as it results in a slightly more interesting text passage in the next steps:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，为了演示目的，我们从数据集中移除前 50 个标记，因为它在接下来的步骤中会产生一个更有趣的文本段落：
- en: '[PRE42]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'One of the easiest and most intuitive ways to create the input–target pairs
    for the next-word prediction task is to create two variables, `x` and `y`, where
    `x` contains the input tokens and `y` contains the targets, which are the inputs
    shifted by 1:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 为下一词预测任务创建输入-目标对的最简单且直观的方法之一是创建两个变量 `x` 和 `y`，其中 `x` 包含输入标记，而 `y` 包含目标，即输入向右移动
    1 位：
- en: '[PRE43]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '#1 The context size determines how many tokens are included in the input.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 上下文大小决定了输入中包含多少个标记。'
- en: 'Running the previous code prints the following output:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 运行之前的代码将打印以下输出：
- en: '[PRE44]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'By processing the inputs along with the targets, which are the inputs shifted
    by one position, we can create the next-word prediction tasks (see figure 2.12),
    as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 通过处理输入以及向右移动一位的目标，我们可以创建下一词预测任务（见图 2.12），如下所示：
- en: '[PRE45]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The code prints
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 代码打印
- en: '[PRE46]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Everything left of the arrow (`---->`) refers to the input an LLM would receive,
    and the token ID on the right side of the arrow represents the target token ID
    that the LLM is supposed to predict. Let’s repeat the previous code but convert
    the token IDs into text:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 箭头左侧的所有内容（`---->`）指的是 LLM 会接收的输入，箭头右侧的标记 ID 代表 LLM 应该预测的目标标记 ID。让我们重复之前的代码，但将标记
    ID 转换为文本：
- en: '[PRE47]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The following outputs show how the input and outputs look in text format:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的输出显示了输入和输出在文本格式下的样子：
- en: '[PRE48]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: We’ve now created the input–target pairs that we can use for LLM training.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经创建了可用于 LLM 训练的输入-目标对。
- en: 'There’s only one more task before we can turn the tokens into embeddings: implementing
    an efficient data loader that iterates over the input dataset and returns the
    inputs and targets as PyTorch tensors, which can be thought of as multidimensional
    arrays. In particular, we are interested in returning two tensors: an input tensor
    containing the text that the LLM sees and a target tensor that includes the targets
    for the LLM to predict, as depicted in figure 2.13\. While the figure shows the
    tokens in string format for illustration purposes, the code implementation will
    operate on token IDs directly since the `encode` method of the BPE tokenizer performs
    both tokenization and conversion into token IDs as a single step.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们将标记转换为嵌入之前，还有一个任务：实现一个高效的数据加载器，该加载器遍历输入数据集，并以 PyTorch 张量的形式返回输入和目标，这可以被视为多维数组。特别是，我们感兴趣的是返回两个张量：一个包含
    LLM 看到的文本的输入张量，以及一个包含 LLM 预测的目标张量，如图 2.13 所示。虽然该图为了说明目的显示了字符串格式的标记，但代码实现将直接操作标记
    ID，因为 BPE 标记化器的 `encode` 方法将标记化和转换为标记 ID 作为单个步骤执行。
- en: '![figure](../Images/2-13.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/2-13.png)'
- en: Figure 2.13 To implement efficient data loaders, we collect the inputs in a
    tensor, `x`, where each row represents one input context. A second tensor, `y`,
    contains the corresponding prediction targets (next words), which are created
    by shifting the input by one position.
  id: totrans-201
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.13 为了实现高效的数据加载器，我们在张量 `x` 中收集输入，其中每一行代表一个输入上下文。第二个张量 `y` 包含相应的预测目标（下一词），这些目标是通过将输入向右移动一位创建的。
- en: Note  For the efficient data loader implementation, we will use PyTorch’s built-in
    `Dataset` and `DataLoader` classes. For additional information and guidance on
    installing PyTorch, please see section A.2.1.3 in appendix A.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：为了高效的数据加载器实现，我们将使用 PyTorch 的内置 `Dataset` 和 `DataLoader` 类。有关安装 PyTorch 的更多信息和建议，请参阅附录
    A 的 A.2.1.3 节。
- en: The code for the dataset class is shown in the following listing.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集类的代码如下所示。
- en: Listing 2.5 A dataset for batched inputs and targets
  id: totrans-204
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.5 批量输入和目标的数据集
- en: '[PRE49]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '#1 Tokenizes the entire text'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将整个文本进行标记化'
- en: '#2 Uses a sliding window to chunk the book into overlapping sequences of max_length'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 使用滑动窗口将书籍分成重叠的最大长度序列'
- en: '#3 Returns the total number of rows in the dataset'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 返回数据集中的总行数'
- en: '#4 Returns a single row from the dataset'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 返回数据集的单行'
- en: The `GPTDatasetV1` class is based on the PyTorch `Dataset` class and defines
    how individual rows are fetched from the dataset, where each row consists of a
    number of token IDs (based on a `max_length`) assigned to an `input_chunk` tensor.
    The `target_ chunk` tensor contains the corresponding targets. I recommend reading
    on to see what the data returned from this dataset looks like when we combine
    the dataset with a PyTorch `DataLoader`—this will bring additional intuition and
    clarity.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '`GPTDatasetV1`类基于PyTorch `Dataset`类，定义了如何从数据集中获取单个行，其中每行由一个或多个token ID（基于`max_length`）分配给`input_chunk`张量。`target_chunk`张量包含相应的目标。我建议继续阅读，以了解当我们将数据集与PyTorch
    `DataLoader`结合时返回的数据是什么样的——这将带来额外的直观性和清晰度。'
- en: Note  If you are new to the structure of PyTorch `Dataset` classes, such as
    shown in listing 2.5, refer to section A.6 in appendix A, which explains the general
    structure and usage of PyTorch `Dataset` and `DataLoader` classes.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果你对PyTorch `Dataset`类的结构不熟悉，例如列表2.5中所示，请参阅附录A中的A.6节，该节解释了PyTorch `Dataset`和`DataLoader`类的一般结构和用法。
- en: The following code uses the `GPTDatasetV1` to load the inputs in batches via
    a PyTorch `DataLoader`.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码使用`GPTDatasetV1`通过PyTorch `DataLoader`以批形式加载输入。
- en: Listing 2.6 A data loader to generate batches with input-with pairs
  id: totrans-213
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.6 一个生成带有输入对的批次的加载器
- en: '[PRE50]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '#1 Initializes the tokenizer'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 初始化分词器'
- en: '#2 Creates dataset'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 创建数据集'
- en: '#3 drop_last=True drops the last batch if it is shorter than the specified
    batch_size to prevent loss spikes during training.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 drop_last=True在训练期间如果最后一个批次比指定的batch_size短则丢弃，以防止损失激增。'
- en: '#4 The number of CPU processes to use for preprocessing'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 用于预处理的CPU进程数'
- en: 'Let’s test the `dataloader` with a batch size of 1 for an LLM with a context
    size of 4 to develop an intuition of how the `GPTDatasetV1` class from listing
    2.5 and the `create_ dataloader_v1` function from listing 2.6 work together:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们测试`dataloader`，使用大小为1的批次和上下文大小为4的LLM，以了解列表2.5中的`GPTDatasetV1`类和列表2.6中的`create_dataloader_v1`函数是如何一起工作的：
- en: '[PRE51]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '#1 Converts dataloader into a Python iterator to fetch the next entry via Python’s
    built-in next() function'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将dataloader转换为Python迭代器，通过Python内置的next()函数获取下一个条目'
- en: 'Executing the preceding code prints the following:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前面的代码会打印以下内容：
- en: '[PRE52]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The `first_batch` variable contains two tensors: the first tensor stores the
    input token IDs, and the second tensor stores the target token IDs. Since the
    `max_length` is set to 4, each of the two tensors contains four token IDs. Note
    that an input size of 4 is quite small and only chosen for simplicity. It is common
    to train LLMs with input sizes of at least 256.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '`first_batch`变量包含两个张量：第一个张量存储输入token ID，第二个张量存储目标token ID。由于`max_length`设置为4，因此两个张量都包含四个token
    ID。请注意，输入大小为4非常小，仅为了简单起见而选择。通常使用至少256的输入大小来训练LLM。'
- en: 'To understand the meaning of `stride=1`, let’s fetch another batch from this
    dataset:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解`stride=1`的含义，让我们从这个数据集中获取另一个批次：
- en: '[PRE53]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The second batch has the following contents:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个批次包含以下内容：
- en: '[PRE54]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: If we compare the first and second batches, we can see that the second batch’s
    token IDs are shifted by one position (for example, the second ID in the first
    batch’s input is 367, which is the first ID of the second batch’s input). The
    `stride` setting dictates the number of positions the inputs shift across batches,
    emulating a sliding window approach, as demonstrated in figure 2.14.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们比较第一和第二个批次，我们可以看到第二个批次的token ID向右移动了一个位置（例如，第一个批次输入中的第二个ID是367，这是第二个批次输入的第一个ID）。`stride`设置决定了输入在批次之间移动的位置数，模拟滑动窗口方法，如图2.14所示。
- en: '![figure](../Images/2-14.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/2-14.png)'
- en: Figure 2.14 When creating multiple batches from the input dataset, we slide
    an input window across the text. If the stride is set to 1, we shift the input
    window by one position when creating the next batch. If we set the stride equal
    to the input window size, we can prevent overlaps between the batches.
  id: totrans-231
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.14 当从输入数据集创建多个批次时，我们在文本上滑动一个输入窗口。如果将步长设置为1，则在创建下一个批次时，我们将输入窗口移动一个位置。如果我们设置步长等于输入窗口大小，我们可以防止批次之间的重叠。
- en: Exercise 2.2 Data loaders with different strides and context sizes
  id: totrans-232
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习2.2 不同步长和上下文大小的数据加载器
- en: To develop more intuition for how the data loader works, try to run it with
    different settings such as `max_length=2` and `stride=2,` and `max_length=8` and
    `stride=2`.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更直观地了解数据加载器的工作原理，尝试使用不同的设置运行它，例如`max_length=2`和`stride=2,`以及`max_length=8`和`stride=2`。
- en: Batch sizes of 1, such as we have sampled from the data loader so far, are useful
    for illustration purposes. If you have previous experience with deep learning,
    you may know that small batch sizes require less memory during training but lead
    to more noisy model updates. Just like in regular deep learning, the batch size
    is a tradeoff and a hyperparameter to experiment with when training LLMs.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 批量大小为1，如我们迄今为止从数据加载器中采样到的，在说明用途上是有用的。如果你有深度学习的先前经验，你可能知道小批量大小在训练期间需要的内存较少，但会导致模型更新更嘈杂。就像常规深度学习一样，批量大小是训练LLM时的权衡和超参数。
- en: 'Let’s look briefly at how we can use the data loader to sample with a batch
    size greater than 1:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 简单地看看我们如何使用数据加载器以大于1的批量大小进行采样：
- en: '[PRE55]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: This prints
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印
- en: '[PRE56]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Note that we increase the stride to 4 to utilize the data set fully (we don’t
    skip a single word). This avoids any overlap between the batches since more overlap
    could lead to increased overfitting.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们将步长增加到4以充分利用数据集（我们不跳过任何单词）。这避免了批次的任何重叠，因为更多的重叠可能导致过拟合增加。
- en: 2.7 Creating token embeddings
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.7 创建标记嵌入
- en: The last step in preparing the input text for LLM training is to convert the
    token IDs into embedding vectors, as shown in figure 2.15. As a preliminary step,
    we must initialize these embedding weights with random values. This initialization
    serves as the starting point for the LLM’s learning process. In chapter 5, we
    will optimize the embedding weights as part of the LLM training.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 准备LLM训练的输入文本的最后一步是将标记ID转换为嵌入向量，如图2.15所示。作为一个初步步骤，我们必须用随机值初始化这些嵌入权重。这个初始化是LLM学习过程的起点。在第5章中，我们将作为LLM训练的一部分优化嵌入权重。
- en: '![figure](../Images/2-15.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-15.png)'
- en: Figure 2.15 Preparation involves tokenizing text, converting text tokens to
    token IDs, and converting token IDs into embedding vectors. Here, we consider
    the previously created token IDs to create the token embedding vectors.
  id: totrans-243
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.15 准备工作包括对文本进行分词，将文本标记转换为标记ID，并将标记ID转换为嵌入向量。在这里，我们考虑之前创建的标记ID来创建标记嵌入向量。
- en: A continuous vector representation, or embedding, is necessary since GPT-like
    LLMs are deep neural networks trained with the backpropagation algorithm.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 连续向量表示，或嵌入，对于像GPT这样的LLM是必要的，因为它们是使用反向传播算法训练的深度神经网络。
- en: Note  If you are unfamiliar with how neural networks are trained with backpropagation,
    please read section A.4 in appendix A.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果你不熟悉如何使用反向传播训练神经网络，请阅读附录A中的A.4节。
- en: 'Let’s see how the token ID to embedding vector conversion works with a hands-on
    example. Suppose we have the following four input tokens with IDs 2, 3, 5, and
    1:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个实际例子看看标记ID到嵌入向量的转换是如何工作的。假设我们有以下四个输入标记，它们的ID分别是2、3、5和1：
- en: '[PRE57]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'For the sake of simplicity, suppose we have a small vocabulary of only 6 words
    (instead of the 50,257 words in the BPE tokenizer vocabulary), and we want to
    create embeddings of size 3 (in GPT-3, the embedding size is 12,288 dimensions):'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化起见，假设我们有一个只有6个单词的小型词汇表（而不是BPE标记器词汇表中的50,257个单词），并且我们想要创建大小为3的嵌入（在GPT-3中，嵌入大小是12,288维）：
- en: '[PRE58]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Using the `vocab_size` and `output_dim`, we can instantiate an embedding layer
    in PyTorch, setting the random seed to `123` for reproducibility purposes:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`vocab_size`和`output_dim`，我们可以在PyTorch中实例化一个嵌入层，为了可重复性，将随机种子设置为`123`：
- en: '[PRE59]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The print statement prints the embedding layer’s underlying weight matrix:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 打印语句打印嵌入层的底层权重矩阵：
- en: '[PRE60]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: The weight matrix of the embedding layer contains small, random values. These
    values are optimized during LLM training as part of the LLM optimization itself.
    Moreover, we can see that the weight matrix has six rows and three columns. There
    is one row for each of the six possible tokens in the vocabulary, and there is
    one column for each of the three embedding dimensions.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层的权重矩阵包含小的随机值。这些值在LLM训练过程中作为LLM优化的一部分进行优化。此外，我们可以看到权重矩阵有六行和三列。每一行对应词汇表中的六个可能的标记中的一个，每一列对应三个嵌入维度中的一个。
- en: 'Now, let’s apply it to a token ID to obtain the embedding vector:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将其应用于一个标记ID以获得嵌入向量：
- en: '[PRE61]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: The returned embedding vector is
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的嵌入向量是
- en: '[PRE62]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: If we compare the embedding vector for token ID 3 to the previous embedding
    matrix, we see that it is identical to the fourth row (Python starts with a zero
    index, so it’s the row corresponding to index 3). In other words, the embedding
    layer is essentially a lookup operation that retrieves rows from the embedding
    layer’s weight matrix via a token ID.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将标记ID 3的嵌入向量与之前的嵌入矩阵进行比较，我们会看到它与第四行相同（Python从零索引开始，所以它是与索引3对应的行）。换句话说，嵌入层本质上是一个查找操作，通过标记ID检索嵌入层权重矩阵中的行。
- en: Note  For those who are familiar with one-hot encoding, the embedding layer
    approach described here is essentially just a more efficient way of implementing
    one-hot encoding followed by matrix multiplication in a fully connected layer,
    which is illustrated in the supplementary code on GitHub at [https://mng.bz/ZEB5](https://mng.bz/ZEB5).
    Because the embedding layer is just a more efficient implementation equivalent
    to the one-hot encoding and matrix-multiplication approach, it can be seen as
    a neural network layer that can be optimized via backpropagation.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：对于那些熟悉one-hot编码的人来说，这里描述的嵌入层方法本质上只是实现one-hot编码后，在全连接层中进行矩阵乘法的一种更有效的方式，这在GitHub上的补充代码中有说明[https://mng.bz/ZEB5](https://mng.bz/ZEB5)。因为嵌入层只是one-hot编码和矩阵乘法方法的一种更有效的实现，所以它可以看作是一个可以通过反向传播进行优化的神经网络层。
- en: 'We’ve seen how to convert a single token ID into a three-dimensional embedding
    vector. Let’s now apply that to all four input IDs (`torch.tensor([2,` `3,` `5,`
    `1])`):'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了如何将单个标记ID转换为三维嵌入向量。现在，让我们将这种方法应用到所有四个输入ID（`torch.tensor([2, 3, 5, 1])`）上：
- en: '[PRE63]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The print output reveals that this results in a 4 × 3 matrix:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 打印输出显示这导致了一个4×3的矩阵：
- en: '[PRE64]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Each row in this output matrix is obtained via a lookup operation from the embedding
    weight matrix, as illustrated in figure 2.16.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 输出矩阵中的每一行都是通过从嵌入权重矩阵中查找操作获得的，如图2.16所示。
- en: '![figure](../Images/2-16.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-16.png)'
- en: Figure 2.16 Embedding layers perform a lookup operation, retrieving the embedding
    vector corresponding to the token ID from the embedding layer’s weight matrix.
    For instance, the embedding vector of the token ID 5 is the sixth row of the embedding
    layer weight matrix (it is the sixth instead of the fifth row because Python starts
    counting at 0). We assume that the token IDs were produced by the small vocabulary
    from section 2.3.
  id: totrans-267
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.16 嵌入层执行查找操作，从嵌入层的权重矩阵中检索与标记ID对应的嵌入向量。例如，标记ID 5的嵌入向量是嵌入层权重矩阵的第六行（它是第六行而不是第五行，因为Python从0开始计数）。我们假设标记ID是由2.3节中的小词汇表生成的。
- en: Having now created embedding vectors from token IDs, next we’ll add a small
    modification to these embedding vectors to encode positional information about
    a token within a text.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经从标记ID创建了嵌入向量，接下来我们将对这些嵌入向量进行小的修改，以编码文本中标记的位置信息。
- en: 2.8 Encoding word positions
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.8 编码词位置
- en: In principle, token embeddings are a suitable input for an LLM. However, a minor
    shortcoming of LLMs is that their self-attention mechanism (see chapter 3) doesn’t
    have a notion of position or order for the tokens within a sequence. The way the
    previously introduced embedding layer works is that the same token ID always gets
    mapped to the same vector representation, regardless of where the token ID is
    positioned in the input sequence, as shown in figure 2.17.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 从原则上讲，标记嵌入是适合作为LLM输入的。然而，LLM的一个小缺点是它们的自注意力机制（见第3章）没有对序列中标记的位置或顺序的概念。之前引入的嵌入层的工作方式是，相同的标记ID总是映射到相同的向量表示，无论该标记ID在输入序列中的位置如何，如图2.17所示。
- en: '![figure](../Images/2-17.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-17.png)'
- en: Figure 2.17 The embedding layer converts a token ID into the same vector representation
    regardless of where it is located in the input sequence. For example, the token
    ID 5, whether it’s in the first or fourth position in the token ID input vector,
    will result in the same embedding vector.
  id: totrans-272
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.17 嵌入层将标记ID转换为相同的向量表示，无论它在输入序列中的位置如何。例如，标记ID 5，无论它在标记ID输入向量中的第一个还是第四个位置，都会得到相同的嵌入向量。
- en: In principle, the deterministic, position-independent embedding of the token
    ID is good for reproducibility purposes. However, since the self-attention mechanism
    of LLMs itself is also position-agnostic, it is helpful to inject additional position
    information into the LLM.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 从原则上讲，对于可重现性目的，标记ID的确定性、位置无关的嵌入是好的。然而，由于LLMs自身的自注意力机制本身也是位置无关的，向LLM中注入额外的位置信息是有帮助的。
- en: 'To achieve this, we can use two broad categories of position-aware embeddings:
    relative positional embeddings and absolute positional embeddings. Absolute positional
    embeddings are directly associated with specific positions in a sequence. For
    each position in the input sequence, a unique embedding is added to the token’s
    embedding to convey its exact location. For instance, the first token will have
    a specific positional embedding, the second token another distinct embedding,
    and so on, as illustrated in figure 2.18.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们可以使用两种广泛的位置感知嵌入类别：相对位置嵌入和绝对位置嵌入。绝对位置嵌入直接与序列中的特定位置相关联。对于输入序列中的每个位置，都会添加一个唯一的嵌入到标记的嵌入中，以传达其确切位置。例如，第一个标记将有一个特定的位置嵌入，第二个标记另一个不同的嵌入，依此类推，如图2.18所示。
- en: '![figure](../Images/2-18.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-18.png)'
- en: Figure 2.18 Positional embeddings are added to the token embedding vector to
    create the input embeddings for an LLM. The positional vectors have the same dimension
    as the original token embeddings. The token embeddings are shown with value 1
    for simplicity.
  id: totrans-276
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.18 位置嵌入被添加到标记嵌入向量中，以创建LLM的输入嵌入。位置向量与原始标记嵌入具有相同的维度。为了简单起见，标记嵌入显示为值1。
- en: Instead of focusing on the absolute position of a token, the emphasis of relative
    positional embeddings is on the relative position or distance between tokens.
    This means the model learns the relationships in terms of “how far apart” rather
    than “at which exact position.” The advantage here is that the model can generalize
    better to sequences of varying lengths, even if it hasn’t seen such lengths during
    training.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 相对位置嵌入的 emphasis 不在于标记的绝对位置，而在于标记之间的相对位置或距离。这意味着模型通过“距离多远”而不是“在哪个确切位置”来学习关系。这里的优势是，模型可以更好地泛化到不同长度的序列，即使它在训练期间没有看到这样的长度。
- en: Both types of positional embeddings aim to augment the capacity of LLMs to understand
    the order and relationships between tokens, ensuring more accurate and context-aware
    predictions. The choice between them often depends on the specific application
    and the nature of the data being processed.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种类型的位置嵌入旨在增强LLMs理解标记之间的顺序和关系的能力，确保更准确和上下文感知的预测。它们之间的选择通常取决于具体的应用和数据处理的性质。
- en: OpenAI’s GPT models use absolute positional embeddings that are optimized during
    the training process rather than being fixed or predefined like the positional
    encodings in the original transformer model. This optimization process is part
    of the model training itself. For now, let’s create the initial positional embeddings
    to create the LLM inputs.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的GPT模型使用的是在训练过程中优化的绝对位置嵌入，而不是像原始transformer模型中的位置编码那样固定或预定义。这个过程是模型训练本身的一部分。现在，让我们创建初始位置嵌入以创建LLM输入。
- en: 'Previously, we focused on very small embedding sizes for simplicity. Now, let’s
    consider more realistic and useful embedding sizes and encode the input tokens
    into a 256-dimensional vector representation, which is smaller than what the original
    GPT-3 model used (in GPT-3, the embedding size is 12,288 dimensions) but still
    reasonable for experimentation. Furthermore, we assume that the token IDs were
    created by the BPE tokenizer we implemented earlier, which has a vocabulary size
    of 50,257:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，我们为了简单起见关注了非常小的嵌入大小。现在，让我们考虑更现实和有用的嵌入大小，并将输入标记编码到256维向量表示中，这比原始GPT-3模型使用的要小（在GPT-3中，嵌入大小是12,288维）但仍然适合实验。此外，我们假设标记ID是由我们之前实现的BPE标记器创建的，该标记器具有50,257个词汇量：
- en: '[PRE65]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Using the previous `token_embedding_layer`, if we sample data from the data
    loader, we embed each token in each batch into a 256-dimensional vector. If we
    have a batch size of 8 with four tokens each, the result will be an 8 × 4 × 256
    tensor.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 使用之前的`token_embedding_layer`，如果我们从数据加载器中采样数据，我们将每个批次中的每个标记嵌入到一个256维向量中。如果我们有一个包含四个标记的8个批次的批次大小，结果将是一个8
    × 4 × 256的张量。
- en: 'Let’s instantiate the data loader (see section 2.6) first:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先实例化数据加载器（见第2.6节）：
- en: '[PRE66]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: This code prints
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码打印
- en: '[PRE67]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: As we can see, the token ID tensor is 8 × 4 dimensional, meaning that the data
    batch consists of eight text samples with four tokens each.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，标记 ID 张量是 8 × 4 维的，这意味着数据批次由八个文本样本组成，每个样本有四个标记。
- en: 'Let’s now use the embedding layer to embed these token IDs into 256-dimensional
    vectors:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用嵌入层将这些标记 ID 嵌入 256 维向量：
- en: '[PRE68]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: The print function call returns
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 打印函数调用返回
- en: '[PRE69]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: The 8 × 4 × 256–dimensional tensor output shows that each token ID is now embedded
    as a 256-dimensional vector.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 8 × 4 × 256 维度的张量输出表明每个标记 ID 现在都嵌入为一个 256 维的向量。
- en: 'For a GPT model’s absolute embedding approach, we just need to create another
    embedding layer that has the same embedding dimension as the `token_embedding_
    layer`:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 GPT 模型的绝对嵌入方法，我们只需要创建另一个与 `token_embedding_ 层` 具有相同嵌入维度的嵌入层：
- en: '[PRE70]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: The input to the `pos_embeddings` is usually a placeholder vector `torch.arange(context_length)`,
    which contains a sequence of numbers 0, 1, ..., up to the maximum input length
    –1\. The `context_length` is a variable that represents the supported input size
    of the LLM. Here, we choose it similar to the maximum length of the input text.
    In practice, input text can be longer than the supported context length, in which
    case we have to truncate the text.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '`pos_embeddings` 的输入通常是一个占位符向量 `torch.arange(context_length)`，它包含一个从 0 到最大输入长度
    -1 的数字序列。`context_length` 是一个表示 LLM 支持的输入大小的变量。在这里，我们选择它与输入文本的最大长度相似。在实践中，输入文本可能比支持的内容长度更长，在这种情况下，我们必须截断文本。'
- en: The output of the print statement is
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 打印语句的输出是
- en: '[PRE71]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'As we can see, the positional embedding tensor consists of four 256-dimensional
    vectors. We can now add these directly to the token embeddings, where PyTorch
    will add the 4 × 256–dimensional `pos_embeddings` tensor to each 4 × 256–dimensional
    token embedding tensor in each of the eight batches:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，位置嵌入张量由四个 256 维向量组成。我们现在可以直接将这些向量添加到标记嵌入中，PyTorch 将 4 × 256 维的 `pos_embeddings`
    张量添加到每个 4 × 256 维的标记嵌入张量中，每个批次都是八个：
- en: '[PRE72]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: The print output is
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 打印输出是
- en: '[PRE73]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: The `input_embeddings` we created, as summarized in figure 2.19, are the embedded
    input examples that can now be processed by the main LLM modules, which we will
    begin implementing in the next chapter.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的 `input_embeddings`，如图 2.19 所示，是现在可以被主要 LLM 模块处理的嵌入输入示例，我们将在下一章开始实现这些模块。
- en: '![figure](../Images/2-19.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/2-19.png)'
- en: Figure 2.19 As part of the input processing pipeline, input text is first broken
    up into individual tokens. These tokens are then converted into token IDs using
    a vocabulary. The token IDs are converted into embedding vectors to which positional
    embeddings of a similar size are added, resulting in input embeddings that are
    used as input for the main LLM layers.
  id: totrans-304
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.19 作为输入处理流程的一部分，输入文本首先被分解为单个标记。然后，使用词汇表将这些标记转换为标记 ID。这些标记 ID 被转换为嵌入向量，然后添加与它们大小相似的定位嵌入，从而得到用于主要
    LLM 层的输入嵌入。
- en: Summary
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: LLMs require textual data to be converted into numerical vectors, known as embeddings,
    since they can’t process raw text. Embeddings transform discrete data (like words
    or images) into continuous vector spaces, making them compatible with neural network
    operations.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM 需要将文本数据转换为数值向量，称为嵌入，因为它们不能处理原始文本。嵌入将离散数据（如单词或图像）转换为连续向量空间，使它们与神经网络操作兼容。
- en: As the first step, raw text is broken into tokens, which can be words or characters.
    Then, the tokens are converted into integer representations, termed token IDs.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为第一步，原始文本被分解为标记，这些标记可以是单词或字符。然后，这些标记被转换为整数表示，称为标记 ID。
- en: Special tokens, such as `<|unk|>` and `<|endoftext|>`, can be added to enhance
    the model’s understanding and handle various contexts, such as unknown words or
    marking the boundary between unrelated texts.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以添加特殊标记，如 `<|unk|>` 和 `<|endoftext|>`，以增强模型的理解并处理各种上下文，例如未知单词或标记无关文本之间的边界。
- en: The byte pair encoding (BPE) tokenizer used for LLMs like GPT-2 and GPT-3 can
    efficiently handle unknown words by breaking them down into subword units or individual
    characters.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于 GPT-2 和 GPT-3 等LLM的字节对编码（BPE）标记化器可以有效地通过将它们分解为子词单元或单个字符来处理未知单词。
- en: We use a sliding window approach on tokenized data to generate input–target
    pairs for LLM training.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在标记化数据上使用滑动窗口方法来生成 LLM 训练的输入-目标对。
- en: Embedding layers in PyTorch function as a lookup operation, retrieving vectors
    corresponding to token IDs. The resulting embedding vectors provide continuous
    representations of tokens, which is crucial for training deep learning models
    like LLMs.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch中的嵌入层充当查找操作，检索与标记ID对应的向量。生成的嵌入向量提供了标记的连续表示，这对于训练像LLMs这样的深度学习模型至关重要。
- en: 'While token embeddings provide consistent vector representations for each token,
    they lack a sense of the token’s position in a sequence. To rectify this, two
    main types of positional embeddings exist: absolute and relative. OpenAI’s GPT
    models utilize absolute positional embeddings, which are added to the token embedding
    vectors and are optimized during the model training.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然标记嵌入为每个标记提供了一致的向量表示，但它们缺乏标记在序列中的位置感。为了纠正这一点，存在两种主要类型的位置嵌入：绝对和相对。OpenAI的GPT模型使用绝对位置嵌入，这些嵌入被添加到标记嵌入向量中，并在模型训练过程中进行优化。
