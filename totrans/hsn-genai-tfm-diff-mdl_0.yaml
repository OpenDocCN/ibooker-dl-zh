- en: Chapter 1\. Diffusion Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬1ç« ã€‚æ‰©æ•£æ¨¡å‹
- en: In late 2020 a little-known class of models called diffusion models began causing
    a stir in the machine-learning world. Researchers figured out how to use these
    models to generate synthetic images at higher quality than any produced by previous
    techniques. A flurry of papers followed, proposing improvements and modifications
    that pushed the quality up even further. By late 2021 there were models like GLIDE
    that showcased incredible results on text-to-image tasks, and a few months later,
    these models had entered the mainstream with tools like DALL-E 2 and Stable Diffusion.
    These models made it easy for anyone to generate images just by typing in a text
    description of what they wanted to see.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨2020å¹´æœ«ï¼Œä¸€ä¸ªåä¸ºæ‰©æ•£æ¨¡å‹çš„é²œä¸ºäººçŸ¥çš„æ¨¡å‹ç±»åˆ«å¼€å§‹åœ¨æœºå™¨å­¦ä¹ é¢†åŸŸå¼•èµ·è½°åŠ¨ã€‚ç ”ç©¶äººå‘˜æ‰¾å‡ºäº†å¦‚ä½•ä½¿ç”¨è¿™äº›æ¨¡å‹ç”Ÿæˆæ¯”ä»¥å‰æŠ€æœ¯äº§ç”Ÿçš„åˆæˆå›¾åƒè´¨é‡æ›´é«˜çš„å›¾åƒã€‚éšåå‡ºç°äº†ä¸€ç³»åˆ—è®ºæ–‡ï¼Œæå‡ºäº†æ”¹è¿›å’Œä¿®æ”¹ï¼Œè¿›ä¸€æ­¥æé«˜äº†è´¨é‡ã€‚åˆ°2021å¹´åº•ï¼Œå‡ºç°äº†åƒGLIDEè¿™æ ·çš„æ¨¡å‹ï¼Œå±•ç¤ºäº†åœ¨æ–‡æœ¬åˆ°å›¾åƒä»»åŠ¡ä¸Šä»¤äººéš¾ä»¥ç½®ä¿¡çš„ç»“æœï¼Œå‡ ä¸ªæœˆåï¼Œè¿™äº›æ¨¡å‹å·²ç»è¿›å…¥äº†ä¸»æµï¼Œå¦‚DALL-E
    2å’ŒStable Diffusionç­‰å·¥å…·ï¼Œä½¿ä»»ä½•äººéƒ½å¯ä»¥é€šè¿‡è¾“å…¥æ‰€éœ€çœ‹åˆ°çš„æ–‡æœ¬æè¿°æ¥ç”Ÿæˆå›¾åƒã€‚
- en: In this chapter, weâ€™re going to dig into the details of how these models work.
    Weâ€™ll outline the key insights that make them so powerful, generate images with
    existing models to get a feel for how they work, and then train our own models
    to deepen this understanding further. The field is still rapidly evolving, but
    the topics covered here should give you a solid foundation to build on. Chapter
    5 will explore more advanced techniques through the lens of a model called Stable
    Diffusion, and chapter 6 will explore applications of these techniques beyond
    simple image generation.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥äº†è§£è¿™äº›æ¨¡å‹çš„å·¥ä½œåŸç†ã€‚æˆ‘ä»¬å°†æ¦‚è¿°ä½¿å®ƒä»¬å¦‚æ­¤å¼ºå¤§çš„å…³é”®è§è§£ï¼Œä½¿ç”¨ç°æœ‰æ¨¡å‹ç”Ÿæˆå›¾åƒï¼Œä»¥äº†è§£å®ƒä»¬çš„å·¥ä½œæ–¹å¼ï¼Œç„¶åè®­ç»ƒæˆ‘ä»¬è‡ªå·±çš„æ¨¡å‹ï¼Œä»¥è¿›ä¸€æ­¥åŠ æ·±å¯¹æ­¤çš„ç†è§£ã€‚è¯¥é¢†åŸŸä»åœ¨å¿«é€Ÿå‘å±•ï¼Œä½†æœ¬ç« æ¶µç›–çš„ä¸»é¢˜åº”è¯¥ä¸ºæ‚¨æ‰“ä¸‹åšå®çš„åŸºç¡€ã€‚ç¬¬5ç« å°†é€šè¿‡ä¸€ä¸ªåä¸ºStable
    Diffusionçš„æ¨¡å‹ï¼Œæ¢ç´¢æ›´é«˜çº§çš„æŠ€æœ¯ï¼Œç¬¬6ç« å°†æ¢è®¨è¿™äº›æŠ€æœ¯åœ¨ç®€å•å›¾åƒç”Ÿæˆä¹‹å¤–çš„åº”ç”¨ã€‚
- en: 'The Key Insight: Iterative Refinement'
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å…³é”®è§è§£ï¼šè¿­ä»£ç»†åŒ–
- en: So what is it that makes diffusion models so powerful? Previous techniques,
    such as VAEs or GANs, generate their final output via a single forward pass of
    the model. This means the model must get everything right on the first try. If
    it makes a mistake, it canâ€™t go back and fix it. Diffusion models, on the other
    hand, generate their output by iterating over many steps. This â€˜iterative refinementâ€™
    allows the model to correct mistakes made in previous steps and gradually improve
    the output. To illustrate this, letâ€™s look at an example of a diffusion model
    in action.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œæ˜¯ä»€ä¹ˆä½¿æ‰©æ•£æ¨¡å‹å¦‚æ­¤å¼ºå¤§å‘¢ï¼Ÿå…ˆå‰çš„æŠ€æœ¯ï¼Œå¦‚VAEsæˆ–GANsï¼Œé€šè¿‡æ¨¡å‹çš„å•æ¬¡å‰å‘ä¼ é€’ç”Ÿæˆæœ€ç»ˆè¾“å‡ºã€‚è¿™æ„å‘³ç€æ¨¡å‹å¿…é¡»åœ¨ç¬¬ä¸€æ¬¡å°è¯•æ—¶å°±åšåˆ°ä¸€åˆ‡æ­£ç¡®ã€‚å¦‚æœå®ƒçŠ¯äº†ä¸€ä¸ªé”™è¯¯ï¼Œå°±æ— æ³•è¿”å›å¹¶ä¿®å¤å®ƒã€‚å¦ä¸€æ–¹é¢ï¼Œæ‰©æ•£æ¨¡å‹é€šè¿‡è¿­ä»£å¤šä¸ªæ­¥éª¤ç”Ÿæˆå…¶è¾“å‡ºã€‚è¿™ç§â€œè¿­ä»£ç»†åŒ–â€å…è®¸æ¨¡å‹çº æ­£ä¹‹å‰æ­¥éª¤ä¸­çš„é”™è¯¯ï¼Œå¹¶é€æ¸æ”¹è¿›è¾“å‡ºã€‚ä¸ºäº†è¯´æ˜è¿™ä¸€ç‚¹ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸ªæ‰©æ•£æ¨¡å‹çš„ç¤ºä¾‹ã€‚
- en: 'We can load a pre-trained model using the Hugging Face diffusers library. The
    pipeline can be used to create images directly, but this doesnâ€™t show us what
    is going on under the hood:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨Hugging Faceæ‰©æ•£å™¨åº“åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ã€‚è¯¥ç®¡é“å¯ç”¨äºç›´æ¥åˆ›å»ºå›¾åƒï¼Œä½†è¿™å¹¶ä¸èƒ½æ˜¾ç¤ºå‡ºåº•å±‚å‘ç”Ÿäº†ä»€ä¹ˆï¼š
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![image](assets/cell-4-output-3.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-4-output-3.png)'
- en: We can re-create the sampling process step by step to get a better look at what
    is happening as the model generates images. We initialize our sample x with random
    noise and then run it through the model for 30 steps. On the right, you can see
    the modelâ€™s prediction for what the final image will look like at specific steps
    - note that the initial predictions are not particularly good! Instead of jumping
    right to that final predicted image, we only modify x by a small amount in the
    direction of the prediction (shown on the left). We then feed this new, slightly
    better x through the model again for the next step, hopefully resulting in a slightly
    improved prediction, which can be used to update x a little more, and so on. With
    enough steps, the model can produce some impressively realistic images.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€æ­¥é‡æ–°åˆ›å»ºé‡‡æ ·è¿‡ç¨‹ï¼Œä»¥æ›´å¥½åœ°äº†è§£æ¨¡å‹ç”Ÿæˆå›¾åƒæ—¶å‘ç”Ÿäº†ä»€ä¹ˆã€‚æˆ‘ä»¬ä½¿ç”¨éšæœºå™ªå£°åˆå§‹åŒ–æˆ‘ä»¬çš„æ ·æœ¬xï¼Œç„¶åé€šè¿‡æ¨¡å‹è¿è¡Œ30æ­¥ã€‚åœ¨å³ä¾§ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°æ¨¡å‹å¯¹ç‰¹å®šæ­¥éª¤çš„æœ€ç»ˆå›¾åƒé¢„æµ‹-è¯·æ³¨æ„ï¼Œæœ€åˆçš„é¢„æµ‹å¹¶ä¸ç‰¹åˆ«å¥½ï¼æˆ‘ä»¬ä¸æ˜¯ç›´æ¥è·³åˆ°æœ€ç»ˆé¢„æµ‹çš„å›¾åƒï¼Œè€Œæ˜¯åªåœ¨é¢„æµ‹çš„æ–¹å‘ä¸Šç¨å¾®ä¿®æ”¹xï¼ˆæ˜¾ç¤ºåœ¨å·¦ä¾§ï¼‰ã€‚ç„¶åï¼Œæˆ‘ä»¬å†æ¬¡å°†è¿™ä¸ªæ–°çš„ã€ç¨å¾®æ›´å¥½çš„xé€šè¿‡æ¨¡å‹è¿›è¡Œä¸‹ä¸€æ­¥çš„å¤„ç†ï¼Œå¸Œæœ›èƒ½äº§ç”Ÿç¨å¾®æ”¹è¿›çš„é¢„æµ‹ï¼Œè¿™å¯ä»¥ç”¨æ¥è¿›ä¸€æ­¥æ›´æ–°xï¼Œä¾æ­¤ç±»æ¨ã€‚ç»è¿‡è¶³å¤Ÿçš„æ­¥éª¤ï¼Œæ¨¡å‹å¯ä»¥ç”Ÿæˆä¸€äº›ä»¤äººå°è±¡æ·±åˆ»çš„é€¼çœŸå›¾åƒã€‚
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![image](assets/cell-5-output-1.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-5-output-1.png)'
- en: '![image](assets/cell-5-output-2.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-5-output-2.png)'
- en: '![image](assets/cell-5-output-3.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-5-output-3.png)'
- en: '![image](assets/cell-5-output-4.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-5-output-4.png)'
- en: Note
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: Donâ€™t worry if that code looks a bit intimidating - weâ€™ll explain how this all
    works over the course of this chapter. For now, just focus on the results.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœé‚£æ®µä»£ç çœ‹èµ·æ¥æœ‰ç‚¹å“äºº-æˆ‘ä»¬å°†åœ¨æœ¬ç« ä¸­è§£é‡Šè¿™ä¸€åˆ‡æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚ç°åœ¨ï¼Œåªéœ€ä¸“æ³¨äºç»“æœã€‚
- en: This core idea of learning how to refine a â€˜corruptedâ€™ input gradually can be
    applied to a wide range of tasks. In this chapter, weâ€™ll focus on unconditional
    image generation - that is, generating images that resemble the training data,
    with no additional controls over what these generated samples look like. Diffusion
    models have also been applied to audio, video, text and more. And while most implementations
    use some variant of the â€˜denoisingâ€™ approach that weâ€™ll cover here, new approaches
    utilizing different types of â€˜corruptionâ€™ together with iterative refinement are
    emerging that may move the field beyond the current focus on denoising diffusion
    specifically. Exciting times!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å­¦ä¹ å¦‚ä½•é€æ¸æ”¹è¿›â€œæŸåâ€çš„è¾“å…¥çš„æ ¸å¿ƒæ€æƒ³å¯ä»¥åº”ç”¨äºå¹¿æ³›çš„ä»»åŠ¡èŒƒå›´ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä¸“æ³¨äºæ— æ¡ä»¶å›¾åƒç”Ÿæˆ-ä¹Ÿå°±æ˜¯è¯´ï¼Œç”Ÿæˆç±»ä¼¼è®­ç»ƒæ•°æ®çš„å›¾åƒï¼Œè€Œä¸å¯¹è¿™äº›ç”Ÿæˆçš„æ ·æœ¬çš„å¤–è§‚è¿›è¡Œé¢å¤–çš„æ§åˆ¶ã€‚æ‰©æ•£æ¨¡å‹ä¹Ÿå·²åº”ç”¨äºéŸ³é¢‘ã€è§†é¢‘ã€æ–‡æœ¬ç­‰ã€‚è™½ç„¶å¤§å¤šæ•°å®ç°ä½¿ç”¨æˆ‘ä»¬å°†åœ¨è¿™é‡Œä»‹ç»çš„æŸç§â€œå»å™ªâ€æ–¹æ³•çš„å˜ä½“ï¼Œä½†æ­£åœ¨å‡ºç°åˆ©ç”¨ä¸åŒç±»å‹çš„â€œæŸåâ€ä»¥åŠè¿­ä»£ç»†åŒ–çš„æ–°æ–¹æ³•ï¼Œè¿™å¯èƒ½ä¼šä½¿è¯¥é¢†åŸŸè¶…è¶Šç›®å‰ä¸“æ³¨äºå»å™ªæ‰©æ•£çš„ç„¦ç‚¹ã€‚ä»¤äººå…´å¥‹çš„æ—¶åˆ»ï¼
- en: Training a Diffusion Model
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ‰©æ•£æ¨¡å‹
- en: 'In this section, weâ€™re going to train a diffusion model from scratch to gain
    a better understanding of how they work. Weâ€™ll start by using components from
    the Hugging Face diffusers library. As the chapter progresses, weâ€™ll gradually
    demystify how each component works. Training a diffusion model is relatively straightforward
    compared to other types of generative models. We repeatedly:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªæ‰©æ•£æ¨¡å‹ï¼Œä»¥æ›´å¥½åœ°äº†è§£å®ƒä»¬çš„å·¥ä½œåŸç†ã€‚æˆ‘ä»¬å°†é¦–å…ˆä½¿ç”¨Hugging Face diffusersåº“ä¸­çš„ç»„ä»¶ã€‚éšç€æœ¬ç« çš„è¿›è¡Œï¼Œæˆ‘ä»¬å°†é€æ¸æ­å¼€æ¯ä¸ªç»„ä»¶çš„å·¥ä½œåŸç†ã€‚ä¸å…¶ä»–ç±»å‹çš„ç”Ÿæˆæ¨¡å‹ç›¸æ¯”ï¼Œè®­ç»ƒæ‰©æ•£æ¨¡å‹ç›¸å¯¹ç®€å•ã€‚æˆ‘ä»¬åå¤è¿›è¡Œï¼š
- en: Load in some images from the training data.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»è®­ç»ƒæ•°æ®ä¸­åŠ è½½ä¸€äº›å›¾åƒã€‚
- en: Add noise in different amounts. Remember, we want the model to do a good job
    estimating how to â€˜fixâ€™ (denoise) both extremely noisy images and images that
    are close to perfect.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»¥ä¸åŒçš„é‡æ·»åŠ å™ªéŸ³ã€‚è®°ä½ï¼Œæˆ‘ä»¬å¸Œæœ›æ¨¡å‹èƒ½å¤Ÿå¾ˆå¥½åœ°ä¼°è®¡å¦‚ä½•â€œä¿®å¤â€ï¼ˆå»å™ªï¼‰æå…¶å˜ˆæ‚çš„å›¾åƒå’Œæ¥è¿‘å®Œç¾çš„å›¾åƒã€‚
- en: Feed the noisy versions of the inputs into the model.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†è¾“å…¥çš„å˜ˆæ‚ç‰ˆæœ¬é¦ˆé€åˆ°æ¨¡å‹ä¸­ã€‚
- en: Evaluate how well the model does at denoising these inputs.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯„ä¼°æ¨¡å‹åœ¨å»å™ªè¿™äº›è¾“å…¥æ—¶çš„è¡¨ç°ã€‚
- en: Use this information to update the model weights.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¿™äº›ä¿¡æ¯æ¥æ›´æ–°æ¨¡å‹æƒé‡ã€‚
- en: To generate new images with a trained model, we begin with a completely random
    input and repeatedly feed it through the model, updating the input on each iteration
    by a small amount based on the model prediction. As weâ€™ll see, there are a number
    of sampling methods that try to streamline this process so that we can generate
    good images with as few steps as possible.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨ç»è¿‡è®­ç»ƒçš„æ¨¡å‹ç”Ÿæˆæ–°å›¾åƒï¼Œæˆ‘ä»¬ä»å®Œå…¨éšæœºçš„è¾“å…¥å¼€å§‹ï¼Œå¹¶é€šè¿‡æ¨¡å‹é‡å¤åœ°å°†å…¶é¦ˆé€åˆ°æ¨¡å‹ä¸­ï¼Œåœ¨æ¯æ¬¡è¿­ä»£ä¸­æ ¹æ®æ¨¡å‹é¢„æµ‹æ›´æ–°è¾“å…¥çš„å°é‡ã€‚æ­£å¦‚æˆ‘ä»¬å°†çœ‹åˆ°çš„ï¼Œæœ‰è®¸å¤šé‡‡æ ·æ–¹æ³•è¯•å›¾ç®€åŒ–è¿™ä¸ªè¿‡ç¨‹ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥å°½å¯èƒ½å°‘çš„æ­¥éª¤ç”Ÿæˆå¥½çš„å›¾åƒã€‚
- en: The Data
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•°æ®
- en: For this example, weâ€™ll use a dataset of images from the Hugging Face Hub- specifically,
    [this collection of 1000 butterfly pictures](https://huggingface.co/datasets/huggan/smithsonian_butterflies_subset).
    Later on, in the projects section, you will see how to use your own data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ¥è‡ªHugging Face Hubçš„å›¾åƒæ•°æ®é›†-å…·ä½“æ¥è¯´ï¼Œ[è¿™ä¸ªåŒ…å«1000å¼ è´è¶å›¾ç‰‡çš„é›†åˆ](https://huggingface.co/datasets/huggan/smithsonian_butterflies_subset)ã€‚åœ¨é¡¹ç›®éƒ¨åˆ†ï¼Œæ‚¨å°†çœ‹åˆ°å¦‚ä½•ä½¿ç”¨è‡ªå·±çš„æ•°æ®ã€‚
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We need to do some preparation before this data can be used to train a model.
    Images are typically represented as a grid of â€˜pixelsâ€™, with color values between
    0 and 255 for each of the three color channels (Red, Green and Blue). To process
    these and make them ready for training, we: - Resize them to a fixed size - (Optional)
    Add some augmentation by randomly flipping them horizontally, effectively doubling
    the size of our dataset - Convert them to a PyTorch tensor (which represents the
    color values as floats between 0 and 1) - Normalize them to have a mean of 0,
    with values between -1 and 1'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä½¿ç”¨è¿™äº›æ•°æ®è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦åšä¸€äº›å‡†å¤‡ã€‚å›¾åƒé€šå¸¸è¡¨ç¤ºä¸ºä¸€ä¸ªâ€œåƒç´ â€ç½‘æ ¼ï¼Œæ¯ä¸ªåƒç´ æœ‰ä¸‰ä¸ªé¢œè‰²é€šé“ï¼ˆçº¢è‰²ã€ç»¿è‰²å’Œè“è‰²ï¼‰çš„é¢œè‰²å€¼åœ¨0åˆ°255ä¹‹é—´ã€‚ä¸ºäº†å¤„ç†è¿™äº›å›¾åƒå¹¶ä½¿å®ƒä»¬å‡†å¤‡å¥½è¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬éœ€è¦ï¼š
    - å°†å®ƒä»¬è°ƒæ•´ä¸ºå›ºå®šå¤§å° - ï¼ˆå¯é€‰ï¼‰é€šè¿‡éšæœºæ°´å¹³ç¿»è½¬æ¥æ·»åŠ ä¸€äº›å¢å¼ºï¼Œæœ‰æ•ˆåœ°ä½¿æˆ‘ä»¬çš„æ•°æ®é›†å¤§å°åŠ å€ - å°†å®ƒä»¬è½¬æ¢ä¸ºPyTorchå¼ é‡ï¼ˆè¡¨ç¤ºé¢œè‰²å€¼ä¸º0åˆ°1ä¹‹é—´çš„æµ®ç‚¹æ•°ï¼‰
    - å°†å®ƒä»¬æ ‡å‡†åŒ–ä¸ºå…·æœ‰å‡å€¼ä¸º0çš„å€¼ï¼Œå€¼åœ¨-1åˆ°1ä¹‹é—´
- en: 'We can do all of this with `torchvision.transforms`:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨`torchvision.transforms`æ¥å®Œæˆæ‰€æœ‰è¿™äº›æ“ä½œï¼š
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we need to create a dataloader to load the data in batches with these
    transforms applied:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªæ•°æ®åŠ è½½å™¨ï¼Œä»¥ä¾¿åŠ è½½åº”ç”¨äº†è¿™äº›è½¬æ¢çš„æ•°æ®æ‰¹æ¬¡ï¼š
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We can check that this worked by loading a single batch and inspecting the images.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡åŠ è½½å•ä¸ªæ‰¹æ¬¡å¹¶æ£€æŸ¥å›¾åƒæ¥æ£€æŸ¥è¿™æ˜¯å¦æœ‰æ•ˆã€‚
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![image](assets/cell-9-output-2.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-9-output-2.png)'
- en: Adding Noise
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ·»åŠ å™ªéŸ³
- en: 'How do we gradually corrupt our data? The most common approach is to add noise
    to the images. The amount of noise we add is controlled by a noise schedule. Different
    papers and approaches tackle this in different ways, which weâ€™ll explore later
    in the chapter. For now, letâ€™s see one common approach in action based on the
    paper [â€œDenoising diffusion probabilistic modelsâ€](https://arxiv.org/abs/2006.11239)
    by Ho et al. In the diffusers library, adding noise is handled by something called
    a scheduler, which takes in a batch of images and a list of â€˜timestepsâ€™ and determines
    how to create the noisy versions of those images:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¦‚ä½•é€æ¸ç ´åæˆ‘ä»¬çš„æ•°æ®ï¼Ÿæœ€å¸¸è§çš„æ–¹æ³•æ˜¯å‘å›¾åƒæ·»åŠ å™ªéŸ³ã€‚æˆ‘ä»¬æ·»åŠ çš„å™ªéŸ³é‡ç”±å™ªéŸ³æ—¶é—´è¡¨æ§åˆ¶ã€‚ä¸åŒçš„è®ºæ–‡å’Œæ–¹æ³•ä»¥ä¸åŒçš„æ–¹å¼å¤„ç†è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å°†åœ¨æœ¬ç« åé¢è¿›è¡Œæ¢è®¨ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ä¸€ç§å¸¸è§çš„æ–¹æ³•ï¼ŒåŸºäºHoç­‰äººçš„è®ºæ–‡â€œå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹â€ã€‚åœ¨æ‰©æ•£å™¨åº“ä¸­ï¼Œæ·»åŠ å™ªéŸ³æ˜¯ç”±ç§°ä¸ºè°ƒåº¦å™¨çš„ä¸œè¥¿å¤„ç†çš„ï¼Œå®ƒæ¥æ”¶ä¸€æ‰¹å›¾åƒå’Œä¸€ä¸ªâ€œæ—¶é—´æ­¥é•¿â€åˆ—è¡¨ï¼Œå¹¶ç¡®å®šå¦‚ä½•åˆ›å»ºè¿™äº›å›¾åƒçš„å˜ˆæ‚ç‰ˆæœ¬ï¼š
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![image](assets/cell-10-output-1.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-10-output-1.png)'
- en: During training, weâ€™ll pick the timesteps at random. The scheduler takes some
    parameters (beta_start and beta_end) which it uses to determine how much noise
    should be present for a given timestep. We will cover schedulers in more detail
    in section X.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†éšæœºé€‰æ‹©æ—¶é—´æ­¥é•¿ã€‚è°ƒåº¦å™¨æ¥æ”¶ä¸€äº›å‚æ•°ï¼ˆbeta_startå’Œbeta_endï¼‰ï¼Œå®ƒç”¨è¿™äº›å‚æ•°æ¥ç¡®å®šç»™å®šæ—¶é—´æ­¥é•¿åº”è¯¥å­˜åœ¨å¤šå°‘å™ªéŸ³ã€‚æˆ‘ä»¬å°†åœ¨ç¬¬XèŠ‚ä¸­æ›´è¯¦ç»†åœ°ä»‹ç»è°ƒåº¦å™¨ã€‚
- en: The UNet
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UNet
- en: UNet is a convolutional neural network invented for tasks such as image segmentation,
    where the desired output has the same spatial extent as the input. It consists
    of a series of â€˜downsamplingâ€™ layers that reduce the spatial size of the input,
    followed by a series of â€˜upsamplingâ€™ layers that increase the spatial extent of
    the input again. The downsampling layers are also typically followed by a â€˜skip
    connectionâ€™ that connects the downsampling layerâ€™s output to the upsampling layerâ€™s
    input. This allows the upsampling layers to â€˜seeâ€™ the higher-resolution representations
    from earlier in the network, which is useful for tasks with image-like outputs
    where this high-resolution information is especially useful.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: UNetæ˜¯ä¸€ç§å·ç§¯ç¥ç»ç½‘ç»œï¼Œç”¨äºè¯¸å¦‚å›¾åƒåˆ†å‰²ä¹‹ç±»çš„ä»»åŠ¡ï¼Œå…¶ä¸­æœŸæœ›çš„è¾“å‡ºä¸è¾“å…¥å…·æœ‰ç›¸åŒçš„ç©ºé—´èŒƒå›´ã€‚å®ƒç”±ä¸€ç³»åˆ—â€œä¸‹é‡‡æ ·â€å±‚ç»„æˆï¼Œç”¨äºå‡å°è¾“å…¥çš„ç©ºé—´å¤§å°ï¼Œç„¶åæ˜¯ä¸€ç³»åˆ—â€œä¸Šé‡‡æ ·â€å±‚ï¼Œç”¨äºå†æ¬¡å¢åŠ è¾“å…¥çš„ç©ºé—´èŒƒå›´ã€‚ä¸‹é‡‡æ ·å±‚é€šå¸¸ä¹Ÿåé¢è·Ÿç€ä¸€ä¸ªâ€œè·³è·ƒè¿æ¥â€ï¼Œå°†ä¸‹é‡‡æ ·å±‚çš„è¾“å‡ºè¿æ¥åˆ°ä¸Šé‡‡æ ·å±‚çš„è¾“å…¥ã€‚è¿™å…è®¸ä¸Šé‡‡æ ·å±‚â€œçœ‹åˆ°â€ç½‘ç»œæ—©æœŸçš„é«˜åˆ†è¾¨ç‡è¡¨ç¤ºï¼Œè¿™å¯¹äºå…·æœ‰å›¾åƒæ ·å¼è¾“å‡ºçš„ä»»åŠ¡ç‰¹åˆ«æœ‰ç”¨ï¼Œå…¶ä¸­è¿™äº›é«˜åˆ†è¾¨ç‡ä¿¡æ¯å°¤ä¸ºé‡è¦ã€‚
- en: The UNet architecture used in the diffusers library is more advanced than the
    [original UNet proposed in 2015](https://arxiv.org/abs/1505.04597) by Ronneberger
    et al, with additions like attention and residual blocks. Weâ€™ll take a closer
    look later, but the key feature here is that it can take in an input (the noisy
    image) and produce a prediction that is the same shape (the predicted noise).
    For diffusion models, the UNet typically also takes in the timestep as additional
    conditioning, which again we will explore in the UNet deep dive section.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰©æ•£åº“ä¸­ä½¿ç”¨çš„UNetæ¶æ„æ¯”2015å¹´Ronnebergerç­‰äººæå‡ºçš„[åŸå§‹UNet](https://arxiv.org/abs/1505.04597)æ›´å…ˆè¿›ï¼Œå¢åŠ äº†æ³¨æ„åŠ›å’Œæ®‹å·®å—ç­‰åŠŸèƒ½ã€‚æˆ‘ä»¬ç¨åä¼šæ›´ä»”ç»†åœ°çœ‹ä¸€ä¸‹ï¼Œä½†è¿™é‡Œçš„å…³é”®ç‰¹ç‚¹æ˜¯å®ƒå¯ä»¥æ¥å—ä¸€ä¸ªè¾“å…¥ï¼ˆå˜ˆæ‚çš„å›¾åƒï¼‰å¹¶äº§ç”Ÿä¸€ä¸ªå½¢çŠ¶ç›¸åŒçš„é¢„æµ‹ï¼ˆé¢„æµ‹çš„å™ªéŸ³ï¼‰ã€‚å¯¹äºæ‰©æ•£æ¨¡å‹ï¼ŒUNeté€šå¸¸è¿˜æ¥å—æ—¶é—´æ­¥ä½œä¸ºé¢å¤–çš„æ¡ä»¶ï¼Œæˆ‘ä»¬å°†åœ¨UNetæ·±å…¥æ¢è®¨éƒ¨åˆ†å†æ¬¡æ¢è®¨è¿™ä¸€ç‚¹ã€‚
- en: 'Hereâ€™s how we might create a UNet and feed our batch of noisy images through
    it:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘ä»¬å¦‚ä½•åˆ›å»ºä¸€ä¸ªUNetå¹¶å°†æˆ‘ä»¬çš„ä¸€æ‰¹å˜ˆæ‚å›¾åƒè¾“å…¥å…¶ä¸­çš„æ–¹æ³•ï¼š
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that the output is the same shape as the input, which is exactly what we
    want.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„è¾“å‡ºä¸è¾“å…¥çš„å½¢çŠ¶ç›¸åŒï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬æƒ³è¦çš„ã€‚
- en: Training
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒ
- en: 'Now that we have our model and our data ready, we can train it. Weâ€™ll use the
    AdamW optimizer with a learning rate of 3e-4\. For each training step, we:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬çš„æ¨¡å‹å’Œæ•°æ®å‡†å¤‡å¥½äº†ï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹è®­ç»ƒã€‚æˆ‘ä»¬å°†ä½¿ç”¨å­¦ä¹ ç‡ä¸º3e-4çš„AdamWä¼˜åŒ–å™¨ã€‚å¯¹äºæ¯ä¸ªè®­ç»ƒæ­¥éª¤ï¼Œæˆ‘ä»¬ï¼š
- en: Load a batch of images.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŠ è½½ä¸€æ‰¹å›¾åƒã€‚
- en: Add noise to the images, choosing random timesteps to determine how much noise
    is added.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‘å›¾åƒæ·»åŠ å™ªéŸ³ï¼Œé€‰æ‹©éšæœºæ—¶é—´æ­¥æ¥ç¡®å®šæ·»åŠ å¤šå°‘å™ªéŸ³ã€‚
- en: Feed the noisy images into the model.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†å˜ˆæ‚å›¾åƒè¾“å…¥æ¨¡å‹ã€‚
- en: Calculate the loss, which is the mean squared error between the modelâ€™s predictions
    and the target - which in this case is the *noise* that we added to the images.
    This is called the noise or â€˜epsilonâ€™ objective. You can find more information
    on the different training objectives in section X.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¡ç®—æŸå¤±ï¼Œå³æ¨¡å‹é¢„æµ‹ä¸ç›®æ ‡ä¹‹é—´çš„å‡æ–¹è¯¯å·® - åœ¨è¿™ç§æƒ…å†µä¸‹æ˜¯æˆ‘ä»¬æ·»åŠ åˆ°å›¾åƒä¸­çš„*å™ªéŸ³*ã€‚è¿™è¢«ç§°ä¸ºå™ªéŸ³æˆ–â€œepsilonâ€ç›®æ ‡ã€‚æ‚¨å¯ä»¥åœ¨ç¬¬XèŠ‚ä¸­æ‰¾åˆ°æœ‰å…³ä¸åŒè®­ç»ƒç›®æ ‡çš„æ›´å¤šä¿¡æ¯ã€‚
- en: Backpropagate the loss and update the model weights with the optimizer.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åå‘ä¼ æ’­æŸå¤±å¹¶ä½¿ç”¨ä¼˜åŒ–å™¨æ›´æ–°æ¨¡å‹æƒé‡ã€‚
- en: 'Hereâ€™s what all of that looks like in code:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»£ç ä¸­ï¼Œæ‰€æœ‰è¿™äº›çœ‹èµ·æ¥æ˜¯è¿™æ ·çš„ï¼š
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'It takes an hour or so to run the above code on a GPU, so get some tea while
    you wait or lower the number of epochs. Hereâ€™s what the loss curve looks like
    after training:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨GPUä¸Šè¿è¡Œä¸Šè¿°ä»£ç å¤§çº¦éœ€è¦ä¸€ä¸ªå°æ—¶ï¼Œæ‰€ä»¥åœ¨ç­‰å¾…æ—¶å–æ¯èŒ¶æˆ–è€…å‡å°‘æ—¶ä»£çš„æ•°é‡ã€‚è®­ç»ƒåæŸå¤±æ›²çº¿å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![image](assets/cell-12-output-1.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-12-output-1.png)'
- en: The loss curve trends downwards as the model learns to denoise the images. The
    curve is fairly noisy, thanks to different amounts of noise being added to the
    images based on the random sampling of timesteps for each iteration. It is hard
    to tell just by looking at the mean squared error of the noise predictions whether
    this model will be any good at generating samples, so letâ€™s move on to the next
    section and see how well it does.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€æ¨¡å‹å­¦ä¹ å»å™ªå›¾åƒï¼ŒæŸå¤±æ›²çº¿å‘ˆä¸‹é™è¶‹åŠ¿ã€‚ç”±äºæ ¹æ®æ¯æ¬¡è¿­ä»£ä¸­éšæœºæ—¶é—´æ­¥çš„éšæœºæŠ½æ ·å‘å›¾åƒæ·»åŠ ä¸åŒæ•°é‡çš„å™ªéŸ³ï¼Œæ›²çº¿ç›¸å½“å˜ˆæ‚ã€‚ä»…é€šè¿‡è§‚å¯Ÿå™ªéŸ³é¢„æµ‹çš„å‡æ–¹è¯¯å·®å¾ˆéš¾åˆ¤æ–­è¿™ä¸ªæ¨¡å‹æ˜¯å¦èƒ½å¤Ÿå¾ˆå¥½åœ°ç”Ÿæˆæ ·æœ¬ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬ç»§ç»­ä¸‹ä¸€èŠ‚ï¼Œçœ‹çœ‹å®ƒçš„è¡¨ç°å¦‚ä½•ã€‚
- en: Sampling
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é‡‡æ ·
- en: 'The diffusers library uses the idea of â€˜pipelinesâ€™ which bundle together all
    of the components needed to generate samples with a diffusion model:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰©æ•£åº“ä½¿ç”¨äº†â€œç®¡é“â€çš„æ¦‚å¿µï¼Œå°†ç”Ÿæˆæ‰©æ•£æ¨¡å‹æ ·æœ¬æ‰€éœ€çš„æ‰€æœ‰ç»„ä»¶æ†ç»‘åœ¨ä¸€èµ·ï¼š
- en: '[PRE12]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![image](assets/cell-13-output-2.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-13-output-2.png)'
- en: 'Of course, offloading the job of creating samples to the pipeline doesnâ€™t really
    show us what is going on. So, here is a simple sampling loop that shows how the
    model is gradually refining the input image:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œå°†åˆ›å»ºæ ·æœ¬çš„å·¥ä½œäº¤ç»™ç®¡é“å¹¶ä¸èƒ½çœŸæ­£å±•ç¤ºå‡ºæˆ‘ä»¬æ­£åœ¨è¿›è¡Œçš„å·¥ä½œã€‚å› æ­¤ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªç®€å•çš„é‡‡æ ·å¾ªç¯ï¼Œå±•ç¤ºäº†æ¨¡å‹å¦‚ä½•é€æ¸æ”¹è¿›è¾“å…¥å›¾åƒï¼š
- en: '[PRE13]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![image](assets/cell-14-output-1.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-14-output-1.png)'
- en: This is the same code we used at the beginning of the chapter to illustrate
    the idea of iterative refinement, but hopefully, now you have a better understanding
    of what is going on here. We start with a completely random input, which is then
    refined by the model in a series of steps. Each step is a small update to the
    input, based on the modelâ€™s prediction for the noise at that timestep. Weâ€™re still
    abstracting away some complexity behind the call to `pipeline.scheduler.step()`
    - in a later chapter we will dive deeper into different sampling methods and how
    they work.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸æˆ‘ä»¬åœ¨æœ¬ç« å¼€å¤´ä½¿ç”¨çš„ä»£ç ç›¸åŒï¼Œç”¨æ¥è¯´æ˜è¿­ä»£æ”¹è¿›çš„æ¦‚å¿µï¼Œä½†å¸Œæœ›ç°åœ¨ä½ å¯¹è¿™é‡Œå‘ç”Ÿçš„äº‹æƒ…æœ‰äº†æ›´å¥½çš„ç†è§£ã€‚æˆ‘ä»¬ä»ä¸€ä¸ªå®Œå…¨éšæœºçš„è¾“å…¥å¼€å§‹ï¼Œç„¶ååœ¨ä¸€ç³»åˆ—æ­¥éª¤ä¸­ç”±æ¨¡å‹è¿›è¡Œæ”¹è¿›ã€‚æ¯ä¸€æ­¥éƒ½æ˜¯å¯¹è¾“å…¥çš„å°æ›´æ–°ï¼ŒåŸºäºæ¨¡å‹å¯¹è¯¥æ—¶é—´æ­¥çš„å™ªéŸ³çš„é¢„æµ‹ã€‚æˆ‘ä»¬ä»ç„¶åœ¨`pipeline.scheduler.step()`çš„è°ƒç”¨èƒŒåæŠ½è±¡äº†ä¸€äº›å¤æ‚æ€§
    - åœ¨åé¢çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ›´æ·±å…¥åœ°æ¢è®¨ä¸åŒçš„é‡‡æ ·æ–¹æ³•ä»¥åŠå®ƒä»¬çš„å·¥ä½œåŸç†ã€‚
- en: Evaluation
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯„ä¼°
- en: Generative model performance can be evaluated using FID scores (FrÃ©chet Inception
    Distance). FID scores measure how closely generated samples match real-world samples
    by comparing statistics between feature maps extracted from both sets of data
    using a pre-trained neural network. The lower the score, the better the quality
    and realism of generated images produced by a given model. FID scores are popular
    due to their ability to provide an â€˜objectiveâ€™ comparison metric for different
    types of generative networks without relying on human judgment.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨FIDåˆ†æ•°ï¼ˆFrÃ©chet Inception Distanceï¼‰æ¥è¯„ä¼°ç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½ã€‚FIDåˆ†æ•°é€šè¿‡æ¯”è¾ƒä»é¢„è®­ç»ƒçš„ç¥ç»ç½‘ç»œä¸­æå–çš„ç‰¹å¾å›¾ä¹‹é—´çš„ç»Ÿè®¡æ•°æ®ï¼Œè¡¡é‡ç”Ÿæˆæ ·æœ¬ä¸çœŸå®æ ·æœ¬çš„ç›¸ä¼¼ç¨‹åº¦ã€‚åˆ†æ•°è¶Šä½ï¼Œç»™å®šæ¨¡å‹ç”Ÿæˆçš„å›¾åƒçš„è´¨é‡å’Œé€¼çœŸåº¦å°±è¶Šå¥½ã€‚FIDåˆ†æ•°å› å…¶èƒ½å¤Ÿæä¾›å¯¹ä¸åŒç±»å‹ç”Ÿæˆç½‘ç»œçš„â€œå®¢è§‚â€æ¯”è¾ƒæŒ‡æ ‡è€Œå—åˆ°æ¬¢è¿ï¼Œè€Œæ— éœ€ä¾èµ–äººç±»åˆ¤æ–­ã€‚
- en: 'As convenient as FID scores are, there are some important caveats to be aware
    of:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡FIDåˆ†æ•°å¾ˆæ–¹ä¾¿ï¼Œä½†ä¹Ÿæœ‰ä¸€äº›é‡è¦çš„æ³¨æ„äº‹é¡¹éœ€è¦æ³¨æ„ï¼š
- en: The FID score for a given model depends on the number of samples used to calculate
    it, so when comparing between model,s we need to make sure both reported scores
    are calculated using the same number of samples. Common practice is to use 50,000
    samples for this purpose, although to save time, you may evaluate on a smaller
    number of samples during development and only do the full evaluation once youâ€™re
    ready to publish the results.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç»™å®šæ¨¡å‹çš„FIDåˆ†æ•°å–å†³äºç”¨äºè®¡ç®—å®ƒçš„æ ·æœ¬æ•°é‡ï¼Œå› æ­¤åœ¨æ¨¡å‹ä¹‹é—´è¿›è¡Œæ¯”è¾ƒæ—¶ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿æŠ¥å‘Šçš„åˆ†æ•°éƒ½æ˜¯ä½¿ç”¨ç›¸åŒæ•°é‡çš„æ ·æœ¬è®¡ç®—çš„ã€‚é€šå¸¸åšæ³•æ˜¯åœ¨è¿™ä¸ªç›®çš„ä¸Šä½¿ç”¨50,000ä¸ªæ ·æœ¬ï¼Œå°½ç®¡ä¸ºäº†èŠ‚çœæ—¶é—´ï¼Œæ‚¨å¯èƒ½åœ¨å¼€å‘è¿‡ç¨‹ä¸­è¯„ä¼°è¾ƒå°‘æ•°é‡çš„æ ·æœ¬ï¼Œåªæœ‰åœ¨å‡†å¤‡å‘å¸ƒç»“æœæ—¶æ‰è¿›è¡Œå®Œæ•´è¯„ä¼°ã€‚
- en: When calculating FID, images are resized to 299px square images. This makes
    it less useful as a metric for extremely low-res or high-res images. There are
    also minor differences between how resizing is handled by different deep learning
    frameworks, which can result in small differences in the FID score! We recommend
    using a library such as `clean-fid` to standardize the FID calculation.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨è®¡ç®—FIDæ—¶ï¼Œå›¾åƒè¢«è°ƒæ•´ä¸º299åƒç´ çš„æ­£æ–¹å½¢å›¾åƒã€‚è¿™ä½¿å¾—å®ƒå¯¹äºæä½åˆ†è¾¨ç‡æˆ–é«˜åˆ†è¾¨ç‡å›¾åƒçš„åº¦é‡å˜å¾—ä¸å¤ªæœ‰ç”¨ã€‚ä¸åŒçš„æ·±åº¦å­¦ä¹ æ¡†æ¶å¤„ç†è°ƒæ•´å¤§å°çš„æ–¹å¼ä¹Ÿæœ‰ç»†å¾®å·®å¼‚ï¼Œè¿™å¯èƒ½å¯¼è‡´FIDåˆ†æ•°æœ‰å°çš„å·®å¼‚ï¼æˆ‘ä»¬å»ºè®®ä½¿ç”¨`clean-fid`è¿™æ ·çš„åº“æ¥æ ‡å‡†åŒ–FIDè®¡ç®—ã€‚
- en: The network used as a feature extractor for FID is typically a model trained
    on the Imagenet classification task. When generating images in a different domain,
    the features learned by this model may be less useful. A more accurate approach
    would be to somehow train a classification network on domain-specific data first,
    but this would make it harder to compare scores between different papers and approaches,
    so for now the imagenet model is the standard choice.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”¨ä½œFIDç‰¹å¾æå–å™¨çš„ç½‘ç»œé€šå¸¸æ˜¯åœ¨Imagenetåˆ†ç±»ä»»åŠ¡ä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚åœ¨ç”Ÿæˆä¸åŒé¢†åŸŸçš„å›¾åƒæ—¶ï¼Œè¿™ä¸ªæ¨¡å‹å­¦åˆ°çš„ç‰¹å¾å¯èƒ½ä¸å¤ªæœ‰ç”¨ã€‚æ›´å‡†ç¡®çš„æ–¹æ³•æ˜¯åœ¨ç‰¹å®šé¢†åŸŸçš„æ•°æ®ä¸Šè®­ç»ƒä¸€ä¸ªåˆ†ç±»ç½‘ç»œï¼Œä½†è¿™ä¼šä½¿å¾—åœ¨ä¸åŒè®ºæ–‡å’Œæ–¹æ³•ä¹‹é—´æ¯”è¾ƒåˆ†æ•°å˜å¾—æ›´åŠ å›°éš¾ï¼Œæ‰€ä»¥ç›®å‰Imagenetæ¨¡å‹æ˜¯æ ‡å‡†é€‰æ‹©ã€‚
- en: If you save generated samples for later evaluation, the format and compression
    can again affect the FID score. Avoid low-quality JPEG images where possible.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä¿å­˜ç”Ÿæˆçš„æ ·æœ¬ä»¥ä¾›ä»¥åè¯„ä¼°ï¼Œæ ¼å¼å’Œå‹ç¼©ä¹Ÿä¼šå½±å“FIDåˆ†æ•°ã€‚å°½é‡é¿å…ä½è´¨é‡çš„JPEGå›¾åƒã€‚
- en: Even if you account for all these caveats, FID scores are just a rough measure
    of quality and do not perfectly capture the nuances of what makes images look
    more â€˜realâ€™. So, use them to get an idea of how one model performs relative to
    another but also look at the actual images generated by each model to get a better
    sense of how they compare. Human preference is still the gold standard for quality
    in what is ultimately a fairly subjective field!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿è€ƒè™‘äº†æ‰€æœ‰è¿™äº›è­¦å‘Šï¼ŒFIDåˆ†æ•°åªæ˜¯è´¨é‡çš„ç²—ç•¥åº¦é‡ï¼Œä¸èƒ½å®Œç¾åœ°æ•æ‰ä½¿å›¾åƒçœ‹èµ·æ¥æ›´â€œçœŸå®â€çš„å¾®å¦™ä¹‹å¤„ã€‚å› æ­¤ï¼Œç”¨å®ƒä»¬æ¥äº†è§£ä¸€ä¸ªæ¨¡å‹ç›¸å¯¹äºå¦ä¸€ä¸ªæ¨¡å‹çš„è¡¨ç°ï¼Œä½†ä¹Ÿè¦çœ‹çœ‹æ¯ä¸ªæ¨¡å‹ç”Ÿæˆçš„å®é™…å›¾åƒï¼Œä»¥æ›´å¥½åœ°äº†è§£å®ƒä»¬çš„æ¯”è¾ƒã€‚äººç±»åå¥½ä»ç„¶æ˜¯è´¨é‡çš„é»„é‡‘æ ‡å‡†ï¼Œæœ€ç»ˆè¿™æ˜¯ä¸€ä¸ªç›¸å½“ä¸»è§‚çš„é¢†åŸŸï¼
- en: 'In Depth: Noise Schedules'
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ·±å…¥ï¼šå™ªéŸ³æ—¶é—´è¡¨
- en: In the training example above, one of the steps was â€˜add noise, in different
    amountsâ€™. We achieved this by picking a random timestep between 0 and 1000 and
    then relying on the scheduler to add the appropriate amount of noise. Likewise,
    during sampling, we again relied on the scheduler to tell us which timesteps to
    use and how to move from one to the next given the model predictions. It turns
    out that choosing how much noise to add is an important design decision that can
    drastically affect the performance of a given model. In this section, weâ€™ll see
    why this is the case and explore some of the different approaches that are used
    in practice.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šé¢çš„è®­ç»ƒç¤ºä¾‹ä¸­ï¼Œå…¶ä¸­ä¸€æ­¥æ˜¯â€œæ·»åŠ ä¸åŒæ•°é‡çš„å™ªéŸ³â€ã€‚æˆ‘ä»¬é€šè¿‡åœ¨0åˆ°1000ä¹‹é—´é€‰æ‹©ä¸€ä¸ªéšæœºæ—¶é—´æ­¥é•¿æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œç„¶åä¾é è°ƒåº¦ç¨‹åºæ·»åŠ é€‚å½“æ•°é‡çš„å™ªéŸ³ã€‚åŒæ ·ï¼Œåœ¨é‡‡æ ·è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å†æ¬¡ä¾é è°ƒåº¦ç¨‹åºå‘Šè¯‰æˆ‘ä»¬ä½¿ç”¨å“ªäº›æ—¶é—´æ­¥é•¿ä»¥åŠå¦‚ä½•æ ¹æ®æ¨¡å‹é¢„æµ‹ä»ä¸€ä¸ªæ—¶é—´æ­¥é•¿ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥é•¿ã€‚äº‹å®è¯æ˜ï¼Œé€‰æ‹©æ·»åŠ å¤šå°‘å™ªéŸ³æ˜¯ä¸€ä¸ªé‡è¦çš„è®¾è®¡å†³ç­–ï¼Œå¯ä»¥æå¤§åœ°å½±å“ç»™å®šæ¨¡å‹çš„æ€§èƒ½ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°ä¸ºä»€ä¹ˆä¼šè¿™æ ·ï¼Œå¹¶æ¢è®¨å®è·µä¸­ä½¿ç”¨çš„ä¸€äº›ä¸åŒæ–¹æ³•ã€‚
- en: Why Add Noise?
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆè¦æ·»åŠ å™ªéŸ³ï¼Ÿ
- en: At the start of this chapter, we said that the key idea behind diffusion models
    is that of iterative refinement. During training, we â€˜corruptâ€™ an input by different
    amounts. During inference, we begin with a â€˜maximally corruptedâ€™ input and iteratively
    â€˜de-corruptâ€™ it, in the hopes that we will eventually end up with a nice final
    result.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« çš„å¼€å¤´ï¼Œæˆ‘ä»¬è¯´æ‰©æ•£æ¨¡å‹èƒŒåçš„å…³é”®æ€æƒ³æ˜¯è¿­ä»£æ”¹è¿›ã€‚åœ¨è®­ç»ƒæœŸé—´ï¼Œæˆ‘ä»¬é€šè¿‡ä¸åŒçš„æ–¹å¼â€œè…è´¥â€è¾“å…¥ã€‚åœ¨æ¨æ–­æœŸé—´ï¼Œæˆ‘ä»¬ä»â€œæœ€å¤§ç¨‹åº¦ä¸Šè…è´¥â€çš„è¾“å…¥å¼€å§‹ï¼Œè¿­ä»£åœ°â€œå»è…è´¥â€å®ƒï¼Œå¸Œæœ›æœ€ç»ˆå¾—åˆ°ä¸€ä¸ªä¸é”™çš„æœ€ç»ˆç»“æœã€‚
- en: 'So far, weâ€™ve focused on one specific kind of â€˜corruptionâ€™: adding Gaussian
    noise. One reason for this is the theoretical underpinnings of diffusion models
    - if we use a different corruption method we are no longer technically doing â€˜diffusionâ€™!
    However, a paper titled [*Cold Diffusion*](https://arxiv.org/abs/2208.09392) by
    Bansal et al dramatically demonstrated that we do not necessarily need to constrain
    ourselves to this method just for theoretical convenience. They showed that a
    diffusion-model-like approach works for many different â€˜corruptionâ€™ methods (see
    [FigureÂ 1-1](#c4_f1)). More recently, models like [MUSE](https://arxiv.org/abs/2301.00704),
    [MaskGIT](https://arxiv.org/abs/2202.04200) and [PAELLA](https://arxiv.org/abs/2211.07292)
    have used random token masking or replacement as an equivalent â€˜corruptionâ€™ method
    for quantized data - that is, data that is represented by discrete tokens rather
    than continuous values.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»ä¸“æ³¨äºä¸€ç§ç‰¹å®šçš„â€œè…è´¥â€ï¼šæ·»åŠ é«˜æ–¯å™ªå£°ã€‚è¿™æ ·åšçš„ä¸€ä¸ªåŸå› æ˜¯æ‰©æ•£æ¨¡å‹çš„ç†è®ºåŸºç¡€ - å¦‚æœæˆ‘ä»¬ä½¿ç”¨ä¸åŒçš„è…è´¥æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨æŠ€æœ¯ä¸Šå°±ä¸å†åšâ€œæ‰©æ•£â€äº†ï¼ç„¶è€Œï¼ŒBansalç­‰äººçš„ä¸€ç¯‡é¢˜ä¸º[*Cold
    Diffusion*](https://arxiv.org/abs/2208.09392)çš„è®ºæ–‡æˆå‰§æ€§åœ°è¡¨æ˜ï¼Œæˆ‘ä»¬ä¸ä¸€å®šéœ€è¦ä»…ä»…å‡ºäºç†è®ºä¸Šçš„ä¾¿åˆ©è€Œé™åˆ¶è‡ªå·±ä½¿ç”¨è¿™ç§æ–¹æ³•ã€‚ä»–ä»¬è¡¨æ˜ï¼Œç±»ä¼¼æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•é€‚ç”¨äºè®¸å¤šä¸åŒçš„â€œè…è´¥â€æ–¹æ³•ï¼ˆè§[å›¾1-1](#c4_f1)ï¼‰ã€‚æœ€è¿‘ï¼Œåƒ[MUSE](https://arxiv.org/abs/2301.00704)ã€[MaskGIT](https://arxiv.org/abs/2202.04200)å’Œ[PAELLA](https://arxiv.org/abs/2211.07292)è¿™æ ·çš„æ¨¡å‹å·²ç»ä½¿ç”¨äº†éšæœºæ ‡è®°å±è”½æˆ–æ›¿æ¢ä½œä¸ºç­‰æ•ˆçš„â€œè…è´¥â€æ–¹æ³•ï¼Œç”¨äºé‡åŒ–æ•°æ®
    - ä¹Ÿå°±æ˜¯è¯´ï¼Œç”¨ç¦»æ•£æ ‡è®°è€Œä¸æ˜¯è¿ç»­å€¼è¡¨ç¤ºçš„æ•°æ®ã€‚
- en: '![image.png](assets/84ca400c-1-image.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![image.png](assets/84ca400c-1-image.png)'
- en: Figure 1-1\. Illustration of the different degradations used in the Cold Diffusion
    Paper
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾1-1ã€‚Cold Diffusion Paperä¸­ä½¿ç”¨çš„ä¸åŒé€€åŒ–çš„ç¤ºæ„å›¾
- en: 'Nonetheless, adding noise remains the most popular approach for several reasons:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å¦‚æ­¤ï¼Œå‡ºäºå‡ ä¸ªåŸå› ï¼Œæ·»åŠ å™ªéŸ³ä»ç„¶æ˜¯æœ€å—æ¬¢è¿çš„æ–¹æ³•ï¼š
- en: We can easily control the amount of noise added, giving a smooth transition
    from â€˜perfectâ€™ to â€˜completely corruptedâ€™. This is not the case for something like
    reducing the resolution of an image, which may result in â€˜discreteâ€™ transitions.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥è½»æ¾æ§åˆ¶æ·»åŠ çš„å™ªéŸ³é‡ï¼Œä»â€œå®Œç¾â€åˆ°â€œå®Œå…¨æŸåâ€å¹³ç¨³è¿‡æ¸¡ã€‚è¿™å¯¹äºåƒå‡å°‘å›¾åƒåˆ†è¾¨ç‡è¿™æ ·çš„äº‹æƒ…å¹¶ä¸é€‚ç”¨ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´â€œç¦»æ•£â€çš„è¿‡æ¸¡ã€‚
- en: We can have many valid random starting points for inference, unlike some methods
    which may only have a limited number of possible initial (fully corrupted) states,
    such as a completely black image or a single-pixel image.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥æœ‰è®¸å¤šæœ‰æ•ˆçš„éšæœºèµ·å§‹ç‚¹è¿›è¡Œæ¨æ–­ï¼Œä¸åƒä¸€äº›æ–¹æ³•å¯èƒ½åªæœ‰æœ‰é™æ•°é‡çš„å¯èƒ½çš„åˆå§‹ï¼ˆå®Œå…¨æŸåï¼‰çŠ¶æ€ï¼Œæ¯”å¦‚å®Œå…¨é»‘è‰²çš„å›¾åƒæˆ–å•åƒç´ å›¾åƒã€‚
- en: So, for the moment at least, weâ€™ll stick with adding noise as our corruption
    method. Next, letâ€™s take a closer look at how we add noise to our images.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œè‡³å°‘ç›®å‰ï¼Œæˆ‘ä»¬å°†åšæŒæ·»åŠ å™ªéŸ³ä½œä¸ºæˆ‘ä»¬çš„æŸåæ–¹æ³•ã€‚æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬æ›´ä»”ç»†åœ°çœ‹çœ‹å¦‚ä½•å‘å›¾åƒæ·»åŠ å™ªéŸ³ã€‚
- en: Starting Simple
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¼€å§‹ç®€å•
- en: We have some images (x) and weâ€™d like to combine them somehow with some random
    noise.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰ä¸€äº›å›¾åƒï¼ˆxï¼‰ï¼Œæˆ‘ä»¬æƒ³ä»¥æŸç§æ–¹å¼å°†å®ƒä»¬ä¸ä¸€äº›éšæœºå™ªéŸ³ç»“åˆèµ·æ¥ã€‚
- en: '[PRE14]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'One way we could do this is to linearly interpolate (lerp) between them by
    some amount. This gives us a function that smoothly transitions from the original
    image x to pure noise as the â€˜amountâ€™ varies from 0 to 1:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çº¿æ€§æ’å€¼ï¼ˆlerpï¼‰å®ƒä»¬ä¹‹é—´çš„ä¸€äº›é‡ã€‚è¿™ç»™æˆ‘ä»¬ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒåœ¨â€œé‡â€ä»0åˆ°1å˜åŒ–æ—¶ï¼Œä»åŸå§‹å›¾åƒxå¹³ç¨³è¿‡æ¸¡åˆ°çº¯å™ªéŸ³ï¼š
- en: '[PRE15]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Letâ€™s see this in action on a batch of data, with the amount of noise varying
    from 0 to 1:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åœ¨ä¸€æ‰¹æ•°æ®ä¸Šçœ‹çœ‹è¿™ä¸ªè¿‡ç¨‹ï¼Œå™ªéŸ³çš„é‡ä»0åˆ°1å˜åŒ–ï¼š
- en: '[PRE16]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![image](assets/cell-17-output-1.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-17-output-1.png)'
- en: 'This seems to be doing exactly what we want, smoothly transitioning from the
    original image to pure noise. Now, weâ€™ve created a noise schedule here that takes
    in a value for â€˜amountâ€™ from 0 to 1\. This is called the â€˜continuous timeâ€™ approach,
    where we represent the full path on a time scale from 0 to 1\. Other approaches
    use a discrete time approach, with some large integer number of â€˜timestepsâ€™ used
    to define the noise scheduler. We can wrap our function into a class that converts
    from continuous time to discrete timesteps and adds noise appropriately:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¼¼ä¹æ­£æ˜¯æˆ‘ä»¬æƒ³è¦çš„ï¼Œä»åŸå§‹å›¾åƒå¹³ç¨³è¿‡æ¸¡åˆ°çº¯å™ªéŸ³ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œåˆ›å»ºäº†ä¸€ä¸ªå™ªéŸ³æ—¶é—´è¡¨ï¼Œå®ƒæ¥å—ä»0åˆ°1çš„â€œé‡â€å€¼ã€‚è¿™è¢«ç§°ä¸ºâ€œè¿ç»­æ—¶é—´â€æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨æ—¶é—´å°ºåº¦ä¸Šè¡¨ç¤ºä»0åˆ°1çš„å®Œæ•´è·¯å¾„ã€‚å…¶ä»–æ–¹æ³•ä½¿ç”¨ç¦»æ•£æ—¶é—´æ–¹æ³•ï¼Œä½¿ç”¨ä¸€äº›å¤§æ•´æ•°çš„â€œæ—¶é—´æ­¥é•¿â€æ¥å®šä¹‰å™ªéŸ³è°ƒåº¦å™¨ã€‚æˆ‘ä»¬å¯ä»¥å°†æˆ‘ä»¬çš„å‡½æ•°å°è£…æˆä¸€ä¸ªç±»ï¼Œå°†è¿ç»­æ—¶é—´è½¬æ¢ä¸ºç¦»æ•£æ—¶é—´æ­¥é•¿ï¼Œå¹¶é€‚å½“æ·»åŠ å™ªéŸ³ï¼š
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![image](assets/cell-18-output-1.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-18-output-1.png)'
- en: 'Now we have something that we can directly compare to the schedulers used in
    the diffusers library, such as the DDPMScheduler we used during training. Letâ€™s
    see how it compares:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€äº›å¯ä»¥ç›´æ¥ä¸æ‰©æ•£åº“ä¸­ä½¿ç”¨çš„è°ƒåº¦å™¨è¿›è¡Œæ¯”è¾ƒçš„ä¸œè¥¿ï¼Œæ¯”å¦‚æˆ‘ä»¬åœ¨è®­ç»ƒä¸­ä½¿ç”¨çš„DDPMSchedulerã€‚è®©æˆ‘ä»¬çœ‹çœ‹å®ƒæ˜¯å¦‚ä½•æ¯”è¾ƒçš„ï¼š
- en: '[PRE18]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![image](assets/cell-19-output-1.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-19-output-1.png)'
- en: The Maths
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•°å­¦
- en: There are many competing notations and approaches in the literature. For example,
    some papers parametrize the noise schedule in *continuous-time* where t runs from
    0 (no noise) to 1 (fully corrupted) - just like our `corrupt` function in the
    previous section. Others use a *discrete-time* approach with integer timesteps
    running from 0 to some large number T, typically 1000\. It is possible to convert
    between these two approaches the way we did with our `SimpleScheduler` class -
    just make sure youâ€™re consistent when comparing different models. Weâ€™ll stick
    with the discrete-time approach here.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: æ–‡çŒ®ä¸­æœ‰è®¸å¤šç«äº‰çš„ç¬¦å·å’Œæ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œä¸€äº›è®ºæ–‡å°†å™ªéŸ³æ—¶é—´è¡¨å‚æ•°åŒ–ä¸ºâ€œè¿ç»­æ—¶é—´â€ï¼Œå…¶ä¸­tä»0ï¼ˆæ— å™ªéŸ³ï¼‰åˆ°1ï¼ˆå®Œå…¨æŸåï¼‰- å°±åƒæˆ‘ä»¬åœ¨ä¸Šä¸€èŠ‚ä¸­çš„`corrupt`å‡½æ•°ä¸€æ ·ã€‚å…¶ä»–äººä½¿ç”¨â€œç¦»æ•£æ—¶é—´â€æ–¹æ³•ï¼Œå…¶ä¸­æ•´æ•°æ—¶é—´æ­¥é•¿ä»0åˆ°æŸä¸ªå¤§æ•°Tï¼Œé€šå¸¸ä¸º1000ã€‚å¯ä»¥åƒæˆ‘ä»¬çš„`SimpleScheduler`ç±»ä¸€æ ·åœ¨è¿™ä¸¤ç§æ–¹æ³•ä¹‹é—´è¿›è¡Œè½¬æ¢-åªéœ€ç¡®ä¿åœ¨æ¯”è¾ƒä¸åŒæ¨¡å‹æ—¶ä¿æŒä¸€è‡´ã€‚æˆ‘ä»¬å°†åœ¨è¿™é‡ŒåšæŒä½¿ç”¨ç¦»æ•£æ—¶é—´æ–¹æ³•ã€‚
- en: A good place to start for pushing deeper into the maths is the DDPM paper mentioned
    earlier. You can find an [annotated implementation here](https://huggingface.co/blog/annotated-diffusion)
    which is a great additional resource for understanding this approach.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±å…¥ç ”ç©¶æ•°å­¦çš„ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹æ˜¯ä¹‹å‰æåˆ°çš„DDPMè®ºæ–‡ã€‚æ‚¨å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°[æ³¨é‡Šå®ç°](https://huggingface.co/blog/annotated-diffusion)ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é¢å¤–èµ„æºï¼Œå¯ä»¥å¸®åŠ©ç†è§£è¿™ç§æ–¹æ³•ã€‚
- en: 'The paper begins by specifying a single noise step to go from timestep t-1
    to timestep t. Hereâ€™s how they write it:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡å¼€å§‹æ—¶æŒ‡å®šäº†ä»æ—¶é—´æ­¥t-1åˆ°æ—¶é—´æ­¥tçš„å•ä¸ªå™ªéŸ³æ­¥éª¤ã€‚è¿™æ˜¯ä»–ä»¬çš„å†™æ³•ï¼š
- en: <math display="block"><mrow><mi>q</mi> <mrow><mo>(</mo> <msub><mi>ğ±</mi> <mi>t</mi></msub>
    <mo>|</mo> <msub><mi>ğ±</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>)</mo></mrow> <mo>=</mo> <mi>ğ’©</mi> <mrow><mo>(</mo> <msub><mi>ğ±</mi> <mi>t</mi></msub>
    <mo>;</mo> <msqrt><mrow><mn>1</mn> <mo>-</mo> <msub><mi>Î²</mi> <mi>t</mi></msub></mrow></msqrt>
    <msub><mi>ğ±</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub> <mo>,</mo>
    <msub><mi>Î²</mi> <mi>t</mi></msub> <mi>ğˆ</mi> <mo>)</mo></mrow> <mo>.</mo></mrow></math>
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>q</mi> <mrow><mo>(</mo> <msub><mi>ğ±</mi> <mi>t</mi></msub>
    <mo>|</mo> <msub><mi>ğ±</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>)</mo></mrow> <mo>=</mo> <mi>ğ’©</mi> <mrow><mo>(</mo> <msub><mi>ğ±</mi> <mi>t</mi></msub>
    <mo>;</mo> <msqrt><mrow><mn>1</mn> <mo>-</mo> <msub><mi>Î²</mi> <mi>t</mi></msub></mrow></msqrt>
    <msub><mi>ğ±</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub> <mo>,</mo>
    <msub><mi>Î²</mi> <mi>t</mi></msub> <mi>ğˆ</mi> <mo>)</mo></mrow> <mo>.</mo></mrow></math>
- en: 'Here <math alttext="beta Subscript t"><msub><mi>Î²</mi> <mi>t</mi></msub></math>
    is defined for all timesteps t and is used to specify how much noise is added
    at each step. This notation can be a little intimidating, but what this equation
    tells us is that the noisier <math alttext="bold x Subscript t"><msub><mi>ğ±</mi>
    <mi>t</mi></msub></math> is a *distribution* with a mean of <math alttext="StartRoot
    1 minus beta Subscript t Baseline EndRoot bold x Subscript t minus 1"><mrow><msqrt><mrow><mn>1</mn>
    <mo>-</mo> <msub><mi>Î²</mi> <mi>t</mi></msub></mrow></msqrt> <msub><mi>ğ±</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></math> and a variance
    of <math alttext="beta Subscript t"><msub><mi>Î²</mi> <mi>t</mi></msub></math>
    . In other words, <math alttext="bold x Subscript t"><msub><mi>ğ±</mi> <mi>t</mi></msub></math>
    is a mix of <math alttext="bold x Subscript t minus 1"><msub><mi>ğ±</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>
    (scaled by <math alttext="StartRoot 1 minus beta Subscript t Baseline EndRoot"><msqrt><mrow><mn>1</mn>
    <mo>-</mo> <msub><mi>Î²</mi> <mi>t</mi></msub></mrow></msqrt></math> ) and some
    random noise, which we can think of as unit-variance noise scaled by <math alttext="StartRoot
    beta Subscript t Baseline EndRoot"><msqrt><msub><mi>Î²</mi> <mi>t</mi></msub></msqrt></math>
    . Given <math alttext="x Subscript t minus 1"><msub><mi>x</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>
    and some noise <math alttext="epsilon"><mi>Ïµ</mi></math> , we can sample from
    this distribution to get <math alttext="x Subscript t"><msub><mi>x</mi> <mi>t</mi></msub></math>
    with:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œ<math alttext="beta Subscript t"><msub><mi>Î²</mi> <mi>t</mi></msub></math>è¢«å®šä¹‰ä¸ºæ‰€æœ‰æ—¶é—´æ­¥é•¿tï¼Œå¹¶ç”¨äºæŒ‡å®šæ¯ä¸ªæ­¥éª¤æ·»åŠ å¤šå°‘å™ªå£°ã€‚è¿™ç§ç¬¦å·å¯èƒ½æœ‰ç‚¹ä»¤äººç”Ÿç•ï¼Œä½†è¿™ä¸ªæ–¹ç¨‹å‘Šè¯‰æˆ‘ä»¬çš„æ˜¯ï¼Œæ›´å˜ˆæ‚çš„<math
    alttext="bold x Subscript t"><msub><mi>ğ±</mi> <mi>t</mi></msub></math>æ˜¯ä¸€ä¸ª*åˆ†å¸ƒ*ï¼Œå…¶å‡å€¼ä¸º<math
    alttext="StartRoot 1 minus beta Subscript t Baseline EndRoot bold x Subscript
    t minus 1"><mrow><msqrt><mrow><mn>1</mn> <mo>-</mo> <msub><mi>Î²</mi> <mi>t</mi></msub></mrow></msqrt>
    <msub><mi>ğ±</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></math>ï¼Œæ–¹å·®ä¸º<math
    alttext="beta Subscript t"><msub><mi>Î²</mi> <mi>t</mi></msub></math>ã€‚æ¢å¥è¯è¯´ï¼Œ<math
    alttext="bold x Subscript t"><msub><mi>ğ±</mi> <mi>t</mi></msub></math>æ˜¯<math alttext="bold
    x Subscript t minus 1"><msub><mi>ğ±</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>ï¼ˆæŒ‰<math
    alttext="StartRoot 1 minus beta Subscript t Baseline EndRoot"><msqrt><mrow><mn>1</mn>
    <mo>-</mo> <msub><mi>Î²</mi> <mi>t</mi></msub></mrow></msqrt></math>ç¼©æ”¾ï¼‰å’Œä¸€äº›éšæœºå™ªå£°çš„æ··åˆï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶è§†ä¸ºæŒ‰<math
    alttext="StartRoot beta Subscript t Baseline EndRoot"><msqrt><msub><mi>Î²</mi>
    <mi>t</mi></msub></msqrt></math>ç¼©æ”¾çš„å•ä½æ–¹å·®å™ªå£°ã€‚ç»™å®š<math alttext="x Subscript t minus
    1"><msub><mi>x</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>å’Œä¸€äº›å™ªå£°<math
    alttext="epsilon"><mi>Ïµ</mi></math>ï¼Œæˆ‘ä»¬å¯ä»¥ä»è¿™ä¸ªåˆ†å¸ƒä¸­é‡‡æ ·å¾—åˆ°<math alttext="x Subscript t"><msub><mi>x</mi>
    <mi>t</mi></msub></math>ï¼š
- en: <math><mrow><msub><mi>ğ±</mi> <mi>t</mi></msub> <mo>=</mo> <msqrt><mrow><mn>1</mn>
    <mo>-</mo> <msub><mi>Î²</mi> <mi>t</mi></msub></mrow></msqrt> <msub><mi>ğ±</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub> <mo>+</mo> <msqrt><msub><mi>Î²</mi>
    <mi>t</mi></msub></msqrt> <mi>Ïµ</mi></mrow></math>
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><msub><mi>ğ±</mi> <mi>t</mi></msub> <mo>=</mo> <msqrt><mrow><mn>1</mn>
    <mo>-</mo> <msub><mi>Î²</mi> <mi>t</mi></msub></mrow></msqrt> <msub><mi>ğ±</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub> <mo>+</mo> <msqrt><msub><mi>Î²</mi>
    <mi>t</mi></msub></msqrt> <mi>Ïµ</mi></mrow></math>
- en: 'To get the noisy input at timestep t, we could begin at t=0 and repeatedly
    apply this single step, but this would be very inefficient. Instead, we can find
    a formula to move to any timestep t in one go. We define <math alttext="alpha
    Subscript t Baseline equals 1 minus beta Subscript t"><mrow><msub><mi>Î±</mi> <mi>t</mi></msub>
    <mo>=</mo> <mn>1</mn> <mo>-</mo> <msub><mi>Î²</mi> <mi>t</mi></msub></mrow></math>
    and then use the following formula:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨æ—¶é—´æ­¥tè·å¾—å˜ˆæ‚çš„è¾“å…¥ï¼Œæˆ‘ä»¬å¯ä»¥ä»t=0å¼€å§‹ï¼Œå¹¶åå¤åº”ç”¨è¿™ä¸€æ­¥ï¼Œä½†è¿™å°†éå¸¸ä½æ•ˆã€‚ç›¸åï¼Œæˆ‘ä»¬å¯ä»¥æ‰¾åˆ°ä¸€ä¸ªå…¬å¼ä¸€æ¬¡æ€§ç§»åŠ¨åˆ°ä»»ä½•æ—¶é—´æ­¥tã€‚æˆ‘ä»¬å®šä¹‰<math
    alttext="alpha Subscript t Baseline equals 1 minus beta Subscript t"><mrow><msub><mi>Î±</mi>
    <mi>t</mi></msub> <mo>=</mo> <mn>1</mn> <mo>-</mo> <msub><mi>Î²</mi> <mi>t</mi></msub></mrow></math>ï¼Œç„¶åä½¿ç”¨ä»¥ä¸‹å…¬å¼ï¼š
- en: <math><mrow><msub><mi>x</mi> <mi>t</mi></msub> <mo>=</mo> <msqrt><msub><mover
    accent="true"><mi>Î±</mi> <mo>Â¯</mo></mover> <mi>t</mi></msub></msqrt> <msub><mi>x</mi>
    <mn>0</mn></msub> <mo>+</mo> <msqrt><mrow><mn>1</mn> <mo>-</mo> <msub><mover accent="true"><mi>Î±</mi>
    <mo>Â¯</mo></mover> <mi>t</mi></msub></mrow></msqrt> <mi>Ïµ</mi></mrow></math>
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><msub><mi>x</mi> <mi>t</mi></msub> <mo>=</mo> <msqrt><msub><mover
    accent="true"><mi>Î±</mi> <mo>Â¯</mo></mover> <mi>t</mi></msub></msqrt> <msub><mi>x</mi>
    <mn>0</mn></msub> <mo>+</mo> <msqrt><mrow><mn>1</mn> <mo>-</mo> <msub><mover accent="true"><mi>Î±</mi>
    <mo>Â¯</mo></mover> <mi>t</mi></msub></mrow></msqrt> <mi>Ïµ</mi></mrow></math>
- en: where - <math alttext="epsilon"><mi>Ïµ</mi></math> is some gaussian noise with
    unit variance - <math alttext="alpha overbar"><mover accent="true"><mi>Î±</mi>
    <mo>Â¯</mo></mover></math> (â€˜alpha_barâ€™) is the cumulative product of all the <math
    alttext="alpha"><mi>Î±</mi></math> values up to the time <math alttext="t"><mi>t</mi></math>
    .
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œ - <math alttext="epsilon"><mi>Ïµ</mi></math> æ˜¯ä¸€äº›æ–¹å·®ä¸ºå•ä½çš„é«˜æ–¯å™ªå£° - <math alttext="alpha
    overbar"><mover accent="true"><mi>Î±</mi> <mo>Â¯</mo></mover></math>ï¼ˆ'alpha_bar'ï¼‰æ˜¯ç›´åˆ°æ—¶é—´<math
    alttext="t"><mi>t</mi></math>çš„æ‰€æœ‰<math alttext="alpha"><mi>Î±</mi></math>å€¼çš„ç´¯ç§¯ä¹˜ç§¯ã€‚
- en: 'So <math alttext="x Subscript t"><msub><mi>x</mi> <mi>t</mi></msub></math>
    is a mixture of <math alttext="x 0"><msub><mi>x</mi> <mn>0</mn></msub></math>
    (scaled by <math alttext="StartRoot alpha overbar Subscript t Baseline EndRoot"><msqrt><msub><mover
    accent="true"><mi>Î±</mi> <mo>Â¯</mo></mover> <mi>t</mi></msub></msqrt></math> )
    and <math alttext="epsilon"><mi>Ïµ</mi></math> (scaled by <math alttext="StartRoot
    1 minus alpha overbar Subscript t Baseline EndRoot"><msqrt><mrow><mn>1</mn> <mo>-</mo>
    <msub><mover accent="true"><mi>Î±</mi> <mo>Â¯</mo></mover> <mi>t</mi></msub></mrow></msqrt></math>
    ). In the diffusers library the <math alttext="alpha overbar"><mover accent="true"><mi>Î±</mi>
    <mo>Â¯</mo></mover></math> values are stored in `scheduler.alphas_cumprod`. Knowing
    this, we can plot the scaling factors for the original image <math alttext="x
    0"><msub><mi>x</mi> <mn>0</mn></msub></math> and the noise <math alttext="epsilon"><mi>Ïµ</mi></math>
    across the different timesteps for a given scheduler:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œ<math alttext="x Subscript t"><msub><mi>x</mi> <mi>t</mi></msub></math>æ˜¯<math
    alttext="x 0"><msub><mi>x</mi> <mn>0</mn></msub></math>ï¼ˆç”±<math alttext="StartRoot
    alpha overbar Subscript t Baseline EndRoot"><msqrt><msub><mover accent="true"><mi>Î±</mi>
    <mo>Â¯</mo></mover> <mi>t</mi></msub></msqrt></math>ç¼©æ”¾ï¼‰å’Œ<math alttext="epsilon"><mi>Ïµ</mi></math>ï¼ˆç”±<math
    alttext="StartRoot 1 minus alpha overbar Subscript t Baseline EndRoot"><msqrt><mrow><mn>1</mn>
    <mo>-</mo> <msub><mover accent="true"><mi>Î±</mi> <mo>Â¯</mo></mover> <mi>t</mi></msub></mrow></msqrt></math>ç¼©æ”¾ï¼‰çš„æ··åˆç‰©ã€‚åœ¨diffusersåº“ä¸­ï¼Œ<math
    alttext="alpha overbar"><mover accent="true"><mi>Î±</mi> <mo>Â¯</mo></mover></math>å€¼å­˜å‚¨åœ¨`scheduler.alphas_cumprod`ä¸­ã€‚çŸ¥é“è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥ç»˜åˆ¶ç»™å®šè°ƒåº¦ç¨‹åºçš„ä¸åŒæ—¶é—´æ­¥éª¤ä¸­åŸå§‹å›¾åƒ<math
    alttext="x 0"><msub><mi>x</mi> <mn>0</mn></msub></math>å’Œå™ªéŸ³<math alttext="epsilon"><mi>Ïµ</mi></math>çš„ç¼©æ”¾å› å­ï¼š
- en: '[PRE19]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![image](assets/cell-21-output-1.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-21-output-1.png)'
- en: 'Our SimpleScheduler above just linearly mixes between the original image and
    noise, as we can see if we plot the scaling factors (equivalent to <math alttext="StartRoot
    alpha overbar Subscript t Baseline EndRoot"><msqrt><msub><mover accent="true"><mi>Î±</mi>
    <mo>Â¯</mo></mover> <mi>t</mi></msub></msqrt></math> and <math alttext="StartRoot
    left-parenthesis 1 minus alpha overbar Subscript t Baseline right-parenthesis
    EndRoot"><msqrt><mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mover accent="true"><mi>Î±</mi>
    <mo>Â¯</mo></mover> <mi>t</mi></msub> <mo>)</mo></mrow></msqrt></math> in the DDPM
    case):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸Šé¢çš„SimpleScheduleråªæ˜¯åœ¨åŸå§‹å›¾åƒå’Œå™ªéŸ³ä¹‹é—´çº¿æ€§æ··åˆï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å¦‚æœæˆ‘ä»¬ç»˜åˆ¶ç¼©æ”¾å› å­ï¼ˆç›¸å½“äº<math alttext="StartRoot
    alpha overbar Subscript t Baseline EndRoot"><msqrt><msub><mover accent="true"><mi>Î±</mi>
    <mo>Â¯</mo></mover> <mi>t</mi></msub></msqrt></math>å’Œ<math alttext="StartRoot left-parenthesis
    1 minus alpha overbar Subscript t Baseline right-parenthesis EndRoot"><msqrt><mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <msub><mover accent="true"><mi>Î±</mi> <mo>Â¯</mo></mover>
    <mi>t</mi></msub> <mo>)</mo></mrow></msqrt></math>åœ¨DDPMæƒ…å†µä¸‹ï¼‰ï¼š
- en: '[PRE20]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![image](assets/cell-22-output-1.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-22-output-1.png)'
- en: 'A good noise schedule will ensure that the model sees a mix of images at different
    noise levels. The best choice will differ based on the training data. Visualizing
    a few more options, note that:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªè‰¯å¥½çš„å™ªéŸ³è°ƒåº¦å°†ç¡®ä¿æ¨¡å‹çœ‹åˆ°ä¸åŒå™ªéŸ³æ°´å¹³çš„å›¾åƒæ··åˆã€‚æœ€ä½³é€‰æ‹©å°†æ ¹æ®è®­ç»ƒæ•°æ®è€Œå¼‚ã€‚å¯è§†åŒ–ä¸€äº›æ›´å¤šé€‰é¡¹ï¼Œæ³¨æ„ï¼š
- en: Setting beta_end too low means we never completely erase the image, so the model
    will never see anything like the random noise used as a starting point for inference.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†beta_endè®¾ç½®å¾—å¤ªä½æ„å‘³ç€æˆ‘ä»¬æ°¸è¿œä¸ä¼šå®Œå…¨æ“¦é™¤å›¾åƒï¼Œå› æ­¤æ¨¡å‹æ°¸è¿œä¸ä¼šçœ‹åˆ°ä»»ä½•ç±»ä¼¼äºç”¨ä½œæ¨ç†èµ·ç‚¹çš„éšæœºå™ªéŸ³çš„ä¸œè¥¿ã€‚
- en: Setting beta_end extremely high means that most of the timesteps are spent on
    almost complete noise, which will result in poor training performance.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†beta_endè®¾ç½®å¾—éå¸¸é«˜æ„å‘³ç€å¤§å¤šæ•°æ—¶é—´æ­¥éª¤éƒ½èŠ±åœ¨å‡ ä¹å®Œå…¨çš„å™ªéŸ³ä¸Šï¼Œè¿™å°†å¯¼è‡´è®­ç»ƒæ€§èƒ½ä¸ä½³ã€‚
- en: Different beta schedules give different curves.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸åŒçš„betaè°ƒåº¦ç»™å‡ºä¸åŒçš„æ›²çº¿ã€‚
- en: The â€˜cosineâ€™ schedule is a popular choice, as it gives a smooth transition from
    the original image to the noise.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: â€œä½™å¼¦â€è°ƒåº¦æ˜¯ä¸€ä¸ªå—æ¬¢è¿çš„é€‰æ‹©ï¼Œå› ä¸ºå®ƒå¯ä»¥ä½¿åŸå§‹å›¾åƒå¹³ç¨³è¿‡æ¸¡åˆ°å™ªéŸ³ã€‚
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![image](assets/cell-23-output-1.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-23-output-1.png)'
- en: Note
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: All of the schedules shown here are called â€˜Variance Preservingâ€™ (VP), meaning
    that the variance of the model input is kept close to 1 across the entire schedule.
    You may also encounter â€˜Variance Explodingâ€™ (VE) formulations where noise is simply
    added to the original image in different amounts (resulting in high-variance inputs).
    Weâ€™ll go into this more in the chapter on sampling. Our SimpleScheduler is almost
    a VP schedule, but the variance is not quite preserved due to the linear interpolation.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¾ç¤ºçš„æ‰€æœ‰è°ƒåº¦éƒ½è¢«ç§°ä¸ºâ€œæ–¹å·®ä¿æŒâ€ï¼ˆVPï¼‰ï¼Œè¿™æ„å‘³ç€æ¨¡å‹è¾“å…¥çš„æ–¹å·®åœ¨æ•´ä¸ªè°ƒåº¦è¿‡ç¨‹ä¸­ä¿æŒæ¥è¿‘1ã€‚æ‚¨å¯èƒ½è¿˜ä¼šé‡åˆ°â€œæ–¹å·®çˆ†ç‚¸â€ï¼ˆVEï¼‰å…¬å¼ï¼Œå…¶ä¸­å™ªéŸ³åªæ˜¯ä»¥ä¸åŒçš„é‡æ·»åŠ åˆ°åŸå§‹å›¾åƒï¼ˆå¯¼è‡´é«˜æ–¹å·®è¾“å…¥ï¼‰ã€‚æˆ‘ä»¬å°†åœ¨é‡‡æ ·ç« èŠ‚ä¸­æ›´è¯¦ç»†åœ°è®¨è®ºè¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬çš„SimpleSchedulerå‡ ä¹æ˜¯ä¸€ä¸ªVPè°ƒåº¦ï¼Œä½†ç”±äºçº¿æ€§æ’å€¼ï¼Œæ–¹å·®å¹¶æ²¡æœ‰å®Œå…¨ä¿æŒã€‚
- en: As with many diffusion-related topics, there is a constant stream of new papers
    exploring the topic of noise schedules, so by the time you read this there will
    likely be a large collection of options to try out!
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è®¸å¤šä¸æ‰©æ•£ç›¸å…³çš„ä¸»é¢˜ä¸€æ ·ï¼Œä¸æ–­æœ‰æ–°çš„è®ºæ–‡æ¢è®¨å™ªéŸ³è°ƒåº¦çš„ä¸»é¢˜ï¼Œå› æ­¤å½“æ‚¨é˜…è¯»æœ¬æ–‡æ—¶ï¼Œå¯èƒ½ä¼šæœ‰å¤§é‡çš„é€‰é¡¹å¯ä¾›å°è¯•ï¼
- en: Effect of Input Resolution and Scaling
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¾“å…¥åˆ†è¾¨ç‡å’Œç¼©æ”¾çš„å½±å“
- en: One aspect of noise schedules that was mostly overlooked until recently is the
    effect of input size and scaling. Many papers test potential schedulers on small-scale
    datasets and at low resolution, and then use the best-performing scheduler to
    train their final models on larger images. The problem with this is can be seen
    if we add the same amount of noise to two images of different sizes.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´åˆ°æœ€è¿‘ï¼Œå™ªéŸ³è°ƒåº¦çš„ä¸€ä¸ªæ–¹é¢å¤§å¤šè¢«å¿½è§†ï¼Œå³è¾“å…¥å¤§å°å’Œç¼©æ”¾çš„å½±å“ã€‚è®¸å¤šè®ºæ–‡åœ¨å°è§„æ¨¡æ•°æ®é›†å’Œä½åˆ†è¾¨ç‡ä¸Šæµ‹è¯•æ½œåœ¨çš„è°ƒåº¦ç¨‹åºï¼Œç„¶åä½¿ç”¨è¡¨ç°æœ€ä½³çš„è°ƒåº¦ç¨‹åºæ¥è®­ç»ƒå…¶æœ€ç»ˆæ¨¡å‹çš„è¾ƒå¤§å›¾åƒã€‚è¿™æ ·åšçš„é—®é¢˜åœ¨äºï¼Œå¦‚æœæˆ‘ä»¬å‘ä¸¤ä¸ªä¸åŒå¤§å°çš„å›¾åƒæ·»åŠ ç›¸åŒæ•°é‡çš„å™ªéŸ³ï¼Œå°±ä¼šçœ‹åˆ°é—®é¢˜ã€‚
- en: '![image](assets/cell-24-output-1.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-24-output-1.png)'
- en: Figure 1-2\. Comparing the effect of adding noise to images of different sizes
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾1-2ã€‚æ¯”è¾ƒå‘ä¸åŒå¤§å°çš„å›¾åƒæ·»åŠ å™ªéŸ³çš„æ•ˆæœ
- en: Images at high resolution tend to contain a lot of redundant information. This
    means that even if a single pixel is obscured by noise, the surrounding pixels
    contain enough information to reconstruct the original image. This is not the
    case for low-resolution images, where a single pixel can contain a lot of information.
    This means that adding the same amount of noise to a low-resolution image will
    result in a much more corrupted image than adding the equivalent amount of noise
    to a high-resolution image.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: é«˜åˆ†è¾¨ç‡çš„å›¾åƒå¾€å¾€åŒ…å«å¤§é‡å†—ä½™ä¿¡æ¯ã€‚è¿™æ„å‘³ç€å³ä½¿å•ä¸ªåƒç´ è¢«å™ªéŸ³é®æŒ¡ï¼Œå‘¨å›´çš„åƒç´ ä¹ŸåŒ…å«è¶³å¤Ÿçš„ä¿¡æ¯æ¥é‡å»ºåŸå§‹å›¾åƒã€‚ä½†å¯¹äºä½åˆ†è¾¨ç‡å›¾åƒæ¥è¯´å¹¶éå¦‚æ­¤ï¼Œå…¶ä¸­å•ä¸ªåƒç´ å¯èƒ½åŒ…å«å¤§é‡ä¿¡æ¯ã€‚è¿™æ„å‘³ç€å‘ä½åˆ†è¾¨ç‡å›¾åƒæ·»åŠ ç›¸åŒé‡çš„å™ªéŸ³å°†å¯¼è‡´æ¯”å‘é«˜åˆ†è¾¨ç‡å›¾åƒæ·»åŠ ç­‰é‡å™ªéŸ³æ›´åŠ æŸåçš„å›¾åƒã€‚
- en: 'This effect was thoroughly investigated in two independent papers, both of
    which came out in January 2023\. Each used the new insights to train models capable
    of generating high-resolution outputs without requiring any of the tricks that
    have previously been necessary. [*Simple diffusion*](https://arxiv.org/abs/2301.11093)
    by Hoogeboom et al introduced a method for adjusting the noise schedule based
    on the input size, allowing a schedule optimized on low-resolution images to be
    appropriately modified for a new target resolution. A paper called [â€œOn the Importance
    of Noise Scheduling for Diffusion Modelsâ€](https://arxiv.org/abs/2301.10972) by
    Ting Chen performed similar experiments, and noted another key variable: input
    scaling. That is, how do we represent our images? If the images are represented
    as floats between 0 and 1 then they will have a lower variance than the noise
    (which is typically unit variance) and thus the signal-to-noise ratio will be
    lower for a given noise level than if the images were represented as floats between
    -1 and 1 (which we used in the training example above) or something else. Scaling
    the input images shifts the signal-to-noise ratio, and so modifying this scaling
    is another way we can adjust when training on larger images.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ•ˆåº”åœ¨ä¸¤ç¯‡ç‹¬ç«‹çš„è®ºæ–‡ä¸­å¾—åˆ°äº†å½»åº•çš„è°ƒæŸ¥ï¼Œè¿™ä¸¤ç¯‡è®ºæ–‡åˆ†åˆ«äº2023å¹´1æœˆå‘è¡¨ã€‚æ¯ç¯‡è®ºæ–‡éƒ½åˆ©ç”¨æ–°çš„è§è§£æ¥è®­ç»ƒèƒ½å¤Ÿç”Ÿæˆé«˜åˆ†è¾¨ç‡è¾“å‡ºçš„æ¨¡å‹ï¼Œè€Œæ— éœ€ä»»ä½•ä»¥å‰å¿…éœ€çš„æŠ€å·§ã€‚Hoogeboomç­‰äººçš„ã€Šç®€å•æ‰©æ•£ã€‹ä»‹ç»äº†ä¸€ç§æ ¹æ®è¾“å…¥å¤§å°è°ƒæ•´å™ªéŸ³è®¡åˆ’çš„æ–¹æ³•ï¼Œå…è®¸åœ¨ä½åˆ†è¾¨ç‡å›¾åƒä¸Šä¼˜åŒ–çš„è®¡åˆ’é€‚å½“åœ°ä¿®æ”¹ä¸ºæ–°çš„ç›®æ ‡åˆ†è¾¨ç‡ã€‚é™ˆå©·çš„ä¸€ç¯‡åä¸ºâ€œå…³äºæ‰©æ•£æ¨¡å‹å™ªéŸ³è°ƒåº¦çš„é‡è¦æ€§â€çš„è®ºæ–‡è¿›è¡Œäº†ç±»ä¼¼çš„å®éªŒï¼Œå¹¶æ³¨æ„åˆ°å¦ä¸€ä¸ªå…³é”®å˜é‡ï¼šè¾“å…¥ç¼©æ”¾ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬å¦‚ä½•è¡¨ç¤ºæˆ‘ä»¬çš„å›¾åƒï¼Ÿå¦‚æœå›¾åƒè¡¨ç¤ºä¸º0åˆ°1ä¹‹é—´çš„æµ®ç‚¹æ•°ï¼Œé‚£ä¹ˆå®ƒä»¬çš„æ–¹å·®å°†ä½äºå™ªéŸ³ï¼ˆé€šå¸¸æ˜¯å•ä½æ–¹å·®ï¼‰ï¼Œå› æ­¤å¯¹äºç»™å®šçš„å™ªéŸ³æ°´å¹³ï¼Œä¿¡å™ªæ¯”å°†ä½äºå¦‚æœå›¾åƒè¡¨ç¤ºä¸º-1åˆ°1ä¹‹é—´çš„æµ®ç‚¹æ•°ï¼ˆæˆ‘ä»¬åœ¨ä¸Šé¢çš„è®­ç»ƒç¤ºä¾‹ä¸­ä½¿ç”¨çš„æ–¹å¼ï¼‰æˆ–å…¶ä»–æ–¹å¼ã€‚ç¼©æ”¾è¾“å…¥å›¾åƒä¼šæ”¹å˜ä¿¡å™ªæ¯”ï¼Œå› æ­¤ä¿®æ”¹è¿™ç§ç¼©æ”¾æ˜¯æˆ‘ä»¬åœ¨è®­ç»ƒæ›´å¤§å›¾åƒæ—¶å¯ä»¥è°ƒæ•´çš„å¦ä¸€ç§æ–¹å¼ã€‚
- en: 'In Depth: UNets and Alternatives'
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ·±å…¥äº†è§£ï¼šUNetså’Œæ›¿ä»£æ–¹æ¡ˆ
- en: Now letâ€™s address the actual model that makes the all-important predictions!
    To recap, this model must be capable of taking in a noisy image and estimating
    how to denoise it. This requires a model that can take in an image of arbitrary
    size and output an image of the same size. Furthermore, the model should be able
    to make precise predictions at the pixel level, while also capturing higher-level
    information about the image as a whole. A popular approach is to use an architecture
    called a UNet. UNets were invented in 2015 for medical image segmentation, and
    have since become a popular choice for various image-related tasks. Like the AutoEncoders
    and VAEs we looked at in the previous chapter, UNets are made up of a series of
    â€˜downsamplingâ€™ and â€˜upsamplingâ€™ blocks. The downsampling blocks are responsible
    for reducing the size of the image, while the upsampling blocks are responsible
    for increasing the size of the image. The downsampling blocks are typically made
    up of a series of convolutional layers, followed by a pooling or downsampling
    layer. The upsampling blocks are typically made up of a series of convolutional
    layers, followed by an upsampling or â€˜transposed convolutionâ€™ layer. The transposed
    convolution layer is a special type of convolutional layer that increases the
    size of the image, rather than reducing it.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬æ¥è®¨è®ºçœŸæ­£è¿›è¡Œé‡è¦é¢„æµ‹çš„æ¨¡å‹ï¼å›é¡¾ä¸€ä¸‹ï¼Œè¿™ä¸ªæ¨¡å‹å¿…é¡»èƒ½å¤Ÿæ¥æ”¶å˜ˆæ‚çš„å›¾åƒå¹¶ä¼°è®¡å¦‚ä½•å»é™¤å™ªéŸ³ã€‚è¿™éœ€è¦ä¸€ä¸ªèƒ½å¤Ÿæ¥æ”¶ä»»æ„å¤§å°çš„å›¾åƒå¹¶è¾“å‡ºç›¸åŒå¤§å°çš„å›¾åƒçš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åº”èƒ½å¤Ÿåœ¨åƒç´ çº§åˆ«è¿›è¡Œç²¾ç¡®é¢„æµ‹ï¼ŒåŒæ—¶ä¹Ÿæ•æ‰å…³äºæ•´ä¸ªå›¾åƒçš„æ›´é«˜çº§åˆ«ä¿¡æ¯ã€‚ä¸€ä¸ªæµè¡Œçš„æ–¹æ³•æ˜¯ä½¿ç”¨ä¸€ç§ç§°ä¸ºUNetçš„æ¶æ„ã€‚UNetæ˜¯åœ¨2015å¹´ä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²è€Œå‘æ˜çš„ï¼Œå¹¶ä¸”åæ¥æˆä¸ºå„ç§ä¸å›¾åƒç›¸å…³çš„ä»»åŠ¡çš„æµè¡Œé€‰æ‹©ã€‚å°±åƒæˆ‘ä»¬åœ¨ä¸Šä¸€ç« ä¸­çœ‹åˆ°çš„AutoEncoderså’ŒVAEsä¸€æ ·ï¼ŒUNetsç”±ä¸€ç³»åˆ—â€œä¸‹é‡‡æ ·â€å’Œâ€œä¸Šé‡‡æ ·â€å—ç»„æˆã€‚ä¸‹é‡‡æ ·å—è´Ÿè´£å‡å°å›¾åƒçš„å¤§å°ï¼Œè€Œä¸Šé‡‡æ ·å—è´Ÿè´£å¢åŠ å›¾åƒçš„å¤§å°ã€‚ä¸‹é‡‡æ ·å—é€šå¸¸ç”±ä¸€ç³»åˆ—å·ç§¯å±‚ç»„æˆï¼Œç„¶åæ˜¯æ± åŒ–æˆ–ä¸‹é‡‡æ ·å±‚ã€‚ä¸Šé‡‡æ ·å—é€šå¸¸ç”±ä¸€ç³»åˆ—å·ç§¯å±‚ç»„æˆï¼Œç„¶åæ˜¯ä¸Šé‡‡æ ·æˆ–â€œè½¬ç½®å·ç§¯â€å±‚ã€‚è½¬ç½®å·ç§¯å±‚æ˜¯ä¸€ç§ç‰¹æ®Šç±»å‹çš„å·ç§¯å±‚ï¼Œå®ƒå¢åŠ å›¾åƒçš„å¤§å°ï¼Œè€Œä¸æ˜¯å‡å°å›¾åƒçš„å¤§å°ã€‚
- en: The reason a regular AutoEncoder or VAE is not a good choice for this task is
    that they are less capable of making precise predictions at the pixel level since
    the output must be entirely re-constructed from the low-dimensional latent space.
    In a UNet, the downsampling and upsampling blocks are connected by â€˜skip connectionsâ€™,
    which allow information to flow directly from the downsampling blocks to the upsampling
    blocks. This allows the model to make precise predictions at the pixel level,
    while also capturing higher-level information about the image as a whole.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¸è§„çš„AutoEncoderæˆ–VAEä¹‹æ‰€ä»¥ä¸é€‚åˆè¿™ä¸ªä»»åŠ¡ï¼Œæ˜¯å› ä¸ºå®ƒä»¬åœ¨åƒç´ çº§åˆ«è¿›è¡Œç²¾ç¡®é¢„æµ‹çš„èƒ½åŠ›è¾ƒå¼±ï¼Œå› ä¸ºè¾“å‡ºå¿…é¡»å®Œå…¨ä»ä½ç»´æ½œåœ¨ç©ºé—´é‡æ–°æ„å»ºã€‚åœ¨UNetä¸­ï¼Œé€šè¿‡â€œè·³è·ƒè¿æ¥â€è¿æ¥ä¸‹é‡‡æ ·å’Œä¸Šé‡‡æ ·å—ï¼Œå…è®¸ä¿¡æ¯ç›´æ¥ä»ä¸‹é‡‡æ ·å—æµå‘ä¸Šé‡‡æ ·å—ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨åƒç´ çº§åˆ«è¿›è¡Œç²¾ç¡®é¢„æµ‹ï¼ŒåŒæ—¶ä¹Ÿæ•æ‰å…³äºæ•´ä¸ªå›¾åƒçš„æ›´é«˜çº§åˆ«ä¿¡æ¯ã€‚
- en: A Simple UNet
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªç®€å•çš„UNet
- en: To better understand the structure of a UNet, letâ€™s build a simple UNet from
    scratch.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´å¥½åœ°ç†è§£UNetçš„ç»“æ„ï¼Œè®©æˆ‘ä»¬ä»å¤´å¼€å§‹æ„å»ºä¸€ä¸ªç®€å•çš„UNetã€‚
- en: '![image.png](assets/595852ce-1-image.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![image.png](assets/595852ce-1-image.png)'
- en: Figure 1-3\. Our simple UNet architecture
  id: totrans-141
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾1-3ã€‚æˆ‘ä»¬ç®€å•çš„UNetæ¶æ„
- en: 'This UNet takes single-channel inputs at 32px resolution and outputs single-channel
    outputs at 32px resolution, which we could use to build a diffusion model for
    the MNIST dataset. There are three layers in the encoding path, and three layers
    in the decoding path. Each layer consists of a convolution followed by an activation
    function and an upsampling or downsampling step (depending on whether we are in
    the encoding or decoding path). The skip connections allow information to flow
    directly from the downsampling blocks to the upsampling blocks, and are implemented
    by adding the output of the downsampling block to the input of the corresponding
    upsampling block. Some UNets instead concatenate the output of the downsampling
    block to the input of the corresponding upsampling block, and may also include
    additional layers in the skip connections. Hereâ€™s what this network looks like
    in code:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªUNetä»¥32pxåˆ†è¾¨ç‡æ¥æ”¶å•é€šé“è¾“å…¥ï¼Œå¹¶ä»¥32pxåˆ†è¾¨ç‡è¾“å‡ºå•é€šé“è¾“å‡ºï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å®ƒæ¥æ„å»ºMNISTæ•°æ®é›†çš„æ‰©æ•£æ¨¡å‹ã€‚ç¼–ç è·¯å¾„ä¸­æœ‰ä¸‰å±‚ï¼Œè§£ç è·¯å¾„ä¸­ä¹Ÿæœ‰ä¸‰å±‚ã€‚æ¯ä¸€å±‚ç”±å·ç§¯åè·Ÿæ¿€æ´»å‡½æ•°å’Œä¸Šé‡‡æ ·æˆ–ä¸‹é‡‡æ ·æ­¥éª¤ï¼ˆå–å†³äºæˆ‘ä»¬æ˜¯å¦åœ¨ç¼–ç æˆ–è§£ç è·¯å¾„ä¸­ï¼‰ç»„æˆã€‚è·³è¿‡è¿æ¥å…è®¸ä¿¡æ¯ç›´æ¥ä»ä¸‹é‡‡æ ·å—æµå‘ä¸Šé‡‡æ ·å—ï¼Œå¹¶é€šè¿‡å°†ä¸‹é‡‡æ ·å—çš„è¾“å‡ºæ·»åŠ åˆ°ç›¸åº”ä¸Šé‡‡æ ·å—çš„è¾“å…¥æ¥å®ç°ã€‚ä¸€äº›UNetå°†ä¸‹é‡‡æ ·å—çš„è¾“å‡ºè¿æ¥åˆ°ç›¸åº”ä¸Šé‡‡æ ·å—çš„è¾“å…¥ï¼Œå¹¶å¯èƒ½è¿˜åœ¨è·³è¿‡è¿æ¥ä¸­åŒ…å«é¢å¤–çš„å±‚ã€‚ä»¥ä¸‹æ˜¯è¿™ä¸ªç½‘ç»œçš„ä»£ç ï¼š
- en: '[PRE22]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'A diffusion model trained with this architecture on MNIST produces the following
    samples (code included in the supplementary material but omitted here for brevity):'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨MNISTä¸Šä½¿ç”¨è¿™ç§æ¶æ„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹äº§ç”Ÿä»¥ä¸‹æ ·æœ¬ï¼ˆä»£ç åŒ…å«åœ¨è¡¥å……ææ–™ä¸­ï¼Œè¿™é‡Œä¸ºäº†ç®€æ´èµ·è§è€Œçœç•¥ï¼‰ï¼š
- en: Improving the UNet
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ”¹è¿›UNet
- en: This simple UNet works for this relatively easy task, but it is far from ideal.
    So, what can we do to improve it?
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç®€å•çš„UNeté€‚ç”¨äºè¿™ä¸ªç›¸å¯¹ç®€å•çš„ä»»åŠ¡ï¼Œä½†è¿œéç†æƒ³ã€‚é‚£ä¹ˆï¼Œæˆ‘ä»¬å¯ä»¥åšäº›ä»€ä¹ˆæ¥æ”¹è¿›å®ƒå‘¢ï¼Ÿ
- en: Add more parameters. This can be accomplished by using multiple convolutional
    layers in each block, by using a larger number of filters in each convolutional
    layer, or by making the network deeper.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ·»åŠ æ›´å¤šå‚æ•°ã€‚å¯ä»¥é€šè¿‡åœ¨æ¯ä¸ªå—ä¸­ä½¿ç”¨å¤šä¸ªå·ç§¯å±‚ï¼Œé€šè¿‡åœ¨æ¯ä¸ªå·ç§¯å±‚ä¸­ä½¿ç”¨æ›´å¤šçš„æ»¤æ³¢å™¨ï¼Œæˆ–è€…é€šè¿‡ä½¿ç½‘ç»œæ›´æ·±æ¥å®ç°ã€‚
- en: Add residual connections. Using ResBlocks instead of regular convolutional layers
    can help the model learn more complex functions while keeping training stable.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ·»åŠ æ®‹å·®è¿æ¥ã€‚ä½¿ç”¨ResBlocksè€Œä¸æ˜¯å¸¸è§„å·ç§¯å±‚å¯ä»¥å¸®åŠ©æ¨¡å‹å­¦ä¹ æ›´å¤æ‚çš„åŠŸèƒ½ï¼ŒåŒæ—¶ä¿æŒè®­ç»ƒç¨³å®šã€‚
- en: Add normalization, such as batch normalization. Batch normalization can help
    the model learn more quickly and reliably, by ensuring that the outputs of each
    layer are centered around 0 and have a standard deviation of 1.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ·»åŠ å½’ä¸€åŒ–ï¼Œå¦‚æ‰¹å½’ä¸€åŒ–ã€‚æ‰¹å½’ä¸€åŒ–å¯ä»¥å¸®åŠ©æ¨¡å‹æ›´å¿«ã€æ›´å¯é åœ°å­¦ä¹ ï¼Œç¡®ä¿æ¯ä¸€å±‚çš„è¾“å‡ºéƒ½å›´ç»•0ä¸­å¿ƒï¼Œå¹¶å…·æœ‰æ ‡å‡†å·®ä¸º1ã€‚
- en: Add regularization, such as dropout. Dropout helps by preventing the model from
    overfitting to the training data, which is important when working with smaller
    datasets.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ·»åŠ æ­£åˆ™åŒ–ï¼Œå¦‚dropoutã€‚Dropoutæœ‰åŠ©äºé˜²æ­¢æ¨¡å‹è¿‡åº¦æ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼Œè¿™åœ¨å¤„ç†è¾ƒå°çš„æ•°æ®é›†æ—¶éå¸¸é‡è¦ã€‚
- en: Add attention. By introducing self-attention layers we allow the model to focus
    on different parts of the image at different times, which can help it learn more
    complex functions. The addition of transformer-like attention layers also lets
    us increase the number of learnable parameters, which can help the model learn
    more complex functions. The downside is that attention layers are much more expensive
    to compute than regular convolutional layers at higher resolutions, so we typically
    only use them at lower resolutions (i.e.Â the lower resolution blocks in the UNet).
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ·»åŠ æ³¨æ„åŠ›ã€‚é€šè¿‡å¼•å…¥è‡ªæ³¨æ„åŠ›å±‚ï¼Œæˆ‘ä»¬å…è®¸æ¨¡å‹åœ¨ä¸åŒæ—¶é—´é›†ä¸­å…³æ³¨å›¾åƒçš„ä¸åŒéƒ¨åˆ†ï¼Œè¿™æœ‰åŠ©äºå­¦ä¹ æ›´å¤æ‚çš„åŠŸèƒ½ã€‚ç±»ä¼¼å˜å‹å™¨çš„æ³¨æ„åŠ›å±‚çš„æ·»åŠ ä¹Ÿå¯ä»¥å¢åŠ å¯å­¦ä¹ å‚æ•°çš„æ•°é‡ï¼Œè¿™æœ‰åŠ©äºæ¨¡å‹å­¦ä¹ æ›´å¤æ‚çš„åŠŸèƒ½ã€‚ç¼ºç‚¹æ˜¯æ³¨æ„åŠ›å±‚åœ¨æ›´é«˜åˆ†è¾¨ç‡æ—¶è®¡ç®—æˆæœ¬è¦æ¯”å¸¸è§„å·ç§¯å±‚é«˜å¾—å¤šï¼Œå› æ­¤æˆ‘ä»¬é€šå¸¸åªåœ¨è¾ƒä½åˆ†è¾¨ç‡ï¼ˆå³UNetä¸­çš„è¾ƒä½åˆ†è¾¨ç‡å—ï¼‰ä½¿ç”¨å®ƒä»¬ã€‚
- en: Add an additional input for the timestep, so that the model can tailor its predicitons
    according to the noise level. This is called timestep conditioning, and is used
    in almost all recent diffusion models. Weâ€™ll see more on conditional models in
    the next chapter.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ·»åŠ ä¸€ä¸ªé¢å¤–çš„è¾“å…¥ç”¨äºæ—¶é—´æ­¥é•¿ï¼Œè¿™æ ·æ¨¡å‹å¯ä»¥æ ¹æ®å™ªéŸ³æ°´å¹³è°ƒæ•´å…¶é¢„æµ‹ã€‚è¿™ç§°ä¸ºæ—¶é—´æ­¥é•¿è°ƒèŠ‚ï¼Œå‡ ä¹æ‰€æœ‰æœ€è¿‘çš„æ‰©æ•£æ¨¡å‹éƒ½åœ¨ä½¿ç”¨ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ç« ä¸­æ›´å¤šåœ°äº†è§£æœ‰æ¡ä»¶çš„æ¨¡å‹ã€‚
- en: 'For comparison, here are the results on MNIST when using the UNet implementation
    in the diffusers library, which features all of the above improvements:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºå¯¹æ¯”ï¼Œåœ¨diffusersåº“ä¸­ä½¿ç”¨UNetå®ç°æ—¶ï¼Œåœ¨MNISTä¸Šçš„ç»“æœå¦‚ä¸‹ï¼Œè¯¥åº“åŒ…å«äº†ä¸Šè¿°æ‰€æœ‰æ”¹è¿›ï¼š
- en: Warning
  id: totrans-154
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è­¦å‘Š
- en: This section will likely be expanded with results and more details in the future.
    We just havenâ€™t gotten around to training variants with the different improvements
    yet!
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€éƒ¨åˆ†å¯èƒ½ä¼šåœ¨æœªæ¥é€šè¿‡ç»“æœå’Œæ›´å¤šç»†èŠ‚è¿›è¡Œæ‰©å±•ã€‚æˆ‘ä»¬è¿˜æ²¡æœ‰å¼€å§‹è®­ç»ƒå…·æœ‰ä¸åŒæ”¹è¿›çš„å˜ä½“ï¼
- en: Alternative Architectures
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ›¿ä»£æ¶æ„
- en: 'More recently, a number of alternative architectures have been proposed for
    diffusion models. These include:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘ï¼Œä¸€äº›æ›¿ä»£æ¶æ„å·²è¢«æå‡ºç”¨äºæ‰©æ•£æ¨¡å‹ã€‚è¿™äº›åŒ…æ‹¬ï¼š
- en: Transformers. The DiT paper ([â€œScalable Diffusion Models with Transformersâ€](https://arxiv.org/abs/2212.09748))
    by Peebles and Xie showed that a transformer-based architecture can be used to
    train a diffusion model, with great results. However, the compute and memory requirements
    of the transformer architecture remain a challenge for very high resolutions.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å˜å‹å™¨ã€‚Peebleså’ŒXieçš„DiTè®ºæ–‡ï¼ˆâ€œå…·æœ‰å˜å‹å™¨çš„å¯æ‰©å±•æ‰©æ•£æ¨¡å‹â€ï¼‰æ˜¾ç¤ºäº†åŸºäºå˜å‹å™¨çš„æ¶æ„å¯ä»¥ç”¨äºè®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œå¹¶å–å¾—äº†å¾ˆå¥½çš„ç»“æœã€‚ç„¶è€Œï¼Œå˜å‹å™¨æ¶æ„çš„è®¡ç®—å’Œå†…å­˜éœ€æ±‚ä»ç„¶æ˜¯éå¸¸é«˜åˆ†è¾¨ç‡çš„æŒ‘æˆ˜ã€‚
- en: The *UViT* architecture from the [Simple Diffusion paper](https://arxiv.org/abs/2301.11093)
    link aims to get the best of both worlds by replacing the middle layers of the
    UNet with a large stack of transformer blocks. A key insight of this paper was
    that focusing the majority of the compute at the lower resolution blocks of the
    UNet allows for more efficient training of high-resolution diffusion models. For
    very high resolutions, they do some additional pre-processing using something
    called a wavelet transform to reduce the spatial resolution of the input image
    while keeping as much information as possible through the use of additional channels,
    again reducing the amount of compute spent on the higher spatial resolutions.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Simple Diffusion paper](https://arxiv.org/abs/2301.11093)é“¾æ¥ä¸­çš„*UViT*æ¶æ„æ—¨åœ¨å…¼é¡¾ä¸¤è€…çš„ä¼˜ç‚¹ï¼Œé€šè¿‡ç”¨ä¸€å¤§å †å˜å‹å™¨å—æ›¿æ¢UNetçš„ä¸­é—´å±‚ã€‚è¯¥è®ºæ–‡çš„ä¸€ä¸ªå…³é”®è§è§£æ˜¯ï¼Œå°†å¤§éƒ¨åˆ†è®¡ç®—é›†ä¸­åœ¨UNetçš„è¾ƒä½åˆ†è¾¨ç‡å—ä¸Šï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°è®­ç»ƒé«˜åˆ†è¾¨ç‡æ‰©æ•£æ¨¡å‹ã€‚å¯¹äºéå¸¸é«˜çš„åˆ†è¾¨ç‡ï¼Œä»–ä»¬ä½¿ç”¨ç§°ä¸ºå°æ³¢å˜æ¢çš„ä¸œè¥¿è¿›è¡Œä¸€äº›é¢å¤–çš„é¢„å¤„ç†ï¼Œä»¥å‡å°‘è¾“å…¥å›¾åƒçš„ç©ºé—´åˆ†è¾¨ç‡ï¼ŒåŒæ—¶é€šè¿‡ä½¿ç”¨é¢å¤–çš„é€šé“å°½å¯èƒ½ä¿ç•™æ›´å¤šä¿¡æ¯ï¼Œå†æ¬¡å‡å°‘åœ¨æ›´é«˜ç©ºé—´åˆ†è¾¨ç‡ä¸ŠèŠ±è´¹çš„è®¡ç®—é‡ã€‚'
- en: Recurrent Interface Networks. The [RIN paper](https://arxiv.org/abs/2212.11972)
    (Jabri et al) takes a similar approach, first mapping the high-resolution inputs
    to a more manageable and lower-dimensional â€˜latentâ€™ representation which is then
    processed by a stack of transformer blocks before being decoded back out to an
    image. Additionally, the RIN paper introduces an idea of â€˜recurrenceâ€™ where information
    is passed to the model from the previous processing step, which can be beneficial
    for the kind of iterative improvement that diffusion models are designed to perform.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¾ªç¯æ¥å£ç½‘ç»œã€‚[RIN paper](https://arxiv.org/abs/2212.11972)ï¼ˆJabriç­‰ï¼‰é‡‡ç”¨ç±»ä¼¼çš„æ–¹æ³•ï¼Œé¦–å…ˆå°†é«˜åˆ†è¾¨ç‡è¾“å…¥æ˜ å°„åˆ°æ›´æ˜“å¤„ç†å’Œä½ç»´çš„â€œæ½œåœ¨â€è¡¨ç¤ºï¼Œç„¶åé€šè¿‡ä¸€å †å˜å‹å™¨å—è¿›è¡Œå¤„ç†ï¼Œç„¶åè§£ç å›åˆ°å›¾åƒã€‚æ­¤å¤–ï¼ŒRINè®ºæ–‡å¼•å…¥äº†â€œå¾ªç¯â€æ¦‚å¿µï¼Œå…¶ä¸­ä¿¡æ¯ä»ä¸Šä¸€ä¸ªå¤„ç†æ­¥éª¤ä¼ é€’ç»™æ¨¡å‹ï¼Œè¿™å¯¹äºæ‰©æ•£æ¨¡å‹æ—¨åœ¨æ‰§è¡Œçš„è¿­ä»£æ”¹è¿›å¯èƒ½æ˜¯æœ‰ç›Šçš„ã€‚
- en: It remains to be seen whether transformer-based approaches completely supplant
    UNets as the go-to architecture for diffusion models, or whether hybrid approaches
    like the UViT and RIN architectures will prove to be the most effective.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: å°šä¸æ¸…æ¥šåŸºäºå˜å‹å™¨çš„æ–¹æ³•æ˜¯å¦ä¼šå®Œå…¨å–ä»£UNetä½œä¸ºæ‰©æ•£æ¨¡å‹çš„é¦–é€‰æ¶æ„ï¼Œè¿˜æ˜¯åƒUViTå’ŒRINæ¶æ„è¿™æ ·çš„æ··åˆæ–¹æ³•å°†è¢«è¯æ˜æ˜¯æœ€æœ‰æ•ˆçš„ã€‚
- en: 'In Depth: Objectives and Pre-Conditioning'
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ·±å…¥ï¼šç›®æ ‡å’Œé¢„å¤„ç†
- en: 'Weâ€™ve spoken about diffusion models taking a noisy input and â€œlearning to denoiseâ€
    it. At first glance, you might assume that the natural prediction target for the
    network is the denoised version of the image, which weâ€™ll call `x0`. However,
    in the code, we compared the model prediction with the unit-variance noise that
    was used to create the noisy version (often called the epsilon objective, `eps`).
    The two appear mathematically identical since if we know the noise and the timestep
    we can derive `x0` and vice versa. While this is true, the choice of objective
    has some subtle effects on how large the loss is at different timesteps, and thus
    which noise levels the model learns to denoise best. To gain some intuition, letâ€™s
    visualize some different objectives across different timesteps:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»è°ˆåˆ°æ‰©æ•£æ¨¡å‹æ¥å—å˜ˆæ‚çš„è¾“å…¥å¹¶â€œå­¦ä¼šå»å™ªâ€å®ƒã€‚ä¹ä¸€çœ‹ï¼Œä½ å¯èƒ½ä¼šè®¤ä¸ºç½‘ç»œçš„è‡ªç„¶é¢„æµ‹ç›®æ ‡æ˜¯å›¾åƒçš„å»å™ªç‰ˆæœ¬ï¼Œæˆ‘ä»¬å°†å…¶ç§°ä¸º`x0`ã€‚ç„¶è€Œï¼Œåœ¨ä»£ç ä¸­ï¼Œæˆ‘ä»¬å°†æ¨¡å‹é¢„æµ‹ä¸ç”¨äºåˆ›å»ºå˜ˆæ‚ç‰ˆæœ¬çš„å•ä½æ–¹å·®å™ªå£°è¿›è¡Œäº†æ¯”è¾ƒï¼ˆé€šå¸¸ç§°ä¸ºepsilonç›®æ ‡ï¼Œ`eps`ï¼‰ã€‚è¿™ä¸¤è€…åœ¨æ•°å­¦ä¸Šçœ‹èµ·æ¥æ˜¯ç›¸åŒçš„ï¼Œå› ä¸ºå¦‚æœæˆ‘ä»¬çŸ¥é“å™ªå£°å’Œæ—¶é—´æ­¥é•¿ï¼Œæˆ‘ä»¬å¯ä»¥æ¨å¯¼å‡º`x0`ï¼Œåä¹‹äº¦ç„¶ã€‚è™½ç„¶è¿™æ˜¯çœŸçš„ï¼Œä½†ç›®æ ‡çš„é€‰æ‹©å¯¹ä¸åŒæ—¶é—´æ­¥çš„æŸå¤±æœ‰ä¸€äº›å¾®å¦™çš„å½±å“ï¼Œå› æ­¤æ¨¡å‹å­¦ä¹ æœ€ä½³å»å™ªå“ªä¸ªå™ªå£°æ°´å¹³ã€‚ä¸ºäº†è·å¾—ä¸€äº›ç›´è§‰ï¼Œè®©æˆ‘ä»¬åœ¨ä¸åŒçš„æ—¶é—´æ­¥ä¸Šå¯è§†åŒ–ä¸€äº›ä¸åŒçš„ç›®æ ‡ï¼š
- en: '![image](assets/cell-26-output-1.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-26-output-1.png)'
- en: At extremely low noise levels, the `x0` objective is trivially easy while predicting
    the noise accurately is almost impossible. Likewise, at extremely high noise levels,
    the `eps` objective is easy while predicting the denoised image accurately is
    almost impossible. Neither case is ideal, and so additional objectives have been
    introduced that have the model predict a mix of `x0` and `eps` at different timesteps.
    The `v` objective (introduced in [â€œProgressive distillation for fast sampling
    of diffusion models.â€](https://arxiv.org/abs/2202.00512) by Salimans and Ho) is
    one such objective, which is defined as <math alttext="v equals StartRoot alpha
    overbar EndRoot dot epsilon plus StartRoot 1 minus alpha overbar EndRoot dot x
    0"><mrow><mi>v</mi> <mo>=</mo> <msqrt><mover accent="true"><mi>Î±</mi> <mo>Â¯</mo></mover></msqrt>
    <mo>Â·</mo> <mi>Ïµ</mi> <mo>+</mo> <msqrt><mrow><mn>1</mn> <mo>-</mo> <mover accent="true"><mi>Î±</mi>
    <mo>Â¯</mo></mover></mrow></msqrt> <mo>Â·</mo> <msub><mi>x</mi> <mn>0</mn></msub></mrow></math>
    . The [EDM paper](https://arxiv.org/abs/2206.00364) by Karras et al introduce
    a similar idea via a parameter called `c_skip`, and unify the different diffusion
    model formulations into a consistent framework. If youâ€™re interested in learning
    more about the different objectives, scalings and other nuances of the different
    diffusion model formulations, we recommend reading their paper for a more in-depth
    discussion.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æä½çš„å™ªå£°æ°´å¹³ä¸‹ï¼Œ`x0`ç›®æ ‡æ˜¯éå¸¸å®¹æ˜“çš„ï¼Œè€Œå‡†ç¡®é¢„æµ‹å™ªå£°å‡ ä¹æ˜¯ä¸å¯èƒ½çš„ã€‚åŒæ ·ï¼Œåœ¨æé«˜çš„å™ªå£°æ°´å¹³ä¸‹ï¼Œ`eps`ç›®æ ‡æ˜¯å®¹æ˜“çš„ï¼Œè€Œå‡†ç¡®é¢„æµ‹å»å™ªå›¾åƒå‡ ä¹æ˜¯ä¸å¯èƒ½çš„ã€‚ä¸¤ç§æƒ…å†µéƒ½ä¸ç†æƒ³ï¼Œå› æ­¤å¼•å…¥äº†å…¶ä»–ç›®æ ‡ï¼Œä½¿æ¨¡å‹åœ¨ä¸åŒçš„æ—¶é—´æ­¥é¢„æµ‹`x0`å’Œ`eps`çš„æ··åˆã€‚[â€œç”¨äºå¿«é€Ÿé‡‡æ ·æ‰©æ•£æ¨¡å‹çš„æ¸è¿›è’¸é¦ã€‚â€](https://arxiv.org/abs/2202.00512)ä¸­å¼•å…¥çš„`v`ç›®æ ‡æ˜¯å…¶ä¸­ä¹‹ä¸€ï¼Œå®ƒè¢«å®šä¹‰ä¸º<math
    alttext="v equals StartRoot alpha overbar EndRoot dot epsilon plus StartRoot 1
    minus alpha overbar EndRoot dot x 0"><mrow><mi>v</mi> <mo>=</mo> <msqrt><mover
    accent="true"><mi>Î±</mi> <mo>Â¯</mo></mover></msqrt> <mo>Â·</mo> <mi>Ïµ</mi> <mo>+</mo>
    <msqrt><mrow><mn>1</mn> <mo>-</mo> <mover accent="true"><mi>Î±</mi> <mo>Â¯</mo></mover></mrow></msqrt>
    <mo>Â·</mo> <msub><mi>x</mi> <mn>0</mn></msub></mrow></math>ã€‚Karrasç­‰äººåœ¨[EDM paper](https://arxiv.org/abs/2206.00364)ä¸­é€šè¿‡ä¸€ä¸ªç§°ä¸º`c_skip`çš„å‚æ•°å¼•å…¥äº†ç±»ä¼¼çš„æƒ³æ³•ï¼Œå¹¶å°†ä¸åŒçš„æ‰©æ•£æ¨¡å‹å…¬å¼ç»Ÿä¸€åˆ°ä¸€ä¸ªä¸€è‡´çš„æ¡†æ¶ä¸­ã€‚å¦‚æœæ‚¨å¯¹äº†è§£ä¸åŒç›®æ ‡ã€ç¼©æ”¾å’Œå…¶ä»–ä¸åŒæ‰©æ•£æ¨¡å‹å…¬å¼çš„å¾®å¦™ä¹‹å¤„æ„Ÿå…´è¶£ï¼Œæˆ‘ä»¬å»ºè®®é˜…è¯»ä»–ä»¬çš„è®ºæ–‡ä»¥è·å¾—æ›´æ·±å…¥çš„è®¨è®ºã€‚
- en: 'Project Time: Train Your Own Diffusion Model'
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é¡¹ç›®æ—¶é—´ï¼šè®­ç»ƒæ‚¨è‡ªå·±çš„æ‰©æ•£æ¨¡å‹
- en: Now that you have an understanding of the basics of diffusion models, itâ€™s time
    to train some for yourself! The supplementary material for this chapter includes
    a notebook that walks you through the process of training a diffusion model on
    your own dataset. As you work through it, check back with this chapter and see
    how the different pieces fit together. The notebook also includes lots of suggested
    changes you can make to better explore how different model architectures and training
    strategies affect the results.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å·²ç»äº†è§£äº†æ‰©æ•£æ¨¡å‹çš„åŸºç¡€çŸ¥è¯†ï¼Œç°åœ¨æ˜¯æ—¶å€™è‡ªå·±è®­ç»ƒä¸€äº›äº†ï¼æœ¬ç« çš„è¡¥å……ææ–™åŒ…æ‹¬ä¸€ä¸ªç¬”è®°æœ¬ï¼ŒæŒ‡å¯¼æ‚¨å¦‚ä½•åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šè®­ç»ƒæ‰©æ•£æ¨¡å‹çš„è¿‡ç¨‹ã€‚åœ¨æ‚¨è¿›è¡Œæ“ä½œæ—¶ï¼Œè¯·å›é¡¾æœ¬ç« ï¼Œçœ‹çœ‹ä¸åŒéƒ¨åˆ†æ˜¯å¦‚ä½•ç›¸äº’é…åˆçš„ã€‚ç¬”è®°æœ¬è¿˜åŒ…æ‹¬è®¸å¤šå»ºè®®çš„æ›´æ”¹ï¼Œä»¥æ›´å¥½åœ°æ¢ç´¢ä¸åŒçš„æ¨¡å‹æ¶æ„å’Œè®­ç»ƒç­–ç•¥å¦‚ä½•å½±å“ç»“æœã€‚
- en: Summary
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ€»ç»“
- en: In this chapter, weâ€™ve seen how the idea of iterative refinement can be applied
    to train a diffusion model capable of turning noise into beautiful images. Youâ€™ve
    seen some of the design choices that go into creating a successful diffusion model,
    and hopefully put them into practice by training your own model. In the next chapter,
    weâ€™ll take a look at some of the more advanced techniques that have been developed
    to improve the performance of diffusion models and to give them extraordinary
    new capabilities!
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†è¿­ä»£æ”¹è¿›çš„æƒ³æ³•å¦‚ä½•åº”ç”¨äºè®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿå°†å™ªéŸ³è½¬åŒ–ä¸ºç¾ä¸½çš„å›¾åƒã€‚æ‚¨å·²ç»çœ‹åˆ°äº†ä¸€äº›è®¾è®¡é€‰æ‹©ï¼Œè¿™äº›é€‰æ‹©æ˜¯åˆ›å»ºæˆåŠŸçš„æ‰©æ•£æ¨¡å‹æ‰€å¿…éœ€çš„ï¼Œå¹¶å¸Œæœ›é€šè¿‡è®­ç»ƒè‡ªå·±çš„æ¨¡å‹æ¥å®è·µè¿™äº›é€‰æ‹©ã€‚åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹çœ‹ä¸€äº›æ›´å…ˆè¿›çš„æŠ€æœ¯ï¼Œè¿™äº›æŠ€æœ¯å·²ç»è¢«å¼€å‘å‡ºæ¥ï¼Œä»¥æ”¹è¿›æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶èµ‹äºˆå®ƒä»¬éå‡¡çš„æ–°èƒ½åŠ›ï¼
- en: References
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: 'Ho, Jonathan, Ajay Jain, and Pieter Abbeel. â€œDenoising diffusion probabilistic
    models.â€ Advances in Neural Information Processing Systems 33 (2020): 6840-6851.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 'Ho, Jonathan, Ajay Jain, and Pieter Abbeel. â€œDenoising diffusion probabilistic
    models.â€ Advances in Neural Information Processing Systems 33 (2020): 6840-6851.'
- en: 'Ronneberger, O., Fischer, P. and Brox, T., 2015\. U-net: Convolutional networks
    for biomedical image segmentation. In Medical Image Computing and Computer-Assisted
    Interventionâ€“MICCAI 2015: 18th International Conference, Munich, Germany, October
    5-9, 2015, Proceedings, Part III 18 (pp. 234-241). Springer International Publishing.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 'Ronneberger, O., Fischer, P. and Brox, T., 2015. U-net: Convolutional networks
    for biomedical image segmentation. In Medical Image Computing and Computer-Assisted
    Interventionâ€“MICCAI 2015: 18th International Conference, Munich, Germany, October
    5-9, 2015, Proceedings, Part III 18 (pp. 234-241). Springer International Publishing.'
- en: 'Bansal, Arpit, Eitan Borgnia, Hong-Min Chu, Jie S. Li, Hamid Kazemi, Furong
    Huang, Micah Goldblum, Jonas Geiping, and Tom Goldstein. â€œCold diffusion: Inverting
    arbitrary image transforms without noise.â€ arXiv preprint arXiv:2208.09392 (2022).'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 'Bansal, Arpit, Eitan Borgnia, Hong-Min Chu, Jie S. Li, Hamid Kazemi, Furong
    Huang, Micah Goldblum, Jonas Geiping, and Tom Goldstein. â€œCold diffusion: Inverting
    arbitrary image transforms without noise.â€ arXiv preprint arXiv:2208.09392 (2022).'
- en: 'Hoogeboom, Emiel, Jonathan Heek, and Tim Salimans. â€œsimple diffusion: End-to-end
    diffusion for high resolution images.â€ arXiv preprint arXiv:2301.11093 (2023).'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 'Hoogeboom, Emiel, Jonathan Heek, and Tim Salimans. â€œsimple diffusion: End-to-end
    diffusion for high resolution images.â€ arXiv preprint arXiv:2301.11093 (2023).'
- en: 'Chang, Huiwen, Han Zhang, Jarred Barber, A. J. Maschinot, Jose Lezama, Lu Jiang,
    Ming-Hsuan Yang et al. â€œMuse: Text-To-Image Generation via Masked Generative Transformers.â€
    arXiv preprint arXiv:2301.00704 (2023).'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 'Chang, Huiwen, Han Zhang, Jarred Barber, A. J. Maschinot, Jose Lezama, Lu Jiang,
    Ming-Hsuan Yang et al. â€œMuse: Text-To-Image Generation via Masked Generative Transformers.â€
    arXiv preprint arXiv:2301.00704 (2023).'
- en: 'Chang, Huiwen, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. â€œMaskgit:
    Masked generative image transformer.â€ In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 11315-11325\. 2022.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 'Chang, Huiwen, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. â€œMaskgit:
    Masked generative image transformer.â€ In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 11315-11325. 2022.'
- en: Rampas, Dominic, Pablo Pernias, Elea Zhong, and Marc Aubreville. â€œFast Text-Conditional
    Discrete Denoising on Vector-Quantized Latent Spaces.â€ arXiv preprint arXiv:2211.07292
    (2022).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Rampas, Dominic, Pablo Pernias, Elea Zhong, and Marc Aubreville. â€œFast Text-Conditional
    Discrete Denoising on Vector-Quantized Latent Spaces.â€ arXiv preprint arXiv:2211.07292
    (2022).
- en: Chen, Ting â€œOn the Importance of Noise Scheduling for Diffusion Models.â€ arXiv
    preprint arXiv:2301.10972 (2023).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Chen, Ting â€œOn the Importance of Noise Scheduling for Diffusion Models.â€ arXiv
    preprint arXiv:2301.10972 (2023).
- en: Peebles, William, and Saining Xie. â€œScalable Diffusion Models with Transformers.â€
    arXiv preprint arXiv:2212.09748 (2022).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Peebles, William, and Saining Xie. â€œScalable Diffusion Models with Transformers.â€
    arXiv preprint arXiv:2212.09748 (2022).
- en: Jabri, Allan, David Fleet, and Ting Chen. â€œScalable Adaptive Computation for
    Iterative Generation.â€ arXiv preprint arXiv:2212.11972 (2022).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Jabri, Allan, David Fleet, and Ting Chen. â€œScalable Adaptive Computation for
    Iterative Generation.â€ arXiv preprint arXiv:2212.11972 (2022).
- en: Salimans, Tim, and Jonathan Ho. â€œProgressive distillation for fast sampling
    of diffusion models.â€ arXiv preprint arXiv:2202.00512 (2022).)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: Salimans, Tim, and Jonathan Ho. â€œProgressive distillation for fast sampling
    of diffusion models.â€ arXiv preprint arXiv:2202.00512 (2022).)
- en: Karras, Tero, Miika Aittala, Timo Aila, and Samuli Laine. â€œElucidating the design
    space of diffusion-based generative models.â€ arXiv preprint arXiv:2206.00364 (2022).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: Karras, Tero, Miika Aittala, Timo Aila, and Samuli Laine. â€œElucidating the design
    space of diffusion-based generative models.â€ arXiv preprint arXiv:2206.00364 (2022).
