- en: Chapter 1\. Diffusion Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In late 2020 a little-known class of models called diffusion models began causing
    a stir in the machine-learning world. Researchers figured out how to use these
    models to generate synthetic images at higher quality than any produced by previous
    techniques. A flurry of papers followed, proposing improvements and modifications
    that pushed the quality up even further. By late 2021 there were models like GLIDE
    that showcased incredible results on text-to-image tasks, and a few months later,
    these models had entered the mainstream with tools like DALL-E 2 and Stable Diffusion.
    These models made it easy for anyone to generate images just by typing in a text
    description of what they wanted to see.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’re going to dig into the details of how these models work.
    We’ll outline the key insights that make them so powerful, generate images with
    existing models to get a feel for how they work, and then train our own models
    to deepen this understanding further. The field is still rapidly evolving, but
    the topics covered here should give you a solid foundation to build on. Chapter
    5 will explore more advanced techniques through the lens of a model called Stable
    Diffusion, and chapter 6 will explore applications of these techniques beyond
    simple image generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Key Insight: Iterative Refinement'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So what is it that makes diffusion models so powerful? Previous techniques,
    such as VAEs or GANs, generate their final output via a single forward pass of
    the model. This means the model must get everything right on the first try. If
    it makes a mistake, it can’t go back and fix it. Diffusion models, on the other
    hand, generate their output by iterating over many steps. This ‘iterative refinement’
    allows the model to correct mistakes made in previous steps and gradually improve
    the output. To illustrate this, let’s look at an example of a diffusion model
    in action.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can load a pre-trained model using the Hugging Face diffusers library. The
    pipeline can be used to create images directly, but this doesn’t show us what
    is going on under the hood:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![image](assets/cell-4-output-3.png)'
  prefs: []
  type: TYPE_IMG
- en: We can re-create the sampling process step by step to get a better look at what
    is happening as the model generates images. We initialize our sample x with random
    noise and then run it through the model for 30 steps. On the right, you can see
    the model’s prediction for what the final image will look like at specific steps
    - note that the initial predictions are not particularly good! Instead of jumping
    right to that final predicted image, we only modify x by a small amount in the
    direction of the prediction (shown on the left). We then feed this new, slightly
    better x through the model again for the next step, hopefully resulting in a slightly
    improved prediction, which can be used to update x a little more, and so on. With
    enough steps, the model can produce some impressively realistic images.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![image](assets/cell-5-output-1.png)'
  prefs: []
  type: TYPE_IMG
- en: '![image](assets/cell-5-output-2.png)'
  prefs: []
  type: TYPE_IMG
- en: '![image](assets/cell-5-output-3.png)'
  prefs: []
  type: TYPE_IMG
- en: '![image](assets/cell-5-output-4.png)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Don’t worry if that code looks a bit intimidating - we’ll explain how this all
    works over the course of this chapter. For now, just focus on the results.
  prefs: []
  type: TYPE_NORMAL
- en: This core idea of learning how to refine a ‘corrupted’ input gradually can be
    applied to a wide range of tasks. In this chapter, we’ll focus on unconditional
    image generation - that is, generating images that resemble the training data,
    with no additional controls over what these generated samples look like. Diffusion
    models have also been applied to audio, video, text and more. And while most implementations
    use some variant of the ‘denoising’ approach that we’ll cover here, new approaches
    utilizing different types of ‘corruption’ together with iterative refinement are
    emerging that may move the field beyond the current focus on denoising diffusion
    specifically. Exciting times!
  prefs: []
  type: TYPE_NORMAL
- en: Training a Diffusion Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we’re going to train a diffusion model from scratch to gain
    a better understanding of how they work. We’ll start by using components from
    the Hugging Face diffusers library. As the chapter progresses, we’ll gradually
    demystify how each component works. Training a diffusion model is relatively straightforward
    compared to other types of generative models. We repeatedly:'
  prefs: []
  type: TYPE_NORMAL
- en: Load in some images from the training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add noise in different amounts. Remember, we want the model to do a good job
    estimating how to ‘fix’ (denoise) both extremely noisy images and images that
    are close to perfect.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feed the noisy versions of the inputs into the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate how well the model does at denoising these inputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use this information to update the model weights.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To generate new images with a trained model, we begin with a completely random
    input and repeatedly feed it through the model, updating the input on each iteration
    by a small amount based on the model prediction. As we’ll see, there are a number
    of sampling methods that try to streamline this process so that we can generate
    good images with as few steps as possible.
  prefs: []
  type: TYPE_NORMAL
- en: The Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this example, we’ll use a dataset of images from the Hugging Face Hub- specifically,
    [this collection of 1000 butterfly pictures](https://huggingface.co/datasets/huggan/smithsonian_butterflies_subset).
    Later on, in the projects section, you will see how to use your own data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to do some preparation before this data can be used to train a model.
    Images are typically represented as a grid of ‘pixels’, with color values between
    0 and 255 for each of the three color channels (Red, Green and Blue). To process
    these and make them ready for training, we: - Resize them to a fixed size - (Optional)
    Add some augmentation by randomly flipping them horizontally, effectively doubling
    the size of our dataset - Convert them to a PyTorch tensor (which represents the
    color values as floats between 0 and 1) - Normalize them to have a mean of 0,
    with values between -1 and 1'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can do all of this with `torchvision.transforms`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to create a dataloader to load the data in batches with these
    transforms applied:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We can check that this worked by loading a single batch and inspecting the images.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![image](assets/cell-9-output-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Adding Noise
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'How do we gradually corrupt our data? The most common approach is to add noise
    to the images. The amount of noise we add is controlled by a noise schedule. Different
    papers and approaches tackle this in different ways, which we’ll explore later
    in the chapter. For now, let’s see one common approach in action based on the
    paper [“Denoising diffusion probabilistic models”](https://arxiv.org/abs/2006.11239)
    by Ho et al. In the diffusers library, adding noise is handled by something called
    a scheduler, which takes in a batch of images and a list of ‘timesteps’ and determines
    how to create the noisy versions of those images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![image](assets/cell-10-output-1.png)'
  prefs: []
  type: TYPE_IMG
- en: During training, we’ll pick the timesteps at random. The scheduler takes some
    parameters (beta_start and beta_end) which it uses to determine how much noise
    should be present for a given timestep. We will cover schedulers in more detail
    in section X.
  prefs: []
  type: TYPE_NORMAL
- en: The UNet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: UNet is a convolutional neural network invented for tasks such as image segmentation,
    where the desired output has the same spatial extent as the input. It consists
    of a series of ‘downsampling’ layers that reduce the spatial size of the input,
    followed by a series of ‘upsampling’ layers that increase the spatial extent of
    the input again. The downsampling layers are also typically followed by a ‘skip
    connection’ that connects the downsampling layer’s output to the upsampling layer’s
    input. This allows the upsampling layers to ‘see’ the higher-resolution representations
    from earlier in the network, which is useful for tasks with image-like outputs
    where this high-resolution information is especially useful.
  prefs: []
  type: TYPE_NORMAL
- en: The UNet architecture used in the diffusers library is more advanced than the
    [original UNet proposed in 2015](https://arxiv.org/abs/1505.04597) by Ronneberger
    et al, with additions like attention and residual blocks. We’ll take a closer
    look later, but the key feature here is that it can take in an input (the noisy
    image) and produce a prediction that is the same shape (the predicted noise).
    For diffusion models, the UNet typically also takes in the timestep as additional
    conditioning, which again we will explore in the UNet deep dive section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how we might create a UNet and feed our batch of noisy images through
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Note that the output is the same shape as the input, which is exactly what we
    want.
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have our model and our data ready, we can train it. We’ll use the
    AdamW optimizer with a learning rate of 3e-4\. For each training step, we:'
  prefs: []
  type: TYPE_NORMAL
- en: Load a batch of images.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add noise to the images, choosing random timesteps to determine how much noise
    is added.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feed the noisy images into the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the loss, which is the mean squared error between the model’s predictions
    and the target - which in this case is the *noise* that we added to the images.
    This is called the noise or ‘epsilon’ objective. You can find more information
    on the different training objectives in section X.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backpropagate the loss and update the model weights with the optimizer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s what all of that looks like in code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'It takes an hour or so to run the above code on a GPU, so get some tea while
    you wait or lower the number of epochs. Here’s what the loss curve looks like
    after training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![image](assets/cell-12-output-1.png)'
  prefs: []
  type: TYPE_IMG
- en: The loss curve trends downwards as the model learns to denoise the images. The
    curve is fairly noisy, thanks to different amounts of noise being added to the
    images based on the random sampling of timesteps for each iteration. It is hard
    to tell just by looking at the mean squared error of the noise predictions whether
    this model will be any good at generating samples, so let’s move on to the next
    section and see how well it does.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The diffusers library uses the idea of ‘pipelines’ which bundle together all
    of the components needed to generate samples with a diffusion model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![image](assets/cell-13-output-2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Of course, offloading the job of creating samples to the pipeline doesn’t really
    show us what is going on. So, here is a simple sampling loop that shows how the
    model is gradually refining the input image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![image](assets/cell-14-output-1.png)'
  prefs: []
  type: TYPE_IMG
- en: This is the same code we used at the beginning of the chapter to illustrate
    the idea of iterative refinement, but hopefully, now you have a better understanding
    of what is going on here. We start with a completely random input, which is then
    refined by the model in a series of steps. Each step is a small update to the
    input, based on the model’s prediction for the noise at that timestep. We’re still
    abstracting away some complexity behind the call to `pipeline.scheduler.step()`
    - in a later chapter we will dive deeper into different sampling methods and how
    they work.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative model performance can be evaluated using FID scores (Fréchet Inception
    Distance). FID scores measure how closely generated samples match real-world samples
    by comparing statistics between feature maps extracted from both sets of data
    using a pre-trained neural network. The lower the score, the better the quality
    and realism of generated images produced by a given model. FID scores are popular
    due to their ability to provide an ‘objective’ comparison metric for different
    types of generative networks without relying on human judgment.
  prefs: []
  type: TYPE_NORMAL
- en: 'As convenient as FID scores are, there are some important caveats to be aware
    of:'
  prefs: []
  type: TYPE_NORMAL
- en: The FID score for a given model depends on the number of samples used to calculate
    it, so when comparing between model,s we need to make sure both reported scores
    are calculated using the same number of samples. Common practice is to use 50,000
    samples for this purpose, although to save time, you may evaluate on a smaller
    number of samples during development and only do the full evaluation once you’re
    ready to publish the results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When calculating FID, images are resized to 299px square images. This makes
    it less useful as a metric for extremely low-res or high-res images. There are
    also minor differences between how resizing is handled by different deep learning
    frameworks, which can result in small differences in the FID score! We recommend
    using a library such as `clean-fid` to standardize the FID calculation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The network used as a feature extractor for FID is typically a model trained
    on the Imagenet classification task. When generating images in a different domain,
    the features learned by this model may be less useful. A more accurate approach
    would be to somehow train a classification network on domain-specific data first,
    but this would make it harder to compare scores between different papers and approaches,
    so for now the imagenet model is the standard choice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you save generated samples for later evaluation, the format and compression
    can again affect the FID score. Avoid low-quality JPEG images where possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even if you account for all these caveats, FID scores are just a rough measure
    of quality and do not perfectly capture the nuances of what makes images look
    more ‘real’. So, use them to get an idea of how one model performs relative to
    another but also look at the actual images generated by each model to get a better
    sense of how they compare. Human preference is still the gold standard for quality
    in what is ultimately a fairly subjective field!
  prefs: []
  type: TYPE_NORMAL
- en: 'In Depth: Noise Schedules'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the training example above, one of the steps was ‘add noise, in different
    amounts’. We achieved this by picking a random timestep between 0 and 1000 and
    then relying on the scheduler to add the appropriate amount of noise. Likewise,
    during sampling, we again relied on the scheduler to tell us which timesteps to
    use and how to move from one to the next given the model predictions. It turns
    out that choosing how much noise to add is an important design decision that can
    drastically affect the performance of a given model. In this section, we’ll see
    why this is the case and explore some of the different approaches that are used
    in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Why Add Noise?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the start of this chapter, we said that the key idea behind diffusion models
    is that of iterative refinement. During training, we ‘corrupt’ an input by different
    amounts. During inference, we begin with a ‘maximally corrupted’ input and iteratively
    ‘de-corrupt’ it, in the hopes that we will eventually end up with a nice final
    result.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we’ve focused on one specific kind of ‘corruption’: adding Gaussian
    noise. One reason for this is the theoretical underpinnings of diffusion models
    - if we use a different corruption method we are no longer technically doing ‘diffusion’!
    However, a paper titled [*Cold Diffusion*](https://arxiv.org/abs/2208.09392) by
    Bansal et al dramatically demonstrated that we do not necessarily need to constrain
    ourselves to this method just for theoretical convenience. They showed that a
    diffusion-model-like approach works for many different ‘corruption’ methods (see
    [Figure 1-1](#c4_f1)). More recently, models like [MUSE](https://arxiv.org/abs/2301.00704),
    [MaskGIT](https://arxiv.org/abs/2202.04200) and [PAELLA](https://arxiv.org/abs/2211.07292)
    have used random token masking or replacement as an equivalent ‘corruption’ method
    for quantized data - that is, data that is represented by discrete tokens rather
    than continuous values.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image.png](assets/84ca400c-1-image.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-1\. Illustration of the different degradations used in the Cold Diffusion
    Paper
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Nonetheless, adding noise remains the most popular approach for several reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: We can easily control the amount of noise added, giving a smooth transition
    from ‘perfect’ to ‘completely corrupted’. This is not the case for something like
    reducing the resolution of an image, which may result in ‘discrete’ transitions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can have many valid random starting points for inference, unlike some methods
    which may only have a limited number of possible initial (fully corrupted) states,
    such as a completely black image or a single-pixel image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, for the moment at least, we’ll stick with adding noise as our corruption
    method. Next, let’s take a closer look at how we add noise to our images.
  prefs: []
  type: TYPE_NORMAL
- en: Starting Simple
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have some images (x) and we’d like to combine them somehow with some random
    noise.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'One way we could do this is to linearly interpolate (lerp) between them by
    some amount. This gives us a function that smoothly transitions from the original
    image x to pure noise as the ‘amount’ varies from 0 to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see this in action on a batch of data, with the amount of noise varying
    from 0 to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![image](assets/cell-17-output-1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This seems to be doing exactly what we want, smoothly transitioning from the
    original image to pure noise. Now, we’ve created a noise schedule here that takes
    in a value for ‘amount’ from 0 to 1\. This is called the ‘continuous time’ approach,
    where we represent the full path on a time scale from 0 to 1\. Other approaches
    use a discrete time approach, with some large integer number of ‘timesteps’ used
    to define the noise scheduler. We can wrap our function into a class that converts
    from continuous time to discrete timesteps and adds noise appropriately:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![image](assets/cell-18-output-1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we have something that we can directly compare to the schedulers used in
    the diffusers library, such as the DDPMScheduler we used during training. Let’s
    see how it compares:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![image](assets/cell-19-output-1.png)'
  prefs: []
  type: TYPE_IMG
- en: The Maths
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are many competing notations and approaches in the literature. For example,
    some papers parametrize the noise schedule in *continuous-time* where t runs from
    0 (no noise) to 1 (fully corrupted) - just like our `corrupt` function in the
    previous section. Others use a *discrete-time* approach with integer timesteps
    running from 0 to some large number T, typically 1000\. It is possible to convert
    between these two approaches the way we did with our `SimpleScheduler` class -
    just make sure you’re consistent when comparing different models. We’ll stick
    with the discrete-time approach here.
  prefs: []
  type: TYPE_NORMAL
- en: A good place to start for pushing deeper into the maths is the DDPM paper mentioned
    earlier. You can find an [annotated implementation here](https://huggingface.co/blog/annotated-diffusion)
    which is a great additional resource for understanding this approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'The paper begins by specifying a single noise step to go from timestep t-1
    to timestep t. Here’s how they write it:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>q</mi> <mrow><mo>(</mo> <msub><mi>𝐱</mi> <mi>t</mi></msub>
    <mo>|</mo> <msub><mi>𝐱</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>)</mo></mrow> <mo>=</mo> <mi>𝒩</mi> <mrow><mo>(</mo> <msub><mi>𝐱</mi> <mi>t</mi></msub>
    <mo>;</mo> <msqrt><mrow><mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mi>t</mi></msub></mrow></msqrt>
    <msub><mi>𝐱</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub> <mo>,</mo>
    <msub><mi>β</mi> <mi>t</mi></msub> <mi>𝐈</mi> <mo>)</mo></mrow> <mo>.</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Here <math alttext="beta Subscript t"><msub><mi>β</mi> <mi>t</mi></msub></math>
    is defined for all timesteps t and is used to specify how much noise is added
    at each step. This notation can be a little intimidating, but what this equation
    tells us is that the noisier <math alttext="bold x Subscript t"><msub><mi>𝐱</mi>
    <mi>t</mi></msub></math> is a *distribution* with a mean of <math alttext="StartRoot
    1 minus beta Subscript t Baseline EndRoot bold x Subscript t minus 1"><mrow><msqrt><mrow><mn>1</mn>
    <mo>-</mo> <msub><mi>β</mi> <mi>t</mi></msub></mrow></msqrt> <msub><mi>𝐱</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></math> and a variance
    of <math alttext="beta Subscript t"><msub><mi>β</mi> <mi>t</mi></msub></math>
    . In other words, <math alttext="bold x Subscript t"><msub><mi>𝐱</mi> <mi>t</mi></msub></math>
    is a mix of <math alttext="bold x Subscript t minus 1"><msub><mi>𝐱</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>
    (scaled by <math alttext="StartRoot 1 minus beta Subscript t Baseline EndRoot"><msqrt><mrow><mn>1</mn>
    <mo>-</mo> <msub><mi>β</mi> <mi>t</mi></msub></mrow></msqrt></math> ) and some
    random noise, which we can think of as unit-variance noise scaled by <math alttext="StartRoot
    beta Subscript t Baseline EndRoot"><msqrt><msub><mi>β</mi> <mi>t</mi></msub></msqrt></math>
    . Given <math alttext="x Subscript t minus 1"><msub><mi>x</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>
    and some noise <math alttext="epsilon"><mi>ϵ</mi></math> , we can sample from
    this distribution to get <math alttext="x Subscript t"><msub><mi>x</mi> <mi>t</mi></msub></math>
    with:'
  prefs: []
  type: TYPE_NORMAL
- en: <math><mrow><msub><mi>𝐱</mi> <mi>t</mi></msub> <mo>=</mo> <msqrt><mrow><mn>1</mn>
    <mo>-</mo> <msub><mi>β</mi> <mi>t</mi></msub></mrow></msqrt> <msub><mi>𝐱</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub> <mo>+</mo> <msqrt><msub><mi>β</mi>
    <mi>t</mi></msub></msqrt> <mi>ϵ</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the noisy input at timestep t, we could begin at t=0 and repeatedly
    apply this single step, but this would be very inefficient. Instead, we can find
    a formula to move to any timestep t in one go. We define <math alttext="alpha
    Subscript t Baseline equals 1 minus beta Subscript t"><mrow><msub><mi>α</mi> <mi>t</mi></msub>
    <mo>=</mo> <mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mi>t</mi></msub></mrow></math>
    and then use the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: <math><mrow><msub><mi>x</mi> <mi>t</mi></msub> <mo>=</mo> <msqrt><msub><mover
    accent="true"><mi>α</mi> <mo>¯</mo></mover> <mi>t</mi></msub></msqrt> <msub><mi>x</mi>
    <mn>0</mn></msub> <mo>+</mo> <msqrt><mrow><mn>1</mn> <mo>-</mo> <msub><mover accent="true"><mi>α</mi>
    <mo>¯</mo></mover> <mi>t</mi></msub></mrow></msqrt> <mi>ϵ</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where - <math alttext="epsilon"><mi>ϵ</mi></math> is some gaussian noise with
    unit variance - <math alttext="alpha overbar"><mover accent="true"><mi>α</mi>
    <mo>¯</mo></mover></math> (‘alpha_bar’) is the cumulative product of all the <math
    alttext="alpha"><mi>α</mi></math> values up to the time <math alttext="t"><mi>t</mi></math>
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'So <math alttext="x Subscript t"><msub><mi>x</mi> <mi>t</mi></msub></math>
    is a mixture of <math alttext="x 0"><msub><mi>x</mi> <mn>0</mn></msub></math>
    (scaled by <math alttext="StartRoot alpha overbar Subscript t Baseline EndRoot"><msqrt><msub><mover
    accent="true"><mi>α</mi> <mo>¯</mo></mover> <mi>t</mi></msub></msqrt></math> )
    and <math alttext="epsilon"><mi>ϵ</mi></math> (scaled by <math alttext="StartRoot
    1 minus alpha overbar Subscript t Baseline EndRoot"><msqrt><mrow><mn>1</mn> <mo>-</mo>
    <msub><mover accent="true"><mi>α</mi> <mo>¯</mo></mover> <mi>t</mi></msub></mrow></msqrt></math>
    ). In the diffusers library the <math alttext="alpha overbar"><mover accent="true"><mi>α</mi>
    <mo>¯</mo></mover></math> values are stored in `scheduler.alphas_cumprod`. Knowing
    this, we can plot the scaling factors for the original image <math alttext="x
    0"><msub><mi>x</mi> <mn>0</mn></msub></math> and the noise <math alttext="epsilon"><mi>ϵ</mi></math>
    across the different timesteps for a given scheduler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![image](assets/cell-21-output-1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our SimpleScheduler above just linearly mixes between the original image and
    noise, as we can see if we plot the scaling factors (equivalent to <math alttext="StartRoot
    alpha overbar Subscript t Baseline EndRoot"><msqrt><msub><mover accent="true"><mi>α</mi>
    <mo>¯</mo></mover> <mi>t</mi></msub></msqrt></math> and <math alttext="StartRoot
    left-parenthesis 1 minus alpha overbar Subscript t Baseline right-parenthesis
    EndRoot"><msqrt><mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mover accent="true"><mi>α</mi>
    <mo>¯</mo></mover> <mi>t</mi></msub> <mo>)</mo></mrow></msqrt></math> in the DDPM
    case):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![image](assets/cell-22-output-1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A good noise schedule will ensure that the model sees a mix of images at different
    noise levels. The best choice will differ based on the training data. Visualizing
    a few more options, note that:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting beta_end too low means we never completely erase the image, so the model
    will never see anything like the random noise used as a starting point for inference.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting beta_end extremely high means that most of the timesteps are spent on
    almost complete noise, which will result in poor training performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different beta schedules give different curves.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ‘cosine’ schedule is a popular choice, as it gives a smooth transition from
    the original image to the noise.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![image](assets/cell-23-output-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: All of the schedules shown here are called ‘Variance Preserving’ (VP), meaning
    that the variance of the model input is kept close to 1 across the entire schedule.
    You may also encounter ‘Variance Exploding’ (VE) formulations where noise is simply
    added to the original image in different amounts (resulting in high-variance inputs).
    We’ll go into this more in the chapter on sampling. Our SimpleScheduler is almost
    a VP schedule, but the variance is not quite preserved due to the linear interpolation.
  prefs: []
  type: TYPE_NORMAL
- en: As with many diffusion-related topics, there is a constant stream of new papers
    exploring the topic of noise schedules, so by the time you read this there will
    likely be a large collection of options to try out!
  prefs: []
  type: TYPE_NORMAL
- en: Effect of Input Resolution and Scaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One aspect of noise schedules that was mostly overlooked until recently is the
    effect of input size and scaling. Many papers test potential schedulers on small-scale
    datasets and at low resolution, and then use the best-performing scheduler to
    train their final models on larger images. The problem with this is can be seen
    if we add the same amount of noise to two images of different sizes.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](assets/cell-24-output-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-2\. Comparing the effect of adding noise to images of different sizes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Images at high resolution tend to contain a lot of redundant information. This
    means that even if a single pixel is obscured by noise, the surrounding pixels
    contain enough information to reconstruct the original image. This is not the
    case for low-resolution images, where a single pixel can contain a lot of information.
    This means that adding the same amount of noise to a low-resolution image will
    result in a much more corrupted image than adding the equivalent amount of noise
    to a high-resolution image.
  prefs: []
  type: TYPE_NORMAL
- en: 'This effect was thoroughly investigated in two independent papers, both of
    which came out in January 2023\. Each used the new insights to train models capable
    of generating high-resolution outputs without requiring any of the tricks that
    have previously been necessary. [*Simple diffusion*](https://arxiv.org/abs/2301.11093)
    by Hoogeboom et al introduced a method for adjusting the noise schedule based
    on the input size, allowing a schedule optimized on low-resolution images to be
    appropriately modified for a new target resolution. A paper called [“On the Importance
    of Noise Scheduling for Diffusion Models”](https://arxiv.org/abs/2301.10972) by
    Ting Chen performed similar experiments, and noted another key variable: input
    scaling. That is, how do we represent our images? If the images are represented
    as floats between 0 and 1 then they will have a lower variance than the noise
    (which is typically unit variance) and thus the signal-to-noise ratio will be
    lower for a given noise level than if the images were represented as floats between
    -1 and 1 (which we used in the training example above) or something else. Scaling
    the input images shifts the signal-to-noise ratio, and so modifying this scaling
    is another way we can adjust when training on larger images.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Depth: UNets and Alternatives'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let’s address the actual model that makes the all-important predictions!
    To recap, this model must be capable of taking in a noisy image and estimating
    how to denoise it. This requires a model that can take in an image of arbitrary
    size and output an image of the same size. Furthermore, the model should be able
    to make precise predictions at the pixel level, while also capturing higher-level
    information about the image as a whole. A popular approach is to use an architecture
    called a UNet. UNets were invented in 2015 for medical image segmentation, and
    have since become a popular choice for various image-related tasks. Like the AutoEncoders
    and VAEs we looked at in the previous chapter, UNets are made up of a series of
    ‘downsampling’ and ‘upsampling’ blocks. The downsampling blocks are responsible
    for reducing the size of the image, while the upsampling blocks are responsible
    for increasing the size of the image. The downsampling blocks are typically made
    up of a series of convolutional layers, followed by a pooling or downsampling
    layer. The upsampling blocks are typically made up of a series of convolutional
    layers, followed by an upsampling or ‘transposed convolution’ layer. The transposed
    convolution layer is a special type of convolutional layer that increases the
    size of the image, rather than reducing it.
  prefs: []
  type: TYPE_NORMAL
- en: The reason a regular AutoEncoder or VAE is not a good choice for this task is
    that they are less capable of making precise predictions at the pixel level since
    the output must be entirely re-constructed from the low-dimensional latent space.
    In a UNet, the downsampling and upsampling blocks are connected by ‘skip connections’,
    which allow information to flow directly from the downsampling blocks to the upsampling
    blocks. This allows the model to make precise predictions at the pixel level,
    while also capturing higher-level information about the image as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: A Simple UNet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To better understand the structure of a UNet, let’s build a simple UNet from
    scratch.
  prefs: []
  type: TYPE_NORMAL
- en: '![image.png](assets/595852ce-1-image.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-3\. Our simple UNet architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This UNet takes single-channel inputs at 32px resolution and outputs single-channel
    outputs at 32px resolution, which we could use to build a diffusion model for
    the MNIST dataset. There are three layers in the encoding path, and three layers
    in the decoding path. Each layer consists of a convolution followed by an activation
    function and an upsampling or downsampling step (depending on whether we are in
    the encoding or decoding path). The skip connections allow information to flow
    directly from the downsampling blocks to the upsampling blocks, and are implemented
    by adding the output of the downsampling block to the input of the corresponding
    upsampling block. Some UNets instead concatenate the output of the downsampling
    block to the input of the corresponding upsampling block, and may also include
    additional layers in the skip connections. Here’s what this network looks like
    in code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'A diffusion model trained with this architecture on MNIST produces the following
    samples (code included in the supplementary material but omitted here for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: Improving the UNet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This simple UNet works for this relatively easy task, but it is far from ideal.
    So, what can we do to improve it?
  prefs: []
  type: TYPE_NORMAL
- en: Add more parameters. This can be accomplished by using multiple convolutional
    layers in each block, by using a larger number of filters in each convolutional
    layer, or by making the network deeper.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add residual connections. Using ResBlocks instead of regular convolutional layers
    can help the model learn more complex functions while keeping training stable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add normalization, such as batch normalization. Batch normalization can help
    the model learn more quickly and reliably, by ensuring that the outputs of each
    layer are centered around 0 and have a standard deviation of 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add regularization, such as dropout. Dropout helps by preventing the model from
    overfitting to the training data, which is important when working with smaller
    datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add attention. By introducing self-attention layers we allow the model to focus
    on different parts of the image at different times, which can help it learn more
    complex functions. The addition of transformer-like attention layers also lets
    us increase the number of learnable parameters, which can help the model learn
    more complex functions. The downside is that attention layers are much more expensive
    to compute than regular convolutional layers at higher resolutions, so we typically
    only use them at lower resolutions (i.e. the lower resolution blocks in the UNet).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add an additional input for the timestep, so that the model can tailor its predicitons
    according to the noise level. This is called timestep conditioning, and is used
    in almost all recent diffusion models. We’ll see more on conditional models in
    the next chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For comparison, here are the results on MNIST when using the UNet implementation
    in the diffusers library, which features all of the above improvements:'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This section will likely be expanded with results and more details in the future.
    We just haven’t gotten around to training variants with the different improvements
    yet!
  prefs: []
  type: TYPE_NORMAL
- en: Alternative Architectures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'More recently, a number of alternative architectures have been proposed for
    diffusion models. These include:'
  prefs: []
  type: TYPE_NORMAL
- en: Transformers. The DiT paper ([“Scalable Diffusion Models with Transformers”](https://arxiv.org/abs/2212.09748))
    by Peebles and Xie showed that a transformer-based architecture can be used to
    train a diffusion model, with great results. However, the compute and memory requirements
    of the transformer architecture remain a challenge for very high resolutions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *UViT* architecture from the [Simple Diffusion paper](https://arxiv.org/abs/2301.11093)
    link aims to get the best of both worlds by replacing the middle layers of the
    UNet with a large stack of transformer blocks. A key insight of this paper was
    that focusing the majority of the compute at the lower resolution blocks of the
    UNet allows for more efficient training of high-resolution diffusion models. For
    very high resolutions, they do some additional pre-processing using something
    called a wavelet transform to reduce the spatial resolution of the input image
    while keeping as much information as possible through the use of additional channels,
    again reducing the amount of compute spent on the higher spatial resolutions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recurrent Interface Networks. The [RIN paper](https://arxiv.org/abs/2212.11972)
    (Jabri et al) takes a similar approach, first mapping the high-resolution inputs
    to a more manageable and lower-dimensional ‘latent’ representation which is then
    processed by a stack of transformer blocks before being decoded back out to an
    image. Additionally, the RIN paper introduces an idea of ‘recurrence’ where information
    is passed to the model from the previous processing step, which can be beneficial
    for the kind of iterative improvement that diffusion models are designed to perform.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It remains to be seen whether transformer-based approaches completely supplant
    UNets as the go-to architecture for diffusion models, or whether hybrid approaches
    like the UViT and RIN architectures will prove to be the most effective.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Depth: Objectives and Pre-Conditioning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We’ve spoken about diffusion models taking a noisy input and “learning to denoise”
    it. At first glance, you might assume that the natural prediction target for the
    network is the denoised version of the image, which we’ll call `x0`. However,
    in the code, we compared the model prediction with the unit-variance noise that
    was used to create the noisy version (often called the epsilon objective, `eps`).
    The two appear mathematically identical since if we know the noise and the timestep
    we can derive `x0` and vice versa. While this is true, the choice of objective
    has some subtle effects on how large the loss is at different timesteps, and thus
    which noise levels the model learns to denoise best. To gain some intuition, let’s
    visualize some different objectives across different timesteps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](assets/cell-26-output-1.png)'
  prefs: []
  type: TYPE_IMG
- en: At extremely low noise levels, the `x0` objective is trivially easy while predicting
    the noise accurately is almost impossible. Likewise, at extremely high noise levels,
    the `eps` objective is easy while predicting the denoised image accurately is
    almost impossible. Neither case is ideal, and so additional objectives have been
    introduced that have the model predict a mix of `x0` and `eps` at different timesteps.
    The `v` objective (introduced in [“Progressive distillation for fast sampling
    of diffusion models.”](https://arxiv.org/abs/2202.00512) by Salimans and Ho) is
    one such objective, which is defined as <math alttext="v equals StartRoot alpha
    overbar EndRoot dot epsilon plus StartRoot 1 minus alpha overbar EndRoot dot x
    0"><mrow><mi>v</mi> <mo>=</mo> <msqrt><mover accent="true"><mi>α</mi> <mo>¯</mo></mover></msqrt>
    <mo>·</mo> <mi>ϵ</mi> <mo>+</mo> <msqrt><mrow><mn>1</mn> <mo>-</mo> <mover accent="true"><mi>α</mi>
    <mo>¯</mo></mover></mrow></msqrt> <mo>·</mo> <msub><mi>x</mi> <mn>0</mn></msub></mrow></math>
    . The [EDM paper](https://arxiv.org/abs/2206.00364) by Karras et al introduce
    a similar idea via a parameter called `c_skip`, and unify the different diffusion
    model formulations into a consistent framework. If you’re interested in learning
    more about the different objectives, scalings and other nuances of the different
    diffusion model formulations, we recommend reading their paper for a more in-depth
    discussion.
  prefs: []
  type: TYPE_NORMAL
- en: 'Project Time: Train Your Own Diffusion Model'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have an understanding of the basics of diffusion models, it’s time
    to train some for yourself! The supplementary material for this chapter includes
    a notebook that walks you through the process of training a diffusion model on
    your own dataset. As you work through it, check back with this chapter and see
    how the different pieces fit together. The notebook also includes lots of suggested
    changes you can make to better explore how different model architectures and training
    strategies affect the results.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ve seen how the idea of iterative refinement can be applied
    to train a diffusion model capable of turning noise into beautiful images. You’ve
    seen some of the design choices that go into creating a successful diffusion model,
    and hopefully put them into practice by training your own model. In the next chapter,
    we’ll take a look at some of the more advanced techniques that have been developed
    to improve the performance of diffusion models and to give them extraordinary
    new capabilities!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Ho, Jonathan, Ajay Jain, and Pieter Abbeel. “Denoising diffusion probabilistic
    models.” Advances in Neural Information Processing Systems 33 (2020): 6840-6851.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ronneberger, O., Fischer, P. and Brox, T., 2015\. U-net: Convolutional networks
    for biomedical image segmentation. In Medical Image Computing and Computer-Assisted
    Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October
    5-9, 2015, Proceedings, Part III 18 (pp. 234-241). Springer International Publishing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bansal, Arpit, Eitan Borgnia, Hong-Min Chu, Jie S. Li, Hamid Kazemi, Furong
    Huang, Micah Goldblum, Jonas Geiping, and Tom Goldstein. “Cold diffusion: Inverting
    arbitrary image transforms without noise.” arXiv preprint arXiv:2208.09392 (2022).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hoogeboom, Emiel, Jonathan Heek, and Tim Salimans. “simple diffusion: End-to-end
    diffusion for high resolution images.” arXiv preprint arXiv:2301.11093 (2023).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Chang, Huiwen, Han Zhang, Jarred Barber, A. J. Maschinot, Jose Lezama, Lu Jiang,
    Ming-Hsuan Yang et al. “Muse: Text-To-Image Generation via Masked Generative Transformers.”
    arXiv preprint arXiv:2301.00704 (2023).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Chang, Huiwen, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. “Maskgit:
    Masked generative image transformer.” In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 11315-11325\. 2022.'
  prefs: []
  type: TYPE_NORMAL
- en: Rampas, Dominic, Pablo Pernias, Elea Zhong, and Marc Aubreville. “Fast Text-Conditional
    Discrete Denoising on Vector-Quantized Latent Spaces.” arXiv preprint arXiv:2211.07292
    (2022).
  prefs: []
  type: TYPE_NORMAL
- en: Chen, Ting “On the Importance of Noise Scheduling for Diffusion Models.” arXiv
    preprint arXiv:2301.10972 (2023).
  prefs: []
  type: TYPE_NORMAL
- en: Peebles, William, and Saining Xie. “Scalable Diffusion Models with Transformers.”
    arXiv preprint arXiv:2212.09748 (2022).
  prefs: []
  type: TYPE_NORMAL
- en: Jabri, Allan, David Fleet, and Ting Chen. “Scalable Adaptive Computation for
    Iterative Generation.” arXiv preprint arXiv:2212.11972 (2022).
  prefs: []
  type: TYPE_NORMAL
- en: Salimans, Tim, and Jonathan Ho. “Progressive distillation for fast sampling
    of diffusion models.” arXiv preprint arXiv:2202.00512 (2022).)
  prefs: []
  type: TYPE_NORMAL
- en: Karras, Tero, Miika Aittala, Timo Aila, and Samuli Laine. “Elucidating the design
    space of diffusion-based generative models.” arXiv preprint arXiv:2206.00364 (2022).
  prefs: []
  type: TYPE_NORMAL
