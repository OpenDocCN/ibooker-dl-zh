<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 10. Using APIs in Data Pipelines"><div class="chapter" id="chapter_10">
<h1><span class="label">Chapter 10. </span>Using APIs in Data Pipelines</h1>

<blockquote data-type="epigraph" epub:type="epigraph">
  <p>In their simplest form, pipelines may extract only data from one source such as a REST API and load to a destination such as a SQL table in a data warehouse. In practice, however, pipelines typically consist of multiple steps ... before delivering data to its final destination.</p>
  <p data-type="attribution">James Densmore <em>Data Pipelines Pocket Reference</em> (O’Reilly, 2021)</p>
</blockquote>

<p>In <a data-type="xref" href="ch09.html#chapter_9">Chapter 9</a>, you used<a data-type="indexterm" data-primary="Data Pipelines Pocket Reference (Densmore)" id="id1966"/><a data-type="indexterm" data-primary="Densmore, James" id="id1967"/> a Jupyter Notebook to query APIs and create data analytics. Querying directly in a notebook is useful for exploratory data analysis, but it requires you to keep querying the API over and over again. <a data-type="indexterm" data-primary="data pipelines" id="id1968"/>When data teams create analytics products for production, they implement scheduled processes to keep an up-to-date copy of source data in the format they need. These structured processes are called <em>data pipelines</em> because source data flows into the pipeline and is prepared and stored to create data products. <a data-type="indexterm" data-primary="Extract, Transform, Load (ETL)" id="id1969"/><a data-type="indexterm" data-primary="Extract, Load, Transform (ELT)" id="id1970"/><a data-type="indexterm" data-primary="ELT (Extract, Load, Transform)" id="id1971"/><a data-type="indexterm" data-primary="ETL (Extract, Transform, Load)" id="id1972"/>Other common terms for these processes are <em>Extract, Transform, Load (ETL)</em> or <em>Extract, Load, Transform (ELT)</em>, depending on the technical details of how they are implemented.<a data-type="indexterm" data-primary="data engineer" id="id1973"/> <em>Data engineer</em> is the specialized role that focuses on the development and operation of data pipelines, but in many organizations, data scientists, data analysts, and infrastructure engineers also perform this work.</p>

<p>In this chapter, you will create a data pipeline to read SportsWorldCentral fantasy football player data using Apache Airflow, a popular open source tool for managing data pipelines using Python.</p>






<section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="Types of Data Sources for Data Pipelines"><div class="sect1" id="id109">
<h1>Types of Data Sources for Data Pipelines</h1>

<p>The potential data sources for data pipelines are almost endless. Here are a few <span class="keep-together">examples:</span><a data-type="indexterm" data-primary="data pipelines" data-secondary="data sources for" id="id1974"/><a data-type="indexterm" data-primary="downloading data" data-secondary="data pipelines" data-seealso="data pipelines" id="id1975"/><a data-type="indexterm" data-primary="data acquisition" data-secondary="data pipeline sources of data" data-seealso="data pipelines" id="id1976"/><a data-type="indexterm" data-primary="REST (Representational State Transfer) APIs" data-secondary="data pipeline source for data" id="id1977"/><a data-type="indexterm" data-primary="APIs" data-secondary="data pipeline source of data" id="id1978"/><a data-type="indexterm" data-primary="bulk files" id="id1979"/></p>
<dl>
<dt>APIs</dt>
<dd>
<p>REST APIs are the focus of this book, and they are an important data source for data pipelines. They are better suited for incremental updates than full loads, because sending the full contents of a data source may require many network calls. Other API styles such as GraphQL and SOAP are also common.</p>
</dd>
<dt>Bulk files</dt>
<dd>
<p>Large datasets are often shared in some type of bulk file that can be downloaded and processed. This is an efficient way to process a very large data source. The file format of these may vary, but CSV and Parquet are popular formats for data science applications.</p>
</dd>
<dt>Streaming data and message queues</dt>
<dd>
<p>For near-real-time updates of data, streaming sources such as Apache Kafka or AWS Kinesis provide continuous feeds of updates.</p>
</dd>
<dt>Message queues</dt>
<dd>
<p>Message queue software such as RabbitMQ or AWS SQS provides asynchronous messaging, which allows transactions to be published in a holding location and picked up later by a subscriber.</p>
</dd>
<dt>Direct database connections</dt>
<dd>
<p>A connection to the source database allows a consumer to get data in its original format. These are more common for sharing data inside organizations than to outside consumers.</p>
</dd>
</dl>

<p>You will be creating a pipeline that uses REST APIs and bulk files in this chapter.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Planning Your Data Pipeline"><div class="sect1" id="id110">
<h1>Planning Your Data Pipeline</h1>

<p>Your goal is to read <a data-type="indexterm" data-primary="data pipelines" data-secondary="planning" id="id1980"/><a data-type="indexterm" data-primary="SportsWorldCentral (SWC)" data-secondary="website API" data-tertiary="data pipeline goal" id="id1981"/><a data-type="indexterm" data-primary="database creation and access" data-secondary="data pipeline plan" id="id1982"/><a data-type="indexterm" data-primary="bulk downloads" data-secondary="data pipeline plan" id="id1983"/><a data-type="indexterm" data-primary="bulk files" data-secondary="planning data pipeline" id="id1984"/><a data-type="indexterm" data-primary="analytics database" data-secondary="planning data pipeline" id="id1985"/><a data-type="indexterm" data-primary="data analytics" data-secondary="analytics database" data-tertiary="planning data pipeline" data-seealso="data pipelines" id="id1986"/><a data-type="indexterm" data-primary="database creation and access" data-secondary="analytics database" data-tertiary="planning data pipeline" id="id1987"/><a data-type="indexterm" data-primary="data pipelines" data-secondary="bulk downloads" data-tertiary="data pipeline plan" id="id1988"/>SportsWorldCentral data and store it in a local database that you can keep up to date. This allows you to create analytics products such as reports and dashboards. For this scenario, you’ll assume that the API does not allow full downloads of the data, so you will need to use a bulk file for the initial load.</p>

<p>After that initial load, you want to get a daily update of any new records or records that have been updated. <a data-type="indexterm" data-primary="deltas" id="id1989"/>These changed records are commonly referred to as <em>delta</em> or <em>deltas</em>, using the mathematical term for “change.” By processing only the changed records, the update process will run more quickly and use fewer resources (and spend less money).</p>

<p><a data-type="xref" href="#date_pipeline_plan_ch10">Figure 10-1</a> displays the data pipeline you are planning.</p>

<figure><div id="date_pipeline_plan_ch10" class="figure">
<img src="assets/haad_1001.png" alt="Plan for your data pipeline" width="919" height="380"/>
<h6><span class="label">Figure 10-1. </span>Plan for your data pipeline</h6>
</div></figure>

<p>The pipeline includes two sources: bulk data files and an API. The rounded boxes represent two ETL processes and they both will update the analytics database, a local database that is used to create analytics products like dashboards and reports.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Orchestrating the Data Pipeline with Apache Airflow"><div class="sect1" id="data_pipeline_apache_airflow">
<h1>Orchestrating the Data Pipeline with Apache Airflow</h1>
<blockquote data-type="epigraph" epub:type="epigraph">
  <p>Airflow is best thought of as a spider in a web: it sits in the middle of your data processes and coordinates work happening across the different (distributed) systems. </p>
  <p data-type="attribution">Julian de Ruiter and Bas Harenslak, <em>Data Pipelines with Apache Airflow</em> (Manning, 2021)</p>
</blockquote>

<p>Running multiple data processing <a data-type="indexterm" data-primary="Data Pipelines with Apache Airflow (de Ruiter and Harenslak)" id="id1990"/><a data-type="indexterm" data-primary="de Ruiter, Julian" id="id1991"/><a data-type="indexterm" data-primary="Harenslak, Bas" id="id1992"/><a data-type="indexterm" data-primary="data pipelines" data-secondary="Apache Airflow orchestrating" id="id1993"/><a data-type="indexterm" data-primary="orchestration of data pipeline with Apache Airflow" data-seealso="Apache Airflow" id="id1994"/><a data-type="indexterm" data-primary="Apache Airflow" data-secondary="about" data-tertiary="orchestration of data pipeline" id="id1995"/><a data-type="indexterm" data-primary="Apache Airflow" data-secondary="about" id="id1996"/>work streams in production gets complicated quickly. Scheduling, error handling, and restarting failed processes require significant planning and design. These tasks are called <em>orchestration</em>, and this is what Apache Airflow is used for. As the number of data pipelines grows, you will benefit from using orchestration software instead of coding all of these tasks yourself. Airflow is a full-featured open source engine that uses Python for its configuration, and it handles many of the recurring tasks involved in data pipelines.</p>

<p>Airflow has some specialized <a data-type="indexterm" data-primary="Apache Airflow" data-secondary="terminology" id="id1997"/><a data-type="indexterm" data-primary="Apache Airflow" data-secondary="terminology" data-tertiary="glossary online" id="id1998"/><a data-type="indexterm" data-primary="resources online" data-secondary="Apache Airflow" data-tertiary="glossary" id="id1999"/>terminology that is not used in other data science programming. Astronomer’s <a href="https://oreil.ly/IjTM4">Airflow glossary</a> is a complete source for these, but I will share some of the most important ones with you.</p>

<p>Airflow uses terminology from mathematical graph theory.<a data-type="indexterm" data-primary="nodes (Airflow)" id="id2000"/><a data-type="indexterm" data-primary="edges (Airflow)" id="id2001"/><a data-type="indexterm" data-primary="DAGs (directed acyclic graphs)" id="id2002"/> In graph theory, a <em>node</em> is a process and an <em>edge</em> is a flow between nodes. Using this terminology, a  <em>directed acyclic graph</em> (DAG) is a top-level process that contains steps proceeding in one direction without any loops or recursive logic.</p>

<p><a data-type="xref" href="#directed_acyclic_graph_ch10">Figure 10-2</a> shows how nodes and edges relate to each other in a DAG.</p>

<figure><div id="directed_acyclic_graph_ch10" class="figure">
<img src="assets/haad_1002.png" alt="Directed acyclic graph" width="420" height="519"/>
<h6><span class="label">Figure 10-2. </span>Directed acyclic graph</h6>
</div></figure>

<p>You will create one Python file for each DAG. <a data-type="indexterm" data-primary="DAGs (directed acyclic graphs)" data-secondary="tasks" id="id2003"/><a data-type="indexterm" data-primary="tasks of Apache Airflow" id="id2004"/><a data-type="indexterm" data-primary="Apache Airflow" data-secondary="tasks as basic units of execution" id="id2005"/>Each of the steps in a DAG is called a <em>task</em>, the basic unit of execution in Airflow. Each task will be displayed as a single box on the graph diagram of a DAG.</p>

<p>An <em>operator</em> is a predefined <a data-type="indexterm" data-primary="tasks of Apache Airflow" data-secondary="operators" id="id2006"/><a data-type="indexterm" data-primary="Apache Airflow" data-secondary="tasks as basic units of execution" data-tertiary="operators" id="id2007"/><a data-type="indexterm" data-primary="operators of Apache Airflow" id="id2008"/>template for a task. <a data-type="indexterm" data-primary="HttpOperator (Apache Airflow)" id="id2009"/><a data-type="indexterm" data-primary="PythonOperator (Apache Airflow)" id="id2010"/>In this chapter, you will use an <code>Http​Op⁠erator</code> to call your API and a <code>PythonOperator</code> to update your analytics database. Airflow has built-in operators to interact with databases, S3 buckets, and several other functions. <a data-type="indexterm" data-primary="Apache Airflow" data-secondary="Operators and Hooks Reference online" id="id2011"/><a data-type="indexterm" data-primary="resources online" data-secondary="Apache Airflow" data-tertiary="Operators and Hooks Reference" id="id2012"/>Dozens more are available from the community and are listed in the <a href="https://oreil.ly/8k6mr">Airflow Operators and Hooks Reference</a>.</p>

<p>The last thing you will learn<a data-type="indexterm" data-primary="XCom (Apache Airflow)" id="id2013"/> to use is an <em>XCom</em>, which stands for <em>cross-communications</em>. XComs are used to pass information and data between tasks.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Installing Apache Airflow in GitHub Codespaces"><div class="sect1" id="id112">
<h1>Installing Apache Airflow in GitHub Codespaces</h1>

<p><a data-type="xref" href="#ch10_architecture_airflow">Figure 10-3</a> shows the high-level architecture of the project you will create in this chapter.<a data-type="indexterm" data-primary="Apache Airflow" data-secondary="installing" id="ch10inst"/><a data-type="indexterm" data-primary="Codespaces (GitHub)" data-secondary="Apache Airflow installation" id="ch10inst2"/><a data-type="indexterm" data-primary="GitHub" data-secondary="Codespaces" data-tertiary="Apache Airflow installation" id="ch10inst3"/><a data-type="indexterm" data-primary="data pipelines" data-secondary="Apache Airflow orchestrating" data-tertiary="installing in Codespaces" id="ch10inst4"/></p>

<figure><div id="ch10_architecture_airflow" class="figure">
<img src="assets/haad_1003.png" alt="Architecture of Airflow project" width="683" height="392"/>
<h6><span class="label">Figure 10-3. </span>Architecture of the Airflow project</h6>
</div></figure>

<p>You will be working with the Part II GitHub Codespace that you created in <a data-type="xref" href="ch08.html#ch08_getting_started">“Getting Started with Your GitHub Codespace”</a>. If you haven’t created your Part II Codespace yet, you can complete that section now.</p>

<p>Before launching the Codespace, <a data-type="indexterm" data-primary="Codespaces (GitHub)" data-secondary="machine type setting" id="id2014"/><a data-type="indexterm" data-primary="GitHub" data-secondary="Codespaces" data-tertiary="machine type setting" id="id2015"/>change the machine type to a four-core machine by clicking the ellipsis next to the Codespace and then clicking “Change machine type.” This is necessary because Airflow runs multiple services at once.</p>

<p>You will be installing Airflow in the Codespace and performing that basic configuration that allows you to create the data pipeline from the diagram. (This will be a non-production setup for demonstration purposes. Before using Airflow in production, additional setup would be required.)</p>

<p>Airflow can be installed<a data-type="indexterm" data-primary="Docker container deployment" data-secondary="Apache Airflow in Docker" id="id2016"/><a data-type="indexterm" data-primary="Docker container deployment" data-secondary="Apache Airflow in Docker" data-tertiary="instructions online" id="id2017"/><a data-type="indexterm" data-primary="resources online" data-secondary="Docker" data-tertiary="running Apache Airflow" id="id2018"/><a data-type="indexterm" data-primary="resources online" data-secondary="Apache Airflow" data-tertiary="running in Docker" id="id2019"/> using Docker or <code>pip</code>. You will be using the Docker version. You will follow the instructions from <a href="https://oreil.ly/ORZKy">“Running Airflow in Docker”</a>, with a few customizations.</p>

<p>To begin, create an <em>airflow</em> directory in the <em>chapter10</em> folder of your Codespace and change to that directory:</p>

<pre data-type="programlisting" data-code-language="shell">.../chapter10 (main) $ mkdir airflow
.../chapter10 (main) $ cd airflow
.../airflow (main) $</pre>

<p>Next, use the <code>curl</code> command<a data-type="indexterm" data-primary="cURL" data-secondary="Apache Airflow installation in Docker" id="id2020"/> to retrieve a copy of the <em>docker-compose.yaml</em> file that is used to run the Docker version of Airflow. Get this from the <a href="https://airflow.apache.org">official Airflow website</a>, and specify the version. This chapter demonstrates with version 2.9.3, but you can follow the latest stable version listed in the <a href="https://oreil.ly/QTlk_">Airflow documentation</a>:</p>

<pre data-type="programlisting" data-code-language="shell">.../airflow (main) $ curl -LfO \
'https://airflow.apache.org/docs/apache-airflow/2.10.0/docker-compose.yaml'
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 11342  100 11342    0     0   410k      0 --:--:-- --:--:-- --:--:--  410k</pre>

<p>The file <em>docker-compose.yaml</em> contains<a data-type="indexterm" data-primary="docker-compose.yaml file" id="id2021"/><a data-type="indexterm" data-primary="Docker Hub" id="id2022"/><a data-type="indexterm" data-primary="resources online" data-secondary="Docker" data-tertiary="Docker Hub" id="id2023"/> instructions for the images to download from <a href="https://oreil.ly/q7y53">Docker Hub</a> along with environment options for configuring the software in your environment.</p>

<p>Open <em>docker_compose.yaml</em> and take a look at the <code>volumes:</code> section:</p>

<pre data-type="programlisting" data-code-language="yaml">  volumes:
    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags
    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs
    - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config
    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins</pre>

<p>This section creates Docker <em>volumes</em>, which are virtual drives available inside the Docker containers that are mapped to files in your Codespace storage. They are <span class="keep-together">relative</span> to the Airflow project directory, which will be <em>airflow</em> in your Codespace. For example, <em>airflow/dags</em> in your Codespace will be referenced as <em>/opt/airflow/dags</em> to the Airflow application running in Docker. (This will be important when you create connections later in this chapter.)</p>

<p>Create the directories that are mapped to those volumes and then configure an environment variable for the Airflow user ID:</p>

<pre data-type="programlisting" data-code-language="shell">.../airflow (main) $ mkdir -p ./dags ./logs ./plugins ./config
.../airflow (main) $ echo -e "AIRFLOW_UID=$(id -u)" &gt; .env</pre>

<p>Create <em>docker-compose.override.yaml</em>:<a data-type="indexterm" data-primary="docker-compose.override.yaml file" id="id2024"/></p>

<pre data-type="programlisting" data-code-language="shell">.../airflow (main) $ touch docker-compose.override.yaml</pre>

<p>You will use this file to override some of the standard configuration settings from the <em>docker-compose.yaml</em> file you downloaded. Using an override file allows you to keep the <em>docker-compose.yaml</em> file exactly like you downloaded it and put all of your customizations together, which makes troubleshooting easier. It also allows you to update <em>docker-compose.yaml</em> with a new version when Airflow is upgraded. Update <em>docker-compose.override.yaml</em> with the following contents:</p>

<pre data-type="programlisting" data-code-language="yaml">#these are overrides to the default docker compose
x-airflow-common:
  &amp;airflow-common
  environment:
    &amp;airflow-common-env
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false' <a class="co" id="co_using_apis_in_data_pipelines_CO1-1" href="#callout_using_apis_in_data_pipelines_CO1-1"><img src="assets/1.png" alt="1" width="12" height="12"/></a>

services:
  airflow-webserver:
    &lt;&lt;: *airflow-common
    command: webserver
    environment:
      &lt;&lt;: *airflow-common-env
      AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX: 'True' <a class="co" id="co_using_apis_in_data_pipelines_CO1-2" href="#callout_using_apis_in_data_pipelines_CO1-2"><img src="assets/2.png" alt="2" width="12" height="12"/></a>
  airflow-scheduler:
    &lt;&lt;: *airflow-common
    command: scheduler
    environment:
      &lt;&lt;: *airflow-common-env
      AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL: '30' <a class="co" id="co_using_apis_in_data_pipelines_CO1-3" href="#callout_using_apis_in_data_pipelines_CO1-3"><img src="assets/3.png" alt="3" width="12" height="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_using_apis_in_data_pipelines_CO1-1" href="#co_using_apis_in_data_pipelines_CO1-1"><img src="assets/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>This setting will hide the built-in Airflow examples so that they are not distracting in this chapter.</p></dd>
<dt><a class="co" id="callout_using_apis_in_data_pipelines_CO1-2" href="#co_using_apis_in_data_pipelines_CO1-2"><img src="assets/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>This setting will allow you to use the Airflow web interface in Codespaces.</p></dd>
<dt><a class="co" id="callout_using_apis_in_data_pipelines_CO1-3" href="#co_using_apis_in_data_pipelines_CO1-3"><img src="assets/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>This setting tells Airflow to look for changes to your code more frequently while you are developing.</p></dd>
</dl>

<p>Now you are ready to initialize<a data-type="indexterm" data-primary="docker commands" data-secondary="docker compose up airflow-init" id="id2025"/> the Docker environment using <em>docker-compose.yaml</em> and <em>docker-compose.override.yaml</em> with the <code>docker compose up airflow-init</code> command. This command will download the Airflow software and provision user IDs and other configuration details. Execute the following command:</p>

<pre data-type="programlisting" data-code-language="shell">.../airflow (main) $ docker compose up airflow-init
[+] Running 44/3
  redis Pulled
  postgres Pulled
  airflow-init Pulled
...
airflow-init-1  | 2.10.0
airflow-init-1 exited with code 0</pre>

<p>This command will run for several minutes, with many commands executed. If the output ends with “exited with code 0” it was successful. Your environment has been initialized, and you don’t need to execute this command again.</p>

<p>You are ready to run Airflow. To launch the Airflow web interface, execute the following command:</p>

<pre data-type="programlisting" data-code-language="shell">.../airflow (main) $ docker compose up -d
+] Running 7/7
 ✔ Container airflow-postgres-1           Healthy
 ✔ Container airflow-redis-1              Healthy
 ✔ Container airflow-airflow-init-1       Exited
 ✔ Container airflow-airflow-webserver-1  Started
 ✔ Container airflow-airflow-triggerer-1  Started
 ✔ Container airflow-airflow-scheduler-1  Started
 ✔ Container airflow-airflow-worker-1     Started</pre>

<p>Although you will see a pop-up window to launch the web UI, I have found that sometimes the web UI takes a few minutes to prepare, so don’t click OK. Instead, wait a couple of minutes and then select the Ports tab in your Codespace. You will see the forwarded address of the web interface. Click the globe icon to open the UI in the browser, as shown in <a data-type="xref" href="#open_in_browser_ch10">Figure 10-4</a>.</p>

<figure><div id="open_in_browser_ch10" class="figure">
<img src="assets/haad_1004.png" alt="Open Airflow web interface" width="826" height="91"/>
<h6><span class="label">Figure 10-4. </span>Open Airflow web interface</h6>
</div></figure>

<p>You will see the login page. Enter a username of <strong><code>airflow</code></strong> and password of <strong><code>airflow</code></strong> and click “Sign in.” (These starter credentials are used for demonstration only.) You will see the web interface of the Airflow application running in your Codespace, as shown in <a data-type="xref" href="#airflow_home_page_ch10">Figure 10-5</a>.  When you begin, there are no DAGs listed. You will learn more about the capabilities of Airflow as you create DAGs to complete your data pipeline requirements.<a data-type="indexterm" data-startref="ch10inst" id="id2026"/><a data-type="indexterm" data-startref="ch10inst2" id="id2027"/><a data-type="indexterm" data-startref="ch10inst3" id="id2028"/><a data-type="indexterm" data-startref="ch10inst4" id="id2029"/></p>

<figure><div id="airflow_home_page_ch10" class="figure">
<img src="assets/haad_1005.png" alt="Airflow home page" width="1513" height="850"/>
<h6><span class="label">Figure 10-5. </span>Airflow home page</h6>
</div></figure>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Creating Your Local Analytics Database"><div class="sect1" id="id200">
<h1>Creating Your Local Analytics Database</h1>

<p>Your data pipeline will be used<a data-type="indexterm" data-primary="database creation and access" data-secondary="analytics database" data-tertiary="creating" id="id2030"/><a data-type="indexterm" data-primary="analytics database" data-secondary="creating" id="id2031"/><a data-type="indexterm" data-primary="data pipelines" data-secondary="analytics database" data-tertiary="creating" id="id2032"/><a data-type="indexterm" data-primary="data analytics" data-secondary="analytics database" data-tertiary="creating" id="id2033"/><a data-type="indexterm" data-primary="SQLite" data-secondary="creating database" data-tertiary="analytics database" id="id2034"/><a data-type="indexterm" data-primary="database creation and access" data-secondary="analytics database" data-tertiary="table created" id="id2035"/> to insert and update player records into a local database. This is a common data science pattern: updating a database from source data and then creating models, metrics, and reports from the database. Change the directory to <em>dags</em> and create a database and the <code>player</code> table as follows:<a data-type="indexterm" data-primary="database creation and access" data-secondary="analytics database" data-tertiary="gsis_id" id="id2036"/><a data-type="indexterm" data-primary="analytics database" data-secondary="gsis_id" id="id2037"/><a data-type="indexterm" data-primary="gsis_id" data-secondary="analytics database" id="id2038"/></p>

<pre data-type="programlisting" data-code-language="shell">.../airflow (main) $ cd dags
.../dags (main) $ sqlite3 analytics_database.db
SQLite version 3.45.3 2024-04-15 13:34:05
Enter ".help" for usage hints.
sqlite&gt; CREATE TABLE player (
    player_id INTEGER PRIMARY KEY,
    gsis_id TEXT,
    first_name TEXT,
    last_name TEXT,
    position TEXT,
    last_changed_date DATE
);
sqlite&gt; .exit</pre>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Launching Your API in Codespaces"><div class="sect1" id="id201">
<h1>Launching Your API in Codespaces</h1>

<p>Your Airflow pipeline needs<a data-type="indexterm" data-primary="data pipelines" data-secondary="API launched in Codespaces" id="id2039"/> a running copy of the SportsWorldCentral API to gather updates. Follow the directions in <a data-type="xref" href="ch08.html#sportsworldcentral">“Running the SportsWorldCentral (SWC) API Locally”</a> to get your API running in a separate terminal window of Codespaces, and copy the base URL from the browser bar. You will configure Airflow to reference the base URL from that API in the next section.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Configuring Airflow Connections"><div class="sect1" id="id202">
<h1>Configuring Airflow Connections</h1>

<p>Airflow <em>connections</em> allow you<a data-type="indexterm" data-primary="data pipelines" data-secondary="Apache Airflow orchestrating" data-tertiary="configuring Airflow connections" id="id2040"/><a data-type="indexterm" data-primary="Apache Airflow" data-secondary="connections configured" id="id2041"/><a data-type="indexterm" data-primary="Apache Airflow" data-secondary="connections configured" data-tertiary="about connections" id="id2042"/> to store information about data sources and targets in the server instead of in your code. This is useful for maintaining separate Airflow environments for development, testing, and production. You will create connections for your API and your analytics database.</p>

<p>In the Airflow UI, select Admin &gt; Connections. Click the plus sign to add a new connection record. Now you will use the <em>volume</em> mappings that you viewed earlier in the <em>docker-compose.yaml</em> file. Use the following values:</p>

<ul>
<li>
<p><em>Connection ID</em>: <strong><code>analytics_database</code></strong></p>
</li>
<li>
<p><em>Connection Type</em>: Sqlite</p>
</li>
<li>
<p><em>Description</em>: <strong><code>Database to store local analytics data.</code></strong></p>
</li>
<li>
<p><em>Schema</em>: <strong><code>/opt/airflow/dags/analytics_database.db</code></strong></p>
</li>
</ul>

<p>Leave the rest of the values empty, and click Save.</p>

<p>Next, add the connection for the API connection:</p>

<ul>
<li>
<p><em>Connection ID</em>: <strong><code>sportsworldcentral_url</code></strong></p>
</li>
<li>
<p><em>Connection Type</em>: HTTP</p>
</li>
<li>
<p><em>Description</em>: <strong><code>URL for calling the SportsWorldCentral API.</code></strong></p>
</li>
<li>
<p><em>Host</em>: Enter the base URL of the API running in Codespaces.</p>
</li>
</ul>

<p>Leave the rest of the values empty, and click Save. You should see two connections listed, as shown in <a data-type="xref" href="#configured_connections_ch10">Figure 10-6</a>.</p>

<figure><div id="configured_connections_ch10" class="figure">
<img src="assets/haad_1006.png" alt="Configured Airflow connections" width="935" height="290"/>
<h6><span class="label">Figure 10-6. </span>Configured Airflow connections</h6>
</div></figure>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Creating Your First DAG"><div class="sect1" id="id113">
<h1>Creating Your First DAG</h1>

<p><a data-type="xref" href="#date_pipeline_airflow_ch10">Figure 10-7</a> displays an<a data-type="indexterm" data-primary="data pipelines" data-secondary="Apache Airflow orchestrating" data-tertiary="creating first DAG" id="ch10dag1"/><a data-type="indexterm" data-primary="Apache Airflow" data-secondary="creating first DAG" id="ch10dag12"/><a data-type="indexterm" data-primary="DAGs (directed acyclic graphs)" data-secondary="creating first DAG" id="ch10dag13"/><a data-type="indexterm" data-primary="analytics database" data-secondary="creating first DAG" id="ch10dag14"/><a data-type="indexterm" data-primary="bulk_player_file_load.py DAG" id="id2043"/><a data-type="indexterm" data-primary="DAGs (directed acyclic graphs)" data-secondary="bulk_player_file_load.py" id="id2044"/><a data-type="indexterm" data-primary="Python" data-secondary="DAGs" data-tertiary="bulk_player_file_load.py" id="id2045"/><a data-type="indexterm" data-primary="data pipelines" data-secondary="analytics database" data-tertiary="DAG initial bulk file load" id="id2046"/><a data-type="indexterm" data-primary="analytics database" data-secondary="DAG initial bulk file load" id="id2047"/><a data-type="indexterm" data-primary="database creation and access" data-secondary="analytics database" data-tertiary="DAG initial bulk file load" id="id2048"/> implementation of your pipeline with Airflow, using two DAGs. The <em>bulk_player_file_load.py</em> DAG would perform an initial load of the analytics database from a bulk file, which was provided in <a data-type="xref" href="part01.html#part_1">Part I</a> of this book. <a data-type="indexterm" data-primary="resources online" data-secondary="chapter code" data-tertiary="chapter 10 complete" data-tertiary-sortas="chapter 9b complete" id="id2049"/>That file is available in the <em>chapter10/complete</em> folder of your repository, but this chapter does not walk through it due to space constraints.</p>

<figure><div id="date_pipeline_airflow_ch10" class="figure">
<img src="assets/haad_1007.png" alt="Airflow components of your data pipeline" width="1251" height="987"/>
<h6><span class="label">Figure 10-7. </span>Airflow components of your data pipeline</h6>
</div></figure>

<p>Create the DAG that uses API data, <em>recurring_player_api_insert_update_dag.py</em>. <span class="keep-together">This DAG performs</span> incremental updates of your database, using the <span class="keep-together">SportsWorldCentral API.</span> Change the directory to <em>dags</em> and create the <em>recurring_player_api_insert_update_dag.py</em> file:<a data-type="indexterm" data-primary="Python" data-secondary="DAGs" data-tertiary="recurring_player_api_insert_update_dag.py" id="id2050"/><a data-type="indexterm" data-primary="DAGs (directed acyclic graphs)" data-secondary="recurring_player_api_insert_update_dag.py" id="id2051"/><a data-type="indexterm" data-primary="recurring_player_api_insert_update_dag.py" id="id2052"/></p>

<pre data-type="programlisting" data-code-language="shell">.../airflow (main) $ cd dags
.../dags (main) $  touch recurring_player_api_insert_update_dag.py</pre>

<p>Add the following contents to the <em>recurring_player_api_insert_update_dag.py</em> file:</p>

<pre data-type="programlisting" data-code-language="python">import datetime
import logging
from airflow.decorators import dag <a class="co" id="co_using_apis_in_data_pipelines_CO2-1" href="#callout_using_apis_in_data_pipelines_CO2-1"><img src="assets/1.png" alt="1" width="12" height="12"/></a>
from airflow.providers.http.operators.http import HttpOperator <a class="co" id="co_using_apis_in_data_pipelines_CO2-2" href="#callout_using_apis_in_data_pipelines_CO2-2"><img src="assets/2.png" alt="2" width="12" height="12"/></a>
from airflow.operators.python import PythonOperator
from shared_functions import upsert_player_data <a class="co" id="co_using_apis_in_data_pipelines_CO2-3" href="#callout_using_apis_in_data_pipelines_CO2-3"><img src="assets/3.png" alt="3" width="12" height="12"/></a>

def health_check_response(response): <a class="co" id="co_using_apis_in_data_pipelines_CO2-4" href="#callout_using_apis_in_data_pipelines_CO2-4"><img src="assets/4.png" alt="4" width="12" height="12"/></a>
    logging.info(f"Response status code: {response.status_code}")
    logging.info(f"Response body: {response.text}")
    return response.status_code == 200 and response.json() == {
        "message": "API health check successful"
    }

def insert_update_player_data(**context): <a class="co" id="co_using_apis_in_data_pipelines_CO2-5" href="#callout_using_apis_in_data_pipelines_CO2-5"><img src="assets/5.png" alt="5" width="12" height="12"/></a>

    player_json = context["ti"].xcom_pull(task_ids="api_player_query") <a class="co" id="co_using_apis_in_data_pipelines_CO2-6" href="#callout_using_apis_in_data_pipelines_CO2-6"><img src="assets/6.png" alt="6" width="12" height="12"/></a>

    if player_json:
        upsert_player_data(player_json) <a class="co" id="co_using_apis_in_data_pipelines_CO2-7" href="#callout_using_apis_in_data_pipelines_CO2-7"><img src="assets/7.png" alt="7" width="12" height="12"/></a>
    else:
        logging.warning("No player data found.")

@dag(schedule_interval=None) <a class="co" id="co_using_apis_in_data_pipelines_CO2-8" href="#callout_using_apis_in_data_pipelines_CO2-8"><img src="assets/8.png" alt="8" width="12" height="12"/></a>
def recurring_player_api_insert_update_dag():

    api_health_check_task = HttpOperator(  <a class="co" id="co_using_apis_in_data_pipelines_CO2-9" href="#callout_using_apis_in_data_pipelines_CO2-9"><img src="assets/9.png" alt="9" width="12" height="12"/></a>
        task_id="check_api_health_check_endpoint",
        http_conn_id="sportsworldcentral_url",
        endpoint="/",
        method="GET",
        headers={"Content-Type": "application/json"},
        response_check=health_check_response,
    )

    temp_min_last_change_date = "2024-04-01" <a class="co" id="co_using_apis_in_data_pipelines_CO2-10" href="#callout_using_apis_in_data_pipelines_CO2-10"><img src="assets/10.png" alt="10" width="12" height="12"/></a>

    api_player_query_task = HttpOperator( <a class="co" id="co_using_apis_in_data_pipelines_CO2-11" href="#callout_using_apis_in_data_pipelines_CO2-11"><img src="assets/11.png" alt="11" width="12" height="12"/></a>
       task_id="api_player_query",
       http_conn_id="sportsworldcentral_url",
       endpoint=(
           f"/v0/players/?skip=0&amp;limit=100000&amp;minimum_last_changed_date="
           f"{temp_min_last_change_date}"
       ),
       method="GET",
       headers={"Content-Type": "application/json"},
   )

    player_sqlite_upsert_task = PythonOperator( <a class="co" id="co_using_apis_in_data_pipelines_CO2-12" href="#callout_using_apis_in_data_pipelines_CO2-12"><img src="assets/12.png" alt="12" width="12" height="12"/></a>
        task_id="player_sqlite_upsert",
        python_callable=insert_update_player_data,
        provide_context=True,
    )

    # Run order of tasks
    api_health_check_task &gt;&gt; api_player_query_task &gt;&gt; player_sqlite_upsert_task<a class="co" id="co_using_apis_in_data_pipelines_CO2-13" href="#callout_using_apis_in_data_pipelines_CO2-13"><img src="assets/13.png" alt="13" width="12" height="12"/></a>

# Instantiate the DAG
dag_instance = recurring_player_api_insert_update_dag() <a class="co" id="co_using_apis_in_data_pipelines_CO2-14" href="#callout_using_apis_in_data_pipelines_CO2-14"><img src="assets/14.png" alt="14" width="12" height="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_using_apis_in_data_pipelines_CO2-1" href="#co_using_apis_in_data_pipelines_CO2-1"><img src="assets/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>This import allows you to define the DAG using a <code>@dag</code> decorator.</p></dd>
<dt><a class="co" id="callout_using_apis_in_data_pipelines_CO2-2" href="#co_using_apis_in_data_pipelines_CO2-2"><img src="assets/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>These two imports allow you to use predefined operators in your tasks.</p></dd>
<dt><a class="co" id="callout_using_apis_in_data_pipelines_CO2-3" href="#co_using_apis_in_data_pipelines_CO2-3"><img src="assets/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>This is an import of a separate Python file with a function that is shared between two DAGs.</p></dd>
<dt><a class="co" id="callout_using_apis_in_data_pipelines_CO2-4" href="#co_using_apis_in_data_pipelines_CO2-4"><img src="assets/4.png" alt="4" width="12" height="12"/></a></dt>
<dd><p>This is the code to verify<a data-type="indexterm" data-primary="health check" data-secondary="DAG verifying API status" id="id2053"/><a data-type="indexterm" data-primary="DAGs (directed acyclic graphs)" data-secondary="API health check" id="id2054"/> the response of <code>api_health_check_task</code> defined below. This is the first task, and it allows the DAG to verify the status of the API before proceeding with other tasks.</p></dd>
<dt><a class="co" id="callout_using_apis_in_data_pipelines_CO2-5" href="#co_using_apis_in_data_pipelines_CO2-5"><img src="assets/5.png" alt="5" width="12" height="12"/></a></dt>
<dd><p>This defines a function that will be called by a task.</p></dd>
<dt><a class="co" id="callout_using_apis_in_data_pipelines_CO2-6" href="#co_using_apis_in_data_pipelines_CO2-6"><img src="assets/6.png" alt="6" width="12" height="12"/></a></dt>
<dd><p>This line of code uses<a data-type="indexterm" data-primary="XCom (Apache Airflow)" id="id2055"/> XCom to retrieve data from the second task.</p></dd>
<dt><a class="co" id="callout_using_apis_in_data_pipelines_CO2-7" href="#co_using_apis_in_data_pipelines_CO2-7"><img src="assets/7.png" alt="7" width="12" height="12"/></a></dt>
<dd><p>Here it passes the data from XCom to the shared <code>upsert_player_data</code> function, which is defined in a separate file.</p></dd>
<dt><a class="co" id="callout_using_apis_in_data_pipelines_CO2-8" href="#co_using_apis_in_data_pipelines_CO2-8"><img src="assets/8.png" alt="8" width="12" height="12"/></a></dt>
<dd><p>This is the main DAG definition. It uses the <code>@dag</code> decorator to define the Python function as a DAG. The tasks are defined within this method.</p></dd>
<dt><a class="co" id="callout_using_apis_in_data_pipelines_CO2-9" href="#co_using_apis_in_data_pipelines_CO2-9"><img src="assets/9.png" alt="9" width="12" height="12"/></a></dt>
<dd><p>The first task uses <a data-type="indexterm" data-primary="HttpOperator (Apache Airflow)" id="id2056"/>an <code>HttpOperator</code> template to call the API’s health check <span class="keep-together">endpoint.</span> It adds a <code>response_check</code> method to check the API’s status before <span class="keep-together">continuing.</span></p></dd>
<dt><a class="co" id="callout_using_apis_in_data_pipelines_CO2-10" href="#co_using_apis_in_data_pipelines_CO2-10"><img src="assets/10.png" alt="10" width="12" height="12"/></a></dt>
<dd><p>The minimum last changed date is hardcoded in this example. In production, an <a href="https://oreil.ly/pFHaG">Airflow template variable</a> could be used to get the last day’s updates.</p></dd>
<dt><a class="co" id="callout_using_apis_in_data_pipelines_CO2-11" href="#co_using_apis_in_data_pipelines_CO2-11"><img src="assets/11.png" alt="11" width="12" height="12"/></a></dt>
<dd><p>The second task uses an <code>HttpOperator</code> to call the API’s player endpoint with a query parameter to restrict the records that are returned.</p></dd>
<dt><a class="co" id="callout_using_apis_in_data_pipelines_CO2-12" href="#co_using_apis_in_data_pipelines_CO2-12"><img src="assets/12.png" alt="12" width="12" height="12"/></a></dt>
<dd><p>The third task is<a data-type="indexterm" data-primary="PythonOperator (Apache Airflow)" id="id2057"/> a <code>PythonOperator</code> that calls the <code>insert_update_player_data</code> function.</p></dd>
<dt><a class="co" id="callout_using_apis_in_data_pipelines_CO2-13" href="#co_using_apis_in_data_pipelines_CO2-13"><img src="assets/13.png" alt="13" width="12" height="12"/></a></dt>
<dd><p>This statement sets the dependency of the tasks using bitshift operators.</p></dd>
<dt><a class="co" id="callout_using_apis_in_data_pipelines_CO2-14" href="#co_using_apis_in_data_pipelines_CO2-14"><img src="assets/14.png" alt="14" width="12" height="12"/></a></dt>
<dd><p>The final statement is required to instantiate the DAG that is defined by the <code>@dag</code> decorator.</p></dd>
</dl>

<p>Take a moment to compare this code to <a data-type="xref" href="#date_pipeline_airflow_ch10">Figure 10-7</a>. The key parts of the DAG file are toward the bottom of this file: the <code>@dag</code> decorator defines the main DAG wrapper. Inside the DAG are three tasks: two that use <code>HttpOperator</code>s to connect to the API and one that uses a <code>PythonOperator</code> to connect to the SQLite database.</p>

<p>The statement <code>api_health_check_task &gt;&gt; api_player_query_task &gt;&gt; player_sqlite_upsert_task</code> sets the <a data-type="indexterm" data-primary="dependency between Apache Airflow tasks (&gt;&gt;)" id="id2058"/><a data-type="indexterm" data-primary="&gt;&gt; (dependency between Apache Airflow tasks)" id="id2059"/><a data-type="indexterm" data-primary="tasks of Apache Airflow" data-secondary="&gt;&gt; setting dependency between" id="id2060"/><a data-type="indexterm" data-primary="Apache Airflow" data-secondary="tasks as basic units of execution" data-tertiary="&gt;&gt; setting dependency between" id="id2061"/>dependency between the tasks using a right-shift operator, <code>&gt;&gt;</code>. These tasks have a very simple sequential dependency, but Airflow is capable of implementing very intricate dependencies between tasks. <a data-type="indexterm" data-primary="Apache Airflow" data-secondary="tasks as basic units of execution" data-tertiary="task dependencies document online" id="id2062"/><a data-type="indexterm" data-primary="resources online" data-secondary="Apache Airflow" data-tertiary="task dependencies document" id="id2063"/><a data-type="indexterm" data-primary="“Manage task and task group dependencies in Airflow” (Astronomer)" data-primary-sortas="Manage task and task group" id="id2064"/>For more information about this capability, read Astronomer’s <a href="https://oreil.ly/PTa4M">“Manage task and task group dependencies in Airflow”</a>.<a data-type="indexterm" data-startref="ch10dag1" id="id2065"/><a data-type="indexterm" data-startref="ch10dag12" id="id2066"/><a data-type="indexterm" data-startref="ch10dag13" id="id2067"/><a data-type="indexterm" data-startref="ch10dag14" id="id2068"/></p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Coding a Shared Function"><div class="sect1" id="id114">
<h1>Coding a Shared Function</h1>

<p>Although the sources of the two<a data-type="indexterm" data-primary="data pipelines" data-secondary="Apache Airflow orchestrating" data-tertiary="coding a shared function" id="ch10shar"/><a data-type="indexterm" data-primary="Apache Airflow" data-secondary="creating first DAG" data-tertiary="coding a shared function" id="ch10shar2"/><a data-type="indexterm" data-primary="DAGs (directed acyclic graphs)" data-secondary="coding a shared function" id="ch10shar3"/><a data-type="indexterm" data-primary="analytics database" data-secondary="coding a shared function" id="ch10shar4"/><a data-type="indexterm" data-primary="upserts" id="id2069"/><a data-type="indexterm" data-primary="analytics database" data-secondary="upserts" id="id2070"/> DAGs are different, they both perform an <em>upsert</em> on the analytics database, which means that if a source record already exists in the database, the code updates it, otherwise it inserts a new record. Because this task is shared between the two DAGs, you will create a separate Python file with a shared function. Create the <em>shared_functions.py</em> file:<a data-type="indexterm" data-primary="shared_functions.py file" id="id2071"/><a data-type="indexterm" data-primary="DAGs (directed acyclic graphs)" data-secondary="shared_functions.py file" id="id2072"/><a data-type="indexterm" data-primary="Python" data-secondary="DAGs" data-tertiary="shared_functions.py" id="id2073"/></p>

<pre data-type="programlisting" data-code-language="shell">.../dags (main) $ touch shared_functions.py</pre>

<p>Add the following contents to the <em>shared_functions.py</em> file:</p>

<pre data-type="programlisting" data-code-language="python">import logging
import json
from airflow.hooks.base import BaseHook 

def upsert_player_data(player_json):
    import sqlite3			<a class="co" id="co_using_apis_in_data_pipelines_CO3-1" href="#callout_using_apis_in_data_pipelines_CO3-1"><img src="assets/1.png" alt="1" width="12" height="12"/></a>
    import pandas as pd

# Fetch the connection object
    database_conn_id = 'analytics_database'
    connection = BaseHook.get_connection(database_conn_id) <a class="co" id="co_using_apis_in_data_pipelines_CO3-2" href="#callout_using_apis_in_data_pipelines_CO3-2"><img src="assets/2.png" alt="2" width="12" height="12"/></a>
    
    sqlite_db_path = connection.schema

    if player_json:

        player_data = json.loads(player_json)
        
        # Use a context manager for the SQLite connection
        with sqlite3.connect(sqlite_db_path) as conn:
            cursor = conn.cursor()			<a class="co" id="co_using_apis_in_data_pipelines_CO3-3" href="#callout_using_apis_in_data_pipelines_CO3-3"><img src="assets/3.png" alt="3" width="12" height="12"/></a>

            # Insert each player record into the 'player' table
            for player in player_data:
                try:
                    cursor.execute(""" <a class="co" id="co_using_apis_in_data_pipelines_CO3-4" href="#callout_using_apis_in_data_pipelines_CO3-4"><img src="assets/4.png" alt="4" width="12" height="12"/></a>
                        INSERT INTO player (
                            player_id, gsis_id, first_name, last_name, 
                            position, last_changed_date
                        ) 
                        VALUES (?, ?, ?, ?, ?, ?) 
                        ON CONFLICT(player_id) DO UPDATE <a class="co" id="co_using_apis_in_data_pipelines_CO3-5" href="#callout_using_apis_in_data_pipelines_CO3-5"><img src="assets/5.png" alt="5" width="12" height="12"/></a>
                        SET
                            gsis_id = excluded.gsis_id,
                            first_name = excluded.first_name,
                            last_name = excluded.last_name,
                            position = excluded.position,
                            last_changed_date = excluded.last_changed_date
                    """, (
                       player['player_id'], player['gsis_id'],
                       player['first_name'],
                       player['last_name'],
                       player['position'],
                       player['last_changed_date']
                   ))
                except Exception as e:
                   logging.error(
                       f"Failed to insert player {player['player_id']}: {e}")
                   raise
                    
    else:
        logging.warning("No player data found.")
         raise ValueError(
           "No player data found. Task failed due to missing data.")</pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_using_apis_in_data_pipelines_CO3-1" href="#co_using_apis_in_data_pipelines_CO3-1"><img src="assets/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>These two import statements are placed inside the Python method. This is because Airflow frequently parses DAG code and will reload imported libraries that are at the top of the Python file.</p></dd>
<dt><a class="co" id="callout_using_apis_in_data_pipelines_CO3-2" href="#co_using_apis_in_data_pipelines_CO3-2"><img src="assets/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>This statement uses an Airflow hook to retrieve the connection that you defined in the Airflow user interface.</p></dd>
<dt><a class="co" id="callout_using_apis_in_data_pipelines_CO3-3" href="#co_using_apis_in_data_pipelines_CO3-3"><img src="assets/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>This uses a database cursor to execute SQL queries on your analytics database.</p></dd>
<dt><a class="co" id="callout_using_apis_in_data_pipelines_CO3-4" href="#co_using_apis_in_data_pipelines_CO3-4"><img src="assets/4.png" alt="4" width="12" height="12"/></a></dt>
<dd><p>This statement uses the database cursor to execute a parameterized SQL query.</p></dd>
<dt><a class="co" id="callout_using_apis_in_data_pipelines_CO3-5" href="#co_using_apis_in_data_pipelines_CO3-5"><img src="assets/5.png" alt="5" width="12" height="12"/></a></dt>
<dd><p>This SQL statement provides the upsert capability, which updates a record if it already exists or inserts it if not.</p></dd>
</dl>

<p>This function receives the data from the API as a parameter and then loads data into the SQLite database using the Airflow connection that you defined in the user interface. This is a parameterized SQL query, in which the input data is referenced with <code>VALUES (?, ?, ?, ?, ?, ?)</code>. This is an important measure to protect against SQL injection attacks, which could occur if a malicious actor inserted code into the source data’s fields, where your process was expecting data.<a data-type="indexterm" data-startref="ch10shar" id="id2074"/><a data-type="indexterm" data-startref="ch10shar2" id="id2075"/><a data-type="indexterm" data-startref="ch10shar3" id="id2076"/><a data-type="indexterm" data-startref="ch10shar4" id="id2077"/></p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Running Your DAG"><div class="sect1" id="id203">
<h1>Running Your DAG</h1>

<p>Before you run the DAG, check<a data-type="indexterm" data-primary="DAGs (directed acyclic graphs)" data-secondary="running your DAG" id="id2078"/><a data-type="indexterm" data-primary="Apache Airflow" data-secondary="creating first DAG" data-tertiary="running your DAG" id="id2079"/><a data-type="indexterm" data-primary="data pipelines" data-secondary="Apache Airflow orchestrating" data-tertiary="running your DAG" id="id2080"/><a data-type="indexterm" data-primary="analytics database" data-secondary="creating first DAG" data-tertiary="running your DAG" id="id2081"/> that your API is up and running. Navigate back to the Airflow UI and you will see your DAG listed, as shown in <a data-type="xref" href="#DAG_listing_main_page_ch10">Figure 10-8</a>. The user interface has too many features to cover in this chapter, but you can read about the user interface at <a href="https://oreil.ly/DfOSC">“UI / Screenshots”</a>.</p>

<figure><div id="DAG_listing_main_page_ch10" class="figure">
<img src="assets/haad_1008.png" alt="DAG listed on Airflow home page" width="1068" height="268"/>
<h6><span class="label">Figure 10-8. </span>DAG listed on the Airflow home page</h6>
</div></figure>

<p>Click <code>recurring_player_api_insert_update_dag</code>, and then Graph. You will see the sequence of Airflow tasks using the <code>task_id</code> names that you assigned in your code, as shown in <a data-type="xref" href="#graph_view_ch10">Figure 10-9</a>.</p>

<figure><div id="graph_view_ch10" class="figure">
<img src="assets/haad_1009.png" alt="Graph view of first DAG" width="1063" height="524"/>
<h6><span class="label">Figure 10-9. </span>Graph view of the first DAG</h6>
</div></figure>

<p>Click the Trigger DAG button, which has a triangle icon to your DAG. If everything is configured correctly with your code and connections, each of the tasks in your DAG should complete with a green box in a minute or so. <a data-type="indexterm" data-primary="DAGs (directed acyclic graphs)" data-secondary="API health check" id="id2082"/><a data-type="indexterm" data-primary="health check" data-secondary="DAG verifying API status" id="id2083"/>Click the first box, labeled <code>check_api_health_check_endpoint</code>. Your view should look similar to <a data-type="xref" href="#successful_dag_run_ch10">Figure 10-10</a>. If you encounter an error, click the task that has the error, and click Logs to diagnose the issue.</p>

<figure><div id="successful_dag_run_ch10" class="figure">
<img src="assets/haad_1010.png" alt="Successful DAG run" width="1298" height="435"/>
<h6><span class="label">Figure 10-10. </span>Successful DAG run</h6>
</div></figure>

<p class="less_space pagebreak-before">To confirm that your analytics database was successfully upserted, go back to the terminal and open the database with SQLite. Query the Player table, to confirm that 1,018 player records are present in the table. These are the records retrieved from your API:</p>

<pre data-type="programlisting" data-code-language="shell">.../dags (main) $ sqlite3 analytics_database.db
SQLite version 3.45.3 2024-04-15 13:34:05
Enter ".help" for usage hints.
sqlite&gt; select count(*) from player;
1018</pre>

<p>Congratulations! You created a data pipeline that updates your database with records from an API!</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id2084">
<h1>Extending Your Portfolio Project</h1>
<p>Here are a few ways to extend the project you created in this chapter:<a data-type="indexterm" data-primary="data pipelines" data-secondary="extending portfolio project" id="id2085"/><a data-type="indexterm" data-primary="portfolio projects" data-secondary="data pipeline project extended" id="id2086"/></p>

<ul>
<li>
<p>Update <em>recurring_player_api_insert_update_dag.py</em> to pass the minimum last changed date using the  <a href="https://oreil.ly/YViYQ">scheduling variables built into Airflow</a>.<a data-type="indexterm" data-primary="recurring_player_api_insert_update_dag.py" data-secondary="updating via Airflow scheduling variables" id="id2087"/><a data-type="indexterm" data-primary="Apache Airflow" data-secondary="scheduling variables" id="id2088"/><a data-type="indexterm" data-primary="data pipelines" data-secondary="Apache Airflow orchestrating" data-tertiary="scheduling variables" id="id2089"/></p>
</li>
<li>
<p>The <em>bulk_player_file_load_dag.py</em> DAG is in the <em>chapter10/complete</em> directory. Create the Airflow HTTP connection mentioned in the code, and get this DAG to work.</p>
</li>
<li>
<p>Add DAGs to process all of the other API endpoints.</p>
</li>
</ul>
</div></aside>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="id390">
<h1>Summary</h1>

<p>In this chapter, you learned how to create a data pipeline calling APIs to maintain current data for analytics products. You installed and configured Apache Airflow, and you created a DAG with multiple tasks to update your database from an API.</p>

<p>In <a data-type="xref" href="ch11.html#chapter_11">Chapter 11</a>, you will create a Streamlit data app using data from an API.</p>
</div></section>
</div></section></div>
</div>
</body></html>