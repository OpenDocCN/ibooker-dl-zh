- en: Chapter 8\. Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章 强化学习
- en: The learning techniques we’ve covered so far in this book fall into the categories
    of supervised or unsupervised learning. In both cases, solving a given problem
    requires a data scientist to design a deep architecture that handles and processes
    input data and to connect the output of the architecture to a loss function suitable
    for the problem at hand. This framework is widely applicable, but not all applications
    fall neatly into this style of thinking. Let’s consider the challenge of training
    a machine learning model to win a game of chess. It seems reasonable to process
    the board as spatial input using a convolutional network, but what would the loss
    entail? None of our standard loss functions such as cross-entropy or *L*² loss
    quite seem to apply.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们在本书中介绍的学习技术属于监督学习或无监督学习的范畴。在这两种情况下，解决给定问题需要数据科学家设计一个处理和处理输入数据的深度架构，并将架构的输出连接到适合当前问题的损失函数。这个框架是广泛适用的，但并非所有应用都能很好地适应这种思维方式。让我们考虑训练一个机器学习模型赢得国际象棋比赛的挑战。将棋盘作为空间输入使用卷积网络处理似乎是合理的，但损失会是什么？我们的标准损失函数如交叉熵或
    *L*² 损失似乎都不太适用。
- en: Reinforcement learning provides a mathematical framework well suited to solving
    games. The central mathematical concept is that of the *Markov decision process*,
    a tool for modeling AI agents that interact with *environments* that offer *rewards*
    upon completion of certain *actions*. This framework proves to be flexible and
    general, and has found a number of applications in recent years. It’s worth noting
    that reinforcement learning as a field is quite mature and has existed in recognizable
    form since the 1970s. However, until recently, most reinforcement learning systems
    were only capable of solving toy problems. Recent work has revealed that these
    limitations were likely due to the lack of sophisticated data intake mechanisms;
    hand-engineered features for many games or robotic environments often did not
    suffice. Deep representation extractions trained end-to-end on modern hardware
    seem to break through the barriers of earlier reinforcement learning systems and
    have achieved notable results in recent years.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习提供了一个数学框架，非常适合解决游戏问题。中心数学概念是 *马尔可夫决策过程*，这是一个用于建模与提供在完成某些 *动作* 后提供 *奖励* 的
    *环境* 互动的 AI 代理的工具。这个框架被证明是灵活和通用的，并在近年来找到了许多应用。值得注意的是，强化学习作为一个领域已经相当成熟，自上世纪70年代以来就以可识别的形式存在。然而，直到最近，大多数强化学习系统只能解决玩具问题。最近的工作揭示了这些限制可能是由于缺乏复杂的数据输入机制；许多游戏或机器人环境的手工设计特征通常不足。在现代硬件上进行端到端训练的深度表示提取似乎突破了早期强化学习系统的障碍，并在近年取得了显著的成果。
- en: Arguably, the first breakthrough in deep reinforcement learning was on ATARI
    arcade games. ATARI arcade games were traditionally played in video game arcades
    and offered users simple games that don’t typically require sophisticated strategizing
    but might require good reflexes. [Figure 8-1](#ch8-breakout) shows a screenshot
    from the popular ATARI game Breakout. In recent years, due to the development
    of good ATARI emulation software, ATARI games have become a testbed for gameplay
    algorithms. At first, reinforcement learning algorithms applied to ATARI didn’t
    achieve superb results; the requirement that the algorithm understand a visual
    game state frustrated most attempts. However, as convolutional networks matured,
    researchers at DeepMind realized that convolutional networks could be combined
    with existing reinforcement learning techniques and trained end-to-end.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 可以说，深度强化学习的第一次突破是在 ATARI 街机游戏上取得的。ATARI 街机游戏传统上是在游戏机厅玩的，提供给用户简单的游戏，通常不需要复杂的策略，但可能需要良好的反应能力。[图8-1](#ch8-breakout)展示了流行的
    ATARI 游戏 Breakout 的截图。近年来，由于良好的 ATARI 模拟软件的开发，ATARI 游戏已成为游戏算法的测试平台。最初，应用于 ATARI
    的强化学习算法并没有取得出色的结果；算法理解视觉游戏状态的要求使大多数尝试受挫。然而，随着卷积网络的成熟，DeepMind 的研究人员意识到卷积网络可以与现有的强化学习技术结合，并进行端到端训练。
- en: '![breakout.jpg](assets/tfdl_0801.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![打砖块.jpg](assets/tfdl_0801.png)'
- en: Figure 8-1\. A screenshot of the ATARI arcade game Breakout. Players have to
    use the paddle at the bottom of the screen to bounce a ball that breaks the tiles
    at the top of the screen.
  id: totrans-5
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-1\. ATARI 街机游戏 Breakout 的截图。玩家必须使用屏幕底部的挡板弹球，打破屏幕顶部的砖块。
- en: The resulting system achieved superb results, and learned to play many ATARI
    games (especially those dependent on quick reflexes) at superhuman standards.
    [Figure 8-2](#ch8-breakoutresults) lists ATARI scores achieved by DeepMind’s DQN
    algorithm. This breakthrough result spurred tremendous growth in the field of
    deep reinforcement learning and inspired legions of researchers to explore the
    potential of deep reinforcement learning techniques. At the same time, DeepMind’s
    ATARI results showed reinforcement learning techniques were capable of solving
    systems dependent on short-term movements. These results didn’t demonstrate that
    deep reinforcement learning systems were capable of solving games that required
    greater strategic planning.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 结果系统取得了出色的成绩，并学会了以超人标准玩许多 ATARI 游戏（尤其是那些依赖快速反应的游戏）。[图8-2](#ch8-breakoutresults)列出了
    DeepMind 的 DQN 算法在 ATARI 游戏中取得的得分。这一突破性成果推动了深度强化学习领域的巨大增长，并激发了无数研究人员探索深度强化学习技术的潜力。与此同时，DeepMind
    的 ATARI 结果显示，强化学习技术能够解决依赖短期动作的系统。这些结果并没有表明深度强化学习系统能够解决需要更大战略规划的游戏。
- en: '![atari_scores.jpg](assets/tfdl_0802.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![atari_scores.jpg](assets/tfdl_0802.png)'
- en: Figure 8-2\. Results of DeepMind’s DQN reinforcement learning algorithm on various
    ATARI games. 100% is the score of a strong human player. Note that DQN achieves
    superhuman performance on many games, but is quite weak on others.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-2\. DeepMind 的 DQN 强化学习算法在各种 ATARI 游戏中的结果。100% 是一个强大的人类玩家的得分。请注意，DQN 在许多游戏中取得了超人类表现，但在其他游戏中表现相当糟糕。
- en: Computer Go
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算机围棋
- en: In 1994, IBM revealed the system Deep Blue, which later succeeded in defeating
    Garry Kasparov in a highly publicized chess match. This system relied on brute
    force computation to expand the tree of possible chess moves (with some help from
    handcrafted chess heuristics) to play master-level chess.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Computer scientists attempted to apply similar techniques to other games such
    as Go. Unfortunately for early experimenters, Go’s 19 × 19 game board is significantly
    larger than chess’s 8 × 8 board. As a result, trees of possible moves explode
    much more quickly than for chess, and simple back-of-the-envelope calculations
    indicated that Moore’s law would take a very long time to enable brute force solution
    of Go in the style of Deep Blue. Complicating matters, there existed no simple
    heuristic for evaluating who’s winning in a half-played Go game (determining whether
    black or white is ahead is a notoriously noisy art for the best human analysts).
    As a result, until very recently, many prominent computer scientists believed
    that strong computer Go play was a decade away at the least.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate the prowess of its reinforcement learning algorithms, DeepMind
    took on the challenge of learning to play Go, a game that requires complex strategic
    planning. In a tour-de-force paper, DeepMind revealed its deep reinforcement learning
    engine, AlphaGo, which combined convolutional networks with tree-based search
    to defeat the human Go master Lee Sedol ([Figure 8-3](#ch8-leesedol)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![lee_sedol.jpg](assets/tfdl_0803.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: Figure 8-3\. Human Go champion Lee Sedol battles AlphaGo. Lee Sedol eventually
    lost the match 1–4, but succeeded in winning one game. It’s unlikely that this
    victory can be replicated against the vastly improved successors of AlphaGo such
    as AlphaZero.
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: AlphaGo convincingly demonstrated that deep reinforcement learning techniques
    were capable of learning to solve complex strategic games. The heart of the breakthrough
    was the realization that convolutional networks could learn to estimate whether
    black or white was ahead in a half-played game, which enabled game trees to be
    truncated at reasonable depths. (AlphaGo also estimates which moves are most fruitful,
    enabling a second pruning of the game tree space.) AlphaGo’s victory really launched
    deep reinforcement learning into prominence, and a host of researchers are working
    to transform AlphaGo-style systems into practical use.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we discuss reinforcement learning algorithms and specifically
    deep reinforcement learning architectures. We then show readers how to successfully
    apply reinforcement learning to the game of tic-tac-toe. Despite the simplicity
    of the game, training a successful reinforcement learner for tic-tac-toe requires
    significant sophistication, as you will soon see.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: The code for this chapter was adapted from the DeepChem reinforcement learning
    library, and in particular from example code created by Peter Eastman and Karl
    Leswing. Thanks to Peter for debugging and tuning help on this chapter’s example
    code.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Markov Decision Processes
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before launching into a discussion of reinforcement learning algorithms, it
    will be useful to pin down the family of problems that reinforcement learning
    methods seek to solve. The mathematical framework of Markov decision processes
    (MDPs) is very useful for formulating reinforcement learning methods. Traditionally,
    MDPs are introduced with a battery of Greek symbols, but we will instead try to
    proceed by providing some basic intuition.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: The heart of MDPs is the pair of an *environment* and an *agent*. An environment
    encodes a “world” in which the agent seeks to act. Example environments could
    include game worlds. For example, a Go board with master Lee Sedol sitting opposite
    is a valid environment. Another potential environment could be the environment
    surrounding a small robot helicopter. In a prominent early reinforcement learning
    success, a team at Stanford led by Andrew Ng trained a helicopter to fly upside
    down using reinforcement learning as shown in [Figure 8-4](#ch8-upside).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: MDPs的核心是一个*环境*和一个*代理*的组合。环境编码了代理寻求行动的“世界”。示例环境可以包括游戏世界。例如，围棋棋盘上，对面坐着李世石大师，这是一个有效的环境。另一个潜在的环境可能是围绕着一架小型机器人直升机的环境。在斯坦福大学的一个著名的早期强化学习成功中，由安德鲁·吴领导的团队训练了一架直升机使用强化学习倒飞，如[图8-4](#ch8-upside)所示。
- en: '![upside_down_helicopter.jpg](assets/tfdl_0804.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![upside_down_helicopter.jpg](assets/tfdl_0804.png)'
- en: Figure 8-4\. Andrew Ng’s team at Stanford, from 2004 to 2010, trained a helicopter
    to learn to fly upside down using reinforcement learning. This work required the
    construction of a sophisticated physically accurate simulator.
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-4. 安德鲁·吴在斯坦福大学的团队，从2004年到2010年，训练了一架直升机学会使用强化学习倒飞。这项工作需要建立一个复杂而准确的物理模拟器。
- en: The agent is the learning entity that acts within the environment. In our first
    example, AlphaGo itself is the agent. In the second, the robot helicopter (or
    more accurately, the control algorithm in the robot helicopter) is the agent.
    Each agent has a set of actions that it can take within the environment. For AlphaGo,
    these constitute valid Go moves. For the robot helicopter, these include control
    of the main and secondary rotors.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 代理是在环境中行动的学习实体。在我们的第一个例子中，AlphaGo本身就是代理。在第二个例子中，机器人直升机（或更准确地说，机器人直升机中的控制算法）是代理。每个代理都有一组可以在环境中采取的行动。对于AlphaGo，这些构成有效的围棋着法。对于机器人直升机，这些包括控制主旋翼和副旋翼。
- en: Actions the agent takes are presumed to have an effect on the environment. In
    the case of AlphaGo, this effect is deterministic (AlphaGo deciding to place a
    Go stone results in the stone being placed). In the case of the helicopter, the
    effect is likely probabilistic (changes in helicopter position may depend on wind
    conditions, which can’t be modeled effectively).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 代理所采取的行动被认为会对环境产生影响。在AlphaGo的情况下，这种影响是确定性的（AlphaGo决定放置一个围棋子导致该子被放置）。在直升机的情况下，影响可能是概率性的（直升机位置的变化可能取决于风况，这不能有效地建模）。
- en: The final piece of the model is the notion of reward. Unlike supervised learning
    where explicit labels are present to learn from, or unsupervised learning where
    the challenge is to learn the underlying structure of the data, reinforcement
    learning operates in a setting of partial, sparse rewards. In Go, rewards are
    achieved at the end of the game upon victory or defeat, while in helicopter flight,
    rewards might be presented for successful flights or completion of trick moves.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的最后一部分是奖励的概念。与监督学习不同，监督学习中存在明确的标签供学习，或者无监督学习中的挑战是学习数据的潜在结构，强化学习在部分、稀疏奖励的环境中运作。在围棋中，奖励是在游戏结束时获得的，无论是胜利还是失败，而在直升机飞行中，奖励可能是为成功飞行或完成特技动作而给出的。
- en: Reward Function Engineering Is Hard
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奖励函数的设计很困难
- en: One of the largest challenges in reinforcement learning is designing rewards
    that induce agents to learn desired behaviors. For even simple win/loss games
    such as Go or tic-tac-toe, this can be surprisingly difficult. How much should
    a loss be punished and how much should a win be rewarded? There don’t yet exist
    good answers.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习中最大的挑战之一是设计奖励，以促使代理人学习所需的行为。即使是简单的赢/输游戏，如围棋或井字棋，这也可能非常困难。输掉应该受到多少惩罚，赢得应该受到多少奖励？目前还没有好的答案。
- en: For more complex behaviors, this can be extremely challenging. A number of studies
    have demonstrated that simple rewards can result in agents learning unexpected
    and even potentially damaging behaviors. These systems spur fears of future agents
    with greater autonomy wreaking havoc when unleashed in the real world after having
    been trained to optimize bad reward functions.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更复杂的行为，这可能非常具有挑战性。许多研究表明，简单的奖励可能导致代理学习出人意料的甚至可能有害的行为。这些系统引发了对未来代理人具有更大自主权的担忧，当它们在真实世界中被释放后，经过训练以优化不良奖励函数时，可能会造成混乱。
- en: In general, reinforcement learning is less mature than supervised learning techniques,
    and we caution that decisions to deploy reinforcement learning in production systems
    should be taken very carefully. Given uncertainty over learned behavior, make
    sure to thoroughly test any deployed reinforcement learned system.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，强化学习比监督学习技术不够成熟，我们警告应该非常谨慎地决定在生产系统中部署强化学习。考虑到对学习行为的不确定性，请确保彻底测试任何部署的强化学习系统。
- en: Reinforcement Learning Algorithms
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习算法
- en: Now that we’ve introduced you to the core mathematical structures underlying
    reinforcement learning, let’s consider how to design algorithms that learn intelligent
    behaviors for reinforcement learning agents. At a high level, reinforcement learning
    algorithms can be separated into the buckets of *model-based* and *model-free*
    algorithms. The central difference is whether the algorithm seeks to learn an
    internal model of how its environment acts. For simpler environments, such as
    tic-tac-toe, the model dynamics are trivial. For more complex environments, such
    as helicopter flight or even ATARI games, the underlying environment is likely
    extraordinarily complex. Avoiding the construction of an explicit model of the
    environment in favor of an implicit model that advises the agent on how to act
    may well be more pragmatic.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经向您介绍了强化学习的核心数学结构，让我们考虑如何设计算法来学习强化学习代理的智能行为。在高层次上，强化学习算法可以分为*基于模型*和*无模型*算法。中心区别在于算法是否试图学习其环境如何行动的内部模型。对于简单的环境，如井字棋，模型动态是微不足道的。对于更复杂的环境，如直升机飞行甚至
    ATARI 游戏，底层环境可能非常复杂。避免构建环境的显式模型，而是采用隐式模型来指导代理如何行动可能更为实际。
- en: Simulations and Reinforcement Learning
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模拟和强化学习
- en: Any reinforcement learning algorithm requires iteratively improving the performance
    of the current agent by evaluating the agent’s current behavior and changing it
    to improve received rewards. These updates to the agent structure often include
    some gradient descent update, as we will see in the following sections. However,
    as you know intimately from previous chapters, gradient descent is a slow training
    algorithm! Millions or even billions of gradient descent steps may be required
    to learn an effective model.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 任何强化学习算法都需要通过评估代理当前行为并改变以改善获得的奖励来迭代地提高当前代理的性能。这些对代理结构的更新通常包括一些梯度下降更新，我们将在接下来的章节中看到。然而，正如您从之前的章节中熟知的那样，梯度下降是一种缓慢的训练算法！可能需要数百万甚至数十亿次梯度下降步骤才能学习到有效的模型。
- en: This poses a problem if the learning environment is in the real world; how can
    an agent interact millions of times with the real world? In most cases it can’t.
    As a result, most sophisticated reinforcement learning systems depend critically
    on simulators that allow interaction with a simulation computational version of
    the environment. For the helicopter flight environment, one of the hardest challenges
    researchers faced was building an accurate helicopter physics simulator that allowed
    learning of effective flight policies computationally.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这在学习环境是真实世界时会带来问题；一个代理如何能与真实世界互动数百万次？在大多数情况下是不可能的。因此，大多数复杂的强化学习系统在很大程度上依赖于模拟器，这些模拟器允许与环境的计算版本进行交互。对于直升机飞行环境，研究人员面临的最大挑战之一是构建一个准确的直升机物理模拟器，以便计算学习有效的飞行策略。
- en: Q-Learning
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Q-Learning
- en: In the framework of Markov decision processes, agents take actions in an environment
    and obtain rewards that are (presumably) tied to agent actions. The *Q* function
    predicts the expected reward for taking a given action in a particular environment
    state. This concept seems very straightforward, but the trickiness arises when
    this expected reward includes discounted rewards from future actions.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在马尔可夫决策过程的框架中，代理在环境中采取行动并获得与代理行动相关的奖励。*Q*函数预测在特定环境状态下采取特定行动的预期奖励。这个概念似乎非常简单，但当这个预期奖励包括来自未来行动的折扣奖励时，就会变得棘手。
- en: Discounting Rewards
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 折扣奖励
- en: The notion of a discounted reward is widespread, and is often introduced in
    the context of finances. Suppose a friend says he’ll pay you $10 next week. That
    future 10 dollars is worth less to you than 10 dollars in your hand right now
    (what if the payment never happens, for one?). So mathematically, it’s common
    practice to introduce a discounting factor γ (typically between 0 and 1) that
    lowers the “present-value” of future payments. For example, say your friend is
    somewhat untrustworthy. You might decide to set γ = 0.5 and value your friend’s
    promise as worth 10γ = 5 dollars today to account for uncertainty in rewards.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 折扣奖励的概念很普遍，并且通常在财务领域引入。假设一个朋友说他下周会给你 10 美元。那未来的 10 美元对你来说不如手头上的 10 美元值钱（如果支付没有发生呢？）。因此，在数学上，引入一个降低未来支付“现值”的折扣因子
    γ（通常在 0 和 1 之间）是常见做法。例如，假设你的朋友有点不太可靠。你可能决定设置 γ = 0.5，并将你朋友的承诺价值设为 10γ = 5 美元今天，以考虑奖励的不确定性。
- en: However, these future rewards depend on actions taken by the agent in the future.
    As a result, the *Q* function must be formulated recursively in terms of itself,
    since expected rewards for one state depend on those for another state. This recursive
    definition makes learning the *Q* function tricky. This recursive relationship
    can be formulated explicitly for simple environments with discrete state spaces
    and solved with dynamic programming methods. For more general environments, *Q*-learning
    methods were not very useful until recently.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些未来的奖励取决于代理未来采取的行动。因此，*Q*函数必须以递归的方式根据自身进行公式化，因为一个状态的预期奖励取决于另一个状态的奖励。这种递归定义使得学习*Q*函数变得棘手。这种递归关系可以在具有离散状态空间的简单环境中明确表达，并且可以用动态规划方法解决。对于更一般的环境，*Q*-learning方法直到最近才变得有用。
- en: Recently, Deep *Q*-networks (DQN) were introduced by DeepMind and used to solve
    ATARI games as mentioned earlier. The key insight underlying DQN is once again
    the universal approximation theorem; since *Q* may be arbitrarily complex, we
    should model it with a universal approximator such as a deep network. While using
    neural networks to model *Q* had been done before, DeepMind also introduced the
    notion of experience replay for these networks, which let them train DQN models
    effectively at scale. Experience replay stores observed game outcomes and transitions
    from past games, and resamples them while training (in addition to training on
    new games) to ensure that lessons from the past are not forgotten by the network.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Deep *Q*-networks（DQN）由DeepMind引入，并被用于解决ATARI游戏，如前面提到的。DQN背后的关键见解再次是通用逼近定理；由于*Q*可能是任意复杂的，我们应该用一个通用逼近器，如深度网络来建模它。虽然以前已经使用神经网络来建模*Q*，但DeepMind还为这些网络引入了经验重播的概念，这让它们能够有效地大规模训练DQN模型。经验重播存储观察到的游戏结果和过去游戏的转换，并在训练时重新采样它们（除了在新游戏上训练）以确保网络不会忘记过去的教训。
- en: Catastrophic Forgetting
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 灾难性遗忘
- en: Neural networks quickly forget the past. In fact, this phenomenon, termed *catastrophic
    forgetting*, can occur very rapidly; a few minibatch updates can be sufficient
    for the network to forget a complex behavior it previously knew. As a result,
    without techniques like experience replay that ensure the network always trains
    on episodes from past matches, it wouldn’t be possible to learn complex behaviors.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络很快忘记过去。事实上，这种现象被称为*灾难性遗忘*，可以非常迅速地发生；几个小批量更新就足以使网络忘记它先前知道的复杂行为。因此，如果没有像经验重播这样的技术，确保网络始终在过去比赛的情节上训练，就不可能学习复杂的行为。
- en: Designing a training algorithm for deep networks that doesn’t suffer from catastrophic
    forgetting is still a major open problem today. Humans notably don’t suffer from
    catastrophic forgetting; even if you haven’t ridden a bike in years, it’s likely
    you still remember how to do so. Creating a neural network that has similar resilience
    might involve the addition of long-term external memory, along the lines of the
    Neural Turing machine. Unfortunately, none of the attempts thus far at designing
    resilient architectures has really worked well.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 设计一个不会遭受灾难性遗忘的深度网络训练算法仍然是一个当今的主要开放问题。人类明显不会遭受灾难性遗忘；即使多年没有骑自行车，您仍然可能记得如何骑。创建一个具有类似韧性的神经网络可能涉及添加长期外部记忆，类似于神经图灵机。不幸的是，迄今为止设计具有韧性架构的尝试都没有真正取得良好的效果。
- en: Policy Learning
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略学习
- en: In the previous section, you learned about *Q*-learning, which seeks to understand
    the expected rewards for taking given actions in given environment states. Policy
    learning is an alternative mathematical framework for learning agent behavior.
    It introduces the policy function π that assigns a probability to each action
    that an agent can take in a given state.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，您了解了*Q*-learning，它旨在了解在给定环境状态下采取特定动作的预期奖励。策略学习是一种学习代理行为的替代数学框架。它引入了策略函数π，为代理在给定状态下可以采取的每个动作分配概率。
- en: Note that a policy is sufficient for defining agent behavior entirely. Given
    a policy, an agent can act just by sampling a suitable action for the current
    environment state. Policy learning is convenient, since policies can be learned
    directly through an algorithm called policy gradient. This algorithm uses a couple
    mathematical tricks to enable policy gradients to be computed directly via backpropagation
    for deep networks. The key concept is the *rollout*. Let an agent act in an environment
    according to its current policy and observe all rewards that come in. Then backpropagate
    to increase the likelihood of those actions that led to more fruitful rewards.
    This description is accurate at a high level, but we will see more implementation
    details later in the chapter.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，策略足以完全定义代理行为。给定一个策略，代理可以通过为当前环境状态抽样一个合适的动作来行动。策略学习很方便，因为策略可以通过称为策略梯度的算法直接学习。这个算法使用一些数学技巧，通过反向传播来计算深度网络的策略梯度。关键概念是*展开*。让一个代理根据其当前策略在环境中行动，并观察所有获得的奖励。然后反向传播，增加那些导致更有益奖励的动作的可能性。这个描述在高层次上是准确的，但我们将在本章后面看到更多的实现细节。
- en: A policy is often associated with a *value function* *V*. This function returns
    the expected discounted reward for following policy π starting from the current
    state of the environment. *V* and *Q* are closely related functions since both
    provide estimates of future rewards starting from present state, but *V* does
    not specify an action to be taken and assumes rather that actions are sampled
    from π.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 策略通常与*值函数* *V*相关联。这个函数返回从环境的当前状态开始遵循策略π的预期折扣奖励。*V*和*Q*是密切相关的函数，因为两者都提供了从当前状态开始估计未来奖励的估计，但*V*不指定要采取的动作，而是假设动作是从π中抽样的。
- en: 'Another commonly defined function is the *advantage* *A*. This function defines
    the difference in expected reward due to taking a particular action *a* in a given
    environment state *s*, as opposed to following the base policy π. Mathematically,
    *A* is defined in terms of *Q* and *V*:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见定义的函数是*优势* *A*。这个函数定义了由于在给定环境状态*s*中采取特定动作*a*而预期奖励的差异，与遵循基本策略π相比。在数学上，*A*是根据*Q*和*V*定义的：
- en: <math display="block"><mrow><mi>A</mi> <mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi>
    <mo>)</mo> <mo>=</mo> <mi>Q</mi> <mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo>
    <mo>-</mo> <mi>V</mi> <mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></math>
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>A</mi> <mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi>
    <mo>)</mo> <mo>=</mo> <mi>Q</mi> <mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo>
    <mo>-</mo> <mi>V</mi> <mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></math>
- en: The advantage is useful in policy-learning algorithms, since it lets an algorithm
    quantify how a particular action may have been better suited than the present
    recommendation of the policy.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 优势在策略学习算法中很有用，因为它让算法能够量化一个特定动作可能比策略当前推荐更合适的程度。
- en: Policy Gradient Outside Reinforcement Learning
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略梯度在强化学习之外
- en: Although we have introduced policy gradient as a reinforcement learning algorithm,
    it can equally be viewed as a tool for learning deep networks with nondifferentiable
    submodules. What does this mean when we unpack the mathematical jargon?
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经将策略梯度介绍为一种强化学习算法，但它同样可以被视为一种学习具有不可微子模块的深度网络的工具。当我们解开数学术语时，这意味着什么？
- en: Let’s suppose we have a deep network that calls an external program within the
    network itself. This external program is a black box; it could be a network call
    or an invocation of a 1970s COBOL routine. How can we learn the rest of the deep
    network when this module has no gradient?
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个深度网络，在网络内部调用一个外部程序。这个外部程序是一个黑匣子；它可能是一个网络调用，也可能是对1970年代COBOL例程的调用。当这个模块没有梯度时，我们如何学习深度网络的其余部分？
- en: It turns out that policy gradient can be repurposed to estimate an “effective”
    gradient for the system. The simple intuition is that multiple “rollouts” can
    be run, which are used to estimate gradients. Expect to see research over the
    next few years extending this idea to create large networks with nondifferential
    modules.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，策略梯度可以被重新用于估计系统的“有效”梯度。简单的直觉是可以运行多个“rollouts”，用于估计梯度。预计在未来几年会看到研究将这个想法扩展到创建具有不可微分模块的大型网络。
- en: Asynchronous Training
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异步训练
- en: A disadvantage of the policy gradient methods presented in the previous section
    is that performing the rollout operations requires evaluating agent behavior in
    some (likely simulated) environment. Most simulators are complicated pieces of
    software that can’t be run on the GPU. As a result, taking a single learning step
    will require running long CPU-bound calculations. This can lead to unreasonably
    slow training.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节介绍的策略梯度方法的一个缺点是，执行rollout操作需要评估代理在某个（可能是模拟的）环境中的行为。大多数模拟器都是复杂的软件，无法在GPU上运行。因此，进行单个学习步骤将需要运行长时间的CPU绑定计算。这可能导致训练速度过慢。
- en: Asynchronous reinforcement learning methods seek to speed up this process by
    using multiple asynchronous CPU threads to perform rollouts independently. These
    worker threads will perform rollouts, estimate gradient updates to the policy
    locally, and then periodically synchronize with the global set of parameters.
    Empirically, asynchronous training appears to significantly speed up reinforcement
    learning and allows for fairly sophisticated policies to be learned on laptops.
    (Without GPUs! The majority of computational power is used on rollouts, so gradient
    update steps are often not the rate limiting aspect of reinforcement learning
    training.) The most popular algorithm for asynchronous reinforcement learning
    currently is the asynchronous actor advantage critic (A3C) algorithm.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 异步强化学习方法通过使用多个异步CPU线程独立执行rollouts来加速这个过程。这些工作线程将执行rollouts，本地估计策略的梯度更新，然后定期与全局参数集进行同步。从经验上看，异步训练似乎显著加速了强化学习，并允许在笔记本电脑上学习相当复杂的策略。（没有GPU！大部分计算能力用于rollouts，因此梯度更新步骤通常不是强化学习训练的速度限制因素。）目前最流行的异步强化学习算法是异步演员优势评论家（A3C）算法。
- en: CPU or GPU?
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CPU还是GPU？
- en: GPUs are necessary for most large deep learning applications, but reinforcement
    learning currently appears to be an exception to this general rule. The reliance
    of reinforcement learning algorithms to perform many rollouts seems to currently
    bias reinforcement learning implementations toward multicore CPU systems. It’s
    likely that in specific applications, individual simulators can be ported to work
    more quickly on GPUs, but CPU-based simulations will likely continue to dominate
    for the near future.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数大型深度学习应用都需要GPU，但目前强化学习似乎是一个例外。强化学习算法依赖于执行许多rollouts，目前似乎偏向于多核CPU系统。在特定应用中，个别模拟器可能会被移植到GPU上更快地运行，但基于CPU的模拟器可能会在不久的将来继续占主导地位。
- en: Limits of Reinforcement Learning
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习的局限性
- en: The framework of Markov decision processes is immensely general. For example,
    behavioral scientists routinely use Markov decision processes to understand and
    model human decision making. The mathematical generality of this framework has
    spurred scientists to posit that solving reinforcement learning might spur the
    creation of artificial general intelligences (AGIs). The stunning success of AlphaGo
    against Lee Sedol amplified this belief, and indeed research groups such as OpenAI
    and DeepMind aiming to build AGIs focus much of their efforts on developing new
    reinforcement learning techniques.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程的框架是非常普遍的。例如，行为科学家通常使用马尔可夫决策过程来理解和建模人类决策过程。这个框架的数学普遍性促使科学家们提出，解决强化学习可能会促使人工通用智能（AGI）的创造。AlphaGo对李世石的惊人成功加强了这种信念，事实上，像OpenAI和DeepMind这样的研究团队致力于构建AGI，他们大部分的努力都集中在开发新的强化学习技术上。
- en: Nonetheless, there are major weaknesses to reinforcement learning as it currently
    exists. Careful benchmarking work has shown that reinforcement learning techniques
    are very susceptible to choice of hyperparameters (even by the standards of deep
    learning, which is already much finickier than other techniques like random forests).
    As we have mentioned, reward function engineering is very immature. Humans are
    capable of internally designing their own reward functions or effectively learning
    to copy reward functions from observation. Although “inverse reinforcement learning”
    algorithms that learn reward functions directly have been proposed, these algorithms
    have many limitations in practice.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，强化学习目前存在重大弱点。仔细的基准测试工作表明，强化学习技术对超参数的选择非常敏感（即使按照深度学习的标准，它已经比其他技术如随机森林更加挑剔）。正如我们所提到的，奖励函数工程非常不成熟。人类能够内部设计自己的奖励函数，或者有效地学习从观察中复制奖励函数。虽然已经提出了直接学习奖励函数的“逆强化学习”算法，但这些算法在实践中存在许多限制。
- en: In addition to these fundamental limitations, there are still many practical
    scaling issues. Humans are capable of playing games that combine high-level strategizing
    with thousands of “micro” moves. For example, master-level play of the strategy
    game StarCraft (see [Figure 8-5](#ch8-starcraft)) requires sophisticated strategic
    ploys combined with careful control of hundreds of units. Games can require thousands
    of local moves to be played to completion. In addition, unlike Go or chess, StarCraft
    has a “fog of war” where players cannot see the entire game state. This combination
    of large game state and uncertainty has foiled reinforcement learning attempts
    on StarCraft. As a result, teams of AI researchers at DeepMind and other groups
    are focusing serious effort on solving StarCraft with deep reinforcement learning
    methods. Despite some serious effort, though, the best StarCraft bots remain at
    amateur level.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些基本限制外，仍然存在许多实际的扩展问题。人类能够玩那些将高级战略与成千上万的“微”动作结合在一起的游戏。例如，玩策略游戏星际争霸的大师级水平（参见[图8-5](#ch8-starcraft)）需要复杂的战略策略，结合对数百个单位的精心控制。游戏可能需要数千次局部移动才能完成。此外，与围棋或国际象棋不同，星际争霸有一个“战争迷雾”，玩家无法看到整个游戏状态。这种大规模游戏状态和不确定性的结合使得强化学习在星际争霸上的尝试失败。因此，DeepMind和其他团队的AI研究人员正在集中精力用深度强化学习方法解决星际争霸。尽管付出了一些努力，但最好的星际争霸机器人仍停留在业余水平。
- en: '![deep_starcraft.png](assets/tfdl_0805.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![deep_starcraft.png](assets/tfdl_0805.png)'
- en: Figure 8-5\. A collection of subtasks required for playing the real-time strategy
    game StarCraft. In this game, players must build an army that they can use to
    defeat the opposing force. Successful StarCraft play requires mastery of resource
    planning, exploration, and complex strategy. The best computer StarCraft agents
    remain at amateur level.
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-5. 为玩实时战略游戏星际争霸所需的一系列子任务。在这个游戏中，玩家必须建立一个可以用来击败对方的军队。成功的星际争霸游戏需要掌握资源规划、探索和复杂策略。最好的计算机星际争霸代理仍停留在业余水平。
- en: In general, there’s wide consensus that reinforcement learning is a useful technique
    that’s likely to be deeply influential over the next few decades, but it’s also
    clear that the many practical limitations of reinforcement learning methods will
    mean that most work will continue to be done in research labs for the near future.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，人们普遍认为强化学习是一种有用的技术，可能在未来几十年内产生深远影响，但也清楚强化学习方法的许多实际限制意味着大部分工作在短期内仍将继续在研究实验室中进行。
- en: Playing Tic-Tac-Toe
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 玩井字棋
- en: Tic-tac-toe is a simple two-player game. Players place Xs and Os on a 3 × 3
    game board until one player succeeds in placing three of her pieces in a row.
    The first player to do so wins. If neither player succeeds in obtaining three
    in a row before the board is filled up, the game ends in a draw. [Figure 8-6](#ch8-tictactoe)
    illustrates a tic-tac-toe game board.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 井字棋是一种简单的双人游戏。玩家在一个3×3的游戏棋盘上放置X和O，直到一名玩家成功地将她的三个棋子排成一行。首先完成这个目标的玩家获胜。如果在棋盘填满之前没有玩家成功排成三个一行，游戏以平局结束。[图8-6](#ch8-tictactoe)展示了一个井字棋游戏棋盘。
- en: '![Tic_tac_toe.png](assets/tfdl_0806.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![Tic_tac_toe.png](assets/tfdl_0806.png)'
- en: Figure 8-6\. A tic-tac-toe game board.
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-6. 一个井字棋游戏棋盘。
- en: Tic-tac-toe is a nice testbed for reinforcement learning techniques. The game
    is simple enough that exorbitant amounts of computational power aren’t required
    to train effective agents. At the same time, despite tic-tac-toe’s simplicity,
    learning an effective agent requires considerable sophistication. The TensorFlow
    code for this section is arguably the most sophisticated example found in this
    book. We will walk you through the design of a TensorFlow tic-tac-toe asynchronous
    reinforcement learning agent in the remainder of this section.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 井字棋是强化学习技术的一个很好的测试平台。这个游戏足够简单，不需要大量的计算能力来训练有效的代理程序。与此同时，尽管井字棋很简单，学习一个有效的代理程序需要相当的复杂性。本节中的TensorFlow代码可以说是本书中最复杂的例子。在本节的其余部分，我们将带领你设计一个TensorFlow井字棋异步强化学习代理程序。
- en: Object Orientation
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 面向对象
- en: The code we’ve introduced thus far in this book has primarily consisted of scripts
    augmented by smaller helper functions. In this chapter, however, we will swap
    to an object-oriented programming style. This style of programming might be new
    to you, especially if you hail from the scientific world rather than from the
    tech world. Briefly, an object-oriented program defines *objects* that model aspects
    of the world. For example, you might want to define `Environment` or `Agent` or
    `Reward` objects that directly correspond to these mathematical concepts. A *class*
    is a template for objects that can be used to *instantiate* (or create) many new
    objects. For example, you will shortly see an `Environment` class definition we
    will use to define many particular `Environment` objects.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本书中介绍的代码主要由脚本和较小的辅助函数组成。然而，在本章中，我们将转向面向对象的编程风格。如果你来自科学界而不是技术界，这种编程风格可能对你来说是新的。简而言之，面向对象的程序定义了*对象*，这些对象模拟了世界的各个方面。例如，你可能想要定义`Environment`、`Agent`或`Reward`对象，这些对象直接对应这些数学概念。*类*是用于*实例化*（或创建）许多新对象的对象模板。例如，你很快就会看到一个`Environment`类定义，我们将用它来定义许多特定的`Environment`对象。
- en: Object orientation is particularly powerful for building complex systems, so
    we will use it to simplify the design of our reinforcement learning system. In
    practice, your real-world deep learning (or reinforcement learning) systems will
    likely need to be object oriented as well, so we encourage taking some time to
    master object-oriented design. There are many superb books that cover the fundamentals
    of object-oriented design, and we recommend that you check them out as necessary.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 面向对象是构建复杂系统的特别强大的工具，因此我们将使用它来简化我们的强化学习系统的设计。实际上，你的现实世界深度学习（或强化学习）系统很可能也需要是面向对象的，因此我们鼓励你花一些时间来掌握面向对象设计。有许多优秀的书籍涵盖了面向对象设计的基础知识，我们建议你在需要时查阅。
- en: Abstract Environment
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 抽象环境
- en: Let’s start by defining an abstract `Environment` object that encodes the state
    of a system in a list of NumPy objects ([Example 8-1](#ch8-envclass)). This `Environment`
    object is quite general (adapted from DeepChem’s reinforcement learning engine)
    so it can easily serve as a template for other reinforcement learning projects
    you might seek to implement.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从定义一个抽象的`Environment`对象开始，它将系统的状态编码为NumPy对象的列表（[示例8-1](#ch8-envclass)）。这个`Environment`对象非常通用（改编自DeepChem的强化学习引擎），因此它可以很容易地作为您可能想要实现的其他强化学习项目的模板。
- en: Example 8-1\. This class defines a template for constructing new environments
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-1。这个类定义了构建新环境的模板
- en: '[PRE0]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Tic-Tac-Toe Environment
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 井字棋环境
- en: We need to specialize the `Environment` class to create a `TicTacToeEnvironment`
    suitable for our needs. To do this, we construct a *subclass* of `Environment`
    that adds on more features, while retaining the core functionality of the original
    *superclass*. In [Example 8-2](#ch8-tictacenvclass), we define `TicTacToeEnvironment`
    as a subclass of `Environment` that adds details specific to tic-tac-toe.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要专门化`Environment`类以创建一个适合我们需求的`TicTacToeEnvironment`。为此，我们构建一个`Environment`的*子类*，添加更多功能，同时保留原始*超类*的核心功能。在[示例8-2](#ch8-tictacenvclass)中，我们将`TicTacToeEnvironment`定义为`Environment`的子类，添加了特定于井字棋的细节。
- en: Example 8-2\. The TicTacToeEnvironment class defines a template for constructing
    new tic-tac-toe environments
  id: totrans-81
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-2。TicTacToeEnvironment类定义了构建新井字棋环境的模板
- en: '[PRE1]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The first interesting tidbit to note here is that we define the board state
    as a NumPy array of shape `(3, 3, 2)`. We use a one-hot encoding of `X` and `O`
    (one-hot encodings aren’t only useful for natural language processing!).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这里要注意的第一个有趣的细节是，我们将棋盘状态定义为形状为`(3, 3, 2)`的NumPy数组。我们使用了`X`和`O`的独热编码（独热编码不仅在自然语言处理中有用！）。
- en: The second important thing to note is that the environment explicitly defines
    the reward function by setting penalties for illegal moves and losses, and rewards
    for draws and wins. This snippet powerfully illustrates the arbitrary nature of
    reward function engineering. Why these particular numbers?
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个需要注意的重要事情是，环境通过设置违规移动和失败的惩罚以及平局和胜利的奖励来明确定义奖励函数。这段代码强有力地说明了奖励函数工程的任意性。为什么是这些特定的数字？
- en: Empirically, these choices appear to result in stable behavior, but we encourage
    you to experiment with alternate reward settings to observe results. In this implementation,
    we specify that the agent always plays `X`, but randomize whether `X` or `O` goes
    first. The function `get_O_move()` simply places an `O` on a random open tile
    on the game board. `TicTacToeEnvironment` encodes an opponent that plays `O` while
    always selecting a random move. The `reset()` function simply clears the board,
    and places an `O` tile randomly if `O` is going first during this game. See [Example 8-3](#ch8-tictacenvclass2).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 从经验上看，这些选择似乎导致了稳定的行为，但我们鼓励您尝试使用替代奖励设置来观察结果。在这个实现中，我们指定代理始终扮演`X`，但随机确定是`X`还是`O`先走。函数`get_O_move()`简单地在游戏棋盘上的随机空格上放置一个`O`。`TicTacToeEnvironment`编码了一个扮演`O`的对手，总是选择一个随机移动。`reset()`函数简单地清空棋盘，并在这个游戏中`O`先走时随机放置一个`O`。参见[示例8-3](#ch8-tictacenvclass2)。
- en: Example 8-3\. More methods from the TicTacToeEnvironment class
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-3。TicTacToeEnvironment类的更多方法
- en: '[PRE2]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The utility function `game_over()` reports that the game has ended if all tiles
    are filled. `check_winner()` checks whether the specified player has achieved
    three in a row and won the game ([Example 8-4](#ch8-tictacenvclass3)).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 实用函数`game_over()`报告游戏是否结束，如果所有方块都填满。`check_winner()`检查指定的玩家是否取得了三连胜并赢得了比赛（[示例8-4](#ch8-tictacenvclass3)）。
- en: Example 8-4\. Utility methods from the TicTacToeEnvironment class for detecting
    when the game has ended and who won
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-4。TicTacToeEnvironment类的用于检测游戏是否结束以及谁赢得比赛的实用方法
- en: '[PRE3]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In our implementation, an action is simply a number between 0 and 8 specifying
    the tile on which the `X` tile is placed. The `step()` method checks whether this
    tile is occupied (returning a penalty if so), then places the tile. If `X` has
    won, a reward is returned. Else, the random `O` opponent is allowed to make a
    move. If `O` won, then a penalty is returned. If the game has ended as a draw,
    then a penalty is returned. Else, the game continues with a `NOT_LOSS` reward.
    See [Example 8-5](#ch8-tictacenvclass4).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实现中，一个动作只是一个介于0和8之间的数字，指定`X`方块放置的位置。`step()`方法检查这个位置是否被占用（如果是，则返回惩罚），然后放置方块。如果`X`赢了，就会返回奖励。否则，允许随机的`O`对手进行移动。如果`O`赢了，就会返回惩罚。如果游戏以平局结束，就会返回惩罚。否则，游戏将继续进行，获得一个`NOT_LOSS`奖励。参见[示例8-5](#ch8-tictacenvclass4)。
- en: Example 8-5\. This method performs a step of the simulation
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-5。这个方法执行模拟的一步
- en: '[PRE4]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The Layer Abstraction
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层次抽象
- en: Running an asynchronous reinforcement learning algorithm such as A3C requires
    that each thread have access to a separate copy of the policy model. These copies
    of the model have to be periodically re-synced with one another for training to
    proceed. What is the easiest way we can construct multiple copies of the TensorFlow
    graph that we can distribute to each thread?
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 运行异步强化学习算法，如A3C，需要每个线程都能访问一个策略模型的单独副本。这些模型的副本必须定期与彼此重新同步，以便进行训练。我们可以构建多个TensorFlow图的最简单方法是什么？
- en: One simple possibility is to create a function that creates a copy of the model
    in a separate TensorFlow graph. This approach works well, but gets to be a little
    messy, especially for sophisticated networks. Using a little bit of object orientation
    can significantly simplify this process. Since our reinforcement learning code
    is adapted from the DeepChem library, we use a simplified version of the TensorGraph
    framework from DeepChem (see [*https://deepchem.io*](https://deepchem.io) for
    information and docs). This framework is similar to other high-level TensorFlow
    frameworks such as Keras. The core abstraction in all such models is the introduction
    of a `Layer` object that encapsulates a portion of a deep network.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的可能性是创建一个函数，在一个单独的 TensorFlow 图中创建模型的副本。这种方法效果很好，但对于复杂的网络来说会变得有点混乱。使用一点面向对象的方法可以显著简化这个过程。由于我们的强化学习代码是从
    DeepChem 库中改编而来的，我们使用了 DeepChem 中的 TensorGraph 框架的简化版本（请参阅 [*https://deepchem.io*](https://deepchem.io)
    获取信息和文档）。这个框架类似于其他高级 TensorFlow 框架，比如 Keras。所有这些模型中的核心抽象是引入一个 `Layer` 对象，它封装了深度网络的一部分。
- en: A `Layer` is a portion of a TensorFlow graph that accepts a list `in_layers`
    of input layers. In this abstraction, a deep architecture consists of a *directed
    graph* of layers. Directed graphs are similar to the undirected graphs you saw
    in [Chapter 6](ch06.html#convolutional_neural_networks), but have directions on
    their edges. In this case, the `in_layers` have edges to the new `Layer`, with
    the direction pointing toward the new layer. You will learn more about this concept
    in the next section.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`Layer` 是 TensorFlow 图中的一个部分，接受输入层列表 `in_layers`。在这个抽象中，深度架构由层的 *有向图* 组成。有向图类似于您在[第6章](ch06.html#convolutional_neural_networks)中看到的无向图，但是它们的边上有方向。在这种情况下，`in_layers`
    有边指向新的 `Layer`，方向指向新层。您将在下一节中了解更多关于这个概念的内容。'
- en: We use `tf.register_tensor_conversion_function`, a utility that allows arbitrary
    classes to register themselves as tensor convertible. This registration will mean
    that a `Layer` can be converted into a TensorFlow tensor via a call to `tf.convert_to_tensor`.
    The `_get_input_tensors()` private method is a utility that uses `tf.convert_to_tensor`
    to transform input layers into input tensors. Each `Layer` is responsible for
    implementing a `create_tensor()` method that specifies the operations to add to
    the TensorFlow computational graph. See [Example 8-6](#ch8-layer).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `tf.register_tensor_conversion_function`，这是一个允许任意类注册为可转换为张量的实用程序。这个注册意味着一个
    `Layer` 可以通过调用 `tf.convert_to_tensor` 被转换为 TensorFlow 张量。`_get_input_tensors()`
    私有方法是一个实用程序，它使用 `tf.convert_to_tensor` 将输入层转换为输入张量。每个 `Layer` 负责实现一个 `create_tensor()`
    方法，该方法指定要添加到 TensorFlow 计算图中的操作。请参见 [示例 8-6](#ch8-layer)。
- en: Example 8-6\. The Layer object is the fundamental abstraction in object-oriented
    deep architectures. It encapsulates a part of the netwok such as a fully connected
    layer or a convolutional layer. This example defines a generic superclass for
    all such layers.
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-6\. Layer 对象是面向对象深度架构中的基本抽象。它封装了网络的一部分，比如全连接层或卷积层。这个示例定义了所有这些层的通用超类。
- en: '[PRE5]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The preceding description is abstract, but in practice easy to use. [Example 8-7](#ch8-squeeze)
    shows a `Squeeze` layer that wraps `tf.squeeze` with a `Layer` (you will find
    this class convenient later). Note that `Squeeze` is a subclass of `Layer`.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的描述是抽象的，但在实践中很容易使用。[示例 8-7](#ch8-squeeze) 展示了一个 `Squeeze` 层，它使用 `Layer` 包装了
    `tf.squeeze`（您稍后会发现这个类很方便）。请注意，`Squeeze` 是 `Layer` 的子类。
- en: Example 8-7\. The Squeeze layer squeezes its input
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-7\. Squeeze 层压缩其输入
- en: '[PRE6]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `Input` layer wraps placeholders for convenience ([Example 8-8](#ch8-input)).
    Note that the `Layer.create_tensor` method must be invoked for each layer we use
    in order to construct a TensorFlow computational graph.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`Input` 层为方便起见包装了占位符（[示例 8-8](#ch8-input)）。请注意，必须为我们使用的每个层调用 `Layer.create_tensor`
    方法，以构建 TensorFlow 计算图。'
- en: Example 8-8\. The Input layer adds placeholders to the computation graph
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 8-8\. Input 层向计算图添加占位符
- en: '[PRE7]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: tf.keras and tf.estimator
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: tf.keras 和 tf.estimator
- en: TensorFlow has now integrated the popular Keras object-oriented frontend into
    the core TensorFlow library. Keras includes a `Layer` class definition that closely
    matches the `Layer` objects you’ve just learned about in this section. In fact,
    the `Layer` objects here were adapted from the DeepChem library, which in turn
    adapted them from an earlier version of Keras.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 现在将流行的 Keras 面向对象前端集成到核心 TensorFlow 库中。Keras 包括一个 `Layer` 类定义，与本节中刚学到的
    `Layer` 对象非常相似。事实上，这里的 `Layer` 对象是从 DeepChem 库中改编的，而 DeepChem 又是从较早版本的 Keras 改编而来的。
- en: It’s worth noting, though, that `tf.keras` has not yet become the standard higher-level
    interface to TensorFlow. The `tf.estimator` module provides an alternative (albeit
    less rich) high-level interface to raw TensorFlow.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，`tf.keras` 还没有成为 TensorFlow 的标准高级接口。`tf.estimator` 模块提供了一个替代（尽管不那么丰富）的高级接口到原始
    TensorFlow。
- en: Regardless of which frontend eventually becomes standard, we think that understanding
    the fundamental design principles for building your own frontend is instructive
    and worthwhile. You might need to build a new system for your organization that
    requires an alternative design, so a solid grasp of design principles will serve
    you well.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 无论最终哪个前端成为标准，我们认为理解构建自己前端的基本设计原则是有益且值得的。您可能需要为您的组织构建一个需要另一种设计的新系统，因此对设计原则的扎实掌握将对您有所帮助。
- en: Defining a Graph of Layers
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义层的图
- en: We mentioned briefly in the previous section that a deep architecture could
    be visualized as a directed graph of `Layer` objects. In this section, we transform
    this intuition into the `TensorGraph` object. These objects are responsible for
    constructing the underlying TensorFlow computation graph.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前一节中简要提到，一个深度架构可以被视为 `Layer` 对象的有向图。在本节中，我们将这种直觉转化为 `TensorGraph` 对象。这些对象负责构建底层的
    TensorFlow 计算图。
- en: A `TensorGraph` object is responsible for maintaining a `tf.Graph`, a `tf.Session`,
    and a list of layers (`self.layers`) internally ([Example 8-9](#ch8-tensorgraph)).
    The directed graph is represented implicitly, by the `in_layers` belonging to
    each `Layer` object. `TensorGraph` also contains utilities for saving this `tf.Graph`
    instance to disk and consequently assigns itself a directory (using `tempfile.mkdtemp()`
    if none is specified) to store checkpoints of the weights associated with its
    underlying TensorFlow graph.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`TensorGraph`对象负责在内部维护一个`tf.Graph`、一个`tf.Session`和一个层列表（`self.layers`）（[示例8-9](#ch8-tensorgraph)）。有向图通过每个`Layer`对象的`in_layers`隐式表示。`TensorGraph`还包含用于将这个`tf.Graph`实例保存到磁盘的实用程序，并因此为自己分配一个目录（如果没有指定，则使用`tempfile.mkdtemp()`）来存储与其底层TensorFlow图相关的权重的检查点。'
- en: Example 8-9\. The TensorGraph contains a graph of layers; TensorGraph objects
    can be thought of as the “model” object holding the deep architecture you want
    to train
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-9。TensorGraph包含一组层的图；TensorGraph对象可以被视为持有您想要训练的深度架构的“模型”对象
- en: '[PRE8]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The private method `_add_layer` does bookkeeping work to add a new `Layer` obect
    to the `TensorGraph` ([Example 8-10](#ch8-add-layer)).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 私有方法`_add_layer`执行一些工作，将一个新的`Layer`对象添加到`TensorGraph`中（[示例8-10](#ch8-add-layer)）。
- en: Example 8-10\. The _add_layer method adds a new Layer object
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-10。_add_layer方法添加一个新的Layer对象
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The layers in a `TensorGraph` must form a directed acyclic graph (there can
    be no loops in the graph). As a result, we can topologically sort these layers.
    Intuitively, a topological sort “orders” the layers in the graph so that each
    `Layer` object’s `in_layers` precede it in the ordered list. This topological
    sort is necessary to make sure all input layers to a given layer are added to
    the graph before the layer itself ([Example 8-11](#ch8-topsort)).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`TensorGraph`中的层必须形成一个有向无环图（图中不能有循环）。因此，我们可以对这些层进行拓扑排序。直观地说，拓扑排序“排序”图中的层，以便每个`Layer`对象的`in_layers`在有序列表中位于它之前。这种拓扑排序是必要的，以确保给定层的所有输入层在该层本身之前添加到图中（[示例8-11](#ch8-topsort)）。'
- en: Example 8-11\. The topsort method orders the layers in the TensorGraph
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-11。topsort方法对TensorGraph中的层进行排序
- en: '[PRE10]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `build()` method takes the responsibility of populating the `tf.Graph` instance
    by calling `layer.create_tensor` for each layer in topological order ([Example 8-12](#ch8-build)).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`build()`方法负责通过按照拓扑顺序调用`layer.create_tensor`来填充`tf.Graph`实例（[示例8-12](#ch8-build)）。'
- en: Example 8-12\. The build method populates the underlying TensorFlow graph
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-12。build方法填充底层的TensorFlow图
- en: '[PRE11]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The method `set_loss()` adds a loss for training to the graph. `add_output()`
    specifies that the layer in question might be fetched from the graph. `set_optimizer()`
    specifies the optimizer used for training ([Example 8-13](#ch8-utilities)).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`set_loss()`方法为训练向图中添加损失。`add_output()`指定所讨论的层可能从图中获取。`set_optimizer()`指定用于训练的优化器（[示例8-13](#ch8-utilities)）。'
- en: Example 8-13\. These methods add necessary losses, outputs, and optimizers to
    the computation graph
  id: totrans-126
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-13。这些方法向计算图中添加必要的损失、输出和优化器
- en: '[PRE12]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The method `get_layer_variables()` is used to fetch the learnable `tf.Variable`
    objects created by a layer. The private method `_get_tf` is used to fetch the
    `tf.Graph` and optimizer instances underpinning the `TensorGraph`. `get_global_step`
    is a convenience method for fetching the current step in the training process
    (starting from 0 at construction). See [Example 8-14](#ch8-get-vars).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_layer_variables()`方法用于获取层创建的可学习`tf.Variable`对象。私有方法`_get_tf`用于获取支持`TensorGraph`的`tf.Graph`和优化器实例。`get_global_step`是一个方便的方法，用于获取训练过程中的当前步骤（从构造时的0开始）。参见[示例8-14](#ch8-get-vars)。'
- en: Example 8-14\. Fetch the learnable variables associated with each layer
  id: totrans-129
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-14。获取与每个层相关的可学习变量
- en: '[PRE13]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Finally, the `restore()` method restores a saved `TensorGraph` from disk ([Example 8-15](#ch8-restore)).
    (As you will see later, the `TensorGraph` is saved automatically during training.)
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`restore()`方法从磁盘恢复保存的`TensorGraph`（[示例8-15](#ch8-restore)）。（正如您将在后面看到的，`TensorGraph`在训练期间会自动保存。）
- en: Example 8-15\. Restore a trained model from disk
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-15。从磁盘恢复训练模型
- en: '[PRE14]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The A3C Algorithm
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: A3C算法
- en: In this section you will learn how to implement A3C, the asynchronous reinforcement
    learning algorithm you saw earlier in the chapter. A3C is a significantly more
    complex training algorithm than those you have seen previously. The algorithm
    requires running gradient descent in multiple threads, interspersed with game
    rollout code, and updating learned weights asynchronously. As a result of this
    extra complexity, we will define the A3C algorithm in an object-oriented fashion.
    Let’s start by defining an `A3C` object.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将学习如何实现A3C，即您在本章前面看到的异步强化学习算法。A3C是一个比您之前看到的训练算法复杂得多的算法。该算法需要在多个线程中运行梯度下降，与游戏回合代码交替进行，并异步更新学习权重。由于这种额外的复杂性，我们将以面向对象的方式定义A3C算法。让我们从定义一个`A3C`对象开始。
- en: The `A3C` class implements the A3C algorithm ([Example 8-16](#ch8-a3c)). A few
    extra bells and whistles are added onto the basic algorithm to encourage learning,
    notably an entropy term and support for generalized advantage estimation. We won’t
    cover all of these details, but encourage you to follow references into the research
    literature (listed in the documentation) to understand more.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`A3C`类实现了A3C算法（[示例8-16](#ch8-a3c)）。在基本算法上添加了一些额外的功能，以鼓励学习，特别是熵项和对广义优势估计的支持。我们不会涵盖所有这些细节，但鼓励您查阅研究文献（在文档中列出）以了解更多。'
- en: Example 8-16\. Define the A3C class encapsulating the asynchronous A3C training
    algorithm
  id: totrans-137
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-16。定义封装异步A3C训练算法的A3C类
- en: '[PRE15]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The heart of the A3C class lies in the `build_graph()` method ([Example 8-17](#ch8-builda3c)),
    which constructs a `TensorGraph` instance (underneath which lies a TensorFlow
    computation graph) encoding the policy learned by the model. Notice how succinct
    this definition is compared with others you have seen previously! There are many
    advantages to using object orientation.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: A3C类的核心在于`build_graph()`方法（示例8-17），它构建了一个`TensorGraph`实例（在其下面是一个TensorFlow计算图），编码了模型学习的策略。请注意，与之前看到的其他定义相比，这个定义是多么简洁！使用面向对象有很多优势。
- en: Example 8-17\. This method builds the computation graph for the A3C algorithm.
    Note that the policy network is defined here using the Layer abstractions you
    saw previously.
  id: totrans-140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-17。这个方法构建了A3C算法的计算图。请注意，策略网络是在这里使用您之前看到的Layer抽象定义的。
- en: '[PRE16]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: There’s a lot of code in this example. Let’s break it down into multiple examples
    and discuss more carefully. [Example 8-18](#ch8-inputa3c) takes the array encoding
    of the `TicTacToeEnvironment` and feeds it into the `Input` instances for the
    graph directly.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例中有很多代码。让我们将其拆分成多个示例，并更仔细地讨论。示例8-18将`TicTacToeEnvironment`的数组编码输入到图的`Input`实例中。
- en: Example 8-18\. This snippet from the build_graph() method feeds in the array
    encoding of TicTacToeEnvironment
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-18。这段代码从build_graph()方法中输入了TicTacToeEnvironment的数组编码。
- en: '[PRE17]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[Example 8-19](#ch8-inputrewarda3c) shows the code used to construct inputs
    for rewards from the environment, advantages observed, and actions taken.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 示例8-19显示了用于从环境中构建奖励、观察到的优势和采取的行动的输入的代码。
- en: Example 8-19\. This snippet from the build_graph() method defines Input objects
    for rewards, advantages, and actions
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-19。这段代码从build_graph()方法中定义了奖励、优势和行动的输入对象
- en: '[PRE18]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The policy network is responsible for learning the policy. In [Example 8-20](#ch8-policya3c),
    the input board state is first flattened into an input feature vector. A series
    of fully connected (or `Dense`) transformations are applied to the flattened board.
    At the very end, a `Softmax` layer is used to predict action probabilities from
    `d5` (note that `out_channels` is set to 9, one for each possible move on the
    tic-tac-toe board).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 策略网络负责学习策略。在示例8-20中，输入的棋盘状态首先被展平为输入特征向量。一系列全连接（或`Dense`）变换被应用于展平的棋盘。最后，使用`Softmax`层来从`d5`预测动作概率（请注意，`out_channels`设置为9，对应井字棋棋盘上的每个可能移动）。
- en: Example 8-20\. This snippet from the build_graph() method defines the policy
    network
  id: totrans-149
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-20。这段代码从build_graph()方法中定义了策略网络
- en: '[PRE19]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Is Feature Engineering Dead?
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程是否已经过时？
- en: In this section, we feed the raw tic-tac-toe game board into TensorFlow for
    training the policy. However, it’s important to note that for more complex games
    than tic-tac-toe, this may not yield satisfactory results. One of the lesser known
    facts about AlphaGo is that DeepMind performs sophisticated feature engineering
    to extract “interesting” patterns of Go pieces upon the board to make AlphaGo’s
    learning easier. (This fact is tucked away into the supplemental information of
    DeepMind’s paper.)
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将原始的井字棋游戏棋盘输入到TensorFlow中，用于训练策略。然而，值得注意的是，对于比井字棋更复杂的游戏，这种方法可能不会产生令人满意的结果。关于AlphaGo的一个较少为人知的事实是，DeepMind进行了复杂的特征工程，以提取棋盘上的“有趣”棋子模式，以便让AlphaGo的学习更容易。（这个事实被藏在DeepMind论文的补充信息中。）
- en: The fact remains that reinforcement learning (and deep learning methods broadly)
    often still need human-guided feature engineering to extract meaningful information
    before learning algorithms can learn effective policies and models. It’s likely
    that as more computational power becomes available through hardware advances,
    this need for feature engineering will be reduced, but for the near term, plan
    on manually extracting information about your systems as needed for performance.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 事实仍然是，强化学习（以及深度学习方法广泛地）通常仍然需要人为引导的特征工程来提取有意义的信息，以便学习算法能够学习有效的策略和模型。随着硬件进步提供更多的计算能力，这种对特征工程的需求可能会减少，但在短期内，计划手动提取关于系统的信息以提高性能。
- en: The A3C Loss Function
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A3C损失函数
- en: 'We now have the object-oriented machinery set in place to define a loss for
    the A3C policy network. This loss function will itself be implemented as a `Layer`
    object (it’s a convenient abstraction that all parts of the deep architecture
    are simply layers). The `A3CLoss` object implements a mathematical loss consisting
    of the sum of three terms: a `policy_loss`, a `value_loss`, and an `entropy` term
    for exploration. See [Example 8-21](#ch8-a3closs).'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经设置好了面向对象的机制，用于为A3C策略网络定义损失。这个损失函数本身将被实现为一个`Layer`对象（这是一个方便的抽象，深度架构的所有部分都只是层）。`A3CLoss`对象实现了一个数学损失，包括三个项的总和：`policy_loss`、`value_loss`和用于探索的`entropy`项。参见示例8-21。
- en: Example 8-21\. This Layer implements the loss function for A3C
  id: totrans-156
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-21。这个层实现了A3C的损失函数
- en: '[PRE20]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: There are a lot of pieces to this definition, so let’s pull out bits of code
    and inspect. The `A3CLoss` layer takes in `reward, action, prob, value, advantage`
    layers as inputs. For mathematical stability, we convert probabilities to log
    probabilities (this is numerically much more stable). See [Example 8-22](#ch8-a3clossinp).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定义有很多部分，让我们提取代码片段并检查。`A3CLoss`层将`reward, action, prob, value, advantage`层作为输入。为了数学稳定性，我们将概率转换为对数概率（这在数值上更稳定）。参见示例8-22。
- en: Example 8-22\. This snippet from A3CLoss takes reward, action, prob, value,
    advantage as input layers and computes a log probability
  id: totrans-159
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-22。这段代码从A3CLoss中获取奖励、动作、概率、价值、优势作为输入层，并计算对数概率
- en: '[PRE21]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The policy loss computes the sum of all advantages observed, weighted by the
    log-probability of the action taken. (Recall that the advantage is the difference
    in reward resulting from taking the given action as opposed to the expected reward
    from the raw policy for that state). The intuition here is that the `policy_loss`
    provides a signal on which actions were fruitful and which were not ([Example 8-23](#ch8-a3cpolicyloss)).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 策略损失计算所有观察到的优势的总和，加权为所采取动作的对数概率。（请记住，优势是采取给定动作与从原始策略获得该状态的预期奖励之间的差异）。这里的直觉是，“policy_loss”提供了哪些动作是有益的信号，哪些是无益的（[示例8-23](#ch8-a3cpolicyloss)）。
- en: Example 8-23\. This snippet from A3CLoss defines the policy loss
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-23。A3CLoss中的此代码段定义了策略损失
- en: '[PRE22]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The value loss computes the difference between our estimate of *V* (`reward`)
    and the actual value of *V* observed (`value`). Note the use of the *L*² loss
    here ([Example 8-24](#ch8-a3cvalueloss)).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 值损失计算我们对*V*（`reward`）的估计值与观察到的*V*的实际值（`value`）之间的差异。请注意这里使用的*L*²损失（[示例8-24](#ch8-a3cvalueloss)）。
- en: Example 8-24\. This snippet from A3CLoss defines the value loss
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-24。A3CLoss中的此代码段定义了值损失
- en: '[PRE23]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The entropy term is an addition that encourages the policy to explore further
    by adding some noise. This term is effectively a form of regularization for A3C
    networks. The final loss computed by `A3CLoss` is a linear combination of these
    component losses. See [Example 8-25](#ch8-a3centropyloss).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 熵项是一个附加项，通过添加一些噪音来鼓励策略进一步探索。这个项实际上是A3C网络的一种正则化形式。由“A3CLoss”计算的最终损失是这些组件损失的线性组合。请参阅[示例8-25](#ch8-a3centropyloss)。
- en: Example 8-25\. This snippet from A3CLoss defines an entropy term added to the
    loss
  id: totrans-168
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-25。A3CLoss中的此代码段定义了添加到损失中的熵项
- en: '[PRE24]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Defining Workers
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义工作者
- en: Thus far, you’ve seen how the policy network is constructed, but you haven’t
    yet seen how the asynchronous training procedure is implemented. Conceptually,
    asynchronous training consists of individual workers running gradient descent
    on locally simulated game rollouts and contributing learned knowledge back to
    a global set of weights periodically. Continuing our object-oriented design, let’s
    introduce the `Worker` class.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经看到了如何构建策略网络，但尚未看到异步训练过程是如何实现的。从概念上讲，异步训练包括各个工作者在本地模拟游戏展开上运行梯度下降，并定期将学到的知识贡献给全局一组权重。继续我们的面向对象设计，让我们介绍“Worker”类。
- en: Each `Worker` instance holds a copy of the model that’s trained asynchronously
    on a separate thread ([Example 8-26](#ch8-worker)). Note that `a3c.build_graph()`
    is used to construct a local copy of the TensorFlow computation graph for the
    thread in question. Take special note of `local_vars` and `global_vars` here.
    We need to make sure to train only the variables associated with this worker’s
    copy of the policy and not with the global copy of the variables (which is used
    to share information across worker threads). As a result `gradients` uses `tf.gradients`
    to take gradients of the loss with respect to only `local_vars`.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 每个“Worker”实例都持有一个模型的副本，该模型在单独的线程上异步训练（[示例8-26](#ch8-worker)）。请注意，“a3c.build_graph（）”用于为相关线程构建TensorFlow计算图的本地副本。在这里特别注意“local_vars”和“global_vars”。我们需要确保仅训练与此Worker策略副本相关的变量，而不是全局变量的副本（用于在线程之间共享信息）。因此，“gradients”使用“tf.gradients”仅对“local_vars”的损失梯度进行计算。
- en: Example 8-26\. The Worker class implements the computation performed by each
    thread
  id: totrans-173
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-26。Worker类实现每个线程执行的计算
- en: '[PRE25]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Worker rollouts
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 工作者展开
- en: Each `Worker` is responsible for simulating game rollouts locally. The `create_rollout()`
    method uses `session.run` to fetch action probabilities from the TensorFlow graph
    ([Example 8-27](#ch8-workerrollout)). It then samples an action from this policy
    using `np.random.choice`, weighted by the per-class probabilities. The reward
    for the action taken is computed from `TicTacToeEnvironment` via a call to `self.env.step(action)`.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 每个“Worker”负责在本地模拟游戏展开。“create_rollout（）”方法使用“session.run”从TensorFlow图中获取动作概率（[示例8-27](#ch8-workerrollout)）。然后使用“np.random.choice”从该策略中加权采样一个动作，权重为每个类别的概率。所采取动作的奖励是通过调用“self.env.step(action)”从“TicTacToeEnvironment”计算的。
- en: Example 8-27\. The create_rollout method simulates a game rollout locally
  id: totrans-177
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-27。create_rollout方法在本地模拟游戏展开
- en: '[PRE26]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The `process_rollouts()` method does preprocessing needed to compute discounted
    rewards, values, actions, and advantages ([Example 8-28](#ch8-processrollout)).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: “process_rollouts（）”方法执行计算折扣奖励、值、动作和优势所需的预处理（[示例8-28](#ch8-processrollout)）。
- en: Example 8-28\. The process_rollout method computes rewards, values, actions,
    and advantages and then takes a gradient descent step against the loss
  id: totrans-180
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-28。process_rollout方法计算奖励、值、动作和优势，然后针对损失采取梯度下降步骤
- en: '[PRE27]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The `Worker.run()` method performs the training step for the `Worker`, relying
    on `process_rollouts()` to issue the actual call to `self.a3c._session.run()`
    under the hood ([Example 8-29](#ch8-processrollout2)).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: “Worker.run（）”方法执行“Worker”的训练步骤，依赖于“process_rollouts（）”在底层发出实际调用“self.a3c._session.run（）”（[示例8-29](#ch8-processrollout2)）。
- en: Example 8-29\. The run() method is the top level invocation for Worker
  id: totrans-183
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-29。run（）方法是Worker的顶层调用
- en: '[PRE28]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Training the Policy
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练策略
- en: The `A3C.fit()` method brings together all the disparate pieces introduced to
    train the model. The `fit()` method takes the responsibility for spawning `Worker`
    threads using the Python `threading` library. Since each `Worker` takes responsibility
    for training itself, the `fit()` method simply is responsible for periodically
    checkpointing the trained model to disk. See [Example 8-30](#ch8-a3cfit).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: “A3C.fit（）”方法将引入的所有不同部分汇集在一起，以训练模型。该“fit（）”方法负责使用Python“threading”库定期生成“Worker”线程。由于每个“Worker”负责自己的训练，因此“fit（）”方法只负责定期将训练好的模型保存到磁盘。请参阅[示例8-30](#ch8-a3cfit)。
- en: Example 8-30\. The fit() method brings everything together and runs the A3C
    training algorithm
  id: totrans-187
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-30。fit（）方法将所有内容汇集在一起，并运行A3C训练算法
- en: '[PRE29]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Challenge for the Reader
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读者的挑战
- en: We strongly encourage you to try training tic-tac-toe models for yourself! Note
    that this example is more involved than other examples in the book, and will require
    greater computational power. We recommend a machine with at least a few CPU cores.
    This requirement isn’t too onerous; a good laptop should suffice. Try using a
    tool like `htop` to check that the code is indeed multithreaded. See how good
    a model you can train! You should be able to beat the random baseline most of
    the time, but this basic implementation won’t give you a model that always wins.
    We recommend exploring the RL literature and expanding upon the base implementation
    to see how well you can do.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们强烈建议您尝试为自己训练井字游戏模型！请注意，这个例子比书中的其他例子更复杂，需要更大的计算能力。我们建议使用至少几个CPU核心的机器。这个要求并不过分；一台好的笔记本电脑应该足够。尝试使用类似`htop`这样的工具来检查代码是否确实是多线程的。看看您能训练出多好的模型！您应该能大部分时间击败随机基线，但这个基本实现不会给您一个总是赢的模型。我们建议探索强化学习文献，并在基础实现的基础上进行扩展，看看您能做得多好。
- en: Review
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回顾
- en: In this chapter, we introduced you to the core concepts of reinforcement learning
    (RL). We walked you through some recent successes of RL methods on ATARI, upside-down
    helicopter flight, and computer Go. We then taught you about the mathematical
    framework of Markov decision processes. We brought it together with a detailed
    case study walking you through the construction of a tic-tac-toe agent. This algorithm
    uses a sophisticated training method, A3C, that makes use of multiple CPU cores
    to speed up training. In [Chapter 9](ch09.html#training_large_deep_networks),
    you’ll learn more about training models with multiple GPUs.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们向您介绍了强化学习（RL）的核心概念。我们带您了解了RL方法在ATARI、倒立直升机飞行和计算机围棋上取得的一些最近的成功。然后，我们教授了您马尔可夫决策过程的数学框架。我们通过一个详细的案例研究，带您了解了如何构建一个井字游戏代理。这个算法使用了一种复杂的训练方法A3C，利用多个CPU核心加快训练速度。在[第9章](ch09.html#training_large_deep_networks)中，您将学习更多关于使用多个GPU训练模型的知识。
