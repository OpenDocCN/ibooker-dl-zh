- en: Chapter 8\. Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The learning techniques we’ve covered so far in this book fall into the categories
    of supervised or unsupervised learning. In both cases, solving a given problem
    requires a data scientist to design a deep architecture that handles and processes
    input data and to connect the output of the architecture to a loss function suitable
    for the problem at hand. This framework is widely applicable, but not all applications
    fall neatly into this style of thinking. Let’s consider the challenge of training
    a machine learning model to win a game of chess. It seems reasonable to process
    the board as spatial input using a convolutional network, but what would the loss
    entail? None of our standard loss functions such as cross-entropy or *L*² loss
    quite seem to apply.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning provides a mathematical framework well suited to solving
    games. The central mathematical concept is that of the *Markov decision process*,
    a tool for modeling AI agents that interact with *environments* that offer *rewards*
    upon completion of certain *actions*. This framework proves to be flexible and
    general, and has found a number of applications in recent years. It’s worth noting
    that reinforcement learning as a field is quite mature and has existed in recognizable
    form since the 1970s. However, until recently, most reinforcement learning systems
    were only capable of solving toy problems. Recent work has revealed that these
    limitations were likely due to the lack of sophisticated data intake mechanisms;
    hand-engineered features for many games or robotic environments often did not
    suffice. Deep representation extractions trained end-to-end on modern hardware
    seem to break through the barriers of earlier reinforcement learning systems and
    have achieved notable results in recent years.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Arguably, the first breakthrough in deep reinforcement learning was on ATARI
    arcade games. ATARI arcade games were traditionally played in video game arcades
    and offered users simple games that don’t typically require sophisticated strategizing
    but might require good reflexes. [Figure 8-1](#ch8-breakout) shows a screenshot
    from the popular ATARI game Breakout. In recent years, due to the development
    of good ATARI emulation software, ATARI games have become a testbed for gameplay
    algorithms. At first, reinforcement learning algorithms applied to ATARI didn’t
    achieve superb results; the requirement that the algorithm understand a visual
    game state frustrated most attempts. However, as convolutional networks matured,
    researchers at DeepMind realized that convolutional networks could be combined
    with existing reinforcement learning techniques and trained end-to-end.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '![breakout.jpg](assets/tfdl_0801.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1\. A screenshot of the ATARI arcade game Breakout. Players have to
    use the paddle at the bottom of the screen to bounce a ball that breaks the tiles
    at the top of the screen.
  id: totrans-5
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The resulting system achieved superb results, and learned to play many ATARI
    games (especially those dependent on quick reflexes) at superhuman standards.
    [Figure 8-2](#ch8-breakoutresults) lists ATARI scores achieved by DeepMind’s DQN
    algorithm. This breakthrough result spurred tremendous growth in the field of
    deep reinforcement learning and inspired legions of researchers to explore the
    potential of deep reinforcement learning techniques. At the same time, DeepMind’s
    ATARI results showed reinforcement learning techniques were capable of solving
    systems dependent on short-term movements. These results didn’t demonstrate that
    deep reinforcement learning systems were capable of solving games that required
    greater strategic planning.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '![atari_scores.jpg](assets/tfdl_0802.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. Results of DeepMind’s DQN reinforcement learning algorithm on various
    ATARI games. 100% is the score of a strong human player. Note that DQN achieves
    superhuman performance on many games, but is quite weak on others.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Computer Go
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 1994, IBM revealed the system Deep Blue, which later succeeded in defeating
    Garry Kasparov in a highly publicized chess match. This system relied on brute
    force computation to expand the tree of possible chess moves (with some help from
    handcrafted chess heuristics) to play master-level chess.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Computer scientists attempted to apply similar techniques to other games such
    as Go. Unfortunately for early experimenters, Go’s 19 × 19 game board is significantly
    larger than chess’s 8 × 8 board. As a result, trees of possible moves explode
    much more quickly than for chess, and simple back-of-the-envelope calculations
    indicated that Moore’s law would take a very long time to enable brute force solution
    of Go in the style of Deep Blue. Complicating matters, there existed no simple
    heuristic for evaluating who’s winning in a half-played Go game (determining whether
    black or white is ahead is a notoriously noisy art for the best human analysts).
    As a result, until very recently, many prominent computer scientists believed
    that strong computer Go play was a decade away at the least.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate the prowess of its reinforcement learning algorithms, DeepMind
    took on the challenge of learning to play Go, a game that requires complex strategic
    planning. In a tour-de-force paper, DeepMind revealed its deep reinforcement learning
    engine, AlphaGo, which combined convolutional networks with tree-based search
    to defeat the human Go master Lee Sedol ([Figure 8-3](#ch8-leesedol)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![lee_sedol.jpg](assets/tfdl_0803.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: Figure 8-3\. Human Go champion Lee Sedol battles AlphaGo. Lee Sedol eventually
    lost the match 1–4, but succeeded in winning one game. It’s unlikely that this
    victory can be replicated against the vastly improved successors of AlphaGo such
    as AlphaZero.
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: AlphaGo convincingly demonstrated that deep reinforcement learning techniques
    were capable of learning to solve complex strategic games. The heart of the breakthrough
    was the realization that convolutional networks could learn to estimate whether
    black or white was ahead in a half-played game, which enabled game trees to be
    truncated at reasonable depths. (AlphaGo also estimates which moves are most fruitful,
    enabling a second pruning of the game tree space.) AlphaGo’s victory really launched
    deep reinforcement learning into prominence, and a host of researchers are working
    to transform AlphaGo-style systems into practical use.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we discuss reinforcement learning algorithms and specifically
    deep reinforcement learning architectures. We then show readers how to successfully
    apply reinforcement learning to the game of tic-tac-toe. Despite the simplicity
    of the game, training a successful reinforcement learner for tic-tac-toe requires
    significant sophistication, as you will soon see.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: The code for this chapter was adapted from the DeepChem reinforcement learning
    library, and in particular from example code created by Peter Eastman and Karl
    Leswing. Thanks to Peter for debugging and tuning help on this chapter’s example
    code.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Markov Decision Processes
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before launching into a discussion of reinforcement learning algorithms, it
    will be useful to pin down the family of problems that reinforcement learning
    methods seek to solve. The mathematical framework of Markov decision processes
    (MDPs) is very useful for formulating reinforcement learning methods. Traditionally,
    MDPs are introduced with a battery of Greek symbols, but we will instead try to
    proceed by providing some basic intuition.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: The heart of MDPs is the pair of an *environment* and an *agent*. An environment
    encodes a “world” in which the agent seeks to act. Example environments could
    include game worlds. For example, a Go board with master Lee Sedol sitting opposite
    is a valid environment. Another potential environment could be the environment
    surrounding a small robot helicopter. In a prominent early reinforcement learning
    success, a team at Stanford led by Andrew Ng trained a helicopter to fly upside
    down using reinforcement learning as shown in [Figure 8-4](#ch8-upside).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '![upside_down_helicopter.jpg](assets/tfdl_0804.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
- en: Figure 8-4\. Andrew Ng’s team at Stanford, from 2004 to 2010, trained a helicopter
    to learn to fly upside down using reinforcement learning. This work required the
    construction of a sophisticated physically accurate simulator.
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The agent is the learning entity that acts within the environment. In our first
    example, AlphaGo itself is the agent. In the second, the robot helicopter (or
    more accurately, the control algorithm in the robot helicopter) is the agent.
    Each agent has a set of actions that it can take within the environment. For AlphaGo,
    these constitute valid Go moves. For the robot helicopter, these include control
    of the main and secondary rotors.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Actions the agent takes are presumed to have an effect on the environment. In
    the case of AlphaGo, this effect is deterministic (AlphaGo deciding to place a
    Go stone results in the stone being placed). In the case of the helicopter, the
    effect is likely probabilistic (changes in helicopter position may depend on wind
    conditions, which can’t be modeled effectively).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: The final piece of the model is the notion of reward. Unlike supervised learning
    where explicit labels are present to learn from, or unsupervised learning where
    the challenge is to learn the underlying structure of the data, reinforcement
    learning operates in a setting of partial, sparse rewards. In Go, rewards are
    achieved at the end of the game upon victory or defeat, while in helicopter flight,
    rewards might be presented for successful flights or completion of trick moves.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Reward Function Engineering Is Hard
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the largest challenges in reinforcement learning is designing rewards
    that induce agents to learn desired behaviors. For even simple win/loss games
    such as Go or tic-tac-toe, this can be surprisingly difficult. How much should
    a loss be punished and how much should a win be rewarded? There don’t yet exist
    good answers.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: For more complex behaviors, this can be extremely challenging. A number of studies
    have demonstrated that simple rewards can result in agents learning unexpected
    and even potentially damaging behaviors. These systems spur fears of future agents
    with greater autonomy wreaking havoc when unleashed in the real world after having
    been trained to optimize bad reward functions.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: In general, reinforcement learning is less mature than supervised learning techniques,
    and we caution that decisions to deploy reinforcement learning in production systems
    should be taken very carefully. Given uncertainty over learned behavior, make
    sure to thoroughly test any deployed reinforcement learned system.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement Learning Algorithms
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we’ve introduced you to the core mathematical structures underlying
    reinforcement learning, let’s consider how to design algorithms that learn intelligent
    behaviors for reinforcement learning agents. At a high level, reinforcement learning
    algorithms can be separated into the buckets of *model-based* and *model-free*
    algorithms. The central difference is whether the algorithm seeks to learn an
    internal model of how its environment acts. For simpler environments, such as
    tic-tac-toe, the model dynamics are trivial. For more complex environments, such
    as helicopter flight or even ATARI games, the underlying environment is likely
    extraordinarily complex. Avoiding the construction of an explicit model of the
    environment in favor of an implicit model that advises the agent on how to act
    may well be more pragmatic.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Simulations and Reinforcement Learning
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Any reinforcement learning algorithm requires iteratively improving the performance
    of the current agent by evaluating the agent’s current behavior and changing it
    to improve received rewards. These updates to the agent structure often include
    some gradient descent update, as we will see in the following sections. However,
    as you know intimately from previous chapters, gradient descent is a slow training
    algorithm! Millions or even billions of gradient descent steps may be required
    to learn an effective model.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: This poses a problem if the learning environment is in the real world; how can
    an agent interact millions of times with the real world? In most cases it can’t.
    As a result, most sophisticated reinforcement learning systems depend critically
    on simulators that allow interaction with a simulation computational version of
    the environment. For the helicopter flight environment, one of the hardest challenges
    researchers faced was building an accurate helicopter physics simulator that allowed
    learning of effective flight policies computationally.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Q-Learning
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the framework of Markov decision processes, agents take actions in an environment
    and obtain rewards that are (presumably) tied to agent actions. The *Q* function
    predicts the expected reward for taking a given action in a particular environment
    state. This concept seems very straightforward, but the trickiness arises when
    this expected reward includes discounted rewards from future actions.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Discounting Rewards
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The notion of a discounted reward is widespread, and is often introduced in
    the context of finances. Suppose a friend says he’ll pay you $10 next week. That
    future 10 dollars is worth less to you than 10 dollars in your hand right now
    (what if the payment never happens, for one?). So mathematically, it’s common
    practice to introduce a discounting factor γ (typically between 0 and 1) that
    lowers the “present-value” of future payments. For example, say your friend is
    somewhat untrustworthy. You might decide to set γ = 0.5 and value your friend’s
    promise as worth 10γ = 5 dollars today to account for uncertainty in rewards.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: However, these future rewards depend on actions taken by the agent in the future.
    As a result, the *Q* function must be formulated recursively in terms of itself,
    since expected rewards for one state depend on those for another state. This recursive
    definition makes learning the *Q* function tricky. This recursive relationship
    can be formulated explicitly for simple environments with discrete state spaces
    and solved with dynamic programming methods. For more general environments, *Q*-learning
    methods were not very useful until recently.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Recently, Deep *Q*-networks (DQN) were introduced by DeepMind and used to solve
    ATARI games as mentioned earlier. The key insight underlying DQN is once again
    the universal approximation theorem; since *Q* may be arbitrarily complex, we
    should model it with a universal approximator such as a deep network. While using
    neural networks to model *Q* had been done before, DeepMind also introduced the
    notion of experience replay for these networks, which let them train DQN models
    effectively at scale. Experience replay stores observed game outcomes and transitions
    from past games, and resamples them while training (in addition to training on
    new games) to ensure that lessons from the past are not forgotten by the network.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Catastrophic Forgetting
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural networks quickly forget the past. In fact, this phenomenon, termed *catastrophic
    forgetting*, can occur very rapidly; a few minibatch updates can be sufficient
    for the network to forget a complex behavior it previously knew. As a result,
    without techniques like experience replay that ensure the network always trains
    on episodes from past matches, it wouldn’t be possible to learn complex behaviors.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Designing a training algorithm for deep networks that doesn’t suffer from catastrophic
    forgetting is still a major open problem today. Humans notably don’t suffer from
    catastrophic forgetting; even if you haven’t ridden a bike in years, it’s likely
    you still remember how to do so. Creating a neural network that has similar resilience
    might involve the addition of long-term external memory, along the lines of the
    Neural Turing machine. Unfortunately, none of the attempts thus far at designing
    resilient architectures has really worked well.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Policy Learning
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, you learned about *Q*-learning, which seeks to understand
    the expected rewards for taking given actions in given environment states. Policy
    learning is an alternative mathematical framework for learning agent behavior.
    It introduces the policy function π that assigns a probability to each action
    that an agent can take in a given state.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Note that a policy is sufficient for defining agent behavior entirely. Given
    a policy, an agent can act just by sampling a suitable action for the current
    environment state. Policy learning is convenient, since policies can be learned
    directly through an algorithm called policy gradient. This algorithm uses a couple
    mathematical tricks to enable policy gradients to be computed directly via backpropagation
    for deep networks. The key concept is the *rollout*. Let an agent act in an environment
    according to its current policy and observe all rewards that come in. Then backpropagate
    to increase the likelihood of those actions that led to more fruitful rewards.
    This description is accurate at a high level, but we will see more implementation
    details later in the chapter.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: A policy is often associated with a *value function* *V*. This function returns
    the expected discounted reward for following policy π starting from the current
    state of the environment. *V* and *Q* are closely related functions since both
    provide estimates of future rewards starting from present state, but *V* does
    not specify an action to be taken and assumes rather that actions are sampled
    from π.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 'Another commonly defined function is the *advantage* *A*. This function defines
    the difference in expected reward due to taking a particular action *a* in a given
    environment state *s*, as opposed to following the base policy π. Mathematically,
    *A* is defined in terms of *Q* and *V*:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>A</mi> <mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi>
    <mo>)</mo> <mo>=</mo> <mi>Q</mi> <mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo>
    <mo>-</mo> <mi>V</mi> <mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></math>
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: The advantage is useful in policy-learning algorithms, since it lets an algorithm
    quantify how a particular action may have been better suited than the present
    recommendation of the policy.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Policy Gradient Outside Reinforcement Learning
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略梯度在强化学习之外
- en: Although we have introduced policy gradient as a reinforcement learning algorithm,
    it can equally be viewed as a tool for learning deep networks with nondifferentiable
    submodules. What does this mean when we unpack the mathematical jargon?
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经将策略梯度介绍为一种强化学习算法，但它同样可以被视为一种学习具有不可微子模块的深度网络的工具。当我们解开数学术语时，这意味着什么？
- en: Let’s suppose we have a deep network that calls an external program within the
    network itself. This external program is a black box; it could be a network call
    or an invocation of a 1970s COBOL routine. How can we learn the rest of the deep
    network when this module has no gradient?
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个深度网络，在网络内部调用一个外部程序。这个外部程序是一个黑匣子；它可能是一个网络调用，也可能是对1970年代COBOL例程的调用。当这个模块没有梯度时，我们如何学习深度网络的其余部分？
- en: It turns out that policy gradient can be repurposed to estimate an “effective”
    gradient for the system. The simple intuition is that multiple “rollouts” can
    be run, which are used to estimate gradients. Expect to see research over the
    next few years extending this idea to create large networks with nondifferential
    modules.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，策略梯度可以被重新用于估计系统的“有效”梯度。简单的直觉是可以运行多个“rollouts”，用于估计梯度。预计在未来几年会看到研究将这个想法扩展到创建具有不可微分模块的大型网络。
- en: Asynchronous Training
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异步训练
- en: A disadvantage of the policy gradient methods presented in the previous section
    is that performing the rollout operations requires evaluating agent behavior in
    some (likely simulated) environment. Most simulators are complicated pieces of
    software that can’t be run on the GPU. As a result, taking a single learning step
    will require running long CPU-bound calculations. This can lead to unreasonably
    slow training.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节介绍的策略梯度方法的一个缺点是，执行rollout操作需要评估代理在某个（可能是模拟的）环境中的行为。大多数模拟器都是复杂的软件，无法在GPU上运行。因此，进行单个学习步骤将需要运行长时间的CPU绑定计算。这可能导致训练速度过慢。
- en: Asynchronous reinforcement learning methods seek to speed up this process by
    using multiple asynchronous CPU threads to perform rollouts independently. These
    worker threads will perform rollouts, estimate gradient updates to the policy
    locally, and then periodically synchronize with the global set of parameters.
    Empirically, asynchronous training appears to significantly speed up reinforcement
    learning and allows for fairly sophisticated policies to be learned on laptops.
    (Without GPUs! The majority of computational power is used on rollouts, so gradient
    update steps are often not the rate limiting aspect of reinforcement learning
    training.) The most popular algorithm for asynchronous reinforcement learning
    currently is the asynchronous actor advantage critic (A3C) algorithm.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 异步强化学习方法通过使用多个异步CPU线程独立执行rollouts来加速这个过程。这些工作线程将执行rollouts，本地估计策略的梯度更新，然后定期与全局参数集进行同步。从经验上看，异步训练似乎显著加速了强化学习，并允许在笔记本电脑上学习相当复杂的策略。（没有GPU！大部分计算能力用于rollouts，因此梯度更新步骤通常不是强化学习训练的速度限制因素。）目前最流行的异步强化学习算法是异步演员优势评论家（A3C）算法。
- en: CPU or GPU?
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CPU还是GPU？
- en: GPUs are necessary for most large deep learning applications, but reinforcement
    learning currently appears to be an exception to this general rule. The reliance
    of reinforcement learning algorithms to perform many rollouts seems to currently
    bias reinforcement learning implementations toward multicore CPU systems. It’s
    likely that in specific applications, individual simulators can be ported to work
    more quickly on GPUs, but CPU-based simulations will likely continue to dominate
    for the near future.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数大型深度学习应用都需要GPU，但目前强化学习似乎是一个例外。强化学习算法依赖于执行许多rollouts，目前似乎偏向于多核CPU系统。在特定应用中，个别模拟器可能会被移植到GPU上更快地运行，但基于CPU的模拟器可能会在不久的将来继续占主导地位。
- en: Limits of Reinforcement Learning
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习的局限性
- en: The framework of Markov decision processes is immensely general. For example,
    behavioral scientists routinely use Markov decision processes to understand and
    model human decision making. The mathematical generality of this framework has
    spurred scientists to posit that solving reinforcement learning might spur the
    creation of artificial general intelligences (AGIs). The stunning success of AlphaGo
    against Lee Sedol amplified this belief, and indeed research groups such as OpenAI
    and DeepMind aiming to build AGIs focus much of their efforts on developing new
    reinforcement learning techniques.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程的框架是非常普遍的。例如，行为科学家通常使用马尔可夫决策过程来理解和建模人类决策过程。这个框架的数学普遍性促使科学家们提出，解决强化学习可能会促使人工通用智能（AGI）的创造。AlphaGo对李世石的惊人成功加强了这种信念，事实上，像OpenAI和DeepMind这样的研究团队致力于构建AGI，他们大部分的努力都集中在开发新的强化学习技术上。
- en: Nonetheless, there are major weaknesses to reinforcement learning as it currently
    exists. Careful benchmarking work has shown that reinforcement learning techniques
    are very susceptible to choice of hyperparameters (even by the standards of deep
    learning, which is already much finickier than other techniques like random forests).
    As we have mentioned, reward function engineering is very immature. Humans are
    capable of internally designing their own reward functions or effectively learning
    to copy reward functions from observation. Although “inverse reinforcement learning”
    algorithms that learn reward functions directly have been proposed, these algorithms
    have many limitations in practice.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，强化学习目前存在重大弱点。仔细的基准测试工作表明，强化学习技术对超参数的选择非常敏感（即使按照深度学习的标准，它已经比其他技术如随机森林更加挑剔）。正如我们所提到的，奖励函数工程非常不成熟。人类能够内部设计自己的奖励函数，或者有效地学习从观察中复制奖励函数。虽然已经提出了直接学习奖励函数的“逆强化学习”算法，但这些算法在实践中存在许多限制。
- en: In addition to these fundamental limitations, there are still many practical
    scaling issues. Humans are capable of playing games that combine high-level strategizing
    with thousands of “micro” moves. For example, master-level play of the strategy
    game StarCraft (see [Figure 8-5](#ch8-starcraft)) requires sophisticated strategic
    ploys combined with careful control of hundreds of units. Games can require thousands
    of local moves to be played to completion. In addition, unlike Go or chess, StarCraft
    has a “fog of war” where players cannot see the entire game state. This combination
    of large game state and uncertainty has foiled reinforcement learning attempts
    on StarCraft. As a result, teams of AI researchers at DeepMind and other groups
    are focusing serious effort on solving StarCraft with deep reinforcement learning
    methods. Despite some serious effort, though, the best StarCraft bots remain at
    amateur level.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '![deep_starcraft.png](assets/tfdl_0805.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
- en: Figure 8-5\. A collection of subtasks required for playing the real-time strategy
    game StarCraft. In this game, players must build an army that they can use to
    defeat the opposing force. Successful StarCraft play requires mastery of resource
    planning, exploration, and complex strategy. The best computer StarCraft agents
    remain at amateur level.
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In general, there’s wide consensus that reinforcement learning is a useful technique
    that’s likely to be deeply influential over the next few decades, but it’s also
    clear that the many practical limitations of reinforcement learning methods will
    mean that most work will continue to be done in research labs for the near future.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Playing Tic-Tac-Toe
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tic-tac-toe is a simple two-player game. Players place Xs and Os on a 3 × 3
    game board until one player succeeds in placing three of her pieces in a row.
    The first player to do so wins. If neither player succeeds in obtaining three
    in a row before the board is filled up, the game ends in a draw. [Figure 8-6](#ch8-tictactoe)
    illustrates a tic-tac-toe game board.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '![Tic_tac_toe.png](assets/tfdl_0806.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
- en: Figure 8-6\. A tic-tac-toe game board.
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tic-tac-toe is a nice testbed for reinforcement learning techniques. The game
    is simple enough that exorbitant amounts of computational power aren’t required
    to train effective agents. At the same time, despite tic-tac-toe’s simplicity,
    learning an effective agent requires considerable sophistication. The TensorFlow
    code for this section is arguably the most sophisticated example found in this
    book. We will walk you through the design of a TensorFlow tic-tac-toe asynchronous
    reinforcement learning agent in the remainder of this section.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Object Orientation
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The code we’ve introduced thus far in this book has primarily consisted of scripts
    augmented by smaller helper functions. In this chapter, however, we will swap
    to an object-oriented programming style. This style of programming might be new
    to you, especially if you hail from the scientific world rather than from the
    tech world. Briefly, an object-oriented program defines *objects* that model aspects
    of the world. For example, you might want to define `Environment` or `Agent` or
    `Reward` objects that directly correspond to these mathematical concepts. A *class*
    is a template for objects that can be used to *instantiate* (or create) many new
    objects. For example, you will shortly see an `Environment` class definition we
    will use to define many particular `Environment` objects.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Object orientation is particularly powerful for building complex systems, so
    we will use it to simplify the design of our reinforcement learning system. In
    practice, your real-world deep learning (or reinforcement learning) systems will
    likely need to be object oriented as well, so we encourage taking some time to
    master object-oriented design. There are many superb books that cover the fundamentals
    of object-oriented design, and we recommend that you check them out as necessary.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Abstract Environment
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start by defining an abstract `Environment` object that encodes the state
    of a system in a list of NumPy objects ([Example 8-1](#ch8-envclass)). This `Environment`
    object is quite general (adapted from DeepChem’s reinforcement learning engine)
    so it can easily serve as a template for other reinforcement learning projects
    you might seek to implement.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从定义一个抽象的`Environment`对象开始，它将系统的状态编码为NumPy对象的列表（[示例8-1](#ch8-envclass)）。这个`Environment`对象非常通用（改编自DeepChem的强化学习引擎），因此它可以很容易地作为您可能想要实现的其他强化学习项目的模板。
- en: Example 8-1\. This class defines a template for constructing new environments
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-1。这个类定义了构建新环境的模板
- en: '[PRE0]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Tic-Tac-Toe Environment
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 井字棋环境
- en: We need to specialize the `Environment` class to create a `TicTacToeEnvironment`
    suitable for our needs. To do this, we construct a *subclass* of `Environment`
    that adds on more features, while retaining the core functionality of the original
    *superclass*. In [Example 8-2](#ch8-tictacenvclass), we define `TicTacToeEnvironment`
    as a subclass of `Environment` that adds details specific to tic-tac-toe.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要专门化`Environment`类以创建一个适合我们需求的`TicTacToeEnvironment`。为此，我们构建一个`Environment`的*子类*，添加更多功能，同时保留原始*超类*的核心功能。在[示例8-2](#ch8-tictacenvclass)中，我们将`TicTacToeEnvironment`定义为`Environment`的子类，添加了特定于井字棋的细节。
- en: Example 8-2\. The TicTacToeEnvironment class defines a template for constructing
    new tic-tac-toe environments
  id: totrans-81
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-2。TicTacToeEnvironment类定义了构建新井字棋环境的模板
- en: '[PRE1]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The first interesting tidbit to note here is that we define the board state
    as a NumPy array of shape `(3, 3, 2)`. We use a one-hot encoding of `X` and `O`
    (one-hot encodings aren’t only useful for natural language processing!).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这里要注意的第一个有趣的细节是，我们将棋盘状态定义为形状为`(3, 3, 2)`的NumPy数组。我们使用了`X`和`O`的独热编码（独热编码不仅在自然语言处理中有用！）。
- en: The second important thing to note is that the environment explicitly defines
    the reward function by setting penalties for illegal moves and losses, and rewards
    for draws and wins. This snippet powerfully illustrates the arbitrary nature of
    reward function engineering. Why these particular numbers?
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个需要注意的重要事情是，环境通过设置违规移动和失败的惩罚以及平局和胜利的奖励来明确定义奖励函数。这段代码强有力地说明了奖励函数工程的任意性。为什么是这些特定的数字？
- en: Empirically, these choices appear to result in stable behavior, but we encourage
    you to experiment with alternate reward settings to observe results. In this implementation,
    we specify that the agent always plays `X`, but randomize whether `X` or `O` goes
    first. The function `get_O_move()` simply places an `O` on a random open tile
    on the game board. `TicTacToeEnvironment` encodes an opponent that plays `O` while
    always selecting a random move. The `reset()` function simply clears the board,
    and places an `O` tile randomly if `O` is going first during this game. See [Example 8-3](#ch8-tictacenvclass2).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 从经验上看，这些选择似乎导致了稳定的行为，但我们鼓励您尝试使用替代奖励设置来观察结果。在这个实现中，我们指定代理始终扮演`X`，但随机确定是`X`还是`O`先走。函数`get_O_move()`简单地在游戏棋盘上的随机空格上放置一个`O`。`TicTacToeEnvironment`编码了一个扮演`O`的对手，总是选择一个随机移动。`reset()`函数简单地清空棋盘，并在这个游戏中`O`先走时随机放置一个`O`。参见[示例8-3](#ch8-tictacenvclass2)。
- en: Example 8-3\. More methods from the TicTacToeEnvironment class
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-3。TicTacToeEnvironment类的更多方法
- en: '[PRE2]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The utility function `game_over()` reports that the game has ended if all tiles
    are filled. `check_winner()` checks whether the specified player has achieved
    three in a row and won the game ([Example 8-4](#ch8-tictacenvclass3)).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 实用函数`game_over()`报告游戏是否结束，如果所有方块都填满。`check_winner()`检查指定的玩家是否取得了三连胜并赢得了比赛（[示例8-4](#ch8-tictacenvclass3)）。
- en: Example 8-4\. Utility methods from the TicTacToeEnvironment class for detecting
    when the game has ended and who won
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-4。TicTacToeEnvironment类的用于检测游戏是否结束以及谁赢得比赛的实用方法
- en: '[PRE3]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In our implementation, an action is simply a number between 0 and 8 specifying
    the tile on which the `X` tile is placed. The `step()` method checks whether this
    tile is occupied (returning a penalty if so), then places the tile. If `X` has
    won, a reward is returned. Else, the random `O` opponent is allowed to make a
    move. If `O` won, then a penalty is returned. If the game has ended as a draw,
    then a penalty is returned. Else, the game continues with a `NOT_LOSS` reward.
    See [Example 8-5](#ch8-tictacenvclass4).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实现中，一个动作只是一个介于0和8之间的数字，指定`X`方块放置的位置。`step()`方法检查这个位置是否被占用（如果是，则返回惩罚），然后放置方块。如果`X`赢了，就会返回奖励。否则，允许随机的`O`对手进行移动。如果`O`赢了，就会返回惩罚。如果游戏以平局结束，就会返回惩罚。否则，游戏将继续进行，获得一个`NOT_LOSS`奖励。参见[示例8-5](#ch8-tictacenvclass4)。
- en: Example 8-5\. This method performs a step of the simulation
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例8-5。这个方法执行模拟的一步
- en: '[PRE4]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The Layer Abstraction
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层次抽象
- en: Running an asynchronous reinforcement learning algorithm such as A3C requires
    that each thread have access to a separate copy of the policy model. These copies
    of the model have to be periodically re-synced with one another for training to
    proceed. What is the easiest way we can construct multiple copies of the TensorFlow
    graph that we can distribute to each thread?
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 运行异步强化学习算法，如A3C，需要每个线程都能访问一个策略模型的单独副本。这些模型的副本必须定期与彼此重新同步，以便进行训练。我们可以构建多个TensorFlow图的最简单方法是什么？
- en: One simple possibility is to create a function that creates a copy of the model
    in a separate TensorFlow graph. This approach works well, but gets to be a little
    messy, especially for sophisticated networks. Using a little bit of object orientation
    can significantly simplify this process. Since our reinforcement learning code
    is adapted from the DeepChem library, we use a simplified version of the TensorGraph
    framework from DeepChem (see [*https://deepchem.io*](https://deepchem.io) for
    information and docs). This framework is similar to other high-level TensorFlow
    frameworks such as Keras. The core abstraction in all such models is the introduction
    of a `Layer` object that encapsulates a portion of a deep network.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: A `Layer` is a portion of a TensorFlow graph that accepts a list `in_layers`
    of input layers. In this abstraction, a deep architecture consists of a *directed
    graph* of layers. Directed graphs are similar to the undirected graphs you saw
    in [Chapter 6](ch06.html#convolutional_neural_networks), but have directions on
    their edges. In this case, the `in_layers` have edges to the new `Layer`, with
    the direction pointing toward the new layer. You will learn more about this concept
    in the next section.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: We use `tf.register_tensor_conversion_function`, a utility that allows arbitrary
    classes to register themselves as tensor convertible. This registration will mean
    that a `Layer` can be converted into a TensorFlow tensor via a call to `tf.convert_to_tensor`.
    The `_get_input_tensors()` private method is a utility that uses `tf.convert_to_tensor`
    to transform input layers into input tensors. Each `Layer` is responsible for
    implementing a `create_tensor()` method that specifies the operations to add to
    the TensorFlow computational graph. See [Example 8-6](#ch8-layer).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-6\. The Layer object is the fundamental abstraction in object-oriented
    deep architectures. It encapsulates a part of the netwok such as a fully connected
    layer or a convolutional layer. This example defines a generic superclass for
    all such layers.
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The preceding description is abstract, but in practice easy to use. [Example 8-7](#ch8-squeeze)
    shows a `Squeeze` layer that wraps `tf.squeeze` with a `Layer` (you will find
    this class convenient later). Note that `Squeeze` is a subclass of `Layer`.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-7\. The Squeeze layer squeezes its input
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `Input` layer wraps placeholders for convenience ([Example 8-8](#ch8-input)).
    Note that the `Layer.create_tensor` method must be invoked for each layer we use
    in order to construct a TensorFlow computational graph.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-8\. The Input layer adds placeholders to the computation graph
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: tf.keras and tf.estimator
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow has now integrated the popular Keras object-oriented frontend into
    the core TensorFlow library. Keras includes a `Layer` class definition that closely
    matches the `Layer` objects you’ve just learned about in this section. In fact,
    the `Layer` objects here were adapted from the DeepChem library, which in turn
    adapted them from an earlier version of Keras.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth noting, though, that `tf.keras` has not yet become the standard higher-level
    interface to TensorFlow. The `tf.estimator` module provides an alternative (albeit
    less rich) high-level interface to raw TensorFlow.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of which frontend eventually becomes standard, we think that understanding
    the fundamental design principles for building your own frontend is instructive
    and worthwhile. You might need to build a new system for your organization that
    requires an alternative design, so a solid grasp of design principles will serve
    you well.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Defining a Graph of Layers
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We mentioned briefly in the previous section that a deep architecture could
    be visualized as a directed graph of `Layer` objects. In this section, we transform
    this intuition into the `TensorGraph` object. These objects are responsible for
    constructing the underlying TensorFlow computation graph.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: A `TensorGraph` object is responsible for maintaining a `tf.Graph`, a `tf.Session`,
    and a list of layers (`self.layers`) internally ([Example 8-9](#ch8-tensorgraph)).
    The directed graph is represented implicitly, by the `in_layers` belonging to
    each `Layer` object. `TensorGraph` also contains utilities for saving this `tf.Graph`
    instance to disk and consequently assigns itself a directory (using `tempfile.mkdtemp()`
    if none is specified) to store checkpoints of the weights associated with its
    underlying TensorFlow graph.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-9\. The TensorGraph contains a graph of layers; TensorGraph objects
    can be thought of as the “model” object holding the deep architecture you want
    to train
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The private method `_add_layer` does bookkeeping work to add a new `Layer` obect
    to the `TensorGraph` ([Example 8-10](#ch8-add-layer)).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-10\. The _add_layer method adds a new Layer object
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The layers in a `TensorGraph` must form a directed acyclic graph (there can
    be no loops in the graph). As a result, we can topologically sort these layers.
    Intuitively, a topological sort “orders” the layers in the graph so that each
    `Layer` object’s `in_layers` precede it in the ordered list. This topological
    sort is necessary to make sure all input layers to a given layer are added to
    the graph before the layer itself ([Example 8-11](#ch8-topsort)).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-11\. The topsort method orders the layers in the TensorGraph
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `build()` method takes the responsibility of populating the `tf.Graph` instance
    by calling `layer.create_tensor` for each layer in topological order ([Example 8-12](#ch8-build)).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-12\. The build method populates the underlying TensorFlow graph
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The method `set_loss()` adds a loss for training to the graph. `add_output()`
    specifies that the layer in question might be fetched from the graph. `set_optimizer()`
    specifies the optimizer used for training ([Example 8-13](#ch8-utilities)).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-13\. These methods add necessary losses, outputs, and optimizers to
    the computation graph
  id: totrans-126
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The method `get_layer_variables()` is used to fetch the learnable `tf.Variable`
    objects created by a layer. The private method `_get_tf` is used to fetch the
    `tf.Graph` and optimizer instances underpinning the `TensorGraph`. `get_global_step`
    is a convenience method for fetching the current step in the training process
    (starting from 0 at construction). See [Example 8-14](#ch8-get-vars).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-14\. Fetch the learnable variables associated with each layer
  id: totrans-129
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Finally, the `restore()` method restores a saved `TensorGraph` from disk ([Example 8-15](#ch8-restore)).
    (As you will see later, the `TensorGraph` is saved automatically during training.)
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-15\. Restore a trained model from disk
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The A3C Algorithm
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section you will learn how to implement A3C, the asynchronous reinforcement
    learning algorithm you saw earlier in the chapter. A3C is a significantly more
    complex training algorithm than those you have seen previously. The algorithm
    requires running gradient descent in multiple threads, interspersed with game
    rollout code, and updating learned weights asynchronously. As a result of this
    extra complexity, we will define the A3C algorithm in an object-oriented fashion.
    Let’s start by defining an `A3C` object.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: The `A3C` class implements the A3C algorithm ([Example 8-16](#ch8-a3c)). A few
    extra bells and whistles are added onto the basic algorithm to encourage learning,
    notably an entropy term and support for generalized advantage estimation. We won’t
    cover all of these details, but encourage you to follow references into the research
    literature (listed in the documentation) to understand more.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-16\. Define the A3C class encapsulating the asynchronous A3C training
    algorithm
  id: totrans-137
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The heart of the A3C class lies in the `build_graph()` method ([Example 8-17](#ch8-builda3c)),
    which constructs a `TensorGraph` instance (underneath which lies a TensorFlow
    computation graph) encoding the policy learned by the model. Notice how succinct
    this definition is compared with others you have seen previously! There are many
    advantages to using object orientation.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-17\. This method builds the computation graph for the A3C algorithm.
    Note that the policy network is defined here using the Layer abstractions you
    saw previously.
  id: totrans-140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: There’s a lot of code in this example. Let’s break it down into multiple examples
    and discuss more carefully. [Example 8-18](#ch8-inputa3c) takes the array encoding
    of the `TicTacToeEnvironment` and feeds it into the `Input` instances for the
    graph directly.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-18\. This snippet from the build_graph() method feeds in the array
    encoding of TicTacToeEnvironment
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[Example 8-19](#ch8-inputrewarda3c) shows the code used to construct inputs
    for rewards from the environment, advantages observed, and actions taken.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-19\. This snippet from the build_graph() method defines Input objects
    for rewards, advantages, and actions
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The policy network is responsible for learning the policy. In [Example 8-20](#ch8-policya3c),
    the input board state is first flattened into an input feature vector. A series
    of fully connected (or `Dense`) transformations are applied to the flattened board.
    At the very end, a `Softmax` layer is used to predict action probabilities from
    `d5` (note that `out_channels` is set to 9, one for each possible move on the
    tic-tac-toe board).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-20\. This snippet from the build_graph() method defines the policy
    network
  id: totrans-149
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Is Feature Engineering Dead?
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we feed the raw tic-tac-toe game board into TensorFlow for
    training the policy. However, it’s important to note that for more complex games
    than tic-tac-toe, this may not yield satisfactory results. One of the lesser known
    facts about AlphaGo is that DeepMind performs sophisticated feature engineering
    to extract “interesting” patterns of Go pieces upon the board to make AlphaGo’s
    learning easier. (This fact is tucked away into the supplemental information of
    DeepMind’s paper.)
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: The fact remains that reinforcement learning (and deep learning methods broadly)
    often still need human-guided feature engineering to extract meaningful information
    before learning algorithms can learn effective policies and models. It’s likely
    that as more computational power becomes available through hardware advances,
    this need for feature engineering will be reduced, but for the near term, plan
    on manually extracting information about your systems as needed for performance.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: The A3C Loss Function
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We now have the object-oriented machinery set in place to define a loss for
    the A3C policy network. This loss function will itself be implemented as a `Layer`
    object (it’s a convenient abstraction that all parts of the deep architecture
    are simply layers). The `A3CLoss` object implements a mathematical loss consisting
    of the sum of three terms: a `policy_loss`, a `value_loss`, and an `entropy` term
    for exploration. See [Example 8-21](#ch8-a3closs).'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-21\. This Layer implements the loss function for A3C
  id: totrans-156
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: There are a lot of pieces to this definition, so let’s pull out bits of code
    and inspect. The `A3CLoss` layer takes in `reward, action, prob, value, advantage`
    layers as inputs. For mathematical stability, we convert probabilities to log
    probabilities (this is numerically much more stable). See [Example 8-22](#ch8-a3clossinp).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-22\. This snippet from A3CLoss takes reward, action, prob, value,
    advantage as input layers and computes a log probability
  id: totrans-159
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The policy loss computes the sum of all advantages observed, weighted by the
    log-probability of the action taken. (Recall that the advantage is the difference
    in reward resulting from taking the given action as opposed to the expected reward
    from the raw policy for that state). The intuition here is that the `policy_loss`
    provides a signal on which actions were fruitful and which were not ([Example 8-23](#ch8-a3cpolicyloss)).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-23\. This snippet from A3CLoss defines the policy loss
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The value loss computes the difference between our estimate of *V* (`reward`)
    and the actual value of *V* observed (`value`). Note the use of the *L*² loss
    here ([Example 8-24](#ch8-a3cvalueloss)).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-24\. This snippet from A3CLoss defines the value loss
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The entropy term is an addition that encourages the policy to explore further
    by adding some noise. This term is effectively a form of regularization for A3C
    networks. The final loss computed by `A3CLoss` is a linear combination of these
    component losses. See [Example 8-25](#ch8-a3centropyloss).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-25\. This snippet from A3CLoss defines an entropy term added to the
    loss
  id: totrans-168
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Defining Workers
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Thus far, you’ve seen how the policy network is constructed, but you haven’t
    yet seen how the asynchronous training procedure is implemented. Conceptually,
    asynchronous training consists of individual workers running gradient descent
    on locally simulated game rollouts and contributing learned knowledge back to
    a global set of weights periodically. Continuing our object-oriented design, let’s
    introduce the `Worker` class.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Each `Worker` instance holds a copy of the model that’s trained asynchronously
    on a separate thread ([Example 8-26](#ch8-worker)). Note that `a3c.build_graph()`
    is used to construct a local copy of the TensorFlow computation graph for the
    thread in question. Take special note of `local_vars` and `global_vars` here.
    We need to make sure to train only the variables associated with this worker’s
    copy of the policy and not with the global copy of the variables (which is used
    to share information across worker threads). As a result `gradients` uses `tf.gradients`
    to take gradients of the loss with respect to only `local_vars`.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-26\. The Worker class implements the computation performed by each
    thread
  id: totrans-173
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Worker rollouts
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Each `Worker` is responsible for simulating game rollouts locally. The `create_rollout()`
    method uses `session.run` to fetch action probabilities from the TensorFlow graph
    ([Example 8-27](#ch8-workerrollout)). It then samples an action from this policy
    using `np.random.choice`, weighted by the per-class probabilities. The reward
    for the action taken is computed from `TicTacToeEnvironment` via a call to `self.env.step(action)`.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-27\. The create_rollout method simulates a game rollout locally
  id: totrans-177
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The `process_rollouts()` method does preprocessing needed to compute discounted
    rewards, values, actions, and advantages ([Example 8-28](#ch8-processrollout)).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-28\. The process_rollout method computes rewards, values, actions,
    and advantages and then takes a gradient descent step against the loss
  id: totrans-180
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The `Worker.run()` method performs the training step for the `Worker`, relying
    on `process_rollouts()` to issue the actual call to `self.a3c._session.run()`
    under the hood ([Example 8-29](#ch8-processrollout2)).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-29\. The run() method is the top level invocation for Worker
  id: totrans-183
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Training the Policy
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `A3C.fit()` method brings together all the disparate pieces introduced to
    train the model. The `fit()` method takes the responsibility for spawning `Worker`
    threads using the Python `threading` library. Since each `Worker` takes responsibility
    for training itself, the `fit()` method simply is responsible for periodically
    checkpointing the trained model to disk. See [Example 8-30](#ch8-a3cfit).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-30\. The fit() method brings everything together and runs the A3C
    training algorithm
  id: totrans-187
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Challenge for the Reader
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We strongly encourage you to try training tic-tac-toe models for yourself! Note
    that this example is more involved than other examples in the book, and will require
    greater computational power. We recommend a machine with at least a few CPU cores.
    This requirement isn’t too onerous; a good laptop should suffice. Try using a
    tool like `htop` to check that the code is indeed multithreaded. See how good
    a model you can train! You should be able to beat the random baseline most of
    the time, but this basic implementation won’t give you a model that always wins.
    We recommend exploring the RL literature and expanding upon the base implementation
    to see how well you can do.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Review
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced you to the core concepts of reinforcement learning
    (RL). We walked you through some recent successes of RL methods on ATARI, upside-down
    helicopter flight, and computer Go. We then taught you about the mathematical
    framework of Markov decision processes. We brought it together with a detailed
    case study walking you through the construction of a tic-tac-toe agent. This algorithm
    uses a sophisticated training method, A3C, that makes use of multiple CPU cores
    to speed up training. In [Chapter 9](ch09.html#training_large_deep_networks),
    you’ll learn more about training models with multiple GPUs.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
