- en: Chapter 8\. Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The learning techniques we’ve covered so far in this book fall into the categories
    of supervised or unsupervised learning. In both cases, solving a given problem
    requires a data scientist to design a deep architecture that handles and processes
    input data and to connect the output of the architecture to a loss function suitable
    for the problem at hand. This framework is widely applicable, but not all applications
    fall neatly into this style of thinking. Let’s consider the challenge of training
    a machine learning model to win a game of chess. It seems reasonable to process
    the board as spatial input using a convolutional network, but what would the loss
    entail? None of our standard loss functions such as cross-entropy or *L*² loss
    quite seem to apply.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning provides a mathematical framework well suited to solving
    games. The central mathematical concept is that of the *Markov decision process*,
    a tool for modeling AI agents that interact with *environments* that offer *rewards*
    upon completion of certain *actions*. This framework proves to be flexible and
    general, and has found a number of applications in recent years. It’s worth noting
    that reinforcement learning as a field is quite mature and has existed in recognizable
    form since the 1970s. However, until recently, most reinforcement learning systems
    were only capable of solving toy problems. Recent work has revealed that these
    limitations were likely due to the lack of sophisticated data intake mechanisms;
    hand-engineered features for many games or robotic environments often did not
    suffice. Deep representation extractions trained end-to-end on modern hardware
    seem to break through the barriers of earlier reinforcement learning systems and
    have achieved notable results in recent years.
  prefs: []
  type: TYPE_NORMAL
- en: Arguably, the first breakthrough in deep reinforcement learning was on ATARI
    arcade games. ATARI arcade games were traditionally played in video game arcades
    and offered users simple games that don’t typically require sophisticated strategizing
    but might require good reflexes. [Figure 8-1](#ch8-breakout) shows a screenshot
    from the popular ATARI game Breakout. In recent years, due to the development
    of good ATARI emulation software, ATARI games have become a testbed for gameplay
    algorithms. At first, reinforcement learning algorithms applied to ATARI didn’t
    achieve superb results; the requirement that the algorithm understand a visual
    game state frustrated most attempts. However, as convolutional networks matured,
    researchers at DeepMind realized that convolutional networks could be combined
    with existing reinforcement learning techniques and trained end-to-end.
  prefs: []
  type: TYPE_NORMAL
- en: '![breakout.jpg](assets/tfdl_0801.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1\. A screenshot of the ATARI arcade game Breakout. Players have to
    use the paddle at the bottom of the screen to bounce a ball that breaks the tiles
    at the top of the screen.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The resulting system achieved superb results, and learned to play many ATARI
    games (especially those dependent on quick reflexes) at superhuman standards.
    [Figure 8-2](#ch8-breakoutresults) lists ATARI scores achieved by DeepMind’s DQN
    algorithm. This breakthrough result spurred tremendous growth in the field of
    deep reinforcement learning and inspired legions of researchers to explore the
    potential of deep reinforcement learning techniques. At the same time, DeepMind’s
    ATARI results showed reinforcement learning techniques were capable of solving
    systems dependent on short-term movements. These results didn’t demonstrate that
    deep reinforcement learning systems were capable of solving games that required
    greater strategic planning.
  prefs: []
  type: TYPE_NORMAL
- en: '![atari_scores.jpg](assets/tfdl_0802.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. Results of DeepMind’s DQN reinforcement learning algorithm on various
    ATARI games. 100% is the score of a strong human player. Note that DQN achieves
    superhuman performance on many games, but is quite weak on others.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Computer Go
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 1994, IBM revealed the system Deep Blue, which later succeeded in defeating
    Garry Kasparov in a highly publicized chess match. This system relied on brute
    force computation to expand the tree of possible chess moves (with some help from
    handcrafted chess heuristics) to play master-level chess.
  prefs: []
  type: TYPE_NORMAL
- en: Computer scientists attempted to apply similar techniques to other games such
    as Go. Unfortunately for early experimenters, Go’s 19 × 19 game board is significantly
    larger than chess’s 8 × 8 board. As a result, trees of possible moves explode
    much more quickly than for chess, and simple back-of-the-envelope calculations
    indicated that Moore’s law would take a very long time to enable brute force solution
    of Go in the style of Deep Blue. Complicating matters, there existed no simple
    heuristic for evaluating who’s winning in a half-played Go game (determining whether
    black or white is ahead is a notoriously noisy art for the best human analysts).
    As a result, until very recently, many prominent computer scientists believed
    that strong computer Go play was a decade away at the least.
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate the prowess of its reinforcement learning algorithms, DeepMind
    took on the challenge of learning to play Go, a game that requires complex strategic
    planning. In a tour-de-force paper, DeepMind revealed its deep reinforcement learning
    engine, AlphaGo, which combined convolutional networks with tree-based search
    to defeat the human Go master Lee Sedol ([Figure 8-3](#ch8-leesedol)).
  prefs: []
  type: TYPE_NORMAL
- en: '![lee_sedol.jpg](assets/tfdl_0803.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-3\. Human Go champion Lee Sedol battles AlphaGo. Lee Sedol eventually
    lost the match 1–4, but succeeded in winning one game. It’s unlikely that this
    victory can be replicated against the vastly improved successors of AlphaGo such
    as AlphaZero.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: AlphaGo convincingly demonstrated that deep reinforcement learning techniques
    were capable of learning to solve complex strategic games. The heart of the breakthrough
    was the realization that convolutional networks could learn to estimate whether
    black or white was ahead in a half-played game, which enabled game trees to be
    truncated at reasonable depths. (AlphaGo also estimates which moves are most fruitful,
    enabling a second pruning of the game tree space.) AlphaGo’s victory really launched
    deep reinforcement learning into prominence, and a host of researchers are working
    to transform AlphaGo-style systems into practical use.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we discuss reinforcement learning algorithms and specifically
    deep reinforcement learning architectures. We then show readers how to successfully
    apply reinforcement learning to the game of tic-tac-toe. Despite the simplicity
    of the game, training a successful reinforcement learner for tic-tac-toe requires
    significant sophistication, as you will soon see.
  prefs: []
  type: TYPE_NORMAL
- en: The code for this chapter was adapted from the DeepChem reinforcement learning
    library, and in particular from example code created by Peter Eastman and Karl
    Leswing. Thanks to Peter for debugging and tuning help on this chapter’s example
    code.
  prefs: []
  type: TYPE_NORMAL
- en: Markov Decision Processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before launching into a discussion of reinforcement learning algorithms, it
    will be useful to pin down the family of problems that reinforcement learning
    methods seek to solve. The mathematical framework of Markov decision processes
    (MDPs) is very useful for formulating reinforcement learning methods. Traditionally,
    MDPs are introduced with a battery of Greek symbols, but we will instead try to
    proceed by providing some basic intuition.
  prefs: []
  type: TYPE_NORMAL
- en: The heart of MDPs is the pair of an *environment* and an *agent*. An environment
    encodes a “world” in which the agent seeks to act. Example environments could
    include game worlds. For example, a Go board with master Lee Sedol sitting opposite
    is a valid environment. Another potential environment could be the environment
    surrounding a small robot helicopter. In a prominent early reinforcement learning
    success, a team at Stanford led by Andrew Ng trained a helicopter to fly upside
    down using reinforcement learning as shown in [Figure 8-4](#ch8-upside).
  prefs: []
  type: TYPE_NORMAL
- en: '![upside_down_helicopter.jpg](assets/tfdl_0804.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-4\. Andrew Ng’s team at Stanford, from 2004 to 2010, trained a helicopter
    to learn to fly upside down using reinforcement learning. This work required the
    construction of a sophisticated physically accurate simulator.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The agent is the learning entity that acts within the environment. In our first
    example, AlphaGo itself is the agent. In the second, the robot helicopter (or
    more accurately, the control algorithm in the robot helicopter) is the agent.
    Each agent has a set of actions that it can take within the environment. For AlphaGo,
    these constitute valid Go moves. For the robot helicopter, these include control
    of the main and secondary rotors.
  prefs: []
  type: TYPE_NORMAL
- en: Actions the agent takes are presumed to have an effect on the environment. In
    the case of AlphaGo, this effect is deterministic (AlphaGo deciding to place a
    Go stone results in the stone being placed). In the case of the helicopter, the
    effect is likely probabilistic (changes in helicopter position may depend on wind
    conditions, which can’t be modeled effectively).
  prefs: []
  type: TYPE_NORMAL
- en: The final piece of the model is the notion of reward. Unlike supervised learning
    where explicit labels are present to learn from, or unsupervised learning where
    the challenge is to learn the underlying structure of the data, reinforcement
    learning operates in a setting of partial, sparse rewards. In Go, rewards are
    achieved at the end of the game upon victory or defeat, while in helicopter flight,
    rewards might be presented for successful flights or completion of trick moves.
  prefs: []
  type: TYPE_NORMAL
- en: Reward Function Engineering Is Hard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the largest challenges in reinforcement learning is designing rewards
    that induce agents to learn desired behaviors. For even simple win/loss games
    such as Go or tic-tac-toe, this can be surprisingly difficult. How much should
    a loss be punished and how much should a win be rewarded? There don’t yet exist
    good answers.
  prefs: []
  type: TYPE_NORMAL
- en: For more complex behaviors, this can be extremely challenging. A number of studies
    have demonstrated that simple rewards can result in agents learning unexpected
    and even potentially damaging behaviors. These systems spur fears of future agents
    with greater autonomy wreaking havoc when unleashed in the real world after having
    been trained to optimize bad reward functions.
  prefs: []
  type: TYPE_NORMAL
- en: In general, reinforcement learning is less mature than supervised learning techniques,
    and we caution that decisions to deploy reinforcement learning in production systems
    should be taken very carefully. Given uncertainty over learned behavior, make
    sure to thoroughly test any deployed reinforcement learned system.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement Learning Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we’ve introduced you to the core mathematical structures underlying
    reinforcement learning, let’s consider how to design algorithms that learn intelligent
    behaviors for reinforcement learning agents. At a high level, reinforcement learning
    algorithms can be separated into the buckets of *model-based* and *model-free*
    algorithms. The central difference is whether the algorithm seeks to learn an
    internal model of how its environment acts. For simpler environments, such as
    tic-tac-toe, the model dynamics are trivial. For more complex environments, such
    as helicopter flight or even ATARI games, the underlying environment is likely
    extraordinarily complex. Avoiding the construction of an explicit model of the
    environment in favor of an implicit model that advises the agent on how to act
    may well be more pragmatic.
  prefs: []
  type: TYPE_NORMAL
- en: Simulations and Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Any reinforcement learning algorithm requires iteratively improving the performance
    of the current agent by evaluating the agent’s current behavior and changing it
    to improve received rewards. These updates to the agent structure often include
    some gradient descent update, as we will see in the following sections. However,
    as you know intimately from previous chapters, gradient descent is a slow training
    algorithm! Millions or even billions of gradient descent steps may be required
    to learn an effective model.
  prefs: []
  type: TYPE_NORMAL
- en: This poses a problem if the learning environment is in the real world; how can
    an agent interact millions of times with the real world? In most cases it can’t.
    As a result, most sophisticated reinforcement learning systems depend critically
    on simulators that allow interaction with a simulation computational version of
    the environment. For the helicopter flight environment, one of the hardest challenges
    researchers faced was building an accurate helicopter physics simulator that allowed
    learning of effective flight policies computationally.
  prefs: []
  type: TYPE_NORMAL
- en: Q-Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the framework of Markov decision processes, agents take actions in an environment
    and obtain rewards that are (presumably) tied to agent actions. The *Q* function
    predicts the expected reward for taking a given action in a particular environment
    state. This concept seems very straightforward, but the trickiness arises when
    this expected reward includes discounted rewards from future actions.
  prefs: []
  type: TYPE_NORMAL
- en: Discounting Rewards
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The notion of a discounted reward is widespread, and is often introduced in
    the context of finances. Suppose a friend says he’ll pay you $10 next week. That
    future 10 dollars is worth less to you than 10 dollars in your hand right now
    (what if the payment never happens, for one?). So mathematically, it’s common
    practice to introduce a discounting factor γ (typically between 0 and 1) that
    lowers the “present-value” of future payments. For example, say your friend is
    somewhat untrustworthy. You might decide to set γ = 0.5 and value your friend’s
    promise as worth 10γ = 5 dollars today to account for uncertainty in rewards.
  prefs: []
  type: TYPE_NORMAL
- en: However, these future rewards depend on actions taken by the agent in the future.
    As a result, the *Q* function must be formulated recursively in terms of itself,
    since expected rewards for one state depend on those for another state. This recursive
    definition makes learning the *Q* function tricky. This recursive relationship
    can be formulated explicitly for simple environments with discrete state spaces
    and solved with dynamic programming methods. For more general environments, *Q*-learning
    methods were not very useful until recently.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, Deep *Q*-networks (DQN) were introduced by DeepMind and used to solve
    ATARI games as mentioned earlier. The key insight underlying DQN is once again
    the universal approximation theorem; since *Q* may be arbitrarily complex, we
    should model it with a universal approximator such as a deep network. While using
    neural networks to model *Q* had been done before, DeepMind also introduced the
    notion of experience replay for these networks, which let them train DQN models
    effectively at scale. Experience replay stores observed game outcomes and transitions
    from past games, and resamples them while training (in addition to training on
    new games) to ensure that lessons from the past are not forgotten by the network.
  prefs: []
  type: TYPE_NORMAL
- en: Catastrophic Forgetting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural networks quickly forget the past. In fact, this phenomenon, termed *catastrophic
    forgetting*, can occur very rapidly; a few minibatch updates can be sufficient
    for the network to forget a complex behavior it previously knew. As a result,
    without techniques like experience replay that ensure the network always trains
    on episodes from past matches, it wouldn’t be possible to learn complex behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: Designing a training algorithm for deep networks that doesn’t suffer from catastrophic
    forgetting is still a major open problem today. Humans notably don’t suffer from
    catastrophic forgetting; even if you haven’t ridden a bike in years, it’s likely
    you still remember how to do so. Creating a neural network that has similar resilience
    might involve the addition of long-term external memory, along the lines of the
    Neural Turing machine. Unfortunately, none of the attempts thus far at designing
    resilient architectures has really worked well.
  prefs: []
  type: TYPE_NORMAL
- en: Policy Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, you learned about *Q*-learning, which seeks to understand
    the expected rewards for taking given actions in given environment states. Policy
    learning is an alternative mathematical framework for learning agent behavior.
    It introduces the policy function π that assigns a probability to each action
    that an agent can take in a given state.
  prefs: []
  type: TYPE_NORMAL
- en: Note that a policy is sufficient for defining agent behavior entirely. Given
    a policy, an agent can act just by sampling a suitable action for the current
    environment state. Policy learning is convenient, since policies can be learned
    directly through an algorithm called policy gradient. This algorithm uses a couple
    mathematical tricks to enable policy gradients to be computed directly via backpropagation
    for deep networks. The key concept is the *rollout*. Let an agent act in an environment
    according to its current policy and observe all rewards that come in. Then backpropagate
    to increase the likelihood of those actions that led to more fruitful rewards.
    This description is accurate at a high level, but we will see more implementation
    details later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: A policy is often associated with a *value function* *V*. This function returns
    the expected discounted reward for following policy π starting from the current
    state of the environment. *V* and *Q* are closely related functions since both
    provide estimates of future rewards starting from present state, but *V* does
    not specify an action to be taken and assumes rather that actions are sampled
    from π.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another commonly defined function is the *advantage* *A*. This function defines
    the difference in expected reward due to taking a particular action *a* in a given
    environment state *s*, as opposed to following the base policy π. Mathematically,
    *A* is defined in terms of *Q* and *V*:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>A</mi> <mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi>
    <mo>)</mo> <mo>=</mo> <mi>Q</mi> <mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>a</mi> <mo>)</mo>
    <mo>-</mo> <mi>V</mi> <mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The advantage is useful in policy-learning algorithms, since it lets an algorithm
    quantify how a particular action may have been better suited than the present
    recommendation of the policy.
  prefs: []
  type: TYPE_NORMAL
- en: Policy Gradient Outside Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although we have introduced policy gradient as a reinforcement learning algorithm,
    it can equally be viewed as a tool for learning deep networks with nondifferentiable
    submodules. What does this mean when we unpack the mathematical jargon?
  prefs: []
  type: TYPE_NORMAL
- en: Let’s suppose we have a deep network that calls an external program within the
    network itself. This external program is a black box; it could be a network call
    or an invocation of a 1970s COBOL routine. How can we learn the rest of the deep
    network when this module has no gradient?
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that policy gradient can be repurposed to estimate an “effective”
    gradient for the system. The simple intuition is that multiple “rollouts” can
    be run, which are used to estimate gradients. Expect to see research over the
    next few years extending this idea to create large networks with nondifferential
    modules.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A disadvantage of the policy gradient methods presented in the previous section
    is that performing the rollout operations requires evaluating agent behavior in
    some (likely simulated) environment. Most simulators are complicated pieces of
    software that can’t be run on the GPU. As a result, taking a single learning step
    will require running long CPU-bound calculations. This can lead to unreasonably
    slow training.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous reinforcement learning methods seek to speed up this process by
    using multiple asynchronous CPU threads to perform rollouts independently. These
    worker threads will perform rollouts, estimate gradient updates to the policy
    locally, and then periodically synchronize with the global set of parameters.
    Empirically, asynchronous training appears to significantly speed up reinforcement
    learning and allows for fairly sophisticated policies to be learned on laptops.
    (Without GPUs! The majority of computational power is used on rollouts, so gradient
    update steps are often not the rate limiting aspect of reinforcement learning
    training.) The most popular algorithm for asynchronous reinforcement learning
    currently is the asynchronous actor advantage critic (A3C) algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: CPU or GPU?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GPUs are necessary for most large deep learning applications, but reinforcement
    learning currently appears to be an exception to this general rule. The reliance
    of reinforcement learning algorithms to perform many rollouts seems to currently
    bias reinforcement learning implementations toward multicore CPU systems. It’s
    likely that in specific applications, individual simulators can be ported to work
    more quickly on GPUs, but CPU-based simulations will likely continue to dominate
    for the near future.
  prefs: []
  type: TYPE_NORMAL
- en: Limits of Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The framework of Markov decision processes is immensely general. For example,
    behavioral scientists routinely use Markov decision processes to understand and
    model human decision making. The mathematical generality of this framework has
    spurred scientists to posit that solving reinforcement learning might spur the
    creation of artificial general intelligences (AGIs). The stunning success of AlphaGo
    against Lee Sedol amplified this belief, and indeed research groups such as OpenAI
    and DeepMind aiming to build AGIs focus much of their efforts on developing new
    reinforcement learning techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Nonetheless, there are major weaknesses to reinforcement learning as it currently
    exists. Careful benchmarking work has shown that reinforcement learning techniques
    are very susceptible to choice of hyperparameters (even by the standards of deep
    learning, which is already much finickier than other techniques like random forests).
    As we have mentioned, reward function engineering is very immature. Humans are
    capable of internally designing their own reward functions or effectively learning
    to copy reward functions from observation. Although “inverse reinforcement learning”
    algorithms that learn reward functions directly have been proposed, these algorithms
    have many limitations in practice.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to these fundamental limitations, there are still many practical
    scaling issues. Humans are capable of playing games that combine high-level strategizing
    with thousands of “micro” moves. For example, master-level play of the strategy
    game StarCraft (see [Figure 8-5](#ch8-starcraft)) requires sophisticated strategic
    ploys combined with careful control of hundreds of units. Games can require thousands
    of local moves to be played to completion. In addition, unlike Go or chess, StarCraft
    has a “fog of war” where players cannot see the entire game state. This combination
    of large game state and uncertainty has foiled reinforcement learning attempts
    on StarCraft. As a result, teams of AI researchers at DeepMind and other groups
    are focusing serious effort on solving StarCraft with deep reinforcement learning
    methods. Despite some serious effort, though, the best StarCraft bots remain at
    amateur level.
  prefs: []
  type: TYPE_NORMAL
- en: '![deep_starcraft.png](assets/tfdl_0805.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-5\. A collection of subtasks required for playing the real-time strategy
    game StarCraft. In this game, players must build an army that they can use to
    defeat the opposing force. Successful StarCraft play requires mastery of resource
    planning, exploration, and complex strategy. The best computer StarCraft agents
    remain at amateur level.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In general, there’s wide consensus that reinforcement learning is a useful technique
    that’s likely to be deeply influential over the next few decades, but it’s also
    clear that the many practical limitations of reinforcement learning methods will
    mean that most work will continue to be done in research labs for the near future.
  prefs: []
  type: TYPE_NORMAL
- en: Playing Tic-Tac-Toe
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tic-tac-toe is a simple two-player game. Players place Xs and Os on a 3 × 3
    game board until one player succeeds in placing three of her pieces in a row.
    The first player to do so wins. If neither player succeeds in obtaining three
    in a row before the board is filled up, the game ends in a draw. [Figure 8-6](#ch8-tictactoe)
    illustrates a tic-tac-toe game board.
  prefs: []
  type: TYPE_NORMAL
- en: '![Tic_tac_toe.png](assets/tfdl_0806.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-6\. A tic-tac-toe game board.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tic-tac-toe is a nice testbed for reinforcement learning techniques. The game
    is simple enough that exorbitant amounts of computational power aren’t required
    to train effective agents. At the same time, despite tic-tac-toe’s simplicity,
    learning an effective agent requires considerable sophistication. The TensorFlow
    code for this section is arguably the most sophisticated example found in this
    book. We will walk you through the design of a TensorFlow tic-tac-toe asynchronous
    reinforcement learning agent in the remainder of this section.
  prefs: []
  type: TYPE_NORMAL
- en: Object Orientation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The code we’ve introduced thus far in this book has primarily consisted of scripts
    augmented by smaller helper functions. In this chapter, however, we will swap
    to an object-oriented programming style. This style of programming might be new
    to you, especially if you hail from the scientific world rather than from the
    tech world. Briefly, an object-oriented program defines *objects* that model aspects
    of the world. For example, you might want to define `Environment` or `Agent` or
    `Reward` objects that directly correspond to these mathematical concepts. A *class*
    is a template for objects that can be used to *instantiate* (or create) many new
    objects. For example, you will shortly see an `Environment` class definition we
    will use to define many particular `Environment` objects.
  prefs: []
  type: TYPE_NORMAL
- en: Object orientation is particularly powerful for building complex systems, so
    we will use it to simplify the design of our reinforcement learning system. In
    practice, your real-world deep learning (or reinforcement learning) systems will
    likely need to be object oriented as well, so we encourage taking some time to
    master object-oriented design. There are many superb books that cover the fundamentals
    of object-oriented design, and we recommend that you check them out as necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Abstract Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start by defining an abstract `Environment` object that encodes the state
    of a system in a list of NumPy objects ([Example 8-1](#ch8-envclass)). This `Environment`
    object is quite general (adapted from DeepChem’s reinforcement learning engine)
    so it can easily serve as a template for other reinforcement learning projects
    you might seek to implement.
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-1\. This class defines a template for constructing new environments
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Tic-Tac-Toe Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need to specialize the `Environment` class to create a `TicTacToeEnvironment`
    suitable for our needs. To do this, we construct a *subclass* of `Environment`
    that adds on more features, while retaining the core functionality of the original
    *superclass*. In [Example 8-2](#ch8-tictacenvclass), we define `TicTacToeEnvironment`
    as a subclass of `Environment` that adds details specific to tic-tac-toe.
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-2\. The TicTacToeEnvironment class defines a template for constructing
    new tic-tac-toe environments
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The first interesting tidbit to note here is that we define the board state
    as a NumPy array of shape `(3, 3, 2)`. We use a one-hot encoding of `X` and `O`
    (one-hot encodings aren’t only useful for natural language processing!).
  prefs: []
  type: TYPE_NORMAL
- en: The second important thing to note is that the environment explicitly defines
    the reward function by setting penalties for illegal moves and losses, and rewards
    for draws and wins. This snippet powerfully illustrates the arbitrary nature of
    reward function engineering. Why these particular numbers?
  prefs: []
  type: TYPE_NORMAL
- en: Empirically, these choices appear to result in stable behavior, but we encourage
    you to experiment with alternate reward settings to observe results. In this implementation,
    we specify that the agent always plays `X`, but randomize whether `X` or `O` goes
    first. The function `get_O_move()` simply places an `O` on a random open tile
    on the game board. `TicTacToeEnvironment` encodes an opponent that plays `O` while
    always selecting a random move. The `reset()` function simply clears the board,
    and places an `O` tile randomly if `O` is going first during this game. See [Example 8-3](#ch8-tictacenvclass2).
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-3\. More methods from the TicTacToeEnvironment class
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The utility function `game_over()` reports that the game has ended if all tiles
    are filled. `check_winner()` checks whether the specified player has achieved
    three in a row and won the game ([Example 8-4](#ch8-tictacenvclass3)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-4\. Utility methods from the TicTacToeEnvironment class for detecting
    when the game has ended and who won
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In our implementation, an action is simply a number between 0 and 8 specifying
    the tile on which the `X` tile is placed. The `step()` method checks whether this
    tile is occupied (returning a penalty if so), then places the tile. If `X` has
    won, a reward is returned. Else, the random `O` opponent is allowed to make a
    move. If `O` won, then a penalty is returned. If the game has ended as a draw,
    then a penalty is returned. Else, the game continues with a `NOT_LOSS` reward.
    See [Example 8-5](#ch8-tictacenvclass4).
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-5\. This method performs a step of the simulation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The Layer Abstraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Running an asynchronous reinforcement learning algorithm such as A3C requires
    that each thread have access to a separate copy of the policy model. These copies
    of the model have to be periodically re-synced with one another for training to
    proceed. What is the easiest way we can construct multiple copies of the TensorFlow
    graph that we can distribute to each thread?
  prefs: []
  type: TYPE_NORMAL
- en: One simple possibility is to create a function that creates a copy of the model
    in a separate TensorFlow graph. This approach works well, but gets to be a little
    messy, especially for sophisticated networks. Using a little bit of object orientation
    can significantly simplify this process. Since our reinforcement learning code
    is adapted from the DeepChem library, we use a simplified version of the TensorGraph
    framework from DeepChem (see [*https://deepchem.io*](https://deepchem.io) for
    information and docs). This framework is similar to other high-level TensorFlow
    frameworks such as Keras. The core abstraction in all such models is the introduction
    of a `Layer` object that encapsulates a portion of a deep network.
  prefs: []
  type: TYPE_NORMAL
- en: A `Layer` is a portion of a TensorFlow graph that accepts a list `in_layers`
    of input layers. In this abstraction, a deep architecture consists of a *directed
    graph* of layers. Directed graphs are similar to the undirected graphs you saw
    in [Chapter 6](ch06.html#convolutional_neural_networks), but have directions on
    their edges. In this case, the `in_layers` have edges to the new `Layer`, with
    the direction pointing toward the new layer. You will learn more about this concept
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: We use `tf.register_tensor_conversion_function`, a utility that allows arbitrary
    classes to register themselves as tensor convertible. This registration will mean
    that a `Layer` can be converted into a TensorFlow tensor via a call to `tf.convert_to_tensor`.
    The `_get_input_tensors()` private method is a utility that uses `tf.convert_to_tensor`
    to transform input layers into input tensors. Each `Layer` is responsible for
    implementing a `create_tensor()` method that specifies the operations to add to
    the TensorFlow computational graph. See [Example 8-6](#ch8-layer).
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-6\. The Layer object is the fundamental abstraction in object-oriented
    deep architectures. It encapsulates a part of the netwok such as a fully connected
    layer or a convolutional layer. This example defines a generic superclass for
    all such layers.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The preceding description is abstract, but in practice easy to use. [Example 8-7](#ch8-squeeze)
    shows a `Squeeze` layer that wraps `tf.squeeze` with a `Layer` (you will find
    this class convenient later). Note that `Squeeze` is a subclass of `Layer`.
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-7\. The Squeeze layer squeezes its input
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `Input` layer wraps placeholders for convenience ([Example 8-8](#ch8-input)).
    Note that the `Layer.create_tensor` method must be invoked for each layer we use
    in order to construct a TensorFlow computational graph.
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-8\. The Input layer adds placeholders to the computation graph
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: tf.keras and tf.estimator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow has now integrated the popular Keras object-oriented frontend into
    the core TensorFlow library. Keras includes a `Layer` class definition that closely
    matches the `Layer` objects you’ve just learned about in this section. In fact,
    the `Layer` objects here were adapted from the DeepChem library, which in turn
    adapted them from an earlier version of Keras.
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth noting, though, that `tf.keras` has not yet become the standard higher-level
    interface to TensorFlow. The `tf.estimator` module provides an alternative (albeit
    less rich) high-level interface to raw TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of which frontend eventually becomes standard, we think that understanding
    the fundamental design principles for building your own frontend is instructive
    and worthwhile. You might need to build a new system for your organization that
    requires an alternative design, so a solid grasp of design principles will serve
    you well.
  prefs: []
  type: TYPE_NORMAL
- en: Defining a Graph of Layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We mentioned briefly in the previous section that a deep architecture could
    be visualized as a directed graph of `Layer` objects. In this section, we transform
    this intuition into the `TensorGraph` object. These objects are responsible for
    constructing the underlying TensorFlow computation graph.
  prefs: []
  type: TYPE_NORMAL
- en: A `TensorGraph` object is responsible for maintaining a `tf.Graph`, a `tf.Session`,
    and a list of layers (`self.layers`) internally ([Example 8-9](#ch8-tensorgraph)).
    The directed graph is represented implicitly, by the `in_layers` belonging to
    each `Layer` object. `TensorGraph` also contains utilities for saving this `tf.Graph`
    instance to disk and consequently assigns itself a directory (using `tempfile.mkdtemp()`
    if none is specified) to store checkpoints of the weights associated with its
    underlying TensorFlow graph.
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-9\. The TensorGraph contains a graph of layers; TensorGraph objects
    can be thought of as the “model” object holding the deep architecture you want
    to train
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The private method `_add_layer` does bookkeeping work to add a new `Layer` obect
    to the `TensorGraph` ([Example 8-10](#ch8-add-layer)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-10\. The _add_layer method adds a new Layer object
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The layers in a `TensorGraph` must form a directed acyclic graph (there can
    be no loops in the graph). As a result, we can topologically sort these layers.
    Intuitively, a topological sort “orders” the layers in the graph so that each
    `Layer` object’s `in_layers` precede it in the ordered list. This topological
    sort is necessary to make sure all input layers to a given layer are added to
    the graph before the layer itself ([Example 8-11](#ch8-topsort)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-11\. The topsort method orders the layers in the TensorGraph
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `build()` method takes the responsibility of populating the `tf.Graph` instance
    by calling `layer.create_tensor` for each layer in topological order ([Example 8-12](#ch8-build)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-12\. The build method populates the underlying TensorFlow graph
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The method `set_loss()` adds a loss for training to the graph. `add_output()`
    specifies that the layer in question might be fetched from the graph. `set_optimizer()`
    specifies the optimizer used for training ([Example 8-13](#ch8-utilities)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-13\. These methods add necessary losses, outputs, and optimizers to
    the computation graph
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The method `get_layer_variables()` is used to fetch the learnable `tf.Variable`
    objects created by a layer. The private method `_get_tf` is used to fetch the
    `tf.Graph` and optimizer instances underpinning the `TensorGraph`. `get_global_step`
    is a convenience method for fetching the current step in the training process
    (starting from 0 at construction). See [Example 8-14](#ch8-get-vars).
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-14\. Fetch the learnable variables associated with each layer
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Finally, the `restore()` method restores a saved `TensorGraph` from disk ([Example 8-15](#ch8-restore)).
    (As you will see later, the `TensorGraph` is saved automatically during training.)
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-15\. Restore a trained model from disk
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The A3C Algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section you will learn how to implement A3C, the asynchronous reinforcement
    learning algorithm you saw earlier in the chapter. A3C is a significantly more
    complex training algorithm than those you have seen previously. The algorithm
    requires running gradient descent in multiple threads, interspersed with game
    rollout code, and updating learned weights asynchronously. As a result of this
    extra complexity, we will define the A3C algorithm in an object-oriented fashion.
    Let’s start by defining an `A3C` object.
  prefs: []
  type: TYPE_NORMAL
- en: The `A3C` class implements the A3C algorithm ([Example 8-16](#ch8-a3c)). A few
    extra bells and whistles are added onto the basic algorithm to encourage learning,
    notably an entropy term and support for generalized advantage estimation. We won’t
    cover all of these details, but encourage you to follow references into the research
    literature (listed in the documentation) to understand more.
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-16\. Define the A3C class encapsulating the asynchronous A3C training
    algorithm
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The heart of the A3C class lies in the `build_graph()` method ([Example 8-17](#ch8-builda3c)),
    which constructs a `TensorGraph` instance (underneath which lies a TensorFlow
    computation graph) encoding the policy learned by the model. Notice how succinct
    this definition is compared with others you have seen previously! There are many
    advantages to using object orientation.
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-17\. This method builds the computation graph for the A3C algorithm.
    Note that the policy network is defined here using the Layer abstractions you
    saw previously.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: There’s a lot of code in this example. Let’s break it down into multiple examples
    and discuss more carefully. [Example 8-18](#ch8-inputa3c) takes the array encoding
    of the `TicTacToeEnvironment` and feeds it into the `Input` instances for the
    graph directly.
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-18\. This snippet from the build_graph() method feeds in the array
    encoding of TicTacToeEnvironment
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[Example 8-19](#ch8-inputrewarda3c) shows the code used to construct inputs
    for rewards from the environment, advantages observed, and actions taken.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-19\. This snippet from the build_graph() method defines Input objects
    for rewards, advantages, and actions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The policy network is responsible for learning the policy. In [Example 8-20](#ch8-policya3c),
    the input board state is first flattened into an input feature vector. A series
    of fully connected (or `Dense`) transformations are applied to the flattened board.
    At the very end, a `Softmax` layer is used to predict action probabilities from
    `d5` (note that `out_channels` is set to 9, one for each possible move on the
    tic-tac-toe board).
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-20\. This snippet from the build_graph() method defines the policy
    network
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Is Feature Engineering Dead?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we feed the raw tic-tac-toe game board into TensorFlow for
    training the policy. However, it’s important to note that for more complex games
    than tic-tac-toe, this may not yield satisfactory results. One of the lesser known
    facts about AlphaGo is that DeepMind performs sophisticated feature engineering
    to extract “interesting” patterns of Go pieces upon the board to make AlphaGo’s
    learning easier. (This fact is tucked away into the supplemental information of
    DeepMind’s paper.)
  prefs: []
  type: TYPE_NORMAL
- en: The fact remains that reinforcement learning (and deep learning methods broadly)
    often still need human-guided feature engineering to extract meaningful information
    before learning algorithms can learn effective policies and models. It’s likely
    that as more computational power becomes available through hardware advances,
    this need for feature engineering will be reduced, but for the near term, plan
    on manually extracting information about your systems as needed for performance.
  prefs: []
  type: TYPE_NORMAL
- en: The A3C Loss Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We now have the object-oriented machinery set in place to define a loss for
    the A3C policy network. This loss function will itself be implemented as a `Layer`
    object (it’s a convenient abstraction that all parts of the deep architecture
    are simply layers). The `A3CLoss` object implements a mathematical loss consisting
    of the sum of three terms: a `policy_loss`, a `value_loss`, and an `entropy` term
    for exploration. See [Example 8-21](#ch8-a3closs).'
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-21\. This Layer implements the loss function for A3C
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: There are a lot of pieces to this definition, so let’s pull out bits of code
    and inspect. The `A3CLoss` layer takes in `reward, action, prob, value, advantage`
    layers as inputs. For mathematical stability, we convert probabilities to log
    probabilities (this is numerically much more stable). See [Example 8-22](#ch8-a3clossinp).
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-22\. This snippet from A3CLoss takes reward, action, prob, value,
    advantage as input layers and computes a log probability
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The policy loss computes the sum of all advantages observed, weighted by the
    log-probability of the action taken. (Recall that the advantage is the difference
    in reward resulting from taking the given action as opposed to the expected reward
    from the raw policy for that state). The intuition here is that the `policy_loss`
    provides a signal on which actions were fruitful and which were not ([Example 8-23](#ch8-a3cpolicyloss)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-23\. This snippet from A3CLoss defines the policy loss
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The value loss computes the difference between our estimate of *V* (`reward`)
    and the actual value of *V* observed (`value`). Note the use of the *L*² loss
    here ([Example 8-24](#ch8-a3cvalueloss)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-24\. This snippet from A3CLoss defines the value loss
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The entropy term is an addition that encourages the policy to explore further
    by adding some noise. This term is effectively a form of regularization for A3C
    networks. The final loss computed by `A3CLoss` is a linear combination of these
    component losses. See [Example 8-25](#ch8-a3centropyloss).
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-25\. This snippet from A3CLoss defines an entropy term added to the
    loss
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Defining Workers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Thus far, you’ve seen how the policy network is constructed, but you haven’t
    yet seen how the asynchronous training procedure is implemented. Conceptually,
    asynchronous training consists of individual workers running gradient descent
    on locally simulated game rollouts and contributing learned knowledge back to
    a global set of weights periodically. Continuing our object-oriented design, let’s
    introduce the `Worker` class.
  prefs: []
  type: TYPE_NORMAL
- en: Each `Worker` instance holds a copy of the model that’s trained asynchronously
    on a separate thread ([Example 8-26](#ch8-worker)). Note that `a3c.build_graph()`
    is used to construct a local copy of the TensorFlow computation graph for the
    thread in question. Take special note of `local_vars` and `global_vars` here.
    We need to make sure to train only the variables associated with this worker’s
    copy of the policy and not with the global copy of the variables (which is used
    to share information across worker threads). As a result `gradients` uses `tf.gradients`
    to take gradients of the loss with respect to only `local_vars`.
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-26\. The Worker class implements the computation performed by each
    thread
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Worker rollouts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Each `Worker` is responsible for simulating game rollouts locally. The `create_rollout()`
    method uses `session.run` to fetch action probabilities from the TensorFlow graph
    ([Example 8-27](#ch8-workerrollout)). It then samples an action from this policy
    using `np.random.choice`, weighted by the per-class probabilities. The reward
    for the action taken is computed from `TicTacToeEnvironment` via a call to `self.env.step(action)`.
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-27\. The create_rollout method simulates a game rollout locally
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The `process_rollouts()` method does preprocessing needed to compute discounted
    rewards, values, actions, and advantages ([Example 8-28](#ch8-processrollout)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-28\. The process_rollout method computes rewards, values, actions,
    and advantages and then takes a gradient descent step against the loss
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The `Worker.run()` method performs the training step for the `Worker`, relying
    on `process_rollouts()` to issue the actual call to `self.a3c._session.run()`
    under the hood ([Example 8-29](#ch8-processrollout2)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-29\. The run() method is the top level invocation for Worker
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Training the Policy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `A3C.fit()` method brings together all the disparate pieces introduced to
    train the model. The `fit()` method takes the responsibility for spawning `Worker`
    threads using the Python `threading` library. Since each `Worker` takes responsibility
    for training itself, the `fit()` method simply is responsible for periodically
    checkpointing the trained model to disk. See [Example 8-30](#ch8-a3cfit).
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-30\. The fit() method brings everything together and runs the A3C
    training algorithm
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Challenge for the Reader
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We strongly encourage you to try training tic-tac-toe models for yourself! Note
    that this example is more involved than other examples in the book, and will require
    greater computational power. We recommend a machine with at least a few CPU cores.
    This requirement isn’t too onerous; a good laptop should suffice. Try using a
    tool like `htop` to check that the code is indeed multithreaded. See how good
    a model you can train! You should be able to beat the random baseline most of
    the time, but this basic implementation won’t give you a model that always wins.
    We recommend exploring the RL literature and expanding upon the base implementation
    to see how well you can do.
  prefs: []
  type: TYPE_NORMAL
- en: Review
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced you to the core concepts of reinforcement learning
    (RL). We walked you through some recent successes of RL methods on ATARI, upside-down
    helicopter flight, and computer Go. We then taught you about the mathematical
    framework of Markov decision processes. We brought it together with a detailed
    case study walking you through the construction of a tic-tac-toe agent. This algorithm
    uses a sophisticated training method, A3C, that makes use of multiple CPU cores
    to speed up training. In [Chapter 9](ch09.html#training_large_deep_networks),
    you’ll learn more about training models with multiple GPUs.
  prefs: []
  type: TYPE_NORMAL
