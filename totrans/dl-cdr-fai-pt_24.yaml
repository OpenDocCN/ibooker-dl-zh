- en: Chapter 19\. A fastai Learner from Scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This final chapter (other than the conclusion and the online chapters) is going
    to look a bit different. It contains far more code and far less prose than the
    previous chapters. We will introduce new Python keywords and libraries without
    discussing them. This chapter is meant to be the start of a significant research
    project for you. You see, we are going to implement many of the key pieces of
    the fastai and PyTorch APIs from scratch, building on nothing other than the components
    that we developed in [Chapter 17](ch17.xhtml#chapter_foundations)! The key goal
    here is to end up with your own `Learner` class and some callbacks—enough to be
    able to train a model on Imagenette, including examples of each of the key techniques
    we’ve studied. On the way to building `Learner`, we will create our own versions
    of `Module`, `Parameter` and a parallel `DataLoader` so you’ll have a very good
    idea of what those PyTorch classes do.
  prefs: []
  type: TYPE_NORMAL
- en: The end-of-chapter questionnaire is particularly important for this chapter.
    This is where we will be pointing you in the many interesting directions that
    you could take, using this chapter as your starting point. We suggest that you
    follow along with this chapter on your computer, and do lots of experiments, web
    searches, and whatever else you need to understand what’s going on. You’ve built
    up the skills and expertise to do this in the rest of this book, so we think you
    are going to do great!
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin by gathering (manually) some data.
  prefs: []
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Have a look at the source to `untar_data` to see how it works. We’ll use it
    here to access the 160-pixel version of Imagenette for use in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To access the image files, we can use `get_image_files`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Or we could do the same thing using just Python’s standard library, with `glob`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: If you look at the source for `get_image_files`, you’ll see it uses Python’s
    `os.walk`; this is a faster and more flexible function than `glob`, so be sure
    to try it out.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can open an image with the Python Imaging Library’s `Image` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/dlcf_19in01.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: That’s going to be the basis of our independent variable. For our dependent
    variable, we can use `Path.parent` from `pathlib`. First we’ll need our vocab
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'and the reverse mapping, thanks to `L.val2idx`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: That’s all the pieces we need to put together our `Dataset`.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A `Dataset` in PyTorch can be anything that supports indexing (`__getitem__`)
    and `len`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We need a list of training and validation filenames to pass to `Dataset.__init__`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can try it out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/dlcf_19in02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you see, our dataset is returning the independent and dependent variables
    as a tuple, which is just what we need. We’ll need to be able to collate these
    into a mini-batch. Generally, this is done with `torch.stack`, which is what we’ll
    use here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s a mini-batch with two items, for testing our `collate`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have a dataset and a collation function, we’re ready to create
    `DataLoader`. We’ll add two more things here: an optional `shuffle` for the training
    set, and a `ProcessPoolExecutor` to do our preprocessing in parallel. A parallel
    data loader is very important, because opening and decoding a JPEG image is a
    slow process. One CPU core is not enough to decode images fast enough to keep
    a modern GPU busy. Here’s our `DataLoader` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s try it out with our training and validation datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This data loader is not much slower than PyTorch’s, but it’s far simpler. So
    if you’re debugging a complex data loading process, don’t be afraid to try doing
    things manually to help you see exactly what’s going on.
  prefs: []
  type: TYPE_NORMAL
- en: 'For normalization, we’ll need image statistics. Generally, it’s fine to calculate
    these on a single training mini-batch, since precision isn’t needed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `Normalize` class just needs to store these stats and apply them (to see
    why the `to_device` is needed, try commenting it out, and see what happens later
    in this notebook):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We always like to test everything we build in a notebook, as soon as we build
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Here `tfm_x` isn’t just applying `Normalize`, but is also permuting the axis
    order from `NHWC` to `NCHW` (see [Chapter 13](ch13.xhtml#chapter_convolutions)
    if you need a reminder of what these acronyms refer to). PIL uses `HWC` axis order,
    which we can’t use with PyTorch, hence the need for this `permute`.
  prefs: []
  type: TYPE_NORMAL
- en: That’s all we need for the data for our model. So now we need the model itself!
  prefs: []
  type: TYPE_NORMAL
- en: Module and Parameter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To create a model, we’ll need `Module`. To create `Module`, we’ll need `Parameter`,
    so let’s start there. Recall that in [Chapter 8](ch08.xhtml#chapter_collab) we
    said that the `Parameter` class “doesn’t add any functionality (other than automatically
    calling `requires_grad_` for us). It’s used only as a ‘marker’ to show what to
    include in `parameters`.” Here’s a definition that does exactly that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The implementation here is a bit awkward: we have to define the special `__new__`
    Python method and use the internal PyTorch method `_make_subclass` because, at
    the time of writing, PyTorch doesn’t otherwise work correctly with this kind of
    subclassing or provide an officially supported API to do this. This may have been
    fixed by the time you read this, so look on the book’s website to see if there
    are updated details.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our `Parameter` now behaves just like a tensor, as we wanted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have this, we can define `Module`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The key functionality is in the definition of `parameters`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This means that we can ask any `Module` for its parameters, and it will return
    them, including for all its child modules (recursively). But how does it know
    what its parameters are? It’s thanks to implementing Python’s special `__setattr__`
    method, which is called for us anytime Python sets an attribute on a class. Our
    implementation includes this line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: As you see, this is where we use our new `Parameter` class as a “marker”—anything
    of this class is added to our `params`.
  prefs: []
  type: TYPE_NORMAL
- en: Python’s `__call__` allows us to define what happens when our object is treated
    as a function; we just call `forward` (which doesn’t exist here, so it’ll need
    to be added by subclasses). Before we do, we’ll call a hook, if it’s defined.
    Now you can see that PyTorch hooks aren’t doing anything fancy at all—they’re
    just calling any hooks have been registered.
  prefs: []
  type: TYPE_NORMAL
- en: Other than these pieces of functionality, our `Module` also provides `cuda`
    and `training` attributes, which we’ll use shortly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can create our first `Module`, which is `ConvLayer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We’re not implementing `F.conv2d` from scratch, since you should have already
    done that (using `unfold`) in the questionnaire in [Chapter 17](ch17.xhtml#chapter_foundations).
    Instead we’re just creating a small class that wraps it up along with bias and
    weight initialization. Let’s check that it works correctly with `Module.parameters`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'And that we can call it (which will result in `forward` being called):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'In the same way, we can implement `Linear`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'And test that it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s also create a testing module to check that if we include multiple parameters
    as attributes, they are all correctly registered:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we have a conv layer and a linear layer, each of which has weights and
    biases, we’d expect four parameters in total:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'We should also find that calling `cuda` on this class puts all these parameters
    on the GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: We can now use those pieces to create a CNN.
  prefs: []
  type: TYPE_NORMAL
- en: Simple CNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we’ve seen, a `Sequential` class makes many architectures easier to implement,
    so let’s make one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The `forward` method here just calls each layer in turn. Note that we have to
    use the `register_modules` method we defined in `Module`, since otherwise the
    contents of `layers` won’t appear in `parameters`.
  prefs: []
  type: TYPE_NORMAL
- en: All the Code Is Here
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Remember that we’re not using any PyTorch functionality for modules here; we’re
    defining everything ourselves. So if you’re not sure what `register_modules` does,
    or why it’s needed, have another look at our code for `Module` to see what we
    wrote!
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a simplified `AdaptivePool` that only handles pooling to a 1×1
    output, and flattens it as well, by just using `mean`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: That’s enough for us to create a CNN!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see if our parameters are all being registered correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can try adding a hook. Note that we’ve left room for only one hook in
    `Module`; you could make it a list, or use something like `Pipeline` to run a
    few as a single function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: We have data and model. Now we need a loss function.
  prefs: []
  type: TYPE_NORMAL
- en: Loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We’ve already seen how to define “negative log likelihood”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Well actually, there’s no log here, since we’re using the same definition as
    PyTorch. That means we need to put the log together with softmax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Combining these gives us our cross-entropy loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Note that the formula
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="log left-parenthesis StartFraction a Over b EndFraction right-parenthesis
    equals log left-parenthesis a right-parenthesis minus log left-parenthesis b right-parenthesis"
    display="block"><mrow><mo form="prefix">log</mo> <mfenced separators="" open="("
    close=")"><mfrac><mi>a</mi> <mi>b</mi></mfrac></mfenced> <mo>=</mo> <mo form="prefix">log</mo>
    <mrow><mo>(</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>-</mo> <mo form="prefix">log</mo>
    <mrow><mo>(</mo> <mi>b</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'gives a simplification when we compute the log softmax, which was previously
    defined as `(x.exp()/(x.exp().sum(-1))).log()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Then, there is a more stable way to compute the log of the sum of exponentials,
    called the [*LogSumExp* trick](https://oreil.ly/9UB0b). The idea is to use the
    following formula
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="log left-parenthesis sigma-summation Underscript j equals 1 Overscript
    n Endscripts e Superscript x Super Subscript j Superscript Baseline right-parenthesis
    equals log left-parenthesis e Superscript a Baseline sigma-summation Underscript
    j equals 1 Overscript n Endscripts e Superscript x Super Subscript j Superscript
    minus a Baseline right-parenthesis equals a plus log left-parenthesis sigma-summation
    Underscript j equals 1 Overscript n Endscripts e Superscript x Super Subscript
    j Superscript minus a Baseline right-parenthesis" display="block"><mrow><mo form="prefix">log</mo>
    <mfenced separators="" open="(" close=")"><munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <msup><mi>e</mi> <msub><mi>x</mi> <mi>j</mi></msub></msup></mfenced>
    <mo>=</mo> <mo form="prefix">log</mo> <mfenced separators="" open="(" close=")"><msup><mi>e</mi>
    <mi>a</mi></msup> <munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <msup><mi>e</mi> <mrow><msub><mi>x</mi> <mi>j</mi></msub>
    <mo>-</mo><mi>a</mi></mrow></msup></mfenced> <mo>=</mo> <mi>a</mi> <mo>+</mo>
    <mo form="prefix">log</mo> <mfenced separators="" open="(" close=")"><munderover><mo>∑</mo>
    <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover> <msup><mi>e</mi>
    <mrow><msub><mi>x</mi> <mi>j</mi></msub> <mo>-</mo><mi>a</mi></mrow></msup></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where *a* is the maximum of <math alttext="x Subscript j"><msub><mi>x</mi> <mi>j</mi></msub></math>
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the same thing in code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: We’ll put that into a function
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'so we can use it for our `log_softmax` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Which gives the same result as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use these to create `cross_entropy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Let’s now combine all those pieces to create a `Learner`.
  prefs: []
  type: TYPE_NORMAL
- en: Learner
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have data, a model, and a loss function; we need only one more thing before
    we can fit a model, and that’s an optimizer! Here’s SGD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'As we’ve seen in this book, life is easier with a `Learner`. The `Learner`
    needs to know our training and validation sets, which means we need `DataLoaders`
    to store them. We don’t need any other functionality, just a place to store them
    and access them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we’re ready to create our `Learner` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: This is the largest class we’ve created in the book, but each method is quite
    small, so by looking at each in turn, you should be able to follow what’s going
    on.
  prefs: []
  type: TYPE_NORMAL
- en: The main method we’ll be calling is `fit`. This loops with
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: and at each epoch calls `self.one_epoch` for each of `train=True` and then `train=False`.
    Then `self.one_epoch` calls `self.one_batch` for each batch in `dls.train` or
    `dls.valid`, as appropriate (after wrapping the `DataLoader` in `fastprogress.progress_bar`).
    Finally, `self.one_batch` follows the usual set of steps to fit one mini-batch
    that we’ve seen throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: Before and after each step, `Learner` calls `self`, which calls `__call__` (which
    is standard Python functionality). `__call__` uses `getattr(cb,name)` on each
    callback in `self.cbs`, which is a Python built-in function that returns the attribute
    (a method, in this case) with the requested name. So, for instance, `self('before_fit')`
    will call `cb.before_fit()` for each callback where that method is defined.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, `Learner` is really just using our standard training loop, except
    that it’s also calling callbacks at appropriate times. So let’s define some callbacks!
  prefs: []
  type: TYPE_NORMAL
- en: Callbacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In `Learner.__init__` we have
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'In other words, every callback knows what learner it is used in. This is critical,
    since otherwise a callback can’t get information from the learner, or change things
    in the learner. Because getting information from the learner is so common, we
    make that easier by defining `Callback` as a subclass of `GetAttr`, with a default
    attribute of `learner`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '`GetAttr` is a fastai class that implements Python’s standard `__getattr__`
    and `__dir__` methods for you, so that anytime you try to access an attribute
    that doesn’t exist, it passes the request along to whatever you have defined as
    `_default`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, we want to move all model parameters to the GPU automatically
    at the start of `fit`. We could do this by defining `before_fit` as `self.learner.model.cuda`;
    however, because `learner` is the default attribute, and we have `SetupLearnerCB`
    inherit from `Callback` (which inherits from `GetAttr`), we can remove the `.learner`
    and just call `self.model.cuda`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: In `SetupLearnerCB`, we also move each mini-batch to the GPU, by calling `to_device(self.batch)`
    (we could also have used the longer `to_device(self.learner.batch)`. Note, however,
    that in the line `self.learner.batch = tfm_x(xb),yb`, we can’t remove `.learner`,
    because here we’re *setting* the attribute, not getting it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we try our `Learner`, let’s create a callback to track and print progress.
    Otherwise, we won’t really know if it’s working properly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: Now we’re ready to use our `Learner` for the first time!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: It’s quite amazing to realize that we can implement all the key ideas from fastai’s
    `Learner` in so little code! Let’s now add some learning rate scheduling.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling the Learning Rate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If we’re going to get good results, we’ll want an LR finder and 1cycle training.
    These are both *annealing* callbacks—that is, they are gradually changing hyperparameters
    as we train. Here’s `LRFinder`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'This shows how we’re using `CancelFitException`, which is itself an empty class,
    used only to signify the type of exception. You can see in `Learner` that this
    exception is caught. (You should add and test `CancelBatchException`, `CancelEpochException`,
    etc. yourself.) Let’s try it out, by adding it to our list of callbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'And take a look at the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/dlcf_19in03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we can define our `OneCycle` training callback:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll try an LR of 0.1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s fit for a while and see how it looks (we won’t show all the output in
    the book—try it in the notebook to see the results):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we’ll check that the learning rate followed the schedule we defined
    (as you see, we’re not using cosine annealing here):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/dlcf_19in04.png)'
  prefs: []
  type: TYPE_IMG
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have explored how the key concepts of the fastai library are implemented
    by re-implementing them in this chapter. Since it’s mostly full of code, you should
    definitely try to experiment with it by looking at the corresponding notebook
    on the book’s website. Now that you know how it’s built, as a next step be sure
    to check out the intermediate and advanced tutorials in the fastai documentation
    to learn how to customize every bit of the library.
  prefs: []
  type: TYPE_NORMAL
- en: Questionnaire
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Experiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the questions here that ask you to explain what a function or class is,
    you should also complete your own code experiments.
  prefs: []
  type: TYPE_NORMAL
- en: What is `glob`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you open an image with the Python imaging library?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does `L.map` do?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does `Self` do?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is `L.val2idx`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What methods do you need to implement to create your own `Dataset`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we call `convert` when we open an image from Imagenette?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does `~` do? How is it useful for splitting training and validation sets?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does `~` work with the `L` or `Tensor` classes? How about NumPy arrays, Python
    lists, or Pandas DataFrames?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is `ProcessPoolExecutor`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does `L.range(self.ds)` work?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is `__iter__`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is `first`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is `permute`? Why is it needed?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a recursive function? How does it help us define the `parameters` method?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a recursive function that returns the first 20 items of the Fibonacci
    sequence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is `super`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do subclasses of `Module` need to override `forward` instead of defining
    `__call__`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In `ConvLayer`, why does `init` depend on `act`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why does `Sequential` need to call `register_modules`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a hook that prints the shape of every layer’s activations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is LogSumExp?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is `log_softmax` useful?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is `GetAttr`? How is it helpful for callbacks?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reimplement one of the callbacks in this chapter without inheriting from `Callback`
    or `GetAttr`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does `Learner.__call__` do?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is `getattr`? (Note the case difference from `GetAttr`!)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is there a `try` block in `fit`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we check for `model.training` in `one_batch`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is `store_attr`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the purpose of `TrackResults.before_epoch`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does `model.cuda` do? How does it work?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we need to check `model.training` in `LRFinder` and `OneCycle`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use cosine annealing in `OneCycle`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further Research
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Write `resnet18` from scratch (refer to [Chapter 14](ch14.xhtml#chapter_resnet)
    as needed), and train it with the `Learner` in this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement a batchnorm layer from scratch and use it in your `resnet18`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a Mixup callback for use in this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add momentum to SGD.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pick a few features that you’re interested in from fastai (or any other library)
    and implement them with the objects created in this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pick a research paper that’s not yet implemented in fastai or PyTorch and do
    so with the objects you created in this chapter. Then:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Port the paper over to fastai.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Submit a pull request to fastai, or create your own extension module and release
    it.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hint: you may find it helpful to use [`nbdev`](https://nbdev.fast.ai) to create
    and deploy your package.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
