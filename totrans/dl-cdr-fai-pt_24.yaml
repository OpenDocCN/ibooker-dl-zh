- en: Chapter 19\. A fastai Learner from Scratch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第19章。从头开始创建一个fastai学习器
- en: This final chapter (other than the conclusion and the online chapters) is going
    to look a bit different. It contains far more code and far less prose than the
    previous chapters. We will introduce new Python keywords and libraries without
    discussing them. This chapter is meant to be the start of a significant research
    project for you. You see, we are going to implement many of the key pieces of
    the fastai and PyTorch APIs from scratch, building on nothing other than the components
    that we developed in [Chapter 17](ch17.xhtml#chapter_foundations)! The key goal
    here is to end up with your own `Learner` class and some callbacks—enough to be
    able to train a model on Imagenette, including examples of each of the key techniques
    we’ve studied. On the way to building `Learner`, we will create our own versions
    of `Module`, `Parameter` and a parallel `DataLoader` so you’ll have a very good
    idea of what those PyTorch classes do.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 这最后一章（除了结论和在线章节）将会有所不同。它包含的代码比以前的章节要多得多，而叙述要少得多。我们将介绍新的Python关键字和库，而不进行讨论。这一章的目的是为您开展一项重要的研究项目。您将看到，我们将从头开始实现fastai和PyTorch
    API的许多关键部分，仅建立在我们在[第17章](ch17.xhtml#chapter_foundations)中开发的组件上！这里的关键目标是最终拥有自己的`Learner`类和一些回调函数，足以训练一个模型在Imagenette上，包括我们学习的每个关键技术的示例。在构建`Learner`的过程中，我们将创建我们自己的`Module`、`Parameter`和并行`DataLoader`的版本，这样您就会对PyTorch类的功能有一个很好的了解。
- en: The end-of-chapter questionnaire is particularly important for this chapter.
    This is where we will be pointing you in the many interesting directions that
    you could take, using this chapter as your starting point. We suggest that you
    follow along with this chapter on your computer, and do lots of experiments, web
    searches, and whatever else you need to understand what’s going on. You’ve built
    up the skills and expertise to do this in the rest of this book, so we think you
    are going to do great!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章末尾的问卷调查对于本章非常重要。这是我们将指导您探索许多有趣方向的地方，使用本章作为起点。我们建议您在计算机上跟着本章进行学习，并进行大量的实验、网络搜索和其他必要的工作，以了解发生了什么。在本书的其余部分，您已经积累了足够的技能和专业知识来做到这一点，所以我们相信您会做得很好！
- en: Let’s begin by gathering (manually) some data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始手动收集一些数据。
- en: Data
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据
- en: 'Have a look at the source to `untar_data` to see how it works. We’ll use it
    here to access the 160-pixel version of Imagenette for use in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 查看`untar_data`的源代码，看看它是如何工作的。我们将在这里使用它来访问Imagene的160像素版本，以在本章中使用：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To access the image files, we can use `get_image_files`:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问图像文件，我们可以使用`get_image_files`：
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Or we could do the same thing using just Python’s standard library, with `glob`:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 或者我们可以使用Python的标准库`glob`来做同样的事情：
- en: '[PRE3]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If you look at the source for `get_image_files`, you’ll see it uses Python’s
    `os.walk`; this is a faster and more flexible function than `glob`, so be sure
    to try it out.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您查看`get_image_files`的源代码，您会发现它使用了Python的`os.walk`；这是一个比`glob`更快、更灵活的函数，所以一定要尝试一下。
- en: 'We can open an image with the Python Imaging Library’s `Image` class:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用Python Imaging Library的`Image`类打开一张图片：
- en: '[PRE5]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](Images/dlcf_19in01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_19in01.png)'
- en: '[PRE6]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: That’s going to be the basis of our independent variable. For our dependent
    variable, we can use `Path.parent` from `pathlib`. First we’ll need our vocab
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这将成为我们独立变量的基础。对于我们的因变量，我们可以使用`pathlib`中的`Path.parent`。首先，我们需要我们的词汇表
- en: '[PRE8]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'and the reverse mapping, thanks to `L.val2idx`:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以及反向映射，感谢`L.val2idx`：
- en: '[PRE10]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: That’s all the pieces we need to put together our `Dataset`.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们需要组合成`Dataset`的所有部分。
- en: Dataset
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: 'A `Dataset` in PyTorch can be anything that supports indexing (`__getitem__`)
    and `len`:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中，`Dataset`可以是任何支持索引（`__getitem__`）和`len`的东西：
- en: '[PRE12]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We need a list of training and validation filenames to pass to `Dataset.__init__`:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个训练和验证文件名列表传递给`Dataset.__init__`：
- en: '[PRE13]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now we can try it out:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以尝试一下：
- en: '[PRE15]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](Images/dlcf_19in02.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_19in02.png)'
- en: 'As you see, our dataset is returning the independent and dependent variables
    as a tuple, which is just what we need. We’ll need to be able to collate these
    into a mini-batch. Generally, this is done with `torch.stack`, which is what we’ll
    use here:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，我们的数据集返回独立变量和因变量作为元组，这正是我们需要的。我们需要将这些整合成一个小批量。通常，可以使用`torch.stack`来完成这个任务，这就是我们将在这里使用的方法：
- en: '[PRE18]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here’s a mini-batch with two items, for testing our `collate`:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个包含两个项目的小批量，用于测试我们的`collate`：
- en: '[PRE19]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now that we have a dataset and a collation function, we’re ready to create
    `DataLoader`. We’ll add two more things here: an optional `shuffle` for the training
    set, and a `ProcessPoolExecutor` to do our preprocessing in parallel. A parallel
    data loader is very important, because opening and decoding a JPEG image is a
    slow process. One CPU core is not enough to decode images fast enough to keep
    a modern GPU busy. Here’s our `DataLoader` class:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了数据集和一个整合函数，我们准备创建`DataLoader`。我们将在这里添加两个东西：一个可选的`shuffle`用于训练集，以及一个`ProcessPoolExecutor`来并行进行预处理。并行数据加载器非常重要，因为打开和解码JPEG图像是一个缓慢的过程。一个CPU核心不足以快速解码图像以使现代GPU保持繁忙。这是我们的`DataLoader`类：
- en: '[PRE21]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Let’s try it out with our training and validation datasets:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一下我们的训练和验证数据集：
- en: '[PRE22]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This data loader is not much slower than PyTorch’s, but it’s far simpler. So
    if you’re debugging a complex data loading process, don’t be afraid to try doing
    things manually to help you see exactly what’s going on.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据加载器的速度不比PyTorch的慢，但它要简单得多。因此，如果您正在调试一个复杂的数据加载过程，不要害怕尝试手动操作，以帮助您准确地了解发生了什么。
- en: 'For normalization, we’ll need image statistics. Generally, it’s fine to calculate
    these on a single training mini-batch, since precision isn’t needed here:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于归一化，我们需要图像统计数据。通常，可以在一个训练小批量上计算这些数据，因为这里不需要精度：
- en: '[PRE24]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Our `Normalize` class just needs to store these stats and apply them (to see
    why the `to_device` is needed, try commenting it out, and see what happens later
    in this notebook):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`Normalize`类只需要存储这些统计数据并应用它们（要查看为什么需要`to_device`，请尝试将其注释掉，然后查看后面的笔记本中会发生什么）：
- en: '[PRE26]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We always like to test everything we build in a notebook, as soon as we build
    it:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总是喜欢在笔记本中测试我们构建的一切，一旦我们构建它：
- en: '[PRE27]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Here `tfm_x` isn’t just applying `Normalize`, but is also permuting the axis
    order from `NHWC` to `NCHW` (see [Chapter 13](ch13.xhtml#chapter_convolutions)
    if you need a reminder of what these acronyms refer to). PIL uses `HWC` axis order,
    which we can’t use with PyTorch, hence the need for this `permute`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这里`tfm_x`不仅仅应用`Normalize`，还将轴顺序从`NHWC`排列为`NCHW`（如果你需要提醒这些首字母缩写指的是什么，请参阅[第13章](ch13.xhtml#chapter_convolutions)）。PIL使用`HWC`轴顺序，我们不能在PyTorch中使用，因此需要这个`permute`。
- en: That’s all we need for the data for our model. So now we need the model itself!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们模型的数据所需的全部内容。现在我们需要模型本身！
- en: Module and Parameter
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Module和Parameter
- en: 'To create a model, we’ll need `Module`. To create `Module`, we’ll need `Parameter`,
    so let’s start there. Recall that in [Chapter 8](ch08.xhtml#chapter_collab) we
    said that the `Parameter` class “doesn’t add any functionality (other than automatically
    calling `requires_grad_` for us). It’s used only as a ‘marker’ to show what to
    include in `parameters`.” Here’s a definition that does exactly that:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个模型，我们需要`Module`。要创建`Module`，我们需要`Parameter`，所以让我们从那里开始。回想一下，在[第8章](ch08.xhtml#chapter_collab)中我们说`Parameter`类“没有添加任何功能（除了自动调用`requires_grad_`）。它只用作一个‘标记’，以显示要包含在`parameters`中的内容。”这里有一个确切的定义：
- en: '[PRE30]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The implementation here is a bit awkward: we have to define the special `__new__`
    Python method and use the internal PyTorch method `_make_subclass` because, at
    the time of writing, PyTorch doesn’t otherwise work correctly with this kind of
    subclassing or provide an officially supported API to do this. This may have been
    fixed by the time you read this, so look on the book’s website to see if there
    are updated details.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的实现有点尴尬：我们必须定义特殊的`__new__` Python方法，并使用内部的PyTorch方法`_make_subclass`，因为在撰写本文时，PyTorch否则无法正确处理这种子类化或提供官方支持的API来执行此操作。也许在你阅读本文时，这个问题已经得到解决，所以请查看本书网站以获取更新的详细信息。
- en: 'Our `Parameter` now behaves just like a tensor, as we wanted:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`Parameter`现在表现得就像一个张量，正如我们所希望的：
- en: '[PRE31]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now that we have this, we can define `Module`:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了这个，我们可以定义`Module`：
- en: '[PRE33]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The key functionality is in the definition of `parameters`:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 关键功能在`parameters`的定义中：
- en: '[PRE34]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This means that we can ask any `Module` for its parameters, and it will return
    them, including for all its child modules (recursively). But how does it know
    what its parameters are? It’s thanks to implementing Python’s special `__setattr__`
    method, which is called for us anytime Python sets an attribute on a class. Our
    implementation includes this line:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们可以询问任何`Module`的参数，并且它将返回它们，包括所有子模块（递归地）。但是它是如何知道它的参数是什么的呢？这要归功于实现Python的特殊`__setattr__`方法，每当Python在类上设置属性时，它就会为我们调用。我们的实现包括这一行：
- en: '[PRE35]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: As you see, this is where we use our new `Parameter` class as a “marker”—anything
    of this class is added to our `params`.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，这是我们将新的`Parameter`类用作“标记”的地方——任何属于这个类的东西都会被添加到我们的`params`中。
- en: Python’s `__call__` allows us to define what happens when our object is treated
    as a function; we just call `forward` (which doesn’t exist here, so it’ll need
    to be added by subclasses). Before we do, we’ll call a hook, if it’s defined.
    Now you can see that PyTorch hooks aren’t doing anything fancy at all—they’re
    just calling any hooks have been registered.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Python的`__call__`允许我们定义当我们的对象被视为函数时会发生什么；我们只需调用`forward`（这里不存在，所以子类需要添加）。在我们这样做之前，如果定义了钩子，我们将调用一个钩子。现在你可以看到PyTorch的钩子并没有做任何花哨的事情——它们只是调用任何已注册的钩子。
- en: Other than these pieces of functionality, our `Module` also provides `cuda`
    and `training` attributes, which we’ll use shortly.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些功能之外，我们的`Module`还提供了`cuda`和`training`属性，我们很快会用到。
- en: 'Now we can create our first `Module`, which is `ConvLayer`:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以创建我们的第一个`Module`，即`ConvLayer`：
- en: '[PRE36]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We’re not implementing `F.conv2d` from scratch, since you should have already
    done that (using `unfold`) in the questionnaire in [Chapter 17](ch17.xhtml#chapter_foundations).
    Instead we’re just creating a small class that wraps it up along with bias and
    weight initialization. Let’s check that it works correctly with `Module.parameters`:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是从头开始实现`F.conv2d`，因为你应该已经在[第17章](ch17.xhtml#chapter_foundations)的问卷中使用`unfold`完成了这个任务。相反，我们只是创建了一个小类，将它与偏置和权重初始化一起包装起来。让我们检查它是否与`Module.parameters`正确工作：
- en: '[PRE37]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'And that we can call it (which will result in `forward` being called):'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 并且我们可以调用它（这将导致`forward`被调用）：
- en: '[PRE39]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'In the same way, we can implement `Linear`:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以实现`Linear`：
- en: '[PRE41]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'And test that it works:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 测试一下是否有效：
- en: '[PRE42]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Let’s also create a testing module to check that if we include multiple parameters
    as attributes, they are all correctly registered:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也创建一个测试模块来检查，如果我们将多个参数作为属性包含，它们是否都被正确注册：
- en: '[PRE44]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Since we have a conv layer and a linear layer, each of which has weights and
    biases, we’d expect four parameters in total:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有一个卷积层和一个线性层，每个层都有权重和偏置，我们期望总共有四个参数：
- en: '[PRE45]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We should also find that calling `cuda` on this class puts all these parameters
    on the GPU:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应该发现，在这个类上调用`cuda`会将所有这些参数放在GPU上：
- en: '[PRE47]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: We can now use those pieces to create a CNN.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用这些部分来创建一个CNN。
- en: Simple CNN
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单的CNN
- en: 'As we’ve seen, a `Sequential` class makes many architectures easier to implement,
    so let’s make one:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，`Sequential`类使许多架构更容易实现，所以让我们创建一个：
- en: '[PRE49]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The `forward` method here just calls each layer in turn. Note that we have to
    use the `register_modules` method we defined in `Module`, since otherwise the
    contents of `layers` won’t appear in `parameters`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的`forward`方法只是依次调用每个层。请注意，我们必须使用我们在`Module`中定义的`register_modules`方法，否则`layers`的内容不会出现在`parameters`中。
- en: All the Code Is Here
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 所有的代码都在这里
- en: Remember that we’re not using any PyTorch functionality for modules here; we’re
    defining everything ourselves. So if you’re not sure what `register_modules` does,
    or why it’s needed, have another look at our code for `Module` to see what we
    wrote!
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，我们在这里没有使用任何PyTorch模块的功能；我们正在自己定义一切。所以如果你不确定`register_modules`做什么，或者为什么需要它，再看看我们为`Module`编写的代码！
- en: 'We can create a simplified `AdaptivePool` that only handles pooling to a 1×1
    output, and flattens it as well, by just using `mean`:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以创建一个简化的`AdaptivePool`，它只处理到1×1输出的池化，并且也将其展平，只需使用`mean`：
- en: '[PRE50]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: That’s enough for us to create a CNN!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这就足够我们创建一个CNN了！
- en: '[PRE51]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Let’s see if our parameters are all being registered correctly:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的参数是否都被正确注册了：
- en: '[PRE52]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Now we can try adding a hook. Note that we’ve left room for only one hook in
    `Module`; you could make it a list, or use something like `Pipeline` to run a
    few as a single function:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以尝试添加一个钩子。请注意，我们在`Module`中只留了一个钩子的空间；您可以将其变成列表，或者使用类似`Pipeline`的东西将几个钩子作为单个函数运行：
- en: '[PRE54]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: We have data and model. Now we need a loss function.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有数据和模型。现在我们需要一个损失函数。
- en: Loss
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失
- en: 'We’ve already seen how to define “negative log likelihood”:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何定义“负对数似然”：
- en: '[PRE56]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Well actually, there’s no log here, since we’re using the same definition as
    PyTorch. That means we need to put the log together with softmax:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这里没有对数，因为我们使用与PyTorch相同的定义。这意味着我们需要将对数与softmax放在一起：
- en: '[PRE57]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Combining these gives us our cross-entropy loss:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些结合起来就得到了我们的交叉熵损失：
- en: '[PRE59]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Note that the formula
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意公式
- en: <math alttext="log left-parenthesis StartFraction a Over b EndFraction right-parenthesis
    equals log left-parenthesis a right-parenthesis minus log left-parenthesis b right-parenthesis"
    display="block"><mrow><mo form="prefix">log</mo> <mfenced separators="" open="("
    close=")"><mfrac><mi>a</mi> <mi>b</mi></mfrac></mfenced> <mo>=</mo> <mo form="prefix">log</mo>
    <mrow><mo>(</mo> <mi>a</mi> <mo>)</mo></mrow> <mo>-</mo> <mo form="prefix">log</mo>
    <mrow><mo>(</mo> <mi>b</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="log左括号分数a除以b右括号等于log左括号a右括号减去log左括号b右括号" display="block"><mrow><mo
    form="prefix">log</mo> <mfenced separators="" open="(" close=")"><mfrac><mi>a</mi>
    <mi>b</mi></mfrac></mfenced> <mo>=</mo> <mo form="prefix">log</mo> <mrow><mo>(</mo>
    <mi>a</mi> <mo>)</mo></mrow> <mo>-</mo> <mo form="prefix">log</mo> <mrow><mo>(</mo>
    <mi>b</mi> <mo>)</mo></mrow></mrow></math>
- en: 'gives a simplification when we compute the log softmax, which was previously
    defined as `(x.exp()/(x.exp().sum(-1))).log()`:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算对数softmax时，这给出了一个简化，之前定义为`(x.exp()/(x.exp().sum(-1))).log()`：
- en: '[PRE61]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Then, there is a more stable way to compute the log of the sum of exponentials,
    called the [*LogSumExp* trick](https://oreil.ly/9UB0b). The idea is to use the
    following formula
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，有一种更稳定的计算指数和的对数的方法，称为[*LogSumExp*技巧](https://oreil.ly/9UB0b)。这个想法是使用以下公式
- en: <math alttext="log left-parenthesis sigma-summation Underscript j equals 1 Overscript
    n Endscripts e Superscript x Super Subscript j Superscript Baseline right-parenthesis
    equals log left-parenthesis e Superscript a Baseline sigma-summation Underscript
    j equals 1 Overscript n Endscripts e Superscript x Super Subscript j Superscript
    minus a Baseline right-parenthesis equals a plus log left-parenthesis sigma-summation
    Underscript j equals 1 Overscript n Endscripts e Superscript x Super Subscript
    j Superscript minus a Baseline right-parenthesis" display="block"><mrow><mo form="prefix">log</mo>
    <mfenced separators="" open="(" close=")"><munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <msup><mi>e</mi> <msub><mi>x</mi> <mi>j</mi></msub></msup></mfenced>
    <mo>=</mo> <mo form="prefix">log</mo> <mfenced separators="" open="(" close=")"><msup><mi>e</mi>
    <mi>a</mi></msup> <munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <msup><mi>e</mi> <mrow><msub><mi>x</mi> <mi>j</mi></msub>
    <mo>-</mo><mi>a</mi></mrow></msup></mfenced> <mo>=</mo> <mi>a</mi> <mo>+</mo>
    <mo form="prefix">log</mo> <mfenced separators="" open="(" close=")"><munderover><mo>∑</mo>
    <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover> <msup><mi>e</mi>
    <mrow><msub><mi>x</mi> <mi>j</mi></msub> <mo>-</mo><mi>a</mi></mrow></msup></mfenced></mrow></math>
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="log左括号sigma-求和下标j等于1上标n上标e上标x上标j下标基准右括号等于log左括号e上标a基准sigma-求和下标j等于1上标n上标e上标x上标j下标减a基准右括号等于a加log左括号sigma-求和下标j等于1上标n上标e上标x上标j下标减a基准右括号"
    display="block"><mrow><mo form="prefix">log</mo> <mfenced separators="" open="("
    close=")"><munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover>
    <msup><mi>e</mi> <msub><mi>x</mi> <mi>j</mi></msub></msup></mfenced> <mo>=</mo>
    <mo form="prefix">log</mo> <mfenced separators="" open="(" close=")"><msup><mi>e</mi>
    <mi>a</mi></msup> <munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <msup><mi>e</mi> <mrow><msub><mi>x</mi> <mi>j</mi></msub>
    <mo>-</mo><mi>a</mi></mrow></msup></mfenced> <mo>=</mo> <mi>a</mi> <mo>+</mo>
    <mo form="prefix">log</mo> <mfenced separators="" open="(" close=")"><munderover><mo>∑</mo>
    <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover> <msup><mi>e</mi>
    <mrow><msub><mi>x</mi> <mi>j</mi></msub> <mo>-</mo><mi>a</mi></mrow></msup></mfenced></mrow></math>
- en: where *a* is the maximum of <math alttext="x Subscript j"><msub><mi>x</mi> <mi>j</mi></msub></math>
    .
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*a*是<math alttext="x下标j"><msub><mi>x</mi> <mi>j</mi></msub></math>的最大值。
- en: 'Here’s the same thing in code:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是相同的代码：
- en: '[PRE63]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: We’ll put that into a function
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其放入一个函数中
- en: '[PRE65]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'so we can use it for our `log_softmax` function:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 因此我们可以将其用于我们的`log_softmax`函数：
- en: '[PRE67]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Which gives the same result as before:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这与之前得到的结果相同：
- en: '[PRE68]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'We can use these to create `cross_entropy`:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这些来创建`交叉熵`：
- en: '[PRE70]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Let’s now combine all those pieces to create a `Learner`.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将所有这些部分组合起来创建一个`学习者`。
- en: Learner
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习者
- en: 'We have data, a model, and a loss function; we need only one more thing before
    we can fit a model, and that’s an optimizer! Here’s SGD:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有数据、模型和损失函数；在我们可以拟合模型之前，我们只需要另一件事，那就是优化器！这里是SGD：
- en: '[PRE71]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'As we’ve seen in this book, life is easier with a `Learner`. The `Learner`
    needs to know our training and validation sets, which means we need `DataLoaders`
    to store them. We don’t need any other functionality, just a place to store them
    and access them:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本书中所看到的，有了`学习者`生活就变得更容易了。`学习者`需要知道我们的训练和验证集，这意味着我们需要`DataLoaders`来存储它们。我们不需要任何其他功能，只需要一个地方来存储它们并访问它们：
- en: '[PRE72]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Now we’re ready to create our `Learner` class:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备创建我们的`学习者`类：
- en: '[PRE73]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: This is the largest class we’ve created in the book, but each method is quite
    small, so by looking at each in turn, you should be able to follow what’s going
    on.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们在本书中创建的最大的类，但每个方法都非常小，所以通过依次查看每个方法，您应该能够理解发生了什么。
- en: The main method we’ll be calling is `fit`. This loops with
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将调用的主要方法是`fit`。这个循环
- en: '[PRE75]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: and at each epoch calls `self.one_epoch` for each of `train=True` and then `train=False`.
    Then `self.one_epoch` calls `self.one_batch` for each batch in `dls.train` or
    `dls.valid`, as appropriate (after wrapping the `DataLoader` in `fastprogress.progress_bar`).
    Finally, `self.one_batch` follows the usual set of steps to fit one mini-batch
    that we’ve seen throughout this book.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 并在每个epoch中分别调用`self.one_epoch`，然后`train=True`，然后`train=False`。然后`self.one_epoch`对`dls.train`或`dls.valid`中的每个批次调用`self.one_batch`，适当地（在将`DataLoader`包装在`fastprogress.progress_bar`之后）。最后，`self.one_batch`遵循我们在本书中看到的适合一个小批量的一系列步骤。
- en: Before and after each step, `Learner` calls `self`, which calls `__call__` (which
    is standard Python functionality). `__call__` uses `getattr(cb,name)` on each
    callback in `self.cbs`, which is a Python built-in function that returns the attribute
    (a method, in this case) with the requested name. So, for instance, `self('before_fit')`
    will call `cb.before_fit()` for each callback where that method is defined.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个步骤之前和之后，`Learner`调用`self`，`self`调用`__call__`（这是标准的Python功能）。`__call__`在`self.cbs`中的每个回调上使用`getattr(cb,name)`，这是Python的内置函数，返回具有请求名称的属性（在本例中是一个方法）。因此，例如，`self('before_fit')`将为每个定义了该方法的回调调用`cb.before_fit()`。
- en: As you can see, `Learner` is really just using our standard training loop, except
    that it’s also calling callbacks at appropriate times. So let’s define some callbacks!
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，`Learner`实际上只是使用了我们的标准训练循环，只是在适当的时候还调用了回调。所以让我们定义一些回调！
- en: Callbacks
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回调
- en: In `Learner.__init__` we have
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Learner.__init__`中，我们有
- en: '[PRE76]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'In other words, every callback knows what learner it is used in. This is critical,
    since otherwise a callback can’t get information from the learner, or change things
    in the learner. Because getting information from the learner is so common, we
    make that easier by defining `Callback` as a subclass of `GetAttr`, with a default
    attribute of `learner`:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，每个回调都知道它是在哪个学习器中使用的。这是至关重要的，否则回调无法从学习器中获取信息，或者更改学习器中的内容。因为从学习器中获取信息是如此常见，我们通过将`Callback`定义为`GetAttr`的子类，并将默认属性定义为`learner`，使其更容易：
- en: '[PRE77]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '`GetAttr` is a fastai class that implements Python’s standard `__getattr__`
    and `__dir__` methods for you, so that anytime you try to access an attribute
    that doesn’t exist, it passes the request along to whatever you have defined as
    `_default`.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`GetAttr`是一个fastai类，为您实现了Python的标准`__getattr__`和`__dir__`方法，因此每当您尝试访问一个不存在的属性时，它会将请求传递给您定义为`_default`的内容。'
- en: 'For instance, we want to move all model parameters to the GPU automatically
    at the start of `fit`. We could do this by defining `before_fit` as `self.learner.model.cuda`;
    however, because `learner` is the default attribute, and we have `SetupLearnerCB`
    inherit from `Callback` (which inherits from `GetAttr`), we can remove the `.learner`
    and just call `self.model.cuda`:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们希望在`fit`开始时自动将所有模型参数移动到GPU。我们可以通过将`before_fit`定义为`self.learner.model.cuda`来实现这一点；然而，由于`learner`是默认属性，并且我们让`SetupLearnerCB`继承自`Callback`（它继承自`GetAttr`），我们可以去掉`.learner`，只需调用`self.model.cuda`：
- en: '[PRE78]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: In `SetupLearnerCB`, we also move each mini-batch to the GPU, by calling `to_device(self.batch)`
    (we could also have used the longer `to_device(self.learner.batch)`. Note, however,
    that in the line `self.learner.batch = tfm_x(xb),yb`, we can’t remove `.learner`,
    because here we’re *setting* the attribute, not getting it.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在`SetupLearnerCB`中，我们还通过调用`to_device(self.batch)`将每个小批量移动到GPU（我们也可以使用更长的`to_device(self.learner.batch)`）。然而，请注意，在`self.learner.batch
    = tfm_x(xb),yb`这一行中，我们不能去掉`.learner`，因为这里我们是*设置*属性，而不是获取它。
- en: 'Before we try our `Learner`, let’s create a callback to track and print progress.
    Otherwise, we won’t really know if it’s working properly:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试我们的`Learner`之前，让我们创建一个回调来跟踪和打印进度。否则，我们将无法真正知道它是否正常工作：
- en: '[PRE79]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: Now we’re ready to use our `Learner` for the first time!
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好第一次使用我们的`Learner`了！
- en: '[PRE80]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: It’s quite amazing to realize that we can implement all the key ideas from fastai’s
    `Learner` in so little code! Let’s now add some learning rate scheduling.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 惊人的是，我们可以用如此少的代码实现fastai的`Learner`中的所有关键思想！现在让我们添加一些学习率调度。
- en: Scheduling the Learning Rate
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调度学习率
- en: 'If we’re going to get good results, we’ll want an LR finder and 1cycle training.
    These are both *annealing* callbacks—that is, they are gradually changing hyperparameters
    as we train. Here’s `LRFinder`:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要获得良好的结果，我们将需要一个LR finder和1cycle训练。这两个都是*退火*回调，也就是说，它们在训练过程中逐渐改变超参数。这是`LRFinder`：
- en: '[PRE82]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'This shows how we’re using `CancelFitException`, which is itself an empty class,
    used only to signify the type of exception. You can see in `Learner` that this
    exception is caught. (You should add and test `CancelBatchException`, `CancelEpochException`,
    etc. yourself.) Let’s try it out, by adding it to our list of callbacks:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这展示了我们如何使用`CancelFitException`，它本身是一个空类，仅用于表示异常的类型。您可以在`Learner`中看到这个异常被捕获。（您应该自己添加和测试`CancelBatchException`，`CancelEpochException`等。）让我们尝试一下，将其添加到我们的回调列表中：
- en: '[PRE83]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'And take a look at the results:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 并查看结果：
- en: '[PRE85]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '![](Images/dlcf_19in03.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_19in03.png)'
- en: 'Now we can define our `OneCycle` training callback:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以定义我们的`OneCycle`训练回调：
- en: '[PRE86]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'We’ll try an LR of 0.1:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将尝试一个LR为0.1：
- en: '[PRE87]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Let’s fit for a while and see how it looks (we won’t show all the output in
    the book—try it in the notebook to see the results):'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们适应一段时间，看看它的样子（我们不会在书中展示所有输出——在笔记本中尝试以查看结果）：
- en: '[PRE88]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Finally, we’ll check that the learning rate followed the schedule we defined
    (as you see, we’re not using cosine annealing here):'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将检查学习率是否遵循我们定义的调度（如您所见，我们这里没有使用余弦退火）：
- en: '[PRE89]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '![](Images/dlcf_19in04.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_19in04.png)'
- en: Conclusion
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: We have explored how the key concepts of the fastai library are implemented
    by re-implementing them in this chapter. Since it’s mostly full of code, you should
    definitely try to experiment with it by looking at the corresponding notebook
    on the book’s website. Now that you know how it’s built, as a next step be sure
    to check out the intermediate and advanced tutorials in the fastai documentation
    to learn how to customize every bit of the library.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经通过在本章中重新实现它们来探索fastai库的关键概念。由于这本书大部分内容都是代码，您应该尝试通过查看书籍网站上相应的笔记本来进行实验。现在您已经了解了它是如何构建的，作为下一步，请务必查看fastai文档中的中级和高级教程，以了解如何自定义库的每一个部分。
- en: Questionnaire
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问卷调查
- en: Experiments
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验
- en: For the questions here that ask you to explain what a function or class is,
    you should also complete your own code experiments.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这里要求您解释函数或类是什么的问题，您还应该完成自己的代码实验。
- en: What is `glob`?
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是`glob`？
- en: How do you open an image with the Python imaging library?
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何使用Python图像处理库打开图像？
- en: What does `L.map` do?
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`L.map`是做什么的？'
- en: What does `Self` do?
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Self`是做什么的？'
- en: What is `L.val2idx`?
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是`L.val2idx`？
- en: What methods do you need to implement to create your own `Dataset`?
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您需要实现哪些方法来创建自己的`Dataset`？
- en: Why do we call `convert` when we open an image from Imagenette?
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们从Imagenette打开图像时为什么要调用`convert`？
- en: What does `~` do? How is it useful for splitting training and validation sets?
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`~`是做什么的？它如何用于拆分训练和验证集？'
- en: Does `~` work with the `L` or `Tensor` classes? How about NumPy arrays, Python
    lists, or Pandas DataFrames?
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`~`是否适用于`L`或`Tensor`类？NumPy数组、Python列表或Pandas DataFrames呢？'
- en: What is `ProcessPoolExecutor`?
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是`ProcessPoolExecutor`？
- en: How does `L.range(self.ds)` work?
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`L.range(self.ds)`是如何工作的？'
- en: What is `__iter__`?
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`__iter__`是什么？'
- en: What is `first`?
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是`first`？
- en: What is `permute`? Why is it needed?
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`permute`是什么？为什么需要它？'
- en: What is a recursive function? How does it help us define the `parameters` method?
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是递归函数？它如何帮助我们定义`parameters`方法？
- en: Write a recursive function that returns the first 20 items of the Fibonacci
    sequence.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个递归函数，返回斐波那契数列的前20个项目。
- en: What is `super`?
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是`super`？
- en: Why do subclasses of `Module` need to override `forward` instead of defining
    `__call__`?
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么`Module`的子类需要重写`forward`而不是定义`__call__`？
- en: In `ConvLayer`, why does `init` depend on `act`?
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`ConvLayer`中，为什么`init`取决于`act`？
- en: Why does `Sequential` need to call `register_modules`?
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么`Sequential`需要调用`register_modules`？
- en: Write a hook that prints the shape of every layer’s activations.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个打印每个层激活形状的钩子。
- en: What is LogSumExp?
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是LogSumExp？
- en: Why is `log_softmax` useful?
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么`log_softmax`有用？
- en: What is `GetAttr`? How is it helpful for callbacks?
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是`GetAttr`？它如何帮助回调？
- en: Reimplement one of the callbacks in this chapter without inheriting from `Callback`
    or `GetAttr`.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新实现本章中的一个回调，而不继承自`Callback`或`GetAttr`。
- en: What does `Learner.__call__` do?
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Learner.__call__`是做什么的？'
- en: What is `getattr`? (Note the case difference from `GetAttr`!)
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是`getattr`？（注意与`GetAttr`的大小写区别！）
- en: Why is there a `try` block in `fit`?
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`fit`中为什么有一个`try`块？
- en: Why do we check for `model.training` in `one_batch`?
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么在`one_batch`中检查`model.training`？
- en: What is `store_attr`?
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是`store_attr`？
- en: What is the purpose of `TrackResults.before_epoch`?
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`TrackResults.before_epoch`的目的是什么？'
- en: What does `model.cuda` do? How does it work?
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`model.cuda`是做什么的？它是如何工作的？'
- en: Why do we need to check `model.training` in `LRFinder` and `OneCycle`?
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们需要在`LRFinder`和`OneCycle`中检查`model.training`？
- en: Use cosine annealing in `OneCycle`.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`OneCycle`中使用余弦退火。
- en: Further Research
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步研究
- en: Write `resnet18` from scratch (refer to [Chapter 14](ch14.xhtml#chapter_resnet)
    as needed), and train it with the `Learner` in this chapter.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从头开始编写`resnet18`（如有需要，请参考[第14章](ch14.xhtml#chapter_resnet)），并在本章中使用`Learner`进行训练。
- en: Implement a batchnorm layer from scratch and use it in your `resnet18`.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从头开始实现一个批归一化层，并在您的`resnet18`中使用它。
- en: Write a Mixup callback for use in this chapter.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为本章编写一个Mixup回调。
- en: Add momentum to SGD.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向SGD添加动量。
- en: Pick a few features that you’re interested in from fastai (or any other library)
    and implement them with the objects created in this chapter.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从fastai（或任何其他库）中挑选几个您感兴趣的特性，并使用本章中创建的对象实现它们。
- en: 'Pick a research paper that’s not yet implemented in fastai or PyTorch and do
    so with the objects you created in this chapter. Then:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一篇尚未在fastai或PyTorch中实现的研究论文，并使用本章中创建的对象进行实现。然后：
- en: Port the paper over to fastai.
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将论文移植到fastai。
- en: Submit a pull request to fastai, or create your own extension module and release
    it.
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向fastai提交拉取请求，或创建自己的扩展模块并发布。
- en: 'Hint: you may find it helpful to use [`nbdev`](https://nbdev.fast.ai) to create
    and deploy your package.'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示：您可能会发现使用[`nbdev`](https://nbdev.fast.ai)来创建和部署您的软件包很有帮助。
