<html><head></head><body><section data-pdf-bookmark="Chapter 5. Can Your LLM Know Too Much?" data-type="chapter" epub:type="chapter"><div class="chapter" id="can_your_llm_know_too_much">
      <h1><span class="label">Chapter 5. </span>Can Your LLM Know Too Much?</h1>
      <p>In 2023, a rash of companies began banning or heavily restricting the usage of LLM services, like ChatGPT, based on concerns about possible leaks of confidential data. A partial list of such companies includes Samsung, JPMorgan Chase, Amazon, Bank of America, Citigroup, Deutsche Bank, Wells Fargo, and Goldman Sachs. These actions by giant finance and tech corporations show substantial concern about LLMs disclosing confidential and sensitive information, but how critical is the risk? As the developer of an LLM application, do you need to care?</p>
      <p>In the Tay story from <a data-type="xref" href="ch01.html#chatbots_breaking_bad">Chapter 1</a>, Microsoft’s chatbot was attacked by  hackers. As bad as the damage was, it was limited because Tay didn’t have access to much sensitive data she could have disclosed. However, the intersection of LLMs with real-world data can harbor the potential of unintended information disclosure, as seen in cases where employees have inadvertently fed sensitive business data to ChatGPT, which then became integrated into the system’s training base so that others could discover it. </p>
      <p>This chapter will dig into the various ways that LLMs acquire access to data. We will examine the three predominant knowledge acquisition methods and the risks associated with your LLM having this access. Along the way, we’ll try to answer the question “Can your LLM know too much?” and discuss how to mitigate the risks associated with your application disclosing sensitive, private, or confidential data.</p>
      <section data-pdf-bookmark="Real-World Examples" data-type="sect1"><div class="sect1" id="real_world_examples">
        <h1>Real-World Examples</h1>
        <p><a contenteditable="false" data-primary="real-world LLM examples" data-type="indexterm" id="ch05.html0"/>Let’s examine two examples of the impacts seen in the real world. We’ll start with a chatbot example, which was somewhat similar to Tay, except the damage was much more significant due to the data to which the chatbot had access and how it was disclosed. Then we’ll look at a copilot example that put its owner at elevated legal and reputational risk.</p>
        <section data-pdf-bookmark="Lee Luda" data-type="sect2"><div class="sect2" id="lee_luda_idh1tCWs">
          <h2>Lee Luda</h2>
          <p><a contenteditable="false" data-primary="Lee Luda" data-type="indexterm" id="ch05.html1"/><a contenteditable="false" data-primary="real-world LLM examples" data-secondary="Lee Luda" data-type="indexterm" id="ch05.html2"/><a contenteditable="false" data-primary="Scatter Lab" data-type="indexterm" id="ch05.html3"/>Seoul-based start-up Scatter Lab, also briefly mentioned in <a data-type="xref" href="ch01.html#chatbots_breaking_bad">Chapter 1</a>, faced severe legal and reputational repercussions due to its irresponsible handling of personal data. The company operated a popular app called Science of Love, which helped users analyze their compatibility with a romantic partner by analyzing their text messages. This service accumulated 9.4 billion conversations from 600,000 users. The company later introduced Lee Luda, <a href="https://oreil.ly/PDF3e">“an A.I. chatbot that people prefer as a conversation partner over a person.”</a> Lee Luda used Science of Love’s massive dataset as its training base—without applying any proper sanitization. Not only did Lee Luda exhibit some of the toxic behavior we saw from Tay, but, more concerning, she began to leak sensitive data such as users’ names, private nicknames, and home addresses.</p>
          <p>South Korea’s Personal Information Protection Commission imposed a fine of 103.3 million won (around US$93k) on Scatter Lab for failing to obtain proper user permissions, marking a precedent in penalizing AI technology firms for data mismanagement in South Korea.</p>
          <p>There was substantial impact from this incident. Let’s look at the various facets:</p>
          <dl>
            <dt>Public exposure of sensitive data</dt>
            <dd>
              <p>The exposure of sensitive data jeopardized user privacy, revealing personal information like names, locations, relationship statuses, and medical information.</p>
            </dd>
            <dt>Financial penalty</dt>
            <dd>
              <p>Scatter Lab incurred a substantial fine for neglecting to manage user data <span class="keep-together">responsibly.</span></p>
            </dd>
            <dt>Reputational damage</dt>
            <dd>
              <p>The incident significantly tarnished Scatter Lab’s reputation, as evidenced by mainstream press coverage and a deluge of negative reviews on Google Play, especially targeting the Science of Love app.</p>
            </dd>
            <dt>Service discontinuation</dt>
            <dd>
              <p>The offending chatbot service, Lee Luda, was shut down following the incident, halting the company’s expansion plans.</p>
            </dd>
          </dl>
          <p>Now, let’s examine the lessons you can learn and apply to your own projects:</p>
          <dl>
            <dt>Stringent data privacy protocols</dt>
            <dd>
              <p>This incident highlights the imperative for robust data privacy protocols to ensure user data is handled with the utmost care and within legal frameworks.</p>
            </dd>
            <dt>User consent</dt>
            <dd>
              <p>Obtaining explicit and informed consent before collecting and processing users’ data is legally mandated and a cornerstone of ethical data practices.</p>
            </dd>
            <dt>Age verification mechanisms</dt>
            <dd>
              <p>In this case, the damage was more severe because some of the data gathered by Science of Love belonged to minors. Data mining from minors requires special care in many regulatory environments.</p>
            </dd>
            <dt>Public awareness</dt>
            <dd>
              <p>Companies must be transparent with users regarding how they will utilize data and effectively communicate the risks.</p>
            </dd>
            <dt>Monitoring and auditing</dt>
            <dd>
              <p>Regular monitoring and auditing of data handling practices can help identify and rectify privacy issues promptly, mitigating the risk of sensitive data exposure.</p>
            </dd>
          </dl>
          <p>This account emphasizes the delicate balance between leveraging user data to enhance LLM capabilities and ensuring the stringent safeguarding of user privacy and data integrity.<a contenteditable="false" data-primary="" data-startref="ch05.html3" data-type="indexterm" id="id356"/><a contenteditable="false" data-primary="" data-startref="ch05.html2" data-type="indexterm" id="id357"/><a contenteditable="false" data-primary="" data-startref="ch05.html1" data-type="indexterm" id="id358"/></p>
        </div></section>
        <section data-pdf-bookmark="GitHub Copilot and OpenAI’s Codex" data-type="sect2"><div class="sect2" id="github_copilot_and_openai_s_codex">
          <h2>GitHub Copilot and OpenAI’s Codex</h2>
          <p><a contenteditable="false" data-primary="Codex" data-type="indexterm" id="ch05.html4"/><a contenteditable="false" data-primary="GitHub Copilot" data-type="indexterm" id="ch05.html5"/><a contenteditable="false" data-primary="OpenAI Codex" data-type="indexterm" id="ch05.html6"/><a contenteditable="false" data-primary="real-world LLM examples" data-secondary="GitHub Copilot and OpenAI Codex" data-type="indexterm" id="ch05.html7"/>A notable incident in 2023 highlighted the risks associated with sensitive data disclosure through LLMs involving GitHub Copilot, a tool powered by OpenAI’s Codex model. GitHub designed Copilot to assist developers by autocompleting code, a feat achieved by training on a vast corpus of code from GitHub’s public repositories. However, the tool soon found itself in a quagmire of legal and ethical challenges. Some developers discovered Copilot suggesting snippets of their copyrighted code—despite the original code being under a license that restricted such use. This possible copyright violation sparked a lawsuit against GitHub, Microsoft, and OpenAI, with the developers alleging copyright, contract, and privacy violations.</p>
          <p>The case unfolded in a US district court. The developers’ argument hinged on two primary claims: Codex’s ability to reproduce portions of their code breached software licensing terms and violated the Digital Millennium Copyright Act by reproducing copyrighted code without the necessary copyright management information. The judge denied a motion to dismiss these two claims, keeping the lawsuit alive. While the court rejected some allegations, the crux of the case revolved around the potential infringement of the developers’ intellectual property rights due to the reproduction of code by Codex and Copilot. </p>
          <p>As of this writing, the lawsuit is still being litigated, and we may not know the full impact for some time. The lawsuit underscores a critical concern in the field of <span class="keep-together">LLMs—the</span> potential for unintentional sensitive data disclosure. The repercussions extended beyond the parties involved, resonating across the tech industry and sparking discussions on the legal and ethical implications of LLMs accessing and learning from publicly available data.</p>
          <p>Even though the full intellectual property issues raised by this case are not yet fully settled, there are several lessons you can take from this and apply to your own projects:</p>
          <dl>
            <dt>Data governance</dt>
            <dd>
              <p>This incident emphasized the importance of robust data governance frameworks, underscoring the need for clear guidelines on data usage, especially concerning publicly available or open source data.</p>
            </dd>
            <dt>Legal clarity</dt>
            <dd>
              <p>The case illuminated the legal gray areas surrounding the interaction of LLMs with real-world data, suggesting a need for more explicit laws and regulations defining the bounds of permissible data usage and copyright adherence.</p>
            </dd>
            <dt>Ethical engagement</dt>
            <dd>
              <p>Beyond legal compliance, the ethical dimensions of data usage by LLMs call for a conscientious approach by developers and organizations, respecting both the letter and spirit of open source contributions and licensing agreements.</p>
            </dd>
            <dt>User awareness</dt>
            <dd>
              <p>The incident also highlighted the importance of user awareness regarding how corporations might utilize their data, suggesting a precedent for more transparent disclosures by organizations employing LLMs.</p>
            </dd>
          </dl>
          <p>The unfolding of this lawsuit provides a real-world tableau illustrating the complex interplay of legal, ethical, and technical factors in the domain of LLM applications. It is a harbinger of the challenges (particularly concerning sensitive data disclosure risks) to come as LLMs evolve and interact with diverse data sources<a contenteditable="false" data-primary="" data-startref="ch05.html7" data-type="indexterm" id="id359"/><a contenteditable="false" data-primary="" data-startref="ch05.html6" data-type="indexterm" id="id360"/><a contenteditable="false" data-primary="" data-startref="ch05.html5" data-type="indexterm" id="id361"/><a contenteditable="false" data-primary="" data-startref="ch05.html4" data-type="indexterm" id="id362"/>.<a contenteditable="false" data-primary="" data-startref="ch05.html0" data-type="indexterm" id="id363"/></p>
        </div></section>
      </div></section>
      <section data-pdf-bookmark="Knowledge Acquisition Methods" data-type="sect1"><div class="sect1" id="knowledge_acquisition_methods">
        <h1>Knowledge Acquisition Methods</h1>
        <p><a contenteditable="false" data-primary="knowledge acquisition methods" data-type="indexterm" id="id364"/>The power of your LLM application will grow with the amount of data it has access to. At the same time, risks associated with that data also increase. If your LLM has been exposed to data of a particular type, you’ll need to manage the risk of disclosure. Let’s look at three common ways that LLMs acquire knowledge.</p>
        <p>Central to an LLM’s knowledge base is its <em>model training</em>. The process begins with <em>foundation model training</em>, where the LLM immerses itself in vast datasets, acquiring a broad grasp of language, context, and worldly insights. This foundational knowledge can then be refined through <em>model fine-tuning</em>, adapting the LLM to cater to more specialized tasks or niche domains using targeted datasets.</p>
        <p>LLMs learn in a distinct, infrequent training phase, which means their information is often out of date, and that limits their use in applications that require up-to-date knowledge. This is where <em>retrieval-augmented generation</em> (RAG) comes into play. LLMs can venture into the expansive realms of the public web, garner real-time updates, or dive deep into structured or unstructured databases. Further amplifying their knowledge spectrum, LLMs can connect with external systems, databases, or online platforms via APIs, enriching their responses with a wealth of external data.</p>
        <p>Some applications can go even further. User interactions like queries, conversations, and feedback enable LLMs to acquire new knowledge continuously. Processing these inputs allows the LLM to expand its understanding, refining its capabilities with each interaction and delivering increasingly personalized and relevant responses. </p>
        <p>Each of these categories—training, retrieval-augmented generation, and user interaction—possesses nuances that can significantly influence the security landscape of your LLM application. While they serve as conduits for knowledge acquisition, they also introduce potential vulnerabilities and challenges that need careful consideration. As we progress through this chapter, we’ll probe each category to expose the crucial security implications inherent in each method. Through this exploration, we aim to equip you with a comprehensive understanding of the potential risks and the measures to mitigate them.</p>
      </div></section>
      <section data-pdf-bookmark="Model Training" data-type="sect1"><div class="sect1" id="model_training">
        <h1>Model Training</h1>
        <p><a contenteditable="false" data-primary="model training" data-type="indexterm" id="ch05.html8"/>Training is a crucial step in developing and refining LLMs. It encompasses two distinct phases: creating the foundation model and its subsequent fine-tuning. The <em>foundation model training</em> establishes broad linguistic and contextual understanding, while <em>fine-tuning</em> hones this generalized knowledge for specific tasks or domains. In this section, we’ll explore the intricacies of both these phases, emphasizing their respective methodologies. Following this, we’ll expand on the crucial security implications inherent in each step, equipping you with insights into potential vulnerabilities and best practices for safeguarding against them.</p>
        <section data-pdf-bookmark="Foundation Model Training" data-type="sect2"><div class="sect2" id="foundation_model_training">
          <h2>Foundation Model Training</h2>
          <p><a contenteditable="false" data-primary="foundation model training" data-secondary="basics" data-type="indexterm" id="id365"/><a contenteditable="false" data-primary="model training" data-secondary="foundation" data-type="indexterm" id="id366"/>Foundation model training is the initial step in building an LLM. In this phase, the model is trained on a vast and diverse dataset, often encompassing various topics, languages, and text formats. The objective is to equip the model with a broad understanding of language, contextual relationships, and general world knowledge. This foundational training forms the base upon which the LLM can generate coherent, contextually relevant, and informed responses, akin to a basic understanding of the world, much like a human before specializing in a particular field.</p>
          <p>At its core, the process of foundation model training an LLM is a sophisticated exercise in pattern recognition. Training involves using advanced algorithms to analyze vast datasets, identify relationships between words, understand context, and generate coherent responses based on this understanding. Let’s look at the steps involved:</p>
<dl>
<dt>1. Pattern recognition</dt>
<dd><p><a contenteditable="false" data-primary="pattern recognition" data-type="indexterm" id="id367"/>The training foundation feeds the model vast text data—sometimes billions of tokens. As it processes this data, the model learns to recognize patterns. For instance, it starts to understand that the word “apple” can be associated with “fruit,” “tree,” “pie,” or “technology,” depending on the context.</p></dd>
<dt>2. Contextual understanding</dt>
<dd><p><a contenteditable="false" data-primary="contextual understanding" data-type="indexterm" id="id368"/>Next, the model starts discerning the nuanced differences in word usage based on context. It learns, for example, that the phrase “Apple’s growth” can refer to the expansion of a tech company or the development of fruit on a tree, based on surrounding words and phrases. Training algorithms will adjust internal parameters, often numbering billions, to capture these intricate contextual relationships.</p></dd>
<dt>3. Response generation</dt>
<dd><p><a contenteditable="false" data-primary="response generation" data-type="indexterm" id="id369"/>The model’s ability to generate responses is developed through repeated iterations of training, continuously refining its understanding of language and context. Unlike human memory recall, the model analyzes input, matches it with learned patterns, understands the context, and constructs a response based on training data. The diversity and breadth of the training data are critical, as they directly influence the model’s capability to produce accurate and contextually appropriate responses.</p></dd>
</dl>
        </div></section>
        <section data-pdf-bookmark="Security Considerations for Foundation Models" data-type="sect2"><div class="sect2" id="security_considerations_for_foundation_models">
          <h2>Security Considerations for Foundation Models</h2>
          <p><a contenteditable="false" data-primary="foundation model training" data-secondary="security considerations" data-type="indexterm" id="id370"/><a contenteditable="false" data-primary="model training" data-secondary="security considerations" data-type="indexterm" id="id371"/>The preceding steps show why training a custom foundation model can be complex and costly. That’s why most projects today start with an existing foundation model. The starting point might be a proprietary model accessed via a SaaS (software as a service) product, such as OpenAI’s GPT series, or a privately hosted open source model, such as Meta’s Llama. In either of those cases, the foundation model’s creator has hopefully done some level of work to ensure that things like personally identifiable information (PII) are kept out of the training base, although that might not always be the case. Choose your foundation model carefully! Even with the best intentions, there are numerous examples of these foundational models accumulating sensitive information that might be inappropriate in some contexts. A few examples of potentially problematic information types to look out for include:</p>
          <ul>
            <li>
              <p>Someone else’s intellectual property, such as copyrighted text</p>
            </li>
            <li>
              <p>Dangerous or illegal information related to weapons, drugs, or other topics</p>
            </li>
            <li>
              <p>Cultural or religious texts that may be inappropriate in specific contexts or <span class="keep-together">discussions</span></p>
            </li>
          </ul>
          <p class="pagebreak-before">If you decide to train your own foundational model, you can achieve a higher level of control over many aspects of your system. This control may be highly advantageous. However, you’re now assuming some responsibility for every part of the training data you use in your model. Keeping it free of sensitive information may prove a significant challenge for you. We’ll discuss this more later in this chapter.</p>
        </div></section>
        <section data-pdf-bookmark="Model Fine-Tuning" data-type="sect2"><div class="sect2" id="model_fine_tuning">
          <h2>Model Fine-Tuning</h2>
          <p><a contenteditable="false" data-primary="model training" data-secondary="fine tuning" data-type="indexterm" id="id372"/>Model fine-tuning is an optional step following foundation model training, aimed at specializing a general-purpose model for specific tasks or domains. You will use a smaller, domain-specific dataset to adjust the model’s weights during fine-tuning. This way, you can refine its responses to perform well in the targeted application. This process significantly enhances the model’s performance, making it more relevant and accurate for the intended use case. The specialized data used for fine-tuning allows the model to adapt its generalized understanding acquired during foundation training to the nuances and specifics of the task, providing a more tailored and <span class="keep-together">effective</span> solution.</p>
          <p>At its core, fine-tuning addresses a fundamental challenge in machine learning: while foundational models have broad knowledge, they often need more depth and specificity for particular tasks. For example, while a general model might have been trained using some medical information, it might generate responses at a different level of precision than those expected by medical professionals. Fine-tuning bridges this gap by adapting the general knowledge of the foundational model to a specific domain or task.</p>
        </div></section>
        <section data-pdf-bookmark="Training Risks" data-type="sect2"><div class="sect2" id="training_risks">
          <h2>Training Risks</h2>
          <p><a contenteditable="false" data-primary="model training" data-secondary="risks" data-type="indexterm" id="ch05.html9"/>Whether training a foundation model from scratch or fine-tuning an existing model, you must carefully consider the risks of incorporating sensitive data into your training set. Any data used in training your model might become a long-term memory. And even with attempts to align your model and provide guardrails against inappropriate disclosure, your model might disclose this information to a third party. </p>
          <p>Here are some risks you’ll want to consider as you craft the dataset for training your model:</p>
          <dl>
            <dt>Direct data leakage</dt>
            <dd>
              <p>If you expose a model to PII or confidential information during training, it might generate outputs that inadvertently disclose this data. </p>
            </dd>
            <dt>Inference attacks</dt>
            <dd>
              <p>An attacker might use prompt injection to extract sensitive data from the model. </p>
            </dd>
            <dt class="pagebreak-before">Regulatory and compliance violations</dt>
            <dd>
              <p>Training models with a dataset that includes PII, especially without user consent, can lead to breaches of data protection regulations like the Health Insurance Portability and Accountability Act (HIPAA), the General Data Protection Regulation (GDPR), or the California Consumer Privacy Act (CCPA). This can result in hefty fines and legal consequences, not to mention reputational damage.</p>
            </dd>
            <dt>Loss of public trust</dt>
            <dd>
              <p>If it becomes public knowledge that a corporation trained its model with PII or confidential data and can leak such data, the organization might face significant backlash and loss of trust.</p>
            </dd>
            <dt>Compromised data anonymization</dt>
            <dd>
              <p>Even if PII is “anonymized” before training, models might still discern patterns that allow data de-anonymization, particularly if they correlate inputs with other publicly available datasets.</p>
            </dd>
            <dt>Increased attractiveness as a target</dt>
            <dd>
              <p>If malicious actors believe that a model contains confidential information or PII, they might be more motivated to launch sophisticated attacks against it, aiming to extract valuable data.</p>
            </dd>
            <dt>Model rollbacks and financial implications</dt>
            <dd>
              <p>If a team later discovers that a model was previously trained using PII, it might need to roll back to a previous version, leading to financial implications and project delays.</p>
            </dd>
          </dl>
          <p>Given these significant risks, it’s crucial to ensure that data used in training is thoroughly sanitized. Furthermore, periodic audits, rigorous data vetting, and advanced differential privacy techniques can help mitigate potential risks.</p>
          <aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id373">
            <h1>Avoiding PII Inclusion in Training</h1>
            <p><a contenteditable="false" data-primary="model training" data-secondary="avoiding PII inclusion" data-type="indexterm" id="ch05.html10"/><a contenteditable="false" data-primary="PII (personally identifiable information)" data-secondary="avoiding PII inclusion in model training" data-type="indexterm" id="ch05.html11"/><a contenteditable="false" data-primary="training data" data-secondary="avoiding PII inclusion in" data-type="indexterm" id="ch05.html12"/>Preventing the inclusion of PII in your training dataset can be a significant practical challenge. No single technique will be sufficient. You’ll probably need to layer several defense mechanisms. Here are some to consider:</p>
            <dl>
              <dt>Data anonymization</dt>
              <dd>
                <p>Replace PII with generic values or replace names with pseudonyms to ensure the data no longer identifies specific individuals.</p>
              </dd>
              <dt>Data aggregation</dt>
              <dd>
                <p>Group individual data points into larger datasets so that the LLM cannot distinguish individual entries.</p>
              </dd>
              <dt>Regular audits</dt>
              <dd>
                <p>Review and clean the training datasets to ensure no PII slips through.</p>
              </dd>
              <dt>Data masking</dt>
              <dd>
                <p>Use techniques to hide original data with modified content structurally similar to the original data, such as transforming “John Doe” into “Xxxx Xxx.” The masked data is a sanitized version where you retain the essence but obscure the sensitive details.</p>
              </dd>
              <dt>Use synthetic data</dt>
              <dd>
                <p>Generate data that is not based on actual user information but retains the same statistical properties as your original dataset.</p>
              </dd>
              <dt>Limit data collection</dt>
              <dd>
                <p>Only collect the minimum data necessary for the task. If you don’t need certain pieces of information, don’t collect them in the first place.</p>
              </dd>
              <dt>Automated scanning</dt>
              <dd>
                <p>Use tools that scan and flag potential PII in datasets.</p>
              </dd>
              <dt>Differential privacy</dt>
              <dd>
                <p>Implement techniques that add noise to the data, ensuring that any single data point (or individual’s data) doesn’t significantly impact the overall dataset and that an attacker cannot reverse engineer the data.</p>
              </dd>
              <dt>Tokenization</dt>
              <dd>
                <p><a contenteditable="false" data-primary="tokenization" data-type="indexterm" id="id374"/>Replace sensitive data elements with nonsensitive equivalents with no exploitable meaning. These tokens act as placeholders for the original data, which is then securely stored in a separate location or data vault.</p>
              </dd>
            </dl>
            <p>By adopting these strategies, organizations can significantly reduce the risk of incorporating PII into their training datasets, ensure regulatory compliance, and maintain trust with users and <a contenteditable="false" data-primary="" data-startref="ch05.html12" data-type="indexterm" id="id375"/><a contenteditable="false" data-primary="" data-startref="ch05.html11" data-type="indexterm" id="id376"/><a contenteditable="false" data-primary="" data-startref="ch05.html10" data-type="indexterm" id="id377"/>stakeholders<a contenteditable="false" data-primary="" data-startref="ch05.html9" data-type="indexterm" id="id378"/>.<a contenteditable="false" data-primary="" data-startref="ch05.html8" data-type="indexterm" id="id379"/></p>
          </div></aside>
        </div></section>
      </div></section>
      <section data-pdf-bookmark="Retrieval-Augmented Generation" data-type="sect1"><div class="sect1" id="retrieval_augmented_generation">
        <h1>Retrieval-Augmented Generation</h1>
        <p><a contenteditable="false" data-primary="retrieval-augmented generation (RAG)" data-type="indexterm" id="ch05.html13"/>RAG is a transformative approach in LLM data acquisition and response generation. Instead of solely relying on a vast internal knowledge base acquired from training, as traditional LLMs do, RAG first retrieves relevant document snippets or <em>passages</em> from an external dataset. Then, the LLM utilizes these passages to inform its generated responses. This two-step approach—retrieving relevant information and then developing an answer based on that retrieval—allows the model to pull in real-time or more updated information that wasn’t part of its original training data.</p>
        <p class="pagebreak-before less_space">RAG is a significant leap forward in the ability of language models to handle large amounts of real-time data. No matter how expansive their training data, traditional LLMs are inherently limited to their last training cutoff, making them potentially outdated for specific topics or real-time events. RAG solves this limitation by allowing LLMs to access and integrate external, up-to-date information seamlessly. This dynamic capability enhances the accuracy and relevance of the model’s outputs and positions LLMs to be more versatile and adaptive in rapidly evolving domains. The ability to fuse retrieval and generation processes promises a new frontier of more informed and context-aware conversational AI.</p>
        <p>However, attaching your LLM to large, live data stores opens up a Pandora’s box of security considerations. One issue is indirect prompt injection, which we discussed in <a data-type="xref" href="ch04.html#prompt_injection">Chapter 4</a>. Prompt injection attacks are possible when you feed your LLM untrusted data as part of a RAG prompt. But, for this chapter, we’ll focus on the risks associated with sensitive data disclosure to help answer the question “Can your LLM know too much?”</p>
        <p>Let’s review some common ways a RAG system gets access to larger data stores. By understanding how your LLM might access these knowledge bases, we can better plan for the security risks and considerations. Here, we’ll look at accessing data directly from the web and accessing databases.</p>
        <section data-pdf-bookmark="Direct Web Access" data-type="sect2"><div class="sect2" id="direct_web_access">
          <h2>Direct Web Access</h2>
          <p><a contenteditable="false" data-primary="direct web access, for retrieval-augmented generation" data-type="indexterm" id="ch05.html14"/><a contenteditable="false" data-primary="retrieval-augmented generation (RAG)" data-secondary="direct web access" data-type="indexterm" id="ch05.html15"/>Providing your LLM with a direct connection to the web can be a powerful mechanism to get real-time or updated information to augment its knowledge base. A web connection enables the model to fetch the latest data, stay current with evolving topics, and provide more accurate and up-to-date responses. By interacting with the web, the LLM can bridge the gap between its last training cutoff and the present, ensuring its information is relevant and timely. This feature significantly enhances the utility of LLMs in dynamic or rapidly changing domains. </p>
          <p>Let’s look at a couple of patterns for accessing the web.</p>
          <section data-pdf-bookmark="Scraping a specific URL" data-type="sect3"><div class="sect3" id="scraping_a_specific_url">
            <h3>Scraping a specific URL</h3>
            <p><a contenteditable="false" data-primary="direct web access, for retrieval-augmented generation" data-secondary="specific URL scraping" data-type="indexterm" id="ch05.html16"/><a contenteditable="false" data-primary="retrieval-augmented generation (RAG)" data-secondary="direct web access" data-tertiary="specific URL scraping" data-type="indexterm" id="ch05.html17"/><a contenteditable="false" data-primary="URL scraping" data-type="indexterm" id="ch05.html18"/>Directly accessing a predetermined URL to extract its content is a particularly useful approach when you know the exact source of the information you want the LLM to access. This technique is appropriate for many cases, such as extracting daily stock prices from a financial news website, pulling regular updates from a specific news source or blog, or retrieving product details or reviews from an ecommerce site.</p>
            <p class="pagebreak-before less_space">For these types of use cases, there are several advantages:</p>
            <dl>
              <dt>Precision</dt>
              <dd>
                <p>Targets the desired web page, eliminating potential noise from unrelated sources.</p>
              </dd>
              <dt>Efficiency</dt>
              <dd>
                <p>Since the URL is predetermined, you can optimize the scraping process for that page’s specific structure and content. </p>
              </dd>
              <dt>Reliability</dt>
              <dd>
                <p>Consistently accessing a single or a set of known URLs can provide more stable results over time.</p>
              </dd>
            </dl>
            <p>But there are also some critical challenges:</p>
            <dl>
              <dt>Page structure changes</dt>
              <dd>
                <p>Web pages often undergo redesigns or structural changes. If the specific URL’s content structure changes, the scraping mechanism might need adjustment.</p>
              </dd>
              <dt>Access restrictions</dt>
              <dd>
                <p>Some websites use CAPTCHAs, rate limits, or <em>robots.txt</em> restrictions to prevent or limit automated access.</p>
              </dd>
              <dt>Legal or ethical challenges</dt>
              <dd>
                <p>If you do not own the content on the web page you’re scraping, you must consider whether the owner of that page could object to how you’re using that data within your system. Consider copyrights and other licensing terms as needed.<a contenteditable="false" data-primary="" data-startref="ch05.html18" data-type="indexterm" id="id380"/><a contenteditable="false" data-primary="" data-startref="ch05.html17" data-type="indexterm" id="id381"/><a contenteditable="false" data-primary="" data-startref="ch05.html16" data-type="indexterm" id="id382"/> </p>
              </dd>
            </dl>
          </div></section>
          <section data-pdf-bookmark="Using a search engine followed by content scraping" data-type="sect3"><div class="sect3" id="using_a_search_engine_followed_by_content_scraping">
            <h3>Using a search engine followed by content scraping</h3>
            <p><a contenteditable="false" data-primary="content scraping" data-type="indexterm" id="ch05.html19"/><a contenteditable="false" data-primary="direct web access, for retrieval-augmented generation" data-secondary="content scraping using search engines" data-type="indexterm" id="ch05.html20"/><a contenteditable="false" data-primary="retrieval-augmented generation (RAG)" data-secondary="direct web access" data-tertiary="content scraping using search engines" data-type="indexterm" id="ch05.html21"/>In this method, you issue a search query to a platform like Google or Bing to find relevant content based on specific keywords or topics and then scrape the content from one or more top search results. This approach is most appropriate for use cases such as researching current public sentiment on a specific topic or product by scraping top news articles or blogs, retrieving recent academic publications or articles on a particular subject, and understanding market trends by analyzing the top results for industry-specific keywords.</p>
            <p>For these types of use cases, there are several advantages:</p>
            <dl>
              <dt>Relevance</dt>
              <dd>
                <p>Search engines rank content based on relevance, ensuring the LLM accesses high-quality and pertinent information.</p>
              </dd>
              <dt>Timeliness</dt>
              <dd>
                <p>Search engines constantly index new content, making them a valuable resource for obtaining recent information on a topic.</p>
              </dd>
              <dt>Diversity</dt>
              <dd>
                <p>By accessing multiple top results, LLMs can gain a more comprehensive understanding of a topic from various perspectives.</p>
              </dd>
            </dl>
            <p>Challenges include:</p>
            <dl>
              <dt>Indirect prompt injection</dt>
              <dd>
                <p><a contenteditable="false" data-primary="indirect prompt injection" data-type="indexterm" id="id383"/><a contenteditable="false" data-primary="prompt injection attacks" data-secondary="direct versus indirect" data-tertiary="indirect" data-type="indexterm" id="id384"/>As discussed in <a data-type="xref" href="ch04.html#prompt_injection">Chapter 4</a>, malicious prompts may not come directly from users. They may be secretly embedded into data included in a prompt in a RAG system. In this case, an attacker may embed malicious data within a web page, leading to an indirect prompt injection attack when the page is parsed and data is included in a prompt passed to the LLM by the application.</p>
              </dd>
              <dt>Dynamic results</dt>
              <dd>
                <p>Search results for a particular query can change over time, introducing variability in the content the LLM accesses.</p>
              </dd>
              <dt>Search limitations</dt>
              <dd>
                <p>Search engines may have request limits, especially for automated queries, which could restrict the number of searches.</p>
              </dd>
              <dt>Depth of scraping</dt>
              <dd>
                <p>Deciding how many top results to scrape can affect the quality and breadth of information. Scraping too many might dilute the relevance; scraping too few might miss out on valuable perspectives.</p>
              </dd>
              <dt>Legal and ethical concerns</dt>
              <dd>
                <p>When scraping content, it’s important to abide by search engines’ terms of service and consider copyright and licensing terms.<a contenteditable="false" data-primary="" data-startref="ch05.html21" data-type="indexterm" id="id385"/><a contenteditable="false" data-primary="" data-startref="ch05.html20" data-type="indexterm" id="id386"/><a contenteditable="false" data-primary="" data-startref="ch05.html19" data-type="indexterm" id="id387"/></p>
              </dd>
            </dl>
          </div></section>
          <section data-pdf-bookmark="Example risks" data-type="sect3"><div class="sect3" id="example_risks">
            <h3>Example risks</h3>
            <p><a contenteditable="false" data-primary="direct web access, for retrieval-augmented generation" data-secondary="risks" data-type="indexterm" id="ch05.html22"/><a contenteditable="false" data-primary="retrieval-augmented generation (RAG)" data-secondary="direct web access" data-tertiary="risks" data-type="indexterm" id="ch05.html23"/>Direct web access or search engines carry various risks related to the unintentional acquisition or disclosure of PII and other sensitive information. Here are some examples of how this might happen:</p>
            <dl>
              <dt>Comment sections and forums</dt>
              <dd>
                <p>A model might scrape a technical article or news piece from a reputable source, but in doing so, it could also unintentionally pull in comments or forum posts attached to the article. These sections often contain personal anecdotes, email addresses, or other identifiable details. For example, a user might ask the LLM for recent discussions on a particular health topic. The model could pull data from a health forum where users have shared personal stories, names, ages, or even specific medical details.</p>
              </dd>
              <dt>User profiles</dt>
              <dd>
                <p>Some websites include user profiles or author bios at the end of articles or posts. Scraping such sites might accidentally gather personal details or contacts in these profiles. For example, an LLM fetching entries from a blogging platform might also scrape the author’s bio, including their full name, location, workplace, and email address.</p>
              </dd>
              <dt>Hidden data in web pages</dt>
              <dd>
                <p>Some web pages store metadata or secret information in the background. While this data may not be visible to human viewers, an LLM with web access might still access and process it. For example, an LLM scraping a corporate website might unintentionally access embedded metadata that contains internal document paths or even confidential revision comments.</p>
              </dd>
              <dt>Inaccurate or broad search queries</dt>
              <dd>
                <p>When using search engines, if the queries are too broad or not accurately defined, the model might pull in unrelated content that contains sensitive information. For example, a query like “John Doe’s presentation” intended to find a public lecture by a notable figure might also yield results from a different John Doe’s blog where he shared his phone number for contact.</p>
              </dd>
              <dt>Advertisements and sponsored content</dt>
              <dd>
                <p>Web scraping might inadvertently gather data from ads or sponsored posts that can sometimes contain personalized content based on prior user behavior or other targeted criteria. For example, an LLM scraping news from a web page might also pull in an ad that says “Special deals for residents of [location],” revealing location data.</p>
              </dd>
              <dt>Dynamic content and pop-ups</dt>
              <dd>
                <p>Many websites have dynamic content that changes based on user interaction, location, or time. Pop-up surveys, chatbots, or feedback forms can contain prompts for personal information. For example, in scraping a service provider’s web page, the LLM might pull a pop-up content asking, “Are you from [city]? Answer this survey!” which can disclose geolocation details.</p>
              </dd>
              <dt>Document metadata and properties</dt>
              <dd>
                <p>When accessing online documents or files, their associated metadata can contain author names, editing histories, or internal comments. For example, the LLM might pull a company’s public financial report, but along with it, the properties might show “last edited by [employee name] from [department],” revealing internal company information<a contenteditable="false" data-primary="" data-startref="ch05.html23" data-type="indexterm" id="id388"/><a contenteditable="false" data-primary="" data-startref="ch05.html22" data-type="indexterm" id="id389"/>.<a contenteditable="false" data-primary="" data-startref="ch05.html15" data-type="indexterm" id="id390"/><a contenteditable="false" data-primary="" data-startref="ch05.html14" data-type="indexterm" id="id391"/></p>
              </dd>
            </dl>
          </div></section>
        </div></section>
        <section data-pdf-bookmark="Accessing a Database" data-type="sect2"><div class="sect2" id="accessing_a_database">
          <h2>Accessing a Database</h2>
          <p><a contenteditable="false" data-primary="database access, for retrieval-augmented generation" data-type="indexterm" id="ch05.html24"/><a contenteditable="false" data-primary="retrieval-augmented generation (RAG)" data-secondary="database access" data-type="indexterm" id="ch05.html25"/>This pattern involves an LLM retrieving data stored in structured or unstructured databases. This approach can include querying traditional databases for specific data or accessing vector databases for embeddings. By leveraging databases, LLMs can provide precise and data-driven responses, making them significantly more valuable in scenarios requiring real-time or historical data retrieval. This method of knowledge acquisition allows LLMs to operate in data-rich environments and provide highly accurate, context-aware, and personalized responses based on the data available in the databases.</p>
          <section data-pdf-bookmark="Relational databases" data-type="sect3"><div class="sect3" id="relational_databases">
            <h3>Relational databases</h3>
            <p><a contenteditable="false" data-primary="database access, for retrieval-augmented generation" data-secondary="relational databases" data-type="indexterm" id="ch05.html26"/><a contenteditable="false" data-primary="retrieval-augmented generation (RAG)" data-secondary="database access" data-tertiary="relational databases" data-type="indexterm" id="ch05.html27"/>Relational databases have been the de facto standard since the late 1970s, underpinning the infrastructure of countless industries and applications. They revolutionized how developers organize and access data using tables and ensure data integrity through established relationships. Their structured approach to data management, paired with the power of SQL (Structured Query Language) for data manipulation, enabled organizations to handle complex datasets efficiently and precisely. While modern technological advancements have brought forth new types of databases, the robustness of relational databases continues to make them a trusted choice for many enterprises.</p>
            <p>Giving your LLM access to the vast data stores inside your enterprise is powerful and thus tempting. The advantages are clear: instant access to enormous amounts of historical and real-time data allows for richer, more informed responses tailored to specific organizational needs and contexts. The LLM can provide insights, answer intricate queries, or even automate tasks that would otherwise take hours for a human to compile. It can transform the user experience, offering a seamless interface between vast data repositories and end users, whether employees, stakeholders, or customers. However, with this immense power comes an equally tremendous responsibility to safeguard sensitive information and ensure data access remains securely and ethically managed. Let’s examine risk areas related to accessing databases as part of your LLM application:</p>
            <dl>
              <dt>Complex relationships amplify exposure</dt>
              <dd>
                <p>Relational databases link structured datasets through relationships. While one table might seem benign, its linkage to another could inadvertently reveal sensitive patterns. For instance, an innocent list of product IDs can become sensitive when linked to specific customer transactions.</p>
              </dd>
              <dt class="pagebreak-before less_space">Unintended queries</dt>
              <dd>
                <p>A misinterpreted command or a poorly phrased question could lead the LLM to fetch data the developer didn’t intend the user to access. Imagine a scenario where a casual inquiry inadvertently brings up a detailed record, revealing more than was asked.</p>
              </dd>
              <dt>Permission oversights</dt>
              <dd>
                <p>Relational databases have intricate permission systems. In the integration process, an LLM might be granted broader access than necessary due to oversight or misconfiguration, opening doors to data that should remain restricted.</p>
              </dd>
              <dt>Inadvertent data inferences</dt>
              <dd>
                <p>LLMs identify patterns. Over multiple interactions, they might collate seemingly nonsensitive data, leading to unintended sensitive insights. For example, while individual purchases might not disclose much, a pattern might hint at a company’s upcoming product launch or a shift in strategy.</p>
              </dd>
              <dt>Auditability and accountability challenges</dt>
              <dd>
                <p>Relational databases traditionally offer robust audit trails, tying actions to specific users. With LLMs as intermediaries, ensuring that every query and data fetch remains traceable is vital. Without clear audit trails, pinpointing the origin of a data breach or understanding unexpected behaviors becomes intricate.</p>
              </dd>
            </dl>
            <p>In conclusion, integrating LLMs with trusted relational databases can improve functionality and performance. Still, it is important to use these integrations with an awareness of the associated risks. Implementing stringent safeguards and oversight can harness the LLM’s capabilities while ensuring data integrity and security.<a contenteditable="false" data-primary="" data-startref="ch05.html27" data-type="indexterm" id="id392"/><a contenteditable="false" data-primary="" data-startref="ch05.html26" data-type="indexterm" id="id393"/></p>
          </div></section>
          <section data-pdf-bookmark="Vector databases" data-type="sect3"><div class="sect3" id="vector_databases">
            <h3>Vector databases</h3>
            <p><a contenteditable="false" data-primary="database access, for retrieval-augmented generation" data-secondary="vector databases" data-type="indexterm" id="ch05.html28"/><a contenteditable="false" data-primary="retrieval-augmented generation (RAG)" data-secondary="database access" data-tertiary="vector databases" data-type="indexterm" id="ch05.html29"/><a contenteditable="false" data-primary="vector databases" data-type="indexterm" id="ch05.html30"/><em>Vector databases</em> represent a significant evolution in the way we think about and handle data, particularly in the context of machine learning and AI operations. Unlike relational databases that organize data into rows and columns, vector databases store data as high-dimensional vectors. These vectors are arrays of numbers that effectively capture the essence of objects or data points in terms of their features or attributes. This structure is advantageous for performing similarity or proximity-based operations in a vector space.</p>
            <p>High-dimensional vectors are adept at handling complex operations like <em>nearest neighbor</em> searches, which are crucial for many AI applications. These searches allow the database to quickly find data points closest to a given query point in the vector space, facilitating operations that rely on finding the most similar items or patterns. By managing data as <em>vectors</em>—essentially mathematical representations that encode information about data items—vector databases excel in rapidly retrieving and <span class="keep-together">comparing</span> data, thereby enabling efficient and accurate similarity searches across vast datasets.</p>
            <p>Integrating your LLM with vector databases via the RAG pattern can supercharge its capabilities. By linking the model to these databases, you can harness the power of similarity-based searches, allowing for contextually richer responses that are more attuned to nuanced user queries. The model can swiftly locate and leverage embeddings that resonate with the query’s intent, serving accurate and relevant results. For certain this is revolutionary. Let’s examine some examples where combining a vector database with the RAG pattern can produce excellent results:</p>
            <dl>
              <dt>Question answering systems</dt>
              <dd>
                <p>Users expect precise and accurate responses when answering questions. RAG systems can retrieve relevant documents or data snippets from the vector DB to inform the LLM’s responses, making the answers more accurate and detailed than those generated from the model’s knowledge alone.</p>
              </dd>
              <dt>Content recommendation</dt>
              <dd>
                <p>For platforms requiring personalized content recommendations—such as news aggregators, streaming services, and ecommerce websites—RAG can enhance recommendation engines by retrieving content from the vector DB that closely matches user profiles or previous interactions, thus improving user engagement and satisfaction.</p>
              </dd>
              <dt>Academic research and summarization</dt>
              <dd>
                <p>RAG systems can significantly speed up the research process by retrieving relevant documents from the vector DB and providing summaries or connections between them.</p>
              </dd>
              <dt>Customer support</dt>
              <dd>
                <p>Chatbots can pull from FAQs, product manuals, and customer interaction logs to provide support agents or automated chatbots with the information needed to answer customer inquiries effectively and efficiently.</p>
              </dd>
              <dt>Legal and compliance review</dt>
              <dd>
                <p>For applications requiring review of large volumes of legal or regulatory documents, RAG can quickly retrieve relevant documents based on queries, thereby aiding in compliance checks or legal research.</p>
              </dd>
              <dt>Medical information systems</dt>
              <dd>
                <p>In health care, RAG can support diagnostic processes, patient management, and medical research by retrieving patient records, scientific studies, and clinical trial results relevant to a doctor’s query or a specific medical condition.</p>
              </dd>
            </dl>
            <p>This architecture has great power. However, the dynamic nature of vector databases and their unique data-handling mechanisms present security challenges that development teams must address:</p>
            <dl>
              <dt>Embedding reversibility</dt>
              <dd>
                <p>While embeddings in vector databases are abstract numerical representations, there is a risk that sophisticated techniques might reverse engineer these embeddings, revealing the sensitive information from which they were derived. For instance, embeddings created from confidential documents might have unique patterns that can hint at the document’s content.</p>
              </dd>
              <dt>Information leakage via similarity searches</dt>
              <dd>
                <p>Similarity searches, the core advantage of vector databases, can pose a risk in the context of sensitive data disclosure. An attacker might infer certain sensitive aspects about the dataset by analyzing the results of proximity-based queries. If, for instance, a user finds that specific queries yield close matches, they might deduce the nature or specifics of the data behind the embeddings.</p>
              </dd>
              <dt>Data granularity and vector representations</dt>
              <dd>
                <p>Depending on the granularity of the embeddings, specific patterns or clusters in the vector space might indirectly disclose information about the nature of the data. For instance, if particular data points are always clustered together, it might reveal relationships or characteristics about the original data.</p>
              </dd>
              <dt>Interactions with other systems</dt>
              <dd>
                <p>Often, vector databases aren’t standalone but interact with other systems. The flow of embeddings or derived vectors between systems can become a point of exposure, especially if data lineage and flow aren’t securely managed.</p>
              </dd>
            </dl>
            <p>In conclusion, while vector databases enhance the capabilities of LLMs by offering a nuanced, similarity-based approach to data, it’s paramount to be vigilant about potential avenues of sensitive data disclosure. These databases’ very strengths can be leveraged by malicious actors if not safeguarded adequately. Understanding these risks and taking proactive measures will be essential in maintaining the integrity and confidentiality of the data they manage.<a contenteditable="false" data-primary="" data-startref="ch05.html30" data-type="indexterm" id="id394"/><a contenteditable="false" data-primary="" data-startref="ch05.html29" data-type="indexterm" id="id395"/><a contenteditable="false" data-primary="" data-startref="ch05.html28" data-type="indexterm" id="id396"/></p>
          </div></section>
          <section data-pdf-bookmark="Reducing database risk" data-type="sect3"><div class="sect3" id="reducing_database_risk">
            <h3>Reducing database risk</h3>
            <p><a contenteditable="false" data-primary="database access, for retrieval-augmented generation" data-secondary="risk reduction" data-type="indexterm" id="ch05.html31"/><a contenteditable="false" data-primary="retrieval-augmented generation (RAG)" data-secondary="database access" data-tertiary="risk reduction" data-type="indexterm" id="ch05.html32"/>Here are some ideas for best practices and mitigation strategies for reducing the risks of sensitive data exposure when connecting your LLM to a database:</p>
            <dl>
              <dt>Role-based access control (RBAC)</dt>
              <dd>
                <p><a contenteditable="false" data-primary="RBAC (role-based access control)" data-type="indexterm" id="id397"/><a contenteditable="false" data-primary="role-based access control (RBAC)" data-type="indexterm" id="id398"/>Ensure that the LLM has restricted access to the database. Grant only the necessary permissions and avoid giving the LLM blanket access. Using roles, you can ensure the LLM can pull only the information that it absolutely needs.</p>
              </dd>
              <dt>Data classification</dt>
              <dd>
                <p>Categorize your data based on sensitivity (public, internal, confidential, restricted). Ensure that LLMs have no access or limited, sanitized access to high-sensitivity data categories.</p>
              </dd>
              <dt>Audit trails</dt>
              <dd>
                <p>Maintain logs of every database query made by the application. Review these logs regularly to identify patterns, anomalies, or unintended data access.</p>
              </dd>
              <dt>Data redaction and masking</dt>
              <dd>
                <p>For sensitive fields, consider using redaction (completely hiding the data) or masking (obfuscating part of the data) to limit the exposure of sensitive data.</p>
              </dd>
              <dt>Input sanitization</dt>
              <dd>
                <p>Ensure that any queries or inputs processed by the LLM to access the database are sanitized and checked to prevent SQL injection or other data manipulation attacks.</p>
              </dd>
              <dt>Automated data scanners</dt>
              <dd>
                <p>Use automated tools to scan and flag sensitive information, ensuring such data is removed or adequately safeguarded before the LLM can access it.</p>
              </dd>
              <dt>Use views instead of direct table access</dt>
              <dd>
                <p>For relational databases, consider providing the LLM with access to views that are sanitized versions of tables, rather than giving access to the actual raw tables.</p>
              </dd>
              <dt>Data retention policies</dt>
              <dd>
                <p>Implement policies that dictate how long a database should retain certain data. Regularly purge data that is no longer needed to reduce the potential data exposure <a contenteditable="false" data-primary="" data-startref="ch05.html32" data-type="indexterm" id="id399"/><a contenteditable="false" data-primary="" data-startref="ch05.html31" data-type="indexterm" id="id400"/>footprint<a contenteditable="false" data-primary="" data-startref="ch05.html25" data-type="indexterm" id="id401"/><a contenteditable="false" data-primary="" data-startref="ch05.html24" data-type="indexterm" id="id402"/>.<a contenteditable="false" data-primary="" data-startref="ch05.html13" data-type="indexterm" id="id403"/></p>
              </dd>
            </dl>
          </div></section>
        </div></section>
      </div></section>
      <section data-pdf-bookmark="Learning from User Interaction" data-type="sect1"><div class="sect1" id="learning_from_user_interaction">
        <h1>Learning from User Interaction</h1>
        <p><a contenteditable="false" data-primary="user interaction" data-type="indexterm" id="ch05.html33"/>While simple LLMs don’t modify their behaviors based on usage, we now see increasingly common scenarios where developers add this capability. By processing queries, feedback, or other forms of input from users, LLMs can refine their understanding, provide more accurate responses, and even learn new information over time. This dynamic interaction allows the LLM to stay updated, learn from user feedback, and tailor its responses to individual or collective user preferences, thus enhancing the user experience and the utility of the LLM in practical applications.</p>
        <p>In <a data-type="xref" href="ch01.html#chatbots_breaking_bad">Chapter 1</a>, we saw one type of risk associated with directly incorporating untrusted user input into your LLM’s knowledge base. In that case, Microsoft’s Tay picked up toxic language and bias. However, there is another set of risks related to sensitive data.</p>
        <p>When an LLM continually interacts with diverse users, there’s a potential influx of sensitive data, intentionally or inadvertently. While the learning capacity of an LLM ensures it evolves and becomes more efficient over time, this continuous learning can also be its Achilles’ heel when it comes to data protection. The very nature of user interaction, being diverse and unpredictable, means there’s a potential for users to input or reference personal, confidential, or proprietary information.</p>
        <p>For instance, consider a business executive using an LLM to draft a message. They might feed the system snippets of confidential business strategies, expecting a more polished output. We’ve seen real-world scenarios of this at Samsung and other major corporations. Or, a user might query the LLM with personal medical symptoms, hoping for insights into potential conditions. In both situations, the user shared sensitive data with your application. If you’re using any of this data in future training or storing it for real-time access, this information could become part of the LLM’s internal knowledge structure, or your application could store it for future reference.</p>
        <p>Furthermore, the challenge with user interactions is that the LLM might only sometimes recognize sensitive data when it sees it. Whereas a human might realize the importance of a Social Security number, proprietary formula, or a unique business strategy, an LLM might treat it as just another piece of information. This lack of understanding could lead to scenarios where an LLM, when queried later by another user on a related topic, might inadvertently disclose fragments of the previously fed sensitive information.</p>
        <p>Moreover, with the rise of multimodal LLMs that can process not just text but also images, audio, and video, the potential for sensitive data disclosure multiplies. A user might input a photo for image recognition, not realizing that the background contains identifiable information or copyrighted material.</p>
        <p>To address these issues, employ the following mitigation strategies:</p>
        <dl>
          <dt>Clear communication</dt>
          <dd>
            <p>Users should be informed about the LLM’s learning capabilities and data retention policies. An initial disclaimer about not sharing personal or sensitive information can be helpful.</p>
          </dd>
          <dt>Data sanitization</dt>
          <dd>
            <p>Implement algorithms that identify and remove potential PII or other sensitive data from user inputs before processing.</p>
          </dd>
          <dt>Temporary memory</dt>
          <dd>
            <p>Consider giving the LLM a temporary memory for user-specific information that the system automatically erases after the session ends, ensuring no long-term retention of sensitive data.</p>
          </dd>
          <dt>No persistent learning</dt>
          <dd>
            <p>Design the LLM so it doesn’t persistently learn from user interactions, thus minimizing the risk of internalizing sensitive data.<a contenteditable="false" data-primary="" data-startref="ch05.html33" data-type="indexterm" id="id404"/></p>
          </dd>
        </dl>
      </div></section>
      <section data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="conclusion_2">
        <h1>Conclusion</h1>
        <p>The core question of this chapter was “Can your LLM know too much?” The answer is clearly yes. We need our LLMs to have access to information to be helpful. However, we must carefully evaluate what types of information we provide to these systems and view that information through a lens asking, “What happens if this information is disclosed?” If the penalty for unintentional disclosure is too high, then you must carefully weigh the risk of training or equipping your model with such data. </p>
        <p>We studied the three main avenues through which LLMs acquire their vast knowledge: training, retrieval-augmented generation, and user interaction. Each method came with its own advantages and unique challenges when guarding against unintentional data exposure. Key insights garnered include:</p>
        <dl>
          <dt>Training</dt>
          <dd>
            <p>The foundation of LLMs. While training equips LLMs with vast knowledge, it is imperative to vet training data meticulously, eliminating any traces of PII, proprietary insights, or controversial content. Periodic audits and employing data sanitization strategies are nonnegotiable.</p>
          </dd>
          <dt>Retrieval-augmented generation</dt>
          <dd>
            <p>A bridge between the LLM and the vast sea of unstructured data on the web. The power of real-time data comes with the responsibility of filtering out sensitive or misleading information. When accessing APIs or databases, setting stringent access controls is crucial.</p>
          </dd>
          <dt>Learning from user interaction</dt>
          <dd>
            <p>The most dynamic knowledge source. Every user query carries the potential of revealing personal or corporate secrets. Protecting against this necessitates clear user communication, data sanitization, and judicious use of persistent learning.</p>
          </dd>
        </dl>
        <p>In conclusion, your LLM’s ability to process vast knowledge stores can be of substantial value, but that’s also where the danger may lie. The key is to balance empowering LLMs with ensuring they don’t inadvertently “know too much.” This chapter was dedicated to understanding this delicate balance, hoping to guide readers in harnessing the power of LLMs responsibly, ensuring they are both potent tools and trustworthy guardians of sensitive information.</p>
      </div></section>
    </div></section></body></html>