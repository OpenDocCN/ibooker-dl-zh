<html><head></head><body><section class="pagenumrestart" data-pdf-bookmark="Chapter 1. Understanding the AI Regulations" data-type="chapter" epub:type="chapter"><div class="chapter" id="chapter_1_understanding_the_ai_regulations_1748539916832819">
<h1><span class="label">Chapter 1. </span>Understanding the AI Regulations</h1>

<p>As people, organizations, and the public sector increasingly rely on AI to drive decision making, the technology must be trustworthy. The <a data-primary="EU AI Act" href="https://oreil.ly/-8wI7">EU AI Act</a><a contenteditable="false" data-primary="EU AI Act" data-type="indexterm" id="eua-1"/> aims <a contenteditable="false" data-primary="AI Act" data-see="EU AI Act" data-type="indexterm" id="id303"/>to provide a legal framework for developing, deploying, and using AI technologies within the European Union, emphasizing safety, transparency, and ethical considerations. It is a regulatory framework for artificial intelligence that includes specific requirements for AI systems of different risk categories within the EU. This book is focused on understanding and implementing the regulatory requirements set by the European Union’s legislation on artificial intelligence. Please note that the guidance it provides is not intended as a substitute for obtaining professional legal advice.</p>

<p>This chapter begins by outlining the motivation behind the EU AI Act, emphasizing the idea of “trustworthy AI,”<a contenteditable="false" data-primary="trustworthy AI" data-type="indexterm" id="id304"/> and identifying the essential requirements for ensuring AI is trustworthy. It then describes the structure of the EU AI Act, including its definitions, key stakeholders, risk classifications (prohibited, high risk, limited risk, and minimal risk), and the implementation timeline. Finally, it briefly compares the EU AI Act with other international regulations and standards related to AI.</p>

<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>The author is not a lawyer, and this book does not provide legal advice. The intersection of law and artificial intelligence is a complex subject that requires expertise beyond the scope of AI, data science, and machine learning. Legal considerations surrounding AI systems can be complex and far-reaching. If you have any legal concerns related to the AI systems you are working on, seek professional legal advice from qualified experts in the field.</p>
</div>

<section data-pdf-bookmark="The Motivation for the EU AI Act: Trustworthy AI" data-type="sect1"><div class="sect1" id="chapter_1_the_motivation_for_the_eu_ai_act_trustworthy_ai_1748539916833084">
<h1>The Motivation for the EU AI Act: Trustworthy AI</h1>

<p>As AI becomes increasingly intertwined with our daily lives, one of the challenges we face is learning to navigate the uncertainty that comes with it. This uncertainty is inherent to AI. AI models’ predictive accuracy has long been considered a core evaluation criterion when building an AI system. However, with the widespread use of AI in critical areas such as human resources, transportation, finance, medicine, and security, there is a growing need for these systems to be trustworthy—and traditional predictive accuracy alone is not sufficient to build trustworthy AI applications.</p>

<p>To better understand trustworthy AI, let’s start with its definition. <em>Trustworthy AI</em> <a contenteditable="false" data-primary="trustworthy AI" data-type="indexterm" id="trust-ai-1"/>is an umbrella term that refers to artificial intelligence systems that are designed and developed with principles such as fairness, privacy, and non-discrimination in mind, and with robust mechanisms to ensure reliability, security, and resilience. Within the AI community, this term is used interchangeably with <em>responsible AI</em>, <em>ethical AI</em>, <em>reliable AI</em>, and <em>values-driven AI</em>. Trustworthy AI systems must be adaptable to diverse and changing environments and robust against various types of disruptions, including cyber threats, data variability, and operational changes. They should operate transparently and be held accountable, with continuous monitoring and evaluation to respect human rights, including privacy and freedom from discrimination, and to ensure adherence to democratic values.</p>

<p>Trustworthy AI is a complex term that incorporates a long list of concepts and principles, which are visualized in <a data-type="xref" href="#chapter_1_figure_1_1748539916811473">Figure 1-1</a>. These concepts lay the foundation for understanding the EU AI Act<a contenteditable="false" data-primary="EU AI Act" data-startref="eua-1" data-type="indexterm" id="id305"/>.</p>

<p>Trustworthiness in AI is grounded in the three<a contenteditable="false" data-primary="lawfulness, ethics, and robustness" data-type="indexterm" id="id306"/> pillars of <em>lawfulness</em>, <em>ethics</em>, and <em>robustness<a contenteditable="false" data-primary="trustworthy AI" data-secondary="pillars of" data-type="indexterm" id="id307"/></em>. First, AI systems should be lawful, meaning they must comply with all relevant regulations to ensure a fair market, promote economic benefits, and protect citizens’ rights. Second, they must be built on ethical principles and values, incorporating input from all stakeholders and establishing appropriate feedback mechanisms. Finally, they must be robust, accounting for potential risks and ensuring safety at every stage of development and deployment.</p>

<figure><div class="figure" id="chapter_1_figure_1_1748539916811473"><img src="assets/taie_0101.png"/>
<h6><span class="label">Figure 1-1. </span>The foundation and seven requirements of trustworthy AI</h6>
</div></figure>

<p>Standing on these three pillars are seven key requirements that AI systems must implement to be deemed<a contenteditable="false" data-primary="trustworthy AI" data-secondary="requirements for" data-type="indexterm" id="id308"/> trustworthy:</p>

<ol>
	<li>
	<p>Human agency and oversight</p>
	</li>
	<li>
	<p>Technical robustness and safety</p>
	</li>
	<li>
	<p>Privacy and data governance</p>
	</li>
	<li>
	<p>Transparency</p>
	</li>
	<li>
	<p>Diversity, non-discrimination, and fairness</p>
	</li>
	<li>
	<p>Societal and environmental well-being</p>
	</li>
	<li>
	<p>Accountability</p>
	</li>
</ol>

<p>These come directly from the requirements outlined in the <a href="https://oreil.ly/nBhE6">Ethics Guidelines for Trustworthy AI</a> developed by the European<a contenteditable="false" data-primary="trustworthy AI" data-startref="trust-ai-1" data-type="indexterm" id="id309"/> Commission’s High-Level Expert Group on AI (AI HLEG)<a contenteditable="false" data-primary="High-Level Expert Group on AI (AI HLEG)" data-type="indexterm" id="id310"/>. In the following sections, I provide an explanation of each.</p>

<section class="pagebreak-before" data-pdf-bookmark="Human Agency and Oversight" data-type="sect2"><div class="sect2" id="chapter_1_human_agency_and_oversight_1748539916833176">
<h2 class="less_space">Human Agency and Oversight</h2>

<p>Human agency and oversight<a contenteditable="false" data-primary="human agency and oversight" data-type="indexterm" id="hao-1"/> are crucial in developing and operating AI systems. <em>Human agency</em> refers to the ability of individuals to make informed decisions and maintain control. AI systems should support this by providing transparency<a contenteditable="false" data-primary="transparency" data-type="indexterm" id="id311"/>, interpretability, and mechanisms for control and intervention that enable humans to understand and influence the system’s decisions and actions.</p>

<p><em>Human oversight</em> involves establishing governance processes and mechanisms that allow for human monitoring, evaluation, and intervention in the operation of AI systems. This includes ensuring transparency and interpretability of the systems’ decision-making processes based on the results produced by the AI models. Additionally, AI systems should provide human control mechanisms (e.g., the ability to override, adjust, or shut down the system when necessary). As depicted in <a data-type="xref" href="#chapter_1_figure_2_1748539916811507">Figure 1-2</a>, I distinguish between four different modes of human oversight:</p>

<dl>
	<dt>Human-in-command</dt>
	<dd>
	<p>This mode represents the highest level of human control<a contenteditable="false" data-primary="human agency and oversight" data-secondary="human-in-command" data-type="indexterm" id="id312"/>, where humans maintain ultimate authority and responsibility over the AI system. Humans must explicitly authorize any AI system action. They are at the core of decision-making processes and always make the final decisions. In this mode, humans oversee the AI system’s overall activity and can decide when and how to use the system in specific situations. For example, the <a href="https://oreil.ly/HFlbo">“AI cockpit” concept</a> demonstrates the human-in-command approach by providing a central user interface that enables operators to monitor AI systems’ effects on different user groups, evaluate results against technical and ethical criteria, and exercise the authority to disable the systems when necessary.</p>
	</dd>
	<dt>Human-in-the-loop</dt>
	<dd>
	<p>AI systems require human interaction<a contenteditable="false" data-primary="human agency and oversight" data-secondary="human-in-the-loop" data-type="indexterm" id="id313"/> at critical points in the decision-making process to add a layer of human experience and contextual understanding that AI currently lacks. Often, this involvement entails guiding the direction of decisions based on AI-generated predictions. The level of intervention and control might depend on the risk level of the system. Medical AI applications, for example, will typically require more intensive human intervention and control than less critical AI applications.</p>
	</dd>
	<dt>Human-on-the-loop</dt>
	<dd>
	<p>The human plays an observer role<a contenteditable="false" data-primary="human agency and oversight" data-secondary="human-on-the-loop" data-type="indexterm" id="id314"/>, monitoring the actions of the AI system and intervening if necessary.</p>
	</dd>
	<dt class="pagebreak-before">Human-out-of-the-loop</dt>
	<dd>
	<p>The AI system operates independently<a contenteditable="false" data-primary="human agency and oversight" data-secondary="human-out-of-the-loop" data-type="indexterm" id="id315"/>, without human intervention or real-time supervision. This scenario is possible when decisions based on the system’s predictions can be programmatically implemented. Often, this mode is applied in trading applications that require data processing at speed, where human oversight is not realistic.</p>
	</dd>
</dl>

<figure><div class="figure" id="chapter_1_figure_2_1748539916811507"><img src="assets/taie_0102.png"/>
<h6><span class="label">Figure 1-2. </span>The different levels of human oversight in AI</h6>
</div></figure>

<p>The modes described here are each suitable for different AI use cases. Regardless of the one you use, proper implementation of the human agency and oversight<a contenteditable="false" data-primary="human agency and oversight" data-startref="hao-1" data-type="indexterm" id="id316"/> requirement is crucial for designing and developing ethical AI systems that benefit from the strengths of AI while mitigating potential risks.</p>
</div></section>

<section data-pdf-bookmark="Technical Robustness and Safety" data-type="sect2"><div class="sect2" id="chapter_1_technical_robustness_and_safety_1748539916833240">
<h2>Technical Robustness and Safety</h2>

<p>To be trustworthy, AI systems<a contenteditable="false" data-primary="technical robustness and safety" data-type="indexterm" id="trs-1"/> need to be accurate, reliable, and able to repeat their results. They should also have a “backup plan” if something goes wrong. In computer science, <em>robustness</em> means a system can keep working correctly even when there are mistakes, unusual inputs, or unexpected situations. For AI, robustness means that a machine learning (ML) model can keep making good, reliable predictions despite facing different conditions, tricky inputs, or changes in the type of data it sees.</p>

<p class="fix_tracking">In simpler terms, a robust ML model can still give you the correct answers even if the information it receives is somewhat noisy, messy, or comes from a different source than the model was originally trained on. If these differences cause its performance to drop suddenly, then it’s not considered robust anymore. Achieving robustness is essential for building trust in AI—especially in areas like self-driving cars, cybersecurity systems, and healthcare, where a single mistake can have serious consequences.</p>

<p>There are three core layers to an AI system: the data it’s trained on, the algorithm it uses, and the software that puts them to use. Correspondingly, there are three levels of robustness:</p>

<ul>
	<li>
	<p><em>Data robustness</em> requires<a contenteditable="false" data-primary="technical robustness and safety" data-secondary="data robustness" data-type="indexterm" id="id317"/> training the system on a wide variety of data types and scenarios. This allows the model to “learn” to handle different situations and ensures it is less likely to fail when confronted with new or unexpected conditions. For example, a self-driving car’s AI should be trained on data from different weather conditions to handle rain, snow, and fog. In cybersecurity, an intrusion detection system should learn from many different types of network traffic and attack methods, so it can catch new threats as they emerge.</p>
	</li>
	<li>
	<p><em>Algorithmic robustness</em> <a contenteditable="false" data-primary="technical robustness and safety" data-secondary="algorithmic robustness" data-type="indexterm" id="id318"/>focuses on defending against attacks that try to trick the AI model itself. Some attacks occur at the moment of decision making by slightly altering the input to confuse the model. Others are executed during the training phase by introducing “poisoned” data that causes the model to make incorrect predictions later. A robust algorithm should be able to handle noisy or altered inputs, changes in data types, and even deliberately manipulated inputs designed to fool it. For example, a robust language model can deal with misspellings, slang, or strange grammar without losing accuracy.</p>
	</li>
	<li>
	<p><em>System-level robustness</em> looks<a contenteditable="false" data-primary="technical robustness and safety" data-secondary="system-level robustness" data-type="indexterm" id="id319"/> at the bigger picture—the entire lifecycle of the AI product, from how data is collected to how the model is built, tested, and finally used in the real world. By examining every step, we can spot risks before they lead to serious problems. This holistic approach ensures the entire AI pipeline, and its interaction with other systems, remains steady and secure.</p>
	</li>
</ul>

<p>In research, we often differentiate two types of robustness:</p>

<dl>
	<dt>Non-adversarial robustness</dt>
	<dd>
	<p>Can the model handle everyday challenges, like noisy inputs or slight changes in the environment, without losing accuracy?<a contenteditable="false" data-primary="technical robustness and safety" data-secondary="non-adversarial robustness" data-type="indexterm" id="id320"/></p>
	</dd>
	<dt>Adversarial robustness</dt>
	<dd>
	<p>Can the model resist “tricks” that attackers use intentionally to confuse it, such as subtly modified inputs designed to produce the wrong answer?<a contenteditable="false" data-primary="technical robustness and safety" data-secondary="adversarial robustness" data-type="indexterm" id="id321"/> </p>
	</dd>
</dl>

<p>To measure how robust an AI model is, practitioners and researchers look at a variety of measurements<a contenteditable="false" data-primary="technical robustness and safety" data-secondary="robustness, measures of" data-secondary-sortas="measures" data-tertiary="accuracy" data-type="indexterm" id="id322"/>:</p>

<dl>
	<dt>Accuracy</dt>
	<dd>
	<p>How many predictions the model gets right.</p>
	</dd>
	<dt>Error rate</dt>
	<dd>
	<p>How many incorrect predictions the model makes.</p>
	</dd>
	<dt>Sensitivity (recall)</dt>
	<dd>
	<p>The model’s ability to correctly identify items that have a given condition (true positives). For example, if we are implementing a binary classifier, of all the cases that should be labeled “positive,” how many are correctly identified?</p>
	</dd>
	<dt>Specificity</dt>
	<dd>
	<p>The model’s ability to correctly identify items that do not have a given condition (true negatives). For example, of all the cases that should be labeled “negative,” how many are correctly rejected?</p>
	</dd>
	<dt>Robustness curves</dt>
	<dd>
	<p>Graphs that show how the model’s performance changes when we introduce challenges, like more noise, missing information, or unusual inputs.</p>
	</dd>
</dl>

<p>All of these measurements can help us understand the model’s strengths and weaknesses. However, the best way to measure robustness typically depends on the specific problem and requirements of the AI system<a contenteditable="false" data-primary="technical robustness and safety" data-startref="trs-1" data-type="indexterm" id="id323"/>.</p>
</div></section>

<section data-pdf-bookmark="Privacy and Data Governance" data-type="sect2"><div class="sect2" id="chapter_1_privacy_and_data_governance_1748539916833316">
<h2>Privacy and Data Governance</h2>

<p>The second requirement of trustworthy AI systems is that they must ensure full respect for privacy<a contenteditable="false" data-primary="privacy" data-type="indexterm" id="priv-1"/> and personal data protection, in line with Articles 7 and 8 <a contenteditable="false" data-primary="Charter of Fundamental Rights of the European Union" data-type="indexterm" id="id324"/>of the <a href="https://oreil.ly/nPDrM">Charter of Fundamental Rights of the European Union</a>.</p>

<p>Many AI systems, such as recommender systems and systems that provide personalized predictions, require some amount of personally identifiable information (PII). Handling of PII is a subject to regulations within the EU and the US, under the General Data Protection Regulation (GDPR)<a contenteditable="false" data-primary="General Data Protection Regulation (GDPR)" data-type="indexterm" id="id325"/> and the California Consumer Privacy Act (CCPA)<a contenteditable="false" data-primary="California Consumer Privacy Act (CCPA)" data-type="indexterm" id="id326"/>.</p>

<p>By examining how an AI system collects, stores, processes, and shares personal or sensitive data, <em>p</em><em>rivacy </em><em>i</em><em>mpact </em><em>a</em><em>ssessments</em> (PIAs) help organizations spot areas where privacy might be at risk and guide them in modifying the system, implementing safeguards, and complying with relevant privacy laws and regulations. PIAs should ideally be conducted early in the design process, to proactively identify, assess, and mitigate potential privacy risks. This is an example of <em>privacy by design<a contenteditable="false" data-primary="privacy" data-secondary="privacy by design" data-type="indexterm" id="id327"/></em>, an engineering philosophy that involves embedding privacy protections into every phase of an AI system’s lifecycle—from data collection and preprocessing to model training, deployment, and maintenance. This ensures that privacy isn’t just treated as a legal afterthought or a check-the-box compliance step at the end of the development process, but as a fundamental design principle that helps organizations maintain user trust and meet regulatory requirements.</p>

<p>Privacy by design incorporates privacy considerations into the foundational components of AI systems. For AI engineers, this might involve using techniques such as minimal data collection, deidentification, and anonymization (encryption, <span class="keep-together">differential</span> privacy, other methods to protect sensitive information before it even reaches the model); adapting model architecture choices (for example, using federated learning); and continuous evaluation and monitoring. The related concept of <em>data governance</em> ensures that privacy principles are consistently applied, maintained, and improved within a structured organizational framework.</p>

<p>Data governance<a contenteditable="false" data-primary="data governance" data-type="indexterm" id="data-governance-1"/> is a data management function that guarantees the availability, usability, integrity, and security of the data collected and used in an organization. Its key elements are visualized in <a data-type="xref" href="#chapter_1_figure_3_1748539916811532">Figure 1-3</a>.</p>

<figure><div class="figure" id="chapter_1_figure_3_1748539916811532"><img src="assets/taie_0103.png"/>
<h6><span class="label">Figure 1-3. </span>Key elements of data governance (adapted from <a class="orm:hideurl" href="https://oreil.ly/4hIZp"><span class="plain">Data Governance: The Definitive Guide</span></a> by Evren Eryurek et al. [O’Reilly])</h6>
</div></figure>

<p>Data governance has become increasingly important as data volumes have grown, organizations have become more data-driven, and access to data has expanded. The ultimate goal of data governance is to enhance the trustworthiness of data, which is fundamental to trustworthy AI. There are three key aspects to this: discoverability, security, and accountability.</p>

<p><em>Discoverability</em> refers to the availability of the dataset’s metadata, data provenance (lineage), and glossary of domain entities. It’s essential for ensuring users and AI systems can easily access the data they need. Data governance establishes procedures that guarantee that the right data is accessed by the appropriate people in the organization and determine what data AI systems can access. <em>D</em><em>ata quality</em> is a related concept that is crucial in building trust in data: data should be correct, complete, timely, and integral.</p>

<p>In the context of AI engineering, data<em> </em><em>security</em> means protecting the data used to train, validate, and run models from unauthorized access, theft, tampering, or loss. Together with privacy, data security is an essential aspect of protecting data and ensuring adherence to regulations such as the GDPR  (or CCPA).</p>

<p>The third aspect of data governance, <em>accountability</em>, involves ensuring that everyone involved with an AI system, from data suppliers to model developers and operators, knows their role, can explain their choices, and can be held responsible for the system working ethically, legally, and safely.</p>

<p>AI development should respect the fundamental right to privacy at all points of the AI application lifecycle, including data collection, processing, and storage and model design, development, and deployment. Privacy issues can arise in various aspects of AI systems, such as when social media platforms using AI to analyze user behaviors and preferences inadvertently expose sensitive information through targeted advertising or data breaches, or when AI is used to enhance the capabilities of surveillance systems, leading to potential overreach in monitoring activities. This can result in a loss of anonymity and freedom, as every action can be watched and recorded. For example, governmental structures using facial recognition technologies in public spaces can track individuals without their consent, potentially leading to misuse of power and privacy violations.</p>

<p>AI systems can also amplify biases<a contenteditable="false" data-primary="bias" data-type="indexterm" id="id328"/> present in their training data. When these biases affect how data is collected, processed, or used, they can disproportionately impact the privacy of certain groups. For instance, AI-driven credit scoring models might use biased data that discriminates against certain racial or gender groups, raising concerns about fairness and privacy in relation to financial data.</p>

<p>Example metrics and mechanisms to define and track privacy<a contenteditable="false" data-primary="privacy" data-startref="priv-1" data-type="indexterm" id="id329"/> and data governance include<a contenteditable="false" data-primary="data governance" data-startref="data-governance-1" data-type="indexterm" id="id330"/>:</p>

<dl>
	<dt>Data encryption levels</dt>
	<dd>
	<p>These determine the degree of data protection during transmission and at rest.</p>
	</dd>
	<dt>Access controls</dt>
	<dd>
	<p>These are policies and tools that manage who can access or alter data.</p>
	</dd>
	<dt>Data retention and deletion policies</dt>
	<dd>
	<p>These ensure compliance<a contenteditable="false" data-primary="compliance" data-type="indexterm" id="compliance-1"/> with data minimization principles and regulations.</p>
	</dd>
</dl>
</div></section>

<section data-pdf-bookmark="Transparency" data-type="sect2"><div class="sect2" id="chapter_1_transparency_1748539916833377">
<h2>Transparency</h2>

<p>Data, AI models, and software systems that include AI components must be transparent and provide traceability. Regardless of the industry or use case, transparency<a contenteditable="false" data-primary="transparency" data-type="indexterm" id="trans-1"/> is the <a href="https://oreil.ly/_pir3">most commonly cited ethical principle</a> in existing AI <span class="keep-together">guidelines</span>.</p>

<p>Transparency in AI systems refers to the ability to understand how an AI system works internally and makes decisions. It provides comprehensible explanations about the AI system’s components, algorithms, decision-making process, and overall <span class="keep-together">functioning</span> to stakeholders such as users, developers, and regulators. Key dimensions of AI transparency include:</p>

<dl>
	<dt>Explainability</dt>
	<dd>
	<p>The ability to clearly articulate in human-understandable<a contenteditable="false" data-primary="transparency" data-secondary="explainability" data-type="indexterm" id="id331"/> terms how and why an AI system, encompassing its data sources, model components, and decision-making processes, arrived at a specific output or decision for a given case. This is important for building trust and accountability. As depicted in <a data-type="xref" href="#chapter_1_figure_4_1748539916811556">Figure 1-4</a>, explainable AI (XAI)<a contenteditable="false" data-primary="explainable AI (XAI)" data-type="indexterm" id="id332"/> models might not be the models with the highest accuracy, so depending on the use case requirements, engineers might select an explainable but less accurate AI algorithm. Techniques for XAI are used at every stage of the AI lifecycle, including analyzing data for model development, incorporating interpretability into the system architecture, and producing post-hoc explanations of system behavior.<sup><a data-type="noteref" href="ch01.html#id333" id="id333-marker">1</a></sup></p>
	</dd>
</dl>

<figure><div class="figure" id="chapter_1_figure_4_1748539916811556"><img src="assets/taie_0104.png"/>
<h6><span class="label">Figure 1-4. </span>Balancing explainability and accuracy is a common challenge in AI systems</h6>
</div></figure>

<dl>
	<dt>Data transparency</dt>
	<dd>
	<p>Openness about the training data used to build<a contenteditable="false" data-primary="transparency" data-secondary="data transparency" data-type="indexterm" id="id334"/> an AI model, including its sources, characteristics, and potential biases or limitations.</p>
	</dd>
	<dt>Algorithmic transparency</dt>
	<dd>
	<p>Visibility into the AI algorithms<a contenteditable="false" data-primary="transparency" data-secondary="algorithmic transparency" data-type="indexterm" id="id335"/> and how they process input data to generate outputs or decisions. This includes understanding the features, weights, and logic the model uses.</p>
	</dd>
	<dt>Governance transparency</dt>
	<dd>
	<p>Documenting key decisions made during an AI system’s development process<a contenteditable="false" data-primary="transparency" data-secondary="governance transparency" data-type="indexterm" id="id336"/>, establishing clear protocols and responsibilities, and ensuring organizational oversight. This aspect is particularly relevant to compliance with the EU AI Act.</p>
	</dd>
	<dt>Communication transparency</dt>
	<dd>
	<p>Sharing information about the AI system’s purpose<a contenteditable="false" data-primary="transparency" data-secondary="communication transparency" data-type="indexterm" id="id337"/>, capabilities, and limitations with relevant stakeholders in a timely, clear, and accessible manner.</p>
	</dd>
</dl>

<p>AI transparency aims to open the “black box” representing the internal operations of AI systems, which are often complex and opaque. This enables humans to understand the inner workings of the systems and audit them for errors or biases, fostering trust in their use.</p>

<p>Examples of metrics to define and track transparency in AI systems include the <span class="keep-together">following</span>:</p>

<dl>
	<dt>Explainability index</dt>
	<dd>
	<p>How easily the system’s decisions can be explained to users. This depends on the availability of explanations throughout the system’s lifecycle, the types of explanations provided (e.g., feature importance, counterfactual examples, visual aids), and the scope of those explanations (whether they address global model behavior or local, individual predictions).</p>
	</dd>
	<dt>Documentation completeness</dt>
	<dd>
	<p>Availability and clarity of documentation on the AI system’s purpose, functionality, limitations, model training details, and feature engineering process.</p>
	</dd>
	<dt>Algorithmic auditability</dt>
	<dd>
	<p>Ease of auditing AI algorithms for compliance<a contenteditable="false" data-primary="compliance" data-startref="compliance-1" data-type="indexterm" id="id338"/> and performance<a contenteditable="false" data-primary="transparency" data-startref="trans-1" data-type="indexterm" id="id339"/>.</p>
	</dd>
</dl>
</div></section>

<section data-pdf-bookmark="Diversity, Non-Discrimination, and Fairness" data-type="sect2"><div class="sect2" id="chapter_1_diversity_non_discrimination_and_fairness_1748539916833442">
<h2>Diversity, Non-Discrimination, and Fairness</h2>

<p>Discussion about bias and fairness<a contenteditable="false" data-primary="diversity, non-discrimination, and fairness" data-type="indexterm" id="dndf-1"/> in machine learning has become a hot topic over the past decade. Because data collected in an unequal manner and processed by non-diverse teams could potentially cause harm, incorporating diversity and inclusion throughout the entire AI system lifecycle is a clear requirement for trustworthy AI. This includes accessibility<a contenteditable="false" data-primary="accessibility" data-type="indexterm" id="id340"/>, a user-centric approach that guarantees that the usability of the AI system takes everyone into account—especially people with disabilities.</p>

<p><a href="https://oreil.ly/n3qlT">Inclusive engineering</a> is<a contenteditable="false" data-primary="diversity, non-discrimination, and fairness" data-secondary="inclusive engineering" data-type="indexterm" id="id341"/> defined as “the process of ensuring that engineering products and services are accessible and inclusive of all users, and are as free as possible from discrimination and bias, throughout their lifecycle.” This approach is crucial in the development of AI systems, extending beyond just technical engineering to encompass broader social considerations.</p>

<p>Potential biases<a contenteditable="false" data-primary="bias" data-type="indexterm" id="bias-1"/> should be identified and addressed at every stage of AI system development. It is vital to establish a well-defined strategy or set of procedures to mitigate bias and promote fairness, both in the collection and use of input data and in the algorithm design. In this book, I focus on the <a href="https://oreil.ly/srSRa">CRISP-ML(Q)</a> development<a contenteditable="false" data-primary="CRISP-ML(Q)" data-type="indexterm" id="id342"/> process model to specify a fair and effective AI system development strategy. This model requires establishing processes for testing and monitoring for potential biases and detecting non-representativeness in data across all phases of development.</p>

<p>Evaluating the complete end-to-end AI development workflow for fairness is important for building successful AI systems. Improving diversity and representativeness in AI systems is a step toward compliance with the EU AI Act and providing value for all users of those systems. Establishing a robust mechanism for flagging issues related to bias, discrimination, or poor performance—such as through bias detection tools, categorized reporting systems, and clear reporting guidance for affected individuals—helps developers and end users become aware of and address these issues.</p>

<p>An often underestimated and less technical aspect of an organization’s data culture is educational and awareness initiatives for bias and fairness in AI. These initiatives are intended to help AI product managers, designers, and engineers become more aware of the possible bias they can inject while designing and developing AI systems.</p>

<p>The EU AI Act ensures that every entity along the AI system’s value chain, from producer to deployer, is responsible for providing users with a fair and ethical experience. Several metrics are used to evaluate fairness in AI systems, and technical approaches for fairness improvement and bias mitigation can be applied before modeling (preprocessing), at the point of modeling (in-processing), or after modeling (post-processing), as illustrated in <a data-type="xref" href="#chapter_1_figure_5_1748539916811583">Figure 1-5</a>.</p>

<figure><div class="figure" id="chapter_1_figure_5_1748539916811583"><img src="assets/taie_0105.png"/>
<h6><span class="label">Figure 1-5. </span>Various metrics and approaches can be implemented at different stages of the modeling process to ensure equitable outcomes and reduce bias (source: <a href="https://oreil.ly/CGjLK"><em>https://oreil.ly/CGjLK</em></a><em>)</em></h6>
</div></figure>

<p><a data-type="xref" href="#chapter_1_table_1_1748539916818522">Table 1-1</a> lists some metrics that are commonly used to evaluate fairness and non-discrimination. These metrics quantify potential biases<a contenteditable="false" data-primary="bias" data-startref="bias-1" data-type="indexterm" id="id343"/> or disparities in the AI model’s predictions across different demographic groups<a contenteditable="false" data-primary="fairness metrics" data-type="indexterm" id="fmetrics"/>.</p>

<table id="chapter_1_table_1_1748539916818522">
	<caption><span class="label">Table 1-1. </span>Preprocessing, in-processing, and post-processing fairness metrics</caption>
	<thead>
		<tr>
			<th>Metric category/name</th>
			<th>Definition</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td class="subheading" colspan="2">Preprocessing</td>
		</tr>
		<tr>
			<td>
			<p>Statistical/demographic parity</p>
			</td>
			<td>
			<p>Ensures equal probability of being classified with positive labels across groups</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Disparate impact</p>
			</td>
			<td>
			<p>Measures the ratio of positive classification rates between unprivileged and privileged groups</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Individual fairness</p>
			</td>
			<td>
			<p>Ensures similar individuals receive similar treatment</p>
			</td>
		</tr>
		<tr>
			<td class="subheading" colspan="2">In-processing</td>
		</tr>
		<tr>
			<td>
			<p>Equal opportunity</p>
			</td>
			<td>
			<p>Ensures equal true positive rates across different groups</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Equalized odds</p>
			</td>
			<td>
			<p>Requires equal true positive and false positive rates across protected groups</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Overall accuracy equality</p>
			</td>
			<td>
			<p>Compares relative accuracy rates between different groups</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Treatment equality</p>
			</td>
			<td>
			<p>Considers the ratio of false negatives to false positives</p>
			</td>
		</tr>
		<tr>
			<td class="subheading" colspan="2">Post-processing</td>
		</tr>
		<tr>
			<td>
			<p>Test fairness/calibration</p>
			</td>
			<td>
			<p>Ensures equal probability of a positive outcome given a particular score across groups</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Well calibration</p>
			</td>
			<td>
			<p>Requires predicted probabilities to match actual probabilities across groups</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Balance for positive/negative class</p>
			</td>
			<td>
			<p>Ensures equal expected scores for positive and negative classes across groups</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Generalized entropy index</p>
			</td>
			<td>
			<p>Measures individual-level fairness impacts of classification outcomes</p>
			</td>
		</tr>
	</tbody>
</table>
</div></section>

<section data-pdf-bookmark="Societal and Environmental Well-Being" data-type="sect2"><div class="sect2" id="chapter_1_societal_and_environmental_well_being_1748539916833507">
<h2>Societal and Environmental Well-Being</h2>

<p>AI can have positive or negative impacts depending<a contenteditable="false" data-primary="societal and environmental well-being" data-type="indexterm" id="sewb-1"/> on how it is used. The EU AI Act addresses the societal and environmental implications of trustworthy AI.</p>

<p>AI technology has proven negative impacts on the environment. The primary environmental concerns associated with AI include energy consumption, carbon footprint, e-waste, and indirect environmental impacts. AI and machine learning models, especially large ones, require significant computational power (as visualized in <a data-type="xref" href="#chapter_1_figure_6_1748539916811608">Figure 1-6</a>). The data centers housing these processors thus require vast amounts of energy to run and to cool, which means large-scale AI systems have a significant carbon footprint. To mitigate this concern, efforts are being made to power data centers with renewable energy sources. The production of AI hardware also requires precious metals and rare earth elements, which can be environmentally damaging (in terms of extraction, resource depletion, and recycling challenges). Additionally, hardware quickly becomes obsolete, contributing to electronic waste.</p>

<figure><div class="figure" id="chapter_1_figure_6_1748539916811608"><img src="assets/taie_0106.png"/>
<h6><span class="label">Figure 1-6. </span>The amount of computing resources used to train deep learning models increased 300,000-fold between 2012 and 2018 (source: <a href="https://oreil.ly/WJOOw"><em>https://oreil.ly/WJOOw</em></a>)</h6>
</div></figure>

<p>Depending on its applications, AI can have indirect effects on the environment as well. For instance, AI-driven automation can lead to increased production capacities and potentially increased resource consumption. Conversely, AI can optimize systems to be more energy-efficient, reduce waste, or enhance resource management, thereby potentially having a positive impact.</p>

<p>As well as model accuracy, the proposed <a href="https://oreil.ly/jGAsk">“Green AI” approach</a> considers<a contenteditable="false" data-primary="Green AI" data-type="indexterm" id="id344"/> <em>efficiency</em>, measured as the number of floating-point operations (flops) required to generate a result, as a key evaluation criterion. Financial operations (FinOps)<a contenteditable="false" data-primary="FinOps" data-type="indexterm" id="id345"/>, a cloud financial management practice that helps organizations efficiently manage their cloud spending, can also be leveraged to address the environmental and financial impacts of AI.</p>

<p>With regard to the societal aspect, AI is changing the way we work in three key ways: by automating tasks, reshaping work processes, and affecting required job skills. Job displacement is an important concern, as AI has the potential to automate many <span class="keep-together">roles—especially</span> those that involve <a href="https://oreil.ly/7Pdjg">repetitive tasks</a>. At the same time, AI technology has the potential to create <a href="https://oreil.ly/zFqNJ">new job roles</a> that demand advanced technological and analytical skills, such as machine learning engineers, data scientists, and AI ethics specialists. Additionally, AI can drive the development of innovative products and services, opening up new career opportunities in emerging sectors like AI-driven digital assistants and smart devices.</p>

<p>A related concern is that the rise of AI is leading to increased <a href="https://oreil.ly/hW0gZ">job polarization</a>, with demand for high-skilled workers growing and low-skilled positions facing obsolescence. This worsens income inequality and creates challenges for those without access to education and training. As a result, AI in the workplace can lead to increased stress, anxiety, and job insecurity due to the fear of job loss and uncertainty about the future.</p>

<p>AI systems also have the potential to negatively impact society at large through misinformation. AI—particularly generative AI—can produce misinformation and disinformation at scale. This can harm democracy by contributing to misinformation in elections, spreading propaganda, and influencing voter behavior. Tools like ChatGPT, pi.ai, and perplexity.ai can easily create realistic but false content, which can be used to spread misinformation. AI-generated “deepfakes” (convincing but fake videos and images)<a contenteditable="false" data-primary="deepfakes" data-type="indexterm" id="id346"/> can be used to manipulate public opinion, and AI-driven content personalization can create “information bubbles” where individuals are exposed only to information that reinforces their existing beliefs, leading to increased polarization and social fragmentation.</p>

<p>The use of AI for surveillance<a contenteditable="false" data-primary="AI for surveillance" data-type="indexterm" id="id347"/> is also an area of concern with regard to societal well-being and privacy. Research has shown that at least 75 countries actively utilize AI technologies for surveillance (see <a data-type="xref" href="#chapter_1_figure_6_1748539916811608">Figure 1-6</a>)<em>.</em> This can lead to unauthorized data collection, privacy violations and unauthorized access to sensitive data, and potential misuse of personal information. In addition, use of AI in law enforcement, such as for predictive policing, can result in biased outcomes and discrimination against certain demographic groups.</p>

<figure><div class="figure" id="chapter_1_figure_7_1748539916811633"><img src="assets/taie_0107.png"/>
<h6><span class="label">Figure 1-7. </span>AI surveillance technology is being adopted by a larger number of countries and at a faster pace than experts typically believe (source: <a href="https://oreil.ly/9_iB7"><em>https://oreil.ly/9_iB7</em></a>)</h6>
</div></figure>

<p>Addressing these challenges requires robust ethical guidelines, transparent regulatory frameworks, and ongoing public dialogue to ensure that AI is developed and deployed to support democratic values and societal well-being<a contenteditable="false" data-primary="societal and environmental well-being" data-startref="sewb-1" data-type="indexterm" id="id348"/>.</p>
</div></section>

<section data-pdf-bookmark="Accountability" data-type="sect2"><div class="sect2" id="chapter_1_accountability_1748539916833564">
<h2>Accountability</h2>

<p>AI systems must be developed and operated responsibly. <em>AI accountability<a contenteditable="false" data-primary="Accountability" data-type="indexterm" id="id349"/></em> involves establishing mechanisms for holding the developers and users of AI systems responsible for the impacts of those systems throughout the entire development cycle.</p>

<p>Accountability implies that information about the system’s purpose, design, data, and processes is available to internal and external auditors. As an example, we can refer to Google’s Responsible Generative AI Toolkit<a contenteditable="false" data-primary="Google's Responsible Generative AI Toolkit" data-type="indexterm" id="id350"/>, which covers risk and mitigation techniques to address safety, privacy, fairness, and accountability (see <a data-type="xref" href="#chapter_1_figure_8_1748539916811664">Figure 1-8</a>). This includes maintaining detailed documentation and records of the AI development process, decision making, and outcomes to enable traceability. Logging and recordkeeping are essential for accountability. Additionally, accountability requires that an AI system can explain or justify its decisions. Finally, accountability ensures that there are mechanisms and processes for redress in place to minimize or correct negative impacts or unfair outcomes caused by AI systems. AI systems should provide appropriate opportunities for feedback, relevant explanations, and defined procedures for escalating concerns.</p>

<figure><div class="figure" id="chapter_1_figure_8_1748539916811664"><img src="assets/taie_0108.png"/>
<h6><span class="label">Figure 1-8. </span>Google’s Responsible Generative AI Toolkit takes a holistic approach to accountability (source: <a href="https://oreil.ly/maEI5"><em>https://oreil.ly/maEI5</em></a>)</h6>
</div></figure>

<p class="pagebreak-before">In the context of trustworthy AI, key mechanisms that should be implemented to create accountability include:</p>

<dl>
	<dt>Clear responsibility guidelines and processes</dt>
	<dd>
	<p>Establish guidelines and clear responsibilities for various stakeholders involved in the AI system lifecycle, including developers, deployers, and users.</p>
	</dd>
	<dt>Transparency and explainability</dt>
	<dd>
	<p>Maintain detailed documentation of the AI development process, training data, algorithms used, and decision-making criteria to enable traceability. Ensure that AI systems are transparent about their capabilities, limitations, and decision-making processes. Provide clear explanations for decisions made based on the AI predictions.</p>
	</dd>
	<dt>Human oversight and intervention</dt>
	<dd>
	<p>Implement human checks and oversight, especially for high-stakes AI decisions, with the ability to override the AI when needed.</p>
	</dd>
	<dt>Auditing and evaluation</dt>
	<dd>
	<p>Conduct regular internal and third-party audits to identify and eliminate biases, ensuring compliance with regulations and ethical standards. By systematically auditing their AI systems using the CRISP-ML(Q) process model<a contenteditable="false" data-primary="CRISP-ML(Q)" data-type="indexterm" id="id351"/>, organizations can assess the systems’ quality, reliability, and trustworthiness. The audit depth and focus areas can be tailored based on the risk and criticality of the use case.</p>
	</dd>
	<dt>Redress and complaint mechanisms</dt>
	<dd>
	<p>Establish user-friendly channels for submitting complaints, feedback, or requests for explanations about AI decisions. Provide clear processes for affected individuals to challenge decisions and seek remedy.</p>
	</dd>
</dl>

<p>Trustworthy AI<a contenteditable="false" data-primary="trustworthy AI" data-type="indexterm" id="id352"/> is the foundational concept behind the legislation of the EU AI Act. You should now have a general understanding of the seven key requirements of trustworthy AI systems. Next, we’ll take a look at how the Act itself is structured.</p>
</div></section>
</div></section>

<section data-pdf-bookmark="The EU AI Act in a Nutshell" data-type="sect1"><div class="sect1" id="chapter_1_the_eu_ai_act_in_a_nutshell_1748539916833653">
<h1>The EU AI Act in a Nutshell</h1>

<p>The EU AI Act<a contenteditable="false" data-primary="EU AI Act" data-type="indexterm" id="eua-2"/>  focuses  on promoting human-centered and trustworthy artificial intelligence. Its primary objective is to foster innovation while ensuring a high level of protection for health, safety, and fundamental rights as outlined in the Charter—including democracy, the rule of law, and environmental protection—and mitigating the potential harmful effects of AI systems in use in the European Union. To fully understand the EU AI Act, it is essential to grasp its scope, who it applies to, and the timeline for compliance with the AI system requirements. This section provides a high-level overview of the EU AI Act. I’ll explain the detailed requirements for different risk categories later in the book.</p>

<p>Per the <a href="https://oreil.ly/6MMz1">General Provisions</a>, the Act establishes:</p>

<ol>
	<li>
	<p>“Harmonized rules for the placing on the market, the putting into service, and the use of AI systems in the Union;</p>
	</li>
	<li>
	<p>Prohibitions of certain AI practices;</p>
	</li>
	<li>
	<p>Specific requirements for high-risk AI systems and obligations for operators of such systems;</p>
	</li>
	<li>
	<p>Harmonized transparency rules for certain AI systems;</p>
	</li>
	<li>
	<p>Harmonized rules for the placing on the market of general-purpose AI models;</p>
	</li>
	<li>
	<p>Rules on market monitoring, market surveillance, governance and enforcement;</p>
	</li>
	<li>
	<p>Measures to support innovation, with a particular focus on [small and medium-sized enterprises], including startups.”</p>
	</li>
</ol>

<p>It is structured into the following main sections<a contenteditable="false" data-primary="EU AI Act" data-secondary="structure" data-type="indexterm" id="id353"/>:</p>

<dl>
	<dt>Chapters</dt>
	<dd>
	<p>The Act is divided into 13 chapters covering different aspects, such as general provisions, prohibited AI practices, requirements for high-risk AI systems, governance, etc.</p>
	</dd>
	<dt>Articles</dt>
	<dd>
	<p>Each chapter contains one or more articles laying out the specific rules and obligations. The articles are numbered sequentially throughout the Act and may be grouped into sections within each chapter. For example, Article 6 under Section 1 of Chapter III covers the “Classification Rules for High-Risk AI Systems.”</p>
	</dd>
	<dt>Annexes</dt>
	<dd>
	<p>The Act has 13 annexes that provide supplementary information such as lists, definitions, and procedures. You can navigate to a specific Annex by its number or title, e.g., “Annex III” or “High-Risk AI Systems Referred to in Article 6(2).”</p>
	</dd>
	<dt>Recitals</dt>
	<dd>
	<p>The recitals are numbered paragraphs that explain the rationale and context behind the Act’s provisions. They can help readers interpret the articles but are not binding.</p>
	</dd>
</dl>

<section class="pagebreak-before" data-pdf-bookmark="Definitions" data-type="sect2"><div class="sect2" id="chapter_1_definitions_1748539916833721">
<h2 class="less_space">Definitions</h2>

<p>Chapter I, Article 3 of the EU AI Act provides a set of definitions for terms used in the legislation. Perhaps most importantly for the purposes of this book, it defines an <em>AI system<a contenteditable="false" data-primary="AI system" data-seealso="AI model" data-type="indexterm" id="id354"/> </em>as:</p>

<blockquote>
<p>A machine-based system that is designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment, and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments.</p>
</blockquote>

<p>Note, however, that given the rapid pace and unpredictability of technological and AI development, this definition is not entirely static, and a dynamic regulatory tool has been integrated into the Act to allow it to adapt over time.</p>

<p>The definition covers a wide range of AI techniques and approaches, including:</p>

<ul>
	<li>
	<p>Machine learning methods (such as supervised, unsupervised, and semi-supervised machine learning)</p>
	</li>
	<li>
	<p>Deep learning methods</p>
	</li>
	<li>
	<p>Reinforcement learning</p>
	</li>
	<li>
	<p>Logic- and knowledge-based methods (such as logic programming, expert systems, inference and deductive engines, reasoning engines)</p>
	</li>
	<li>
	<p>Statistical approaches</p>
	</li>
	<li>
	<p>Bayesian methods</p>
	</li>
	<li>
	<p>Search and optimization approaches</p>
	</li>
</ul>

<p>Pretrained generative models such as BERT, DALL·E, Claude, Mistral, and GPT have become increasingly popular in recent years. They are developed using large-scale self-supervised learning on massive datasets and can be adjusted to various downstream tasks. These are commonly referred to as <em>general-purpose AI (GPAI)<a contenteditable="false" data-primary="AI model" data-type="indexterm" id="id355"/> model</em><em>s</em>, <a contenteditable="false" data-primary="general-purpose AI (GPAI) models" data-type="indexterm" id="id356"/>which the Act defines as follows:</p>

<blockquote>
<p>An AI model . . . trained with a large amount of data using self-supervision at scale, that displays significant generality and is capable of competently performing a wide range of distinct tasks regardless of the way the model is placed on the market and that can be integrated into a variety of downstream systems or applications, except AI models that are used for research, development or prototyping activities before they are placed on the market</p>
</blockquote>

<p>The EU AI Act aims to ensure that the previously mentioned techniques used in digital and physical products, services, or systems are safe and respect existing laws on fundamental rights and European Union values.</p>
</div></section>

<section data-pdf-bookmark="Key Players from Creation to Market Operation" data-type="sect2"><div class="sect2" id="chapter_1_key_players_from_creation_to_market_operation_1748539916833778">
<h2>Key Players from Creation to Market Operation</h2>

<p>The EU AI Act is a legal framework for developing, distributing, and using AI in the EU. This regulatory framework applies to companies and persons that make, bring in, or distribute AI systems or general-purpose AI models in the EU, even if those entities are located in a country outside the EU. <a data-type="xref" href="#chapter_1_figure_10_1748539916811704">Figure 1-9</a> visualizes the key players<a contenteditable="false" data-primary="key players" data-type="indexterm" id="id357"/> who are affected by the EU AI Act throughout the complete AI system lifecycle.</p>

<figure><div class="figure" id="chapter_1_figure_10_1748539916811704"><img src="assets/taie_0109.png"/>
<h6><span class="label">Figure 1-9. </span>Key players</h6>
</div></figure>

<p>Article 3 defines these roles as follows:</p>

<ul>
	<li>
	<p>A <em>provider<a contenteditable="false" data-primary="key players" data-secondary="provider" data-type="indexterm" id="id358"/></em> is a natural or legal person (company, organization, or other body) that develops an AI system or GPAI model (or has one developed) and puts it on the market or uses it, whether for payment or for free, under their own name or brand.</p>
	</li>
	<li>
	<p>An <em>importer<a contenteditable="false" data-primary="key players" data-secondary="importer" data-type="indexterm" id="id359"/></em> is a natural or legal person in the EU who places on the market an AI system bearing the name or trademark of an entity in a non-EU country.</p>
	</li>
	<li>
	<p>A <em>distributor<a contenteditable="false" data-primary="key players" data-secondary="distributor" data-type="indexterm" id="id360"/></em> is a natural or legal person in the supply chain, other than the provider or the importer, that makes an AI system available on the EU market.</p>
	</li>
	<li>
	<p>An <em>authorized representative<a contenteditable="false" data-primary="key players" data-secondary="authorized representative" data-type="indexterm" id="id361"/></em> is a natural or legal person in the EU who has been given written permission by the provider of an AI system or GPAI model to carry out the responsibilities and procedures outlined in the Act on their behalf.</p>
	</li>
	<li>
	<p>A <em>deployer<a contenteditable="false" data-primary="key players" data-secondary="deployer" data-type="indexterm" id="id362"/></em> is a natural or legal person using an AI system for professional <span class="keep-together">activities</span>.</p>
	</li>
</ul>

<p>A <em>user</em> <a contenteditable="false" data-primary="key players" data-secondary="user" data-type="indexterm" id="id363"/>is considered to be any natural person or group of persons who use or are otherwise affected by an AI system.</p>
</div></section>

<section data-pdf-bookmark="Classification of AI Systems by Risk Levels" data-type="sect2"><div class="sect2" id="chapter_1_classification_of_ai_systems_by_risk_levels_1748539916833835">
<h2>Classification of AI Systems by Risk Levels</h2>

<p>Another important aspect of the EU AI Act is its risk-based approach<a contenteditable="false" data-primary="risk levels" data-type="indexterm" id="risk-levels"/><a contenteditable="false" data-primary="AI system" data-secondary="risk level classification" data-see="risk levels" data-type="indexterm" id="id364"/>. According to the regulation’s classification system, AI systems are categorized into four risk levels:</p>

<ol class="two-col">
	<li>
	<p>Prohibited</p>
	</li>
	<li>
	<p>High risk</p>
	</li>
	<li>
	<p>Limited risk</p>
	</li>
	<li>
	<p>Minimal risk</p>
	</li>
</ol>

<p>It is also important to note that GPAI models are specifically regulated and classified under this Act. In <a data-type="xref" href="ch07.html#chapter_7_toward_trustworthy_general_purpose_ai_and_generati_1748539924538638">Chapter 7</a>, I will lay out the foundation for understanding the requirements for compliance with the Act for GPAI models, and their relation to the currently popular generative AI.</p>

<p>The EU AI Act prohibits certain AI systems that are considered to pose an unacceptable risk<a contenteditable="false" data-primary="risk levels" data-secondary="prohibited risk" data-type="indexterm" id="id365"/>. The bans are aimed at AI systems that could heavily influence or harm people’s decision making or infringe upon their rights. Specifically, the Act prohibits AI practices such as using manipulative subliminal techniques, exploiting vulnerabilities based on age, disability, or social circumstances, and making high-stakes assessments based on profiling or predictive traits without sufficient human oversight. The legislation also prohibits the unregulated use of “real-time” biometric identification in public spaces, unless under strict conditions for law enforcement purposes related to significant public safety concerns. (See <a href="https://oreil.ly/0KGOH">Chapter II, Article 5</a>, for more information.)</p>

<p><a data-type="xref" href="#chapter_1_figure_11_1748539916811723">Figure 1-10</a> illustrates the risk categories and their expected distribution. Roughly 20% of all AI systems are expected to be classified as high risk.</p>

<figure><div class="figure" id="chapter_1_figure_11_1748539916811723"><img src="assets/taie_0110.png"/>
<h6><span class="label">Figure 1-10. </span>AI system risk categories</h6>
</div></figure>

<p class="pagebreak-before">High-risk AI systems<a contenteditable="false" data-primary="risk levels" data-secondary="high risk" data-type="indexterm" id="id366"/>, as defined by the EU AI Act, either are intended to be used as safety components of products or are products themselves (see <a href="https://oreil.ly/_o2pw">Chapter III</a>). These AI systems are subject to specific legislation and require third-party conformity assessments. In addition, high-risk AI systems specified in a designated list (Annex III) are subject to stringent compliance requirements due to their potential impact on the health, safety, or fundamental rights of individuals. These include systems in areas such as biometrics, critical infrastructure, education, etc.</p>

<p>Like many products traded on the extended Single Market in the European Economic Area (EEA), AI systems classified as high risk must receive the CE marking (<a data-type="xref" href="#chapter_1_figure_12_1748539916811741">Figure 1-11</a>)<a contenteditable="false" data-primary="CE marking" data-type="indexterm" id="id367"/> to be certified within the EU. This mark indicates that products sold in the EEA have been evaluated to meet stringent safety, health, and environmental protection requirements.</p>

<figure><div class="figure" id="chapter_1_figure_12_1748539916811741"><img src="assets/taie_0111.png"/>
<h6><span class="label">Figure 1-11. </span>The CE mark</h6>
</div></figure>

<p>Limited-risk AI systems<a contenteditable="false" data-primary="risk levels" data-secondary="limited risk" data-type="indexterm" id="id368"/> pose lower risks, mostly in the form of manipulation, deception, or impersonation. This category includes systems like chatbots and generative AI systems capable of producing deepfakes. For limited-risk AI systems, the main obligation is transparency—providers must disclose that the output is AI-generated and users must be made aware they are interacting with an AI system. There are also requirements to label deepfakes<a contenteditable="false" data-primary="deepfakes" data-type="indexterm" id="id369"/> clearly.</p>

<p>The last category is minimal-risk AI systems<a contenteditable="false" data-primary="risk levels" data-secondary="minimal risk" data-type="indexterm" id="id370"/>. According to the EU AI Act, these are systems that pose little to no risk to people’s safety, fundamental rights, or privacy. They include AI applications like video games, spam filters, or simple image editing tools that perform narrow tasks with limited decision-making capabilities.</p>

<p>It is important to understand that the EU AI Act classifies use cases (AI systems and GPAI models) and not the AI/ML technologies or algorithms themselves. Proper classification has an impact on the estimation of the AI system’s requirements, because different risk categories imply different governance and MLOps architectural decisions and obligations. However, determining the risk level of an AI system can be challenging as it relies on various factors and involves classifying how the capabilities of a nondeterministic system will affect users and systems that may interact with it in the future. In <a data-type="xref" href="ch04.html#chapter_4_ai_system_assessment_and_tailoring_ai_engineering_1748539919034657">Chapter 4</a>, I outline a risk classification framework for AI systems.</p>

<p>Note that scientific research and military applications of AI fall outside the scope of the EU AI Act. However, so-called “dual use” technologies—systems that can be used for both civilian and military purposes—may still be subject to the Act if used in civilian contexts.<a contenteditable="false" data-primary="risk levels" data-startref="risk-levels" data-type="indexterm" id="id371"/></p>
</div></section>

<section data-pdf-bookmark="Enforcement and Implementation" data-type="sect2"><div class="sect2" id="chapter_1_enforcement_and_implementation_1748539916833890">
<h2>Enforcement and Implementation</h2>

<p>The implementation of the EU AI Act<a contenteditable="false" data-primary="enforcement and implementation" data-type="indexterm" id="ei-1"/> follows a structured timeline<a contenteditable="false" data-primary="enforcement and implementation" data-secondary="timeline" data-type="indexterm" id="id372"/>. Here are the key milestones and deadlines:</p>

<dl>
	<dt>EU AI Act enters into force (August 2024)</dt>
	<dd>
	<p>The Act is effective 20 days after it is published in the Official Journal of the EU.</p>
	</dd>
	<dt>Six months after entry into force (Q4 2024–Q1 2025)</dt>
	<dd>
	<p>Prohibitions on unacceptable-risk AI systems are effective.</p>
	</dd>
	<dt>Nine months after entry into force (Q1 2025)</dt>
	<dd>
	<p>Codes of practice for GPAI models must be finalized.</p>
	</dd>
	<dt>Twelve months after entry into force (Q2–Q3 2025)</dt>
	<dd>
	<p>Obligations on providers of GPAI models are effective.</p>
	</dd>
	<dt>Eighteen months after entry into force (Q4 2025–Q1 2026)</dt>
	<dd>
	<p>The Commission implements acts on post-market monitoring for high-risk AI systems.</p>
	</dd>
	<dt>Twenty-four months after entry into force (Q2–Q3 2026)</dt>
	<dd>
	<p>Obligations on high-risk AI systems listed in Annex III become applicable. Member states must have implemented rules on penalties and established AI regulatory sandboxes.</p>
	</dd>
	<dt>Thirty-six months after entry into force (Q4 2026–Q1 2027)</dt>
	<dd>
	<p>Obligations for high-risk AI systems not prescribed in Annex III but subject to existing EU product safety laws are effective.</p>
	</dd>
	<dt>By the end of 2030</dt>
	<dd>
	<p>Obligations for specific AI systems that are components of large-scale EU IT systems in areas like security and justice become applicable.</p>
	</dd>
</dl>

<p>This timeline, depicted in <a data-type="xref" href="#chapter_1_figure_13_1748539916811763">Figure 1-12</a>, implies that organizations that create high-risk AI systems today have a grace period to prepare their internal processes. They are responsible for complying fully with the provisions of the Act by the end of the grace period (June 2026).</p>

<figure><div class="figure" id="chapter_1_figure_13_1748539916811763"><img src="assets/taie_0112.png"/>
<h6><span class="label">Figure 1-12. </span>Timeline outlining the key milestones and deadlines for the implementation of the EU AI Act</h6>
</div></figure>
</div></section>

<section data-pdf-bookmark="The Full Picture of Compliance" data-type="sect2"><div class="sect2" id="chapter_1_the_full_picture_of_compliance_1748539916833947">
<h2>The Full Picture of Compliance</h2>

<p>Many<a contenteditable="false" data-primary="compliance" data-type="indexterm" id="compliance-2"/> organizations using and embedding AI technology into their products are asking the same question: “What does the EU AI Act mean for us?” The Act aims to ensure that all digital and physical products that incorporate AI—whether as a feature or as a core function—are used in a safe and ethical manner, in line with EU fundamental rights. To this end, the Act treats AI systems as regulated products. Like other products in the EU market, AI systems that are considered high risk must be CE marked to indicate compliance, as described earlier, or they are not permitted for use. <a data-type="xref" href="#chapter_1_table_2_1748539916818552">Table 1-2</a> briefly outlines the process steps needed to meet the EU AI Act’s requirements for this category of AI systems<a contenteditable="false" data-primary="compliance" data-secondary="operationalization" data-type="indexterm" id="id373"/>.</p>

<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Again, the author is not a lawyer. This book does not provide legal advice.</p>
</div>

<table class="striped" id="chapter_1_table_2_1748539916818552">
	<caption><span class="label">Table 1-2. </span>The end-to-end process for EU AI Act compliance</caption>
	<thead>
		<tr>
			<th>EU AI Act compliance step</th>
			<th>Guiding questions</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>AI system inventory and risk classification</p>
			</td>
			<td>
			<ul>
				<li>How many AI systems are in place already or are intended to be put in production?</li>
				<li>What risk categories do each of these AI systems belong to?</li>
				<li>Do any of them utilize models that are classified as systemic risk GPAI models?</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Identification of compliance requirements</p>
			</td>
			<td>
			<ul>
				<li>What requirements do we need to fulfill?</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Compliance operationalization</p>
			</td>
			<td>
			<ul>
				<li>What processes, structures, engineering practices, and roles need to be established to comply with the Act?</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Pre-market compliance verification</p>
			</td>
			<td>
			<ul>
				<li>What has to be done before placing AI systems on the market and putting them into service?</li>
				<li>What internal and external conformity assessments are required for compliance verification?</li>
				<li>How do we CE-mark our AI systems?</li>
				<li>Where should our AI systems be registered?</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Post-market continuous compliance</p>
			</td>
			<td>
			<ul>
				<li>What has to be done to ensure compliance after putting the AI systems into service?</li>
			</ul>
			</td>
		</tr>
	</tbody>
</table>

<p>Let’s take a closer look at these essential steps for providers and deployers of AI systems  to ensure conformity with the EU AI Act:</p>

<dl>
	<dt>1. AI system inventory and risk classification</dt>
	<dd>
	<p>Before setting<a contenteditable="false" data-primary="compliance" data-secondary="AI system inventory and risk classification" data-type="indexterm" id="id374"/> up the technical and organizational processes for achieving compliance, you must determine the required scope of compliance measures. Start with an inventory of all AI systems that are currently in use or in development. Then assign a risk level to each of those systems (I discuss risk classification for AI systems in detail in <a data-type="xref" href="ch04.html#chapter_4_ai_system_assessment_and_tailoring_ai_engineering_1748539919034657">Chapter 4</a>). Any AI use cases that fall into the prohibited category must be discontinued.</p>
	</dd>
	<dt>2. Identification of compliance requirements</dt>
	<dd>
	<p>The compliance measures to implement will depend on the results of the risk classification. It’s important to distinguish between obligations for AI systems and for GPAI models used in those systems, which may be subject to transparency obligations or systemic risk requirements. The Act lays out concrete requirements for both categories.</p>
	</dd>
	<dt>3. Compliance operationalization</dt>
	<dd>
	<p>Identifying compliance requirements allows you to define the scope of the technical and organizational processes needed for conformity with the EU AI Act. This includes establishing governance structures, engineering practices, and clearly defined roles. The Act outlines specific obligations, such as implementing risk management measures, maintaining technical documentation, ensuring human oversight, and guaranteeing the accuracy, robustness, and security of AI systems. This phase, which is the focus of this book, also involves setting up a quality management system that covers testing, incident reporting, data governance, record retention, and logging.</p>
	</dd>
	<dt>4. Pre-market compliance verification</dt>
	<dd>
	<p>The next step is to demonstrate compliance through internal and, where necessary, external conformity assessments. Once these are completed, the AI systems must be CE marked and registered in the EU database. These steps are mandatory before the systems can be placed on the market or put into service.</p>
	</dd>
	<dt>5. Post-market continuous compliance</dt>
	<dd>
	<p>The final phase focuses on ensuring that the AI systems continue to meet the requirements of the Act throughout their lifecycle. This involves demonstrating continued adherence to compliance standards, regardless of changes to the system, through continuous monitoring of system performance and addressing any issues that arise to maintain alignment with regulatory requirements.</p>
	</dd>
</dl>
</div></section>

<section data-pdf-bookmark="Penalties for EU AI Act Violation" data-type="sect2"><div class="sect2" id="chapter_1_penalties_for_eu_ai_act_violation_1748539916834006">
<h2>Penalties for EU AI Act Violation</h2>

<p>The EU AI Act introduces significant penalties<a contenteditable="false" data-primary="penalties" data-type="indexterm" id="pen-1"/> and fines for companies that violate its rules and requirements. <a data-type="xref" href="#chapter_1_table_3_1748539916818575">Table 1-3</a> provides an overview.</p>

<table class="striped" id="chapter_1_table_3_1748539916818575">
	<caption><span class="label">Table 1-3. </span>Penalties for violating the terms of EU AI Act</caption>
	<thead>
		<tr>
			<th>Violation</th>
			<th>Penalties</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Prohibited AI practices (AI practices listed in Article 5, such as exploiting vulnerabilities, social scoring, real-time biometric identification in public spaces, etc.)</p>
			</td>
			<td>
			<p>Administrative fines of up to €35 million or 7% of total worldwide annual turnover for the preceding financial year, whichever is higher</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Noncompliance with requirements for high-risk AI systems under Article 10</p>
			</td>
			<td>
			<p>Administrative fines of up to €30 million or 6% of total worldwide annual turnover, whichever is higher</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Noncompliance with other obligations (apart from Articles 5 and 10)</p>
			</td>
			<td>
			<p>Administrative fines of up to €20 million or 4% of total worldwide annual turnover, whichever is higher</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Providing incorrect, incomplete, or misleading information to authorities</p>
			</td>
			<td>
			<p>Administrative fines of up to €10 million or 2% of total worldwide annual turnover, whichever is higher</p>
			</td>
		</tr>
	</tbody>
</table>
</div></section>
</div></section>

<section data-pdf-bookmark="Existing AI Regulations and Standards" data-type="sect1"><div class="sect1" id="chapter_1_existing_ai_regulations_and_standards_1748539916834063">
<h1>Existing AI Regulations and Standards</h1>

<p>The EU AI Act serves as a comprehensive regulatory model for non-EU nations, underscoring the balance between innovation and trustworthy AI<a contenteditable="false" data-primary="trustworthy AI" data-type="indexterm" id="id375"/>. Its strict regulations on biometric systems and high-risk AI applications set a high standard for AI governance<a contenteditable="false" data-primary="AI governance" data-type="indexterm" id="id376"/> globally. Let’s briefly review the existing<a contenteditable="false" data-primary="UNESCO AI ethics recommendations" data-type="indexterm" id="id377"/> landscape of AI regulations:</p>

<dl>
	<dt>UNESCO AI ethics recommendations</dt>
	<dd>
	<p>The <a href="https://oreil.ly/Qr8eM">Recommendation on the Ethics of Artificial Intelligence</a>, adopted in November 2021 by UNESCO’s 193 Member States, provides a framework for ethical AI development. It outlines core values and principles for the ethical development and deployment of AI, including respect for human rights, inclusion and diversity, fairness and non-discrimination, transparency and explainability, accountability<a contenteditable="false" data-primary="Accountability" data-type="indexterm" id="id378"/>, safety and security, and sustainability. The recommendation aims to influence ethical AI practices globally and includes innovative tools and methodologies to translate ethical<a contenteditable="false" data-primary="US executive order on trustworthy AI" data-type="indexterm" id="id379"/> principles into practice.</p>
	</dd>
	<dt>US executive order on trustworthy AI</dt>
	<dd>
	<p>The <a href="https://oreil.ly/zRfHi">Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence</a>, issued in October 2023, emphasizes the potential benefits and risks of AI. It highlights the importance of responsible AI governance to address societal issues and prevent negative consequences such as fraud, bias, and threats to national security. The order stresses the need for collaboration across government, industry, academia, and civil society to ensure the safe and responsible development and application of AI<a contenteditable="false" data-primary="China generative AI services law" data-type="indexterm" id="id380"/>.</p>
	</dd>
	<dt>China generative AI services law</dt>
	<dd>
	<p>The <a href="https://oreil.ly/Axc-r">Interim Measures for the Administration of Generative Artificial Intelligence Services</a>, jointly issued by the Cyberspace Administration of China (CAC) and six other government agencies in July 2023, signal China’s proactive approach to regulating generative AI services (including models and related technologies that produce text, graphics, audio, and video). The regulation applies to all entities offering these services to the general Chinese population. The Interim Measures recognize the potential for foreign investment while also promoting innovation and research. Future artificial intelligence laws are expected to expand the regulation’s scope beyond generative AI. Given the potential penalties or shutdowns<a contenteditable="false" data-primary="penalties" data-startref="pen-1" data-type="indexterm" id="id381"/> for noncompliant services operating in China, it is essential to ensure compliance.</p>
	</dd>
	<dt>National Institute of Standards and Technology (NIST) AI Risk Management <span class="keep-together">Framework</span></dt>
	<dd>
	<p>The NIST <a href="https://oreil.ly/sISXu">Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile</a> released in July 2024 identifies 12 key risks unique to or exacerbated by GenAI technologies<a contenteditable="false" data-primary="National Institute of Standards and Technology (NIST)" data-type="indexterm" id="id382"/> that organizations<a contenteditable="false" data-primary="National Institute of Standards and Technology (NIST)" data-secondary="AI Risk Management Framework" data-type="indexterm" id="id383"/> should take care to identify and mitigate. These include risks related to chemical, biological, radiological, or nuclear information, dangerous or violent recommendations, data privacy, environmental impacts, information integrity and security, and intellectual property. The framework proposes over 400 actions organizations can take to manage these risks<a contenteditable="false" data-primary="IEEE SA recommended practice" data-type="indexterm" id="id384"/>.</p>
	</dd>
	<dt>Institute of Electrical and Electronics Engineers Standards Association (IEEE SA) recommended practice</dt>
	<dd>
	<p>The IEEE’s <a href="https://oreil.ly/Pd2qj">Recommended Practice for Organizational Governance of Artificial Intelligence</a> outlines governance criteria for AI development and use within organizations, including safety, transparency, accountability, responsibility, and bias minimization. It also provides steps for implementation, auditing, training, and compliance. The document aims to promote responsible AI practices by integrating internal governance structures with external AI governance <span class="keep-together">instruments</span>.</p>
	</dd>
</dl>

<p>These laws and frameworks collectively aim to regulate AI in a way that promotes innovation while ensuring safety, transparency, and respect for human rights, similar to the objectives of the <a contenteditable="false" data-primary="EU AI Act" data-startref="eua-2" data-type="indexterm" id="id385"/>EU AI Act<a contenteditable="false" data-primary="compliance" data-startref="compliance-2" data-type="indexterm" id="id386"/>.</p>
</div></section>

<section data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="chapter_1_conclusion_1748539916834114">
<h1>Conclusion</h1>

<p>The EU AI Act focuses on promoting the development and use of human-centered and trustworthy artificial intelligence, to safeguard health, safety, fundamental rights, democracy, the rule of law, and environmental protection within the European Union. In this first chapter, we examined the concept of trustworthiness in AI, which is based on three pillars: lawfulness, ethics, and robustness. I outlined the seven key requirements for trustworthy AI systems, which include human agency and oversight; technical robustness and safety; privacy and data governance; transparency; diversity, non-discrimination, and fairness; societal and environmental well-being; and accountability.</p>

<p>Understanding the Act requires knowledge of its scope, who it applies to, and the established timeline for AI system compliance. The EU AI Act is a comprehensive risk-based legal framework that establishes rules for the development, distribution, and use of AI systems and GPAI models. It applies to businesses and individuals both within and outside the European Union, including providers, importers, distributors, authorized representatives, deployers, and users of AI systems.</p>

<p>The Act classifies AI systems into four categories based on risk and establishes corresponding obligations. It prohibits AI systems that pose unacceptable risks, imposes strict requirements on high-risk systems, and mandates transparency and labeling for limited-risk systems. The Act also lays out a specific set of obligations for GPAI models, with additional rules for models deemed to pose systemic risk. These regulations apply not only to the AI systems themselves but to all organizations involved in their development and deployment across the AI value chain.</p>

<p>Classifying the AI systems in use in your organization and identifying the corresponding compliance requirements will help determine the scope of required data and AI governance measures, as well as the necessary AI engineering practices. In the next chapter, I’ll discuss the CRISP-ML(Q) structured AI system development process and MLOps, which provides technical and organizational best practices for AI system operationalization.</p>
</div></section>

<section data-pdf-bookmark="Further Reading" data-type="sect1"><div class="sect1" id="chapter_1_further_reading_1748539916834167">
<h1>Further Reading</h1>

<p>Ali, Sajid, Tamer Abuhmed, Shaker El-Sappagh, Khan Muhammad, José M. Alonso-Moral, Roberto Confalonieri, Riccardo Guidotti, et al. “Explainable Artificial Intelligence (XAI): What We Know and What Is Left to Attain Trustworthy Artificial Intelligence.” <em>Information Fusion</em> 99 (November 2023): 101805. <a href="https://oreil.ly/WERsh"><em>https://oreil.ly/WERsh</em></a>.</p>

<p>Braiek, Houssem Ben, and Foutse Khomh. “Machine Learning Robustness: A <span class="keep-together">Primer</span>.” arXiv preprint arXiv:2404.00897, May 2024. <a href="https://oreil.ly/6iJoo"><em>https://oreil.ly/6iJoo</em></a>.</p>

<p>Caton, Simon, and Christian Haas. “Fairness in Machine Learning: A Survey.” <em>ACM Computing Surveys</em> 56, no. 7 (2024): 166. <a href="https://oreil.ly/ykP9I"><em>https://oreil.ly/ykP9I</em></a>.</p>

<p>High-Level Expert Group on AI (AI HLEG). <em>The Assessment List for Trustworthy Artificial Intelligence (ALTAI) for Self-Assessment</em>. European Commission, July 2020. <a href="https://oreil.ly/QNzCW"><em>https://oreil.ly/QNzCW</em></a>.</p>

<p>Li, Bo, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei, Jinfeng Yi, and Bowen Zhou. “Trustworthy AI: From Principles to Practices.” <em>ACM Computing Surveys</em> 55, no. 9 (2023): 177. <a href="https://oreil.ly/sAQ99"><em>https://oreil.ly/sAQ99</em></a>.</p>

<p>Malhotra, Tanya. “Top Artificial Intelligence (AI) Governance Laws and Frameworks.” <em>Marktechpost</em>, May 2, 2024. <a href="https://oreil.ly/OvYAI"><em>https://oreil.ly/OvYAI</em></a>.</p>

<p>Monteith, Scott, Tasha Glenn, John R. Geddes, Peter C. Whybrow, Eric Achtyes, and Micheal Bauer. “Artificial Intelligence and Increasing Misinformation.” <em>The <span class="keep-together">British</span> Journal of Psychiatry</em> 224, no. 2 (2024): 33–35. <a href="https://oreil.ly/KgesS"><em>https://oreil.ly/KgesS</em></a>.</p>

<p>Novelli, Claudio, Mariarosaria Taddeo, and Luciano Floridi. “Accountability in Artificial Intelligence: What It Is and How It Works.” <em>AI &amp; SOCIETY</em> 39 (2023): <span class="keep-together">1871–82</span>. <a href="https://oreil.ly/yEJ1b"><em>https://oreil.ly/yEJ1b</em></a>.</p>

<p>Radclyffe, Charless, Mafalda Ribeiro, and Robert H. Wortham. “The Assessment List for Trustworthy Artificial Intelligence: A Review and Recommendations.” <em><span class="keep-together">Frontiers</span> in Artificial Intelligence</em> 6 (2023): 1020592. <a href="https://oreil.ly/bmSIN"><em>https://oreil.ly/bmSIN</em></a>.</p>

<p>Reinsel, David, John Gantz, and John Rydning. <em>Data Age 2025: The Evolution of Data to Life-Critical</em>. Seagate, April 2017. <a href="https://oreil.ly/PYxM2"><em>https://oreil.ly/PYxM2</em></a>.</p>

<p>Repetto, Marco. “Fostering Robust AI: Understanding Its Importance and Navigating the EU Artificial Intelligence Act.” CertX, June 12, 2023. <a href="https://oreil.ly/U2WUK"><em>https://oreil.ly/U2WUK</em></a>.</p>

<p>Schwartz, Roy, Jesse Dodge, Noah A. Smith, and Oren Etzioni. “Green AI.” <em>Communications of the ACM</em> 63, no. 12 (2020): 54–63. <a href="https://oreil.ly/S7AJT"><em>https://oreil.ly/S7AJT</em></a>.</p>

<p>Studer, Stefan, Thanh Binh Bui, Christian Drescher, Alexander Hanuschkin, Ludwig Winkler, Steven Peters, and Klaus-Robert Mueller. “Towards CRISP-ML(Q): A Machine Learning Process Model with Quality Assurance Methodology.” arXiv preprint arXiv:2003.05155, February 2021. <a href="https://oreil.ly/srSRa"><em>https://oreil.ly/srSRa</em></a>.</p>

<p>Turri, Violet. “What Is Explainable AI?” <em>SEI Blog</em> (Carnegie Mellon University Software Engineering Institute), January 17, 2022. <a href="https://oreil.ly/pciy_"><em>https://oreil.ly/pciy_</em></a>.</p>

<p>Wu, Carole-Jean, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, et al. “Sustainable AI: Environmental Implications, Challenges and Opportunities.” arXiv preprint arXiv:2111.00364, January 2022. <a href="https://oreil.ly/vplRJ"><em>https://oreil.ly/vplRJ</em></a>.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id333"><sup><a href="ch01.html#id333-marker">1</a></sup> For further reading on this and other topics introduced in this chapter, see the list of references at the end of the chapter.</p></div></div></section></body></html>