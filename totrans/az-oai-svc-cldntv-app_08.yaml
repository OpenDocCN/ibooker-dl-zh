- en: Chapter 7\. Exploring the Big Picture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter includes the last pieces of knowledge for your generative AI learning
    with Azure OpenAI and other Microsoft technologies. It includes some future visions,
    interviews with experts, and success stories. Remember, generative AI (and artificial
    intelligence in general) is a highly evolving domain, so use this book as your
    entry to a whole universe of knowledge and learning assets.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start this last chapter by discussing what’s next, from an Azure OpenAI
    perspective. For an avid learner and AI adopter like you, what are the other areas
    you should explore?
  prefs: []
  type: TYPE_NORMAL
- en: What’s Next? The Evolution Toward Microsoft Copilot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Azure OpenAI Service is part of a wider ecosystem. All architectures, APIs,
    and integrations with other generative AI building blocks contribute to the notion
    of AI copilots, which we mentioned in the first chapter.
  prefs: []
  type: TYPE_NORMAL
- en: AI copilots are technology-enabled assistants, companions that help human agents
    become better, more efficient, workers. The principle behind them is to provide
    an interface (written or spoken) that helps people perform complex tasks, such
    as finding specific information or adding information to a third-party system
    (e.g., a CRM, a support ticketing system).
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in [Figure 7-1](#fig_1_the_end_to_end_copilot_architecture),
    the end-to-end Microsoft vision for AI copilots includes models from Azure OpenAI,
    but also their connection with other systems such as Microsoft 365, which already
    includes [its own copilot](https://oreil.ly/Jwuff). This can be expanded with
    additional capabilities by leveraging the data from the [Microsoft Graph API](https://oreil.ly/3qy8E),
    the development interface to access data from the 365 suite (including calendar
    and emails from Outlook, and meeting recordings and transcript from Teams, to
    name a few), and the [Microsoft Dataverse](https://oreil.ly/onOnS), previously
    known as Common Data Model, a data store for the Power Platform and [Dynamics
    365](https://oreil.ly/omP3a) ecosystems.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aoas_0701.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-1\. The end-to-end copilot architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The combination of all these building blocks enables new development patterns,
    augmenting generative AI models with other data sources and systems, and the notion
    of a copilot will certainly evolve over the next few years. You will see this
    kind of end-to-end architecture becoming an industry standard, so I recommend
    you understand how all these pieces connect and enable new productivity and generative
    AI scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: That said, it would take two or three more books to explore all these pieces,
    but to leverage some official Microsoft resources, take a look at the [Learning
    Pathways site](https://oreil.ly/kljOi) from the Microsoft UK team, and check the
    [AI Learning Companion path](https://oreil.ly/SntTJ) as it contains a huge variety
    of videos, articles, and training programs. You can also explore an [illustrative
    example](https://oreil.ly/1_p3Q) of the Microsoft Copilot technology stack, which
    includes Azure OpenAI and other Microsoft services—very useful if you have a technical
    background.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now go to what I consider the hidden gem of this book: valuable and exclusive
    insights from interviews with some of the biggest experts out there, which will
    complement the content of this book with diverse perspectives on related topics
    such as design, data quality, the future of AI, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: Expert Insights for the Generative AI Era
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is pretty rare, and an extraordinary privilege, to get access to some of
    the most relevant experts on generative AI, people who have a hand in shaping
    generative AI and how organizations are adopting Azure OpenAI and other related
    building blocks.
  prefs: []
  type: TYPE_NORMAL
- en: 'This section includes a series of interviews with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[David Carmona](https://oreil.ly/FOSlv)'
  prefs: []
  type: TYPE_NORMAL
- en: Vice president and CTO for Strategic Incubations at Microsoft and author of
    the [*The AI Organization* (O’Reilly)](https://oreil.ly/JvO8O). This interview
    includes a discussion on AI adoption and advanced use cases, as well as his vision
    of the future of generative AI. We’ll gain top insights from a visionary leader.
  prefs: []
  type: TYPE_NORMAL
- en: '[Brendan Burns](https://oreil.ly/e_vU5)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Corporate vice president at Microsoft, an authentic legend of the cloud native
    ecosystem thanks to his role as Kubernetes cofounder and the author of multiple
    O’Reilly books such as [*Kubernetes: Up and Running*](https://oreil.ly/c86hW),
    [*Kubernetes Best Practices*](https://oreil.ly/DIsrj), [*Managing Kubernetes*](https://oreil.ly/cJDzq),
    and [*Designing Distributed Systems*](https://oreil.ly/5GcFO). This conversation
    discusses the convergence between the generative AI era cloud native architectures.'
  prefs: []
  type: TYPE_NORMAL
- en: '[John Maeda](https://oreil.ly/4jxQ0)'
  prefs: []
  type: TYPE_NORMAL
- en: Vice president of Engineering and head of Computational Design/AI Platform at
    Microsoft, and the main sponsor of the Semantic Kernel project. This is an amazing
    exploration of the role of design for AI solutions and the importance of LLM orchestration
    technologies.
  prefs: []
  type: TYPE_NORMAL
- en: '[Sarah Bird](https://oreil.ly/N_NK9)'
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft’s chief product officer of Responsible AI. This interview includes
    a conversation with the leader in charge of RAI developments for the Azure AI
    platform, including Azure OpenAI. Sarah gives us a different perspective on such
    an important topic.
  prefs: []
  type: TYPE_NORMAL
- en: '[Tim Ward](https://oreil.ly/cersn)'
  prefs: []
  type: TYPE_NORMAL
- en: CEO at CluedIn, is a great source for data management topics. This discussion
    dives into data quality as an enabler for generative AI developments, but also
    looks at how AI is changing the way companies perform master data management (MDM)
    and quality control.
  prefs: []
  type: TYPE_NORMAL
- en: '[Seth Juarez](https://oreil.ly/oIrgI)'
  prefs: []
  type: TYPE_NORMAL
- en: Principal program manager for the AI Platform at Microsoft. Seth has been one
    of the more visible faces of the Azure OpenAI era, thanks to his role as host
    of the [AI Show](https://oreil.ly/h-Fvw). Seth is one of the most well-known professionals
    for Azure OpenAI and Azure AI Studio topics, and a great storyteller who makes
    complex topics look a bit simpler.
  prefs: []
  type: TYPE_NORMAL
- en: '[Saurabh Tiwary](https://oreil.ly/qUK9P)'
  prefs: []
  type: TYPE_NORMAL
- en: Corporate VP for Microsoft Copilot & Turing. This interview includes a great
    exchange about the vision for Microsoft Copilot as the end-to-end architecture
    that leverages Azure OpenAI Service.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dig into these interviews!
  prefs: []
  type: TYPE_NORMAL
- en: 'David Carmona: AI Adoption and the Future of Generative AI'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**A.G.**: So, I know your background, I know about your career at Microsoft,
    but who is David and what’s your role at the Microsoft organization?'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aoas_07in01.png)'
  prefs: []
  type: TYPE_IMG
- en: '**D.C.:** Well, thank you for inviting me. It’s a big pleasure. I think we
    share the pain of writing a book, so I am always in awe when somebody takes that
    big adventure. It’s amazing that you did it, so congratulations for that. I think
    when I look at my role in Microsoft at the end of the day, it’s all about creating
    new incubation businesses. I’ve been in Microsoft for almost 23 years now, and
    it’s always been very focused on that function. I’m originally from Spain, I was
    working for Microsoft in Western Europe, then I moved 15 years ago to Corp in
    Seattle. I was part of the cloud incubation, which was amazing to be part of that.'
  prefs: []
  type: TYPE_NORMAL
- en: Then after that, when the cloud became mainstream, and I didn’t have to prove
    all the time the importance of the cloud, then I was offered to lead AI incubation.
    It was just when cloud was becoming mainstream at that time, which was eight,
    nine years ago as it was something that started in Microsoft Research. I was working
    with Microsoft Research at that time, and it was all about creating a new business
    category for Microsoft. Just when that started to be mainstream too, I recently—two
    years ago, so just when I didn’t have to prove again in every conversation the
    importance of AI—I moved to the next businesses. I’m working now on areas like
    the future of AI, which are the new frontiers of AI that we will see in the future.
    For example, applying AI to science, which is an amazing use case scenario. Then
    other areas like quantum computing, which I also have the pleasure to incubate,
    and some others like space, communications, future of communication, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: You’re a good person to talk to about the vision of generative AI,
    artificial intelligence in general, and how we will be using them in the next
    few years. What’s the potential and the cool things that you’re seeing now? What’s
    your vision for this generative AI era?'
  prefs: []
  type: TYPE_NORMAL
- en: '**D.C.**: For me, the big difference is the scenarios that you can address
    with this new AI that you couldn’t address before, of course. The whole concept
    of reasoning on top of language, or any other modality, and not only data, that
    is something super powerful and we can speak about a lot. But for me, the big
    transformation, what is really the revolution of this new generation of AI is
    that it is broader, it is more generalized. In the past, to create an AI model,
    you required a specific dataset and a specific model. I still remember those early
    times of AI when we were creating these milestones for AI in Microsoft Research
    of image classification of human parity, speech recognition, etc. All of those
    require a very specific team, super specialized on that, with a very specific
    dataset and model.'
  prefs: []
  type: TYPE_NORMAL
- en: The big change of that, the big impact of that is the possibility. Now that
    model has become something that is not only for data scientists to create, but
    for even end users to customize and use in their daily lives and jobs, which is
    the concept of Copilot. The rest is history after that. But for me, that is the
    core difference of this new AI.
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Exactly. Because we have been using AI. We had AI in different products,
    but most people weren’t aware that they were using AI or being subject to AI.
    Now it’s natural, and that’s the concept of democratization, access to technology,
    because we use language, which is the purest way to communicate. I think it’s
    very exciting.'
  prefs: []
  type: TYPE_NORMAL
- en: '**D.C.**: This is just the beginning. As you know, I’m super excited about
    what is going to come next. Not only on the technology itself, of course, the
    technology will evolve, but also I think as we understand the technology better,
    and we start using it in more use cases, we’re going to see scenarios that we
    can not even think about today. The one that I mentioned at the beginning that
    I work on a lot lately is, of course, the applicability to scientific discovery,
    which is going to open areas that are just amazing, that we cannot even imagine
    yet.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.:** Yes. Bringing that scalability to situations where maybe in the traditional
    world we couldn’t take care of that. There’s this public case with the SERMAS,
    the health department of Madrid in Spain, with Julian Isla that you probably know.
    They’re trying to leverage generative AI to spot the good information, to retrieve
    the good information for rare diseases. Usually, you have a business case behind
    everything, and with traditional AI, you will say, OK, there’s not enough of a
    target public because it’s a rare disease. With this, you can actually bring that
    to all the doctors in the world, and they can spot the situation in a faster and
    more efficient way. That’s a perfect example of the kind of things that even if
    they don’t sound super advanced because it’s just retrieving information, I personally
    love it.'
  prefs: []
  type: TYPE_NORMAL
- en: '**D.C.**: Yeah. I’m in love with that use case. I’m also part of the nonprofit
    [Foundation 29](https://oreil.ly/DCC5D) that Julian Isla leads, with Carlos Mascias
    and others. For me, it is a great example, as you said, because it’s focused,
    as you know, on diagnosing rare diseases. The problem that we have with a profession
    like doctors is that it relies a lot on the experience of the doctor. That works
    perfectly fine when you are diagnosing a common disease. But the exposure to rare
    diseases in primary care doctors is very low. It’s very difficult for them to
    diagnose those diseases. Consider that the average time of diagnosis of a rare
    disease is seven years. Those are seven years that you are not applying the right
    treatment to that disease. With things like this where a model can help because
    it’s always helping the doctor, so guiding the doctor, giving them clues on where
    the disease could come from, that is an amazing tool that I think is a great example
    of this new paradigm of humans and machines working together.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Exactly, and improving the status quo. Something that is impossible
    to deny is that there’s something that we can improve with technology. You have
    mentioned the models, you have mentioned the platform. This book is about Azure
    OpenAI, but how do you see Azure OpenAI as a piece of technology or as a platform,
    as a technology enabler for this era? How do you see this going? It’s about the
    model, it’s about the platform, the different layers, the thing around it. I have
    my opinion, but I want to hear yours.'
  prefs: []
  type: TYPE_NORMAL
- en: '**D.C.**: I think it’s a deeper conversation, I could say. If you look at this
    only at one particular layer of the ecosystem, you are probably missing a lot
    of things. For me, AI is more than a technology, it’s a new paradigm, it’s a new
    economy actually. You look at the impact that AI is going to have even for core
    GDP growth and it’s huge. We are addressing it with just a layer of the stack,
    it’s not enough. You need to take a look at the entire ecosystem. In that entire
    ecosystem, you have many players. Of course, you have at the bottom of it, you
    have even the chips, even the pure hardware that you need to consider for this.
    On top of that, you have the big data centers that you need to address and to
    target one of these applications. Then on top of that, you have the foundational
    models, which are super visible, of course, they are a critical part of it. But
    on top of that, you also need the tooling, the platform to really get the most
    of these models. It’s very easy and you know this more than anybody, it’s very
    easy to start a proof of concept on a service and model. Very easy to start doing
    prompts and start getting more of that model. But to create a real use case, to
    create a full scenario, you need way more than that. You need to start talking
    about grounding, safety, things like services integration, such as plug-ins and
    many other areas that are satellite to the model but are equally important. Then
    on top of that, you still have more layers. You have responsibility, which is
    critical. You have the applications, you have the distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: In the case of Azure OpenAI, I think the key thing, of course it’s a critical
    piece of that full stack. But in our case, the principle that we have from the
    very beginning since we started with AI, is that we believe in a speed of innovation
    that goes side by side with the platform. Even internally in Microsoft, the way
    that we look at innovation is providing that innovation as a platform to the rest
    of the company. Then at the same time, we bring that platform to Azure so our
    customers can use it. So Azure OpenAI is a perfect example of that because what
    we did was create the concept of model as a service, making it super easy to be
    accessible from customers, and making it like a first-class citizen of Azure.
    You access it just like any other service, which is again, bringing that to the
    broader platform for developers to create new applications with it.
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Yes. With all these layers that you have mentioned from the platform,
    that’s why I was asking the question, because usually the discussion is around
    the model. We are creating a bigger model and this is, before it was like more
    parameters, now it’s the one that is better on the benchmark. But I think the
    models are becoming a commodity, very expensive and difficult to create. But now
    the value, the real value is in the combination of these models with the rest
    of the platform. That’s what I have liked the most from the evolution of Azure
    OpenAI and Azure AI Studio in general during this 2023-2024 period.'
  prefs: []
  type: TYPE_NORMAL
- en: '**D.C.**: Yeah. Completely agree.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.:** You have a general view of everything that’s happening at Microsoft,
    like internally, externally, like platforms, models, cool projects, Microsoft
    research, papers, the new things that are coming. What’s the part that gets you
    most excited? Is this about the large language models, the small language models—do
    you have any preference?'
  prefs: []
  type: TYPE_NORMAL
- en: '**D.C.:** From the research probably because, of course, in that full stack
    there are things happening on every layer. I’m excited with each of them because
    they are super important. In my heart, I’m a software developer. I’m especially
    passionate about anything that has to do with the platform because it’s what really
    enables developers to create important cool stuff on top of it. I’m a super fan
    of, for example, Azure AI Studio and all the tooling that is there. Anything that
    has to do with the orchestration of the entire lifecycle of models, super big
    fan. In my initial role, I was very focused on how we can transform the development
    process with the cloud. The concept of DevOps, continuous integration, continuous
    deployment, and so on. We released what was at that time called [Visual Studio
    Online (VSO)](https://oreil.ly/IJeOu), now it’s Azure DevOps. I think that we
    always forget that part of the stack and it is hugely important. There’s no way
    that you can be successful in an enterprise adopting AI if you just take a very
    specific tooling and model approach, so you need to look at the entire lifecycle,
    and orchestrate that lifecycle. That, I’m super passionate about.'
  prefs: []
  type: TYPE_NORMAL
- en: But now, if you ask me about the wow stuff, the things that are coming from
    research that gets me excited, I have to say that, probably because of my current
    job, I’m a huge fan of all the work coming on applying AI to science. There are
    things in there that are just mind-blowing, that we’re just scratching the surface
    of right now. One example that we recently announced was the application of some
    of these models to an actual scientific discovery. In this case, it was a battery
    discovery, the rest of the material to create a battery. It was discovered fully
    with these tools. They are the three things that these models can do, but in a
    sense, at the core of it the concept is as simple as saying, hey, just like AI
    can reason on top of text, just like AI can reason on top of images, video, and
    so on, AI can also reason on top of graphs. A very important graph that is all
    around us is molecules. They are just graphs of atoms. The possibility of AI to
    reason on top of those structures, on top of molecules, is just amazing. We see
    the same concept that we see with generative AI apply for images. Think of DALL·E
    when you write a prompt and the model will deliver an image like an output. We’re
    starting to see that and Microsoft Research has already delivered some of those
    externally on models that can do that with molecules. Think of explaining the
    model, what are the properties that you are looking for in a particular model,
    and the model creating a lot of variance and a lot of possibilities for that molecule.
    That is mind-blowing, think of the possibilities of that.
  prefs: []
  type: TYPE_NORMAL
- en: But then, that is the generation part. Then the second part is the simulation.
    With AI, we can simulate the properties and the interactions of these molecules,
    which are, imagine the equivalent of that could be to go to a “wet lab” and do
    that in person. Now, if you are able to do it with AI, accelerating thousands
    of times, the time that was needed on traditional compute, what that allows you
    is to just expand your search space. Now, you can screen millions of molecules
    to find those properties. Then the last one is also helping us to synthesize those
    molecules, giving us the best and more efficient ways of synthesizing those molecules.
    The implications of that in any science—so from materials to health, to sustainability,
    to climate change, to many other domains—is just amazing. When you combine it
    with the concept of reasoning on top of knowledge, now you have on one side AI
    models that can simulate nature. On the other side, you have the concept of a
    copilot for the scientists, where the scientists can use it to reason with all
    the past scientific knowledge and all the current knowledge of that domain. It’s
    just mind-blowing the possibilities of it.
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: It’s impressive, the impact at all levels, even the academic level.
    People that are learning can retrieve all the information, and they can accelerate
    their learning, and they can contribute more and more to the research. You see,
    that’s why I invited you, because you have the vision of these kinds of things.'
  prefs: []
  type: TYPE_NORMAL
- en: '**D.C.:** It’s funny because I think what we always talk about, the work that
    AI can do on behalf of humans, but with this concept of AI reasoning on top of
    the collective knowledge of the scientific community, what it can do is actually
    bring in that community even closer, because right now there’s a big barrier for
    scientists to reason on top of the knowledge that was created by other scientists,
    because there’s so much. It’s almost impossible for a scientist to be on top of
    all the collective knowledge of the community. Now, with these tools, it will
    make it easier for scientists to build on top of the discoveries and the progress
    of others, which is amazing.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: To finish the discussion, I’ll come back to your book, *The AI Organization*.
    Students are wondering OK, is this relevant? Descriptive AI, like traditional
    AI, do I need to learn this when I’m talking about generative AI? Now we have
    so many new experts talking about the topic. I said, yes, of course, the kind
    of consideration, the technical consideration, but also organizational considerations
    of adoption and the barriers and the tricks and the things that you need to do
    and the data component, the data strategy of the companies, this is very important.
    And I feel like your book includes a lot of good examples. I remember the one
    with Telefonica and Chema, Alonso, that I like especially because it’s very illustrative
    and very creative. But what would be, if you had to sell the value of that book
    for the generative AI adopters at the company level, what would be the value of
    it?'
  prefs: []
  type: TYPE_NORMAL
- en: '**D.C.:** Yeah, I mean, the book was written just thinking of the learnings
    that I was seeing with big companies embracing AI, right? So, I’ve seen many of
    those in the early days, right? So, early days in AI are like eight years ago.
    So not a long time ago. And it’s funny because usually the blockers that I was
    seeing for adopting AI at scale had nothing to do with technology. So that made
    me wonder, hey, there’s a lot of books talking about the technology, but there’s
    a gap in there on telling the broader story that leaders in organizations of any
    level should know to be successful with AI. So that was the approach to the book.
    I identify four big areas that you need to address to be successful in adopting
    AI at scale. So again, not proof of concept, not specific use cases, but really
    transforming your company with AI, right? And becoming that concept of AI organization.
    And it’s, yeah, technology is one of them. So, of course, it’s there and I talk
    a lot about technology, but I talk about the strategy. You need to have a full
    comprehensive strategy that is inclusive of the short term, but also the long
    term and connecting between those two, right? So I share the learnings in Microsoft
    as well. How we approach that, we call it the horizon framework and how we actually
    make sure that we balance those investments across the horizon. We have a connected
    strategy that is investing in the short term because it has value for the short
    term, but also in the long term and how to connect both, right? So that is good.
    I also talk about the importance of having an approach that is from the technology
    to the business, but then also from the business to the technology, right? That
    is critical. I see, and I was seeing at that time, a lot of conversations that
    started on the possibilities of the technologies, but not what the business needed,
    right? So you need a framework. And I also share the framework that we use in
    Microsoft to have a conversation that is business centric. And then connecting
    that to the technology to focus on identifying the use cases and the long-term
    bets in my company. So that’s the strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: The second one is culture because that’s another critical one. This AI transformation
    is not something that happens in a laboratory. It’s not that you can create a
    center of excellence of AI and consider it like a black box and forget about this
    problem. This is something that will impact, as a leader, you need to know that
    this is something that will impact the entire organization. So every employee
    has to be part of it. And that’s something that requires specific action and need.
    And I also share ways of doing that, some learnings from Microsoft. We have a
    lot of learnings on that one. And you realize how important it is if you compare
    failures with successes, you see that culture is usually a huge part of that.
    When you have the organization not fully bought in, where you have things that
    are isolated, that are not connected, it’s very difficult to have an impact in
    the business by doing that. And then the last one is responsibility. So that is
    a critical one, as you know. And it’s something that we tend to think that it’s
    just creating principles for AI. It’s far more than that, right? You need to turn
    those principles into reality. And now even more as at that time, there was no
    regulation, but now with regulation coming, it’s not going to be like a good addition.
    It’s going to be absolutely critical that every company does that. And it’s not
    something that you can think of at the end of the process, it’s something that
    you have to consider in every step of your development, from the ideation to the
    development to the deploying and monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: I totally agree. And look, these four pillars are exactly the same
    today. We have the same kind of cases, people get excited about technology, then
    forget the overall strategy of the company, creating a case that has nothing to
    do with the strategy of the company, or the return investment, or the potential
    value for the company. The culture, all the education parts, now it’s becoming
    more obvious that people need to learn about genetic AI, and we see that trend
    beside the technology teams. And the responsible AI part, which is, I call it
    accountability. It’s not AI at this point, because it’s accountable. It’s trustworthy,
    it’s responsible, it’s ethical, everything you want, but there’s a regulation.
    So now we want to be compliant with regulations. So it’s still the same. And that’s
    why I think that it’s still a very good classic for any generative AI adopter
    out there.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Brendan Burns: The Role of Cloud Native for Generative AI Developments'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**A.G.**: I’m very happy to have you here. I know that a lot of people know
    you, but what’s your current role and journey at Microsoft?'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aoas_07in02.png)'
  prefs: []
  type: TYPE_IMG
- en: '**B.B.**: Sure. I’m currently the corporate vice president for cloud native
    open source and the Azure Management platform. So that’s a focus on, I guess the
    best summary is maybe all things DevOps and modern application development on
    Azure, with a special focus on containers and Linux.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: So everything related to cloud native and the Microsoft ecosystem,
    you’re there.'
  prefs: []
  type: TYPE_NORMAL
- en: '**B.B.**: Yep. As well as the Azure Resource Manager, which is sort of the
    API gateway with policy, and all sort of infrastructure as code tooling.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Very important for the kind of architecture we discuss in the book.
    And even if it’s an obvious question, what’s your experience in cloud native and
    Kubernetes?'
  prefs: []
  type: TYPE_NORMAL
- en: '**B.B.**: Sure, yeah. So I mean, obviously I started the Kubernetes project.
    It’s closing in on a decade actually, which is kind of, I guess, why there’s some
    gray hair, but, you know, I was responsible for the early days of that project,
    shaping, helping shape the community. And then I came to Azure and focused on
    really figuring out how Azure can be the best place to run open source and cloud
    native workloads. And as part of that also, I think helping a lot of enterprises,
    traditional Microsoft customers with their transition to cloud native applications.
    I think there’s a sense that cloud native is like a new startup thing, but actually
    the truth is that I think most of the cloud native applications that are being
    built today are being built by large companies that need this kind of development
    agility and reliability for their applications.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Yeah, which is connected to the current era of generative AI here
    at Microsoft. What’s your personal point of view on this new wave?'
  prefs: []
  type: TYPE_NORMAL
- en: '**B.B.**: Well, I’m super excited about it. I think everybody’s excited about
    it. I’m really excited about it at a personal level, because it’s actually helped
    me. I think using things like GitHub Copilot actually really does speed up, especially
    when I’m learning something new. I was just learning Rust over maybe six months
    ago. And I found that when you’re in a new language, it just made a huge difference
    in the speed with which I could pick up the idioms. And you think, because sometimes,
    when you’re learning a new language, you’re programming it like the other language.
    So you end up writing Python, like you used to write Java, for example. I think
    having access to those idiomatic patterns helps you become fluent in the language
    a lot faster. Plus I found Rust also is a little bit, the error messages are not
    as good as they could be, I think. And so again, having that ability to be like,
    please fix this error message for me, right? It would just give me the code snippet
    that I needed. And that was pretty useful as well. So I think that’s cool.'
  prefs: []
  type: TYPE_NORMAL
- en: I think it’s really exciting also how we can help our customers have similar
    reductions in complexity, whether it’s for programming languages, or their infrastructure,
    or you know, any number of things. Becoming cloud native is a good example, actually,
    infrastructure as code (IaC) can be hard for people and enabling people to transition
    from, you know, ClickOps in the portal to infrastructure as code easily is really
    great. And things like mechanical export of an infrastructure as code template
    hasn’t always been that great. And I think generative AI gives us the opportunity
    to go in a different direction and get better, more fluid templates than we do
    if we just, you know, sort of write code that tries to do it.
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Yeah, and I think it’s exciting because it goes both ways, no? We
    can leverage generative AI for all cloud native purposes. And we can leverage
    the good practices of cloud native to implement generative AI on existing and
    new applications.'
  prefs: []
  type: TYPE_NORMAL
- en: '**B.B.**: Absolutely. Yeah, I mean, it’s interesting to think, right? I mean,
    it sounds sort of grandiose to claim that generative AI wouldn’t exist without
    Kubernetes. But I think actually, it’s kind of true, right? In the sense of not
    like Kubernetes is special, but in the sense of it enabled a lot of people who
    wanted to build large-scale systems to kind of forget about machine management.
    The first step to doing AI inferences is no longer figuring out how to get a bunch
    of machines to work together. Containers and Kubernetes took care of that for
    you. And so you can just say, OK, I’ve got this fleet of machines with GPUs and
    everything else. How do I get my application out there to do the training? And
    I think that that’s the history of computer science in general, building higher
    levels of abstraction to enable the next platform to build on top.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: And that’s one of the cases I love the most. If you check the success
    stories at *kubernetes.io* or [CNCF](https://oreil.ly/epXxD), they talk about
    [OpenAI with Cloud Native](https://oreil.ly/b5EfH) and how that was enabling all
    the kinds of things that we have seen, like wider scale, a lot of people can connect
    to ChatGPT. Of course, there is the AI infrastructure from Microsoft behind that.
    But that’s a new enabler for all these areas of applications and the AI compilers.'
  prefs: []
  type: TYPE_NORMAL
- en: '**B.B.**: Yeah. And a reduction in complexity again, too. So not only can generative
    AI reduce complexity, but having that orchestrator reduces the complexity for
    the AI engineers who just don’t need to worry about that problem. And then when
    you get it from Azure, you don’t even have to worry about running it. It just
    takes care of it for you.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Saving a lot of people like me.'
  prefs: []
  type: TYPE_NORMAL
- en: '**B.B.**: I think it’s also always the goal, right? It’s much easier to consume
    the idea than it is to implement the idea. You can say, OK I know how to use a
    sorting algorithm. You can probably write a sorting algorithm, but it’s going
    to take you a lot more time to write it than it is to use it. It empowers a lot
    of people, which is great.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Yeah. And accelerates the implementation, something that would take
    so long before now is becoming like a, I wouldn’t say commodity, but certainly
    like something easier to implement.'
  prefs: []
  type: TYPE_NORMAL
- en: '**B.B.**: Yeah. And I think you’ll see, I think you see that creative explosion
    then afterwards, where a lot of people who maybe wouldn’t have the patience or
    the skillset to implement generative AI, but they can have really creative ideas
    about how to use it. And so when you make that capability available, you’re going
    to just generate a ton of creativity about how to use it.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Certainly. From the cloud native perspective at Microsoft, how do
    you see all this explosion of the [technology stack for Copilot](https://oreil.ly/jGrqE),
    Semantic Kernel, all the different cool pieces that we mention in this book? What’s
    your personal opinion on that?'
  prefs: []
  type: TYPE_NORMAL
- en: '**B.B.**: Well, I mean, you still have to run your application somewhere, right?
    You know, you don’t get to, generative AI doesn’t enable you to not have a webpage
    there or not having a Restful API somewhere. And so not only do tools like Azure
    Machine Learning build on top of Azure Kubernetes Service (AKS), but we’re actually
    also seeing a lot of people building, you know, the frontend application or the
    APIs that they need to have, even plug-ins for OpenAI on top of AKS. It’s still
    a really great place to host code and integrate there. And of course, we have
    GPU support in AKS, so there are people who are doing their own inference or building
    their own models. And some of our largest clusters actually are built to do that
    kind of AI for a variety of different groups. And again, I think it’s about simplicity,
    right? Because if you want to focus on AI, you don’t want to focus on what it
    takes to run a 5,000-node Kubernetes cluster. It’s not the easiest thing to do.
    And if you can just click a bunch of buttons or do an infrastructure as code template
    and have 5,000 GPU nodes, that’s pretty good. And then know that my team is on
    call for those. Saves you a lot of time.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Yes. And that’s what we’re seeing with Azure AI Studio now, and with
    all these applications and the ability to deploy any kind of model, because the
    book is about Azure OpenAI and a proprietary model. How do you see the role of
    open source as an enabler for generative AI?'
  prefs: []
  type: TYPE_NORMAL
- en: '**B.B.**: Yeah. I think, over time, I suspect that there’s going to be more
    and more models that happen, that people tune, that people build for different
    situations. I mean, you’re already seeing that kind of model sharing and model
    retraining happening. I think that’s really great. I think you look at something,
    you know, Semantic Kernel is out in open source sharing our best practices. LangChain
    is out there in open source. I think it’s all rooted in open source. I think also
    one of the things that is going to happen over time, I think will be higher level
    frameworks, too. I think people are still trying to figure out exactly what it
    takes to build a complete copilot. I think there’s a lot of what I would call
    sort of vertical copilots, you know, copilots that are good at one thing. But
    I think there’s value in sort of saying, well, actually, there are some really
    broad areas out there and you may actually want something that knows how to choose
    between copilots. I think of it sort of like search, maybe, right, when you do
    a web search, maps, video? Like there’s a variety of different kinds of content
    you could be searching for. And I think the same thing is going to be true with
    Copilot. There’s going to be multiple levels in terms of choosing what you want
    to generate. I mean, in some sense, it’s like your friends, I guess. Like you
    go to one friend for tech advice, you go to one friend for sports or whatever.
    And you’re going to find the same thing.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: On one side, there are all the building blocks that are being created
    and the orchestrators that you have mentioned, different approaches like retrieval
    (RAG), like the kind of knowledge bases that we can use, that can be databases,
    etc. And I feel like that’s evolving a lot, of course.'
  prefs: []
  type: TYPE_NORMAL
- en: '**B.B.**: For sure. And I think there’s a little bit of one of the questions
    I’ve had that, you know, I don’t necessarily have the right answer for, which
    is, when do you do retraining versus when do you do retrieval-augmented generation?
    Because they kind of both do the same thing at some level, you can influence what
    your results are either by retraining or retraining on your corpus or by doing
    retrieval-augmented generation. I think questions like that people are going to
    need to struggle with for a while.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Yes. There’s no single answer for that. In one of the chapters from
    this book, I mention (very carefully as it is such a new topic) that we need to
    try and test depending on the dataset, depending on the kind of retraining, the
    kind of fine-tuning you want to have or the general behavior of the LLM compared
    to the kind of tasks that you’re assigning.'
  prefs: []
  type: TYPE_NORMAL
- en: '**B.B.**: Or even the app. You’re probably not going to be able to retrain
    it for every single customer. You may have to do it because you’re like, well,
    I have such a diverse set of users. I want to provide personalized content for
    each user, but I can’t retrain every user, so I’m going to use retrieval-augmented
    generation. But on the other hand, you can be like, I am my company and it’s worth
    it to retrain because I know my company and I’m only going to have results for
    my company. I think it’s interesting stuff.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.:** Right, and maybe it’s a combination with segmentation or a recommender
    system, something that will pre-filter the kind of users you have in front of
    you. And then based on the ability that user may have to access the information,
    the knowledge base based on the active directory or whatever, you can customize
    that answer then.'
  prefs: []
  type: TYPE_NORMAL
- en: '**B.B.:** I mean, role-based access control (RBAC) is a fascinating part. We
    have this challenge even in the Azure Resource Graph, which is used for at-scale
    querying. It’s an index of all your Azure resources. Applying access control to
    that is a very interesting problem. Because obviously you can’t build an index
    for each user, right? There’s one index of all the resources. And so then you
    have to basically be like, OK, I did the query. I found some data. Now, which
    of the data that I got back did this user actually have access to or put it into
    the query itself and actually say, as I do my search query, only show me things
    that also this person has access to. And yeah, obviously, it’s important to get
    right.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.:** Totally. With all this complexity, what would be your recommendation
    in terms of upskilling, for people to follow in this area, like any kind of thing
    that will help learners and readers to keep track of everything that’s going on?'
  prefs: []
  type: TYPE_NORMAL
- en: '**B.B.**: The two things I would say is, I would definitely recommend playing
    around with it. I think the Bing chat is a great way to get in and give it a try,
    because it’s really important, I think, to get a sense for what it’s good at and
    what it’s not good at. Because I think when you see or read the articles or you
    hear, or even when you see examples, they’ve been kind of cherry-picked. They’re
    never going to show you bad examples. And I think it’s really valuable to get
    in there and realize that it’s not perfect. Even beyond the hallucinations, which
    I think people are getting a handle on how to deal with, I think with some questions
    it just isn’t very good. And I think that experiential is the way to go. Give
    yourself a task, try and figure out what the system is good at.'
  prefs: []
  type: TYPE_NORMAL
- en: In particular, I would say I think it’s really good at summarization in general.
    I found that it’s quite good at taking information and distilling it down. It
    can be good at things like error messages for compilers. It can be really bad
    also sometimes. I think you have to get your own sense of what you think it is
    good at and what you think it’s bad at. Because it will give you a sense of which
    ideas you could use it for. Because you may think, I could use generative AI to
    do this and you’re like, well, in practice, that’s not going to work out very
    well. So that’s part one, and then I think part two is that I’m a big believer
    in getting your hands dirty with a toy project that’s meaningful to you. I do
    a lot of hacking with random stuff in my house to turn on lights or whatever.
    Don’t just go through the toy examples because you don’t have a personal connection.
    And I think that personal connection helps you build. Of course, you can’t build
    the whole app at the start. You need something small and constrained to make sure
    you continue to make progress. I think that’s usually the way I go when I’m learning
    new tech. I really want to get a sense of how it works and how I put a whole piece
    of it together. That skeleton. And then, you know, then you can move on to saying,
    OK, I now have the knowledge to go build the real app that I was thinking about
    building.
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: I think those two points are very precise on what acquiring the experience
    is like. Of course with Azure OpenAI, but also the different technologies out
    there or the flavors of Azure OpenAI on different products, and what the limitations
    and the advantages are. Because there are very good advantages, but also limitations.
    For example, I was checking something related to finding information related to
    a specific person. Maybe that’s not always the best use case scenario because
    it’s linguistics, you have Adrián González from Microsoft and another Adrián González,
    the baseball player. So, yeah, I totally agree on that.'
  prefs: []
  type: TYPE_NORMAL
- en: And remind me, you have several O’Reilly books as well, right? You’re in the
    club of authors with several books. Can you tell us a bit about them and what
    they are about?
  prefs: []
  type: TYPE_NORMAL
- en: '**B.B.**: Well, I’ve written a couple of different books on Kubernetes. [*Kubernetes:
    Up and Running*](https://oreil.ly/NZcsJ), which I wrote with Kelsey Hightower
    and Joe Beda. And then more recently, [the third edition](https://oreil.ly/IvoU6)
    was written with Lachlan Evanson, who’s another person at Microsoft. And then
    actually right now I’m working on the [second edition](https://oreil.ly/7HkaR)
    of [*Designing Distributed Systems*](https://oreil.ly/vbUwd). And it’s actually
    going to go into a little bit, probably not in as much depth as your book, but
    go into a little bit of how to build AI systems in the context of distributed
    systems.'
  prefs: []
  type: TYPE_NORMAL
- en: And then actually the most exciting chapter that I’m adding there for the second
    edition is what I’m going to call the greatest hits chapter, which is all of the
    problems that people had, which discusses all of the mistakes that people make
    that keep coming up over and over again. Because we go to live sites and you sit
    through outages and postmortems and all this kind of stuff. And after you do it
    for a few years you see that there are patterns that repeat. And I’ve been taking
    some notes and I’ve written down a bunch of the ones that repeat over and over
    again. For example, one of the ones that comes up a lot is our monitoring didn’t
    think that the absence of errors should be an error. If there were a lot of errors
    you’d notice. But if it goes absolutely quiet and there’s nothing, it could mean
    you’re totally OK, but it could also mean that you’re not processing anything.
    And several times we’ve seen systems where they have a monitoring gap, where for
    some reason they stopped processing anything. With this idea that no news was
    treated as good news, they didn’t alert anybody until a customer was like, hey,
    wait a minute, where are my deliveries? You could be monitoring delivery lanes,
    anything like that in terms of an online retailer, right? An online retailer could
    monitor how long it takes a package to get from point A to point B from my delivery
    center to the customer. And they could alert if that goes over 12 hours or whatever.
    But if you stop delivering all packages, that alert does not fire. Because there’s
    no deliveries, it didn’t take any time. It’s subtleties like that and it doesn’t
    occur to you in the first place because you’re so used to the steady state.
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: I think that applies to what we’ll see in future editions of this
    book. Like the kind of learnings from the industry, the kind of things we don’t
    know because we don’t know it yet. It will be based on experience. We are just
    starting this wave for generative AI, but certainly the same case here.'
  prefs: []
  type: TYPE_NORMAL
- en: '**B.B.**: Yeah. Oh, yeah. I imagine it’s going to change rapidly, actually,
    as more and more people get in there. The first couple of years when people got
    in, the same thing happened with cloud native open source, right? Even with UI
    frameworks. I think mostly people use React now, but like there was a solid two
    or three years where I felt like people were changing every three months. It seemed
    like every time I talked to somebody, they’d switch their JavaScript framework.
    I’m sure the same thing will happen with AI, right? Because I think it takes a
    little bit for people to figure out what abstractions actually work. What are
    the abstractions that make sense? What are the common problems that we can turn
    into libraries? I think there’s a lot of free-form prompt engineering happening
    right now. I think there’s going to be a lot more science that comes into that.
    And I don’t know if science is the right word, but a lot more like rigor that
    comes into that kind of stuff over time as people figure out kind of what works
    and what doesn’t work. The templates, operations, the best practices, the countermeasures.
    I think at some point you’ll probably just be able to hit a checkbox and get a
    bunch of the fixes and all that kind of stuff, hallucination prevention and stuff
    like that.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Hey, just a last question, because you mentioned the postmortem,
    but there’s something we mentioned in the book, which is the premortem. Did you
    use the notion of a premortem to see what could go wrong?'
  prefs: []
  type: TYPE_NORMAL
- en: '**B.B.:** Yeah, it’s sort of like what we call red teaming as well, where you’re
    trying to break that, you know, you’re purposely trying to break stuff. And yeah,
    I think that’s really important. I think you check both bad stuff, obviously,
    like there’s been stuff in the press and otherwise about, you know, ways you can
    trick these models, but also honestly, just to see if it does a good job. I think
    it’s more prosaic. You know, nobody writes a headline about something that they’re
    like, this query was not answered very well. But obviously, if you’re building
    a product, that’s really important to understand: does it actually work? And I
    think actually measuring, that’s the other thing I think is going to be really
    interesting. And a lot of growth is happening. It’s like measuring the quality
    of a model. I don’t think we’ve done a ton of rigorous scientific measurement.
    I mean, there are scoreboards and things like that to measure against benchmarks
    but it’s not clear that’s 100 percent connected to the reality of a user experience
    once you build it into a product. And I think just as we’ve done a lot of work,
    you know, in the Azure portal and things like that on figuring out, where we are
    confusing the user? You know, where do we have a UI that’s not great? I think
    we’re going to do the same thing with these chat systems, right, where it’s probably
    going to be, how many times did people click on the prompts we suggested or how
    many times did they hit the clear button or, you know, there’s a lot of ways you
    can figure out, are we giving them the answers that they want?'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Totally, because right now with the benchmarks and the evals type
    of projects on LangChain and Azure AI Studio, we are focusing on the core model
    parts. But then you are mentioning all the quantitative and qualitative measures
    that we usually do in product analytics, for example. That’s something I mention
    in the book. You’ll see it in some of the chapters because obviously that part
    will evolve a lot, but getting the sense of the metrics, how good or bad you are
    doing from a user perspective, is crucial. I think that would also be very useful
    for companies.'
  prefs: []
  type: TYPE_NORMAL
- en: '**B.B.**: For sure. Yeah, absolutely. I think that it’s in its infancy right
    now. It’s going to be really interesting to see how we figure that out. So I also
    am pretty excited. Microsoft is also really big into accessibility and computing
    for all. And I think that it’s also going to be a game changer in terms of usability,
    because we see people with challenges and we do a lot of work in our UX for accessibility,
    but I think a chat or a voice-based UX that is actually empowered by generative
    language could be significantly better than what we provide with a mouse-based
    or sound-based UI.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: I’m loving this case with the Portuguese government [creating an
    avatar](https://oreil.ly/jMgb6) for people who cannot write and then you have
    another for people who can write but cannot talk. I think that’s where we are
    leading to. We’re saying that this generative AI is equivalent to what the visual
    interface was to command lines. And I believe that’s kind of true.'
  prefs: []
  type: TYPE_NORMAL
- en: '**B.B.**: Yeah, I think it’s going to be really exciting to see how it transforms
    things. And it’s fun to be a part of it. I guess that’s why we’re always here.
    It’s fun to be involved in the transformation as well.'
  prefs: []
  type: TYPE_NORMAL
- en: 'John Maeda: About AI Design and Orchestration'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**A.G.**: I know you very well just because I’m some sort of groupie of what
    you are doing with your learning resources, but let’s learn a little bit about
    who John is and what your role at Microsoft is as well as your previous background.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aoas_07in03.png)'
  prefs: []
  type: TYPE_IMG
- en: '**J.M.**: I’m lucky to get to work in the middle of the AI Superstorm and there
    is a project called Semantic Kernel that I am helping to advance. It’s a way to
    enable more enterprises to take advantage of this new kind of AI. Before that,
    I was in the physical security industry. I was a chief technology officer of a
    mid-cap security company called Everbridge. We took care of the world, countries,
    cities, and corporations. Before that I worked in places like venture capitals.
    I was at MIT for a while and did research and I also worked in a late-stage startup
    to really understand where the world is heading.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Amazing. Such an interesting background. One of the things I really
    like about you is that you are converging the design world and the AI world, which
    is very intuitive for some people. Like, of course, if we’re interacting with
    artificial intelligence, we want to have an interface and a design with a human-centric
    process. But what’s your opinion on the importance of this kind of design process
    and design thinking for AI applications, including generative AI?'
  prefs: []
  type: TYPE_NORMAL
- en: '**J.M.**: Yeah, well, I give an annual report at South by Southwest about the
    intersection of design, technology, and business. This year was called [Design
    Against AI](https://oreil.ly/pOZmj), which has two meanings. One meaning is design
    protest against AI, and the other is design competes against AI. So one is more
    of a kind of like, you know, give up. Stop it. The other is to, say, maybe I’ll
    take it on. I think creative people should be competing with AI instead. Trying
    to see how to advance their craft. Many people say it’s about collaborating with
    AI instead of just competing. That said, I think that this kind of AI is not about
    the pictures or the text. It really is about the tools, functions, actions. That’s
    why in Semantic Kernel we say plug-ins, planners, personas. I’ve heard nowadays
    people say large action model instead of just large language model where large
    action model assumes you’re using functions, plug-ins, function calling. I think
    that verb aspect of AI is going to be the thing that unlocks much more value than
    we could ever imagine.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Yeah, because it’s the interaction with tools. And in general, people
    are worried about AI replacing some basic functions of society. But some people
    are skipping the part where generative AI can be the interface to interact with
    very complex functions like designing 3D or analyzing SQL databases. So that draws
    the sign as an interface. Models and tools like Semantic Kernel.'
  prefs: []
  type: TYPE_NORMAL
- en: '**J.M.**: Well, I think plug-ins are so powerful. Whether you call them functions
    or tools or whatever you want to call them, when you integrate them with a large
    language model, of course, you get the kind of planning capability. And that’s
    what we saw with Semantic Kernel. When you use GPT-4, you give it plug-ins that
    can plan. And once it can plan, it’s basically writing code that you could never
    have written. It writes code on the fly, basically. From a design perspective,
    a lot of time has been spent making the perfect user experience. That’s something
    very hard to do. We’ll build a journey to take you step by step through it. In
    reality, though, with function calling, you don’t need a journey. You just say,
    I want to do this, and it’s done. You didn’t have to have a user interface. That’s
    why you hear people calling it sort of like a zero UI era, where you don’t have
    a journey, you teleport to the goal.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: I love that notion because from my perspective with the book, I think
    that function calling and the planning part was the most difficult part to explain,
    to be honest.'
  prefs: []
  type: TYPE_NORMAL
- en: '**J.M.**: It is. It’s so hard. It’s hard because if you’re a developer right
    now, you’re just too busy shipping regular code. You’re tired at the end of the
    week. You know, it’s a weekend, you want to take a break, and like, what, this
    new thing, what, embeddings? What, you’ve got to do language model understanding,
    testing, what, these are all new tools. You know, Python may not be your thing
    you do every day too. It’s like, oh, I don’t want to, I played with Python, whatever.
    And so that’s why we’re trying to make it easier for enterprise devs who live
    in .NET or Java or boring languages. So I tell people, Semantic Kernel is for
    boring AI people.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Boring AI people. That’s such good marketing.'
  prefs: []
  type: TYPE_NORMAL
- en: '**J.M.:** Well, it’s because enterprise likes “boring.” I mean, we also have
    a Python branch, but I find that the Python stuff is so advanced that actually
    integrating into an enterprise, it’s not so easy because it’s a different developer.
    An app dev is more about shipping “real code.” So we need an easier way to do
    that. That’s why Semantic Kernel exists.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: That’s very smart positioning. And how can you define Semantic Kernel?
    You have explained the plug-ins, the personas, but if you think of Semantic Kernel
    as a thing, today and in the future, if we’re able to get a sneak peek here, what’s
    your vision? How is it helping companies?'
  prefs: []
  type: TYPE_NORMAL
- en: '**J.M.**: Well, you know, I think the biggest way it helps companies is it
    helps you be boring because the latest thing is the latest thing, but the problem
    with the latest thing is it just got new today. And so you’re just so distracted.
    Like, what do I do? Oh my gosh, it changes every day. So Semantic Kernel is good
    insurance for building on a middleware layer. When the lower parts change, it’s
    easy to adapt it at the middleware level. So it’s like insurance for the high
    speed of AI change. And it’s grounded in the plug-in because the plug-ins are
    where function calling becomes valuable. We have so many ways to do plug-ins with
    native code or native plus semantic code, you know, pick your own language. And
    the planners are designed to not just leverage the plug-ins to call them automatically,
    but to generate a script basically that you can read yourself, not a Python program,
    but a handlebars-formatted plan. We found many enterprises are happy that AI generated
    the plan and they want to freeze the plan because they know it works. They don’t
    need it to invent something new. So frozen plans. And we’re all talking about
    agents now. So it also incorporates agents.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Agents, we’re talking about the difference with someone I cannot
    disclose right now, the difference between agents and copilots.'
  prefs: []
  type: TYPE_NORMAL
- en: '**J.M.**: I don’t know if I could get into that conversation. It’s very meta,
    I believe.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: It’s very meta. I think it’s a matter of the audience. The people
    talking about agents are probably a more developer-oriented audience.'
  prefs: []
  type: TYPE_NORMAL
- en: '**J.M.**: Yes. Good point. Well, if you remember the shift to object oriented
    programming, I remember that being a radical idea. It’s like, how do you do that?
    I’m so used to programming in this linear, compartmentalized way. Object? What’s
    an object? The number one thing you learn in object oriented programming is don’t
    make everything into an object. I think agent oriented programming also, sometimes
    agents are useful. Sometimes they’re not. It’s just a new pattern, I believe.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Yes, totally. I like that example because I was born during the object
    oriented era. And I could see how the previous era was like, this doesn’t make
    sense. You’re doing this in a linear way when you need to create relations between
    objects.'
  prefs: []
  type: TYPE_NORMAL
- en: '**J.M.**: And you remember, you suddenly make everything into an object and
    then you can’t understand it anymore. You create some kind of compromise. I think
    agents are a new way of improving the output of models through iteration, through
    feedback loops. It’s a more clever way to do prompting. It’s more compartmentalized.
    But sometimes if you need a linear workflow, that might be what your application
    requires. In that case, you don’t need agents. In that case.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Interesting. And from an Azure OpenAI perspective and any kind of
    generative AI in Azure, how do you see that connection with Semantic Kernel? And
    how do you see in general the role of orchestration? Like in Copilot, we’re talking
    about Prometheus and other orchestration engines. How do you make sense of this?
    There are so many things in this area.'
  prefs: []
  type: TYPE_NORMAL
- en: '**J.M.**: Well, you know, there’s people who want to call the model directly,
    call the APIs. I’m sure you’ve seen things like [Ollama](https://oreil.ly/Eyl-u)
    or [LM Studio](https://oreil.ly/qQBmG), they’re all adapting to the OpenAI API
    specification. I kind of feel like OpenAI has become a kind of interfacing body.
    And because Azure OpenAI is super tight, a fast follow, I think anyone in that
    ecosystem gets to take advantage of that. And then you may want to orchestrate
    directly to talk to the API, or you want to talk in a layer. And a layer is like
    clothing. There’s all kinds of brands of clothing, basically. And the particular
    brand of clothing that is Symantec Kernel is plug-ins first. And then the plug-ins
    are the foundation. And the neat thing is planners are also plug-ins, and also
    our agents, personas, are plug-ins too. We say we’re plug-ins all the way down.
    So we’re very boring.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: This is like a multilayer architecture in which they’re communicating,
    and then you have different options to communicate with this service and another.'
  prefs: []
  type: TYPE_NORMAL
- en: '**J.M.**: Everything is just code. We’re not trying to make a magical spell
    that you’re not programming anymore. You’re still programming. And everything
    is a computational unit. It’s a plug-in. And you can make plans that are also
    plug-ins, or you can make agents that are plug-ins. And it’s just like connecting
    those dots.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.:** Amazing. Let me ask this question. I’ve got several people asking
    the same question. And you can tell me if this is not a good question, but what’s
    the difference, convergence, compatibility between Semantic Kernel and LangChain?'
  prefs: []
  type: TYPE_NORMAL
- en: '**J.M.**: Very common question. Yeah. LangChain and Semantic Kernel are both
    open source projects. Open source projects support each other. All I have is good
    things to say about LangChain and also [Harrison](https://oreil.ly/ttbiM), that
    community. I also love [LlamaIndex](https://oreil.ly/QZtYV), which I think of
    as like a sister or cousin project, which I adore. And the difference really is
    in the fact that LangChain is running the fastest with the latest and greatest
    AI ideas. Semantic Kernel, that’s not the role. The role of Semantic Kernel is
    to enable enterprises to leverage this large language model or large action model
    revolution. And they’re gonna tend to move slower and need more sense of safety
    and security. So Semantic Kernel is architected with very few package dependencies,
    if at all. It’s designed to be loved by the CISO (Chief Information Security Officer).
    And it’s designed to be loved by procurement because it’s free, but it’s also
    part of the Microsoft world.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.:** Yeah. Which makes total sense. I think both of them are necessary.'
  prefs: []
  type: TYPE_NORMAL
- en: '**J.M.:** Yeah, yeah, yeah. I mean, like I say that if you want to drive a
    Tesla Model S Plaid, then LangChain’s fun. If you wanna drive a Toyota Camry XLE
    Hybrid, then you’ve got Semantic Kernel. And the neat thing is with a Python branch,
    everything is becoming 1.0\. .NET became 1.0 first. We’re aligning Python and
    Java releases. If you’re a Python team, usually a data science–oriented team,
    and you use Semantic Kernel, all of your YAML files and everything easily shift
    to the App Dev. So that’s the advantage.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.:** And what’s the role of orchestration? I know that it’s a bit of a
    stretch, but what’s the role of orchestration to bring the proper information
    and the proper format and good timing in a timely manner for compliance? I’m located
    in Madrid, in Spain, Europe, AI Act, similar things in Canada, in the future in
    the United States. I feel like there’s a lot of potential for that layer in the
    middle to distribute information that is required at log level.'
  prefs: []
  type: TYPE_NORMAL
- en: '**J.M.**: Well, I mean, that’s a nice thing about Semantic Kernel having been
    built 1.0 first with .NET C#, because it has all the logging everywhere. It’s
    got all the Azure kind of security, safeness built into how it’s architected.
    You often hear people who love how Semantic Kernel has been architected, because
    it’s been Microsoft architected. If you haven’t seen, or if any of your readers
    or viewers haven’t seen how we wrap plug-ins, you’re gonna be pleasantly surprised
    because it’s so little code to enable function calling of complex plug-ins. And
    you’re thinking, wait, this is all the code I need? And you’re like, yep, we can
    get going. People love that. And that was architected by [Stephen Toub](https://oreil.ly/i7kqx),
    one of the .NET architect legends. I remember when he said it’s gotta be this
    way. And we’re like, OK. And he said wow, that’s really good. It’s really good.
    But anyone who sees it, they’re kind of, where’s the code? It’s all there because
    it’s using the abstractions available already of an enterprise-class language.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.:** Learning is the goal of this interview and I know that you are a
    humble person because you are not talking that much about your activities and
    your background, but you’re creating learning resources, which I personally love.
    That’s why I’m writing this book. That’s why we’re creating all this stuff. And
    I have two examples, LinkedIn Learning and DeepLearning.AI. What are they about,
    for people to continue learning?'
  prefs: []
  type: TYPE_NORMAL
- en: '**J.M.:** Oh, thanks. Let’s see, I have a [LinkedIn Learning course](https://oreil.ly/TXE5e).
    Now I have [multiple](https://oreil.ly/G6grC), including one for [AI engineering
    for leadership](https://oreil.ly/z-nXp). Because AI engineering is about leading
    change. And most developers love to be introverts, but they sometimes become managers
    and they have to lead people. And this AI stuff is kind of scary for people. It’s
    also very technical to understand. I have a whole new course themed on the kitchen.
    Also, Microsoft Dev Channel has a new show we’ve made, called [Mr. Maeda’s Cozy
    AI Kitchen](https://oreil.ly/N9oCg). Yes, we cook AI every two weeks in my kitchen.
    And we have guests and they try the AI.'
  prefs: []
  type: TYPE_NORMAL
- en: 'And the [DeepLearning.AI course](https://oreil.ly/nxusL) was an opportunity
    to talk with [Andrew Ng](https://oreil.ly/CDLm-), who I think is one of the great
    minds of our time. And he gave this talk at the Wall Street Journal CIO Summit,
    where someone asked him if this is going to change how people’s jobs are, and
    all the fear around it. And he said the best thing I’ve ever heard anyone say:
    you should think of AI as automating tasks, not jobs. Any given job has many tasks.
    And if there are a lot of tasks that you don’t like to do as a human, that aren’t
    of high value as a human, then automating them with AI makes a lot of sense and
    can improve your job. Whether it’s a gnarly testing function you’re writing where
    you’re thinking, “Oh, that’s going to be such a pain with all the cases,” and
    boom! It’s there. Or something like a shell script that is always different in
    every language with a little subtlety. You just say I need a shell script. Just
    a half an hour ago, I did that. I need a shell script. And then find out, oh,
    that was easy. And you debug it yourself too. So, it’s taking tasks that I don’t
    like to do and having it be done for me.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.:** I can really see that. Like, right after this discussion, I’m creating
    the transcription and I’m creating the action points and the summary of the most
    important information. No one likes to do that and that’s maybe 10% of my job
    because we are having so many meetings. I just want to go back to your design
    background. Looking at this adoption pattern where companies and people are using
    generative AI, LLMs, and they are learning how to evaluate them, how to use them,
    how to orchestrate, from a design point of view, do you see anything happening
    in the near future that will be radically different from a design UX, UI point
    of view?'
  prefs: []
  type: TYPE_NORMAL
- en: '**J.M.**: Yeah. Well, I definitely think that this zero UI revolution is happening
    where you don’t need a lot of user interface, user experience, psychology when
    the machine can discover your intent and execute the task. There’s this thing
    called [Jobs to Be Done by Clayton Christensen](https://oreil.ly/Aq12m). It’s
    almost as if we create user experiences to get a job done, but if the machine
    knows what job you want to get done and you tell it what to do and it does it,
    did you really need any experience in the first place?'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: That’s amazing, that is funny. Just today I was getting that question
    from a student about how to define the jobs to be done for artificial intelligence.
    I was like, I don’t know, I don’t know.'
  prefs: []
  type: TYPE_NORMAL
- en: '**J.M.**: Yeah, because with tool calling and function calling, you give it,
    like in Semantic Kernel, just last week I had this weird moment where I gave it
    five plug-ins I wrote, and then I didn’t have to construct the logic of how to
    make them all work. It was actually too hard for me to write the logic, and the
    planner just constructed the flow the way that I couldn’t write.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Wow, that’s incredible. And just to finish, since you mentioned the
    kitchen, if you had to choose, did you have one recipe that you say, this is something
    that someone needs to learn for the next stage of adoption?'
  prefs: []
  type: TYPE_NORMAL
- en: '**J.M.**: Oh, good question. Yeah, I tell everyone that in the kitchen that
    you have to realize there’s two kinds of AI models. One AI model does completion,
    the other does similarity. And this is called the embeddings model. This is called
    the completion or chat completion model. It’s a combination of these two together
    that are making this revolution amazing. If you only have one, it’s no good. If
    you have chat completion or completion, it’s going to be ungrounded. It’ll say
    things that make no sense. If you have similarity models, which are basically
    search, you can find something, but you can’t synthesize. The two together make
    an incredible pair. It’s like one is butter and one is flour. Like together you
    can make great cookies. And this is the core recipe for everything with large
    language model AIs. You can create function calling models, you can create sophisticated
    chat, you can create supply chain automation, all from these two models. But one
    model alone is not good enough, you need the two together.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: You’re right. And I think this in the human comparison would be something
    like IQ and EQ together. Like the ability to remember information, traditional
    intelligence, but that ability to explain in a proper way that is adapting to
    the audience. Yes, I love it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sarah Bird: Responsible AI for LLMs and Generative AI'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**A.G.**: Do you want to start by explaining your role in the organization
    and what you are doing at Microsoft?'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aoas_07in04.png)'
  prefs: []
  type: TYPE_IMG
- en: '**S.B.**: Yeah. I’m Microsoft’s chief product officer of responsible AI. What
    that means is my team is responsible for figuring out how we take a new AI technology
    and ensure that it’s developed responsibly. In the case of a lot of the AI we
    build at Microsoft, we’re figuring that out ourselves. If we’re partnering with
    other organizations such as OpenAI, then we work with them to ensure that the
    right things are happening as they develop the AI. But then it’s not just about
    the model, it’s really about how we ship a complete application safely. We take
    that new AI technology and look at what the entire approach we need to follow
    is to use this technology effectively.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, for GPT-4, an exciting new piece of technology, the first place
    we shipped this was in Microsoft Copilot, originally called Bing Chat. Our team
    went in and basically led the responsible AI (RAI) development of that. We developed
    new mitigations, we developed new testing tools, we developed new techniques for
    red teaming. All that we learned, we built into the Azure AI platform and that
    enables it to power all of the AI at Microsoft as well as enable our customers
    who are building their own AI applications to use the same best practices. That’s
    the mission of the team, figuring out how we really put AI into practice, and
    then ensuring that we’re using those best practices across Microsoft and empowering
    others to do that as well.
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: That’s a lovely mission. And it’s not a new one. There’s a journey
    at Microsoft with responsible AI even before the GPT models.'
  prefs: []
  type: TYPE_NORMAL
- en: '**S.B.**: Yeah, it’s something that we’ve actually been doing a long time.
    I was fortunate to be part of founding the first research group in responsible
    AI in Microsoft, and this is the [FATE group](https://oreil.ly/hmo69), back in
    2015\. This is something we’ve been doing for almost 10 years. But we’ve come
    a long way during that time. It went from just some ideas in research to, the
    next thing that we founded was the Office of Responsible AI, which was really
    starting to set what is the policy or the standard we want to follow. But even
    creating a policy without much experience in implementation is really hard. A
    lot of the journey since then has been figuring out how we really do this, and
    iterating between policy, engineering, and research to really mature our practices,
    tools, and technology. But even with generative AI, for a lot of people, the first
    moment they were aware of it was ChatGPT. But actually, well before ChatGPT came
    out, Microsoft shipped GitHub Copilot, which was really the first generative AI
    application that we produced at scale. A lot of the things that we used in Bing
    Chat and other applications were actually originally developed for GitHub Copilot,
    because that was the first real-time generative AI application. [Azure AI Content
    Safety](https://oreil.ly/uB6d-), the safety system that we use in our gen AI applications
    today, was actually first developed for GitHub Copilot.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: It’s funny because a lot of people forget, including ourselves, when
    we are talking about different Copilots, that GitHub Copilot is actually the patient
    zero, the first one and the original one.'
  prefs: []
  type: TYPE_NORMAL
- en: '**S.B.**: It was eye-opening for us working on it because the GPT technology
    was exciting, but it felt like it could still be, it was still a toy. Then when
    the GitHub team really showed the early prototypes of GitHub Copilot, we were
    like, wow, this is real, this is really exciting. But at that time, we weren’t
    sure, is it just this one application? How narrow is the technology? How many
    more GitHub Copilots will there be? Then when the next wave of it came out, going
    from GPT-3 to GPT-4, GPT-4 was when we were like…oh, this is not narrow anymore.
    There will be many more Copilots that are possible. That jump in the technology,
    I think, really unlocked many more applications, but GitHub Copilot really showed
    the way first.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Yes, and I think from an RAI perspective, the idea or the adoption
    pattern of having a regular completion, something that is a singular interaction
    with the machine, and then moving to something that is chat-related, with memory,
    with all the benefits and all the considerations. I think that that’s probably
    the evolution of that learning, the engineering and policy duet that you’re talking
    about.'
  prefs: []
  type: TYPE_NORMAL
- en: '**S.B.**: Yes, certainly. There are nuances in the GitHub Copilot application.
    I actually really loved the design of it because it is a paradigm people are already
    familiar with, with the autosuggest. We’re already comfortable with the idea of…hey,
    the suggestion might not be perfect, but if I like it, I can keep it and I can
    still go and edit it. We all know that it makes us go faster in natural language.
    But then knowing that actually was going to be effective for code, that wasn’t
    obvious. But we did have to look at both with that natural language risk, hateful
    content, violent content, things like that, and also code risks, like the ability
    to produce security vulnerabilities or known weaknesses in the code. We had to
    address both of those dimensions. Because the application is only useful if it
    goes faster than people actually can type, there were really extreme latency requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, the transition to Bing and Copilot since then in the chat applications,
    as you said, adds this multiturn dimension. Now, if you’re trying to look at an
    interaction and say “Hey, did the AI system do the right thing?” you actually
    have to score a multiturn natural language conversation, and that’s much more
    challenging. There’s a much bigger diversity of topics and types of interaction
    that the system is going to look at. We started with a strong foundation with
    GitHub Copilot, but certainly with the power of GPT-4 and the power of the search
    engine, and the breadth of things that we wanted to cover there, we really had
    to look much broader. So that’s when you start having conversations about things
    like hallucination, because accuracy really matters, or missing disinformation
    because the search engine is so connected with information integrity. And so the
    aperture really broadened with that application.
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: It must be very interesting, that moment that we realized we actually
    need new metrics, because you have mentioned the performance, and we had the ROC
    curve, the F1 and F2 scores for classification topics and stuff. And then we arrived
    there and we said, OK, we have a new kind of application that is based on something
    called generative AI. We need to test the performance of this. We have the metrics
    from traditional linguistics like BLUE and ROUGE. How was that? At the AI level,
    like what do we do now?'
  prefs: []
  type: TYPE_NORMAL
- en: '**S.B.**: You know, the thing is we always knew we needed metrics to actually
    address these risks, right? It’s very hard to understand if mitigation is effective
    or if a risk is present without actual metrics. And one of the big challenges
    in RAI for a long time is that these metrics were really difficult to get. For
    example, let’s go back to saying, “How do I rate a multiturn conversation?” So
    if you’re looking at doing that for “hate” (as an AI content safety metric), our
    guidelines internally are more than 20 pages long to kind of score that conversation,
    and they’re built for expert linguists. And so that meant that we can measure
    for response by risk, but only very infrequently as sort of the outer loop. OK,
    an application is basically ready to ship. We can run one set of tests that are
    very manual, and have the human reviewer score them. And if the results look good,
    great, we can ship it. But with that, you’re not able to really innovate in the
    inner loop and really try different things, and find which one works the best.'
  prefs: []
  type: TYPE_NORMAL
- en: Actually, one of the most memorable things for me about developing Bing Chat
    quite early, as we were using GPT-4, was realizing that it actually had the potential
    to help us automate these metrics. We were actually able to use GPT-4, with a
    lot of prompt engineering, and get it to score similar to the level of those expert
    humans. And so that meant we went from…hey, we’re gonna be able to check this
    very rarely, maybe once a month, maybe at the very end, to every single night
    when we make a change to the system, we can run the safety test overnight, look
    at the scores, and iterate. And so that unlocked just a whole new wave of responsible
    AI innovation. The technology is obviously a significant breakthrough for AI,
    but it’s also a significant breakthrough for responsible AI and safety and security
    because it’s this amazing new technology that just understands language and context
    so much more. We’ve really put that to use in our own development of AI.
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: And you have mentioned the key words like safety, security, even
    compliance, regulations, and responsible AI. Everything is converging at this
    point. Everything is going towards something that in the beginning was the ethical
    way to do things, like the willingness to do something that is good, towards something
    that is responsible, that is accountable. And I think that that’s a wonderful
    thing from a technology perspective, that organic evolution.'
  prefs: []
  type: TYPE_NORMAL
- en: '**S.B.**: Yeah, I think with generative AI, one of the things that’s been exciting,
    but also challenging, frankly, is that with a lot of the responsible AI work we
    did before, just the AI developer could manage it. And everyone kind of got the
    benefit, but they didn’t need to really understand the details as long as you
    work with a great AI provider like Microsoft. You were set. With generative AI,
    we really end up needing both for safety and security to use a defense in depth
    approach where the model developer needs to do things, the safety system developer
    needs to do things, the application developer needs to look at the meta-prompt
    and the grounding information, the final application developer needs to look at
    how the human interacts. What does that UX look like?'
  prefs: []
  type: TYPE_NORMAL
- en: There’s so much more that needs to be done to use this technology effectively.
    And it’s not surprising, it’s a much more general-purpose, more powerful technology.
    This went from something that was really just housed in a small number of responsible
    AI experts to something that now every organization, every security professional,
    every AI developer needs to think about. It’s been really fun to see the interesting
    growth and support for the work, but also the explosion of demand means there’s
    so much more that we have to do. And that’s really exciting, but also can be challenging.
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: It is very exciting. And I think it’s very aligned with the kind
    of artifacts and material that the [Responsible AI Initiative at Microsoft](https://oreil.ly/tCL6L)
    is putting out there, available for organizations at both the technical and organizational
    level. I’m thinking about the [impact assessment](https://oreil.ly/bJAeg), the
    [HAX toolkit](https://oreil.ly/AtDRJ) for the interfaces. What’s your favorite?
    If you had to choose different pieces of material that are useful for organizations
    in terms of responsible AI, what would be your selection?'
  prefs: []
  type: TYPE_NORMAL
- en: '**S.B.**: Oh, it’s so hard. I love all the responsible AI things. But I think
    what you’ve called out is really important because it is a mix of practices, policies,
    and technologies. And you really have to look at the whole spectrum and customers
    and organizations are asking us for that. So, for example, one of the things I
    really like is that we’ve put out our [Responsible AI Standard](https://oreil.ly/j5tBY),
    which is really the guide for how we do this overall out there. So organizations
    can look at it. They can adopt something similar if that works for them. We also
    put it out there so we can get feedback. People can tell us what they think we’re
    missing, what they’re finding works, what they’re finding doesn’t work. And so
    that’s really kind of where everything starts with us. But then if you want to
    go and put that into practice, you need to first start with a process like an
    impact assessment where you’re really mapping the risk. You then need to be able
    to measure risk effectively. And so actually we just released [new safety evaluations
    for generative AI](https://oreil.ly/GcCSo), which are the tests that we run ourselves
    to really measure these risks. And that’s actually the breakthrough I was telling
    you about earlier.'
  prefs: []
  type: TYPE_NORMAL
- en: And then you also need to be able to mitigate the risk. Azure AI Content Safety
    is a great way we mitigate the risk. That’s the safety system layer. The HAX toolkit
    really helps with the application, the UX layer. We’ve also put out prompt engineering
    guides and meta-prompt templates to help with the prompt layer. You really have
    to look holistically across all of these to adopt that. Another one that we get
    asked about a lot from customers is how to red team, how to do that kind of final
    expert validation. We put out [red teaming guidelines](https://oreil.ly/oDBO_),
    but we know red teams are limited resources. So we’ve just released [PyRIT](https://oreil.ly/azSbW),
    which is a tool that helps accelerate the productivity of red teamers by helping
    them get more ideas for the next thing to try, basically using AI to assist them
    the same way we’re using AI to assist many other roles now with what we’ve developed.
  prefs: []
  type: TYPE_NORMAL
- en: We’re finding that people really need all of these pieces. A lot of the work
    we’re doing is trying to make sure that they understand that complete spectrum
    of practices and policies and tools that they’re going to need to achieve this.
    And we want to make it easy for everybody to just pick these up and go running
    with them, but also customize them as they need. We know different domains are
    different, organizations are different, and so we don’t want it to be just the
    Microsoft way. We just want to make it really easy for people to start with the
    responsible AI state of the art and then adapt it to them.
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.:** Yeah, and this is useful. In my case, I’m using it with partners,
    with integrators, with consulting firms, with clients also who are asking for
    inspiration or some good practices on how they can approach responsible AI. Traditionally,
    it was about defining the AI principles, like we want to be accountable, transparent,
    etc. But now we’re going farther on how to approach this at an organizational
    and technical level.'
  prefs: []
  type: TYPE_NORMAL
- en: '**S.B.**: Yeah, and I think we have people ask both, how do I do a practice
    like red teaming or evaluation that we mentioned? Or they ask, how do I address
    a particular potential risk, such as hallucination or prompt injection attacks?
    We see people looking for guidance in both of those dimensions. And the answer
    for something like hallucination is…here are the steps: here is where you identify
    that risk, here’s how you measure that risk, how you red team it, here’s layers
    of mitigation for that. They’re actually a horizontal and vertical pattern, but
    we hear people asking for guidance in both of those ways.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Yes, indeed. And I think you have mentioned the experiences with
    GitHub, or Microsoft Bing/Copilot, that I think have been extremely illustrative
    for everyone trying to create their own copilot or whatever platform, even for
    competitors. I remember people saying…“Hey, Jordi Ribas (CVP Microsoft) and the
    team are releasing learnings every week, and this is so useful now for everyone.”
    So that’s very exciting, that movement from model to platform, and all the learnings
    that go with it.'
  prefs: []
  type: TYPE_NORMAL
- en: '**S.B.**: Yeah, and we’re still learning every day now, as technology becomes
    more accessible to more people, all of the exciting new use cases that people
    can think of. But I think those early days were very special. The rate of learning
    was just insanely high. And we had the first ones where we had brought experts
    from around the company to work on this. Quite a few folks from Microsoft Research
    volunteered to work on that full time. We had these great minds all working together,
    iterating every day. And I think for a lot of people, that experience also kind
    of changed their work after that in their research directions, because they went
    in and really saw what the real challenges we have right now are, but also the
    real amazing potential of the technology. That hands-on experience where you were
    learning so much and we were all learning together, I think was really shaped
    in the way we’ve done AI at Microsoft and many different people’s outlook on that.
    So that certainly I think was a really special innovative time for us.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: That must be amazing. I can imagine those days and those discussions,
    the daily work. It was very exciting also from the consumer point of view, just
    to see the news and all the new functionalities, not only the models, but everything
    that goes with that. What’s your vision for the next…it’s too difficult not to
    say two or three years, but just for the next year, your vision of how this will
    evolve, the kind of things that we may see, the kind of challenges that we may
    have, what do you think will happen?'
  prefs: []
  type: TYPE_NORMAL
- en: '**S.B.**: Yeah, I think there’s a couple of patterns that we’re seeing. Certainly
    one of them is multimodal, right? A lot of the applications are still kind of
    mostly text, but there’s so much more potential when you can understand together
    different modalities, images, audio, video, etc. We’re starting to see very exciting
    examples of that technology. I think over the next year, a lot more multimodal
    will come in, and that certainly brings new types of risks from a responsible
    AI point of view. I think there’s a lot of excitement about the next wave of technology
    and AI agents, having the technology that can perform more actions. That of course
    greatly increases the space of things you need to think about in terms of responsible
    AI, but also the level of quality and inaccuracy that you need, because if you’re
    taking an action, a mistake can have a much bigger impact.'
  prefs: []
  type: TYPE_NORMAL
- en: Those are kind of two big ones that are on my mind, but maybe in the kind of
    bigger picture sense, Kevin Scott (Microsoft CTO) says regularly that right now
    this technology is on an exponential curve, but we only get to see the next points
    on the curve every year or two, when the next wave of technology comes out. I
    think a lot of us are asking ourselves, “Is the next one really gonna be exponentially
    better than GPT-4?” And if it is, what does that really mean? Like our minds have
    a hard time thinking in exponentials, we really kind of project out linearly.
    There’s also this chance that we’re gonna be just seeing another extreme breakthrough,
    sometime soon. And so, I think, one of the exciting open questions is just how
    much better will the next wave of technology be?
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Yeah, I think it’s exponential, the kind of performance that we’ll
    see, and the kind of considerations that you’re mentioning. Like I said, there
    are multiple dimensions that we need to consider on that journey of new applications
    being created. A good example is what we saw with [OpenAI releasing Sora](https://oreil.ly/ppSjf)
    and just putting it out there, and showing the benefits but also sharing it with
    different parts of the community to analyze the potential considerations, because
    we can do a lot of things with this technology. But it’s exciting.'
  prefs: []
  type: TYPE_NORMAL
- en: '**S.B.**: Yeah, I think it’s as a technologist, it’s your hope, but certainly
    not expectation, that you’re gonna be around when the technology goes through
    a critical transformation, really crosses the threshold from being something that’s
    an exciting research idea to something that is really ready for practice. And
    so, I remind my team every day, we need to be enjoying every moment of this, because
    obviously I think the impact of the technology will only grow and that will be
    really exciting as well, but nothing is quite like the beginning in terms of the
    change that that brings, and the rate of learning and everything. And so, we’re
    just enjoying the ride, but also very, very aware that we’re in a position of
    leadership where we need to steer the direction of the future of this technology,
    and we need to help the world be able to use it in effective ways, but also ensure
    that it is not used in ways that I think society really doesn’t want. And so,
    I think we’re also very aware of the weight of the responsibility of being here
    in this position at the beginning and really making sure that we make decisions
    we think are going to be right for the future.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tim Ward: The Impact of Data Quality on LLM Implementations'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**A.G.**: Of course, you’re the CEO of CluedIn, but what’s your role in the
    company? What’s CluedIn doing in terms of data management, data quality, and so
    on?'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aoas_07in05.png)'
  prefs: []
  type: TYPE_IMG
- en: '**T.W.**: Yeah, sure. I’m actually joining you from a hotel room in Seattle.
    I am literally about 200 meters away from the Microsoft headquarters in Redmond,
    so I’ve been working with that group all week. This has a little bit to do with
    my role. I run the CluedIn team, I’m the CEO of CluedIn, but I come from a very
    product-driven software engineering background. I’ve been architecting products
    and building enterprise-grade products for some time. What we do at CluedIn is
    bring some pretty critical and necessary elements to Microsoft customers, and
    that is in the form of data quality and master data management (MDM).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data quality is probably one of those aspects we’re all aware of, we know we
    need to fix. MDM is somewhat one of those mysterious topics that I think people
    might even say is synonymous with data quality. What’s MDM versus data quality?
    At CluedIn, we really see there’s quite a lot of similarities between what data
    quality fixes and MDM. What we’ve really done is find those different elements
    or categories, and here is the kicker: CluedIn is a tool that’s really targeted
    at nontechnical users. That’s because we think that data quality doesn’t seem
    like one of those stubborn things that we’re always tripping over, and we know
    we need to do it. What we believe and what we’ve seen with our customers is we
    were never able to bring the business in and make them responsible for this. Often,
    the tooling was a little bit too complex, and so what I’m happy to say is that
    we bring those capabilities to anyone that’s in that Microsoft ecosystem. They’ve
    got Fabric, they’ve got maybe Purview, they’ve got Azure Data Factory. But at
    some point, they need to go, how do I bring the business in and have them play
    a role in this supply chain of data as well.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Well, that’s amazing because I’m pretty sure you heard with this
    initial generative AI wave like…oh, we don’t need data anymore, so we don’t need
    to care about the quality. Then what happened, people were saying they would like
    to customize development and use their own data, but wait, we haven’t taken care
    of the data quality for a while, so what to do?'
  prefs: []
  type: TYPE_NORMAL
- en: '**T.W.**: Exactly. Now, there’s somewhat a race condition here in that to yield
    value and insights with data, and specifically AI, we probably need AI to help
    us with the data quality piece. There’s this quite self-fulfilling recursive nature
    to the yin and the yang between solving data quality and actually yielding value
    AI. Spot on with, I think, your analogy there.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Yes. Before we go into the details of data quality, because I think
    it deserves some discussion here, but how was 2023 for CluedIn from a generative
    AI perspective? I know that you have been working on a lot of things, including
    your own product. How did you experience this?'
  prefs: []
  type: TYPE_NORMAL
- en: '**T.W.**: Many facets. Number one thing, being a company just “slightly” smaller
    than Microsoft, just slightly, that I guess we adopted AI ourselves, just internally
    as a business very early on, and it started with GitHub Copilot. It then progressed.
    Fortunately, due to our great relationship and partnership with Microsoft, we
    were given early access to Azure OpenAI in a private preview. That was something
    we instantly realized, wow, that’s how we’re going to build this in our own products.
    This also gave us a bit of time to learn about the guardrails that were necessary.'
  prefs: []
  type: TYPE_NORMAL
- en: You and I both know, Adrián, GenAI forms some pretty spectacular demonstrations,
    but being in the data management space and data governance and data quality, often
    the discussion goes to how do I make sure that our generative AI initiatives are
    actually going to survive the tumultuous nature of the enterprise? Is it secure?
    Is it governed? Do I have an audit trail of what happened? Is someone responsible
    for the data that’s being used? What about all the data sovereignty questions
    about where is data? Pretty early, we were having those discussions internally,
    but also with our early customer adopters that were saying, as soon as you guys
    start to implement AI in your platform, please let me know because there just
    seems like such an opportunity to apply AI to the actual data management practices
    itself, not just use it as an end consumption piece of software.
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Which makes total sense from a Copilot point of view of interacting,
    of adding something to the user interface, we are saying, MDM or data quality
    in general, they are traditionally some sort of technical task, but we want to
    bring this to a business because they know their data, they know the information,
    so we can put this layer to infuse generative AI, and that’s what you guys did,
    and did it very early.'
  prefs: []
  type: TYPE_NORMAL
- en: '**T.W.**: Yeah. I would argue that’s been the biggest gap that we haven’t been
    able to bring the business into, because we often give them this software and
    we say, hey I bought this great MDM platform for you, just put all your data quality
    rules in there. Then someone comes in and says, OK, it says put a regular expression.
    Sorry, what’s a regular expression? I’ve been a software engineer for 19 years,
    and I still don’t know how to build regular expressions, but we’re asking people
    to somehow do this and that’s how they’ll play a role. And I think that’s why
    often these initiatives get thrown back to IT naturally because this seems like
    it’s for them. Then IT says, “No, I’ve got my tools, I’ve got Fabric, and I’ve
    got Azure Data Factory, that allows me to play my role, but it asks me to be very
    technical.” You could argue, Adrián, haven’t we been trying to bring the business
    in for 30 years? What has changed? Well, apart from the fact technology has just
    changed in general, getting access to different software is easier and easier
    all the time, and the cloud of course brought part of that.'
  prefs: []
  type: TYPE_NORMAL
- en: The other piece is we’ve been handed in this nice little wrapped up bow an easy
    way to interact with LLMs; that is that chasm, it’s that bridge between, you can
    tell me what you intend and I’ll translate it underneath into what the underlying
    system needs. Because the thing is, for detecting patterns in data, especially
    in a deterministic way, regular expressions are somewhat just the way we do that.
    You need some underlying function to be able to do that, especially in a cost-efficient
    economic way. We can’t at this point, which is one of the things I’m looking forward
    to, we can’t just throw a large language model at every problem. Actually, we
    shouldn’t. I personally wouldn’t sleep as well if I realized my whole supply chain
    was just running off a model that sometimes gets things right and sometimes gets
    things wrong. It’s that bridge, how do I use the general knowledge to bridge that
    technical thing that these tools will still ask you to do, but now it’s not so
    much in this very like, oh, I need to be technical to do it.
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: These are the cases where when we’re interacting with the tools,
    or we are processing information, or we are trying to fulfill some JSON file by
    using generative AI, the engine itself becomes more deterministic. It’s like we
    are not giving that much creativity because we are trying to find that connection
    with the system. In your case, you have software, you have a backend layer with
    all the data, you are connected to the data. I think that that’s the perfect example
    of evolution on the interfaces. When Bill Gates said this is like the evolution
    from the command line to Windows, and then from Windows to this kind of generative
    AI interface. How do you see the relationship? I know that this is not an answer
    for now, it could be an answer for later, and for later, and for later on the
    roadmap of the different products. But how do you see the current relationship
    between a company like CluedIn, or even the MDM and data quality solution, and
    the Azure OpenAI engine?'
  prefs: []
  type: TYPE_NORMAL
- en: '**T.W.**: So I think the relationship is somewhat symbiotic, and a good example
    is the plug-in architecture for Azure OpenAI. The fact that you can plug in something
    like Uber, KAYAK, or TripAdvisor, and the LLM knows. I know when you want general
    chat and general knowledge, but then I also can do the smart thing of saying,
    actually, when do you just want to talk to KAYAK or TripAdvisor, and book a trip?
    A very similar thing happens on the data management side, via CluedIn’s copilot
    that we have in our platforms, very similar to what you have in Microsoft 365
    or Power BI, or up-and-coming Fabric. If you had a big dataset with a million
    records, right now, without actually training your own model on that data, there’s
    really no easy way via context windows to just, not in an economically viable
    way anyway, to say what’s the value in column 4 in row 464,000? But the symbiosis
    is, how do I translate that language into an underlying language that can then
    do that query in a very efficient way? That could be translating it locally into
    SQL. In our case, it’s translating it locally into something like an elastic search
    query that says, I’ll build the query, so the LLM is not actually looking at a
    million records, it’s transposing into the local environment.'
  prefs: []
  type: TYPE_NORMAL
- en: Listen, I think at some point you will have these unlimited token sizes where
    you can either just say, I want the whole one million rows in the context, or
    potentially it’s going to be…load that data into a model, and your copilot is
    running off your custom model. And Azure AI Studio is a great tool that makes
    it so easy already to build your own copilot of Llama and Mistral and things like
    this, and also throw your own data from quite heterogeneous file types as well.
    Everything from PDF to images, to CSV, to Excel, to text, to video, and even C#
    and SQL files, it can swallow that stuff up. At some point, you are going to get
    to a point where you might not even need to do that local translation in all situations.
    You could literally talk to your entire data estate with a native feel in chat.
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Yes, there is another discussion with Dr. John Maeda, he was mentioning
    the notion of plug-ins, everything is interacting, and we are even building the
    code based on needs. It’s like function calling, but imagine automatic function
    calling, on which the model can realize, I need to check on my storage or Cosmos
    DB, I need to check on whatever source of information I have. Even more, if I
    was imagining (and I know this interview is about asking you questions), but just
    imagining the future, and this is not even related to roadmaps or whatever, but
    imagine in the future you have your data state, then you are handling general
    data governance with a solution like Purview, then you are going to the details
    of data quality and MDM to prepare all the data, and then there’s a smooth two-clicks
    way to push to a data store.'
  prefs: []
  type: TYPE_NORMAL
- en: '**T.W.**: I then have to comment on this, Adrián, because very early on when
    OpenAI came out, almost within a matter of days, this concept of LangChain came
    out as well, in that I want to chain multiple things together, and of course this
    was one thing we said is we have to have this included, because what we want via
    the plug-in architecture is to say, go get me all of our employee files that we
    have, bring them in, map them into the same concept, and trade the semantics of
    column names for me. If you’ve got F name, first name, first, of course it easily
    swallows that up, but in many cases you might bring on an SAP system, and it’s
    column names, not that obvious, they are German acronyms in a lot of cases. And
    for it to be able to chew that up and say, “I know what you mean,” but then to
    chain things, and then after that, check every column and apply appropriate data
    quality checks, and that’s the one thing I love, the fact that you can just be
    very dynamic. That you’re not being prescriptive and saying, “No please enforce
    this standard of phone numbers,” that will work, but also the fact you can be
    very dynamic and fluid in the way you interact.'
  prefs: []
  type: TYPE_NORMAL
- en: I am in the data government space, but to be honest, I don’t know ISO codes
    off the top of my head, I don’t know how fun that person would be at a party if
    they actually did know that, and you’re wanting the large language model—the truth
    is it knows that stuff, it knows the ISO codes, it knows what they do and that
    chaining of things, I think this is what takes the use of GenAI from something
    that saves you 5, 10 seconds, to something that genuinely saves hours of research
    or trial and error. And that’s where I think the key thing is in bringing it into
    products, we have this kind of standard or set of ethics we have on the use of
    AI in our product, and we take a couple of these from inspiration from Microsoft
    as well, one of them being your data is your data, we’re never going to use cross-customer
    data to train this general model.
  prefs: []
  type: TYPE_NORMAL
- en: But one of the ones we’ve added ourselves is, no AI for AI’s sake, and what
    that means is, if we build something on our platform, and actually you could probably
    do the same thing in the old way, probably faster or relatively the same, why
    bother using AI? You know a good example would be, it’s technically impressive
    in a chat to say “Find me all the employees that are over 64,” but actually by
    the time you’ve just used our rule builder, you’ve probably taken the same amount
    of time to do it by hand than using AI, and at that point it’s like, what’s the
    value there? And I would argue, it’s not so much. There are cases where it is
    smart, for example if I said, “Go get me all of our customers in the Nordic region,”
    and I don’t have to say, “where the country is Denmark or Iceland” or this or
    this. Now that’s great you’ve saved 15 seconds, but really the things we should
    be focusing on is, how did I save you complexity? How did I increase simplicity?
    And what were the things that saved me one or two hours, three days, that’s what
    we’re really trying to focus on here at CluedIn.
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Yes, I love that vision of how the end-to-end architecture chains
    different functions that handle the data and AI activities of any company. But
    I am seeing cases in which people are using generative AI to re-create chatbots
    all the time, for example, because I want it to be deterministic, I want to use
    it like a knowledge base, and then I have 10 questions, 10 answers, and I say
    that’s not necessarily something we maybe want to do with generative AI. Do you
    have interesting stories or insights on how data quality is already impacting,
    either in a negative or positive way, generative AI implementations with Azure
    OpenAI, or any other technology? Like clients saying that because we have been
    working on data quality, and we have this data state properly done, we have seen
    the difference.'
  prefs: []
  type: TYPE_NORMAL
- en: '**T.W.**: One of the things that’s so interesting about the form factor of
    chatting to your data is that it surfaces bad data quality quicker than probably
    any other form factor like search, or anything like this. It becomes abundantly
    clear, and I think also it’s because people have high expectations of LLMs, so
    even when it does something slightly silly, in my head I go like, “I appreciate
    this so much, this is so amazing,” like with my children, I will forgive my ChatGPT
    more often than I don’t, and I think one of the cases is when you start throwing
    your own data into a LLM, what happens is the chat interface starts to surface
    your data quality issues really clearly.'
  prefs: []
  type: TYPE_NORMAL
- en: A good example would be a case where, pulling in HR data on employees as part
    of a HR onboarding process, to make new employees feel a little bit like they
    don’t have to go and find out, “Who do I talk to about this and that?” Now of
    course they have a HR system where it’s tagged with information like, this person
    is a software engineer, and they’ve got these responsibilities, but also with
    the large amount of employees, it was improbable that that would be the best way
    to do it, so the form factor of being able to use your own natural language was
    great. What happened is when you typed in something like, “Can you give me the
    contact details for the person that knows the most about Azure OpenAI?” or something
    like this, it would come back very confidently and say, “Not a problem, I’ve got
    this and this and this phone number.” And you know, there’s a couple of challenges
    with that, number one is, you’re more confused now, the second thing is, without
    proper attribution, you’re not 100% aware if the AI is making this up.
  prefs: []
  type: TYPE_NORMAL
- en: One of the great elements about Azure AI Studio, and if you’re using that particular
    place to host your copilots and anything you’ve done with your custom models,
    you get attribution at a file input level, for free. The challenge is that the
    data lineage doesn’t start there, it started a long time ago in a different place,
    but you get it at one of the places where it landed, so you could put it into
    your AI model, but actually the lineage of what happened to that file, where was
    the source system, what happened along the way, who changed what, why did they
    change this…this is some of the lineage that things like Microsoft Purview brings
    in at an asset level, and CluedIn is bringing us at a record level. Purview can
    say, these four assets on employee data were fed into your model, great. Then
    CluedIn says, see that Martin there, and that Martin there, there’s no way for
    me to stitch this data together, but I actually put those together into the same
    record, and so once you’re using the Copilot on this cleaner data, the answers
    are much more precise, and much more trustworthy because you can see…here is the
    phone number for Martin, oh, and that’s where I got it from, that’s what made
    me decide on it, so it becomes so abundantly clear, when you start to use the
    copilot, you’re like technology is great and super interesting, did it just make
    this up? And you’ll know from these AI models, they’re not self-aware. They cannot
    let you know if they made something up. Isn’t that a weird difference between
    the large language models and us? Like I am self-aware if I’ve made something
    up, but the LLM is not.
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: And it depends. I really like that, because imagine you are a data
    scientist, and you are performing an exploratory data analysis. If you don’t have
    the context on the business, you’re not able to understand if something that you
    see on the data, or even your own analysis, will be actually true, and I was thinking
    about that notion of EDA or exploratory data analysis, like the next barrier,
    because you have probably seen it in your projects. The best EDAs are those that
    include people who are from a mathematical and technical background, but also
    those that are from the business side, and usually they (business folks) don’t
    perform EDAs because they don’t have the technical means to explore the data,
    and to ask questions to the data.'
  prefs: []
  type: TYPE_NORMAL
- en: But this notion that you’re bringing, exploring the data and understanding that
    something is wrong, leads to the discussion where people are talking about “model
    hallucination.” I don’t like that expression, hallucination, because it’s not
    like a human, but they were talking about the model, and I feel like this chaining
    of capabilities shows that it’s not only about the model, it’s also about the
    data. Because you can have the best model, GPT-5 or whatever, and even if it’s
    very precise, we combine it with our information, which is feasible for a lot
    of scenarios, and we need to take care of that data so the LLM retrieves the good
    information. What’s your vision for these topics on generative AI in regards to
    CluedIn for this and the years to come? How do you see this evolution of a platform
    at a functional level?
  prefs: []
  type: TYPE_NORMAL
- en: '**T.W.**: You know, the part where I realized that LLMs were something super
    powerful was the first time it clicked that I can translate my input to a targeted
    output, where I can say, here’s what I want, and can you actually return your
    answer, like this JSON structure? And it was at that moment that I went back to
    our team, and I said, all right, let’s look through all of the functionality that
    includes, and I want us to go through and really understand, could I use AI in
    every different part of the platform? And you could look at some of the things,
    like that when people make a change we cause an audit trail, and you can think,
    no, that’s like a log, why would AI have anything to do with that? And you realize,
    well, some of these records over time, they change a lot, and instead of having
    to go through a huge changelog, can I summarize the history of the change? And
    you could literally put this in all different places and have a net positive.'
  prefs: []
  type: TYPE_NORMAL
- en: For me, the vision is, where are the biggest wins? Where are the wins that are
    2 hours instead of 20 seconds? We need to focus on the things that actually are
    time-saving, and hit some type of business metric, make more money, lower operational
    costs, lower risk, complexity, etc. so we can just get things done without stressing
    all the time. So, I would argue to some degree, this space of AI is moving so
    fast, that for ChatGPT or even the DaVinci model, we haven’t milked all the value
    out of that. There’s just so much you could start to do, and, of course, the beautiful
    part is we get to wake up every day, and think…that same prompt, it’s just more
    reliable with an answer now, and I didn’t really have to do anything besides change
    to a model that also supports that functionality, like function calls, or completions,
    or something like that. For me, the vision of CluedIn’s use of GenAI is very much
    self-aware that you need AI to solve the data quality problem in a more complete
    way. You will still use traditional techniques that are kind of deterministic,
    I can sleep at night because it’ll do the same thing, every time, you still need
    those.
  prefs: []
  type: TYPE_NORMAL
- en: And then, I think, what our job here at CluedIn is, I need to bring that to
    the data management process, so we can finally bring the business in, and make
    them responsible for data quality, because up to this point, I think it’s one
    of those elephants in the room we never talk about. Like why is data quality never
    cracked? Why is it never cracked? That’s just hard, and yes, it is, but actually,
    the thing is, we’ve never brought the business in and said, “Right, see this list
    of clients, that data comes from across 15 different places,” and so I love the
    fact that, even in their own 15 systems, it’s perfect.
  prefs: []
  type: TYPE_NORMAL
- en: The problem, when we start to bring data together, is that there are things,
    despite all the governance that we could set up in the world, that take their
    own path, and someone needs to be responsible for that. And IT will still play
    a role in that, definitely in things like, “I can get the data to you reliably,
    I can get it with rollback, we can process it again really fast, we can scale,”
    but at the end of the day, someone from the business comes in and says, “That’s
    wrong and I know it because I work with this data every day.” And being an engineer
    myself, I know how to work with data, but I don’t know how to do those things,
    I have no idea how to just look at a record and go check if it meets all the right
    patterns. But I didn’t know that those were actually the same company, and there’s
    some real impact that happens if you don’t crack that, it could be as simple as
    sending the invoice to the wrong email address, and then you wonder why aren’t
    they paying us, and then you realize that’s not the team that pays it. And you
    go, “Wow, I’m now 30 days overdue,” and realistically, I need to send them another
    invoice and wait another 30 days,and cause maybe a cash flow issue. And you realize
    that’s where it really hits the bottom line, and you need to be exposed to that
    reality to then appreciate the effort of cracking that data quality. AI just in
    a way exacerbates the need for it, because as soon as you use it, it will stand
    out really obviously because of the form factor.
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: I love this part of the discussion. I was thinking about the roles
    and responsibilities of data governance in the company, even if you take a framework
    like DAMA or whatever, with the different roles, this is a new archetype, this
    is like the Ultron (Marvel reference) of the DAMA roles, like you have an Ultron
    (well, maybe not an Ultron, let’s get a Jarvis), a Jarvis working for you, to
    do the things that, let’s be honest, humans are not doing, because it’s a manual
    process, and we don’t have time to do that every time, every day, in a proper
    way.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Seth Juarez: From Generative AI Models to a Full LLM Platform'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**A.G.**: So, what’s your role at Microsoft? What are you doing in the organization?'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aoas_07in06.png)'
  prefs: []
  type: TYPE_IMG
- en: '**S.J.**: So, I work in the Azure AI platform as a program manager. My job
    is to work in incubations and narrative. Those are the two mandates that I have.
    Incubations, meaning we build stuff. For example, if we want to explain how something
    works, we will build prototypes, etc. Wherever those prototypes may differ from
    what our product is doing, we feed back into our product prioritization and what
    it is that we build, just to make sure that the narratives work out, which leads
    us to the second part. We also do technical narrative, which is to explain what
    is going on and how these things work. Again, anytime the ideal story doesn’t
    match with product truth, we feed back into our product group, and sometimes they
    even put us in charge of certain features. We’re like every person, the primary
    goal being incubations, building samples and stuff that help people understand
    how to do this, and then the technical narrative that follows from that.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: You are like Marvel’s The Watcher, seeing everything that’s happening
    at the product level, accelerators, repositories, prototypes, and new functionalities.'
  prefs: []
  type: TYPE_NORMAL
- en: '**S.J.**: That’s right. But it’s not just me. Obviously, there’s a couple of
    us that work on this. For example, you may know [Cassie](https://oreil.ly/33PBM).
    She’s appeared on the [AI Show](https://oreil.ly/h-Fvw) as well, but her and I
    work together on this stuff.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Wonderful. Thank you for the introduction. You have seen the whole
    evolution of Azure OpenAI Service, and the convergence with the rest of Azure
    AI Studio. How do you see something that started as a model, and now is becoming
    a platform, and a very good one with a lot of functionality?'
  prefs: []
  type: TYPE_NORMAL
- en: '**S.J.**: That’s a great question. It turns out that there’s a lot of AI models
    and the Azure AI platform has been in the business of surfacing models, enabling
    you to customize these models and then enabling you to create your own models
    for a number of years, maybe half a decade already. The idea that a new model
    comes in, the reason why it was awesome for us is because we already had the infrastructure
    in place to make these models shine. While the new GPT series of models, and LLMs
    and AI models in general, may seem like a new thing when it comes to infrastructure
    and how these things are actually run, it’s not new to us. We were able to ramp
    up quickly and deliver these models to folks at scale, which is really nice, obviously
    in partnership with OpenAI.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: That was one of the things I was surprised about during these recent
    months, to see this model as a service. Basically, being able to consume the APIs
    so quickly and so easily, that was almost magic for any developer out there.'
  prefs: []
  type: TYPE_NORMAL
- en: '**S.J.**: Yeah, and the cool thing is that the reason why it seemed magical
    is because we’ve been doing this, like I said, for a number of years with our
    Cognitive Services or AI services. We’ve been delivering, for example, text-to-speech,
    speech-to-text, translation, etc. as APIs. And those, if you think about them,
    are basically models as a service as well. They just happen to be more at the
    application layer, but it’s actually the same thing. These models as a service
    are similar. The weights happen to be different, and the model structure is different,
    but the things that are needed to run them are actually quite similar.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Yes, I like this convergence with the AI Studio and the model catalog.
    Just to see all the models available out there, and not only Azure OpenAI. How
    do you see this platform evolving, in which we have different models, a full catalog,
    so easy to deploy, with the APIs out there, we just consume them, we have Llama,
    Mistral, and then we have this ecosystem of prompt flow that I know is another
    part of the platform, all the evaluation, etc. What’s going on? How do you see
    it?'
  prefs: []
  type: TYPE_NORMAL
- en: '**S.J.**: Yeah, that’s a wonderful question. It turns out that we want to commoditize
    the ability for folks to find, consume, and refine models. That’s basically what
    we want to do. So generative AI happens to be one of the most exciting in my opinion.
    For example, if you have an idea and you want to add AI to it, the basic thing
    is go to the model catalog, see if you can find something, look at some of our
    services, and see if you can incorporate it. My particular opinion on this, and
    this is something that I’m coming to realize, is that much like in the early 2000-2005s,
    if you didn’t have a phone app, an iPhone app, people were like…are you a technical
    company?'
  prefs: []
  type: TYPE_NORMAL
- en: My sense is that people will think the same thing when it comes to AI experiences
    inside of your applications, where people will think if you do not include these
    things that normalize human interaction with computers, they’re going to feel
    like your app might be fundamentally broken. We’re coming to a place where we
    will no longer have to adapt to computers, but computers will adapt to us using
    AI, making experiences more natural. So my sense is that we need to start thinking
    about how we can add those niceties and soften the edges around our software,
    and how can AI help with those experiences? My sense is that we’re going to start
    to see these things included wholesale into almost everything we do, to the point
    where someone’s going to come to you in 2025 and be like, wow, your app doesn’t
    have AI? Maybe you should add it, because people will feel like it may be fundamentally
    broken.
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: I love it. This is very related to the discussion we were having
    with Dr. John Maeda on Semantic Kernel, and the influence of design for artificial
    intelligence, and then this reflection about how AI just brings a new kind of
    interface, which is way beyond just the visual interface that we know today. And
    I know that this is just a personal opinion, something that you imagine, but what’s
    your vision of what’s going to happen in the next one or two years?'
  prefs: []
  type: TYPE_NORMAL
- en: '**S.J.**: Two things, and they’re going to seem diametrically opposed, but
    they work together. The first is the expansion and proliferation of the use of
    these models inside of software. You’re going to see these models being used to
    do all sorts of things, to make experiences more delightful, to make experiences
    more user and customer focused. You’re going to see a big expansion of the use
    of these models, and we’re already seeing that today. I mean, it’s only been like
    a year and a couple of months since ChatGPT came out, and it’s actually now everyone’s
    expecting it to be part of the experience. So that’s getting bigger in terms of
    the volume of people using these things.'
  prefs: []
  type: TYPE_NORMAL
- en: But there are also things that are going to get smaller. There’s going to be
    more specialized GenAI models that are going to become smaller and that are going
    to be used in a more targeted way to do specific things. Like the GenAI models
    we have now are quite general and big. However, you can make small language models
    (SLM), much smaller but more targeted. The smaller the model, the more targeted
    the task you need to make it do. You’re going to see the proliferation of small
    language models included perhaps even on devices within the next two to three
    years. You’re going to have those experiences move into a native local experience
    as well as a larger cloud experience together, and together those models are going
    to work to really get to laser focus into what the customer experience is for
    each task that it’s solving, as well as a general way to actually solve other
    language problems. This is for language models, for example. Two diametrically
    opposed things, things are going to get bigger in terms of volume of people using,
    and things are going to get smaller in terms of the models that people use, and
    those two things are going to be used in combination, I think quite effectively.
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: I believe so. For the second point that you mentioned, this kind
    of multilayer or multimodal architecture on which we could dispatch our first
    model, depending on the topic that it identifies. We can even fine-tune that first
    model and then use GPT-4 for one purpose and another model for another purpose.
    How do you see it?'
  prefs: []
  type: TYPE_NORMAL
- en: '**S.J.**: I think that’s a great way of putting it. We’re going to get into
    this multimodal, which means multiple models, as well as multimodal, which means
    multiple modalities like maybe speech, vision, and text, for example.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: What about combining models of different providers?'
  prefs: []
  type: TYPE_NORMAL
- en: '**S.J.**: Yeah, I think that’s great. In Azure, on Azure AI Studio, we really
    don’t care what models you bring, and we are trying to forge partnerships with
    multiple folks. We’ve [announced a partnership with Meta last year](https://oreil.ly/ehdf3),
    and some of the Llama 2 models are already in there. We recently [announced another
    partnership with Mistral](https://oreil.ly/SU0PT), and we have Mistral Large directly
    in our model catalog, and some of these models you can even fine-tune.'
  prefs: []
  type: TYPE_NORMAL
- en: But the reality is that the Azure AI platform is built on top of something we
    call Azure Machine Learning Studio, a general-purpose machine learning, MLOps
    platform for you to build any model that you like. In theory, you could start
    from something like PyTorch or TensorFlow, build your own model or do any code,
    and you could train those models directly in Azure Machine Learning, and surface
    them in AI Studio. The reality is, we really don’t care what AI models you bring,
    whether they’re stuff that we’ve forged through partnerships or things that you
    literally make yourselves, all of those things should and will be available to
    you in your applications.
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Even that curation of models from Hugging Face, we have basically
    a variety of good models out there.'
  prefs: []
  type: TYPE_NORMAL
- en: '**S.J.**: Yeah, Hugging Face is an amazing partner. They’ve done a really good
    job of surfacing tons of functionalities, and we hope that with those functionalities,
    you will be able to deploy and use them reliably in your applications.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Yes, and you mentioned machine learning operations (MLOps), now LLM
    operations, and we know that this is a new area. We’ve been discussing responsible
    AI. But if we go to the core aspect of measuring the performance of the models,
    how do you see this? Because this is evolving and getting more and more complex
    but also more intuitive. Because one year ago, it was not easy to know how to
    measure the performance of an LLM. I think that is getting clear now. Where are
    we going with that part?'
  prefs: []
  type: TYPE_NORMAL
- en: '**S.J.**: Great question. We’ll start first with DevOps. DevOps is not an old
    concept, but it’s one that’s pretty well known. DevOps is a union of people, processes,
    and products to enable the continuous delivery of value. MLOps is the same thing.
    The union of people, processes, and products enable a continuous delivery of value,
    but with machine learning, LLMOps is the same thing but with LLMs. The idea of
    unifying this three-legged stool, people, process, and products, is super important.
    Because the thing about DevOps, LLMOps, and MLOps is that a product is not going
    to solve your process problem, and it’s not going to solve a people problem. If
    people don’t buy into the process, then no tool is going to satisfy that need.'
  prefs: []
  type: TYPE_NORMAL
- en: I think that’s the first thing to bring out is that there is no magic bullet
    or elixir or product that’s going to solve a process thing, and the fact that
    people buy into the process. That’s the first thing. The second thing is, once
    you do want to buy into a process, people want to buy into a process, the question
    is what do we do in LLMOps to make this process reliable and useful, when it comes
    to how we evaluate the prompts that we do, and how to evaluate or how to make
    sure that the thing is working in production. In Azure AI Studio, we have a number
    of ways to do that.
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s two kinds of evaluations that I think exist (but obviously, this is
    an evolving space, and so we’re learning a ton here too). But the two kinds are
    unsupervised checks and then supervised checks. Supervised checks are probably
    the easiest. Imagine you have a call to an LLM and you want to make sure that
    it gets a certain answer out that’s similar to what you have. Basically, you would
    need to have a dataset of inputs and the expected outputs. To me, that’s a supervised
    test because you have the answer that you want. There’s a number of metrics that
    you can do. Here’s a simple one: for example, you can take the answer that’s ground
    truth, project that into, for example, an Ada embedding or any other embedding,
    and take the answer that the LLM gives you, project that into an LLM embedding
    and then measure the angle. Maybe there’s a particular tolerance that you have
    that gives a semantic closeness or meaning. That’s one way of checking.'
  prefs: []
  type: TYPE_NORMAL
- en: You can also use what we like to call GPT star metrics where you have the ground
    truth, and once you get the ground truth, you give that to a GPT star metric like
    similarity, and you ask the LLM as a language problem, how similar are these things
    on a scale of one to five? You give it some few-shot learning. That’s an example
    of two supervised learning methodologies, one that’s more empirical and one that’s
    more stochastic because it’s using the actual LLM to do it. The other one is also
    stochastic because it’s using embeddings, but you have a way of just projecting
    these things and then measuring an actual metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then there’s these other ones that I like to call unsupervised that look at
    the structure of the actual thing that’s going into the prompt and the context,
    and with the answer it measures what the structural integrity of the entire thing
    is. Let me give you one example: groundedness, which is a measure of how grounded
    the answer is in the context that was given to the LLM. For example, and this
    is the canonical example we use, we have a “Contoso” outdoor store and you ask
    a question about tents. Notice you have the question and then whatever the answer
    is, but the internal structure of the prompt flow or LangChain or whatever you’re
    using fetches some information from a data source, which we know to be true. That
    to me is the context, and that generally gets embedded directly into the prompt
    in something we call retrieval-augmented generation (RAG). You get the question,
    you fetch some data, you put it into the prompt. With this particular call to
    the LLM, you have a question, you have the answer, and then you have the context
    that was fetched. With those three things, we’re going to measure something called
    groundedness, which is how grounded the answer is that we fetched in the context,
    the answer that it generated from the context that we fetched. This is a normal
    interaction. You might ask me a question and I’ll start answering with facts and
    good faith, and you might say, “No, that’s not what I meant” and that’s totally
    cool. We want the LLM to be grounded in that way.'
  prefs: []
  type: TYPE_NORMAL
- en: There’s a measurement on a scale of one to five of how grounded the answer is
    in the context we fetched and the question that was given, which is super nice.
    It turns out that this measurement is another GPT star metric where we give the
    question/context/answer, and then we say, “On a scale of one to five, how grounded
    is the answer in the context?” and then we do some few-shot learning in the actual
    prompt. This is something that you can also control. For example, you can change
    the entire groundedness prompt to directly match your few-shot learning priorities.
    Let’s just say you’re an outdoor company, you put those things in there and it’s
    able to do that. Those are the two kinds of evaluations that I see coming out.
    A supervised one where you have ground truth and you measure ground truth against
    the answers, and then unsupervised evaluations where you’re measuring the internal
    consistency of the truth versus the answers that you provide.
  prefs: []
  type: TYPE_NORMAL
- en: There are other ones like fluency, coherence, relevance, and those are all GPT
    star unsupervised metrics, and you can even invent your own. There’s some clever
    ones that folks have invented like the apology metric, which is how many times
    it apologizes, and do we want to minimize that. But you’ve entered basically a
    world where you can evaluate these things in a way that is tailored to your business
    needs, your voice, and maybe even your ground truth, if that makes sense.
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: That’s why the platform evolves so much along with the evaluation
    flows that we have on the platform, and what we can do with these metrics. By
    the way, you have explained this in a better way than the book. This is why I
    wanted to interview you, because I know that you are so good at explaining these
    terms! This is amazing.'
  prefs: []
  type: TYPE_NORMAL
- en: '**S.J.**: Yeah. It’s like I said, I remember talking to some nontechnical folks,
    business folks, and they were concerned about using these LLMs because they’re
    like, well, how do we ensure that they’re doing the right thing? I showed them
    and they’re like, oh, so I can use English or the language of my choice, because
    they are trained in multiple languages, to actually evaluate these models. And
    I said, absolutely, and that’s not all. For those unsupervised tests, notice that
    you do not need ground truth. As you deploy in Azure AI Studio, you have something
    called a model data collector, which enables you to capture the input/context/answer
    and store it in your storage (we don’t see any of this stuff, we take this very
    seriously). Then even in production, you’re able to create jobs that look at this
    data and measure those same metrics, and even alerts you when those things go
    out of whack.'
  prefs: []
  type: TYPE_NORMAL
- en: Now we’re getting into the actual LLMOps, which enables the continuous delivery
    of value. That’s the thing. If the value goes down, you need to be alerted and
    to fix it, and then have that go back into the process. With these unsupervised
    evaluations and metrics, you’re able to actually run them on a subset of your
    production data if you so choose, which makes this even better for folks who are
    concerned about these things staying on track in inference time or production.
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: That’s amazing. I just want to see the roadmap of what’s coming next
    in this aspect, because there is so much discussion on LLMOps, but it’s such an
    initial discussion for obvious reasons. This is a pretty new area. If you had
    to recommend one resource besides the AI Show, and the documentation, and all
    the official resources from Microsoft, do you have something in mind that you
    would recommend to learners and people who are listening to the discussion, that
    you consider good for their upskilling journey?'
  prefs: []
  type: TYPE_NORMAL
- en: '**S.J.**: Yes. I will say this, and this is going to be counterintuitive again
    because I like being counterintuitive. You are your own ultimate resource. My
    sense is that there is no amount of reading, or looking, or thinking about this
    stuff that really beats getting in there and trying something. That’s what I would
    suggest you do. Try something, make a prompt, have the answer come out and see
    what happens. In my mental model, maybe this will help: you should not think of
    these “LLM things” as repositories of knowledge. They’re not databases that have
    information. They are basically language calculators or language synthesizers.
    Think of your prompt as the language arithmetic that you’re putting into the LLM
    and think of the response as the answer.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, this paragraph plus this paragraph minus this paragraph, what does
    that look like? I use LLMs like this all the time, even this morning. I wrote
    down a jumble of thoughts that I wanted to make so that it was smoothed out and
    GPT-4 was super nice and said, yeah, you should write it like this. It wasn’t
    perfect, but it enabled me to start with garbage, get something more refined,
    and now I could become an editor. Editing is much easier than creating. Think
    of LLMs as language calculators and start using them to solve tasks. Once you
    do that as your own resource, you’re going to get super far. These things are
    not hard to get started with, you just see the endpoint and you need to just put
    in a prompt and have something come out.
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Yeah, there are plenty of examples of notebooks where people can
    play with the API and test and see how it reacts. This notion of a calculator,
    I love it. It’s aligned with the notion of a copilot for a person that will be
    interacting with the system by using the linguistic ability of the models. It’s
    amazing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Saurabh Tiwary: The New Microsoft Copilot Era'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**A.G.**: For those who don’t know, could you please explain who you are and
    what your role at Microsoft is with which unit?'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aoas_07in07.png)'
  prefs: []
  type: TYPE_IMG
- en: '**S.T.**: I lead a team called Turing, and the team has been training LLMs
    and applying them. It’s an applied team, so it uses these models in a variety
    of products from the Edge browser to Bing question answering, or if you are getting
    emails in Outlook, you will see those text predictions as you type, you will see
    the sentences complete, so a lot of features like that. More recently, my team
    has been driving the Copilot experience across many of the Copilots that Microsoft
    has announced. Most of the heavily used ones like the Windows Copilot, Edge Copilot,
    Bing, in a similar way, even on the enterprise side. The backend of all of those
    Copilots is the same with an extensibility model, and so my team is building that.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: It’s amazing. Legend has it that you created the Turing team, along
    with that first set of GPUs with which you were preparing the first models. That
    was some time ago.'
  prefs: []
  type: TYPE_NORMAL
- en: '**S.T.**: Yeah, it was quite some time back, like maybe eight or nine years
    back, at least on the product side. Obviously, Microsoft Research has been pushing
    the state of the art for a very long period of time. But on the product side,
    I kind of bought the first GPUs, then built the first clusters, and ran the software
    layer on top of it, so that you can do some large-scale (which is not large-scale
    by any standards now), but some large-scale training on that small cluster, then
    evolve that to a bunch of GPU running in Azure, and now there we are, where even
    the inference GPUs are massive.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: You mentioned Microsoft Bing Chat, and Microsoft Copilot now. How
    was that journey? Because I think that after GitHub Copilot, this has been the
    best exponent of experimentation and learnings; it has even been shared by Jordi
    Ribas and the team on the blog, and it was amazing just to see what you were doing.
    How was that?'
  prefs: []
  type: TYPE_NORMAL
- en: '**S.T.:** This is a phenomenal experience. Obviously, the team has been pushing
    very hard in terms of adding features, improving the experiences, etc. Let me
    maybe share a little bit of the backstory behind this. As I was mentioning, we
    were training our own LLMs in the past, and we had this conviction that conversational
    AI will be the next step in the journey. Even before ChatGPT or GPT-4 came out,
    we had our own conversational experience like a chat experience, which we were
    running in a stealth mode in India and MSIB countries (Malaysia, Singapore, Indonesia,
    and Philippines). We were working on it for a few years, I would say a couple
    of years before ChatGPT came out. We were iterating safety mechanisms, like not
    touching controversial topics, how do you address jailbreaks, etc. A lot of these
    things we were already experimenting with at a much smaller scale, and the surface
    area and the interaction mechanism was also a little bit different.'
  prefs: []
  type: TYPE_NORMAL
- en: But, even within that experiment, we were finding that there was a lot of user
    engagement, in the sense that I remember one of the longer conversations was running
    for 13 or 14 hours. The user was talking to the bot for 13 to 14 hours straight
    with, I don’t know, maybe 1,800 messages back and forth going between the user
    and the bot, etc. That actually, that initial experiment gave us some baseline
    footing, so that when we got access to GPT-4, we had our paths somewhat mapped
    out. Hence, within a fairly short period of time, I think we got access to GPT-4
    somewhere around August or September of 2022, and then we released it within four
    or five months on February 7, 2023\. We released what was called the Bing Chat
    experience, which is right now the Microsoft Copilot experience.
  prefs: []
  type: TYPE_NORMAL
- en: It has been a fantastic journey, lots of late nights. Actually, the team worked
    through Thanksgiving and holidays and stuff like that, but it was a very exciting
    journey and seeing it populate across all the different surfaces that Microsoft
    has, whether it be Word, PowerPoint, M365, Edge, Bing as well as across our app
    family, the mobile app family, it has been just amazing. The partnerships have
    been great. The company has been working like one to propagate this belief or
    mission about Copilot across all these surfaces. If you take a step back, being
    able to do something like this is just phenomenal.
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: It’s true. From the field, I see it like this belief that there is
    a notion of Copilot that is everywhere, aligned with all the products. Even this
    end-to-end architecture of what a Copilot is, that is certainly new, but it’s
    something exciting. I feel like one of the most incredible things was to see the
    day-to-day, night-to-night, week-to-week progress of Bing Chat, Microsoft Copilot,
    all the learnings that you were sharing with the industry, even with the competitors
    on the blog, I was like, “This is gold!” All the improvements, I could really
    feel the improvements on the product. That pace of innovation is something that
    I think is difficult to replicate.'
  prefs: []
  type: TYPE_NORMAL
- en: '**S.T.**:I will say kudos to the team members who have been working super hard,
    and across Microsoft, working towards this common goal, and providing a delightful
    as well as a useful experience. It’s not just conversations. At the end of the
    day, we have our mission statement of making every person and organization in
    the world more productive. Along that particular goal, we don’t just want just
    a chit-chat experience. We also want people to accomplish things, do things. Within
    that mission, how do we connect all of the Copilots, add features like these plug-ins
    and GPTs that we have added? Actually, some of the things which might be coming
    up soon are even going towards task completion, etc. We are trying to evolve the
    product in a very significant way.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Yes, and the notion of the end-to-end Copilot. People ask, what’s
    Copilot exactly? Then there’s this and the other Copilot, all the Copilots on
    the products. But this notion of end-to-end Copilot that goes way beyond the model…it’s
    not just the model, it is the orchestration, it is the combination with Copilot
    for 365, etc. All this architecture, how do you design something that is so massive
    and so interesting from a combination of capabilities point of view?'
  prefs: []
  type: TYPE_NORMAL
- en: '**S.T.**: Yeah. The design principle of Copilot is that it is in the true sense
    a Copilot, as there is in aircraft. That if you are interacting with any piece
    of software from Microsoft, whether it be Teams or Outlook or wherever you are,
    the Copilot will be there next to you to help you. Obviously, it won’t be a static
    experience depending upon which interface you are in, so if you are at the operating
    system level, at Windows level, you may want to do system-level commands, like
    for example, turn on focus mode or change my Bluetooth settings or open an app.
    If you are in Outlook, you may want to summarize your email while having general-purpose
    conversations as well.'
  prefs: []
  type: TYPE_NORMAL
- en: The way we have architected it is so that irrespective of whatever surface you
    are using the Copilot in, you get a slightly different experience, which is conditioned
    on what surface area you are in. That’s where we are pushing this default idea
    of a Copilot, that it will be there for you to help you do your work or do your
    task or whatever you’re planning to do, with a much better experience than if
    you were to do it alone.
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Yes, and one of the original tasks, I mean, the main one is search.
    How did you and the team reinvent this notion of internet search by combining
    LLMs with traditional search? How was that, getting to that idea? I feel like
    it’s mind-blowing.'
  prefs: []
  type: TYPE_NORMAL
- en: '**S.T.**: Yes, I have been working in search for quite some time. One of the
    things that we were thinking was that, if you look at us as a user, either it’s
    me or you or anyone else. Why do we want to search? Search is actually a simplification
    of what the technology can offer to us and users or us as humanity have gotten
    used to using it in a particular way, which is that, for example, you will see
    in search, if you look into search logs, you will see a lot of queries which are
    very like two words, three words each. You will see something like “best elementary
    school,” for example. Now, it’s very abstract. The reason why people search, the
    uber problem that the person may want to solve with a search like “best elementary
    school’” is something like…I have a kid who has been going to this prep school
    and is looking for something that is nearby and good quality, or maybe the user
    is planning to buy a house, and they’re wondering whether they should buy the
    house in this particular neighborhood or somewhere else and so on. There might
    be a lot of deeper intent, which the user does not express in search engines,
    because they have learned from past experience.'
  prefs: []
  type: TYPE_NORMAL
- en: People who have been in this business for a long time may remember [Ask Jeeves](https://oreil.ly/yuhNk),
    which existed around 20 to 25 years back. With Ask Jeeves, you were told to just
    express yourself in long sentences and it will find the information. At that time
    the technology was not great, and most of the time when you ask questions like,
    “I’m trying to buy a house, can you share what will be the good elementary school
    in this particular area?” and so on, it wouldn’t find the information, which is
    why search engines trained humans to type in very specific keywords. Even when
    you type those keywords, if you look into a user session or how we engage, what
    happens is we try to click on a search result, we read some content, then we click
    on something else, then we modify the queries, and for this example, we may find
    that there are public school and private school options. We might decide we should
    look into those depending upon whether they’re affordable or not, and then you
    iterate and so on. Given that the large language models have become a lot more
    powerful, we can try to compress this complex effort, which as humans we have
    broken down into these very specific sets. Let’s issue this first query, look
    at results, then modify it, then ask the other thing, and then the other thing.
    At the end of the day, you may open a map and then say, where is that region and
    where is the house, and what’s the distance, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of that, with LLMs and with this Copilot experience, you can type what
    you really want. You can say, “I’m new to this area, I’m looking into buying a
    house, I have two young kids, where should I find good schools in this region?”
    etc. You can express all of that. Then the model is actually, as a Copilot, breaking
    that, because they know they have access to the search engine, the model itself.
    It will now break your complex scenario into smaller subcomponents, issue search
    queries, look at results, follow up again, etc., and then provide that comprehensive
    view. Instead of you doing all of these tasks, the model is helping you do that.
    That was our initial thing, because we have seen our users struggle in a session,
    trying lots of different things, iterating, making changes, etc. Anyways, that’s
    the story behind this improved search experience with the Copilot.
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: I guess you had started with the models, and then there was the orchestration
    part, now we talk about LlamaIndex, LangChain, Semantic Kernel, but [Prometheus](https://oreil.ly/Vkgtt)
    was there. That concept of orchestrating knowledge and combining pieces, skills,
    etc.'
  prefs: []
  type: TYPE_NORMAL
- en: '**S.T.**: Yes, one of the other things for people who have been playing, or
    trying to build bots using LLMs, with one of these options like LangChain and
    stuff like that, one of the challenges that initially doesn’t show up, but it
    happens when you start doing more complex things, is that when you have a database,
    or an interface through which you can pull in information, it’s very easy to write
    a prompt, and you can add things over there. In Microsoft’s case, there are many,
    many different complex things which the system can access. For example, even for
    Bing, there is a search index which is there, and that is one interface. But we
    also have our Ads engine, we have introduced image creation as a capability, we
    have GPT4-V for image understanding. There are many, we have actually introduced
    this notion of plug-ins if you want real-time flight information from KAYAK, being
    able to access that.'
  prefs: []
  type: TYPE_NORMAL
- en: Once you start adding many of these pieces, what happens is if you start using
    the raw prompt-based method, the prompt will become just ginormous, and even the
    model quality will suffer because it now has to follow instructions, a very long
    instruction set, and just like humans where if you give too many instructions,
    you will be confused at the end of the page about what was written at the top.
    You see similar behaviors over here as well, that the model quality may not follow
    everything to the right intent of what is written. Hence, we had to build a more
    sophisticated orchestration engine which can do state-based prompting, it can
    do dynamic prompting so that the prompt which is sent to the model is a lot smaller.
    There’s a lot of sophistication which has gone inside it so that we can provide
    a really compelling experience for our users.
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Totally. There’s the model, the orchestration, and the third leg
    of this chair, I think, is the user interface. How did you experiment with that
    experience, how to adapt, because there is a learning process for users from traditional
    keyword search to something where we will be interacting and then understanding
    the results? How was that, all that experimentation?'
  prefs: []
  type: TYPE_NORMAL
- en: '**S.T.**: Yeah, right now, there are some standard interfaces which a lot of
    companies who are entering this area have settled on. But since we were almost
    first to market, we had to design and iterate on this a lot. From day zero when
    we released the product to where it is now, it has gone through a ton of iterative
    improvements. One of the first things that we did was we introduced the conversational
    interface on top of our search engine. So the question was, well, most of the
    users don’t even know this product exists. So how would they engage? Because maybe
    you and I are more connected to technology, and maybe we have read the latest
    news, but how does a common user engage with this kind of technology? How do they
    even know this is there? One of the very subtle things that we did was on Bing.com,
    you would have icons to go to Copilot, like you can click and go on. But if you
    do a mouse scroll, you can switch seamlessly between conversations and search
    results. So that provides a very natural way in which I type this query and most
    of the time, the user will be typing the query for the search results. They get
    the search results, but let me just do a mouse scroll, just a couple of clicks.
    You land into the conversational interface and you can have that conversation.'
  prefs: []
  type: TYPE_NORMAL
- en: There were other very subtle things that we did, for example, for question answering
    or weather, sports, etc. We had these follow-up queries, which we show on the
    Bing search page itself. When you click on that, you land into the chat interface.
    This is how we were trying to educate, instead of having a tutorial or a notification
    bar that now you can do this, we were having these subtle ways in which, now if
    you click on this, you see you go into a conversation interface, the Copilot will
    respond to your follow-on question and then the user knows that, OK, there is
    something else, something smarter, richer, which is behind the scenes, and then
    they can engage further with that.
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Yeah, totally product-led. So people will just get into a product
    and they will learn in an organic way how to handle this new functionality, very
    smart. So, because this book is about Azure OpenAI, then this transition, this
    evolution towards the Microsoft Copilot notion, this new era of AI that we are
    living today…what’s your vision of what’s next and the evolution in the industry
    or even the upcoming research topics that you think may be interesting to take
    a look at?'
  prefs: []
  type: TYPE_NORMAL
- en: '**S.T.**: Yeah, I mean, that’s a very important and challenging question. A
    lot of people have argued that the last year has been transformational and a lot
    of stuff has been happening. But given the trend lines that we are seeing, I think,
    and it is hard to believe, but I think the rate of change will actually accelerate
    and not slow down, that’s our belief. I know we have released Copilot across many
    surfaces, but the way we see the Copilot today, I believe it is going to evolve
    from these conversational text-based interfaces to a much richer experience very,
    very quickly. Yes, typing is something and there are certain places, so for example,
    messaging apps, etc., have educated us that you can type and you can get back
    and forth conversations, etc. We are following that mode. But models are becoming
    more powerful, think about true multimodality, just like we have as humans.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, when I’m talking to you, I’m seeing this Microsoft Teams window,
    I’m seeing your face, I also see your name written next to it. As a human, I’m
    not going into a text mode or an image mode like, I’m just looking at the face
    and now I’m going to read the text, etc. Everything is very seamless for me when
    I’m looking at your screen. In a similar way, even for the Copilot, we will start
    seeing things where the engagement will be very natural. People could speak and
    the model could produce an image. They could actually put in an image with some
    text and it may respond back, etc. Very similar, it’s not going through pipelines
    of like, this model has got called and then text to speech, and so on. So that
    is one axis which will be fairly powerful that the models may start operating
    at the pixel level. So instead of a text model or image model, it just looks at
    pixels and some of those pixels end up being text and some of them end up being
    images and that’s OK.
  prefs: []
  type: TYPE_NORMAL
- en: Another thing that I feel is where the world is heading is towards this behavior
    of agents. Meaning, right now, I was giving the search example earlier that you
    had a very complex task, and as a user we were interacting with the search engine
    by breaking down that problem into very small pieces, issuing those queries, looking
    at results, and we were the orchestrator in our mind, doing this complex thing
    even though we never explicitly said that we are the orchestrator. I think the
    same is true even with our Copilot today, they are limited to a certain extent
    because they are a lot about information gathering. We are trying to do some of
    the task completions with the KAYAK reference that I was adding.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, what you want to do is, one of the popular examples in our Copilot
    is, “Can you plan an itinerary for me?” If I’m going on vacation, do whatever,
    pick your favorite place. London, for example. It will search, pick up various
    places, etc. It can create an itinerary, and it can also recommend hotels and
    so on. But ultimately, what you want to do is, you want to just plan the vacation.
    Because right now, for example, my wife hands that task to me, and it’s super
    painful. I need to go read all the reviews, make sure that the hotels are good,
    nearby to the places we want to go, etc. I have to do a lot of these things, then
    go to the website, search for the hotel, do the booking, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: I would say it is not a simple process. It takes quite a bit of cognitive load
    as well as time and energy. But if you think about it, a lot of these capabilities
    are there on the web. They are designed for humans because we can open a browser,
    we can click on results, we can book stuff, and so on. I think where these models
    are heading is towards having this agentic behavior or an agent-like behavior
    where you just tell them, obviously with some user interface, etc. Obviously,
    you can’t run it in a fully autonomous way. “I want to go to London for X number
    of days. Can you plan that trip?” But plan here means really planning that trip
    and not just a text interface. Where it goes and it starts doing booking, looking
    for flights, and all of that. Checking weather, that’s another thing, whether
    the weather is good or not. If it is not, if it is going to rain on a particular
    day, maybe you want to plan some different activities on those days, etc. Being
    able to do that where the Copilot on users’ behalf can start doing those types
    of things like open web pages, clicking on things, and these are all arbitrary
    interfaces. I feel like that’s where the world is heading towards.
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Yes, I think so. It’s not even orchestration. It’s a multilayer architecture
    in which you are going and performing all these actions, and instead of being
    the agent ourselves and booking all the vacations, the hotel, and the trip, etc.,
    we have a system that is relying on the existing interfaces that we have today,
    the frontends and the backends.'
  prefs: []
  type: TYPE_NORMAL
- en: '**S.T.**: Think of it as your own personal assistant. If you had a personal
    assistant who really knew you well, obviously, it’s just like if you were to hire
    a new personal assistant for yourself. Initially, it will be like, OK, just do
    simple things, etc. But at some point when the trust grows, and you know that
    person can do these things and they understand my interests and boundaries and
    so on, they can start acting on your behalf much, much more. I think that’s where
    I feel like these models will go. On day zero, obviously, they won’t do anything
    because people will be freaked out like, why did you make this particular choice
    or that particular choice? But as things evolve, I think we are going toward that
    future.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: I think so, it’s very promising what we’ll see in the next one, two,
    three years. That’s why I don’t ask for your vision for the next five years because
    I know that it’s impossible to do so. Look, this is super interesting, wonderful
    insights. I’m just so happy that you are sharing all this information here. Did
    you have a last recommendation for our readers to continue exploring?'
  prefs: []
  type: TYPE_NORMAL
- en: '**S.T.**: I think this field is moving so fast that, personally for me because
    I have been in this area for the last 9 to 10 years, I think a lot of the social
    sites generally are useful, Twitter, if you follow the right set of people and
    so on. Right now, actually, things have evolved a lot. There are newsletters that
    are there, which actually provide the latest, and are actually using LLMs to simplify
    and summarize a lot of the conversation so that you can have a synthesized set
    of information and obviously you can drill down a little bit more if something
    is of interest. I think those are probably the more interesting routes.'
  prefs: []
  type: TYPE_NORMAL
- en: I can give one other example. Generally, conferences were used for knowledge
    dissemination. Right now, if you look into the deep learning space, conferences
    are mostly for networking. The papers were already put on [arXiv](https://oreil.ly/O9tW3)
    probably three months to six months back. If they were useful, they would have
    already been discussed till death by the time the conference happens. Even the
    research community has shaped itself. Then I will say the same is true for us
    as well. I would say just keep being on the cutting edge and the latest edge.
    I think things are evolving very quickly in this world.
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: So quick. Even this Azure OpenAI book has evolved a lot, I don’t
    know how many iterations every week, just to keep updating and adding all the
    information.'
  prefs: []
  type: TYPE_NORMAL
- en: '**S.T.**: Maybe even after publishing, you may have to iterate on, I don’t
    know, like a book pointed to an appendix or something.'
  prefs: []
  type: TYPE_NORMAL
- en: '**A.G.**: Yeah, appendix, second, third edition…almost every month.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The experts interviewed in this chapter offer exclusive insights related to
    the current state of generative AI and LLMs (e.g., why data quality matters for
    RAG patterns, new LLMOps trends for model performance tracking, advanced use cases,
    the underlying cloud native infrastructure) but also insights into its future.
    There are two recurrent topics here: multimodality for models to analyze different
    kinds of information, and the evolution of AI agents, as a sort of combined automation
    of steps leading to complete complex tasks—all of this without forgetting the
    responsible and safe implementation of generative AI systems.'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, this last chapter was my way to bring together all the topics from
    Chapters [1](ch01.html#introduction_to_generative_ai_and_azure_openai_ser) to
    [6](ch06.html#elaborating_generative_ai_business_cases), and to discuss them in
    a highly applied manner with an amazing set of professionals. From technology
    building blocks and architectures to organizational considerations. From Azure
    OpenAI Service to the rest of the related “pieces” that enable your cloud native
    generative AI developments.
  prefs: []
  type: TYPE_NORMAL
- en: And here we are, at the end of this book’s journey. More than 200 pages full
    of explanations, examples, and technical topics related to generative AI and Azure
    OpenAI. But this is, of course, just the beginning. Companies are adopting Azure
    OpenAI, and the Microsoft product teams are working to continue evolving the platform,
    by adding not only new AI models, but also product features related to the operationalization
    of enterprise-level deployments. This is an amazing race, and the generative era
    has just started. This book is a small contribution for AI adopters around the
    world to get the most from Azure OpenAI Service. Let’s keep innovating.
  prefs: []
  type: TYPE_NORMAL
