- en: ConvNet architecture patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://deeplearningwithpython.io/chapters/chapter09_convnet-architecture-patterns](https://deeplearningwithpython.io/chapters/chapter09_convnet-architecture-patterns)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'A model’s “architecture” is the sum of the choices that went into creating
    it: which layers to use, how to configure them, in what arrangement to connect
    them. These choices define the *hypothesis space* of your model: the space of
    possible functions that gradient descent can search over, parameterized by the
    model’s weights. Like feature engineering, a good hypothesis space encodes *prior
    knowledge* that you have about the problem at hand and its solution. For instance,
    using convolution layers means that you know in advance that the relevant patterns
    present in your input images are translation-invariant. To effectively learn from
    data, you need to make assumptions about what you’re looking for.'
  prefs: []
  type: TYPE_NORMAL
- en: Model architecture is often the difference between success and failure. If you
    make inappropriate architecture choices, your model may be stuck with suboptimal
    metrics, and no amount of training data will save it. Inversely, a good model
    architecture will accelerate learning and will enable your model to make efficient
    use of the training data available, reducing the need for large datasets. A good
    model architecture is one that *reduces the size of the search space* or otherwise
    *makes it easier to converge to a good point of the search space*. Just like feature
    engineering and data curation, model architecture is all about *making the problem
    simpler* for gradient descent to solve — and remember that gradient descent is
    a pretty stupid search process, so it needs all the help it can get.
  prefs: []
  type: TYPE_NORMAL
- en: 'Model architecture is more an art than a science. Experienced machine learning
    engineers are able to intuitively cobble together high-performing models on their
    first try, while beginners often struggle to create a model that trains at all.
    The keyword here is *intuitively*: no one can give you a clear explanation of
    what works and what doesn’t. Experts rely on pattern matching, an ability that
    they acquire through extensive practical experience. You’ll develop your own intuition
    throughout this book. However, it’s not *all* about intuition either — there isn’t
    much in the way of actual science, but like in any engineering discipline, there
    are best practices.'
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we’ll review a few essential ConvNet architecture
    best practices, in particular, *residual connections*, *batch normalization*,
    and *separable convolution*. Once you master how to use them, you will be able
    to build highly effective image models. We will demonstrate how to apply them
    on our dogs-versus-cats classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start from the bird’s-eye view: the modularity-hierarchy-reuse (MHR)
    formula for system architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: Modularity, hierarchy, and reuse
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you want to make a complex system simpler, there’s a universal recipe you
    can apply: just structure your amorphous soup of complexity into *modules*, organize
    the modules into a *hierarchy*, and start *reusing* the same modules in multiple
    places as appropriate (“reuse” is another word for *abstraction*). That’s the
    modularity-hierarchy-reuse (MHR) formula (see figure 9.1), and it underlies system
    architecture across pretty much every domain where the term *architecture* is
    used. It’s at the heart of the organization of any system of meaningful complexity,
    whether it’s a cathedral, your own body, the US Navy, or the Keras codebase.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b7809a140aeaa20dd22850588281fe8c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 9.1](#figure-9-1): Complex systems follow a hierarchical structure
    and are organized into distinct modules, which are reused multiple times (such
    as your 4 limbs, which are all variants of the same blueprint, or your 20 fingers).'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re a software engineer, you’re already keenly familiar with these principles:
    an effective codebase is one that is modular, hierarchical, and where you don’t
    reimplement the same thing twice but instead rely on reusable classes and functions.
    If you factor your code by following these principles, you could say you’re doing
    “software architecture.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep learning itself is simply the application of this recipe to continuous
    optimization via gradient descent: you take a classic optimization technique (gradient
    descent over a continuous function space), and you structure the search space
    into modules (layers), organized into a deep hierarchy (often just a stack, the
    simplest kind of hierarchy), where you reuse whatever you can (for instance, convolutions
    are all about reusing the same information in different spatial locations).'
  prefs: []
  type: TYPE_NORMAL
- en: Likewise, deep learning model architecture is primarily about making clever
    use of modularity, hierarchy, and reuse. You’ll notice that all popular ConvNet
    architectures are not only structured into layers, they’re structured into repeated
    groups of layers (called *blocks* or *modules*). For instance, Xception architecture
    (used in the previous chapter) is structured into repeated `SeparableConv` - `SeparableConv`
    - `MaxPooling` blocks (see figure 9.2).
  prefs: []
  type: TYPE_NORMAL
- en: 'Further, most ConvNets often feature pyramid-like structures (*feature hierarchies*).
    Recall, for example, the progression in the number of convolution filters we used
    in the first ConvNet we built in the previous chapter: 32, 64, 128. The number
    of filters grows with layer depth, while the size of the feature maps shrinks
    accordingly. You’ll notice the same pattern in the blocks of the Xception model
    (see figure 9.2).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ad5cf0efed908d8fa63d5e0a20a13cb5.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 9.2](#figure-9-2): The “entry flow” of the Xception architecture: note
    the repeated layer blocks and the gradually shrinking and deepening feature maps,
    going from 299 x 299 x 3 to 19 x 19 x 728.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deeper hierarchies are intrinsically good because they encourage feature reuse
    and, therefore, abstraction. In general, a deep stack of narrow layers performs
    better than a shallow stack of large layers. However, there’s a limit to how deep
    you can stack layers: the problem of *vanishing gradients*. This leads us to our
    first essential model architecture pattern: residual connections.'
  prefs: []
  type: TYPE_NORMAL
- en: Residual connections
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You probably know about the game of *telephone*, also called *Chinese whispers*
    in the UK and *téléphone arabe* in France, where an initial message is whispered
    in the ear of a player, who then whispers it in the ear of the next player, and
    so on. The final message ends up bearing little resemblance to its original version.
    It’s a fun metaphor for the cumulative errors that occur in sequential transmission
    over a noisy channel.
  prefs: []
  type: TYPE_NORMAL
- en: 'As it happens, backpropagation in a sequential deep learning model is pretty
    similar to the game of telephone. You’ve got a chain of functions, like this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '`y = f4(f3(f2(f1(x))))`'
  prefs: []
  type: TYPE_NORMAL
- en: The name of the game is to adjust the parameters of each function in the chain
    based on the error recorded on the output of `f4` (the loss of the model). To
    adjust `f1`, you’ll need to percolate error information through `f2`, `f3`, and
    `f4`. However, each successive function in the chain introduces some amount of
    noise in the process. If your function chain is too deep, this noise starts overwhelming
    gradient information, and backpropagation stops working. Your model won’t train
    at all. This is called the *vanishing gradients* problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The fix is simple: just force each function in the chain to be nondestructive
    — to retain a noiseless version of the information contained in the previous input.
    The easiest way to implement this is called a *residual connection*. It’s dead
    easy: just add the input of a layer or block of layers back to its output (see
    figure 9.3). The residual connection acts as an *information shortcut* around
    destructive or noisy blocks (such as blocks that contain ReLU activations or dropout
    layers), enabling error gradient information from early layers to propagate noiselessly
    through a deep network. This technique was introduced in 2015 with the ResNet
    family of models (developed by He et al. at Microsoft).^([[1]](#footnote-1))'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b0f6da876f4c751d9ae3f0376cf433c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 9.3](#figure-9-3): A residual connection around a processing block'
  prefs: []
  type: TYPE_NORMAL
- en: In practice, you’d implement a residual connection like the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 9.1](#listing-9-1): A residual connection in pseudocode'
  prefs: []
  type: TYPE_NORMAL
- en: Note that adding the input back to the output of a block implies that the output
    should have the same shape as the input. This is not the case if your block includes
    convolutional layers with an increased number of filters or a max pooling layer.
    In such cases, use a 1 × 1 `Conv2D` layer with no activation to linearly project
    the residual to the desired output shape. You’d typically use `padding="same"`
    in the convolution layers in your target block to avoid spatial downsampling due
    to padding, and you’d use strides in the residual projection to match any downsampling
    caused by a max pooling layer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 9.2](#listing-9-2): The target block changing the number of output
    filters'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 9.3](#listing-9-3): The target block including a max pooling layer'
  prefs: []
  type: TYPE_NORMAL
- en: 'To make these ideas more concrete, here’s an example of a simple ConvNet structured
    into a series of blocks, each made of two convolution layers and one optional
    max pooling layer, with a residual connection around each block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s take a look at the model summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'With residual connections, you can build networks of arbitrary depth, without
    having to worry about vanishing gradients. Now, let’s move on to the next essential
    ConvNet architecture pattern: *batch normalization*.'
  prefs: []
  type: TYPE_NORMAL
- en: Batch normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Normalization* in machine learning is a broad category of methods that seek
    to make different samples seen by a machine learning model more similar to each
    other, which helps the model learn and generalize well to new data. The most common
    form of data normalization is one you’ve seen several times in this book already:
    centering the data on zero by subtracting the mean from the data and giving the
    data a unit standard deviation by dividing the data by its standard deviation.
    In effect, this makes the assumption that the data follows a normal (or Gaussian)
    distribution and makes sure this distribution is centered and scaled to unit variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Previous examples you saw in this book normalized data before feeding it into
    models. But data normalization may be a concern after every transformation performed
    by the network: even if the data entering a `Dense` or `Conv2D` network has a
    0 mean and unit variance, there’s no reason to expect a priori that this will
    be the case for the data coming out. Could normalizing intermediate activations
    help?'
  prefs: []
  type: TYPE_NORMAL
- en: Batch normalization does just that. It’s a type of layer (`BatchNormalization`
    in Keras) introduced in 2015 by Ioffe and Szegedy;^([[2]](#footnote-2)) it can
    adaptively normalize data even as the mean and variance change over time during
    training. During training, it uses the mean and variance of the current batch
    of data to normalize samples, and during inference (when a big enough batch of
    representative data may not be available), it uses an exponential moving average
    of the batchwise mean and variance of the data seen during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although Ioffe and Szegedy’s original paper suggested that batch normalization
    operates by “reducing internal covariate shift,” no one really knows for sure
    why batch normalization helps. There are various hypotheses but no certitudes.
    You’ll find that this is true of many things in deep learning — deep learning
    is not an exact science but a set of ever-changing, empirically derived engineering
    best practices, woven together by unreliable narratives. You will sometimes feel
    like the book you have in hand tells you *how* to do something but doesn’t quite
    satisfactorily say *why* it works: that’s because we know the how but we don’t
    know the why. Whenever a reliable explanation is available, we make sure to mention
    it. Batch normalization isn’t one of those cases.'
  prefs: []
  type: TYPE_NORMAL
- en: In practice, the main effect of batch normalization appears to be that it helps
    with gradient propagation — much like residual connections — and thus allows for
    deeper networks. Some very deep networks can only be trained if they include multiple
    `BatchNormalization` layers. For instance, batch normalization is used liberally
    in many of the advanced ConvNet architectures that come packaged with Keras, such
    as ResNet50, EfficientNet, and Xception.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `BatchNormalization` layer can be used after any layer — `Dense`, `Conv2D`,
    and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Importantly, I would generally recommend placing the previous layer’s activation
    *after* the batch normalization layer (although this is still a subject of debate).
    So instead of doing
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 9.4](#listing-9-4): How not to use batch normalization'
  prefs: []
  type: TYPE_NORMAL
- en: 'you would actually do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 9.5](#listing-9-5): How to use batch normalization'
  prefs: []
  type: TYPE_NORMAL
- en: 'The intuitive reason why is that batch normalization will center your inputs
    on zero, while your ReLU activation uses zero as a pivot for keeping or dropping
    activated channels: doing normalization before the activation maximizes the utilization
    of the ReLU. That said, this ordering best practice is not exactly critical, so
    if you do convolution-activation-batch normalization, your model will still train,
    and you won’t necessarily see worse results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s take a look at the last architecture pattern in our series: depthwise
    separable convolutions.'
  prefs: []
  type: TYPE_NORMAL
- en: Depthwise separable convolutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What if we told you that there’s a layer you can use as a drop-in replacement
    for `Conv2D` that will make your model smaller (fewer trainable weight parameters),
    leaner (fewer floating-point operations), and cause it to perform a few percentage
    points better on its task? That is precisely what the *depthwise separable convolution*
    layer does (`SeparableConv2D` in Keras). This layer performs a spatial convolution
    on each channel of its input, independently, before mixing output channels via
    a pointwise convolution (a 1 × 1 convolution), as shown in figure 9.4.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/315d488d3210705766e9597d3efd192b.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 9.4](#figure-9-4): Depthwise separable convolution: a depthwise convolution
    followed by a pointwise convolution'
  prefs: []
  type: TYPE_NORMAL
- en: This is equivalent to separating the learning of spatial features and the learning
    of channel-wise features. In much the same way that convolution relies on the
    assumption that the patterns in images are not tied to specific locations, depthwise
    separable convolution relies on the assumption that *spatial locations* in intermediate
    activations are *highly correlated*, but *different channels* are *highly independent*.
    Because this assumption is generally true for the image representations learned
    by deep neural networks, it serves as a useful prior that helps the model make
    more efficient use of its training data. A model with stronger priors about the
    structure of the information it will have to process is a better model — as long
    as the priors are accurate.
  prefs: []
  type: TYPE_NORMAL
- en: Depthwise separable convolution requires significantly fewer parameters and
    involves fewer computations compared to regular convolution, while having comparable
    representational power. They result in smaller models that converge faster and
    are less prone to overfitting. These advantages become especially important when
    you’re training small models from scratch on limited data.
  prefs: []
  type: TYPE_NORMAL
- en: 'When it comes to larger-scale models, depthwise separable convolutions are
    the basis of the Xception architecture, a high-performing ConvNet that comes packaged
    with Keras. You can read more about the theoretical grounding for depthwise separable
    convolutions and Xception in the paper “Xception: Deep Learning with Depthwise
    Separable Convolutions.”^([[3]](#footnote-3))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Putting it together: A mini Xception-like model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As a reminder, here are the ConvNet architecture principles you’ve learned
    so far:'
  prefs: []
  type: TYPE_NORMAL
- en: Your model should be organized into repeated *blocks* of layers, usually made
    of multiple convolution layers and a max pooling layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of filters in your layers should increase as the size of the spatial
    feature maps decreases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep and narrow is better than broad and shallow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing residual connections around blocks of layers helps you train deeper
    networks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be beneficial to introduce batch normalization layers after your convolution
    layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be beneficial to replace `Conv2D` layers with `SeparableConv2D` layers,
    which are more parameter efficient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s bring all of these ideas together into a single model. Its architecture
    resembles a smaller version of Xception. We’ll apply it to the dogs-versus-cats
    task from last chapter. For data loading and model training, simply reuse the
    exact same setup as what we used in chapter 8, section 8.2 — but replace the model
    definition with the following ConvNet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This ConvNet has a trainable parameter count of 721,857, significantly lower
    than the 1,569,089 trainable parameters of the model from the previous chapter,
    yet it achieves better results. Figure 9.5 shows the training and validation curves.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e74199fd16c5ef19962976eb03f81004.png) ![](../Images/26fa751a5f2d173423e98a7e769209b1.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 9.5](#figure-9-5): Training and validation metrics with a Xception-like
    architecture'
  prefs: []
  type: TYPE_NORMAL
- en: You’ll find that our new model achieves a test accuracy of 90.8% — compared
    to 83.9% for the previous model. As you can see, following architecture best practices
    does have an immediate, sizeable effect on model performance!
  prefs: []
  type: TYPE_NORMAL
- en: At this point, if you want to further improve performance, you should start
    systematically tuning the hyperparameters of your architecture — a topic we cover
    in detail in chapter 18. We haven’t gone through this step here, so the configuration
    of the previous model is purely from the best practices we outlined, plus, when
    it comes to gauging model size, a small amount of intuition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Beyond convolution: Vision Transformers'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While ConvNets have been dominating the field of computer vision since the
    mid-2010s, they’ve been recently competing with an alternative architecture: Vision
    Transformers (or ViTs for short). It may well be that ViTs will end up replacing
    ConvNets in the long term — though, for now, ConvNets remain your best option
    in most cases.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You don’t yet know what Transformers are because we’ll cover them in chapter
    15. In short, the Transformer architecture was developed to process text — it’s
    fundamentally a sequence-processing architecture. And Transformers are very good
    at it, which has led to the question: could we also use them for images?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Because ViTs are a type of Transformer, they also process sequences: they split
    up an image into a 1D sequence of patches, turn each patch into a flat vector,
    and process the vector sequence. The Transformer architecture allows ViTs to capture
    long-range relationships between different parts of the image, something ConvNets
    can sometimes struggle with.'
  prefs: []
  type: TYPE_NORMAL
- en: Our general experience with Transformers is that they’re a great choice if you’re
    working with a massive dataset. They’re simply better at utilizing large amounts
    of data. However, for smaller datasets, they tend to be suboptimal for two reasons.
    First, they lack the spatial prior of ConvNets — the 2D patch-based architecture
    of ConvNets incorporates more assumptions about the local structure of the visual
    space, making them more data efficient. Second, for ViTs to shine, they need to
    be really large. They end up being unwieldy for anything smaller than ImageNet.
  prefs: []
  type: TYPE_NORMAL
- en: The battle for image recognition supremacy is far from over, but ViTs have undoubtedly
    opened a new and exciting chapter. You’ll probably work with this architecture
    in the context of large-scale generative image models — a topic we’ll cover in
    chapter 17. For your small-scale image classification needs, however, ConvNets
    remain your best bet.
  prefs: []
  type: TYPE_NORMAL
- en: 'This concludes our introduction to essential ConvNet architecture best practices.
    With these principles in hand, you’ll be able to develop higher-performing models
    across a wide range of computer vision tasks. You’re now well on your way to becoming
    a proficient computer vision practitioner. To further deepen your expertise, there’s
    one last important topic we need to cover: interpreting how a model arrives at
    its predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The architecture of a deep learning model encodes key assumptions about the
    nature of the problem at hand.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The modularity-hierarchy-reuse formula underpins the architecture of nearly
    all complex systems, including deep learning models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key architecture patterns for computer vision include residual connections,
    batch normalization, and depthwise separable convolutions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vision Transformers are an up-and-coming alternative to ConvNets for large-scale
    computer vision tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Footnotes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kaiming He et al., “Deep Residual Learning for Image Recognition,” Conference
    on Computer Vision and Pattern Recognition (2015), [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385).
    [[↩]](#footnote-link-1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sergey Ioffe and Christian Szegedy, “Batch Normalization: Accelerating Deep
    Network Training by Reducing Internal Covariate Shift,” *Proceedings of the 32nd
    International Conference on Machine Learning* (2015), [https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167).
    [[↩]](#footnote-link-2)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'François Chollet, “Xception: Deep Learning with Depthwise Separable Convolutions,”
    Conference on Computer Vision and Pattern Recognition (2017), [https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357).
    [[↩]](#footnote-link-3)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
