- en: ConvNet architecture patterns
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ConvNet架构模式
- en: 原文：[https://deeplearningwithpython.io/chapters/chapter09_convnet-architecture-patterns](https://deeplearningwithpython.io/chapters/chapter09_convnet-architecture-patterns)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://deeplearningwithpython.io/chapters/chapter09_convnet-architecture-patterns](https://deeplearningwithpython.io/chapters/chapter09_convnet-architecture-patterns)
- en: 'A model’s “architecture” is the sum of the choices that went into creating
    it: which layers to use, how to configure them, in what arrangement to connect
    them. These choices define the *hypothesis space* of your model: the space of
    possible functions that gradient descent can search over, parameterized by the
    model’s weights. Like feature engineering, a good hypothesis space encodes *prior
    knowledge* that you have about the problem at hand and its solution. For instance,
    using convolution layers means that you know in advance that the relevant patterns
    present in your input images are translation-invariant. To effectively learn from
    data, you need to make assumptions about what you’re looking for.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的“架构”是创建它时所做的所有选择的总和：使用哪些层，如何配置它们，以及如何连接它们的排列。这些选择定义了您模型的*假设空间*：梯度下降可以搜索的可能函数空间，由模型的权重参数化。就像特征工程一样，一个好的假设空间编码了您对当前问题和其解决方案的*先验知识*。例如，使用卷积层意味着您事先知道您输入图像中存在的相关模式是平移不变的。为了有效地从数据中学习，您需要就您要寻找的内容做出假设。
- en: Model architecture is often the difference between success and failure. If you
    make inappropriate architecture choices, your model may be stuck with suboptimal
    metrics, and no amount of training data will save it. Inversely, a good model
    architecture will accelerate learning and will enable your model to make efficient
    use of the training data available, reducing the need for large datasets. A good
    model architecture is one that *reduces the size of the search space* or otherwise
    *makes it easier to converge to a good point of the search space*. Just like feature
    engineering and data curation, model architecture is all about *making the problem
    simpler* for gradient descent to solve — and remember that gradient descent is
    a pretty stupid search process, so it needs all the help it can get.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 模型架构通常是成功与失败之间的区别。如果您做出不适当的架构选择，您的模型可能会陷入次优指标，并且无论多少训练数据都无法挽救它。相反，一个好的模型架构将加速学习，并使您的模型能够有效地利用可用的训练数据，减少对大型数据集的需求。一个好的模型架构是那种*减少搜索空间大小*或以其他方式*使搜索空间中的良好点更容易收敛*的架构。就像特征工程和数据整理一样，模型架构的全部都是为了*使问题对梯度下降来说更简单*——并且请记住，梯度下降是一个相当愚蠢的搜索过程，所以它需要所有它能得到的帮助。
- en: 'Model architecture is more an art than a science. Experienced machine learning
    engineers are able to intuitively cobble together high-performing models on their
    first try, while beginners often struggle to create a model that trains at all.
    The keyword here is *intuitively*: no one can give you a clear explanation of
    what works and what doesn’t. Experts rely on pattern matching, an ability that
    they acquire through extensive practical experience. You’ll develop your own intuition
    throughout this book. However, it’s not *all* about intuition either — there isn’t
    much in the way of actual science, but like in any engineering discipline, there
    are best practices.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 模型架构更是一门艺术而非科学。经验丰富的机器学习工程师能够直观地组合出高性能模型，而初学者往往难以创建一个能够训练的模型。这里的关键词是*直观*：没有人能给出一个明确的解释，说明什么有效，什么无效。专家们依赖于模式匹配，这是一种他们通过大量实践经验获得的能力。您将在本书中发展自己的直觉。然而，这并不是*全部*关于直觉的——实际上并没有多少真正的科学，但就像任何工程学科一样，有最佳实践。
- en: In the following sections, we’ll review a few essential ConvNet architecture
    best practices, in particular, *residual connections*, *batch normalization*,
    and *separable convolution*. Once you master how to use them, you will be able
    to build highly effective image models. We will demonstrate how to apply them
    on our dogs-versus-cats classification problem.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将回顾一些基本的ConvNet架构最佳实践，特别是*残差连接*、*批量归一化*和*可分离卷积*。一旦您掌握了如何使用它们，您将能够构建高度有效的图像模型。我们将演示如何在我们的狗与猫分类问题上应用它们。
- en: 'Let’s start from the bird’s-eye view: the modularity-hierarchy-reuse (MHR)
    formula for system architecture.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从宏观的角度开始：系统架构的模块化-层次-重用（MHR）公式。
- en: Modularity, hierarchy, and reuse
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模块化、层次和重用
- en: 'If you want to make a complex system simpler, there’s a universal recipe you
    can apply: just structure your amorphous soup of complexity into *modules*, organize
    the modules into a *hierarchy*, and start *reusing* the same modules in multiple
    places as appropriate (“reuse” is another word for *abstraction*). That’s the
    modularity-hierarchy-reuse (MHR) formula (see figure 9.1), and it underlies system
    architecture across pretty much every domain where the term *architecture* is
    used. It’s at the heart of the organization of any system of meaningful complexity,
    whether it’s a cathedral, your own body, the US Navy, or the Keras codebase.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想使一个复杂系统变得简单，有一个通用的配方你可以应用：只需将你的无序复杂汤结构化为*模块*，将模块组织成一个*层次结构*，并在适当的地方开始*重复使用*相同的模块（“重复使用”是*抽象*的另一个词）。这就是模块化-层次结构-重复（MHR）公式（见图9.1），它几乎在所有使用术语*架构*的领域中都构成了系统架构的基础。它是任何有意义的复杂系统组织的核心，无论是大教堂、你自己的身体、美国海军，还是Keras代码库。
- en: '![](../Images/b7809a140aeaa20dd22850588281fe8c.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/b7809a140aeaa20dd22850588281fe8c.png)'
- en: '[Figure 9.1](#figure-9-1): Complex systems follow a hierarchical structure
    and are organized into distinct modules, which are reused multiple times (such
    as your 4 limbs, which are all variants of the same blueprint, or your 20 fingers).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9.1](#figure-9-1)：复杂系统遵循层次结构，并组织成不同的模块，这些模块被多次重复使用（例如你的4条肢体，它们都是同一蓝图的不同变体，或者你的20个手指）。'
- en: 'If you’re a software engineer, you’re already keenly familiar with these principles:
    an effective codebase is one that is modular, hierarchical, and where you don’t
    reimplement the same thing twice but instead rely on reusable classes and functions.
    If you factor your code by following these principles, you could say you’re doing
    “software architecture.”'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是一名软件工程师，你已经非常熟悉这些原则：一个有效的代码库是一个模块化、层次化的代码库，你不会两次实现相同的事情，而是依赖于可重用的类和函数。如果你通过遵循这些原则来分解你的代码，你可以说你正在做“软件架构”。
- en: 'Deep learning itself is simply the application of this recipe to continuous
    optimization via gradient descent: you take a classic optimization technique (gradient
    descent over a continuous function space), and you structure the search space
    into modules (layers), organized into a deep hierarchy (often just a stack, the
    simplest kind of hierarchy), where you reuse whatever you can (for instance, convolutions
    are all about reusing the same information in different spatial locations).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习本身仅仅是将这个配方应用于通过梯度下降的连续优化：你采用一个经典的优化技术（在连续函数空间上的梯度下降），并将搜索空间结构化为模块（层），组织成一个深度层次结构（通常只是一个堆栈，最简单的层次结构），在那里你可以重复使用任何可以重复的内容（例如，卷积全部关于在不同空间位置重复相同的信息）。
- en: Likewise, deep learning model architecture is primarily about making clever
    use of modularity, hierarchy, and reuse. You’ll notice that all popular ConvNet
    architectures are not only structured into layers, they’re structured into repeated
    groups of layers (called *blocks* or *modules*). For instance, Xception architecture
    (used in the previous chapter) is structured into repeated `SeparableConv` - `SeparableConv`
    - `MaxPooling` blocks (see figure 9.2).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，深度学习模型架构主要关于巧妙地使用模块化、层次结构和重复。你会注意到所有流行的ConvNet架构不仅被结构化为层，还被结构化为重复的层组（称为*块*或*模块*）。例如，Xception架构（在上一章中使用）被结构化为重复的`SeparableConv`
    - `SeparableConv` - `MaxPooling`块（见图9.2）。
- en: 'Further, most ConvNets often feature pyramid-like structures (*feature hierarchies*).
    Recall, for example, the progression in the number of convolution filters we used
    in the first ConvNet we built in the previous chapter: 32, 64, 128. The number
    of filters grows with layer depth, while the size of the feature maps shrinks
    accordingly. You’ll notice the same pattern in the blocks of the Xception model
    (see figure 9.2).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，大多数ConvNets通常具有金字塔状结构（*特征层次结构*）。回想一下，例如，我们在上一章中构建的第一个ConvNet中使用的卷积滤波器数量的进展：32、64、128。滤波器的数量随着层深度的增加而增加，而特征图的大小相应缩小。你会在Xception模型的块中注意到相同的模式（见图9.2）。
- en: '![](../Images/ad5cf0efed908d8fa63d5e0a20a13cb5.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/ad5cf0efed908d8fa63d5e0a20a13cb5.png)'
- en: '[Figure 9.2](#figure-9-2): The “entry flow” of the Xception architecture: note
    the repeated layer blocks and the gradually shrinking and deepening feature maps,
    going from 299 x 299 x 3 to 19 x 19 x 728.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9.2](#figure-9-2)：Xception架构的“入口流”：注意重复的层块和逐渐缩小和加深的特征图，从299 x 299 x 3变为19
    x 19 x 728。'
- en: 'Deeper hierarchies are intrinsically good because they encourage feature reuse
    and, therefore, abstraction. In general, a deep stack of narrow layers performs
    better than a shallow stack of large layers. However, there’s a limit to how deep
    you can stack layers: the problem of *vanishing gradients*. This leads us to our
    first essential model architecture pattern: residual connections.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 深层层次结构本质上是有益的，因为它们鼓励特征重用，因此也鼓励抽象。一般来说，深层窄层堆叠比浅层大层堆叠表现更好。然而，层堆叠的深度有一个限制：*梯度消失*问题。这使我们来到了第一个基本模型架构模式：残差连接。
- en: Residual connections
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 残差连接
- en: You probably know about the game of *telephone*, also called *Chinese whispers*
    in the UK and *téléphone arabe* in France, where an initial message is whispered
    in the ear of a player, who then whispers it in the ear of the next player, and
    so on. The final message ends up bearing little resemblance to its original version.
    It’s a fun metaphor for the cumulative errors that occur in sequential transmission
    over a noisy channel.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能听说过游戏*电话*，在英国也称为*Chinese whispers*，在法国称为*telephone arabe*，在这个游戏中，一个初始信息被悄悄地告诉一个玩家，然后该玩家再悄悄地告诉下一个玩家，以此类推。最终的信息与原始版本相差甚远。这是一个有趣的隐喻，描述了在噪声信道上顺序传输过程中累积的错误。
- en: 'As it happens, backpropagation in a sequential deep learning model is pretty
    similar to the game of telephone. You’ve got a chain of functions, like this one:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，在顺序深度学习模型中的反向传播与电话游戏非常相似。你有一系列函数，就像这样：
- en: '`y = f4(f3(f2(f1(x))))`'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`y = f4(f3(f2(f1(x))))`'
- en: The name of the game is to adjust the parameters of each function in the chain
    based on the error recorded on the output of `f4` (the loss of the model). To
    adjust `f1`, you’ll need to percolate error information through `f2`, `f3`, and
    `f4`. However, each successive function in the chain introduces some amount of
    noise in the process. If your function chain is too deep, this noise starts overwhelming
    gradient information, and backpropagation stops working. Your model won’t train
    at all. This is called the *vanishing gradients* problem.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏的目的是根据记录在`f4`输出上的错误（模型的损失）来调整链中每个函数的参数。要调整`f1`，你需要通过`f2`、`f3`和`f4`传递错误信息。然而，链中的每个后续函数在过程中都会引入一些噪声。如果你的函数链太深，这种噪声开始压倒梯度信息，反向传播就不再起作用。你的模型将无法进行训练。这被称为*梯度消失*问题。
- en: 'The fix is simple: just force each function in the chain to be nondestructive
    — to retain a noiseless version of the information contained in the previous input.
    The easiest way to implement this is called a *residual connection*. It’s dead
    easy: just add the input of a layer or block of layers back to its output (see
    figure 9.3). The residual connection acts as an *information shortcut* around
    destructive or noisy blocks (such as blocks that contain ReLU activations or dropout
    layers), enabling error gradient information from early layers to propagate noiselessly
    through a deep network. This technique was introduced in 2015 with the ResNet
    family of models (developed by He et al. at Microsoft).^([[1]](#footnote-1))'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 修复方法很简单：只需强制链中的每个函数都是非破坏性的——保留前一个输入中包含的无噪声版本的信息。实现这一点的最简单方法被称为*残差连接*。这非常简单：只需将层或层块的输入添加回其输出（见图9.3）。残差连接在破坏性或噪声块（例如包含ReLU激活或dropout层的块）周围充当*信息捷径*，使早期层的错误梯度信息能够无噪声地通过深度网络传播。这项技术于2015年随着ResNet系列模型（由微软的He等人开发）的引入而提出。（^[[1]](#footnote-1)）
- en: '![](../Images/2b0f6da876f4c751d9ae3f0376cf433c.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b0f6da876f4c751d9ae3f0376cf433c.png)'
- en: '[Figure 9.3](#figure-9-3): A residual connection around a processing block'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9.3](#figure-9-3)：一个处理块周围的残差连接'
- en: In practice, you’d implement a residual connection like the following listing.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，你会实现一个像以下列表那样的残差连接。
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[Listing 9.1](#listing-9-1): A residual connection in pseudocode'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码清单9.1](#listing-9-1)：伪代码中的残差连接'
- en: Note that adding the input back to the output of a block implies that the output
    should have the same shape as the input. This is not the case if your block includes
    convolutional layers with an increased number of filters or a max pooling layer.
    In such cases, use a 1 × 1 `Conv2D` layer with no activation to linearly project
    the residual to the desired output shape. You’d typically use `padding="same"`
    in the convolution layers in your target block to avoid spatial downsampling due
    to padding, and you’d use strides in the residual projection to match any downsampling
    caused by a max pooling layer.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，将输入加回到块的输出意味着输出应该与输入具有相同的形状。如果你的块包含具有增加的滤波器数量的卷积层或最大池化层，则这种情况不成立。在这种情况下，使用一个没有激活的1
    × 1 `Conv2D`层将残差线性投影到所需的输出形状。你通常会在目标块中的卷积层中使用`padding="same"`来避免由于填充造成的空间下采样，并且你会在残差投影中使用步长来匹配由最大池化层引起的任何下采样。
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[Listing 9.2](#listing-9-2): The target block changing the number of output
    filters'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 9.2](#listing-9-2)：改变输出滤波器数量的目标块'
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[Listing 9.3](#listing-9-3): The target block including a max pooling layer'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 9.3](#listing-9-3)：包含最大池化层的目标块'
- en: 'To make these ideas more concrete, here’s an example of a simple ConvNet structured
    into a series of blocks, each made of two convolution layers and one optional
    max pooling layer, with a residual connection around each block:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这些想法更加具体，这里有一个简单的卷积神经网络示例，它由一系列块组成，每个块由两个卷积层和一个可选的最大池化层组成，每个块周围都有一个残差连接：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let’s take a look at the model summary:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看模型摘要：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'With residual connections, you can build networks of arbitrary depth, without
    having to worry about vanishing gradients. Now, let’s move on to the next essential
    ConvNet architecture pattern: *batch normalization*.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 使用残差连接，你可以构建任意深度的网络，无需担心梯度消失问题。现在，让我们继续探讨下一个关键的卷积神经网络架构模式：*批量归一化*。
- en: Batch normalization
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量归一化
- en: '*Normalization* in machine learning is a broad category of methods that seek
    to make different samples seen by a machine learning model more similar to each
    other, which helps the model learn and generalize well to new data. The most common
    form of data normalization is one you’ve seen several times in this book already:
    centering the data on zero by subtracting the mean from the data and giving the
    data a unit standard deviation by dividing the data by its standard deviation.
    In effect, this makes the assumption that the data follows a normal (or Gaussian)
    distribution and makes sure this distribution is centered and scaled to unit variance:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，*归一化*是一个广泛的类别，旨在使机器学习模型看到的不同样本彼此更加相似，这有助于模型学习并很好地泛化到新数据。最常见的数据归一化形式是你在这本书中已经看到几次的：通过从数据中减去均值来将数据中心化在零点，并通过将数据除以其标准差来给数据一个单位标准差。实际上，这假设数据遵循正态（或高斯）分布，并确保这个分布是中心化和缩放到单位方差的：
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Previous examples you saw in this book normalized data before feeding it into
    models. But data normalization may be a concern after every transformation performed
    by the network: even if the data entering a `Dense` or `Conv2D` network has a
    0 mean and unit variance, there’s no reason to expect a priori that this will
    be the case for the data coming out. Could normalizing intermediate activations
    help?'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你在这本书中看到的先前示例在将数据输入模型之前对数据进行归一化。但是，数据归一化可能是网络执行的每个转换之后的一个问题：即使进入`Dense`或`Conv2D`网络的数据具有0均值和单位方差，也没有理由事先期望输出数据也会是这样。对中间激活进行归一化能有所帮助吗？
- en: Batch normalization does just that. It’s a type of layer (`BatchNormalization`
    in Keras) introduced in 2015 by Ioffe and Szegedy;^([[2]](#footnote-2)) it can
    adaptively normalize data even as the mean and variance change over time during
    training. During training, it uses the mean and variance of the current batch
    of data to normalize samples, and during inference (when a big enough batch of
    representative data may not be available), it uses an exponential moving average
    of the batchwise mean and variance of the data seen during training.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '批量归一化正是如此。这是一种层（在Keras中为`BatchNormalization`），由Ioffe和Szegedy于2015年引入；^([[2]](#footnote-2))它可以在训练过程中，即使均值和方差随时间变化，也能自适应地归一化数据。在训练期间，它使用当前数据批次的均值和方差来归一化样本，在推理期间（当可能没有足够大的代表性数据批次时），它使用训练期间看到的批处理均值和方差的指数移动平均值。 '
- en: 'Although Ioffe and Szegedy’s original paper suggested that batch normalization
    operates by “reducing internal covariate shift,” no one really knows for sure
    why batch normalization helps. There are various hypotheses but no certitudes.
    You’ll find that this is true of many things in deep learning — deep learning
    is not an exact science but a set of ever-changing, empirically derived engineering
    best practices, woven together by unreliable narratives. You will sometimes feel
    like the book you have in hand tells you *how* to do something but doesn’t quite
    satisfactorily say *why* it works: that’s because we know the how but we don’t
    know the why. Whenever a reliable explanation is available, we make sure to mention
    it. Batch normalization isn’t one of those cases.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Ioffe和Szegedy的原始论文建议批归一化是通过“减少内部协变量偏移”来操作的，但没有人真正知道为什么批归一化有帮助。有各种假设但没有确定性。你会发现这在深度学习中很常见——深度学习不是一门精确的科学，而是一套不断变化、基于经验得出的工程最佳实践，由不可靠的叙述编织在一起。你有时会感觉你手中的书告诉你
    *如何* 做某事，但并没有很好地解释 *为什么* 它有效：这是因为我们知道如何做，但不知道为什么。每当有可靠的解释时，我们都会确保提及它。批归一化不是那种情况。
- en: In practice, the main effect of batch normalization appears to be that it helps
    with gradient propagation — much like residual connections — and thus allows for
    deeper networks. Some very deep networks can only be trained if they include multiple
    `BatchNormalization` layers. For instance, batch normalization is used liberally
    in many of the advanced ConvNet architectures that come packaged with Keras, such
    as ResNet50, EfficientNet, and Xception.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，批归一化的主要效果似乎是有助于梯度传播——就像残差连接一样——从而允许更深的网络。一些非常深的网络只有在包含多个 `BatchNormalization`
    层的情况下才能进行训练。例如，批归一化在 Keras 包含的许多高级 ConvNet 架构中被广泛使用，如 ResNet50、EfficientNet 和
    Xception。
- en: 'The `BatchNormalization` layer can be used after any layer — `Dense`, `Conv2D`,
    and so on:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`BatchNormalization` 层可以在任何层之后使用——`Dense`、`Conv2D` 等等：'
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Importantly, I would generally recommend placing the previous layer’s activation
    *after* the batch normalization layer (although this is still a subject of debate).
    So instead of doing
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，我通常会推荐将前一个层的激活放在批归一化层之后（尽管这仍然是一个有争议的话题）。所以，而不是这样做
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[Listing 9.4](#listing-9-4): How not to use batch normalization'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 9.4](#listing-9-4)：如何不使用批归一化'
- en: 'you would actually do the following:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 你实际上会做以下操作：
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[Listing 9.5](#listing-9-5): How to use batch normalization'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 9.5](#listing-9-5)：如何使用批归一化'
- en: 'The intuitive reason why is that batch normalization will center your inputs
    on zero, while your ReLU activation uses zero as a pivot for keeping or dropping
    activated channels: doing normalization before the activation maximizes the utilization
    of the ReLU. That said, this ordering best practice is not exactly critical, so
    if you do convolution-activation-batch normalization, your model will still train,
    and you won’t necessarily see worse results.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 直觉上，这是因为批归一化会将你的输入中心化在零点，而你的 ReLU 激活使用零作为保持或丢弃激活通道的支点：在激活之前进行归一化最大化了 ReLU 的利用率。尽管如此，这种排序最佳实践并不是绝对的，所以如果你进行卷积-激活-批归一化，你的模型仍然可以训练，你也不一定会看到更差的结果。
- en: 'Now, let’s take a look at the last architecture pattern in our series: depthwise
    separable convolutions.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看我们系列中的最后一个架构模式：深度可分离卷积。
- en: Depthwise separable convolutions
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度可分离卷积
- en: What if we told you that there’s a layer you can use as a drop-in replacement
    for `Conv2D` that will make your model smaller (fewer trainable weight parameters),
    leaner (fewer floating-point operations), and cause it to perform a few percentage
    points better on its task? That is precisely what the *depthwise separable convolution*
    layer does (`SeparableConv2D` in Keras). This layer performs a spatial convolution
    on each channel of its input, independently, before mixing output channels via
    a pointwise convolution (a 1 × 1 convolution), as shown in figure 9.4.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们告诉你有一个层可以作为 `Conv2D` 的直接替换，这将使你的模型更小（更少的可训练权重参数）、更精简（更少的浮点运算），并在其任务上提高几个百分点？这正是
    *深度可分离卷积* 层所做的（在 Keras 中为 `SeparableConv2D`）。这个层对其输入的每个通道独立地进行空间卷积，然后通过点卷积（一个
    1 × 1 卷积）混合输出通道，如图 9.4 所示。
- en: '![](../Images/315d488d3210705766e9597d3efd192b.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/315d488d3210705766e9597d3efd192b.png)'
- en: '[Figure 9.4](#figure-9-4): Depthwise separable convolution: a depthwise convolution
    followed by a pointwise convolution'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9.4](#figure-9-4)：深度可分离卷积：先进行深度卷积然后进行点卷积'
- en: This is equivalent to separating the learning of spatial features and the learning
    of channel-wise features. In much the same way that convolution relies on the
    assumption that the patterns in images are not tied to specific locations, depthwise
    separable convolution relies on the assumption that *spatial locations* in intermediate
    activations are *highly correlated*, but *different channels* are *highly independent*.
    Because this assumption is generally true for the image representations learned
    by deep neural networks, it serves as a useful prior that helps the model make
    more efficient use of its training data. A model with stronger priors about the
    structure of the information it will have to process is a better model — as long
    as the priors are accurate.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当于将空间特征的学习和通道特征的学习分开。与卷积依赖于图像中的模式与特定位置无关的假设类似，深度可分离卷积依赖于中间激活中的空间位置*高度相关*，但*不同通道*高度独立的假设。由于这个假设通常适用于深度神经网络学习的图像表示，它作为一个有用的先验，有助于模型更有效地利用其训练数据。具有关于它将必须处理的信息结构的更强先验的模型是一个更好的模型——只要这些先验是准确的。
- en: Depthwise separable convolution requires significantly fewer parameters and
    involves fewer computations compared to regular convolution, while having comparable
    representational power. They result in smaller models that converge faster and
    are less prone to overfitting. These advantages become especially important when
    you’re training small models from scratch on limited data.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 与常规卷积相比，深度可分离卷积需要显著更少的参数，涉及的计算也更少，同时具有可比的表示能力。它们导致模型更小，收敛更快，并且不太容易过拟合。当你在有限的数据上从头开始训练小型模型时，这些优势变得尤为重要。
- en: 'When it comes to larger-scale models, depthwise separable convolutions are
    the basis of the Xception architecture, a high-performing ConvNet that comes packaged
    with Keras. You can read more about the theoretical grounding for depthwise separable
    convolutions and Xception in the paper “Xception: Deep Learning with Depthwise
    Separable Convolutions.”^([[3]](#footnote-3))'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '当涉及到更大规模的模型时，深度可分离卷积是Xception架构的基础，这是一个高性能的ConvNet，它包含在Keras中。你可以在论文“Xception:
    Deep Learning with Depthwise Separable Convolutions.”中了解更多关于深度可分离卷积和Xception的理论基础。[^[[3]](#footnote-3)]'
- en: 'Putting it together: A mini Xception-like model'
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将其整合：一个类似Xception的小型模型
- en: 'As a reminder, here are the ConvNet architecture principles you’ve learned
    so far:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，以下是你迄今为止学到的ConvNet架构原则：
- en: Your model should be organized into repeated *blocks* of layers, usually made
    of multiple convolution layers and a max pooling layer.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的模型应该组织成重复的*块*层，通常由多个卷积层和一个最大池化层组成。
- en: The number of filters in your layers should increase as the size of the spatial
    feature maps decreases.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的层中的滤波器数量应随着空间特征图大小的减小而增加。
- en: Deep and narrow is better than broad and shallow.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深而窄优于宽而浅。
- en: Introducing residual connections around blocks of layers helps you train deeper
    networks.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在层块周围引入残差连接有助于你训练更深的网络。
- en: It can be beneficial to introduce batch normalization layers after your convolution
    layers.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的卷积层之后引入批量归一化层可能有益。
- en: It can be beneficial to replace `Conv2D` layers with `SeparableConv2D` layers,
    which are more parameter efficient.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`Conv2D`层替换为更参数高效的`SeparableConv2D`层可能有益。
- en: 'Let’s bring all of these ideas together into a single model. Its architecture
    resembles a smaller version of Xception. We’ll apply it to the dogs-versus-cats
    task from last chapter. For data loading and model training, simply reuse the
    exact same setup as what we used in chapter 8, section 8.2 — but replace the model
    definition with the following ConvNet:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将所有这些想法整合成一个单一模型。其架构类似于Xception的一个更小版本。我们将将其应用于上一章中的狗与猫的任务。对于数据加载和模型训练，只需重复使用与第8章第8.2节中完全相同的设置——但将模型定义替换为以下ConvNet：
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This ConvNet has a trainable parameter count of 721,857, significantly lower
    than the 1,569,089 trainable parameters of the model from the previous chapter,
    yet it achieves better results. Figure 9.5 shows the training and validation curves.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 该ConvNet的可训练参数数量为721,857，显著低于上一章模型中1,569,089个可训练参数，但取得了更好的结果。图9.5显示了训练和验证曲线。
- en: '![](../Images/e74199fd16c5ef19962976eb03f81004.png) ![](../Images/26fa751a5f2d173423e98a7e769209b1.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图片1](../Images/e74199fd16c5ef19962976eb03f81004.png) ![图片2](../Images/26fa751a5f2d173423e98a7e769209b1.png)'
- en: '[Figure 9.5](#figure-9-5): Training and validation metrics with a Xception-like
    architecture'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9.5](#figure-9-5)：具有类似 Xception 架构的训练和验证指标'
- en: You’ll find that our new model achieves a test accuracy of 90.8% — compared
    to 83.9% for the previous model. As you can see, following architecture best practices
    does have an immediate, sizeable effect on model performance!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现我们的新模型达到了 90.8% 的测试准确率——相比之下，前一个模型的准确率是 83.9%。正如你所看到的，遵循架构最佳实践确实对模型性能有立即和显著的影响！
- en: At this point, if you want to further improve performance, you should start
    systematically tuning the hyperparameters of your architecture — a topic we cover
    in detail in chapter 18. We haven’t gone through this step here, so the configuration
    of the previous model is purely from the best practices we outlined, plus, when
    it comes to gauging model size, a small amount of intuition.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，如果你想进一步提高性能，你应该开始系统地调整你架构的超参数——这是我们在第 18 章中详细讨论的主题。我们在这里没有进行这一步骤，所以前一个模型的配置纯粹是基于我们概述的最佳实践，以及，在评估模型大小时，一点直觉。
- en: 'Beyond convolution: Vision Transformers'
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超越卷积：视觉 Transformer
- en: 'While ConvNets have been dominating the field of computer vision since the
    mid-2010s, they’ve been recently competing with an alternative architecture: Vision
    Transformers (or ViTs for short). It may well be that ViTs will end up replacing
    ConvNets in the long term — though, for now, ConvNets remain your best option
    in most cases.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 ConvNets 自 2010 年中叶以来一直主导着计算机视觉领域，但它们最近正与一种替代架构竞争：视觉 Transformer（简称 ViTs）。很可能
    ViTs 最终会取代 ConvNets，尽管目前来说，在大多数情况下 ConvNets 仍然是你的最佳选择。
- en: 'You don’t yet know what Transformers are because we’ll cover them in chapter
    15. In short, the Transformer architecture was developed to process text — it’s
    fundamentally a sequence-processing architecture. And Transformers are very good
    at it, which has led to the question: could we also use them for images?'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你还不知道什么是 Transformer，因为我们将在第 15 章中介绍它们。简而言之，Transformer 架构是为了处理文本而开发的——它本质上是一个序列处理架构。Transformer
    在此方面非常出色，这引发了一个问题：我们是否也可以将它们用于图像？
- en: 'Because ViTs are a type of Transformer, they also process sequences: they split
    up an image into a 1D sequence of patches, turn each patch into a flat vector,
    and process the vector sequence. The Transformer architecture allows ViTs to capture
    long-range relationships between different parts of the image, something ConvNets
    can sometimes struggle with.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 ViTs 是一种 Transformer，它们也处理序列：它们将图像分割成一系列 1D 补丁，将每个补丁转换成一个平面向量，并处理向量序列。Transformer
    架构允许 ViTs 捕获图像不同部分之间的长距离关系，这是 ConvNets 有时难以做到的。
- en: Our general experience with Transformers is that they’re a great choice if you’re
    working with a massive dataset. They’re simply better at utilizing large amounts
    of data. However, for smaller datasets, they tend to be suboptimal for two reasons.
    First, they lack the spatial prior of ConvNets — the 2D patch-based architecture
    of ConvNets incorporates more assumptions about the local structure of the visual
    space, making them more data efficient. Second, for ViTs to shine, they need to
    be really large. They end up being unwieldy for anything smaller than ImageNet.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对 Transformer 的总体经验是，如果你正在处理大量数据集，它们是一个很好的选择。它们在利用大量数据方面表现得更好。然而，对于较小的数据集，由于两个原因，它们往往不是最佳选择。首先，它们缺乏
    ConvNets 的空间先验——ConvNets 的基于 2D 补丁的架构包含了更多关于视觉空间局部结构的假设，这使得它们更有效率。其次，为了使 ViTs
    发挥出色，它们需要非常大。对于小于 ImageNet 的任何东西，它们最终都会变得难以处理。
- en: The battle for image recognition supremacy is far from over, but ViTs have undoubtedly
    opened a new and exciting chapter. You’ll probably work with this architecture
    in the context of large-scale generative image models — a topic we’ll cover in
    chapter 17. For your small-scale image classification needs, however, ConvNets
    remain your best bet.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图像识别霸权的竞争远未结束，但 ViTs 无疑开启了一个新的、令人兴奋的篇章。你可能会在大规模生成图像模型的背景下使用这种架构——这是我们在第 17 章中将要讨论的主题。然而，对于你的小规模图像分类需求，ConvNets
    仍然是你的最佳选择。
- en: 'This concludes our introduction to essential ConvNet architecture best practices.
    With these principles in hand, you’ll be able to develop higher-performing models
    across a wide range of computer vision tasks. You’re now well on your way to becoming
    a proficient computer vision practitioner. To further deepen your expertise, there’s
    one last important topic we need to cover: interpreting how a model arrives at
    its predictions.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对关键卷积神经网络架构最佳实践的介绍。掌握这些原则后，你将能够开发出适用于广泛计算机视觉任务的性能更高的模型。你现在正朝着成为一名熟练的计算机视觉实践者的道路迈进。为了进一步深化你的专业知识，我们还需要讨论最后一个重要主题：解释模型如何得出其预测结果。
- en: Summary
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The architecture of a deep learning model encodes key assumptions about the
    nature of the problem at hand.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习模型的架构编码了对待解决问题本质的关键假设。
- en: The modularity-hierarchy-reuse formula underpins the architecture of nearly
    all complex systems, including deep learning models.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模块化-层次-重用公式是几乎所有复杂系统架构的基础，包括深度学习模型。
- en: Key architecture patterns for computer vision include residual connections,
    batch normalization, and depthwise separable convolutions.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机视觉的关键架构模式包括残差连接、批归一化和深度可分离卷积。
- en: Vision Transformers are an up-and-coming alternative to ConvNets for large-scale
    computer vision tasks.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视觉Transformer是卷积神经网络在大型计算机视觉任务中的新兴替代方案。
- en: Footnotes
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 脚注
- en: Kaiming He et al., “Deep Residual Learning for Image Recognition,” Conference
    on Computer Vision and Pattern Recognition (2015), [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385).
    [[↩]](#footnote-link-1)
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kaiming He 等人，“用于图像识别的深度残差学习”，计算机视觉与模式识别会议（2015年），[https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)。[[↩]](#footnote-link-1)
- en: 'Sergey Ioffe and Christian Szegedy, “Batch Normalization: Accelerating Deep
    Network Training by Reducing Internal Covariate Shift,” *Proceedings of the 32nd
    International Conference on Machine Learning* (2015), [https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167).
    [[↩]](#footnote-link-2)'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sergey Ioffe 和 Christian Szegedy，“通过减少内部协变量偏移加速深度网络训练：批归一化”，*第32届国际机器学习会议论文集*（2015年），[https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)。[[↩]](#footnote-link-2)
- en: 'François Chollet, “Xception: Deep Learning with Depthwise Separable Convolutions,”
    Conference on Computer Vision and Pattern Recognition (2017), [https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357).
    [[↩]](#footnote-link-3)'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: François Chollet，“Xception：使用深度可分离卷积的深度学习”，计算机视觉与模式识别会议（2017年），[https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357)。[[↩]](#footnote-link-3)
