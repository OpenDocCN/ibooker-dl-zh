- en: Chapter 1\. Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI is no longer the realm of science fiction novels and dystopian Hollywood
    movies. It is fast becoming an integral part of people’s lives. Most of us interact
    with AI on a daily basis, often without even realizing it.
  prefs: []
  type: TYPE_NORMAL
- en: Current progress in AI has to a large extent been driven by advances in language
    modeling. Large language models (LLMs) represent one of the most significant technological
    advances in recent times, marking a new epoch in the world of tech. Similar inflection
    points in the past include the advent of the computer that ushered in the digital
    revolution, the birth of the internet and the World Wide Web that laid the foundation
    for a hyperconnected world, and the emergence of the smartphone that reshaped
    human communication. The ongoing AI revolution is poised to make a similar transformative
    impact.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs belong to a class of models referred to as generative AI. The distinguishing
    factor is the ability of these models to generate responses to user queries, called
    *prompts*. Generative AI encompasses models that generate images, videos, speech,
    music, and of course text. While there is an increasing focus on bringing all
    these modalities together into a single model, in this book we will stick to language
    and LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will introduce language models and define what makes a language
    model *large*. We will provide a brief history of LLMs, contextualizing their
    place within the field of natural language processing (NLP) and their evolution.
    We will highlight the impact LLMs are already having in the world and showcase
    key use cases, while discussing their strengths and limitations. We will also
    introduce LLM prompting and show how to interact with an LLM effectively, either
    through a user interface or through an API. Finally, we will end this chapter
    with a quick tutorial on building a *Chat with my PDF* chatbot prototype. We will
    then discuss the limitations of the prototype and the factors limiting its suitability
    for production use cases, thus setting the stage for the rest of the book.
  prefs: []
  type: TYPE_NORMAL
- en: Defining LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A model is an approximation of a real-world concept or phenomenon. A faithful
    model will be able to make predictions about the concept it is approximating.
    A language model approximates human language and is built by training over a large
    body of text, thus imbuing it with various properties of language, including aspects
    of grammar (syntax) and meaning (semantics).
  prefs: []
  type: TYPE_NORMAL
- en: One way to train a language model is to teach it to predict the next token (this
    is equivalent to a word or a subword, but we will ignore this distinction for
    now) in a known text sequence. The model is trained over a large number of such
    sequences, and its *parameters* are updated iteratively such that it gets better
    at its predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider the following text sequence appearing in a training dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: and the language model predicts the next word that comes after “…​ and proceeded
    to walk toward the **_*”*
  prefs: []
  type: TYPE_NORMAL
- en: '*There are a large number of valid continuations to this text sequence. It
    could be “building” or “shelter,” but it could also be “embankment” or “catacomb.”
    However, it is definitely not “the” or “is,” because that would break the rules
    of the English language. After training on a sufficiently large body of text,
    the model learns that neither “the” nor “is” are valid continuations. Thus, you
    can see how a simple task like learning to predict the next word in a text sequence
    can lead the model to learning the grammar of the language in its parameters,
    as well as even more complex skills.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In practice, language models don’t exactly output a single word or subword as
    the next token in a text sequence. They output a probability distribution over
    the entire vocabulary. (We will explore how this vocabulary is defined and constructed
    in [Chapter 3](ch03.html#chapter-LLM-tokenization)). A well-trained model will
    have high probabilities for valid continuations and very low probabilities for
    invalid continuations.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 1-1](#next-token-prediction) describes the model training process in
    a nutshell. The output of the model prediction is a probability distribution over
    the entire vocabulary of the language. This is compared to the original sequence,
    and the parameters of the model are updated according to an algorithm so that
    it makes better predictions in the future. This is repeated over a very large
    dataset. We will describe the model training process in detail in the next three
    chapters.'
  prefs: []
  type: TYPE_NORMAL
- en: '![next-token-prediction](assets/dllm_0101.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-1\. Model training using next token prediction
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Is there a limit to what a model can learn from next-token prediction alone?
    This is a very important question that determines how powerful LLMs can eventually
    be. There is plenty of disagreement in the research community, with [some researchers](https://oreil.ly/sUAcl)
    arguing next-token prediction is enough to achieve human-level intelligence in
    models, and [others pointing out](https://oreil.ly/7QG-l) the shortfalls of this
    paradigm. We will come back to this question throughout the book, and especially
    in [Chapter 8](ch08.html#ch8), where we will discuss skills like reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: Modern-day language models are based on neural networks. Several types of neural
    network architectures are used to train LLMs, the most prominent being the Transformer.
    We will learn more about neural networks, Transformers, and other architectures
    in detail in [Chapter 4](ch04.html#chapter_transformer-architecture).
  prefs: []
  type: TYPE_NORMAL
- en: Language models can be trained to model not just human languages but also programming
    languages like Python or Java. In fact, the Transformer architecture and the next-token
    prediction objective can be applied to sequences that are not languages in the
    traditional sense at all, such as representations of chess moves, DNA sequences,
    or airline schedules.
  prefs: []
  type: TYPE_NORMAL
- en: For example, Adam Karvonen trained [Chess-GPT](https://oreil.ly/oluZN), a model
    trained only on chess games represented in portable game notation (PGN) strings.
    PGN strings for chess look like “1\. e4 d5 2\. exd5 Qxd5…” and so on. Even without
    providing the rules of the game explicitly, and just training the model to predict
    the next character in the PGN sequence, the model was able to learn the rules
    of the game including moves like castling, check, and checkmate; and it could
    even win chess games against experts. This shows the power of the next-token prediction
    objective and the Transformer architecture that forms the basis of the model.
    In [Chapter 4](ch04.html#chapter_transformer-architecture), we will learn how
    to train our own Chess-GPT from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Another such example is the [Geneformer](https://oreil.ly/31DXq), a model trained
    on millions of single-cell transcriptomes (representations of RNA molecules in
    a single cell), which can be used for making predictions in network biology, including
    disease progression, gene-dosage sensitivity, and therapeutic candidates.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, I encourage you to think beyond the realm of human language when
    brainstorming novel use cases for language models. If you have a concept or phenomenon
    that can be encoded in a discrete sequence using a finite vocabulary (we will
    more formally define vocabulary in [Chapter 3](ch03.html#chapter-LLM-tokenization)),
    then we can potentially train a useful model on it.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Is there something special about the structure of language that makes it amenable
    to be modeled using the next-token prediction objective? Or is the word “language”
    in language models just a historical accident and any stream of tokens can be
    modeled using this paradigm? While this is still a [topic of debate](https://oreil.ly/nJiQW)
    in the research community, directly modeling speech, video, etc. using this paradigm
    hasn’t been as effective, perhaps showing that the discrete nature of text and
    the structure provided by language, be it a human language like English, a programming
    language like Python, or a domain-specific code like DNA sequences, is crucial
    to modeling success.
  prefs: []
  type: TYPE_NORMAL
- en: Around 2019, researchers realized that increasing the size of the language model
    (typically measured by the number of parameters) predictably improved performance,
    with no saturation point in sight. This led to Kaplan et al.’s work on LLM scaling
    laws (see the following sidebar), which derives a mathematical formula describing
    the relationship between the amount of computation (henceforth referred to as
    “compute”) for training the model, the training dataset size, and the model size.
    Ever since then, companies and organizations have been training increasingly larger
    models.
  prefs: []
  type: TYPE_NORMAL
- en: There is no accepted convention about when a language model is considered “large.”
    In fact, as the largest models get even larger, some models that would have been
    designated as LLMs only a couple of years ago are now termed small language models
    (SLMs). In this book, we will remain generous and continue to refer to all language
    models over a billion parameters as “large.”
  prefs: []
  type: TYPE_NORMAL
- en: Another way in which a “large” language model differs from smaller ones is the
    emergent capabilities it possesses. First hypothesized by [Wei et al.](https://oreil.ly/RQfii),
    emergent capabilities are those capabilities exhibited by larger models but not
    smaller ones.
  prefs: []
  type: TYPE_NORMAL
- en: According to this theory, for tasks that require these capabilities, the performance
    of smaller models is close to random. However, when the model size reaches a threshold,
    the performance suddenly starts to increase with size. Examples include multi-digit
    arithmetic operations, arithmetic and logical reasoning, etc. This also suggests
    that certain capabilities that are completely absent in current models could be
    exhibited by future larger models.
  prefs: []
  type: TYPE_NORMAL
- en: These thresholds are not absolute, and as we see more advances in language modeling,
    data quality improvements, etc., we can expect the thresholds to come down.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Schaeffer et al.](https://oreil.ly/OXk6I) claim that the sudden jump in performance
    for certain tasks at a particular model size threshold is just an artifact of
    the evaluation metrics used to judge performance. This happens because many metrics
    do not assign partial credit and only reward fully solving the task, so model
    improvements might not be tracked. On the other hand, one could argue that for
    tasks like multi-step arithmetic, getting the answer partially right is just as
    useless as getting it completely wrong.'
  prefs: []
  type: TYPE_NORMAL
- en: The question of what abilities are emergent is still being explored in the research
    community. In [Chapter 5](ch05.html#chapter_utilizing_llms), we will discuss its
    implications for selecting the right model for our desired use case.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Unfortunately the phrase “emergent properties” has multiple meanings in the
    literature. In some papers, the phrase is used to describe those capabilities
    that the model is not explicitly trained for. In this book, we will stick to [Wei
    et al.’s definition](https://oreil.ly/bkVoj).
  prefs: []
  type: TYPE_NORMAL
- en: To understand how current LLMs came to be, it is instructive to walk through
    a brief history of them. As more historical details are out of scope for the book,
    we will provide links to external resources for further reading throughout the
    section.*  *# A Brief History of LLMs
  prefs: []
  type: TYPE_NORMAL
- en: To present the history of LLMs, we need to start from the history of NLP, the
    field that LLMs originated from. For a more detailed history of NLP, refer to
    Daniel Jurafsky’s seminal book, [*Speech and Language Processing*, 2nd edition](https://oreil.ly/zzU9R).
  prefs: []
  type: TYPE_NORMAL
- en: Early Years
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The field traces its origins to the 1950s, driven by demand for *machine translation*,
    the task of automatically translating from one language to another. The early
    days were dominated by symbolic approaches; these were rule-based algorithms based
    on [linguistic theories](https://oreil.ly/ELKSe) influenced by the works of linguists
    like Noam Chomsky.
  prefs: []
  type: TYPE_NORMAL
- en: In the mid-1960s, Joseph Weizenbaum released ELIZA, a chatbot program that applied
    pattern matching using [regular expressions](https://oreil.ly/rIAWY) on the user’s
    input and selected response templates to generate an output. ELIZA consisted of
    several scripts, the most famous one being DOCTOR, that simulated a psychotherapist.
    This variant would respond by rephrasing the user’s input in the form of a question,
    similar to how a therapist would. The rephrasing was performed by filling in predefined
    templates with pattern-matched words from the input.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You can try chatting with [ELIZA online](https://oreil.ly/5g0e_). Even in the
    era of ChatGPT, ELIZA can hold a somewhat convincing conversation, despite the
    fact that it is just rules-based.
  prefs: []
  type: TYPE_NORMAL
- en: Rule-based systems are brittle, hard to construct, and a maintenance nightmare.
    As the decades rolled by, the limitations of symbolic approaches became more and
    more evident, and the relative effectiveness of statistical approaches ensured
    that they became more commonplace. NLP researcher [Frederick Jelinek](https://oreil.ly/AmtvE)
    famously quipped, “Every time I fire a linguist, the performance of the speech
    recognizer goes up.”
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning–based approaches became more widely used in the 1990s and 2000s.
    Traditional machine learning relied on human-driven feature engineering and feature
    selection, the process of identifying features (characteristics of the input)
    that are predictive to solve a task. These features could be statistical, like
    the average word length, or linguistic, like parts of speech. To learn more about
    traditional statistical NLP, I recommend reading Christopher Manning’s book, [*Foundations
    of Statistical Natural Language Processing*](https://oreil.ly/MIC70).
  prefs: []
  type: TYPE_NORMAL
- en: The relevance of linguistics to modern-day NLP application development is a
    point of debate. Many university courses on NLP have completely dropped content
    related to linguistics. Even though I don’t directly use linguistics in my work,
    I find that I rely on them to develop intuitions about model behavior more than
    I expect. As such, I recommend Emily Bender’s books on [syntax](https://oreil.ly/hWR8S)
    and [semantics](https://oreil.ly/7liiS) to understand the basics of this field.
  prefs: []
  type: TYPE_NORMAL
- en: The 2010s saw the advent of deep learning and its widespread impact on NLP.
    Deep learning is characterized by multi-layer neural network models that learn
    informative features by themselves given only raw input, thus removing the need
    for cumbersome feature engineering. Deep learning forms the foundation for modern
    NLP and LLMs. To dig deeper into the principles of deep learning and neural networks,
    I recommend [Goodfellow et al.’s book](https://oreil.ly/0gv0D). For more hands-on
    deep learning training, I recommend Zhang et al.’s [*Dive into Deep Learning*](https://oreil.ly/YN_3Y).
  prefs: []
  type: TYPE_NORMAL
- en: During the early years of deep learning, it was customary to construct a task-specific
    architecture to solve each task. Some of the types of neural network architectures
    used include multi-layer perceptrons, convolutional neural networks, recurrent
    neural networks, and recursive neural networks. To learn more about this era of
    NLP, I recommend [*Neural Network Methods for Natural Language Processing*](https://oreil.ly/MCOp4)
    by Yoav Goldberg (Springer Cham).
  prefs: []
  type: TYPE_NORMAL
- en: The Modern LLM Era
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In 2017, the [Transformer architecture](https://oreil.ly/AAuvL) was invented,
    quickly followed by the invention of efficient *transfer learning* techniques
    pioneered by [Howard et al.](https://oreil.ly/E15Yn) among others and Transformer-based
    language models like [BERT](https://oreil.ly/-Yhwz). These advances removed the
    need for constructing complex task-specific architectures. Instead, one could
    use the same Transformer model to train a variety of tasks. This new paradigm
    divided the training step into two stages: *pre-training* and *fine-tuning*. An
    initial large-scale pre-training step initialized the Transformer model with general
    language capabilities. Subsequently, the pre-trained model could be trained on
    more concrete tasks, like information extraction or sentiment detection, using
    a process called fine-tuning. We will cover fine-tuning extensively throughout
    the book.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While academia and open-source collectives have made crucial and critical contributions
    to language modeling, large tech companies like OpenAI, Google, Meta, and Anthropic
    have taken the lead in training and releasing progressively larger LLMs. OpenAI
    in particular has played a pioneering role in advancing language modeling technology.
    The trajectory of the evolution of LLMs in the modern era can be traced through
    the advances ushered in by each version of the GPT (Generative Pre-trained Transformer)
    family of models trained by OpenAI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[GPT-1](https://oreil.ly/dFPSE)'
  prefs: []
  type: TYPE_NORMAL
- en: This version demonstrated unsupervised pre-training on large-scale data, followed
    by task-specific supervised fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: '[GPT-2](https://oreil.ly/JL-VO)'
  prefs: []
  type: TYPE_NORMAL
- en: This version was one of the first models to be trained on large-scale web data.
    This version also marked the rise of natural language prompting as a means of
    interacting with a language model. It showed that pre-trained models could solve
    a variety of tasks *zero-shot* (solving a task without needing any examples) without
    any task-specific fine-tuning. We will discuss zero-shot and prompting in detail
    later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '[GPT-3](https://oreil.ly/lIwad)'
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by the scaling laws, this model is a hundred times larger than GPT-2
    and popularized in-context/few-shot learning, where the model is fed with a few
    examples on how to solve a given task in the prompt, without needing to fine-tune
    the model. We will learn more about few-shot learning later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '[GPT-4](https://oreil.ly/gY1HL)'
  prefs: []
  type: TYPE_NORMAL
- en: A key aspect of this release is the *alignment training* used to make the model
    more controllable and adhere to the principles and values of the model trainer.
    We will learn about alignment training in [Chapter 8](ch08.html#ch8).
  prefs: []
  type: TYPE_NORMAL
- en: '[o1](https://oreil.ly/XJSMN)'
  prefs: []
  type: TYPE_NORMAL
- en: This is a new family of models released by OpenAI that focuses on improving
    reasoning capabilities. This is one of the first models to focus on scaling inference-time
    computation. We will discuss more about inference-time computation in [Chapter 8](ch08.html#ch8).
  prefs: []
  type: TYPE_NORMAL
- en: 'You might have noticed a trend here: through the years, the field has been
    experiencing a consolidation effect, with more and more parts of the NLP task
    pipeline being performed *end-to-end*, i.e., by a single model. Throughout this
    book, we will point out the consolidation effect where it is apparent and discuss
    its implications for the future of LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: A history of LLMs wouldn’t be complete without mentioning the impact of open
    source contributions to this field. Open source models, datasets, model architectures,
    and various developer libraries and tools have all had significant impacts on
    the development of this field. This book places a special importance on open source,
    providing a thorough survey of the open source LLM landscape and showcasing many
    open source models and datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s explore how LLMs are being adopted and their impact on society so
    far.
  prefs: []
  type: TYPE_NORMAL
- en: The Impact of LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The tech world has long been susceptible to hype cycles, with exhilarating booms
    and depressing busts. More recently, we have witnessed the crypto/blockchain and
    Web3 booms, both of which have yet to live up to their promises. Is AI heading
    toward a similar fate? We have hard evidence that it is not.
  prefs: []
  type: TYPE_NORMAL
- en: At my company Hudson Labs, we [analyzed discussions](https://oreil.ly/_mTAs)
    in the quarterly earnings calls of the 4,000 largest publicly listed companies
    in the United States to track adoption of crypto, Web3, and AI in the enterprise.
  prefs: []
  type: TYPE_NORMAL
- en: We observed that 85 companies discussed Web3 in their earnings calls, with even
    fewer tangibly working on it. Crypto fared better, with 313 companies discussing
    it. Meanwhile, LLMs were discussed and adopted by 2,195 companies, meaning that
    at least 50% of America’s largest public companies are using LLMs to drive value,
    and it is strategically so important to them to merit discussion in their quarterly
    earnings call. Effective or not, LLM adoption in the enterprise is already a reality.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 1-2](#web3) shows the number of companies discussing Web3 in their
    earnings calls over time. As you can see, the Web3 hype seems to be tapering off.'
  prefs: []
  type: TYPE_NORMAL
- en: '![web3](assets/dllm_0102.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-2\. Companies that discussed Web3 in their earnings calls across time
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Similarly, [Figure 1-3](#crypto) shows the number of companies discussing crypto/blockchain
    in their earnings calls over time. As you can see, only 5% of companies discussed
    crypto at its peak.
  prefs: []
  type: TYPE_NORMAL
- en: '![crypto](assets/dllm_0103.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-3\. Companies that discussed crypto/blockchain in their earnings calls
    across time
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Finally, let’s look at AI. As mentioned before, AI has reached levels of adoption
    in the enterprise that no other recent technology trend has managed in the recent
    past. The trend is only accelerating, as shown in [Figure 1-4](#ai-adoption),
    which shows the number of companies that were asked questions about AI by analysts
    during their earnings calls in just the first two months of the year. The sharp
    spike in 2024 shows no sign of abating.
  prefs: []
  type: TYPE_NORMAL
- en: '![ai-adoption](assets/dllm_0104.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-4\. Companies that were asked questions about AI in their earnings
    calls during the first two months of the year
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that these statistics only include generative AI/LLM adoption and not data
    science/data analytics, whose adoption is even more ubiquitous in the enterprise.
    AI adoption is also not limited to tech companies, with companies ranging from
    real estate companies to insurance firms joining in on the fun.
  prefs: []
  type: TYPE_NORMAL
- en: LLM Usage in the Enterprise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From the same analysis, we observed the key ways in which LLMs are used in
    the enterprise:'
  prefs: []
  type: TYPE_NORMAL
- en: Employee productivity
  prefs: []
  type: TYPE_NORMAL
- en: The primary means by which employee productivity has improved through LLM usage
    is with coding assistants like GitHub Copilot. LLMs are also widely used to help
    draft marketing and promotional text and automate marketing campaigns. In fact,
    the first major LLM commercial success stories were marketing startups like [Jasper
    AI](https://oreil.ly/Byw26) and [Copy.ai](https://oreil.ly/QmhJC). Another key
    LLM-driven productivity enhancement is question-answering assistants over a company’s
    extensive knowledge base drawn from heterogeneous data sources.
  prefs: []
  type: TYPE_NORMAL
- en: Report generation
  prefs: []
  type: TYPE_NORMAL
- en: These include summarizing documents, completing mundane paperwork, and even
    drafting contracts. Summarization use cases include summarizing financial reports,
    research papers, or even meeting minutes from audio or call transcripts.
  prefs: []
  type: TYPE_NORMAL
- en: Chatbots
  prefs: []
  type: TYPE_NORMAL
- en: LLM-driven chatbots increasingly are being deployed as customer service agents.
    They are also being used as an interface to a company’s documentation or product
    page.
  prefs: []
  type: TYPE_NORMAL
- en: Information extraction and sequence tagging
  prefs: []
  type: TYPE_NORMAL
- en: Over the years, a large number of enterprises have developed complex NLP pipelines
    for language processing tasks. Many of these pipelines are being fully or partially
    replaced by LLMs. These pipelines are used to solve common NLP tasks like sentiment
    analysis, information extraction tasks like entity extraction and relation extraction,
    and sequence tagging tasks like named entity recognition (NER). For a detailed
    list of NLP tasks and their descriptions, see [Fabio Chiusano’s blog](https://oreil.ly/_11rN).
  prefs: []
  type: TYPE_NORMAL
- en: Translation
  prefs: []
  type: TYPE_NORMAL
- en: Translation tasks include translating text from one language to another as well
    as tasks where text is converted to a different form but in the same language,
    for example, converting informal text to formal text, abusive text to polite text
    and so on. Real-time translation apps like [Erudite’s Instant Voice Translate
    promise](https://oreil.ly/xxENs) to make embarrassing language-barrier moments
    for tourists a thing of the past.
  prefs: []
  type: TYPE_NORMAL
- en: Workflows
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are gradually being used to facilitate workflow automation, where a sequence
    of tasks can be performed by LLM-driven software systems, called agents. Agents
    can interact with their environment (search and retrieve data, run code, connect
    to other systems) and potentially operate autonomously. We will more formally
    define agents and explore how to build them in [Chapter 10](ch10.html#ch10).
  prefs: []
  type: TYPE_NORMAL
- en: Prompting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have our fundamentals in place, let’s begin learning how to effectively
    use LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: The process by which you interact with an LLM is called prompting. Even though
    some companies attempt to anthropomorphize LLMs by giving them a name or a persona,
    it is good to remember that when you are interacting with an LLM, you are *prompting*
    them and not chatting with them as you would with a human being. Remember that
    LLMs are next-word predictors. This means that the text they generate is heavily
    dependent on the text they are fed, which includes the input (called the *prompt*)
    and the output tokens generated so far by the model. This is collectively called
    the *context*.
  prefs: []
  type: TYPE_NORMAL
- en: 'By feeding the LLM the right text in the context, you are priming it to generate
    the type of output you need. The ideal prompt would be the answer to this question:
    “What would be the best prefix of N tokens that, when fed to an LLM, will lead
    it to generate the correct answer with the highest probability?”'
  prefs: []
  type: TYPE_NORMAL
- en: As of the book’s writing, language models simply aren’t smart enough for you
    to prompt a model exactly the way you would speak to a human and expect best results.
    As language models get better over time, prompts can become more like human conversation.
    Those of you who remember the early days of search engines might recall that effectively
    using a search engine by entering the right form of queries was seen as a skill
    that is not trivial to acquire, but as search engines got better, search queries
    could become more free-form.
  prefs: []
  type: TYPE_NORMAL
- en: When I started writing this book, I solicited opinions from the target readership
    on the topics they would like covered. I received the most requests for the topic
    of prompting, with practitioners wanting to understand how to effectively create
    prompts for their specific use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Prompting is an important aspect of modern-day LLMs. In fact, you will probably
    end up spending a significant amount of your time on any LLM-based project iterating
    on prompts, very inaccurately referred to as *prompt engineering*.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There have been attempts to automatically optimize prompts, like [automatic
    prompt optimization (APO)](https://oreil.ly/SekPA) and [AutoPrompt](https://oreil.ly/upVKC).
    We will discuss this further in [Chapter 13](ch13.html#ch13).
  prefs: []
  type: TYPE_NORMAL
- en: It is important to manage one’s expectations about the effectiveness of prompt
    engineering. Prompts aren’t magical incantations that unlock hidden LLM capabilities.
    It is very unlikely that there are companies with a significant advantage over
    others just by using a superior prompting technique unknown to others. On the
    flip side, not following basic prompting principles can severely hamper the performance
    of your LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Umpteen prompting tutorials are available online. I recommend [Learn Prompting’s
    prompting guide](https://oreil.ly/CQrzi) in particular. You do not need to know
    all the prompting techniques to become well-versed in prompting. Most of what
    you need to know about prompting can be learned in a couple of hours. What matters
    more is interacting with the LLMs you use frequently to observe their outputs
    and developing intuition about their behavior.
  prefs: []
  type: TYPE_NORMAL
- en: If you have programming experience, I suggest viewing prompting through the
    lens of programming. In programming, instructions need to be explicit with no
    room for ambiguity. The challenge with prompting is that it is done in natural
    language, which is inherently ambiguous. Still, the best prompts state instructions
    that are explicit, detailed, and structured, leaving very little room for ambiguity.
    We will learn more prompting nuances in Chapters [5](ch05.html#chapter_utilizing_llms)
    and [13](ch13.html#ch13).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'A fun fact: language models are insensitive to word order. This property has
    been [observed](https://oreil.ly/gtDFg) even in [earlier models](https://oreil.ly/qI_IZ)
    like BERT. For example, ask ChatGPT or your favorite LLM provider the question
    “How do I tie my shoelaces?” in jumbled form, say “shoe tie my I how do laces?”
    ChatGPT responds with “Certainly! Here are step-by-step instructions on how to
    tie your shoelaces:…​” as if you asked a straightforward question.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s discuss a few prompting modes.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-Shot Prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the standard approach to prompting, where you provide the LLM with an
    instruction and, optionally, some input text. The term *zero-shot* refers to the
    fact that no examples or demonstrations are provided on how to solve the task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider an example where your task is to assess the sentiment expressed in
    a restaurant review. To achieve this through zero-shot prompting, you can issue
    the following prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Prompt:* Classify the given passage according to its sentiment. The output
    can be one of Positive, Negative, Neutral.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Passage: “The mashed potatoes took me back to my childhood school meals. I
    was so looking forward to having them again. NOT!”'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Sentiment:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'A good zero-shot prompt will:'
  prefs: []
  type: TYPE_NORMAL
- en: Provide the instruction in a precise and explicit manner.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe the output space or the range of acceptable outputs and output format.
    In this example, we state the output should be one of three values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prime it to generate the correct text. By ending the prompt with “Sentiment:,”
    we are increasing the probability of the LLM generating the sentiment value as
    the next token.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The better the model, the less you have to worry about getting these things
    right.
  prefs: []
  type: TYPE_NORMAL
- en: In real-world settings, your output format needs to be highly controllable in
    order for it to fit in automated systems. We will discuss more techniques for
    ensuring controllability of outputs in [Chapter 5](ch05.html#chapter_utilizing_llms).
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Prompts are sensitive to model changes. You might painstakingly construct a
    prompt that seems to work well, but you might notice that the same prompt does
    not work for a different model. In fact, the same prompt might see degraded performance
    on the same API endpoint if the underlying model is updated in the meanwhile.
    We call this *prompt drift*. It is a good idea to version control prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Few-Shot Prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our example for zero-shot prompting, the LLM was able to solve the task without
    explaining it how to solve it. This is because the task is simple and clearly
    defined. In many cases, the tasks might be not so easy to describe in natural
    language. We can then add some examples in our prompt consisting of either outputs
    or input-output pairs. While this is called few-shot learning colloquially, the
    language model is not updated in any way through this prompting technique.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of few-shot prompting:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Prompt:* A palindrome is a word that has the same letters when spelled left
    to right'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: or right to left.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Examples of words that are palindromes: kayak, civic, madam, radar'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Examples of words that are not palindromes: kayla, civil, merge, moment'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Answer the question with either *Yes* or *No*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Is the word *rominmor* a palindrome?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Answer:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Chain-of-Thought Prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you are going to learn only one prompting technique, let that be chain-of-thought
    (CoT) prompting, because it is one of the most impactful prompting techniques
    in existence.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed earlier, the context of the LLM determines the next token it predicts.
    Therefore, we need to optimize the content in the context (the user prompt + output
    tokens generated so far) to maximize the probability of the LLM generating the
    correct future tokens. One way to do this is to prompt the LLM to *think* before
    generating. This elicits the LLM to generate the process to get to the answer
    instead of directly generating the answer. This might involve breaking the input
    task into subtasks and solving them one after the other.
  prefs: []
  type: TYPE_NORMAL
- en: When the LLM is eventually at the cusp of generating the answer, it can rely
    on a more relevant context that increases its probability of generating the right
    answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Prompt:* Solve the equation. 34 + 44 + 3 * 23 / 3 * 2\. Think step by step.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'After receiving the instruction “Think step by step,” the LLM then breaks down
    the problem and solves each step sequentially:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Many LLMs solve tasks step-by-step without being explicitly prompted to do so.
    This is because they have been *instruction-tuned* to solve tasks that way. We
    will learn more about instruction-tuning in Chapters [5](ch05.html#chapter_utilizing_llms)
    and [6](ch06.html#llm-fine-tuning). LLMs that have been instruction-tuned are
    easier to prompt.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of LLMs accessible through a user interface, a hidden prompt (called
    a system prompt) by the LLM provider might apply CoT prompting to relevant user
    prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Should we add the “think step-by-step” CoT instruction for every prompt, like
    a cheat code to a game? [Sprague et al.](https://oreil.ly/3zJDC) evaluated CoT
    prompting over a wide variety of tasks and found that CoT primarily helps with
    tasks that need mathematical or logical reasoning. For tasks involving common-sense
    reasoning, they found that gains by CoT are limited. For knowledge-based tasks,
    CoT might even hurt.
  prefs: []
  type: TYPE_NORMAL
- en: Note that arithmetic and logical reasoning could also be performed by delegating
    them to external tools like symbolic solvers and code interpreters. We will discuss
    this in detail in [Chapter 10](ch10.html#ch10).
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Using CoT prompting significantly increases the number of tokens generated by
    the model to solve a task, leading to higher costs.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Chaining
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Often, your tasks need multiple steps and a large number of instructions. One
    way to go about this is by stuffing all the instructions into a single prompt.
    An alternative is to break the task into multiple subtasks and chain the prompts
    such that the output of one prompt determines the input to another. I have observed
    that prompt chaining consistently performs better than managing the entire task
    through a single prompt.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, consider the task of extracting information from the text provided
    in a form and formatting the output in a structured manner. If there are missing
    or outlier values, then some special postprocessing rules are to be applied. In
    this case, it is good practice to split the task into two prompts, with the initial
    prompt performing the information extraction and the second prompt handling the
    postprocessing of the extracted information.
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial Prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You might notice that, for some queries, the LLM declines to execute your request.
    This is because it has been specifically trained to refuse certain kinds of requests
    (We will learn how to achieve this behavior in [Chapter 8](ch08.html#ch8)). This
    kind of training, which we will call *alignment training*, is imparted to the
    model to align it with the values and preferences of the entity developing the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: For example, asking any decent LLM directly for instructions to build a bomb
    will result in a refusal. However, as of today, alignment training provides only
    a weak layer of security, as it can be bypassed by cleverly prompting the LLM,
    called *adversarial prompting*. Adversarial prompts can be generated either manually
    or using algorithms. These cleverly phrased prompts trick the LLM into generating
    a response even if it was trained not to.
  prefs: []
  type: TYPE_NORMAL
- en: These clever prompting schemes are not just useful for illicit purposes. In
    many cases, the LLM simply does not respond the way you want it to, and clever
    prompting schemes might help. These clever prompting schemes range from asking
    the LLM to adopt a specific persona to outright emotional blackmail (“If you don’t
    respond correctly to this query, many children will suffer!”). While there has
    been [some work](https://oreil.ly/q1I_7) showing that adding emotion to a prompt
    may lead to better performance, there is no hard, sustained evidence that this
    is universally effective for a given model. Thus, I would not recommend using
    these in production applications.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing LLMs Through an API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You most likely have already interacted with an LLM through a chat interface
    like ChatGPT, Gemini, or Claude. Let’s now explore how to access them using the
    API. We will use the OpenAI API as an example to access its GPT family of models.
    Most other proprietary models expose similar parameters through their API.
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT-4o mini and GPT-4o can be accessed through OpenAI’s Chat Completion API.
    Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Roles can be system, user, assistant, or tool.
  prefs: []
  type: TYPE_NORMAL
- en: The system role is used to specify an overarching prompt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The user role refers to user inputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The assistant role refers to the model responses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tool role is used to interact with external software tools.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will discuss tools in more detail in [Chapter 10](ch10.html#ch10).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: What is the difference between the system and user roles? Which instructions
    should go into the system prompt and which ones into the user prompt? System prompts
    are used for dictating the high-level overarching behavior of an LLM, like “You
    are a financial expert well versed in writing formal reports.” If you are allowing
    your users to directly interact with the LLM, then the system prompt can be used
    to provide your own instruction to the LLM along with the user request. My experiments
    have shown that it doesn’t matter much if you place your instructions in the system
    prompt versus user prompt. What does matter is the length and number of instructions.
    LLMs typically can handle only a few instructions at a time. Instructions at the
    end or the beginning of the prompt are more likely to be adhered to.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of the parameters made available by OpenAI:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n`'
  prefs: []
  type: TYPE_NORMAL
- en: The number of completions the model has to generate for each input. For example,
    if we used n = 5 in the given example, it would generate five different children’s
    stories.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For tasks with high reliability requirements, I advise generating multiple completions,
    that is, n > 1 and then using a postprocessing function (which could involve an
    LLM call) to choose the best one. This is because the LLM samples the generated
    text from a probability distribution, and in some cases the answer might be wrong/bad
    just due to an unlucky token sampling. You might have to balance this process
    against your budget limitations.
  prefs: []
  type: TYPE_NORMAL
- en: '`stop` and `max_completion_tokens`'
  prefs: []
  type: TYPE_NORMAL
- en: Used to limit the length of the generated output. `stop` allows you to specify
    end tokens that, if generated, would stop the generation process. An example stop
    sequence is the newline token. If you ask the model to adhere to a particular
    output format, like a numbered list of sentences, then to stop generating after
    a particular number of sentences have been output, you can just provide the final
    number as a stop parameter.
  prefs: []
  type: TYPE_NORMAL
- en: '`presence_penalty` and `frequency_penalty`'
  prefs: []
  type: TYPE_NORMAL
- en: Used to limit the repetitiveness of the generated output. By penalizing the
    probability for tokens that have already appeared in the output, we can ensure
    that the model isn’t being too repetitive. These parameters can be used while
    performing more creative tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '`logit_bias`'
  prefs: []
  type: TYPE_NORMAL
- en: Using `logit_bias`, we can specify the tokens whose generation probability we
    want to increase or decrease.
  prefs: []
  type: TYPE_NORMAL
- en: '`top_p` and `temperature`'
  prefs: []
  type: TYPE_NORMAL
- en: Both parameters relate to decoding strategies. LLMs produce a distribution of
    token probabilities and will sample from this distribution to generate the next
    token. There are many strategies to choose the next token to generate given the
    token probability distribution. We will discuss them in detail in [Chapter 5](ch05.html#chapter_utilizing_llms).
    For now, just remember that a higher temperature setting results in more creative
    and diverse outputs, and a lower temperature setting results in more predictable
    outputs. This [cheat sheet](https://oreil.ly/DAa66) provides some recommended
    values for various use cases.
  prefs: []
  type: TYPE_NORMAL
- en: '`logprobs`'
  prefs: []
  type: TYPE_NORMAL
- en: Provides the most probable tokens for each output token along with their log
    probabilities. OpenAI limits this to the top 20 most probable tokens. In later
    chapters, we will discuss how we can leverage `logprobs` information in various
    forms.
  prefs: []
  type: TYPE_NORMAL
- en: Strengths and Limitations of LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Developing intuition about the strengths and limitations of LLMs is a crucial
    skill in being able to build useful LLM applications. Using the information in
    this book, and with ample hands-on practice, you will be able to build that intuition.
    In general, LLMs are proficient at language tasks. You will almost never see them
    make spelling or grammar errors. They are a vast improvement over previous techniques
    for understanding user instructions and intent. They also exhibit state-of-the-art
    performance on most NLP tasks like entity and relationship extraction and NER.
    And they are particularly strong at generating code, which is where LLMs have
    arguably found their greatest success through tools like [GitHub Copilot](https://oreil.ly/qvriE).
  prefs: []
  type: TYPE_NORMAL
- en: Most LLM limitations boil down to the fact that LLMs are just not intelligent
    enough. Even state-of-the-art models suffer from significant limitations in reasoning,
    including arithmetic reasoning, logical reasoning, and common-sense reasoning.
    (We will define reasoning more formally in [Chapter 8](ch08.html#ch8).) LLMs are
    also unable to adhere to factuality, because of their lack of connection to the
    real world. Therefore, they tend to generate text that might be inconsistent with
    real-world facts and principles, colloquially called *hallucinations*. Hallucinations
    are the bane of LLMs and one of the key reasons for hesitations in adopting them.
    In [Chapter 8](ch08.html#ch8), we will dive deep into various methods to tackle
    hallucinations and address reasoning limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Tons of LLM-generated articles are being uploaded to the web daily, and many
    of them make their way to the top of search engine results. For example, for a
    short while, for the query “Can you melt eggs?”, [Google showed](https://oreil.ly/ivvv_)
    “Yes, an egg can be melted,” due to an AI-generated web page containing the incorrect
    answer. This kind of text is colloquially referred to as AI slop. Thus, there
    is a very strong incentive for search engines to accurately detect AI-generated
    text. Note that since LLMs are primarily trained on web text, future LLMs can
    be contaminated by polluted text as well.
  prefs: []
  type: TYPE_NORMAL
- en: While LLMs are frequently used to aid creative tasks, they are nowhere near
    the level of professional authors. Fiction authored by current LLMs is still unlikely
    to be a bestseller. LLM-generated text lacks the sheer ingenuity and the ability
    to evoke human emotions that human authors possess. Once you have read through
    enough LLM-generated text, it is not that difficult to spot it.
  prefs: []
  type: TYPE_NORMAL
- en: Every LLM generates text with a distinct signature, some more apparent to humans
    than others. For example, you might have noticed that ChatGPT tends to overuse
    certain words like “delve,” “tapestry,” “bustling,” etc. ChatGPT also tends to
    generate sentences with an explanatory final clause, like “He ate the entire pizza,
    indicating he was hungry.” Or “The vampire sent a thousand text messages in a
    month, suggesting effective use of digital technologies.” However, it is [extremely
    hard](https://oreil.ly/4skjI) to detect AI-generated text with 100% accuracy.
    Bad actors are also employing evasion techniques, for instance by asking another
    LLM to rephrase LLM-generated text to dilute the signature of the original LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, plagiarism detection has become even more challenging, including cases
    of students being [unfairly accused of plagiarism](https://oreil.ly/hetca) due
    to inaccurate AI-text detectors. These trends are prompting universities worldwide
    to rethink how students are evaluated, depending less on essays. Students are
    some of the heaviest users of LLM products, as shown by a [decline in ChatGPT
    usage numbers](https://oreil.ly/5xECI) during summer months.
  prefs: []
  type: TYPE_NORMAL
- en: While words like “delve” are known to be overused by LLMs, single-token frequencies
    should not be relied upon as a means of detecting LLM-generated text. Having grown
    up in India learning Indian English, the word “delve” appears in my vocabulary
    a lot more frequently than the average Westerner, and this can be found in my
    writing and publications well before the launch of ChatGPT. These nuances show
    that more robust techniques need to be developed to discover LLM-generated text.
  prefs: []
  type: TYPE_NORMAL
- en: One promising approach uses syntactic templates, a sequence of tokens having
    a particular order of part-of-speech (POS) tags, typically 5–8 tokens long. [Shaib
    et al.](https://oreil.ly/n_dXJ) show that some of these templates appear in generated
    text even when text generation strategies (also called decoding strategies, described
    in detail in [Chapter 5](ch05.html#chapter_utilizing_llms)) aimed to increase
    token diversity are used. They show that these templates are learned during the
    early stages of the pre-training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example template is:'
  prefs: []
  type: TYPE_NORMAL
- en: 'VBN IN JJ NNS: VBN (Past Participle Verb) + IN (Preposition) + JJ (Adjective)
    + NNS (Plural Noun).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples of phrases that follow this template include:'
  prefs: []
  type: TYPE_NORMAL
- en: Engaged in complex tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trained in advanced techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Entangled in deep emotions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Immersed in vivid memories
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have you noticed any LLMs frequently using or overusing this template?
  prefs: []
  type: TYPE_NORMAL
- en: Building Your First Chatbot Prototype
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s get into the weeds and start building!
  prefs: []
  type: TYPE_NORMAL
- en: Over the last couple of years, a healthy ecosystem of libraries has made experimenting
    and prototyping LLM applications much easier. In fact, you can build a *Chat with
    your PDF* question-answering chatbot in about a hundred lines of code!
  prefs: []
  type: TYPE_NORMAL
- en: Let’s implement a simple application that allows the user to upload a PDF document
    and provides a chat interface through which the user can ask questions about the
    PDF content and receive conversational responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'The intended workflow for this application is:'
  prefs: []
  type: TYPE_NORMAL
- en: The user uploads a PDF of their choice through the user interface.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The application parses the PDF using a PDF parsing library and splits the extracted
    text into manageable chunks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The chunks are converted into vector form, called embeddings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When a user issues a query through the chat interface, the query is also converted
    into vector form.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The vector similarity between the query vector and each of the chunk vectors
    is calculated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The text corresponding to the top-k most similar vectors are retrieved.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The retrieved text is fed, along with the query and any other additional instructions,
    to an LLM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The LLM uses the given information to generate an answer to the user query.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The response is displayed on the user interface.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The user can now respond (clarification question, new question, gratitude, etc.).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The entire conversation history is fed back to the LLM during each turn of the
    conversation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Figure 1-5](#chatbot-prototype) illustrates this workflow.'
  prefs: []
  type: TYPE_NORMAL
- en: '![chatbot-prototype](assets/dllm_0105.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-5\. Workflow of a chatbot application
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s begin by installing the required libraries. For this setup, we are going
    to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[LangChain](https://oreil.ly/g833p)'
  prefs: []
  type: TYPE_NORMAL
- en: This very popular framework enables building LLM application pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: '[Gradio](https://oreil.ly/XHqfT)'
  prefs: []
  type: TYPE_NORMAL
- en: This library allows you to build LLM-driven user interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: '[Unstructured](https://oreil.ly/sIFEX)'
  prefs: []
  type: TYPE_NORMAL
- en: This is a PDF parsing suite that supports a variety of methods for extracting
    text from PDFs.
  prefs: []
  type: TYPE_NORMAL
- en: '[Sentence Transformers](https://oreil.ly/UyN1k)'
  prefs: []
  type: TYPE_NORMAL
- en: This library facilitates embeddings generation from texts.
  prefs: []
  type: TYPE_NORMAL
- en: '[OpenAI](https://oreil.ly/zbroe)'
  prefs: []
  type: TYPE_NORMAL
- en: This API provides access to the GPT family of models from OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the required libraries and functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s implement the PDF loading and parsing function. LangChain supports
    several PDF parsing libraries. PDF parsing can be performed in a variety of ways,
    including using LLMs. For this example, we will choose the Unstructured library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `data` variable contains the parsed PDF that has been split into paragraphs.
    We will refer to each paragraph as a chunk. Each chunk is now converted into its
    vector representation using an embedding model. LangChain supports a wide variety
    of embedding models. For this example, we will use the *all-MiniLM-L6-V2* embedding
    model, available through the Hugging Face platform:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have loaded the embedding model, we can generate the vectors from
    the data and store them in a vector database. Several vector database integrations
    are available on LangChain. We will use Chroma for this example, as it is the
    simplest to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The vector database is ready with the vectors! We can ask queries and get responses.
    For instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This code retrieves the paragraph in the PDF whose vector is most similar to
    the vector representing the user query. Since vectors encode the meaning of the
    text, this means that the paragraph representing the similar vector has content
    similar to the content of the query.
  prefs: []
  type: TYPE_NORMAL
- en: Note that it is not guaranteed that the paragraph contains the answer to the
    query. Using embeddings, we can only get text that is similar to the query. The
    matched text need not contain the answer or even be relevant to answering the
    query.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will depend on the LLM to distinguish between irrelevant and relevant context.
    We provide the LLM with the query and the retrieved text and ask it to answer
    the query given the provided information. This workflow can be implemented using
    a `chain` in LangChain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the `ConversationalRetrievalChain`, which supports the following workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: Takes the previous conversational history, if it exists, and the current response/query
    from the user and creates a standalone question.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Uses a chosen retrieval method to retrieve top-k most similar chunks to the
    question.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Takes the retrieved chunks, the conversational history, the current user/response
    query, and instructions and feeds it to the LLM. The LLM generates the answer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can call the chain and append the result to the chat history:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Our chatbot is ready. Let’s wrap it up by connecting it with a user interface.
    We will use [Gradio](https://oreil.ly/dzYJv), a lightweight Python framework for
    building LLM-driven user interfaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We need some more code for writing the event handlers that wait for user events.
    Refer to the full code on the book’s [GitHub repo](https://oreil.ly/llm-playbooks).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we initialize the application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Our chatbot application is ready!
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Why can’t we feed the entire PDF to the LLM instead of breaking it down into
    chunks and retrieving only the relevant information? This depends on the maximum
    effective context length supported by the LLM, which limits the size of the input
    it can accept. Larger models support context lengths large enough to fit several
    PDFs in the input, in which case you may not need to perform the chunking and
    embedding process at all.
  prefs: []
  type: TYPE_NORMAL
- en: From Prototype to Production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Is building LLM applications that easy? Unfortunately, no. We have built a prototype,
    and a decent one at that. For many noncritical use cases, the performance of this
    application might even be sufficient. However, a large number of use cases demand
    accuracy and reliability guarantees that this application is not able to meet.
    This book aims to address the gap between prototype and production.
  prefs: []
  type: TYPE_NORMAL
- en: In the prototype tutorial, we treated LLMs as a black box. But if you are building
    serious applications using LLMs, it is important to understand what happens under
    the hood, even if you might never train an LLM yourself. Therefore, in Chapters
    [2](ch02.html#ch02), [3](ch03.html#chapter-LLM-tokenization), and [4](ch04.html#chapter_transformer-architecture),
    we will walk through each of the ingredients that go into making an LLM and show
    how they are trained. Developing a strong understanding of what LLMs are made
    of and how they are trained will come in handy when debugging failure modes.
  prefs: []
  type: TYPE_NORMAL
- en: In the tutorial, we used a proprietary LLM from OpenAI, without putting much
    thought into whether it is the optimal LLM to use for the application. Today,
    hundreds or even thousands of LLMs are available for commercial use. In [Chapter 5](ch05.html#chapter_utilizing_llms),
    we will explore the LLM landscape, covering both open source and proprietary models,
    the relevant dimensions along which models differ, and how to choose the right
    model that satisfies the criteria for a given use case. For example, one of the
    criteria for our PDF chatbot might be to operate within a severe budgetary restriction.
    We will learn how to evaluate LLMs and assess their limitations and capabilities
    for a given use case, develop evaluation metrics and benchmark datasets, and understand
    the pitfalls involved in both automated evaluation and human evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: What if the PDFs we intend to upload to the PDF chatbot belong to a specialized
    domain that the LLM doesn’t seem to be adept at? What if the LLM is unable to
    follow the instructions in user queries? We might need to update the model’s parameters
    by fine-tuning it over data from the specialized domain. In [Chapter 6](ch06.html#llm-fine-tuning),
    we will introduce model fine-tuning, understand the scenarios in which it might
    be useful, and demonstrate how to construct a fine-tuning dataset.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible that standard fine-tuning might not be suitable for our purposes.
    Maybe it is too expensive or ineffective. In [Chapter 7](ch07.html#ch07), we will
    learn about techniques like parameter-efficient fine-tuning that update only a
    small subset of the model’s parameters.
  prefs: []
  type: TYPE_NORMAL
- en: We may notice that our chatbot is hallucinating, or that it is having difficulty
    answering questions because of faulty reasoning. In [Chapter 8](ch08.html#ch8),
    we will discuss methods for detecting and mitigating hallucinations as well as
    methods for enhancing reasoning capabilities, including various inference-time
    compute techniques.
  prefs: []
  type: TYPE_NORMAL
- en: A production-grade PDF chatbot will need to satisfy a lot of nonfunctional requirements,
    including minimizing latency (the time the user needs to wait for the model response)
    and cost. In [Chapter 9](ch09.html#ch09), we will discuss techniques for inference
    optimization, including caching, distillation, and quantization.
  prefs: []
  type: TYPE_NORMAL
- en: We may want to extend functionality of our chatbot by connecting the LLM to
    code interpreters, databases, and APIs. We might also want the chatbot to answer
    complex queries that need to be broken into multiple steps. In [Chapter 10](ch10.html#ch10),
    we’ll explore how to interface LLMs with external tools and data sources and enable
    LLMs to break down tasks, make autonomous decisions, and interface with their
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: In the tutorial, we demonstrated a rudimentary method to parse, chunk, and embed
    documents. But during usage, we might notice that the vector similarity measures
    might be ineffective and often return irrelevant document chunks. Or that the
    retrieved chunks do not contain all the information to answer the query. In [Chapter 11](ch11.html#chapter_llm_interfaces),
    we will explore embeddings in more detail and learn how to fine-tune our own embeddings.
    We will also show how to more effectively chunk data.
  prefs: []
  type: TYPE_NORMAL
- en: The PDF chatbot follows a paradigm called retrieval-augmented generation (RAG).
    RAG refers to systems where LLMs are connected to external data sources, like
    the PDFs uploaded by users in our chatbot use case. In [Chapter 12](ch12.html#ch12),
    we will define a comprehensive RAG pipeline and learn how to architect robust
    RAG systems.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in [Chapter 13](ch13.html#ch13) we will discuss design patterns and
    programming paradigms for developing LLM applications.
  prefs: []
  type: TYPE_NORMAL
- en: These topics and more will be covered in the rest of the book. I am excited
    to go on this journey with you, hopefully providing you with the tools, techniques,
    and intuition to develop production-grade LLM applications!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced language models, provided a brief history, and
    discussed the impact they are already having on the world. We showed how to effectively
    interact with the model using various prompting techniques. We also gave an overview
    of the strengths and limitations of language models. We showed how easy it is
    to build prototype applications and highlighted the challenges involved in taking
    them to production. In the next chapter, we will begin our journey into the world
    of LLMs by introducing the ingredients that go into making an LLM.*
  prefs: []
  type: TYPE_NORMAL
