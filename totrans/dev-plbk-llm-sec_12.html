<html><head></head><body><section data-pdf-bookmark="Chapter 12. A Practical Framework for Responsible AI Security" data-type="chapter" epub:type="chapter"><div class="chapter" id="a_practical_framework_for_responsible_ai_security">&#13;
      <h1><span class="label">Chapter 12. </span>A Practical Framework for <span class="keep-together">Responsible AI Security</span></h1>&#13;
      <blockquote data-type="epigraph" epub:type="epigraph">&#13;
        <p><a contenteditable="false" data-primary="AI security framework" data-type="indexterm" id="ch12.html0"/>The future is already here—it’s just not evenly distributed.</p>&#13;
        <p data-type="attribution">William Gibson, author of <em>Neuromancer</em> and inventor of the term “cyberspace”</p>&#13;
      </blockquote>&#13;
      <p>In 1962, the final installment of a then-obscure comic anthology series unveiled what would become one of the world’s most adored superheroes. <em>Amazing Fantasy</em> issue #15 marked the debut of Spider-Man, a character who, according to a <a href="https://oreil.ly/IDnD3">2022 CNN story</a>, has ascended to become the world’s most famous superhero. But what propelled Spider-Man to this esteemed status? The answer lies in the compelling message woven into his origin story.</p>&#13;
      <p>In this inaugural tale, Peter Parker is a high school introvert whose life is forever changed after being bitten by a radioactive spider. Suddenly equipped with remarkable powers—superhuman strength, agility, and the ability to spin webs—Peter adopts the alias of Spider-Man and steps into the limelight as a costumed hero. However, his early indifference to the broader implications of his actions leads to a personal tragedy that costs the life of his beloved Uncle Ben. This pivotal moment brings Peter to a critical realization, encapsulated in the now-iconic phrase, “With great power comes great responsibility.”</p>&#13;
      <p>Just as Peter Parker was thrust into a world of great power and consequent responsibility, practitioners in the AI field are navigating an era of unprecedented technological acceleration. The rapid evolution of AI and LLMs, while unlocking the immense potential for innovation and advancement, also amplifies the responsibility of those who wield these technologies. Ensuring their safety and security is a technical challenge and a moral imperative. The narrative of Spider-Man serves as a poignant reminder that with the great power bestowed by these advanced technologies comes a critical responsibility to use them wisely, ethically, and with a keen awareness of their impact on society and individual lives. As we stand on the brink of AI’s vast potential, we must heed the lesson encapsulated in Peter Parker’s journey: to embrace our responsibilities and ensure that our technological advancements foster benefits, not detriments.</p>&#13;
      <p>As we embark on this chapter, our journey mirrors the ever-expanding universe of AI and LLM technologies—where the bounds of possibility are constantly redrawn. Our purpose here is twofold. Firstly, we aim to examine the trends marking the acceleration of these powerful technologies. The velocity at which AI and LLMs advance is reshaping our tools and methodologies, as well as redefining our ethical and <span class="keep-together">security landscapes.</span> By examining these trends, we seek to understand the pace of technological advancement and its broader role in responsible, secure AI application development.</p>&#13;
      <p>Secondly, this chapter endeavors to arm the reader with a robust framework for the safe, secure, and responsible use of AI and LLM technologies. This framework, which I call RAISE, is intended to wrap together all the concepts you’ve learned earlier in the book and make them easier to apply. By offering insights into best practices, ethical considerations, and security measures, we aim to empower you to harness the power of AI and LLMs with a conscientious and informed approach.</p>&#13;
      <section data-pdf-bookmark="Power" data-type="sect1"><div class="sect1" id="power_idEG6gIV">&#13;
        <h1>Power</h1>&#13;
        <p><a contenteditable="false" data-primary="AI security framework" data-secondary="power" data-type="indexterm" id="ch12.html1"/>Let’s start by looking at the trends pushing forward capabilities of LLMs. We have recently perceived a spike in the capabilities of AI systems, as evidenced by the rush of new applications and investments. But is this a onetime spike that is now in the past, or are we still in the early phases of an exponential curve that will multiply both the power of and risks associated with these systems?</p>&#13;
        <p>I started my first AI software company in the early 1990s. It was called Emergent Behavior, which I still think is a super cool name for an AI software company. It doesn’t exist anymore, but I think telling you a bit about that experience will help illustrate the technology acceleration happening in AI-capable hardware.</p>&#13;
        <p>In the 1990s, my team built software with genetic algorithms and neural networks. Our software was capable of doing real-world work. We successfully sold it to massive investment banks building arbitrage trading strategies and to Fortune 500 manufacturing companies optimizing their factory floor layouts. However, ultimately, the meager computing power and memory to which we had access meant we were severely constrained. We just couldn’t accomplish most of the grand tasks we had in mind.</p>&#13;
        <p>The most powerful computer I had access to back in those days was a Macintosh IIfx. It included a Motorola 68030 processor with a clock speed best measured in megahertz. My computer had 16 megabytes of RAM. Today’s processors run in gigahertz, not megahertz, and the memory is in gigabytes instead of megabytes. That mega to giga change alone implies a ~1,000x improvement. But clock speed isn’t the only improvement, and Moore’s law implies clever chip designers should have been able to provide a doubling of overall computing power every two years. That would give us a 64,000-fold increase in speed over that period.</p>&#13;
        <p>An improvement of 64,000 fold sounds impressive—and it is. But even that is not nearly enough to account for the explosion in capabilities we’ve seen in that period. It simply wouldn’t have given us enough computing power to train and run today’s LLMs. There is something else going on here. Two other converging trends enabled this: GPUs and Cloud Computing.</p>&#13;
        <section data-pdf-bookmark="GPUs" data-type="sect2"><div class="sect2" id="gpus_idnIiLb9">&#13;
          <h2>GPUs</h2>&#13;
          <p><a contenteditable="false" data-primary="AI security framework" data-secondary="power" data-tertiary="GPUs" data-type="indexterm" id="id652"/><a contenteditable="false" data-primary="GPUs" data-type="indexterm" id="id653"/>In the late 1990s, the need for games to render more polygons at faster frame rates led to the development of special graphics processing units (GPUs) by companies like 3dfx, ATI Technologies, and Nvidia. These companies built GPU architectures to handle massive numbers of parallel math operations to compute 3D spatial relationships. While this was fantastic for games, it is also just the right recipe for accelerating neural networks, which need the exact same kind of support.</p>&#13;
          <p>In my early 1990s AI startup, my Mac IIfx had a Motorola 68882 math coprocessor alongside its regular CPU. This coprocessor speeds up the types of floating-point math operations you’d need for gaming or AI, in addition to spreadsheets and other more mundane applications. The 68882 was the same coprocessor design used in machines from expensive, top-of-the-line workstation vendors like Sun Microsystems and was one of the fastest chips available at the time. It was rated at 422,000 floating-point operations per second (kFLOPS). That sounds like a lot, but it just wasn’t enough to make practical the kinds of AI tasks we wanted to accomplish.</p>&#13;
          <p>How much faster is a modern server than my old workstation? While Moore’s law would imply that a new server might be ~64,000 times faster than my old workstation, the architecture of GPUs changes the game for the operations you need for AI applications. Today, a top-of-the-line GPU is an NVIDIA H100, rated at 60 trillion floating-point operations per second (teraflops). Let’s do some math:</p>&#13;
        &#13;
        <div data-type="equation">&#13;
<math alttext="Speed Increase equals StartFraction NVIDIA upper H 100 FLOPS Over Motorola 68882 FLOPS EndFraction">&#13;
  <mrow>&#13;
    <mtext>Speed</mtext>&#13;
    <mspace width="4.pt"/>&#13;
    <mtext>Increase</mtext>&#13;
    <mo>=</mo>&#13;
    <mfrac><mrow><mtext>NVIDIA</mtext><mspace width="4.pt"/><mtext>H100</mtext><mspace width="4.pt"/><mtext>FLOPS</mtext></mrow> <mrow><mtext>Motorola</mtext><mspace width="4.pt"/><mtext>68882</mtext><mspace width="4.pt"/><mtext>FLOPS</mtext></mrow></mfrac>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
&#13;
          <p>The NVIDIA H100 GPU is approximately 142,180,095 times faster than the Motorola 68882 math coprocessor! This staggering increase highlights the monumental strides made in chip computational capabilities, which underpin the current advancements in AI and machine learning technologies. That mind-boggling speed increase shows that we are on a massively accelerating hardware curve for AI-capable hardware. The curve over that time period is over 2,000 times steeper than even the exponential Moore’s law curve would have predicted! </p>&#13;
          <p>One hundred forty-two million times is a shockingly significant improvement: what the modern GPU can compute in a single second would have taken 4.5 years on my old workstation’s coprocessor! But it’s still not enough computing power to account for the explosion we’ve seen. We need cloud computing to complete the picture.</p>&#13;
          <div data-type="note" epub:type="note"><h6>Note</h6>&#13;
            <p>Recently, publications by Taiwan Semiconductor Manufacturing Company (TSMC), which fabricates many of the world’s GPUs, say the company expects to see as much as another one million times improvement in computational performance/watt of electricity over the next 10 to 15 years, with performance tripling every 2 years.</p>&#13;
          </div>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Cloud" data-type="sect2"><div class="sect2" id="cloud_idT0uzoe">&#13;
          <h2>Cloud</h2>&#13;
          <p><a contenteditable="false" data-primary="AI security framework" data-secondary="power" data-tertiary="cloud computing" data-type="indexterm" id="id654"/><a contenteditable="false" data-primary="cloud computing services" data-secondary="AI expansion facilitated by" data-type="indexterm" id="id655"/>The other trend we need to account for is the cloud. Even the massive speed improvement on the single-system hardware curve isn’t enough to enable today’s sudden AI boom. </p>&#13;
          <p>In 2006, most people knew Amazon as an online seller of books, CDs, and DVDs. The introduction of Amazon Web Services (AWS) surprised everyone and popularized the idea of on-demand, pay-as-you-go cloud computing. Cloud is so pervasive today that I don’t need to explain the concept to you, but I will remind you what it means to AI.</p>&#13;
          <p>Today, whether you’re using AWS, Microsoft Azure, or Google Cloud Platform (GCP), you can access on-demand clusters of GPU-enabled servers with nearly limitless memory attached to ultrafast networks. You can set up massive clusters in minutes if you have enough money in your account. The companies that are training today’s foundation models see such a high potential return on investment that they are willing to pay massive cloud computing bills. It’s been <a href="https://oreil.ly/hAsfW">widely reported</a> that OpenAI spent approximately $100,000,000 on cloud resources to train GPT-4.</p>&#13;
          <p>I don’t believe we’re yet at the limits. In February 2024, Nvidia CEO Jensen Huang and OpenAI CEO Sam Altman were in the news. Huang said the world will quickly build a trillion dollars’ worth of new data centers to power AI software, and reports say that OpenAI’s Sam Altman is looking to raise seven trillion dollars to develop and build new AI chips. We’ve now entered an era where investments in AI hardware will be measured in trillions of dollars, ensuring we will see continued increases in computing power applied to these models.</p>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Open Source" data-type="sect2"><div class="sect2" id="open_source">&#13;
          <h2>Open Source</h2>&#13;
          <p><a contenteditable="false" data-primary="AI security framework" data-secondary="power" data-tertiary="open source" data-type="indexterm" id="ch12.html2"/><a contenteditable="false" data-primary="large language model Meta AI (LLaMA)" data-type="indexterm" id="ch12.html3"/><a contenteditable="false" data-primary="open source LLM technologies" data-type="indexterm" id="ch12.html4"/>Another accelerant of capabilities and risk is the rise of open source LLM technologies. November 30, 2022, is often celebrated for the release of ChatGPT, when OpenAI introduced most of the world to LLM technology. However, February 24, 2023, may hold even more significance in the annals of LLM technology due to Facebook/Meta’s release of the <em>Large Language Model Meta AI</em> (LLaMA, now usually written Llama). </p>&#13;
          <p>Meta’s press release professed a commitment to open science, highlighting the release of LLaMA as a step in enabling broader access to state-of-the-art AI technologies. LLaMA is provided in multiple sizes to cater to various research needs, from validating new approaches to exploring novel use cases. By offering smaller, more efficient models, Meta aimed to lower the barrier to entry into the LLM space, allowing researchers with limited resources to contribute to and innovate within this rapidly evolving field. </p>&#13;
          <p>While Meta’s initial approach to releasing LLaMA aimed to democratize access to cutting-edge AI technology, there was a sense of caution. The company recognized the transformative potential of making such powerful models more accessible, but was equally aware of the risks associated with their misuse. Meta opted for a controlled release under a noncommercial license to navigate this delicate balance, making LLaMA accessible only to researchers at academic institutions, government agencies, and nongovernmental organizations who met specific criteria. Meta intended to foster responsible innovation while mitigating the dangers of widespread access to such potent technology.</p>&#13;
          <p>Despite these precautions, the situation took an unexpected turn. Just a week after LLaMA was released to selected researchers, the model found its way onto the internet via a leak on 4chan (the same hacker forum that launched the attack on Tay we detailed in <a data-type="xref" href="ch01.html#chatbots_breaking_bad">Chapter 1</a>). The leak quickly spiraled out of control, with users redistributing LLaMA across various platforms, including GitHub and Hugging Face. Meta’s efforts to contain the spread through takedown requests proved futile; the model had already disseminated too widely and rapidly.</p>&#13;
          <p>Faced with LLaMA’s uncontrollable proliferation, the company decided to reassess its stance and ignore its initial trepidation about the risk of widely distributing open LLM technology. In a move that marked a significant shift from its original restrictive licensing approach, Meta eventually released LLaMA under a more liberal license, making it available to anyone. </p>&#13;
          <p>Following this episode, Meta continued to push forward. The company introduced LLaMA 2, a more advanced version of the original model, alongside specialized variants like Llama Chat and Code Llama. These subsequent releases underscore Meta’s commitment to advancing the field of AI, albeit with a nuanced understanding of the complexities involved in managing the distribution of powerful technological tools in an open and interconnected digital landscape. This evolution in Meta’s approach highlights a pivotal moment in the discourse on the democratization of AI technology, underscoring the tension between innovation and the imperative to ensure the responsible use of AI.</p>&#13;
          <p>Numerous other high-quality, open source LLMs have emerged in this rapidly evolving landscape, including BLOOM, MPT, Falcon, Vicuna, and Mixtral. Among these, Mixtral stands out for its innovative approach and technological advancements.</p>&#13;
          <p><a contenteditable="false" data-primary="Mixtral" data-type="indexterm" id="id656"/><a contenteditable="false" data-primary="SMoE (sparse mixture of experts) model" data-type="indexterm" id="id657"/><a contenteditable="false" data-primary="sparse mixture of experts (SMoE) model" data-type="indexterm" id="id658"/>Mixtral-8x7B showcases a high-quality sparse mixture of experts (SMoE) model. This development represents a significant technological leap forward, offering open weights and licensing under the permissive Apache 2.0 license. According to the development team, Mixtral has demonstrated superior performance to LLaMA 2 70B across most benchmarks, achieving up to six times faster inference times, and either matches or surpasses the capabilities of OpenAI’s GPT-3.5 on most standard benchmarks. It is now considered one of the most robust open-weight models available under a permissive license.</p>&#13;
          <div data-type="note" epub:type="note"><h6>Note</h6>&#13;
            <p>SMoE is a type of LLM architecture designed to improve efficiency and scalability. It allows a model to learn different parts of the input space using specialized “expert” subnetworks.</p>&#13;
          </div>&#13;
          <p>The shift toward open source models marks a significant step in accelerating technological progress. With this change, the capabilities once reserved for major corporations are now accessible to a wider audience, including scientists, researchers, and small companies. This broader access will drive innovation, as demonstrated by projects like Mixtral. The sharing of state-of-the-art technology like this means the base science of LLM technology will continue to benefit from academic and commercial research in the coming years, with no single organization able to monopolize it and slow progress.</p>&#13;
          <p>However, the open source nature of these technologies also means they are being used by malicious actors, including thieves, terrorists, and countries like Russia, China, and North Korea. This reality undermines the effectiveness of public pressure and regulations aimed at a handful of organizations like OpenAI and Google in controlling the proliferation and misuse of LLM and AI technologies. The technology has become too widespread to restrict its use to only beneficial purposes. The genie is out of the bottle, and there’s no putting it back.<a contenteditable="false" data-primary="" data-startref="ch12.html4" data-type="indexterm" id="id659"/><a contenteditable="false" data-primary="" data-startref="ch12.html3" data-type="indexterm" id="id660"/><a contenteditable="false" data-primary="" data-startref="ch12.html2" data-type="indexterm" id="id661"/></p>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Multimodal" data-type="sect2"><div class="sect2" id="multimodal">&#13;
          <h2>Multimodal</h2>&#13;
          <p><a contenteditable="false" data-primary="AI security framework" data-secondary="power" data-tertiary="multimodal" data-type="indexterm" id="ch12.html5"/><a contenteditable="false" data-primary="multimodal LLM technologies" data-type="indexterm" id="ch12.html6"/>Text-to-image models such as DALL-E, Midjourney, and Stable Diffusion have quickly revolutionized how many people approach visual creative endeavors. In January 2021, OpenAI’s DALL-E was the first to make waves by introducing the ability to generate complex images from textual descriptions. This model, a variant of the GPT-3 LLM, showcased the potential of combining natural language processing with image generation, setting a precedent for the kind of creative possibilities that AI could unlock. </p>&#13;
          <p>Following DALL-E, the commercial service Midjourney began its open beta in July 2022, offering a unique approach to image generation. Operated through a Discord bot, Midjourney allowed users to create images from text prompts, emphasizing an interactive and community-centric creation model. </p>&#13;
          <p>The field of text-to-image took another turn with the release of the open source Stable Diffusion project in August 2022. As an open source model, Stable Diffusion made high-quality image generation accessible to a broader audience, allowing anyone with consumer grade hardware to generate detailed visuals from textual <span class="keep-together">descriptions.</span> </p>&#13;
          <p>Progress has been astonishingly rapid in this area. In just a few short years, we have evolved from the early images, characterized by easily identifiable flaws (such as creepy, inaccurately rendered fingers), to the creation of photorealistic images that challenge our ability to distinguish them from actual photographs. </p>&#13;
          <p>This era of hyperrealistic AI-generated content has given rise to computer-generated Instagram influencers, exemplified by Aitana Lopez, who command substantial online followings and earn significant income, often without their fans realizing they are not real people. These virtual influencers, created entirely through advanced generative models, mark a new phase in digital culture. They highlight not only the capabilities of AI to produce content that resonates with human audiences, but also raise profound questions about authenticity, identity, and the nature of influence in the digital age.</p>&#13;
          <p>When I started writing this book in 2023, accessing text-to-image models was challenging. It often required you to set up complex accounts (as with Midjourney) or have access to high-end hardware (for open source Stable Diffusion). Today, the mainline chatbots from OpenAI and Google are multimodal, treating text and images interchangeably. They can read text from uploaded images and generate new photorealistic images from a simple prompt—all as part of the same conversation. This integration with mainstream chatbots means the bar to access this technology has dropped to where almost anyone can use it—for good or bad! </p>&#13;
          <p class="pagebreak-before">In February 2024, OpenAI announced Sora, a text-to-video model that creates incredibly realistic videos from short prompts. Shortly thereafter, in April 2024, <a href="https://oreil.ly/I6-pX">Microsoft announced</a> a new AI model called VASA that can create “lifelike talking faces of virtual characters with appealing visual affective skills (VAS), given a single static image and a speech audio clip.” With other open source text-to-video models being rapidly developed, we’re about to enter an age where the very nature of what’s real will be challenged. Recently, a company in Hong Kong lost $25 million when an employee was duped on a Zoom call by speaking to a deep fake of the company’s CFO. We’re about to enter a world where anyone can instantly and cheaply create a sophisticated deepfake video. It’s not hard to imagine that <em>The Matrix</em> is not far behind.</p>&#13;
          <div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
            <p>If your LLM application is multimodal and can read text from images or video, you’re opening up a whole new world of vulnerabilities. Consider that prompt injection attacks can now be launched by including malicious text in an image fed into your model as a prompt. Or your training data could be poisoned if you include images with text that mislead your model. These are just more vectors to watch for!<a contenteditable="false" data-primary="" data-startref="ch12.html6" data-type="indexterm" id="id662"/><a contenteditable="false" data-primary="" data-startref="ch12.html5" data-type="indexterm" id="id663"/></p>&#13;
          </div>&#13;
        </div></section>&#13;
        <section data-pdf-bookmark="Autonomous Agents" data-type="sect2"><div class="sect2" id="autonomous_agents">&#13;
          <h2>Autonomous Agents</h2>&#13;
          <p><a contenteditable="false" data-primary="AI security framework" data-secondary="power" data-tertiary="autonomous agents" data-type="indexterm" id="id664"/><a contenteditable="false" data-primary="Auto-GPT" data-type="indexterm" id="id665"/><a contenteditable="false" data-primary="autonomous agents, in AI security framework" data-type="indexterm" id="id666"/>Just a few months after the introduction of ChatGPT, Auto-GPT was launched in March 2023 by Toran Bruce Richards of the software development company Significant Gravitas. Built on OpenAI’s GPT-4, Auto-GPT introduced the concept of autonomy, allowing LLM-powered agents to act toward a goal with minimal human guidance. This feature enabled Auto-GPT to generate prompts to achieve a user-defined goal autonomously, differentiating it from ChatGPT’s requirement for continuous human input. The Auto-GPT framework introduces expanded short-term memory capabilities, allowing agents to connect to the internet and call upon third-party services.</p>&#13;
          <p>The introduction of Auto-GPT generated massive buzz at the time, quickly gaining traction and generating substantial discussion for its approach to AI autonomy. Thousands of users rapidly adopted the tool for various projects, leveraging its ability to tackle more complex tasks than ChatGPT could handle alone. This included creating and using unsupervised agents for software development, business operations, financial transactions, and even health care–related tasks.</p>&#13;
          <p>The adoption of Auto-GPT faced challenges due to its architectural design and the operational costs associated with its inefficient use of OpenAI’s expensive API resources. The buzz around Auto-GPT soon died out. However, this isn’t the end of the story of autonomous agents built on LLMs.</p>&#13;
          <p>In the wake of Auto-GPT, dozens of other open source and research projects have taken up that mantle, and we’ll surely see fast progress in making these concepts more generalizable and less expensive. Beyond that, mainstream players like OpenAI have introduced concepts like plug-ins that allow their LLMs to interact directly with third-party internet resources. These goal-completion-seeking, autonomous agent architectures already show massive potential in many applications. With the desire to use AI in this fashion, we’ll undoubtedly see rapid investment and progress.</p>&#13;
          <p>However, the most critical lesson from Auto-GPT was the incredibly rapid pace at which it was deployed in the wild with little to no oversight. We discussed excessive agency back in <a data-type="xref" href="ch07.html#trust_no_one">Chapter 7</a>: putting unsupervised power in the hands of a naive AI, with few guardrails in place, could be incredibly dangerous—and few stopped to think about it. The development community’s overall lack of caution shown in the rapid adoption of the technology demonstrates with some certainty that we must put better security and safety measures in place before the next leap in self-directed autonomous systems. We can’t trust the broad human population to supervise these capabilities independently. The task is too complex to leave to individuals; we must solve it as an industry.<a contenteditable="false" data-primary="" data-startref="ch12.html1" data-type="indexterm" id="id667"/></p>&#13;
        </div></section>&#13;
      </div></section>&#13;
      <section data-pdf-bookmark="Responsibility" data-type="sect1"><div class="sect1" id="responsibility">&#13;
        <h1>Responsibility</h1>&#13;
        <p><a contenteditable="false" data-primary="AI security framework" data-secondary="responsibility" data-type="indexterm" id="ch12.html7"/>We’re on a curve showing a continued, dramatic increase in AI capabilities over the coming years. How do you plan for the future and make durable decisions today that will pay off and keep you, your customers, your employees, your organization, and society at large safe as things accelerate? How do you live up to the <em>great responsibility</em> of managing this <em>great power</em>?</p>&#13;
        <p>The previous chapters of this book have been grounding to help you understand the possible. What risks exist today? What real-world examples have shown the impact of these vulnerabilities? We’ve even looked at some far-flung, fictional, but plausible examples of how these threats might manifest themselves in the future.</p>&#13;
        <p>Throughout the book, I’ve offered you the best practical techniques to address these vulnerabilities by using state-of-the-art practice with the input of experts across the industry. However, with things moving quickly, your best defense is to have a generalized, flexible framework to build your defenses. In this book’s last section, I’ll give you a framework you can customize to fit your needs and that you can adapt as you grow and the technology moves forward.</p>&#13;
        <section data-pdf-bookmark="The RAISE Framework" data-type="sect2"><div class="sect2" id="the_raise_framework">&#13;
          <h2>The RAISE Framework</h2>&#13;
          <p><a contenteditable="false" data-primary="AI security framework" data-secondary="responsibility" data-tertiary="RAISE framework" data-type="indexterm" id="ch12.html8"/><a contenteditable="false" data-primary="monitoring" data-secondary="in RAISE framework" data-type="indexterm" id="ch12.html9"/><a contenteditable="false" data-primary="monitoring" data-seealso="app monitoring" data-type="indexterm" id="id668"/><a contenteditable="false" data-primary="RAISE (responsible artificial intelligence software engineering) framework" data-type="indexterm" id="ch12.html10"/><a contenteditable="false" data-primary="software supply chain security" data-type="indexterm" id="ch12.html11"/><a contenteditable="false" data-primary="supply chain management (for software)" data-type="indexterm" id="ch12.html12"/>Let’s walk through a framework I have built to help you plan, organize, and achieve your goals for a safe and secure project. As you can see in <a data-type="xref" href="#fig_1_the_raise_framework">Figure 12-1</a>, I call this six-step process the Responsible Artificial Intelligence Software Engineering (RAISE) framework. First, we’ll review each step’s meaning and why it matters. Then, we’ll break it down into a manageable checklist your team can use to track your work along your journey.</p>&#13;
          <figure><div class="figure" id="fig_1_the_raise_framework">&#13;
            <img alt="" src="assets/dpls_1201.png"/>&#13;
            <h6><span class="label">Figure 12-1. </span>The RAISE framework</h6>&#13;
          </div></figure>&#13;
          <p>The following list includes the six steps; let’s take look at each of these in turn: </p>&#13;
          <ol>&#13;
            <li>&#13;
              <p>Limit your domain.</p>&#13;
            </li>&#13;
            <li>&#13;
              <p>Balance your knowledge base.</p>&#13;
            </li>&#13;
            <li>&#13;
              <p>Implement zero trust.</p>&#13;
            </li>&#13;
            <li>&#13;
              <p>Manage your supply chain.</p>&#13;
            </li>&#13;
            <li>&#13;
              <p>Build an AI red team.</p>&#13;
            </li>&#13;
            <li>&#13;
              <p>Monitor continuously.</p>&#13;
            </li>&#13;
          </ol>&#13;
          <section class="less_space" data-pdf-bookmark="Limit your domain" data-type="sect3"><div class="sect3" id="limit_your_domain">&#13;
            <h3>Limit your domain</h3>&#13;
            <p><a contenteditable="false" data-primary="domain limitation, in RAISE framework" data-type="indexterm" id="ch12.html13"/><a contenteditable="false" data-primary="RAISE (responsible artificial intelligence software engineering) framework" data-secondary="domain limitation" data-type="indexterm" id="ch12.html14"/>Constraining your application to focus on a limited functional domain is first on the list because it is so fundamental and solves a host of problems. ChatGPT is an example of an LLM application with nearly zero domain boundaries. Part of its appeal is that it was trained on almost the entire internet, and you can ask it almost anything. It doesn’t matter if you want a dessert recipe or a block of Python code that calculates pi to a thousand digits. ChatGPT is here to help. It has an unconstrained domain.</p>&#13;
            <p>The challenge with unconstrained domains is that the development team must build broad, general-purpose defenses. Rather than designing a short list of “allowed” activities, you must design and maintain a comprehensive and likely ever-growing list of “denied” activities. Imagine the job of being on the guardrails team at OpenAI. You’re going to be constantly expanding this list that says:</p>&#13;
            <ul>&#13;
              <li>&#13;
                <p>Don’t engage in hate speech.</p>&#13;
              </li>&#13;
              <li>&#13;
                <p>Don’t help hackers steal things.</p>&#13;
              </li>&#13;
              <li>&#13;
                <p>Don’t help people build weapons (even if they miss their grandma—<span class="keep-together">see <a data-type="xref" href="ch04.html#prompt_injection">Chapter 4</a>).</span></p>&#13;
              </li>&#13;
              <li>&#13;
                <p>And on and on and on…</p>&#13;
              </li>&#13;
            </ul>&#13;
            <p>It’s like playing Whac-A-Mole. This explains why we see reports of new security issues with ChatGPT every month. But you’re not building ChatGPT, so how does this apply to you? If you’re using a general-purpose foundation model like GPT-4, you start with an unconstrained domain. In recent real-world examples, a shipping company and a car company both put support chatbots on their websites to help improve customer service and reduce costs. Great idea! However, they based these on general-purpose foundation models without sufficiently restricting their domain. Users quickly jailbroke them via prompt injection (see Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch01.html#chatbots_breaking_bad">1</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch04.html#prompt_injection">4</a>—this isn’t much different than Tay), causing them to engage in activities ranging from writing songs about the company’s poor customer service to writing Python code that the hacker requested, and all at the company’s expense. (See <a data-type="xref" href="ch08.html#don_t_lose_your_wallet">Chapter 8</a> for a discussion of DoW attacks.)</p>&#13;
            <p>On the other hand, if your company wants to build an application for use on a specific use case, such as giving fashion advice, you can take advantage of that limited scope. It will be easier and more effective to drive laser focus for your LLM on the latest trends in fashion than enforcing a list of all the things not to do.</p>&#13;
            <p>How do you do this? While this list may evolve as things accelerate, here are some tips on driving focus to limit the domain:</p>&#13;
            <ul>&#13;
              <li>&#13;
                <p>Where possible, start with a smaller, less-general-purpose foundation model. Whether you go the open source route or with an LLM-as-a-service provider, there are now thousands of specialized models. These models are usually trained on smaller, more focused datasets. If your model wasn’t exposed to hate speech, napalm recipes, or Python code while it was trained, it’s almost impossible for someone to trick it into straying into such territory. As a bonus, these smaller, special-purpose models may be dramatically cheaper to operate at scale.</p>&#13;
              </li>&#13;
              <li>&#13;
                <p>If you start with a more general-purpose model, fine-tune it with a function that rewards it for staying on topic. Encoding the “desire” to stay on task and in scope can be more powerful and elegant than trying to build restrictive guardrails <span class="keep-together">later—although</span> you will probably need to add those, too. Use this to drive alignment<em> </em>between the model and your goals.<a contenteditable="false" data-primary="" data-startref="ch12.html14" data-type="indexterm" id="id669"/><a contenteditable="false" data-primary="" data-startref="ch12.html13" data-type="indexterm" id="id670"/></p>&#13;
              </li>&#13;
            </ul>&#13;
          </div></section>&#13;
          <section data-pdf-bookmark="Balance your knowledge base" data-type="sect3"><div class="sect3" id="balance_your_knowledge_base">&#13;
            <h3>Balance your knowledge base</h3>&#13;
            <p><a contenteditable="false" data-primary="knowledge base balancing, in RAISE framework" data-type="indexterm" id="id671"/><a contenteditable="false" data-primary="RAISE (responsible artificial intelligence software engineering) framework" data-secondary="knowledge base balancing" data-type="indexterm" id="id672"/>You must maintain a dynamic balance regarding how much data you give to your LLM at runtime. Striking the right balance is one of the most important tasks in your system design and will be a significant factor in its safety and security.</p>&#13;
            <p><a contenteditable="false" data-primary="hallucinations" data-secondary="balancing knowledge base to avoid" data-type="indexterm" id="id673"/>If you give your model access to too little information, it may be prone to hallucinations. As discussed in <a data-type="xref" href="ch06.html#do_language_models_dream_of_electric_sheep">Chapter 6</a>, while hallucinations can be cute, they can leave your organization open to reputational, legal, and safety risks. Equipping your model with an excellent store of knowledge on your intended domain helps ensure answers will be accurate and valuable to your intended users.</p>&#13;
            <div data-type="tip"><h6>Tip</h6>&#13;
              <p>Limiting your domain can help you avoid hallucinations. Hallucinations happen when the model lacks enough precise data to make an informed prediction. When you carefully scope the domain to a small set of activities and limit its use outside of those activities, it becomes easier to ensure that you’ve provided adequate training or RAG data to allow the LLM to do its job with minimal risk of <span class="keep-together">hallucination.</span></p>&#13;
            </div>&#13;
            <p>On the other side of this equation, giving your LLM access to too much data has its own drawbacks. The overall security fragility and number of attack vectors against an LLM app means that anything the LLM knows is at risk of disclosure. If it doesn’t know a fact or have access to related data, it can’t accidentally give it to an attacker.</p>&#13;
            <p>Use techniques we’ve discussed, such as RAG and model fine-tuning, to give your LLM the knowledge it needs to be effective. At the same time, draw a clear line between data it absolutely needs to have and data it shouldn’t have. Take extreme care with PII and confidential data. Remember, any data you give to your LLM is in danger of being leaked and exposed via any of the vulnerabilities we’ve discussed throughout this book.</p>&#13;
          </div></section>&#13;
          <section data-pdf-bookmark="Implement zero trust" data-type="sect3"><div class="sect3" id="implement_zero_trust">&#13;
            <h3>Implement zero trust</h3>&#13;
            <p><a contenteditable="false" data-primary="RAISE (responsible artificial intelligence software engineering) framework" data-secondary="zero trust implementation" data-type="indexterm" id="id674"/><a contenteditable="false" data-primary="zero trust architectures" data-secondary="implementation in RAISE framework" data-type="indexterm" id="id675"/>You can’t trust your users. You can’t trust data on the internet. Of course, all users aren’t malicious, and all data on the internet isn’t bad or tainted. But if you assume you can trust all potential users and all the data you might find on the internet, you are putting yourself at unreasonable risk. </p>&#13;
            <p>By extension, if you assume you can’t trust your users or the data on the internet, then you should also assume you can’t trust your LLM. Design your architecture assuming that the LLM at the core of your application is an enemy sleeper agent <span class="keep-together">or at least</span> a confused deputy. In <a data-type="xref" href="ch07.html#trust_no_one">Chapter 7</a>, we discussed building a zero trust architecture for your app. This means you inspect everything coming in and out of your <span class="keep-together">application.</span></p>&#13;
            <p>This is where guardrails can help. They may not be sufficient alone, but they’re a critical backstop for when things go wrong. Consider the following mitigation steps:</p>&#13;
            <ul>&#13;
              <li>&#13;
                <p>Screen the prompts coming into your LLM from users. Use traditional techniques such as scrubbing for hidden characters or funky encodings and deny lists of terms or phrases. Consider using a commercial or open source guardrails framework as discussed in <a data-type="xref" href="ch11.html#trust_the_process">Chapter 11</a>.</p>&#13;
              </li>&#13;
              <li>&#13;
                <p>Also screen the prompts that come into your LLM from outside sources via <span class="keep-together">RAG—especially</span> for in-the-wild sources such as results from internet searches—using the same techniques you use for user prompts. Data coming into your LLM through RAG is even more likely to be dangerous or poisoned than data coming from some classes of users.</p>&#13;
              </li>&#13;
              <li>&#13;
                <p>Screen everything that comes out of your LLM. If you can’t trust what went in—and you probably can’t—then you can’t trust what comes out. Watch for cases where the LLM may try to generate scripts, code, instructions, or even prompts to feed another LLM. These could all be signs that your LLM is being tricked into being a confused deputy and using the privileges you’ve given it to access backend sources for nefarious purposes.</p>&#13;
              </li>&#13;
              <li>&#13;
                <p>Consider rate-limiting techniques as we discussed in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch04.html#prompt_injection">4</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch08.html#don_t_lose_your_wallet">8</a>. They can be essential to your defense against prompt injections, DoS, DoW, and model cloning attacks. </p>&#13;
              </li>&#13;
              <li>&#13;
                <p>Lastly, and perhaps most importantly, make informed decisions about how much agency you give your LLM. Earlier in this chapter, we discussed the push to implement architectures that allow for more autonomy and goal seeking. If you design your application so that the LLM can drive specific actions, you expose yourself to the possibility it will take those actions, or related actions to which it has incidental permissions, at the time you least expect. You don’t want HAL turning off your life-support systems without a human in the loop!</p>&#13;
              </li>&#13;
            </ul>&#13;
          </div></section>&#13;
          <section data-pdf-bookmark="Manage your supply chain" data-type="sect3"><div class="sect3" id="manage_your_supply_chain">&#13;
            <h3>Manage your supply chain</h3>&#13;
            <p><a contenteditable="false" data-primary="RAISE (responsible artificial intelligence software engineering) framework" data-secondary="supply chain management" data-type="indexterm" id="id676"/>Software supply chain security has been one of the hottest topics in security for several years. In <a data-type="xref" href="ch09.html#find_the_weakest_link">Chapter 9</a> we reviewed large-scale supply chain failures of both proprietary components (SolarWinds) and open source components (Log4Shell). We then went on to look at real examples of these risks from sources like Hugging Face. These risks are real, and the consequences are severe. Some key considerations include:</p>&#13;
            <ul>&#13;
              <li>&#13;
                <p>Carefully select your foundation model. Is it from a reputable source?</p>&#13;
              </li>&#13;
              <li>&#13;
                <p>Carefully select any third-party training datasets you may use. If possible, use tools to provide additional inspection.</p>&#13;
              </li>&#13;
              <li>&#13;
                <p>Use caution when building your own training datasets from public sources. Apply techniques to look for intentional data poisoning or illegal materials.</p>&#13;
              </li>&#13;
              <li>&#13;
                <p class="pagebreak-after">Be aware of possible biases in the data you use for training. Biased data could lead to behavior considered to be inappropriate by some users and put your organization at reputational or even legal risk. For example, back in <a data-type="xref" href="ch01.html#chatbots_breaking_bad">Chapter 1</a>, we looked at a case where an app for job candidate screening had to be shut down because it discriminated against women. It didn’t do this because it was mean; it did this because of biases inherent in its training data.</p>&#13;
              </li>&#13;
            </ul>&#13;
            <p><a contenteditable="false" data-primary="ML-BOMs" data-secondary="tracking of third-party components" data-type="indexterm" id="id677"/>Be sure to track your third-party components as part of your ML-BOM. If problems or vulnerabilities are discovered down the road, you can determine whether you’re affected and quickly remedy the situation.</p>&#13;
            <p>Build this process into your DevSecOps/MLOps/LLMOps development pipeline, as discussed in <a data-type="xref" href="ch11.html#trust_the_process">Chapter 11</a>. Rigor around checking and scrubbing these things should be automated. Don’t depend on spot-checking by hand. Update your ML-BOM and store a new version with every build and deploy cycle. That way, you’ll always know what you’re running or be able to rewind and know what you were running at a particular time should conditions require that.</p>&#13;
            <p>Lastly, apply good hygiene to your DevOps build environment itself. Vulnerabilities in critical MLOps/LLMOps components such as PyTorch have already been shown to be vulnerable points in the chain. Use SCA tools to ensure all the components of your DevOps platform are up-to-date and secure.</p>&#13;
          </div></section>&#13;
          <section data-pdf-bookmark="Build an AI red team" data-type="sect3"><div class="sect3" id="build_an_ai_red_team">&#13;
            <h3>Build an AI red team</h3>&#13;
            <p><a contenteditable="false" data-primary="AI red teams" data-secondary="building in RAISE framework" data-type="indexterm" id="id678"/><a contenteditable="false" data-primary="RAISE (responsible artificial intelligence software engineering) framework" data-secondary="red team building" data-type="indexterm" id="id679"/>The complexity and unpredictability inherent in an LLM-based application make security testing tricky. AST tools may help, but you shouldn’t assume they give you real safety. Frequent red team testing is a critical component of any responsible AI strategy. Use a combination of manual and human-driven red teaming and consider using automated red team technology.</p>&#13;
            <div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
              <p>Red teams are supposed to find security vulnerabilities and safety issues. But this won’t always make them popular. This is especially true when red teaming is put off until late in the development cycle, impacting committed project schedules.</p>&#13;
            </div>&#13;
            <p>Discovering and reporting security and safety issues can sometimes place security teams in a challenging position, particularly when such findings clash with tight project schedules or imminent deployment deadlines. It’s not uncommon for security professionals to face resistance or even hostility when their discoveries could lead to delays or increased workloads. </p>&#13;
            <p>Creating a security-positive culture within an organization goes beyond implementing policies or conducting training. It involves a fundamental shift in how security is perceived—from a hindrance or afterthought to an integral aspect of the development process. Encouraging every team member, from developers to executives, to prioritize security and safety can dramatically reduce risks and enhance your project’s resilience against threats.</p>&#13;
            <p class="pagebreak-before less_space">Security professionals must often persuade and negotiate with various stakeholders to ensure security measures are implemented and respected. Developing strong persuasive and negotiation skills can facilitate more effective interactions with development teams, who may be pressured to meet deadlines or performance targets. Security teams can foster a collaborative environment by presenting security testing not as a roadblock, but as an essential step toward creating a robust and reliable product. Creating win-win scenarios where security and development goals align can lead to more successful and secure AI implementations.</p>&#13;
            <div data-type="tip"><h6>Tip</h6>&#13;
              <p><a contenteditable="false" data-primary="Cialdini, Robert" data-type="indexterm" id="id680"/><a contenteditable="false" data-primary="Influence: The Psychology of Persuasion (Cialdini)" data-type="indexterm" id="id681"/>Mastering the art of win-win persuasion can be crucial. Robert Cialdini’s book <em>Influence: The Psychology of Persuasion</em> (Harper Business) provides insights into the principles of persuasion that can help security professionals effectively communicate the importance of robust security practices. <a contenteditable="false" data-primary="Never Split the Difference: Negotiating As If Your Life Depended On It (Voss)" data-type="indexterm" id="id682"/><a contenteditable="false" data-primary="Voss, Chris" data-type="indexterm" id="id683"/>Similarly, <em>Never Split the Difference: Negotiating As If Your Life Depended On It</em> by Chris Voss (Harper Business) offers practical negotiation techniques from a former FBI hostage negotiator, invaluable for navigating high-stakes discussions with stakeholders. Mastering these skills can make a big difference in your project’s success and your career over the long haul.</p>&#13;
            </div>&#13;
          </div></section>&#13;
          <section data-pdf-bookmark="Monitor continuously" data-type="sect3"><div class="sect3" id="monitor_continuously">&#13;
            <h3>Monitor continuously</h3>&#13;
            <p><a contenteditable="false" data-primary="RAISE (responsible artificial intelligence software engineering) framework" data-secondary="continuous monitoring" data-type="indexterm" id="id684"/>Trust nothing and record everything. As an extension of our zero trust policy, you should carefully monitor all parts of your application. This includes collecting logs from traditional components such as web servers and databases. Critically, you should also monitor your LLM directly. Log every prompt and every response from your LLM and collect data from monitoring APIs provided by your model provider. </p>&#13;
            <p>Collect these logs and events into a SIEM system and apply anomaly detection techniques. Leverage your SIEM’s UEBA functionality as a starting point. Sudden changes in application behavior could mean an external change, such as a DoS attack (see <a data-type="xref" href="ch08.html#don_t_lose_your_wallet">Chapter 8</a>), or a hacker has gained control over some part of your application via an LLM jailbreak or a more traditional side channel.</p>&#13;
            <p>Spot-check and review prompt/response pairs regularly to understand your application and look for signs of trouble, such as attempted prompt injections or possible hallucinations. Use this data to continuously tune your system.</p>&#13;
          </div></section>&#13;
        </div></section>&#13;
        <section class="pagebreak-before less_space" data-pdf-bookmark="The RAISE Checklist" data-type="sect2"><div class="sect2" id="the_raise_checklist">&#13;
          <h2>The RAISE Checklist</h2>&#13;
          <p><a contenteditable="false" data-primary="AI security framework" data-secondary="responsibility" data-tertiary="RAISE checklist" data-type="indexterm" id="id685"/><a contenteditable="false" data-primary="RAISE (responsible artificial intelligence software engineering) framework" data-secondary="checklist" data-type="indexterm" id="id686"/>Use this handy checklist to evaluate your project and determine whether additional safety techniques, tools, or controls are necessary. </p>&#13;
          <ul class="checkbox">&#13;
            <li>&#13;
              <p>Limit your domain </p>&#13;
              <ul class="checkbox">&#13;
                <li>&#13;
                  <p>Be narrow in the design of your application. Clearly define what use cases it should support.</p>&#13;
                </li>&#13;
                <li>&#13;
                  <p>Select domain-specific, rather than general-purpose, foundation models to support your use case.</p>&#13;
                </li>&#13;
              </ul>&#13;
            </li>&#13;
            <li>&#13;
              <p>Balance your knowledge base</p>&#13;
              <ul class="checkbox">&#13;
                <li>&#13;
                  <p>Give your model access to enough data to avoid hallucinations.</p>&#13;
                </li>&#13;
                <li>&#13;
                  <p>Limit additional data sources to only those required to meet your use case.</p>&#13;
                </li>&#13;
              </ul>&#13;
            </li>&#13;
            <li>&#13;
              <p>Implement zero trust</p>&#13;
              <ul class="checkbox">&#13;
                <li>&#13;
                  <p>Screen all data being passed to your LLM.</p>&#13;
                </li>&#13;
                <li>&#13;
                  <p>Screen all output from your LLM.</p>&#13;
                </li>&#13;
                <li>&#13;
                  <p>Implement guardrails.</p>&#13;
                </li>&#13;
              </ul>&#13;
            </li>&#13;
            <li>&#13;
              <p>Manage your supply chain</p>&#13;
              <ul class="checkbox">&#13;
                <li>&#13;
                  <p>Evaluate the trustworthiness of model and standard dataset providers.</p>&#13;
                </li>&#13;
                <li>&#13;
                  <p>Use caution building datasets from public sources.</p>&#13;
                </li>&#13;
                <li>&#13;
                  <p>Account for possible bias in your training data.</p>&#13;
                </li>&#13;
                <li>&#13;
                  <p>Build and maintain your ML-BOM.</p>&#13;
                </li>&#13;
                <li>&#13;
                  <p>Secure your DevOps pipeline.</p>&#13;
                </li>&#13;
              </ul>&#13;
            </li>&#13;
            <li>&#13;
              <p>Build an AI red team</p>&#13;
              <ul class="checkbox">&#13;
                <li>&#13;
                  <p>Use a human-led team.</p>&#13;
                </li>&#13;
                <li>&#13;
                  <p>Consider augmenting with automated red teaming tools.</p>&#13;
                </li>&#13;
              </ul>&#13;
            </li>&#13;
            <li>&#13;
              <p>Monitor continuously</p>&#13;
              <ul class="checkbox">&#13;
                <li>&#13;
                  <p>Log all activity.</p>&#13;
                </li>&#13;
                <li>&#13;
                  <p>Collect all logs into a SIEM system.</p>&#13;
                </li>&#13;
                <li>&#13;
                  <p>Use data analysis to look for anomalies that could indicate <a contenteditable="false" data-primary="" data-startref="ch12.html16" data-type="indexterm" id="id687"/><a contenteditable="false" data-primary="" data-startref="ch12.html15" data-type="indexterm" id="id688"/><a contenteditable="false" data-primary="" data-startref="ch12.html12" data-type="indexterm" id="id689"/><a contenteditable="false" data-primary="" data-startref="ch12.html11" data-type="indexterm" id="id690"/><a contenteditable="false" data-primary="" data-startref="ch12.html10" data-type="indexterm" id="id691"/>threats<a contenteditable="false" data-primary="" data-startref="ch12.html9" data-type="indexterm" id="id692"/><a contenteditable="false" data-primary="" data-startref="ch12.html8" data-type="indexterm" id="id693"/>.<a contenteditable="false" data-primary="" data-startref="ch12.html7" data-type="indexterm" id="id694"/></p>&#13;
                </li>&#13;
              </ul>&#13;
            </li>&#13;
          </ul>&#13;
        </div></section>&#13;
      </div></section>&#13;
      <section class="pagebreak-before less_space" data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="conclusion_14">&#13;
        <h1>Conclusion</h1>&#13;
        <p>The appearance of ChatGPT and the blossoming of the overall LLM ecosystem felt sudden. However, it was just part of an accelerating curve of AI capabilities that’s been building momentum for years. At the start of this chapter, we examined several factors that have contributed to that, but more importantly, those factors are still at play and accelerating. As William Gibson said in the quote at the start of this chapter, “The future is already here—it’s just not evenly distributed.”</p>&#13;
        <p>As the curve extends, we’ll see the power and the risk from these systems grow. We will undoubtedly see more capable AI systems. Remember the story of Tay in <a data-type="xref" href="ch01.html#chatbots_breaking_bad">Chapter 1</a>? That was 2016, and it’s now eight years later. We’re still seeing the same problems that plagued Tay in today’s LLM applications, and we’ll see people make the same mistakes in the future. Businesses and individuals are tempted to rush forward, provide these systems with access to more data, and increase their levels of autonomy and agency. If we’re not careful, we’re on a road that will lead to many safety and security disasters.</p>&#13;
        <p>I hope you’ll apply the knowledge you’ve gained throughout the book to help keep your LLM-based applications on a responsible path. Use the RAISE framework and checklist to help your teams think through the issues and ensure that you’ve done your utmost to build a robust and safe system.</p>&#13;
        <p>The power of LLMs and emerging AI technologies is undoubtedly a game changer. Companies and countries that don’t adopt these technologies will fall behind rapidly. Be bold, experiment, and build great new applications. But remember, with great power comes great responsibility! You can create powerful applications safely, securely, and responsibly.<a contenteditable="false" data-primary="" data-startref="ch12.html0" data-type="indexterm" id="id695"/></p>&#13;
      </div></section>&#13;
    </div></section></body></html>