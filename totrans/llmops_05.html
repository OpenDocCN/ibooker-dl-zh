<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 5. Model Domain Adaptation for LLM-Based Applications"><div class="chapter" id="ch05_model_domain_adaptation_for_llm_based_applications_1748896666813361">
<h1><span class="label">Chapter 5. </span>Model Domain Adaptation for <span class="keep-together">LLM-Based Applications</span></h1>

<p>In <a contenteditable="false" data-primary="model domain adaptation" data-type="indexterm" id="icd509x"/>the previous chapter, we discussed different architectures for model deployment. In this chapter, we will talk about how to do domain adaptation for your models. Practitioners frequently refer to <em>domain adaptation</em> as “fine-tuning,” but fine-tuning is actually just one of many ways to make a model work well in your domain.</p>

<p>In this chapter, we will look at several model adaptation methods, including prompt engineering, fine-tuning, and retrieval augmented generation (RAG).</p>

<p>We will also look at how to optimize LLMs to run them in resource-constrained environments that require model compression. Finally, we will discuss best practices and scaling laws to show you how to determine how much data your LLMs need to run effectively.</p>

<section data-type="sect1" data-pdf-bookmark="Training LLMs from Scratch"><div class="sect1" id="ch05_training_llms_from_scratch_1748896666813641">
<h1>Training LLMs from Scratch</h1>

<p>Training<a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="training from scratch" data-type="indexterm" id="icd501"/> LLMs<a contenteditable="false" data-primary="model domain adaptation" data-secondary="training LLMs from scratch" data-type="indexterm" id="icd510x"/> from scratch can be simple or resource intensive, depending on your application. For most applications, it makes sense to use an existing <a contenteditable="false" data-primary="open source LLMs" data-secondary="training from scratch" data-type="indexterm" id="id818"/>open source LLM or <a contenteditable="false" data-primary="proprietary (closed-source) LLMs" data-secondary="training from scratch" data-type="indexterm" id="id819"/>proprietary LLM. On the other hand, there’s no better way to learn how an LLM works than to train one from scratch.</p>

<p>Training an LLM from scratch is a complex, resource-intensive task requiring a comprehensive pipeline that necessitates data preparation, model architecture selection, training configuration, and monitoring. Let’s walk through a structured approach to training an LLM from scratch.</p>

<section data-type="sect2" data-pdf-bookmark="Step 1: Pick a Task"><div class="sect2" id="ch05_step_1_pick_a_task_1748896666813707">
<h2>Step 1: Pick a Task</h2>

<p>Determine<a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="training from scratch" data-tertiary="task selection" data-type="indexterm" id="id820"/> why you’re building this model, the domain it will serve, and the tasks it will perform (such as text generation, summarization, or code generation). Decide on success criteria, such as perplexity, accuracy, or other domain-specific evaluation metrics.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Step 2: Prepare the Data"><div class="sect2" id="ch05_step_2_prepare_the_data_1748896666813761">
<h2>Step 2: Prepare the Data</h2>

<p>Before<a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="training from scratch" data-tertiary="data preparation" data-type="indexterm" id="id821"/> you feed data into a model, the model preprocessing step makes sure that the input is in a form the model can handle effectively. This involves tokenizing text, removing noise, normalizing formats, and sometimes simplifying complex structures into components the model can understand more easily. Preprocessing can also include feature selection, which is about picking the most relevant data so the model’s “focus” is on what really matters. This step includes:</p>

<dl>
	<dt>Collecting large-scale text data</dt>
	<dd>
	<p>High-quality sources include books, articles, websites, research papers, code repositories, and domain-specific texts if the model is specialized (such as for use in the legal or medical fields).</p>
	</dd>
	<dt>Cleaning the data</dt>
	<dd>
	<p>This<a contenteditable="false" data-primary="data cleaning" data-type="indexterm" id="id822"/><a contenteditable="false" data-primary="cleaning data" data-type="indexterm" id="id823"/> includes removing non-useful elements (like advertisements or formatting artifacts) and handling misspellings. Use libraries like Hugging Face for this task.</p>
	</dd>
	<dt>Tokenizing the data</dt>
	<dd>
	<p>You can do this using<a contenteditable="false" data-primary="tokenization" data-type="indexterm" id="id824"/> subword tokenization methods like byte-pair encoding (BPE) or SentencePiece, as is done in models like BERT and GPT-3. You can also use Hugging Face’s <code>AutoTokenizer</code> for this task. Tokenization is essential for handling large vocabularies and avoiding the need for excessive parameters.</p>
	</dd>
</dl>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Step 3: Decide on the Model Architecture"><div class="sect2" id="ch05_step_3_decide_on_the_model_architecture_1748896666813818">
<h2>Step 3: Decide on the Model Architecture</h2>

<p>Choose<a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="training from scratch" data-tertiary="architecture selection" data-type="indexterm" id="id825"/> a model size appropriate for your data, resources, and goals. Model configurations range from smaller models (hundreds of millions of parameters) to full-scale LLMs (billions or even trillions of parameters). As discussed in <a data-type="xref" href="ch01.html#ch01_introduction_to_large_language_models_1748895465615150">Chapter 1</a>, adapt the base architecture to fit your specific needs, whether that’s changing the number of layers, changing the attention mechanism, or adding specialized components (such as retrieval-augmented<a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="architectures" data-type="indexterm" id="id826"/> mechanisms<a contenteditable="false" data-primary="encoder–decoder models (sequence-to-sequence models)" data-type="indexterm" id="id827"/><a contenteditable="false" data-primary="decoder-only models (AutoRegressive models)" data-type="indexterm" id="id828"/><a contenteditable="false" data-primary="encoder-only models (AutoEncoding models)" data-type="indexterm" id="id829"/> if <a contenteditable="false" data-primary="sequence-to-sequence models (encoder–decoder models)" data-type="indexterm" id="id830"/><a contenteditable="false" data-primary="AutoRegressive models (decoder-only models)" data-type="indexterm" id="id831"/><a contenteditable="false" data-primary="AutoEncoding models (encoder-only models)" data-type="indexterm" id="id832"/>focusing on knowledge-intensive tasks). Three general types of architecture are shown in <a data-type="xref" href="#ch05_figure_1_1748896666799655">Figure 5-1</a>.</p>

<figure><div id="ch05_figure_1_1748896666799655" class="figure"><img alt="" src="assets/llmo_0501.png" width="1445" height="2097"/>
<h6><span class="label">Figure 5-1. </span>Three types of LLM architectures (source: <a href="https://oreil.ly/bR9E1">Abhinav Kimothi</a>)</h6>
</div></figure>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Step 4: Set Up Your Training Infrastructure"><div class="sect2" id="ch05_step_4_set_up_your_training_infrastructure_1748896666813872">
<h2>Step 4: Set Up Your Training Infrastructure</h2>

<p>Training<a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="training from scratch" data-tertiary="infrastructure setup" data-type="indexterm" id="id833"/> a large model typically requires distributed training across multiple GPUs or TPUs,<a contenteditable="false" data-primary="GPUs (graphics processing units)" data-secondary="training infrastructure setup" data-type="indexterm" id="id834"/> ideally with high memory (16 GB+) and fast interconnects (like NVLink). Frameworks like PyTorch’s Distributed Data Parallel (DDP) or TensorFlow’s <code>MultiWorkerMirroredStrategy</code> can come in handy. Those coming from an MLOps background may already know of libraries like DeepSpeed <a contenteditable="false" data-primary="DeepSpeed" data-type="indexterm" id="id835"/>and Megatron-LM<a contenteditable="false" data-primary="Megatron-LM" data-type="indexterm" id="id836"/> that are designed <a contenteditable="false" data-primary="optimizers" data-type="indexterm" id="id837"/>to optimize memory and computation for large-scale model <span class="keep-together">training.</span></p>

<p>Although there are plenty of optimizers for training ML models, including stochastic gradient descent (SGD) and Autograd, we suggest selecting an optimizer suitable for large models, such as Adam or AdamW, and use mixed-precision training (for instance, FP16) to reduce memory usage and accelerate training.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Step 5: Implement Training"><div class="sect2" id="ch05_step_5_implement_training_1748896666813923">
<h2>Step 5: Implement Training</h2>

<p>Train<a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="training from scratch" data-tertiary="implementation of" data-type="indexterm" id="icd502"/> the model on the task. While implementing the training, there are a few things to think of. What will your hyperparameters be? What will be your seed value? You can view one of the simplest implementations<a contenteditable="false" data-primary="Karpathy, Andrej" data-type="indexterm" id="id838"/> for training LLMs from scratch in this <a href="https://oreil.ly/PfnyZ">one-hour video by Andrej Karpathy</a> (see <a data-type="xref" href="#ch05_example_1_1748896666808696">Example 5-1</a>).</p>

<div data-type="example" id="ch05_example_1_1748896666808696">
<h5><span class="label">Example 5-1. </span>Implementation for training an LLM from scratch by Andrej Karpathy (used with permission)</h5>

<pre data-type="programlisting">
import torch
import torch.nn as nn
from torch.nn import functional as F

# define your hyperparameters
batch_size = 16 # how many independent sequences will we process in parallel?
block_size = 32 # what is the maximum context length for predictions?
max_iters = 5000
eval_interval = 100
learning_rate = 1e-3
device = 'cuda' if torch.cuda.is_available() else 'cpu'
eval_iters = 200
n_embd = 64
n_head = 4
n_layer = 4
dropout = 0.0
# ------------

torch.manual_seed(1337)

URL="https://raw.githubusercontent.com/karpathy/char-rnn/master/data/
    tinyshakespeare/input.txt"
wget $URL
with open('input.txt', 'r', encoding='utf-8') as f:
    text = f.read()

# here are all the unique characters that occur in this text
chars = sorted(list(set(text)))
vocab_size = len(chars)
# create a mapping from characters to integers
stoi = { ch:i for i,ch in enumerate(chars) }
itos = { i:ch for i,ch in enumerate(chars) }
encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of 
integers
decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, 
output a string

# Train and test splits
data = torch.tensor(encode(text), dtype=torch.long)
n = int(0.9*len(data)) # first 90% will be train, rest val
train_data = data[:n]
val_data = data[n:]

# data loading
def get_batch(split):
    # generate a small batch of data of inputs x and targets y
    data = train_data if split == 'train' else val_data
    ix = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([data[i:i+block_size] for i in ix])
    y = torch.stack([data[i+1:i+block_size+1] for i in ix])
    x, y = x.to(device), y.to(device)
    return x, y

@torch.no_grad()
def estimate_loss():
    out = {}
    model.eval()
    for split in ['train', 'val']:
        losses = torch.zeros(eval_iters)
        for k in range(eval_iters):
            X, Y = get_batch(split)
            logits, loss = model(X, Y)
            losses[k] = loss.item()
        out[split] = losses.mean()
    model.train()
    return out

class Head(nn.Module):
    """ one head of self-attention """

    def __init__(self, head_size):
        super().__init__()
        self.key = nn.Linear(n_embd, head_size, bias=False)
        self.query = nn.Linear(n_embd, head_size, bias=False)
        self.value = nn.Linear(n_embd, head_size, bias=False)
        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))

        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        B,T,C = x.shape
        k = self.key(x)   # (B,T,C)
        q = self.query(x) # (B,T,C)
        # compute attention scores ("affinities")
        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -&gt; (B, T, T)
        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)
        wei = F.softmax(wei, dim=-1) # (B, T, T)
        wei = self.dropout(wei)
        # perform the weighted aggregation of the values
        v = self.value(x) # (B,T,C)
        out = wei @ v # (B, T, T) @ (B, T, C) -&gt; (B, T, C)
        return out

class MultiHeadAttention(nn.Module):
    """ multiple heads of self-attention in parallel """

    def __init__(self, num_heads, head_size):
        super().__init__()
        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])
        self.proj = nn.Linear(n_embd, n_embd)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        out = torch.cat([h(x) for h in self.heads], dim=-1)
        out = self.dropout(self.proj(out))
        return out

class FeedFoward(nn.Module):
    """ a simple linear layer followed by a non-linearity """

    def __init__(self, n_embd):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(n_embd, 4 * n_embd),
            nn.ReLU(),
            nn.Linear(4 * n_embd, n_embd),
            nn.Dropout(dropout),
        )

    def forward(self, x):
        return self.net(x)

class Block(nn.Module):
    """ Transformer block: communication followed by computation """

    def __init__(self, n_embd, n_head):
        # n_embd: embedding dimension, n_head: the number of heads we'd like
        super().__init__()
        head_size = n_embd // n_head
        self.sa = MultiHeadAttention(n_head, head_size)
        self.ffwd = FeedFoward(n_embd)
        self.ln1 = nn.LayerNorm(n_embd)
        self.ln2 = nn.LayerNorm(n_embd)

    def forward(self, x):
        x = x + self.sa(self.ln1(x))
        x = x + self.ffwd(self.ln2(x))
        return x

# super simple bigram model
class BigramLanguageModel(nn.Module):

    def __init__(self):
        super().__init__()
        # each token directly reads off the logits for the next token from a lookup 
        table
        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)
        self.position_embedding_table = nn.Embedding(block_size, n_embd)
        self.blocks = nn.Sequential(
            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)]
        )
        self.ln_f = nn.LayerNorm(n_embd) # final layer norm
        self.lm_head = nn.Linear(n_embd, vocab_size)

    def forward(self, idx, targets=None):
        B, T = idx.shape

        # idx and targets are both (B,T) tensor of integers
        tok_emb = self.token_embedding_table(idx) # (B,T,C)
        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) 
        x = tok_emb + pos_emb # (B,T,C)
        x = self.blocks(x) # (B,T,C)
        x = self.ln_f(x) # (B,T,C)
        logits = self.lm_head(x) # (B,T,vocab_size)

        if targets is None:
            loss = None
        else:
            B, T, C = logits.shape
            logits = logits.view(B*T, C)
            targets = targets.view(B*T)
            loss = F.cross_entropy(logits, targets)

        return logits, loss

    def generate(self, idx, max_new_tokens):
        # idx is (B, T) array of indices in the current context
        for _ in range(max_new_tokens):
            # crop idx to the last block_size tokens
            idx_cond = idx[:, -block_size:]
            # get the predictions
            logits, loss = self(idx_cond)
            # focus only on the last time step
            logits = logits[:, -1, :] # becomes (B, C)
            # apply softmax to get probabilities
            probs = F.softmax(logits, dim=-1) # (B, C)
            # sample from the distribution
            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)
            # append sampled index to the running sequence
            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)
        return idx

model = BigramLanguageModel()
m = model.to(device)
# print the number of parameters in the model
print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')

# create a PyTorch optimizer
optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)

for iter in range(max_iters):

    # every once in a while evaluate the loss on train and val sets
    if iter % eval_interval == 0 or iter == max_iters - 1:
        losses = estimate_loss()
        print(
            f"step {iter}: "
            f"train loss {losses['train']:.4f}, "
            f"val loss {losses['val']:.4f}"
        )

    # sample a batch of data
    xb, yb = get_batch('train')

    # evaluate the loss
    logits, loss = model(xb, yb)
    optimizer.zero_grad(set_to_none=True)
    loss.backward()
    optimizer.step()

# generate from the model
context = torch.zeros((1, 1), dtype=torch.long, device=device)
print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))</pre>
</div>

<p>Now that you know how a simple LLM works, let’s look into how to <a contenteditable="false" data-primary="model domain adaptation" data-secondary="training LLMs from scratch" data-startref="icd510x" data-type="indexterm" id="id839"/>combine different <a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="training from scratch" data-startref="icd502" data-tertiary="implementation of" data-type="indexterm" id="id840"/>model <a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="training from scratch" data-startref="icd501" data-type="indexterm" id="id841"/>architectures.</p>
</div></section>
</div></section>

<section class="pagebreak-before" data-type="sect1" data-pdf-bookmark="Model Ensembling Approaches"><div class="sect1" id="ch05_model_ensembling_approaches_1748896666813980">
<h1 class="less_space">Model Ensembling Approaches</h1>

<p><em>Ensembling</em> refers<a contenteditable="false" data-primary="ensembling" data-type="indexterm" id="icd503"/> to<a contenteditable="false" data-primary="model ensembling" data-see="ensembling" data-type="indexterm" id="id842"/> combining<a contenteditable="false" data-primary="model domain adaptation" data-secondary="ensembling" data-type="indexterm" id="icd511x"/> multiple models to get better results than any single model can provide on its own. Thus, each model contributes something unique, balancing one another’s weaknesses and complementing their strengths. Ensembling LLMs is a powerful approach for boosting performance, enhancing robustness, and increasing the interpretability of language models. While ensembling has traditionally been more common in smaller ML models, such as random forests or smaller-scale NLP models, it’s becoming increasingly relevant for LLMs due to their specialized behavior and varied responses.</p>

<p>There are, of course, trade-offs with ensembling LLMs. One of the main challenges is computational cost—running multiple large models in parallel can be resource intensive. Memory usage and inference time increase significantly, which can sometimes be prohibitive for real-time, low-latency applications.</p>

<p>Another challenge is complexity in deployment. Deploying an ensemble of LLMs requires orchestrating several models, managing dependencies, and possibly integrating ensemble-specific logic. Model ensembling, however, can often be optimized by using quantized versions of models, caching predictions, or limiting the ensemble to run only when certain criteria are met (for example, if a model’s confidence is low). These techniques will be discussed in <a data-type="xref" href="ch09.html#ch09_scaling_hardware_infrastructure_and_resource_ma_1748896826216961">Chapter 9</a>.</p>

<p>However, for most people and companies, model domain adaptation is one of the most common and cost-effective ways to improve LLM accuracy. Let’s look into a few ways to ensemble LLMs effectively, along with some code examples.</p>

<section data-type="sect2" data-pdf-bookmark="Model Averaging and Blending"><div class="sect2" id="ch05_model_averaging_and_blending_1748896666814036">
<h2>Model Averaging and Blending</h2>

<p>One<a contenteditable="false" data-primary="ensembling" data-secondary="model averaging and blending" data-type="indexterm" id="id843"/><a contenteditable="false" data-primary="averaging and blending multiple model predictions" data-type="indexterm" id="id844"/> straightforward method is to average the predictions of multiple models. This is useful when working with models that have different strengths, such as one model that excels at generating fact-based text and another that is more creative. When we average their responses or blend their outputs, we get a more balanced response. This can be as simple as computing the probability distribution across models and averaging them. In practice, it can also look like generating the softmax probability distributions for each model and averaging them for final predictions.</p>

<p>The code in <a data-type="xref" href="#ch05_example_2_1748896666808730">Example 5-2</a> simply iterates over each model, sums up their outputs, and divides by the number of models to get the average prediction.</p>

<div data-type="example" id="ch05_example_2_1748896666808730" class="pagebreak-before">
<h5 class="less_space"><span class="label">Example 5-2. </span>Averaging the predictions of multiple models</h5>

<pre data-type="programlisting">
import torch

def average_ensemble(models, input_text):
    avg_output = None
    for model in models:
        outputs = model(input_text)
        if avg_output is None:
            avg_output = outputs
        else:
            avg_output += outputs
    avg_output /= len(models)
    return avg_output</pre>
</div>

<p>Here, <code>models</code> is a list of model instances, and <code>input_text</code> is the text prompt.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Weighted Ensembling"><div class="sect2" id="ch05_weighted_ensembling_1748896666814084">
<h2>Weighted Ensembling</h2>

<p>Sometimes<a contenteditable="false" data-primary="ensembling" data-secondary="weighted ensembling" data-type="indexterm" id="id845"/><a contenteditable="false" data-primary="weighted ensembling" data-type="indexterm" id="id846"/> it’s beneficial to give more weight to certain models based on their accuracy or performance on specific tasks. For instance, if Model A is known to perform better on summarization tasks, it can be given a higher weight than Model B in the ensemble. <em>Weighted ensembling</em> allows us to incorporate domain expertise or empirical model evaluations directly into the ensemble. In <a data-type="xref" href="#ch05_example_3_1748896666808750">Example 5-3</a>, <code>weights</code> is a list with the same length as <code>models</code>, containing the weight for each respective model.</p>

<div data-type="example" id="ch05_example_3_1748896666808750">
<h5><span class="label">Example 5-3. </span>Weighted ensembling</h5>

<pre data-type="programlisting">
def weighted_ensemble(models, weights, input_text):
    weighted_output = torch.zeros_like(models[0](input_text))
    for model, weight in zip(models, weights):
        output = model(input_text)
        weighted_output += output * weight
    return weighted_output</pre>
</div>

<p>The output is a weighted combination that can be adjusted depending on the desired emphasis for each model in the ensemble.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Stacked Ensembling (Two-Stage Model)"><div class="sect2" id="ch05_stacked_ensembling_two_stage_model_1748896666814134">
<h2>Stacked Ensembling (Two-Stage Model)</h2>

<p>In <a contenteditable="false" data-primary="ensembling" data-secondary="stacked ensembling" data-type="indexterm" id="id847"/><a contenteditable="false" data-primary="stacked ensembling (two-stage model)" data-type="indexterm" id="id848"/><a contenteditable="false" data-primary="two-stage model (stacked ensembling)" data-type="indexterm" id="id849"/>a <em>stacked ensembling </em>approach, the outputs from multiple models are fed into a secondary model (often a smaller, simpler model) that learns to combine their outputs effectively. This metamodel learns patterns in the output spaces of the LLMs. This can be particularly useful for complex tasks like summarization or translation, where different models might capture different nuances of the input.</p>

<p>The method in <a data-type="xref" href="#ex-5-4">Example 5-4</a> uses an SKlearn model as a metamodel, which is trained on the outputs of the LLMs. It requires a training phase as it learns to make sense of each LLM’s predictions.</p>

<div data-type="example" id="ex-5-4">
<h5><span class="label">Example 5-4. </span>Stacked ensembling</h5>

<pre data-type="programlisting">
from sklearn.linear_model import LogisticRegression
import numpy as np

def stacked_ensemble(models, meta_model, input_texts):
    model_outputs = []
    for model in models:
        outputs = [model(text).numpy() for text in input_texts]
        model_outputs.append(outputs)
    stacked_features = np.hstack(model_outputs)
    meta_model.fit(stacked_features, labels) # assuming labels for training
    return meta_model.predict(stacked_features)</pre>
</div>

<p>Now, what if you don’t want to combine different models but simply different model architectures? Well, with LLMs you can do that too, since every model architecture has its own strengths.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Diverse Ensembles for Robustness"><div class="sect2" id="ch05_diverse_ensembles_for_robustness_1748896666814184">
<h2>Diverse Ensembles for Robustness</h2>

<p>Using<a contenteditable="false" data-primary="ensembling" data-secondary="diverse ensembles for robustness" data-type="indexterm" id="id850"/><a contenteditable="false" data-primary="robustness" data-secondary="ensembling diverse ensembles for" data-type="indexterm" id="id851"/> diverse models—like a mixture of encoder–decoder architectures and transformer-based<a contenteditable="false" data-primary="transformers and transformer models" data-type="indexterm" id="id852"/> language models—can be very effective for handling edge cases or generating more comprehensive answers. This diversity brings complementary strengths to the models and tends to be more resistant to errors in any single model. For example, if one model is prone to <a contenteditable="false" data-primary="hallucinations" data-type="indexterm" id="id853"/>hallucinations (a known issue in some LLMs), the other models can serve as a balancing force, correcting or limiting this effect.</p>

<p>Diversity in ensembling also opens doors for specialized responses using models that focus on different aspects of language, like factuality or creativity. For example, ensembling a smaller factual model with a generative transformer-based model can yield an LLM that provides both creativity and accurate information.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Multi-Step Decoding and Voting Mechanisms"><div class="sect2" id="ch05_multi_step_decoding_and_voting_mechanisms_1748896666814235">
<h2>Multi-Step Decoding and Voting Mechanisms</h2>

<p>A<a contenteditable="false" data-primary="ensembling" data-secondary="multi-step decoding and voting mechanisms" data-type="indexterm" id="id854"/><a contenteditable="false" data-primary="multi-step decoding and voting mechanisms" data-type="indexterm" id="id855"/> unique way to generate text in high-latency, high-accuracy applications is to use voting mechanisms, where models vote on the next token or phrase. Voting schemes like majority, weighted, or ranked voting help ensure that common tokens across models have a higher chance of being selected, while outlier tokens are filtered out. This process can significantly improve the coherence and consistency of generated text, especially for complex prompts or tasks requiring precise language. <a data-type="xref" href="#ch05_example_4_1748896666808768">Example 5-5</a> provides code for a majority vote.</p>

<div data-type="example" id="ch05_example_4_1748896666808768">
<h5><span class="label">Example 5-5. </span>Majority voting</h5>

<pre data-type="programlisting">
from collections import Counter

def voting_ensemble(models, input_text):
   all_predictions = []
   for model in models:
       output = model.generate(input_text)
       all_predictions.append(output)

   # Count the most common output
   majority_vote = Counter(all_predictions).most_common(1)[0][0]
   return majority_vote</pre>
</div>

<p>Here, <code>voting_ensemble</code> uses a majority vote to select the most common output from each model. If there’s a tie, additional logic can be added to consider weighted voting or random selection among the tied options.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Composability"><div class="sect2" id="ch05_composability_1748896666814282">
<h2>Composability</h2>

<p>In <a contenteditable="false" data-primary="ensembling" data-secondary="composability" data-type="indexterm" id="id856"/><a contenteditable="false" data-primary="composability" data-type="indexterm" id="id857"/>ensembling, another popular technique is composability. <em>Composability</em> is the ability to mix and match models or parts of models flexibly. Some ensemble methods might combine outputs from multiple models by averaging them, while others might chain models so that the output of one becomes the input for another. This setup allows you avoid using a massive, all-encompassing model for complex tasks by using smaller, specialized models instead.</p>

<p>For example, suppose we have a summarization model, a translation model, and a sentiment-analysis model. Instead of retraining a single, monolithic model that can handle all three tasks, we can compose these individual models in a pipeline where each one processes the output of the previous one. This modular approach allows for adaptability and maintenance, as each model can be fine-tuned independently, reducing the overall computational cost and development time (see <a data-type="xref" href="#ch05_example_5_1748896666808785">Example 5-6</a>).</p>

<div data-type="example" id="ch05_example_5_1748896666808785">
<h5><span class="label">Example 5-6. </span>Composing a model</h5>

<pre data-type="programlisting">
def compose_pipeline(input_text, models):
    """
    Process the input text through a pipeline of models.
    Each model in the list `models` applies a specific transformation.
    """
    for model in models:
        input_text = model(input_text) 
    return input_text

# Define models for translation, summarization, and sentiment analysis
translated_text = translate_model("Translate this text to French.")
summarized_text = summarize_model(translated_text)
sentiment_result = sentiment_model(summarized_text)</pre>
</div>

<p>Here, <code>translate_model</code>, <code>summarize_model</code>, and <code>sentiment_model</code> can be individually updated or replaced, which is especially beneficial if one of the models becomes outdated or needs retuning.</p>

<p>There are many benefits to composability. For instance, it provides modularity, since different models can be plugged in as needed, enhancing flexibility. Composability allows for easy extension by adding or swapping individual models, making these models scalable. It is also efficient, since you can optimize individual components without affecting the rest of the pipeline.</p>

<p>However, composability doesn’t come without challenges. First, errors in one component can propagate downstream, potentially compounding inaccuracies. Secondly, each stage adds processing time, which may affect real-time applications. And finally, ensuring coherent responses across multiple models requires careful coordination and tuning.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Soft Actor–Critic"><div class="sect2" id="ch05_soft_actor_critic_1748896666814334">
<h2>Soft Actor–Critic</h2>

<p>This <a contenteditable="false" data-primary="ensembling" data-secondary="SAC technique" data-type="indexterm" id="icd504"/><a contenteditable="false" data-primary="SAC (soft actor–critic) technique" data-type="indexterm" id="icd505"/><a contenteditable="false" data-primary="soft actor–critic (SAC) technique" data-type="indexterm" id="icd505a"/>is where the <em>soft actor–critic</em> (SAC) technique comes in. SAC can be advantageous for LLMs where the goal is not just to maximize accuracy but to achieve a balance between different qualitative aspects, such as creativity and coherence. SAC is a reinforcement learning technique that helps the ensemble balance exploration with exploitation. One of the unique features of SAC is its use of “soft” reward maximization, introducing entropy regularization. It promotes exploratory actions, encouraging the model to try different responses rather than always choosing the most predictable one, an approach that can lead to more natural and varied responses in language tasks.</p>

<p>When we ensemble LLMs, SAC can fine-tune how the models interact, making them more adaptable to new information or tasks without overcommitting to one approach. It’s particularly useful for dynamic environments where responses need to adapt based on user feedback or other shifting factors as well as for developing generalized models.</p>

<p>In LLMs, you can use SAC to adjust model outputs to maximize rewards associated with desirable behaviors. For example, in a customer service chatbot, rewards might be based on user satisfaction, response brevity, and politeness. SAC allows the LLM to learn a policy that maximizes these rewards through trial and error, iteratively improving its responses based on feedback.</p>

<p>SAC operates with two<a contenteditable="false" data-primary="actor network, in SAC technique" data-type="indexterm" id="id858"/> core components. The <em>actor network</em> proposes actions (in LLMs, possible responses or actions in dialogue), while <em>critic networks<a contenteditable="false" data-primary="critic networks, in SAC technique" data-type="indexterm" id="id859"/> </em>evaluate the value of the proposed actions, considering both immediate and future rewards.</p>

<p>Implementing SAC for LLMs involves defining a reward function tailored to the task, setting up the actor and critic networks, and training the policy over several episodes, as shown in <a data-type="xref" href="#ch05_example_6_1748896666808799">Example 5-7</a>.</p>

<div data-type="example" id="ch05_example_6_1748896666808799">
<h5><span class="label">Example 5-7. </span>SAC implementation</h5>

<pre data-type="programlisting">
import torch
import torch.nn as nn
import torch.optim as optim

class Actor(nn.Module):
    def __init__(self):
        super(Actor, self).__init__()
        self.layer = nn.Linear(768, 768)  # Assuming LLM output size
        self.output = nn.Softmax(dim=-1)

    def forward(self, x):
        x = self.layer(x)
        return self.output(x)

class Critic(nn.Module):
    def __init__(self):
        super(Critic, self).__init__()
        self.layer = nn.Linear(768, 1)

    def forward(self, x):
        return self.layer(x)

# Initialize actor, critic, and optimizers
actor = Actor()
critic = Critic()
actor_optimizer = optim.Adam(actor.parameters(), lr=1e-4)
critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3)

for episode in range(num_episodes):
    text_output = language_model(input_text)  # Generate response
    reward = compute_reward(text_output) 

    critic_value = critic(text_output)
    critic_loss = torch.mean((critic_value - reward) ** 2)
    critic_optimizer.zero_grad()
    critic_loss.backward()
    critic_optimizer.step()

    actor_loss = -critic_value + entropy_coefficient * torch.mean(actor(text_output))
    actor_optimizer.zero_grad()
    actor_loss.backward()
    actor_optimizer.step()</pre>
</div>

<p>SAC comes with its own benefits. For instance, its entropy-based exploration ensures that responses remain varied, which is ideal for creative language tasks. SAC can also adapt responses over time based on live feedback, improving adaptability in real-world applications like chatbots. In addition, it allows for custom reward functions to tune behavior, making it suitable for multiobjective tasks.</p>

<p>One challenge of SAC is that reward functions must be carefully crafted for each task, balancing multiple objectives, which is a hard task. Second, as with many reinforcement learning algorithms, training such models can be sensitive to hyperparameters, requiring significant tuning. And most importantly, SAC can be computationally intensive, especially <a contenteditable="false" data-primary="model domain adaptation" data-secondary="ensembling" data-startref="icd511x" data-type="indexterm" id="id860"/>for <a contenteditable="false" data-primary="ensembling" data-secondary="SAC technique" data-startref="icd504" data-type="indexterm" id="id861"/><a contenteditable="false" data-primary="SAC (soft actor–critic) technique" data-startref="icd505" data-type="indexterm" id="id862"/><a contenteditable="false" data-primary="soft actor–critic (SAC) technique" data-startref="icd505a" data-type="indexterm" id="id863"/>large <a contenteditable="false" data-primary="ensembling" data-startref="icd503" data-type="indexterm" id="id864"/>models.</p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Model Domain Adaptation"><div class="sect1" id="ch05_model_domain_adaptation_1748896666814388">
<h1>Model Domain Adaptation</h1>

<p>While<a contenteditable="false" data-primary="LLM-based applications" data-secondary="model domain adaptation for" data-type="indexterm" id="icd506"/><a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="model domain adaptation for" data-type="indexterm" id="icd506a"/> LLMs are generally powerful, they often lack specific knowledge or contextual nuances for specialized domains. For example, ChatGPT may understand everyday medical terms, but it might not accurately interpret complex medical jargon or nuanced legal phrases without additional tuning. By fine-tuning an LLM on domain-specific data, you can enhance its ability to understand and produce domain-relevant content, improving both accuracy and coherence in that context.</p>

<p><em>Model adaptation</em> means refining a pretrained model to make it perform better on specific tasks or respond to unique contexts. This approach is particularly useful for LLMs that have been pretrained on diverse but general-purpose data. When applied to specialized areas like legal, medical, or scientific text, domain adaptation helps these models understand and generate more accurate, relevant, and context-sensitive responses.</p>

<p>Domain adaptation methods vary in complexity, from simple fine-tuning on domain-specific datasets to more advanced techniques that adapt models to the specialized vocabulary, terminology, and stylistic nuances of a particular field.</p>

<p>Overall, there are three core benefits to model domain adaptation<a contenteditable="false" data-primary="model domain adaptation" data-secondary="benefits of" data-type="indexterm" id="id865"/>:</p>

<ul>
	<li>
	<p>Improvement of LLMs’ performance on tasks in underrepresented domains, such as medical texts or legal documents.</p>
	</li>
	<li>
	<p>Reduction of the need to collect and label large amounts of data for each new domain. This can be especially useful for domains where data is scarce or expensive to collect.</p>
	</li>
	<li>
	<p>The ability to make LLMs more accessible to a wider range of users, even if the user does not have expertise in that specific domain.</p>
	</li>
</ul>

<p>Let’s look at an example to clarify further. Say your target domain has unique vocabulary, such as chemical compounds or legal citations. You can update the tokenizer and embedding layers to include domain-specific tokens to improve the model’s performance, as shown in <a data-type="xref" href="#ch05_example_7_1748896666808815">Example 5-8</a>.</p>

<div data-type="example" id="ch05_example_7_1748896666808815">
<h5><span class="label">Example 5-8. </span>Adding domain-specific tokens</h5>

<pre data-type="programlisting">
# Custom vocabulary
custom_vocab = ["moleculeA", "compoundX", "geneY"]

# Add new tokens to the tokenizer
tokenizer.add_tokens(custom_vocab)
model.resize_token_embeddings(len(tokenizer))</pre>
</div>

<p>These custom tokenizers can identify unique entities (like chemical formulas or legal citations) as atomic tokens, ensuring that the model recognizes them as distinct units rather than breaking them down into subwords. Embedding these domain-specific tokens helps the model better grasp domain-relevant information and retain consistency across complex terminologies.</p>

<p>There are three techniques for model domain adaptation: prompt engineering, RAG, and fine-tuning. Strictly speaking, RAG is a form of dynamic prompt engineering where developers use a retrieval system to add content to an existing prompt, but RAG systems are used so often that it’s worth discussing them separately.</p>

<p>One critical difference with fine-tuning is that you must have access to the model’s weights, information that is usually not available with cloud-based, proprietary LLMs.</p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Prompt Engineering"><div class="sect1" id="ch05_prompt_engineering_1748896666814442">
<h1>Prompt Engineering</h1>

<p>In <em>prompt engineering</em>, <a contenteditable="false" data-primary="model domain adaptation" data-secondary="prompt engineering" data-type="indexterm" id="icd507"/><a contenteditable="false" data-primary="prompt engineering" data-seealso="RAG" data-type="indexterm" id="icd508"/>we customize the prompts or questions we give the model to get more accurate or insightful responses. The way a prompt is structured has a massive impact on how well a model understands the task at hand and, ultimately, how well it performs. Given LLMs’ versatility, prompt engineering has become an important skill for getting the most out of these models across different domains and tasks.</p>

<p>The key is to understand how different prompt structures lead to different model behaviors. There are various strategies—ranging from simple one-shot prompting to more complex techniques like chain-of-thought prompting—that can significantly improve the effectiveness of LLMs. Let’s look into some common techniques.</p>

<section data-type="sect2" data-pdf-bookmark="One-Shot Prompting"><div class="sect2" id="ch05_one_shot_prompting_1748896666814500">
<h2>One-Shot Prompting</h2>

<p><em>One-shot prompting </em>refers<a contenteditable="false" data-primary="prompt engineering" data-secondary="one-shot prompting" data-type="indexterm" id="id866"/><a contenteditable="false" data-primary="one-shot prompting" data-type="indexterm" id="id867"/> to providing the model with a single example of a prompt and the kind of output you’re expecting. This is a relatively simple approach. The idea is to show the model what kind of answer or action you want by giving a clear and concise example. One-shot prompting works best when the task is simple and well-defined and doesn’t require the model to infer many patterns. If you’re asking the model to translate text, a one-shot prompt might look like this:</p>

<pre data-type="programlisting">
<em>Prompt</em>: Translate the following English sentence to French: 'Hello, how are you?'
<em>French translation</em>: 'Bonjour, comment ça va ?'</pre>

<p>After showing the example, you can then ask the model to translate a new sentence:</p>

<pre data-type="programlisting">
<em>Prompt</em>: Translate the following English sentence to French: 'Good morning, I 
          hope you're doing well.'</pre>

<p>For more complex tasks, one-shot prompting may not provide enough context for the model to generate reliable or meaningful results.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Few-Shot Prompting"><div class="sect2" id="ch05_few_shot_prompting_1748896666814551">
<h2>Few-Shot Prompting</h2>

<p><em>Few-shot prompting</em> takes<a contenteditable="false" data-primary="prompt engineering" data-secondary="few-shot prompting" data-type="indexterm" id="id868"/><a contenteditable="false" data-primary="few-shot prompting" data-type="indexterm" id="id869"/> things a step further by providing the model with multiple examples of the desired output. This method is useful when the task involves identifying patterns or when the model needs more context to perform well. These examples give the model a better understanding of what the output should look like, and it can then apply those patterns to unseen examples.</p>

<p>Few-shot prompting is particularly useful when the task involves generating specific types of responses, such as text generation in a particular style or format. The more examples you give, the better the model becomes at identifying the task’s underlying structure.</p>

<p>For example, imagine you’re asking the model to generate math word problems, and you give it a few examples to show how to generate them from given data:</p>

<pre data-type="programlisting">
<em>Prompt</em>: Here's how to create a word problem based on the following math equation:
1. 3 + 5 = 8 
'If you have 3 apples and pick 5 more, how many apples do you have in total?'
2. 10 – 4 = 6 
'A store had 10 apples, but 4 were sold. How many apples are left in the store?'</pre>

<p>Now, you can ask the model to generate a new problem:</p>

<pre data-type="programlisting">
<em>Prompt</em>: Create a word problem based on the following math equation: 7 + 2 = 9.</pre>

<p>With few-shot examples, the model is more likely to generate a relevant word problem that matches the style and logic of the examples provided. Few-shot prompting is highly effective in tasks like text summarization, translation, and question generation.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Chain-of-Thought Prompting"><div class="sect2" id="ch05_chain_of_thought_prompting_1748896666814599">
<h2>Chain-of-Thought Prompting</h2>

<p><em>Chain-of-thought prompting</em> encourages<a contenteditable="false" data-primary="prompt engineering" data-secondary="chain-of-thought prompting" data-type="indexterm" id="icd501x"/><a contenteditable="false" data-primary="chain-of-thought agents and prompting" data-type="indexterm" id="icd502x"/> the model to break down its reasoning <span class="keep-together">process</span> step-by-step, making the reasoning process more explicit and understandable, rather than just providing the final answer. This approach is particularly valuable for complex tasks that require logical reasoning, multiple steps, or problem-solving, such as mathematical reasoning, decision-making, or any situation where intermediate steps are important. It helps models avoid making incorrect or oversimplified assumptions by encouraging them to evaluate different aspects of the task before reaching a conclusion.</p>

<p>Let’s say you’re asking the model to solve a math problem that involves multiple steps. Using chain-of-thought prompting, you would encourage the model to reason through the problem rather than simply provide an answer:</p>

<pre data-type="programlisting">
<em>Prompt</em>: Let's solve this step-by-step:
What is 8 × 6?
Step 1: First, break it into smaller numbers: 8 × (5 + 1).
Step 2: Now calculate: 8 × 5 = 40.
Step 3: Then calculate 8 × 1 = 8.
Step 4: Add the results: 40 + 8 = 48.
So, 8 × 6 = 48.</pre>

<p>Now, you can ask the model to solve a new problem:</p>

<pre data-type="programlisting">
<em>Prompt</em>: Let's solve this step-by-step: What is 12 × 7?</pre>

<p>Chain-of-thought prompting helps the model demonstrate its reasoning and ensures that it isn’t skipping over crucial details.</p>

<p>One powerful strategy is to combine these different types of prompting to leverage their individual strengths. For example, you might start with few-shot prompting to give the model some context and examples, then switch to chain-of-thought prompting to guide it through the reasoning process. This hybrid approach can be highly effective for more intricate tasks that require both pattern recognition and logical <span class="keep-together">reasoning.</span></p>

<pre data-type="programlisting">
<em>Prompt</em>: Here are some examples of how to generate creative descriptions for 
objects:
1. 'A tall oak tree with thick branches reaching out, casting a large shadow on 
the grass.'
2. 'A small, round pebble with smooth edges and a soft, pale color.'
Now, describe this object: 'A rusty old bicycle.' Let's break it down step-by-
step.</pre>

<p>This combined approach would help the model generate a detailed and coherent description by first learning from a few examples and then reasoning through the unique features of the <a contenteditable="false" data-primary="prompt engineering" data-secondary="chain-of-thought prompting" data-startref="icd501x" data-type="indexterm" id="id870"/><a contenteditable="false" data-primary="chain-of-thought agents and prompting" data-startref="icd502x" data-type="indexterm" id="id871"/>object.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Retrieval-Augmented Generation"><div class="sect2" id="ch05_retrieval_augmented_generation_rag_1748896666814649">
<h2>Retrieval-Augmented Generation</h2>

<p><em>Retrieval-augmented generation</em> (RAG) is<a contenteditable="false" data-primary="RAG (retrieval-augmented generation)" data-secondary="overview of" data-type="indexterm" id="icd509"/><a contenteditable="false" data-primary="model domain adaptation" data-secondary="RAG" data-type="indexterm" id="icd510"/> one<a contenteditable="false" data-primary="retrieval-augmented generation" data-see="RAG" data-type="indexterm" id="id872"/> of the most powerful techniques for combining pretrained language models with external knowledge sources. It uses retrieval-based methods to improve the generative model’s ability to handle complex queries or provide more accurate, fact-based responses. RAG models combine the power of information retrieval with text generation, making them especially useful for tasks where knowledge from large external corpora or databases is required.</p>

<p>RAG works by retrieving relevant documents or pieces of information from a knowledge base or search engine, which are then used to inform the generation process. This method enables the model to reference real-world data, producing responses that are not limited by the model’s preexisting knowledge.</p>

<p>In a typical RAG model, the input query goes through two main stages. First, in the <em>retrieval</em> stage, a retrieval system fetches relevant documents or text snippets from a knowledge base, search engine, or database. Then, in the <em>generation</em> stage, the LLM generates output based on the input query and the retrieved text snippets.</p>

<p>RAG allows the model to effectively handle complex questions, fact-check its responses, and dynamically reference a broad range of external information. For example, a question-answering system built with a RAG model could provide more accurate answers by first retrieving relevant documents or Wikipedia entries and then generating a response based on those documents.</p>

<p>The code in <a data-type="xref" href="#ch05_example_8_1748896666808830">Example 5-9</a> demonstrates how you can implement a simple RAG-based model.</p>

<div data-type="example" id="ch05_example_8_1748896666808830">
<h5><span class="label">Example 5-9. </span>RAG implementation</h5>

<pre data-type="programlisting">
from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration

tokenizer = RagTokenizer.from_pretrained("facebook/rag-token-nq")
retriever = RagRetriever.from_pretrained("facebook/rag-token-nq")

# Load the RAG model
model = RagSequenceForGeneration.from_pretrained("facebook/rag-token-nq")

question = "What is the capital of France?"

inputs = tokenizer(question, return_tensors="pt")

retrieved_docs = retriever.retrieve(question, return_tensors="pt")

# Generate an answer using the RAG model and the retrieved documents
outputs = model.generate(input_ids=inputs['input_ids'],
                context_input_ids=retrieved_docs['context_input_ids'],
                context_attention_mask=retrieved_docs['context_attention_mask'])

answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(answer)</pre>
</div>

<p>RAG can be particularly useful in scenarios like open-domain question answering, where the model may need to access and reference up-to-date or highly <a contenteditable="false" data-primary="RAG (retrieval-augmented generation)" data-secondary="overview of" data-startref="icd509" data-type="indexterm" id="id873"/><a contenteditable="false" data-primary="model domain adaptation" data-secondary="RAG" data-startref="icd510" data-type="indexterm" id="id874"/>specific information.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Semantic Kernel"><div class="sect2" id="ch05_semantic_kernel_1748896666814701">
<h2>Semantic Kernel</h2>

<p><a href="https://oreil.ly/ljuli">Semantic Kernel</a> is<a contenteditable="false" data-primary="prompt engineering" data-secondary="Semantic Kernel" data-type="indexterm" id="icd511"/><a contenteditable="false" data-primary="Semantic Kernel" data-type="indexterm" id="icd512"/> a framework designed to simplify integrating LLMs into applications that require dynamic knowledge, reasoning, and state tracking. It’s particularly useful when you want to build complex, modular AI systems that can interact with external APIs, knowledge bases, or decision-making processes. Semantic Kernel focuses on building more flexible AI systems that can handle a variety of tasks beyond just generating text. It allows for modularity, enabling developers to easily combine different components—such as embeddings, prompt templates, and custom functions—in a cohesive manner.</p>

<p>It supports asynchronous processing, which is useful for managing long-running tasks or interacting with external services, and can be used in conjunction with models that require reasoning through complex steps, like those using chain-of-thought prompting. The framework supports maintaining and<a contenteditable="false" data-primary="semantic memory" data-type="indexterm" id="id875"/> retrieving <em>semantic memory</em>, which helps the model to remember past interactions or previously retrieved information to generate more consistent results. Finally, Semantic Kernel can integrate external functions and APIs, making it easy to combine model inference with real-world data.</p>

<p>As <a data-type="xref" href="#ch05_example_9_1748896666808845">Example 5-10</a> shows, you can use Semantic Kernel to build a modular assistant that can:</p>

<ul>
	<li>
	<p>Retrieve historical information from a knowledge base</p>
	</li>
	<li>
	<p>Use an external API to fetch live data (such as stock prices)</p>
	</li>
	<li>
	<p>Process natural language instructions</p>
	</li>
	<li>
	<p>Perform complex reasoning tasks by chaining multiple AI functions together</p>
	</li>
</ul>

<div data-type="example" id="ch05_example_9_1748896666808845">
<h5><span class="label">Example 5-10. </span>Semantic Kernel</h5>

<pre data-type="programlisting">
from semantic_kernel import Kernel
from semantic_kernel.ai.openai import OpenAITextCompletion
from semantic_kernel.memory import MemoryStore
from semantic_kernel.plugins import AzureTextPlugin

kernel = Kernel()
kernel.add_ai("openai", OpenAITextCompletion(api_key="your-openai-api-key"))

# Set up memory for semantic memory handling
memory = MemoryStore()
kernel.add_memory("semantic_memory", memory)

# Define a simple chain-of-thought function
def chain_of_thought(input_text: str) -&gt; str:
    response = kernel.run_ai("openai", "text-davinci-003", input_text)
    return f"Thought Process: {response}"

kernel.add_function("chain_of_thought", chain_of_thought)

user_input = "How does quantum computing work?"

reasoned_output = kernel.invoke("chain_of_thought", user_input)
print("Reasoning Output:", reasoned_output)

kernel.add_plugin("external_api", AzureTextPlugin(api_key="your-azure-api-key"))
external_output = kernel.invoke("external_api", "fetch_knowledge", user_input)
print("External Output:", external_output)</pre>
</div>

<p>You can also integrate external functions (like fetching data from Azure APIs) to further enrich the model’s responses and maintain state across interactions. This makes Semantic Kernel an excellent choice for creating sophisticated AI-driven applications.</p>

<p>While RAG enhances generative models by integrating external knowledge sources for fact-based responses, Semantic Kernel provides a flexible framework for building modular AI systems with advanced reasoning and stateful interactions. If you want to make behavioral changes in a model, however, you <a contenteditable="false" data-primary="prompt engineering" data-secondary="Semantic Kernel" data-startref="icd511" data-type="indexterm" id="id876"/><a contenteditable="false" data-primary="Semantic Kernel" data-startref="icd512" data-type="indexterm" id="id877"/>should <a contenteditable="false" data-primary="model domain adaptation" data-secondary="prompt engineering" data-startref="icd507" data-type="indexterm" id="id878"/><a contenteditable="false" data-primary="prompt engineering" data-startref="icd508" data-type="indexterm" id="id879"/>use <em>fine-tuning.</em></p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Fine-Tuning"><div class="sect1" id="ch05_fine_tuning_1748896666814754">
<h1>Fine-Tuning</h1>

<p>Compared<a contenteditable="false" data-primary="model domain adaptation" data-secondary="fine-tuning" data-type="indexterm" id="icd513"/><a contenteditable="false" data-primary="fine-tuning" data-type="indexterm" id="icd514"/> to training from scratch, which requires massive amounts of data and compute, <em>fine-tuning</em> allows you to adapt an already-trained model to new tasks with fewer resources. By modifying the model’s parameters based on specific data or behaviors, fine-tuning makes LLMs more effective for specialized applications, whether for handling a particular industry’s terminology or modulating the model’s style and tone. Note that fine-tuning changes a model’s weights, and this means you must have access to it, either directly through a model checkpoint or indirectly, like OpenAI provides through its fine-tuning APIs.</p>

<p>Fine-tuning offers a range of strategies to adapt pretrained models to specialized tasks, improve their efficiency, and ensure they align with user expectations. Techniques like adaptive fine-tuning, adapters, and parameter-efficient methods help tailor LLMs to specific domains, all while reducing resource requirements. Fine-tuning isn’t just about improving task accuracy; it also focuses on adjusting model behavior, ensuring that outputs are ethically sound, efficient, and user-friendly. Whether you’re working with a complex domain-specific model or a general-purpose assistant, fine-tuning makes your models smarter, more efficient, and more aligned with your needs.</p>

<p>In this section, we’ll dive into several key strategies for fine-tuning LLMs, from adaptive fine-tuning to techniques like prefix tuning and parameter-efficient fine-tuning (PEFT), each serving different needs while maintaining efficiency.</p>

<section data-type="sect2" data-pdf-bookmark="Adaptive Fine-Tuning"><div class="sect2" id="ch05_adaptive_fine_tuning_1748896666814805">
<h2>Adaptive Fine-Tuning</h2>

<p><em>Adaptive fine-tuning</em> is<a contenteditable="false" data-primary="fine-tuning" data-secondary="adaptive" data-type="indexterm" id="id880"/><a contenteditable="false" data-primary="adaptive fine-tuning" data-type="indexterm" id="id881"/> the process of updating a model’s parameters so it can better handle a specific dataset or task. It involves training the model on new data that is more closely aligned with the task at hand. For example, if you have an LLM that has been pretrained on general web text, adaptive fine-tuning can help it specialize in a particular area like medical texts, legal jargon, or customer service interactions. The goal of adaptive fine-tuning is to adjust the model’s weights in a way that enables it to capture more domain-specific knowledge without forgetting the general understanding it already possesses.</p>

<p>Suppose you’re fine-tuning a model for medical question answering. Your base model might be trained on a diverse dataset, but for the fine-tuning dataset, you’ll use a collection of medical-related texts—such as research papers, clinical notes, and FAQs. Consider the following prompt:</p>

<pre data-type="programlisting">
<em>Question</em>: What are the symptoms of a heart attack?
<em>Answer</em>: Symptoms of a heart attack include chest pain, shortness of breath, 
nausea, and cold sweats.</pre>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Adapters (Single, Parallel, and Scaled Parallel)"><div class="sect2" id="ch05_adapters_single_parallel_and_scaled_parallel_1748896666814852">
<h2>Adapters (Single, Parallel, and Scaled Parallel)</h2>

<p><em>Adapters</em> are<a contenteditable="false" data-primary="fine-tuning" data-secondary="adapters" data-type="indexterm" id="id882"/><a contenteditable="false" data-primary="adapters" data-type="indexterm" id="id883"/> a powerful method for efficient fine-tuning. Instead of retraining the entire model, adapters introduce small, task-specific modules that are trained while leaving the original model’s parameters frozen. This approach makes fine-tuning much more computationally efficient since only a small part of the model is modified. Adapters are particularly useful when you need to apply fine-tuning across multiple tasks, as they allow the model to maintain its general capabilities while adapting to specific contexts. Methods for using adapters include:</p>

<dl>
	<dt>Single adapter</dt>
	<dd>
	<p>A <a contenteditable="false" data-primary="single adapters" data-type="indexterm" id="id884"/>single task-specific adapter is added to the model, allowing it to focus on one task. The rest of the model stays unchanged.</p>
	</dd>
	<dt>Parallel adapters</dt>
	<dd>
	<p>Multiple<a contenteditable="false" data-primary="parallel adapters" data-type="indexterm" id="id885"/> adapters can be trained in parallel for different tasks. Each adapter handles its task, and the original model’s weights remain frozen.</p>
	</dd>
	<dt>Scaled parallel adapters</dt>
	<dd>
	<p>For<a contenteditable="false" data-primary="scaled parallel adapters" data-type="indexterm" id="id886"/> more complex use cases, multiple adapters can be trained at different scales, allowing the model to handle more complex tasks and achieve higher performance without overburdening its architecture.</p>
	</dd>
</dl>

<p>Say you’re applying the model to two tasks: text summarization and sentiment analysis. You could introduce two parallel adapters, one fine-tuned for summarization and the other for sentiment analysis. The model would utilize the appropriate adapter for each task while still benefiting from its general knowledge.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Behavioral Fine-Tuning"><div class="sect2" id="ch05_behavioral_fine_tuning_1748896666814900">
<h2>Behavioral Fine-Tuning</h2>

<p><em>Behavioral fine-tuning</em> focuses<a contenteditable="false" data-primary="fine-tuning" data-secondary="behavioral" data-type="indexterm" id="id887"/><a contenteditable="false" data-primary="behavioral fine-tuning" data-type="indexterm" id="id888"/> on adjusting the model’s behavior to match specific expectations, such as producing more ethical, polite, or user-friendly outputs. In many real-world applications, it’s crucial that language models align with human values, especially when interacting with sensitive topics or making decisions that affect users. Through fine-tuning on data that reflects the desired behavior, the model can learn to produce responses that better adhere to a code of conduct or ethical guidelines. This is particularly useful in customer-service chatbots, healthcare assistants, and other models that interact directly with users.</p>

<p>For a chatbot that provides mental health advice, you could fine-tune the model using datasets that emphasize empathetic responses, ensuring that the model’s replies are both helpful and compassionate. Consider the following prompt and output:</p>

<pre data-type="programlisting">
<em>User</em>: I'm feeling really down today.
<em>Model (after behavioral fine-tuning)</em>: I'm so sorry to hear that. It's important 
to talk to someone when you're feeling this way. Would you like to share more?</pre>

<p>Behavioral fine-tuning is vital in ensuring that models don’t just deliver accurate responses but also reflect the right tone and ethics.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Prefix Tuning"><div class="sect2" id="ch05_prefix_tuning_1748896666814947">
<h2>Prefix Tuning</h2>

<p><em>Prefix tuning</em> is<a contenteditable="false" data-primary="fine-tuning" data-secondary="prefix tuning" data-type="indexterm" id="id889"/><a contenteditable="false" data-primary="prefix tuning" data-type="indexterm" id="id890"/> a technique for fine-tuning a model’s behavior for specific tasks without drastically changing its structure or altering its core weights. Instead of modifying the entire model, prefix tuning adjusts only a small, tunable part of the model: the <em>prefix</em>, a small input sequence that is prepended to the input data. The model uses the prefix to adapt its outputs to a specific task.</p>

<p>This method is highly efficient because it requires fewer resources than traditional fine-tuning and allows for specialized adaptations without having to retrain the entire model. If you are fine-tuning a model to generate poetry, the prefix might include a sequence that sets the tone or style of the poem, while the model generates the rest of the content accordingly:</p>

<pre data-type="programlisting">
<em>Prefix</em>: Write a romantic poem in the style of Shakespeare.
<em>Input</em>: 'The evening sky is painted in hues of orange.'</pre>

<p>Here, only the prefix is adjusted to favor the Shakespearean style, but the rest of the model remains unchanged.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Parameter-Efficient Fine-Tuning"><div class="sect2" id="ch05_parameter_efficient_fine_tuning_peft_1748896666814994">
<h2>Parameter-Efficient Fine-Tuning</h2>

<p><em>Parameter-efficient fine-tuning</em> (PEFT) <a contenteditable="false" data-primary="fine-tuning" data-secondary="PEFT" data-type="indexterm" id="id891"/><a contenteditable="false" data-primary="PEFT (parameter-efficient fine-tuning)" data-type="indexterm" id="id892"/>is a technique designed to fine-tune large models using minimal resources. Traditional fine-tuning involves modifying the entire model’s parameters, which can be both time-consuming and costly. PEFT techniques, like<a contenteditable="false" data-primary="LoRA (low-rank adaptation)" data-type="indexterm" id="id893"/> <em>low-rank adaptation</em> (LoRA) and<a contenteditable="false" data-primary="qLoRA (quantized LoRA)" data-type="indexterm" id="id894"/> quantized LoRA (qLoRA), focus on modifying only a small, low-ranked portion of the model’s weights, saving on memory and compute resources while maintaining model performance. They are particularly useful when working with very large models, such as GPT-3 or GPT-4, where full fine-tuning would be prohibitively expensive. LoRA introduces a low-rank approximation for the weight updates, reducing the number of parameters that need to be fine-tuned. This makes the process more efficient without sacrificing accuracy. And qLoRA builds on LoRA by incorporating quantization to reduce storage requirements even further, making it ideal for large-scale deployments.</p>

<p>For an LLM deployed in a resource-constrained environment, you could apply LoRA to adjust just the weights that handle specific tasks, such as summarization or question answering. This allows for quicker updates and lowers computational costs.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Instruction Tuning and Reinforcement Learning from Human Feedback"><div class="sect2" id="ch05_instruction_tuning_and_reinforcement_learning_from_1748896666815044">
<h2>Instruction Tuning and Reinforcement Learning <span class="keep-together">from Human Feedback</span></h2>

<p><em>Instruction tuning</em> involves<a contenteditable="false" data-primary="fine-tuning" data-secondary="instruction tuning" data-type="indexterm" id="id895"/><a contenteditable="false" data-primary="instruction tuning" data-type="indexterm" id="id896"/> fine-tuning a model so that it follows explicit instructions in a more precise and reliable way. This can be particularly useful when you need the model to consistently perform specific tasks based on user instructions or prompts.</p>

<p>With <em>reinforcement learning from human feedback </em>(RLHF),<a contenteditable="false" data-primary="fine-tuning" data-secondary="RLHF" data-type="indexterm" id="id897"/><a contenteditable="false" data-primary="RLHF (reinforcement learning from human feedback)" data-type="indexterm" id="id898"/><a contenteditable="false" data-primary="reinforcement learning from human feedback (RLHF)" data-type="indexterm" id="id899"/> the model receives feedback from humans on its outputs, allowing it to improve over time. This feedback loop helps the model better align with user expectations and improve the relevance, coherence, and overall quality of its responses.</p>

<p>RLHF is often used to fine-tune models for conversational agents or other interactive systems, ensuring that the model’s responses are not only accurate but also helpful and appropriate to the context.</p>

<p>For a virtual assistant, you might first fine-tune the model using instruction tuning to ensure it answers questions directly. Then, using RLHF, you would gather feedback from users on the helpfulness of responses and adjust the model to improve its conversational behavior.</p>

<p>An instruction-tuning prompt might be:</p>

<pre data-type="programlisting">
Answer the following question directly: What is the capital of France?</pre>

<p>The output:</p>

<pre data-type="programlisting">
The capital of France is Paris.</pre>

<p>An RLHF prompt might be:</p>

<pre data-type="programlisting">
How do I change the oil in my car?</pre>

<p>The output:</p>

<pre data-type="programlisting">
Changing the oil in your car involves draining the old oil, replacing the oil 
filter, and refilling with fresh oil. Would you like a step-by-step guide?</pre>

<p>With RLHF, the model can continue to learn and improve, aligning its behavior with real-world user needs.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Choosing Between Fine-Tuning and Prompt Engineering"><div class="sect2" id="ch05_choosing_between_fine_tuning_and_prompt_engineerin_1748896666815096">
<h2>Choosing Between Fine-Tuning and Prompt Engineering</h2>

<p>If <a contenteditable="false" data-primary="fine-tuning" data-secondary="prompt engineering versus" data-type="indexterm" id="id900"/><a contenteditable="false" data-primary="prompt engineering" data-secondary="fine-tuning versus" data-type="indexterm" id="id901"/>you don’t have access to a way to modify the model’s weights and you want to adapt your model for a specific domain, you will have to use prompt engineering. But when both choices are available, which one should you use?</p>

<p>The first thing to consider is the cost, as fine-tuning is expensive in terms of computational costs. You can usually get to a better prompt with a few hours of experimentation, but running a fine-tuning experiment can cost thousands of dollars. A recent price list from OpenAI<a contenteditable="false" data-primary="OpenAI" data-secondary="GPT series" data-type="indexterm" id="id902"/><a contenteditable="false" data-primary="GPT (generative pretrained transformer) series" data-type="indexterm" id="id903"/><a contenteditable="false" data-primary="generative pretrained transformer (GPT) series" data-type="indexterm" id="id904"/> (as of this writing) lists the fine-tuning cost for its latest GPT-4o model at $25,000 per million tokens.</p>

<p>While fine-tuning charges the costs up front, prompt engineering is more like a mortgage. Developing a larger prompt through prompt engineering will increase your costs for every request, whereas inference costs the same whether the model is fine-tuned or not. One additional thing to consider is that there is a lot of change in the LLM space these days, so the time horizons to recoup costs are likely to be short. If fine-tuning and prompt engineering have the same performance and costs over a 10-year horizon, but the model you’re using will have a 2-year life span, it’s not cost-effective to prepay for 10 years of something that you’re only going to use for 2 years. Prompt engineering in this case would be a better choice in terms of cost.</p>

<p>Even if you have access to the model weights, don’t need to worry about costs, and only want to focus on performance, it helps to know that fine-tuning and prompt engineering solve different problems. Prompt engineering changes what the model <em>knows about</em>, giving it more context. RAG does what prompt engineering does, but on a much larger scale, using a system to generate prompts dynamically based on inputs. On the other hand, fine-tuning changes how the model <em>behaves</em>. The quadrant diagram in <a data-type="xref" href="#ch05_figure_2_1748896666799695">Figure 5-2</a> illustrates<a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="context optimization" data-type="indexterm" id="id905"/> this.</p>

<p>For example, let’s assume the model you’re using has all the knowledge it needs, but you want it to generate answers using a specific XML format instead of the usual chat outputs because your output will be consumed by another system. In this case, fine-tuning the model will yield much better performance than giving it lots of examples of how you want the output to look through prompt engineering.</p>

<figure class="width-75"><div id="ch05_figure_2_1748896666799695" class="figure"><img alt="" src="assets/llmo_0502.png" width="1178" height="656"/>
<h6><span class="label">Figure 5-2. </span>LLM and context optimization</h6>
</div></figure>

<p>It’s worthwhile to point out an unexpected consequence of changing how a model behaves through fine-tuning: it can cause the model to stop doing things it did before. Let’s say you are using a model like GPT-3.5-turbo to generate blog posts about your product, and it’s doing a good job, but the posts are not very technical. An AI engineer suggests fine-tuning the model using the messages in the “product chat” channel inside your company, where people discuss technical features of the product. After fine-tuning the model, you ask it to “generate a 500-word blog post about feature <em>X</em> of our product,” something that it would do reasonably well before, just without much technical depth. Now it answers: “I’m too busy.” Fine-tuning changed how the model behaves. In this case, a RAG solution that searches the product chat data and creates a prompt describing feature <em>X</em> to the old model would work a lot <a contenteditable="false" data-primary="model domain adaptation" data-secondary="fine-tuning" data-startref="icd513" data-type="indexterm" id="id906"/><a contenteditable="false" data-primary="fine-tuning" data-startref="icd514" data-type="indexterm" id="id907"/>better.</p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Mixture of Experts"><div class="sect1" id="ch05_mixture_of_experts_moe_1748896666815148">
<h1>Mixture of Experts</h1>

<p>Adaptive<a contenteditable="false" data-primary="model domain adaptation" data-secondary="MoE" data-type="indexterm" id="icd515"/><a contenteditable="false" data-primary="MoE (mixture of experts)" data-type="indexterm" id="icd516"/><a contenteditable="false" data-primary="mixture of experts (MoE)" data-type="indexterm" id="icd516a"/> fine-tuning and PEFT methods optimize the adaptation of large language models by selectively updating parameters or leveraging instruction tuning. A more recent technique, <em>mixture of experts</em> (MoE), approaches optimization from a different angle, that of architectural modularity and conditional computation. Unlike fine-tuning, which changes the model’s parameters after training, MoE changes the model’s structure itself by leveraging many “experts,” which are smaller specialized subnetworks inside one big model (see <a data-type="xref" href="#ch05_figure_3_1748896666799717">Figure 5-3</a>).</p>

<p>Instead of using the whole model to make inferences, a gating system selects and activates only a few of these experts, based on the input. This means that the model uses only part of its capacity to answer each query. The benefit? You can build a huge model with trillions of parameters but keep the computation cost low, because only a small piece of it runs each time.</p>

<p>This is different from adaptive fine-tuning or parameter-efficient tuning in which you update the model to better handle new tasks. MoE lets the model specialize inside itself, which means that some experts get better at certain types of tasks or data, while others focus elsewhere. The model learns to route inputs dynamically, making it flexible across many domains without retraining the whole thing every time.</p>

<p>That said, MoEs aren’t perfect. If the gating system doesn’t spread the work evenly, only a few experts do most of the work, wasting the rest of the model and reducing efficiency. Also, training these models is more complex and requires special software and hardware support to get the speed and cost benefits.  </p>

<figure><div id="ch05_figure_3_1748896666799717" class="figure"><img alt="" src="assets/llmo_0503.png" width="992" height="709"/>
<h6><span class="label">Figure 5-3. </span>A visualization of how MoE works (source: <a href="https://oreil.ly/r4ys1">“Mixture of Experts”</a>)</h6>
</div></figure>

<p>MoE models like<a contenteditable="false" data-primary="GShard" data-type="indexterm" id="id908"/> GShard, DeepSeek<a contenteditable="false" data-primary="DeepSeek" data-type="indexterm" id="id909"/>, and others change how LLMs handle scale by splitting the model into many smaller expert subnetworks and selectively activating only a few for each input token. The key to this capability is the<a contenteditable="false" data-primary="gating network, in MoE models" data-type="indexterm" id="icd505x"/> gating network, a small module that uses the hidden state of each token to produce a score for every expert. In GShard, these scores go through a softmax function, which converts a vector of raw prediction scores into a probability distribution over all experts. The top two experts per token are selected, and the token’s representation is sent to both experts, weighted by their gating probabilities. This routing of work to two experts can improve the model’s expressiveness but increases communication overhead, since tokens must be sent to multiple experts on different devices.</p>

<p>Switch Transformer, on the other hand, simplifies this by using<a contenteditable="false" data-primary="hard routing" data-type="indexterm" id="id910"/> hard routing. <em>Hard routing</em> requires the gating to pick the single expert with the highest score for each token. This means each token activates only one expert, reducing cross-device communication and memory usage. The gating output is a one-shot vector indicating which expert is responsible, and this top-one routing cuts down the data that needs to move between accelerators.</p>

<p>One common challenge for all MoE models<a contenteditable="false" data-primary="load balancing" data-type="indexterm" id="id911"/> is <em>load balancing</em>. Without constraints, the gating tends to funnel most tokens to a small set of popular experts, causing some experts to be overloaded while others remain idle. This expert collapse wastes capacity and slows training convergence. To fix this, training adds a load-balancing loss term to the main objective. This loss term measures how tokens are distributed across experts by first calculating the fraction of tokens assigned and the gating probability mass for each expert. It then computes the coefficient of variation across these values and penalizes uneven distributions, forcing the gating network to spread tokens more uniformly. This keeps all experts busy and fully utilizes the model’s parameter budget.</p>

<p>Each expert has a fixed capacity in that it can process only a limited number of tokens per batch to fit within memory constraints. If too many tokens are routed to an expert, excess tokens are either dropped or rerouted. Although such token dropping can prevent memory overflow, it can also cause some input data to be ignored during training—a trade-off that needs careful tuning.</p>

<p>During<a contenteditable="false" data-primary="backpropagation" data-type="indexterm" id="id912"/> <em>backpropagation</em>, gradients flow only through the experts that were activated for each token. Experts not involved in processing a given token receive no gradient updates. Thus, computation and memory use are less than they would otherwise be, given the model’s huge parameter count. This sparsity in gradient flow is one reason MoEs can scale efficiently.</p>

<p>Training MoEs is tricky, and the process can be unstable. Researchers use several techniques to enhance stability. For example, gating weights are carefully initialized to avoid extreme outputs early on, dropout can be applied to gating outputs to prevent the gating network from becoming overconfident based on a few experts, and gradient clipping can be used to keep updates <a contenteditable="false" data-primary="gating network, in MoE models" data-startref="icd505x" data-type="indexterm" id="id913"/>​stable. Load balancing losses not only improves utilization but also helps to stabilize routing decisions during training.</p>

<p>Overall, MoE is another way to make huge models more scalable and adaptable. That said, it often complements rather than replaces fine-tuning <a contenteditable="false" data-primary="model domain adaptation" data-secondary="MoE" data-startref="icd515" data-type="indexterm" id="id914"/><a contenteditable="false" data-primary="MoE (mixture of experts)" data-startref="icd516" data-type="indexterm" id="id915"/><a contenteditable="false" data-primary="mixture of experts (MoE)" data-startref="icd516a" data-type="indexterm" id="id916"/>methods.</p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Model Optimization for Resource-Constrained Devices"><div class="sect1" id="ch05_model_optimization_for_resource_constrained_device_1748896666815198">
<h1>Model Optimization for Resource-Constrained Devices</h1>

<p>Optimizing<a contenteditable="false" data-primary="model domain adaptation" data-secondary="resource-constrained devices" data-type="indexterm" id="id917"/> a model for resource-limited devices ensures that it runs smoothly on low-power hardware like mobile devices or edge<a contenteditable="false" data-primary="compression techniques" data-type="indexterm" id="icd504x"/> devices.</p>

<p><em>Compression techniques</em> help reduce the computational and memory footprint of these models while maintaining their performance. This is particularly important for deploying LLMs on edge devices or optimizing their runtime in cloud environments. There are several techniques to compress LLMs. Let’s look into them one by one:</p>

<dl>
	<dt>Prompt caching</dt>
	<dd>
	<p>Prompt caching <a contenteditable="false" data-primary="prompt caching" data-type="indexterm" id="id918"/><a contenteditable="false" data-primary="caching" data-secondary="prompt" data-type="indexterm" id="id919"/>involves storing previously computed responses for frequently occurring prompts. Instead of rerunning the entire model, the cached results are quickly retrieved and returned. This is particularly useful for scenarios where the same or similar prompts are repeatedly queried, such as with customer support chatbots or knowledge retrieval systems, and for accelerating inference by avoiding redundant computations or reducing costs in high-traffic systems.</p>
	</dd>
	<dt>Key–value caching</dt>
	<dd>
	<p>Key–value (KV) caching<a contenteditable="false" data-primary="KV (key–value) caching" data-type="indexterm" id="id920"/><a contenteditable="false" data-primary="caching" data-secondary="key–value" data-type="indexterm" id="id921"/> <a contenteditable="false" data-primary="key–value (KV) caching" data-type="indexterm" id="id922"/>optimizes transformer-based LLMs by caching attention-related computations, especially in scenarios involving sequential or streaming input data. By storing the key (K) and value (V) tensors computed during forward passes, subsequent tokens can reuse these precomputed tensors, reducing redundancy in computation.</p>
	</dd>
	<dd>
	<p>KV caching comes in handy when speeding up autoregressive generation (as with GPT-style models) or improving latency for generating long texts.</p>
	</dd>
	<dt>Quantization</dt>
	<dd>
	<p>Quantization<a contenteditable="false" data-primary="quantization" data-type="indexterm" id="id923"/> reduces the precision of the model’s weights, such as from 32-bit floating point to 8-bit or even 4-bit. This drastically reduces memory requirements and speeds up inference without a substantial loss in model accuracy. There are different types of quantization techniques. In  <em>static quantization<a contenteditable="false" data-primary=" static quantization" data-type="indexterm" id="id924"/></em>, weights and activations are quantized before runtime. In  <em>dynamic quantization<a contenteditable="false" data-primary="dynamic quantization" data-type="indexterm" id="id925"/></em>, activations are quantized on the fly during runtime. Finally,<a contenteditable="false" data-primary="QAT (quantization-aware training)" data-type="indexterm" id="id926"/> with <em>quantization-aware training</em> (QAT), the model is trained to account for quantization effects, leading to better accuracy after quantization.</p>
	</dd>
	<dt>Pruning</dt>
	<dd>
	<p>Pruning<a contenteditable="false" data-primary="pruning" data-type="indexterm" id="id927"/> removes less significant weights, neurons, or entire layers from the model, reducing its size and computational complexity. <em>Structured pruning<a contenteditable="false" data-primary="structured pruning" data-type="indexterm" id="id928"/></em> removes components systematically (like neurons in a layer), while <em>unstructured pruning<a contenteditable="false" data-primary="unstructured pruning" data-type="indexterm" id="id929"/></em> removes individual weights based on importance.</p>
	</dd>
	<dt>Distillation</dt>
	<dd>
	<p>Model<a contenteditable="false" data-primary="distillation" data-type="indexterm" id="id930"/> distillation trains a smaller “student” model to replicate the behavior of a larger “teacher” model. The student model learns by mimicking the teacher’s outputs, including logits or intermediate <a contenteditable="false" data-primary="compression techniques" data-startref="icd504x" data-type="indexterm" id="id931"/>​layer <a contenteditable="false" data-primary="LLM-based applications" data-secondary="model domain adaptation for" data-startref="icd506" data-type="indexterm" id="id932"/><a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="model domain adaptation for" data-startref="icd506a" data-type="indexterm" id="id933"/>representations.</p>
	</dd>
</dl>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Lessons for Effective LLM Development"><div class="sect1" id="ch05_lessons_for_effective_llm_development_1748896666815250">
<h1>Lessons for Effective LLM Development</h1>

<p>Training<a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="lessons for effective development of" data-type="indexterm" id="icd517"/> LLMs is a complex process requiring precise strategies to balance efficiency, cost, and performance. Best practices in this space keep evolving. This section looks at some optimizations we haven’t covered yet, including scaling laws, model size <span class="keep-together">versus</span> data trade-offs, learning-rate optimizations, pitfalls like overtraining, and innovative techniques like speculative sampling.</p>

<section data-type="sect2" data-pdf-bookmark="Scaling Law"><div class="sect2" id="ch05_scaling_law_1748896666815300">
<h2>Scaling Law</h2>

<p>Scaling <a contenteditable="false" data-primary="scaling" data-secondary="scaling laws" data-type="indexterm" id="id934"/><a contenteditable="false" data-primary="model domain adaptation" data-secondary="scaling laws" data-type="indexterm" id="id935"/>laws (<a data-type="xref" href="#ch05_example_10_1748896666808859">Example 5-11</a>) describe how model performance improves with increases in data, model size, or compute. Research has shown that performance gains often follow a predictable curve, with diminishing returns beyond certain thresholds. The balance lies in optimizing the interplay between model size and training data. Doubling both the model size and the training dataset typically results in better performance than doubling only one. It’s also important to know that models can become undertrained<a contenteditable="false" data-primary="undertrained LLMs" data-type="indexterm" id="id936"/> or overparameterized if the data isn’t scaled appropriately.</p>

<div data-type="example" id="ch05_example_10_1748896666808859">
<h5><span class="label">Example 5-11. </span>Scaling law</h5>

<pre data-type="programlisting">
import matplotlib.pyplot as plt
import numpy as np

# Simulate scaling law data
model_sizes = np.logspace(1, 4, 100)  # Model sizes from 10^1 to 10^4
performance = np.log(model_sizes) / np.log(10)  # Simulated performance improvement

# Plot the scaling law
plt.plot(model_sizes, performance, label="Scaling Law")
plt.xscale("log")
plt.xlabel("Model Size (log scale)")
plt.ylabel("Performance")
plt.title("Scaling Law for LLMs")
plt.legend()
plt.show()</pre>
</div>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Chinchilla Models"><div class="sect2" id="ch05_chinchilla_models_1748896666815346">
<h2>Chinchilla Models</h2>

<p>Chinchilla<a contenteditable="false" data-primary="Chinchilla models" data-type="indexterm" id="id937"/><a contenteditable="false" data-primary="model domain adaptation" data-secondary="Chinchilla models" data-type="indexterm" id="id938"/> models (<a data-type="xref" href="#ch05_example_11_1748896666808874">Example 5-12</a>) challenge the paradigm of building increasingly larger models. Instead, they prioritize training on more data while keeping the model size fixed. This approach achieves comparable or even better performance at lower costs. For a fixed compute budget, smaller models trained on larger datasets outperform larger models trained on limited data.</p>

<div data-type="example" id="ch05_example_11_1748896666808874">
<h5><span class="label">Example 5-12. </span>Chinchilla model</h5>

<pre data-type="programlisting">
model_size = "medium"  # Fixed model size
data_multiplier = 4    # Increase dataset size

model = load_model(size=model_size)
dataset = augment_dataset(original_dataset, multiplier=data_multiplier)

train_model(model, dataset)
evaluate_model(model)</pre>
</div>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Learning-Rate Optimization"><div class="sect2" id="ch05_learning_rate_optimization_1748896666815393">
<h2>Learning-Rate Optimization</h2>

<p>Choosing<a contenteditable="false" data-primary="learning-rate optimization" data-type="indexterm" id="id939"/><a contenteditable="false" data-primary="model domain adaptation" data-secondary="learning-rate optimization" data-type="indexterm" id="id940"/> the right learning rate is critical for effective training. An optimal learning rate allows models to converge faster and avoid pitfalls like vanishing gradients or oscillations. Gradually increase the learning rate at the start of training to stabilize convergence. Then, smoothly reduce the learning rate over time for better final convergence. To do this, try running the code in <a data-type="xref" href="#ch05_example_12_1748896666808889">Example 5-13</a>.</p>

<div data-type="example" id="ch05_example_12_1748896666808889">
<h5><span class="label">Example 5-13. </span>Optimizing the learning rate</h5>

<pre data-type="programlisting">
    print(f"Epoch {epoch}, Learning Rate: {scheduler.get_last_lr()}")
    from torch.optim.lr_scheduler import CosineAnnealingLR import torch

model = torch.nn.Linear(10, 2)
optimizer = torch.optim.Adam(model.parameters(), lr=0.1)

# Cosine learning rate scheduler
scheduler = CosineAnnealingLR(optimizer, T_max=50)

# Training loop
for epoch in range(100):
    # Forward pass, loss computation, backpropagation...
    optimizer.step()
    scheduler.step()</pre>
</div>

<p><em>Overtraining </em><a contenteditable="false" data-primary="overtraining" data-type="indexterm" id="id941"/>occurs when a model becomes too well adapted to the training dataset and thus overly specialized, leading to poor generalization on unseen data. If you find that validation loss increases while training loss decreases, or if your model’s predictions on test data are overly confident but incorrect, the model may be overtrained. Early stopping and other regularization techniques help mitigate this.<a contenteditable="false" data-primary="regularization" data-type="indexterm" id="id942"/> <em>Regularization</em> involves adding a penalty term to the model’s loss function to discourage it from learning overly complex relationships with the training data. With<a contenteditable="false" data-primary="early stopping" data-type="indexterm" id="id943"/> <em>early stopping</em> (<a data-type="xref" href="#ch05_example_13_1748896666808902">Example 5-14</a>), a performance metric (like accuracy or loss) is monitored on a validation set, and the training is halted when this metric plateaus or deteriorates.</p>

<div data-type="example" id="ch05_example_13_1748896666808902">
<h5><span class="label">Example 5-14. </span>Early stopping</h5>

<pre data-type="programlisting">
from pytorch_lightning.callbacks import EarlyStopping

# Define early stopping
early_stopping = EarlyStopping(monitor="val_loss", patience=3, verbose=True)

trainer = Trainer(callbacks=[early_stopping])
trainer.fit(model, train_dataloader, val_dataloader)</pre>
</div>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Speculative Sampling"><div class="sect2" id="ch05_speculative_sampling_1748896666815441">
<h2>Speculative Sampling</h2>

<p><em>Speculative sampling</em> is<a contenteditable="false" data-primary="speculative sampling" data-type="indexterm" id="id944"/><a contenteditable="false" data-primary="model domain adaptation" data-secondary="speculative sampling" data-type="indexterm" id="id945"/> a method to speed up autoregressive decoding during inference. It involves using a smaller, faster model to predict multiple token candidates, which are then verified by the larger model. This can be really useful for applications requiring low-latency generation, like real-time conversational agents.</p>

<p>Understanding different training strategies and pitfalls is important for optimizing LLMs. Techniques like scaling laws and Chinchilla models guide compute-efficient training, while learning-rate optimization and speculative sampling improve both training and inference dynamics. Also, avoiding overtraining ensures that models generalize well to real-world data. Incorporating these lessons will lead to more robust and cost-effective <a contenteditable="false" data-primary="LLMs (large language models)" data-secondary="lessons for effective development of" data-startref="icd517" data-type="indexterm" id="id946"/>LLMs.</p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Conclusion"><div class="sect1" id="ch05_conclusion_1748896666815492">
<h1>Conclusion</h1>

<p>In this chapter, you learned about critical aspects of optimizing the deployment of LLMs. From understanding the methods of domain adaptation like prompt engineering, fine-tuning, and retrieval-augmented generation (RAG) to exploring efficient model deployment strategies, the chapter covered the foundational knowledge you need to adapt LLMs for specific tasks and resource constraints. Each method has unique strengths, allowing developers to align the model’s behavior, knowledge, or outputs with organizational needs and technical limitations. We know historically that there will be naming and renaming of a lot terms and techniques in AI/ML. By the time this book comes out, you may hear terms like context engineering, the fundamentals of which we have already covered in this book. Regardless, the term you use doesn’t matter for engineering LLMs as long as you build for the key goals of LLMOps systems: reliability, scalability, robustness, and security.</p>

<p>The chapter also examined how to optimize LLMs for resource-constrained environments through techniques such as quantization, pruning, and distillation, with an emphasis on the importance of balancing computational cost <a contenteditable="false" data-primary="model domain adaptation" data-startref="icd509x" data-type="indexterm" id="id947"/>with performance.</p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="References"><div class="sect1" id="ch05_references_1748896666815537">
<h1>References</h1>



<p>Karpathy, Andrej. <a href="https://oreil.ly/PfnyZ">“Let’s Build GPT: From Scratch, in Code, Spelled Out”</a>, YouTube, January 17, 2023.</p>

<p>Kimothi, Abhinav. <a href="https://oreil.ly/A-C1L">“3 LLM Architectures”</a>, <em>Medium</em>, July 24, 2023.</p>

<p>Microsoft. <a href="https://oreil.ly/xrjkE">“Introduction to Semantic Kernel”</a>, June 24, 2024.</p>

<p>Wang, Zian (Andy). <a href="https://oreil.ly/stFQa">“Mixture of Experts: How an Ensemble of AI Models Decide As One”</a>, Deepgram, June 27, 2024.</p>
</div></section>
</div></section></div></div></body></html>