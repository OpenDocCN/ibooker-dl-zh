- en: Chapter 4\. Architectures and Learning Objectives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In Chapters [2](ch02.html#ch02) and [3](ch03.html#chapter-LLM-tokenization),
    we discussed some of the key ingredients that go into making a language model:
    the training datasets, and the vocabulary and tokenizer. Next, let’s complete
    the puzzle by learning about the models themselves, the architectures underpinning
    them, and their learning objectives.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn the composition of language models and their
    structure. Modern-day language models are predominantly based on the Transformer
    architecture, and hence we will devote most of our focus to understanding it,
    by going through each component of the architecture in detail. Over the last few
    years, several variants and alternatives to the original Transformer architecture
    have been proposed. We will go through the promising ones, including Mixture of
    Experts (MoE) models. We will also examine commonly used learning objectives the
    language models are trained over, including next-token prediction. Finally, we
    will bring together the concepts of the last three chapters in practice by learning
    how to pre-train a language model from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Preliminaries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just about every contemporary language model is based on neural networks, composed
    of processing units called *neurons*. While modern neural networks do not resemble
    the workings of the human brain at all, many of the ideas behind neural networks
    and the terminology used is inspired by the field of neuroscience.
  prefs: []
  type: TYPE_NORMAL
- en: The neurons in a neural network are connected to each other according to some
    configuration. Each connection between a pair of neurons is associated with a
    weight (also called *parameter*), indicating the strength of the connection. The
    role these neurons play and the way they are connected to each other constitutes
    the *architecture* of the model.
  prefs: []
  type: TYPE_NORMAL
- en: The early 2010s saw the proliferation of multi-layer architectures, with layers
    of neurons stacked on top of each other, each layer extracting progressively more
    complex features of the input. This paradigm is called *deep learning.*
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-1](#MLP-00) depicts a simple multi-layer neural network, also called
    the multi-layer perceptron.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Transformer](assets/dllm_0401.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-1\. Multi-layer perceptron
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For a more comprehensive treatment of neural networks, refer to [Goldberg’s
    book](https://oreil.ly/oDc6x) on neural network–based natural language processing.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in [Chapter 1](ch01.html#chapter_llm-introduction), language models
    are primarily pre-trained using self-supervised learning. Input text from the
    training dataset is tokenized and converted to vector form. The input is then
    propagated through the neural network, affected by its weights and *activation
    functions*, the latter introducing nonlinearity to the model. The output of the
    model is compared to the expected output, called the gold truth. The weights of
    the output are adapted such that next time for the same input, the output can
    be closer to the gold truth.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, this adaptation process is implemented through a *loss function*.
    The goal of the model is to minimize the loss, which is the difference between
    the model output and the gold truth. To minimize the loss, the weights are updated
    using a gradient-descent based method, called backpropagation. I strongly recommend
    developing an intuitive understanding of this algorithm before diving into model
    training.
  prefs: []
  type: TYPE_NORMAL
- en: Representing Meaning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While describing neural network–based architectures in the previous section,
    we glossed over the fact that the input text is converted into vectors and then
    propagated through the network. What are these vectors composed of and what do
    they represent? Ideally, after the model is trained, these vectors should accurately
    represent some aspect of the meaning of the underlying text, including its social
    connotations. Developing the right representations for modalities like text or
    images is a very active field of research, called *representation learning*.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When training a language model from scratch, these vectors initially mean nothing,
    as they are randomly generated. In practice, there are initialization algorithms
    used like Glorot, He, etc. Refer to [this report](https://oreil.ly/A8Iro) for
    a primer on neural network initialization.
  prefs: []
  type: TYPE_NORMAL
- en: How can a list of numbers represent meaning? It is hard for humans to describe
    the meaning of a word or sentence, let alone represent it in numerical form that
    can be processed by a computer. The *form* of a word, i.e., the letters that comprise
    it, usually do not give any information whatsoever about the meaning it represents.
    For example, the sequence of letters in the word *umbrella* contains no hints
    about its meaning, even if you are already exposed to thousands of other words
    in the English language.
  prefs: []
  type: TYPE_NORMAL
- en: 'The prominent way of representing meaning in numerical form is through the
    *distributional hypothesis* framework. The distributional hypothesis states that
    words that have similar meaning occur in similar contexts. The implication of
    this hypothesis is best represented by the adage:'
  prefs: []
  type: TYPE_NORMAL
- en: You shall know a word by the company it keeps.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: John Rupert Firth, 1957
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is one of the primary ways in which we pick up the meaning of words we
    haven’t encountered previously, without needing to look them up in a dictionary.
    A large number of words we know weren’t learned from the dictionary or by explicitly
    learning the meaning of a word but by estimating meaning based on the contexts
    words appear in.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s investigate how the distributional hypothesis works in practice. The Natural
    Language Toolkit (NLTK) library provides a feature called *concordance view*,
    which presents you with the surrounding contexts that a given word appears in
    a corpus.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s see the contexts in which the word “nervous” occurs in the
    Jane Austen classic *Emma*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The output looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The Transformer Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have developed an intuition on how text is represented in vector
    form, let’s further explore the canonical architecture used for training language
    models today, the Transformer.
  prefs: []
  type: TYPE_NORMAL
- en: In the mid 2010s, the predominant architectures used for NLP tasks were recurrent
    neural networks, specifically a variant called long short-term memory (LSTM).
    While knowledge of recurrent neural networks is not a prerequisite for this book,
    I recommend [*Neural Network Methods for Natural Language Processing*](https://oreil.ly/CHCTd)
    for more details.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent neural networks were sequence models, which means they processed text
    one token at a time, sequentially. A single vector was used to represent the state
    of the entire sequence, so as the sequence grew longer, more and more information
    needed to be captured in the single state vector. Because of the sequential nature
    of processing, long-range dependencies were harder to capture, as the content
    from the beginning of the sequence would be harder to retain.
  prefs: []
  type: TYPE_NORMAL
- en: 'This issue was candidly articulated by Ray Mooney, a senior computer scientist
    who remarked at the Association for Computational Linguistics (ACL) 2014 conference:'
  prefs: []
  type: TYPE_NORMAL
- en: You can’t cram the meaning of a whole %&!$# sentence into a single $&!#* vector!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ray Mooney, 2014
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Thus, there was a need for an architecture that solved for the deficiencies
    of LSTM: the limitations in representing long-range dependencies, the dependence
    on a single vector for representing the state of the entire sequence, and more.
    The Transformer architecture was designed to address these issues.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-2](#Transformer0) depicts the original Transformer architecture,
    developed in 2017 by [Vaswani et al.](https://oreil.ly/tIvGZ) As we can see in
    the figure, a Transformer model is typically composed of Transformer blocks stacked
    on top of each other, called *layers*. The key components of each block are:'
  prefs: []
  type: TYPE_NORMAL
- en: Self-attention
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Positional encoding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feedforward networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalization blocks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Transformer](assets/dllm_0402.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-2\. The Transformer architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'At the beginning of the first block is a special layer called the *embedding*
    layer. This is where the tokens in the input text are mapped to their corresponding
    vector. The embedding layer is a matrix whose size is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'On Hugging Face, we can inspect the embedding layer as such, using the `transformers`
    library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The embedding vectors are the inputs that are then propagated through the rest
    of the network.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s go through each of the components in a Transformer block in detail
    and explore their role in the modeling process.
  prefs: []
  type: TYPE_NORMAL
- en: Self-Attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The self-attention mechanism draws on the same principle as the distributional
    hypothesis introduced in [“Representing Meaning”](#representing-meaning), emphasizing
    the role of context in shaping the meaning of a token. This operation generates
    representations for each token in a text sequence, capturing various aspects of
    language like syntax, semantics, and even pragmatics.
  prefs: []
  type: TYPE_NORMAL
- en: In the standard self-attention implementation, the representation of each token
    is a function of the representation of all other tokens in the sequence. Given
    a token for which we are calculating its representation, tokens in the sequence
    that contribute more to the meaning of the token are given more weight.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider the sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 4-3](#Attention-map) depicts how the representation for the token *he*
    is heavily weighted by the representation of the token *Mark*. In this case, the
    token *he* is a pronoun used to describe Mark in shorthand. In NLP, mapping a
    pronoun to its referent is called *co-reference resolution*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Attention-map](assets/dllm_0403.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-3\. Attention map
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In practice, self-attention in the Transformer is calculated using three sets
    of weight matrices called queries, keys, and values. Let’s go through them in
    detail. [Figure 4-4](#kqv) shows how the query, key, and value matrices are used
    in the self-attention calculation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each token is represented by its embedding vector. This vector is multiplied
    with the query, key, and value weight matrices to generate three input vectors.
    Self-attention for each token is then calculated like this:'
  prefs: []
  type: TYPE_NORMAL
- en: For each token, the dot products of its query vector with the key vectors of
    all the tokens (including itself) are taken. The resulting values are called attention
    scores.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The scores are scaled down by dividing them by the square root of the dimension
    of the key vectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The scores are then passed through a [*softmax function*](https://oreil.ly/b6gHV)
    to turn them into a probability distribution that sums to 1\. The softmax activation
    function tends to amplify larger values, hence the reason for scaling down the
    attention scores in the previous step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The normalized attention scores are then multiplied by the value vector for
    the corresponding token. The normalized attention score can be interpreted as
    the proportion that each token contributes to the representation of a given token.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In practice, there are multiple sets of query, key, and value vectors, calculating
    parallel representations. This is called multi-headed attention. The idea behind
    using multiple heads is that the model gets sufficient capacity to model various
    aspects of the input. The more the number of heads, the more chances that the
    *right* aspects of the input are being represented.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![kqv](assets/dllm_0404.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-4\. Self-attention calculation
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This is how we implement self-attention in code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In some Transformer variants, self-attention is calculated only on a subset
    of tokens in the sequence; thus the vector representation of a token is a function
    of the representations of only some and not all the tokens in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Positional Encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As discussed earlier, pre-Transformer architectures like LSTM were sequence
    models, with tokens being processed one after the other. Thus the positional information
    about the tokens, i.e., the relative positions of the tokens in a sequence, was
    implicitly baked into the model. However, for Transformers all calculations are
    done in parallel, and positional information should be fed to the model explicitly.
    Several methods have been proposed to add positional information, and this is
    still a very active field of research. Some of the common methods used in LLMs
    today include:'
  prefs: []
  type: TYPE_NORMAL
- en: Absolute positional embeddings
  prefs: []
  type: TYPE_NORMAL
- en: These were used in the original Transformer implementation by [Vaswani et al.](https://oreil.ly/CDq60);
    examples of models using absolute positional embeddings include earlier models
    like BERT and RoBERTa.
  prefs: []
  type: TYPE_NORMAL
- en: Attention with Linear Biases (ALiBi)
  prefs: []
  type: TYPE_NORMAL
- en: In this technique, the attention scores are [penalized](https://arxiv.org/abs/2108.12409)
    with a bias term proportional to the distance between the query token and the
    key token. This technique also enables modeling sequences of longer length during
    inference than what was encountered in the training process.
  prefs: []
  type: TYPE_NORMAL
- en: Rotary Position Embedding (RoPE)
  prefs: []
  type: TYPE_NORMAL
- en: Just like ALiBi, this [technique](https://arxiv.org/abs/2104.09864) has the
    property of relative decay; there is a decay in the attention scores as the distance
    between the query token and the key token increases.
  prefs: []
  type: TYPE_NORMAL
- en: No Positional Encoding (NoPE)
  prefs: []
  type: TYPE_NORMAL
- en: A contrarian [technique](https://oreil.ly/QM9dW) argues that positional embeddings
    in fact are not required and that Transformers implicitly capture positional information.
  prefs: []
  type: TYPE_NORMAL
- en: Models these days are mostly using ALiBi or RoPE, although this is one aspect
    of the Transformer architecture that is still actively improving.
  prefs: []
  type: TYPE_NORMAL
- en: Feedforward Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The output from a self-attention block is fed through a [*feedforward network*](https://oreil.ly/Bdphg).
    Each token representation is independently fed through the network. The feedforward
    network incorporates a nonlinear activation function like [Rectified Linear Unit
    (ReLU)](https://oreil.ly/KUqtP) or [Gaussian Error Linear Units (GELU)](https://oreil.ly/MSDKE),
    thus enabling the model to learn more complex features from the data. For more
    details on these activation functions, refer to this [blog post from v7](https://oreil.ly/NfOb0).
  prefs: []
  type: TYPE_NORMAL
- en: 'The feedforward layers are implemented in code in this way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Layer Normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Layer normalization is performed to ensure training stability and faster training
    convergence. While the original Transformer architecture performed normalization
    at the beginning of the block, modern implementations do it at the end of the
    block. The normalization is performed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Given an input of batch size `b`, sequence length `n`, and vector dimension
    `d`, calculate the mean and variance across each vector dimension.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Normalize the input by subtracting the mean and dividing it by the square root
    of the variance. A small epsilon value is added to the denominator for numerical
    stability.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Multiply by a scale parameter and add a shift parameter to the resulting values.
    These parameters are learned during the training process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This is how it is represented in code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Loss Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have discussed all the components of each Transformer block. For
    the next token-prediction learning objective, the input is propagated through
    the Transformer layers to generate the final output, which is a probability distribution
    across all tokens. During training, the loss is calculated by comparing the output
    distribution with the gold truth. The gold truth distribution assigns a 1 to the
    gold truth token and 0 to all other tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many possible ways to quantify the difference between the output
    and the gold truth. The most popular one is cross-entropy, which is calculated
    by the formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, consider the sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s say the gold truth token is *good*, and the output probability distribution
    is (*terrible*: 0.65, *bad*:0.12, *good*:011,…​)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The cross-entropy is calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the gold truth distribution values are 0 for all but the correct token,
    the equation can be simplified to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Once the loss is calculated, the gradient of the loss with respect to the parameters
    of the model is calculated and the weights are updated, using the backpropagation
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Intrinsic Model Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How do we know if the backpropagation algorithm is actually working and that
    the model is getting better over time? We can use either intrinsic model evaluation
    or extrinsic model evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Extrinsic model evaluation involves testing the model’s performance on real-world
    downstream tasks. These tasks directly test the performance of the model but only
    on a narrow range of the model’s capabilities. In contrast, intrinsic model evaluation
    involves a more general evaluation of the model’s ability to model language, but
    with no guarantee that its performance in the intrinsic evaluation metric is directly
    proportional to its performance across all possible downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The most common intrinsic evaluation metric is *perplexity*. Perplexity measures
    the ability of a language model to accurately predict the next token in a sequence.
    A model that can always correctly predict the next token has a perplexity of 1\.
    The higher the perplexity, the worse the language model. In the worst case, if
    the model is predicting at random, with probability of predicting each token in
    a vocabulary of size V being 1/V, then the perplexity is V.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perplexity is related to cross-entropy by this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Transformer Backbones
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we described the components of the canonical version of the Transformer.
    In practice, three major types of architecture backbones are used to implement
    the Transformer:'
  prefs: []
  type: TYPE_NORMAL
- en: Encoder-only
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encoder-decoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decoder-only
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s look at each of these in detail.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-5](#enc-dec) depicts encoder-only, encoder-decoder, and decoder-only
    architectures.'
  prefs: []
  type: TYPE_NORMAL
- en: '![enc-dec](assets/dllm_0405.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-5\. Visualization of various Transformer backbones
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Encoder-Only Architectures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Encoder-only architectures were all the rage when Transformer-based language
    models first burst on the scene. Iconic language models from yesteryear (circa
    2018) that use encoder-only architectures include BERT, RoBERTa, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'There haven’t been many encoder-only LLMs trained since 2021 for a few reasons,
    including:'
  prefs: []
  type: TYPE_NORMAL
- en: They are relatively harder to train.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The masked language modeling objective typically used to train them provides
    a learning signal in only a small percentage of tokens (the masking rate), thus
    needing a lot more data to reach the same level of performance as decoder-only
    models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For every downstream task, you need to train a separate task-specific head,
    making usage inefficient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, the release of ModernBERT seems to have reinvigorated this space.
  prefs: []
  type: TYPE_NORMAL
- en: The creators of the UL2 language model claim that encoder-only models should
    be considered obsolete. I personally wouldn’t go that far; encoder-only models
    are still great choices for classification tasks. Moreover, if you already have
    a satisfactory pipeline for your use case built around encoder-only models, I
    would say if it ain’t broke, why fix it?
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some guidelines for adopting encoder-only models:'
  prefs: []
  type: TYPE_NORMAL
- en: RoBERTa performs better than BERT most of the time, since it is trained a lot
    longer on more data, and it has adopted best practices learned after the release
    of BERT.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DeBERTa and ModernBERT are currently regarded as the best-performing encoder-only
    models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The distilled versions of encoder-only models like DistilBERT, etc., are not
    too far off from the original models in terms of performance, and they should
    be considered if you are operating under resource constraints.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Several embedding models are built from encoder-only models. For example, one
    of the most important libraries in the field of NLP, the Swiss Army knife of NLP
    tools, *sentence transformers*, provides encoder-only embedding models that are
    very widely used. all-mpnet-base-v2, based on an encoder-only model called MPNet,
    and fine-tuned on several task datasets, is still competitive with much larger
    embedding models.
  prefs: []
  type: TYPE_NORMAL
- en: Encoder-Decoder Architectures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the original architecture of the Transformer, as it was first proposed.
    The T5 series of models uses this architecture type.
  prefs: []
  type: TYPE_NORMAL
- en: In encoder-decoder models, the input is text and the output is also text. A
    standardized interface ensures that the same model and training procedure can
    be used for multiple tasks. The inputs are handled by an encoder, and the outputs
    by the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: Decoder-Only Architectures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A majority of LLMs trained today use decoder-only models. Decoder-only models
    came into fashion starting from the original GPT model from OpenAI. Decoder-only
    models excel at zero-shot and few-shot learning.
  prefs: []
  type: TYPE_NORMAL
- en: Decoder models can be causal and noncausal. Noncausal models have bidirectionality
    over the input sequence, while the output is still autoregressive (you cannot
    look ahead).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'While the field is still evolving, there has been some [compelling evidence](https://oreil.ly/Sb7JS)
    for the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: Decoder-only models are the best choice for zero-shot and few-shot generalization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encoder-decoder models are the best choice for multi-task fine tuning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The best of both worlds is to combine the two: start with auto-regressive training,
    and then in an adaptation step, pre-train further with a noncausal setup using
    a span corruption objective.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we discussed how architectural backbones can be classified
    according to how they use the architecture’s encoder and decoder. Another architectural
    backbone type that is making inroads in the past year is the Mixture of Experts
    (MoE) paradigm. Let’s explore that in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Mixture of Experts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Remarkably, in the seven years since the invention of the Transformer architecture,
    the Transformer implementation used in current language models isn’t too different
    from the original version, despite hundreds of papers proposing modifications
    to it. The original architecture has proven to be surprisingly robust, with most
    proposed variants barely moving the needle in terms of performance. However, some
    components of the Transformer have seen changes, like positional encodings as
    discussed earlier in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: MoE models have been seeing a lot of success in the past couple of years. Examples
    include OpenAI’s GPT-4 (unconfirmed), Google’s Switch, DeepSeek’s DeepSeek V3,
    and Mistral’s Mixtral. In this section, we will learn the motivations behind developing
    this architecture and how it works in practice.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in [Chapter 1](ch01.html#chapter_llm-introduction), the scaling laws
    dictate that performance of the language model increases as you increase the size
    of the model and its training data. However, increasing the model capacity implies
    more compute is needed for both training and inference. This is undesirable, especially
    at inference time, when latency requirements can be stringent. Can we increase
    the capacity of a model without increasing the required compute?
  prefs: []
  type: TYPE_NORMAL
- en: One way to achieve this is using conditional computation; each input (either
    a token or the entire sequence) sees a different subset of the model, interacting
    with only the parameters that are best suited to process it. This is achieved
    by composing the architecture to be made up of several components called experts,
    with only a subset of experts being activated for each input.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-6](#MoE-0) depicts a canonical MoE model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![MoE](assets/dllm_0406.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-6\. Mixture of Experts
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A key component of the MoE architecture is the *gating function*. The gating
    function helps decide which expert is more suited to process a given input. The
    gating function is implemented as a weight applied to each expert.
  prefs: []
  type: TYPE_NORMAL
- en: The experts are typically added to the feedforward component of the Transformer.
    Therefore, if there are eight experts, then there will be eight feedforward networks
    instead of one. Based on the routing strategy used, only a small subset of these
    networks will be activated for a given input.
  prefs: []
  type: TYPE_NORMAL
- en: 'The routing strategy determines the number and type of experts activated. Two
    types of popular routing strategies exist:'
  prefs: []
  type: TYPE_NORMAL
- en: Tokens choose
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experts choose
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the tokens choose strategy, each token chooses k experts. k is typically
    a small number (~2). The disadvantage of using this strategy is the need for load
    balancing. If in a given input batch, most of the tokens end up using the same
    experts, then additional time is needed to finish the computation as we cannot
    benefit from the parallelization afforded by multiple experts.
  prefs: []
  type: TYPE_NORMAL
- en: In the experts choose strategy, each expert picks the tokens that it is most
    equipped to handle. This solves the load balancing problem as we can specify that
    each expert choose the same number of tokens. However, this also leads to inefficient
    token-expert matching, as each expert is limited to picking only a finite number
    of tokens in a batch.
  prefs: []
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have discussed the architecture of language models, let’s turn our
    focus to understanding the tasks they are trained on during the pre-training process.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier in the chapter, language models are pre-trained in a self-supervised
    manner. The scale of data we need to train them makes it prohibitively expensive
    to perform supervised learning, where (input, output) examples need to come from
    humans. Instead, we use a form of training called self-supervision, where the
    data itself contains the target labels. The goal of self-supervised learning is
    to learn a task which acts as a proxy for learning the syntax and semantics of
    a language, as well as skills like reasoning, arithmetic and logical manipulation,
    and other cognitive tasks, and (hopefully) eventually leading up to general human
    intelligence. How does this work?
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s take the canonical language modeling task: predicting the
    next word that comes in a sequence. Consider the sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'and the language model is asked to predict the next token. The total number
    of possible answers is the size of the vocabulary. There are many valid continuations
    to this sequence, like (hedge, fence, barbecue, sandcastle, etc.), but many continuations
    to this sequence would violate English grammar rules like (is, of, the). During
    the training process, after seeing billions of sequences, the model will know
    that it is highly improbable for the word “the” to be followed by the word “is”
    or “of,” regardless of the surrounding context. Thus, you can see how just predicting
    the next token is such a powerful tool: in order to correctly predict the next
    token you can eventually learn more and more complex functions that you can encode
    in your model connections. However, whether this paradigm is all we need to develop
    general intelligence is an open question.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Self-supervised learning objectives used for pre-training LLMs can be broadly
    classified (nonexhaustively) into three types:'
  prefs: []
  type: TYPE_NORMAL
- en: Full language modeling (FLM)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Masked language modeling (MLM)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prefix language modeling (PrefixLM)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s explore these in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Full Language Modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Figure 4-7](#full-language-modeling) shows the canonical FLM objective at
    work.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Full Language Modeling](assets/dllm_0407.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-7\. Full language modeling
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is the canonical language modeling objective of learning to predict the
    next token in a sequence and currently the simplest and most common training objective,
    used by GPT-4 and a vast number of open source models. The loss is computed for
    every token the model sees, i.e., every single token in the training set that
    is being asked to be predicted by the language model provides a learning signal
    for the model, making it very efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore an example, using the GPT Neo model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we continue pre-training the GPT Neo model from its publicly available
    checkpoint, using the full language modeling objective. Let’s say the current
    training sequence is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'You can run this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This code tokenizes the input text `Language models are` and feeds it to the
    model by invoking the `generate()` function. The function predicts the continuation,
    given the sequence “Language models are.” It outputs only one token and stops
    generating because `max_new_tokens` is set to 1\. The rest of the code enables
    it to output the top 20 list of tokens with the highest score, prior to applying
    the softmax at the last layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The top 20 tokens with the highest prediction score are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Every word in the top 20 seems to be a valid continuation of the sequence. The
    ground truth is the token `ubiquitous`, which we can use to calculate the loss
    and initiate the backpropagation process for learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'As another example, consider the text sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the same code as previously, except for this change:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The top 20 output tokens are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The correct answer has the 17th highest score. A lot of numbers appear in the
    top 10, showing that the model is more or less randomly guessing the answer, which
    is not surprising for a smaller model like GPT Neo.
  prefs: []
  type: TYPE_NORMAL
- en: 'The OpenAI API provides the `logprobs` parameter that allows you to specify
    the number of tokens along with their log probabilities that need to be returned.
    As of the book’s writing, only the `logprobs` of the 20 most probable tokens are
    available. The tokens returned are in order of their log probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This code calls the older gpt-4o model, asking it to generate a maximum of
    one token. The output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: gpt-4o is pretty confident that the answer is 13, and rightfully so. The rest
    of the top probability tokens are all related to output formatting.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: During inference, we don’t necessarily need to generate the token with the highest
    score. Several *decoding strategies* allow you to generate more diverse text.
    We will discuss these strategies in [Chapter 5](ch05.html#chapter_utilizing_llms).
  prefs: []
  type: TYPE_NORMAL
- en: Prefix Language Modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prefix LM is similar to the FLM setting. The difference is that FLM is fully
    causal, i.e., in a left-to-right writing system like English, tokens do not attend
    to tokens to the right (future). In the prefix LM setting, a part of the text
    sequence, called the prefix, is allowed to attend to future tokens in the prefix.
    The prefix part is thus noncausal. For training prefix LMs, a random prefix length
    is sampled, and the loss is calculated over only the tokens in the suffix.
  prefs: []
  type: TYPE_NORMAL
- en: Masked Language Modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Figure 4-8](#masked-language-modeling-bert) shows the canonical MLM objective
    at work.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Masked Language Modeling in BERT](assets/dllm_0408.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-8\. Masked Language Modeling in BERT
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the MLM setting, rather than predict the next token in a sequence, we ask
    the model to predict masked tokens within the sequence. In the most basic form
    of MLM implemented in the BERT model, 15% of tokens are randomly chosen to be
    masked and are replaced with a special mask token, and the language model is asked
    to predict the original tokens.
  prefs: []
  type: TYPE_NORMAL
- en: The T5 model creators used a modification of the original MLM objective. In
    this variant, 15% of tokens are randomly chosen to be removed from a sequence.
    Consecutive dropped-out tokens are replaced by a single unique special token called
    the *sentinel token*. The model is then asked to predict and generate the dropped
    tokens, delineated by the sentinel tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, consider this sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: Tempura has always been a source of conflict in the family due to unexplained
    reasons
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Let’s say we drop the tokens “has,” “always,” “of,” and “conflict.” The sequence
    is now:'
  prefs: []
  type: TYPE_NORMAL
- en: Tempura <S1> been a source <S2> in the family due to unexplained reasons
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'with S1, S2 being the sentinel tokens. The model is expected to output:'
  prefs: []
  type: TYPE_NORMAL
- en: <S1> has always <S2> of conflict <E>
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The output sequence is terminated by a special token indicating the end of the
    sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Generating only the dropped tokens and not the entire sequence is computationally
    more efficient and saves training time. Note that unlike in Full Language Modeling,
    the loss is calculated over only a small proportion of tokens (the masked tokens)
    in the input sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s explore this on Hugging Face:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The targets can be prepared using a simple templating function.
  prefs: []
  type: TYPE_NORMAL
- en: 'More generally, MLM can be interpreted as a *denoising autoencoder*. You corrupt
    your input by adding noise (masking, dropping tokens), and then you train a model
    to regenerate the original input. BART takes this to the next level by using five
    different types of span corruptions:'
  prefs: []
  type: TYPE_NORMAL
- en: Random token masking
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-9](#bart-enoiser-objectives1) depicts the corruption and denoising
    steps.'
  prefs: []
  type: TYPE_NORMAL
- en: '![BART Denoiser Objectives1](assets/dllm_0409.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-9\. Random token masking in BART
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Random token deletion
  prefs: []
  type: TYPE_NORMAL
- en: The model needs to predict the positions in the text where tokens have been
    deleted. [Figure 4-10](#bart-enoiser-objectives2) depicts the corruption and denoising
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: '![BART Denoiser Objectives2](assets/dllm_0410.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-10\. Random token deletion in BART
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Span masking
  prefs: []
  type: TYPE_NORMAL
- en: Text spans are sampled from text, with span lengths coming from a Poisson distribution.
    This means zero-length spans are possible. The spans are deleted from the text
    and replaced with a single mask token. Therefore, the model now has to also predict
    the number of tokens deleted. [Figure 4-11](#bart-enoiser-objectives3) depicts
    the corruption and denoising steps.
  prefs: []
  type: TYPE_NORMAL
- en: '![BART Denoiser Objectives3](assets/dllm_0411.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-11\. Span masking in BART
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Document shuffling
  prefs: []
  type: TYPE_NORMAL
- en: Sentences in the input document are shuffled. The model is taught to arrange
    them in the right order. [Figure 4-12](#bart-enoiser-objectives4) depicts the
    corruption and denoising steps.
  prefs: []
  type: TYPE_NORMAL
- en: '![BART Denoiser Objectives4](assets/dllm_0412.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-12\. Document shuffling objective in BART
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Document rotation
  prefs: []
  type: TYPE_NORMAL
- en: The document is rotated so that it starts from an arbitrary token. The model
    is trained to detect the correct start of the document. [Figure 4-13](#bart-enoiser-objectives5)
    depicts the corruption and denoising steps.
  prefs: []
  type: TYPE_NORMAL
- en: '![BART Denoiser Objectives5](assets/dllm_0413.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-13\. Document rotation objective in BART
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Which Learning Objectives Are Better?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It has been shown that models trained with FLM are better at generation, and
    models trained with MLM are better at classification tasks. However, it is inefficient
    to use different language models for different use cases. The consolidation effect
    continues to take hold, with the introduction of [UL2](https://oreil.ly/xJc3U),
    a paradigm that combines the best of different learning objective types in a single
    model.
  prefs: []
  type: TYPE_NORMAL
- en: UL2 mimics the effect of PLMs, MLMs, and PrefixLMs in a single paradigm called
    *Mixture of Denoisers*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The denoisers used are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: R-Denoiser
  prefs: []
  type: TYPE_NORMAL
- en: This is similar to the T5 span corruption task. Spans between length 2–5 tokens
    are replaced by a single mask token. [Figure 4-14](#ul2-mixture-denoisers1) depicts
    the workings of the R-denoiser.
  prefs: []
  type: TYPE_NORMAL
- en: '![UL2''s Mixture of Denoisers1](assets/dllm_0414.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-14\. UL2’s R-Denoiser
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: S-Denoiser
  prefs: []
  type: TYPE_NORMAL
- en: Similar to prefix LM, the text is divided into a prefix and a suffix. The suffix
    is masked, while the prefix has access to bidirectional context. [Figure 4-15](#ul2-mixture-denoisers2)
    depicts the workings of the S-Denoiser.
  prefs: []
  type: TYPE_NORMAL
- en: '![UL2''s Mixture of Denoisers2](assets/dllm_0415.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-15\. UL2’s S-Denoiser
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: X-Denoiser
  prefs: []
  type: TYPE_NORMAL
- en: This stands for extreme denoising, where a large proportion of text is masked
    (often over 50%). [Figure 4-16](#ul2-mixture-denoisers3) depicts the workings
    of the X-Denoiser.
  prefs: []
  type: TYPE_NORMAL
- en: '![UL2''s Mixture of Denoisers3](assets/dllm_0416.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-16\. UL2’s X-Denoiser
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Pre-Training Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have learned about the ingredients that go into a language model
    in detail, let’s learn how to pre-train one from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: 'The language models of today are learning to model two types of concepts with
    one model:'
  prefs: []
  type: TYPE_NORMAL
- en: Language, the vehicle used to communicate facts, opinions, and feelings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The underlying phenomena that led to the construction of text in the language.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For many application areas, we are far more interested in learning to model
    the latter than the former. While a language model that is fluent in the language
    is welcome, we would prefer to see it get better at domains like science or law
    and skills like reasoning and arithmetic.
  prefs: []
  type: TYPE_NORMAL
- en: These concepts and skills are expressed in languages like English, which primarily
    serve a social function. Human languages are inherently ambiguous, contain lots
    of redundancies, and in general are inefficient vehicles to transmit underlying
    concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 'This brings us to the question: are human languages even the best vehicle for
    language models to learn underlying skills and concepts? Can we separate the process
    of modeling the language from modeling the underlying concepts expressed through
    language?'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s put this theory to the test using an example. Consider training an LLM
    from scratch to learn to play the game of chess.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall the ingredients of a language model from [Chapter 2](ch02.html#ch02).
    We need:'
  prefs: []
  type: TYPE_NORMAL
- en: A pre-training dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A vocabulary and tokenization scheme
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A model architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A learning objective
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For training the chess language model, we can choose the Transformer architecture
    with the next-token prediction learning objective, which is the de facto paradigm
    used today.
  prefs: []
  type: TYPE_NORMAL
- en: For the pre-training dataset, we can use the chess games dataset from [Lichess](https://oreil.ly/XmWvv),
    containing billions of games. We select a subset of 20 million chess games for
    our training.
  prefs: []
  type: TYPE_NORMAL
- en: This dataset is in the Portable Game Notation (PGN) format, which is used to
    represent the sequence of chess moves in a concise notation.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have to choose the vocabulary of the model. Since the only purpose
    of this model is to learn chess, we don’t need to support an extensive English
    vocabulary. In fact, we can take advantage of the PGN notation to assign tokens
    to specific chess concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of a chess game in PGN format, taken from [pgnmentor.com](https://oreil.ly/H3yOs):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The rows of the board are assigned letters a–h and the columns are assigned
    numbers 1–8\. Except for pawns, each piece type is assigned a capital letter,
    with N for knight, R for rook, B for bishop, Q for queen, and K for king. A +
    appended to a move indicates a check, a % appended to the move indicates a checkmate,
    and 0-0 is used to indicate castling. If you are unfamiliar with the rules of
    chess, refer to [this piece for a primer](https://oreil.ly/EbcfQ).
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on this notation, the vocabulary can consist of:'
  prefs: []
  type: TYPE_NORMAL
- en: A separate token for each square on the board, with 64 total (a1, a2, a3…​h6,
    h7, h8)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A separate token for each piece type (N, B, R, K, Q)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokens for move numbers (1., 2., 3., etc.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokens for special moves (+ for check, x for capture, etc.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s train a language model from scratch on this chess dataset using our
    special domain-specific vocabulary. The model is directly learning from the PGN
    notation with no human language text present in the dataset. The book’s [GitHub
    repo](https://oreil.ly/llm-playbooks) contains the code and setup for training
    this model.
  prefs: []
  type: TYPE_NORMAL
- en: After training the model for three epochs, let’s test the model’s ability to
    play chess. We can see that the model seems to have learned the rules of the game
    without having to be provided the rules explicitly in natural language. In fact,
    the model can even beat human players some of the time and can execute moves like
    castling.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this model was able to learn the concepts (chess) using a domain-specific
    language (PGN). How will we fare if the concepts were taught in natural language?
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s explore this in another experiment. Take the same dataset used to pre-train
    the chess language model and run it through an LLM to convert each move in PGN
    to a sentence in English. An example game would look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '*White moves pawn to e4*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Black moves bishop to g7*'
  prefs: []
  type: TYPE_NORMAL
- en: and so on. Train a new language model on the same number of games as the previous
    one, but this time with the English-language dataset. Let the vocabulary of this
    model be the standard English vocabulary generated by training the tokenizer over
    the training set.
  prefs: []
  type: TYPE_NORMAL
- en: How does this compare to the chess LM trained on the PGN dataset? The model
    trained on English descriptions of chess moves performs worse and doesn’t seem
    to have understood the rules of the game yet, despite being trained on the same
    number of games as the other model.
  prefs: []
  type: TYPE_NORMAL
- en: This shows that natural language is not necessarily the most efficient vehicle
    for a model to learn skills and concepts, and domain-specific languages and notations
    perform better.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, language design is an important skill to acquire, enabling you to create
    domain-specific languages for learning concepts and skills. For your application
    areas, you could use existing domain-specific languages or create a new one yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the various components of the Transformer architecture
    in detail, including self-attention, feedforward networks, position encodings,
    and layer normalization. We also discussed several variants and configurations
    such as encoder-only, encoder-decoder, decoder-only, and MoE models. Finally,
    we learned how to put our knowledge of language models together to train our own
    model from scratch and how to design domain-specific languages for more efficient
    learning.
  prefs: []
  type: TYPE_NORMAL
