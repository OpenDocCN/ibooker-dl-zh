- en: Chapter 4\. Architectures and Learning Objectives
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章\. 架构和学习目标
- en: 'In Chapters [2](ch02.html#ch02) and [3](ch03.html#chapter-LLM-tokenization),
    we discussed some of the key ingredients that go into making a language model:
    the training datasets, and the vocabulary and tokenizer. Next, let’s complete
    the puzzle by learning about the models themselves, the architectures underpinning
    them, and their learning objectives.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二章 [2](ch02.html#ch02) 和第三章 [3](ch03.html#chapter-LLM-tokenization) 中，我们讨论了构建语言模型的一些关键要素：训练数据集、词汇和分词器。接下来，让我们通过了解模型本身、支撑它们的架构以及它们的学习目标来完善这个拼图。
- en: In this chapter, we will learn the composition of language models and their
    structure. Modern-day language models are predominantly based on the Transformer
    architecture, and hence we will devote most of our focus to understanding it,
    by going through each component of the architecture in detail. Over the last few
    years, several variants and alternatives to the original Transformer architecture
    have been proposed. We will go through the promising ones, including Mixture of
    Experts (MoE) models. We will also examine commonly used learning objectives the
    language models are trained over, including next-token prediction. Finally, we
    will bring together the concepts of the last three chapters in practice by learning
    how to pre-train a language model from scratch.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习语言模型的组成及其结构。现代语言模型主要基于Transformer架构，因此我们将主要关注理解它，通过详细研究架构的每个组件。在过去的几年里，已经提出了许多原始Transformer架构的变体和替代方案。我们将探讨其中一些有前景的方案，包括混合专家（MoE）模型。我们还将检查语言模型训练中常用的学习目标，包括下一个标记预测。最后，我们将通过学习如何从头开始预训练语言模型，将前三章的概念在实践中结合起来。
- en: Preliminaries
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: Just about every contemporary language model is based on neural networks, composed
    of processing units called *neurons*. While modern neural networks do not resemble
    the workings of the human brain at all, many of the ideas behind neural networks
    and the terminology used is inspired by the field of neuroscience.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有的当代语言模型都是基于神经网络，由称为*神经元*的处理单元组成。虽然现代神经网络与人类大脑的工作方式完全不相似，但神经网络背后的许多思想和使用的术语都受到了神经科学领域的影响。
- en: The neurons in a neural network are connected to each other according to some
    configuration. Each connection between a pair of neurons is associated with a
    weight (also called *parameter*), indicating the strength of the connection. The
    role these neurons play and the way they are connected to each other constitutes
    the *architecture* of the model.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的神经元按照某种配置相互连接。每对神经元之间的连接都关联着一个权重（也称为*参数*），表示连接的强度。这些神经元所扮演的角色以及它们相互连接的方式构成了模型的*架构*。
- en: The early 2010s saw the proliferation of multi-layer architectures, with layers
    of neurons stacked on top of each other, each layer extracting progressively more
    complex features of the input. This paradigm is called *deep learning.*
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 2010年代初，多层架构大量涌现，神经元层堆叠在一起，每一层提取输入的越来越复杂的特征。这种范式被称为*深度学习*。
- en: '[Figure 4-1](#MLP-00) depicts a simple multi-layer neural network, also called
    the multi-layer perceptron.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-1](#MLP-00) 展示了一个简单的多层神经网络，也称为多层感知器。'
- en: '![Transformer](assets/dllm_0401.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![Transformer](assets/dllm_0401.png)'
- en: Figure 4-1\. Multi-layer perceptron
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-1\. 多层感知器
- en: Tip
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: For a more comprehensive treatment of neural networks, refer to [Goldberg’s
    book](https://oreil.ly/oDc6x) on neural network–based natural language processing.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 想要更全面地了解神经网络，请参阅关于基于神经网络的自然语言处理的[Goldberg的书籍](https://oreil.ly/oDc6x)。
- en: As discussed in [Chapter 1](ch01.html#chapter_llm-introduction), language models
    are primarily pre-trained using self-supervised learning. Input text from the
    training dataset is tokenized and converted to vector form. The input is then
    propagated through the neural network, affected by its weights and *activation
    functions*, the latter introducing nonlinearity to the model. The output of the
    model is compared to the expected output, called the gold truth. The weights of
    the output are adapted such that next time for the same input, the output can
    be closer to the gold truth.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第1章](ch01.html#chapter_llm-introduction)所述，语言模型主要是通过自监督学习进行预训练的。训练数据集中的输入文本被标记化并转换为向量形式。然后，输入通过神经网络传播，受到其权重和*激活函数*的影响，后者为模型引入了非线性。模型的输出与预期的输出（称为黄金真理）进行比较。输出权重被调整，以便下次对于相同的输入，输出可以更接近黄金真理。
- en: In practice, this adaptation process is implemented through a *loss function*.
    The goal of the model is to minimize the loss, which is the difference between
    the model output and the gold truth. To minimize the loss, the weights are updated
    using a gradient-descent based method, called backpropagation. I strongly recommend
    developing an intuitive understanding of this algorithm before diving into model
    training.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这个过程是通过一个*损失函数*实现的。模型的目标是使损失最小化，损失是模型输出和黄金真理之间的差异。为了最小化损失，使用基于梯度下降的方法，称为反向传播，来更新权重。我强烈建议在深入模型训练之前，先对这种算法有一个直观的理解。
- en: Representing Meaning
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 表示意义
- en: While describing neural network–based architectures in the previous section,
    we glossed over the fact that the input text is converted into vectors and then
    propagated through the network. What are these vectors composed of and what do
    they represent? Ideally, after the model is trained, these vectors should accurately
    represent some aspect of the meaning of the underlying text, including its social
    connotations. Developing the right representations for modalities like text or
    images is a very active field of research, called *representation learning*.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中描述基于神经网络的架构时，我们忽略了这样一个事实：输入文本被转换成向量，然后通过网络传播。这些向量由什么组成，它们代表什么？理想情况下，在模型训练完成后，这些向量应该准确地代表文本的某些意义方面，包括其社会含义。为文本或图像等模态开发正确的表示是一个非常活跃的研究领域，被称为*表示学习*。
- en: Note
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: When training a language model from scratch, these vectors initially mean nothing,
    as they are randomly generated. In practice, there are initialization algorithms
    used like Glorot, He, etc. Refer to [this report](https://oreil.ly/A8Iro) for
    a primer on neural network initialization.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当从头开始训练语言模型时，这些向量最初没有任何意义，因为它们是随机生成的。在实践中，使用了如Glorot、He等初始化算法。有关神经网络初始化的入门指南，请参阅[这份报告](https://oreil.ly/A8Iro)。
- en: How can a list of numbers represent meaning? It is hard for humans to describe
    the meaning of a word or sentence, let alone represent it in numerical form that
    can be processed by a computer. The *form* of a word, i.e., the letters that comprise
    it, usually do not give any information whatsoever about the meaning it represents.
    For example, the sequence of letters in the word *umbrella* contains no hints
    about its meaning, even if you are already exposed to thousands of other words
    in the English language.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 数字列表如何代表意义？人类描述一个词或句子的意义已经很困难，更不用说用可以被计算机处理的数值形式来表示它了。一个词的*形式*，即构成它的字母，通常不会提供关于它所代表的意义的任何信息。例如，单词*umbrella*中的字母序列没有任何关于其意义的线索，即使你已经接触到了成千上万的英语单词。
- en: 'The prominent way of representing meaning in numerical form is through the
    *distributional hypothesis* framework. The distributional hypothesis states that
    words that have similar meaning occur in similar contexts. The implication of
    this hypothesis is best represented by the adage:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在数值形式中表示意义的突出方式是通过*分布假设*框架。分布假设指出，具有相似意义的词出现在相似的环境中。这个假设的含义最好用一句谚语来表示：
- en: You shall know a word by the company it keeps.
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你可以通过一个词所伴随的词来了解这个词。
- en: ''
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: John Rupert Firth, 1957
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 约翰·鲁珀特·费尔思，1957
- en: This is one of the primary ways in which we pick up the meaning of words we
    haven’t encountered previously, without needing to look them up in a dictionary.
    A large number of words we know weren’t learned from the dictionary or by explicitly
    learning the meaning of a word but by estimating meaning based on the contexts
    words appear in.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们获取之前未曾遇到的词语意义的主要方式之一，无需查阅词典。我们知道的许多词语并非来自词典或通过明确学习词语的意义，而是通过根据词语出现的上下文来估计意义。
- en: Let’s investigate how the distributional hypothesis works in practice. The Natural
    Language Toolkit (NLTK) library provides a feature called *concordance view*,
    which presents you with the surrounding contexts that a given word appears in
    a corpus.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们调查分布假设在实际中是如何工作的。自然语言工具包（NLTK）库提供了一个名为*共现视图*的功能，它向您展示给定词语在语料库中出现的周围上下文。
- en: 'For example, let’s see the contexts in which the word “nervous” occurs in the
    Jane Austen classic *Emma*:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们看看“nervous”一词在简·奥斯汀的经典作品《爱玛》中出现的上下文：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output looks like this:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 输出看起来像这样：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The Transformer Architecture
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer架构
- en: Now that we have developed an intuition on how text is represented in vector
    form, let’s further explore the canonical architecture used for training language
    models today, the Transformer.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经对文本以向量形式表示有了直觉，让我们进一步探索今天用于训练语言模型的典型架构，即Transformer。
- en: In the mid 2010s, the predominant architectures used for NLP tasks were recurrent
    neural networks, specifically a variant called long short-term memory (LSTM).
    While knowledge of recurrent neural networks is not a prerequisite for this book,
    I recommend [*Neural Network Methods for Natural Language Processing*](https://oreil.ly/CHCTd)
    for more details.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在2010年代中期，用于自然语言处理任务的占主导地位的架构是循环神经网络，特别是称为长短期记忆（LSTM）的一种变体。虽然对循环神经网络的知识不是本书的先决条件，但我推荐[*《自然语言处理中的神经网络方法》*](https://oreil.ly/CHCTd)以获取更多详细信息。
- en: Recurrent neural networks were sequence models, which means they processed text
    one token at a time, sequentially. A single vector was used to represent the state
    of the entire sequence, so as the sequence grew longer, more and more information
    needed to be captured in the single state vector. Because of the sequential nature
    of processing, long-range dependencies were harder to capture, as the content
    from the beginning of the sequence would be harder to retain.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络是序列模型，这意味着它们逐个标记地、顺序地处理文本。一个向量被用来表示整个序列的状态，因此随着序列变长，越来越多的信息需要被捕捉到单个状态向量中。由于处理的顺序性，长距离依赖关系更难捕捉，因为序列开头的内容会更难保留。
- en: 'This issue was candidly articulated by Ray Mooney, a senior computer scientist
    who remarked at the Association for Computational Linguistics (ACL) 2014 conference:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题被资深计算机科学家Ray Mooney坦率地阐述，他在2014年的计算语言学（ACL）会议上评论说：
- en: You can’t cram the meaning of a whole %&!$# sentence into a single $&!#* vector!
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你不能把整个句子的意义塞进一个单一的$&!#*向量！
- en: ''
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ray Mooney, 2014
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Ray Mooney，2014
- en: 'Thus, there was a need for an architecture that solved for the deficiencies
    of LSTM: the limitations in representing long-range dependencies, the dependence
    on a single vector for representing the state of the entire sequence, and more.
    The Transformer architecture was designed to address these issues.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，需要一个能够解决LSTM缺陷的架构：在表示长距离依赖关系、依赖于单个向量来表示整个序列的状态以及更多方面。Transformer架构被设计来解决这个问题。
- en: '[Figure 4-2](#Transformer0) depicts the original Transformer architecture,
    developed in 2017 by [Vaswani et al.](https://oreil.ly/tIvGZ) As we can see in
    the figure, a Transformer model is typically composed of Transformer blocks stacked
    on top of each other, called *layers*. The key components of each block are:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-2](#Transformer0)展示了2017年由[Vaswani等人](https://oreil.ly/tIvGZ)开发的原始Transformer架构。如图所示，Transformer模型通常由堆叠在一起的Transformer块组成，称为*层*。每个块的关键组件包括：'
- en: Self-attention
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自注意力
- en: Positional encoding
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置编码
- en: Feedforward networks
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前馈网络
- en: Normalization blocks
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 归一化块
- en: '![Transformer](assets/dllm_0402.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![Transformer](assets/dllm_0402.png)'
- en: Figure 4-2\. The Transformer architecture
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-2\. Transformer架构
- en: 'At the beginning of the first block is a special layer called the *embedding*
    layer. This is where the tokens in the input text are mapped to their corresponding
    vector. The embedding layer is a matrix whose size is:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个块的开头有一个特殊的层，称为*嵌入层*。这是将输入文本中的标记映射到它们对应向量的地方。嵌入层是一个矩阵，其大小为：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'On Hugging Face, we can inspect the embedding layer as such, using the `transformers`
    library:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hugging Face上，我们可以使用`transformers`库这样检查嵌入层：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4] ''Mark told Sam that he was planning to resign.'' [PRE5] import torch
    import torch.nn as nn import torch.nn.functional as F  q = wQ(input_embeddings)
    k = WK(input_embeddings) v = WV(input_embeddings) dim_k = k.size(-1)  attn_scores
    = torch.matmul(q, k.transpose(-2, -1)) scaled_attn_scores = attn_scores/torch.sqrt(torch.tensor(dim_k,   dtype=torch.float32))  normalized_attn_scores
    = F.softmax(scaled_attn_scores, dim=-1)  output = torch.matmul(normalized_attn_scores,
    v) [PRE6] import torch import torch.nn as nn  class FeedForward(nn.Module):     def
    __init__(self, input_dim, hidden_dim):         super(FeedForward, self).__init__()         self.l1
    = nn.Linear(input_dim, hidden_dim)         self.l2 = nn.Linear(hidden_dim, input_dim)         self.selu
    = nn.SeLU()      def forward(self, x):         x = self.selu(self.l1(x))         x
    = self.l2(x)         return x   feed_forward = FeedForward(input_dim, hidden_dim)
    outputs = feed_forward(inputs) [PRE7] import torch import torch.nn as nn  class
    LayerNorm(nn.Module):     def __init__(self, dimension, gamma=None, beta=None,
    epsilon=1e-5):         super(LayerNorm, self).__init__()         self.epsilon
    = epsilon         self.gamma = gamma if gamma is not None else         nn.Parameter(torch.ones(dimension))         self.beta
    = beta if beta is not None else         nn.Parameter(torch.zeros(dimension))       def
    forward(self, x):         mean = x.mean(-1, keepdim=True)         variance = x.var(-1,
    keepdim=True, unbiased=False)         x_normalized = (x - mean) / torch.sqrt(variance
    + self.epsilon)         return self.gamma * x_normalized + self.beta  layer_norm
    = LayerNorm(embedding_dim) outputs = layer_norm(inputs) [PRE8]`  [PRE9]` # Loss
    Functions    So far, we have discussed all the components of each Transformer
    block. For the next token-prediction learning objective, the input is propagated
    through the Transformer layers to generate the final output, which is a probability
    distribution across all tokens. During training, the loss is calculated by comparing
    the output distribution with the gold truth. The gold truth distribution assigns
    a 1 to the gold truth token and 0 to all other tokens.    There are many possible
    ways to quantify the difference between the output and the gold truth. The most
    popular one is cross-entropy, which is calculated by the formula:    [PRE10]    For
    example, consider the sequence:    [PRE11]    Let’s say the gold truth token is
    *good*, and the output probability distribution is (*terrible*: 0.65, *bad*:0.12,
    *good*:011,…​)    The cross-entropy is calculated as:    [PRE12]    Since the
    gold truth distribution values are 0 for all but the correct token, the equation
    can be simplified to:    [PRE13]    Once the loss is calculated, the gradient
    of the loss with respect to the parameters of the model is calculated and the
    weights are updated, using the backpropagation algorithm.    # Intrinsic Model
    Evaluation    How do we know if the backpropagation algorithm is actually working
    and that the model is getting better over time? We can use either intrinsic model
    evaluation or extrinsic model evaluation.    Extrinsic model evaluation involves
    testing the model’s performance on real-world downstream tasks. These tasks directly
    test the performance of the model but only on a narrow range of the model’s capabilities.
    In contrast, intrinsic model evaluation involves a more general evaluation of
    the model’s ability to model language, but with no guarantee that its performance
    in the intrinsic evaluation metric is directly proportional to its performance
    across all possible downstream tasks.    The most common intrinsic evaluation
    metric is *perplexity*. Perplexity measures the ability of a language model to
    accurately predict the next token in a sequence. A model that can always correctly
    predict the next token has a perplexity of 1\. The higher the perplexity, the
    worse the language model. In the worst case, if the model is predicting at random,
    with probability of predicting each token in a vocabulary of size V being 1/V,
    then the perplexity is V.    Perplexity is related to cross-entropy by this formula:    [PRE14]    #
    Transformer Backbones    So far, we described the components of the canonical
    version of the Transformer. In practice, three major types of architecture backbones
    are used to implement the Transformer:    *   Encoder-only           *   Encoder-decoder           *   Decoder-only              Let’s
    look at each of these in detail.    [Figure 4-5](#enc-dec) depicts encoder-only,
    encoder-decoder, and decoder-only architectures.  ![enc-dec](assets/dllm_0405.png)  ######
    Figure 4-5\. Visualization of various Transformer backbones    ## Encoder-Only
    Architectures    Encoder-only architectures were all the rage when Transformer-based
    language models first burst on the scene. Iconic language models from yesteryear
    (circa 2018) that use encoder-only architectures include BERT, RoBERTa, etc.    There
    haven’t been many encoder-only LLMs trained since 2021 for a few reasons, including:    *   They
    are relatively harder to train.           *   The masked language modeling objective
    typically used to train them provides a learning signal in only a small percentage
    of tokens (the masking rate), thus needing a lot more data to reach the same level
    of performance as decoder-only models.           *   For every downstream task,
    you need to train a separate task-specific head, making usage inefficient.              However,
    the release of ModernBERT seems to have reinvigorated this space.    The creators
    of the UL2 language model claim that encoder-only models should be considered
    obsolete. I personally wouldn’t go that far; encoder-only models are still great
    choices for classification tasks. Moreover, if you already have a satisfactory
    pipeline for your use case built around encoder-only models, I would say if it
    ain’t broke, why fix it?    Here are some guidelines for adopting encoder-only
    models:    *   RoBERTa performs better than BERT most of the time, since it is
    trained a lot longer on more data, and it has adopted best practices learned after
    the release of BERT.           *   DeBERTa and ModernBERT are currently regarded
    as the best-performing encoder-only models.           *   The distilled versions
    of encoder-only models like DistilBERT, etc., are not too far off from the original
    models in terms of performance, and they should be considered if you are operating
    under resource constraints.              Several embedding models are built from
    encoder-only models. For example, one of the most important libraries in the field
    of NLP, the Swiss Army knife of NLP tools, *sentence transformers*, provides encoder-only
    embedding models that are very widely used. all-mpnet-base-v2, based on an encoder-only
    model called MPNet, and fine-tuned on several task datasets, is still competitive
    with much larger embedding models.    ## Encoder-Decoder Architectures    This
    is the original architecture of the Transformer, as it was first proposed. The
    T5 series of models uses this architecture type.    In encoder-decoder models,
    the input is text and the output is also text. A standardized interface ensures
    that the same model and training procedure can be used for multiple tasks. The
    inputs are handled by an encoder, and the outputs by the decoder.    ## Decoder-Only
    Architectures    A majority of LLMs trained today use decoder-only models. Decoder-only
    models came into fashion starting from the original GPT model from OpenAI. Decoder-only
    models excel at zero-shot and few-shot learning.    Decoder models can be causal
    and noncausal. Noncausal models have bidirectionality over the input sequence,
    while the output is still autoregressive (you cannot look ahead).    ###### Tip    While
    the field is still evolving, there has been some [compelling evidence](https://oreil.ly/Sb7JS)
    for the following results:    *   Decoder-only models are the best choice for
    zero-shot and few-shot generalization.           *   Encoder-decoder models are
    the best choice for multi-task fine tuning.              The best of both worlds
    is to combine the two: start with auto-regressive training, and then in an adaptation
    step, pre-train further with a noncausal setup using a span corruption objective.    In
    this section, we discussed how architectural backbones can be classified according
    to how they use the architecture’s encoder and decoder. Another architectural
    backbone type that is making inroads in the past year is the Mixture of Experts
    (MoE) paradigm. Let’s explore that in detail.    ## Mixture of Experts    Remarkably,
    in the seven years since the invention of the Transformer architecture, the Transformer
    implementation used in current language models isn’t too different from the original
    version, despite hundreds of papers proposing modifications to it. The original
    architecture has proven to be surprisingly robust, with most proposed variants
    barely moving the needle in terms of performance. However, some components of
    the Transformer have seen changes, like positional encodings as discussed earlier
    in the chapter.    MoE models have been seeing a lot of success in the past couple
    of years. Examples include OpenAI’s GPT-4 (unconfirmed), Google’s Switch, DeepSeek’s
    DeepSeek V3, and Mistral’s Mixtral. In this section, we will learn the motivations
    behind developing this architecture and how it works in practice.    As shown
    in [Chapter 1](ch01.html#chapter_llm-introduction), the scaling laws dictate that
    performance of the language model increases as you increase the size of the model
    and its training data. However, increasing the model capacity implies more compute
    is needed for both training and inference. This is undesirable, especially at
    inference time, when latency requirements can be stringent. Can we increase the
    capacity of a model without increasing the required compute?    One way to achieve
    this is using conditional computation; each input (either a token or the entire
    sequence) sees a different subset of the model, interacting with only the parameters
    that are best suited to process it. This is achieved by composing the architecture
    to be made up of several components called experts, with only a subset of experts
    being activated for each input.    [Figure 4-6](#MoE-0) depicts a canonical MoE
    model.  ![MoE](assets/dllm_0406.png)  ###### Figure 4-6\. Mixture of Experts    A
    key component of the MoE architecture is the *gating function*. The gating function
    helps decide which expert is more suited to process a given input. The gating
    function is implemented as a weight applied to each expert.    The experts are
    typically added to the feedforward component of the Transformer. Therefore, if
    there are eight experts, then there will be eight feedforward networks instead
    of one. Based on the routing strategy used, only a small subset of these networks
    will be activated for a given input.    The routing strategy determines the number
    and type of experts activated. Two types of popular routing strategies exist:    *   Tokens
    choose           *   Experts choose              In the tokens choose strategy,
    each token chooses k experts. k is typically a small number (~2). The disadvantage
    of using this strategy is the need for load balancing. If in a given input batch,
    most of the tokens end up using the same experts, then additional time is needed
    to finish the computation as we cannot benefit from the parallelization afforded
    by multiple experts.    In the experts choose strategy, each expert picks the
    tokens that it is most equipped to handle. This solves the load balancing problem
    as we can specify that each expert choose the same number of tokens. However,
    this also leads to inefficient token-expert matching, as each expert is limited
    to picking only a finite number of tokens in a batch.    # Learning Objectives    Now
    that we have discussed the architecture of language models, let’s turn our focus
    to understanding the tasks they are trained on during the pre-training process.    As
    mentioned earlier in the chapter, language models are pre-trained in a self-supervised
    manner. The scale of data we need to train them makes it prohibitively expensive
    to perform supervised learning, where (input, output) examples need to come from
    humans. Instead, we use a form of training called self-supervision, where the
    data itself contains the target labels. The goal of self-supervised learning is
    to learn a task which acts as a proxy for learning the syntax and semantics of
    a language, as well as skills like reasoning, arithmetic and logical manipulation,
    and other cognitive tasks, and (hopefully) eventually leading up to general human
    intelligence. How does this work?    For example, let’s take the canonical language
    modeling task: predicting the next word that comes in a sequence. Consider the
    sequence:    [PRE15]    and the language model is asked to predict the next token.
    The total number of possible answers is the size of the vocabulary. There are
    many valid continuations to this sequence, like (hedge, fence, barbecue, sandcastle,
    etc.), but many continuations to this sequence would violate English grammar rules
    like (is, of, the). During the training process, after seeing billions of sequences,
    the model will know that it is highly improbable for the word “the” to be followed
    by the word “is” or “of,” regardless of the surrounding context. Thus, you can
    see how just predicting the next token is such a powerful tool: in order to correctly
    predict the next token you can eventually learn more and more complex functions
    that you can encode in your model connections. However, whether this paradigm
    is all we need to develop general intelligence is an open question.    Self-supervised
    learning objectives used for pre-training LLMs can be broadly classified (nonexhaustively)
    into three types:    *   Full language modeling (FLM)           *   Masked language
    modeling (MLM)           *   Prefix language modeling (PrefixLM)              Let’s
    explore these in detail.    ## Full Language Modeling    [Figure 4-7](#full-language-modeling)
    shows the canonical FLM objective at work.  ![Full Language Modeling](assets/dllm_0407.png)  ######
    Figure 4-7\. Full language modeling    This is the canonical language modeling
    objective of learning to predict the next token in a sequence and currently the
    simplest and most common training objective, used by GPT-4 and a vast number of
    open source models. The loss is computed for every token the model sees, i.e.,
    every single token in the training set that is being asked to be predicted by
    the language model provides a learning signal for the model, making it very efficient.    Let’s
    explore an example, using the GPT Neo model.    Suppose we continue pre-training
    the GPT Neo model from its publicly available checkpoint, using the full language
    modeling objective. Let’s say the current training sequence is:    [PRE16]    You
    can run this code:    [PRE17]    This code tokenizes the input text `Language
    models are` and feeds it to the model by invoking the `generate()` function. The
    function predicts the continuation, given the sequence “Language models are.”
    It outputs only one token and stops generating because `max_new_tokens` is set
    to 1\. The rest of the code enables it to output the top 20 list of tokens with
    the highest score, prior to applying the softmax at the last layer.    The top
    20 tokens with the highest prediction score are:    [PRE18]    Every word in the
    top 20 seems to be a valid continuation of the sequence. The ground truth is the
    token `ubiquitous`, which we can use to calculate the loss and initiate the backpropagation
    process for learning.    As another example, consider the text sequence:    [PRE19]    Run
    the same code as previously, except for this change:    [PRE20]    The top 20
    output tokens are:    [PRE21]    The correct answer has the 17th highest score.
    A lot of numbers appear in the top 10, showing that the model is more or less
    randomly guessing the answer, which is not surprising for a smaller model like
    GPT Neo.    The OpenAI API provides the `logprobs` parameter that allows you to
    specify the number of tokens along with their log probabilities that need to be
    returned. As of the book’s writing, only the `logprobs` of the 20 most probable
    tokens are available. The tokens returned are in order of their log probabilities:    [PRE22]    This
    code calls the older gpt-4o model, asking it to generate a maximum of one token.
    The output is:    [PRE23]    gpt-4o is pretty confident that the answer is 13,
    and rightfully so. The rest of the top probability tokens are all related to output
    formatting.    ###### Tip    During inference, we don’t necessarily need to generate
    the token with the highest score. Several *decoding strategies* allow you to generate
    more diverse text. We will discuss these strategies in [Chapter 5](ch05.html#chapter_utilizing_llms).    ##
    Prefix Language Modeling    Prefix LM is similar to the FLM setting. The difference
    is that FLM is fully causal, i.e., in a left-to-right writing system like English,
    tokens do not attend to tokens to the right (future). In the prefix LM setting,
    a part of the text sequence, called the prefix, is allowed to attend to future
    tokens in the prefix. The prefix part is thus noncausal. For training prefix LMs,
    a random prefix length is sampled, and the loss is calculated over only the tokens
    in the suffix.    ## Masked Language Modeling    [Figure 4-8](#masked-language-modeling-bert)
    shows the canonical MLM objective at work.  ![Masked Language Modeling in BERT](assets/dllm_0408.png)  ######
    Figure 4-8\. Masked Language Modeling in BERT    In the MLM setting, rather than
    predict the next token in a sequence, we ask the model to predict masked tokens
    within the sequence. In the most basic form of MLM implemented in the BERT model,
    15% of tokens are randomly chosen to be masked and are replaced with a special
    mask token, and the language model is asked to predict the original tokens.    The
    T5 model creators used a modification of the original MLM objective. In this variant,
    15% of tokens are randomly chosen to be removed from a sequence. Consecutive dropped-out
    tokens are replaced by a single unique special token called the *sentinel token*.
    The model is then asked to predict and generate the dropped tokens, delineated
    by the sentinel tokens.    As an example, consider this sequence:    > Tempura
    has always been a source of conflict in the family due to unexplained reasons    Let’s
    say we drop the tokens “has,” “always,” “of,” and “conflict.” The sequence is
    now:    > Tempura <S1> been a source <S2> in the family due to unexplained reasons    with
    S1, S2 being the sentinel tokens. The model is expected to output:    > <S1> has
    always <S2> of conflict <E>    The output sequence is terminated by a special
    token indicating the end of the sequence.    Generating only the dropped tokens
    and not the entire sequence is computationally more efficient and saves training
    time. Note that unlike in Full Language Modeling, the loss is calculated over
    only a small proportion of tokens (the masked tokens) in the input sequence.    Let’s
    explore this on Hugging Face:    [PRE24]`` `targets` `=` `tokenizer``(``"<extra_id_0>
    has always <extra_id_1> of conflict` [PRE25] `loss` `=` `model``(``input_ids``=``input_ids``,`
    `labels``=``labels``)``.``loss` [PRE26]` [PRE27]   [PRE28]  [PRE29]  [PRE30] [PRE31]`py
    [PRE32]'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
