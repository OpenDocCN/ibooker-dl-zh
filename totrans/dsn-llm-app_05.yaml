- en: Chapter 4\. Architectures and Learning Objectives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In Chapters [2](ch02.html#ch02) and [3](ch03.html#chapter-LLM-tokenization),
    we discussed some of the key ingredients that go into making a language model:
    the training datasets, and the vocabulary and tokenizer. Next, let’s complete
    the puzzle by learning about the models themselves, the architectures underpinning
    them, and their learning objectives.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn the composition of language models and their
    structure. Modern-day language models are predominantly based on the Transformer
    architecture, and hence we will devote most of our focus to understanding it,
    by going through each component of the architecture in detail. Over the last few
    years, several variants and alternatives to the original Transformer architecture
    have been proposed. We will go through the promising ones, including Mixture of
    Experts (MoE) models. We will also examine commonly used learning objectives the
    language models are trained over, including next-token prediction. Finally, we
    will bring together the concepts of the last three chapters in practice by learning
    how to pre-train a language model from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Preliminaries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just about every contemporary language model is based on neural networks, composed
    of processing units called *neurons*. While modern neural networks do not resemble
    the workings of the human brain at all, many of the ideas behind neural networks
    and the terminology used is inspired by the field of neuroscience.
  prefs: []
  type: TYPE_NORMAL
- en: The neurons in a neural network are connected to each other according to some
    configuration. Each connection between a pair of neurons is associated with a
    weight (also called *parameter*), indicating the strength of the connection. The
    role these neurons play and the way they are connected to each other constitutes
    the *architecture* of the model.
  prefs: []
  type: TYPE_NORMAL
- en: The early 2010s saw the proliferation of multi-layer architectures, with layers
    of neurons stacked on top of each other, each layer extracting progressively more
    complex features of the input. This paradigm is called *deep learning.*
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-1](#MLP-00) depicts a simple multi-layer neural network, also called
    the multi-layer perceptron.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Transformer](assets/dllm_0401.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-1\. Multi-layer perceptron
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For a more comprehensive treatment of neural networks, refer to [Goldberg’s
    book](https://oreil.ly/oDc6x) on neural network–based natural language processing.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in [Chapter 1](ch01.html#chapter_llm-introduction), language models
    are primarily pre-trained using self-supervised learning. Input text from the
    training dataset is tokenized and converted to vector form. The input is then
    propagated through the neural network, affected by its weights and *activation
    functions*, the latter introducing nonlinearity to the model. The output of the
    model is compared to the expected output, called the gold truth. The weights of
    the output are adapted such that next time for the same input, the output can
    be closer to the gold truth.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, this adaptation process is implemented through a *loss function*.
    The goal of the model is to minimize the loss, which is the difference between
    the model output and the gold truth. To minimize the loss, the weights are updated
    using a gradient-descent based method, called backpropagation. I strongly recommend
    developing an intuitive understanding of this algorithm before diving into model
    training.
  prefs: []
  type: TYPE_NORMAL
- en: Representing Meaning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While describing neural network–based architectures in the previous section,
    we glossed over the fact that the input text is converted into vectors and then
    propagated through the network. What are these vectors composed of and what do
    they represent? Ideally, after the model is trained, these vectors should accurately
    represent some aspect of the meaning of the underlying text, including its social
    connotations. Developing the right representations for modalities like text or
    images is a very active field of research, called *representation learning*.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When training a language model from scratch, these vectors initially mean nothing,
    as they are randomly generated. In practice, there are initialization algorithms
    used like Glorot, He, etc. Refer to [this report](https://oreil.ly/A8Iro) for
    a primer on neural network initialization.
  prefs: []
  type: TYPE_NORMAL
- en: How can a list of numbers represent meaning? It is hard for humans to describe
    the meaning of a word or sentence, let alone represent it in numerical form that
    can be processed by a computer. The *form* of a word, i.e., the letters that comprise
    it, usually do not give any information whatsoever about the meaning it represents.
    For example, the sequence of letters in the word *umbrella* contains no hints
    about its meaning, even if you are already exposed to thousands of other words
    in the English language.
  prefs: []
  type: TYPE_NORMAL
- en: 'The prominent way of representing meaning in numerical form is through the
    *distributional hypothesis* framework. The distributional hypothesis states that
    words that have similar meaning occur in similar contexts. The implication of
    this hypothesis is best represented by the adage:'
  prefs: []
  type: TYPE_NORMAL
- en: You shall know a word by the company it keeps.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: John Rupert Firth, 1957
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is one of the primary ways in which we pick up the meaning of words we
    haven’t encountered previously, without needing to look them up in a dictionary.
    A large number of words we know weren’t learned from the dictionary or by explicitly
    learning the meaning of a word but by estimating meaning based on the contexts
    words appear in.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s investigate how the distributional hypothesis works in practice. The Natural
    Language Toolkit (NLTK) library provides a feature called *concordance view*,
    which presents you with the surrounding contexts that a given word appears in
    a corpus.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s see the contexts in which the word “nervous” occurs in the
    Jane Austen classic *Emma*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The output looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The Transformer Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have developed an intuition on how text is represented in vector
    form, let’s further explore the canonical architecture used for training language
    models today, the Transformer.
  prefs: []
  type: TYPE_NORMAL
- en: In the mid 2010s, the predominant architectures used for NLP tasks were recurrent
    neural networks, specifically a variant called long short-term memory (LSTM).
    While knowledge of recurrent neural networks is not a prerequisite for this book,
    I recommend [*Neural Network Methods for Natural Language Processing*](https://oreil.ly/CHCTd)
    for more details.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent neural networks were sequence models, which means they processed text
    one token at a time, sequentially. A single vector was used to represent the state
    of the entire sequence, so as the sequence grew longer, more and more information
    needed to be captured in the single state vector. Because of the sequential nature
    of processing, long-range dependencies were harder to capture, as the content
    from the beginning of the sequence would be harder to retain.
  prefs: []
  type: TYPE_NORMAL
- en: 'This issue was candidly articulated by Ray Mooney, a senior computer scientist
    who remarked at the Association for Computational Linguistics (ACL) 2014 conference:'
  prefs: []
  type: TYPE_NORMAL
- en: You can’t cram the meaning of a whole %&!$# sentence into a single $&!#* vector!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ray Mooney, 2014
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Thus, there was a need for an architecture that solved for the deficiencies
    of LSTM: the limitations in representing long-range dependencies, the dependence
    on a single vector for representing the state of the entire sequence, and more.
    The Transformer architecture was designed to address these issues.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-2](#Transformer0) depicts the original Transformer architecture,
    developed in 2017 by [Vaswani et al.](https://oreil.ly/tIvGZ) As we can see in
    the figure, a Transformer model is typically composed of Transformer blocks stacked
    on top of each other, called *layers*. The key components of each block are:'
  prefs: []
  type: TYPE_NORMAL
- en: Self-attention
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Positional encoding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feedforward networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalization blocks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Transformer](assets/dllm_0402.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-2\. The Transformer architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'At the beginning of the first block is a special layer called the *embedding*
    layer. This is where the tokens in the input text are mapped to their corresponding
    vector. The embedding layer is a matrix whose size is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'On Hugging Face, we can inspect the embedding layer as such, using the `transformers`
    library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4] ''Mark told Sam that he was planning to resign.'' [PRE5] import torch
    import torch.nn as nn import torch.nn.functional as F  q = wQ(input_embeddings)
    k = WK(input_embeddings) v = WV(input_embeddings) dim_k = k.size(-1)  attn_scores
    = torch.matmul(q, k.transpose(-2, -1)) scaled_attn_scores = attn_scores/torch.sqrt(torch.tensor(dim_k,   dtype=torch.float32))  normalized_attn_scores
    = F.softmax(scaled_attn_scores, dim=-1)  output = torch.matmul(normalized_attn_scores,
    v) [PRE6] import torch import torch.nn as nn  class FeedForward(nn.Module):     def
    __init__(self, input_dim, hidden_dim):         super(FeedForward, self).__init__()         self.l1
    = nn.Linear(input_dim, hidden_dim)         self.l2 = nn.Linear(hidden_dim, input_dim)         self.selu
    = nn.SeLU()      def forward(self, x):         x = self.selu(self.l1(x))         x
    = self.l2(x)         return x   feed_forward = FeedForward(input_dim, hidden_dim)
    outputs = feed_forward(inputs) [PRE7] import torch import torch.nn as nn  class
    LayerNorm(nn.Module):     def __init__(self, dimension, gamma=None, beta=None,
    epsilon=1e-5):         super(LayerNorm, self).__init__()         self.epsilon
    = epsilon         self.gamma = gamma if gamma is not None else         nn.Parameter(torch.ones(dimension))         self.beta
    = beta if beta is not None else         nn.Parameter(torch.zeros(dimension))       def
    forward(self, x):         mean = x.mean(-1, keepdim=True)         variance = x.var(-1,
    keepdim=True, unbiased=False)         x_normalized = (x - mean) / torch.sqrt(variance
    + self.epsilon)         return self.gamma * x_normalized + self.beta  layer_norm
    = LayerNorm(embedding_dim) outputs = layer_norm(inputs) [PRE8]`  [PRE9]` # Loss
    Functions    So far, we have discussed all the components of each Transformer
    block. For the next token-prediction learning objective, the input is propagated
    through the Transformer layers to generate the final output, which is a probability
    distribution across all tokens. During training, the loss is calculated by comparing
    the output distribution with the gold truth. The gold truth distribution assigns
    a 1 to the gold truth token and 0 to all other tokens.    There are many possible
    ways to quantify the difference between the output and the gold truth. The most
    popular one is cross-entropy, which is calculated by the formula:    [PRE10]    For
    example, consider the sequence:    [PRE11]    Let’s say the gold truth token is
    *good*, and the output probability distribution is (*terrible*: 0.65, *bad*:0.12,
    *good*:011,…​)    The cross-entropy is calculated as:    [PRE12]    Since the
    gold truth distribution values are 0 for all but the correct token, the equation
    can be simplified to:    [PRE13]    Once the loss is calculated, the gradient
    of the loss with respect to the parameters of the model is calculated and the
    weights are updated, using the backpropagation algorithm.    # Intrinsic Model
    Evaluation    How do we know if the backpropagation algorithm is actually working
    and that the model is getting better over time? We can use either intrinsic model
    evaluation or extrinsic model evaluation.    Extrinsic model evaluation involves
    testing the model’s performance on real-world downstream tasks. These tasks directly
    test the performance of the model but only on a narrow range of the model’s capabilities.
    In contrast, intrinsic model evaluation involves a more general evaluation of
    the model’s ability to model language, but with no guarantee that its performance
    in the intrinsic evaluation metric is directly proportional to its performance
    across all possible downstream tasks.    The most common intrinsic evaluation
    metric is *perplexity*. Perplexity measures the ability of a language model to
    accurately predict the next token in a sequence. A model that can always correctly
    predict the next token has a perplexity of 1\. The higher the perplexity, the
    worse the language model. In the worst case, if the model is predicting at random,
    with probability of predicting each token in a vocabulary of size V being 1/V,
    then the perplexity is V.    Perplexity is related to cross-entropy by this formula:    [PRE14]    #
    Transformer Backbones    So far, we described the components of the canonical
    version of the Transformer. In practice, three major types of architecture backbones
    are used to implement the Transformer:    *   Encoder-only           *   Encoder-decoder           *   Decoder-only              Let’s
    look at each of these in detail.    [Figure 4-5](#enc-dec) depicts encoder-only,
    encoder-decoder, and decoder-only architectures.  ![enc-dec](assets/dllm_0405.png)  ######
    Figure 4-5\. Visualization of various Transformer backbones    ## Encoder-Only
    Architectures    Encoder-only architectures were all the rage when Transformer-based
    language models first burst on the scene. Iconic language models from yesteryear
    (circa 2018) that use encoder-only architectures include BERT, RoBERTa, etc.    There
    haven’t been many encoder-only LLMs trained since 2021 for a few reasons, including:    *   They
    are relatively harder to train.           *   The masked language modeling objective
    typically used to train them provides a learning signal in only a small percentage
    of tokens (the masking rate), thus needing a lot more data to reach the same level
    of performance as decoder-only models.           *   For every downstream task,
    you need to train a separate task-specific head, making usage inefficient.              However,
    the release of ModernBERT seems to have reinvigorated this space.    The creators
    of the UL2 language model claim that encoder-only models should be considered
    obsolete. I personally wouldn’t go that far; encoder-only models are still great
    choices for classification tasks. Moreover, if you already have a satisfactory
    pipeline for your use case built around encoder-only models, I would say if it
    ain’t broke, why fix it?    Here are some guidelines for adopting encoder-only
    models:    *   RoBERTa performs better than BERT most of the time, since it is
    trained a lot longer on more data, and it has adopted best practices learned after
    the release of BERT.           *   DeBERTa and ModernBERT are currently regarded
    as the best-performing encoder-only models.           *   The distilled versions
    of encoder-only models like DistilBERT, etc., are not too far off from the original
    models in terms of performance, and they should be considered if you are operating
    under resource constraints.              Several embedding models are built from
    encoder-only models. For example, one of the most important libraries in the field
    of NLP, the Swiss Army knife of NLP tools, *sentence transformers*, provides encoder-only
    embedding models that are very widely used. all-mpnet-base-v2, based on an encoder-only
    model called MPNet, and fine-tuned on several task datasets, is still competitive
    with much larger embedding models.    ## Encoder-Decoder Architectures    This
    is the original architecture of the Transformer, as it was first proposed. The
    T5 series of models uses this architecture type.    In encoder-decoder models,
    the input is text and the output is also text. A standardized interface ensures
    that the same model and training procedure can be used for multiple tasks. The
    inputs are handled by an encoder, and the outputs by the decoder.    ## Decoder-Only
    Architectures    A majority of LLMs trained today use decoder-only models. Decoder-only
    models came into fashion starting from the original GPT model from OpenAI. Decoder-only
    models excel at zero-shot and few-shot learning.    Decoder models can be causal
    and noncausal. Noncausal models have bidirectionality over the input sequence,
    while the output is still autoregressive (you cannot look ahead).    ###### Tip    While
    the field is still evolving, there has been some [compelling evidence](https://oreil.ly/Sb7JS)
    for the following results:    *   Decoder-only models are the best choice for
    zero-shot and few-shot generalization.           *   Encoder-decoder models are
    the best choice for multi-task fine tuning.              The best of both worlds
    is to combine the two: start with auto-regressive training, and then in an adaptation
    step, pre-train further with a noncausal setup using a span corruption objective.    In
    this section, we discussed how architectural backbones can be classified according
    to how they use the architecture’s encoder and decoder. Another architectural
    backbone type that is making inroads in the past year is the Mixture of Experts
    (MoE) paradigm. Let’s explore that in detail.    ## Mixture of Experts    Remarkably,
    in the seven years since the invention of the Transformer architecture, the Transformer
    implementation used in current language models isn’t too different from the original
    version, despite hundreds of papers proposing modifications to it. The original
    architecture has proven to be surprisingly robust, with most proposed variants
    barely moving the needle in terms of performance. However, some components of
    the Transformer have seen changes, like positional encodings as discussed earlier
    in the chapter.    MoE models have been seeing a lot of success in the past couple
    of years. Examples include OpenAI’s GPT-4 (unconfirmed), Google’s Switch, DeepSeek’s
    DeepSeek V3, and Mistral’s Mixtral. In this section, we will learn the motivations
    behind developing this architecture and how it works in practice.    As shown
    in [Chapter 1](ch01.html#chapter_llm-introduction), the scaling laws dictate that
    performance of the language model increases as you increase the size of the model
    and its training data. However, increasing the model capacity implies more compute
    is needed for both training and inference. This is undesirable, especially at
    inference time, when latency requirements can be stringent. Can we increase the
    capacity of a model without increasing the required compute?    One way to achieve
    this is using conditional computation; each input (either a token or the entire
    sequence) sees a different subset of the model, interacting with only the parameters
    that are best suited to process it. This is achieved by composing the architecture
    to be made up of several components called experts, with only a subset of experts
    being activated for each input.    [Figure 4-6](#MoE-0) depicts a canonical MoE
    model.  ![MoE](assets/dllm_0406.png)  ###### Figure 4-6\. Mixture of Experts    A
    key component of the MoE architecture is the *gating function*. The gating function
    helps decide which expert is more suited to process a given input. The gating
    function is implemented as a weight applied to each expert.    The experts are
    typically added to the feedforward component of the Transformer. Therefore, if
    there are eight experts, then there will be eight feedforward networks instead
    of one. Based on the routing strategy used, only a small subset of these networks
    will be activated for a given input.    The routing strategy determines the number
    and type of experts activated. Two types of popular routing strategies exist:    *   Tokens
    choose           *   Experts choose              In the tokens choose strategy,
    each token chooses k experts. k is typically a small number (~2). The disadvantage
    of using this strategy is the need for load balancing. If in a given input batch,
    most of the tokens end up using the same experts, then additional time is needed
    to finish the computation as we cannot benefit from the parallelization afforded
    by multiple experts.    In the experts choose strategy, each expert picks the
    tokens that it is most equipped to handle. This solves the load balancing problem
    as we can specify that each expert choose the same number of tokens. However,
    this also leads to inefficient token-expert matching, as each expert is limited
    to picking only a finite number of tokens in a batch.    # Learning Objectives    Now
    that we have discussed the architecture of language models, let’s turn our focus
    to understanding the tasks they are trained on during the pre-training process.    As
    mentioned earlier in the chapter, language models are pre-trained in a self-supervised
    manner. The scale of data we need to train them makes it prohibitively expensive
    to perform supervised learning, where (input, output) examples need to come from
    humans. Instead, we use a form of training called self-supervision, where the
    data itself contains the target labels. The goal of self-supervised learning is
    to learn a task which acts as a proxy for learning the syntax and semantics of
    a language, as well as skills like reasoning, arithmetic and logical manipulation,
    and other cognitive tasks, and (hopefully) eventually leading up to general human
    intelligence. How does this work?    For example, let’s take the canonical language
    modeling task: predicting the next word that comes in a sequence. Consider the
    sequence:    [PRE15]    and the language model is asked to predict the next token.
    The total number of possible answers is the size of the vocabulary. There are
    many valid continuations to this sequence, like (hedge, fence, barbecue, sandcastle,
    etc.), but many continuations to this sequence would violate English grammar rules
    like (is, of, the). During the training process, after seeing billions of sequences,
    the model will know that it is highly improbable for the word “the” to be followed
    by the word “is” or “of,” regardless of the surrounding context. Thus, you can
    see how just predicting the next token is such a powerful tool: in order to correctly
    predict the next token you can eventually learn more and more complex functions
    that you can encode in your model connections. However, whether this paradigm
    is all we need to develop general intelligence is an open question.    Self-supervised
    learning objectives used for pre-training LLMs can be broadly classified (nonexhaustively)
    into three types:    *   Full language modeling (FLM)           *   Masked language
    modeling (MLM)           *   Prefix language modeling (PrefixLM)              Let’s
    explore these in detail.    ## Full Language Modeling    [Figure 4-7](#full-language-modeling)
    shows the canonical FLM objective at work.  ![Full Language Modeling](assets/dllm_0407.png)  ######
    Figure 4-7\. Full language modeling    This is the canonical language modeling
    objective of learning to predict the next token in a sequence and currently the
    simplest and most common training objective, used by GPT-4 and a vast number of
    open source models. The loss is computed for every token the model sees, i.e.,
    every single token in the training set that is being asked to be predicted by
    the language model provides a learning signal for the model, making it very efficient.    Let’s
    explore an example, using the GPT Neo model.    Suppose we continue pre-training
    the GPT Neo model from its publicly available checkpoint, using the full language
    modeling objective. Let’s say the current training sequence is:    [PRE16]    You
    can run this code:    [PRE17]    This code tokenizes the input text `Language
    models are` and feeds it to the model by invoking the `generate()` function. The
    function predicts the continuation, given the sequence “Language models are.”
    It outputs only one token and stops generating because `max_new_tokens` is set
    to 1\. The rest of the code enables it to output the top 20 list of tokens with
    the highest score, prior to applying the softmax at the last layer.    The top
    20 tokens with the highest prediction score are:    [PRE18]    Every word in the
    top 20 seems to be a valid continuation of the sequence. The ground truth is the
    token `ubiquitous`, which we can use to calculate the loss and initiate the backpropagation
    process for learning.    As another example, consider the text sequence:    [PRE19]    Run
    the same code as previously, except for this change:    [PRE20]    The top 20
    output tokens are:    [PRE21]    The correct answer has the 17th highest score.
    A lot of numbers appear in the top 10, showing that the model is more or less
    randomly guessing the answer, which is not surprising for a smaller model like
    GPT Neo.    The OpenAI API provides the `logprobs` parameter that allows you to
    specify the number of tokens along with their log probabilities that need to be
    returned. As of the book’s writing, only the `logprobs` of the 20 most probable
    tokens are available. The tokens returned are in order of their log probabilities:    [PRE22]    This
    code calls the older gpt-4o model, asking it to generate a maximum of one token.
    The output is:    [PRE23]    gpt-4o is pretty confident that the answer is 13,
    and rightfully so. The rest of the top probability tokens are all related to output
    formatting.    ###### Tip    During inference, we don’t necessarily need to generate
    the token with the highest score. Several *decoding strategies* allow you to generate
    more diverse text. We will discuss these strategies in [Chapter 5](ch05.html#chapter_utilizing_llms).    ##
    Prefix Language Modeling    Prefix LM is similar to the FLM setting. The difference
    is that FLM is fully causal, i.e., in a left-to-right writing system like English,
    tokens do not attend to tokens to the right (future). In the prefix LM setting,
    a part of the text sequence, called the prefix, is allowed to attend to future
    tokens in the prefix. The prefix part is thus noncausal. For training prefix LMs,
    a random prefix length is sampled, and the loss is calculated over only the tokens
    in the suffix.    ## Masked Language Modeling    [Figure 4-8](#masked-language-modeling-bert)
    shows the canonical MLM objective at work.  ![Masked Language Modeling in BERT](assets/dllm_0408.png)  ######
    Figure 4-8\. Masked Language Modeling in BERT    In the MLM setting, rather than
    predict the next token in a sequence, we ask the model to predict masked tokens
    within the sequence. In the most basic form of MLM implemented in the BERT model,
    15% of tokens are randomly chosen to be masked and are replaced with a special
    mask token, and the language model is asked to predict the original tokens.    The
    T5 model creators used a modification of the original MLM objective. In this variant,
    15% of tokens are randomly chosen to be removed from a sequence. Consecutive dropped-out
    tokens are replaced by a single unique special token called the *sentinel token*.
    The model is then asked to predict and generate the dropped tokens, delineated
    by the sentinel tokens.    As an example, consider this sequence:    > Tempura
    has always been a source of conflict in the family due to unexplained reasons    Let’s
    say we drop the tokens “has,” “always,” “of,” and “conflict.” The sequence is
    now:    > Tempura <S1> been a source <S2> in the family due to unexplained reasons    with
    S1, S2 being the sentinel tokens. The model is expected to output:    > <S1> has
    always <S2> of conflict <E>    The output sequence is terminated by a special
    token indicating the end of the sequence.    Generating only the dropped tokens
    and not the entire sequence is computationally more efficient and saves training
    time. Note that unlike in Full Language Modeling, the loss is calculated over
    only a small proportion of tokens (the masked tokens) in the input sequence.    Let’s
    explore this on Hugging Face:    [PRE24]`` `targets` `=` `tokenizer``(``"<extra_id_0>
    has always <extra_id_1> of conflict` [PRE25] `loss` `=` `model``(``input_ids``=``input_ids``,`
    `labels``=``labels``)``.``loss` [PRE26]` [PRE27]   [PRE28]  [PRE29]  [PRE30] [PRE31]`py
    [PRE32]'
  prefs: []
  type: TYPE_NORMAL
