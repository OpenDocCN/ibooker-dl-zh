- en: Chapter 4\. Architectures and Learning Objectives
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章. 架构和学习目标
- en: 'In Chapters [2](ch02.html#ch02) and [3](ch03.html#chapter-LLM-tokenization),
    we discussed some of the key ingredients that go into making a language model:
    the training datasets, and the vocabulary and tokenizer. Next, let’s complete
    the puzzle by learning about the models themselves, the architectures underpinning
    them, and their learning objectives.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 [2](ch02.html#ch02) 章和第 [3](ch03.html#chapter-LLM-tokenization) 章中，我们讨论了构建语言模型的关键要素：训练数据集、词汇表和分词器。接下来，让我们通过学习模型本身、支撑它们的架构以及它们的学习目标来完善这个拼图。
- en: In this chapter, we will learn the composition of language models and their
    structure. Modern-day language models are predominantly based on the Transformer
    architecture, and hence we will devote most of our focus to understanding it,
    by going through each component of the architecture in detail. Over the last few
    years, several variants and alternatives to the original Transformer architecture
    have been proposed. We will go through the promising ones, including Mixture of
    Experts (MoE) models. We will also examine commonly used learning objectives the
    language models are trained over, including next-token prediction. Finally, we
    will bring together the concepts of the last three chapters in practice by learning
    how to pre-train a language model from scratch.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习语言模型的组成及其结构。现代语言模型主要基于 Transformer 架构，因此我们将主要关注理解它，通过详细研究架构的每个组件。在过去的几年里，已经提出了几种原始
    Transformer 架构的变体和替代方案。我们将探讨其中一些有前景的方案，包括混合专家（MoE）模型。我们还将检查语言模型训练中常用的学习目标，包括下一个标记预测。最后，我们将通过学习如何从头开始预训练语言模型，将前三章的概念在实践中结合起来。
- en: Preliminaries
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: Just about every contemporary language model is based on neural networks, composed
    of processing units called *neurons*. While modern neural networks do not resemble
    the workings of the human brain at all, many of the ideas behind neural networks
    and the terminology used is inspired by the field of neuroscience.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有的当代语言模型都是基于神经网络，由称为*神经元*的处理单元组成。虽然现代神经网络与人类大脑的工作方式完全不相似，但许多神经网络背后的思想和使用的术语都受到了神经科学领域的影响。
- en: The neurons in a neural network are connected to each other according to some
    configuration. Each connection between a pair of neurons is associated with a
    weight (also called *parameter*), indicating the strength of the connection. The
    role these neurons play and the way they are connected to each other constitutes
    the *architecture* of the model.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的神经元按照某种配置相互连接。每对神经元之间的连接都关联着一个权重（也称为*参数*），表示连接的强度。这些神经元所扮演的角色以及它们相互连接的方式构成了模型的*架构*。
- en: The early 2010s saw the proliferation of multi-layer architectures, with layers
    of neurons stacked on top of each other, each layer extracting progressively more
    complex features of the input. This paradigm is called *deep learning.*
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 2010 年代初，多层架构开始流行，神经元层堆叠在一起，每一层提取输入的更复杂特征。这种范式被称为*深度学习*。
- en: '[Figure 4-1](#MLP-00) depicts a simple multi-layer neural network, also called
    the multi-layer perceptron.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-1](#MLP-00) 展示了一个简单的多层神经网络，也称为多层感知器。'
- en: '![Transformer](assets/dllm_0401.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![Transformer](assets/dllm_0401.png)'
- en: Figure 4-1\. Multi-layer perceptron
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-1\. 多层感知器
- en: Tip
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: For a more comprehensive treatment of neural networks, refer to [Goldberg’s
    book](https://oreil.ly/oDc6x) on neural network–based natural language processing.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 若想更全面地了解神经网络，请参阅 [Goldberg 的书籍](https://oreil.ly/oDc6x)关于基于神经网络的自然语言处理。
- en: As discussed in [Chapter 1](ch01.html#chapter_llm-introduction), language models
    are primarily pre-trained using self-supervised learning. Input text from the
    training dataset is tokenized and converted to vector form. The input is then
    propagated through the neural network, affected by its weights and *activation
    functions*, the latter introducing nonlinearity to the model. The output of the
    model is compared to the expected output, called the gold truth. The weights of
    the output are adapted such that next time for the same input, the output can
    be closer to the gold truth.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第1章](ch01.html#chapter_llm-introduction)所述，语言模型主要是通过自监督学习进行预训练的。训练数据集中的输入文本被标记化并转换为向量形式。然后，输入通过神经网络传播，受到其权重和*激活函数*的影响，后者为模型引入了非线性。模型的输出与预期输出（称为黄金真实值）进行比较。输出权重被调整，以便下次对于相同的输入，输出可以更接近黄金真实值。
- en: In practice, this adaptation process is implemented through a *loss function*.
    The goal of the model is to minimize the loss, which is the difference between
    the model output and the gold truth. To minimize the loss, the weights are updated
    using a gradient-descent based method, called backpropagation. I strongly recommend
    developing an intuitive understanding of this algorithm before diving into model
    training.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这个过程是通过*损失函数*实现的。模型的目标是最小化损失，即模型输出与黄金真实值之间的差异。为了最小化损失，使用基于梯度下降的方法，称为反向传播，来更新权重。我强烈建议在深入模型训练之前，先对这种算法有一个直观的理解。
- en: Representing Meaning
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 表示意义
- en: While describing neural network–based architectures in the previous section,
    we glossed over the fact that the input text is converted into vectors and then
    propagated through the network. What are these vectors composed of and what do
    they represent? Ideally, after the model is trained, these vectors should accurately
    represent some aspect of the meaning of the underlying text, including its social
    connotations. Developing the right representations for modalities like text or
    images is a very active field of research, called *representation learning*.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中描述基于神经网络的架构时，我们忽略了这样一个事实，即输入文本被转换为向量，然后通过网络传播。这些向量由什么组成，它们代表什么？理想情况下，在模型训练后，这些向量应该准确地代表文本的某些方面，包括其社会含义。为文本或图像等模态开发正确的表示是一个非常活跃的研究领域，称为*表示学习*。
- en: Note
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: When training a language model from scratch, these vectors initially mean nothing,
    as they are randomly generated. In practice, there are initialization algorithms
    used like Glorot, He, etc. Refer to [this report](https://oreil.ly/A8Iro) for
    a primer on neural network initialization.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当从头开始训练语言模型时，这些向量最初没有任何意义，因为它们是随机生成的。在实践中，使用了如Glorot、He等初始化算法。有关神经网络初始化的入门指南，请参阅[这份报告](https://oreil.ly/A8Iro)。
- en: How can a list of numbers represent meaning? It is hard for humans to describe
    the meaning of a word or sentence, let alone represent it in numerical form that
    can be processed by a computer. The *form* of a word, i.e., the letters that comprise
    it, usually do not give any information whatsoever about the meaning it represents.
    For example, the sequence of letters in the word *umbrella* contains no hints
    about its meaning, even if you are already exposed to thousands of other words
    in the English language.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一系列数字如何表示意义？人类很难描述一个词或句子的意义，更不用说以计算机可以处理的形式来表示它了。一个词的*形式*，即构成它的字母，通常不会提供关于它所代表的意义的任何信息。例如，单词*umbrella*中的字母序列没有任何关于其意义的线索，即使你已经接触到了成千上万的英语单词。
- en: 'The prominent way of representing meaning in numerical form is through the
    *distributional hypothesis* framework. The distributional hypothesis states that
    words that have similar meaning occur in similar contexts. The implication of
    this hypothesis is best represented by the adage:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在数值形式中表示意义的主要方式是通过*分布假设*框架。分布假设指出，具有相似意义的词出现在相似语境中。这个假设的含义最好通过以下谚语来表示：
- en: You shall know a word by the company it keeps.
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你可以通过一个词所伴随的词群来了解这个词。
- en: ''
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: John Rupert Firth, 1957
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 约翰·鲁伯特·费尔思，1957
- en: This is one of the primary ways in which we pick up the meaning of words we
    haven’t encountered previously, without needing to look them up in a dictionary.
    A large number of words we know weren’t learned from the dictionary or by explicitly
    learning the meaning of a word but by estimating meaning based on the contexts
    words appear in.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们获取我们之前未曾遇到过的词语意义的主要方式之一，而不需要查阅词典。我们知道的许多词语并不是从词典中学习，也不是通过明确学习一个词语的意义来学习的，而是通过根据词语出现的上下文估计意义。
- en: Let’s investigate how the distributional hypothesis works in practice. The Natural
    Language Toolkit (NLTK) library provides a feature called *concordance view*,
    which presents you with the surrounding contexts that a given word appears in
    a corpus.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们调查分布假设在实际中是如何工作的。自然语言工具包（NLTK）库提供了一个名为*共现视图*的功能，它向您展示给定词语在语料库中出现的上下文。
- en: 'For example, let’s see the contexts in which the word “nervous” occurs in the
    Jane Austen classic *Emma*:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们看看“nervous”一词在简·奥斯汀的经典作品*Emma*中出现的上下文：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output looks like this:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 输出看起来是这样的：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The Transformer Architecture
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer架构
- en: Now that we have developed an intuition on how text is represented in vector
    form, let’s further explore the canonical architecture used for training language
    models today, the Transformer.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对文本以向量形式表示有了直观的理解，让我们进一步探索今天用于训练语言模型的典型架构，即Transformer。
- en: In the mid 2010s, the predominant architectures used for NLP tasks were recurrent
    neural networks, specifically a variant called long short-term memory (LSTM).
    While knowledge of recurrent neural networks is not a prerequisite for this book,
    I recommend [*Neural Network Methods for Natural Language Processing*](https://oreil.ly/CHCTd)
    for more details.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在2010年代中期，用于NLP任务的占主导地位的架构是循环神经网络，特别是称为长短期记忆（LSTM）的变体。虽然对循环神经网络的知识不是本书的先决条件，但我推荐[*自然语言处理中的神经网络方法*](https://oreil.ly/CHCTd)以获取更多详细信息。
- en: Recurrent neural networks were sequence models, which means they processed text
    one token at a time, sequentially. A single vector was used to represent the state
    of the entire sequence, so as the sequence grew longer, more and more information
    needed to be captured in the single state vector. Because of the sequential nature
    of processing, long-range dependencies were harder to capture, as the content
    from the beginning of the sequence would be harder to retain.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络是序列模型，这意味着它们逐个标记地、顺序地处理文本。单个向量用于表示整个序列的状态，因此随着序列变长，越来越多的信息需要被捕获在单个状态向量中。由于处理的顺序性，长距离依赖性更难捕捉，因为序列开头的内容更难保留。
- en: 'This issue was candidly articulated by Ray Mooney, a senior computer scientist
    who remarked at the Association for Computational Linguistics (ACL) 2014 conference:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题由高级计算机科学家Ray Mooney坦率地阐述，他在2014年的计算语言学（ACL）会议上评论道：
- en: You can’t cram the meaning of a whole %&!$# sentence into a single $&!#* vector!
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你不能把整个句子%&!$#的意义塞进一个单一的$&!#*向量中！
- en: ''
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ray Mooney, 2014
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Ray Mooney，2014
- en: 'Thus, there was a need for an architecture that solved for the deficiencies
    of LSTM: the limitations in representing long-range dependencies, the dependence
    on a single vector for representing the state of the entire sequence, and more.
    The Transformer architecture was designed to address these issues.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，需要一个能够解决LSTM缺陷的架构：在表示长距离依赖性方面的局限性、依赖于单个向量来表示整个序列的状态，以及更多。Transformer架构被设计用来解决这些问题。
- en: '[Figure 4-2](#Transformer0) depicts the original Transformer architecture,
    developed in 2017 by [Vaswani et al.](https://oreil.ly/tIvGZ) As we can see in
    the figure, a Transformer model is typically composed of Transformer blocks stacked
    on top of each other, called *layers*. The key components of each block are:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-2](#Transformer0)展示了2017年由[Vaswani等人](https://oreil.ly/tIvGZ)开发的原始Transformer架构。如图所示，一个Transformer模型通常由堆叠在一起的Transformer块组成，称为*层*。每个块的关键组件包括：'
- en: Self-attention
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自注意力
- en: Positional encoding
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置编码
- en: Feedforward networks
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前馈网络
- en: Normalization blocks
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 归一化块
- en: '![Transformer](assets/dllm_0402.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![Transformer](assets/dllm_0402.png)'
- en: Figure 4-2\. The Transformer architecture
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-2\. Transformer架构
- en: 'At the beginning of the first block is a special layer called the *embedding*
    layer. This is where the tokens in the input text are mapped to their corresponding
    vector. The embedding layer is a matrix whose size is:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个块的开头有一个称为 *嵌入层* 的特殊层。这是将输入文本中的标记映射到其对应向量的地方。嵌入层是一个矩阵，其大小为：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'On Hugging Face, we can inspect the embedding layer as such, using the `transformers`
    library:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hugging Face上，我们可以使用`transformers`库检查嵌入层，如下所示：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The embedding vectors are the inputs that are then propagated through the rest
    of the network.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入向量是随后通过网络传播的输入。
- en: Next, let’s go through each of the components in a Transformer block in detail
    and explore their role in the modeling process.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们详细探讨Transformer块中的每个组件及其在建模过程中的作用。
- en: Self-Attention
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自注意力
- en: The self-attention mechanism draws on the same principle as the distributional
    hypothesis introduced in [“Representing Meaning”](#representing-meaning), emphasizing
    the role of context in shaping the meaning of a token. This operation generates
    representations for each token in a text sequence, capturing various aspects of
    language like syntax, semantics, and even pragmatics.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力机制借鉴了在[“表示意义”](#representing-meaning)中引入的分布假设原理，强调上下文在塑造标记意义中的作用。此操作为文本序列中的每个标记生成表示，捕捉语言的各种方面，如句法、语义，甚至语用学。
- en: In the standard self-attention implementation, the representation of each token
    is a function of the representation of all other tokens in the sequence. Given
    a token for which we are calculating its representation, tokens in the sequence
    that contribute more to the meaning of the token are given more weight.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准的自注意力实现中，每个标记的表示是序列中所有其他标记表示的函数。对于我们要计算其表示的标记，对标记意义贡献更大的序列中的标记被赋予更大的权重。
- en: 'For example, consider the sequence:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑以下序列：
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[Figure 4-3](#Attention-map) depicts how the representation for the token *he*
    is heavily weighted by the representation of the token *Mark*. In this case, the
    token *he* is a pronoun used to describe Mark in shorthand. In NLP, mapping a
    pronoun to its referent is called *co-reference resolution*.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-3](#Attention-map)描述了标记 *he* 的表示受到标记 *Mark* 表示的强烈影响。在这种情况下，标记 *he* 是一个用来简短描述Mark的代词。在NLP中，将代词映射到其指代物称为
    *共指消解*。'
- en: '![Attention-map](assets/dllm_0403.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![Attention-map](assets/dllm_0403.png)'
- en: Figure 4-3\. Attention map
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-3\. 注意力图
- en: In practice, self-attention in the Transformer is calculated using three sets
    of weight matrices called queries, keys, and values. Let’s go through them in
    detail. [Figure 4-4](#kqv) shows how the query, key, and value matrices are used
    in the self-attention calculation.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，Transformer中的自注意力使用三组称为查询、键和值的权重矩阵来计算。让我们详细了解一下。[图4-4](#kqv)展示了查询、键和值矩阵在自注意力计算中的使用。
- en: 'Each token is represented by its embedding vector. This vector is multiplied
    with the query, key, and value weight matrices to generate three input vectors.
    Self-attention for each token is then calculated like this:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 每个标记由其嵌入向量表示。该向量与查询、键和值权重矩阵相乘以生成三个输入向量。然后像这样计算每个标记的自注意力：
- en: For each token, the dot products of its query vector with the key vectors of
    all the tokens (including itself) are taken. The resulting values are called attention
    scores.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个标记，计算其查询向量与所有标记（包括自身）的键向量的点积。这些结果值被称为注意力分数。
- en: The scores are scaled down by dividing them by the square root of the dimension
    of the key vectors.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分数通过除以键向量维度的平方根来缩小。
- en: The scores are then passed through a [*softmax function*](https://oreil.ly/b6gHV)
    to turn them into a probability distribution that sums to 1\. The softmax activation
    function tends to amplify larger values, hence the reason for scaling down the
    attention scores in the previous step.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后将分数通过一个[*softmax函数*](https://oreil.ly/b6gHV)转换为概率分布，其总和为1。softmax激活函数倾向于放大较大的值，这就是为什么在上一步中需要缩小注意力分数的原因。
- en: The normalized attention scores are then multiplied by the value vector for
    the corresponding token. The normalized attention score can be interpreted as
    the proportion that each token contributes to the representation of a given token.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后将归一化的注意力分数乘以相应标记的值向量。归一化的注意力分数可以解释为每个标记对给定标记表示的贡献比例。
- en: In practice, there are multiple sets of query, key, and value vectors, calculating
    parallel representations. This is called multi-headed attention. The idea behind
    using multiple heads is that the model gets sufficient capacity to model various
    aspects of the input. The more the number of heads, the more chances that the
    *right* aspects of the input are being represented.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实际上，存在多个查询、键和值向量的集合，计算并行表示。这被称为多头注意力。使用多个头背后的想法是模型获得足够的容量来模拟输入的各个方面。头越多，表示输入的正确方面就越有可能。
- en: '![kqv](assets/dllm_0404.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![kqv](assets/dllm_0404.png)'
- en: Figure 4-4\. Self-attention calculation
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-4\. 自注意力计算
- en: 'This is how we implement self-attention in code:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们如何在代码中实现自注意力的方式：
- en: '[PRE5]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In some Transformer variants, self-attention is calculated only on a subset
    of tokens in the sequence; thus the vector representation of a token is a function
    of the representations of only some and not all the tokens in the sequence.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些Transformer变体中，自注意力仅在序列中的子集标记上计算；因此，标记的向量表示是序列中某些标记的表示的函数，而不是所有标记的函数。
- en: Positional Encoding
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 位置编码
- en: 'As discussed earlier, pre-Transformer architectures like LSTM were sequence
    models, with tokens being processed one after the other. Thus the positional information
    about the tokens, i.e., the relative positions of the tokens in a sequence, was
    implicitly baked into the model. However, for Transformers all calculations are
    done in parallel, and positional information should be fed to the model explicitly.
    Several methods have been proposed to add positional information, and this is
    still a very active field of research. Some of the common methods used in LLMs
    today include:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Transformer之前的架构，如LSTM，是序列模型，标记是依次处理的。因此，标记的位置信息，即标记在序列中的相对位置，被隐式地嵌入到模型中。然而，对于Transformer，所有计算都是并行进行的，位置信息应该显式地输入到模型中。已经提出了几种方法来添加位置信息，并且这仍然是一个非常活跃的研究领域。今天在LLM中使用的常见方法包括：
- en: Absolute positional embeddings
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 绝对位置嵌入
- en: These were used in the original Transformer implementation by [Vaswani et al.](https://oreil.ly/CDq60);
    examples of models using absolute positional embeddings include earlier models
    like BERT and RoBERTa.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这些在原始Transformer实现中由[Vaswani等人](https://oreil.ly/CDq60)使用；使用绝对位置嵌入的模型示例包括早期的BERT和RoBERTa。
- en: Attention with Linear Biases (ALiBi)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 带线性偏置的注意力（ALiBi）
- en: In this technique, the attention scores are [penalized](https://arxiv.org/abs/2108.12409)
    with a bias term proportional to the distance between the query token and the
    key token. This technique also enables modeling sequences of longer length during
    inference than what was encountered in the training process.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种技术中，注意力分数被一个与查询标记和键标记之间的距离成比例的偏置项[惩罚](https://arxiv.org/abs/2108.12409)。这种技术还使得在推理过程中能够建模比训练过程中遇到的更长的序列。
- en: Rotary Position Embedding (RoPE)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 旋转位置嵌入（RoPE）
- en: Just like ALiBi, this [technique](https://arxiv.org/abs/2104.09864) has the
    property of relative decay; there is a decay in the attention scores as the distance
    between the query token and the key token increases.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 就像ALiBi一样，这种[技术](https://arxiv.org/abs/2104.09864)具有相对衰减的特性；随着查询标记和键标记之间距离的增加，注意力分数会衰减。
- en: No Positional Encoding (NoPE)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 无位置编码（NoPE）
- en: A contrarian [technique](https://oreil.ly/QM9dW) argues that positional embeddings
    in fact are not required and that Transformers implicitly capture positional information.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一种[相反的技术](https://oreil.ly/QM9dW)认为，位置嵌入实际上不是必需的，并且Transformers隐式地捕获位置信息。
- en: Models these days are mostly using ALiBi or RoPE, although this is one aspect
    of the Transformer architecture that is still actively improving.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的模型大多使用ALiBi或RoPE，尽管这是Transformer架构中仍在积极改进的一个方面。
- en: Feedforward Networks
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前馈网络
- en: The output from a self-attention block is fed through a [*feedforward network*](https://oreil.ly/Bdphg).
    Each token representation is independently fed through the network. The feedforward
    network incorporates a nonlinear activation function like [Rectified Linear Unit
    (ReLU)](https://oreil.ly/KUqtP) or [Gaussian Error Linear Units (GELU)](https://oreil.ly/MSDKE),
    thus enabling the model to learn more complex features from the data. For more
    details on these activation functions, refer to this [blog post from v7](https://oreil.ly/NfOb0).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力块的输出通过一个[*前馈网络*](https://oreil.ly/Bdphg)进行处理。每个标记表示独立地通过网络。前馈网络包含非线性激活函数，如[修正线性单元
    (ReLU)](https://oreil.ly/KUqtP)或[高斯误差线性单元 (GELU)](https://oreil.ly/MSDKE)，从而使得模型能够从数据中学习更复杂的特征。有关这些激活函数的更多详细信息，请参阅v7的这篇[博客文章](https://oreil.ly/NfOb0)。
- en: 'The feedforward layers are implemented in code in this way:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈层在代码中的实现方式如下：
- en: '[PRE6]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Layer Normalization
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层归一化
- en: 'Layer normalization is performed to ensure training stability and faster training
    convergence. While the original Transformer architecture performed normalization
    at the beginning of the block, modern implementations do it at the end of the
    block. The normalization is performed as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 层归一化被用来确保训练的稳定性和更快的训练收敛。虽然原始的Transformer架构在块的开始处进行归一化，但现代实现是在块的末尾进行。归一化过程如下：
- en: Given an input of batch size `b`, sequence length `n`, and vector dimension
    `d`, calculate the mean and variance across each vector dimension.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定一个批大小`b`，序列长度`n`和向量维度`d`的输入，计算每个向量维度的均值和方差。
- en: Normalize the input by subtracting the mean and dividing it by the square root
    of the variance. A small epsilon value is added to the denominator for numerical
    stability.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过减去均值并除以方差的平方根来归一化输入。为了数值稳定性，在分母中添加了一个小的epsilon值。
- en: Multiply by a scale parameter and add a shift parameter to the resulting values.
    These parameters are learned during the training process.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将一个缩放参数乘以结果值，并将一个偏移参数加到结果值上。这些参数在训练过程中学习。
- en: 'This is how it is represented in code:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在代码中的表示方式：
- en: '[PRE7]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Loss Functions
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失函数
- en: So far, we have discussed all the components of each Transformer block. For
    the next token-prediction learning objective, the input is propagated through
    the Transformer layers to generate the final output, which is a probability distribution
    across all tokens. During training, the loss is calculated by comparing the output
    distribution with the gold truth. The gold truth distribution assigns a 1 to the
    gold truth token and 0 to all other tokens.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了每个Transformer块的所有组件。对于下一个标记预测学习目标，输入通过Transformer层传播以生成最终输出，这是一个跨越所有标记的概率分布。在训练过程中，通过比较输出分布与黄金真实值来计算损失。黄金真实分布将1分配给黄金真实标记，并将0分配给所有其他标记。
- en: 'There are many possible ways to quantify the difference between the output
    and the gold truth. The most popular one is cross-entropy, which is calculated
    by the formula:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多可能的方法来量化输出和黄金真实值之间的差异。最流行的一种是交叉熵，其计算公式如下：
- en: '[PRE8]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'For example, consider the sequence:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑以下序列：
- en: '[PRE9]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let’s say the gold truth token is *good*, and the output probability distribution
    is (*terrible*: 0.65, *bad*:0.12, *good*:011,…​)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '假设正确的标记是*good*，而输出概率分布是（*terrible*: 0.65, *bad*:0.12, *good*:011,…）'
- en: 'The cross-entropy is calculated as:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵按以下方式计算：
- en: '[PRE10]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Since the gold truth distribution values are 0 for all but the correct token,
    the equation can be simplified to:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 由于正确的标记分布值除了正确的标记外都是0，因此方程可以简化为：
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Once the loss is calculated, the gradient of the loss with respect to the parameters
    of the model is calculated and the weights are updated, using the backpropagation
    algorithm.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦计算了损失，就计算损失相对于模型参数的梯度，并使用反向传播算法更新权重。
- en: Intrinsic Model Evaluation
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内在模型评估
- en: How do we know if the backpropagation algorithm is actually working and that
    the model is getting better over time? We can use either intrinsic model evaluation
    or extrinsic model evaluation.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何知道反向传播算法实际上是否在起作用，以及模型是否随着时间的推移而变得更好？我们可以使用内在模型评估或外在模型评估。
- en: Extrinsic model evaluation involves testing the model’s performance on real-world
    downstream tasks. These tasks directly test the performance of the model but only
    on a narrow range of the model’s capabilities. In contrast, intrinsic model evaluation
    involves a more general evaluation of the model’s ability to model language, but
    with no guarantee that its performance in the intrinsic evaluation metric is directly
    proportional to its performance across all possible downstream tasks.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 外部模型评估涉及在真实世界的下游任务上测试模型的表现。这些任务直接测试模型的表现，但仅限于模型能力的狭窄范围内。相比之下，内部模型评估涉及对模型建模语言能力的更一般性评估，但无法保证其在内部评估指标上的表现与所有可能的下游任务的表现成比例。
- en: The most common intrinsic evaluation metric is *perplexity*. Perplexity measures
    the ability of a language model to accurately predict the next token in a sequence.
    A model that can always correctly predict the next token has a perplexity of 1\.
    The higher the perplexity, the worse the language model. In the worst case, if
    the model is predicting at random, with probability of predicting each token in
    a vocabulary of size V being 1/V, then the perplexity is V.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的内在评估指标是*困惑度*。困惑度衡量语言模型准确预测序列中下一个标记的能力。如果一个模型总能正确预测下一个标记，其困惑度为1。困惑度越高，语言模型的表现越差。在最坏的情况下，如果模型随机预测，每个标记在词汇表中的预测概率为1/V，那么困惑度为V。
- en: 'Perplexity is related to cross-entropy by this formula:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 困惑度与交叉熵通过以下公式相关：
- en: '[PRE12]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Transformer Backbones
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer骨干
- en: 'So far, we described the components of the canonical version of the Transformer.
    In practice, three major types of architecture backbones are used to implement
    the Transformer:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们描述了Transformer规范版本的组件。在实践中，三种主要的架构骨干被用来实现Transformer：
- en: Encoder-only
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅编码器
- en: Encoder-decoder
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器-解码器
- en: Decoder-only
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅解码器
- en: Let’s look at each of these in detail.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看这些内容。
- en: '[Figure 4-5](#enc-dec) depicts encoder-only, encoder-decoder, and decoder-only
    architectures.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-5](#enc-dec)展示了仅编码器、编码器-解码器和仅解码器架构。'
- en: '![enc-dec](assets/dllm_0405.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![enc-dec](assets/dllm_0405.png)'
- en: Figure 4-5\. Visualization of various Transformer backbones
  id: totrans-120
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-5.各种Transformer骨干的可视化
- en: Encoder-Only Architectures
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 仅编码器架构
- en: Encoder-only architectures were all the rage when Transformer-based language
    models first burst on the scene. Iconic language models from yesteryear (circa
    2018) that use encoder-only architectures include BERT, RoBERTa, etc.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当基于Transformer的语言模型首次出现时，仅编码器架构非常流行。使用仅编码器架构的标志性语言模型（大约在2018年）包括BERT、RoBERTa等。
- en: 'There haven’t been many encoder-only LLMs trained since 2021 for a few reasons,
    including:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 自2021年以来，由于几个原因，包括：
- en: They are relatively harder to train.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们相对较难训练。
- en: The masked language modeling objective typically used to train them provides
    a learning signal in only a small percentage of tokens (the masking rate), thus
    needing a lot more data to reach the same level of performance as decoder-only
    models.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常用于训练它们的掩码语言建模目标只在少量标记（掩码率）中提供学习信号，因此需要更多的数据才能达到仅解码器模型相同的表现水平。
- en: For every downstream task, you need to train a separate task-specific head,
    making usage inefficient.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个下游任务，您需要训练一个单独的任务特定头，这使得使用效率低下。
- en: However, the release of ModernBERT seems to have reinvigorated this space.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，ModernBERT的发布似乎重新激发了这一领域。
- en: The creators of the UL2 language model claim that encoder-only models should
    be considered obsolete. I personally wouldn’t go that far; encoder-only models
    are still great choices for classification tasks. Moreover, if you already have
    a satisfactory pipeline for your use case built around encoder-only models, I
    would say if it ain’t broke, why fix it?
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: UL2语言模型的创造者声称，应将仅编码器模型视为过时。我个人不会走那么远；对于分类任务，仅编码器模型仍然是很好的选择。此外，如果您已经围绕仅编码器模型构建了满足您用例的满意管道，我会说如果它没有坏，为什么要修复它？
- en: 'Here are some guidelines for adopting encoder-only models:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一些采用仅编码器模型的指南：
- en: RoBERTa performs better than BERT most of the time, since it is trained a lot
    longer on more data, and it has adopted best practices learned after the release
    of BERT.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RoBERTa在大多数情况下表现优于BERT，因为它在更多数据上训练了更长的时间，并且它采用了BERT发布后学到的最佳实践。
- en: DeBERTa and ModernBERT are currently regarded as the best-performing encoder-only
    models.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeBERTa和ModernBERT目前被认为是表现最佳的仅编码器模型。
- en: The distilled versions of encoder-only models like DistilBERT, etc., are not
    too far off from the original models in terms of performance, and they should
    be considered if you are operating under resource constraints.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如DistilBERT等仅编码器模型的蒸馏版本在性能上与原始模型相差不远，如果你在资源受限的情况下操作，应考虑使用它们。
- en: Several embedding models are built from encoder-only models. For example, one
    of the most important libraries in the field of NLP, the Swiss Army knife of NLP
    tools, *sentence transformers*, provides encoder-only embedding models that are
    very widely used. all-mpnet-base-v2, based on an encoder-only model called MPNet,
    and fine-tuned on several task datasets, is still competitive with much larger
    embedding models.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 一些嵌入模型是从仅编码器模型构建的。例如，在NLP领域最重要的库之一，NLP工具的瑞士军刀——*sentence transformers*，提供了广泛使用的仅编码器嵌入模型。基于称为MPNet的仅编码器模型并针对几个任务数据集进行微调的all-mpnet-base-v2，仍然与更大的嵌入模型具有竞争力。
- en: Encoder-Decoder Architectures
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码器-解码器架构
- en: This is the original architecture of the Transformer, as it was first proposed.
    The T5 series of models uses this architecture type.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Transformer的原始架构，正如它最初被提出的那样。T5系列模型使用这种架构类型。
- en: In encoder-decoder models, the input is text and the output is also text. A
    standardized interface ensures that the same model and training procedure can
    be used for multiple tasks. The inputs are handled by an encoder, and the outputs
    by the decoder.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在编码器-解码器模型中，输入是文本，输出也是文本。一个标准化的接口确保可以使用相同的模型和训练程序用于多个任务。输入由编码器处理，输出由解码器处理。
- en: Decoder-Only Architectures
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 仅解码器架构
- en: A majority of LLMs trained today use decoder-only models. Decoder-only models
    came into fashion starting from the original GPT model from OpenAI. Decoder-only
    models excel at zero-shot and few-shot learning.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 目前训练的大多数LLM使用仅解码器模型。仅解码器模型从OpenAI的原始GPT模型开始流行起来。仅解码器模型在零样本和少样本学习中表现出色。
- en: Decoder models can be causal and noncausal. Noncausal models have bidirectionality
    over the input sequence, while the output is still autoregressive (you cannot
    look ahead).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器模型可以是因果的和非因果的。非因果模型在输入序列上具有双向性，而输出仍然是自回归的（你不能向前看）。
- en: Tip
  id: totrans-140
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'While the field is still evolving, there has been some [compelling evidence](https://oreil.ly/Sb7JS)
    for the following results:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然该领域仍在不断发展，但已有一些[令人信服的证据](https://oreil.ly/Sb7JS)支持以下结果：
- en: Decoder-only models are the best choice for zero-shot and few-shot generalization.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅解码器模型是零样本和少样本泛化的最佳选择。
- en: Encoder-decoder models are the best choice for multi-task fine tuning.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器-解码器模型是多任务微调的最佳选择。
- en: 'The best of both worlds is to combine the two: start with auto-regressive training,
    and then in an adaptation step, pre-train further with a noncausal setup using
    a span corruption objective.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 两者兼得的方法是将它们结合起来：首先进行自回归训练，然后在适应步骤中，使用跨度损坏目标以非因果设置进一步预训练。
- en: In this section, we discussed how architectural backbones can be classified
    according to how they use the architecture’s encoder and decoder. Another architectural
    backbone type that is making inroads in the past year is the Mixture of Experts
    (MoE) paradigm. Let’s explore that in detail.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了如何根据它们如何使用架构的编码器和解码器来对架构骨干进行分类。在过去一年中取得进展的另一种架构骨干类型是专家混合（MoE）范式。让我们详细探讨一下。
- en: Mixture of Experts
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 专家混合
- en: Remarkably, in the seven years since the invention of the Transformer architecture,
    the Transformer implementation used in current language models isn’t too different
    from the original version, despite hundreds of papers proposing modifications
    to it. The original architecture has proven to be surprisingly robust, with most
    proposed variants barely moving the needle in terms of performance. However, some
    components of the Transformer have seen changes, like positional encodings as
    discussed earlier in the chapter.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，自从Transformer架构发明以来的七年里，当前语言模型中使用的Transformer实现与原始版本并没有太大的不同，尽管有数百篇论文提出了对其的修改。原始架构已被证明出人意料地稳健，大多数提出的变体在性能方面几乎没有移动指针。然而，Transformer的一些组件已经发生了变化，如本章前面讨论的位置编码。
- en: MoE models have been seeing a lot of success in the past couple of years. Examples
    include OpenAI’s GPT-4 (unconfirmed), Google’s Switch, DeepSeek’s DeepSeek V3,
    and Mistral’s Mixtral. In this section, we will learn the motivations behind developing
    this architecture and how it works in practice.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 过去几年中，MoE模型取得了很大的成功。例如，OpenAI的GPT-4（未确认）、Google的Switch、DeepSeek的DeepSeek V3和Mistral的Mixtral。在本节中，我们将学习开发这种架构的动机以及它在实际中的工作方式。
- en: As shown in [Chapter 1](ch01.html#chapter_llm-introduction), the scaling laws
    dictate that performance of the language model increases as you increase the size
    of the model and its training data. However, increasing the model capacity implies
    more compute is needed for both training and inference. This is undesirable, especially
    at inference time, when latency requirements can be stringent. Can we increase
    the capacity of a model without increasing the required compute?
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第1章](ch01.html#chapter_llm-introduction)所示，缩放定律规定，随着模型大小和训练数据的增加，语言模型的表现也会提高。然而，增加模型容量意味着在训练和推理过程中都需要更多的计算资源。这在推理时间尤其不受欢迎，因为延迟要求可能非常严格。我们能否在不增加所需计算量的情况下增加模型的容量？
- en: One way to achieve this is using conditional computation; each input (either
    a token or the entire sequence) sees a different subset of the model, interacting
    with only the parameters that are best suited to process it. This is achieved
    by composing the architecture to be made up of several components called experts,
    with only a subset of experts being activated for each input.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这一目标的一种方法是通过条件计算；每个输入（无论是标记还是整个序列）都看到模型的不同子集，只与最适合处理它的参数进行交互。这是通过将架构组合成由多个称为专家的组件组成来实现的，每个输入只激活专家的一个子集。
- en: '[Figure 4-6](#MoE-0) depicts a canonical MoE model.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-6](#MoE-0)展示了典型的MoE模型。'
- en: '![MoE](assets/dllm_0406.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![MoE](assets/dllm_0406.png)'
- en: Figure 4-6\. Mixture of Experts
  id: totrans-153
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-6\. 专家混合
- en: A key component of the MoE architecture is the *gating function*. The gating
    function helps decide which expert is more suited to process a given input. The
    gating function is implemented as a weight applied to each expert.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: MoE架构的关键组成部分是*门控函数*。门控函数有助于决定哪个专家更适合处理给定的输入。门控函数被实现为应用于每个专家的权重。
- en: The experts are typically added to the feedforward component of the Transformer.
    Therefore, if there are eight experts, then there will be eight feedforward networks
    instead of one. Based on the routing strategy used, only a small subset of these
    networks will be activated for a given input.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 专家通常被添加到Transformer的前馈组件中。因此，如果有八个专家，那么将会有八个前馈网络而不是一个。根据所使用的路由策略，只有这些网络中的一小部分会在给定输入时被激活。
- en: 'The routing strategy determines the number and type of experts activated. Two
    types of popular routing strategies exist:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 路由策略决定了激活的专家的数量和类型。存在两种流行的路由策略：
- en: Tokens choose
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记选择
- en: Experts choose
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 专家选择
- en: In the tokens choose strategy, each token chooses k experts. k is typically
    a small number (~2). The disadvantage of using this strategy is the need for load
    balancing. If in a given input batch, most of the tokens end up using the same
    experts, then additional time is needed to finish the computation as we cannot
    benefit from the parallelization afforded by multiple experts.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在标记选择策略中，每个标记选择k个专家。k通常是一个较小的数字（~2）。使用这种策略的缺点是需要负载均衡。如果在给定的输入批次中，大多数标记最终使用相同的专家，那么需要额外的时间来完成计算，因为我们不能从多个专家提供的并行化中受益。
- en: In the experts choose strategy, each expert picks the tokens that it is most
    equipped to handle. This solves the load balancing problem as we can specify that
    each expert choose the same number of tokens. However, this also leads to inefficient
    token-expert matching, as each expert is limited to picking only a finite number
    of tokens in a batch.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在专家选择策略中，每个专家选择它最能处理的标记。这解决了负载均衡问题，因为我们可以指定每个专家选择相同数量的标记。然而，这也导致了不高效的标记-专家匹配，因为每个专家只能选择一批中有限数量的标记。
- en: Learning Objectives
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习目标
- en: Now that we have discussed the architecture of language models, let’s turn our
    focus to understanding the tasks they are trained on during the pre-training process.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了语言模型的架构，让我们将注意力转向理解它们在预训练过程中训练的任务。
- en: As mentioned earlier in the chapter, language models are pre-trained in a self-supervised
    manner. The scale of data we need to train them makes it prohibitively expensive
    to perform supervised learning, where (input, output) examples need to come from
    humans. Instead, we use a form of training called self-supervision, where the
    data itself contains the target labels. The goal of self-supervised learning is
    to learn a task which acts as a proxy for learning the syntax and semantics of
    a language, as well as skills like reasoning, arithmetic and logical manipulation,
    and other cognitive tasks, and (hopefully) eventually leading up to general human
    intelligence. How does this work?
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章前面所述，语言模型是以自监督的方式进行预训练的。我们需要训练它们的数据规模使得进行监督学习变得成本过高，在监督学习中（输入，输出）示例需要来自人类。相反，我们使用一种称为自监督的训练形式，其中数据本身包含目标标签。自监督学习的目标是学习一个作为学习语言语法和语义、推理、算术和逻辑操作以及其他认知任务等技能的代理任务，并（希望）最终达到一般人类智能。这是如何工作的？
- en: 'For example, let’s take the canonical language modeling task: predicting the
    next word that comes in a sequence. Consider the sequence:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们以典型的语言建模任务为例：预测序列中下一个出现的单词。考虑以下序列：
- en: '[PRE13]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'and the language model is asked to predict the next token. The total number
    of possible answers is the size of the vocabulary. There are many valid continuations
    to this sequence, like (hedge, fence, barbecue, sandcastle, etc.), but many continuations
    to this sequence would violate English grammar rules like (is, of, the). During
    the training process, after seeing billions of sequences, the model will know
    that it is highly improbable for the word “the” to be followed by the word “is”
    or “of,” regardless of the surrounding context. Thus, you can see how just predicting
    the next token is such a powerful tool: in order to correctly predict the next
    token you can eventually learn more and more complex functions that you can encode
    in your model connections. However, whether this paradigm is all we need to develop
    general intelligence is an open question.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 并且要求语言模型预测下一个标记。可能的答案总数是词汇表的大小。有许多有效的序列延续，如（hedge，fence，barbecue，sandcastle
    等），但许多延续会违反英语语法规则，如（is，of，the）。在训练过程中，在看到数十亿个序列之后，模型将知道单词“the”后面跟着“is”或“of”的可能性非常高，无论周围的环境如何。因此，你可以看到仅预测下一个标记是多么强大的工具：为了正确预测下一个标记，你最终可以学习到越来越多、越来越复杂的函数，你可以将这些函数编码到你的模型连接中。然而，这种范式是否是我们开发通用智能所需要的全部，这是一个悬而未决的问题。
- en: 'Self-supervised learning objectives used for pre-training LLMs can be broadly
    classified (nonexhaustively) into three types:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 用于预训练 LLM 的自监督学习目标可以大致分为（非穷尽性地）三种类型：
- en: Full language modeling (FLM)
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完整语言建模（FLM）
- en: Masked language modeling (MLM)
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 掩码语言建模（MLM）
- en: Prefix language modeling (PrefixLM)
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前缀语言建模（PrefixLM）
- en: Let’s explore these in detail.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细探讨这些内容。
- en: Full Language Modeling
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完整语言建模
- en: '[Figure 4-7](#full-language-modeling) shows the canonical FLM objective at
    work.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-7](#full-language-modeling) 展示了典型 FLM 目标的工作情况。'
- en: '![Full Language Modeling](assets/dllm_0407.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![完整语言建模](assets/dllm_0407.png)'
- en: Figure 4-7\. Full language modeling
  id: totrans-175
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-7\. 完整语言建模
- en: This is the canonical language modeling objective of learning to predict the
    next token in a sequence and currently the simplest and most common training objective,
    used by GPT-4 and a vast number of open source models. The loss is computed for
    every token the model sees, i.e., every single token in the training set that
    is being asked to be predicted by the language model provides a learning signal
    for the model, making it very efficient.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在序列中预测下一个标记的典型语言建模目标，目前是最简单、最常用的训练目标，被 GPT-4 和大量开源模型所使用。损失是对模型看到的每个标记计算的，即，训练集中被要求由语言模型预测的每个单个标记都为模型提供学习信号，这使得它非常高效。
- en: Let’s explore an example, using the GPT Neo model.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过 GPT Neo 模型来探讨一个例子。
- en: 'Suppose we continue pre-training the GPT Neo model from its publicly available
    checkpoint, using the full language modeling objective. Let’s say the current
    training sequence is:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们继续从 GPT Neo 模型的公开可用的检查点使用完整语言建模目标进行预训练。假设当前的训练序列是：
- en: '[PRE14]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'You can run this code:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以运行以下代码：
- en: '[PRE15]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This code tokenizes the input text `Language models are` and feeds it to the
    model by invoking the `generate()` function. The function predicts the continuation,
    given the sequence “Language models are.” It outputs only one token and stops
    generating because `max_new_tokens` is set to 1\. The rest of the code enables
    it to output the top 20 list of tokens with the highest score, prior to applying
    the softmax at the last layer.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将输入文本`Language models are`进行标记化，并通过调用`generate()`函数将其输入到模型中。该函数根据给定的序列“Language
    models are.”预测后续内容。它只输出一个标记并停止生成，因为`max_new_tokens`设置为1。其余的代码使其能够输出在应用最后一层的softmax之前得分最高的前20个标记列表。
- en: 'The top 20 tokens with the highest prediction score are:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 预测得分最高的前20个标记是：
- en: '[PRE16]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Every word in the top 20 seems to be a valid continuation of the sequence. The
    ground truth is the token `ubiquitous`, which we can use to calculate the loss
    and initiate the backpropagation process for learning.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 前20个标记中的每个词似乎都是序列的有效后续。真实情况是标记`ubiquitous`，我们可以用它来计算损失并启动学习过程中的反向传播过程。
- en: 'As another example, consider the text sequence:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 作为另一个例子，考虑以下文本序列：
- en: '[PRE17]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Run the same code as previously, except for this change:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 运行与之前相同的代码，但有所变化：
- en: '[PRE18]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The top 20 output tokens are:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 得分最高的前20个输出标记是：
- en: '[PRE19]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The correct answer has the 17th highest score. A lot of numbers appear in the
    top 10, showing that the model is more or less randomly guessing the answer, which
    is not surprising for a smaller model like GPT Neo.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 正确答案的得分是第17高。前10名中出现了很多数字，这表明模型或多或少是在随机猜测答案，这对于像GPT Neo这样的较小模型来说并不奇怪。
- en: 'The OpenAI API provides the `logprobs` parameter that allows you to specify
    the number of tokens along with their log probabilities that need to be returned.
    As of the book’s writing, only the `logprobs` of the 20 most probable tokens are
    available. The tokens returned are in order of their log probabilities:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI API提供了`logprobs`参数，允许您指定需要返回的标记数量及其对数概率。截至本书编写时，只有20个最可能标记的`logprobs`可用。返回的标记按其对数概率的顺序排列：
- en: '[PRE20]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This code calls the older gpt-4o model, asking it to generate a maximum of
    one token. The output is:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码调用旧的gpt-4o模型，要求它生成最多一个标记。输出如下：
- en: '[PRE21]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: gpt-4o is pretty confident that the answer is 13, and rightfully so. The rest
    of the top probability tokens are all related to output formatting.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: gpt-4o对答案是13非常自信，这是有道理的。其余的前概率标记都与输出格式化相关。
- en: Tip
  id: totrans-198
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: During inference, we don’t necessarily need to generate the token with the highest
    score. Several *decoding strategies* allow you to generate more diverse text.
    We will discuss these strategies in [Chapter 5](ch05.html#chapter_utilizing_llms).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中，我们不一定需要生成得分最高的标记。几种*解码策略*允许您生成更多样化的文本。我们将在[第5章](ch05.html#chapter_utilizing_llms)中讨论这些策略。
- en: Prefix Language Modeling
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前缀语言模型
- en: Prefix LM is similar to the FLM setting. The difference is that FLM is fully
    causal, i.e., in a left-to-right writing system like English, tokens do not attend
    to tokens to the right (future). In the prefix LM setting, a part of the text
    sequence, called the prefix, is allowed to attend to future tokens in the prefix.
    The prefix part is thus noncausal. For training prefix LMs, a random prefix length
    is sampled, and the loss is calculated over only the tokens in the suffix.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 前缀LM与FLM设置类似。区别在于FLM是完全因果的，即在一个从左到右书写的系统（如英语）中，标记不会关注右边的标记（未来）。在前缀LM设置中，文本序列的一部分，称为前缀，被允许关注前缀中的未来标记。因此，前缀部分是非因果的。为了训练前缀LM，会采样一个随机的前缀长度，并且损失只计算后缀中的标记。
- en: Masked Language Modeling
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 掩码语言模型
- en: '[Figure 4-8](#masked-language-modeling-bert) shows the canonical MLM objective
    at work.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-8](#masked-language-modeling-bert)显示了标准MLM目标在工作中的情况。'
- en: '![Masked Language Modeling in BERT](assets/dllm_0408.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![BERT中的掩码语言模型](assets/dllm_0408.png)'
- en: Figure 4-8\. Masked Language Modeling in BERT
  id: totrans-205
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-8\. BERT中的掩码语言模型
- en: In the MLM setting, rather than predict the next token in a sequence, we ask
    the model to predict masked tokens within the sequence. In the most basic form
    of MLM implemented in the BERT model, 15% of tokens are randomly chosen to be
    masked and are replaced with a special mask token, and the language model is asked
    to predict the original tokens.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在MLM设置中，我们不是预测序列中的下一个标记，而是要求模型预测序列中的掩码标记。在BERT模型中实现的MLM最基本的形式中，15%的标记被随机选择进行掩码，并用特殊的掩码标记替换，然后语言模型被要求预测原始标记。
- en: The T5 model creators used a modification of the original MLM objective. In
    this variant, 15% of tokens are randomly chosen to be removed from a sequence.
    Consecutive dropped-out tokens are replaced by a single unique special token called
    the *sentinel token*. The model is then asked to predict and generate the dropped
    tokens, delineated by the sentinel tokens.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: T5模型的创建者使用了对原始MLM目标的修改。在这个变体中，15%的标记被随机选择从序列中删除。连续删除的标记被替换为单个独特的特殊标记，称为*哨兵标记*。然后模型被要求预测并生成由哨兵标记界定的删除的标记。
- en: 'As an example, consider this sequence:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑这个序列：
- en: Tempura has always been a source of conflict in the family due to unexplained
    reasons
  id: totrans-209
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 由于不明原因，Tempura一直是家庭中的冲突来源
- en: 'Let’s say we drop the tokens “has,” “always,” “of,” and “conflict.” The sequence
    is now:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们删除了“has”、“always”、“of”和“conflict”这些标记。现在序列如下：
- en: Tempura <S1> been a source <S2> in the family due to unexplained reasons
  id: totrans-211
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Tempura <S1> 一直是由于不明原因 <S2> 家庭的来源
- en: 'with S1, S2 being the sentinel tokens. The model is expected to output:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 其中S1，S2是哨兵标记。模型预期输出：
- en: <S1> has always <S2> of conflict <E>
  id: totrans-213
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: <S1> has always <S2> of conflict <E>
- en: The output sequence is terminated by a special token indicating the end of the
    sequence.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 输出序列以一个表示序列结束的特殊标记结束。
- en: Generating only the dropped tokens and not the entire sequence is computationally
    more efficient and saves training time. Note that unlike in Full Language Modeling,
    the loss is calculated over only a small proportion of tokens (the masked tokens)
    in the input sequence.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 仅生成删除的标记而不是整个序列在计算上更高效，并且可以节省训练时间。注意，与全语言模型不同，损失是在输入序列中只有一小部分标记（掩码标记）上计算的。
- en: 'Let’s explore this on Hugging Face:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在Hugging Face上探索这个问题：
- en: '[PRE22]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The targets can be prepared using a simple templating function.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 目标可以使用简单的模板函数来准备。
- en: 'More generally, MLM can be interpreted as a *denoising autoencoder*. You corrupt
    your input by adding noise (masking, dropping tokens), and then you train a model
    to regenerate the original input. BART takes this to the next level by using five
    different types of span corruptions:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地说，MLM可以被解释为一种*去噪自动编码器*。你通过添加噪声（掩码、删除标记）来损坏你的输入，然后训练一个模型来重新生成原始输入。BART通过使用五种不同类型的跨度损坏将这一过程提升到下一个层次。
- en: Random token masking
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 随机标记掩码
- en: '[Figure 4-9](#bart-enoiser-objectives1) depicts the corruption and denoising
    steps.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-9](#bart-enoiser-objectives1)展示了损坏和去噪步骤。'
- en: '![BART Denoiser Objectives1](assets/dllm_0409.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![BART Denoiser Objectives1](assets/dllm_0409.png)'
- en: Figure 4-9\. Random token masking in BART
  id: totrans-223
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-9\. BART中的随机标记掩码
- en: Random token deletion
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 随机标记删除
- en: The model needs to predict the positions in the text where tokens have been
    deleted. [Figure 4-10](#bart-enoiser-objectives2) depicts the corruption and denoising
    steps.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 模型需要预测文本中标记被删除的位置。[图4-10](#bart-enoiser-objectives2)展示了损坏和去噪步骤。
- en: '![BART Denoiser Objectives2](assets/dllm_0410.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![BART Denoiser Objectives2](assets/dllm_0410.png)'
- en: Figure 4-10\. Random token deletion in BART
  id: totrans-227
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-10\. BART中的随机标记删除
- en: Span masking
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 跨度掩码
- en: Text spans are sampled from text, with span lengths coming from a Poisson distribution.
    This means zero-length spans are possible. The spans are deleted from the text
    and replaced with a single mask token. Therefore, the model now has to also predict
    the number of tokens deleted. [Figure 4-11](#bart-enoiser-objectives3) depicts
    the corruption and denoising steps.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 文本跨度从文本中采样，跨度长度来自泊松分布。这意味着可能存在零长度跨度。这些跨度从文本中删除，并用一个单独的掩码标记替换。因此，模型现在还必须预测删除的标记数量。[图4-11](#bart-enoiser-objectives3)展示了损坏和去噪步骤。
- en: '![BART Denoiser Objectives3](assets/dllm_0411.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![BART Denoiser Objectives3](assets/dllm_0411.png)'
- en: Figure 4-11\. Span masking in BART
  id: totrans-231
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-11\. BART中的跨度掩码
- en: Document shuffling
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 文档打乱
- en: Sentences in the input document are shuffled. The model is taught to arrange
    them in the right order. [Figure 4-12](#bart-enoiser-objectives4) depicts the
    corruption and denoising steps.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 输入文档中的句子被打乱。模型被训练以按正确的顺序排列它们。[图4-12](#bart-enoiser-objectives4)展示了损坏和去噪步骤。
- en: '![BART Denoiser Objectives4](assets/dllm_0412.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![BART Denoiser Objectives4](assets/dllm_0412.png)'
- en: Figure 4-12\. Document shuffling objective in BART
  id: totrans-235
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-12\. BART中的文档打乱目标
- en: Document rotation
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 文档旋转
- en: The document is rotated so that it starts from an arbitrary token. The model
    is trained to detect the correct start of the document. [Figure 4-13](#bart-enoiser-objectives5)
    depicts the corruption and denoising steps.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 文档被旋转，以便它从一个任意的标记开始。模型被训练以检测文档的正确开始。[图4-13](#bart-enoiser-objectives5)展示了损坏和去噪步骤。
- en: '![BART Denoiser Objectives5](assets/dllm_0413.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![BART去噪器目标5](assets/dllm_0413.png)'
- en: Figure 4-13\. Document rotation objective in BART
  id: totrans-239
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-13\. BART的文档旋转目标
- en: Which Learning Objectives Are Better?
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 哪种学习目标更好？
- en: It has been shown that models trained with FLM are better at generation, and
    models trained with MLM are better at classification tasks. However, it is inefficient
    to use different language models for different use cases. The consolidation effect
    continues to take hold, with the introduction of [UL2](https://oreil.ly/xJc3U),
    a paradigm that combines the best of different learning objective types in a single
    model.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 已经证明，使用FLM训练的模型在生成方面表现更好，而使用MLM训练的模型在分类任务上表现更好。然而，为不同的用例使用不同的语言模型是不高效的。随着[UL2](https://oreil.ly/xJc3U)的引入，这种结合了不同学习目标类型最佳特性的单一模型范式，整合效应仍在持续。
- en: UL2 mimics the effect of PLMs, MLMs, and PrefixLMs in a single paradigm called
    *Mixture of Denoisers*.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: UL2通过一个称为“混合去噪器”的单一范式模拟了PLMs、MLMs和PrefixLMs的效果。
- en: 'The denoisers used are as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的去噪器如下：
- en: R-Denoiser
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: R-Denoiser
- en: This is similar to the T5 span corruption task. Spans between length 2–5 tokens
    are replaced by a single mask token. [Figure 4-14](#ul2-mixture-denoisers1) depicts
    the workings of the R-denoiser.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 这与T5的span corruption任务类似。长度为2-5个token的span被替换为一个单一的掩码token。[图4-14](#ul2-mixture-denoisers1)展示了R-denoiser的工作原理。
- en: '![UL2''s Mixture of Denoisers1](assets/dllm_0414.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![UL2的混合去噪器1](assets/dllm_0414.png)'
- en: Figure 4-14\. UL2’s R-Denoiser
  id: totrans-247
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-14\. UL2的R-Denoiser
- en: S-Denoiser
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: S-Denoiser
- en: Similar to prefix LM, the text is divided into a prefix and a suffix. The suffix
    is masked, while the prefix has access to bidirectional context. [Figure 4-15](#ul2-mixture-denoisers2)
    depicts the workings of the S-Denoiser.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 与前缀LM类似，文本被分为前缀和后缀。后缀被掩码，而前缀可以访问双向上下文。[图4-15](#ul2-mixture-denoisers2)展示了S-Denoiser的工作原理。
- en: '![UL2''s Mixture of Denoisers2](assets/dllm_0415.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![UL2的混合去噪器2](assets/dllm_0415.png)'
- en: Figure 4-15\. UL2’s S-Denoiser
  id: totrans-251
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-15\. UL2的S-Denoiser
- en: X-Denoiser
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: X-Denoiser
- en: This stands for extreme denoising, where a large proportion of text is masked
    (often over 50%). [Figure 4-16](#ul2-mixture-denoisers3) depicts the workings
    of the X-Denoiser.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这代表极端去噪，其中大量文本被掩码（通常超过50%）。[图4-16](#ul2-mixture-denoisers3)展示了X-Denoiser的工作原理。
- en: '![UL2''s Mixture of Denoisers3](assets/dllm_0416.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![UL2的混合去噪器3](assets/dllm_0416.png)'
- en: Figure 4-16\. UL2’s X-Denoiser
  id: totrans-255
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-16\. UL2的X-Denoiser
- en: Pre-Training Models
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预训练模型
- en: Now that we have learned about the ingredients that go into a language model
    in detail, let’s learn how to pre-train one from scratch.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经详细了解了构成语言模型的成分，让我们学习如何从头开始预训练一个语言模型。
- en: 'The language models of today are learning to model two types of concepts with
    one model:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 当今的语言模型正在学习使用一个模型来模拟两种类型的概念：
- en: Language, the vehicle used to communicate facts, opinions, and feelings.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言，用于传达事实、观点和情感的交流工具。
- en: The underlying phenomena that led to the construction of text in the language.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导致在语言中构建文本的底层现象。
- en: For many application areas, we are far more interested in learning to model
    the latter than the former. While a language model that is fluent in the language
    is welcome, we would prefer to see it get better at domains like science or law
    and skills like reasoning and arithmetic.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多应用领域，我们更感兴趣的是学习如何模拟后者而不是前者。虽然一个能够流利使用语言的模型是受欢迎的，但我们更希望看到它在科学或法律等领域的技能以及推理和算术等技能上有所提高。
- en: These concepts and skills are expressed in languages like English, which primarily
    serve a social function. Human languages are inherently ambiguous, contain lots
    of redundancies, and in general are inefficient vehicles to transmit underlying
    concepts.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这些概念和技能用英语等语言表达，这些语言主要服务于社会功能。人类语言本质上具有歧义性，包含大量冗余，并且通常不是传递底层概念的高效载体。
- en: 'This brings us to the question: are human languages even the best vehicle for
    language models to learn underlying skills and concepts? Can we separate the process
    of modeling the language from modeling the underlying concepts expressed through
    language?'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了一个问题：人类语言是否真的是语言模型学习底层技能和概念的最佳载体？我们能否将语言建模的过程与通过语言表达的概念建模过程分开？
- en: Let’s put this theory to the test using an example. Consider training an LLM
    from scratch to learn to play the game of chess.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来测试这个理论。考虑从头开始训练一个LLM来学习如何下棋。
- en: 'Recall the ingredients of a language model from [Chapter 2](ch02.html#ch02).
    We need:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下[第2章](ch02.html#ch02)中语言模型的成分。我们需要：
- en: A pre-training dataset
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个预训练数据集
- en: A vocabulary and tokenization scheme
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词汇表和标记化方案
- en: A model architecture
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型架构
- en: A learning objective
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习目标
- en: For training the chess language model, we can choose the Transformer architecture
    with the next-token prediction learning objective, which is the de facto paradigm
    used today.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练国际象棋语言模型，我们可以选择具有下一个标记预测学习目标的Transformer架构，这是今天实际使用的范式。
- en: For the pre-training dataset, we can use the chess games dataset from [Lichess](https://oreil.ly/XmWvv),
    containing billions of games. We select a subset of 20 million chess games for
    our training.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 对于预训练数据集，我们可以使用来自[Lichess](https://oreil.ly/XmWvv)的棋局数据集，包含数十亿场比赛。我们为训练选择了2000万场棋局的子集。
- en: This dataset is in the Portable Game Notation (PGN) format, which is used to
    represent the sequence of chess moves in a concise notation.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集采用便携式棋谱（PGN）格式，该格式用于以简洁的符号表示棋步序列。
- en: Finally, we have to choose the vocabulary of the model. Since the only purpose
    of this model is to learn chess, we don’t need to support an extensive English
    vocabulary. In fact, we can take advantage of the PGN notation to assign tokens
    to specific chess concepts.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们必须选择模型的词汇表。由于此模型唯一的目的就是学习国际象棋，我们不需要支持广泛的英语词汇。实际上，我们可以利用PGN符号将标记分配给特定的国际象棋概念。
- en: 'Here is an example of a chess game in PGN format, taken from [pgnmentor.com](https://oreil.ly/H3yOs):'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个PGN格式的国际象棋游戏示例，摘自[pgnmentor.com](https://oreil.ly/H3yOs)：
- en: '[PRE23]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The rows of the board are assigned letters a–h and the columns are assigned
    numbers 1–8\. Except for pawns, each piece type is assigned a capital letter,
    with N for knight, R for rook, B for bishop, Q for queen, and K for king. A +
    appended to a move indicates a check, a % appended to the move indicates a checkmate,
    and 0-0 is used to indicate castling. If you are unfamiliar with the rules of
    chess, refer to [this piece for a primer](https://oreil.ly/EbcfQ).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 车盘的行分配字母a-h，列分配数字1-8。除了兵外，每种棋子类型都分配一个大写字母，N代表骑士，R代表车，B代表象，Q代表后，K代表王。移动后附加+表示将军，移动后附加%表示将死，0-0用于表示王车易位。如果您不熟悉国际象棋的规则，请参阅[本指南](https://oreil.ly/EbcfQ)。
- en: 'Based on this notation, the vocabulary can consist of:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此符号，词汇表可以包括：
- en: A separate token for each square on the board, with 64 total (a1, a2, a3…​h6,
    h7, h8)
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为棋盘上的每个方格分配一个单独的标记，总共有64个（a1, a2, a3…h6, h7, h8）
- en: A separate token for each piece type (N, B, R, K, Q)
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每种棋子类型分配一个单独的标记（N, B, R, K, Q）
- en: Tokens for move numbers (1., 2., 3., etc.)
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移动数字的标记（1., 2., 3., 等。）
- en: Tokens for special moves (+ for check, x for capture, etc.)
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特殊移动的标记（+表示将军，x表示捕获等。）
- en: Now, let’s train a language model from scratch on this chess dataset using our
    special domain-specific vocabulary. The model is directly learning from the PGN
    notation with no human language text present in the dataset. The book’s [GitHub
    repo](https://oreil.ly/llm-playbooks) contains the code and setup for training
    this model.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用我们特殊的领域特定词汇在这个国际象棋数据集上从头开始训练一个语言模型。该模型直接从PGN符号中学习，数据集中没有人类语言文本。本书的[GitHub仓库](https://oreil.ly/llm-playbooks)包含训练此模型的代码和设置。
- en: After training the model for three epochs, let’s test the model’s ability to
    play chess. We can see that the model seems to have learned the rules of the game
    without having to be provided the rules explicitly in natural language. In fact,
    the model can even beat human players some of the time and can execute moves like
    castling.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型三个epoch后，让我们测试模型下棋的能力。我们可以看到，模型似乎已经学会了游戏的规则，而无需在自然语言中明确提供规则。事实上，该模型有时甚至能打败人类玩家，并能执行诸如王车易位之类的移动。
- en: Note that this model was able to learn the concepts (chess) using a domain-specific
    language (PGN). How will we fare if the concepts were taught in natural language?
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，此模型能够使用领域特定语言（PGN）学习概念（国际象棋）。如果概念是用自然语言教授的，我们又会如何呢？
- en: 'Let’s explore this in another experiment. Take the same dataset used to pre-train
    the chess language model and run it through an LLM to convert each move in PGN
    to a sentence in English. An example game would look like:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在另一个实验中探讨这个问题。使用与预训练国际象棋语言模型相同的相同数据集，将其通过一个LLM运行，将PGN中的每个移动转换为英文句子。一个示例游戏可能如下所示：
- en: '*White moves pawn to e4*'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '*白方将兵移动到e4*'
- en: '*Black moves bishop to g7*'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '*黑方将象移动到g7*'
- en: and so on. Train a new language model on the same number of games as the previous
    one, but this time with the English-language dataset. Let the vocabulary of this
    model be the standard English vocabulary generated by training the tokenizer over
    the training set.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 等等。在相同数量的游戏上训练一个新的语言模型，但这次使用的是英语数据集。让这个模型的词汇表是通过对训练集进行训练生成的标准英语词汇表。
- en: How does this compare to the chess LM trained on the PGN dataset? The model
    trained on English descriptions of chess moves performs worse and doesn’t seem
    to have understood the rules of the game yet, despite being trained on the same
    number of games as the other model.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 与在 PGN 数据集上训练的棋类语言模型相比如何？在棋步描述上训练的模型表现更差，并且似乎还没有理解游戏的规则，尽管它与其他模型一样在相同数量的游戏上进行了训练。
- en: This shows that natural language is not necessarily the most efficient vehicle
    for a model to learn skills and concepts, and domain-specific languages and notations
    perform better.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明，自然语言不一定是最有效的模型学习技能和概念的工具，而特定领域的语言和符号表现更好。
- en: Thus, language design is an important skill to acquire, enabling you to create
    domain-specific languages for learning concepts and skills. For your application
    areas, you could use existing domain-specific languages or create a new one yourself.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，语言设计是一项重要的技能，它使你能够为学习概念和技能创建特定领域的语言。对于你的应用领域，你可以使用现有的特定领域语言或自己创建一个新的。
- en: Summary
  id: totrans-292
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed the various components of the Transformer architecture
    in detail, including self-attention, feedforward networks, position encodings,
    and layer normalization. We also discussed several variants and configurations
    such as encoder-only, encoder-decoder, decoder-only, and MoE models. Finally,
    we learned how to put our knowledge of language models together to train our own
    model from scratch and how to design domain-specific languages for more efficient
    learning.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们详细讨论了 Transformer 架构的各个组成部分，包括自注意力、前馈网络、位置编码和层归一化。我们还讨论了几个变体和配置，如仅编码器、编码器-解码器、仅解码器和
    MoE 模型。最后，我们学习了如何将我们对语言模型的知识结合起来，从头开始训练我们自己的模型，以及如何设计特定领域的语言以实现更有效的学习。
