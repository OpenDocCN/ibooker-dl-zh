["```py\n# Only run these commands on Colab or Kaggle!\n%pip install -q -U gym\n%pip install -q -U gym[classic_control,box2d,atari,accept-rom-license]\n```", "```py\nimport gym\n\nenv = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n```", "```py\n>>> obs, info = env.reset(seed=42)\n>>> obs\narray([ 0.0273956 , -0.00611216,  0.03585979,  0.0197368 ], dtype=float32)\n>>> info\n{}\n```", "```py\n>>> img = env.render()\n>>> img.shape  # height, width, channels (3 = Red, Green, Blue)\n(400, 600, 3)\n```", "```py\n>>> env.action_space\nDiscrete(2)\n```", "```py\n>>> action = 1  # accelerate right\n>>> obs, reward, done, truncated, info = env.step(action)\n>>> obs\narray([ 0.02727336,  0.18847767,  0.03625453, -0.26141977], dtype=float32)\n>>> reward\n1.0\n>>> done\nFalse\n>>> truncated\nFalse\n>>> info\n{}\n```", "```py\ndef basic_policy(obs):\n    angle = obs[2]\n    return 0 if angle < 0 else 1\n\ntotals = []\nfor episode in range(500):\n    episode_rewards = 0\n    obs, info = env.reset(seed=episode)\n    for step in range(200):\n        action = basic_policy(obs)\n        obs, reward, done, truncated, info = env.step(action)\n        episode_rewards += reward\n        if done or truncated:\n            break\n\n    totals.append(episode_rewards)\n```", "```py\n>>> import numpy as np\n>>> np.mean(totals), np.std(totals), min(totals), max(totals)\n(41.698, 8.389445512070509, 24.0, 63.0)\n```", "```py\nimport tensorflow as tf\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(5, activation=\"relu\"),\n    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n])\n```", "```py\ndef play_one_step(env, obs, model, loss_fn):\n    with tf.GradientTape() as tape:\n        left_proba = model(obs[np.newaxis])\n        action = (tf.random.uniform([1, 1]) > left_proba)\n        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n\n    grads = tape.gradient(loss, model.trainable_variables)\n    obs, reward, done, truncated, info = env.step(int(action))\n    return obs, reward, done, truncated, grads\n```", "```py\ndef play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n    all_rewards = []\n    all_grads = []\n    for episode in range(n_episodes):\n        current_rewards = []\n        current_grads = []\n        obs, info = env.reset()\n        for step in range(n_max_steps):\n            obs, reward, done, truncated, grads = play_one_step(\n                env, obs, model, loss_fn)\n            current_rewards.append(reward)\n            current_grads.append(grads)\n            if done or truncated:\n                break\n\n        all_rewards.append(current_rewards)\n        all_grads.append(current_grads)\n\n    return all_rewards, all_grads\n```", "```py\ndef discount_rewards(rewards, discount_factor):\n    discounted = np.array(rewards)\n    for step in range(len(rewards) - 2, -1, -1):\n        discounted[step] += discounted[step + 1] * discount_factor\n    return discounted\n\ndef discount_and_normalize_rewards(all_rewards, discount_factor):\n    all_discounted_rewards = [discount_rewards(rewards, discount_factor)\n                              for rewards in all_rewards]\n    flat_rewards = np.concatenate(all_discounted_rewards)\n    reward_mean = flat_rewards.mean()\n    reward_std = flat_rewards.std()\n    return [(discounted_rewards - reward_mean) / reward_std\n            for discounted_rewards in all_discounted_rewards]\n```", "```py\n>>> discount_rewards([10, 0, -50], discount_factor=0.8)\narray([-22, -40, -50])\n>>> discount_and_normalize_rewards([[10, 0, -50], [10, 20]],\n...                                discount_factor=0.8)\n...\n[array([-0.28435071, -0.86597718, -1.18910299]),\n array([1.26665318, 1.0727777 ])]\n```", "```py\nn_iterations = 150\nn_episodes_per_update = 10\nn_max_steps = 200\ndiscount_factor = 0.95\n```", "```py\noptimizer = tf.keras.optimizers.Nadam(learning_rate=0.01)\nloss_fn = tf.keras.losses.binary_crossentropy\n```", "```py\nfor iteration in range(n_iterations):\n    all_rewards, all_grads = play_multiple_episodes(\n        env, n_episodes_per_update, n_max_steps, model, loss_fn)\n    all_final_rewards = discount_and_normalize_rewards(all_rewards,\n                                                       discount_factor)\n    all_mean_grads = []\n    for var_index in range(len(model.trainable_variables)):\n        mean_grads = tf.reduce_mean(\n            [final_reward * all_grads[episode_index][step][var_index]\n             for episode_index, final_rewards in enumerate(all_final_rewards)\n                 for step, final_reward in enumerate(final_rewards)], axis=0)\n        all_mean_grads.append(mean_grads)\n\n    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n```", "```py\ntransition_probabilities = [  # shape=[s, a, s']\n    [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n    [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n    [None, [0.8, 0.1, 0.1], None]\n]\nrewards = [  # shape=[s, a, s']\n    [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n    [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n    [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]\n]\npossible_actions = [[0, 1, 2], [0, 2], [1]]\n```", "```py\nQ_values = np.full((3, 3), -np.inf)  # -np.inf for impossible actions\nfor state, actions in enumerate(possible_actions):\n    Q_values[state, actions] = 0.0  # for all possible actions\n```", "```py\ngamma = 0.90  # the discount factor\n\nfor iteration in range(50):\n    Q_prev = Q_values.copy()\n    for s in range(3):\n        for a in possible_actions[s]:\n            Q_values[s, a] = np.sum([\n                    transition_probabilities[s][a][sp]\n                    * (rewards[s][a][sp] + gamma * Q_prev[sp].max())\n                for sp in range(3)])\n```", "```py\n>>> Q_values\narray([[18.91891892, 17.02702702, 13.62162162],\n [ 0\\.        ,        -inf, -4.87971488],\n [       -inf, 50.13365013,        -inf]])\n```", "```py\n>>> Q_values.argmax(axis=1)  # optimal action for each state\narray([0, 0, 1])\n```", "```py\ndef step(state, action):\n    probas = transition_probabilities[state][action]\n    next_state = np.random.choice([0, 1, 2], p=probas)\n    reward = rewards[state][action][next_state]\n    return next_state, reward\n```", "```py\ndef exploration_policy(state):\n    return np.random.choice(possible_actions[state])\n```", "```py\nalpha0 = 0.05  # initial learning rate\ndecay = 0.005  # learning rate decay\ngamma = 0.90  # discount factor\nstate = 0  # initial state\n\nfor iteration in range(10_000):\n    action = exploration_policy(state)\n    next_state, reward = step(state, action)\n    next_value = Q_values[next_state].max()  # greedy policy at the next step\n    alpha = alpha0 / (1 + iteration * decay)\n    Q_values[state, action] *= 1 - alpha\n    Q_values[state, action] += alpha * (reward + gamma * next_value)\n    state = next_state\n```", "```py\ninput_shape = [4]  # == env.observation_space.shape\nn_outputs = 2  # == env.action_space.n\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape),\n    tf.keras.layers.Dense(32, activation=\"elu\"),\n    tf.keras.layers.Dense(n_outputs)\n])\n```", "```py\ndef epsilon_greedy_policy(state, epsilon=0):\n    if np.random.rand() < epsilon:\n        return np.random.randint(n_outputs)  # random action\n    else:\n        Q_values = model.predict(state[np.newaxis], verbose=0)[0]\n        return Q_values.argmax()  # optimal action according to the DQN\n```", "```py\nfrom collections import deque\n\nreplay_buffer = deque(maxlen=2000)\n```", "```py\ndef sample_experiences(batch_size):\n    indices = np.random.randint(len(replay_buffer), size=batch_size)\n    batch = [replay_buffer[index] for index in indices]\n    return [\n        np.array([experience[field_index] for experience in batch])\n        for field_index in range(6)\n    ]  # [states, actions, rewards, next_states, dones, truncateds]\n```", "```py\ndef play_one_step(env, state, epsilon):\n    action = epsilon_greedy_policy(state, epsilon)\n    next_state, reward, done, truncated, info = env.step(action)\n    replay_buffer.append((state, action, reward, next_state, done, truncated))\n    return next_state, reward, done, truncated, info\n```", "```py\nbatch_size = 32\ndiscount_factor = 0.95\noptimizer = tf.keras.optimizers.Nadam(learning_rate=1e-2)\nloss_fn = tf.keras.losses.mean_squared_error\n\ndef training_step(batch_size):\n    experiences = sample_experiences(batch_size)\n    states, actions, rewards, next_states, dones, truncateds = experiences\n    next_Q_values = model.predict(next_states, verbose=0)\n    max_next_Q_values = next_Q_values.max(axis=1)\n    runs = 1.0 - (dones | truncateds)  # episode is not done or truncated\n    target_Q_values = rewards + runs * discount_factor * max_next_Q_values\n    target_Q_values = target_Q_values.reshape(-1, 1)\n    mask = tf.one_hot(actions, n_outputs)\n    with tf.GradientTape() as tape:\n        all_Q_values = model(states)\n        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n\n    grads = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n```", "```py\nfor episode in range(600):\n    obs, info = env.reset()\n    for step in range(200):\n        epsilon = max(1 - episode / 500, 0.01)\n        obs, reward, done, truncated, info = play_one_step(env, obs, epsilon)\n        if done or truncated:\n            break\n\n    if episode > 50:\n        training_step(batch_size)\n```", "```py\ntarget = tf.keras.models.clone_model(model)  # clone the model's architecture\ntarget.set_weights(model.get_weights())  # copy the weights\n```", "```py\nnext_Q_values = target.predict(next_states, verbose=0)\n```", "```py\nif episode % 50 == 0:\n    target.set_weights(model.get_weights())\n```", "```py\ndef training_step(batch_size):\n    experiences = sample_experiences(batch_size)\n    states, actions, rewards, next_states, dones, truncateds = experiences\n    next_Q_values = model.predict(next_states, verbose=0)  # \u2260 target.predict()\n    best_next_actions = next_Q_values.argmax(axis=1)\n    next_mask = tf.one_hot(best_next_actions, n_outputs).numpy()\n    max_next_Q_values = (target.predict(next_states, verbose=0) * next_mask\n                        ).sum(axis=1)\n    [...]  # the rest is the same as earlier\n```", "```py\ninput_states = tf.keras.layers.Input(shape=[4])\nhidden1 = tf.keras.layers.Dense(32, activation=\"elu\")(input_states)\nhidden2 = tf.keras.layers.Dense(32, activation=\"elu\")(hidden1)\nstate_values = tf.keras.layers.Dense(1)(hidden2)\nraw_advantages = tf.keras.layers.Dense(n_outputs)(hidden2)\nadvantages = raw_advantages - tf.reduce_max(raw_advantages, axis=1,\n                                            keepdims=True)\nQ_values = state_values + advantages\nmodel = tf.keras.Model(inputs=[input_states], outputs=[Q_values])\n```"]