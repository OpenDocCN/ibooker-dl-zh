<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 9. Advanced Techniques for Image Generation with Stable Diffusion"><div class="chapter" id="advanced_image_09">
<h1><span class="label">Chapter 9. </span>Advanced Techniques for Image Generation with Stable Diffusion</h1>


<p>Most work with AI images only requires simple prompt engineering techniques, but there are more powerful tools available when you need more creative control over your output, or want to train custom models for specific tasks. These more complex abilities often requires more technical ability and structured thinking as part of the workflow of creating the final image.</p>

<p>All images in this chapter are generated by Stable Diffusion XL unless otherwise noted, as in the sections relying on extensions such as ControlNet, where more methods are supported with the older v1.5 model. The techniques discussed were devised to be transferrable to any future or alternative model. We make extensive use of AUTOMATIC1111’s Stable Diffusion WebUI and have provided detailed setup instructions that were current as of the time of writing, but please consult the <a href="https://oreil.ly/hs_fS">official repository</a> for up-to-date instructions, and to diagnose any issues you encounter.</p>






<section data-type="sect1" data-pdf-bookmark="Running Stable Diffusion"><div class="sect1" id="id126">
<h1>Running Stable Diffusion</h1>

<p>Stable Diffusion is an open source <a data-type="indexterm" data-primary="Stable Diffusion" data-secondary="running locally" id="stbdfrc"/>image generation model,  so you can run it locally on your computer for free, if you have an NVIDIA or AMD GPU, or Apple Silicon, as powers the M1, M2, or M3 Macs. It was common to <a data-type="indexterm" data-primary="Stable Diffusion" data-secondary="Google Colab" id="id1247"/><a data-type="indexterm" data-primary="Google Colab" id="id1248"/>run the first popular version (1.4) of <a href="https://oreil.ly/OmBuR">Stable Diffusion in a Google Colab notebook</a>, which provides access to a free GPU in the cloud (though you may need to upgrade to a paid account if Google limits the free tier).</p>

<p>Visit the <a href="https://oreil.ly/2WGxQ">Google Colab website</a> if you haven’t used it before or to find the latest information on limits. A copy of this Python notebook is saved in the <a href="https://oreil.ly/uauNn">GitHub repository</a> for this book, but you should upload it to Google Drive and run it in Google Colab to avoid setup issues.</p>

<p>Installing Stable Diffusion can be <a data-type="indexterm" data-primary="Hugging Face API" data-secondary="Stable Diffusion installation" id="id1249"/>done via the Hugging Face diffusers libary, alongside a handful of dependencies. In the Google Colab the following code installs the necessary dependencies (you would drop the exclamation marks (!) if installing locally rather than in a Jupyter Notebook or Google Colab):</p>

<pre data-type="programlisting">!pip install diffusers==0.11.1
!pip install transformers scipy ftfy accelerate</pre>

<p>To download and use the model, you first build an inference pipeline (what runs when we use the model):</p>

<pre id="stable_diffusion_pipe_colab" data-type="programlisting" data-code-language="python"><code class="c1"># create an inference pipeline</code>
<code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">from</code> <code class="nn">diffusers</code> <code class="kn">import</code> <code class="n">StableDiffusionPipeline</code>

<code class="n">pipe</code> <code class="o">=</code> <code class="n">StableDiffusionPipeline</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code>
    <code class="s2">"CompVis/stable-diffusion-v1-4"</code><code class="p">,</code>
    <code class="n">torch_dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">float16</code><code class="p">)</code>

<code class="n">pipe</code> <code class="o">=</code> <code class="n">pipe</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="s2">"cuda"</code><code class="p">)</code></pre>

<p>Let’s break down the script line by line:</p>
<dl>
<dt><code>import torch</code></dt>
<dd>
<p>This line is importing the torch library, also known as <a href="https://pytorch.org">PyTorch</a>. PyTorch is an open source machine learning library, used for applications such as computer vision and natural language processing.</p>
</dd>
<dt><code>from diffusers import StableDiffusionPipeline</code></dt>
<dd>
<p>Here the script is importing the <code>StableDiffusionPipeline</code> class from the <code>diffusers</code> library. This specific class is probably a pipeline for using diffusion models, of which Stable Diffusion is the most popular example.</p>
</dd>
<dt><code>pipe = StableDiffusionPipeline.from_pretrained("CompVis/stable-diffusion-v1-4", torch_dtype=torch.float16)</code></dt>
<dd>
<p>This is creating an instance of the <code>StableDiffusionPipeline</code> class with pretrained weights. The method <code>from_pretrained</code> loads the weights of a pretrained model—in this case, the model is <code>CompVis/stable-diffusion-v1-4</code>.</p>

<p>The <code>torch_dtype=torch.float16</code> argument specifies that the data type used in the model should be float16, which is a half-precision floating-point format. Using float16 can speed up model computation and reduce memory usage (necessary to stay within the Google Colab free tier limits).</p>
</dd>
<dt><code>pipe = pipe.to("cuda")</code></dt>
<dd>
<p>This line moves the pipe model to the GPU. The string <code>"cuda"</code> refers to CUDA, a parallel computing platform and application programming interface (API) model created by Nvidia. By doing this, all computations performed by the pipe model will be executed on the GPU, which can be significantly faster than running them on a CPU for large-scale models and data.</p>
</dd>
</dl>

<p>Now that we have our pipe, we can pass in a prompt and other parameters for the model, like a random seed (change this to get a different image each time), the number of inference steps (more steps takes time but results in a higher-quality image), and the guidance scale (how closely the image matches the prompt):</p>

<pre id="stable_diffusion_inference_colab" data-type="programlisting" data-code-language="python"><code class="c1"># run inference on a prompt</code>
<code class="n">prompt</code> <code class="o">=</code> <code class="s2">"a photograph of an astronaut riding a horse"</code>

<code class="n">generator</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">Generator</code><code class="p">(</code><code class="s2">"cuda"</code><code class="p">)</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">1024</code><code class="p">)</code>

<code class="n">image</code> <code class="o">=</code> <code class="n">pipe</code><code class="p">(</code><code class="n">prompt</code><code class="p">,</code> <code class="n">num_inference_steps</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code>
    <code class="n">guidance_scale</code><code class="o">=</code><code class="mi">7</code><code class="p">,</code> <code class="n">generator</code><code class="o">=</code><code class="n">generator</code>
    <code class="p">)</code><code class="o">.</code><code class="n">images</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="c1"># image here is in PIL format</code>

<code class="c1"># Now to display an image you can either save it such as:</code>
<code class="n">image</code><code class="o">.</code><code class="n">save</code><code class="p">(</code><code class="sa">f</code><code class="s2">"astronaut_rides_horse.png"</code><code class="p">)</code>

<code class="c1"># If you're in a google colab you can directly display:</code>
<code class="n">image</code></pre>

<p><a data-type="xref" href="#figure-9-1">Figure 9-1</a> shows the output.</p>

<p>Let’s walk through this script to explain what it does:</p>
<dl>
<dt><code>prompt = "a photograph of an astronaut riding a horse"</code></dt>
<dd>
<p>This is the prompt that will be passed into the model to guide the generation of an image.</p>
</dd>
<dt><code>generator = torch.Generator("cuda").manual_seed(1024)</code></dt>
<dd>
<p>In this line, a PyTorch generator is created and assigned to the generator variable. The generator is initialized with <code>"cuda"</code>, which means that it will be using a GPU for computations. The <code>manual_seed(1024)</code> function is used to set the random seed for generating random numbers, ensuring that the results are reproducible. If you run this code with the same model, you should get the exact same image.</p>
</dd>
<dt><code>image = pipe(prompt, num_inference_steps=50, guidance_scale=7, generator=generator).images[0]</code></dt>
<dd>
<p>This line runs the pipe model on the prompt to generate an image. The <code>num_inference_steps</code> argument is set to 50, meaning that the model will perform 50 steps of inference. The <code>guidance_scale</code> argument is set to 7, which adjusts how strongly the prompt guides the generated image (higher values tend to get grainy and less diverse). The generator argument passes in the random number generator created earlier. The result is an array of generated images, and <code>images[0]</code> selects the first image from this array.</p>
</dd>
<dt><code>image.save(f"astronaut_rides_horse.png")</code></dt>
<dd>
<p>This line saves the generated image to a file.</p>
</dd>
<dt><code>image</code></dt>
<dd>
<p>This line of code will display the image if the code is running in an environment like a Jupyter Notebook or Google Colab. This happens because these environments automatically display the result of the last line of code in a code cell if it is not assigned to a variable.</p>
</dd>
</dl>

<figure><div id="figure-9-1" class="figure">
<img src="assets/pega_0901.png" alt="pega 0901" width="512" height="512"/>
<h6><span class="label">Figure 9-1. </span>Photograph of an astronaut riding a horse</h6>
</div></figure>

<p>It’s powerful to be able to run an open source model locally or in the cloud and customize it to meet your needs. However, custom coding your own inference pipelines and building a user interface on top is likely overkill unless you are an extremely advanced user with deep machine learning knowledge or your intention is to build your own AI image <a data-type="indexterm" data-primary="Stable Diffusion" data-secondary="Dream Studio" id="id1250"/><a data-type="indexterm" data-primary="Dream Studio" id="id1251"/>generation product. Stablity AI, the company funding development of Stable Diffusion, has a hosted web interface called Dream Studio (<a data-type="xref" href="#figure-9-2">Figure 9-2</a>), which is similar to the DALL-E playground, also operating on a credit system and offering advanced functionality such as inpainting.</p>

<figure><div id="figure-9-2" class="figure">
<img src="assets/pega_0902.png" alt="pega 0902" width="600" height="326"/>
<h6><span class="label">Figure 9-2. </span>Stability AI Dream-Studio</h6>
</div></figure>

<p>Like DALL-E, Dream-Studio offers access via API, which can be convenient for building AI image applications or running programmatic scripts for generating lots of images, without the encumberance of hosting and running your own Stable Diffusion model. Visit <a href="https://oreil.ly/X3Ilb" class="bare"><em class="hyperlink">https://oreil.ly/X3Ilb</em></a> once you have created an account to get your API key, and top up with credits (at time of writing, 1,000 credits cost $10 and can generate approximately 5,000 images). The following code is included in the <a href="https://oreil.ly/aGLeX">GitHub repository</a> for this book:</p>

<pre data-type="programlisting">import os
import base64
import requests
from IPython.display import Image

engine_id = "stable-diffusion-xl-1024-v1-0"
api_host = os.getenv('API_HOST', 'https://api.stability.ai')
api_key = os.getenv("STABILITY_API_KEY")

image_description = "computers being tied together"
prompt = f"""an illustration of {image_description}. in the
style of corporate memphis, white background, professional,
clean lines, warm pastel colors"""

response = requests.post(
    f"{api_host}/v1/generation/{engine_id}/text-to-image",
    headers={
        "Content-Type": "application/json",
        "Accept": "application/json",
        "Authorization": f"Bearer {api_key}"
    },
    json={
        "text_prompts": [
            {
                "text": prompt,
            }
        ],
        "cfg_scale": 7,
        "height": 1024,
        "width": 1024,
        "samples": 1,
        "steps": 30,
    },
)

if response.status_code != 200:
    raise Exception(
        "Non-200 response: " + str(response.text))

data = response.json()

image_paths = []

# if there's no /out folder, create it
if not os.path.exists("./out"):
    os.makedirs("./out")

for i, image in enumerate(data["artifacts"]):
    filename = f"./out/image-{i}.png"
    with open(filename, "wb") as f:
        f.write(base64.b64decode(image["base64"]))

    image_paths.append(filename)

# display the first image
Image(filename=image_paths[0])</pre>

<p><a data-type="xref" href="#figure-9-3">Figure 9-3</a> shows the output.</p>

<figure><div id="figure-9-3" class="figure">
<img src="assets/pega_0903.png" alt="pega 0903" width="600" height="600"/>
<h6><span class="label">Figure 9-3. </span>Corporate Memphis illustration from the Dream-Studio API</h6>
</div></figure>

<p>Let’s break down this code step-by-step:</p>
<ol>
<li>
<p>First, set up the required environment variables:</p>

<ul>
<li>
<p><code>engine_id</code>: This refers to a specific model version at <code>stability.ai</code>.</p>
</li>
<li>
<p><code>api_host</code>: This retrieves the API host URL from environment variables. If not set, it defaults to <code>'https://api.stability.ai'</code>.</p>
</li>
<li>
<p><code>api_key</code>: This retrieves the API key from environment variables.</p>
</li>
</ul>
</li>
<li>
<p>The <code>prompt</code>: This defines how the image should look, including the style and colors.</p>
</li>
<li>
<p>A <code>POST</code> request is made to the URL derived from <code>api_host</code> and <code>engine_id</code>.</p>

<ul>
<li>
<p>The headers for the request are set to accept and send JSON data and include an authorization header with the <code>api_key</code>.</p>
</li>
<li>
<p>The JSON body of the request specifies the prompt (description of the image), the desired scale of the image, its dimensions, the number of samples, and the number of steps.</p>
</li>
</ul>
</li>
<li>
<p>If the status code of the response is not 200 (indicating a successful request), an exception is raised with the response text to indicate something went wrong. Otherwise, the response is parsed into JSON format.</p>
</li>
<li>
<p>If there isn’t a directory named <em>out</em>, one is created. For each artifact (image) in the response, the code does the following:</p>

<ul>
<li>
<p>Sets a filename path.</p>
</li>
<li>
<p>Decodes the base64-encoded image data from the response.</p>
</li>
<li>
<p>Writes the decoded image data to a file.</p>
</li>
<li>
<p>Appends the file’s path to the <code>image_paths</code> list.</p>
</li>
<li>
<p>This is typically where you would save the image to <a href="https://oreil.ly/YsuBw">Google Cloud Storage</a> or Amazon Simple Storage Service (S3) to display later in your application.</p>
</li>
</ul>
</li>
<li>
<p>The first image from the <code>image_paths</code> list (the only one, in this case) is displayed (only in Jupyter Notebooks or Google Colab) using the <code>Image</code> class from <code>IPython.display</code>.</p>
</li>

</ol>

<p>The downside of using Stability AI’s service is a lack of control over customization. One of the great benefits of Stable Diffusion being open source is the ability to modify almost any aspect of the model and make use of community-built advanced functionality. In addition, there is no guarantee that functions or features you rely on for your scripts today will still be there in the future, as Stability AI strives to live up to the expectations of their investors, legal team, and corporate customers. For example, the popular (and more permissive) version 1.5 model has been deprecated in favor of the new Stable Diffusion 2.0 and XL models, causing problems for those who had finely tuned their workflows, parameters, and prompts <a data-type="indexterm" data-primary="Stable Diffusion" data-secondary="running locally" data-startref="stbdfrc" id="id1252"/>to work with v1.5.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="AUTOMATIC1111 Web User Interface"><div class="sect1" id="id127">
<h1>AUTOMATIC1111 Web User Interface</h1>

<p>Heavy users of Stable Diffusion typically <a data-type="indexterm" data-primary="Stable Diffusion" data-secondary="AUTOMATIC1111" id="id1253"/><a data-type="indexterm" data-primary="AUTOMATIC1111" id="id1254"/>recommend the <a href="https://oreil.ly/r-2vm">AUTOMATIC1111</a> (pronounced “automatic eleven eleven”) web user interface, because it is feature-rich and comes with multiple extensions built by Stable Diffusion power users. This project is the gateway to taking advantage of the best aspect of Stable Diffusion: the vibrant open source community that has dedicated countless hours to integrating advanced functionality to the tool. Advanced users may also want to explore <a href="https://oreil.ly/LWVvC">ComfyUI</a>, as it supports more advanced workflows and increased flexibility (including <a href="https://oreil.ly/dh7jR">image-to-video</a>), but we deemed this too complex for the majority of use cases, which can easily be handled by AUTOMATIC1111.</p>

<p class="less_space pagebreak-before">You can use the normal text-to-image Stable Diffusion model, but also run image-to-image (similar to the base image feature in Midjourney), as well as upscaling finished images for higher quality, and inpainting (as is offered by DALL-E). It’s even possible to train and run custom models within this interface, and there are thousands of models shared publicly in communities such as <a href="https://oreil.ly/t5T7p">Hugging Face</a> and <a href="https://civitai.com">Civitai</a>.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Some custom open source models are NSFW (not safe for work), so be careful when browsing websites like Civitai.</p>
</div>

<p>Running Stable Diffusion locally with AUTOMATIC1111 requires some technical setup, and it’s best to look for an up-to-date guide in the AUTOMATIC1111 Wiki:</p>

<ul>
<li>
<p><a href="https://oreil.ly/DsKyU">Install and run on NVidia GPUs</a></p>
</li>
<li>
<p><a href="https://oreil.ly/Oc7ix">Install and run on AMD GPUs</a></p>
</li>
<li>
<p><a href="https://oreil.ly/Ob2VK">Install and run on Apple Silicon</a></p>
</li>
</ul>

<p>Installation generally involves <a data-type="indexterm" data-primary="Stable Diffusion" data-secondary="AUTOMATIC1111" data-tertiary="installation" id="id1255"/><a data-type="indexterm" data-primary="AUTOMATIC1111" data-secondary="installation" id="id1256"/>ensuring you have Git and Python installed (as well as any other <a href="https://oreil.ly/vBOVI">dependencies</a>), and downloading Stable Diffusion, as well as the Automatic1111 <a href="https://oreil.ly/x0BMn">code</a> to your local computer. The images in this chapter use <a href="https://oreil.ly/DIvUz">the XL 1.0 version</a> of Stable Diffusion, though many still use the older <a href="https://oreil.ly/FNxf9">version 1.5</a> as it is considered more permissive and has a wealth of custom community-trained models. The techniques work the same across models, though the results and quality will differ: it’s commonly believed that removing NSFW images from the training data for version 2.0 led to worse performance at generating (even nonexplicit) images of realistic human figures (though this seems largely corrected in the XL version).</p>

<p>As the model is open source, you can get SDXL v1.0 on your local computer by visiting the model page on Hugging Face for the base and refiner models, and downloading the <em>.safetensors</em> files from the “Files and Versions” tab. This format is safer than the previous <em>.ckpt</em> file format, as it does not execute code on your computer when running:</p>

<ul>
<li>
<p><a href="https://oreil.ly/wtHRj">Base model</a>: <em>sd_xl_base_1.0.safetensors</em></p>
</li>
<li>
<p><a href="https://oreil.ly/0Dlbv">Refiner model</a>: <em>sd_xl_refiner_1.0.safetensors</em></p>
</li>
</ul>

<p class="less_space pagebreak-before">These models take time to download, so start downloading them now and later you will place them in your models/Stable-diffusion folder once you have installed the AUTOMATIC111 interface. If you want to use the older v1.5 Stable Diffusuion model, download the <em>v1-5-pruned-emaonly.ckpt</em> file from <a href="https://oreil.ly/hwblq">Hugging Face</a>, and move that into the models folder where you placed the base and refiner models.</p>

<p>Once you have everything installed, the web interface is accessed by running a script that launches the application locally, which will show up as a web address in your browser. As one example, here are the current instructions (at time of writing) for Windows, with a computer that has an Nvidia GPU:</p>
<ol>
<li>
<p>Install <a href="https://oreil.ly/kGiyi">Python 3.10.6</a> (selecting Add to PATH) and <a href="https://oreil.ly/Pdzb0">Git</a>.</p>
</li>
<li>
<p>Open the command prompt from search bar, and type <code><strong>git clone <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui" class="bare"><em class="hyperlink">https://github.com/AUTOMATIC1111/stable-diffusion-webui</em></a></strong></code>.</p>
</li>
<li>
<p>Remember to move the  sd_xl_base_1.0.safetensors and sd_xl_refiner_1.0.safetensors models into the stable-diffusion-webui/models/Stable-diffusion folder.</p>
</li>
<li>
<p>Double-click the <em>webui-user.bat</em> file and visit the address the interface is running on (usually <em><a href="http://127.0.0.1:7860" class="bare"><em class="hyperlink">http://127.0.0.1:7860</em></a></em>). For Mac or Linux, you would run <code>bash webui.sh</code> in the terminal.</p>
</li>

</ol>

<p>From this interface, shown in <a data-type="xref" href="#figure-9-4">Figure 9-4</a> (taken from the official <a href="https://oreil.ly/OOpas">GitHub repository</a>), you can enter your prompt (top left, under the “txt2img” tab) and click Generate to get your image.</p>

<p>If you run into an error or if you downloaded AUTOMATIC1111 web UI a while ago and need to update it, you can <a data-type="indexterm" data-primary="Stable Diffusion" data-secondary="AUTOMATIC1111" data-tertiary="download errors" id="id1257"/><a data-type="indexterm" data-primary="AUTOMATIC1111" data-secondary="download errors" id="id1258"/>enter the stable-diffusion-webui folder in your terminal and run <code>git pull</code>. If you are running into errors, you may reset your implementation (move any files you want to save first) by running <code>git checkout -f master</code> in the stable-diffusion-webui folder.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Resetting AUTOMATIC1111 this way will delete any files in the folder, along with any customizations. We recommend you make a local copy in a different folder for recovery.</p>
</div>

<figure><div id="figure-9-4" class="figure">
<img src="assets/pega_0904.png" alt="pega 0904" width="600" height="470"/>
<h6><span class="label">Figure 9-4. </span>Stable Diffusion web UI</h6>
</div></figure>

<p>The box immediately below the prompt input is where you can add negative prompts to remove concepts from an image and ensure they don’t show up (see <a data-type="xref" href="ch08.html#standard_image_08">Chapter 8</a> for more on negative prompts). Underneath, you’ll find a number of settings including the Seed (set to –1 for a new image each time), number of Sampling (inference) Steps, Batch Count (Number of generations to run one after another), and Batch Size (number of images processed in each batch at the cost of higher VRAM needed). When images are generated, you can download them from the interface directly, send them to various tabs with the buttons below, or visit the stable-diffusion-webui/outputs folder where they are organized by method (<code>text2img</code>, <code>img2img</code>) and date:</p>

<pre data-type="programlisting">stable-diffusion-webui/
    outputs/
        txt2img-images/
            2023-10-05/
                your_image.png</pre>

<p class="less_space pagebreak-before">When you run the AUTOMATIC1111 web UI, any models you downloaded will appear in the Stable Diffusion Checkpoint drop-down menu at the top. Select the base model and enter your prompt as well as adjusting your settings as normal. Make sure you set the image size to 1024x1024. For now, set the “Switch at” parameter under Refiner to <code>1</code> to run only the base model, as in <a data-type="xref" href="#figure-9-5">Figure 9-5</a>.</p>

<figure><div id="figure-9-5" class="figure">
<img src="assets/pega_0905.png" alt="pega 0905" width="600" height="404"/>
<h6><span class="label">Figure 9-5. </span>Standard settings for SDXL</h6>
</div></figure>

<p>The sampling methods available are relatively <a data-type="indexterm" data-primary="Stable Diffusion" data-secondary="AUTOMATIC1111" data-tertiary="sampling methods" id="id1259"/><a data-type="indexterm" data-primary="AUTOMATIC1111" data-secondary="sampling methods" id="id1260"/>complex and technical to explain, but the trade-offs are generally between speed, quality, and randomness. <code>Euler</code> is the simplest sampler, and <code>DDIM</code> was the first designed specifically for Diffusion models. The sampling methods that have an <em>a</em> in the name, for example <code>Euler a</code>, are ancestral samplers, which inject noise into the image as part of the process. This gives less reproducible results as the image does not converge (there is some randomness to the image each time you run the model). The <code>DPM++ 2M Karras</code> and <code>UniPC</code> sampler running for 20–30 steps are excellent choices for robust, stable, and reproducible images. For higher-quality but slower and more random images, try the <code>DPM++ SDE Karras</code>  or <code>DDIM</code> samplers with 10–15 steps.</p>

<p class="less_space pagebreak-before">Another important parameter is the CFG Scale (Classifier Free Guidance—the same as the <code>guidance_scale</code> introduced in the Stable Diffusion Inference Google Colab example). As a rule of thumb, here are common values for CFG Scale and what they equate to:</p>

<ul>
<li>
<p><em>1</em>: Mostly ignore the prompt.</p>
</li>
<li>
<p><em>3</em>: Feel free to be creative.</p>
</li>
<li>
<p><em>7</em>: A good balance between the prompt and creativity.</p>
</li>
<li>
<p><em>15</em>: Adhere to the prompt.</p>
</li>
<li>
<p><em>30</em>: Strictly follow the prompt.</p>
</li>
</ul>

<p>You can change the size of the image generated with Height and Width, as well as the number of images using Batch Count. The checkbox Highres fix uses an upscaler to generate a larger high-resolution image (more on this later), the Restore faces checkbox uses a face restoration model (by default <code>Codeformer</code>) to fix the defects in human faces that often occur with Stable Diffusion, and the Tiling checkbox creates an image that can be tiled in a repeating pattern. There’s also the ability to save and insert styles that are just prompts you want to reuse regularly. There are many <a href="https://oreil.ly/MiSt1">powerful features</a> in the different tabs, as well as community-built extensions you can add, with more added as they become available.</p>

<p>AUTOMATIC1111 supports prompt weights, or weighted terms, much like Midjourney (covered in <a data-type="xref" href="ch08.html#standard_image_08">Chapter 8</a>). The way you access them is slightly different, as instead of separating by double colons like in Midjourney, you use parentheses. For example, <code>(pirate)</code> would emphasize pirate features by 10% or 1.1, and double parentheses <code>((pirate))</code> would multiply it again, so the weight would be 1.1 x 1.1 = 1.21. You can also control the weights precisely by inputting your own number in the form of (keyword: factor), for example <code>(pirate: 1.5)</code>, for the model to pay 50% more attention to those tokens.</p>

<p id="prompt_weights_sd">Input:</p>

<pre data-type="programlisting">Marilyn Monroe as a (pirate:1.5) on a desert island, detailed clothing,
by Stanley Artgerm Lau and Alphonse Mucha</pre>

<p>Negative:</p>

<pre data-type="programlisting">racy, nudity, cleavage</pre>

<p><a data-type="xref" href="#figure-9-6">Figure 9-6</a> shows the output.</p>

<figure><div id="figure-9-6" class="figure">
<img src="assets/pega_0906.png" alt="pega 0906" width="600" height="339"/>
<h6><span class="label">Figure 9-6. </span>Marilyn Monroe pirate</h6>
</div></figure>

<p>Square brackets <code>[pirate]</code> work the same way but in reverse, de-emphasising a term in a prompt by 10%. So for example, <code>[hat]</code> would be the same as a weight of 0.9, or <code>(hat:0.9)</code>. Note this is not the same as a negative prompt, because the term will still be present in the generation of the image, just dialed down. Prompt weights work in the negative prompt box as well, acting to more aggressively remove that concept from the image or reduce their effects. This can be used to ensure unwanted elements or styles don’t appear when a negative prompt isn’t enough.</p>
<div data-type="tip"><h1>Give Direction</h1>
<p>Providing more or less emphasis on specific <a data-type="indexterm" data-primary="Give Direction principle" data-secondary="AUTOMATIC1111" id="id1261"/>words or sections of a prompt can give you more fine-grained control over what the model pays attention to.</p>
</div>

<p>A more advanced <a data-type="indexterm" data-primary="Stable Diffusion" data-secondary="AUTOMATIC1111" data-tertiary="prompt editing" id="id1262"/><a data-type="indexterm" data-primary="AUTOMATIC1111" data-secondary="prompt editing" id="id1263"/><a data-type="indexterm" data-primary="Stable Diffusion" data-secondary="AUTOMATIC1111" data-tertiary="prompt switching" id="id1264"/><a data-type="indexterm" data-primary="AUTOMATIC1111" data-secondary="prompt switching" id="id1265"/><a data-type="indexterm" data-primary="prompt editing" id="id1266"/><a data-type="indexterm" data-primary="prompt switching" id="id1267"/>technique used by power users of AUTOMATIC1111 is <em>prompt editing</em>, also known as <em>prompt switching</em>. During the diffusion process the early steps move from random noise to a fuzzy outline of the general shapes expected to be in the image, before the final details are filled in. Prompt editing allows you to pass a different prompt to the early or later steps in the diffusion process, giving you more creative control. The syntax is <code>[from:to:when]</code>, where <code>from</code> is your starting prompt, <code>to</code> is your finishing prompt, and <code>when</code> is when to make the switch, denoted in number of steps or a decimal representing a percentage. The prompt <code>[Emma Watson: Amber Heard: 0.5]</code> would start generating an image of Emma Watson, before switching halfway to generating an image of Amber Heard on top of the last frame, finishing with a mixture of the two actresses. This is a useful trick for creating images of people that look attractive and vaguely familiar, without being recongizeable as any specific celebrity, and therefore may be seen as more ethical and legally sound than simply copying a celebrity’s likeness (seek your own legal counsel):</p>

<p class="less_space pagebreak-before" id="prompt_editing">Input:</p>

<pre data-type="programlisting">vogue fashion shoot of [Emma Watson: Amber Heard: 0.5],
highly realistic, high resolution, highly detailed,
dramatic, 8k</pre>

<p><a data-type="xref" href="#figure-9-7">Figure 9-7</a> shows the output.</p>

<figure><div id="figure-9-7" class="figure">
<img src="assets/pega_0907.png" alt="pega 0907" width="512" height="512"/>
<h6><span class="label">Figure 9-7. </span>Emma Watson and Amber Heard mixed</h6>
</div></figure>
<div data-type="tip"><h1>Providing Direction</h1>
<p>Prompt editing is an advanced technique that gets deep into the actual workings of the diffusion model. Interfering with what layers respond to what concepts can lead to very creative results if you know what you’re doing and are willing to undergo enough trial and error.</p>
</div>

<p>If you want the model to alternate between two concepts, the syntax is <code>[Emma Watson | Amber Heard]</code>, which will make the switch at every step, ending with a more blended mixture. There are many advanced uses of prompt editing, though it is seen as something of a dark art. In some cases experts report being able to get around difficult generations, for example starting by generating something easy for the model to generate, before switching to what is really needed in the final details phase. In practice we have found limited use out of this technique, but you should experiment and see what you can discover.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Img2Img"><div class="sect1" id="id128">
<h1>Img2Img</h1>

<p>The AUTOMATIC1111 web UI supports <code>Img2Img</code> (<a data-type="xref" href="#figure-9-8">Figure 9-8</a>), which is the functional equivalent to Midjourney’s ability <a data-type="indexterm" data-primary="Stable Diffusion" data-secondary="AUTOMATIC1111" data-tertiary="Img2Img" id="sbffucii"/><a data-type="indexterm" data-primary="AUTOMATIC1111" data-secondary="Img2Img" id="autmgmg"/><a data-type="indexterm" data-primary="Img2Img" data-secondary="AUTOMATIC1111" id="igigutma"/>to submit an image along with the prompt. It grants you more control over the style and composition of your resulting image, by uploading an image for the model to use as guidance. To get good results with <code>Img2Img</code>, try using <code>Euler</code> sampling, 50 sampling steps, and a higher than usual CFG scale of 20 to 30.</p>

<figure><div id="figure-9-8" class="figure">
<img src="assets/pega_0908.png" alt="pega 0908" width="600" height="326"/>
<h6><span class="label">Figure 9-8. </span>Img2Img</h6>
</div></figure>

<p>The parameters are the same as the normal <code>Text2Image</code> mode with the addition of <em>denoising strength</em>, which controls <a data-type="indexterm" data-primary="denoising" data-secondary="strength" id="id1268"/><a data-type="indexterm" data-primary="Img2Img" data-secondary="denoising strength" id="id1269"/><a data-type="indexterm" data-primary="Text2Image" id="id1270"/>how much random noise is added to your base image before running the generation process. A value of 0 will add zero noise, so your output will look exactly like your input, and a value of 1 will completely replace your input with noise (functionally the same as using <code>Text2Image</code>). Often you need to experiment with different combinations of values for Denoising Strength, CFG scale, and Seed alongside the words in your prompt. The following example in <a data-type="xref" href="#figure-9-9">Figure 9-9</a> creates a character in Pixar style just for fun: we wouldn’t recommend using protected IP in your prompt for commercial use.</p>

<p>Input:</p>

<pre data-type="programlisting">headshot of a man in an office,  as a Pixar Disney character
from Up ( 2 0 0 9 ), unreal engine, octane render, 3 d
render, photorealistic, in the style of Pixar</pre>

<p><a data-type="xref" href="#figure-9-9">Figure 9-9</a> shows the output.</p>

<figure><div id="figure-9-9" class="figure">
<img src="assets/pega_0909.png" alt="pega 0909" width="344" height="800"/>
<h6><span class="label">Figure 9-9. </span>The effect of different denoising strength values on an image</h6>
</div></figure>

<p>If you want to test many different values for a parameter in AUTOMATIC1111 and generate a grid as is shown in <a data-type="xref" href="#figure-9-9">Figure 9-9</a>, that is supported in the Script drop-down at the bottom, where you can select X/Y/Z Plot and choose up to three parameters to generate multiple values for. For example, you may try also adjusting the CFG scale to see how it interacts with Denoising. <a data-type="xref" href="#figure-9-10">Figure 9-10</a> shows how to select multiple values for the Denoising strength parameter. When you click the Generate button, a grid of images will be made, and you can find each individual image that populates the grid in your Output folder under the method (i.e., <code>Text2Image</code>, or <code>Img2Img</code>) and today’s date.</p>

<figure><div id="figure-9-10" class="figure">
<img src="assets/pega_0910.png" alt="pega 0910" width="600" height="379"/>
<h6><span class="label">Figure 9-10. </span>X/Y/Z plot of denoising parameter</h6>
</div></figure>
<div data-type="tip"><h1>Evaluate Quality</h1>
<p>Generating a grid of many different parameter <a data-type="indexterm" data-primary="Evaluate Quality principle" data-secondary="Img2Img" id="id1271"/>combinations or values is one of the powerful advantages of running Stable Diffusion locally. Although it may take time to generate lots of images, there’s no better way to visually identify exactly what a parameter does and where the sweet spot is in terms of quality.</p>
</div>

<p>If you forgot what settings or prompt you used to generate an image, AUTOMATIC1111 saves this as metadata on every image generated. You can visit the PNG Info tab (<a data-type="xref" href="#figure-9-11">Figure 9-11</a>) to read that metadata whenever needed. This also works with images you get from other users of the web interface, but only if they have posted the image on a website that doesn’t strip out this metadata.</p>

<p>The Resize Mode options are there to determine what happens when you upload an image that doesn’t match the dimensions of your base image, for example going from 1000 × 500 to 512 × 512, either stretching the aspect ratio to fit with Just Resize, cropping a part of the image in the right aspect ratio with Crop and Resize, adding noise to pad out the image with Resize and Fill, or generating an image in the new <a data-type="indexterm" data-primary="Stable Diffusion" data-secondary="AUTOMATIC1111" data-tertiary="Img2Img" data-startref="sbffucii" id="id1272"/><a data-type="indexterm" data-primary="AUTOMATIC1111" data-secondary="Img2Img" data-startref="autmgmg" id="id1273"/><a data-type="indexterm" data-primary="Img2Img" data-secondary="AUTOMATIC1111" data-startref="igigutma" id="id1274"/>dimensions with Just Resize (latent upscale).</p>

<figure><div id="figure-9-11" class="figure">
<img src="assets/pega_0911.png" alt="pega 0911" width="600" height="260"/>
<h6><span class="label">Figure 9-11. </span>PNG Info tab</h6>
</div></figure>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Upscaling Images"><div class="sect1" id="id129">
<h1>Upscaling Images</h1>

<p>There’s also the ability to upscale images to <a data-type="indexterm" data-primary="images" data-secondary="upscaling" id="mgapscl"/><a data-type="indexterm" data-primary="upscaling images" id="upslmga"/><a data-type="indexterm" data-primary="Stable Diffusion" data-secondary="AUTOMATIC1111" data-tertiary="upscaling images" id="bdfpsclm"/><a data-type="indexterm" data-primary="AUTOMATIC1111" data-secondary="upscaling images" id="autpsmg"/>higher resolution in AUTOMATIC1111’s Img2Img tab, just like you can in Midjourney but with more control. Upload your image and add a generic prompt like <code>highly detailed</code> in the prompt box. This is necessary because the upscaler works by breaking the image into tiles, expanding so there are gaps between the tiles, and then <em>filling in</em> the gaps using the prompt and context of the surrounding pixels. Go down to Scripts at the bottom and select the SD Upscale script, and then choose an upscaler (<a data-type="xref" href="#figure-9-12">Figure 9-12</a>).</p>

<figure><div id="figure-9-12" class="figure">
<img src="assets/pega_0912.png" alt="pega 0912" width="600" height="282"/>
<h6><span class="label">Figure 9-12. </span>SD Upscale interface</h6>
</div></figure>

<p>Typically we have found <a data-type="indexterm" data-primary="R-ESRGAN 4x+ upscaler" id="id1275"/>the R-ESRGAN 4x+ upscaler as a good default, but this can sometimes give a cartoonish quality, as shown in <a data-type="xref" href="#figure-9-12">Figure 9-12</a> with the grass. There are <a href="https://openmodeldb.info">more models</a> available to test if you aren’t getting good results. When you download a new model (a <em>.pth</em> file), you just need to place it in the ESRGAN folder and restart the web interface for them to show up (in your terminal). You can also get good results with upscaling by modifying the prompt, particularly if you are losing some detail or the style is changing too much. However, it is not advised to use your original prompt, as that would have the strange effect of inpainting the same image in each tile. To show a wider quality difference, we have used the v1.5 model to generate the original image (SDXL creates images that are 4x larger, and at a higher quality, so upscaling is less needed).</p>

<figure><div id="figure-9-13" class="figure">
<img src="assets/pega_0913.png" alt="pega 0913" width="379" height="800"/>
<h6><span class="label">Figure 9-13. </span>The impact of upscaling on sections of an image</h6>
</div></figure>
<div data-type="tip"><h1>Specify Format</h1>
<p>If you’re going to use the images you generate <a data-type="indexterm" data-primary="Specify Format principle" data-secondary="upscaling images" id="id1276"/>in the real world, often you can’t just use a square 512 x 512 image in low resolution. Using upscaling you can generate an image in any size and whatever the required resolution.</p>
</div>

<p>As with all things Stable Diffusion, it helps to experiment, but for good results we recommend a high number of steps (150–200+), a CFG scale of 8–15, and a Denoising strength of 0.1–0.2 to keep the base image intact. You can click Generate to get the resulting upscaled image (512 x 512 becomes 1024 x 1024), and then you can either download the higher resolution image or click Send to Img2Img and click Generate again to double the size of the image again. The process can take a significant amount of time due to the multiple tile generations and large number of sampling <a data-type="indexterm" data-primary="images" data-secondary="upscaling" data-startref="mgapscl" id="id1277"/><a data-type="indexterm" data-primary="upscaling images" data-startref="upslmga" id="id1278"/><a data-type="indexterm" data-primary="Stable Diffusion" data-secondary="AUTOMATIC1111" data-tertiary="upscaling images" data-startref="bdfpsclm" id="id1279"/><a data-type="indexterm" data-primary="AUTOMATIC1111" data-secondary="upscaling images" data-startref="autpsmg" id="id1280"/>steps, approximately 10–30 minutes on a M2 MacBbook Air.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Interrogate CLIP"><div class="sect1" id="id187">
<h1>Interrogate CLIP</h1>

<p>In the Img2Img tab the CLIP embeddings <a data-type="indexterm" data-primary="Img2Img" data-secondary="CLIP embeddings model" id="id1281"/><a data-type="indexterm" data-primary="embeddings" data-secondary="Img2Img CLIP embeddings model" id="id1282"/>model (which is also used by Stable Diffusion) is implemented in the Interrogate CLIP button (in some versions shown as a paperclip), which allows you to reverse engineer the prompt from an image, similar to Midjourney’s Describe feature, covered in <a data-type="xref" href="ch08.html#standard_image_08">Chapter 8</a>. Once you click the button and the script has run, the prompt will appear in your prompt box (<a data-type="xref" href="#figure-9-14">Figure 9-14</a>).</p>

<figure><div id="figure-9-14" class="figure">
<img src="assets/pega_0914.png" alt="pega 0914" width="600" height="135"/>
<h6><span class="label">Figure 9-14. </span>Interrogate CLIP</h6>
</div></figure>

<p>Output<a data-type="indexterm" data-primary="Img2Img" data-secondary="CLIP embeddings model" data-startref="ggcbdd" id="id1283"/><a data-type="indexterm" data-primary="embeddings" data-secondary="Img2Img CLIP embeddings model" data-startref="bdgmgcl" id="id1284"/>:</p>

<pre data-type="programlisting">a painting of a woman in a pirate costume on the beach
with a pirate hat on her head and a pirate ship in the background,
a fine art painting, Chris Rallis, fantasy art, stanley artgerm lau</pre>
</div></section>






<section data-type="sect1" class="pagebreak-before less_space" data-pdf-bookmark="SD Inpainting and Outpainting"><div class="sect1" id="id188">
<h1>SD Inpainting and Outpainting</h1>

<p>Img2Img also supports inpainting and <a data-type="indexterm" data-primary="Stable Diffusion" data-secondary="Img2Img" data-tertiary="inpainting" id="sdfggpg"/><a data-type="indexterm" data-primary="Stable Diffusion" data-secondary="Img2Img" data-tertiary="outpainting" id="sbdgmgp"/><a data-type="indexterm" data-primary="inpainting" data-secondary="Img2Img" id="ipmgmg"/><a data-type="indexterm" data-primary="outpainting" data-secondary="Img2Img" id="optgmgm"/>outpainting and provides a simple canvas tool for creating the mask. To use inpainting or outpainting, click the Inpaint subtab in the Img2Img tab and upload your image. It’s optionally recommended to use a specific inpainting model for better results, which you can install by <a href="https://oreil.ly/s_trl">downloading</a> the <em>sd-v1-5-inpainting.ckpt</em> file and moving it into your Models &gt; Stable-Diffusion folder. Restart the interface; the model should appear in the top left drop-down. The canvas allows you to use a brush to remove parts of the image just like in DALL-E (see <a data-type="xref" href="ch08.html#standard_image_08">Chapter 8</a>), which is adjustable in size for fine-grained control. In <a data-type="xref" href="#figure-9-15">Figure 9-15</a>, the center of a stone circle in the middle of a castle courtyard has been removed.</p>

<figure><div id="figure-9-15" class="figure">
<img src="assets/pega_0915.png" alt="pega 0915" width="600" height="587"/>
<h6><span class="label">Figure 9-15. </span>Inpainting canvas in Img2Img</h6>
</div></figure>

<p>The advice typically given for DALL-E, which also supports inpainting, is to use your prompt to describe the entire image, not just the inpainted area. This is a good default and should be tried first. Make sure Inpaint area is set to <em>Whole picture</em> rather than <em>Only masked</em>, or it’ll try to fit the whole scene in the masked area (don’t worry, even if you select <em>Whole picture</em>, it will only paint in your masked area). It can also help to carry over your Seed from the original image if it was AI generated. However, adding to or changing the prompt to include specifics about the region you want modified or fixed tends to get better results in our experience. At the very least you should change the subject of the prompt; for example, in <a data-type="xref" href="#figure-9-15">Figure 9-15</a>, the prompt changed from <code>castle</code> to <code>statue</code> because that’s what we wanted to appear in the courtyard. You can also try only prompting for the infilled region, though that risks getting an image that isn’t globally consistent in style.</p>

<p>Input:</p>

<pre data-type="programlisting">statue of a king, texture, intricate, details, highly
detailed, masterpiece, architecture, building, trending on
artstation, focus, sharp focus, concept art, digital
painting, fantasy, sunny, day, midday, in the style of
high fantasy art</pre>

<p><a data-type="xref" href="#figure-9-16">Figure 9-16</a> shows the output.</p>

<figure><div id="figure-9-16" class="figure">
<img src="assets/pega_0916.png" alt="pega 0916" width="512" height="512"/>
<h6><span class="label">Figure 9-16. </span>Inpainting to add a statue to an image</h6>
</div></figure>
<div data-type="tip"><h1>Providing Direction</h1>
<p>Inpainting is so powerful because it gives you control. The ability to isolate an individual part of an image and give detailed directions on how to fix it gives you a more efficient workflow, without affecting the rest of the image.</p>
</div>

<p>If it’s a small adjustment to the inpainted area, use Original as the masked content option and use a Denoising Strength of 0.2 to 0.4. If you’re totally replacing an element of the image, you may need the Latent Noise option and as high as 0.8 for Denoising Strength, though any time you get above 0.4 you start to see globally inconsistent elements and hallucinations in the image, so it can take time to iterate toward something that works. The Fill option is also useful as it matches the colors of the surrounding area. If you’re getting ugly seams at the edge of the inpainting area, you can increase the Mask Blur, but typically the default of 4 works well. Inpainting is an iterative process. We recommend working on fixing one issue or artifact at a time, applying it as many times as you want, and experimenting with different parameters until you’re satisfied with the final image.</p>

<p>Outpainting doesn’t work the same as in Midjourney (see <a data-type="xref" href="ch08.html#standard_image_08">Chapter 8</a>), which has the ability to specify 1.5x or 2x zoom, or a custom aspect ratio. Instead in AUTOMATIC1111, outpainting is implemented by scrolling down to the Script drop-down and selecting “Poor man’s outpainting.” You need to set the Resize mode to Resize and fill in the Img2Img Inpaint tab, and set a relatively high Denoising Strength to make this work. This extension allows you to expand the pixels on different sides of the image, while setting the Masked Content and Mask Blur parameters as usual for these gaps on the side to be inpainted.</p>

<p><a data-type="xref" href="#figure-9-17">Figure 9-17</a> shows the output.</p>

<figure><div id="figure-9-17" class="figure">
<img src="assets/pega_0917.png" alt="pega 0917" width="600" height="600"/>
<h6><span class="label">Figure 9-17. </span>Outpainting in Img2Img</h6>
</div></figure>

<p>As you can see in <a data-type="xref" href="#figure-9-17">Figure 9-17</a>, with the extra castle being added to the sky, the potential for hallucination is high and the quality can be low. It often takes a lot of experimentation and iteration to get this process right. This is a similar technique to how early adopters of generative AI would add extra empty space on the sides of photos in Photoshop, before inpainting them to match the rest of the image in Stable Diffusion. This technique is essentially just inpainting with extra <a data-type="indexterm" data-primary="Stable Diffusion" data-secondary="Img2Img" data-tertiary="inpainting" data-startref="sdfggpg" id="id1285"/><a data-type="indexterm" data-primary="Stable Diffusion" data-secondary="Img2Img" data-tertiary="outpainting" data-startref="sbdgmgp" id="id1286"/><a data-type="indexterm" data-primary="inpainting" data-secondary="Img2Img" data-startref="ipmgmg" id="id1287"/><a data-type="indexterm" data-primary="outpainting" data-secondary="Img2Img" data-startref="optgmgm" id="id1288"/>steps, so all of the same advice previously listed applies. This can be quicker than using the outpainting functionality in AUTOMATIC1111 because of the poor quality and limitations of not having a proper canvas.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="ControlNet"><div class="sect1" id="id130">
<h1>ControlNet</h1>

<p>Using prompting and Img2Img or base images, it’s possible to control the style of an image, but often the pose of people in <a data-type="indexterm" data-primary="images" data-secondary="input, conditioning" id="imgpcd"/><a data-type="indexterm" data-primary="conditioning input images" id="cdgnpig"/><a data-type="indexterm" data-primary="ControlNet" id="ctrlntn"/>the image, composition of the scene, or structure of the objects will differ greatly in the final image. ControlNet is an advanced way of conditioning input images for image generation models like Stable Diffusion.</p>

<p>It allows you to gain more control over the final image generated through various techniques like edge detection, pose, depth, and many more. You upload an image you want to emulate and use one of the pretrained model options for processing the image to input alongside your prompt, resulting in a matching image composition with a different style (<a data-type="xref" href="#figure-9-18">Figure 9-18</a>, from the <a href="https://oreil.ly/suOJz">ControlNet paper</a>).</p>

<p>What’s referred to as ControlNet is really a series of <a href="https://oreil.ly/E-bjw">open source models</a> released following the paper “Adding Conditional Control to Text-to-Image Diffusion Models” (<a href="https://oreil.ly/ZH-Ow">Zhang, Rao, and Agrawala, 2023</a>). While it is possible to code this in Python and build your own user interface for it, the quickest and easiest way to to get up and running is via the <a href="https://oreil.ly/Dw2rs">ControlNet</a> extension for AUTOMATIC1111. As of the time of writing, not all ControlNet methods are available for SDXL, so we are using Stable Diffusion v1.5 (make sure you use a ControlNet model that matches the version of Stable Diffusion you’re using).</p>

<figure><div id="figure-9-18" class="figure">
<img src="assets/pega_0918.png" alt="pega 0918" width="600" height="539"/>
<h6><span class="label">Figure 9-18. </span>ControlNet Stable Diffusion with canny edge map</h6>
</div></figure>

<p>You can install the extension following these instructions:</p>
<ol>
<li>
<p>Navigate to the Extensions tab and click the subtab labeled Available.</p>
</li>
<li>
<p>Click the Load from button.</p>
</li>
<li>
<p>In the Search box type <code><strong>sd-webui-controlnet</strong></code> to find the Extension.</p>
</li>
<li>
<p>Click Install in the Action column to the far right.</p>
</li>
<li>
<p>Web UI will now download the necessary files and install ControlNet on your local version of Stable Diffusion.</p>
</li>

</ol>

<p class="pagebreak-before">If you have trouble executing the preceding steps, you can try the following alternate method:</p>
<ol>
<li>
<p>Navigate to the Extensions tab and click Install from URL subtab.</p>
</li>
<li>
<p>In the URL field for the Git repository, paste the link to the extension: <em><a href="https://github.com/Mikubill/sd-webui-controlnet"><em class="hyperlink">https://github.com/Mikubill/sd-webui-controlnet</em></a></em>.</p>
</li>
<li>
<p>Click Install.</p>
</li>
<li>
<p>WebUI will download and install the necessary files for ControlNet.</p>
</li>

</ol>

<p>Now that you have ControlNet installed, restart AUTOMATIC1111 from your terminal or command line, or visit Settings and click “Apply and restart UI.”</p>

<p>The extension will appear below the normal parameter options you get for Stable Diffusion, in an accordion tab (<a data-type="xref" href="#figure-9-19">Figure 9-19</a>). You first upload an image and then click Enable before selecting the ControlNet preprocessor and model you want to use. If your system has less than 6 GB of VRAM (Video Random Access Memory), you should check the Low VRAM box. Depending on the task at hand, you might want to experiment with a number of models and make adjustments to the parameters of those models in order to see which gets results.</p>

<p>Control Weight is analogous to prompt weight or influence, similar to putting words in brackets with a weighting <code>(prompt words: 1.2)</code>, but for the ControlNet input. The Starting Control Steps and Ending Control Steps are when in the diffusion process the ControlNet applies, by default from start to finish (0 to 1), akin to prompt editing/shifting such as <code>[prompt words::0.8]</code> (apply this part of the prompt from the beginning until 80% of the total steps are complete). Because the image diffuses from larger elements down to finer details, you can achieve different results by controlling where in that process the ControlNet applies; for example; removing the last 20% of steps (Ending Control Step = 0.8) may allow the model more creativity when filling in finer detail. The Preprocessor Resolution also helps maintain control here, determining how much fine detail there is in the intermediate image processing step. Some models have their own unique parameters, such as the Canny Low and High Thresholds, which determine what pixels constitute an <em>edge</em>. Finally, the Control Mode determines how much the model follows the ControlNet input relative to your prompt.</p>

<figure><div id="figure-9-19" class="figure">
<img src="assets/pega_0919.png" alt="pega 0919" width="600" height="636"/>
<h6><span class="label">Figure 9-19. </span>ControlNet extension interface in AUTOMATIC1111</h6>
</div></figure>

<p>When you first install ControlNet, you won’t have any models downloaded. For them to populate in the drop-down, you should install them by downloading them from the  <a href="https://oreil.ly/csYK_">models page</a> and then dropping them in the Models &gt; ControlNet folder. If you’re unsure of which model to try, start with <a href="https://oreil.ly/z9XC6">Canny edge detection</a> as it <a data-type="indexterm" data-primary="Canny edge detection" id="id1289"/><a data-type="indexterm" data-primary="ControlNet" data-secondary="Canny edge detection" id="id1290"/>is the most generally useful. Each model is relatively large (in the order of a few gigabytes), so only download the ones you plan to use. Following are examples from some of the more common models. All images in this section are generated with the <code>DPM++ SDE Karras</code> sampler, a CFG scale of 1.5, Control Mode set to Balanced, Resize Mode set to Crop and Resize (the uploaded image is cropped to match the dimensions of the generated image, 512  × 512), and 30 sampling steps, with the default settings for each ControlNet model. Version 1.5 of Stable Diffusion was used as not all of these ControlNet models are available for Stable Diffusion XL at the time of writing, but the techniques should be transferrable between models.</p>

<p id="controlnet_canny">Canny edge detection creates simple, sharp pixel outlines around areas of high contrast. It can be very detailed and give excellent results but can also pick up unwanted noise and give too much control of the image to ControlNet. In images where there is a high degree of detail that needs to be transferred to a new image with a different style, Canny excels and should be used as the default option. For example, redrawing a city skyline in a specific style works very well with the Canny model, as we did with an image of New York City (by <a href="https://oreil.ly/wEPLB">Robert Bye</a> on <a href="https://oreil.ly/_iyxU">Unsplash</a>) in <a data-type="xref" href="#figure-9-20">Figure 9-20</a>.</p>

<p>Input:</p>

<pre data-type="programlisting">New York City by Studio Ghibli</pre>

<p><a data-type="xref" href="#figure-9-20">Figure 9-20</a> shows the output.</p>

<figure><div id="figure-9-20" class="figure">
<img src="assets/pega_0920.png" alt="pega 0920" width="600" height="221"/>
<h6><span class="label">Figure 9-20. </span>ControlNet Canny</h6>
</div></figure>

<p id="controlnet_depth">Sometimes in traditional img2img prompting, some elements of an image get confused or merged, because Stable Diffusion doesn’t understand the depth of those objects in relation to each other. The Depth model creates a depth map estimation based on the image, which provides control over the composition and spatial position of image elements. If you’re not familiar with depth maps, whiter areas are closer to the viewer, and blacker are farther away. This can be seen in <a data-type="xref" href="#figure-9-21">Figure 9-21</a>, where an image of a band (by <a href="https://oreil.ly/tlCrf">Hans Vivek</a> on <a href="https://oreil.ly/BOKJ7">Unsplash</a>) is turned into an image of soldiers with the same positions and depth of field.</p>

<p>Input:</p>

<pre data-type="programlisting">US military unit on patrol in Afghanistan</pre>

<p><a data-type="xref" href="#figure-9-21">Figure 9-21</a> shows the output.</p>

<figure><div id="figure-9-21" class="figure">
<img src="assets/pega_0921.png" alt="pega 0921" width="600" height="221"/>
<h6><span class="label">Figure 9-21. </span>ControlNet Depth</h6>
</div></figure>

<p id="controlnet_normal">The Normal model creates a mapping estimation that functions as a 3-D model of objects in the image. The colors red, green, and blue are used by 3-D programs to determine how smooth or bumpy an object is, with each color corresponding to a direction (left/right, up/down, close/far). This is just an estimation, however, so it can have unintended consequences in some cases. This method tends to excel if you need more textures and lighting to be taken into consideration but can sometimes offer too much detail in the case of faces, constraining the creativity of the output. In <a data-type="xref" href="#figure-9-22">Figure 9-22</a>, a woman playing a keyboard (by <a href="https://oreil.ly/RP1Ei">Soundtrap</a> on <a href="https://oreil.ly/I3QGY">Unsplash</a>) is transported back in time to the <em>Great Gatsby</em> era.</p>

<p>Input:</p>

<pre data-type="programlisting">woman playing piano at a Great Gatsby flapper party, 1920s,
symmetrical face</pre>

<p><a data-type="xref" href="#figure-9-22">Figure 9-22</a> shows the output.</p>

<figure><div id="figure-9-22" class="figure">
<img src="assets/pega_0922.png" alt="pega 0922" width="600" height="221"/>
<h6><span class="label">Figure 9-22. </span>ControlNet Normal</h6>
</div></figure>

<p id="controlnet_openpose">The OpenPose method creates a skeleton for a figure by determining its posture, hand placement, and facial expression. For this model to work you typically need to have a human subject with the full body visible, though there are portrait options. It is very common practice to use multiple OpenPose skeletons and compose them together into a single image, if multiple people are required in the scene. <a data-type="xref" href="#figure-9-23">Figure 9-23</a> transposes the <a href="https://oreil.ly/7n02i">Mona Lisa’s pose</a> onto an image of Rachel Weisz.</p>

<p>Input:</p>

<pre data-type="programlisting">painting of Rachel Weisz</pre>

<p><a data-type="xref" href="#figure-9-23">Figure 9-23</a> shows the output.</p>

<figure><div id="figure-9-23" class="figure">
<img src="assets/pega_0923.png" alt="pega 0923" width="600" height="221"/>
<h6><span class="label">Figure 9-23. </span>ControlNet OpenPose</h6>
</div></figure>

<p id="controlnet_mlsd">The M-LSD (Mobile Line Segment Detection) technique is <a data-type="indexterm" data-primary="M-LSD (Mobile Line Segment Detection)" id="id1291"/><a data-type="indexterm" data-primary="Mobile Line Segment Detection (M-LSD)" id="id1292"/>quite often used in architecture and interior design, as it’s well suited to tracing straight lines. Straight lines tend only to appear in man-made objects, so it isn’t well suited to nature scenes (though it might create an interesting effect). Man-made objects like houses are well suited to this approach, as shown in the image of a modern apartment (by <a href="https://oreil.ly/OtV_O">Collov Home Design</a> on <a href="https://oreil.ly/z38do">Unsplash</a>) reimagined for the <em>Mad Men</em> era, in <a data-type="xref" href="#figure-9-24">Figure 9-24</a>.</p>

<p>Input:</p>

<pre data-type="programlisting">1960s Mad Men style apartment</pre>

<p><a data-type="xref" href="#figure-9-24">Figure 9-24</a> shows the output.</p>

<figure><div id="figure-9-24" class="figure">
<img src="assets/pega_0924.png" alt="pega 0924" width="600" height="221"/>
<h6><span class="label">Figure 9-24. </span>ControlNet M-LSD</h6>
</div></figure>

<p class="pagebreak-before" id="controlnet_softedge">The SoftEdge technique, also known as HED (holistically-nested edge detection), is an alternative to Canny edge detection, creating smoother outlines around objects. It is very commonly used and provides good detail like Canny but can be less noisy and deliver more aesthetically pleasing results. This method is great for stylizing and recoloring images, and it tends to allow for better manipulation of faces compared to Canny. Thanks to ControlNet, you don’t need to enter too much of a detailed prompt of the overall image and can just prompt for the change you want to see. <a data-type="xref" href="#figure-9-25">Figure 9-25</a> shows a reimagining of <a href="https://oreil.ly/RjUur">Vermeer’s <em>Girl with a Pearl Earring</em></a>, with Scarlett Johansson:</p>

<p>Input:</p>

<pre data-type="programlisting">Scarlett Johansson, best quality, extremely detailed</pre>

<p>Negative:</p>

<pre data-type="programlisting">monochrome, lowres, bad anatomy, worst quality, low quality</pre>

<p><a data-type="xref" href="#figure-9-25">Figure 9-25</a> shows the output.</p>

<figure><div id="figure-9-25" class="figure">
<img src="assets/pega_0925.png" alt="pega 0925" width="600" height="221"/>
<h6><span class="label">Figure 9-25. </span>ControlNet SoftEdge</h6>
</div></figure>

<p id="controlnet_segmentation">Another popular technique for architecture is segmentation, which divides the image into related areas or segments that are somewhat related to one another. It is roughly analogous to using an image mask in Img2Img, except with better results. Segmentation can be used when you require greater command over various objects within an image. One powerful use case is on outdoor scenes, which can vary for the time of day and surroundings, or even the era. Take a look at <a data-type="xref" href="#figure-9-26">Figure 9-26</a>, showing a modern-day photograph of a castle (by <a href="https://oreil.ly/SG9CT">Richard Clark</a> on <a href="https://oreil.ly/2FlyI">Unsplash</a>), turned into a fantasy-style castle illustration.</p>

<p>Input:</p>

<pre data-type="programlisting">A beautiful magical castle viewed from the outside, texture,
intricate, details, highly detailed, masterpiece,
architecture, building, trending on artstation, focus, sharp
focus, concept art, digital painting, fantasy, sunny, day,
midday, in the style of high fantasy art</pre>

<p><a data-type="xref" href="#figure-9-26">Figure 9-26</a> shows the output.</p>

<figure><div id="figure-9-26" class="figure">
<img src="assets/pega_0926.png" alt="pega 0926" width="600" height="221"/>
<h6><span class="label">Figure 9-26. </span>ControlNet segmentation</h6>
</div></figure>

<p id="controlnet_scribble">One powerful feature is the ability to draw on a canvas and use that in ControlNet. You can also draw offline and take a picture to upload your image, but it can be quicker for simple images to click the pencil emoji in the Stable Diffusion web UI, and draw with the provided brush. Even a simple scribble is often sufficient, and the edges don’t have to be perfect, as shown in <a data-type="xref" href="#figure-9-27">Figure 9-27</a>.</p>

<p>Input:</p>

<pre data-type="programlisting">The Happy Goldfish, illustrated children's book</pre>

<p><a data-type="xref" href="#figure-9-27">Figure 9-27</a> shows the output.</p>

<figure><div id="figure-9-27" class="figure">
<img src="assets/pega_0927.png" alt="pega 0927" width="600" height="221"/>
<h6><span class="label">Figure 9-27. </span>ControlNet scribble</h6>
</div></figure>
<div data-type="tip"><h1>Provide Examples</h1>
<p>ControlNet gives an AI artist the ability to <a data-type="indexterm" data-primary="Provide Examples principle" data-secondary="ControlNet" id="id1293"/>make an image that <em>looks like</em> another image in terms of composition, simply by providing an example image to emulate. This allows more control over visual consistency and more flexibility in making more <span class="keep-together">sophisticated</span> images.</p>
</div>

<p>Each of these ControlNet methods has its own preprocessor, and they must match the model for the image to make sense. For example, if you’re using a Canny preprocessor, you should use a Canny model like control_v11p_sd15_canny. It’s also important to choose a model that gives enough freedom for the task you’re trying to accomplish; for example, an image of a cat with the SoftEdge model might perhaps have too much detail to be turned into a lion, and you might want to try something less fine-grained. As with all things Stable Diffusion, finding the exact combination of model and parameters requires experimentation, with new functionality and options proliferating all the time.</p>

<p>ControlNet supports being run with a simple prompt or even without a prompt at all. It will match the existing image you submit and ensure a high level of consistency. You can run a generic prompt like <code>a professional, detailed, high-quality image</code> and get a good version of the existing image. Most often, however, you’ll be attempting to change certain aspects of the image and will want to input a full prompt, as in the <a data-type="indexterm" data-primary="images" data-secondary="input, conditioning" data-startref="imgpcd" id="id1294"/><a data-type="indexterm" data-primary="conditioning input images" data-startref="cdgnpig" id="id1295"/><a data-type="indexterm" data-primary="ControlNet" data-startref="ctrlntn" id="id1296"/>previous examples. The resulting image will match both the prompt and the ControlNet output, and you can experiment with adjusting the parameters available to see what gets results.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Segment Anything Model (SAM)"><div class="sect1" id="id131">
<h1>Segment Anything Model (SAM)</h1>

<p>When working on an AI-generated image, it is often <a data-type="indexterm" data-primary="images" data-secondary="SAM (Segment Anything Model)" id="mgsmgym"/><a data-type="indexterm" data-primary="SAM (Segment Anything Model)" id="samsgym"/><a data-type="indexterm" data-primary="Segment Anything Model (SAM)" id="sgytma"/>beneficial to be able to separate out a <em>mask</em> representing a specific person, object, or element. For example, dividing an image of a person from the background of the image would allow you to inpaint a new background behind that person. This can take a long time and lead to mistakes when using a brush tool, so it can be helpful to be able to automatically segment the image based on an AI model’s interpretation of where the lines are.</p>

<p>The most popular and powerful model for doing this is <em>SAM</em>, which stands for Segment Anything Model, <a href="https://oreil.ly/BuunX">released open source on GitHub</a> by Meta. The model is trained on a dataset of 11 million images and 1.1 billion masks, and is able to infer where the image mask should be based on user input (clicking to add one to three dots to the image where masks should be), or it can automatically mask all the elements individually in an image. These masks can then be exported for use in inpainting,in ControlNet, or as base images.</p>

<p class="pagebreak-after">You can use SAM in the AUTOMATIC1111 interface using the <a href="https://oreil.ly/rFMJN"><em>sd-webui-segment-anything</em></a> extension. Once AUTOMATIC1111 is installed and running, you can install the SAM extension following these instructions:</p>
<ol>
<li>
<p>Navigate to the Extensions tab and click the subtab labeled “Available.”</p>
</li>
<li>
<p>Click the “Load from” button.</p>
</li>
<li>
<p>In the Search box type in: <code><strong>sd-webui-segment-anything</strong></code> to find the extension.</p>
</li>
<li>
<p>Click Install in the Action column to the far right.</p>
</li>
<li>
<p>WebUI will now download the necessary files and install SAM on your local version of Stable Diffusion.</p>
</li>

</ol>

<p>If you have trouble executing the preceding steps, you can try the following alternate method:</p>
<ol>
<li>
<p>Navigate to the “Extensions” tab and click the “Install from URL” subtab.</p>
</li>
<li>
<p>In the URL field for the Git repository, paste the link to the extension: <em><a href="https://github.com/continue-revolution/sd-webui-segment-anything"><em class="hyperlink">https://github.com/continue-revolution/sd-webui-segment-anything</em></a></em>.</p>
</li>
<li>
<p>Click Install.</p>
</li>
<li>
<p>WebUI will download and install the necessary files for SAM on your local version of Stable Diffusion.</p>
</li>

</ol>

<p>You also need to download the actual SAM model weights, linked to <a href="https://oreil.ly/IqrbI">from the repository</a>. The 1.25 GB <em>sam_vit_l_0b3195.pth</em> is what’s being used in this chapter. If you encounter issues with low VRAM (your computer freezes or lags), you should switch to smaller models. Move the model you downloaded into the <em>stable-diffusion-webui/sd-webui-segment-anything/models/sam</em> folder.</p>

<p>Now that you have SAM fully installed, restart AUTOMATIC1111 from your terminal or command line, or visit Settings and click “Apply and restart UI.”</p>

<p>You should see the extension in the Img2Img tab, by scrolling down past the canvas and Seed parameter, in an accordion component alongside the ControlNet extension. Upload an image here (we used the photo for <a data-type="xref" href="#figure-9-28">Figure 9-28</a> by <a href="https://oreil.ly/Lb3xE">Luca Baini</a> on <a href="https://oreil.ly/jvCjz">Unsplash</a>) and click the image to select individual prompt points. These prompt points go along to SAM as user input to help the model determine what should be segmented out from the image. You can click Preview to see what mask will be created, and iteratively add or remove plot points until the mask is correct. There is a checkbox labeled “Preview automatically when add/remove points,” which updates the mask with each click. Often SAM gets it right with a single plot point, but if you are struggling, you can also add negative plot points to parts of the image you don’t want to mask by right-clicking. Select the mask you want (<a data-type="xref" href="#figure-9-28">Figure 9-28</a>) from the three options provided (counting from 0 to 2).</p>

<figure><div id="figure-9-28" class="figure">
<img src="assets/pega_0928.png" alt="pega 0928" width="600" height="683"/>
<h6><span class="label">Figure 9-28. </span>Adding plot points</h6>
</div></figure>

<p>When your mask is ready, make sure the box Copy to Inpaint Upload &amp; img2img ControlNet Inpainting is checked, and click the Switch to Inpaint Upload button. You won’t see anything happen visually, but when you switch to the Inpainting tab, you should be able to generate your prompt with the mask generated by SAM. There is no need to upload the picture or mask to the Inpainting tab. You can also download your mask for later upload in the “Inpaint upload” tab. This method was unreliable during our testing, and there may be a better supported method for inpainting with SAM and Stable Diffusion made available.</p>
<div data-type="tip"><h1>Divide Labor</h1>
<p>Generative models like Midjourney and Stable Diffusion are powerful, but they can’t do everything. In training a separate image segmentation model, Meta has made it possible to generate more complex images by splitting out the elements of an image into different masks, which can be worked <a data-type="indexterm" data-primary="images" data-secondary="SAM (Segment Anything Model)" data-startref="mgsmgym" id="id1297"/><a data-type="indexterm" data-primary="SAM (Segment Anything Model)" data-startref="samsgym" id="id1298"/><a data-type="indexterm" data-primary="Segment Anything Model (SAM)" data-startref="sgytma" id="id1299"/><a data-type="indexterm" data-primary="Divide Labor principle" data-secondary="SAM (Segment Anything Model)" id="id1300"/>on separately before being aggregated together for the final product.</p>
</div>
</div></section>






<section data-type="sect1" data-pdf-bookmark="DreamBooth Fine-Tuning"><div class="sect1" id="id132">
<h1>DreamBooth Fine-Tuning</h1>

<p>The original Stable Diffusion model cost a reported <a href="https://oreil.ly/s739b">$600,000 to train</a> using a <a data-type="indexterm" data-primary="Stable Diffusion" data-secondary="DreamBooth and" id="stdfdbo"/><a data-type="indexterm" data-primary="DreamBooth" id="drmbhoo"/>total of 150,000 GPU hours, so training your own foundational model is likely out of the question for most organizations. However, it is possible to build on top of Stable Diffusion, using the Dreambooth technique, which was introduced in the paper “DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation” (<a href="https://oreil.ly/ZqdjB">Ruiz et al., 2022</a>). DreamBooth allows you to fine-tune or train the model to understand a new concept it hasn’t encountered yet in its training data. Not having to start from scratch to build a new model means significantly less time and resources: about 45 minutes to an hour on 1 GPU. DreamBooth actually updates the weights of the new model, which gives you a new 2 GB model file to use in AUTOMATIC1111 instead of the base Stable Diffusion model.</p>

<p>There are many DreamBooth-based models available on websites like <a href="https://oreil.ly/2efOO">Hugging Face</a> and <a href="https://civitai.com">Civitai</a>. To use these models in AUTOMATIC1111, you simply download them and move them into the stable-diffusion-webui/models/Stable-diffusion/ folder. Dreambooth models often have a specific word or token needed for triggering the style or subject, which must be included in the prompt. For example, the <a href="https://oreil.ly/spsy3">Inkpunk Diffusion</a> model <a data-type="indexterm" data-primary="Inkpunk Diffusion" id="id1301"/>requires the word <em>nvinkpunk</em>. Note: the underlying base model here is v1.5 of Stable Diffusion, so reset your image size to 512 × 512.</p>

<p id="nvinkpunk_dreambooth">Input:</p>

<pre data-type="programlisting">skateboarding in Times Square nvinkpunk</pre>

<p><a data-type="xref" href="#figure-9-29">Figure 9-29</a> shows the output.</p>

<figure><div id="figure-9-29" class="figure">
<img src="assets/pega_0929.png" alt="pega 0929" width="512" height="512"/>
<h6><span class="label">Figure 9-29. </span>InkPunk skateboarder</h6>
</div></figure>
<div data-type="tip"><h1>Divide Labor</h1>
<p>The mistake many people make with AI <a data-type="indexterm" data-primary="Divide Labor principle" data-secondary="DreamBooth and" id="id1302"/>is assuming there’s one model to rule them all. In reality there are many creative models out there, and often training on a specific task yields better results than the general foundational models. While the foundation models like Stable Diffusion XL are what most practicioners start with, commonly they begin to experiment with fine-tuning their own models on specific tasks, often based on smaller, more efficient models like v1.5.</p>
</div>

<p>The preferred method for training a DreamBooth model is <a href="https://oreil.ly/AJnnL">Shivam Shrirao’s repository</a>, which uses HuggingFace’s <code>diffusers</code> library. What follows is an explanation of the code in <a href="https://oreil.ly/790FZ">Google Colab</a>. Version 1.5 is used in this notebook, as it is a smaller model, and is able to be trained in a few hours in the Google Colab environment for free. A copy of this Python notebook is saved in the <a href="https://oreil.ly/NzzGm">GitHub repository</a> for this book for posterity, but it should be noted that it will only run on an Nvidia GPU, not on a MacBook.</p>

<p class="pagebreak-before">First the Colab checks whether there is access to an Nvidia GPU. This is one good reason to run Dreambooth on Google Colab, because you are given access to the right resource to run the code without any configuration needed:</p>

<pre data-type="programlisting">!nvidia-smi --query-gpu=name,memory.total, \
    memory.free --format=csv,noheader</pre>

<p>Next the necessary libraries are installed, including the <code>diffusers</code> library from <span class="keep-together">Hugging</span> Face:</p>

<pre data-type="programlisting">!wget -q https://github.com/ShivamShrirao/diffusers/raw/ \
    main/examples/dreambooth/train_dreambooth.py
!wget -q https://github.com/ShivamShrirao/diffusers/raw/ \
    main/scripts/convert_diffusers_to_original_stable_ \
    diffusion.py
%pip install -qq \
git+https://github.com/ShivamShrirao/diffusers
%pip install -q -U --pre triton
%pip install -q accelerate transformers ftfy \
bitsandbytes==0.35.0 gradio natsort safetensors xformers</pre>

<p>Run the next cell to set the output directory of the model when it is finished running. It’s recommended to save the model to Google Drive (even if temporarily) because you can more reliably download large files (4–5 GB) from there than you can from the Google Colab filesystem. Ensure that you have selected the right base model from the Hugging Face hub <code>runwayml/stable-diffusion-v1-5</code> and choose a name for your token for the output directory (usually <em>ukj</em> or <em>zwx</em>; more on this later):</p>

<pre data-type="programlisting" data-code-language="python"><code class="c1">#@markdown If model weights should be saved directly in</code>
<code class="c1">#@markdown google drive (takes around 4-5 GB).</code>
<code class="n">save_to_gdrive</code> <code class="o">=</code> <code class="kc">False</code>
<code class="k">if</code> <code class="n">save_to_gdrive</code><code class="p">:</code>
    <code class="kn">from</code> <code class="nn">google.colab</code> <code class="kn">import</code> <code class="n">drive</code>
    <code class="n">drive</code><code class="o">.</code><code class="n">mount</code><code class="p">(</code><code class="s1">'/content/drive'</code><code class="p">)</code>

<code class="c1">#@markdown Name/Path of the initial model.</code>
<code class="n">MODEL_NAME</code> <code class="o">=</code> <code class="s2">"runwayml/stable-diffusion-v1-5"</code> \
    <code class="c1">#@param {type:"string"}</code>

<code class="c1">#@markdown Enter the directory name to save model at.</code>

<code class="n">OUTPUT_DIR</code> <code class="o">=</code> <code class="s2">"stable_diffusion_weights/ukj"</code> \
    <code class="c1">#@param {type:"string"}</code>
<code class="k">if</code> <code class="n">save_to_gdrive</code><code class="p">:</code>
    <code class="n">OUTPUT_DIR</code> <code class="o">=</code> <code class="s2">"/content/drive/MyDrive/"</code> <code class="o">+</code> <code class="n">OUTPUT_DIR</code>
<code class="k">else</code><code class="p">:</code>
    <code class="n">OUTPUT_DIR</code> <code class="o">=</code> <code class="s2">"/content/"</code> <code class="o">+</code> <code class="n">OUTPUT_DIR</code>

<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"[*] Weights will be saved at </code><code class="si">{</code><code class="n">OUTPUT_DIR</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>

<code class="err">!</code><code class="n">mkdir</code> <code class="o">-</code><code class="n">p</code> <code class="err">$</code><code class="n">OUTPUT_DIR</code></pre>

<p>Before training, you need to add the concepts you want to train on. In our experience, training on multiple concepts tends to harm performance, so typically we would train on only one subject or style. You can merge models later in the Checkpoint Merger tab of AUTOMATIC1111, although this gets into more advanced territory not covered in this book. The instance prompt includes the token you’ll use in your prompt to trigger the model, and ideally it’s a word that doesn’t have any other meaning, like <em>zwx</em> or <em>ukj</em>. The class prompt is a starting point for the training, so if you’re training a model of a specific person, you start from <code>photo of a person</code> to make the training more effective:</p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># You can also add multiple concepts here.</code>
<code class="c1"># Try tweaking `--max_train_steps` accordingly.</code>

<code class="n">concepts_list</code> <code class="o">=</code> <code class="p">[</code>
     <code class="p">{</code>
         <code class="s2">"instance_prompt"</code><code class="p">:</code>      <code class="s2">"photo of ukj person"</code><code class="p">,</code>
         <code class="s2">"class_prompt"</code><code class="p">:</code>         <code class="s2">"photo of a person"</code><code class="p">,</code>
         <code class="s2">"instance_data_dir"</code><code class="p">:</code>    <code class="s2">"/content/data/ukj"</code><code class="p">,</code>
         <code class="s2">"class_data_dir"</code><code class="p">:</code>       <code class="s2">"/content/data/person"</code>
     <code class="p">}</code>
<code class="p">]</code>

<code class="c1"># `class_data_dir` contains regularization images</code>
<code class="kn">import</code> <code class="nn">json</code>
<code class="kn">import</code> <code class="nn">os</code>
<code class="k">for</code> <code class="n">c</code> <code class="ow">in</code> <code class="n">concepts_list</code><code class="p">:</code>
    <code class="n">os</code><code class="o">.</code><code class="n">makedirs</code><code class="p">(</code><code class="n">c</code><code class="p">[</code><code class="s2">"instance_data_dir"</code><code class="p">],</code> <code class="n">exist_ok</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>

<code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="s2">"concepts_list.json"</code><code class="p">,</code> <code class="s2">"w"</code><code class="p">)</code> <code class="k">as</code> <code class="n">f</code><code class="p">:</code>
    <code class="n">json</code><code class="o">.</code><code class="n">dump</code><code class="p">(</code><code class="n">concepts_list</code><code class="p">,</code> <code class="n">f</code><code class="p">,</code> <code class="n">indent</code><code class="o">=</code><code class="mi">4</code><code class="p">)</code></pre>

<p>Next, we upload the images through Google Colab. Dreambooth can work with as few as 5 images, but typically it’s recommended you use about 20–30 images, although some train with hundreds of images. One creative use case is to use the Consistent Characters method discussed in <a data-type="xref" href="ch08.html#standard_image_08">Chapter 8</a> to generate 20 different images of the same AI-generated character and use them to train a Dreambooth model on. Alternatively, you could upload 20 pictures of yourself to create an AI profile photo, or 20 pictures of a product your company sells to generate AI product photography. You can upload the files locally to the <em>instance_data_dir</em> in the Google Colab filesystem (which can be faster)  or run the next cell to get an upload button:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">os</code>
<code class="kn">from</code> <code class="nn">google.colab</code> <code class="kn">import</code> <code class="n">files</code>
<code class="kn">import</code> <code class="nn">shutil</code>

<code class="k">for</code> <code class="n">c</code> <code class="ow">in</code> <code class="n">concepts_list</code><code class="p">:</code>
    <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"""Uploading instance images for</code>
<code class="s2">`</code><code class="si">{</code><code class="n">c</code><code class="p">[</code><code class="s1">'instance_prompt'</code><code class="p">]</code><code class="si">}</code><code class="s2">`"""</code><code class="p">)</code>
    <code class="n">uploaded</code> <code class="o">=</code> <code class="n">files</code><code class="o">.</code><code class="n">upload</code><code class="p">()</code>
    <code class="k">for</code> <code class="n">filename</code> <code class="ow">in</code> <code class="n">uploaded</code><code class="o">.</code><code class="n">keys</code><code class="p">():</code>
        <code class="n">dst_path</code> <code class="o">=</code> <code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">c</code><code class="p">[</code><code class="s1">'instance_data_dir'</code><code class="p">],</code>
            <code class="n">filename</code><code class="p">)</code>
        <code class="n">shutil</code><code class="o">.</code><code class="n">move</code><code class="p">(</code><code class="n">filename</code><code class="p">,</code> <code class="n">dst_path</code><code class="p">)</code></pre>

<p>Now the actual training begins! This code runs on the GPU and outputs the final weights when finished. Make sure to change <code>save_sample_prompt</code> before running to use the token you assigned, in this case <code>photo of ukj person</code>:</p>

<pre data-type="programlisting">!python3 train_dreambooth.py \
  --pretrained_model_name_or_path=$MODEL_NAME \
  --pretrained_vae_name_or_path="stabilityai/sd-vae-ft-mse" \
  --output_dir=$OUTPUT_DIR \
  --revision="fp16" \
  --with_prior_preservation --prior_loss_weight=1.0 \
  --seed=1337 \
  --resolution=512 \
  --train_batch_size=1 \
  --train_text_encoder \
  --mixed_precision="fp16" \
  --use_8bit_adam \
  --gradient_accumulation_steps=1 \
  --learning_rate=1e-6 \
  --lr_scheduler="constant" \
  --lr_warmup_steps=0 \
  --num_class_images=50 \
  --sample_batch_size=4 \
  --max_train_steps=800 \
  --save_interval=10000 \
  --save_sample_prompt="photo of ukj person" \
  --concepts_list="concepts_list.json"</pre>

<p>Now that the training is complete, the next two cells of code define the directory and then display a grid of images so you can see visually whether the model correctly understood your concept and is now capable of generating useful images of your style of subject:</p>

<pre data-type="programlisting">WEIGHTS_DIR = ""
if WEIGHTS_DIR == "":
    from natsort import natsorted
    from glob import glob
    import os
    WEIGHTS_DIR = natsorted(glob(OUTPUT_DIR + os.sep + \
        "*"))[-1]
print(f"[*] WEIGHTS_DIR={WEIGHTS_DIR}")

#@markdown Run to generate a grid of preview images from the last saved weights.
import os
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

weights_folder = OUTPUT_DIR
folders = sorted([f for f in os.listdir(weights_folder) \
    if f != "0"], key=lambda x: int(x))

row = len(folders)
col = len(os.listdir(os.path.join(weights_folder,
    folders[0], "samples")))
scale = 4
fig, axes = plt.subplots(row, col, figsize=(col*scale,
    row*scale), gridspec_kw={'hspace': 0, 'wspace': 0})

for i, folder in enumerate(folders):
    folder_path = os.path.join(weights_folder, folder)
    image_folder = os.path.join(folder_path, "samples")
    images = [f for f in os.listdir(image_folder)]
    for j, image in enumerate(images):
        if row == 1:
            currAxes = axes[j]
        else:
            currAxes = axes[i, j]
        if i == 0:
            currAxes.set_title(f"Image {j}")
        if j == 0:
            currAxes.text(-0.1, 0.5, folder, rotation=0,
            va='center', ha='center',
            transform=currAxes.transAxes)
        image_path = os.path.join(image_folder, image)
        img = mpimg.imread(image_path)
        currAxes.imshow(img, cmap='gray')
        currAxes.axis('off')

plt.tight_layout()
plt.savefig('grid.png', dpi=72)</pre>

<p>Finally, you want to run the conversion process to get a <em>.ckpt</em> file, which is what you will use in AUTOMATIC1111:</p>

<pre data-type="programlisting" data-code-language="python"><code class="c1">#@markdown Run conversion.</code>
<code class="n">ckpt_path</code> <code class="o">=</code> <code class="n">WEIGHTS_DIR</code> <code class="o">+</code> <code class="s2">"/model.ckpt"</code>

<code class="n">half_arg</code> <code class="o">=</code> <code class="s2">""</code>
<code class="c1">#@markdown Convert to fp16, takes half the space (2GB).</code>
<code class="n">fp16</code> <code class="o">=</code> <code class="kc">True</code> <code class="c1">#@param {type: "boolean"}</code>
<code class="k">if</code> <code class="n">fp16</code><code class="p">:</code>
    <code class="n">half_arg</code> <code class="o">=</code> <code class="s2">"--half"</code>
<code class="err">!</code><code class="n">python</code> <code class="n">convert_diffusers_to_original_stable_diffusion</code><code class="o">.</code><code class="n">py</code> \
    <code class="o">--</code><code class="n">model_path</code> <code class="err">$</code><code class="n">WEIGHTS_DIR</code>  <code class="o">--</code><code class="n">checkpoint_path</code> \
    <code class="err">$</code><code class="n">ckpt_path</code> <code class="err">$</code><code class="n">half_arg</code>
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"[*] Converted ckpt saved at </code><code class="si">{</code><code class="n">ckpt_path</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code></pre>

<p>You can then visit the weights directory <em>stable_diffusion_weights/zwx</em> to find the model and download it. If you are having issues downloading such a large file from the Google Colab filesystem, try checking the option to save to Google Drive before running the model, and download from there. We recommend renaming the model before dropping it into your <em>stable-diffusion-webui/models/Stable-diffusion/</em> folder so you can tell what model it is when using it later.</p>

<p id="dreambooth_profile_photo">Input:</p>

<pre data-type="programlisting">a professional headshot of ukj person, standing with his
arms crossed and smiling at the camera with his arms
crossed, a character portrait, Adam Bruce Thomson, private
press, professional photo</pre>

<p><a data-type="xref" href="#figure-9-30">Figure 9-30</a> shows the output.</p>

<figure><div id="figure-9-30" class="figure">
<img src="assets/pega_0930.png" alt="pega 0930" width="600" height="337"/>
<h6><span class="label">Figure 9-30. </span>A Dreambooth model image of one of the authors</h6>
</div></figure>

<p>There is also <a href="https://oreil.ly/xbt2d">an extension</a> for training Dreambooth models via Automatic1111, based on Shivam Shrirao’s method. This extension can be installed in the same way as you installed ControlNet and Segment Anything in previous sections of this chapter. This tool is for advanced users as it exposes a significant number of features and settings for experimentation, many of which you need to be a machine learning expert to understand. To start learning what these parameters and settings mean so you can experiment with different options, check out the <a href="https://oreil.ly/gfdY3">beginner’s guide to training</a> in the extension wiki. The benefit of using this method instead of Google Colab is that it runs locally on your computer, so you can leave it running without worrying it will time out and lose progress.</p>
<div data-type="tip"><h1>Provide Examples</h1>
<p>Dreambooth helps you personalize your <a data-type="indexterm" data-primary="Provide Examples principle" data-secondary="DreamBooth and" id="id1303"/>experience with generative AI. You just need to supply 5–30 images that serve as examples of a concept, and in less than an hour of training time, you can have a fully personalized custom model.</p>
</div>

<p>There are other training and fine-tuning methods available besides Dreambooth, but this technique is currently the most commonly used. An older technique is <a href="https://oreil.ly/GgnJV">Textual Inversion</a>, which doesn’t update the model weights but instead approximates the right location for a token to represent your concept, though this tends to perform far worse than Dreambooth. One promising new technique is LoRA, from the paper “LoRA: Low-Rank Adaptation of Large Language Models” (<a href="https://oreil.ly/NtoiB">Hu et al., 2021</a>), also prevalent in the text-generation space with LLMs. This technique adds new layers into the model and trains just those new layers to build a custom model without expending too many resources. There are also Hypernetworks, which train parameters that can then generate these new layers, as <a href="https://oreil.ly/zFH0-">introduced by Kurumuz</a> in the Medium article “NovelAI Improvements on Stable Diffusion.” Both of these methods are experimental and only make up a small number of the models on Civitai at the time of writing (less than 10%), as well as having in general <a data-type="indexterm" data-primary="Stable Diffusion" data-secondary="DreamBooth and" data-startref="stdfdbo" id="id1304"/><a data-type="indexterm" data-primary="DreamBooth" data-startref="drmbhoo" id="id1305"/>lower user ratings in terms of quality.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Stable Diffusion XL Refiner"><div class="sect1" id="id133">
<h1>Stable Diffusion XL Refiner</h1>

<p>The SDXL v1.0 model has 6.6 billion parameters, <a data-type="indexterm" data-primary="Stable Diffusion" data-secondary="SDXL (Stable Diffusion XL) Refiner" id="sbdfsxdx"/><a data-type="indexterm" data-primary="SDXL (Stable Diffusion XL) refiner" id="sxsdxrf"/>compared to 0.98 billion for the v1.5 model (<a href="https://oreil.ly/vc1zS">Rombach et al., 2023</a>). The increased firepower yields impressive results, and as such the model is starting to win over die-hard 1.5 enthusiasts. Part of the power of SDXL comes from the division of labor between the base model, which sets the global composition, and a refiner model (<a data-type="xref" href="#figure-9-31">Figure 9-31</a>), which adds finer details (optional).</p>

<figure><div id="figure-9-31" class="figure">
<img src="assets/pega_0931.png" alt="pega 0931" width="600" height="155"/>
<h6><span class="label">Figure 9-31. </span>Stable Diffusion XL base and refiner model</h6>
</div></figure>

<p>The underlying language model that infers <a data-type="indexterm" data-primary="OpenClip" id="id1306"/><a data-type="indexterm" data-primary="CLIP ViT-L" id="id1307"/>meaning from your prompts is a combination of OpenClip (ViT-G/14) and OpenAI’s CLIP ViT-L. Stable Diffusion v2 used OpenClip alone and therefore prompts that worked on v1.5 were not as transferable: that problem has been largely solved with SDXL. Additionally, the SDXL model has been trained with a more diverse set of image sizes, leading to better results when you need an image that isn’t the standard square aspect ratio.  <a href="https://oreil.ly/_b7xX">Stablity AI’s research</a> indicates that users overwhelmingly prefer the XL model over v1.5 (<a data-type="xref" href="#figure-9-32">Figure 9-32</a>).</p>

<figure><div id="figure-9-32" class="figure">
<img src="assets/pega_0932.png" alt="pega 0932" width="600" height="450"/>
<h6><span class="label">Figure 9-32. </span>Relative performance preference</h6>
</div></figure>

<p>To make use of the refiner model, you must utilize the “Switch at” functionality in the AUTOMATIC1111 interface. This value controls at which step the pipeline switches to the refiner model. For example, switching at 0.6 with 30 steps means the base model will be used for the first 18 steps, and then it will switch to the refiner model for the final 12 steps (<a data-type="xref" href="#figure-9-33">Figure 9-33</a>).</p>

<figure><div id="figure-9-33" class="figure">
<img src="assets/pega_0933.png" alt="pega 0933" width="600" height="392"/>
<h6><span class="label">Figure 9-33. </span>Refiner—Switch at parameter</h6>
</div></figure>

<p>Common advice is to switch between 0.4 and 1.0 (a value of 1.0 will not switch and only uses the base model), with 20–50 sampling steps for the best results. In our experience, switching at 0.6 with 30 sampling steps produces the highest-quality image, but like all things Stable Diffusion, you must experiment to discover what gets the best results for your image. Setting the refiner to switch at 0.6 gives the output shown in <a data-type="xref" href="#figure-9-35">Figure 9-35</a>.</p>

<p>Input:</p>

<pre data-type="programlisting">anime cat girl with pink hair and a cat ears outfit is posing for a picture
in front of a gaze, photorealistic, 1girl, a character portrait, floral print,
Alice Prin, sots art, official art, sunlight, wavy hair, looking at viewer</pre>

<p>Negative:</p>

<pre data-type="programlisting">disfigured, ugly, bad, immature, photo, amateur, overexposed, underexposed</pre>

<p><a data-type="xref" href="#figure-9-34">Figure 9-34</a> shows the output.</p>

<figure><div id="figure-9-34" class="figure">
<img src="assets/pega_0934.png" alt="pega 0934" width="600" height="220"/>
<h6><span class="label">Figure 9-34. </span>Anime cat girl with SDXL base model versus refiner at 0.6</h6>
</div></figure>
<div data-type="tip"><h1>Divide Labor</h1>
<p>The architecture of SDXL is a perfect example of <a data-type="indexterm" data-primary="Divide Labor principle" data-secondary="Stable Diffusion" id="id1308"/>splitting a task into multiple jobs, and using the right model for the job. The base model sets the scene and guides the composition of the image, while the refiner increases fine detail.</p>
</div>

<p>One quality-of-life modification you can make is to install the aspect ratio selector extension, which can be loaded with image sizes or aspect ratios you use regularly, allowing one-click setting of the correct size and aspect ratio for either model.</p>

<p>To install the extension, browse to the Extensions tab, go to Install from URL, paste in <em><a href="https://github.com/alemelis/sd-webui-ar"><em class="hyperlink">https://github.com/alemelis/sd-webui-ar</em></a></em>, and click Install. Go to the extension folder stable-diffusion-webui/extensions/sd-webui-ar and add the following to the <em>resolutions.txt</em> file (or replace what’s there for cleanliness):</p>

<pre data-type="programlisting">SD1:1, 512, 512 # 1:1 square
XL1:1, 1024, 1024 # 1:1 square
SD3:2, 768, 512 # 3:2 landscape
XL3:2, 1216, 832 # 3:2 landscape
SD9:16, 403, 716 # 9:16 portrait
XL9:16, 768, 1344 # 9:16 portrait</pre>

<p>Clicking one of these preset buttons will automatically adjust the width and height accordingly. You may also replace <em>aspect ratios.txt</em> with the following, allowing you to automatically calculate the aspect ratio based on the height value you have set in the <a data-type="indexterm" data-primary="Stable Diffusion" data-secondary="SDXL (Stable Diffusion XL) Refiner" data-startref="sbdfsxdx" id="id1309"/><a data-type="indexterm" data-primary="SDXL (Stable Diffusion XL) refiner" data-startref="sxsdxrf" id="id1310"/>web UI, and they’ll show in the web UI interface (<a data-type="xref" href="#figure-9-35">Figure 9-35</a>):</p>

<pre data-type="programlisting">Square 1:1, 1.0 # 1:1 ratio based on minimum dimension
Landscape 3:2, 3/2 # Set width based on 3:2 ratio to height
Portrait 9:16, 9/16 # Set width based on 9:16 ratio to height</pre>

<figure><div id="figure-9-35" class="figure">
<img src="assets/pega_0935.png" alt="pega 0935" width="600" height="279"/>
<h6><span class="label">Figure 9-35. </span>Aspect ratios</h6>
</div></figure>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="id359">
<h1>Summary</h1>

<p>In this chapter, you learned advanced techniques for image generation using Stable Diffusion, an open source model. If you followed along, you  successfully installed Stable Diffusion and built an inference pipeline using the HuggingFace <em>diffusers</em> library. You hopefully generated images based on prompts using the Stable Diffusion inference model in Google Colab. Additionally, this chapter recommended exploring the open source community and user interfaces like AUTOMATIC1111 for running Stable Diffusion with advanced features.</p>

<p>The chapter also introduced the concept of ControlNet, which allows for controlling the style of an image using prompting and base images, and Segment Anything, a model for masking specific parts of an image. By applying these techniques, you are now able to customize generated images to meet your specific needs. You also learned about  techniques for personalization, specifically DreamBooth fine-tuning, allowing you to train a model to understand new concepts not encountered in its training data.</p>

<p>In the next chapter, you’ll get the chance to put everything you’ve learned throughout this book into action. We’ll be exploring how to build an AI blog post generator that produces both the blog text and an accompanying image. That final exciting chapter will take you through the process of creating an end-to-end system that generates high-quality blog posts based on user input, complete with custom illustrations in a consistent visual style. You’ll learn how to optimize prompts, generate engaging titles, and create AI-generated images that match your desired style!</p>
</div></section>
</div></section></div></div></body></html>