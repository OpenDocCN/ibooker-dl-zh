- en: 'Chapter 3\. RAG Part II: Chatting with Your Data'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章 RAG第二部分：与数据聊天
- en: In the previous chapter, you learned how to process your data and create and
    store embeddings in a vector store. In this chapter, you’ll learn how to efficiently
    retrieve the most relevant embeddings and chunks of documents based on a user’s
    query. This enables you to construct a prompt that contains relevant documents
    as context, improving the accuracy of the LLM’s final output.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学习了如何处理你的数据，并在向量存储中创建和存储嵌入。在本章中，你将学习如何根据用户的查询高效地检索最相关的嵌入和文档片段。这使你能够构建一个包含相关文档作为上下文的提示，从而提高LLM最终输出的准确性。
- en: This process—which involves embedding a user’s query, retrieving similar documents
    from a data source, and then passing them as context to the prompt sent to the
    LLM—is formally known as *retrieval-augmented generation* (RAG).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程——涉及嵌入用户的查询，从数据源检索相似文档，然后将它们作为上下文传递给发送给LLM的提示——正式称为 *检索增强生成* (RAG)。
- en: RAG is an essential component of building chat-enabled LLM apps that are accurate,
    efficient, and up-to-date. In this chapter, you’ll progress from basics to advanced
    strategies to build an effective RAG system for various data sources (such as
    vector stores and databases) and data structures (structured and unstructured).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 是构建准确、高效且最新的聊天型LLM应用的一个关键组件。在本章中，你将从基础知识进步到高级策略，学习如何为各种数据源（如向量存储和数据库）和数据结构（结构化和非结构化）构建有效的RAG系统。
- en: But first, let’s define RAG and discuss its benefits.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先，让我们定义RAG并讨论其优势。
- en: Introducing Retrieval-Augmented Generation
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍检索增强生成
- en: RAG is a technique used to enhance the accuracy of outputs generated by LLMs
    by providing context from external sources. The term was originally coined in
    a paper by Meta AI researchers who discovered that RAG-enabled models are more
    factual and specific than non-RAG models.^([1](ch03.html#id547))
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: RAG是一种技术，通过提供来自外部来源的上下文来增强LLM生成输出的准确性。这个术语最初是由Meta AI研究人员在一篇论文中提出的，他们发现RAG启用的模型比非RAG模型更具有事实性和具体性.^([1](ch03.html#id547))
- en: 'Without RAG, the LLM relies solely on its pretrained data, which may be outdated.
    For example, let’s ask ChatGPT a question about a current event and see its response:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 没有RAG，LLM完全依赖于其预训练的数据，这些数据可能已经过时。例如，让我们向ChatGPT提出一个关于当前事件的问题，看看它的回答：
- en: '*Input*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*输入*'
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*Output*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出*'
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The response by the LLM is factually incorrect and outdated. The latest winner
    at the time of this book’s publication is Argentina, who won the World Cup in
    2022\. While this example question may be trivial, LLM hallucination can have
    disastrous consequences if its answers are relied upon for fact-checking or important
    decision making.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的响应在事实上是不正确的，并且是过时的。本书出版时最新的赢家是阿根廷，他们在2022年赢得了世界杯。虽然这个例子中的问题可能微不足道，但如果LLM的答案被用于事实核查或重要决策，LLM的幻觉可能会产生灾难性的后果。
- en: 'To combat this problem, we need to provide the LLM with factual, up-to-date
    information from which it can formulate an accurate response. Continuing on from
    the previous example, let’s go over to Wikipedia’s page for the [FIFA World Cup](https://oreil.ly/LpLOV),
    copy the introduction paragraph, and then append it as *context* to our prompt
    to ChatGPT:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们需要向LLM提供事实性、最新的信息，以便它可以据此制定准确的响应。继续上一个例子，让我们转到维基百科的[FIFA世界杯](https://oreil.ly/LpLOV)页面，复制简介段落，然后将它作为
    *上下文* 添加到我们的ChatGPT提示中：
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Note that the last sentence contains the necessary context the LLM can use
    to provide an accurate answer. Here’s the response from the LLM:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，最后一句话包含了LLM可以使用以提供准确答案的必要上下文。以下是LLM的响应：
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Because of the up-to-date additional context provided, the LLM was able to generate
    an accurate response to the prompt. But copying and pasting relevant information
    as context isn’t practical nor scalable for a production AI application. We need
    an automated system to fetch relevant information based on a user’s query, append
    it as context to the prompt, and then execute the generation request to the LLM.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于提供了最新的附加上下文，LLM 能够对提示生成准确的响应。但是，将相关信息作为上下文复制粘贴并不适用于生产AI应用，也不具备可扩展性。我们需要一个自动化的系统，根据用户的查询获取相关信息，将其作为上下文添加到提示中，然后向LLM执行生成请求。
- en: Retrieving Relevant Documents
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检索相关文档
- en: 'A RAG system for an AI app typically follows three core stages:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一个AI应用的RAG系统通常遵循三个核心阶段：
- en: Indexing
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 索引
- en: This stage involves preprocessing the external data source and storing embeddings
    that represent the data in a vector store where they can be easily retrieved.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个阶段涉及预处理外部数据源并将表示数据的嵌入存储在向量存储中，以便可以轻松检索。
- en: Retrieval
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 检索
- en: This stage involves retrieving the relevant embeddings and data stored in the
    vector store based on a user’s query.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这个阶段涉及根据用户的查询检索向量存储中存储的相关嵌入和数据。
- en: Generation
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 生成
- en: This stage involves synthesizing the original prompt with the retrieved relevant
    documents as one final prompt sent to the model for a prediction.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这个阶段涉及将检索到的相关文档与原始提示合并为一个最终的提示，并将其发送到模型进行预测。
- en: The three basic stages look like [Figure 3-1](#ch03_figure_1_1736545666780536).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 三个基本阶段看起来像[图3-1](#ch03_figure_1_1736545666780536)。
- en: '![A diagram of a document  Description automatically generated](assets/lelc_0301.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![文档的示意图  自动生成的描述](assets/lelc_0301.png)'
- en: Figure 3-1\. The key stages of RAG
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-1\. RAG的关键阶段
- en: The indexing stage of this process was covered extensively in [Chapter 2](ch02.html#ch02_rag_part_i_indexing_your_data_1736545662500927),
    where you learned how to use document loaders, text splitters, embeddings, and
    vector stores.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第二章](ch02.html#ch02_rag_part_i_indexing_your_data_1736545662500927)中详细介绍了这个过程的索引阶段，其中你学习了如何使用文档加载器、文本分割器、嵌入和向量存储。
- en: 'Let’s run through an example from scratch again, starting with the indexing
    stage:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次从头开始运行一个例子，从索引阶段开始：
- en: '*Python*'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*JavaScript*'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[Chapter 2](ch02.html#ch02_rag_part_i_indexing_your_data_1736545662500927)
    has more details on the indexing stage.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[第二章](ch02.html#ch02_rag_part_i_indexing_your_data_1736545662500927)中有关于索引阶段的更多细节。'
- en: The indexing stage is now complete. In order to execute the retrieval stage,
    we need to perform similarity search calculations—such as cosine similarity—between
    the user’s query and our stored embeddings, so relevant chunks of our indexed
    document are retrieved (see [Figure 3-2](#ch03_figure_2_1736545666780585)).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 索引阶段现在已完成。为了执行检索阶段，我们需要在用户的查询和存储的嵌入之间执行相似度搜索计算——例如余弦相似度——以检索索引文档中的相关片段（见[图3-2](#ch03_figure_2_1736545666780585)）。
- en: '![Screenshot 2024-02-12 at 1.36.56 PM.png](assets/lelc_0302.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![2024-02-12 上午1:36:56的屏幕截图.png](assets/lelc_0302.png)'
- en: Figure 3-2\. An example flow of indexing documents alongside retrieval of relevant
    documents from a vector store; the Hierarchical Navigable Small World (HNSW) box
    depicts calculating similarity of documents against the user’s query
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-2\. 从向量存储检索相关文档的同时索引文档的示例流程；HNSW（分层可导航小世界）框表示计算文档与用户查询的相似度
- en: '[Figure 3-2](#ch03_figure_2_1736545666780585) illustrates the steps in the
    retrieval process:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-2](#ch03_figure_2_1736545666780585)展示了检索过程中的步骤：'
- en: Convert the user’s query into embeddings.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将用户的查询转换为嵌入。
- en: Calculate the embeddings in the vector store that are most similar to the user’s
    query.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算与用户查询最相似的向量存储中的嵌入。
- en: Retrieve the relevant document embeddings and their corresponding text chunk.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检索相关文档的嵌入及其对应的文本片段。
- en: 'We can represent these steps programmatically using LangChain as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用LangChain以编程方式表示这些步骤如下：
- en: '*Python*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '*JavaScript*'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Note that we are using a vector store method you haven’t seen before: `as_retriever`.
    This function abstracts the logic of embedding the user’s query and the underlying
    similarity search calculations performed by the vector store to retrieve the relevant
    documents.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们使用了一个之前未见过的向量存储方法：`as_retriever`。这个函数抽象了将用户的查询嵌入以及向量存储执行的相关相似度搜索计算的底层逻辑。
- en: 'There is also an argument `k`, which determines the number of relevant documents
    to fetch from the vector store. For example:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 也有一个参数 `k`，它决定了从向量存储中检索相关文档的数量。例如：
- en: '*Python*'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '*JavaScript*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In this example, the argument `k` is specified as 2\. This tells the vector
    store to return the two most relevant documents based on the user’s query.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，参数 `k` 被指定为2。这告诉向量存储根据用户的查询返回两个最相关的文档。
- en: It may seem counterintuitive to use a low `k` value, but retrieving more documents
    is not always better. The more documents are retrieved, the slower your application
    will perform, the larger the prompt (and associated cost of generation) will be,
    and the greater the likelihood of retrieving chunks of text that contain irrelevant
    information, which will cause the LLM to hallucinate.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用低 `k` 值可能看起来不太直观，但检索更多文档并不总是更好的选择。检索的文档越多，你的应用程序运行速度越慢，提示（以及相关的生成成本）越大，检索到包含无关信息的文本片段的可能性也越大，这会导致
    LLM 发生幻觉。
- en: Now that we’ve completed the retrieval stage of the RAG system, let’s move on
    to the final generation stage.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了 RAG 系统的检索阶段，接下来让我们进入最终的生成阶段。
- en: Generating LLM Predictions Using Relevant Documents
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用相关文档生成 LLM 预测
- en: Once we’ve retrieved the relevant documents based on the user’s query, the final
    step is to add them to the original prompt as context and then invoke the model
    to generate a final output ([Figure 3-3](#ch03_figure_3_1736545666780616)).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦根据用户的查询检索到相关文档，下一步就是将它们添加到原始提示中作为上下文，然后调用模型生成最终输出 ([图 3-3](#ch03_figure_3_1736545666780616))
- en: '![A diagram of a diagram  Description automatically generated](assets/lelc_0303.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![一个图表的图表  描述自动生成](assets/lelc_0303.png)'
- en: Figure 3-3\. An example flow demonstrating indexing documents, retrieval of
    relevant documents from a vector store, and inclusion of retrieved documents as
    context in the LLM prompt
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-3\. 一个示例流程，展示了索引文档、从向量存储中检索相关文档，以及将检索到的文档作为 LLM 提示中的上下文
- en: 'Here’s a code example continuing on from our previous example:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个代码示例，延续我们之前的例子：
- en: '*Python*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE10] `"""``)`  `llm` `=` `ChatOpenAI``(``model_name``=``"gpt-3.5-turbo"``,`
    `temperature``=``0``)`  `chain` `=` `prompt` `|` `llm`  `# fetch relevant documents`  `docs`
    `=` `retriever``.``get_relevant_documents``(``"""Who are the key figures in the`   `ancient
    greek history of philosophy?"""``)`  `# run` `chain``.``invoke``({``"context"``:`
    `docs``,``"question"``:` `"""Who are the key figures in the`   `ancient greek
    history of philosophy?"""``})` [PRE11]'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE10] `"""``)`  `llm` `=` `ChatOpenAI``(``model_name``=``"gpt-3.5-turbo"``,`
    `temperature``=``0``)`  `chain` `=` `prompt` `|` `llm`  `# 获取相关文档`  `docs` `=`
    `retriever``.``get_relevant_documents``(``"""Who are the key figures in the`   `ancient
    greek history of philosophy?"""``)`  `# 运行` `chain``.``invoke``({``"context"``:`
    `docs``,``"question"``:` `"""Who are the key figures in the`   `ancient greek
    history of philosophy?"""``})` [PRE11]'
- en: '[PRE12]` [PRE13] *JavaScript*    [PRE14]py    Note the following changes:    *   We
    implement dynamic `context` and `question` variables into our prompt, which allows
    us to define a `ChatPromptTemplate` the model can use to generate a response.           *   We
    define a `ChatOpenAI` interface to act as our LLM. Temperature is set to 0 to
    eliminate the creativity in outputs from the model.           *   We create a
    chain to compose the prompt and LLM. A reminder: the `|` operator (or `pipe` method
    in JS) takes the output of `prompt` and uses it as the input to `llm`.           *   We
    `invoke` the chain passing in the `context` variable (our retrieved relevant docs)
    and the user’s question to generate a final output.              We can encapsulate
    this retrieval logic in a single function:    *Python*    [PRE15]py `"""``)`  `llm`
    `=` `ChatOpenAI``(``model``=``"gpt-3.5-turbo"``,` `temperature``=``0``)`  `@chain`
    `def` `qa``(``input``):`     `# fetch relevant documents`      `docs` `=` `retriever``.``get_relevant_documents``(``input``)`     `#
    format prompt`     `formatted` `=` `prompt``.``invoke``({``"context"``:` `docs``,`
    `"question"``:` `input``})`     `# generate answer`     `answer` `=` `llm``.``invoke``(``formatted``)`     `return`
    `answer`  `# run` `qa``.``invoke``(``"Who are the key figures in the ancient greek
    history of philosophy?"``)` [PRE16]py   [PRE17]`py [PRE18]py[PRE19]``py # Query
    Transformation    One of the major problems with a basic RAG system is that it
    relies too heavily on the quality of a user’s query to generate an accurate output.
    In a production setting, a user is likely to construct their query in an incomplete,
    ambiguous, or poorly worded manner that leads to model hallucination.    *Query
    transformation* is a subset of strategies designed to modify the user’s input
    to answer the first RAG problem question: How do we handle the variability in
    the quality of a user’s input? [Figure 3-5](#ch03_figure_5_1736545666780682) illustrates
    the range of query transformation strategies, ranging from those that make a user’s
    input more or less abstract in order to generate an accurate LLM output. The next
    section begins with a middle ground strategy.  ![A diagram of a question  Description
    automatically generated](assets/lelc_0305.png)  ###### Figure 3-5\. Various methods
    to transform a user’s query based on the abstraction level    ## Rewrite-Retrieve-Read    The
    Rewrite-Retrieve-Read strategy proposed by a Microsoft Research team simply prompts
    the LLM to rewrite the user’s query before performing retrieval.^([2](ch03.html#id572))
    To illustrate, let’s return to the chain we built in the previous section, this
    time invoked with a poorly worded user query:    *Python*    [PRE20]py    *JavaScript*    [PRE21]py    *The
    output* (remember: if you rerun it, your output might be different from this):    [PRE22]py    The
    model failed to answer the question because it was distracted by the irrelevant
    information provided in the user’s query.    Now let’s implement the Rewrite-Retrieve-Read
    prompt:    *Python*    [PRE23]py    *JavaScript*    [PRE24]py    *The output:*    [PRE25]py    Notice
    that we have had an LLM rewrite the user’s initial distracted query into a much
    clearer one, and it is that more focused query that is passed to the retriever
    to fetch the most relevant documents. Note: this technique can be used with any
    retrieval method, be that a vector store such as we have here or, for instance,
    a web search tool. The downside of this approach is that it introduces additional
    latency into your chain, because now we need to perform two LLM calls in sequence.    ##
    Multi-Query Retrieval    A user’s single query can be insufficient to capture
    the full scope of information required to answer the query comprehensively. The
    multi-query retrieval strategy resolves this problem by instructing an LLM to
    generate multiple queries based on a user’s initial query, executing a parallel
    retrieval of each query from the data source and then inserting the retrieved
    results as prompt context to generate a final model output. [Figure 3-6](#ch03_figure_6_1736545666780704)
    illustrates.  ![A diagram of a diagram of a document  Description automatically
    generated with medium confidence](assets/lelc_0306.png)  ###### Figure 3-6\. Demonstration
    of the multi-query retrieval strategy    This strategy is particularly useful
    for use cases where a single question may rely on multiple perspectives to provide
    a comprehensive answer.    Here’s a code example of multi-query retrieval in action:    *Python*    [PRE26]py    *JavaScript*    [PRE27]py    Note
    that the prompt template is designed to generate variations of questions based
    on the user’s initial query.    Next we take the list of generated queries, retrieve
    the most relevant docs for each of them in parallel, and then combine to get the
    unique union of all the retrieved relevant documents:    *Python*    [PRE28]py    *JavaScript*    [PRE29]py    Because
    we’re retrieving documents from the same retriever with multiple (related) queries,
    it’s likely at least some of them are repeated. Before using them as context to
    answer the question, we need to deduplicate them, to end up with a single instance
    of each. Here we dedupe docs by using their content (a string) as the key in a
    dictionary (or object in JS), because a dictionary can only contain one entry
    for each key. After we’ve iterated through all docs, we simply get all the dictionary
    values, which is now free of duplicates.    Notice our use as well of `.batch`,
    which runs all generated queries in parallel and returns a list of the results—in
    this case, a list of lists of documents, which we then flatten and dedupe as described
    earlier.    This final step is to construct a prompt, including the user’s question
    and combined retrieved relevant documents, and a model interface to generate the
    prediction:    *Python*    [PRE30]py `"""``)`  `@chain` `def` `multi_query_qa``(``input``):`     `#
    fetch relevant documents`      `docs` `=` `retrieval_chain``.``invoke``(``input``)`     `#
    format prompt`     `formatted` `=` `prompt``.``invoke``({``"context"``:` `docs``,`
    `"question"``:` `input``})`     `# generate answer`     `answer` `=` `llm``.``invoke``(``formatted``)`     `return`
    `answer`  `# run` `multi_query_qa``.``invoke``(``"""Who are some key figures in
    the ancient greek history`   `of philosophy?"""``)` [PRE31]py   [PRE32]`py[PRE33][PRE34]
    from langchain.prompts import ChatPromptTemplate from langchain_openai import
    ChatOpenAI  prompt_rag_fusion = ChatPromptTemplate.from_template("""You are a
    helpful   assistant that generates multiple search queries based on a single input   query.
    \n `Generate multiple search queries related to:` `{question}` `\n` [PRE35] [PRE36][PRE37][PRE38]
    import {ChatPromptTemplate} from ''@langchain/core/prompts''; import {ChatOpenAI}
    from ''@langchain/openai''; import {RunnableLambda} from ''@langchain/core/runnables'';  const
    perspectivesPrompt = ChatPromptTemplate.fromTemplate(`You are a helpful   assistant
    that generates multiple search queries based on a single input   query. \n  Generate
    multiple search queries related to: {question} \n  Output (4 queries):`)  const
    queryGen = perspectivesPrompt.pipe(llm).pipe(message => {   return message.content.split(''\n'')
    }) [PRE39] def reciprocal_rank_fusion(results: list[list], k=60):     """reciprocal
    rank fusion on multiple lists of ranked documents   and an optional parameter
    k used in the RRF formula  """          # Initialize a dictionary to hold fused
    scores for each document     # Documents will be keyed by their contents to ensure
    uniqueness     fused_scores = {}     documents = {}      # Iterate through each
    list of ranked documents     for docs in results:         # Iterate through each
    document in the list,         # with its rank (position in the list)         for
    rank, doc in enumerate(docs):             # Use the document contents as the key
    for uniqueness             doc_str = doc.page_content             # If the document
    hasn''t been seen yet,             # - initialize score to 0             # - save
    it for later             if doc_str not in fused_scores:                 fused_scores[doc_str]
    = 0                 documents[doc_str] = doc             # Update the score of
    the document using the RRF formula:             # 1 / (rank + k)             fused_scores[doc_str]
    += 1 / (rank + k)      # Sort the documents based on their fused scores in descending
    order      # to get the final reranked results     reranked_doc_strs = sorted(         fused_scores,
    key=lambda d: fused_scores[d], reverse=True     )     # retrieve the corresponding
    doc for each doc_str     return [         documents[doc_str]         for doc_str
    in reranked_doc_strs     ]  retrieval_chain = generate_queries | retriever.batch
    | reciprocal_rank_fusion [PRE40] function reciprocalRankFusion(results, k = 60)
    {   // Initialize a dictionary to hold fused scores for each document   // Documents
    will be keyed by their contents to ensure uniqueness   const fusedScores = {}   const
    documents = {}    results.forEach(docs => {     docs.forEach((doc, rank) => {       //
    Use the document contents as the key for uniqueness       const key = doc.pageContent       //
    If the document hasn''t been seen yet,       // - initialize score to 0       //
    - save it for later       if (!(key in fusedScores)) {         fusedScores[key]
    = 0         documents[key] = 0       }       // Update the score of the document
    using the RRF formula:       // 1 / (rank + k)       fusedScores[key] += 1 / (rank
    + k)     })   })    // Sort the documents based on their fused scores in descending
    order    // to get the final reranked results   const sorted = Object.entries(fusedScores).sort((a,
    b) => b[1] - a[1])   // retrieve the corresponding doc for each key   return sorted.map(([key])
    => documents[key]) }  const retrievalChain = queryGen   .pipe(retriever.batch.bind(retriever))   .pipe(reciprocalRankFusion)
    [PRE41] prompt = ChatPromptTemplate.from_template("""Answer the following question
    based   on this context:  {context} `Question:` `{question}` [PRE42] [PRE43]``
    [PRE44] const rewritePrompt = ChatPromptTemplate.fromTemplate(`Answer the following   question
    based on this context:  {context}  Question: {question} `)  const llm = new ChatOpenAI({temperature:
    0})  const multiQueryQa = RunnableLambda.from(async input => {   // fetch relevant
    documents   const docs = await retrievalChain.invoke(input)   // format prompt   const
    formatted = await prompt.invoke({context: docs, question: input})   // generate
    answer   const answer = await llm.invoke(formatted)   return answer })  await
    multiQueryQa.invoke(`Who are some key figures in the ancient greek   history of
    philosophy?`) [PRE45]` [PRE46][PRE47][PRE48][PRE49][PRE50] from langchain.prompts
    import ChatPromptTemplate from langchain_core.output_parsers import StrOutputParser
    from langchain_openai import ChatOpenAI  prompt_hyde = ChatPromptTemplate.from_template("""Please
    write a passage to   answer the question.\n Question: {question} \n Passage:""")  generate_doc
    = (     prompt_hyde | ChatOpenAI(temperature=0) | StrOutputParser()  ) [PRE51]
    import {ChatOpenAI} from ''@langchain/openai'' import {ChatPromptTemplate} from
    ''@langchain/core/prompts'' import {RunnableLambda} from ''@langchain/core/runnables'';  const
    prompt = ChatPromptTemplate.fromTemplate(`Please write a passage to   answer the
    question Question: {question} Passage:`)  const llm = new ChatOpenAI({temperature:
    0})  const generateDoc = prompt.pipe(llm).pipe(msg => msg.content) [PRE52] retrieval_chain
    `=` generate_doc `|` retriever  [PRE53] const retrievalChain = generateDoc.pipe(retriever)
    [PRE54] prompt = ChatPromptTemplate.from_template("""Answer the following question
    based   on this context:  {context} `Question:` `{question}` [PRE55] [PRE56]``
    [PRE57] const prompt = ChatPromptTemplate.fromTemplate(`Answer the following   question
    based on this context:  {context}  Question: {question} `)  const llm = new ChatOpenAI({temperature:
    0})  const qa = RunnableLambda.from(async input => {   // fetch relevant documents
    from the hyde retrieval chain defined earlier   const docs = await retrievalChain.invoke(input)   //
    format prompt   const formatted = await prompt.invoke({context: docs, question:
    input})   // generate answer   const answer = await llm.invoke(formatted)   return
    answer })  await qa.invoke(`Who are some key figures in the ancient greek history
    of   philosophy?`) [PRE58]` [PRE59][PRE60][PRE61][PRE62][PRE63]  [PRE64]`py[PRE65]````'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
