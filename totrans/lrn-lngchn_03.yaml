- en: 'Chapter 3\. RAG Part II: Chatting with Your Data'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章\. RAG第二部分：与数据对话
- en: In the previous chapter, you learned how to process your data and create and
    store embeddings in a vector store. In this chapter, you’ll learn how to efficiently
    retrieve the most relevant embeddings and chunks of documents based on a user’s
    query. This enables you to construct a prompt that contains relevant documents
    as context, improving the accuracy of the LLM’s final output.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学习了如何处理数据，并在向量存储中创建和存储嵌入。在本章中，你将学习如何根据用户的查询高效地检索最相关的嵌入和文档片段。这使你能够构建一个包含相关文档作为背景的提示，从而提高LLM最终输出的准确性。
- en: This process—which involves embedding a user’s query, retrieving similar documents
    from a data source, and then passing them as context to the prompt sent to the
    LLM—is formally known as *retrieval-augmented generation* (RAG).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程——涉及嵌入用户的查询，从数据源检索相似文档，然后将它们作为背景传递给发送给LLM的提示——正式上被称为 *检索增强生成* (RAG)。
- en: RAG is an essential component of building chat-enabled LLM apps that are accurate,
    efficient, and up-to-date. In this chapter, you’ll progress from basics to advanced
    strategies to build an effective RAG system for various data sources (such as
    vector stores and databases) and data structures (structured and unstructured).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: RAG是构建准确、高效且最新的聊天功能LLM应用的一个关键组件。在本章中，你将从基础知识进步到高级策略，以构建针对各种数据源（如向量存储和数据库）和数据结构（结构化和非结构化）的有效RAG系统。
- en: But first, let’s define RAG and discuss its benefits.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先，让我们定义RAG并讨论其优势。
- en: Introducing Retrieval-Augmented Generation
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍检索增强生成
- en: RAG is a technique used to enhance the accuracy of outputs generated by LLMs
    by providing context from external sources. The term was originally coined in
    a paper by Meta AI researchers who discovered that RAG-enabled models are more
    factual and specific than non-RAG models.^([1](ch03.html#id547))
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: RAG是一种技术，通过提供外部来源的上下文来增强LLM生成输出的准确性。这个术语最初是由Meta AI研究人员在一篇论文中提出的，他们发现启用RAG的模型比非RAG模型更具事实性和具体性.^([1](ch03.html#id547))
- en: 'Without RAG, the LLM relies solely on its pretrained data, which may be outdated.
    For example, let’s ask ChatGPT a question about a current event and see its response:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 没有RAG，LLM完全依赖于其预训练的数据，这些数据可能已经过时。例如，让我们向ChatGPT提出一个关于当前事件的问题，看看它的响应：
- en: '*Input*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*输入*'
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*Output*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出*'
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The response by the LLM is factually incorrect and outdated. The latest winner
    at the time of this book’s publication is Argentina, who won the World Cup in
    2022\. While this example question may be trivial, LLM hallucination can have
    disastrous consequences if its answers are relied upon for fact-checking or important
    decision making.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的响应在事实上是不正确的，并且过时。在本书出版时，最新的冠军是阿根廷，他们在2022年赢得了世界杯。虽然这个例子中的问题可能微不足道，但如果LLM的答案被用于事实核查或重要决策，LLM的幻觉可能会产生灾难性的后果。
- en: 'To combat this problem, we need to provide the LLM with factual, up-to-date
    information from which it can formulate an accurate response. Continuing on from
    the previous example, let’s go over to Wikipedia’s page for the [FIFA World Cup](https://oreil.ly/LpLOV),
    copy the introduction paragraph, and then append it as *context* to our prompt
    to ChatGPT:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们需要向LLM提供事实性、最新的信息，以便它能据此形成准确的响应。继续上一个例子，让我们转到维基百科的 [FIFA世界杯](https://oreil.ly/LpLOV)
    页面，复制简介段落，然后将它作为 *背景* 添加到我们的ChatGPT提示中：
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Note that the last sentence contains the necessary context the LLM can use
    to provide an accurate answer. Here’s the response from the LLM:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到最后一句包含了LLM可以使用以提供准确答案的必要上下文。以下是LLM的响应：
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Because of the up-to-date additional context provided, the LLM was able to generate
    an accurate response to the prompt. But copying and pasting relevant information
    as context isn’t practical nor scalable for a production AI application. We need
    an automated system to fetch relevant information based on a user’s query, append
    it as context to the prompt, and then execute the generation request to the LLM.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于提供了最新的附加背景信息，LLM 能够对提示生成准确的响应。但是，将相关信息作为背景复制粘贴并不适用于生产AI应用，也不具备可扩展性。我们需要一个自动化的系统，根据用户的查询获取相关信息，将其作为背景添加到提示中，然后向LLM执行生成请求。
- en: Retrieving Relevant Documents
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检索相关文档
- en: 'A RAG system for an AI app typically follows three core stages:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一个AI应用的RAG系统通常遵循三个核心阶段：
- en: Indexing
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 索引
- en: This stage involves preprocessing the external data source and storing embeddings
    that represent the data in a vector store where they can be easily retrieved.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 此阶段涉及预处理外部数据源并将表示数据的嵌入存储在向量存储中，以便可以轻松检索。
- en: Retrieval
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 检索
- en: This stage involves retrieving the relevant embeddings and data stored in the
    vector store based on a user’s query.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 此阶段涉及根据用户查询从向量存储中检索相关嵌入和数据。
- en: Generation
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 生成
- en: This stage involves synthesizing the original prompt with the retrieved relevant
    documents as one final prompt sent to the model for a prediction.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 此阶段涉及将检索到的相关文档与原始提示合并为一个最终提示，并将其发送到模型进行预测。
- en: The three basic stages look like [Figure 3-1](#ch03_figure_1_1736545666780536).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 三个基本阶段看起来像[图 3-1](#ch03_figure_1_1736545666780536)。
- en: '![A diagram of a document  Description automatically generated](assets/lelc_0301.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![文档的示意图  自动生成的描述](assets/lelc_0301.png)'
- en: Figure 3-1\. The key stages of RAG
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-1\. RAG 的关键阶段
- en: The indexing stage of this process was covered extensively in [Chapter 2](ch02.html#ch02_rag_part_i_indexing_your_data_1736545662500927),
    where you learned how to use document loaders, text splitters, embeddings, and
    vector stores.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此过程的索引阶段在[第 2 章](ch02.html#ch02_rag_part_i_indexing_your_data_1736545662500927)中进行了详细说明，其中你学习了如何使用文档加载器、文本分割器、嵌入和向量存储。
- en: 'Let’s run through an example from scratch again, starting with the indexing
    stage:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次从头开始运行一个示例，从索引阶段开始：
- en: '*Python*'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*JavaScript*'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[Chapter 2](ch02.html#ch02_rag_part_i_indexing_your_data_1736545662500927)
    has more details on the indexing stage.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[第 2 章](ch02.html#ch02_rag_part_i_indexing_your_data_1736545662500927)有更多关于索引阶段的详细信息。'
- en: The indexing stage is now complete. In order to execute the retrieval stage,
    we need to perform similarity search calculations—such as cosine similarity—between
    the user’s query and our stored embeddings, so relevant chunks of our indexed
    document are retrieved (see [Figure 3-2](#ch03_figure_2_1736545666780585)).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 索引阶段现已完成。为了执行检索阶段，我们需要在用户查询和我们的存储嵌入之间执行相似度搜索计算（例如余弦相似度），以便检索我们索引文档的相关片段（参见[图
    3-2](#ch03_figure_2_1736545666780585)）。
- en: '![Screenshot 2024-02-12 at 1.36.56 PM.png](assets/lelc_0302.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![2024-02-12 下午 1:36:56 的屏幕截图.png](assets/lelc_0302.png)'
- en: Figure 3-2\. An example flow of indexing documents alongside retrieval of relevant
    documents from a vector store; the Hierarchical Navigable Small World (HNSW) box
    depicts calculating similarity of documents against the user’s query
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-2\. 从向量存储中检索相关文档的同时索引文档的示例流程；分层可导航小世界（HNSW）框表示计算文档与用户查询的相似度
- en: '[Figure 3-2](#ch03_figure_2_1736545666780585) illustrates the steps in the
    retrieval process:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-2](#ch03_figure_2_1736545666780585)说明了检索过程中的步骤：'
- en: Convert the user’s query into embeddings.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将用户查询转换为嵌入。
- en: Calculate the embeddings in the vector store that are most similar to the user’s
    query.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算与用户查询最相似的向量存储中的嵌入。
- en: Retrieve the relevant document embeddings and their corresponding text chunk.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检索相关文档嵌入及其对应的文本片段。
- en: 'We can represent these steps programmatically using LangChain as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 LangChain 以编程方式表示这些步骤，如下所示：
- en: '*Python*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '*JavaScript*'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Note that we are using a vector store method you haven’t seen before: `as_retriever`.
    This function abstracts the logic of embedding the user’s query and the underlying
    similarity search calculations performed by the vector store to retrieve the relevant
    documents.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们正在使用一个你之前没有见过的向量存储方法：`as_retriever`。此函数抽象了将用户查询嵌入以及向量存储执行的相关文档检索的底层相似度搜索计算的逻辑。
- en: 'There is also an argument `k`, which determines the number of relevant documents
    to fetch from the vector store. For example:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个参数 `k`，它决定了从向量存储中检索的相关文档数量。例如：
- en: '*Python*'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '*JavaScript*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In this example, the argument `k` is specified as 2\. This tells the vector
    store to return the two most relevant documents based on the user’s query.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，参数 `k` 被指定为 2。这告诉向量存储根据用户查询返回两个最相关的文档。
- en: It may seem counterintuitive to use a low `k` value, but retrieving more documents
    is not always better. The more documents are retrieved, the slower your application
    will perform, the larger the prompt (and associated cost of generation) will be,
    and the greater the likelihood of retrieving chunks of text that contain irrelevant
    information, which will cause the LLM to hallucinate.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用低`k`值可能看起来有些反直觉，但检索更多文档并不总是更好的。检索到的文档越多，你的应用程序性能越慢，提示（以及相关的生成成本）越大，检索到包含不相关信息文本块的可能性也越大，这会导致LLM产生幻觉。
- en: Now that we’ve completed the retrieval stage of the RAG system, let’s move on
    to the final generation stage.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了RAG系统的检索阶段，让我们继续到最后生成阶段。
- en: Generating LLM Predictions Using Relevant Documents
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用相关文档生成LLM预测
- en: Once we’ve retrieved the relevant documents based on the user’s query, the final
    step is to add them to the original prompt as context and then invoke the model
    to generate a final output ([Figure 3-3](#ch03_figure_3_1736545666780616)).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们根据用户的查询检索到相关文档，下一步就是将它们添加到原始提示中作为上下文，然后调用模型生成最终输出([图3-3](#ch03_figure_3_1736545666780616))。
- en: '![A diagram of a diagram  Description automatically generated](assets/lelc_0303.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![一个图表的图表 描述自动生成](assets/lelc_0303.png)'
- en: Figure 3-3\. An example flow demonstrating indexing documents, retrieval of
    relevant documents from a vector store, and inclusion of retrieved documents as
    context in the LLM prompt
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-3\. 一个示例流程，展示了索引文档、从向量存储中检索相关文档，并将检索到的文档作为上下文包含在LLM提示中
- en: 'Here’s a code example continuing on from our previous example:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个代码示例，继续我们之前的例子：
- en: '*Python*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '*JavaScript*'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Note the following changes:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 注意以下更改：
- en: We implement dynamic `context` and `question` variables into our prompt, which
    allows us to define a `ChatPromptTemplate` the model can use to generate a response.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将动态的`context`和`question`变量实现到我们的提示中，这使得我们可以定义一个模型可以使用的`ChatPromptTemplate`来生成响应。
- en: We define a `ChatOpenAI` interface to act as our LLM. Temperature is set to
    0 to eliminate the creativity in outputs from the model.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们定义了一个`ChatOpenAI`接口来作为我们的LLM。温度设置为0以消除模型输出中的创造性。
- en: 'We create a chain to compose the prompt and LLM. A reminder: the `|` operator
    (or `pipe` method in JS) takes the output of `prompt` and uses it as the input
    to `llm`.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们创建一个链来组合提示和LLM。提醒：`|`运算符（或在JS中的`pipe`方法）将`prompt`的输出用作`llm`的输入。
- en: We `invoke` the chain passing in the `context` variable (our retrieved relevant
    docs) and the user’s question to generate a final output.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们`调用`链，传入`context`变量（我们检索到的相关文档）和用户的问题来生成最终输出。
- en: 'We can encapsulate this retrieval logic in a single function:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个检索逻辑封装在一个单独的函数中：
- en: '*Python*'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '*JavaScript*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE13]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Notice how we now have a new runnable `qa` function that can be called with
    just a question and takes care to first fetch the relevant docs for context, format
    them into the prompt, and finally generate the answer. In the Python code, the
    `@chain` decorator turns the function into a runnable chain. This notion of encapsulating
    multiple steps into a single function will be key to building interesting apps
    with LLMs.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们现在有一个新的可运行的`qa`函数，只需一个问题和调用它，它会首先获取相关的文档作为上下文，将它们格式化为提示，并最终生成答案。在Python代码中，`@chain`装饰器将函数转换为可运行的链。将多个步骤封装到单个函数中的这种概念将是构建有趣LLM应用的关键。
- en: 'You can also return the retrieved documents for further inspection:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以返回检索到的文档以供进一步检查：
- en: '*Python*'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '*JavaScript*'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE15]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Congratulations! You’ve now built a basic RAG system to power an AI app for
    personal use.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你现在已经构建了一个基本的RAG系统，用于为个人使用提供AI应用。
- en: 'However, a production-ready AI app used by multiple users requires a more advanced
    RAG system. In order to build a robust RAG system, we need to answer the following
    questions effectively:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一个由多个用户使用的生产就绪AI应用需要一个更高级的RAG系统。为了构建一个健壮的RAG系统，我们需要有效地回答以下问题：
- en: How do we handle the variability in the quality of a user’s input?
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何处理用户输入质量的变化？
- en: How do we route queries to retrieve relevant data from a variety of data sources?
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何路由查询以从各种数据源检索相关数据？
- en: How do we transform natural language to the query language of the target data
    source?
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何将自然语言转换为目标数据源的查询语言？
- en: How do we optimize our indexing process, i.e., embedding, text splitting?
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何优化我们的索引过程，即嵌入、文本分割？
- en: Next we’ll discuss the latest research-backed strategies to answer these questions
    and build a production-ready RAG system. These strategies can be summarized in
    [Figure 3-4](#ch03_figure_4_1736545666780656).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论最新的基于研究策略来回答这些问题并构建一个生产就绪的RAG系统。这些策略可以总结为[图3-4](#ch03_figure_4_1736545666780656)。
- en: '![A diagram of a software development process  Description automatically generated](assets/lelc_0304.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![软件开发流程图  自动生成的描述](assets/lelc_0304.png)'
- en: Figure 3-4\. Effective strategies to optimize the accuracy of your RAG system
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-4\. 优化RAG系统准确性的有效策略
- en: Note
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: All code blocks in the rest of this chapter use the vector store we set up at
    the beginning of the chapter.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 本章剩余部分的所有代码块都使用我们在本章开头设置的向量存储。
- en: Query Transformation
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查询转换
- en: One of the major problems with a basic RAG system is that it relies too heavily
    on the quality of a user’s query to generate an accurate output. In a production
    setting, a user is likely to construct their query in an incomplete, ambiguous,
    or poorly worded manner that leads to model hallucination.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 基本RAG系统的一个主要问题是它过于依赖用户查询的质量来生成准确的输出。在生产环境中，用户很可能会以不完整、含糊或措辞不当的方式构建他们的查询，这会导致模型产生幻觉。
- en: '*Query transformation* is a subset of strategies designed to modify the user’s
    input to answer the first RAG problem question: How do we handle the variability
    in the quality of a user’s input? [Figure 3-5](#ch03_figure_5_1736545666780682)
    illustrates the range of query transformation strategies, ranging from those that
    make a user’s input more or less abstract in order to generate an accurate LLM
    output. The next section begins with a middle ground strategy.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*查询转换*是策略的子集，旨在修改用户的输入以回答第一个RAG问题：我们如何处理用户输入质量的变化？[图3-5](#ch03_figure_5_1736545666780682)展示了查询转换策略的范围，从那些使用户的输入更抽象或更具体以生成准确的LLM输出的策略。下一节将从一种中间策略开始。'
- en: '![A diagram of a question  Description automatically generated](assets/lelc_0305.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![问题图  自动生成的描述](assets/lelc_0305.png)'
- en: Figure 3-5\. Various methods to transform a user’s query based on the abstraction
    level
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-5\. 基于抽象级别转换用户查询的各种方法
- en: Rewrite-Retrieve-Read
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重写-检索-阅读
- en: 'The Rewrite-Retrieve-Read strategy proposed by a Microsoft Research team simply
    prompts the LLM to rewrite the user’s query before performing retrieval.^([2](ch03.html#id572))
    To illustrate, let’s return to the chain we built in the previous section, this
    time invoked with a poorly worded user query:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 微软研究团队提出的重写-检索-阅读策略简单地将用户的查询重写，然后再进行检索。[2](ch03.html#id572) 为了说明这一点，让我们回到上一节中构建的链，这次使用一个措辞不当的用户查询：
- en: '*Python*'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE16]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '*JavaScript*'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE17]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '*The output* (remember: if you rerun it, your output might be different from
    this):'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出*（记住：如果你重新运行它，你的输出可能与这个不同）：'
- en: '[PRE18]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The model failed to answer the question because it was distracted by the irrelevant
    information provided in the user’s query.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 模型未能回答问题，因为它被用户查询中提供的不相关信息所分心。
- en: 'Now let’s implement the Rewrite-Retrieve-Read prompt:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来实现重写-检索-阅读提示：
- en: '*Python*'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE19]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '*JavaScript*'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE20]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '*The output:*'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出：*'
- en: '[PRE21]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Notice that we have had an LLM rewrite the user’s initial distracted query
    into a much clearer one, and it is that more focused query that is passed to the
    retriever to fetch the most relevant documents. Note: this technique can be used
    with any retrieval method, be that a vector store such as we have here or, for
    instance, a web search tool. The downside of this approach is that it introduces
    additional latency into your chain, because now we need to perform two LLM calls
    in sequence.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到我们让LLM将用户的初始分心查询重写为一个更清晰的查询，并且是那个更集中的查询被传递给检索器以获取最相关的文档。注意：这项技术可以与任何检索方法一起使用，无论是我们这里使用的向量存储，还是例如网络搜索工具。这种方法的缺点是它将额外的延迟引入到你的链中，因为现在我们需要连续进行两次LLM调用。
- en: Multi-Query Retrieval
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多查询检索
- en: A user’s single query can be insufficient to capture the full scope of information
    required to answer the query comprehensively. The multi-query retrieval strategy
    resolves this problem by instructing an LLM to generate multiple queries based
    on a user’s initial query, executing a parallel retrieval of each query from the
    data source and then inserting the retrieved results as prompt context to generate
    a final model output. [Figure 3-6](#ch03_figure_6_1736545666780704) illustrates.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 一个用户的一个查询可能不足以捕捉回答查询所需的所有信息范围。多查询检索策略通过指示LLM根据用户的初始查询生成多个查询来解决此问题，从数据源并行检索每个查询，然后将检索到的结果作为提示上下文来生成最终的模型输出。[图3-6](#ch03_figure_6_1736545666780704)说明了这一点。
- en: '![A diagram of a diagram of a document  Description automatically generated
    with medium confidence](assets/lelc_0306.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![文档的图示  描述由中等置信度自动生成](assets/lelc_0306.png)'
- en: Figure 3-6\. Demonstration of the multi-query retrieval strategy
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-6\. 多查询检索策略的演示
- en: This strategy is particularly useful for use cases where a single question may
    rely on multiple perspectives to provide a comprehensive answer.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这种策略特别适用于那些一个问题可能需要多个视角来提供全面答案的场景。
- en: 'Here’s a code example of multi-query retrieval in action:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个多查询检索操作的代码示例：
- en: '*Python*'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE22]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '*JavaScript*'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE23]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Note that the prompt template is designed to generate variations of questions
    based on the user’s initial query.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 注意提示模板是设计用来根据用户的初始查询生成问题变体的。
- en: 'Next we take the list of generated queries, retrieve the most relevant docs
    for each of them in parallel, and then combine to get the unique union of all
    the retrieved relevant documents:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们取生成的查询列表，并行检索每个查询的最相关文档，然后将它们组合以获得所有检索到的相关文档的唯一并集：
- en: '*Python*'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE24]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '*JavaScript*'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE25]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Because we’re retrieving documents from the same retriever with multiple (related)
    queries, it’s likely at least some of them are repeated. Before using them as
    context to answer the question, we need to deduplicate them, to end up with a
    single instance of each. Here we dedupe docs by using their content (a string)
    as the key in a dictionary (or object in JS), because a dictionary can only contain
    one entry for each key. After we’ve iterated through all docs, we simply get all
    the dictionary values, which is now free of duplicates.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们从同一个检索器中检索多个（相关）查询，因此很可能其中至少有一些是重复的。在使用它们作为回答问题的上下文之前，我们需要去重，以确保每个实例都是唯一的。在这里，我们通过使用它们的内容（一个字符串）作为字典（或JS中的对象）中的键来进行去重，因为字典只能为每个键包含一个条目。在迭代完所有文档后，我们只需简单地获取所有字典值，现在这些值已经没有重复了。
- en: Notice our use as well of `.batch`, which runs all generated queries in parallel
    and returns a list of the results—in this case, a list of lists of documents,
    which we then flatten and dedupe as described earlier.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们也使用了`.batch`，它并行运行所有生成的查询，并返回一个结果列表——在这种情况下，是一个文档列表的列表，然后我们将其展平并去重，如前所述。
- en: 'This final step is to construct a prompt, including the user’s question and
    combined retrieved relevant documents, and a model interface to generate the prediction:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是构建一个提示，包括用户的问题和组合检索到的相关文档，以及一个模型接口来生成预测：
- en: '*Python*'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE26]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '*JavaScript*'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE27]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Notice how this isn’t that different from our previous QA chains, as all the
    new logic for multi-query retrieval is contained in `retrieval_chain`. This is
    key to making good use of these techniques—implementing each technique as a standalone
    chain (in this case, `retrieval_chain`), which makes it easy to adopt them and
    even to combine them.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这并不与我们的先前的QA链有很大不同，因为所有新的多查询检索逻辑都包含在`retrieval_chain`中。这是有效利用这些技术的关键——将每个技术实现为一个独立的链（在这种情况下，`retrieval_chain`），这使得它们易于采用，甚至可以组合使用。
- en: RAG-Fusion
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RAG-Fusion
- en: The RAG-Fusion strategy shares similarities with the multi-query retrieval strategy,
    except we will apply a final reranking step to all the retrieved documents.^([3](ch03.html#id582))
    This reranking step makes use of the *reciprocal rank fusion* (RRF) algorithm,
    which involves combining the ranks of different search results to produce a single,
    unified ranking. By combining ranks from different queries, we pull the most relevant
    documents to the top of the final list. RRF is well-suited for combining results
    from queries that might have different scales or distributions of scores.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: RAG-Fusion策略与多查询检索策略相似，但我们将对所有检索到的文档应用一个最终的重新排序步骤.^([3](ch03.html#id582)) 这个重新排序步骤使用了*互逆排名融合*（RRF）算法，该算法涉及结合不同搜索结果的排名以产生单一的、统一的排名。通过结合来自不同查询的排名，我们将最相关的文档拉到最终列表的顶部。RRF非常适合结合可能具有不同规模或分数分布的查询的结果。
- en: 'Let’s demonstrate RAG-Fusion in code. First, we craft a prompt similar to the
    multi-query retrieval strategy to generate a list of queries based on the user
    query:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过代码演示RAG-Fusion。首先，我们创建一个与多查询检索策略相似的提示，以根据用户查询生成查询列表：
- en: '*Python*'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE28]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '*JavaScript*'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE29]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Once we’ve generated our queries, we fetch relevant documents for each query
    and pass them into a function to *rerank* (that is, *reorder* according to relevancy)
    the final list of relevant documents.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们生成了我们的查询，我们就为每个查询检索相关文档，并将它们传递给一个函数以*重新排序*（即根据相关性重新排序）最终的文档列表。
- en: The function `reciprocal_rank_fusion` takes a list of the search results of
    each query, so a list of lists of documents, where each inner list of documents
    is sorted by their relevance to that query. The RRF algorithm then calculates
    a new score for each document based on its ranks (or positions) in the different
    lists and sorts them to create a final reranked list.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`reciprocal_rank_fusion`函数接受每个查询的搜索结果列表，即文档列表的列表，其中每个内部文档列表按其与该查询的相关性排序。然后，RRF算法根据文档在不同列表中的排名（或位置）计算每个文档的新分数，并对其进行排序以创建最终的重新排序列表。'
- en: 'After calculating the fused scores, the function sorts the documents in descending
    order of these scores to get the final reranked list, which is then returned:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算融合分数后，该函数按这些分数的降序对文档进行排序，以获得最终的重新排序列表，然后将其返回：
- en: '*Python*'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE30]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '*JavaScript*'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE31]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Notice that the function also takes a `k` parameter, which determines how much
    influence documents in each query’s result sets have over the final list of documents.
    A higher value indicates that lower-ranked documents have more influence.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，该函数还接受一个`k`参数，该参数决定了每个查询结果集中文档对最终文档列表的影响程度。值越高表示排名较低的文档对最终列表的影响越大。
- en: 'Finally, we combine our new retrieval chain (now using RRF) with the full chain
    we’ve seen before:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将我们新的检索链（现在使用RRF）与之前看到的完整链结合：
- en: '*Python*'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE32]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '*JavaScript*'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE33]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: RAG-Fusion’s strength lies in its ability to capture the user’s intended expression,
    navigate complex queries, and broaden the scope of retrieved documents, enabling
    serendipitous discovery.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: RAG-Fusion的优势在于其捕捉用户意图表达、导航复杂查询和扩展检索文档范围的能力，从而实现偶然发现。
- en: Hypothetical Document Embeddings
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 假设文档嵌入
- en: '*Hypothetical Document Embeddings* (HyDE) is a strategy that involves creating
    a hypothetical document based on the user’s query, embedding the document, and
    retrieving relevant documents based on vector similarity.^([4](ch03.html#id592))
    The intuition behind HyDE is that an LLM-generated hypothetical document will
    be more similar to the most relevant documents than the original query, as shown
    in [Figure 3-7](#ch03_figure_7_1736545666780724).'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '*假设文档嵌入*（HyDE）是一种策略，涉及根据用户查询创建一个假设文档，嵌入该文档，并根据向量相似性检索相关文档.^([4](ch03.html#id592))
    HyDE背后的直觉是，由LLM生成的假设文档将比原始查询更接近最相关的文档，如图3-7所示。'
- en: '![Screenshot 2024-02-12 at 1.12.45 PM.png](assets/lelc_0307.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![2024-02-12 下午1:12:45的屏幕截图](assets/lelc_0307.png)'
- en: Figure 3-7\. An illustration of HyDE closer in the vector space to the document
    embeddings than the plain query embeddings
  id: totrans-163
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-7\. HyDE在向量空间中比普通查询嵌入更接近文档嵌入的示意图
- en: 'First, define a prompt to generate a hypothetical document:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，定义一个提示以生成一个假设文档：
- en: '*Python*'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE34]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '*JavaScript*'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE35]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Next, we take the hypothetical document and use it as input to the `retriever`,
    which will generate its embedding and search for similar documents in the vector
    store:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将假设文档作为输入传递给`retriever`，它将生成其嵌入并搜索向量存储中的相似文档：
- en: '*Python*'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE36]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '*JavaScript*'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE37]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Finally, we take the retrieved documents, pass them as context to the final
    prompt, and instruct the model to generate an output:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们获取检索到的文档，将它们作为上下文传递给最终提示，并指示模型生成输出：
- en: '*Python*'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE38]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '*JavaScript*'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE39]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'To recap what we covered in this section, query transformation consists of
    taking the user’s original query and doing the following:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回顾本节中我们所学的内容，查询转换包括以下步骤：获取用户的原始查询并执行以下操作：
- en: Rewriting into one or more queries
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将查询重写为一个或多个查询
- en: Combining the results of those queries into a single set of the most relevant
    results
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将这些查询的结果合并成一个包含最相关结果的单一集合
- en: 'Rewriting the query can take many forms, but it’s usually done in a similar
    fashion: take the user’s original query—a prompt you wrote—and ask an LLM to write
    a new query or queries. Some examples of typical changes made are:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 查询重写可以采取多种形式，但通常是以类似的方式进行的：获取用户的原始查询——您编写的提示——并要求LLM编写新的查询或查询。一些典型的更改示例包括：
- en: Removing irrelevant/unrelated text from the query.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从查询中删除无关或不相关的文本。
- en: Grounding the query with past conversation history. For instance, to make sense
    of a query such as *and what about in LA,* we need to combine it with a hypothetical
    past question about the weather in SF, to arrive at a useful query such as *weather
    in LA*.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将查询与过去的对话历史联系起来。例如，为了理解像*关于洛杉矶怎么样*这样的查询，我们需要将其与关于旧金山天气的假设性问题结合起来，以得到一个有用的查询，如*洛杉矶的天气*。
- en: Casting a wider net for relevant documents by also fetching documents for related
    queries.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过也检索相关查询的文档来扩大检索相关文档的范围。
- en: Decomposing a complex question into multiple, simpler questions and then including
    results for all of them in the final prompt to generate an answer.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将复杂的问题分解成多个更简单的问题，然后在最终提示中包含所有这些问题的结果以生成答案。
- en: The right rewriting strategy to use will depend on your use case.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 应用的正确重写策略将取决于您的用例。
- en: 'Now that we’ve covered the main query transformation strategies, let’s discuss
    the second major question to answer in order to build a robust RAG system: How
    do we route queries to retrieve relevant data from multiple data sources?'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了主要的查询转换策略，让我们讨论构建一个健壮的RAG系统时需要回答的第二个主要问题：我们如何路由查询以从多个数据源检索相关数据？
- en: Query Routing
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查询路由
- en: Although using a single vector store is useful, the required data may live in
    a variety of data sources, including relational databases or other vector stores.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然使用单个向量存储很有用，但所需的数据可能存在于各种数据源中，包括关系数据库或其他向量存储。
- en: 'For example, you may have two vector stores: one for LangChain Python documentation
    and another for LangChain JS documentation. Given a user’s question, we would
    like to *route* the query to the appropriate inferred data source to retrieve
    relevant docs. *Query routing* is a strategy used to forward a user’s query to
    the relevant data source.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您可能有两个向量存储：一个用于LangChain Python文档，另一个用于LangChain JS文档。给定一个用户的问题，我们希望将查询*路由*到适当推断的数据源以检索相关文档。*查询路由*是将用户的查询转发到相关数据源的一种策略。
- en: Logical Routing
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑路由
- en: In *logical routing*, we give the LLM knowledge of the various data sources
    at our disposal and then let the LLM reason which data source to apply based on
    the user’s query, as shown in [Figure 3-8](#ch03_figure_8_1736545666780744).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在*逻辑路由*中，我们向LLM提供我们可用的各种数据源的知识，然后让LLM根据用户的查询推理出应用哪个数据源，如图[图3-8](#ch03_figure_8_1736545666780744)所示。
- en: '![A diagram of a brain  Description automatically generated](assets/lelc_0308.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![大脑的示意图  描述由自动生成](assets/lelc_0308.png)'
- en: Figure 3-8\. Query routing to relevant data sources
  id: totrans-195
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-8\. 查询路由到相关数据源
- en: 'In order to achieve this, we make use of function-calling models like GPT-3.5
    Turbo to help classify each query into one of the available routes. A *function
    call* involves defining a schema that the model can use to generate arguments
    of a function based on the query. This enables us to generate structured outputs
    that can be used to run other functions. The following Python code defines the
    schema for our router based on three docs for different languages:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们使用像GPT-3.5 Turbo这样的函数调用模型来帮助将每个查询分类到可用的路由之一。*函数调用*涉及定义一个模型可以使用的模式，该模式可以根据查询生成函数的参数。这使得我们能够生成可以用于运行其他函数的结构化输出。以下Python代码定义了基于三种不同语言的三个文档的路由器模式：
- en: '*Python*'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE40]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '*JavaScript*'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE41]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now we invoke the LLM to extract the data source based on the predefined schema:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们调用LLM根据预定义的模式提取数据源：
- en: '*Python*'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE42]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '*JavaScript*'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE43]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '*The output:*'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出：*'
- en: '[PRE44]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Notice how the LLM produced JSON output, conforming to the schema we defined
    earlier. This will be useful in many other tasks.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 注意LLM如何生成符合我们之前定义的模式的JSON输出。这将在许多其他任务中非常有用。
- en: 'Once we’ve extracted the relevant data source, we can pass the value into another
    function to execute additional logic as required:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们提取了相关的数据源，我们就可以将值传递给另一个函数以执行所需的附加逻辑：
- en: '*Python*'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE45]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '*JavaScript*'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE46]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Notice how we don’t do an exact string comparison but instead first turn the
    generated output to lowercase, and then do a substring match. This makes our chain
    more resilient to the LLM going off script and producing output that doesn’t quite
    conform to the schema we asked for.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们并没有进行精确的字符串比较，而是首先将生成的输出转换为小写，然后进行子字符串匹配。这使得我们的链对LLM偏离脚本并产生不完全符合我们请求的模式输出具有更强的鲁棒性。
- en: Tip
  id: totrans-215
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Resilience to the random nature of LLM outputs is an important theme to keep
    in mind when building your LLM applications.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 对LLM输出随机性的鲁棒性是构建LLM应用程序时需要牢记的重要主题。
- en: Logical routing is most suitable when you have a defined list of data sources
    from which relevant data can be retrieved and utilized by the LLM to generate
    an accurate output. These can range from vector stores to databases and even APIs.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑路由最适合当你有一个定义明确的数据源列表，可以从其中检索相关数据，并由LLM利用以生成准确输出时。这些可以是从向量存储到数据库甚至API的范围。
- en: Semantic Routing
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语义路由
- en: Unlike logical routing, *semantic routing* involves embedding various prompts
    that represent various data sources alongside the user’s query and then performing
    vector similarity search to retrieve the most similar prompt. [Figure 3-9](#ch03_figure_9_1736545666780765)
    illustrates.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 与逻辑路由不同，*语义路由*涉及将代表各种数据源的提示嵌入到用户的查询中，然后执行向量相似度搜索以检索最相似的提示。[图3-9](#ch03_figure_9_1736545666780765)展示了这一点。
- en: '![A diagram of a diagram of a notepad and a brain  Description automatically
    generated](assets/lelc_0309.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![一个笔记本和大脑的示意图  自动生成的描述](assets/lelc_0309.png)'
- en: Figure 3-9\. Semantic routing to improve the accuracy of retrieved documents
  id: totrans-221
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-9\. 语义路由以提高检索文档的准确性
- en: 'The following is an example of semantic routing:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个语义路由的示例：
- en: '*Python*'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE47]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '*JavaScript*'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE48]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Now that you’ve seen how to route a user’s query to the relevant data source,
    let’s discuss the third major question when building a robust RAG system: “How
    do we transform natural language to the query language of the target data source?”'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经看到了如何将用户的查询路由到相关的数据源，让我们讨论构建健壮RAG系统时的第三个主要问题：“我们如何将自然语言转换为目标数据源的查询语言？”
- en: Query Construction
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查询构建
- en: As discussed earlier, RAG is an effective strategy to embed and retrieve relevant
    unstructured data from a vector store based on a query. But most data available
    for use in production apps is structured and typically stored in relational databases.
    In addition, unstructured data embedded in a vector store also contains structured
    metadata that possesses important information.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，RAG是一种有效的策略，可以根据查询从基于向量的存储中嵌入和检索相关的非结构化数据。但大多数可用于生产应用程序的数据是结构化的，通常存储在关系数据库中。此外，嵌入在向量存储中的非结构化数据还包含具有重要信息的结构化元数据。
- en: '*Query construction* is the process of transforming a natural language query
    into the query language of the database or data source you are interacting with.
    See [Figure 3-10](#ch03_figure_10_1736545666780785).'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '*查询构建*是将自然语言查询转换为与您交互的数据库或数据源的查询语言的过程。参见[图3-10](#ch03_figure_10_1736545666780785)。'
- en: '![A diagram of data types  Description automatically generated](assets/lelc_0310.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![数据类型的示意图  自动生成的描述](assets/lelc_0310.png)'
- en: Figure 3-10\. Illustration of query languages for various data sources
  id: totrans-232
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-10\. 各种数据源的查询语言示意图
- en: 'For example, consider the query: *What* *are movies about aliens in the year
    1980?* This question contains an unstructured topic that can be retrieved via
    embeddings (*aliens*), but it also contains potential structured components (*year
    == 1980*).'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑以下查询：*1980年关于外星人的电影是什么？*这个问题包含一个可以通过嵌入（*外星人*）检索的非结构化主题，但它还包含潜在的有序组件（*年份
    == 1980*）。
- en: The following sections dive deeper into the various forms of query construction.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 以下章节将更深入地探讨查询构建的各种形式。
- en: Text-to-Metadata Filter
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本到元数据过滤器
- en: Most vector stores provide the ability to limit your vector search based on
    metadata. During the embedding process, we can attach metadata key-value pairs
    to vectors in an index and then later specify filter expressions when you query
    the index.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数向量存储都提供了基于元数据限制向量搜索的能力。在嵌入过程中，我们可以将元数据键值对附加到索引中的向量上，然后在查询索引时指定过滤器表达式。
- en: 'LangChain provides a `SelfQueryRetriever` that abstracts this logic and makes
    it easier to translate natural language queries into structured queries for various
    data sources. The self-querying utilizes an LLM to extract and execute the relevant
    metadata filters based on a user’s query and predefined metadata schema:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 提供了一个 `SelfQueryRetriever`，它抽象了这种逻辑，并使得将自然语言查询转换为针对各种数据源的格式化查询变得更加容易。自查询利用一个
    LLM 根据用户的查询和预定义的元数据模式提取和执行相关的元数据过滤器：
- en: '*Python*'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE49]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '*JavaScript*'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE50]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'This results in a retriever that will take a user query, and split it into:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致检索器将用户查询分成：
- en: A filter to apply on the metadata of each document first
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先应用于每个文档元数据的过滤器
- en: A query to use for semantic search on the documents
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于在文档上进行语义搜索的查询
- en: 'To do this, we have to describe which fields the metadata of our documents
    contain; that description will be included in the prompt. The retriever will then
    do the following:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，我们必须描述我们的文档元数据包含哪些字段；该描述将包含在提示中。检索器接下来将执行以下操作：
- en: Send the query generation prompt to the LLM.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将查询生成提示发送到 LLM。
- en: Parse metadata filter and rewritten search query from the LLM output.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解析来自 LLM 输出的元数据过滤器和重写的搜索查询。
- en: Convert the metadata filter generated by the LLM to the format appropriate for
    our vector store.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 LLM 生成的元数据过滤器转换为适合我们向量存储的格式。
- en: Issue a similarity search against the vector store, filtered to only match documents
    whose metadata passes the generated filter.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对向量存储执行相似度搜索，仅匹配元数据通过生成的过滤器的文档。
- en: Text-to-SQL
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本到 SQL
- en: SQL and relational databases are important sources of structured data, but they
    don’t interact directly with natural language. Although we can simply use the
    LLM to translate a user’s query to SQL queries, there is little margin for error.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: SQL 和关系数据库是结构化数据的重要来源，但它们不直接与自然语言交互。尽管我们可以简单地使用 LLM 将用户的查询翻译成 SQL 查询，但错误的空间很小。
- en: 'Here are some useful strategies for effective text to SQL translations:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些有效的文本到 SQL 转换策略：
- en: Database description
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库描述
- en: 'To ground SQL queries, an LLM must be provided with an accurate description
    of the database. One common text-to-SQL prompt employs an idea reported in this
    paper and others: provide the LLM with a `CREATE TABLE` description for each table,
    including column names and types.^([5](ch03.html#id622)) We can also provide a
    few (for instance, three) example rows from the table.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使 SQL 查询具体化，LLM 必须提供数据库的准确描述。一种常见的文本到 SQL 提示采用本文和其他论文中报道的想法：为每个表提供 `CREATE
    TABLE` 描述，包括列名和类型.^([5](ch03.html#id622)) 我们还可以提供一些（例如，三个）表中的示例行。
- en: Few-shot examples
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本示例
- en: Feeding the prompt with few-shot examples of question-query matches can improve
    the query generation accuracy. This can be achieved by simply appending standard
    static examples in the prompt to guide the agent on how it should build queries
    based on questions.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提供问题-查询匹配的少样本示例来填充提示，可以提高查询生成的准确性。这可以通过在提示中简单地附加标准静态示例来实现，以指导代理如何根据问题构建查询。
- en: See [Figure 3-11](#ch03_figure_11_1736545666780807) for a visual of the process.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 见 [图 3-11](#ch03_figure_11_1736545666780807) 以了解过程的视觉表示。
- en: '![A close-up of a machine  Description automatically generated](assets/lelc_0311.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![机器的特写，描述自动生成](assets/lelc_0311.png)'
- en: Figure 3-11\. A user’s query transformed to a SQL query
  id: totrans-259
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-11\. 将用户查询转换为 SQL 查询
- en: 'Here’s a full code example:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个完整的代码示例：
- en: '*Python*'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE51]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '*JavaScript*'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE52]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We first convert the user’s query to a SQL query appropriate to the dialect
    of our database. Then we execute that query on our database. Note that executing
    arbitrary SQL queries on your database generated by an LLM from user input is
    dangerous in a production application. To use these ideas in production, you need
    to consider a number of security measures to reduce the risk of unintended queries
    being run in your database. Here are some examples:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将用户的查询转换为适合我们数据库方言的SQL查询。然后我们在数据库上执行该查询。请注意，在生产应用程序中，由LLM从用户输入生成的任意SQL查询在数据库上执行是危险的。要在生产中使用这些想法，你需要考虑一系列安全措施以降低意外查询在数据库中运行的风险。以下是一些示例：
- en: Run the queries on your database with a user with read-only permissions.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用具有只读权限的用户在你的数据库上运行查询。
- en: The database user running the queries should have access only to the tables
    you wish to make available for querying.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行查询的数据库用户应只能访问你希望用于查询的表。
- en: Add a time-out to the queries run by this application; this would ensure that
    even if an expensive query is generated, it is canceled before taking up too many
    of your database resources.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为此应用程序运行的查询添加超时；这将确保即使生成昂贵的查询，它也会在占用太多数据库资源之前被取消。
- en: This is not an exhaustive list of security considerations. Security of LLM applications
    is an area currently in development, with more security measures being added to
    the recommendations as new vulnerabilities are discovered.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是安全考虑的完整列表。LLM应用的安全是一个目前处于开发中的领域，随着新漏洞的发现，更多的安全措施被添加到建议中。
- en: Summary
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter discussed various state-of-the-art strategies to efficiently retrieve
    the most relevant documents based on a user’s query and synthesize them with your
    prompt to aid the LLM to generate an accurate, up-to-date output.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了各种最先进策略，以高效地根据用户的查询检索最相关的文档，并使用你的提示将它们综合起来，以帮助LLM生成准确、最新的输出。
- en: As discussed, a robust, production-ready RAG system requires a wide range of
    effective strategies that can execute query transformation, query construction,
    routing, and indexing optimization.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，一个健壮、生产就绪的RAG系统需要一系列有效的策略，这些策略可以执行查询转换、查询构建、路由和索引优化。
- en: Query transformation enables your AI app to transform an ambiguous or malformed
    user query into a representative query that’s optimal for retrieval. Query construction
    enables your AI app to convert the user’s query into the syntax of the query language
    of the database or data source where structured data lives. Routing enables your
    AI app to dynamically route the user’s query to retrieve relevant information
    from the relevant data source.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 查询转换使你的AI应用能够将模糊或格式错误的用户查询转换为代表性查询，该查询对检索是最优的。查询构建使你的AI应用能够将用户的查询转换为存储结构化数据的数据库或数据源的查询语言语法。路由使你的AI应用能够动态地将用户的查询路由到相关数据源以检索相关信息。
- en: In [Chapter 4](ch04.html#ch04_using_langgraph_to_add_memory_to_your_chatbot_1736545668266431),
    we’ll build on this knowledge to add memory to your AI chatbot so that it can
    remember and learn from each interaction. This will enable users to “chat” with
    the application in multiturn conversations like ChatGPT.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](ch04.html#ch04_using_langgraph_to_add_memory_to_your_chatbot_1736545668266431)中，我们将在此基础上构建知识，为你的AI聊天机器人添加记忆功能，使其能够记住并从每次交互中学习。这将使用户能够像ChatGPT一样进行多轮对话。
- en: ^([1](ch03.html#id547-marker)) Patrick Lewis et al., [“Retrieval-Augmented Generation
    for Knowledge-Intensive NLP Tasks”](https://oreil.ly/Qzd2K), arXiv, April 12,
    2021\.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch03.html#id547-marker)) Patrick Lewis等人，[“检索增强生成用于知识密集型NLP任务”](https://oreil.ly/Qzd2K)，arXiv，2021年4月12日。
- en: ^([2](ch03.html#id572-marker)) Xinbei Ma et al., [“Query Rewriting for Retrieval-Augmented
    Large Language Models”](https://oreil.ly/zyw5E), arXiv, October 23, 2023\. Research
    commissioned by Microsoft Research Asia.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch03.html#id572-marker)) Xinbei Ma等人，[“检索增强大型语言模型的查询重写”](https://oreil.ly/zyw5E)，arXiv，2023年10月23日。由微软亚洲研究院资助的研究。
- en: '^([3](ch03.html#id582-marker)) Zackary Rackauckas, [“RAG-Fusion: A New Take
    on Retrieval-Augmented Generation”](https://oreil.ly/k7TTY), arXiv, February 21,
    2024\. From the *International Journal on Natural Language Computing*, vol. 13,
    no. 1 (February 2024).'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch03.html#id582-marker)) Zackary Rackauckas，[“RAG-Fusion：检索增强生成的新视角”](https://oreil.ly/k7TTY)，arXiv，2024年2月21日。来自《国际自然语言计算杂志》，第13卷，第1期（2024年2月）。
- en: ^([4](ch03.html#id592-marker)) Luyu Gao et al., [“Precise Zero-Shot Dense Retrieval
    Without Relevance Labels”](https://oreil.ly/7aTnS), arXiv, December 20, 2022\.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch03.html#id592-marker)) 高露宇等，[“无相关性标签的精确零样本密集检索”](https://oreil.ly/7aTnS)，arXiv，2022年12月20日。
- en: ^([5](ch03.html#id622-marker)) Nitarshan Rajkumar et al., [“Evaluating the Text-to-SQL
    Capabilities of Large Language Models”](https://oreil.ly/WOrzt), arXiv, March
    15, 2022\.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch03.html#id622-marker)) 拉吉库马尔·尼塔沙恩等，[“评估大型语言模型的文本到SQL能力”](https://oreil.ly/WOrzt)，arXiv，2022年3月15日。
