- en: 'Chapter 3\. RAG Part II: Chatting with Your Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you learned how to process your data and create and
    store embeddings in a vector store. In this chapter, you’ll learn how to efficiently
    retrieve the most relevant embeddings and chunks of documents based on a user’s
    query. This enables you to construct a prompt that contains relevant documents
    as context, improving the accuracy of the LLM’s final output.
  prefs: []
  type: TYPE_NORMAL
- en: This process—which involves embedding a user’s query, retrieving similar documents
    from a data source, and then passing them as context to the prompt sent to the
    LLM—is formally known as *retrieval-augmented generation* (RAG).
  prefs: []
  type: TYPE_NORMAL
- en: RAG is an essential component of building chat-enabled LLM apps that are accurate,
    efficient, and up-to-date. In this chapter, you’ll progress from basics to advanced
    strategies to build an effective RAG system for various data sources (such as
    vector stores and databases) and data structures (structured and unstructured).
  prefs: []
  type: TYPE_NORMAL
- en: But first, let’s define RAG and discuss its benefits.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Retrieval-Augmented Generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RAG is a technique used to enhance the accuracy of outputs generated by LLMs
    by providing context from external sources. The term was originally coined in
    a paper by Meta AI researchers who discovered that RAG-enabled models are more
    factual and specific than non-RAG models.^([1](ch03.html#id547))
  prefs: []
  type: TYPE_NORMAL
- en: 'Without RAG, the LLM relies solely on its pretrained data, which may be outdated.
    For example, let’s ask ChatGPT a question about a current event and see its response:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Input*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*Output*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The response by the LLM is factually incorrect and outdated. The latest winner
    at the time of this book’s publication is Argentina, who won the World Cup in
    2022\. While this example question may be trivial, LLM hallucination can have
    disastrous consequences if its answers are relied upon for fact-checking or important
    decision making.
  prefs: []
  type: TYPE_NORMAL
- en: 'To combat this problem, we need to provide the LLM with factual, up-to-date
    information from which it can formulate an accurate response. Continuing on from
    the previous example, let’s go over to Wikipedia’s page for the [FIFA World Cup](https://oreil.ly/LpLOV),
    copy the introduction paragraph, and then append it as *context* to our prompt
    to ChatGPT:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the last sentence contains the necessary context the LLM can use
    to provide an accurate answer. Here’s the response from the LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Because of the up-to-date additional context provided, the LLM was able to generate
    an accurate response to the prompt. But copying and pasting relevant information
    as context isn’t practical nor scalable for a production AI application. We need
    an automated system to fetch relevant information based on a user’s query, append
    it as context to the prompt, and then execute the generation request to the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieving Relevant Documents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A RAG system for an AI app typically follows three core stages:'
  prefs: []
  type: TYPE_NORMAL
- en: Indexing
  prefs: []
  type: TYPE_NORMAL
- en: This stage involves preprocessing the external data source and storing embeddings
    that represent the data in a vector store where they can be easily retrieved.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval
  prefs: []
  type: TYPE_NORMAL
- en: This stage involves retrieving the relevant embeddings and data stored in the
    vector store based on a user’s query.
  prefs: []
  type: TYPE_NORMAL
- en: Generation
  prefs: []
  type: TYPE_NORMAL
- en: This stage involves synthesizing the original prompt with the retrieved relevant
    documents as one final prompt sent to the model for a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: The three basic stages look like [Figure 3-1](#ch03_figure_1_1736545666780536).
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a document  Description automatically generated](assets/lelc_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. The key stages of RAG
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The indexing stage of this process was covered extensively in [Chapter 2](ch02.html#ch02_rag_part_i_indexing_your_data_1736545662500927),
    where you learned how to use document loaders, text splitters, embeddings, and
    vector stores.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s run through an example from scratch again, starting with the indexing
    stage:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[Chapter 2](ch02.html#ch02_rag_part_i_indexing_your_data_1736545662500927)
    has more details on the indexing stage.'
  prefs: []
  type: TYPE_NORMAL
- en: The indexing stage is now complete. In order to execute the retrieval stage,
    we need to perform similarity search calculations—such as cosine similarity—between
    the user’s query and our stored embeddings, so relevant chunks of our indexed
    document are retrieved (see [Figure 3-2](#ch03_figure_2_1736545666780585)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Screenshot 2024-02-12 at 1.36.56 PM.png](assets/lelc_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. An example flow of indexing documents alongside retrieval of relevant
    documents from a vector store; the Hierarchical Navigable Small World (HNSW) box
    depicts calculating similarity of documents against the user’s query
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 3-2](#ch03_figure_2_1736545666780585) illustrates the steps in the
    retrieval process:'
  prefs: []
  type: TYPE_NORMAL
- en: Convert the user’s query into embeddings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the embeddings in the vector store that are most similar to the user’s
    query.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Retrieve the relevant document embeddings and their corresponding text chunk.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can represent these steps programmatically using LangChain as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we are using a vector store method you haven’t seen before: `as_retriever`.
    This function abstracts the logic of embedding the user’s query and the underlying
    similarity search calculations performed by the vector store to retrieve the relevant
    documents.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is also an argument `k`, which determines the number of relevant documents
    to fetch from the vector store. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the argument `k` is specified as 2\. This tells the vector
    store to return the two most relevant documents based on the user’s query.
  prefs: []
  type: TYPE_NORMAL
- en: It may seem counterintuitive to use a low `k` value, but retrieving more documents
    is not always better. The more documents are retrieved, the slower your application
    will perform, the larger the prompt (and associated cost of generation) will be,
    and the greater the likelihood of retrieving chunks of text that contain irrelevant
    information, which will cause the LLM to hallucinate.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve completed the retrieval stage of the RAG system, let’s move on
    to the final generation stage.
  prefs: []
  type: TYPE_NORMAL
- en: Generating LLM Predictions Using Relevant Documents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once we’ve retrieved the relevant documents based on the user’s query, the final
    step is to add them to the original prompt as context and then invoke the model
    to generate a final output ([Figure 3-3](#ch03_figure_3_1736545666780616)).
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a diagram  Description automatically generated](assets/lelc_0303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. An example flow demonstrating indexing documents, retrieval of
    relevant documents from a vector store, and inclusion of retrieved documents as
    context in the LLM prompt
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here’s a code example continuing on from our previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10] `"""``)`  `llm` `=` `ChatOpenAI``(``model_name``=``"gpt-3.5-turbo"``,`
    `temperature``=``0``)`  `chain` `=` `prompt` `|` `llm`  `# fetch relevant documents`  `docs`
    `=` `retriever``.``get_relevant_documents``(``"""Who are the key figures in the`   `ancient
    greek history of philosophy?"""``)`  `# run` `chain``.``invoke``({``"context"``:`
    `docs``,``"question"``:` `"""Who are the key figures in the`   `ancient greek
    history of philosophy?"""``})` [PRE11]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]` [PRE13] *JavaScript*    [PRE14]py    Note the following changes:    *   We
    implement dynamic `context` and `question` variables into our prompt, which allows
    us to define a `ChatPromptTemplate` the model can use to generate a response.           *   We
    define a `ChatOpenAI` interface to act as our LLM. Temperature is set to 0 to
    eliminate the creativity in outputs from the model.           *   We create a
    chain to compose the prompt and LLM. A reminder: the `|` operator (or `pipe` method
    in JS) takes the output of `prompt` and uses it as the input to `llm`.           *   We
    `invoke` the chain passing in the `context` variable (our retrieved relevant docs)
    and the user’s question to generate a final output.              We can encapsulate
    this retrieval logic in a single function:    *Python*    [PRE15]py `"""``)`  `llm`
    `=` `ChatOpenAI``(``model``=``"gpt-3.5-turbo"``,` `temperature``=``0``)`  `@chain`
    `def` `qa``(``input``):`     `# fetch relevant documents`      `docs` `=` `retriever``.``get_relevant_documents``(``input``)`     `#
    format prompt`     `formatted` `=` `prompt``.``invoke``({``"context"``:` `docs``,`
    `"question"``:` `input``})`     `# generate answer`     `answer` `=` `llm``.``invoke``(``formatted``)`     `return`
    `answer`  `# run` `qa``.``invoke``(``"Who are the key figures in the ancient greek
    history of philosophy?"``)` [PRE16]py   [PRE17]`py [PRE18]py[PRE19]``py # Query
    Transformation    One of the major problems with a basic RAG system is that it
    relies too heavily on the quality of a user’s query to generate an accurate output.
    In a production setting, a user is likely to construct their query in an incomplete,
    ambiguous, or poorly worded manner that leads to model hallucination.    *Query
    transformation* is a subset of strategies designed to modify the user’s input
    to answer the first RAG problem question: How do we handle the variability in
    the quality of a user’s input? [Figure 3-5](#ch03_figure_5_1736545666780682) illustrates
    the range of query transformation strategies, ranging from those that make a user’s
    input more or less abstract in order to generate an accurate LLM output. The next
    section begins with a middle ground strategy.  ![A diagram of a question  Description
    automatically generated](assets/lelc_0305.png)  ###### Figure 3-5\. Various methods
    to transform a user’s query based on the abstraction level    ## Rewrite-Retrieve-Read    The
    Rewrite-Retrieve-Read strategy proposed by a Microsoft Research team simply prompts
    the LLM to rewrite the user’s query before performing retrieval.^([2](ch03.html#id572))
    To illustrate, let’s return to the chain we built in the previous section, this
    time invoked with a poorly worded user query:    *Python*    [PRE20]py    *JavaScript*    [PRE21]py    *The
    output* (remember: if you rerun it, your output might be different from this):    [PRE22]py    The
    model failed to answer the question because it was distracted by the irrelevant
    information provided in the user’s query.    Now let’s implement the Rewrite-Retrieve-Read
    prompt:    *Python*    [PRE23]py    *JavaScript*    [PRE24]py    *The output:*    [PRE25]py    Notice
    that we have had an LLM rewrite the user’s initial distracted query into a much
    clearer one, and it is that more focused query that is passed to the retriever
    to fetch the most relevant documents. Note: this technique can be used with any
    retrieval method, be that a vector store such as we have here or, for instance,
    a web search tool. The downside of this approach is that it introduces additional
    latency into your chain, because now we need to perform two LLM calls in sequence.    ##
    Multi-Query Retrieval    A user’s single query can be insufficient to capture
    the full scope of information required to answer the query comprehensively. The
    multi-query retrieval strategy resolves this problem by instructing an LLM to
    generate multiple queries based on a user’s initial query, executing a parallel
    retrieval of each query from the data source and then inserting the retrieved
    results as prompt context to generate a final model output. [Figure 3-6](#ch03_figure_6_1736545666780704)
    illustrates.  ![A diagram of a diagram of a document  Description automatically
    generated with medium confidence](assets/lelc_0306.png)  ###### Figure 3-6\. Demonstration
    of the multi-query retrieval strategy    This strategy is particularly useful
    for use cases where a single question may rely on multiple perspectives to provide
    a comprehensive answer.    Here’s a code example of multi-query retrieval in action:    *Python*    [PRE26]py    *JavaScript*    [PRE27]py    Note
    that the prompt template is designed to generate variations of questions based
    on the user’s initial query.    Next we take the list of generated queries, retrieve
    the most relevant docs for each of them in parallel, and then combine to get the
    unique union of all the retrieved relevant documents:    *Python*    [PRE28]py    *JavaScript*    [PRE29]py    Because
    we’re retrieving documents from the same retriever with multiple (related) queries,
    it’s likely at least some of them are repeated. Before using them as context to
    answer the question, we need to deduplicate them, to end up with a single instance
    of each. Here we dedupe docs by using their content (a string) as the key in a
    dictionary (or object in JS), because a dictionary can only contain one entry
    for each key. After we’ve iterated through all docs, we simply get all the dictionary
    values, which is now free of duplicates.    Notice our use as well of `.batch`,
    which runs all generated queries in parallel and returns a list of the results—in
    this case, a list of lists of documents, which we then flatten and dedupe as described
    earlier.    This final step is to construct a prompt, including the user’s question
    and combined retrieved relevant documents, and a model interface to generate the
    prediction:    *Python*    [PRE30]py `"""``)`  `@chain` `def` `multi_query_qa``(``input``):`     `#
    fetch relevant documents`      `docs` `=` `retrieval_chain``.``invoke``(``input``)`     `#
    format prompt`     `formatted` `=` `prompt``.``invoke``({``"context"``:` `docs``,`
    `"question"``:` `input``})`     `# generate answer`     `answer` `=` `llm``.``invoke``(``formatted``)`     `return`
    `answer`  `# run` `multi_query_qa``.``invoke``(``"""Who are some key figures in
    the ancient greek history`   `of philosophy?"""``)` [PRE31]py   [PRE32]`py[PRE33][PRE34]
    from langchain.prompts import ChatPromptTemplate from langchain_openai import
    ChatOpenAI  prompt_rag_fusion = ChatPromptTemplate.from_template("""You are a
    helpful   assistant that generates multiple search queries based on a single input   query.
    \n `Generate multiple search queries related to:` `{question}` `\n` [PRE35] [PRE36][PRE37][PRE38]
    import {ChatPromptTemplate} from ''@langchain/core/prompts''; import {ChatOpenAI}
    from ''@langchain/openai''; import {RunnableLambda} from ''@langchain/core/runnables'';  const
    perspectivesPrompt = ChatPromptTemplate.fromTemplate(`You are a helpful   assistant
    that generates multiple search queries based on a single input   query. \n  Generate
    multiple search queries related to: {question} \n  Output (4 queries):`)  const
    queryGen = perspectivesPrompt.pipe(llm).pipe(message => {   return message.content.split(''\n'')
    }) [PRE39] def reciprocal_rank_fusion(results: list[list], k=60):     """reciprocal
    rank fusion on multiple lists of ranked documents   and an optional parameter
    k used in the RRF formula  """          # Initialize a dictionary to hold fused
    scores for each document     # Documents will be keyed by their contents to ensure
    uniqueness     fused_scores = {}     documents = {}      # Iterate through each
    list of ranked documents     for docs in results:         # Iterate through each
    document in the list,         # with its rank (position in the list)         for
    rank, doc in enumerate(docs):             # Use the document contents as the key
    for uniqueness             doc_str = doc.page_content             # If the document
    hasn''t been seen yet,             # - initialize score to 0             # - save
    it for later             if doc_str not in fused_scores:                 fused_scores[doc_str]
    = 0                 documents[doc_str] = doc             # Update the score of
    the document using the RRF formula:             # 1 / (rank + k)             fused_scores[doc_str]
    += 1 / (rank + k)      # Sort the documents based on their fused scores in descending
    order      # to get the final reranked results     reranked_doc_strs = sorted(         fused_scores,
    key=lambda d: fused_scores[d], reverse=True     )     # retrieve the corresponding
    doc for each doc_str     return [         documents[doc_str]         for doc_str
    in reranked_doc_strs     ]  retrieval_chain = generate_queries | retriever.batch
    | reciprocal_rank_fusion [PRE40] function reciprocalRankFusion(results, k = 60)
    {   // Initialize a dictionary to hold fused scores for each document   // Documents
    will be keyed by their contents to ensure uniqueness   const fusedScores = {}   const
    documents = {}    results.forEach(docs => {     docs.forEach((doc, rank) => {       //
    Use the document contents as the key for uniqueness       const key = doc.pageContent       //
    If the document hasn''t been seen yet,       // - initialize score to 0       //
    - save it for later       if (!(key in fusedScores)) {         fusedScores[key]
    = 0         documents[key] = 0       }       // Update the score of the document
    using the RRF formula:       // 1 / (rank + k)       fusedScores[key] += 1 / (rank
    + k)     })   })    // Sort the documents based on their fused scores in descending
    order    // to get the final reranked results   const sorted = Object.entries(fusedScores).sort((a,
    b) => b[1] - a[1])   // retrieve the corresponding doc for each key   return sorted.map(([key])
    => documents[key]) }  const retrievalChain = queryGen   .pipe(retriever.batch.bind(retriever))   .pipe(reciprocalRankFusion)
    [PRE41] prompt = ChatPromptTemplate.from_template("""Answer the following question
    based   on this context:  {context} `Question:` `{question}` [PRE42] [PRE43]``
    [PRE44] const rewritePrompt = ChatPromptTemplate.fromTemplate(`Answer the following   question
    based on this context:  {context}  Question: {question} `)  const llm = new ChatOpenAI({temperature:
    0})  const multiQueryQa = RunnableLambda.from(async input => {   // fetch relevant
    documents   const docs = await retrievalChain.invoke(input)   // format prompt   const
    formatted = await prompt.invoke({context: docs, question: input})   // generate
    answer   const answer = await llm.invoke(formatted)   return answer })  await
    multiQueryQa.invoke(`Who are some key figures in the ancient greek   history of
    philosophy?`) [PRE45]` [PRE46][PRE47][PRE48][PRE49][PRE50] from langchain.prompts
    import ChatPromptTemplate from langchain_core.output_parsers import StrOutputParser
    from langchain_openai import ChatOpenAI  prompt_hyde = ChatPromptTemplate.from_template("""Please
    write a passage to   answer the question.\n Question: {question} \n Passage:""")  generate_doc
    = (     prompt_hyde | ChatOpenAI(temperature=0) | StrOutputParser()  ) [PRE51]
    import {ChatOpenAI} from ''@langchain/openai'' import {ChatPromptTemplate} from
    ''@langchain/core/prompts'' import {RunnableLambda} from ''@langchain/core/runnables'';  const
    prompt = ChatPromptTemplate.fromTemplate(`Please write a passage to   answer the
    question Question: {question} Passage:`)  const llm = new ChatOpenAI({temperature:
    0})  const generateDoc = prompt.pipe(llm).pipe(msg => msg.content) [PRE52] retrieval_chain
    `=` generate_doc `|` retriever  [PRE53] const retrievalChain = generateDoc.pipe(retriever)
    [PRE54] prompt = ChatPromptTemplate.from_template("""Answer the following question
    based   on this context:  {context} `Question:` `{question}` [PRE55] [PRE56]``
    [PRE57] const prompt = ChatPromptTemplate.fromTemplate(`Answer the following   question
    based on this context:  {context}  Question: {question} `)  const llm = new ChatOpenAI({temperature:
    0})  const qa = RunnableLambda.from(async input => {   // fetch relevant documents
    from the hyde retrieval chain defined earlier   const docs = await retrievalChain.invoke(input)   //
    format prompt   const formatted = await prompt.invoke({context: docs, question:
    input})   // generate answer   const answer = await llm.invoke(formatted)   return
    answer })  await qa.invoke(`Who are some key figures in the ancient greek history
    of   philosophy?`) [PRE58]` [PRE59][PRE60][PRE61][PRE62][PRE63]  [PRE64]`py[PRE65]````'
  prefs: []
  type: TYPE_NORMAL
