- en: 'Chapter 3\. RAG Part II: Chatting with Your Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you learned how to process your data and create and
    store embeddings in a vector store. In this chapter, you’ll learn how to efficiently
    retrieve the most relevant embeddings and chunks of documents based on a user’s
    query. This enables you to construct a prompt that contains relevant documents
    as context, improving the accuracy of the LLM’s final output.
  prefs: []
  type: TYPE_NORMAL
- en: This process—which involves embedding a user’s query, retrieving similar documents
    from a data source, and then passing them as context to the prompt sent to the
    LLM—is formally known as *retrieval-augmented generation* (RAG).
  prefs: []
  type: TYPE_NORMAL
- en: RAG is an essential component of building chat-enabled LLM apps that are accurate,
    efficient, and up-to-date. In this chapter, you’ll progress from basics to advanced
    strategies to build an effective RAG system for various data sources (such as
    vector stores and databases) and data structures (structured and unstructured).
  prefs: []
  type: TYPE_NORMAL
- en: But first, let’s define RAG and discuss its benefits.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Retrieval-Augmented Generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RAG is a technique used to enhance the accuracy of outputs generated by LLMs
    by providing context from external sources. The term was originally coined in
    a paper by Meta AI researchers who discovered that RAG-enabled models are more
    factual and specific than non-RAG models.^([1](ch03.html#id547))
  prefs: []
  type: TYPE_NORMAL
- en: 'Without RAG, the LLM relies solely on its pretrained data, which may be outdated.
    For example, let’s ask ChatGPT a question about a current event and see its response:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Input*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*Output*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The response by the LLM is factually incorrect and outdated. The latest winner
    at the time of this book’s publication is Argentina, who won the World Cup in
    2022\. While this example question may be trivial, LLM hallucination can have
    disastrous consequences if its answers are relied upon for fact-checking or important
    decision making.
  prefs: []
  type: TYPE_NORMAL
- en: 'To combat this problem, we need to provide the LLM with factual, up-to-date
    information from which it can formulate an accurate response. Continuing on from
    the previous example, let’s go over to Wikipedia’s page for the [FIFA World Cup](https://oreil.ly/LpLOV),
    copy the introduction paragraph, and then append it as *context* to our prompt
    to ChatGPT:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the last sentence contains the necessary context the LLM can use
    to provide an accurate answer. Here’s the response from the LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Because of the up-to-date additional context provided, the LLM was able to generate
    an accurate response to the prompt. But copying and pasting relevant information
    as context isn’t practical nor scalable for a production AI application. We need
    an automated system to fetch relevant information based on a user’s query, append
    it as context to the prompt, and then execute the generation request to the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieving Relevant Documents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A RAG system for an AI app typically follows three core stages:'
  prefs: []
  type: TYPE_NORMAL
- en: Indexing
  prefs: []
  type: TYPE_NORMAL
- en: This stage involves preprocessing the external data source and storing embeddings
    that represent the data in a vector store where they can be easily retrieved.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval
  prefs: []
  type: TYPE_NORMAL
- en: This stage involves retrieving the relevant embeddings and data stored in the
    vector store based on a user’s query.
  prefs: []
  type: TYPE_NORMAL
- en: Generation
  prefs: []
  type: TYPE_NORMAL
- en: This stage involves synthesizing the original prompt with the retrieved relevant
    documents as one final prompt sent to the model for a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: The three basic stages look like [Figure 3-1](#ch03_figure_1_1736545666780536).
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a document  Description automatically generated](assets/lelc_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. The key stages of RAG
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The indexing stage of this process was covered extensively in [Chapter 2](ch02.html#ch02_rag_part_i_indexing_your_data_1736545662500927),
    where you learned how to use document loaders, text splitters, embeddings, and
    vector stores.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s run through an example from scratch again, starting with the indexing
    stage:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[Chapter 2](ch02.html#ch02_rag_part_i_indexing_your_data_1736545662500927)
    has more details on the indexing stage.'
  prefs: []
  type: TYPE_NORMAL
- en: The indexing stage is now complete. In order to execute the retrieval stage,
    we need to perform similarity search calculations—such as cosine similarity—between
    the user’s query and our stored embeddings, so relevant chunks of our indexed
    document are retrieved (see [Figure 3-2](#ch03_figure_2_1736545666780585)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Screenshot 2024-02-12 at 1.36.56 PM.png](assets/lelc_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. An example flow of indexing documents alongside retrieval of relevant
    documents from a vector store; the Hierarchical Navigable Small World (HNSW) box
    depicts calculating similarity of documents against the user’s query
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 3-2](#ch03_figure_2_1736545666780585) illustrates the steps in the
    retrieval process:'
  prefs: []
  type: TYPE_NORMAL
- en: Convert the user’s query into embeddings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the embeddings in the vector store that are most similar to the user’s
    query.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Retrieve the relevant document embeddings and their corresponding text chunk.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can represent these steps programmatically using LangChain as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we are using a vector store method you haven’t seen before: `as_retriever`.
    This function abstracts the logic of embedding the user’s query and the underlying
    similarity search calculations performed by the vector store to retrieve the relevant
    documents.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is also an argument `k`, which determines the number of relevant documents
    to fetch from the vector store. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the argument `k` is specified as 2\. This tells the vector
    store to return the two most relevant documents based on the user’s query.
  prefs: []
  type: TYPE_NORMAL
- en: It may seem counterintuitive to use a low `k` value, but retrieving more documents
    is not always better. The more documents are retrieved, the slower your application
    will perform, the larger the prompt (and associated cost of generation) will be,
    and the greater the likelihood of retrieving chunks of text that contain irrelevant
    information, which will cause the LLM to hallucinate.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve completed the retrieval stage of the RAG system, let’s move on
    to the final generation stage.
  prefs: []
  type: TYPE_NORMAL
- en: Generating LLM Predictions Using Relevant Documents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once we’ve retrieved the relevant documents based on the user’s query, the final
    step is to add them to the original prompt as context and then invoke the model
    to generate a final output ([Figure 3-3](#ch03_figure_3_1736545666780616)).
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a diagram  Description automatically generated](assets/lelc_0303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. An example flow demonstrating indexing documents, retrieval of
    relevant documents from a vector store, and inclusion of retrieved documents as
    context in the LLM prompt
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here’s a code example continuing on from our previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Note the following changes:'
  prefs: []
  type: TYPE_NORMAL
- en: We implement dynamic `context` and `question` variables into our prompt, which
    allows us to define a `ChatPromptTemplate` the model can use to generate a response.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We define a `ChatOpenAI` interface to act as our LLM. Temperature is set to
    0 to eliminate the creativity in outputs from the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We create a chain to compose the prompt and LLM. A reminder: the `|` operator
    (or `pipe` method in JS) takes the output of `prompt` and uses it as the input
    to `llm`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We `invoke` the chain passing in the `context` variable (our retrieved relevant
    docs) and the user’s question to generate a final output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can encapsulate this retrieval logic in a single function:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Notice how we now have a new runnable `qa` function that can be called with
    just a question and takes care to first fetch the relevant docs for context, format
    them into the prompt, and finally generate the answer. In the Python code, the
    `@chain` decorator turns the function into a runnable chain. This notion of encapsulating
    multiple steps into a single function will be key to building interesting apps
    with LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also return the retrieved documents for further inspection:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Congratulations! You’ve now built a basic RAG system to power an AI app for
    personal use.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, a production-ready AI app used by multiple users requires a more advanced
    RAG system. In order to build a robust RAG system, we need to answer the following
    questions effectively:'
  prefs: []
  type: TYPE_NORMAL
- en: How do we handle the variability in the quality of a user’s input?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we route queries to retrieve relevant data from a variety of data sources?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we transform natural language to the query language of the target data
    source?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we optimize our indexing process, i.e., embedding, text splitting?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next we’ll discuss the latest research-backed strategies to answer these questions
    and build a production-ready RAG system. These strategies can be summarized in
    [Figure 3-4](#ch03_figure_4_1736545666780656).
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a software development process  Description automatically generated](assets/lelc_0304.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-4\. Effective strategies to optimize the accuracy of your RAG system
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: All code blocks in the rest of this chapter use the vector store we set up at
    the beginning of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Query Transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the major problems with a basic RAG system is that it relies too heavily
    on the quality of a user’s query to generate an accurate output. In a production
    setting, a user is likely to construct their query in an incomplete, ambiguous,
    or poorly worded manner that leads to model hallucination.
  prefs: []
  type: TYPE_NORMAL
- en: '*Query transformation* is a subset of strategies designed to modify the user’s
    input to answer the first RAG problem question: How do we handle the variability
    in the quality of a user’s input? [Figure 3-5](#ch03_figure_5_1736545666780682)
    illustrates the range of query transformation strategies, ranging from those that
    make a user’s input more or less abstract in order to generate an accurate LLM
    output. The next section begins with a middle ground strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a question  Description automatically generated](assets/lelc_0305.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-5\. Various methods to transform a user’s query based on the abstraction
    level
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Rewrite-Retrieve-Read
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Rewrite-Retrieve-Read strategy proposed by a Microsoft Research team simply
    prompts the LLM to rewrite the user’s query before performing retrieval.^([2](ch03.html#id572))
    To illustrate, let’s return to the chain we built in the previous section, this
    time invoked with a poorly worded user query:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '*The output* (remember: if you rerun it, your output might be different from
    this):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The model failed to answer the question because it was distracted by the irrelevant
    information provided in the user’s query.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s implement the Rewrite-Retrieve-Read prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '*The output:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that we have had an LLM rewrite the user’s initial distracted query
    into a much clearer one, and it is that more focused query that is passed to the
    retriever to fetch the most relevant documents. Note: this technique can be used
    with any retrieval method, be that a vector store such as we have here or, for
    instance, a web search tool. The downside of this approach is that it introduces
    additional latency into your chain, because now we need to perform two LLM calls
    in sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Query Retrieval
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A user’s single query can be insufficient to capture the full scope of information
    required to answer the query comprehensively. The multi-query retrieval strategy
    resolves this problem by instructing an LLM to generate multiple queries based
    on a user’s initial query, executing a parallel retrieval of each query from the
    data source and then inserting the retrieved results as prompt context to generate
    a final model output. [Figure 3-6](#ch03_figure_6_1736545666780704) illustrates.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a diagram of a document  Description automatically generated
    with medium confidence](assets/lelc_0306.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-6\. Demonstration of the multi-query retrieval strategy
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This strategy is particularly useful for use cases where a single question may
    rely on multiple perspectives to provide a comprehensive answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a code example of multi-query retrieval in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Note that the prompt template is designed to generate variations of questions
    based on the user’s initial query.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we take the list of generated queries, retrieve the most relevant docs
    for each of them in parallel, and then combine to get the unique union of all
    the retrieved relevant documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Because we’re retrieving documents from the same retriever with multiple (related)
    queries, it’s likely at least some of them are repeated. Before using them as
    context to answer the question, we need to deduplicate them, to end up with a
    single instance of each. Here we dedupe docs by using their content (a string)
    as the key in a dictionary (or object in JS), because a dictionary can only contain
    one entry for each key. After we’ve iterated through all docs, we simply get all
    the dictionary values, which is now free of duplicates.
  prefs: []
  type: TYPE_NORMAL
- en: Notice our use as well of `.batch`, which runs all generated queries in parallel
    and returns a list of the results—in this case, a list of lists of documents,
    which we then flatten and dedupe as described earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'This final step is to construct a prompt, including the user’s question and
    combined retrieved relevant documents, and a model interface to generate the prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Notice how this isn’t that different from our previous QA chains, as all the
    new logic for multi-query retrieval is contained in `retrieval_chain`. This is
    key to making good use of these techniques—implementing each technique as a standalone
    chain (in this case, `retrieval_chain`), which makes it easy to adopt them and
    even to combine them.
  prefs: []
  type: TYPE_NORMAL
- en: RAG-Fusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The RAG-Fusion strategy shares similarities with the multi-query retrieval strategy,
    except we will apply a final reranking step to all the retrieved documents.^([3](ch03.html#id582))
    This reranking step makes use of the *reciprocal rank fusion* (RRF) algorithm,
    which involves combining the ranks of different search results to produce a single,
    unified ranking. By combining ranks from different queries, we pull the most relevant
    documents to the top of the final list. RRF is well-suited for combining results
    from queries that might have different scales or distributions of scores.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s demonstrate RAG-Fusion in code. First, we craft a prompt similar to the
    multi-query retrieval strategy to generate a list of queries based on the user
    query:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Once we’ve generated our queries, we fetch relevant documents for each query
    and pass them into a function to *rerank* (that is, *reorder* according to relevancy)
    the final list of relevant documents.
  prefs: []
  type: TYPE_NORMAL
- en: The function `reciprocal_rank_fusion` takes a list of the search results of
    each query, so a list of lists of documents, where each inner list of documents
    is sorted by their relevance to that query. The RRF algorithm then calculates
    a new score for each document based on its ranks (or positions) in the different
    lists and sorts them to create a final reranked list.
  prefs: []
  type: TYPE_NORMAL
- en: 'After calculating the fused scores, the function sorts the documents in descending
    order of these scores to get the final reranked list, which is then returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the function also takes a `k` parameter, which determines how much
    influence documents in each query’s result sets have over the final list of documents.
    A higher value indicates that lower-ranked documents have more influence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we combine our new retrieval chain (now using RRF) with the full chain
    we’ve seen before:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: RAG-Fusion’s strength lies in its ability to capture the user’s intended expression,
    navigate complex queries, and broaden the scope of retrieved documents, enabling
    serendipitous discovery.
  prefs: []
  type: TYPE_NORMAL
- en: Hypothetical Document Embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Hypothetical Document Embeddings* (HyDE) is a strategy that involves creating
    a hypothetical document based on the user’s query, embedding the document, and
    retrieving relevant documents based on vector similarity.^([4](ch03.html#id592))
    The intuition behind HyDE is that an LLM-generated hypothetical document will
    be more similar to the most relevant documents than the original query, as shown
    in [Figure 3-7](#ch03_figure_7_1736545666780724).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Screenshot 2024-02-12 at 1.12.45 PM.png](assets/lelc_0307.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-7\. An illustration of HyDE closer in the vector space to the document
    embeddings than the plain query embeddings
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'First, define a prompt to generate a hypothetical document:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we take the hypothetical document and use it as input to the `retriever`,
    which will generate its embedding and search for similar documents in the vector
    store:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we take the retrieved documents, pass them as context to the final
    prompt, and instruct the model to generate an output:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'To recap what we covered in this section, query transformation consists of
    taking the user’s original query and doing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Rewriting into one or more queries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining the results of those queries into a single set of the most relevant
    results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rewriting the query can take many forms, but it’s usually done in a similar
    fashion: take the user’s original query—a prompt you wrote—and ask an LLM to write
    a new query or queries. Some examples of typical changes made are:'
  prefs: []
  type: TYPE_NORMAL
- en: Removing irrelevant/unrelated text from the query.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grounding the query with past conversation history. For instance, to make sense
    of a query such as *and what about in LA,* we need to combine it with a hypothetical
    past question about the weather in SF, to arrive at a useful query such as *weather
    in LA*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Casting a wider net for relevant documents by also fetching documents for related
    queries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decomposing a complex question into multiple, simpler questions and then including
    results for all of them in the final prompt to generate an answer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The right rewriting strategy to use will depend on your use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’ve covered the main query transformation strategies, let’s discuss
    the second major question to answer in order to build a robust RAG system: How
    do we route queries to retrieve relevant data from multiple data sources?'
  prefs: []
  type: TYPE_NORMAL
- en: Query Routing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although using a single vector store is useful, the required data may live in
    a variety of data sources, including relational databases or other vector stores.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, you may have two vector stores: one for LangChain Python documentation
    and another for LangChain JS documentation. Given a user’s question, we would
    like to *route* the query to the appropriate inferred data source to retrieve
    relevant docs. *Query routing* is a strategy used to forward a user’s query to
    the relevant data source.'
  prefs: []
  type: TYPE_NORMAL
- en: Logical Routing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In *logical routing*, we give the LLM knowledge of the various data sources
    at our disposal and then let the LLM reason which data source to apply based on
    the user’s query, as shown in [Figure 3-8](#ch03_figure_8_1736545666780744).
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a brain  Description automatically generated](assets/lelc_0308.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-8\. Query routing to relevant data sources
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In order to achieve this, we make use of function-calling models like GPT-3.5
    Turbo to help classify each query into one of the available routes. A *function
    call* involves defining a schema that the model can use to generate arguments
    of a function based on the query. This enables us to generate structured outputs
    that can be used to run other functions. The following Python code defines the
    schema for our router based on three docs for different languages:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we invoke the LLM to extract the data source based on the predefined schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '*The output:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Notice how the LLM produced JSON output, conforming to the schema we defined
    earlier. This will be useful in many other tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we’ve extracted the relevant data source, we can pass the value into another
    function to execute additional logic as required:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Notice how we don’t do an exact string comparison but instead first turn the
    generated output to lowercase, and then do a substring match. This makes our chain
    more resilient to the LLM going off script and producing output that doesn’t quite
    conform to the schema we asked for.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Resilience to the random nature of LLM outputs is an important theme to keep
    in mind when building your LLM applications.
  prefs: []
  type: TYPE_NORMAL
- en: Logical routing is most suitable when you have a defined list of data sources
    from which relevant data can be retrieved and utilized by the LLM to generate
    an accurate output. These can range from vector stores to databases and even APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic Routing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike logical routing, *semantic routing* involves embedding various prompts
    that represent various data sources alongside the user’s query and then performing
    vector similarity search to retrieve the most similar prompt. [Figure 3-9](#ch03_figure_9_1736545666780765)
    illustrates.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a diagram of a notepad and a brain  Description automatically
    generated](assets/lelc_0309.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-9\. Semantic routing to improve the accuracy of retrieved documents
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The following is an example of semantic routing:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that you’ve seen how to route a user’s query to the relevant data source,
    let’s discuss the third major question when building a robust RAG system: “How
    do we transform natural language to the query language of the target data source?”'
  prefs: []
  type: TYPE_NORMAL
- en: Query Construction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed earlier, RAG is an effective strategy to embed and retrieve relevant
    unstructured data from a vector store based on a query. But most data available
    for use in production apps is structured and typically stored in relational databases.
    In addition, unstructured data embedded in a vector store also contains structured
    metadata that possesses important information.
  prefs: []
  type: TYPE_NORMAL
- en: '*Query construction* is the process of transforming a natural language query
    into the query language of the database or data source you are interacting with.
    See [Figure 3-10](#ch03_figure_10_1736545666780785).'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of data types  Description automatically generated](assets/lelc_0310.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-10\. Illustration of query languages for various data sources
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'For example, consider the query: *What* *are movies about aliens in the year
    1980?* This question contains an unstructured topic that can be retrieved via
    embeddings (*aliens*), but it also contains potential structured components (*year
    == 1980*).'
  prefs: []
  type: TYPE_NORMAL
- en: The following sections dive deeper into the various forms of query construction.
  prefs: []
  type: TYPE_NORMAL
- en: Text-to-Metadata Filter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most vector stores provide the ability to limit your vector search based on
    metadata. During the embedding process, we can attach metadata key-value pairs
    to vectors in an index and then later specify filter expressions when you query
    the index.
  prefs: []
  type: TYPE_NORMAL
- en: 'LangChain provides a `SelfQueryRetriever` that abstracts this logic and makes
    it easier to translate natural language queries into structured queries for various
    data sources. The self-querying utilizes an LLM to extract and execute the relevant
    metadata filters based on a user’s query and predefined metadata schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in a retriever that will take a user query, and split it into:'
  prefs: []
  type: TYPE_NORMAL
- en: A filter to apply on the metadata of each document first
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A query to use for semantic search on the documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To do this, we have to describe which fields the metadata of our documents
    contain; that description will be included in the prompt. The retriever will then
    do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Send the query generation prompt to the LLM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Parse metadata filter and rewritten search query from the LLM output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert the metadata filter generated by the LLM to the format appropriate for
    our vector store.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Issue a similarity search against the vector store, filtered to only match documents
    whose metadata passes the generated filter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Text-to-SQL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SQL and relational databases are important sources of structured data, but they
    don’t interact directly with natural language. Although we can simply use the
    LLM to translate a user’s query to SQL queries, there is little margin for error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some useful strategies for effective text to SQL translations:'
  prefs: []
  type: TYPE_NORMAL
- en: Database description
  prefs: []
  type: TYPE_NORMAL
- en: 'To ground SQL queries, an LLM must be provided with an accurate description
    of the database. One common text-to-SQL prompt employs an idea reported in this
    paper and others: provide the LLM with a `CREATE TABLE` description for each table,
    including column names and types.^([5](ch03.html#id622)) We can also provide a
    few (for instance, three) example rows from the table.'
  prefs: []
  type: TYPE_NORMAL
- en: Few-shot examples
  prefs: []
  type: TYPE_NORMAL
- en: Feeding the prompt with few-shot examples of question-query matches can improve
    the query generation accuracy. This can be achieved by simply appending standard
    static examples in the prompt to guide the agent on how it should build queries
    based on questions.
  prefs: []
  type: TYPE_NORMAL
- en: See [Figure 3-11](#ch03_figure_11_1736545666780807) for a visual of the process.
  prefs: []
  type: TYPE_NORMAL
- en: '![A close-up of a machine  Description automatically generated](assets/lelc_0311.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-11\. A user’s query transformed to a SQL query
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here’s a full code example:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'We first convert the user’s query to a SQL query appropriate to the dialect
    of our database. Then we execute that query on our database. Note that executing
    arbitrary SQL queries on your database generated by an LLM from user input is
    dangerous in a production application. To use these ideas in production, you need
    to consider a number of security measures to reduce the risk of unintended queries
    being run in your database. Here are some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: Run the queries on your database with a user with read-only permissions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The database user running the queries should have access only to the tables
    you wish to make available for querying.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add a time-out to the queries run by this application; this would ensure that
    even if an expensive query is generated, it is canceled before taking up too many
    of your database resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is not an exhaustive list of security considerations. Security of LLM applications
    is an area currently in development, with more security measures being added to
    the recommendations as new vulnerabilities are discovered.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter discussed various state-of-the-art strategies to efficiently retrieve
    the most relevant documents based on a user’s query and synthesize them with your
    prompt to aid the LLM to generate an accurate, up-to-date output.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed, a robust, production-ready RAG system requires a wide range of
    effective strategies that can execute query transformation, query construction,
    routing, and indexing optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Query transformation enables your AI app to transform an ambiguous or malformed
    user query into a representative query that’s optimal for retrieval. Query construction
    enables your AI app to convert the user’s query into the syntax of the query language
    of the database or data source where structured data lives. Routing enables your
    AI app to dynamically route the user’s query to retrieve relevant information
    from the relevant data source.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 4](ch04.html#ch04_using_langgraph_to_add_memory_to_your_chatbot_1736545668266431),
    we’ll build on this knowledge to add memory to your AI chatbot so that it can
    remember and learn from each interaction. This will enable users to “chat” with
    the application in multiturn conversations like ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch03.html#id547-marker)) Patrick Lewis et al., [“Retrieval-Augmented Generation
    for Knowledge-Intensive NLP Tasks”](https://oreil.ly/Qzd2K), arXiv, April 12,
    2021\.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch03.html#id572-marker)) Xinbei Ma et al., [“Query Rewriting for Retrieval-Augmented
    Large Language Models”](https://oreil.ly/zyw5E), arXiv, October 23, 2023\. Research
    commissioned by Microsoft Research Asia.
  prefs: []
  type: TYPE_NORMAL
- en: '^([3](ch03.html#id582-marker)) Zackary Rackauckas, [“RAG-Fusion: A New Take
    on Retrieval-Augmented Generation”](https://oreil.ly/k7TTY), arXiv, February 21,
    2024\. From the *International Journal on Natural Language Computing*, vol. 13,
    no. 1 (February 2024).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch03.html#id592-marker)) Luyu Gao et al., [“Precise Zero-Shot Dense Retrieval
    Without Relevance Labels”](https://oreil.ly/7aTnS), arXiv, December 20, 2022\.
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch03.html#id622-marker)) Nitarshan Rajkumar et al., [“Evaluating the Text-to-SQL
    Capabilities of Large Language Models”](https://oreil.ly/WOrzt), arXiv, March
    15, 2022\.
  prefs: []
  type: TYPE_NORMAL
