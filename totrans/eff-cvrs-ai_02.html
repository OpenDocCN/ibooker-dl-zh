<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">2</span> </span> <span class="chapter-title-text">Building a conversational AI</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">Building an FAQ conversational AI</li> 
    <li class="readable-text" id="p3">Building a process-oriented conversational AI</li> 
    <li class="readable-text" id="p4">Using generative AI inside of your conversational AI</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p5"> 
   <p>In production, conversational AI can be quite complex, and throughout this book, we’ll cover many techniques that address the real-world problems you’ll face as you build and deploy your own solutions. In this chapter, we’ll build Cake Bot, a conversational AI solution with elements from several different kinds of conversational AIs. This will give us a solid foundation for understanding conversational AI structure.</p> 
  </div> 
  <div class="readable-text intended-text" id="p6"> 
   <p>We’ll follow a fictitious small American bakery from Ohio called Cake Shop. The company makes custom cakes and takes orders for delivery or pickup. They want to add a conversational AI solution to their website to help their customers. Since they have never built a bot before, they intend to start small but hope to quickly expand the scope and capability of their solution. They decide to start with an AI solution that answers their most frequently asked questions.</p> 
  </div> 
  <div class="readable-text intended-text" id="p7"> 
   <p>Many of the tasks in this chapter <em>could</em> be done with large language models. However, this bakery is cautious. They especially want to control the wording of responses given for several question types. Thus, their solution will blend traditional and generative techniques.</p> 
  </div> 
  <div class="readable-text intended-text" id="p8"> 
   <p>We will demonstrate the building process using a conversational AI platform (IBM’s watsonx Assistant), and we’ll later fold in a generative AI platform (IBM’s watsonx.ai). The key concepts we demonstrate are applicable across many different AI platforms. You can easily use your platform of choice for conversational AI and generative AI.</p> 
  </div> 
  <div class="readable-text" id="p9"> 
   <h2 class=" readable-text-h2"><span class="num-string">2.1</span> Building an FAQ bot</h2> 
  </div> 
  <div class="readable-text" id="p10"> 
   <p>Most conversational AI builders start with a question-answering bot. Also known as FAQ bots, these AI solutions deliver a response directly to a user’s question, often without any follow-up questions. The user asks a question, the bot returns an answer, and the conversation is done when the user is finished asking questions. These bots work especially well when there are a small number of (frequently asked) questions.</p> 
  </div> 
  <div class="readable-text intended-text" id="p11"> 
   <p>In this section, we will build an FAQ bot for Cake Shop. Some questions will have a static response that will be the same no matter how the question is asked. Other questions will have a dynamic response that will change based on information in the question. But before we train the bot on any question-answering, we will first put some basic scaffolding in place.</p> 
  </div> 
  <div class="readable-text" id="p12"> 
   <h3 class=" readable-text-h3"><span class="num-string">2.1.1</span> FAQ bot foundations</h3> 
  </div> 
  <div class="readable-text" id="p13"> 
   <p>Every conversational AI needs to be able to start a conversation and react when it doesn’t know what to do. Most conversational AI platforms provide this capability by default when creating a new chatbot. It’s worth quickly checking these configurations and adapting them to your needs.</p> 
  </div> 
  <div class="readable-text intended-text" id="p14"> 
   <p>Cake Shop starts building their conversational AI (the “assistant”), and they title it “Cake Bot.” From the conversational AI’s main menu, their developer navigates to Actions, which lists all the assistant’s capabilities. The first list is titled “Created by you” and is empty; the second list is titled “Set by assistant,” and it lists the default capabilities, which are outlined in table 2.1.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p15"> 
   <h5 class=" browsable-container-h5"><span class="num-string">Table 2.1</span> Default capabilities in a new assistant</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Capability 
       </div></th> 
      <th> 
       <div>
         Executed when 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  Greet customer <br/></td> 
      <td>  The assistant is first opened or engaged with. Opening the assistant starts a conversation. <br/></td> 
     </tr> 
     <tr> 
      <td>  No action matches <br/></td> 
      <td>  No action can be matched to the user’s message (the message is not understood). Other platforms may call this the “fallback intent.” <br/></td> 
     </tr> 
     <tr> 
      <td>  Trigger word detected <br/></td> 
      <td>  Keywords like profanity are detected. <br/></td> 
     </tr> 
     <tr> 
      <td>  Fallback <br/></td> 
      <td>  The user needs to leave the chatbot. <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p16"> 
   <p>The first capability is the most important to customize, as it gives us our first chance to personalize the assistant. The default text is “Welcome, how can I assist you?” The Cake Shop team changes this text to “Welcome to Cake Bot. How can I help you?” This is a minimum level of customization—it would be better to include additional information, like what the bot can do for users. However, the bot does not have any capabilities yet, so the Cake Shop team leaves this message as is.</p> 
  </div> 
  <div class="readable-text intended-text" id="p17"> 
   <p>Next, the “No action matches” action should be reviewed. This action will be invoked when the bot does not understand the user. Since the bot has no training yet, this action will be the default response to any user input. The default configuration is shown in figure 2.1.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p18">  
   <img alt="figure" src="../Images/CH02_F01_Freed2.png" width="635" height="532"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.1</span> Default configuration of "No action matches" in the assistant</h5>
  </div> 
  <div class="readable-text" id="p19"> 
   <p>This configuration is summarized as follows:</p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p20"> The action counts how many times it has been invoked in the conversation. </li> 
   <li class="readable-text" id="p21"> If three or less times, the response is “I’m afraid I don’t understand. Please rephrase your question.” </li> 
   <li class="readable-text" id="p22"> If four or more times, it deflects to a fallback routine. (The default fallback routine is to offer a human agent.) </li> 
  </ol> 
  <div class="readable-text" id="p23"> 
   <p>The Cake Shop team decides to reduce this threshold by changing the 3 to a 1. This keeps their users from getting stuck.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p24"> 
    <h5 class=" callout-container-h5 readable-text-h5">Fallback action and connection to a human agent</h5> 
   </div> 
   <div class="readable-text" id="p25"> 
    <p>Most conversational AI platforms have no-code and low-code integrations to connect users to a human agent through chat or voice. We will not dive deeper into this, since the details are platform-specific. Suffice it to say that this is a common pattern. For the sake of this chapter, we will focus on the conversational design and AI training.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p26"> 
   <p>At this point, we have a chatbot that does three things:</p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p27"> When the user opens the chat, they are greeted with “Welcome to Cake Bot. How can I help you?” </li> 
   <li class="readable-text" id="p28"> Whatever they say next, the chatbot responds that it doesn’t understand. </li> 
   <li class="readable-text" id="p29"> Whatever they say after that, the chatbot offers a human agent. </li> 
  </ol> 
  <div class="readable-text" id="p30"> 
   <p>Boring! Let’s train this bot to answer some questions properly.</p> 
  </div> 
  <div class="readable-text" id="p31"> 
   <h3 class=" readable-text-h3"><span class="num-string">2.1.2</span> Static question and answering</h3> 
  </div> 
  <div class="readable-text" id="p32"> 
   <p>Let’s start with a mental model of the chatbot components involved in answering questions.</p> 
  </div> 
  <div class="readable-text intended-text" id="p33"> 
   <p>In some platforms, you can directly connect questions to answers. In others, an additional layer is introduced to categorize similar questions into groups called <em>intents</em>. An intent-based question-answering system gives the builder full control over responses generated by the conversational AI. A generalized version of this design is shown in figure 2.2, using Cake Bot as our example.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p34">  
   <img alt="figure" src="../Images/CH02_F02_Freed2.png" width="820" height="178"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.2</span> Question-answering bots map user utterances to intents, which map to answers.</h5>
  </div> 
  <div class="readable-text" id="p35"> 
   <p>Let’s review the terminology in this diagram:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p36"> <em>Utterance</em>—This is the input provided to the chatbot. For a question-answering bot, these are questions. </li> 
   <li class="readable-text" id="p37"> <em>Intent</em>—This is a logical grouping of utterances with similar meanings. </li> 
   <li class="readable-text" id="p38"> <em>Response</em>—This is the output from the chatbot. For a question-answering bot, these are answers. </li> 
  </ul> 
  <div class="readable-text" id="p39"> 
   <p>For your first chatbot, intents save a lot of time. Notice that, as a builder, you do not have to distinguish between questions with similar meaning. “What time are you open?” and “What are your hours?” both relate to the operating hours of your store. It’s not critical for the bot to differentiate these. We give them the same “meaning” via the <code>#store_hours</code> intent. “What cakes do you sell?” has a different meaning and thus a different intent of <code>#cake_options</code>.</p> 
  </div> 
  <div class="readable-text intended-text" id="p40"> 
   <p>For each intent your bot serves, the bot is trained with example utterances. Modern intent-based systems require as few as five example utterances per intent. This is not a bad trade-off; there are nearly an infinite number of ways to ask for store hours, and by providing a handful of examples, you can train your bot well.</p> 
  </div> 
  <div class="readable-text intended-text" id="p41"> 
   <p>Intent-based question-answering systems are a blessing and a curse: for each intent you train, you can control the response, which offers pros and cons.</p> 
  </div> 
  <div class="readable-text intended-text" id="p42"> 
   <p>Pros:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p43"> You have complete design control over the response. You can copyedit it, format the text, and even include graphical elements. You know the exact contents of the response. </li> 
   <li class="readable-text" id="p44"> For a small number of intents, this can be done quickly. You can set up your first chatbot in as little as an hour. </li> 
  </ul> 
  <div class="readable-text intended-text" id="p45"> 
   <p>Cons:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p46"> As the number of intents increases, it becomes more difficult to train the bot to recognize them all. </li> 
   <li class="readable-text" id="p47"> The responses do not adapt to nuances in the user’s questions. For “Are you open today?” the bot still responds generically: “We’re open every day.” </li> 
   <li class="readable-text" id="p48"> Inaccurate or untuned responses give the user a painful feeling of “chatbot doesn’t understand.” </li> 
  </ul> 
  <div class="readable-text" id="p49"> 
   <p>We’ll address several of these downsides to question-answering bots in the next few chapters: how to collect the right data to train your bot (chapter 4), how to use that data to train stronger intents (chapter 5), how to supplement those intents with answers from documents and generative AI (chapter 6), and how to use generative AI for a few more training and testing tasks (chapter 7).</p> 
  </div> 
  <div class="readable-text intended-text" id="p50"> 
   <p>Let’s start by training our chatbot on its first question-answering capabilities. For each one, we need a user intent, a set of related user utterances, and a response. The first set of questions and answers we’ll define will cover the background on Cake Shop, operating hours for their stores, the kinds of cakes offered, the approximate cost of cakes, and information about their Cake Club. These intent-based question-answering responses are shown in table 2.2.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p51"> 
   <h5 class=" browsable-container-h5"><span class="num-string">Table 2.2</span> Initial set of FAQ intents, with associated utterances and responses</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Intent 
       </div></th> 
      <th> 
       <div>
         Example utterances 
       </div></th> 
      <th> 
       <div>
         Response 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  <code>#background</code> <br/></td> 
      <td>  Tell me about Cake Shop <br/>  What's the background on your business? <br/>  History of Cake Shop <br/></td> 
      <td>  Founded by Grandma Cake in 1980, we've made over 10,000 cakes for local residents! <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>#store_hours</code> <br/></td> 
      <td>  Store hours <br/>  What are your store hours? <br/>  When are you open? <br/></td> 
      <td>  We are open Monday through Friday, 9:00 a.m. to 9:00 p.m. <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>#cake_options</code> <br/></td> 
      <td>  Cake options <br/>  Do you make wedding cakes? <br/>  What kinds of cakes do you sell? <br/></td> 
      <td>  We offer cakes for many occasions, such as weddings, birthdays, anniversaries, retirement, and all- occasion cakes. <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>#cost</code> <br/></td> 
      <td>  How much does a cake cost? <br/>  Is there a minimum order value? <br/>  Is there a surcharge for delivery? <br/></td> 
      <td>  Our cakes typically cost around $30, with a $5 delivery fee. <br/></td> 
     </tr> 
     <tr> 
      <td>  <code>#cake_club</code> <br/></td> 
      <td>  Cake rewards <br/>  Cake Club <br/>  Any special promotions or discounts? <br/></td> 
      <td>  Our Cake Club rewards program earns you a $10 gift certificate for every ten cakes you purchase. <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p52"> 
   <p>In the assistant, we define an action that detects an intent and gives a response—a question-answering action. This is the simplest kind of action we can define in any conversational AI platform. Figure 2.3 shows the user interface that starts this action definition.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p53">  
   <img alt="figure" src="../Images/CH02_F03_Freed2.png" width="504" height="243"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.3</span> User interface to create our first action</h5>
  </div> 
  <div class="readable-text" id="p54"> 
   <p>For each of these actions, we need to configure how they start (the user utterances) and what they do (respond with an answer). You’ll notice that these are the right-most columns in table 2.2. Some conversational AI platforms also use the intent label for the action; ours labels the action based on one of the user utterances that triggers it. We start our journey of defining the utterances that trigger the <code>#background</code> action in figure 2.4.</p> 
  </div> 
  <div class="readable-text intended-text" id="p55"> 
   <p>Note that the user interface points out that the chatbot’s recognition of this action will improve with more examples. For the sake of our demo, we will use three examples per action, which is enough to get us started. We will demonstrate multiple ways to find additional training examples in subsequent chapters.</p> 
  </div> 
  <div class="readable-text intended-text" id="p56"> 
   <p>Our question-answering action is almost complete. We have the questions that trigger it; now we need to define the chatbot’s response. The response for our <code>#background</code> action is shown in figure 2.5. This action has three parts:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p57"> <em>Conditional logic</em>—For a static question-answering action, no logic is needed. The action only starts when the intent is detected. </li> 
   <li class="readable-text" id="p58"> <em>Response</em>—“Assistant says” is the response to the user. Our response is simple text. </li> 
   <li class="readable-text" id="p59"> <em>Next step</em>—For a static question-answering action, no next step is needed. Giving the answer ends the action. </li> 
  </ul> 
  <div class="browsable-container figure-container" id="p60">  
   <img alt="figure" src="../Images/CH02_F04_Freed2.png" width="932" height="595"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.4</span> Defining the utterances that trigger an action<span class="aframe-location"/></h5>
  </div> 
  <div class="browsable-container figure-container" id="p61">  
   <img alt="figure" src="../Images/CH02_F05_Freed2.png" width="920" height="614"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.5</span> Defining the response for a question-answering action. The simplest form has only one step after detecting the intent—give the response.</h5>
  </div> 
  <div class="readable-text" id="p62"> 
   <p>We’ll repeat these action-creation steps for each of the five intents. Each action is trained with the examples that trigger it and the response it should give. Each of these actions is a single-step action that ends once the answer is given. </p> 
  </div> 
  <div class="readable-text intended-text" id="p63"> 
   <p>When all five actions have been created, we are ready to do some testing. Figure 2.6 shows the testing interface for our chatbot.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p64">  
   <img alt="figure" src="../Images/CH02_F06_Freed2.png" width="565" height="205"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.6</span> Chat preview link</h5>
  </div> 
  <div class="readable-text" id="p65"> 
   <p>Let’s ask some questions! Figure 2.7 shows the test results for a sample question. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p66">  
   <img alt="figure" src="../Images/CH02_F07_Freed2.png" width="575" height="261"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.7</span> Example question-answering response from Cake Bot</h5>
  </div> 
  <div class="readable-text" id="p67"> 
   <p>Note that the question asked does not exactly match any of our training examples. This indicates that the bot has learned the meaning in the examples. The following listing shows additional tests of the bot.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p68"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 2.1</span> Testing Cake Bot with more questions</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">User: hours of operation?
Bot: We are open Monday through Friday, 9am to 9pm.
User: why did the chicken cross the road
Bot: I'm afraid I don't understand. Please rephrase your question.
User: cost for a cake?
Bot: Our cakes typically cost around $30, with a $5 delivery fee.</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p69"> 
   <p>This is a great start for our bot. We can train it on more intents, and we can make it more accurate by giving it more examples for those intents. But let’s consider something different.</p> 
  </div> 
  <div class="readable-text intended-text" id="p70"> 
   <p>All the question-answering actions we’ve created have been single-step actions. The user gets the same response no matter what they ask. In the next section, you’ll see how to evolve a static response into a dynamic response based on additional information. </p> 
  </div> 
  <div class="readable-text" id="p71"> 
   <h3 class=" readable-text-h3"><span class="num-string">2.1.3</span> Dynamic question and answering</h3> 
  </div> 
  <div class="readable-text" id="p72"> 
   <p>Cake Shop presently has four locations: Columbus, Dublin, Westerville, and Grandview. When the bot was first created, all four locations had the same operating hours: 9:00 a.m. to 9:00 p.m. on weekdays. Circumstances have shifted—the Columbus store needs to open and close one hour earlier (8:00 a.m. to 8:00 p.m.). A single chatbot response doesn’t cover all the stores anymore. Now when a user asks about store hours, we need to figure out which store they need the hours for. If they don’t specify, we’ll need to ask them a clarifying question. </p> 
  </div> 
  <div class="readable-text intended-text" id="p73"> 
   <p>The next listing shows how we want the bot to handle store hours questions in a series of sample questions.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p74"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 2.2</span> Sample conversations for store hours, dependent on location</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">User: hours of operation? <span class="aframe-location"/> #1
Bot: To view our store hours, please select a location. 
Bot: (Columbus, Dublin, Westerville, Grandview)
User: Columbus 
Bot: Our Columbus store is open Monday through Friday, 8am - 8pm.

User: hours of operation?
Bot: To view our store hours, please select a location.
Bot: (Columbus, Dublin, Westerville, Grandview)
User: Dublin 
Bot: Our Dublin store is open Monday through Friday, 9am - 9pm.

User: hours of operation for Grandview?<span class="aframe-location"/> #2
Bot: Our Grandview store is open Monday through Friday, 9am - 9pm.</pre> 
    <div class="code-annotations-overlay-container">
     #1 Ambiguous question is now clarified before answering
     <br/>#2 Unambiguous question is answered directly
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p75"> 
   <p>We can also draw a flow diagram covering these sample conversations, as shown in figure 2.8. It’s helpful to create a flow diagram and sample conversations when your conversation has dynamism. Some of your team members will prefer the diagrams, others the conversations, and some will need both.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p76">  
   <img alt="figure" src="../Images/CH02_F08_Freed2.png" width="855" height="173"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.8</span> Process flow for a location-specific <code>#store_hours</code> intent</h5>
  </div> 
  <div class="readable-text intended-text" id="p77"> 
   <p>The “store hours” flow can be implemented in three steps:</p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p78"> Display “To view our store hours, please select a location” and a list of locations. The user must choose a location. </li> 
   <li class="readable-text" id="p79"> If step 1 = “Columbus,” display Columbus hours, and end the action. </li> 
   <li class="readable-text" id="p80"> Display the hours for the step 1 store, and end the action. </li> 
  </ol> 
  <div class="readable-text" id="p81"> 
   <p>This works because steps “fall through” in our platform. Here’s how a few conversations work:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p82"> The user types “store hours,” and step 1 fires. The user selects “Columbus,” and step 2 fires and completes the action. </li> 
   <li class="readable-text" id="p83"> The user types “store hours,” and step 1 fires. The user selects “Grandview,” and the step 2 condition is not met. Step 3 fires and completes the action. </li> 
   <li class="readable-text" id="p84"> The user types “store hours for Columbus.” The step 1 exit conditions are met, so step 2 fires and completes the action. </li> 
   <li class="readable-text" id="p85"> The user types “store hours for Grandview.” The step 1 exit conditions are met, and the step 2 condition is not met. Step 3 fires and completes the action. </li> 
  </ul> 
  <div class="readable-text" id="p86"> 
   <p>Figure 2.9 shows how these steps are implemented in our assistant.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p87">  
   <img alt="figure" src="../Images/CH02_F09_Freed2.png" width="636" height="721"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.9</span> The three steps for the <code>#store_hours</code> action</h5>
  </div> 
  <div class="readable-text" id="p88"> 
   <p>Cake Bot is off to a good start. It can answer some basic questions about Cake Shop, and it even has a little dynamism. Grandma Cake won’t have to answer so many repetitive questions on the phone! But Cake Bot cannot take any action for the users yet. We’ll look at that in the next section.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p89"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercises</h5> 
   </div> 
   <ol> 
    <li class="readable-text" id="p90"> Download this chapter’s chatbot code from the book’s GitHub site: <a href="https://github.com/andrewrfreed/EffectiveConversationalAI">https://github.com/andrewrfreed/EffectiveConversationalAI</a>. Load the chatbot in watsonx Assistant, and use the Preview panel to test the chatbot’s question and answering flows. </li> 
    <li class="readable-text buletless-item" id="p91"> Alternatively, implement Cake Bot in your preferred conversational AI platform: 
     <ul> 
      <li> Define a greeting message. </li> 
      <li> Define a fallback intent and/or fallback message. </li> 
      <li> Implement the five intents from table 2.2. </li> 
     </ul></li> 
   </ol> 
  </div> 
  <div class="readable-text" id="p92"> 
   <h2 class=" readable-text-h2"><span class="num-string">2.2</span> Routing agents and process-oriented bots</h2> 
  </div> 
  <div class="readable-text" id="p93"> 
   <p>Not all bots are question-answering bots. Q&amp;A bots are great at delivering answers, but what if the user needs more than an answer—what if they need the bot to act? For Cake Shop, we’d love for customers to be able to order cakes from the bot. If all we have is question-answering capability, figure 2.10 is the best we can do.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p94">  
   <img alt="figure" src="../Images/CH02_F10_Freed2.png" width="784" height="178"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.10</span> Cake Shop’s cake order process as question-answering. But it doesn't really answer the question!</h5>
  </div> 
  <div class="readable-text" id="p95"> 
   <p>The user wants to complete a process but cannot do that inside the bot. They only get <em>instructions</em> on how to complete the process. A question-answering bot is thus often an early iteration of a more capable solution. </p> 
  </div> 
  <div class="readable-text" id="p96"> 
   <h3 class=" readable-text-h3"><span class="num-string">2.2.1</span> Routing agents</h3> 
  </div> 
  <div class="readable-text" id="p97"> 
   <p>Cake Shop offers a wide variety of cakes with different flavoring and decoration options. There are decoration packages for weddings, graduations, birthdays, and more. There are flavoring options including vanilla, chocolate, and strawberry. Plus, there are payment and delivery methods. Given all these options, it’s reasonable to assume the user may want or need to talk this process through with a human.</p> 
  </div> 
  <div class="readable-text intended-text" id="p98"> 
   <p>For many chatbot developers, the next logical iteration of their chatbot is a routing agent. The routing agent detects the intent from the user’s utterance and determines who can best help fulfill the intent. Figure 2.11 reimagines our Cake Bot with routing agent capabilities.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p99">  
   <img alt="figure" src="../Images/CH02_F11_Freed2.png" width="847" height="131"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.11</span> A routing agent detects user intents and routes them to an appropriate specialist.</h5>
  </div> 
  <div class="readable-text" id="p100"> 
   <p>For the original Q&amp;A requests, the bot works as it did before. But for cake-ordering requests, this bot does not attempt to answer the question at all—it just routes the call to an appropriate specialist. See our implementation in figure 2.12. The action has one step once the intent is detected: route the user to a specialist.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p101">  
   <img alt="figure" src="../Images/CH02_F12_Freed2.png" width="1015" height="412"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.12</span> Routing agent configuration for <code>#cake_orders</code>. As soon as the intent is detected, the user is deflected to a human specialist.</h5>
  </div> 
  <div class="readable-text" id="p102"> 
   <p>This routing agent is just triaging incoming requests, which can be transferred to human agents or to specialized AI solutions. The human agents could use the telephone or live web chat. In this book, we’ll generically refer to these humans as <em>call center agents</em>.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p103"> 
    <h5 class=" callout-container-h5 readable-text-h5">Press 1 for appointments . . .</h5> 
   </div> 
   <div class="readable-text" id="p104"> 
    <p>You’ve probably phoned an interactive voice response (IVR) system that recites a menu of options and prompts you to select one (“press 1 for appointments”). This is also a routing agent. One downside to these systems is the length of time it takes to read the menu. A conversational AI routing agent lets you speak your intent, increasing the convenience over listening to a lengthy menu.</p> 
   </div> 
   <div class="readable-text" id="p105"> 
    <p>Routing agents let you implement conversational AI solutions iteratively rather than needing to handle everything at once.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p106"> 
   <p>The human agents in routing agent systems often know what type of request they are receiving but little else. In figure 2.12, they were only told that the user wanted to order a cake. For some process flows with high degrees of complexity and sensitivity, this may be ideal. For instance, a “report fraud” intent should probably connect to a human right away. </p> 
  </div> 
  <div class="readable-text intended-text" id="p107"> 
   <p>In other scenarios, an early deflection to a human agent is mundane for the agent and expensive for the employer. For insurance systems handling claim statuses, member IDs and claim dates must be collected before getting to higher value tasks like explaining what has happened with a claim. Here the AI assistant could first collect the member ID and claim date before directing the conversation to a human.</p> 
  </div> 
  <div class="readable-text intended-text" id="p108"> 
   <p>Thus, the next evolution of a routing agent is to shift more of the work to automation. Let’s build this for Cake Bot.</p> 
  </div> 
  <div class="readable-text" id="p109"> 
   <h3 class=" readable-text-h3"><span class="num-string">2.2.2</span> Transitioning from a routing agent to a process-oriented bot</h3> 
  </div> 
  <div class="readable-text" id="p110"> 
   <p>The generalized process flow for ordering cakes is shown in figure 2.13. It includes four steps to clarify details about the cake being ordered, then a confirmation step, and finally fulfillment. (For brevity, we will omit the fulfillment details for the rest of the chapter—example code is available at our GitHub site: <a href="https://github.com/andrewrfreed/EffectiveConversationalAI">https://github.com/andrewrfreed/EffectiveConversationalAI</a>.)<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p111">  
   <img alt="figure" src="../Images/CH02_F13_Freed2.png" width="925" height="168"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.13</span> Process flow for ordering a cake from Cake Shop</h5>
  </div> 
  <div class="readable-text" id="p112"> 
   <p>With the full process flow designed, we can transition from a routing agent toward a process-oriented bot. Cake Bot will handle part of the cake-ordering process by collecting a few details before routing to a human agent to complete the process. Figure 2.14 shows the design for the first iteration of Cake Bot’s transition.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p113">  
   <img alt="figure" src="../Images/CH02_F14_Freed2.png" width="772" height="193"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.14</span> Transitioning a routing agent to a process-oriented bot. The bot now collects two pieces of information before handing off to a human.</h5>
  </div> 
  <div class="readable-text" id="p114"> 
   <p>Our process used to have one step (figure 2.12). Now we’ll have four:</p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p115"> The bot will start the process by responding, “I can help with your cake order.” </li> 
   <li class="readable-text" id="p116"> Ask which size cake is needed, and provide options (small, medium, large). </li> 
   <li class="readable-text" id="p117"> Ask the occasion for the cake, and provide options (birthday, wedding, anniversary, retirement, all-occasion). </li> 
   <li class="readable-text" id="p118"> Transfer the user to a human agent. This is the original first step from the routing agent, but the message to the agent has changed from “User wants to order a cake” to “User wants to order a &lt;size&gt; &lt;occasion&gt; cake.” The assistant will inject the responses from steps 2 and 3 into the message. </li> 
  </ol> 
  <div class="readable-text" id="p119"> 
   <p>These steps are executed sequentially. Figure 2.15 shows step 2.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p120">  
   <img alt="figure" src="../Images/CH02_F15_Freed2.png" width="849" height="624"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.15</span> The step to collect the cake size offers explicit choices to the user.</h5>
  </div> 
  <div class="readable-text" id="p121"> 
   <p>Note that the bot can offer distinct options to the user as buttons, letting the user know which options are available. The bot can also allow the user to type their response if they prefer. You can explore these options in the sample code provided on the book’s GitHub site.</p> 
  </div> 
  <div class="readable-text intended-text" id="p122"> 
   <p>Also note that each of these options will support a “fall through.” If the user starts the conversation with “I want to order a cake,” they will be asked for size and occasion. If they say “I want to order a large anniversary cake,” they will skip the size and occasion questions since they already provided that information.</p> 
  </div> 
  <div class="readable-text intended-text" id="p123"> 
   <p>The rest of the cake-ordering process has been implemented on the book’s GitHub site following the steps described in this section. A sample conversation is provided in the following listing.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p124"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 2.3</span> Sample conversations for cake ordering</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">User: I want to order a cake
Bot: I can take your cake order.
Bot: What size cake do you need?
Bot: (small, medium, large)
User: large 
Bot: (birthday, wedding, anniversary, retirement, all-occasion)
User: birthday
Bot: What flavor cake would you like?
Bot: (vanilla, chocolate, marble)
User: vanilla
Bot: Which frosting would you like?
User: strawberry
Bot: Ok, I've got you down for a small vanilla birthday cake with  <span class="aframe-location"/> #1
strawberry frosting. Is that correct? 
User: yes
Bot: Great. Your order has been accepted. <span class="aframe-location"/> #2
Your order subtotal is $30.00. 
Bot: Would you like to schedule your order for pickup or delivery? Please note there is a $5 delivery fee.
Bot: (pickup, delivery)
User: delivery
(fulfillment details are omitted)</pre> 
    <div class="code-annotations-overlay-container">
     #1 The confirmation message in step 7 plays back the information collected in previous steps.
     <br/>#2 The order confirmation in step 8 triggers conditional logic for the price of the cake.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p125"> 
   <p>Cake Bot is getting more capable. It has static question-answering capability about cakes, dynamic question-answering capability about store hours, and a process-oriented flow for ordering cakes. The Cake Shop team deploys its chatbot and is pleased with the results (and the users are pleased with their cakes!). Next, we’ll take on our final challenge of the chapter: adding generative AI capability with a large language model (LLM).</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p126"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercises</h5> 
   </div> 
   <ol> 
    <li class="readable-text" id="p127"> Refer to this chapter’s chatbot code that you downloaded from the book’s GitHub site (<a href="https://github.com/andrewrfreed/EffectiveConversationalAI">https://github.com/andrewrfreed/EffectiveConversationalAI</a>). Load the chatbot in watsonx Assistant, and use the Preview panel to test the chatbot’s cake-ordering flow. </li> 
    <li class="readable-text buletless-item" id="p129"> Alternatively, implement Cake Bot’s ordering process in your preferred conversational AI platform: 
     <ul> 
      <li> Detect the cake-ordering intent. </li> 
      <li> Route the intent directly to a human agent. </li> 
      <li> Collect all four cake data points, and conclude with a summary. </li> 
     </ul></li> 
   </ol> 
  </div> 
  <div class="readable-text" id="p130"> 
   <h2 class=" readable-text-h2"><span class="num-string">2.3</span> Responding to the user with generative AI</h2> 
  </div> 
  <div class="readable-text" id="p131"> 
   <p>Cake Bot only uses traditional conversational AI technology so far. The question-answering is done by an intent-based classifier. The ordering process is done with a sequential series of rules. This has worked well for the needs of Cake Shop so far.</p> 
  </div> 
  <div class="readable-text intended-text" id="p132"> 
   <p>When the Cake Shop team reviews the performance of the Cake Bot, they see an unusual trend. Users are asking the bot for recipes they intend to serve for dinner before the cake. There’s no other pattern to recipe requests—there are requests for casseroles, salads, stir fries, and more. The team is heartened by the diversity of their users but does not know how to handle these requests in the Cake Bot. How could they detect all these different types of recipes, let alone respond to them all?</p> 
  </div> 
  <div class="readable-text intended-text" id="p133"> 
   <p>This is an excellent place for the Cake Shop team to incorporate some generative AI into their solution. They can use the existing intent mechanism to detect recipe requests and then route those to an LLM to generate an answer. They will need to integrate an LLM into their chatbot generally and send specific requests to that LLM.</p> 
  </div> 
  <div class="readable-text intended-text" id="p134"> 
   <p>Let’s see how they can do that.</p> 
  </div> 
  <div class="readable-text" id="p135"> 
   <h3 class=" readable-text-h3"><span class="num-string">2.3.1</span> Integrating with an LLM</h3> 
  </div> 
  <div class="readable-text" id="p136"> 
   <p>For many conversational AI platforms, the primary way to integrate with external systems is through application programming interfaces (APIs). These are ubiquitous integration patterns and fortunately are supported by a large variety of generative AI platforms that expose LLMs. The specific way APIs are integrated into conversational AI will vary by platform. In some platforms, this integration is done with code; others are low-code and visual interfaces. Differing platforms have different names for their integration capabilities, such as <em>extensions</em>, <em>integrations</em>, and <em>fulfillments</em>. Many let you integrate APIs via OpenAPI specifications.</p> 
  </div> 
  <div class="readable-text intended-text" id="p137"> 
   <p>We will add a generative AI platform as an extension to do LLM-based text generation. There are four steps to adding an extension in our platform (the details of the steps are included in the book’s GitHub repository):</p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p138"> From the Integrations menu, select Build a Custom Extension. </li> 
   <li class="readable-text" id="p139"> Provide a name and description, like “Generative AI platform API call.” </li> 
   <li class="readable-text" id="p140"> Provide an OpenAPI specification file. This file documents the capabilities of the extension, including the methods it exposes, its required and optional parameters, and the responses it provides. OpenAPI specification files are a common documentation format for APIs. They are usually provided by generative AI platforms. </li> 
   <li class="readable-text" id="p141"> Provide connectivity and authentication details, such as the URL of the API implementation and the API key needed to access it. </li> 
  </ol> 
  <div class="readable-text" id="p142"> 
   <p>We add the extension and visually explore it from inside the assistant. Figure 2.16 shows the extension for the LLM text generation API in our platform.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p143">  
   <img alt="figure" src="../Images/CH02_F16_Freed2.png" width="1017" height="573"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.16</span> OpenAPI spec for our LLM text generation API with a subset of the possible request parameters</h5>
  </div> 
  <div class="readable-text" id="p144"> 
   <p>At the time of writing, our text generation API includes 15 input parameters and 6 output parameters—more than fit in figure 2.16! There are also a handful of parameters available without any customization, like the HTTP status code for the response. Other generative AI platforms will have a similar parameter set, perhaps with different parameter names or locations. Let’s review the most significant parameters:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p145"> <code>input</code> (request)—The prompt to the LLM. It will include the instructions, context, and data for the LLM. Some of that data may come directly from the user. </li> 
   <li class="readable-text" id="p146"> <code>model_id</code> (request)—Identifier of the LLM to use for the task. Most generative AI platforms let you pick from several models. </li> 
   <li class="readable-text" id="p147"> <code>parameters</code> (request)—Key-value pairs that tweak the LLM’s behavior. These include the decoding method (greedy or sampling), number of output tokens to generate, and several other parameters. </li> 
   <li class="readable-text" id="p148"> <code>generated_text</code> (response)—The output from the LLM. </li> 
  </ul> 
  <div class="readable-text" id="p149"> 
   <p>We can use an extension from any step in any action. Earlier in this chapter, we used capabilities like “Assistant says,” “Continue to next step,” and “Connect to Agent.” For extensions, the capability is called “Use an extension.” Figure 2.17 shows what that extension invocation looks like for our recipe action. Other LLM tasks would look similar but have different configuration values. This parameter set is tuned for providing recipes.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p150">  
   <img alt="figure" src="../Images/CH02_F17_Freed2.png" width="1012" height="714"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.17</span> Invoking an LLM text-generation API from an action in the assistant</h5>
  </div> 
  <div class="readable-text" id="p151"> 
   <p>Let’s look at how we can connect this all together in Cake Bot.</p> 
  </div> 
  <div class="readable-text" id="p152"> 
   <h3 class=" readable-text-h3"><span class="num-string">2.3.2</span> Routing requests to an LLM</h3> 
  </div> 
  <div class="readable-text" id="p153"> 
   <p>The flow diagram in figure 2.18 outlines how the recipe generation will be covered in Cake Bot. We first create a new action. Just like our question-answering actions, we start with some example utterances that trigger this action. Our first three utterances are “Show me a recipe for,” “How can I make,” and “Tell me how to bake a.” Given the huge variety of possible recipes, we do not include the names of the dishes, just the way that recipe requests are likely to look.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p154">  
   <img alt="figure" src="../Images/CH02_F18_Freed2.png" width="818" height="103"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.18</span> Flow diagram for recipe generation in Cake Bot via LLM</h5>
  </div> 
  <div class="readable-text" id="p155"> 
   <p>Step 1 of the new action is to store the entirety of the user’s original utterance (from the system variable <code>input.text</code>) in a variable called <code>recipe_query_text</code>. This is a technique we have not done in previous steps. For the cake-ordering action, each option had an explicit and finite set of responses. Even if the user said, “large cake, please” we only wanted to store “large.” For the recipe request, we have no idea what the user will say, so we will capture their entire utterance.</p> 
  </div> 
  <div class="readable-text intended-text" id="p156"> 
   <p>Step 2 of the action is to define the prompt for the LLM. We concatenate a simple system prompt with the user’s request. The next listing demonstrates the expression used in building the <code>recipe_prompt</code> variable.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p157"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><strong><span class="num-string">Listing 2.4</span> Building the recipe prompt, which is stored in the <code>recipe_prompt</code> variable</strong></h5> 
   <div class="code-area-container"> 
    <pre class="code-area">"You are a helpful kitchen assistant. Create a recipe as instructed by 
the user.\n\nInput: ".append(recipe_query_text).append("\n\nOutput: ")</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p158"> 
   <p>Step 3 of the action is to call the LLM. The parameters were shown in figure 2.17, but let’s dive into the specific values here:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p159"> <code>input</code>—We assign the <code>recipe_prompt</code> variable value as input. This injects the user’s recipe request into the generalized prompt format shown in listing 2.4. </li> 
   <li class="readable-text" id="p160"> <code>model_id</code>—There are many models available, but at the time of writing, mistralai/mixtral-8x7b-instruct-v01 has performed well on this generation task. </li> 
   <li class="readable-text" id="p161"> <code>project_id</code>—This is an identifier from the generative AI platform project instance. </li> 
   <li class="readable-text" id="p162"> <code>min_tokens</code> and <code>max_tokens</code>—These are set to <code>0</code> and <code>1000</code> respectively, increased from the defaults of <code>0</code> and <code>200</code>, because recipes tend to be a bit lengthy. </li> 
   <li class="readable-text" id="p163"> <code>decoding_method</code>—Set to <code>greedy</code> for ease of debugging, so the model responds identically if the same input is given. </li> 
   <li class="readable-text" id="p164"> <code>repetition_penalty</code>—Set to <code>1</code> (no penalty), since recipes on the internet typically have some repetition in them. </li> 
   <li class="readable-text" id="p165"> <code>version</code>—Indicates the minor version of the API requested. We used the default at the time of writing: 2023-05-29. </li> 
  </ul> 
  <div class="readable-text" id="p166"> 
   <p>Step 4 of the action is to check that the API call was successful, and if so, to display the response to the user. We’ll generate a response starting with some static text, “Here’s your recipe,” and append the LLM response. We don’t need to write any JSON parsing code; the OpenAPI specification tells us we can reference the value of <code>generated_text</code>. Figure 2.19 shows the action response configuration.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p167">  
   <img alt="figure" src="../Images/CH02_F19_Freed2.png" width="618" height="227"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.19</span> Displaying the output from the LLM call to the user</h5>
  </div> 
  <div class="readable-text" id="p168"> 
   <p>This was a lot of fun to build inside Cake Bot! Listing 2.5 shows one example conversation we had with the bot. The recipe is lengthy, so most of it is omitted for brevity. Load up Cake Bot and try it for yourself!</p> 
  </div> 
  <div class="browsable-container listing-container" id="p169"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 2.5</span> Sample abbreviated conversation with Cake Bot</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">User: I need a quick recipe for peanut butter pie
Bot: Sure! I'm looking up a recipe for you. Please wait a few seconds.
Bot: Here's your recipe:
...
Ingredients:
1 pre-made graham cracker crust
1 cup creamy peanut butter
...

Instructions:
In a large mixing bowl, beat together the peanut butter, powdered sugar, and softened cream cheese until smooth and creamy.
...
Serve chilled and enjoy your quick and easy peanut butter pie!
Let me know if you need any further assistance or clarification.</pre>  
   </div> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p170"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercises</h5> 
   </div> 
   <ol> 
    <li class="readable-text" id="p171"> Refer to this chapter’s chatbot code that you downloaded from the book’s GitHub site (<a href="https://github.com/andrewrfreed/EffectiveConversationalAI">https://github.com/andrewrfreed/EffectiveConversationalAI</a>). Load the chatbot in watsonx Assistant, and follow the instructions to integrate with watsonx.ai. Use the Preview panel to test the chatbot’s recipe flow. </li> 
    <li class="readable-text buletless-item" id="p172"> Alternatively, implement Cake Bot’s ordering process in your preferred conversational and generative AI platforms: 
     <ul> 
      <li> Detect the recipe intent. </li> 
      <li> Build a prompt from a set of instructions and the user’s input. </li> 
      <li> Direct the LLM’s response to the user. </li> 
     </ul></li> 
   </ol> 
  </div> 
  <div class="readable-text" id="p173"> 
   <h2 class=" readable-text-h2">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p174"> Question-answering (Q&amp;A) bots are a great way to start building your first conversational AI. </li> 
   <li class="readable-text" id="p175"> Training Q&amp;A bots with examples of questions lets you provide predefined answers to related groups of questions (intents). </li> 
   <li class="readable-text" id="p176"> Actions start with an intent and can have many outcomes: answering a question, deflecting a user to a human agent, asking follow-up questions, and making API calls. </li> 
   <li class="readable-text" id="p177"> A routing agent identifies intents and passes information to human agents. It is a great method for incrementally adding capability to a conversational AI while leaning on human capability. </li> 
   <li class="readable-text" id="p178"> Conversational AI can use a combination of traditional and rules-based techniques along with generative AI. </li> 
  </ul>
 </div></div></body></html>