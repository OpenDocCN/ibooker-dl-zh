<?xml version="1.0" encoding="utf-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"><html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <title>chapter-7</title>
  <link href="../../stylesheet.css" rel="stylesheet" type="text/css" />
 </head>
 <body>
  <div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">7</span> </span><span class="chapter-title-text"><span class="CharOverride-1"></span><span class="CharOverride-1"></span>Evolving RAGOps stack</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header"><span class="CharOverride-2">This chapter covers</span></h3> 
   <ul> 
    <li class="readable-text" id="p2"><span class="CharOverride-3">The design of RAG systems</span></li> 
    <li class="readable-text" id="p3"><span class="CharOverride-3">Available tools and technologies that enable a RAG system</span></li> 
    <li class="readable-text" id="p4"><span class="CharOverride-3">Production best practices for RAG systems</span></li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p5"> 
   <p>So far, we have discussed the indexing pipeline, generation pipeline, and evaluation of a retrieval-augmented generation (RAG) system. Chapter 6 also covered some advanced strategies and techniques that are useful when building production-grade RAG systems. These strategies help improve the accuracy of retrieval and generation and, in some cases, reduce the system latency. With all this information, you should be able to stitch together a RAG system for your use cases. Chapter 2 briefly laid out the design of a RAG system. This chapter elaborates on that design.</p> 
  </div> 
  <div class="readable-text intended-text" id="p6"> 
   <p>A RAG system is composed of standard application layers, as well as layers specific to generative AI applications. Stacked together, these layers create a robust RAG system.</p> 
  </div> 
  <div class="readable-text intended-text" id="p7"> 
   <p>These layers are supported by a technology infrastructure. We delve into these layers and the available technologies and tools offered by popular service providers that can be used in crafting a RAG system. Some providers have started offering managed end-to-end RAG solutions, which we touch upon in this chapter. </p> 
  </div> 
  <div class="readable-text intended-text" id="p8"> 
   <p>We wrap up the chapter with some learnings and best practices for putting RAG systems in production. Chapter 7 also marks the end of part 3 of the book. </p> 
  </div> 
  <div class="readable-text intended-text" id="p9"> 
   <p>By the end of this chapter, you should </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p10">Understand the details of the layers in a RAG (RAGOps) stack. </li> 
   <li class="readable-text" id="p11">Be familiar with a host of service providers and the tools and technologies they offer for RAG systems.</li> 
   <li class="readable-text" id="p12">Know some of the pitfalls and best practices of putting RAG systems in production.</li> 
  </ul> 
  <div class="readable-text" id="p13"> 
   <p>A RAG system includes a lot of additional components compared to traditional software applications. Vector stores and embeddings models are essential components of the indexing pipeline. Knowledge graphs are becoming increasingly popular indexing structures. The generation component can have different kinds of language models. In addition, prompt management is becoming increasingly complex. The production ecosystem for RAG and LLM (large language models) applications is still evolving, but early tooling and design patterns have emerged. RAGOps refers to the operational practices, tools, and processes involved in deploying, maintaining, and optimizing RAG systems in production environments.</p> 
  </div> 
  <div class="readable-text" id="p14"> 
   <h2 class=" readable-text-h2"><span class="num-string">7.1</span> The evolving RAGOps stack</h2> 
  </div> 
  <div class="readable-text" id="p15"> 
   <p>This section describes different components required to build a RAG system in layers. These layers come together to form the operations stack for RAG. We will also take this opportunity to revise the workflow of the RAG system discussed in this book.</p> 
  </div> 
  <div class="readable-text intended-text" id="p16"> 
   <p>It should be noted that RAG, like generative AI in general, is an evolving technology, and therefore, the operations stack continues to evolve. You may find varying definitions and structures. This chapter provides a holistic view and discusses the components from the perspective of their criticality to the RAG system. We look at the layers divided into the following three categories: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p17">Critical layers that are fundamental to the operation of a RAG system. A RAG system is likely to fail if any of these layers are missing or are incomplete.</li> 
   <li class="readable-text" id="p18">Essential layers that are important for performance, reliability, and safety of the system. These essential components bring the system to a standard that provides value to the user. </li> 
   <li class="readable-text" id="p19">Enhancement layers that improve the efficiency, scalability, and usability of the system. These components are used to make the RAG system better and are selected based on the end requirements.</li> 
  </ul> 
  <div class="readable-text" id="p20"> 
   <h3 class=" readable-text-h3"><span class="num-string">7.1.1</span> Critical layers</h3> 
  </div> 
  <div class="readable-text" id="p21"> 
   <p>The indexing pipeline and the generation pipeline (discussed in detail in chapters 3 and 4) form the core of a RAG system. Figure 7.1 illustrates the indexing pipeline that facilitates the creation of the knowledge base for RAG systems and the generation pipeline that uses the knowledge base to generate context-aware responses. </p> 
  </div> 
  <div class="browsable-container figure-container " id="p22">  
   <img src="../Images/CH07_F01_Kimothi.png" alt="" style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 7.1</span><span class=""> </span><span class="">Indexing and generation pipelines forming the core of a RAG system</span></h5>
  </div> 
  <div class="readable-text" id="p23"> 
   <p>Layers enabling these two pipelines form the critical layers of the RAGOps stack.</p> 
  </div> 
  <div class="readable-text" id="p24"> 
   <h4 class=" readable-text-h4">Data layer</h4> 
  </div> 
  <div class="readable-text" id="p25"> 
   <p>The data layer serves the critical role of creating and storing the knowledge base for RAG. It is responsible for collecting data from source systems, transforming it into a usable format, and storing it for efficient retrieval. Here are some components of the data layer: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p26"><em>Data ingestion componen</em><em>t</em>—It collects data from source systems such as databases, content management systems, file systems, APIs, devices, and even the internet. The data can be ingested in batches or as a stream, depending on the use case. For ingesting data, your choice of tool can depend on factors such as data volume, types of data source, ingestion frequency, cost, and ease of setup. Data ingestion is not specific to RAG but is a mainstream component in modern software applications. AWS Glue, Azure Data Factory, Google Cloud Dataflow, Fivetran, Apache NiFi, Apache Kafka, and Airbyte are among tools available for use. For rapid prototyping and proof of concepts (PoCs), frameworks such as Lang&shy;Chain and LlamaIndex have inbuilt functions that can assist in connecting to some sources and extracting information.</li> 
   <li class="readable-text" id="p27"><em>Data transformation componen</em><em>t</em>—It converts the ingested data from a raw to a usable form. A core process in the indexing pipeline is the <em>chunking</em> of data. We know that <em>embeddings</em> is the preferred format of choice for RAG applications because it makes it easier to apply semantic search. <em>Graph structures</em> are becoming increasingly popular in advanced systems. Certain pre-processing steps such as cleaning, de-duplication, metadata enrichment, and masking of sensitive information are also a part of this phase. While the volume of data and the nature of transformation play an important role in any data-transformation step, they are especially critical in RAG systems. All the extract–transform–load (ETL) tools mentioned in the data ingestion step in conjunction with tools such as Apache Spark and dbt also allow transformations. However, if we focus just on RAG, Unstructured.io specializes in processing and transforming unstructured data for use in LLM applications. It offers open source libraries as well as managed services. Constructing knowledge graphs from unstructured data has evolved today from early semantic networks and ontologies into robust frameworks. <br />Microsoft’s GraphRAG is a framework that has pioneered the use of LLMs to extract entities and relationships from text.</li> 
   <li class="readable-text" id="p28"><em>Data storage componen</em><em>t</em>—It stores the transformed data in a way that allows for fast and efficient retrieval. We have discussed that to store embeddings, <em>vector databases</em> are widely used because they are efficient in similarity search. For graph structures, <em>graph databases</em> are used. Most traditional database providers are incorporating vector search capabilities into their systems. Cost, scale, and speed are the primary drivers in the choice of data storage. We have used a vector index such as FAISS in this book. Pinecone is a fully managed cloud-native service. Milvus, Qdrant, and Chroma are among the open source vector databases. Weviate is another database that also has a GraphQL-based interface for knowledge graphs. Neo4j is a leading graph database for storing and querying graph data. A comparison of popular vector databases is available at <a href="https://www.superlinked.com/vector-db-comparison"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/www.superlinked.com/vector-db-comparison</span></a>.</li> 
  </ul> 
  <div class="readable-text" id="p29"> 
   <p>The flow from source systems to data storage via the ingestion and transformation components that lead to the creation of the knowledge base is shown in figure 7.2.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p30">  
   <img src="../Images/CH07_F02_Kimothi.png" alt="" style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 7.2</span><span class=""> </span><span class="">Data layer: Creating the knowledge base by extracting, transforming, and loading (ETL) data from source systems</span></h5>
  </div> 
  <div class="readable-text" id="p31"> 
   <p>A strong data layer is the foundation of an efficient RAG system. The data layer also comes in handy when there is a need for fine-tuning of models. We discuss this feature briefly later in the chapter. Next, we look at the model layer, which includes the embeddings models used to transform text into vectors and the LLMs used in generation.</p> 
  </div> 
  <div class="readable-text" id="p32"> 
   <h4 class=" readable-text-h4">Model layer</h4> 
  </div> 
  <div class="readable-text" id="p33"> 
   <p>Predictive models enable generative AI applications. Some models are provided by third parties, and some need to be custom trained or fine-tuned. Generating quick and cost-effective model responses is also an important aspect of using predictive models. The model layer includes the following three components:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p34"><em>Model librar</em><em>y</em>—It contains the list of models that have been chosen for the application. The most popular models are the LLMs that generate text and other generative models that can generate images, video, and audio. We saw that in the data layer, raw text is transformed into vector embeddings, and this is done using embeddings models. Apart from this, there are other models used in RAG systems: 
    <ul> 
     <li>Embeddings models are used to transform data into vector format. We have discussed embeddings models in detail in chapter 3. Recall that the choice of embeddings model depends on the domain, use case, and cost considerations. Providers such as OpenAI, Gemini by Google, Voyage AI, and Cohere provide a variety of embeddings model choices, and a host of open source embeddings models can also be used via Hugging Face transformers. Multimodal embeddings map data of different modalities into a shared embeddings space.</li> 
     <li>Foundation models or the pre-trained <span class="_Underline">LLMs</span> are used for the generation of outputs, as well as for evaluation and adaptive tasks where LLMs are used to judge. We have discussed LLMs as part of the generation pipeline in chapter 4. Recall that the GPT series by OpenAI, Gemini Series by Google, Claude Series by Anthropic, and Command R series by Cohere are popular proprietary LLMs. The llama series by Meta and Mistral are open source models that have gained popularity. Most LLMs now include multimodal capabilities and are continuously evolving.</li> 
     <li>Task-specific models are machine learning models that are not core to RAG but come in handy for various tasks. These models are used in advanced RAG pipelines. Query classification models for efficient routing and intent detection, NER models to detect entities for metadata, query-expansion models, hallucination-detection models, and bias- and toxicity-moderation models are some examples of task-specific models useful in RAG systems. While task-specific models are generally custom trained, providers such as OpenAI, Hugging Face, and Google also offer these services.</li> 
    </ul> </li> 
   <li class="readable-text" id="p35"><em>Model training and fine-tuning componen</em><em>t</em>—This component is responsible for building custom models and fine-tuning foundation models on custom data. In chapter 4, we discussed that fine-tuning of LLMs is sometimes required for domain adaptation. Fine-tuning can also be done for embeddings models. Additionally, the task-specific models can be trained on custom data. This component supports the algorithms used for training and fine-tuning the models. For training data, this component interacts with the data layer where the training data can be created and managed. A regular MLOps layer is also recommended for the development and maintenance of the models. This is enabled via ML platforms such as Hugging Face, AWS SageMaker, Azure ML, and similar.</li> 
   <li class="readable-text" id="p36"><span class="CharOverride-4">Inference optimization componen</span><span class="CharOverride-4">t</span>—This component is responsible for generating responses quickly and cost-effectively, which can be done by employing a variety of methods such as quantization, batching, KV(Key Value)-caching, and similar. ONNX and NVIDIA TensorRT-LLM are popular frameworks that optimize inferencing.</li> 
  </ul> 
  <div class="readable-text" id="p37"> 
   <p>Figure 7.3 illustrates different components of the model layer. It shows how the model layer helps in deciding which models to use in the RAG system, facilitates training and fine-tuning of the model, and optimizes the models for efficient serving. </p> 
  </div> 
  <div class="browsable-container figure-container " id="p38">  
   <img src="../Images/CH07_F03_Kimothi.png" alt="" style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 7.3</span><span class=""> </span><span class="">The model layer: The model library is the store for all models selected for the application, model training and fine-tuning interact with the data layer to source training data and train custom models, while the inference optimization component is responsible for efficient serving of the model.</span></h5>
  </div> 
  <div class="readable-text" id="p39"> 
   <h4 class=" readable-text-h4">Model deployment</h4> 
  </div> 
  <div class="readable-text" id="p40"> 
   <p>This layer is responsible for making the RAG system available to the application layer. It handles the infrastructure of the models. It also ensures that the models can be accessed reliably. There are four main methods by which the models can be deployed: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p41"><em>Fully managed deploymen</em><em>t</em>—It<strong> </strong>can be provided by proprietary model providers such as OpenAI, Google, Anthropic, and Cohere, where all infrastructure for model deployment, serving, and scaling is managed and optimized by these providers. Services such as AWS SageMaker, Google Vertex AI, Azure Machine Learning, and Hugging Face offer platforms to deploy, serve, and monitor both open source and custom-developed models. Amazon Bedrock is another fully managed service that provides access to a variety of foundation models, both proprietary and open source, simplifying model access and deployment.</li> 
   <li class="readable-text" id="p42"><em>Self-hosted deploymen</em><em>t</em>—This type of deployment is enabled by cloud VM providers such as AWS, GCP, Azure, and hardware providers such as Nvidia. In this scenario, models are deployed in private clouds or on-premises, and the infrastructure is managed by the application developer. Tools such as Kubernetes and Docker are widely used for containerization and orchestration of models, while Nvidia Triton Inference Server can optimize inference on Nvidia hardware.</li> 
   <li class="readable-text" id="p43"><em>Local/edge deploymen</em><em>t</em>—It involves running optimized versions of models on local hardware or edge devices, ensuring data privacy, reduced latency, and offline functionality. Local/edge deployment typically requires model compression techniques such as quantization and pruning, and smaller models tailored for resource-constrained environments. Tools such as ONNX, TensorFlow Lite, and PyTorch Mobile enable efficient deployment on mobile and embedded platforms, while GGML and NVIDIA TensorRT support CPU and GPU optimizations. GPT4All is a popular open source solution for running quantized LLMs locally on devices such as laptops, IoT devices, and edge servers without relying on cloud infrastructure. These frameworks facilitate low-latency, power-efficient execution, making AI accessible in decentralized environments. </li> 
  </ul> 
  <div class="readable-text" id="p44"> 
   <p>Model deployment is a relatively complex task that requires engineering skills when self-hosted and local/edge deployment is done. Figure 7.4 illustrates the three ways in which models are deployed.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p45">  
   <img src="../Images/CH07_F04_Kimothi.png" alt="" style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 7.4</span><span class=""> </span><span class="">The model deployment layer manages the infrastructure for hosting and deployment for efficient serving of all the models in the RAG system.</span> </h5>
  </div> 
  <div class="readable-text" id="p46"> 
   <p>With the data and the model layers, the most essential components of the RAG system are in place. Now we need a layer that manages the co-ordination between the data and the models. This is the responsibility of the application orchestration layer.</p> 
  </div> 
  <div class="readable-text" id="p47"> 
   <h4 class=" readable-text-h4">Application orchestration layer</h4> 
  </div> 
  <div class="readable-text" id="p48"> 
   <p>When we hear the term <em>orchestration</em>, a musical conductor leading a group of musicians in an orchestra comes to mind. An application orchestration layer is somewhat similar. It is responsible for managing the interactions among the other layers in the system. It is a central coordinator that enables communication between data, retrieval systems, generation models, and other services. The major components of the orchestration layer are </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p49"><em>Query orchestration componen</em><em>t</em>—Responsible for receiving and orchestrating user queries. All pre-retrieval query optimization steps such as query classification, expansion, and rewriting are orchestrated by this component. The query orchestration layer may coordinate with the end application layer to receive the input, and the model layer to access the models required for the query optimization. This component will generally pass on the processed query to the retrieval coordination and the generation coordination components.</li> 
   <li class="readable-text" id="p50"><em>Retrieval coordination componen</em><em>t</em>—Hosts the various retrieval logics. Depending on the input from the query orchestration module, it selects the appropriate retrieval method (dense retrieval or hybrid retrieval) and interacts with the data layer. Depending on the retrieval strategy, it may also interact with the model layer if any recursive or adaptive retrieval method is invoked.</li> 
   <li class="readable-text" id="p51"><em>Generation coordination componen</em><em>t</em>—Receives the query and the context from the previous components and coordinates all the post-retrieval steps. Its primary function is to interact with the model layer and prompt the LLM to generate the output. Apart from generation, all the post-retrieval steps such as re-ranking and contextual compression are coordinated by this component. Post-generation tasks such as reflection, fact-checking, and moderation can be coordinated by the generation component. This component can also be made responsible for passing the output to the application layer.</li> 
  </ul> 
  <div class="readable-text" id="p52"> 
   <p>These are the three primary components of the orchestration layer. There are two additional components to consider:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p53"><em>Multi-agent orchestration componen</em><em>t</em>—Used for agentic RAG where multiple agents handle specific tasks. We will take a deeper look at agentic RAG in chapter 8. The orchestration layer is responsible for managing agent interactions and coordination.</li> 
   <li class="readable-text" id="p54"><em>Workflow automation componen</em><em>t</em>—Sometimes employed for managing the flow and the movement of data between different components. This component is not specific to RAG systems but is commonly employed in data products. Apache Airflow and Dagster are popular tools used for workflow automation.</li> 
  </ul> 
  <div class="readable-text" id="p55"> 
   <p>Figure 7.5 illustrates the orchestration layer components interacting with the application layer, which is supported by the model deployment and data layer.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p56">  
   <img src="../Images/CH07_F05_Kimothi.png" alt="" style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 7.5</span><span class=""> </span><span class="">The app orchestration layer accepts the user query from the application layer and sends the response back to the application layer.</span></h5>
  </div> 
  <div class="readable-text" id="p57"> 
   <p>LangChain and LlamaIndex are the most common orchestration frameworks used to develop RAG systems. They provide abstractions for different components. Microsoft’s AutoGen and CrewAI are upcoming frameworks for multi-agent orchestration. </p> 
  </div> 
  <div class="readable-text intended-text" id="p58"> 
   <p>With these four layers (i.e., data, model, model deployment, and application orchestration), the critical RAG system is complete. This core system can interact with the end-software application layer, which acts as the interface between the RAG system and the user. While the application layer is generally custom built, platforms such as Streamlit, Vercel, and Heroku are popular for hosting the application. Figure 7.6 summarizes the critical layers of the RAGOps stack.</p> 
  </div> 
  <div class="readable-text" id="p59"> 
   <p>Now that you are familiar with the core layers of the stack, let’s look next at the essential layers that improve the performance and reliability of the system. </p> 
  </div> 
  <div class="browsable-container figure-container " id="p60">  
   <img src="../Images/CH07_F06_Kimothi.png" alt="" style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 7.6</span><span class=""> </span><span class="">Core RAGOps stack where data, model, model deployment, and app orchestration layers interact with source systems and managed service providers, and co-ordinate with the application layer to interface with the user</span></h5>
  </div> 
  <div class="readable-text" id="p61"> 
   <h3 class=" readable-text-h3"><span class="num-string">7.1.2</span> Essential layers</h3> 
  </div> 
  <div class="readable-text" id="p62"> 
   <p>While the critical layers form the core of the stack, they do not evaluate or monitor the system. They do not test the prompting strategies or offer any protection against the vulnerabilities of LLMs. These layers are essential to the system.</p> 
  </div> 
  <div class="readable-text" id="p63"> 
   <h4 class=" readable-text-h4">Prompt layer</h4> 
  </div> 
  <div class="readable-text" id="p64"> 
   <p>While the generation coordination component of the orchestration layer can simply put together the user query and the retrieved context, poor prompting can lead to hallucinations and subpar results. Proper engineering and evaluation of the prompts are vital to guiding the model toward generating relevant, grounded, and accurate responses. This process often involves experimentation. Developers create prompts, observe the results, and then iterate on the prompts to improve the effectiveness of the app. This also requires tracking and collaboration. Azure Prompt Flow, Lang&shy;Chain Expression Language (LCEL), Weights &amp; Biases prompts, and PromptLayer are among the several applications that can be used to create and manage prompts.</p> 
  </div> 
  <div class="readable-text" id="p65"> 
   <h4 class=" readable-text-h4">Evaluation layer</h4> 
  </div> 
  <div class="readable-text" id="p66"> 
   <p>Chapter 5 discussed RAG evaluations at length. Regular evaluation of retrieval accuracy, context relevance, faithfulness, and answer relevance of the system is necessary to ensure the quality of responses. TruLens by TruEra, Ragas, and Weights &amp; Biases are commonly used platforms and frameworks for evaluation.</p> 
  </div> 
  <div class="readable-text" id="p67"> 
   <h4 class=" readable-text-h4">Monitoring layer</h4> 
  </div> 
  <div class="readable-text" id="p68"> 
   <p>Continuous monitoring ensures the long-term health of the RAG system. Observing the execution of the processing chain is essential for understanding system behavior and identifying points of failure. Assessing the relevance and adequacy of information provided to the language model is also critical. Apart from this, regular system metrics tracking such as resource utilization, latency, and error rates form the part of the monitoring layer. ARISE, RAGAS, and ARES are evaluation frameworks that are also used in monitoring. TraceLoop, TruLens, and Galileo are examples of providers that offer monitoring services.</p> 
  </div> 
  <div class="readable-text" id="p69"> 
   <h4 class=" readable-text-h4">LLM security and privacy layer</h4> 
  </div> 
  <div class="readable-text" id="p70"> 
   <p>While security and privacy are features of any software system, in the context of RAG, there are additional aspects to this. RAG systems rely on large knowledge bases stored in vector databases, which can contain sensitive information. They need to follow all data privacy regulations. AI models are susceptible to manipulation and poisoning. Prompt injection is a malicious attack via prompts to retrieve sensitive information. Data protection strategies such as anonymization, encryption, and differential privacy should be employed. Query validation, sanitization, and output filtering assist in protection against attacks. Implementing guardrails, access controls, monitoring, and auditing are also components of the security and privacy layer.</p> 
  </div> 
  <div class="readable-text" id="p71"> 
   <h4 class=" readable-text-h4">Caching layer</h4> 
  </div> 
  <div class="readable-text" id="p72"> 
   <p>Caching has become a very important component of any LLM-based application. This is because of the high costs and inherent latency of generative AI models. With the addition of a retrieval layer, the costs and latency increase further in RAG systems. One way to control this increase is to cache responses to frequently asked queries. In principle, caching LLM responses is like caching in any other software application, but for generative AI apps, it becomes more important.</p> 
  </div> 
  <div class="readable-text intended-text" id="p73"> 
   <p>These essential layers stacked together with the critical layers create a robust, accurate, and high-performing RAG system. Figure 7.7 adds the essential layers and their components to the critical RAGOps stack.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p74">  
   <img src="../Images/CH07_F07_Kimothi.png" alt="" style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 7.7</span><span class=""> </span><span class="">Adding essential layers to the critical RAGOps stack lays the path to a robust RAG system for user applications. </span></h5>
  </div> 
  <div class="readable-text" id="p75"> 
   <p>Table 7.1 is a recap of the critical and essential layers of the RAGOps stack. </p> 
  </div> 
  <div class="browsable-container browsable-table-container" id="p76"> 
   <h5 class=" browsable-container-h5">Table 7.1 Critical and essential layers of the RAGOps stack</h5> 
   <table id="table001" class="No-Table-Style TableOverride-1"> 
    <colgroup> 
     <col class="_idGenTableRowColumn-1" /> 
     <col class="_idGenTableRowColumn-2" /> 
     <col class="_idGenTableRowColumn-3" /> 
     <col class="_idGenTableRowColumn-4" /> 
    </colgroup> 
    <tbody> 
     <tr class="No-Table-Style _idGenTableRowColumn-5"> 
      <td class="No-Table-Style CellOverride-1"> <p class="_TableHead">Layer</p> </td> 
      <td class="No-Table-Style CellOverride-2"> <p class="_TableHead">Category</p> </td> 
      <td class="No-Table-Style CellOverride-2"> <p class="_TableHead">Description</p> </td> 
      <td class="No-Table-Style CellOverride-3"> <p class="_TableHead">Example tools</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-6"> 
      <td class="No-Table-Style CellOverride-4"> <p class="_TableBody">Data layer</p> </td> 
      <td class="No-Table-Style CellOverride-5"> <p class="_TableBody">Critical</p> </td> 
      <td class="No-Table-Style CellOverride-5"> <p class="_TableBody">Responsible for creating and storing the knowledge base via ingestion from various sources, transformation into embeddings or graph structures, and storing for retrieval </p> </td> 
      <td class="No-Table-Style CellOverride-6"> <p class="_TableBody">AWS Glue, Apache Kafka, FAISS, Pinecone, Neo4j, Weaviate, Milvus</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-7"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Model layer</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Critical</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Contains the models required for generation and retrieval in RAG; includes embeddings models for vector generation, LLMs for text generation, and models for query classification, hallucination detection, or re-ranking</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">OpenAI, Hugging Face Transformers, Google Gemini, Llama, Anthropic</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-8"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Model deployment</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Critical</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Ensures the models are accessible, performant, and scalable; responsible for serving models and optimizing inference for fast response times</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">SageMaker, Vertex AI, NVIDIA Triton, Hugging Face</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-6"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Application orchestration layer</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Critical</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Manages the interaction between layers and services, ensures that queries flow through retrieval and generation stages, and coordinates retrieval methods and generation tasks </p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">LangChain, Haystack, Dagster, Apache Airflow, AutoGen, CrewAI</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-9"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Prompt layer</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Essential</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Designs and maintains the input queries to ensure the LLM generates relevant, high-quality outputs; ensures continuous prompt refinement to avoid hallucinations and improve accuracy</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Weights &amp; Biases Prompts, Azure Prompt Flow</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-10"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Evaluation layer</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Essential</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Evaluates the performance of the retrieval and generation stages, ensuring that the outputs are relevant, factual, and accurate.</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">TruLens by TruEra, Ragas, Weights &amp; Biases</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-11"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Monitoring layer</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Essential</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Continuously monitors the performance, health, and resource usage of the RAG system; tracks key metrics such as latency, resource consumption, and error rates to ensure system stability.</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Prometheus, Grafana, TruLens, Galileo</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-12"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">LLM security &amp; privacy layer</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Essential</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Ensures that the RAG system adheres to data privacy regulations and protects against prompt injection or other forms of AI manipulation; implements security strategies such as encryption, access control, and guardrails</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">AWS KMS, Azure Key Vault, Prompt Injection Guards</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-11"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Model training/Fine-tuning layer</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Essential</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Handles the training and fine-tuning of models for specific domains or tasks; fine-tuning models such as embeddings or LLMs using domain-specific datasets ensure better performance for specialized use cases.</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Hugging Face, AWS SageMaker, Google Vertex AI, Azure ML</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-13"> 
      <td class="No-Table-Style CellOverride-10"> <p class="_TableBody">Caching layer</p> </td> 
      <td class="No-Table-Style CellOverride-11"> <p class="_TableBody">Essential</p> </td> 
      <td class="No-Table-Style CellOverride-11"> <p class="_TableBody">Caching frequently used queries and responses to reduce the latency and cost associated with repeated retrieval and generation tasks; ensures faster response times for common queries and minimizes resource usage for repeated tasks.</p> </td> 
      <td class="No-Table-Style CellOverride-12"> <p class="_TableBody">Redis, Varnish, ElasticCache</p> </td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p77"> 
   <p>We will now briefly look at a few enhancement layers, which are not mandatory but may be employed to further improve the RAG systems. Note that there can be several enhancement layers and that they should be tailored to the use case requirements.</p> 
  </div> 
  <div class="readable-text" id="p78"> 
   <h3 class=" readable-text-h3"><span class="num-string">7.1.3</span> Enhancement layers</h3> 
  </div> 
  <div class="readable-text" id="p79"> 
   <p>Enhancement layers are the parts of the RAGOps stack that are optional but can lead to significant gains, depending on the use case environment. They focus on the efficiency, usability, and scalability of the system. Some possible layers are described in the following. </p> 
  </div> 
  <div class="readable-text" id="p80"> 
   <h4 class=" readable-text-h4">Human-in-the-loop layer</h4> 
  </div> 
  <div class="readable-text" id="p81"> 
   <p>This layer provides critical oversight where human judgment is necessary, especially for use cases requiring higher accuracy or ethical considerations. It helps reduce model hallucinations and bias. </p> 
  </div> 
  <div class="readable-text" id="p82"> 
   <h4 class=" readable-text-h4">Cost optimization layer</h4> 
  </div> 
  <div class="readable-text" id="p83"> 
   <p>RAG systems can become very costly, especially with multiple calls to the LLMs for advanced techniques, evaluations, guardrails, and monitoring. This layer helps manage resources efficiently, which is particularly important for large-scale systems. Optimizing infrastructure can save significant costs but is not critical to the system functioning.</p> 
  </div> 
  <div class="readable-text" id="p84"> 
   <h4 class=" readable-text-h4">Explainability and interpretability layer</h4> 
  </div> 
  <div class="readable-text" id="p85"> 
   <p>This layer helps provide transparency for system decisions, especially important for domains requiring accountability (e.g., legal and healthcare). However, many applications can still function without this in nonregulated environments. </p> 
  </div> 
  <div class="readable-text" id="p86"> 
   <h4 class=" readable-text-h4">Collaboration and experimentation layer</h4> 
  </div> 
  <div class="readable-text" id="p87"> 
   <p>This layer is useful for teams working on development and experimentation but noncritical for system operation. This layer enhances productivity and iterative improvements. Weights &amp; Biases is a popular platform that helps track experiments.</p> 
  </div> 
  <div class="readable-text intended-text" id="p88"> 
   <p>These enhancement layers should be chosen depending on the application requirements. There may be other layers that you may deem fit for your use case.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p89"> 
    <h5 class=" callout-container-h5 readable-text-h5">Managed RAG solutions</h5> 
   </div> 
   <div class="readable-text" id="p90"> 
    <p>Building a RAG system can be complex if you don’t have prior knowledge, budget, or time. To address these challenges, service providers offer managed RAG solutions. </p> 
   </div> 
   <div class="readable-text" id="p91"> 
    <p>OpenAI offers the File Search tool that automatically parses and chunks your documents, creates and stores the embeddings, and uses both vector and keyword search to retrieve relevant content to answer user queries. AWS offers Amazon Bedrock Knowledge Bases, which is fully managed support for end-to-end RAG workflow. Azure AI, such as OpenAI file search, provides indexing and querying. Anthropic offers Claude projects where users can upload documents and provide context to have focused chats. </p> 
   </div> 
   <div class="readable-text" id="p92"> 
    <p>Several other providers offer RAG as a service and can handle video and audio transcription, image content extraction, and document parsing. For quick and easy deployment of a RAG solution, managed service providers can be considered.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p93"> 
   <p>We have also discussed several service providers, tools, and technologies that you can use in the development of RAG systems. The choice of these tools and technologies may depend on factors such as </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p94"><em>Scalability and performance require</em><em>d</em>—RAG systems need to handle large volumes of data efficiently, while maintaining low latency. As data scales or traffic spikes, the system must remain performant to ensure fast response times. Choose cloud platforms that allow for auto-scaling and variable loads. For high-performance and scalable retrieval, choose the vector databases that can handle millions of embeddings with low-latency search capabilities. Use inference optimization tools to help reduce latency during the generation phase.</li> 
   <li class="readable-text" id="p95"><em>Integration with existing stac</em><em>k</em>—Seamless integration with your current technology stack minimizes disruption and reduces complexity. If your system already operates on AWS, GCP, or Azure, using services that integrate well with these platforms can streamline development and maintenance. Choosing tools that natively integrate with your cloud provider, offer strong API support, and ensure that the chosen frameworks support these tools can be highly beneficial.</li> 
   <li class="readable-text" id="p96"><em>Cost efficienc</em><em>y</em>—LLMs require much more resources than traditional ML models. Costs, even with pay-as-you-go models, can escalate quickly with scale. Caching and inference optimization can help manage the costs.</li> 
   <li class="readable-text" id="p97"><em>Domain adaptatio</em><em>n</em>—RAG systems often need to be adapted to specific industries or domains (e.g., healthcare and legal). Pre-trained models might not be fully effective for specific use cases unless fine-tuned with domain-specific data. For domain adaptation, models that can be easily fine-tuned should be chosen. Existing domain-specific models can also be considered. </li> 
   <li class="readable-text" id="p98"><em>Vendor lock-in constraint</em><em>s</em>—Since generative AI is an evolving field, using proprietary tools or services from a single vendor may lead to vendor lock-in, making it difficult to migrate to other platforms or adjust your stack as requirements change. Using open source or interoperable technologies where possible helps in maintaining flexibility. Choosing tools that are cloud-agnostic or support multi-cloud deployments to reduce dependency on a single vendor. A modular architecture is advised to swap components without a system redesign.</li> 
   <li class="readable-text" id="p99"><em>Community suppor</em><em>t</em>—Strong community support means access to resources, tutorials, troubleshooting, and regular updates, which can accelerate development and reduce debugging time. This is especially true for rapidly evolving fields such as LLMs and RAG. Tools with active communities such as Hugging Face, LangChain, and similar are more likely to offer frequent updates, plugins, and third-party integrations.</li> 
  </ul> 
  <div class="readable-text" id="p100"> 
   <p>With the knowledge of the critical, essential, and enhancement layers, you should be ready to put together a technology stack to build your RAG system. Let’s now look at some common pitfalls and best practices to consider when building and deploying production-grade RAG system.</p> 
  </div> 
  <div class="readable-text" id="p101"> 
   <h2 class=" readable-text-h2"><span class="num-string">7.2</span> Production best practices</h2> 
  </div> 
  <div class="readable-text" id="p102"> 
   <p>Despite earnest efforts in designing and planning the RAG system, some problems will inevitably creep up during development and deployment. Although RAG is still in its nascent form, some early trends of common mishaps and best practices have emerged. There have been many experiments and learnings derived from them to make RAG systems work. This section discusses five such practices:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p103"><em class="CharOverride-4">Latency of the system</em>—RAG systems can introduce latency due to the need for multiple steps: retrieval, reranking, and generation. High latency can significantly degrade user experience, especially in real-time applications like chatbots or interactive search engines, which happens because each component adds processing time. Effective classification and routing of the queries can help in optimizing latency. A filtering approach is useful in hybrid retrieval, which first filters the embeddings based on keywords or sparse retrieval techniques and then uses similarity search on the filtered results. This reduces the time taken to calculate similarity, especially in large knowledge bases. </li> 
   <li class="readable-text" id="p104"><em>Continued hallucinatio</em><em>n</em>—Despite best efforts, LLMs may continue to generate responses that are factually incorrect or irrelevant to the retrieved content. This may happen if the retrieved data is ambiguous or incomplete. Post-processing validation steps may be required to address these. A common approach is to make RAG systems recommendation oriented rather than action oriented. This means that a human is looped into the system for verification and final action. </li> 
   <li class="readable-text" id="p105"><em class="CharOverride-4">Insufficient scalability planning</em>—Early prototypes of RAG systems often work well on small datasets but can struggle as the volume of data or the number of concurrent users grows. Managed vector database services with autoscaling features can be an easier way to plan for growth in demand and computation requirements. Similarly, autoscaling can also be used for the overall application using cloud-native solutions such as AWS Lambda.</li> 
   <li class="readable-text" id="p106"><em>Domain-adaptation challenge</em><em>s</em>—The embeddings and language models may not work well in niche or specialized domains. Also, the retrieval model and the language model may not always complement each other well, leading to disjointed or incoherent results. Retrieval models and LLMs are often developed and fine-tuned independently, which can cause a mismatch between the content retrieved and the way the LLM generates responses. It becomes important to fine-tune both the retrieval and generation models together for highly specialized domains.</li> 
   <li class="readable-text" id="p107"><em class="CharOverride-4">Inadequate handling of data privacy and PII</em>—Pre-trained models may generate content that includes sensitive information (e.g., personal data and confidential details) due to biases in training data. RAG systems may inadvertently leak sensitive information or personally identifiable information (PII) in their responses, leading to privacy breaches. Data exfiltration, also known as data theft, extrusion, or exportation, is a major threat in the digital world. The solution is to use PII masking and data redaction during both the pre- and post-processing stages. Ensure compliance with privacy regulations such as GDPR or HIPAA and deploy models with privacy filters.</li> 
  </ul> 
  <div class="readable-text" id="p108"> 
   <p>The list of best practices continues to evolve. Latency and scalability are critical for managing user experience and access. The promise of hallucination-free generation and data safety needs to be maintained for the reliability of the system. Table 7.2 summarizes the challenges of and potential solutions to putting RAG systems into production.</p> 
  </div> 
  <div class="browsable-container browsable-table-container" id="p109"> 
   <h5 class=" browsable-container-h5">Table 7.2 Production challenges and potential solutions</h5> 
   <table id="table002" class="No-Table-Style TableOverride-1"> 
    <colgroup> 
     <col class="_idGenTableRowColumn-14" /> 
     <col class="_idGenTableRowColumn-15" /> 
     <col class="_idGenTableRowColumn-15" /> 
    </colgroup> 
    <tbody> 
     <tr class="No-Table-Style _idGenTableRowColumn-16"> 
      <td class="No-Table-Style CellOverride-1"> <p class="_TableHead">Challenge</p> </td> 
      <td class="No-Table-Style CellOverride-2"> <p class="_TableHead">Description</p> </td> 
      <td class="No-Table-Style CellOverride-3"> <p class="_TableHead">Solution</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-17"> 
      <td class="No-Table-Style CellOverride-4"> <p class="_TableBody">Latency of the system</p> </td> 
      <td class="No-Table-Style CellOverride-5"> <p class="_TableBody">RAG systems add latency due to retrieval, re-ranking, and generation steps, affecting real-time performance.</p> </td> 
      <td class="No-Table-Style CellOverride-6"> <p class="_TableBody">Use query classification, hybrid retrieval filtering, and limit similarity searches</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-18"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Continued hallucination</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">LLMs may generate incorrect or irrelevant responses due to ambiguous or incomplete data.</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Add post-processing validation and make systems recommendation-based with human verification.</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-10"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Insufficient scalability planning</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Early RAG systems struggle with scalability as data and user load grow.</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Use autoscaling vector databases and cloud solutions such as AWS Lambda.</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-19"> 
      <td class="No-Table-Style CellOverride-7"> <p class="_TableBody">Domain-adaptation challenges</p> </td> 
      <td class="No-Table-Style CellOverride-8"> <p class="_TableBody">Embeddings and LLMs may perform poorly in specialized domains, leading to incoherent results.</p> </td> 
      <td class="No-Table-Style CellOverride-9"> <p class="_TableBody">Fine-tune both retrieval and generation models for niche use cases.</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-10"> 
      <td class="No-Table-Style CellOverride-10"> <p class="_TableBody">Inadequate handling of data privacy and PII</p> </td> 
      <td class="No-Table-Style CellOverride-11"> <p class="_TableBody">Models may expose sensitive data or PII, leading to privacy issues.</p> </td> 
      <td class="No-Table-Style CellOverride-12"> <p class="_TableBody">Apply PII masking, data redaction, and privacy filters, ensuring compliance with regulations.</p> </td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p110"> 
   <p>In this chapter, we have looked at a holistic RAGOps stack that enables the building of production-grade RAG systems. You also learned about some commonly available tools and technologies, along with a few best practices. This brings us to a close in our discussion of the RAGOps stack. We have now completed part 3 of the book, which means you should be ready to build RAG systems and put them into production. In the last part of this book, we discuss some emerging patterns in RAG-like multimodal capabilities, agentic RAG, and graphRAG, along with closing comments on future directions and continued learning.</p> 
  </div> 
  <div class="readable-text" id="p111"> 
   <h2 class=" readable-text-h2">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p112">RAGOps stack is a layered approach to designing a RAG system. </li> 
   <li class="readable-text" id="p113">These layers are categorized into critical, essential, and enhancement layers.</li> 
   <li class="readable-text" id="p114">Critical layers are fundamental for operation; essential layers ensure performance and reliability; and enhancement layers improve efficiency, scalability, and usability.</li> 
  </ul> 
  <div class="readable-text" id="p115"> 
   <h3 class=" readable-text-h3">Critical layers</h3> 
  </div> 
  <ul> 
   <li class="readable-text" id="p116"><em>Data laye</em><em>r</em>—Responsible for collecting, transforming, and storing the knowledge base. Ingestion tools such as AWS Glue, Azure Data Factory, and Apache Kafka enable data collection. Data transformation includes chunking, metadata enrichment, and converting data into vector formats. Tools such as FAISS, Pinecone, and Neo4j are used for storing embeddings and graph data.</li> 
   <li class="readable-text" id="p117"><em>Model laye</em><em>r</em>—Includes embeddings models and LLMs for generation. Embeddings models transform the text into vectors, with options from OpenAI, Google, Cohere, and Hugging Face. Foundation models (LLMs) such as GPT, Claude, and Llama generate outputs and evaluate tasks. Task-specific models handle specialized tasks such as query classification and bias detection.</li> 
   <li class="readable-text" id="p118"><em>Model deploymen</em><em>t</em>—Manages hosting and serving of LLMs and embeddings models. Popular platforms include AWS SageMaker, Google Vertex, and Hugging Face. Inference optimization reduces response time and costs with methods such as quantization and batching.</li> 
   <li class="readable-text" id="p119"><em>Application orchestration laye</em><em>r</em>—Coordinates data flow between different components: 
    <ul> 
     <li>Query orchestration handles query classification and optimization.</li> 
     <li>Retrieval coordination manages retrieval methods like dense or hybrid search.</li> 
     <li>Generation coordination handles prompt generation and post-retrieval tasks such as re-ranking.</li> 
    </ul> </li> 
  </ul> 
  <div class="readable-text" id="p120"> 
   <h3 class=" readable-text-h3">Essential layers</h3> 
  </div> 
  <ul> 
   <li class="readable-text" id="p121"><em class="CharOverride-4">Prompt layer</em>—Ensures prompts are well-engineered to guide LLMs for relevant, accurate responses. Tools such as LangChain and Azure Prompt Flow assist in prompt management.</li> 
   <li class="readable-text" id="p122"><em>Evaluation laye</em><em>r</em>—Monitors system performance by evaluating retrieval accuracy, faithfulness, and context relevance. Tools such as TruLens and Ragas provide evaluation frameworks.</li> 
   <li class="readable-text" id="p123"><em class="CharOverride-4">Monitoring layer</em>—Tracks system health, resource usage, and latency. Platforms such as TraceLoop and Galileo provide monitoring services.</li> 
   <li class="readable-text" id="p124"><em>LLM security and privacy laye</em><em>r</em>—Protects against data breaches and prompt injection attacks. Tools such as encryption, anonymization, and differential privacy should be used to safeguard sensitive data.</li> 
   <li class="readable-text" id="p125"><em class="CharOverride-4">Caching layer</em>—Caches frequently generated responses to reduce costs and latency in RAG systems.</li> 
  </ul> 
  <div class="readable-text" id="p126"> 
   <h3 class=" readable-text-h3">Enhancement layers</h3> 
  </div> 
  <ul> 
   <li class="readable-text" id="p127"><em>Human-in-the-loop laye</em><em>r</em>—Adds human oversight to ensure higher accuracy and ethical decision-making.</li> 
   <li class="readable-text" id="p128"><em>Cost optimization laye</em><em>r</em>—Reduces infrastructure costs, especially in large-scale RAG systems.</li> 
   <li class="readable-text" id="p129"><em>Explainability and interpretability laye</em><em>r</em>—Provides transparency into system decisions, critical for domains such as healthcare and legal.</li> 
   <li class="readable-text" id="p130"><em>Collaboration and experimentation laye</em><em>r</em>—Useful for team-based development and continuous improvement.</li> 
  </ul> 
  <div class="readable-text" id="p131"> 
   <h3 class=" readable-text-h3">Production best practices</h3> 
  </div> 
  <ul> 
   <li class="readable-text" id="p132"><em>Latenc</em><em>y</em>—RAG systems often introduce latency due to multiple steps. Using techniques such as filtering in hybrid retrieval can help reduce response times.</li> 
   <li class="readable-text" id="p133"><em>Hallucinatio</em><em>n</em>—LLMs may still generate incorrect responses. Post-processing validation and human-in-the-loop systems help mitigate this.</li> 
   <li class="readable-text" id="p134"><em>Scalabilit</em><em>y</em>—Early prototypes may struggle to scale. Managed vector database services with autoscaling can help plan for growth.</li> 
   <li class="readable-text" id="p135"><em>Domain adaptatio</em><em>n</em>—Embeddings and language models may not perform well in niche domains. Fine-tuning both retrieval and generation models is necessary.</li> 
   <li class="readable-text" id="p136"><em>Data privac</em><em>y</em>—Models may leak sensitive information. PII masking, encryption, and compliance with data regulations are essential for protecting user data.</li> 
  </ul>
 </body>
</html>