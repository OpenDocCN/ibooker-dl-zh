<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Introduction to TensorFlow, PyTorch, JAX, and Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Introduction to TensorFlow, PyTorch, JAX, and Keras</h1>
<blockquote>原文：<a href="https://deeplearningwithpython.io/chapters/chapter03_introduction-to-ml-frameworks">https://deeplearningwithpython.io/chapters/chapter03_introduction-to-ml-frameworks</a></blockquote>


<aside>
<p>This chapter covers
</p>
<ul>
<li>A closer look at all major deep learning frameworks and their relationships</li>
<li>An overview of how core deep learning concepts translate to code across
  all frameworks</li>
</ul>
</aside>

<p>This chapter is meant to give you everything you need to start doing deep
learning in practice. First, you’ll get familiar with three popular deep learning
frameworks that can be used with Keras:</p>
<ul>
<li>TensorFlow (<a href="https://tensorflow.org">https://tensorflow.org</a>)</li>
<li>PyTorch (<a href="https://pytorch.org/">https://pytorch.org/</a>)</li>
<li>JAX (<a href="https://jax.readthedocs.io/">https://jax.readthedocs.io/</a>)</li>
</ul>
<p>Then, building on top of the first contact you’ve had with Keras in chapter 2,
we’ll review the core components of neural networks and how they translate
to Keras APIs.</p>
<p>By the end of this chapter, you’ll be ready to move on to practical, real-world
applications — which will start with chapter 4.</p>
<h2 id="a-brief-history-of-deep-learning-frameworks">A brief history of deep learning frameworks</h2>
<p>In the real world, you’re not going to be writing low-level code from scratch like
we did at the end of chapter 2. Instead, you’re going to use a framework.
Besides Keras, the main deep learning frameworks today are JAX, TensorFlow, and PyTorch.
This book will teach you about all four.</p>
<p>If you’re just getting started with deep learning,
it may seem like all these frameworks have been here forever.
In reality, they’re all quite recent, with Keras being the oldest among the four (launched in March 2015).
The ideas behind these frameworks, however, have a long history —
the first paper about automatic differentiation was published in 1964<sup class="footnote-link" id="footnote-link-1"><a href="#footnote-1">[1]</a></sup></p>
<p>All these frameworks combine three key features:</p>
<ul>
<li>A way to compute gradients for arbitrary differentiable functions (automatic differentiation)</li>
<li>A way to run tensor computations on CPUs and GPUs (and possibly even on other specialized deep learning hardware)</li>
<li>A way to distribute computation across multiple devices or multiple computers, such as multiple GPUs on one computer, or
even multiple GPUs across multiple separate computers</li>
</ul>
<p>Together, these three simple features unlock all modern deep learning.</p>
<p>It took a long time for the field to develop robust solutions for all three problems
and package those solutions in a reusable form. Since its inception in the 1960s and until the 2000s,
autodifferentiation had no practical applications in machine learning — folks who worked with neural networks
simply wrote their own gradient logic by hand, usually in a language like C++. Meanwhile, GPU programming was all but impossible.</p>
<p>Things started to slowly change in the late 2000s. First,
Python and its ecosystem were slowly rising in popularity in the scientific community, gaining traction
over MATLAB and C++. Second, NVIDIA released CUDA in 2006, unlocking the possibility of building
neural networks that could run on consumer GPUs. The initial focus on CUDA was on physics simulation
rather than machine learning, but that didn’t stop machine learning researchers from starting to implement
CUDA-based neural networks from 2009 onward. They were typically one-off implementations that ran on a single GPU
without any autodifferentiation.</p>
<p>The first framework to enable autodifferentiation and GPU computation to train deep learning models
was Theano, circa 2009. Theano is the conceptual ancestor of all modern deep learning tools.
It started getting good traction in the machine learning research community in 2013–2014,
after the results of the ImageNet 2012 competition ignited the world’s interest in deep learning.
Around the same time, a few other GPU-enabled deep learning libraries started gaining popularity in the computer vision world —
in particular, Torch 7 (Lua-based) and Caffe (C++-based). Keras launched in early 2015 as a higher-level,
easier-to-use deep learning library powered by Theano, and it quickly gained traction with the few thousands of people who were into deep learning
at the time.</p>
<p>Then in late 2015, Google launched TensorFlow, which took many of the key ideas from Theano and added support for large-scale
distributed computation. The release of TensorFlow was a watershed moment that precipitated deep learning in
the mainstream developer zeitgeist. Keras immediately added support for TensorFlow. By mid-2016, over half of all TensorFlow users
were using it through Keras.</p>
<p>In response to TensorFlow, Meta (named Facebook at the time) launched PyTorch about one year later, taking
ideas from Chainer (a niche but innovative framework launched in mid-2015, now long dead) and NumPy-Autograd, a CPU-only autodifferentiation
library for NumPy released by Maclaurin et al. in 2014. Meanwhile, Google released TPUs as an alternative to GPUs,
alongside XLA, a high-performance compiler developed to enable TensorFlow to run on TPUs.</p>
<p>A few years later, at Google, Matthew Johnson — one of the developers who worked on NumPy-Autograd — released JAX as an alternative
way to use autodifferentiation with XLA. JAX quickly gained traction with researchers thanks to its minimalistic API and
high scalability. Today, Keras, TensorFlow, PyTorch, and JAX are the top frameworks in the deep learning world.</p>
<p>Looking back on this chaotic history, we can ask, What’s next? Will a new framework arise tomorrow?
Will we switch to a new programming language or a new hardware platform?</p>
<p>If you ask me, three things today are certain:</p>
<ul>
<li>Python has won. Its machine learning and data science ecosystem simply has too much momentum at this point.
There won’t be a brand new language to replace it — at least not in the next 15 years.</li>
<li>We’re in a multiframework world — all four frameworks are well established and are unlikely to go anywhere in the next few years.
It’s a good idea for you to learn a little bit about each one.
However, it’s highly possible that <em>new</em> frameworks will gain popularity in the future,
in addition to them; Apple’s recently released MLX could be one such example.
In this context, using Keras is a considerable advantage: you should be able to run your existing Keras models
on any new up-and-coming framework via a new Keras backend. Keras will keep providing future-proof stability
to machine learning developers in the future, like it has since 2015 — back when neither TensorFlow nor PyTorch nor JAX existed.</li>
<li>New chips may certainly arise in the future, alongside NVIDIA’s GPUs and Google’s TPUs.
For instance, AMD’s GPU line likely has bright days ahead.
But any new such chip will have to work with the existing frameworks to gain traction.
New hardware is unlikely to disrupt your workflows.</li>
</ul>
<h2 id="how-these-frameworks-relate-to-each-other">How these frameworks relate to each other</h2>
<p>Keras, TensorFlow, PyTorch, and JAX don’t all have the same feature set and aren’t interchangeable.
They have some overlap, but to a large extent, they serve different roles for different use cases.
The biggest difference is between Keras and the three others. Keras is a high-level framework,
while the others are lower level. Imagine building a house. Keras is like a prefabricated building kit:
it provides a streamlined interface for setting up and training neural networks.
In contrast, TensorFlow, PyTorch, and JAX are like the raw materials used in construction.</p>
<p>As you saw in the previous chapters, training a neural network revolves
around the following concepts:</p>
<ul>
<li><em>First, low-level tensor manipulation</em> — The infrastructure that underlies
  all modern machine learning. This translates to low-level APIs found in TensorFlow,
  PyTorch<sup class="footnote-link" id="footnote-link-2"><a href="#footnote-2">[2]</a></sup>, and JAX:<ul>
<li><em>Tensors</em>, including special tensors that store the network’s state (<em>variables</em>)</li>
<li><em>Tensor operations</em> such as addition, <code>relu</code>, or <code>matmul</code></li>
<li><em>Backpropagation</em>, a way to compute the gradient of mathematical expressions</li>
</ul>
</li>
<li><em>Second, high-level deep learning concepts</em> — This translates to Keras APIs:<ul>
<li><em>Layers</em>, which are combined into a <em>model</em></li>
<li>A <em>loss function</em>, which defines the feedback signal used for learning</li>
<li>An <em>optimizer</em>, which determines how learning proceeds</li>
<li><em>Metrics</em> to evaluate model performance, such as accuracy</li>
<li>A <em>training loop</em> that performs mini-batch stochastic gradient descent</li>
</ul>
</li>
</ul>
<p>Further, Keras is unique in that it isn’t a fully standalone framework. It needs a <em>backend engine</em> to run, (see figure 3.4),
much like a prefabricated house-building kit needs to source building materials from somewhere.
TensorFlow, PyTorch, and JAX can all be used as Keras backends.
In addition, Keras can run on NumPy, but since NumPy does not provide an API for gradients,
Keras workflows on NumPy are restricted to making predictions from a model — training is impossible.</p>
<p>Now that you have a clearer understanding of how all these frameworks came to be and how they relate
to each other, let’s dive into what it’s like to work with them. We’ll cover them in chronological order:
TensorFlow first, then PyTorch, and finally JAX.</p>
<h2 id="introduction-to-tensorflow">Introduction to TensorFlow</h2>
<p>TensorFlow is a Python-based open source machine learning framework
developed primarily by Google. Its initial release was in November 2015,
followed by a v1 release in February 2017, and a v2 release in October 2019.
TensorFlow is heavily used in production-grade machine learning applications across the industry.</p>
<p>It’s important to keep in mind that TensorFlow is more than a single library.
It’s really a platform, home to a vast ecosystem of components, some
developed by Google, some developed by third parties. For instance, there’s
TFX for industry-strength machine learning workflow management,
TF-Serving for production deployment,
the TF Optimization Toolkit for model quantization and pruning,
and TFLite and MediaPipe for mobile application deployment.</p>
<p>Together, these components cover a very wide range of use cases,
from cutting-edge research to large-scale production applications.</p>
<h3 id="first-steps-with-tensorflow">First steps with TensorFlow</h3>
<p>Over the next paragraphs, you’ll get familiar with all the basics of TensorFlow. We’ll cover the following
key concepts:</p>
<ul>
<li>Tensors and variables</li>
<li>Numerical operations in TensorFlow</li>
<li>Computing gradients with a <code>GradientTape</code></li>
<li>Making TensorFlow functions fast by using just-in-time compilation</li>
</ul>
<p>We’ll then conclude the introduction with an end-to-end example: a pure-TensorFlow
implementation of linear regression.</p>
<p>Let’s get those tensors flowing.</p>
<h4 id="tensors-and-variables-in-tensorflow">Tensors and variables in TensorFlow</h4>
<p>To do anything in TensorFlow, we’re going to need some tensors. There are a few
different ways you can create them.</p>
<h5 id="constant-tensors">Constant tensors</h5>
<p>Tensors need to be created with some initial value, so common ways to create
tensors are via <code>tf.ones</code> (equivalent to <code>np.ones</code>) and <code>tf.zeros</code> (equivalent
to <code>np.zeros</code>). You can also create a tensor from Python or NumPy values using
<code>tf.constant</code>.</p>
<figure id="listing-3-1">
<pre><code class="language-python">&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; # Equivalent to np.ones(shape=(2, 1))
&gt;&gt;&gt; tf.ones(shape=(2, 1))</code>
<code class="language-output">tf.Tensor([[1.], [1.]], shape=(2, 1), dtype=float32)</code>
<code class="language-python">&gt;&gt;&gt; # Equivalent to np.zeros(shape=(2, 1))
&gt;&gt;&gt; tf.zeros(shape=(2, 1))</code>
<code class="language-output">tf.Tensor([[0.], [0.]], shape=(2, 1), dtype=float32)</code>
<code class="language-python">&gt;&gt;&gt; # Equivalent to np.array([1, 2, 3], dtype="float32")
&gt;&gt;&gt; tf.constant([1, 2, 3], dtype="float32")</code>
<code class="language-output">tf.Tensor([1., 2., 3.], shape=(3,), dtype=float32)</code></pre>
<figcaption>
<a href="#listing-3-1">Listing 3.1</a>: All-ones or all-zeros tensors
</figcaption>
</figure>

<h5 id="random-tensors">Random tensors</h5>
<p>You can also create tensors filled with random values via
one of the methods of the <code>tf.random</code> submodule (equivalent to
the <code>np.random</code> submodule).</p>
<figure id="listing-3-2">
<pre><code class="language-python">&gt;&gt;&gt; # Tensor of random values drawn from a normal distribution with
&gt;&gt;&gt; # mean 0 and standard deviation 1. Equivalent to
&gt;&gt;&gt; # np.random.normal(size=(3, 1), loc=0., scale=1.).
&gt;&gt;&gt; x = tf.random.normal(shape=(3, 1), mean=0., stddev=1.)
&gt;&gt;&gt; print(x)</code>
<code class="language-output">tf.Tensor(
[[-0.14208166]
 [-0.95319825]
 [ 1.1096532 ]], shape=(3, 1), dtype=float32)</code>
<code class="language-python">&gt;&gt;&gt; # Tensor of random values drawn from a uniform distribution between
&gt;&gt;&gt; # 0 and 1. Equivalent to np.random.uniform(size=(3, 1), low=0.,
&gt;&gt;&gt; # high=1.).
&gt;&gt;&gt; x = tf.random.uniform(shape=(3, 1), minval=0., maxval=1.)
&gt;&gt;&gt; print(x)</code>
<code class="language-output">tf.Tensor(
[[0.33779848]
 [0.06692922]
 [0.7749394 ]], shape=(3, 1), dtype=float32)</code></pre>
<figcaption>
<a href="#listing-3-2">Listing 3.2</a>: Random tensors
</figcaption>
</figure>

<h5 id="tensor-assignment-and-the-variable-class">Tensor assignment and the Variable class</h5>
<p>A significant difference between NumPy arrays and TensorFlow tensors is that
TensorFlow tensors aren’t assignable: they’re constant. For instance, in
NumPy, you can do the following.</p>
<figure id="listing-3-3">
<pre><code class="language-python">import numpy as np

x = np.ones(shape=(2, 2))
x[0, 0] = 0.0
</code></pre>
<figcaption>
<a href="#listing-3-3">Listing 3.3</a>: NumPy arrays are assignable
</figcaption>
</figure>

<p>Try to do the same thing in TensorFlow: you will get an error,
<code>EagerTensor object does not support item assignment</code>.</p>
<figure id="listing-3-4">
<pre><code class="language-python">x = tf.ones(shape=(2, 2))
# This will fail, as a tensor isn't assignable.
x[0, 0] = 0.0
</code></pre>
<figcaption>
<a href="#listing-3-4">Listing 3.4</a>: TensorFlow tensors are not assignable
</figcaption>
</figure>

<p>To train a model, we’ll need to update its state, which is a set of tensors.
If tensors aren’t assignable, how do we do it, then?
That’s where variables come in. <code>tf.Variable</code> is the
class meant to manage modifiable state in TensorFlow.</p>
<p>To create a variable, you need to provide some initial value, such as a random
tensor.</p>
<figure id="listing-3-5">
<pre><code class="language-python">&gt;&gt;&gt; v = tf.Variable(initial_value=tf.random.normal(shape=(3, 1)))
&gt;&gt;&gt; print(v)</code>
<code class="language-output">array([[-0.75133973],
       [-0.4872893 ],
       [ 1.6626885 ]], dtype=float32)&gt;</code></pre>
<figcaption>
<a href="#listing-3-5">Listing 3.5</a>: Creating a <code>tf.Variable</code>
</figcaption>
</figure>

<p>The state of a variable can be modified via its <code>assign</code> method.</p>
<figure id="listing-3-6">
<pre><code class="language-python">&gt;&gt;&gt; v.assign(tf.ones((3, 1)))</code>
<code class="language-output">array([[1.],
       [1.],
       [1.]], dtype=float32)&gt;</code></pre>
<figcaption>
<a href="#listing-3-6">Listing 3.6</a>: Assigning a value to a <code>Variable</code>
</figcaption>
</figure>

<p>Assignment also works for a subset of the coefficients.</p>
<figure id="listing-3-7">
<pre><code class="language-python">&gt;&gt;&gt; v[0, 0].assign(3.)</code>
<code class="language-output">array([[3.],
       [1.],
       [1.]], dtype=float32)&gt;</code></pre>
<figcaption>
<a href="#listing-3-7">Listing 3.7</a>: Assigning a value to a subset of a <code>Variable</code>
</figcaption>
</figure>

<p>Similarly, <code>assign_add</code> and <code>assign_sub</code> are efficient equivalents of
<code>+=</code> and <code>-=</code>.</p>
<figure id="listing-3-8">
<pre><code class="language-python">&gt;&gt;&gt; v.assign_add(tf.ones((3, 1)))</code>
<code class="language-output">array([[2.],
       [2.],
       [2.]], dtype=float32)&gt;</code></pre>
<figcaption>
<a href="#listing-3-8">Listing 3.8</a>: Using <code>assign_add</code>
</figcaption>
</figure>

<h4 id="tensor-operations-doing-math-in-tensorflow">Tensor operations: Doing math in TensorFlow</h4>
<p>Just like NumPy, TensorFlow offers a large collection of tensor operations
to express mathematical formulas. Here are a few examples.</p>
<figure id="listing-3-9">
<pre><code class="language-python">a = tf.ones((2, 2))
# Takes the square, same as np.square
b = tf.square(a)
# Takes the square root, same as np.sqrt
c = tf.sqrt(a)
# Adds two tensors (element-wise)
d = b + c
# Takes the product of two tensors (see chapter 2), same as np.matmul
e = tf.matmul(a, b)
# Concatenates a and b along axis 0, same as np.concatenate
f = tf.concat((a, b), axis=0)
</code></pre>
<figcaption>
<a href="#listing-3-9">Listing 3.9</a>: A few basic math operations in TensorFlow
</figcaption>
</figure>

<p>Here’s an equivalent of the <code>Dense</code> layer we saw in chapter 2:</p>
<figure>
<pre><code class="language-python">def dense(inputs, W, b):
    return tf.nn.relu(tf.matmul(inputs, W) + b)
</code></pre>
</figure>

<h4 id="gradients-in-tensorflow-a-second-look-at-the-gradienttape-api">Gradients in TensorFlow: A second look at the GradientTape API</h4>
<p>So far, TensorFlow seems to look a lot like NumPy. But here’s something NumPy
can’t do: retrieve the gradient of any differentiable expression with respect
to any of its inputs. Just open a <code>GradientTape</code> scope, apply some computation
to one or several input tensors, and retrieve the gradient of the result with
respect to the inputs.</p>
<figure id="listing-3-10">
<pre><code class="language-python">input_var = tf.Variable(initial_value=3.0)
with tf.GradientTape() as tape:
    result = tf.square(input_var)
gradient = tape.gradient(result, input_var)
</code></pre>
<figcaption>
<a href="#listing-3-10">Listing 3.10</a>: Using the <code>GradientTape</code>
</figcaption>
</figure>

<p>This is most commonly used to retrieve the gradients of the loss of a model
with respect to its weights: <code>gradients = tape.gradient(loss, weights)</code>.</p>
<p>In chapter 2, you saw how the <code>GradientTape</code> works on either a single
input or a list of inputs and how inputs could be either scalars
or high-dimensional tensors.</p>
<p>So far, you’ve only seen the case where the input tensors in <code>tape.gradient()</code>
were TensorFlow variables. It’s actually possible for these inputs
to be any arbitrary tensor. However, only <em>trainable variables</em> are being tracked
by default.
With a constant tensor, you’d have to manually mark it as being tracked,
by calling <code>tape.watch()</code> on it.</p>
<figure id="listing-3-11">
<pre><code class="language-python">input_const = tf.constant(3.0)
with tf.GradientTape() as tape:
    tape.watch(input_const)
    result = tf.square(input_const)
gradient = tape.gradient(result, input_const)
</code></pre>
<figcaption>
<a href="#listing-3-11">Listing 3.11</a>: Using the <code>GradientTape</code> with constant tensor inputs
</figcaption>
</figure>

<p>Why? Because it would be too expensive to preemptively store
the information required to compute the gradient of anything with respect
to anything. To avoid wasting resources, the tape needs to know what to watch.
Trainable variables are watched by default because computing the gradient
of a loss with regard to a list of trainable variables is the most common use
case of the gradient tape.</p>
<p>The gradient tape is a powerful utility, even capable of computing
<em>second-order gradients</em> — that is, the gradient of a gradient.
For instance, the gradient of the position of an object with
regard to time is the speed of that object, and the second-order gradient
is its acceleration.</p>
<p>If you measure the position of a falling apple along a vertical axis over time,
and find that it verifies <code>position(time) = 4.9 * time ** 2</code>,
what is its acceleration? Let’s use two nested gradient tapes to find out.</p>
<figure id="listing-3-12">
<pre><code class="language-python">time = tf.Variable(0.0)
with tf.GradientTape() as outer_tape:
    with tf.GradientTape() as inner_tape:
        position = 4.9 * time**2
    speed = inner_tape.gradient(position, time)
# We use the outer tape to compute the gradient of the gradient from
# the inner tape. Naturally, the answer is 4.9 * 2 = 9.8.
acceleration = outer_tape.gradient(speed, time)
</code></pre>
<figcaption>
<a href="#listing-3-12">Listing 3.12</a>: Using nested gradient tapes to compute second-order gradients
</figcaption>
</figure>

<h4 id="making-tensorflow-functions-fast-using-compilation">Making TensorFlow functions fast using compilation</h4>
<p>All the TensorFlow code you’ve written so far has been executing “eagerly.”
This means operations are executed one after the other in the Python runtime,
much like any Python code or NumPy code. Eager execution is great for debugging,
but it is typically quite slow. It can often be beneficial
to parallelize some computation, or “fuse” operations — replacing two consecutive operations,
like <code>matmul</code> followed by <code>relu</code>, with a single, more efficient operation that does the same
thing without materializing the intermediate output.</p>
<p>This can be achieved via <em>compilation</em>. The general idea of compilation is to take
certain functions you’ve written in Python, lift them out of Python, automatically rewrite
them into a faster and more efficient “compiled program,” and then call that program from the Python
runtime.</p>
<p>The main benefit of compilation is improved performance. There’s a drawback too: the code you write is no longer
the code that gets executed, which can make the debugging experience painful. Only turn on compilation after you’ve already debugged your code
in the Python runtime.</p>
<p>You can apply compilation to any TensorFlow function by wrapping it in a <code>tf.function</code> decorator, like this:</p>
<figure>
<pre><code class="language-python">@tf.function
def dense(inputs, W, b):
    return tf.nn.relu(tf.matmul(inputs, W) + b)
</code></pre>
</figure>

<p>When you do this, any call to <code>dense()</code> is replaced with a call to a compiled program that implements
a more optimized version of the function. The first call to the function will take a bit longer, because TensorFlow
will be compiling your code. This only happens once — all subsequent calls to the same function will be fast.</p>
<p>TensorFlow has two compilation modes:</p>
<ul>
<li>First, the default one, which we refer to as “graph mode.” Any function
  decorated with <code>@tf.function</code> runs in graph mode.</li>
<li>Second, compilation with XLA, a high-performance compiler for ML (it’s short
  for Accelerated Linear Algebra). You can turn it on by specifying
  <code>jit_compile=True</code>, like this:</li>
</ul>
<figure>
<pre><code class="language-python">@tf.function(jit_compile=True)
def dense(inputs, W, b):
    return tf.nn.relu(tf.matmul(inputs, W) + b)
</code></pre>
</figure>

<p>It is often the case that compiling a function with XLA will make it run faster than graph mode — though it takes more time to execute the function
the first time, since the compiler has more work to do.</p>
<h3 id="an-end-to-end-example-a-linear-classifier-in-pure-tensorflow">An end-to-end example: A linear classifier in pure TensorFlow</h3>
<p>You know about tensors, variables, and tensor operations, and you know how to
compute gradients. That’s enough to build any TensorFlow-based machine learning model based
on gradient descent. Let’s walk through an end-to-end example to make sure everything is
crystal clear.</p>
<p>In a machine learning job interview, you may be asked to implement a linear
classifier from scratch: a very simple task
that serves as a filter between candidates who have some minimal machine
learning background, and those who don’t. Let’s get you past that filter,
and use your newfound knowledge of TensorFlow to implement such
a linear classifier.</p>
<p>First, let’s come up with some nicely linearly separable
synthetic data to work with: two classes of points in a 2D plane.</p>
<figure id="listing-3-13">
<pre><code class="language-python">import numpy as np

num_samples_per_class = 1000
negative_samples = np.random.multivariate_normal(
    # Generates the first class of points: 1,000 random 2D points with
    # specified "mean" and "covariance matrix." Intuitively, the
    # "covariance matrix" describes the shape of the point cloud, and
    # the "mean" describes its position in the plane. `cov=[[1,
    # 0.5],[0.5, 1]]` corresponds to "an oval-like point cloud oriented
    # from bottom left to top right."
    mean=[0, 3], cov=[[1, 0.5], [0.5, 1]], size=num_samples_per_class
)
positive_samples = np.random.multivariate_normal(
    # Generates the other class of points with a different mean and the
    # same covariance matrix (point cloud with a different position and
    # the same shape)
    mean=[3, 0], cov=[[1, 0.5], [0.5, 1]], size=num_samples_per_class
)
</code></pre>
<figcaption>
<a href="#listing-3-13">Listing 3.13</a>: Generating two classes of random points in a 2D plane
</figcaption>
</figure>

<p><code>negative_samples</code> and <code>positive_samples</code> are both arrays with shape <code>(1000, 2)</code>.
Let’s stack them into a single array with shape <code>(2000, 2)</code>.</p>
<figure id="listing-3-14">
<pre><code class="language-python">inputs = np.vstack((negative_samples, positive_samples)).astype(np.float32)
</code></pre>
<figcaption>
<a href="#listing-3-14">Listing 3.14</a>: Stacking the two classes into an array with shape <code>(2000, 2)</code>
</figcaption>
</figure>

<p>Let’s generate the corresponding target labels, an array of 0s and 1s of
shape <code>(2000, 1)</code>, where <code>targets[i, 0]</code> is 0 if <code>inputs[i]</code> belongs to class 0
(and inversely).</p>
<figure id="listing-3-15">
<pre><code class="language-python">targets = np.vstack(
    (
        np.zeros((num_samples_per_class, 1), dtype="float32"),
        np.ones((num_samples_per_class, 1), dtype="float32"),
    )
)
</code></pre>
<figcaption>
<a href="#listing-3-15">Listing 3.15</a>: Generating the corresponding targets (0 and 1)
</figcaption>
</figure>

<p>Let’s plot our data with Matplotlib, a well-known Python data visualization
library (it comes preinstalled in Colab, so no need for you to install it
yourself), as shown in figure 3.1.</p>
<figure id="listing-3-16">
<pre><code class="language-python">import matplotlib.pyplot as plt

plt.scatter(inputs[:, 0], inputs[:, 1], c=targets[:, 0])
plt.show()
</code></pre>
<figcaption>
<a href="#listing-3-16">Listing 3.16</a>: Plotting the two point classes
</figcaption>
</figure>

<figure id="figure-3-1">
<img src="../Images/4ff4f19ade555d51b5dc7ea520e28a8b.png" data-original-src="https://deeplearningwithpython.io/images/ch03/linear_model_inputs.282fc3b6.png"/>
<figcaption>
<a href="#figure-3-1">Figure 3.1</a>: Our synthetic data: two classes of random points in the 2D plane
</figcaption>
</figure>

<p>Now, let’s create a linear classifier that can learn to separate these two blobs.
A linear classifier is an affine transformation (<code>prediction = matmul(input, W) + b</code>)
trained to minimize the square of the difference between predictions
and the targets.</p>
<p>As you’ll see, it’s actually a much simpler example
than the end-to-end example of a toy two-layer neural network from
the end of chapter 2. However, this time,
you should be able to understand everything about the code, line by line.</p>
<p>Let’s create our variables <code>W</code> and <code>b</code>, initialized with
random values and with zeros, respectively.</p>
<figure id="listing-3-17">
<pre><code class="language-python"># The inputs will be 2D points.
input_dim = 2
# The output predictions will be a single score per sample (close to 0
# if the sample is predicted to be in class 0, and close to 1 if the
# sample is predicted to be in class 1).
output_dim = 1
W = tf.Variable(initial_value=tf.random.uniform(shape=(input_dim, output_dim)))
b = tf.Variable(initial_value=tf.zeros(shape=(output_dim,)))
</code></pre>
<figcaption>
<a href="#listing-3-17">Listing 3.17</a>: Creating the linear classifier variables
</figcaption>
</figure>

<p>Here’s our forward pass function.</p>
<figure id="listing-3-18">
<pre><code class="language-python">def model(inputs, W, b):
    return tf.matmul(inputs, W) + b
</code></pre>
<figcaption>
<a href="#listing-3-18">Listing 3.18</a>: The forward pass function
</figcaption>
</figure>

<p>Because our linear classifier operates on 2D inputs, <code>W</code> is really just two
scalar coefficients: <code>W = [[w1], [w2]]</code>.
Meanwhile, <code>b</code> is a single scalar coefficient. As such, for given input point
<code>[x, y]</code>, its prediction value is
<code>prediction = [[w1], [w2]] • [x, y] + b = w1 * x + w2 * y + b</code>.</p>
<p>Here’s our loss function.</p>
<figure id="listing-3-19">
<pre><code class="language-python">def mean_squared_error(targets, predictions):
    # per_sample_losses will be a tensor with the same shape as targets
    # and predictions, containing per-sample loss scores.
    per_sample_losses = tf.square(targets - predictions)
    # We need to average these per-sample loss scores into a single
    # scalar loss value: reduce_mean does this.
    return tf.reduce_mean(per_sample_losses)
</code></pre>
<figcaption>
<a href="#listing-3-19">Listing 3.19</a>: The mean squared error loss function
</figcaption>
</figure>

<p>Now, we move to the training step, which receives some training data and updates the
weights <code>W</code> and <code>b</code> to minimize the loss on the data.</p>
<figure id="listing-3-20">
<pre><code class="language-python">learning_rate = 0.1

# Wraps the function in a tf.function decorator to speed it up
@tf.function(jit_compile=True)
def training_step(inputs, targets, W, b):
    # Forward pass, inside of a gradient tape scope
    with tf.GradientTape() as tape:
        predictions = model(inputs, W, b)
        loss = mean_squared_error(predictions, targets)
    # Retrieves the gradient of the loss with regard to weights
    grad_loss_wrt_W, grad_loss_wrt_b = tape.gradient(loss, [W, b])
    # Updates the weights
    W.assign_sub(grad_loss_wrt_W * learning_rate)
    b.assign_sub(grad_loss_wrt_b * learning_rate)
    return loss
</code></pre>
<figcaption>
<a href="#listing-3-20">Listing 3.20</a>: The training-step function
</figcaption>
</figure>

<p>For simplicity, we’ll do <em>batch training</em> instead of <em>mini-batch training</em>:
we’ll run each training step (gradient computation and weight update) on the
entire data, rather than iterate over the data in small batches. On one hand,
this means that each training step will take much longer to run, since we
compute the forward pass and the gradients for 2,000 samples at once.
On the other hand, each gradient update will be much more effective at reducing
the loss on the training data, since it will encompass information from all
training samples instead of, say, only 128 random samples.
As a result, we will need many fewer steps of training, and we should use
a larger learning rate than what we would typically use for mini-batch training
(we’ll use <code>learning_rate = 0.1</code>, as previously defined).</p>
<figure id="listing-3-21">
<pre><code class="language-python">for step in range(40):
    loss = training_step(inputs, targets, W, b)
    print(f"Loss at step {step}: {loss:.4f}")
</code></pre>
<figcaption>
<a href="#listing-3-21">Listing 3.21</a>: The batch training loop
</figcaption>
</figure>

<p>After 40 steps, the training loss seems to have stabilized around 0.025.
Let’s plot how our linear model classifies the training data points, as shown in figure 3.2.
Because our targets are 0s and 1s, a given input point
will be classified as “0” if its prediction value is below 0.5,
and as “1” if it is above 0.5:</p>
<figure>
<pre><code class="language-python">predictions = model(inputs, W, b)
plt.scatter(inputs[:, 0], inputs[:, 1], c=predictions[:, 0] &gt; 0.5)
plt.show()
</code></pre>
</figure>

<figure id="figure-3-2">
<img src="../Images/874d3f2bae456f9cb13b84e03bbe8466.png" data-original-src="https://deeplearningwithpython.io/images/ch03/linear_model_predictions.3e5424ac.png"/>
<figcaption>
<a href="#figure-3-2">Figure 3.2</a>: Our model’s predictions on the training inputs: pretty similar to the training targets
</figcaption>
</figure>

<p>Recall that the prediction value for a given point <code>[x, y]</code> is simply
<code>prediction == [[w1], [w2]] • [x, y] + b == w1 * x + w2 * y + b</code>.
 Thus, class “0” is defined as
<code>w1 * x + w2 * y + b &lt; 0.5</code> and class “1” is defined as
<code>w1 * x + w2 * y + b &gt; 0.5</code>. You’ll notice that what you’re looking at is
really the equation of a line in the 2D plane: <code>w1 * x + w2 * y + b = 0.5</code>.
Class 1 is above the line; class 0 is below the line.
You may be used to seeing line equations in the format <code>y = a * x + b</code>; in the same
format, our line becomes <code>y = - w1 / w2 * x + (0.5 - b) / w2</code>.</p>
<p>Let’s plot this line, as shown in figure 3.3:</p>
<figure>
<pre><code class="language-python"># Generates 100 regularly spaced numbers between -1 and 4, which we
# will use to plot our line
x = np.linspace(-1, 4, 100)
# This is our line's equation.
y = -W[0] / W[1] * x + (0.5 - b) / W[1]
# Plots our line (`"-r"` means "plot it as a red line")
plt.plot(x, y, "-r")
# Plots our model's predictions on the same plot
plt.scatter(inputs[:, 0], inputs[:, 1], c=predictions[:, 0] &gt; 0.5)
</code></pre>
</figure>

<figure id="figure-3-3">
<img src="../Images/dfe1caf49e1828628a7c2c086715ca4d.png" data-original-src="https://deeplearningwithpython.io/images/ch03/linear_model_with_plotted_line.fd88e7bc.png"/>
<figcaption>
<a href="#figure-3-3">Figure 3.3</a>: Our model, visualized as a line
</figcaption>
</figure>

<p>This is really what a linear classifier is all about: finding the parameters
of a line (or, in higher-dimensional spaces, a hyperplane) neatly separating
two classes of data.</p>
<h3 id="what-makes-the-tensorflow-approach-unique">What makes the TensorFlow approach unique</h3>
<p>You’re now familiar with all the basic APIs that underlie TensorFlow-based workflows,
and you’re about to dive into more frameworks — in particular, PyTorch and JAX. What makes
working with TensorFlow different from working with any other framework? When should you use TensorFlow,
and when could you use something else?</p>
<p>If you ask us, here are the main benefits of TensorFlow:</p>
<ul>
<li>Thanks to graph mode and XLA compilation, it’s fast. It’s usually significantly faster than PyTorch and NumPy, though JAX is often even faster.</li>
<li>It is extremely feature complete. Unique among all frameworks, it has support for string tensors as well as “ragged tensors” (tensors where different entries
  may have different dimensions — very useful for handling sequences without requiring to pad them to a shared length). It also has outstanding support for data
  preprocessing, via the highly performant <code>tf.data</code> API. <code>tf.data</code> is so good that even JAX recommends it for data preprocessing.
  Whatever you need to do, TensorFlow has a solution for it.</li>
<li>Its ecosystem for production deployment is the most mature among all frameworks, especially when it comes to deploying on mobile or in the browser.</li>
</ul>
<p>However, TensorFlow also has some noticeable flaws:</p>
<ul>
<li>It has a sprawling API — the flipside of being very feature complete. TensorFlow includes thousands of different operations.</li>
<li>Its numerical API is occasionally inconsistent with the NumPy API, making it a bit harder to approach if you’re already familiar with NumPy.</li>
<li>The popular pretrained model-sharing platform Hugging Face has less support for TensorFlow, which means that
  the latest generative AI models may not always be available in TensorFlow.</li>
</ul>
<p>Now, let’s move on to PyTorch.</p>
<h2 id="introduction-to-pytorch">Introduction to PyTorch</h2>
<p>PyTorch is a Python-based open source machine learning framework developed primarily by Meta (formerly Facebook)
It was originally released in September 2016 (as a response to the release of TensorFlow),
with its 1.0 version launched in 2018, and its 2.0 version launched in 2023.
PyTorch inherits its programming style from the now-defunct Chainer framework, which was itself inspired by NumPy-Autograd.
PyTorch is used extensively in the machine learning research community.</p>
<p>Like TensorFlow, PyTorch is at the center of a large ecosystem of related packages, such as <code>torchvision</code>, <code>torchaudio</code>,
or the popular model-sharing platform Hugging Face.</p>
<p>The PyTorch API is higher level than that of TensorFlow and JAX: it includes layers and optimizers, like Keras.
These layers and optimizers are compatible with Keras workflows when you use Keras with the PyTorch backend.</p>
<h3 id="first-steps-with-pytorch">First steps with PyTorch</h3>
<p>Over the next paragraphs, you’ll get familiar with all the basics of PyTorch. We’ll cover the following
key concepts:</p>
<ul>
<li>Tensors and parameters</li>
<li>Numerical operations in PyTorch</li>
<li>Computing gradients with the <code>backward()</code> method</li>
<li>Packaging computation with the <code>Module</code> class</li>
<li>Speeding up PyTorch by using compilation</li>
</ul>
<p>We’ll conclude the introduction by reimplementing our linear regression end-to-end example in pure PyTorch.</p>
<h4 id="tensors-and-parameters-in-pytorch">Tensors and parameters in PyTorch</h4>
<p>A first gotcha about PyTorch is that the package isn’t named <code>pytorch</code>. It’s actually named <code>torch</code>.
You’d install it via <code>pip install torch</code> and you’d import it via <code>import torch</code>.</p>
<p>Like in NumPy and TensorFlow, the object at the heart of the framework is the tensor. First, let’s get our hands on some PyTorch tensors.</p>
<h5 id="constant-tensors_1">Constant tensors</h5>
<p>Here are some constant tensors.</p>
<figure id="listing-3-22">
<pre><code class="language-python">&gt;&gt;&gt; import torch
&gt;&gt;&gt; # Unlike in other frameworks, the shape argument is named "size"
&gt;&gt;&gt; # rather than "shape."
&gt;&gt;&gt; torch.ones(size=(2, 1))</code>
<code class="language-output">tensor([[1.], [1.]])</code>
<code class="language-python">&gt;&gt;&gt; torch.zeros(size=(2, 1))</code>
<code class="language-output">tensor([[0.], [0.]])</code>
<code class="language-python">&gt;&gt;&gt; # Unlike in other frameworks, you cannot pass dtype="float32" as a
&gt;&gt;&gt; # string. The dtype argument must be a torch dtype instance.
&gt;&gt;&gt; torch.tensor([1, 2, 3], dtype=torch.float32)</code>
<code class="language-output">tensor([1., 2., 3.])</code></pre>
<figcaption>
<a href="#listing-3-22">Listing 3.22</a>: All-ones or all-zeros tensors
</figcaption>
</figure>

<h5 id="random-tensors_1">Random tensors</h5>
<p>Random tensor creation is similar to NumPy and TensorFlow, but with divergent syntax.
Consider the function <code>normal</code>: it doesn’t take a shape argument. Instead,
the mean and standard deviation should be provided as PyTorch tensors with the expected output shape.</p>
<figure id="listing-3-23">
<pre><code class="language-python">&gt;&gt;&gt; # Equivalent to tf.random.normal(shape=(3, 1), mean=0., stddev=1.)
&gt;&gt;&gt; torch.normal(
... mean=torch.zeros(size=(3, 1)),
... std=torch.ones(size=(3, 1)))</code>
<code class="language-output">tensor([[-0.9613],
        [-2.0169],
        [ 0.2088]])</code></pre>
<figcaption>
<a href="#listing-3-23">Listing 3.23</a>: Random tensors
</figcaption>
</figure>

<p>As for creating a random uniform tensor, you’d do that via <code>torch.rand</code>. Unlike <code>np.random.uniform</code> or <code>tf.random.uniform</code>,
the output shape should be provided as independent arguments for each dimension, like this:</p>
<figure>
<pre><code class="language-python">&gt;&gt;&gt; # Equivalent to tf.random.uniform(shape=(3, 1), minval=0.,
&gt;&gt;&gt; # maxval=1.)
&gt;&gt;&gt; torch.rand(3, 1)</code></pre>
</figure>

<h5 id="tensor-assignment-and-the-parameter-class">Tensor assignment and the Parameter class</h5>
<p>Like NumPy arrays, but unlike TensorFlow tensors, PyTorch tensors are assignable. You can do operations like this:</p>
<figure>
<pre><code class="language-python">&gt;&gt;&gt; x = torch.zeros(size=(2, 1))
&gt;&gt;&gt; x[0, 0] = 1.
&gt;&gt;&gt; x</code>
<code class="language-output">tensor([[1.],
        [0.]])</code></pre>
</figure>

<p>While you can just use a regular <code>torch.Tensor</code> to store the trainable state of a model,
PyTorch does provide a specialized tensor subclass for that purpose, the <code>torch.nn.parameter.Parameter</code> class.
Compared to a regular tensor, it provides semantic clarity — if you see a <code>Parameter</code>, you’ll know it’s a piece of trainable state, whereas a <code>Tensor</code>
could be anything. As a result, it enables PyTorch to automatically track and retrieve the <code>Parameters</code> you assign
to PyTorch models — similar to what Keras does with Keras <code>Variable</code> instances.</p>
<p>Here’s a <code>Parameter</code>.</p>
<figure id="listing-3-24">
<pre><code class="language-python">&gt;&gt;&gt; x = torch.zeros(size=(2, 1))
&gt;&gt;&gt; # A Parameter can only be created using a torch.Tensor value — no
&gt;&gt;&gt; # NumPy arrays allowed.
&gt;&gt;&gt; p = torch.nn.parameter.Parameter(data=x)</code></pre>
<figcaption>
<a href="#listing-3-24">Listing 3.24</a>: Creating a PyTorch parameter
</figcaption>
</figure>

<h4 id="tensor-operations-doing-math-in-pytorch">Tensor operations: Doing math in PyTorch</h4>
<p>Math in PyTorch works just the same as math in NumPy or TensorFlow, although much like TensorFlow,
the PyTorch API often diverges in subtle ways from the NumPy API.</p>
<figure id="listing-3-25">
<pre><code class="language-python">a = torch.ones((2, 2))
# Takes the square, same as np.square
b = torch.square(a)
# Takes the square root, same as np.sqrt
c = torch.sqrt(a)
# Adds two tensors (element-wise)
d = b + c
# Takes the product of two tensors (see chapter 2), same as np.matmul
e = torch.matmul(a, b)
# Concatenates a and b along axis 0, same as np.concatenate
f = torch.cat((a, b), dim=0)
</code></pre>
<figcaption>
<a href="#listing-3-25">Listing 3.25</a>: A few basic math operations in PyTorch
</figcaption>
</figure>

<p>Here’s a dense layer:</p>
<figure>
<pre><code class="language-python">def dense(inputs, W, b):
    return torch.nn.relu(torch.matmul(inputs, W) + b)
</code></pre>
</figure>

<h4 id="computing-gradients-with-pytorch">Computing gradients with PyTorch</h4>
<p>There’s no explicit “gradient tape” in PyTorch. A similar mechanism does
exist: when you run any computation in PyTorch, the framework creates a one-time
computation graph (a “tape”) that records what just happened.
However, that tape is hidden from the user. The public API for using it
is at the level of tensors themselves: you can call
<code>tensor.backward()</code> to run backpropagation through all operations previously executed
that led to that tensor. Doing this will populate the <code>.grad</code> attribute of
all tensors that are tracking gradients.</p>
<figure id="listing-3-26">
<pre><code class="language-python">&gt;&gt;&gt; # To compute gradients with respect to a tensor, it must be created
&gt;&gt;&gt; # with requires_grad=True.
&gt;&gt;&gt; input_var = torch.tensor(3.0, requires_grad=True)
&gt;&gt;&gt; result = torch.square(input_var)
&gt;&gt;&gt; # Calling backward() populates the "grad" attribute on all tensors
&gt;&gt;&gt; # create with requires_grad=True.
&gt;&gt;&gt; result.backward()
&gt;&gt;&gt; gradient = input_var.grad
&gt;&gt;&gt; gradient</code>
<code class="language-output">tensor(6.)</code></pre>
<figcaption>
<a href="#listing-3-26">Listing 3.26</a>: Computing a gradient with <code>.backward()</code>
</figcaption>
</figure>

<p>If you call <code>backward()</code> multiple times in a row, the <code>.grad</code> attribute will “accumulate” gradients: each
new call will sum the new gradient with the preexisting one. For instance, in the following code,
<code>input_var.grad</code> is not the gradient of <code>square(input_var)</code> with respect to <code>input_var</code>; rather, it is the sum
of that gradient and the previously computed gradient — its value has doubled since our last code snippet:</p>
<figure>
<pre><code class="language-python">&gt;&gt;&gt; result = torch.square(input_var)
&gt;&gt;&gt; result.backward()
&gt;&gt;&gt; # .grad will sum all gradient values from each time backward() is
&gt;&gt;&gt; # called.
&gt;&gt;&gt; input_var.grad</code>
<code class="language-output">tensor(12.)</code></pre>
</figure>

<p>To reset gradients, you can just set <code>.grad</code> to <code>None</code>:</p>
<figure>
<pre><code class="language-python">&gt;&gt;&gt; input_var.grad = None</code></pre>
</figure>

<p>Now let’s put this into practice!</p>
<h3 id="an-end-to-end-example-a-linear-classifier-in-pure-pytorch">An end-to-end example: A linear classifier in pure PyTorch</h3>
<p>You now know enough to rewrite our linear classifier in PyTorch. It will stay very similar to the TensorFlow one — the only major difference is how we compute the gradients.</p>
<p>Let’s start by creating our model variables. Don’t forget to pass <code>requires_grad=True</code> so we can compute gradients with respect to them:</p>
<figure>
<pre><code class="language-python">input_dim = 2
output_dim = 1

W = torch.rand(input_dim, output_dim, requires_grad=True)
b = torch.zeros(output_dim, requires_grad=True)
</code></pre>
</figure>

<p>This is our model — no difference so far. We just went from <code>tf.matmul</code> to <code>torch.matmul</code>:</p>
<figure>
<pre><code class="language-python">def model(inputs, W, b):
    return torch.matmul(inputs, W) + b
</code></pre>
</figure>

<p>This is our loss function. We just switch from <code>tf.square</code> to <code>torch.square</code> and from <code>tf.reduce_mean</code> to <code>torch.mean</code>:</p>
<figure>
<pre><code class="language-python">def mean_squared_error(targets, predictions):
    per_sample_losses = torch.square(targets - predictions)
    return torch.mean(per_sample_losses)
</code></pre>
</figure>

<p>Now for the training step. Here’s how it works:</p>
<ol>
<li><code>loss.backward()</code> runs backpropagation starting from the <code>loss</code> output node and populates
the <code>tensor.grad</code> attribute on all tensors that were involved in the computation of <code>loss</code>.
<code>tensor.grad</code> represents the gradient of the loss with regard to that tensor.</li>
<li>We use the <code>.grad</code> attribute to recover the gradients of the loss with regard to <code>W</code> and <code>b</code>.</li>
<li>We update <code>W</code> and <code>b</code> using those gradients. Because these updates are not intended to be
part of the backward pass, we do them inside a <code>torch.no_grad()</code> scope, which skips gradient
computation for everything inside it.</li>
<li>We reset the contents of the <code>.grad</code> property of our <code>W</code> and <code>b</code> parameters, by setting it to <code>None</code>.
If we didn’t do this, gradient values would accumulate across multiple calls to <code>training_step()</code>,
resulting in invalid values:</li>
</ol>
<figure>
<pre><code class="language-python">learning_rate = 0.1

def training_step(inputs, targets, W, b):
    # Forward pass
    predictions = model(inputs)
    loss = mean_squared_error(targets, predictions)
    # Computes gradients
    loss.backward()
    # Retrieves gradients
    grad_loss_wrt_W, grad_loss_wrt_b = W.grad, b.grad
    with torch.no_grad():
        # Updates weights inside a no_grad scope
        W -= grad_loss_wrt_W * learning_rate
        b -= grad_loss_wrt_b * learning_rate
    # Resets gradients
    W.grad = None
    b.grad = None
    return loss
</code></pre>
</figure>

<p>This could be made even simpler — let’s see how.</p>
<h4 id="packaging-state-and-computation-with-the-module-class">Packaging state and computation with the Module class</h4>
<p>PyTorch also has a higher-level, object-oriented API for performing backpropagation, which requires
relying on two new classes: the <code>torch.nn.Module</code> class and an optimizer class from
the <code>torch.optim</code> module, such as <code>torch.optim.SGD</code> (the equivalent of <code>keras.optimizers.SGD</code>).</p>
<p>The general idea is to define a subclass of <code>torch.nn.Module</code>, which will</p>
<ul>
<li>Hold some <code>Parameters</code>, to store state variables. Those are defined in the <code>__init__()</code> method.</li>
<li>Implement the forward pass computation in the <code>forward()</code> method.</li>
</ul>
<p>It should look just like the following.</p>
<figure id="listing-3-27">
<pre><code class="language-python">class LinearModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.W = torch.nn.Parameter(torch.rand(input_dim, output_dim))
        self.b = torch.nn.Parameter(torch.zeros(output_dim))

    def forward(self, inputs):
        return torch.matmul(inputs, self.W) + self.b
</code></pre>
<figcaption>
<a href="#listing-3-27">Listing 3.27</a>: Defining a <code>torch.nn.Module</code>
</figcaption>
</figure>

<p>We can now instantiate our <code>LinearModel</code>:</p>
<figure>
<pre><code class="language-python">model = LinearModel()
</code></pre>
</figure>

<p>When using an instance of <code>torch.nn.Module</code>, rather than calling the <code>forward()</code>
method directly, you’d use <code>__call__()</code> (i.e., directly call the model class on
inputs), which redirects to <code>forward()</code> but adds a few framework hooks to it:</p>
<figure>
<pre><code class="language-python">torch_inputs = torch.tensor(inputs)
output = model(torch_inputs)
</code></pre>
</figure>

<p>Now, let’s get our hands on a PyTorch optimizer. To instantiate it, you will
need to provide the list of parameters that the optimizer is intended to update.
You can retrieve it from our <code>Module</code> instance via <code>.parameters()</code>:</p>
<figure>
<pre><code class="language-python">optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
</code></pre>
</figure>

<p>Using our <code>Module</code> instance and the PyTorch <code>SGD</code> optimizer, we can run a simplified training step:</p>
<figure>
<pre><code class="language-python">def training_step(inputs, targets):
    predictions = model(inputs)
    loss = mean_squared_error(targets, predictions)
    loss.backward()
    optimizer.step()
    model.zero_grad()
    return loss
</code></pre>
</figure>

<p>Previously, updating the model parameters looked like this:</p>
<figure>
<pre><code class="language-python">with torch.no_grad():
    W -= grad_loss_wrt_W * learning_rate
    b -= grad_loss_wrt_b * learning_rate
</code></pre>
</figure>

<p>Now we can just do <code>optimizer.step()</code>.</p>
<p>Similarly, previously we needed to reset parameter gradients by hand by doing <code>tensor.grad = None</code> on each one.
Now we can just do <code>model.zero_grad()</code>.</p>
<p>Overall, this may feel a bit confusing — somehow the loss tensor, the optimizer, and the <code>Module</code> instance
all seem to be aware of each other through some hidden background mechanism.
They’re all interacting with one another via spooky action at a distance. Don’t worry though — you
can just treat this sequence of steps (<code>loss.backward()</code> - <code>optimizer.step()</code> - <code>model.zero_grad()</code>)
as a magic incantation to be recited any time you need to write a training step function. Just make sure not to forget
<code>model.zero_grad()</code>. That would be a major bug (and it is unfortunately quite common)!</p>
<h4 id="making-pytorch-modules-fast-using-compilation">Making PyTorch modules fast using compilation</h4>
<p>One last thing. Similarly to how TensorFlow lets you compile functions for better performance, PyTorch lets
you compile functions or even <code>Module</code> instances via the <code>torch.compile()</code> utility.
This API uses PyTorch’s very own compiler, named Dynamo.</p>
<p>Let’s try it on our linear regression <code>Module</code>:</p>
<figure>
<pre><code class="language-python">compiled_model = torch.compile(model)
</code></pre>
</figure>

<p>The resulting object is intended to work identically to the original — except the forward and backward pass should run faster.</p>
<p>You can also use <code>torch.compile()</code> as a function decorator:</p>
<figure>
<pre><code class="language-python">@torch.compile
def dense(inputs, W, b):
    return torch.nn.relu(torch.matmul(inputs, W) + b)
</code></pre>
</figure>

<p>In practice, most PyTorch code out there does not use compilation and simply runs eagerly,
as the compiler may not always work with all models and may not always result in a speedup when it does work. Unlike in TensorFlow and
Jax where compilation was built in from the inception of the library, PyTorch’s compiler is a relatively recent addition.</p>
<h3 id="what-makes-the-pytorch-approach-unique">What makes the PyTorch approach unique</h3>
<p>Compared to TensorFlow and JAX, which we will cover next, what makes PyTorch
stand out? Why should you use it or not use it?</p>
<p>Here are PyTorch’s two key strengths:</p>
<ul>
<li>PyTorch code executes eagerly by default, making it easy to debug.
Note that this is also the case for TensorFlow code and JAX code, but a big difference is that PyTorch is generally intended to be
run eagerly at all times, whereas any serious TensorFlow or JAX project will inevitably need compilation at some point, which can significantly hurt the debugging experience.</li>
<li>The popular pretrained model-sharing platform Hugging Face has first-class support for PyTorch, which
means that any model you’d like to use is likely available in PyTorch.
This is the primary drive behind PyTorch adoption today.</li>
</ul>
<p>Meanwhile, there are also some downsides to using PyTorch:</p>
<ul>
<li>Like with TensorFlow, the PyTorch API is inconsistent with NumPy. Further, it’s also internally inconsistent. For instance, the commonly used keyword <code>axis</code> is occasionally named <code>dim</code> instead, depending on the function.
Some pseudo-random number generation operations take a <code>seed</code> argument; others don’t. And so on.
This can make PyTorch frustrating to learn, especially when coming from NumPy.</li>
<li>Due to its focus on eager execution, PyTorch is quite slow — it’s the slowest
of all the major frameworks by a large margin. For most models, you may see a 20% or 30% speedup with JAX.
For some models — especially large ones — you may even see a 3× or a 5× speedup with JAX, even after using <code>torch.compile()</code>.</li>
<li>While it is possible to make PyTorch code faster via <code>torch.compile()</code>, the PyTorch Dynamo compiler
remains at this time (in 2025) quite ineffective and full of trapdoors. As a result, only a very small percentage of the
PyTorch user base uses compilation. Perhaps this will be improved in future versions!</li>
</ul>
<h2 id="introduction-to-jax">Introduction to JAX</h2>
<p>JAX is an open source library for differentiable computation, primarily developed by Google.
After its release in 2018, JAX quickly gained traction in the research community, particularly for its ability to use Google’s TPUs at scale.
Today, JAX is in use by most of the top players in the generative AI space — companies like DeepMind, Apple, Midjourney, Anthropic, Cohere, and so on.</p>
<p>JAX embraces a <em>stateless</em> approach to computation, meaning that functions in JAX do not maintain any persistent state. This contrasts with traditional imperative programming, where variables can hold values between function calls.</p>
<p>The stateless nature of JAX functions has several advantages. In particular, it enables effective automatic parallelization and distributed computation, as functions can be executed independently without the need for synchronization. The extreme scalability of JAX is essential for handling the very large-scale machine learning problems faced by companies like Google and DeepMind.</p>
<h3 id="first-steps-with-jax">First steps with JAX</h3>
<p>We’ll go over the following key concepts:</p>
<ul>
<li>The <code>array</code> class</li>
<li>Random operations in JAX</li>
<li>Numerical operations in JAX</li>
<li>Computing gradients via <code>jax.grad</code> and <code>jax.value_and_grad</code></li>
<li>Making JAX functions fast by leveraging just-in-time compilation</li>
</ul>
<p>Let’s get started.</p>
<h3 id="tensors-in-jax">Tensors in JAX</h3>
<p>One of the best features of JAX is that it doesn’t try to implement its own independent, similar-to-NumPy-but-slightly-divergent
numerical API. Instead, it just implements the NumPy API, as is. It is available as the <code>jax.numpy</code> namespace, and you
will often see it imported as <code>jnp</code> for short.</p>
<p>Here are some JAX arrays.</p>
<figure id="listing-3-28">
<pre><code class="language-python">&gt;&gt;&gt; from jax import numpy as jnp
&gt;&gt;&gt; jnp.ones(shape=(2, 1))</code>
<code class="language-output">Array([[1.],
       [1.]], dtype=float32)</code>
<code class="language-python">&gt;&gt;&gt; jnp.zeros(shape=(2, 1))</code>
<code class="language-output">Array([[0.],
       [0.]], dtype=float32)</code>
<code class="language-python">&gt;&gt;&gt; jnp.array([1, 2, 3], dtype="float32")</code>
<code class="language-output">Array([1., 2., 3.], dtype=float32)</code></pre>
<figcaption>
<a href="#listing-3-28">Listing 3.28</a>: All-ones or all-zeros tensors
</figcaption>
</figure>

<p>There are, however, two minor differences between <code>jax.numpy</code> and the actual NumPy API: random number generation and array assignment. Let’s take a look.</p>
<h3 id="random-number-generation-in-jax">Random number generation in JAX</h3>
<p>The first difference between JAX and NumPy has to do with the way JAX handles random operations — what is known as “PRNG” (Pseudo-Random Number Generation) operations.
We said earlier that JAX is <em>stateless</em>, which implies that JAX code can’t rely on any hidden global state. Consider the following NumPy code.</p>
<figure id="listing-3-29">
<pre><code class="language-python">&gt;&gt;&gt; np.random.normal(size=(3,))</code>
<code class="language-output">array([-1.68856166,  0.16489586,  0.67707523])</code>
<code class="language-python">&gt;&gt;&gt; np.random.normal(size=(3,))</code>
<code class="language-output">array([-0.73671259,  0.3053194 ,  0.84124895])</code></pre>
<figcaption>
<a href="#listing-3-29">Listing 3.29</a>: Random tensors
</figcaption>
</figure>

<p>How did the second call to <code>np.random.normal()</code> know to return a different value from the first call? That’s right — it’s a hidden piece of global state.
You can actually retrieve that global state via <code>np.random.get_state()</code> and set it via <code>np.random.seed(seed)</code>.</p>
<p>In a stateless framework, we can’t have any such global state. The same API call must always return the same value. As a result, in a stateless version of NumPy, you would have to rely on passing different seed arguments to your <code>np.random</code> calls to get different values.</p>
<p>Now, it’s often the case that your PRNG calls are going to be in functions that get called multiple times and that are intended to use different random values each time. If you don’t want to rely on any global state, this requires you to manage your seed state outside of the target function, like this:</p>
<figure>
<pre><code class="language-python">def apply_noise(x, seed):
    np.random.seed(seed)
    x = x * np.random.normal((3,))
    return x

seed = 1337
y = apply_noise(x, seed)
seed += 1
z = apply_noise(x, seed)
</code></pre>
</figure>

<p>It’s basically the same in JAX. However, JAX doesn’t use integer seeds. It uses
special array structures called <em>keys</em>. You can create one from an integer value, like this:</p>
<figure>
<pre><code class="language-python">import jax

seed_key = jax.random.key(1337)
</code></pre>
</figure>

<p>To force you to always provide a seed “key” to PRNG calls, all JAX PRNG-using operations take <code>key</code> (the random seed) as their first positional argument. Here’s how to use <code>random.normal()</code>:</p>
<figure>
<pre><code class="language-python">&gt;&gt;&gt; seed_key = jax.random.key(0)
&gt;&gt;&gt; jax.random.normal(seed_key, shape=(3,))</code>
<code class="language-output">Array([ 1.8160863 , -0.48262316,  0.33988908], dtype=float32)</code></pre>
</figure>

<p>Two calls to <code>random.normal()</code> that receive the same seed key will always return the same value.</p>
<figure id="listing-3-30">
<pre><code class="language-python">&gt;&gt;&gt; seed_key = jax.random.key(123)
&gt;&gt;&gt; jax.random.normal(seed_key, shape=(3,))</code>
<code class="language-output">Array([-0.1470326,  0.5524756,  1.648498 ], dtype=float32)</code>
<code class="language-python">&gt;&gt;&gt; jax.random.normal(seed_key, shape=(3,))</code>
<code class="language-output">Array([-0.1470326,  0.5524756,  1.648498 ], dtype=float32)</code></pre>
<figcaption>
<a href="#listing-3-30">Listing 3.30</a>: Using a random seed in Jax
</figcaption>
</figure>

<p>If you need a new seed key, you can simply create a new one from an existing one using the <code>jax.random.split()</code> function. It is deterministic, so the same sequence of splits will always result in the same final seed key:</p>
<figure>
<pre><code class="language-python">&gt;&gt;&gt; seed_key = jax.random.key(123)
&gt;&gt;&gt; jax.random.normal(seed_key, shape=(3,))</code>
<code class="language-output">Array([-0.1470326,  0.5524756,  1.648498 ], dtype=float32)</code>
<code class="language-python">&gt;&gt;&gt; # You could even split your key into multiple new keys at once!
&gt;&gt;&gt; new_seed_key = jax.random.split(seed_key, num=1)[0]
&gt;&gt;&gt; jax.random.normal(new_seed_key, shape=(3,))</code>
<code class="language-output">Array([ 0.5362355, -1.1920372,  2.450225 ], dtype=float32)</code></pre>
</figure>

<p>This is definitely more work than <code>np.random</code>! But the benefits of statelessness far outweigh the costs: it makes your code <em>vectorizable</em> (i.e., the JAX compiler can automatically turn it into highly parallel code) while maintaining determinism (i.e., you can run the same code twice with the same results). That is impossible to achieve with a global PRNG state.</p>
<h4 id="tensor-assignment">Tensor assignment</h4>
<p>The second difference between JAX and NumPy is tensor assignment.
Like in TensorFlow, JAX arrays are not assignable in place. That’s because any sort of in-place modification would go against JAX’s stateless design.
Instead, if you need to update a tensor, you must create a new tensor with the desired value. JAX makes this easy by providing
the <code>at()</code>/<code>set()</code> API. These methods allow you to create a new tensor with an updated element at a specific index. Here’s an example of how you would update the first element of a JAX array to a new value.</p>
<figure id="listing-3-31">
<pre><code class="language-python">&gt;&gt;&gt; x = jnp.array([1, 2, 3], dtype="float32")
&gt;&gt;&gt; new_x = x.at[0].set(10)</code></pre>
<figcaption>
<a href="#listing-3-31">Listing 3.31</a>: Modifying values in a JAX array
</figcaption>
</figure>

<p>Simple enough!</p>
<h4 id="tensor-operations-doing-math-in-jax">Tensor operations: Doing math in JAX</h4>
<p>Doing math in JAX looks exactly the same as it does in NumPy. No need to learn anything new this time!</p>
<figure id="listing-3-32">
<pre><code class="language-python">a = jnp.ones((2, 2))
# Takes the square
b = jnp.square(a)
# Takes the square root
c = jnp.sqrt(a)
# Adds two tensors (element-wise)
d = b + c
# Takes the product of two tensors (see chapter 2)
e = jnp.matmul(a, b)
# Multiplies two tensors (element-wise)
e *= d
</code></pre>
<figcaption>
<a href="#listing-3-32">Listing 3.32</a>: A few basic math operations in JAX
</figcaption>
</figure>

<p>Here’s a dense layer:</p>
<figure>
<pre><code class="language-python">def dense(inputs, W, b):
    return jax.nn.relu(jnp.matmul(inputs, W) + b)
</code></pre>
</figure>

<h4 id="computing-gradients-with-jax">Computing gradients with JAX</h4>
<p>Unlike TensorFlow and PyTorch, JAX takes a <em>metaprogramming</em> approach
to gradient computation. Metaprogramming refers to the idea of having <em>functions that return functions</em>
— you could call them “meta-functions.” In practice, JAX lets you <em>turn a loss-computation function into a gradient-computation function</em>.
So computing gradients in JAX is a three-step process:</p>
<ol>
<li>Define a loss function, <code>compute_loss()</code>.</li>
<li>Call <code>grad_fn = jax.grad(compute_loss)</code> to retrieve a gradient-computation function.</li>
<li>Call <code>grad_fn</code> to retrieve the gradient values.</li>
</ol>
<p>The loss function should verify the following properties:</p>
<ul>
<li>It should return a scalar loss value.</li>
<li>Its first argument (which, in the following example, is also the only argument) should contain the state arrays we need gradients for.
This argument is usually named <code>state</code>. For instance, this first argument could be a single array, a list of arrays, or a dict of arrays.</li>
</ul>
<p>Let’s take a look at a simple example. Here’s a loss-computation function that takes a single scalar, <code>input_var</code> and returns a scalar loss
value — just the square of the input:</p>
<figure>
<pre><code class="language-python">def compute_loss(input_var):
    return jnp.square(input_var)
</code></pre>
</figure>

<p>We can now call the JAX utility <code>jax.grad()</code> on this loss function.
It returns a gradient-computation function — a function that takes the
same arguments as the original loss function and returns the gradient of the loss with respect to <code>input_var</code>:</p>
<figure>
<pre><code class="language-python">grad_fn = jax.grad(compute_loss)
</code></pre>
</figure>

<p>Once you’ve obtained <code>grad_fn()</code>, you can call it with the same arguments as <code>compute_loss()</code>, and it will return gradients arrays
corresponding to the first argument of <code>compute_loss()</code>. In our case, our first argument was a single array, so <code>grad_fn()</code> directly
returns the gradient of the loss with respect to that one array:</p>
<figure>
<pre><code class="language-python">input_var = jnp.array(3.0)
grad_of_loss_wrt_input_var = grad_fn(input_var)
</code></pre>
</figure>

<h4 id="jax-gradient-computation-best-practices">JAX gradient-computation best practices</h4>
<p>So far so good! Metaprogramming is a big word, but it turns out to be quite simple.
Now, in real-world use cases, there are a few more things you’ll need to take into account. Let’s take a look.</p>
<h5 id="returning-the-loss-value">Returning the loss value</h5>
<p>It’s usually the case that you don’t just need the gradient array; you also need the loss
value. It would be quite inefficient to recompute it independently outside of <code>grad_fn()</code>, so instead, you
can just configure your <code>grad_fn()</code> to also return the loss value. This is done by using the JAX utility
<code>jax.value_and_grad()</code> instead of <code>jax.grad()</code>. It works identically, but it returns a tuple of values,
where the first entry is the loss value, and the second entry is the gradient(s):</p>
<figure>
<pre><code class="language-python">grad_fn = jax.value_and_grad(compute_loss)
output, grad_of_loss_wrt_input_var = grad_fn(input_var)
</code></pre>
</figure>

<h5 id="getting-gradients-for-a-complex-function">Getting gradients for a complex function</h5>
<p>Now, what if you need gradients for more than a single variable?
And what if your <code>compute_loss()</code> function has more than one input?</p>
<p>Let’s say your state contains three variables, <code>a</code>, <code>b</code>, and <code>c</code>, and your loss function has two inputs, <code>x</code> and <code>y</code>.
You would simply structure it like this:</p>
<figure>
<pre><code class="language-python"># state contains a, b, and c. It must be the first argument.
def compute_loss(state, x, y):
    ...
    return loss

grad_fn = jax.value_and_grad(compute_loss)
state = (a, b, c)
# grads_of_loss_wrt_state has the same structure as state.
loss, grads_of_loss_wrt_state = grad_fn(state, x, y)
</code></pre>
</figure>

<p>Note that <code>state</code> doesn’t have to be a tuple — it could be a dict, a list, or any nested structure of tuples, dicts, and lists. In
JAX parlance, such a nested structure is called a <em>tree</em>.</p>
<h5 id="returning-auxiliary-outputs">Returning auxiliary outputs</h5>
<p>Finally, what if your <code>compute_loss()</code> function needs to return more than just the loss?
Let’s say you want to return an additional value <code>output</code> that’s computed as a by-product of the loss computation.
How to get it out?</p>
<p>You would use the <code>has_aux</code> argument:</p>
<ol>
<li>Edit the loss function to return a tuple where the first entry is the loss, and the second entry is your extra output.</li>
<li>Pass the argument <code>has_aux=True</code> to <code>value_and_grad()</code>. This tells <code>value_and_grad()</code> to return not just the gradient
but also the “auxiliary” output(s) of <code>compute_loss()</code>, like this:</li>
</ol>
<figure>
<pre><code class="language-python">def compute_loss(state, x, y):
    ...
    # Returns a tuple
    return loss, output

# Passes has_aux=True here
grad_fn = jax.value_and_grad(compute_loss, has_aux=True)
# Gets back a nested tuple
loss, (grads_of_loss_wrt_state, output) = grad_fn(state, x, y)
</code></pre>
</figure>

<p>Admittedly, things are starting to be pretty convoluted at this point.
Don’t worry, though; this is about as hard as JAX gets! Almost everything else is simpler by comparison.</p>
<h4 id="making-jax-functions-fast-with-jaxjit">Making JAX functions fast with @jax.jit</h4>
<p>One more thing. As a JAX user, you will frequently use the <code>@jax.jit</code> decorator, which behaves
identically to the <code>@tf.function(jit_compile=True)</code> decorator. It turns any
stateless JAX function into an XLA-compiled piece of code, typically delivering a considerable execution speedup:</p>
<figure>
<pre><code class="language-python">@jax.jit
def dense(inputs, W, b):
    return jax.nn.relu(jnp.matmul(inputs, W) + b)
</code></pre>
</figure>

<p>Be mindful that you can only decorate a stateless function — any tensors that get updated by the
function should be part of its return values.</p>
<h3 id="an-end-to-end-example-a-linear-classifier-in-pure-jax">An end-to-end example: A linear classifier in pure JAX</h3>
<p>Now you know enough JAX to write the JAX version of our linear classifier example. There are two major differences
from the TensorFlow and PyTorch versions you’ve already seen:</p>
<ul>
<li>All functions we will create will be <em>stateless</em>. That means the state (the arrays <code>W</code> and <code>b</code>) will be provided
as function arguments, and if they get modified by the function, their new value will be returned by the function.</li>
<li>Gradients are computed using the JAX <code>value_and_grad()</code> utility.</li>
</ul>
<p>Let’s get started.
The model function and the mean squared error function should look familiar:</p>
<figure>
<pre><code class="language-python">def model(inputs, W, b):
    return jnp.matmul(inputs, W) + b

def mean_squared_error(targets, predictions):
    per_sample_losses = jnp.square(targets - predictions)
    return jnp.mean(per_sample_losses)
</code></pre>
</figure>

<p>To compute gradients, we need to package loss computation in
a single <code>compute_loss()</code> function. It returns the total loss as a scalar,
and it takes <code>state</code> as its first argument — a tuple of all
the tensors we need gradients for:</p>
<figure>
<pre><code class="language-python">def compute_loss(state, inputs, targets):
    W, b = state
    predictions = model(inputs, W, b)
    loss = mean_squared_error(targets, predictions)
    return loss
</code></pre>
</figure>

<p>Calling <code>jax.value_and_grad()</code> on this function gives us a new
function, with the same argument as <code>compute_loss</code>, which returns
both the loss and the gradients of the loss with regard to the elements
of <code>state</code>:</p>
<figure>
<pre><code class="language-python">grad_fn = jax.value_and_grad(compute_loss)
</code></pre>
</figure>

<p>Next, we can set up our training step function. It looks straightforward.
Be mindful that, unlike its TensorFlow and PyTorch equivalents, it needs
to be stateless, and so it must return the updated values of the <code>W</code> and <code>b</code>
tensors:</p>
<figure>
<pre><code class="language-python">learning_rate = 0.1

# We use the jax.jit decorator to take advantage of XLA compilation.
@jax.jit
def training_step(inputs, targets, W, b):
    # Computes the forward pass and backward pass in one go
    loss, grads = grad_fn((W, b), inputs, targets)
    grad_wrt_W, grad_wrt_b = grads
    # Updates W and b
    W = W - grad_wrt_W * learning_rate
    b = b - grad_wrt_b * learning_rate
    # Make sure to return the new values of W and b in addition to the
    # loss!
    return loss, W, b
</code></pre>
</figure>

<p>Because we won’t change the <code>learning_rate</code> during our example, we can
consider it part of the function itself and not our model’s state. If we
wanted to modify our learning rate during training, we’d need to pass it through
as well.</p>
<p>Finally, we’re ready to run the full training loop. We initialize <code>W</code> and <code>b</code>,
and we repeatedly update them via stateless calls to <code>training_step()</code>:</p>
<figure>
<pre><code class="language-python">input_dim = 2
output_dim = 1

W = jax.numpy.array(np.random.uniform(size=(input_dim, output_dim)))
b = jax.numpy.array(np.zeros(shape=(output_dim,)))
state = (W, b)
for step in range(40):
    loss, W, b = training_step(inputs, targets, W, b)
    print(f"Loss at step {step}: {loss:.4f}")
</code></pre>
</figure>

<p>That’s it! You’re now able to write a custom training loop in JAX.</p>
<h3 id="what-makes-the-jax-approach-unique">What makes the JAX approach unique</h3>
<p>The main thing that makes JAX unique among modern machine learning frameworks is its functional, stateless philosophy. While it may seem to cause friction at first,
it is what unlocks the power of JAX — its ability to compile to extremely fast code and to scale to arbitrarily large models and arbitrarily many devices.</p>
<p>There’s a lot to like about JAX:</p>
<ul>
<li>It’s fast. For most models, it is the fastest of all frameworks you’ve seen so far.</li>
<li>Its numerical API is fully consistent with NumPy, making it pleasant to learn.</li>
<li>It’s the best fit for training models on TPUs, as it was developed from the ground up for XLA and TPUs.</li>
</ul>
<p>Using JAX can also come with some amount of developer friction:</p>
<ul>
<li>Its use of metaprogramming and compilation can make it significantly harder to debug compared to pure eager execution.</li>
<li>Low-level training loops tend to be more verbose and more difficult to write than in TensorFlow or PyTorch.</li>
</ul>
<p>At this point, you know the basics of TensorFlow, PyTorch, and JAX, and you can use these frameworks
to implement a basic linear classifier from scratch. That’s a solid foundation to
build upon. It’s now time to move on to a more productive path to deep learning: the Keras API.</p>
<h2 id="introduction-to-keras">Introduction to Keras</h2>
<p>Keras is a deep learning API for Python that provides a convenient way to define and train
any kind of deep learning model. It was released in March 2015, with its v2 in 2017 and its v3 in 2023.</p>
<p>Keras users range from academic researchers, engineers, and data scientists
at both startups and large companies to graduate students and hobbyists.
Keras is used at Google, Netflix, Uber, YouTube, CERN, NASA, Yelp, Instacart,
Square, Waymo, YouTube, and thousands of smaller organizations
working on a wide range of problems across every industry.
Your YouTube recommendations originate from Keras models.
The Waymo self-driving cars rely on Keras models for processing sensor data.
Keras is also a popular framework on Kaggle, the machine learning competition website.</p>
<p>Because Keras has a diverse user base, it doesn’t force you to follow
a single “true” way of building and training models. Rather, it enables
a wide range of different workflows,
from the very high-level to the very low-level,
corresponding to different user profiles. For instance, you have an array
of ways to build models and an array of ways to train them,
each representing a certain tradeoff between usability and flexibility.
In chapter 7, we’ll review
in detail a good fraction of this spectrum of workflows.</p>
<h3 id="first-steps-with-keras">First steps with Keras</h3>
<p>Before we get to writing Keras code, there are a few things to consider when
setting up the library before it’s imported.</p>
<h4 id="picking-a-backend-framework">Picking a backend framework</h4>
<p>Keras can be used together with JAX, TensorFlow, or PyTorch. They’re the
“backend frameworks” of Keras. Through these backend frameworks,
Keras can run on top of different types of hardware
(see figure 3.4) — GPU, TPU, or plain CPU —
can be seamlessly scaled to thousands of machines, and can be deployed to a variety of platforms.</p>
<figure id="figure-3-4">
<img src="../Images/552c279c016c6f17a2d903764c737676.png" data-original-src="https://deeplearningwithpython.io/images/ch03/keras_and_backends.7fcf768f.png"/>
<figcaption>
<a href="#figure-3-4">Figure 3.4</a>: Keras and its backends. A backend is a low-level tensor-computing platform; Keras is a high-level deep learning API.
</figcaption>
</figure>

<p>Backend frameworks are pluggable: you can switch to a different backend framework
<em>after</em> you’ve written some Keras code. You aren’t locked into a single framework and a single
ecosystem — you can move your models from JAX to TensorFlow to PyTorch depending on your current needs.
For instance, when you develop a Keras model, you could debug it with PyTorch,
train it on TPU with JAX for maximum efficiency, and finally run inference
with the excellent tooling from the TensorFlow ecosystem.</p>
<p>The default backend for Keras right now is TensorFlow, so if you run <code>import keras</code> in a fresh
environment, without having configured anything, you will be running on top of TensorFlow.
There are two ways to pick a different backend:</p>
<ul>
<li>Set the environment variable <code>KERAS_BACKEND</code>. Before you start your <code>python</code> repl, you can
run the following shell command to use JAX as your Keras backend: <code>export KERAS_BACKEND=jax</code>.
Alternatively, you can add the following code snippet at the top of your Python file or notebook
(note that it must imperatively go before the first <code>import keras</code>):</li>
</ul>
<figure>
<pre><code class="language-python">import os

# Sets the environment variable from within the Python runtime
os.environ["KERAS_BACKEND"] = "jax"

# Only then should you import Keras.
import keras
</code></pre>
</figure>

<ul>
<li>Edit your local Keras configuration file at <code>~/.keras/keras.json</code>. If you have already imported 
Keras once, this file has already been created with default settings.
You can use any text editor to open and modify it — it’s a human-readable JSON file. It should look like this:</li>
</ul>
<figure>
<pre><code class="language-python">{
    # Default floating-point precision. It should typically not be
    # changed.
    "floatx": "float32",
    # Default numerical fuzzing factor. It should typically not be
    # changed.
    "epsilon": 1e-07,
    # Change "tensorflow" to "jax" or "torch."
    "backend": "tensorflow",
    # This is the default image layout. We'll talk about this in
    # chapter 8.
    "image_data_format": "channels_last",
}
</code></pre>
</figure>

<aside>
<p>When configuring the Keras backend, you should use the string <code>"torch"</code> to refer
to the PyTorch backend, rather than the string <code>"pytorch"</code>, which would be invalid.
This is because the PyTorch package name is <code>torch</code> (as in <code>import torch</code> or <code>pip install torch</code>).</p>
</aside>

<p>Now, you may ask, which backend should I be picking? It’s really your own choice:
all Keras code examples in the rest of the book will be compatible with all three backends.
If the need for backend-specific code arises (as in chapter 7, for instance),
I will show you all three versions — TensorFlow, PyTorch, JAX.
If you have no particular backend preference,
my personal recommendation is JAX. It’s usually the most performant backend.</p>
<p>Once your backend is configured, you can start actually building and training Keras models. Let’s take a look.</p>
<h3 id="layers-the-building-blocks-of-deep-learning">Layers: The building blocks of deep learning</h3>
<p>The fundamental data structure in
neural networks is the <em>layer</em>, to which you were introduced in chapter 2. A
layer is a data processing module that takes as input one or more tensors and
that outputs one or more tensors. Some layers are stateless, but more
frequently layers have a state: the layer’s <em>weights</em>, one or
several tensors learned with stochastic gradient descent, which together contain
the network’s <em>knowledge</em>.</p>
<p>Different types of layers are appropriate for different tensor formats
and different types of data processing.
For instance, simple vector data, stored in 2D
tensors of shape <code>(samples, features)</code>, is often processed by
<em>densely connected</em> layers, also called <em>fully connected</em>
or <em>dense</em> layers (the <code>Dense</code> class in Keras). Sequence data, stored in 3D
tensors of shape <code>(samples, timesteps, features)</code>, is typically processed by
<em>recurrent</em> layers, such as an <code>LSTM</code> layer, or 1D convolution layers (<code>Conv1D</code>).
Image data, stored in rank-4 tensors, is usually processed by 2D convolution
layers (<code>Conv2D</code>).</p>
<p>You can think of layers as the LEGO bricks of deep learning, a metaphor that is
made explicit by Keras. Building deep learning models in Keras
is done by clipping together compatible layers to form useful
data transformation pipelines.</p>
<h4 id="the-base-layer-class-in-keras">The base <code>Layer</code> class in Keras</h4>
<p>A simple API should have a single abstraction around which everything is centered.
In Keras, that’s the <code>Layer</code> class. Everything in Keras is either a <code>Layer</code> or
something that closely interacts with a <code>Layer</code>.</p>
<p>A <code>Layer</code> is an object that encapsulates some state (weights) and some computation
(a forward pass). The weights are typically defined in a <code>build()</code> (although they
could also be created in the constructor <code>__init__()</code>), and the computation is
defined in the <code>call()</code> method.</p>
<p>In the previous chapter, we implemented a <code>NaiveDense</code> class that contained
two weights <code>W</code> and <code>b</code> and applied the computation
<code>output = activation(matmul(input, W) + b)</code>. The following is what the same layer would
look like in Keras.</p>
<figure id="listing-3-33">
<pre><code class="language-python">import keras

# All Keras layers inherit from the base Layer class.
class SimpleDense(keras.Layer):
    def __init__(self, units, activation=None):
        super().__init__()
        self.units = units
        self.activation = activation

    # Weight creation takes place in the build() method.
    def build(self, input_shape):
        batch_dim, input_dim = input_shape
        # add_weight is a shortcut method for creating weights. It's
        # also possible to create standalone variables and assign them
        # as layer attributes, like self.W = keras.Variable(shape=...,
        # initializer=...).
        self.W = self.add_weight(
            shape=(input_dim, self.units), initializer="random_normal"
        )
        self.b = self.add_weight(shape=(self.units,), initializer="zeros")

    # We define the forward pass computation in the call() method.
    def call(self, inputs):
        y = keras.ops.matmul(inputs, self.W) + self.b
        if self.activation is not None:
            y = self.activation(y)
        return y
</code></pre>
<figcaption>
<a href="#listing-3-33">Listing 3.33</a>: A simple dense layer from scratch in Keras
</figcaption>
</figure>

<p>In the next section, we’ll cover in detail the purpose of these <code>build()</code> and
<code>call()</code> methods. Don’t worry if you don’t understand everything just yet!</p>
<p>Once instantiated, a layer like this can be used just like a function, taking as
input a tensor:</p>
<figure>
<pre><code class="language-python">&gt;&gt;&gt; # Instantiates our layer, defined previously
&gt;&gt;&gt; my_dense = SimpleDense(units=32, activation=keras.ops.relu)
&gt;&gt;&gt; # Creates some test inputs
&gt;&gt;&gt; input_tensor = keras.ops.ones(shape=(2, 784))
&gt;&gt;&gt; # Calls the layer on the inputs, just like a function
&gt;&gt;&gt; output_tensor = my_dense(input_tensor)
&gt;&gt;&gt; print(output_tensor.shape)</code>
<code class="language-output">(2, 32)</code></pre>
</figure>

<p>Now, you’re probably wondering, why did we have to implement <code>call()</code>
and <code>build()</code>, since we ended up using our layer by plainly calling it, that is
to say, by using its <code>__call__</code> method? It’s because we want to be able to
create the state just in time. Let’s see how that works.</p>
<h4 id="automatic-shape-inference-building-layers-on-the-fly">Automatic shape inference: Building layers on the fly</h4>
<p>Just like with LEGO bricks, you can only “clip” together layers
that are <em>compatible</em>. The notion of <em>layer compatibility</em> here
refers specifically to the fact that every layer will
only accept input tensors of a certain shape and will return output tensors of
a certain shape. Consider the following example:</p>
<figure>
<pre><code class="language-python">from keras import layers

# A dense layer with 32 output units
layer = layers.Dense(32, activation="relu")
</code></pre>
</figure>

<p>This layer will return a tensor whose non-batch dimension is 32. It can only be
connected to a downstream layer that expects 32-dimensional vectors as its
input.</p>
<p>When using Keras, you don’t have to worry about size compatibility
most of the time because the layers you add to your models are dynamically
built to match the shape of the incoming inputs. For instance, suppose you
write the following:</p>
<figure>
<pre><code class="language-python">from keras import models
from keras import layers

model = models.Sequential(
    [
        layers.Dense(32, activation="relu"),
        layers.Dense(32),
    ]
)
</code></pre>
</figure>

<p>The layers didn’t receive any information about the shape of their inputs.
Instead, they automatically inferred their input shape as being the
shape of the first inputs they see.</p>
<p>In the toy version of a <code>Dense</code> layer that we’ve implemented in chapter 2,
we had to pass the layer’s input size explicitly
to the constructor in order to be able to create its weights.
That’s not ideal, because it would lead to
models that look like this, where each new layer needs to be made aware
of the shape of the layer before it:</p>
<figure>
<pre><code class="language-python">model = NaiveSequential(
    [
        NaiveDense(input_size=784, output_size=32, activation="relu"),
        NaiveDense(input_size=32, output_size=64, activation="relu"),
        NaiveDense(input_size=64, output_size=32, activation="relu"),
        NaiveDense(input_size=32, output_size=10, activation="softmax"),
    ]
)
</code></pre>
</figure>

<p>It would be even worse when the rules used by a layer to produce its output
shape are complex. For instance, what if our layer returned outputs of shape
<code>(batch, input_size * 2 if input_size % 2 == 0 else input_size * 3)</code>?</p>
<p>If we were to reimplement our <code>NaiveDense</code> layer as a Keras layer capable of
automatic shape inference, it would look like the <code>SimpleDense</code> layer,
with its <code>build()</code> and <code>call()</code> methods.</p>
<p>In the Keras <code>SimpleDense</code>, we no longer create weights
in the constructor like in the previous example. Instead,
we create them in a dedicated state-creation method <code>build()</code>,
which receives as argument the first input shape seen by the layer.
The <code>build()</code> method is called automatically the first time the layer is called
(via its <code>__call__()</code> method). In fact, that’s why we defined the computation
in a separate <code>call()</code> method rather than in the <code>__call__()</code> method directly!
The <code>__call__()</code> method of the base layer schematically looks like this:</p>
<figure>
<pre><code class="language-python">def __call__(self, inputs):
    if not self.built:
        self.build(inputs.shape)
        self.built = True
    return self.call(inputs)
</code></pre>
</figure>

<p>With automatic shape inference, our previous example becomes simple and neat:</p>
<figure>
<pre><code class="language-python">model = keras.Sequential(
    [
        SimpleDense(32, activation="relu"),
        SimpleDense(64, activation="relu"),
        SimpleDense(32, activation="relu"),
        SimpleDense(10, activation="softmax"),
    ]
)
</code></pre>
</figure>

<p>Note that automatic shape inference is not the only thing that the <code>Layer</code>
class’s <code>__call__()</code> method handles. It takes care of many more things,
in particular routing between <em>eager</em> and <em>graph</em> execution,
and input masking (which we cover in chapter 14).
For now, just remember: when implementing your own layers,
put the forward pass in the <code>call()</code> method.</p>
<h3 id="from-layers-to-models">From layers to models</h3>
<p>A deep learning model is a graph of layers.
In Keras, that’s the <code>Model</code> class.
For now, you’ve only seen <code>Sequential</code> models (a subclass of <code>Model</code>),
which are simple stacks of layers, mapping a single input to a single output.
But as you move forward, you’ll be exposed to a much broader variety of network
topologies. Some common ones are</p>
<ul>
<li>Two-branch networks</li>
<li>Multihead networks</li>
<li>Residual connections</li>
</ul>
<p>Network topology can get quite involved. For instance, figure 3.5 shows topology
of the graph of layers of a Transformer, a common architecture designed to
process text data. </p>
<figure id="figure-3-5">
<img src="../Images/268b0a6aaeb461be967b2cbfca279054.png" data-original-src="https://deeplearningwithpython.io/images/ch03/transformer.cb3f137f.png"/>
<figcaption>
<a href="#figure-3-5">Figure 3.5</a>: The Transformer architecture. There’s a lot going on here. Throughout the next few chapters, you’ll climb your way up to understanding it (in chapter 15).
</figcaption>
</figure>

<p>There are generally two ways of building such models in Keras: you
can directly subclass the <code>Model</code> class, or you can use the Functional API,
which lets you do more with less code. We’ll cover both approaches in chapter 7.</p>
<p>The topology of a model defines a <em>hypothesis space</em>. You may remember that
in chapter 1, we described machine learning as “searching for useful
representations of some input data, within a predefined
<em>space of possibilities</em>, using guidance from a feedback signal.”
By choosing a network topology, you constrain your space of possibilities
(hypothesis space) to a specific series of tensor operations, mapping
input data to output data. What you’ll then be searching for is a good set
of values for the weight tensors involved in these tensor operations.</p>
<p>To learn from data, you have to make assumptions about it. These assumptions
define what can be learned. As such, the structure of your hypothesis space —
the architecture of your model — is extremely important.
It encodes the assumptions you make about your problem,
the prior knowledge that the model starts with. For instance,
if you’re working on a two-class classification problem with a model made
of a single <code>Dense</code> layer with no activation (a pure affine transformation),
you are assuming that your two classes are linearly separable.</p>
<p>Picking the right network architecture is more an art than a science, and
although there are some best practices and principles you can rely on, only
practice can help you become a proper neural network architect. The next few
chapters will both teach you explicit principles for building neural networks
and help you develop intuition as to what works or doesn’t work for specific
problems. You’ll build a solid intuition about what type of
model architectures work for different kinds of problems, how to build
these networks in practice, how to pick the
right learning configuration, and how to tweak a model until it yields the
results you want to see.</p>
<h3 id="the-compile-step-configuring-the-learning-process">The “compile” step: Configuring the learning process</h3>
<p>Once the model architecture is defined, you still have to
choose three more things:</p>
<ul>
<li><em>Loss function (objective function)</em>  —  The quantity that will
be minimized during training. It represents a measure of success for
the task at hand.</li>
</ul>
<ul>
<li><em>Optimizer</em>  —  Determines how the network will be updated based on the loss
function. It implements a specific variant of stochastic gradient descent (SGD).</li>
</ul>
<ul>
<li><em>Metrics</em> — The measures of success you want to monitor during training and
validation, such as classification accuracy.
Unlike the loss, training will not optimize directly for these metrics.
As such, metrics don’t need to be differentiable.</li>
</ul>
<p>Once you’ve picked your loss, optimizer, and metrics, you can use the
built-in <code>compile()</code> and <code>fit()</code> methods to start training your model.
Alternatively, you can write your own custom training loops —
we cover how to do this in chapter 7. It’s a lot more work!
For now, let’s take a look at <code>compile()</code> and <code>fit()</code>.</p>
<p>The <code>compile()</code> method configures the training process — you’ve already been
introduced to it in your very first neural network example in chapter 2.
It takes the arguments <code>optimizer</code>, <code>loss</code>, and <code>metrics</code> (a list):</p>
<figure>
<pre><code class="language-python"># Defines a linear classifier
model = keras.Sequential([keras.layers.Dense(1)])
model.compile(
    # Specifies the optimizer by name: RMSprop (it's case-insensitive)
    optimizer="rmsprop",
    # Specifies the loss by name: mean squared error
    loss="mean_squared_error",
    # Specifies a list of metrics: in this case, only accuracy
    metrics=["accuracy"],
)
</code></pre>
</figure>

<p>In the previous call to <code>compile()</code>, we passed the optimizer, loss, and metrics
as strings (such as <code>"rmsprop"</code>). These strings are actually
shortcuts that get converted to Python objects. For instance, <code>"rmsprop"</code> becomes
<code>keras.optimizers.RMSprop()</code>. Importantly, it’s also possible to specify these
arguments as object instances, like this:</p>
<figure>
<pre><code class="language-python">model.compile(
    optimizer=keras.optimizers.RMSprop(),
    loss=keras.losses.MeanSquaredError(),
    metrics=[keras.metrics.BinaryAccuracy()],
)
</code></pre>
</figure>

<p>This is useful if you want to pass your own custom losses or metrics or if
you want to further configure the objects you’re using — for instance, by
passing a <code>learning_rate</code> argument to the optimizer:</p>
<figure>
<pre><code class="language-python">model.compile(
    optimizer=keras.optimizers.RMSprop(learning_rate=1e-4),
    loss=my_custom_loss,
    metrics=[my_custom_metric_1, my_custom_metric_2],
)
</code></pre>
</figure>

<p>In chapter 7, we cover how to create custom losses and metrics. In general,
you won’t have to create your own losses, metrics,
or optimizers from scratch because Keras offers a wide range of built-in
options that is likely to include what you need:</p>
<ul>
<li><em>Optimizers</em><ul>
<li><code>SGD()</code> (with or without momentum)</li>
<li><code>RMSprop()</code></li>
<li><code>Adam()</code></li>
<li>Etc.</li>
</ul>
</li>
<li><em>Losses</em><ul>
<li><code>CategoricalCrossentropy()</code></li>
<li><code>SparseCategoricalCrossentropy()</code></li>
<li><code>BinaryCrossentropy()</code></li>
<li><code>MeanSquaredError()</code></li>
<li><code>KLDivergence()</code></li>
<li><code>CosineSimilarity()</code></li>
<li>Etc.</li>
</ul>
</li>
<li><em>Metrics</em><ul>
<li><code>CategoricalAccuracy()</code></li>
<li><code>SparseCategoricalAccuracy()</code></li>
<li><code>BinaryAccuracy()</code></li>
<li><code>AUC()</code></li>
<li><code>Precision()</code></li>
<li><code>Recall()</code></li>
<li>Etc.</li>
</ul>
</li>
</ul>
<p>Throughout this book, you’ll see concrete applications of many of these options.</p>
<h3 id="picking-a-loss-function">Picking a loss function</h3>
<p>Choosing the right loss function for the right problem is extremely
important: your network will take any shortcut it can to minimize the loss.
So if the objective doesn’t fully correlate with success for the task at hand,
your network will end up doing things you may not have wanted. Imagine a
stupid, omnipotent AI trained via SGD, with this poorly chosen objective
function: “Maximize the average well-being of all humans alive.” To make its
job easier, this AI might choose to kill all humans except a few and focus on
the well-being of the remaining ones because average well-being isn’t affected
by how many humans are left. That might not be what you intended! Just
remember that all neural networks you build will be just as ruthless in
lowering their loss function, so choose the objective wisely, or you’ll have to
face unintended side effects.</p>
<p>Fortunately, when it comes to common problems such as classification,
regression, and sequence prediction, there are simple guidelines you can follow
to choose the correct loss. For instance, you’ll use binary crossentropy for a
two-class classification problem, categorical crossentropy for a many-class
classification problem, and so on. Only when you’re working on truly
new research problems will you have to develop your own loss functions.
In the next few chapters, we’ll detail explicitly which loss functions to
choose for a wide range of common tasks.</p>
<h3 id="understanding-the-fit-method">Understanding the fit method</h3>
<p>After <code>compile()</code> comes <code>fit()</code>. The <code>fit</code> method implements the training loop
itself. Its key arguments are</p>
<ul>
<li>The <em>data</em> (inputs and targets) to train on. It will typically be passed
either in the form of NumPy arrays or a TensorFlow <code>Dataset</code> object. You’ll
learn more about the <code>Dataset</code> API in the next chapters.</li>
<li>The number of <em>epochs</em> to train for: how many times the training loop
should iterate over the data passed.</li>
<li>The batch size to use within each epoch of mini-batch gradient descent:
the number of training examples considered to compute the gradients for
one weight update step.</li>
</ul>
<figure id="listing-3-34">
<pre><code class="language-python">history = model.fit(
    # The input examples, as a NumPy array
    inputs,
    # The corresponding training targets, as a NumPy array
    targets,
    # The training loop will iterate over the data 5 times.
    epochs=5,
    # The training loop will iterate over the data in batches of 128
    # examples.
    batch_size=128,
)
</code></pre>
<figcaption>
<a href="#listing-3-34">Listing 3.34</a>: Calling <code>fit</code> with NumPy data
</figcaption>
</figure>

<p>The call to <code>fit</code> returns a <code>History</code> object. This object contains
a <code>history</code> field, which is a dict mapping key, such as <code>"loss"</code> or specific
metric names to the list of their per-epoch values:</p>
<figure>
<pre><code class="language-python">&gt;&gt;&gt; history.history</code>
<code class="language-output">{"binary_accuracy": [0.855, 0.9565, 0.9555, 0.95, 0.951],
 "loss": [0.6573270302042366,
  0.07434618508815766,
  0.07687718723714351,
  0.07412414988875389,
  0.07617757616937161]}</code></pre>
</figure>

<h3 id="monitoring-loss-and-metrics-on-validation-data">Monitoring loss and metrics on validation data</h3>
<p>The goal of machine learning is not to obtain models that perform well on the
training data, which is easy — all you have to do is follow the gradient.
The goal is to obtain models that perform well in
general, particularly on data points that the model has never encountered
before. Just because a model performs well on its training data doesn’t mean
it will perform well on data it has never seen! For
instance, it’s possible that your model could end up merely <em>memorizing</em> a
mapping between your training samples and their targets, which would be
useless for the task of predicting targets for data the model has never seen
before. We’ll go over this point in much more detail in the chapter 5.</p>
<p>To keep an eye on how the model does on new data, it’s standard practice
to reserve a subset of the training data as “validation data”: you won’t
be training the model on this data, but you will use it to compute a loss value
and metrics value. You do this by using the <code>validation_data</code> argument in <code>fit()</code>.
Like the training data, the validation data could be passed as NumPy arrays
or as a TensorFlow <code>Dataset</code> object.</p>
<figure id="listing-3-35">
<pre><code class="language-python">model = keras.Sequential([keras.layers.Dense(1)])
model.compile(
    optimizer=keras.optimizers.RMSprop(learning_rate=0.1),
    loss=keras.losses.MeanSquaredError(),
    metrics=[keras.metrics.BinaryAccuracy()],
)

# To avoid having samples from only one class in the validation data,
# shuffles the inputs and targets using a random indices permutation
indices_permutation = np.random.permutation(len(inputs))
shuffled_inputs = inputs[indices_permutation]
shuffled_targets = targets[indices_permutation]

# Reserves 30% of the training inputs and targets for "validation."
# (We'll exclude these samples from training and reserve them to
# compute the "validation loss" and metrics).
num_validation_samples = int(0.3 * len(inputs))
val_inputs = shuffled_inputs[:num_validation_samples]
val_targets = shuffled_targets[:num_validation_samples]
training_inputs = shuffled_inputs[num_validation_samples:]
training_targets = shuffled_targets[num_validation_samples:]
model.fit(
    # Training data, used to update the weights of the model
    training_inputs,
    training_targets,
    epochs=5,
    batch_size=16,
    # Validation data, used only to monitor the "validation loss" and
    # metrics
    validation_data=(val_inputs, val_targets),
)
</code></pre>
<figcaption>
<a href="#listing-3-35">Listing 3.35</a>: Using the validation data argument
</figcaption>
</figure>

<p>The value of the loss on the validation data is called the
<em>validation loss</em>, to distinguish it from the <em>training loss</em>. Note that
it’s essential to keep the training data and validation data strictly separate:
the purpose of validation is to monitor whether what the model is learning is
actually useful on new data. If any of the validation data has been seen
by the model during training, your validation loss and metrics will be flawed.</p>
<p>If you want to compute the validation loss and metrics after training
is complete, you can call the <code>evaluate</code> method:</p>
<p><code>loss_and_metrics = model.evaluate(val_inputs, val_targets, batch_size=128)</code></p>
<p><code>evaluate()</code> will iterate in batches (of size <code>batch_size</code>) over the data passed
and return a list of scalars, where the first entry is the validation loss
and the following entries are the validation metrics. If the model has no
metrics, only the validation loss is returned (rather than a list).</p>
<h3 id="inference-using-a-model-after-training">Inference: Using a model after training</h3>
<p>Once you’ve trained your model, you’re going to want to use it to make predictions
on new data. This is called <em>inference</em>.
To do this, a naive approach would simply be to <code>__call__</code> the model:</p>
<figure>
<pre><code class="language-python"># Takes a NumPy array or a tensor for your current backend and returns
# a tensor for your current backend
predictions = model(new_inputs)
</code></pre>
</figure>

<p>However, this will process all inputs in <code>new_inputs</code> at once, which may
not be feasible if you’re looking at a lot of data (in particular, it may
require more memory than your GPU has).</p>
<p>A better way to do inference is to use the <code>predict()</code> method. It will iterate
over the data in small batches and return a NumPy array of predictions.
And unlike <code>__call__</code>, it can also process TensorFlow <code>Dataset</code> objects:</p>
<figure>
<pre><code class="language-python"># Takes a NumPy array or a Dataset and returns a NumPy array
predictions = model.predict(new_inputs, batch_size=128)
</code></pre>
</figure>

<p>For instance, if we use <code>predict()</code> on some of our validation data with the linear
model we trained earlier, we get scalar scores that correspond to the model’s
prediction for each input sample:</p>
<figure>
<pre><code class="language-python">&gt;&gt;&gt; predictions = model.predict(val_inputs, batch_size=128)
&gt;&gt;&gt; print(predictions[:10])</code>
<code class="language-output">[[0.3590725 ]
 [0.82706255]
 [0.74428225]
 [0.682058  ]
 [0.7312616 ]
 [0.6059811 ]
 [0.78046083]
 [0.025846  ]
 [0.16594526]
 [0.72068727]]</code></pre>
</figure>

<p>For now, this is all you need to know about Keras models. At this point, you
are ready to move on to solving real-world machine problems with Keras,
in the next chapter.</p>
<h2 id="summary">Summary</h2>
<ul>
<li>TensorFlow, PyTorch, and JAX are three popular low-level
frameworks for numerical computation and autodifferentiation.
They all have their own way of doing things and their own strengths and weaknesses.</li>
<li>Keras is a high-level API for building and training neural networks. It can be used with either
TensorFlow, PyTorch, or JAX — just pick the backend you like best.</li>
<li>The central class of Keras is the <code>Layer</code>. A layer encapsulates some weights
and some computation. Layers are assembled into models.</li>
<li>Before you start training a model, you need to pick an optimizer, a loss,
and some metrics, which you specify via the <code>model.compile()</code> method.</li>
<li>To train a model, you can use the <code>fit()</code> method, which runs mini-batch gradient
descent for you. You can also use it to monitor your loss and metrics on
validation data, a set of inputs that the model doesn’t see during training.</li>
<li>Once your model is trained, you can use the <code>model.predict()</code> method to generate
predictions on new inputs.</li>
</ul>

&#13;

  <h3>Footnotes</h3>
  <ol>

    <li id="footnote-1">
      R. E. Wengert, “A Simple Automatic Derivative Evaluation Program,” Communications of the ACM, 7 no. 8 (1964).
      <a class="footnote-backlink" href="#footnote-link-1">[↩]</a>
    </li>

    <li id="footnote-2">
      Note that PyTorch is a bit of an intermediate case: while it is mainly a lower-level framework,   it also includes its own layers and its own optimizers. However, if you use PyTorch in conjunction with Keras, then you   will only interact with low-level PyTorch APIs such as tensor operations.
      <a class="footnote-backlink" href="#footnote-link-2">[↩]</a>
    </li>

  </ol>
    
</body>
</html>