<html><head></head><body>
  <h1 class="tochead" id="heading_id_2">5 <a id="idTextAnchor000"/><a id="idTextAnchor001"/><a id="idTextAnchor002"/><a id="idTextAnchor003"/><a id="idTextAnchor004"/><a id="idTextAnchor005"/><a id="idTextAnchor006"/>Managing data with GitHub Copilot and Copilot Chat</h1>

  <p class="co-summary-head">This chapter covers<a id="idIndexMarker000"/><a id="idIndexMarker001"/><a id="marker-90"/></p>

  <ul class="calibre5">
    <li class="co-summary-bullet">Persisting data into a relational database</li>

    <li class="co-summary-bullet">Streaming data using Apache Kafka</li>

    <li class="co-summary-bullet">Incorporating event-driven principles</li>

    <li class="co-summary-bullet">Analyzing data to monitor the location using Spark</li>
  </ul>

  <p class="body">The last chapter laid the foundation for our information technology asset management (ITAM) system. However, this application will not fulfill our requirements without data. Data is the lifeblood of every application. That is what this chapter is all about: the various ways we can use generative AI to create data, stream data, transform data, react to data, and learn from data. <a id="idIndexMarker002"/></p>

  <p class="body">Perceptive individuals may have noticed in the last chapter that our data access pattern would not have worked as it was incomplete. The opening section of this chapter will address this. After that, we will set up our database, fix the classes that access this data, and load some sample data to use in the rest of the chapter.</p>

  <h2 class="fm-head" id="heading_id_3">5.1 Amassing our dataset</h2>

  <p class="body"><a id="marker-91"/>Our first task will be to construct a substantial corpus of data to assist our experimentation in the remainder of the chapter. First we will use GitHub Copilot to generate 1,000 rows of asset information. We will soon find, however, that this may not be the tool best suited to this task. A key driver behind using these tools is the idea of discovery: testing their boundaries, pushing against them, and occasionally pushing back. But the journey is often where the joy is found. Once we have found this edge, we will be introduced to a new, previously unseen tool: GitHub Copilot Chat. Finally, when we have created our list of assets, we will add location information for those assets, again using GitHub Copilot Chat. <a id="idIndexMarker003"/></p>

  <p class="body">We need to get our database running before building our initial dataset. Docker makes this task trivial, allowing us to quickly spin up an empty PostgreSQL (or other RDBMS/NoSQL server) with minimal effort. Have you forgotten the command to do this? No worries—we can ask Copilot. Open a new file called data/initial_data_load.sql and enter the following prompt at the top of your newly minted SQL file:<a id="idIndexMarker004"/></p>
  <pre class="programlistinge">-- Question: What is the command to run a docker container, running PostgreSQL for a database called itam_db. I would like to specify the password for this database.</pre>

  <p class="body">Copilot will slowly reveal the Docker command:</p>
  <pre class="programlistinge">--Answer: docker run --name itam_db 
    -e POSTGRES_PASSWORD=postgres -d 
    -p 5432:5432 postgres</pre>

  <p class="body">Once you run this command at your terminal or command line, we can build out our dataset. You should be able to connect to the locally running database. Notice that a database called <code class="fm-code-in-text">itam_db</code> is running in it. However, this database has no schema, tables, or data. Let’s first set up a new schema.</p>

  <p class="body">In our initial_data_load.sql, we will add a prompt to have Copilot draft the schema creation command. The following prompt (and response from Copilot) will allow you to create a new schema called <code class="fm-code-in-text">itam</code> if executed from within your database client application (e.g., DataGrip, SQuirreL, pdAdmin, or even using the Docker <code class="fm-code-in-text">exec</code> command <code class="fm-code-in-text">docker exec -i itam_db psql -U postgres -c "create schema itam"</code>):<a id="idIndexMarker005"/><a id="idIndexMarker006"/></p>
  <pre class="programlistinge">-- create a schema called itam
create schema itam;</pre>

  <p class="body">Next, we should add a user for use in our application. This user will be able to perform CRUD (create, read, update, delete) operations on our data but will not be able to affect the structure of the database tables or procedures.</p>

  <p class="fm-callout"><span class="fm-callout-head">Note</span> The lines that start with double dashes (<code class="fm-code-in-text">--</code>) are comments in SQL. Commenting out these lines is optional from Copilot’s perspective, as it will generate solutions without the comments; it makes it easier to copy and paste the code directly into our database tool of choice. <a id="idIndexMarker007"/></p>

  <p class="body">While we are at it, we will also add an administrative account to perform the operations that our read-write users cannot, such as creating or dropping tables.</p>

  <p class="fm-code-listing-caption">Listing 5.1 Prompt to create new users</p>
  <pre class="programlisting"><b class="fm-bold">-- create two users called itam_user and itam_admin. itam_user should be able to read from and write to the tables in the itam schema. itam_admin should be able to read from and write to the tables in the itam schema and create and drop tables in the itam schema.</b>
create user itam_user with password 'itam_user';
create user itam_admin with password 'itam_admin';
grant usage on schema itam to itam_user;
grant usage on schema itam to itam_admin;
grant select, insert, update, delete on all tables in schema itam to itam_user;
grant select, insert, update, delete on all tables in schema itam to itam_admin;
grant usage, drop on schema itam to itam_admin;</pre>

  <p class="body"><a id="marker-92"/>Next we will grant ownership of this schema to the <code class="fm-code-in-text">itam_admin</code> account. Transferring this ownership will ensure that only this account can change the table structure (the data definition): <a id="idIndexMarker008"/></p>
  <pre class="programlistinge">-- grant ownership of the itam schema to itam_admin
alter schema itam owner to itam_admin;</pre>

  <p class="body">With the setup, account creation, and ownership of the system out of the way, we can start to focus on the data. We will begin by adding the reference data, which supports the assets: the depreciation strategies. This data is more static in nature; it changes less frequently, if at all. Let’s define and store these strategies.</p>

  <p class="fm-code-listing-caption">Listing 5.2 Prompt to create the <code class="fm-code-in-text">depreciation_strategy</code> table<a id="idIndexMarker009"/></p>
  <pre class="programlisting"><b class="fm-bold">-- create a table called depreciation_strategy in the itam schema. the table should have the following columns: id (int), name (varchar), and description (varchar). the table should have a primary key on id.</b>
<b class="fm-bold">-- id needs to be in quotes because it is a reserved word in postgresql</b>
<b class="fm-bold">-- there are two values for depreciation_strategy: straight line and double declining balance</b>
create table itam.depreciation_strategy (
    "id" int primary key,
    "name" varchar,
    "description" varchar
);</pre>

  <p class="body">We will use a sequence as this table’s primary key. Although this is not strictly necessary for a table that will not be very large and that has known values we can and will enter manually, adding this sequence will allow us to work with Copilot more and have it make some suggestions. Moreover, it is amusing to ask Copilot questions and have Copilot answer in a text file.</p>

  <p class="fm-code-listing-caption">Listing 5.3 Prompt to create a sequence to use as primary key</p>
  <pre class="programlisting"><b class="fm-bold">-- create a sequence called depreciation_strategy_seq, which should start at 1 and increment by 1 and should be used as the primary key for the depreciation_strategy table.</b>
create sequence itam.depreciation_strategy_seq start 1 increment 1;</pre>

  <p class="body"><a id="marker-93"/>Naturally, with the sequence in our proverbial hand, we need to know how to associate the sequence with the primary key column of the <code class="fm-code-in-text">depreciation_strategy</code> table. Luckily, Copilot has the answer. <a id="idIndexMarker010"/></p>

  <p class="fm-code-listing-caption">Listing 5.4 Asking Copilot how to associate the sequence with the primary key</p>
  <pre class="programlisting"><b class="fm-bold">-- question: how do I make the sequence the primary key for the depreciation_strategy table?</b>
 
-- answer: use the following command
alter table itam.depreciation_strategy 
    alter column "id" 
    set default
    nextval('itam.depreciation_strategy_seq'
            ::regclass);</pre>

  <p class="body">Finally, we complete this table by inserting the following static entries into it. We will only use two depreciation strategies for now: straight-line and double declining balance.</p>

  <p class="fm-code-listing-caption">Listing 5.5 Adding the static entries to the <code class="fm-code-in-text">depreciation_strategy</code> table<a id="idIndexMarker011"/></p>
  <pre class="programlisting">insert into depreciation_strategy (id, name, description) 
    values (1, 'straight line', 
    'straight line');
  
insert into depreciation_strategy (id, name, description) 
    values (2, 'double declining balance', 
    'double declining balance');</pre>

  <p class="body">Next we will move on to the <code class="fm-code-in-text">funding_details</code> table. This information tells us how we financed our equipment, the resale value, and instructions for what should be done with an asset once its useful life is over. The sequence of steps will be identical to what we did for the depreciation strategies, with the exception that we will not add static entries, as this data is directly related to an individual asset. We will define the table, create the sequence, and apply that sequence to the table, for which it functions as the primary key. <a id="idIndexMarker012"/></p>

  <p class="fm-code-listing-caption">Listing 5.6 Complete code listing for the <code class="fm-code-in-text">funding_details</code> table</p>
  <pre class="programlisting"><b class="fm-bold">-- create a table called funding_details in the itam schema. the table should have the following columns: id (int), name (varchar),depreciation_strategy_id (int) and depreciation_rate (float). the table should have a primary key on id.</b>
<b class="fm-bold">-- depreciation_strategy_id is a foreign key to the depreciation_strategy table.</b>
<b class="fm-bold">-- id needs to be in quotes because it is a reserved word in postgresql</b>
create table itam.funding_details (
    "id" int primary key,
    "name" varchar,
    "depreciation_strategy_id" int,
    "depreciation_rate" float
);
  
<b class="fm-bold">-- create a sequence called funding_details_seq, which should start at 1 and increment by 1 and should be used as the primary key for the funding_details table.</b>
create sequence itam.funding_details_seq start 1 increment 1;
alter table itam.funding_details 
alter column "id" 
set default 
nextval('itam.funding_details_seq'
    ::regclass);</pre>

  <p class="body">The final information that we will define and generate is the assets themselves. This listing, too, is redundant but included for completeness. Finally, we create the table, make the sequence, and use it as the primary key.<a id="marker-94"/></p>

  <p class="fm-code-listing-caption">Listing 5.7 Complete code listing for the <code class="fm-code-in-text">assets</code> table<a id="idIndexMarker013"/></p>
  <pre class="programlisting"><b class="fm-bold">-- create a table called assets in the itam schema. the table should have the following columns:</b> 
<b class="fm-bold">-- id (int), name (varchar), status (varchar), category (varchar), cost (float), useful_life (int), salvage_value (float), purchase_date (date), funding_details_id (int). The table should have a primary key on id and a foreign key on funding_details_id.</b>
<b class="fm-bold">-- id needs to be in quotes because it is a reserved word in postgresql</b>
<b class="fm-bold">-- the table should have a sequence called assets_id_seq, which should start at 1 and increment by 1 and should be used as the primary key for the assets table.</b>
create table itam.assets (
    "id" int primary key,
    "name" varchar,
    "status" varchar,
    "category" varchar,
    "cost" float,
    "useful_life" int,
    "salvage_value" float,
    "purchase_date" date,
    "funding_details_id" int
);
  
<b class="fm-bold">-- create a sequence called assets_seq, which should start at 1 and increment by 1 and should be used as the primary key for the assets table.</b>
<b class="fm-bold">create sequence itam.assets_seq start 1 increment 1;</b>
  
alter table itam.assets alter column "id" 
set default 
nextval('itam.assets_seq'::
    regclass);</pre>

  <p class="body">With the tables defined and created, we will now focus on creating the data. In our text file, we instruct Copilot with parameters for the dataset we are looking for. Copilot will likely attempt to assist you in outlining the attributes surrounding your new dataset.<a id="marker-95"/></p>

  <p class="fm-code-listing-caption">Listing 5.8 Creating a dataset for the assets table</p>
  <pre class="programlisting"><b class="fm-bold">-- Generate a dataset of assets for an ITAM system. The dataset should include the following columns: id (int), name (varchar), status (varchar), category (varchar), cost (float), useful_life (int), salvage_value (float), purchase_date (date), funding_details_id (int). The dataset should have 1000 rows, sorted by id. Each row should have the following characteristics:</b>
<b class="fm-bold">-- - id should be a unique integer and sequential starting at 1.</b>
<b class="fm-bold">-- - name should be a random string of characters between 1 and 50 characters long.</b>
<b class="fm-bold">-- - status should be a random selection from the following valid asset statuses: in use, in storage, disposed of, in repair, in transit, other.</b>
<b class="fm-bold">-- - category should be a random selection from the following valid categories: hardware, software, other.</b>
<b class="fm-bold">-- - cost should be a random float between 0 and 100000.</b>
<b class="fm-bold">-- - useful_life should be a random int between 1 and 10.</b>
<b class="fm-bold">-- - salvage_value should be a random float greater than 0 but less than the cost of the asset.</b>
<b class="fm-bold">-- - purchase_date should be a random date between 1/1/2019 and 12/31/2022.</b>
<b class="fm-bold">-- - funding_details_id should be a random integer either 1 or 2.</b>
<b class="fm-bold">-- The dataset should be saved as a CSV file named assets.csv in the data directory. The file should have a header row and the columns should have the following data types: id (int), name (varchar), status (varchar), category (varchar), cost (float), useful_life (float), salvage_value (float), funding_details_id (int)</b></pre>

  <p class="body">The solution that Copilot provides is novel. It builds a large series using a Postgres built-in function, meaning this solution would not be portable. However, given that this is the database we will use, it is an appropriate enough solution. The resulting dataset is refined. We would have gotten better results if we had used Python and asked for Copilot’s assistance in coding a script to generate a file to load into Postgres. However, given that this dataset is only for playing with the application, we do not need to be overly concerned with the data quality for now—although in the real world, data quality is everything.</p>

  <p class="fm-code-listing-caption">Listing 5.9 Copilot’s response: an <code class="fm-code-in-text">insert</code> statement built off of a series<a id="idIndexMarker014"/><a id="marker-96"/></p>
  <pre class="programlisting">insert into itam.assets (id, name, status, 
    category, cost, useful_life, salvage_value, 
    purchase_date, funding_details_id)
select
    id,
    name,
    status,
    category,
    cost,
    useful_life,
    salvage_value,
    purchase_date,
    funding_details_id
from (
    select
        row_number() over (order by random()) as id,
        md5(random()::text) as name,
        case
            when random() &lt; 0.2 then 'in use'
            when random() &lt; 0.4 then 'in storage'
            when random() &lt; 0.6 then 'disposed of'
            when random() &lt; 0.8 then 'in repair'
            when random() &lt; 0.9 then 'in transit'
            else 'other'
        end as status,
        case
            when random() &lt; 0.5 then 'hardware'
            when random() &lt; 0.9 then 'software'
            else 'other'
        end as category,
        random() * 100000 as cost,
        (random() * 100)::int as useful_life,
        random() * (random() * 100000) as salvage_value,
        -- generate a random date between 1/1/2019 and 12/31/2022
        -- this does not work please fix
        -- '2019-01-01'::date + random() * 
            ('2022-12-31'::date - '2019-01-01'::date) 
            as purchase_date,
        '2019-01-01'::date + (random() * 
          (DATE '2022-12-31' - DATE '2019-01-01')
          ::integer)::integer as purchase_date
        case
            when random() &lt; 0.5 then 1
            else 2
        end as funding_details_id
    from generate_series(1, 1000)
) as assets;</pre>

  <p class="body">If we switch back to ChatGPT for a minute, we can get a second opinion about how to create such a dataset. ChatGPT suggests the Python library <code class="fm-code-in-text">faker</code>. The <code class="fm-code-in-text">faker</code> package is used to generate fake data, such as common English first names. <code class="fm-code-in-text">numpy</code> is used to generate the random float values for cost, useful life, and salvage value. <code class="fm-code-in-text">pandas</code> is used to manage the data in a <code class="fm-code-in-text">DataFrame</code> (the table). Additionally, we can save the <code class="fm-code-in-text">DataFrame</code> to a CSV file using the method <code class="fm-code-in-text">df.to_csv('assets.csv', index=False)</code>.<a id="idIndexMarker015"/><a id="idIndexMarker016"/><a id="idIndexMarker017"/><a id="idIndexMarker018"/></p>

  <p class="fm-code-listing-caption">Listing 5.10 ChatGPT suggests <code class="fm-code-in-text">Faker</code> to generate the fake dataset</p>
  <pre class="programlisting">import pandas as pd
import numpy as np
import random
from faker import Faker
  
# Create Faker object
fake = Faker()
  
# Define our valid statuses, categories, funding_details and depreciation_strategies
valid_statuses = ['in use', 'in storage', 
'disposed', 'in repair', 'in transit', 'other']
valid_categories = ['hardware']
valid_funding_details = ['purchased']
valid_depreciation_strategies = ['straight line']
  
# Generate the data
data = {
    'id': list(range(1, 1001)),
    'name': [fake.first_name() for _ in range(1000)],
    'status': [random.choice(valid_statuses) for _ in range(1000)],
    'category': [random.choice(valid_categories) for _ in range(1000)],
    'cost': np.random.uniform(0, 100000, 1000),
    'useful_life': np.random.uniform(1, 10, 1000),
    'salvage_value': np.random.uniform(0, 10000, 1000),
    'funding_details': [random.choice(valid_funding_details)
 for _ in range(1000)],
    'depreciation_strategy': [random.choice(
    valid_depreciation_strategies) 
    for _ in range(1000)],
}
  
# Create DataFrame
df = pd.DataFrame(data)
  
# Ensure the DataFrame is sorted by id
df = df.sort_values('id')
 
# Show the DataFrame
print(df)</pre>

  <p class="body"><a id="marker-97"/>For each of these assets, we will need funding details as well: how they were financed (purchased, in this case) and the depreciation details. Unsurprisingly, we get a similar solution from Copilot: generate a series of entries using a prompt similar to the one we used for the assets. We will need to ensure that for each of the asset identifiers (1–1000), we have a corresponding funding details entry. Otherwise we risk getting null pointers when running this code. <a id="idIndexMarker019"/></p>

  <p class="fm-code-listing-caption">Listing 5.11 Creating a dataset for the <code class="fm-code-in-text">funding_details</code> table</p>
  <pre class="programlisting"><b class="fm-bold">-- Generate a dataset of funding details for an ITAM system. The dataset should include the following columns: id (int), name (varchar), depreciation_strategy_id (int), depreciation_rate (float). The dataset should have 1000 rows, sorted by id.</b> 
<b class="fm-bold">-- Each row should have the following characteristics:</b>
<b class="fm-bold">-- - id should be a unique integer and sequential starting at 1.</b>
<b class="fm-bold">-- - name should be a random string of characters between 1 and 10 characters long.</b>
<b class="fm-bold">-- - depreciation_strategy_id should be a random integer either 1 or 2.</b>
<b class="fm-bold">-- - depreciation_rate should be a random float between 0 and .4.</b>
  
insert into itam.funding_details (id, name, 
    depreciation_strategy_id, depreciation_rate)
select
    id,
    name,
    depreciation_strategy_id,
    depreciation_rate
from (
    select
        row_number() over (order by random()) as id,
        md5(random()::text) as name,
        case
            when random() &lt; 0.5 then 1
            else 2
        end as depreciation_strategy_id,
        random() * 0.4 as depreciation_rate
    from generate_series(1, 1000)
) as funding_details;</pre>

  <p class="body">With the dataset generated and stored in the database, we should be able to wire up the remainder of our application to store and display assets using the REST APIs. However, because we previously stripped out all the metadata for SQLAlchemy during our build phase (see chapter 4), we need a way to wire this metadata with our adapters differently.</p>

  <p class="body"><a id="marker-98"/>At this point we have reached the edge of Copilot’s capabilities. We are perplexed by what comes next and how we can solve our most recent dilemma. Tempting as it is, we cannot give up and go home. Therefore, it is time to introduce the most recent addition to the Copilot product suite: Copilot Chat. Copilot Chat is a GPT-4 model embedded in an IDE (currently supported only by Visual Studio Code). We will open the chat dialog and ask how to keep our business model clean while still using SQLAlchemy’s object-relational model (ORM) features. Figure 5.1 shows the response from ChatGPT.<a id="idIndexMarker020"/></p>

  <p class="body">Copilot Chat suggests that we create a separate data access layer. This approach maps nicely onto the ports and adapters approach we have used thus far. In addition, Copilot Chat recommends modeling these classes similarly to the domain classes but including the metadata required for ORM functionality to work correctly. The resulting code is shown in listing 5.12.<a id="idIndexMarker021"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH05_F01_Crocker2.png"/></p>

    <p class="figurecaption">Figure 5.1 GitHub Copilot Chat’s solution for how to solve our most recent quandary</p>
  </div>

  <p class="fm-code-listing-caption">Listing 5.12 ORM support outside of the domain classes</p>
  <pre class="programlisting">Base  = declarative_base()
  
class FundingDetailsModel(Base):
    __tablename__ = 'funding_details'
    id = Column(Integer, primary_key=True)
    depreciation_rate = Column(Float)
    depreciation_strategy_id = Column(Integer)
  
    def get_depreciation_strategy(self) -&gt; DepreciationStrategy:
        if self.depreciation_strategy_id is 1:
            return StraightLineDepreciationStrategy()
        else:
            return DoubleDecliningDepreciationStrategy()
  
class AssetModel(Base):
    __tablename__ = 'assets'
    id = Column(Integer, primary_key=True)
    name = Column(String)
    status = Column(String)
    category = Column(String)
    cost = Column(Float)
    useful_life = Column(Float)
    salvage_value = Column(Float)
    purchase_date = Column(Date)
    funding_details_id = Column(Integer, ForeignKey('funding_details.id'))
    funding_details = relationship('FundingDetailsModel')</pre>

  <p class="body"><a id="marker-99"/>Now that the external model classes have been created, we must map these ORM instances to our domain model before returning them to the system’s core. This may seem like over-engineered code for such a simple application, but it gives us great flexibility in how our domain model can operate. For example, our model can perform complex operations beyond just CRUD. We would be limited to these operations if we kept our domain model identity to the model used in the data access layer.</p>

  <p class="body">Next we will use Copilot and Copilot Chat to explore incorporating event-driven ideas into our application. Event-driven concepts will allow us to track our IT assets in real time: their location, status, and market value, for example.</p>

  <h2 class="fm-head" id="heading_id_4">5.2 Monitoring our assets in real time with Kafka</h2>

  <p class="body">We will monitor our assets in real time to motivate our exploration of using generative AI in conjunction with event-driven architecture. We will take it as a given that some system external to the ISAM system fires events as our assets move from one location to another.<a id="idIndexMarker022"/><a id="idIndexMarker023"/><a id="idIndexMarker024"/><a id="marker-100"/></p>

  <p class="body">To delve into ITAM events, we will need to configure a few additional services. In this case, we will use Apache Kafka. Apache Kafka is a distributed streaming platform that is used to build real-time data pipelines and streaming apps. It’s designed to handle data streams from multiple sources and deliver them to multiple consumers, effectively acting as a middleman for our real-time data.</p>

  <p class="body">To start, we will ask Copilot Chat how to run Kafka locally using Docker. Apache Kafka has an undeserved reputation for being difficult to install and configure, and running in Docker will allow us to side-step this controversy. Using Copilot Chat, we can produce a Docker Compose file. However, as is often the case, the versions are very old, to the point of not supporting some hardware. Listing 5.13 is an updated listing from Confluent’s (the company that offers commercial support for Kafka) official GitHub repository. Notice that the Docker Compose file’s contents include both Kafka and Zookeeper. Zookeeper is a distributed coordination service that Kafka uses to manage and coordinate the brokers in the cluster, at least for now. Future versions aim to remove dependency on Zookeeper.</p>

  <p class="fm-code-listing-caption">Listing 5.13 Docker Compose file to launch Kafka with Zookeeper</p>
  <pre class="programlisting">version: '2.1'
  
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.2
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_SERVERS: zoo1:2888:3888
  
  kafka:
    image: confluentinc/cp-kafka:7.3.2
    hostname: kafka
    container_name: kafka
    ports:
      - "9092:9092"
      - "29092:29092"
      - "9999:9999"
    environment:
      KAFKA_ADVERTISED_LISTENERS: 
            INTERNAL://kafka:19092,EXTERNAL://
            ${DOCKER_HOST_IP:127.0.0.1}:9092,
            DOCKER://host.docker.internal:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,
DOCKER:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
      KAFKA_BROKER_ID: 1
      KAFKA_LOG4J_LOGGERS: "kafka.controller=
            INFO,kafka.producer.async
            .DefaultEventHandler=INFO,
            state.change.logger=INFO"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_JMX_PORT: 9999
      KAFKA_JMX_HOSTNAME: ${DOCKER_HOST_IP:-127.0.0.1}
      KAFKA_AUTHORIZER_CLASS_NAME: kafka.security.authorizer.AclAuthorizer
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "true"
    depends_on:
      - zookeeper</pre>

  <p class="body">With our new Kafka instance running, we now need a consumer that will pull the updated locations off the topic <code class="fm-code-in-text">asset_location</code> (which we will create shortly) and update the inventory in the database. Again, we can ask Copilot Chat to provide us with a suggestion for how to use Python to subscribe to the topic:</p>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="7%"/>
      <col class="contenttable-0-col" span="1" width="93%"/>
    </colgroup>

    <tbody class="calibre7">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figure1"><img alt="" class="calibre8" src="../Images/logo-NC.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-table-bodyt">In our ITAM project, we would like a Python consumer for the asset_location Kafka topic. Once it receives a new updated location, it should update the domain model to reflect the new location. How should we do this?</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body"><a id="marker-101"/>The code that Copilot Chat generates creates a consumer, listens to the topic, and uses a reference to the <code class="fm-code-in-text">AssetManager</code> class to update the location of the affected asset. <a id="idIndexMarker025"/></p>

  <p class="fm-code-listing-caption">Listing 5.14 Copilot Chat code to monitor assets’ locations</p>
  <pre class="programlisting">from kafka import KafkaConsumer
from asset_manager import AssetManager
  
consumer = KafkaConsumer(
    'asset_location',
    bootstrap_servers=['localhost:9092'],
    auto_offset_reset='earliest',
    enable_auto_commit=True,
    group_id='my-group',
    value_deserializer=lambda x: 
            tuple(map(float, x.decode('utf-8')
            .split(',')))
)
  
asset_manager = AssetManager()
  
for message in consumer:
    asset_id, latitude, longitude = message.value
    asset_manager.update_asset_location(asset_id, (latitude, longitude))</pre>

  <p class="body">However, should the adapter be aware of <code class="fm-code-in-text">AssetManager</code>? The <code class="fm-code-in-text">AssetManager</code> does not directly sit inside the domain model; it is a critical intermediary. This coupling level between <code class="fm-code-in-text">AssetManager</code> and the adapter could arguably violate the hexagon in the hexagonal architecture model. Let’s have Copilot Chat weigh in. We can ask it this:</p>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="7%"/>
      <col class="contenttable-0-col" span="1" width="93%"/>
    </colgroup>

    <tbody class="calibre7">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figure1"><img alt="" class="calibre8" src="../Images/logo-NC.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-table-bodyt">Given that this project uses hexagonal architecture, is it a good idea to have an adapter aware of the AssetManager?</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body"><a id="marker-102"/>Looking at figure 5.2, it appears that Copilot Chat agrees that this would indeed be considered a breach of the contractual responsibility of an adapter. We could add a new port to our <code class="fm-code-in-text">AssetManager</code> class that would use the Kafka port. However, let’s see if Copilot Chat has any other suggestions: <a id="idIndexMarker026"/></p>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="7%"/>
      <col class="contenttable-0-col" span="1" width="93%"/>
    </colgroup>

    <tbody class="calibre7">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figure1"><img alt="" class="calibre8" src="../Images/logo-NC.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-table-bodyt">We do not want the Kafka consumer to interact directly with the AssetManager. Are there any ways to accomplish this?</p>
        </td>
      </tr>
    </tbody>
  </table>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH05_F02_Crocker2.png"/></p>

    <p class="figurecaption">Figure 5.2 GitHub Copilot Chat’s acceptance that it is, in fact, imperfect</p>
  </div>

  <p class="body">Copilot Chat suggests that we apply the <code class="fm-code-in-text">Mediator</code> pattern, which resembles a port and adapter combination.<a id="idIndexMarker027"/></p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title">Mediator Pattern</p>

    <p class="fm-sidebar-text">The Mediator design pattern is a behavioral pattern that promotes loose coupling between objects by encapsulating their interactions in a mediator object. The mediator object acts as a central hub that facilitates communication between objects without the objects having to know about each other.<a id="idIndexMarker028"/></p>
  </div>

  <p class="body">Let’s first take a look at the <code class="fm-code-in-text">Mediator</code> class that Copilot Chat generates. The class sits between <code class="fm-code-in-text">AssetManager</code> and the Kafka consumer. <a id="idIndexMarker029"/><a id="marker-103"/></p>

  <p class="fm-code-listing-caption">Listing 5.15 <code class="fm-code-in-text">Mediator</code> class between <code class="fm-code-in-text">AssetManager</code> and Kafka consumer</p>
  <pre class="programlisting">class AssetLocationMediator:
    def __init__(self):
        self.handlers = {}
  
    def register_handler(self, event_type, handler):
        if event_type not in self.handlers:
            self.handlers[event_type] = []
        self.handlers[event_type].append(handler)
  
    def publish(self, event):
        event_type = type(event)
        if event_type in self.handlers:
            for handler in self.handlers[event_type]:
                handler(event)</pre>

  <p class="body">This implementation closely resembles a message queue or message publisher, which is precisely the point: decoupling.</p>

  <p class="fm-callout"><span class="fm-callout-head">Note</span> Eagle-eyed readers will likely notice that we are playing a little fast and loose with the distinction between an <code class="fm-code-in-text">Asset</code> and an asset of type <code class="fm-code-in-text">Hardware</code>. In the original domain model, only <code class="fm-code-in-text">Hardware</code> had a location. Generally, we do not think of software as having a location. Of course, you could say that software is installed in its location, but it is arguable how convincing this argument is. As this project continues, we flatten the domain model for simplicity’s sake, as polymorphic structures in a persistence layer are a distractingly complex topic. <a id="idIndexMarker030"/><a id="idIndexMarker031"/></p>

  <p class="body">Now that we have a strategy to decouple the Kafka consumer from <code class="fm-code-in-text">AssetManager</code>, we should update the Kafka consumer to take advantage of it. We need to pass the mediator into the class in its constructor. This way, <code class="fm-code-in-text">AssetManager</code> and the consumer will have access to the same instance, and messages can freely flow back and forth— or rather, in this case, the flow will be unidirectional. You should note that we intend to read and write JSON on this topic, so our value deserializer needs to understand this.</p>

  <p class="fm-code-listing-caption">Listing 5.16 Incorporating the mediator into the Kafka consumer class</p>
  <pre class="programlisting">from kafka import KafkaConsumer
from itam.domain.events.asset_location_updated import AssetLocationUpdated
import json
  
class AssetLocationKafkaConsumer:
    def __init__(self, mediator):
        self.mediator = mediator
  
        self.consumer = KafkaConsumer(
            'asset_location',
            bootstrap_servers=['localhost:9092'],
            enable_auto_commit=True,
            group_id='itam-group',
            value_deserializer=lambda m: json.loads(m.decode('utf-8'))
        )
  
    def poll(self):
        print("Polling for asset location updates")
        for message in self.consumer:
            asset_id = message.value['asset_id']
            latitude = message.value['latitude']
            longitude = message.value['longitude']
            timestamp = message.value['timestamp']
            event = AssetLocationUpdated(asset_id, latitude, longitude, timestamp)
            self.mediator.publish(event)</pre>

  <p class="body">Next we will examine the changes that the <code class="fm-code-in-text">AssetManager</code> class requires to incorporate the ability to track these locations. <a id="idIndexMarker032"/><a id="marker-104"/></p>

  <p class="fm-callout"><span class="fm-callout-head">Note</span> To run this project in its entirety, you would need to modify the <code class="fm-code-in-text">AssetManager</code>, <code class="fm-code-in-text">SQLAlchemyAssetRepository</code>, and <code class="fm-code-in-text">Asset</code> classes and also create a new table in your database called <code class="fm-code-in-text">itam.asset_locations</code>. The complete and updated source code is available on the book’s website (<a class="url" href="https://www.manning.com/books/ai-powered-developer">www.manning.com/books/ai-powered-developer</a>) and in the book’s GitHub repository (<a class="url" href="https://github.com/nathanbcrocker/ai_assisted_dev_public">https://github.com/nathanbcrocker/ai_assisted_dev_public</a>). For now, we will focus on the changes needed to get the events flowing through our system and use the repository for reference if the reader so chooses. <a id="idIndexMarker033"/><a id="idIndexMarker034"/><a id="idIndexMarker035"/></p>

  <p class="body">Figure 5.3 shows the changes required to the <code class="fm-code-in-text">AssetManager</code> class to begin to track the location of our assets in real time.<a id="idIndexMarker036"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH05_F03_Crocker2.png"/></p>

    <p class="figurecaption">Figure 5.3 <code class="fm-code-in-text">AssetManager</code> requires the addition of another constructor parameter and a method to handle the updates to its location objects.</p>
  </div>

  <p class="body">There are two required changes for the <code class="fm-code-in-text">AssetManager</code> class. First, we need to add the <code class="fm-code-in-text">AssetLocationMediator</code> to the constructor, registering it to handle the <code class="fm-code-in-text">AssetLocationUpdated</code> event. And second, we need to add a method that will handle this event. In this case, we call the method <code class="fm-code-in-text">update_asset_location</code>. The abridged code is shown next.<a id="idIndexMarker037"/><a id="idIndexMarker038"/><a id="idIndexMarker039"/><a id="idIndexMarker040"/></p>

  <p class="fm-code-listing-caption">Listing 5.17 Updated constructor and an event handler for <code class="fm-code-in-text">AssetManager</code><a id="idIndexMarker041"/></p>
  <pre class="programlisting">from itam.infrastructure.mediators.asset_location_mediator import 
  
class AssetManager:
    def __init__(self, base_repository: 
            BaseRepository[Asset], 
            mediator: AssetLocationMediator):
        self._repository = base_repository
        self.mediator = mediator
        self.mediator.register_handler(
            AssetLocationUpdated, 
            self.update_asset_location)
  
    def update_asset_location(self, event: AssetLocationUpdated) -&gt; None:
        asset = self.read(event.asset_id)
        asset.add_location(event.latitude, 
            event.longitude, event.timestamp)
        #self.update(asset)
        print(f"Asset {asset.id} location updated 
            to {event.latitude}, {event.longitude} 
            at {event.timestamp}")</pre>

  <p class="body">The <code class="fm-code-in-text">add_location</code> method of the <code class="fm-code-in-text">Asset</code> class merely appends the new <code class="fm-code-in-text">Location</code> to the end of a list of <code class="fm-code-in-text">Location</code>s. More sophisticated domain models may include a <code class="fm-code-in-text">current_location</code> attribute, relegating the rest to a list of historical locations; however, given that we are trying to get our events flowing through the system, it behooves us to keep things simple. <a id="idIndexMarker042"/><a id="idIndexMarker043"/><a id="idIndexMarker044"/><a id="marker-105"/></p>

  <p class="body">There is only one final item on our to-do list: create the topic. How do we do this? That is a good question. Fortunately, all the tools we need are available in our running Docker container. So, let’s log in to our Kafka Docker instance. We use the following command (assuming that your Docker instance is named <code class="fm-code-in-text">kafka</code>):</p>
  <pre class="programlisting">docker exec -it kafka /bin/bash</pre>

  <p class="body">The first thing to check is whether any topics are already created. We can do that with the following command:</p>
  <pre class="programlisting">kafka-topics --list --bootstrap-server localhost:9092</pre>

  <p class="body">This command lists all the existing topics running on this Kafka cluster. As you can see, there aren’t any.</p>

  <p class="body">Given the need for a topic, let’s create it. Use the following command:</p>
  <pre class="programlisting">kafka-topics --create --bootstrap-server localhost:9092 
    --replication-factor 1 
    --partitions 1 
    --topic asset_location</pre>

  <p class="body">If you run the <code class="fm-code-in-text">kafka-topics --list</code> command again, you will see the new topic. The partitions and replication-factor instructions we included in the create-topic command inform Kafka that we want one partition and a replication factor of 1. If we were setting this up for production or any purpose other than testing, we would likely want them to be greater than that to ensure the availability of data. Table 5.1 provides you with some of the commonly used Kafka commands that you will need for this and other projects.<a id="idIndexMarker045"/></p>

  <p class="fm-table-caption">Table 5.1 Summary of Kafka console commands</p>

  <table border="1" class="contenttable-1-table" id="table001" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="15%"/>
      <col class="contenttable-0-col" span="1" width="85%"/>
    </colgroup>

    <thead class="calibre7">
      <tr class="contenttable-0-tr">
        <th class="contenttable-1-th">
          <p class="fm-table-head">Action</p>
        </th>

        <th class="contenttable-1-th">
          <p class="fm-table-head">Command</p>
        </th>
      </tr>
    </thead>

    <tbody class="calibre7">
      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Create</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body"><code class="fm-code-in-text">kafka-topics --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic asset_location</code></p>
        </td>
      </tr>

      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Read</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body"><code class="fm-code-in-text">kafka-console-consumer --broker-list localhost:9092 --topic asset_location –from-beginning</code></p>
        </td>
      </tr>

      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Write</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body"><code class="fm-code-in-text">kafka-console-producer --broker-list localhost:9092 --topic asset_location</code></p>
        </td>
      </tr>

      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Delete</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body"><code class="fm-code-in-text">kafka-topics --delete --topic asset_location --bootstrap-server localhost:9092</code></p>
        </td>
      </tr>

      <tr class="contenttable-0-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">List</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body"><code class="fm-code-in-text">kafka-topics --list --bootstrap-server localhost:9092</code></p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body">Now comes the fun part: observing the application in action. Kafka comes with a console producer that will allow us to publish messages to Kafka from standard input. To do this, launch the console producer with the following command:</p>
  <pre class="programlisting">kafka-console-producer --broker-list localhost:9092 --topic asset_location</pre>

  <p class="body">You will enter an interactive session allowing you to publish a message with every line. Let’s publish a few messages simulating our asset moving around or near Chicago.</p>

  <p class="fm-code-listing-caption">Listing 5.18 Entries for the Kafka console producer<a id="idIndexMarker046"/><a id="marker-106"/></p>
  <pre class="programlisting">{"asset_id": 1, "latitude": 41.8781, "longitude": -87.6298, 
    "timestamp": "2022-01-01T00:00:00Z"}
{"asset_id": 1, "latitude": 41.9000, "longitude": -87.6244, 
    "timestamp": "2022-01-01T00:10:00Z"}
{"asset_id": 1, "latitude": 41.8676, "longitude": -87.6270, 
    "timestamp": "2022-01-01T00:20:00Z"}
{"asset_id": 1, "latitude": 41.8788, "longitude": -87.6359, 
    "timestamp": "2022-01-01T00:30:00Z"}
{"asset_id": 1, "latitude": 41.8740, "longitude": -87.6298, "timestamp": "2022-01-01T00:40:00Z"}</pre>

  <p class="body">As you enter these messages, you should see the output from your application indicating that the location has been updated.</p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title">Deleting a topic</p>

    <p class="fm-sidebar-text">For the sake of completeness, there is one more command you should be aware of. You might make a mistake when entering these messages, and an invalid message could potentially break your consumer. One possible solution is to delete the topic. Deleting a topic may sound dramatic, but it will solve the problem. So here is that command:</p>
    <pre class="programlisting">kafka-topics --delete --topic asset_location --bootstrap-server 
localhost:9092</pre>
  </div>

  <p class="body">In this section, we have added the ability to see changes in the location of our <code class="fm-code-in-text">Asset</code>s in real-time tracking using Apache Kafka. In the final section of this chapter, we will work with Copilot Chat to extend the capacity by monitoring our assets in real time and attempting to determine if they are where they should be. We will explore using Spark and Kafka together to accomplish this analysis. Once completed, we will win the thanks of our Information Security team, who fear that too much of our core business and intellectual property exists on and in these <code class="fm-code-in-text">Asset</code>s.<a id="idIndexMarker047"/><a id="idIndexMarker048"/><a id="idIndexMarker049"/></p>

  <h2 class="fm-head" id="heading_id_5">5.3 Analyzing, learning, and tracking with Apache Spark</h2>

  <p class="body"><a id="marker-107"/>Real-time tracking of assets is a business-critical function. Your IT assets contain sensitive business data, client lists, sales figures, profit-and-loss (PnL) projections, and sales strategies, among many other items. A lost asset can be an existential event for a company. Therefore, careful management and monitoring are priority one for many InfoSec professionals. In this section, we aim to make their jobs substantially easier. Modern data platforms make it trivial to track your assets in real time and send notifications if questionable conditions arise. Let’s get into it.<a id="idIndexMarker050"/><a id="idIndexMarker051"/><a id="idIndexMarker052"/></p>

  <p class="body">Apache Spark is a powerful open source data-processing engine built around speed, ease of use, and sophisticated analytics. It was developed to provide an improved alternative to MapReduce for processing big datasets and can handle batch and real-time analytics. Spark provides APIs for Scala, Java, Python, and R and a built-in module for SQL queries. Its core data structure, the resilient distributed dataset (RDD), enables fault-tolerant operation and allows data to be processed in parallel across a cluster of computers.<a id="idIndexMarker053"/></p>

  <p class="body">Spark also includes several libraries to broaden its capabilities, including MLlib for machine learning, Spark Streaming for processing live data streams, and Spark SQL and DataFrames for processing structured data. These tools make it well-suited for tasks ranging from machine learning to real-time data streaming and batch processing. Its in-memory processing capabilities make Spark significantly faster than its predecessor, so it is a popular choice for big data processing.</p>

  <p class="body">First we will ask Copilot Chat to recommend a strategy for using Apache Spark to track our assets:</p>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="7%"/>
      <col class="contenttable-0-col" span="1" width="93%"/>
    </colgroup>

    <tbody class="calibre7">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figure1"><img alt="" class="calibre8" src="../Images/logo-NC.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-table-bodyt">Let's imagine that our company is in Chicago. We want to create a class called AssetLocationSparkAdapter that will continuously stream AssetLocation json messages from a Kafka topic called asset_location. An AssetLocation has the following attributes asset_id: int, latitude: float, longitude: float, and timestamp: datetime. The AssetLocations are stored in a Postgres database in a table called itam.asset_locations. AssetLocationSparkAdapter should calculate if the new AssetLocation is more than 25 miles from Chicago. If it is, then it should write a message to the console. This should be using the latest version of Spark. How would we do this?</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body">Copilot Chat generates a class that you should be able to put into a file called asset_location_spark_adapter.py in the infrastructure package. Helpfully, it also includes comments for each line, so you should find the generated code easy to follow. The <code class="fm-code-in-text">import</code> statements include the Spark libraries as well as <code class="fm-code-in-text">geopy</code>.<a id="idIndexMarker054"/></p>

  <p class="fm-code-listing-caption">Listing 5.19 The <code class="fm-code-in-text">import</code>s required to run Spark</p>
  <pre class="programlisting">from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, expr
from pyspark.sql.types import StructType, 
    StructField, IntegerType, DoubleType, 
    TimestampType
from geopy.distance import distance</pre>

  <p class="body">The class begins with an overstuffed constructor that defines the schema Spark will use when it translates the JSON to a DataFrame.</p>

  <p class="fm-callout"><span class="fm-callout-head">Note</span> The <code class="fm-code-in-text">AssetLocationSparkAdapter</code>, as defined, is a blocking process. Therefore, your FastAPI application will not “fully” boot until the Spark process has been killed. You want this to be a standalone process, or you need to introduce an asynchronous framework to have these two processes run concomitantly. <a id="idIndexMarker055"/><a id="marker-108"/></p>

  <p class="body">Next it starts up a local Spark instance/session that will allow Spark to connect to the Kafka topic and continuously stream in the records.</p>

  <p class="fm-code-listing-caption">Listing 5.20 <code class="fm-code-in-text">AssessLocationSparkAdapter</code>, which processes the Kafka topic<a id="idIndexMarker056"/></p>
  <pre class="programlisting">class AssetLocationSparkAdapter:
    def __init__(self):
        # Define the schema for the incoming JSON data
        self.schema = StructType([
            StructField("asset_id", IntegerType()),
            StructField("latitude", DoubleType()),
            StructField("longitude", DoubleType()),
            StructField("timestamp", TimestampType())
        ])
  
        # Create a SparkSession
        self.spark = SparkSession.builder \
            .appName("AssetLocationSparkAdapter") \
            .getOrCreate()
  
        # Create a streaming DataFrame from the asset_location topic
        self.df = self.spark \
            .readStream \
            .format("kafka") \
            .option("kafka.bootstrap.servers", "localhost:9092") \
            .option("subscribe", "asset_location") \
            .option("startingOffsets", "earliest") \
            .load() \
            .selectExpr("CAST(value AS STRING)")
  
        # Parse the incoming JSON data
        self.parsed_stream = self.df \
            .select(from_json(col("value"), self.schema).alias("data")) \
            .select("data.*")</pre>

  <p class="body">The final section of the <code class="fm-code-in-text">AssetLocationSparkAdapter</code> class calculates the distance from the asset’s current location to Chicago. If the difference is greater than 25 miles, it sends the result set to the console. Additionally, it provides a method to start and stop the adapter.</p>

  <p class="fm-code-listing-caption">Listing 5.21 Calculating the distance from the <code class="fm-code-in-text">Asset</code> location to Chicago<a id="idIndexMarker057"/><a id="marker-109"/></p>
  <pre class="programlisting">        <b class="fm-bold"># Calculate the distance between the current location and Chicago for each asset</b>
        self.distance = self.parsed_stream \
            .withColumn("distance", 
            expr("calculate_distance(latitude, 
            longitude, 41.8781, -87.6298)")) \
            .select(col("asset_id"), col("timestamp"), col("distance")) \
            .filter(col("distance") &gt; 25)
  
        # Write the results to the console
        self.query = self.distance \
            .writeStream \
            .outputMode("append") \
            .format("console") \
            .start()
  
    def run(self):
        # Start the streaming query
        self.query.awaitTermination()
  
    def stop(self):
        # Stop the streaming query and SparkSession
        self.query.stop()
        self.spark.stop()</pre>

  <p class="body">The <code class="fm-code-in-text">calculate_distance</code> method takes the longitude and latitude of the asset’s location and determines the distance from Chicago using the <code class="fm-code-in-text">geopy.distance</code> function. <a id="idIndexMarker058"/><a id="idIndexMarker059"/></p>

  <p class="fm-code-listing-caption">Listing 5.22 Function to calculate the distance between Chi-town and <code class="fm-code-in-text">Asset</code></p>
  <pre class="programlisting">def calculate_distance(lat1, lon1, lat2, lon2):
    return distance((lat1, lon1), (lat2, lon2)).miles</pre>

  <p class="body">In this instance, the code that Copilot Chat produced had some problems preventing it from running locally. After running it locally, encountering these problems, and trolling Stack Overflow, you would find a solution to the two main problems with the code: a missing environmental variable for running locally and failing to register your UDF (User Defined Function). Fortunately, you do not need to do the testing and research—a solution is provided in the following listing.</p>

  <p class="fm-code-listing-caption">Listing 5.23 Edits required to run the application locally</p>
  <pre class="programlisting">os.environ['PYSPARK_SUBMIT_ARGS'] = 
    '--packages org.apache.spark:
         spark-streaming-kafka-0-10_2.12:3.2.0,
         org.apache.spark:
         spark-sql-kafka-0-10_2.12:3.2.0 
         pyspark-shell'
  
class AssetLocationSparkAdapter:
    def __init__(self):
        # Create a SparkSession
        self.spark = SparkSession.builder \
            .appName("AssetLocationSparkAdapter") \
            .getOrCreate()
         self.spark.udf.register("calculate_distance", calculate_distance)</pre>

  <p class="body">Finally, to run your Spark application, update main.py with the following code in the <code class="fm-code-in-text">main</code> function.<a id="idIndexMarker060"/><a id="marker-110"/></p>

  <p class="fm-code-listing-caption">Listing 5.24 Updates to the <code class="fm-code-in-text">main</code> function</p>
  <pre class="programlisting">if __name__ == "__main__":
    adapter = AssetLocationSparkAdapter()
    adapter.run()</pre>

  <p class="body">As you enter locations for your asset into the Kafka console producer that are further than 25 miles from downtown Chicago, you will notice that entries are written to the console. It would be trivial to update the class to output these results to Twilio’s SMS API or an email service such as SendGrid.<a id="idIndexMarker061"/><a id="idIndexMarker062"/></p>

  <p class="fm-code-listing-caption">Listing 5.25 The streaming output from your asset location</p>
  <pre class="programlisting">+--------+-------------------+------------------+
|asset_id|          timestamp|          distance|
+--------+-------------------+------------------+
|       1|2021-12-31 20:30:00| 712.8314662207446|
+--------+-------------------+------------------+</pre>

  <p class="body">Congratulations! You are tracking your assets in real time and sending real-time alerts in case your corporate resources grow legs and walk away.<a id="idIndexMarker063"/><a id="idIndexMarker064"/></p>

  <h2 class="fm-head" id="heading_id_6">Summary</h2>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">GitHub Copilot Chat is an innovative tool that brings together the comprehensive language understanding of ChatGPT and the handy features of Copilot. It’s a noteworthy development in the realm of programming assistance, particularly for providing detailed and contextually relevant suggestions in real time, fostering a more efficient coding experience.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The Mediator design pattern is a distinct behavioral pattern that facilitates a high level of decoupling between objects, thus enhancing the modularity of your code. By encompassing the interactions between objects in a mediator object, objects can communicate indirectly, which reduces dependencies and promotes code reusability and ease of modification.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Apache Kafka is a robust, distributed streaming platform engineered for creating real-time data pipelines and streaming applications. It can effectively handle data streams from a multitude of sources and transmit them to various consumers, making it an ideal solution for use cases that require handling substantial volumes of real-time or near-real-time data. It’s important to remember that Kafka is optimized for append-only, immutable data and not for use cases that need record updates or deletions, or complex querying.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Apache Spark is a high-performance, distributed data processing engine renowned for its speed, user-friendliness, and advanced analytics capabilities. It’s highly suitable for scenarios necessitating real-time data processing or for operations on enormous datasets. However, for simpler tasks such as basic analytics and straightforward aggregations, a traditional relational database may be a more appropriate choice.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Generative AI, despite its rapid evolution, is not infallible. It’s crucial to meticulously review all generated output to ensure that it aligns with your specific requirements and quality standards. Generative AI is not a substitute for deep domain knowledge or coding expertise, but it significantly enhances productivity by providing valuable insights and reducing the time spent on routine tasks.<a id="marker-111"/></p>
    </li>
  </ul>
</body></html>