<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 5. Testing with Copilot"><div class="chapter" id="ch05">
<h1><span class="label">Chapter 5. </span>Testing with Copilot</h1>

<p><a contenteditable="false" data-primary="GitHub Copilot" data-secondary="testing with" data-type="indexterm" id="xi_GitHubCopilottestingwith544"/><a contenteditable="false" data-primary="testing" data-type="indexterm" id="xi_testing544"/>Now that you understand how Copilot works and how to interact with it through the editor and chat interfaces, we can move on to other ways it increases productivity. Copilot simplifies routine tasks that can consume a lot of time and resources. Automating such work allows you to devote your cycles, thinking, and focus to the more complex tasks needed to create software.</p>

<p>This chapter focuses on one particular capability: using Copilot to generate tests. In the following sections, you’ll see how Copilot can do the following:</p>

<ul>
	<li>Provide guidance on testing</li>
	<li>Create standard test cases for unit testing and integration testing</li>
	<li>Build out edge cases</li>
	<li>Utilize custom testing instructions</li>
	<li>Write tests using the framework of your choice</li>
	<li>Help implement best practices, like <a href="https://oreil.ly/-nm1N">test-driven development</a></li>
	<li>Use Copilot’s Agent mode to help drive test creation</li>
</ul>

<div data-type="warning" epub:type="warning">
<h1>Generative AI and Testing</h1>

<p><a contenteditable="false" data-primary="generative AI" data-secondary="testing and" data-type="indexterm" id="id661"/><a contenteditable="false" data-primary="testing" data-secondary="generative AI and" data-type="indexterm" id="id662"/><a contenteditable="false" data-primary="AI (artificial intelligence)" data-secondary="generative" data-type="indexterm" id="id663"/>When generating tests, Copilot’s results may vary significantly in content, suitability, and even accuracy. This usually depends on the amount of context provided, the interface, and the prompt.</p>

<p>Per the nature of generative AI, nothing is guaranteed to be exactly as you want. So, it is important to review the suggested tests to ensure that they are valid and a good fit. If they’re not what you expected, you may need to edit them or refactor your prompt and try again.</p>
</div>

<p>After reading this chapter, you’ll have a solid framework for harnessing this capability. Using that, you’ll be able to leverage Copilot in multiple ways to help ensure that you have the testing coverage you need.</p>

<p>Let’s start by asking Copilot the broadest question about testing: “How do I test my code?”</p>

<section data-type="sect1" data-pdf-bookmark="How Do I Test?"><div class="sect1" id="id50">
<h1>How Do I Test?</h1>

<p><a contenteditable="false" data-primary="testing" data-secondary="process of" data-type="indexterm" id="xi_testingprocessof5334"/>Being able to quickly pivot and learn new programming languages and frameworks is a standard expectation for most software professionals. You’re likely already comfortable with how to write tests for any language you’ve been using regularly. But having to switch or migrate code to a different environment can pose a substantial learning curve. As discussed previously, one of the Copilot features that is helpful here is the ability to ask how to write or translate code in the new language. The same can be done for testing.</p>

<div data-type="note" epub:type="note">
<h1>Have Code Active Before Prompt</h1>

<p>For the prompts and queries we’re covering in this chapter, the best results will usually occur when you have one or more files that are part of your project open and active in the editor. If you don’t have any files active, Copilot may fall back to telling you how to do things in more general terms for your project’s type, rather than giving you specifics for your project’s content.</p>
</div>

<p class="pagebreak-before">We’ll use a project for a Kubernetes Operator written in Go as our example here. Suppose you are new to Go and have little or no idea how to test the code you’ve just produced. You could do research on <a href="https://stackoverflow.com">Stack Overflow</a><a contenteditable="false" data-primary="Stack Overflow" data-type="indexterm" id="id664"/> or any other number of forums or sites, or ask colleagues. But you can also ask Copilot:</p>

<pre data-type="programlisting" translate="no">
How do I add tests to my project? </pre>

<div data-type="tip">
<h1>More Specific Queries</h1>

<p><a contenteditable="false" data-primary="queries and querying" data-secondary="specificity of" data-type="indexterm" id="id665"/>If you need to be more specific about the part of your project that you want to test, you can include the file name in the prompt, use a chat variable, or add items as context as discussed in <a data-type="xref" href="ch03.html#ch03">Chapter 3</a>. Here’s an example:</p>

<pre data-type="programlisting" translate="no">
How do I test the code in #file:prime.py?
</pre>
</div>

<p>Copilot first analyzes the project. Then, since your prompt is general, Copilot tells you how to create an example test for the project. It uses the <em><a contenteditable="false" data-primary="Copilot Chat" data-secondary="participants" data-type="indexterm" id="id666"/><a contenteditable="false" data-primary="participants (chat)" data-secondary="Copilot Chat" data-type="indexterm" id="id667"/><a contenteditable="false" data-primary="chat participants" data-secondary="Copilot Chat" data-type="indexterm" id="id668"/>chat participant </em>and <em>slash command<a contenteditable="false" data-primary="slash commands" data-type="indexterm" id="id669"/></em> default of <code translate="no">@workspace /setupTests</code> to produce the basic info.</p>

<p>Copilot provides a summary of its suggestions along with buttons to make changes and take actions. See <a data-type="xref" href="#initial-suggestions-f">Figure 5-1</a>.</p>

<figure><div id="initial-suggestions-f" class="figure"><img alt="" src="assets/lghc_0501.png" width="1244" height="675"/>
<h6><span class="label">Figure 5-1. </span>Initial suggestions for adding tests</h6>
</div></figure>

<p class="pagebreak-before">In the same response, Copilot also provides an explanation for the suggestions along with a terminal command to run the test (<a data-type="xref" href="#further-explanation-o">Figure 5-2</a>).</p>

<figure><div id="further-explanation-o" class="figure"><img alt="" src="assets/lghc_0502.png" width="1247" height="674"/>
<h6><span class="label">Figure 5-2. </span>Further explanation on adding tests</h6>
</div></figure>

<p>If you click the Apply Changes button<a contenteditable="false" data-primary="Apply Changes button" data-type="indexterm" id="id670"/> (shown in <a data-type="xref" href="#initial-suggestions-f">Figure 5-1</a>), the file will be created for you, as shown in the left part of <a data-type="xref" href="#file-created-via-appl">Figure 5-3</a>. Note that Copilot’s response also includes information on how to run your tests. This is pretty impressive for one general query!</p>

<figure><div id="file-created-via-appl" class="figure"><img alt="" src="assets/lghc_0503.png" width="1661" height="937"/>
<h6><span class="label">Figure 5-3. </span>A file created via the Apply Changes button</h6>
</div></figure>

<p class="pagebreak-before">The intent of <code translate="no">/setupTests<a contenteditable="false" data-primary="/setupTests" data-type="indexterm" id="id671"/></code> is to do macro-level setup of testing code and related changes. At the time of this writing, this feature is still experimental, and you may find its usefulness varies depending on the situation. Likewise, having Copilot default to using the <code translate="no">@workspace<a contenteditable="false" data-primary="@workspace chat participant" data-type="indexterm" id="id672"/></code> chat participant does not always yield the best results. You may see better results not using these defaults when initially generating tests. The result of using the “rerun without” option on the previous query provides a more comprehensive set of steps:</p>

<ol>
	<li>Decide on a testing framework.</li>
	<li>Set up the testing framework.</li>
	<li>Create a test directory.</li>
	<li>Write test files.</li>
	<li>Write test cases.</li>
	<li>Run the tests.</li>
	<li>Automate testing.</li>
</ol>

<p>Included with these is text that describes in more detail what to do along with sample code. <a data-type="xref" href="#example-output-from-u">Figure 5-4</a> shows an excerpt from the results of using “rerun without.”</p>

<figure><div id="example-output-from-u" class="figure"><img alt="" src="assets/lghc_0504.png" width="1240" height="679"/>
<h6><span class="label">Figure 5-4. </span>Example output from using the “rerun without” option</h6>
</div></figure>

<p>Options for working around <code translate="no">/setupTests</code> are discussed in the following sidebar. The same types of options can be utilized for working around other default commands being added.</p>

<aside data-type="sidebar" epub:type="sidebar" class="pagebreak-before less_space"><div class="sidebar" id="id673">
<h1>Bypassing Default Chat Participants and Default Slash Commands</h1>

<p><a contenteditable="false" data-primary="Copilot Chat" data-secondary="bypassing default participants" data-type="indexterm" id="xi_CopilotChatbypassingdefaultparticipants5994"/><a contenteditable="false" data-primary="participants (chat)" data-secondary="Copilot Chat" data-type="indexterm" id="xi_participantschatCopilotChat5994"/><a contenteditable="false" data-primary="slash commands" data-type="indexterm" id="xi_slashcommandsbypassing5994"/><a contenteditable="false" data-primary="chat participants" data-secondary="Copilot Chat" data-type="indexterm" id="xi_chatparticipantsCopilotChat5994"/>To avoid prompts running with default chat participants and default slash commands, you have several options.</p>

<p>The first is simply to remove them from the prompt when they are pre-populated.</p>

<p>The second is letting the operation run with the defaults and then selecting “rerun without” at the top of the chat output. This executes the prompt again but <em>without</em> the defaults. (<a data-type="xref" href="#initial-suggestions-f">Figure 5-1</a> shows the “rerun without” link at the end of the Workspace line.) <a data-type="xref" href="#output-after-running">Figure 5-5</a> shows an example of the output after using “rerun without.”</p>

<figure><div id="output-after-running" class="figure"><img alt="" src="assets/lghc_0505.png" width="1137" height="666"/>
<h6><span class="label">Figure 5-5. </span>Output after using the “rerun without” link</h6>
</div></figure>

<p>Another way to accomplish this with one run is to choose a different option when you submit text for Copilot. The submit button that looks like an arrowhead in the dialog has a smaller arrow next to it where you can specify different ways to submit the prompt or query. Several of these options (shown in <a data-type="xref" href="#send-options-for-subm">Figure 5-6</a>) were discussed in <a data-type="xref" href="ch03.html#ch03">Chapter 3</a>. Typically, either Send or Send to @workspace are good choices for more relevant responses.</p>

<figure><div id="send-options-for-subm" class="figure"><img alt="" src="assets/lghc_0506.png" width="736" height="282"/>
<h6><span class="label">Figure 5-6. </span>Send options for submitting a query or prompt</h6>
</div></figure>

<p>Finally, <a contenteditable="false" data-startref="xi_CopilotChatbypassingdefaultparticipants5994" data-type="indexterm" id="id674"/><a contenteditable="false" data-startref="xi_participantschatCopilotChat5994" data-type="indexterm" id="id675"/><a contenteditable="false" data-startref="xi_slashcommandsbypassing5994" data-type="indexterm" id="id676"/><a contenteditable="false" data-startref="xi_chatparticipantsCopilotChat5994" data-type="indexterm" id="id677"/>you can disable <code translate="no">/setupTests</code> through the VS Code settings. Just open the Settings section and search for <code><strong>setupTests</strong></code> (<a data-type="xref" href="#setting-to-enabledis">Figure 5-7</a>).</p>

<figure><div id="setting-to-enabledis" class="figure"><img alt="" src="assets/lghc_0507.png" width="847" height="391"/>
<h6><span class="label">Figure 5-7. </span>The setting to enable/disable <code translate="no">setupTests</code></h6>
</div></figure>
</div></aside>

<p>For an additional example, let’s tackle something less familiar to most: testing<a contenteditable="false" data-primary="testing" data-secondary="SQL code" data-type="indexterm" id="id678"/><a contenteditable="false" data-primary="SQL (Structured Query Language)" data-secondary="testing SQL code" data-type="indexterm" id="id679"/> SQL code. I have a large demo file of SQL statements that I use for some of my training courses, which creates tables to populate a database for a university curriculum, schedule, faculty, etc.</p>

<p>If I open that file in the editor and ask Copilot how to test its content, Copilot replies as shown in <a data-type="xref" href="#initial-sql-test-resp">Figure 5-8</a>. It generates a step-by-step plan for the testing and then creates a new file that implements the plan (shown on the left). The proposed new file contains SQL commands that can be used for doing 10 tests with a test database.</p>

<p>This file can then be reviewed and saved as your new tests. Copilot uses <code translate="no">/tests</code> to quickly generate this output. However, if you don’t want to use the quick version, you can use any of the strategies we discussed previously for “rerun without” to execute the prompt again without using the shortcut.</p>

<figure><div id="initial-sql-test-resp" class="figure"><img alt="" src="assets/lghc_0508.png" width="1668" height="820"/>
<h6><span class="label">Figure 5-8. </span>Initial SQL test response</h6>
</div></figure>

<p>Again, this is pretty impressive for a simple query against one file. And note that this will work for any language, not just Python.</p>

<div data-type="warning" epub:type="warning">
<h1>Trusting Generated Tests</h1>

<p><a contenteditable="false" data-primary="trust, for generated tests" data-type="indexterm" id="id680"/>Since we’re still early in the discussion about having Copilot generate tests, it’s important to remember to review them just as you would any other generated content. Tests generated by AI may include false positives, duplicate assertions, and incorrect logic based on misinterpreting the code. Fortunately, test code is generally easy to read, which can make it easier to quickly review and verify.</p>
</div>

<p>This section has shown you how to utilize Copilot to create instructions and code for testing when you need to start from scratch. More commonly, you may be coding in a language you already know and just want Copilot to help create the <em>boilerplate</em> code for tasks like unit testing. In the next section, you’ll see several ways to do that<a contenteditable="false" data-primary="" data-startref="xi_testingprocessof5334" data-type="indexterm" id="id681"/>.</p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Creating Unit Tests"><div class="sect1" id="id51">
<h1>Creating Unit Tests</h1>

<p><a contenteditable="false" data-primary="testing" data-secondary="creating unit tests" data-type="indexterm" id="xi_testingcreatingunittests51464"/><a contenteditable="false" data-primary="unit tests, creating" data-type="indexterm" id="xi_unittestscreating51464"/>Given the necessity of creating tests for cases like continuous integration and test-driven development, manually crafting tests can represent a significant portion of your time and workload on a project. With Copilot, you can automate test generation by using several approaches. And you can choose from varying degrees of complexity.</p>

<p>Many times, the tests you generate with Copilot will be unit tests, designed to perform basic testing for a single function or procedure. However, the range and depth of Copilot-generated tests can vary with the prompt and interface used. We’ll explore the various approaches in this section.</p>

<section data-type="sect2" data-pdf-bookmark="Using the /tests Command"><div class="sect2" id="id52">
<h2>Using the /tests Command</h2>

<p><a contenteditable="false" data-primary="/tests command" data-type="indexterm" id="xi_testscommandcreatingunittestsusing51534"/>The easiest way to have Copilot generate tests for you is using the built-in slash command <code translate="no">/tests </code>inline (in the IDE’s editor). This command operates on the code you select or reference in the IDE and then attempts to create basic unit tests appropriate to the code. You can enter this command through any of the chat interfaces available in the IDE.</p>

<p>Assume you’re again working with a simple function to determine whether a number is prime. The implementation doesn’t matter, but here’s one version:</p>

<pre data-type="programlisting" translate="no">
def is_prime(number):
    if number &lt;= 1:
        return False
    for n in range(2, int(number**0.5) + 1):
        if number % n == 0:
            return False
    return True
</pre>

<p><a data-type="xref" href="#invoking-copilot-to-g">Figure 5-9</a> shows an example of using the <code translate="no">/tests</code> command via the inline chat. The most straightforward usage is highlighting the code to be tested and then entering the command in the chat interface.</p>

<figure><div id="invoking-copilot-to-g" class="figure"><img alt="" src="assets/lghc_0509.png" width="1075" height="368"/>
<h6><span class="label">Figure 5-9. </span>Invoking Copilot to generate tests via the <code>/tests</code> command</h6>
</div></figure>

<p>After running that command, Copilot generates assert-based tests for primes, nonprimes, one, zero, large primes, and large nonprimes. As <a data-type="xref" href="#tests-from-inline-cha">Figure 5-10</a> shows, when going through the inline chat, Copilot proposes the changes as a new file in the editor with the corresponding name <em>test_is_prime.py</em>. If you like the suggested tests, you can click the Accept button.</p>

<figure><div id="tests-from-inline-cha" class="figure"><img alt="" src="assets/lghc_0510.png" width="1667" height="828"/>
<h6><span class="label">Figure 5-10. </span>Tests from the inline chat option</h6>
</div></figure>

<p>The same results as running <code translate="no">/tests</code> in the inline chat interface can be achieved by choosing Copilot &gt; Generate Tests from the context menu; see <a data-type="xref" href="#generating-tests-via">Figure 5-11</a>. In fact, Copilot will want to use the <code translate="no">/tests </code>shortcut in most cases.</p>

<figure><div id="generating-tests-via" class="figure"><img alt="" src="assets/lghc_0511.png" width="818" height="483"/>
<h6><span class="label">Figure 5-11. </span>Generating tests via the context menu</h6>
</div></figure>

<p>If you run the <code translate="no">/tests</code> command in the separate chat interface, you’ll likely get similar results, but with more of an explanation in the chat area about the approach Copilot is going to take, the framework it has chosen, and the type of tests it will create.</p>

<p>A new section just above the text entry area shows any files changed. Keep/Undo buttons are available for all files. Proposed individual files are shown by their first line and have their own controls to keep (the checkmark) and undo (the curved left arrow). The control that looks like a document icon with + and - in it is used to show the diff introduced by the proposed changes in the editor. <a data-type="xref" href="#tests-from-tests-com">Figure 5-12</a> shows a portion of the output.</p>

<figure><div id="tests-from-tests-com" class="figure"><img alt="" src="assets/lghc_0512.png" width="1440" height="953"/>
<h6><span class="label">Figure 5-12. </span>Tests from the <code>/tests</code> command in the main chat</h6>
</div></figure>

<div data-type="warning" epub:type="warning">
<h1>Changing Icons and Controls</h1>

<p><a contenteditable="false" data-primary="icons, changing" data-type="indexterm" id="id682"/><a contenteditable="false" data-primary="controls, changing" data-type="indexterm" id="id683"/>Copilot commonly changes the controls used in their various interfaces and moves them around, unfortunately. By the time you are reading this, Copilot has likely changed things again. </p>

<p>In general, the following hold true across releases for proposed edits:</p>

<ul>
	<li>There will be controls to accept/keep and reject/undo changes.</li>
	<li>Checkmarks double as keep controls.</li>
	<li>There will be a control to see the suggested diffs as a file in the editor or apply suggested diffs to a file in the editor.</li>
	<li>The diff controls may or may not set up the file with a reasonable name. You should always make sure to save your files yourself after reviewing and choosing which changes to keep.</li>
</ul>
</div>

<p>These responses cover a basic range of use cases. But what if you want to have additional tests generated and/or more cases covered? You can approach that by supplying more explicit prompts, the topic of the next section<a contenteditable="false" data-primary="" data-startref="xi_testscommandcreatingunittestsusing51534" data-type="indexterm" id="id684"/>.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Creating Tests from Explicit Prompts"><div class="sect2" id="id53">
<h2>Creating Tests from Explicit Prompts</h2>

<p><a contenteditable="false" data-primary="explicit prompts, creating unit tests using" data-type="indexterm" id="xi_explicitpromptscreatingunittestsusing52144"/><a contenteditable="false" data-primary="prompts" data-secondary="creating unit tests using explicit" data-type="indexterm" id="xi_promptscreatingunittestsusingexplicit52144"/>Another way to create tests is by providing prompts through the chat interface. A simple prompt like “Create tests for the selected code” will usually result in Copilot running the <code translate="no">/tests</code> command again (assuming that command is available in your IDE). However, you can supply a more directed prompt to have Copilot suggest tests for other cases. For example, if you’ve incorporated the tests already generated, you can select the current test file and the implementation file as context and then prompt Copilot about any other edge cases with a prompt like this:</p>

<pre data-type="programlisting" translate="no">
are there any other edge cases that should be tested
</pre>

<p>Copilot responds by identifying some additional tests cases, along with explanations for them:</p>

<pre data-type="programlisting" translate="no">
Yes, here are some additional edge cases you might want to test:

1. Smallest prime number: Ensure the function correctly identifies 
2 as a prime number.

2. Even number greater than 2: Test an even number like 10 to 
confirm it is not prime.

3. Very large composite number: Test a large number that is not 
prime, such as 10^6.

4. Floating-point input: Test how the function handles non-integer
 inputs like 3.5.

5. String input: Test how the function handles invalid input types
 like "seven".

6. Negative even number: Test a negative even number like -4 to 
confirm it is not prime.</pre>

<p>Copilot further provides the code that can be added for those test cases:</p>

<pre data-type="programlisting" translate="no">
def test_smallest_prime(self):
    self.assertTrue(is_prime(2)) # 2 is the smallest prime number

def test_even_number_greater_than_two(self):
    self.assertFalse(is_prime(10))&amp;nbsp; # 10 is not a prime number

def test_very_large_composite_number(self):
    self.assertFalse(is_prime(10**6)) # 10^6 is not a prime number

def test_floating_point_input(self):
    with self.assertRaises(TypeError): # Assuming the function
 raises TypeError for non-integers
        is_prime(3.5)

def test_string_input(self):
    with self.assertRaises(TypeError): # Assuming the function 
 raises TypeError for non-integers
        is_prime("seven")

def test_negative_even_number(self):
    self.assertFalse(is_prime(-4)) # Negative even numbers are not
 prime</pre>

<p>This might make you wonder whether you can extend the prompt to cover more use cases. Indeed, you can! For example, you can be more prescriptive, telling Copilot to add test cases for other inputs:</p>

<pre data-type="programlisting" translate="no">
add test cases for other kinds of inputs</pre>

<p>With this prompt, Copilot adds additional test cases for your consideration and review:</p>

<pre data-type="programlisting" translate="no">
def test_float_input(self):
    with self.assertRaises(TypeError):
        is_prime(7.1)

def test_string_input(self):
    with self.assertRaises(TypeError):
        is_prime("7")
</pre>

<p>A key part of getting the most value-add from Copilot is asking it additional questions beyond the scope of what it may generate with default prompts or commands. To use our analogy of Copilot as someone new to the team, you may want or need to prod them to think beyond the usual test cases about other scenarios. The same holds true for prompting Copilot to get more comprehensive results.</p>

<p>If you prefer to create some quick tests inline with your code, a variation on the prompt approach can easily do that. We’ll look at that option next<a contenteditable="false" data-primary="" data-startref="xi_explicitpromptscreatingunittestsusing52144" data-type="indexterm" id="id685"/><a contenteditable="false" data-primary="" data-startref="xi_promptscreatingunittestsusingexplicit52144" data-type="indexterm" id="id686"/>.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Creating Tests from Comments"><div class="sect2" id="id54">
<h2>Creating Tests from Comments</h2>

<p><a contenteditable="false" data-primary="comments" data-secondary="creating unit tests using" data-type="indexterm" id="xi_commentscreatingunittestsusing52944"/>Sometimes you may not want (or need) to engage the chat feature to create your tests. You may be actively working in your editor <em>in the flow</em> and want to do the equivalent of telling Copilot, “Insert some simple, quick tests here.” That can be done by creating a comment as a directive to Copilot. Then you simply need to accept the suggestions for individual tests or testing functions that it returns. The process follows the same interactive flow for acceptance, editing, etc., as discussed in <a data-type="xref" href="ch02.html#ch02">Chapter 2</a>.</p>

<p>The advantage of this method is that you can initiate it from a simple (or complex) comment line within the code. The disadvantage is that you may have to repeatedly step through accepting parts of suggestions or get generic suggestions that aren’t as useful.</p>

<p>Here’s a simple example of a directive comment placed inline after the code for the <code translate="no">is_prime </code>function:</p>

<pre data-type="programlisting" translate="no">
# Generate 10 simple test cases for the is_prime function</pre>

<p>In response, Copilot suggests an inline function with a set of simple asserts, as shown in <a data-type="xref" href="#simple-set-of-assert">Figure 5-13</a>. All the lines from 10–23 were automatically generated by Copilot in response to the comment on line 9.</p>

<figure><div id="simple-set-of-assert" class="figure"><img alt="" src="assets/lghc_0513.png" width="1357" height="743"/>
<h6><span class="label">Figure 5-13. </span>A simple set of assert test cases generated from the comment</h6>
</div></figure>

<p>Alternatively, you could use a more generic comment such as this one:</p>

<pre data-type="programlisting" translate="no">
# Create tests for the code above
</pre>

<p>In this case, Copilot generates similar test cases to the other methods, but in the same file. <a data-type="xref" href="#another-example-of-te">Figure 5-14</a> shows an excerpt of the generated tests. The only line that was typed in was the first comment. All the other lines were generated and accepted based on that one.</p>

<figure><div id="another-example-of-te" class="figure"><img alt="" src="assets/lghc_0514.png" width="1215" height="739"/>
<h6><span class="label">Figure 5-14. </span>Another example of tests generated from a comment</h6>
</div></figure>

<p>Finally, in this section, let’s look at a different way of using Copilot to help us test: validating inputs to functions inline<a contenteditable="false" data-primary="" data-startref="xi_commentscreatingunittestsusing52944" data-type="indexterm" id="id687"/>.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Validating Inputs"><div class="sect2" id="id55">
<h2>Validating Inputs</h2>

<p><a contenteditable="false" data-primary="validation" data-secondary="of inputs during testing" data-type="indexterm" id="xi_validationofinputsduringtesting53274"/><a contenteditable="false" data-primary="inputs, validating during testing" data-type="indexterm" id="xi_inputsvalidatingduringtesting53274"/>As another angle on testing, Copilot can also assist with generating code to check that inputs going into a function are valid. With our example function, we can tell Copilot to do the following:</p>

<pre data-type="programlisting" translate="no">
generate asserts to ensure that the inputs to the function are valid
</pre>

<p>From here, Copilot responds and suggests assertions, as requested, to validate the inputs:</p>

<pre data-type="programlisting" translate="no">
To ensure that the function inputs are valid, we can add assertions
at the start of the function. We want to make sure that the input 
is an integer and that it is not negative. Here's how you can do it:


def is_prime(num):
    assert isinstance(num, int), "Input must be an integer"
    assert num &gt;= 0, "Input must be a non-negative integer"

    if num &lt; 2:
        return False
    for i in range(2, int(num ** 0.5) + 1):
        if num % i == 0:
            return False
    return True
</pre>

<p>We can also be less specific about using asserts and ask Copilot to generate checks:</p>

<pre data-type="programlisting" translate="no">
generate checks to ensure that the inputs to the function are valid</pre>

<p>This allows Copilot to generate code to validate the inputs, but with a more standard coding mechanism to surface any issues:</p>

<pre data-type="programlisting" translate="no">
To ensure that the inputs to the `is_prime` function are valid, you
can add checks at the start of the function.
 Here's how you can do it:

```python
def is_prime(num):
    if not isinstance(num, int):
        raise TypeError("Input must be an integer")
    if num &lt; 0:
        raise ValueError("Input must be a non-negative integer")

    if num &lt; 2:
        return False
    for i in range(2, int(num ** 0.5) + 1):
        if num % i == 0:
            return False
    return True
```

In this code, the `if` statements will raise a `TypeError` if the
 input is not an integer or a `ValueError` if it's a negative 
number.
 This way, you can ensure that the inputs to the function are valid.

</pre>

<p>Each approach may be useful in different cases depending on how simple you want the results to be versus the need for broader coverage. Remember that since you are interacting with an AI, prompting and setting the context are the keys to getting explicit results (versus generic results or ones rendered quickly inline). Depending on where you are at with your overall progress on a project and experience with the languages and frameworks you are using, you may find that a combination of these approaches works well.</p>

<p>Before we leave this section on creating unit tests, here’s a generic prompt template for creating a set of unit tests (adapted from the <a href="https://oreil.ly/NOS8j">Copilot documentation</a>):</p>

<pre data-type="programlisting" translate="no">
Develop a comprehensive suite of unit tests for the 
&lt;method/function&gt; in &lt;language/framework&gt;. Write multiple test 
methods that cover a wide range of scenarios, including edge cases,
exception handling, and data validation."
</pre>

<p>The words enclosed by &lt; and &gt; are meant to be replaced with specific terms by you. This template has several key elements:</p>

<dl>
	<dt>Scope</dt>
	<dd>
	<p>It explicitly asks for a comprehensive suite of unit tests, so the AI generates more than just basic tests and covers a wide variety of potential use cases.</p>
	</dd>
	<dt>Scenarios</dt>
	<dd>
	<p>It specifies that the tests should include:</p>

	<ul>
		<li>
		<p>Edge cases to test for boundary conditions</p>
		</li>
		<li>
		<p>Exception handling for errors that should be raised when invalid actions occur</p>
		</li>
		<li>
		<p>Data validation to ensure that inputs are correctly validated and handled by the methods</p>
		</li>
	</ul>
	</dd>
</dl>

<p>Now that we’ve covered the unit-test approaches, let’s look at how we can leverage Copilot to help us write integration tests<a contenteditable="false" data-primary="" data-startref="xi_testingcreatingunittests51464" data-type="indexterm" id="id688"/><a contenteditable="false" data-primary="" data-startref="xi_unittestscreating51464" data-type="indexterm" id="id689"/><a contenteditable="false" data-primary="" data-startref="xi_validationofinputsduringtesting53274" data-type="indexterm" id="id690"/><a contenteditable="false" data-primary="" data-startref="xi_inputsvalidatingduringtesting53274" data-type="indexterm" id="id691"/>.</p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Creating Integration Tests"><div class="sect1" id="id56">
<h1>Creating Integration Tests</h1>

<p><a contenteditable="false" data-primary="testing" data-secondary="creating integration tests" data-type="indexterm" id="xi_testingcreatingintegrationtests54304"/><a contenteditable="false" data-primary="integration tests, creating" data-type="indexterm" id="xi_integrationtestscreating54304"/>You can use Copilot to create integration tests by telling it to focus on the way multiple components interact rather than creating tests for isolated units of code. To do this, you need to write your prompts with clear instructions that emphasize the collaboration between services, modules, and/or external systems.</p>

<p>The keys to having Copilot create successful integration tests are as follows:</p>

<dl>
	<dt><a contenteditable="false" data-primary="prompts" data-secondary="specificity of" data-type="indexterm" id="id692"/>Craft specific prompts</dt>
	<dd>Ask Copilot to write integration tests for interaction between components rather than just generating tests for code.</dd>
	<dt><a contenteditable="false" data-primary="mocks" data-type="indexterm" id="id693"/>Use mocks for external dependencies<a contenteditable="false" data-primary="external dependencies" data-type="indexterm" id="id694"/></dt>
	<dd>Integration tests frequently need to simulate an external service such as a database or an API. You can instruct Copilot to use mocks so tests can verify the functionality without having to have real implementations.</dd>
	<dt>Request coverage of interactions</dt>
	<dd>Ask Copilot to verify that methods are called, data flows as expected between components, or that expected side effects occur.</dd>
</dl>

<p>A generic template for a prompt for integration tests could look something like this:</p>

<pre data-type="programlisting" translate="no">
Write integration tests for the &lt;method/function&gt; in the &lt;class&gt;.
Use mocks to simulate the &lt;external service&gt; and verify that it 
works correctly when &lt;condition&gt;.</pre>

<p>(Again, the terms in &lt;&gt; are meant to be replaced for your specific use case.)  As an example, suppose that we have an Express application with two modules:</p>

<dl>
	<dt><code>UserService</code></dt>
	<dd>
	<p>Handles user data</p>
	</dd>
	<dt><code>AuthController</code></dt>
	<dd>Handles login requests and delegates to <code>UserService</code></dd>
</dl>

<p>We want to write integration tests to ensure that the <code>AuthController</code> correctly interacts with <code>UserService</code> and the HTTP layer. Here is a prompt to do this:</p>

<pre data-type="programlisting" translate="no">
Write integration tests for the login endpoint in the 
AuthController. Mock the UserService to simulate user lookup and 
password verification, and verify that the controller returns the 
correct HTTP status codes and messages
</pre>

<p>Here’s the response that Copilot generates based on that prompt:</p>

<pre data-type="programlisting" translate="no">
Here is an example of integration tests for the <em>login</em> endpoint in 
the <em>AuthController</em>. These tests use Jest and mock the <em>UserService</em> to
 simulate user lookup and password verification.

const request = require('supertest');
const express = require('express');
const AuthController = require('../controllers/AuthController');
const UserService = require('../services/UserService');

jest.mock('../services/UserService'); // Mock the UserService

const app = express();
app.use(express.json());
app.use('/auth', AuthController);

describe('AuthController - Login Endpoint', () =&gt; {
  afterEach(() =&gt; {
    jest.clearAllMocks(); // Clear mocks after each test
  });

  it('should return 200 and success message for valid credentials',
 async () =&gt; {
    // Mock UserService.authenticate to return a valid user
    UserService.authenticate.mockResolvedValue({
      id: 1,
      username: 'user1',
      password: 'password1',
    });

    const response = await request(app)
      .post('/auth/login')
      .send({ username: 'user1', password: 'password1' });

    expect(response.status).toBe(200);
    expect(response.body).toEqual({
      message: 'Login successful',
      user: { id: 1, username: 'user1', password: 'password1' },
    });
    expect(UserService.authenticate).toHaveBeenCalledWith('user1', 
'password1');
  });

  it('should return 401 and error message for invalid credentials',
 async () =&gt; {
    // Mock UserService.authenticate to return null for invalid
 credentials
    UserService.authenticate.mockResolvedValue(null);

    const response = await request(app)
      .post('/auth/login')
      .send({ username: 'user1', password: 'wrongpassword' });

    expect(response.status).toBe(401);
    expect(response.body).toEqual({
      message: 'Invalid credentials',
    });
    expect(UserService.authenticate).toHaveBeenCalledWith('user1', 
'wrongpassword');
  });

  it('should return 400 if username or password is missing', 
async () =&gt; {
    const response = await request(app)
      .post('/auth/login')
      .send({ username: 'user1' }); // Missing password

    expect(response.status).toBe(400);
    expect(response.body).toEqual({
      message: 'Username and password are required',
    });
  });
});

<strong>Key Points:</strong>
<strong>1. Mocking <em>UserService</em>:</strong>
   - `jest.mock('../services/UserService')` is used to mock the 
`UserService` module.
   - `UserService.authenticate` is mocked to simulate different 
scenarios (valid user, invalid credentials).

<strong>2. Test Cases:</strong>
   - **Valid Credentials**: Ensures the endpoint returns a 200 
status and the correct success message.
   - **Invalid Credentials**: Ensures the endpoint returns a 401 
status and an error message.
   - **Missing Fields**: Ensures the endpoint returns a 400 status 
if required fields are missing.

<strong>3. Supertest:</strong>
   - `supertest` is used to make HTTP requests to the Express app 
and validate the responses.

<strong>4. Setup:</strong>
   - The `AuthController` is mounted on the `/auth` route in the 
test app.

Run the tests using:

npx jest

</pre>

<p>You may need to do additional tweaking on the integration test code to ensure everything works as intended, but this gives you a solid starting point to work from.</p>

<p>Regardless of the kinds of tests you are creating through Copilot, you may want to have them tailored for your project or per testing guidelines you must adhere to. In the next section, we’ll look at how to set up custom testing instructions for Copilot<a contenteditable="false" data-primary="" data-startref="xi_testingcreatingintegrationtests54304" data-type="indexterm" id="id695"/><a contenteditable="false" data-primary="" data-startref="xi_integrationtestscreating54304" data-type="indexterm" id="id696"/>.</p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Defining Custom Testing Instructions"><div class="sect1" id="id57">
<h1>Defining Custom Testing Instructions</h1>

<p><a contenteditable="false" data-primary="testing" data-secondary="defining custom instructions" data-type="indexterm" id="xi_testingdefiningcustominstructions55834"/>You can customize how Copilot generates tests for your project by providing custom test-generation instructions. This ensures that Copilot produces tests that align with your preferred frameworks, coding standards, and workflows.</p>

<p>Test-generation instructions can be set up at two levels: user level and workspace level.</p>

<p>For a particular workspace, you can add instructions directly to your project’s <em>.vscode/settings.json</em> file. If these are persisted, they customize Copilot’s behavior for everyone working with that project.</p>

<p>To create instructions that apply across all projects you work with, you can instead add instructions to your <em>global</em> VS Code settings.</p>

<p>At either of these levels, you can also create a separate Markdown file with more detailed instructions and then point your settings to the file. This is useful for complex or evolving standards that you want to reuse across multiple projects.</p>

<p>When adding custom test instructions in your settings file, they should be placed under the key <code translate="no">github.copilot.chat.testGeneration.instructions</code>. Here’s an example with both specific instructions spelled out and a referenced file:</p>

<pre data-type="programlisting" translate="no">
"github.copilot.chat.testGeneration.instructions": [
  {
    "text": "Prefer Mocha and Chai for testing Node.js modules."
  },
  {
    "text": "Use Pytest for all Python test cases."
  },
  {
    "file": "test-guidelines.md" // import instructions from file
  }
]
</pre>

<p>The content in the file <em>test-guidelines.md</em> could be as follows:</p>

<pre data-type="programlisting" translate="no">
Ensure all test functions have descriptive names.
Mock external API calls in integration tests.
</pre>

<p>You may have to iterate to get the best form for the instructions, but once they’re set up, Copilot should honor them for typical operations like <code translate="no">@workspace /tests</code>.</p>

<div data-type="warning" epub:type="warning">
<h1>Custom Test Instructions Are Experimental</h1>

<p>As of this writing, the custom test instructions feature is still labeled as <em>experimental.<a contenteditable="false" data-primary="experimental features" data-type="indexterm" id="id697"/></em></p>
</div>

<p>For the final part of this chapter, let’s look at how Copilot can help with using frameworks and creating tests <em>before</em> writing implementation code. This is useful for techniques like test-driven development<a contenteditable="false" data-primary="" data-startref="xi_testingdefiningcustominstructions55834" data-type="indexterm" id="id698"/>.</p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Testing Before the Coding and Leveraging Frameworks"><div class="sect1" id="id58">
<h1>Testing Before the Coding and Leveraging Frameworks</h1>

<p><em><a contenteditable="false" data-primary="testing" data-secondary="test-driven development (TDD)" data-type="indexterm" id="xi_testingtestdrivendevelopmentTDD56308"/><a contenteditable="false" data-primary="TDD (test-driven development)" data-type="indexterm" id="xi_TDDtestdrivendevelopment56308"/><a contenteditable="false" data-primary="frameworks, leveraging" data-type="indexterm" id="xi_frameworksleveraging56308"/>Test-driven development</em> (<em>TDD</em>) has already been mentioned in this chapter. If you’re unfamiliar with the term, it’s an approach to software development that emphasizes writing test cases for code before writing the actual code itself. TDD is considered a best practice or requirement in many coding projects and groups.</p>

<div data-type="note" epub:type="note">
<h1>More on TDD</h1>

<p>If you are new to TDD or interested in learning more about the practice, the web has many references. A relatively quick but informative read can be found at the <a href="https://oreil.ly/VaMiK">testdriven.io</a><a contenteditable="false" data-primary="TestDrivenio" data-type="indexterm" id="id699"/> website.</p>
</div>

<p>You can use Copilot to create test cases for TDD and then implement the code to be tested. Consider a simple example of creating a test class and tests for code that will help manage student records at a university. We’ll use <a href="https://site.mockito.org">Mockito</a><a contenteditable="false" data-primary="Mockito" data-type="indexterm" id="id700"/> in our testing <span class="keep-together">framework</span>.</p>

<div data-type="note" epub:type="note">
<h1>About Mockito</h1>

<p>Mockito specializes in helping Java applications manage dependencies and simulating external interactions from other components without requiring the implementation or presence of the components themselves—i.e., <em>mocking</em>. Previously in the chapter, we showed how Copilot can incorporate mocking for generating integration tests.</p>
</div>

<p>To start, suppose we begin with an empty project called <em>students</em>, and we have Copilot create a <a href="https://oreil.ly/Qx29p"><em>pom.xml</em></a> file for a Java project with desired versions of dependencies. Here’s a prompt for that:</p>

<pre data-type="programlisting" translate="no">
add a pom.xml file with a mockito dependency version 3.3.3, and 
compiler source and target version 1.8
</pre>

<p>As shown in <a data-type="xref" href="#pomxml-creation">Figure 5-15</a>, this prompt triggers Copilot to try to understand our intent and then run the <code>/new</code> shortcut command to set up a structure if needed. After that, it figures out the code for the <em>pom.xml</em> file we asked for and gives a link to see it and a button shortcut to create it.</p>

<figure><div id="pomxml-creation" class="figure"><img alt="" src="assets/lghc_0515.png" width="1539" height="914"/>
<h6><span class="label">Figure 5-15. </span><span class="plain">pom.xml</span> creation</h6>
</div></figure>

<p>To build out our testing code, let’s create an appropriate test class and initial set of tests. We’ll do this in the separate Chat interface since we can expect a significant amount of output and we may want to put it in a separate file.</p>

<p>In the prompt, we tell Copilot to reference the content of the <em>pom.xml</em> file to make sure it knows to pick up the Mockito dependency:</p>

<pre data-type="programlisting" translate="no">
Referencing #file:pom.xml, create a StudentTest class for students
 enrolled at a university and add tests</pre>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id701">
<h1>Ensuring That Files Are Picked Up for Context</h1>

<p><a contenteditable="false" data-primary="explicit references" data-type="indexterm" id="id702"/>In most cases, we probably would not need to specifically add the <code translate="no">#file:pom.xml</code> reference. Assuming the file has been saved and added to the workspace, Copilot would default to using the <code translate="no">@workspace<a contenteditable="false" data-primary="@workspace chat participant" data-type="indexterm" id="id703"/> </code>agent and should pick up the Mockito context from there. In addition, the automatic index it creates would also know about it. As a best practice, though, always make sure any files you create in the editor are saved and added to the workspace before expecting them to be automatically pulled in for Copilot’s context.</p>

<p><a data-type="xref" href="#using-explicit-refere">Figure 5-16</a> shows an example of Copilot using the specific reference to <em>pom.xml</em> for a similarly targeted prompt.</p>

<figure><div id="using-explicit-refere" class="figure"><img alt="" src="assets/lghc_0516.png" width="926" height="483"/>
<h6><span class="label">Figure 5-16. </span>Using an explicit reference</h6>
</div></figure>
</div></aside>

<p>This then makes our initial query something like the following:</p>

<pre data-type="programlisting" translate="no">
Referencing #file:pom.xml, create a StudentTest class for students
 enrolled at a university and add tests 
</pre>

<p>Copilot returns this in response:</p>

<pre data-type="programlisting" translate="no">
Here is a StudentTest class with some basic unit tests using 
Mockito for testing students enrolled at a university:


package com.example;

import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;
import org.mockito.Mockito;

import java.util.ArrayList;
import java.util.List;

import static org.junit.jupiter.api.Assertions.assertEquals;
import static org.mockito.Mockito.*;

class StudentTest {

    private University university;
    private Student student;

    @BeforeEach
    void setUp() {
        university = Mockito.mock(University.class);
        student = new Student("John Doe", "12345");
    }

    @Test
    void testEnrollStudent() {
        // Arrange
        List&lt;Student&gt; enrolledStudents = new ArrayList&lt;&gt;();
        when(university.getEnrolledStudents()).thenReturn
(enrolledStudents);

        // Act
        university.enrollStudent(student);
        enrolledStudents.add(student);

        // Assert
        verify(university, times(1)).enrollStudent(student);
        assertEquals(1, university.getEnrolledStudents().size());
        assertEquals(student, university.getEnrolledStudents().
get(0));
    }

    @Test
    void testGetStudentDetails() {
        // Arrange
        when(university.getStudentDetails("12345")).thenReturn
(student);

        // Act
        Student result = university.getStudentDetails("12345");

        // Assert
        verify(university, times(1)).getStudentDetails("12345");
        assertEquals("John Doe", result.getName());
        assertEquals("12345", result.getId());
    }
}
```

Explanation:
1. **Dependencies**: 
The `mockito-core` dependency in your pom.xml 
is used for mocking the `University` class.
2. **Test Setup**: 
The `@BeforeEach` method initializes the mock 
`University` object and a sample `Student` object.
3. **Test Cases**:
   - `testEnrollStudent`: Verifies that a student is enrolled in the
 university and checks the list of enrolled students.
   - `testGetStudentDetails`: Verifies that the correct student 
details are returned when queried by ID.

Make sure you have the `Student` and `University` classes 
implemented in your project for these tests to work. Let me know if
 you need help creating those classes!
</pre>

<p>This is helpful and a good set of suggested starting tests. We can take this and save it if we want. However, if what we really want is just a simple test case for a <code translate="no">Student </code>class, this may be overkill as it adds items like enrollments. To avoid unneeded items, restructure our prompt to be more specific about what we want. Here’s one option:</p>

<pre data-type="programlisting" translate="no">
Referencing #file:pom.xml, create only a StudentTest class for a
 student enrolled at a university. A student will have personal
 attributes such as a first and last name, a phone number, an 
address, and a contact email.
</pre>

<p>This includes the word<em> only </em>to ensure that Copilot creates only the test class right now. Otherwise, it may go ahead and create the implementation class as well. Here’s the revised testing code:</p>

<pre data-type="programlisting" translate="no">
Here is the `StudentTest` class for testing a student with personal
 attributes:

package com.example;

import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;

import static org.junit.jupiter.api.Assertions.*;

class StudentTest {

    private Student student;

    @BeforeEach
    void setUp() {
        student = new Student("John", "Doe", "123-456-7890", 
"123 Main St", "john.doe@example.com");
    }

    @Test
    void testStudentAttributes() {
        // Verify that the student's attributes are correctly set
        assertEquals("John", student.getFirstName());
        assertEquals("Doe", student.getLastName());
        assertEquals("123-456-7890", student.getPhoneNumber());
        assertEquals("123 Main St", student.getAddress());
        assertEquals("john.doe@example.com", student.getEmail());
    }

    @Test
    void testUpdatePhoneNumber() {
        // Update the phone number and verify the change
        student.setPhoneNumber("987-654-3210");
        assertEquals("987-654-3210", student.getPhoneNumber());
    }

    @Test
    void testUpdateAddress() {
        // Update the address and verify the change
        student.setAddress("456 Elm St");
        assertEquals("456 Elm St", student.getAddress());
    }

    @Test
    void testUpdateEmail() {
        // Update the email and verify the change
        student.setEmail("new.email@example.com");
        assertEquals("new.email@example.com", student.getEmail());
    }
}

### Explanation:
1. **Attributes Tested**:
   - First name
   - Last name
   - Phone number
   - Address
   - Email
2. **Test Cases**:
   - `testStudentAttributes`: Ensures the attributes are initialized
 correctly.
   - `testUpdatePhoneNumber`, `testUpdateAddress`, `testUpdateEmail`
: Verify that updates to attributes work as expected.

This test assumes the existence of a `Student` class with 
appropriate getters and setters. Let me know if you need help
 creating the `Student` class!
</pre>

<p>This looks more like the starting point we wanted, so we save this in our workspace as <em>src/test/java/com/example/StudentTest.java</em>. If we execute the test, it fails since an implementation of the <code translate="no">Student</code> class doesn’t exist yet.</p>

<p>Following the TDD methodology, we should next create the minimum code to make this test pass. Since we have Copilot available, we’ll have it take a pass at that and then run the tests. We could do this in individual steps, or we can leverage Copilot’s Agent mode as discussed in <a data-type="xref" href="ch04.html#ch04">Chapter 4</a>. Let’s let Agent mode help us out.</p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Leveraging Agent Mode for Testing Workflows"><div class="sect1" id="id59">
<h1>Leveraging Agent Mode for Testing Workflows</h1>

<p><a contenteditable="false" data-primary="Agent mode" data-secondary="using for testing workflows" data-type="indexterm" id="xi_Agentmodeusingfortestingworkflows58584"/><a contenteditable="false" data-primary="workflows" data-secondary="testing using Agent mode" data-type="indexterm" id="xi_workflowstestingusingAgentmode58584"/>To continue working on our TDD workflow, first, we’ll switch into the mode by using the chat interface (<a data-type="xref" href="#switching-to-agent-mo">Figure 5-17</a>).</p>

<figure><div id="switching-to-agent-mo" class="figure"><img alt="" src="assets/lghc_0517.png" width="865" height="255"/>
<h6><span class="label">Figure 5-17. </span>Switching to Agent mode</h6>
</div></figure>

<p>In Agent mode, we’ll enter a prompt for the pieces we want Copilot to do:</p>

<pre data-type="programlisting" translate="no">
Install any needed dependencies for testing with mvn and then run
 tests</pre>

<p>While not necessary, if you have this code open in the editor, you can specify to use that as the key reference:</p>

<pre data-type="programlisting" translate="no">
Referencing #editor, create a student class. 
</pre>

<p>Copilot proceeds to suggest the <code translate="no">brew install maven</code> command to install<a contenteditable="false" data-primary="installing" data-secondary="Maven" data-type="indexterm" id="id704"/> <a contenteditable="false" data-primary="Maven" data-type="indexterm" id="id705"/>Maven and runs it in a terminal after we tell it to proceed. From here, it can proceed to run <code translate="no">mvn test</code>. Copilot encounters some errors but figures out what is needed to resolve them:</p>

<pre data-type="programlisting" translate="no">
mvn test

The Maven test compilation failed due to the following issues:

1. JUnit Dependencies Missing: 
The org.junit.jupiter.api package is not found, indicating that the
 JUnit 5 (Jupiter) dependency is missing in the pom.xml.

2. Student Class Not Found: The Student class is not being
 recognized, which could be due to incorrect package structure or
 missing compilation.

3. Annotations Not Recognized: Annotations like @BeforeEach and 
@Test are not recognized, which is also related to the missing 
JUnit dependency.

I will now:

Add the required JUnit 5 dependency to the pom.xml.
Ensure the Student class is correctly compiled and accessible.
Re-run the tests after fixing these issues

</pre>

<p>Notice that the Copilot agent has already planned what it should try next to fix the issue (under the <code>I will now</code> section). After we tell Copilot to proceed, it first updates the <em>pom.xml</em> file with the missing JUnit dependency and then tries to run the tests again. They fail because the <code>Student</code> implementation class isn’t implemented yet. The output from Copilot for this run is shown here:</p>

<pre data-type="programlisting" translate="no">
&lt;dependencies&gt;
    &lt;!-- ...existing dependencies... --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.junit.jupiter&lt;/groupId&gt;
        &lt;artifactId&gt;junit-jupiter&lt;/artifactId&gt;
        &lt;version&gt;5.10.0&lt;/version&gt;
        &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;



Made changes.

The Maven test run failed because the `Student` class could not be
 found during the test execution. This indicates that the `Student` 
class might not be compiled or is not in the correct package
 structure.

I will verify the `Student` class's location and ensure it is 
properly compiled and accessible. Then, I will re-run the tests.
</pre>

<p>Once again, the agent has figured out what it needs to try next. After we review and tell Copilot to proceed, it generates a new <em>Student.java</em> file with the correct content and in the correct location, and then proceeds to run the tests again. This time, they succeed because the agent has completed all needed changes. <a data-type="xref" href="#successful-completion">Figure 5-18</a> shows the success via the following:</p>

<ul class="pagebreak-before">
	<li>The new <em>Student.java</em> file in the upper left with its new content waiting to be approved</li>
	<li>The successful build/test run in the lower left</li>
	<li>The agent’s assessment that the tests all passed, the <code translate="no">Student</code> class is implemented, and the project is working as expected, in the upper right</li>
	<li>The “files changed” area in the lower right showing that updates were made by the agent to the <em>pom.xml </em>file and the <em>Student.java</em> file to get everything working</li>
</ul>

<figure><div id="successful-completion" class="figure"><img alt="" src="assets/lghc_0518.png" width="1638" height="941"/>
<h6><span class="label">Figure 5-18. </span>Successful completion via the agent</h6>
</div></figure>

<p>Certainly, we could have done this without the agent via several passes and individual actions. But, where we can leverage Copilot’s agentic abilities, we can save ourselves some time and effort.</p>

<p>From here, we can review the changes, save the updated files into the workspace, and add other functionality if we need it. Similar multistep processes for testing workflows can be done with this step-by-step approach, whether using the agent or not.</p>

<p>As always, the more context and direction you can give Copilot through the prompt, workspace files, editor, etc., the more likely it will produce good content and suggestions for you to incorporate. If, however, you don’t get what you expect, don’t be afraid to iterate on your prompt/query until you get the results you want<a contenteditable="false" data-primary="" data-startref="xi_testingtestdrivendevelopmentTDD56308" data-type="indexterm" id="id706"/><a contenteditable="false" data-primary="" data-startref="xi_TDDtestdrivendevelopment56308" data-type="indexterm" id="id707"/><a contenteditable="false" data-primary="" data-startref="xi_frameworksleveraging56308" data-type="indexterm" id="id708"/><a contenteditable="false" data-primary="" data-startref="xi_Agentmodeusingfortestingworkflows58584" data-type="indexterm" id="id709"/><a contenteditable="false" data-primary="" data-startref="xi_workflowstestingusingAgentmode58584" data-type="indexterm" id="id710"/>.</p>
</div></section>

<section data-type="sect1" class="pagebreak-before" data-pdf-bookmark="Conclusion"><div class="sect1" id="id60">
<h1 class="less_space">Conclusion</h1>

<p>In this chapter, we’ve focused on how to apply Copilot’s completion suggestions and chat capabilities in the context of creating tests for code. As shown in the different sections, you can use various approaches to have Copilot generate tests relevant to code in your workspace. Inline completion and suggestions from Copilot can be utilized, as well as the chat capabilities.</p>

<p>Most options for generating tests rely on passing the request to the chat interface in some form—either by using shortcut (slash) commands or entering a prompt through the inline or separate chat interfaces. For any substantial or complex, lengthy tests, working through the chat functionality will likely provide the best results. For quick, boilerplate unit tests or simple unit-testing functions, using specific inline comments as prompts can work well.</p>

<p>Most basic commands for creating tests through Copilot produce unit tests. Copilot can be used to create integration tests by ensuring that the prompt includes sections targeted toward interaction of components, mocking, and verifying the expected <span class="keep-together">outcomes</span>.</p>

<p>Copilot can be used to support best practices such as TDD. With this strategy, Copilot can be used to generate initial testing code and then later generate the appropriate implementation to pass the test. As with all approaches mentioned in the chapter, it’s important to review the results to make sure they are usable and what you intended. You should always feel free to disregard suggestions or code from Copilot and/or reframe prompts and queries to get more accurate results. Also, you can define more explicit context to have Copilot consider for generating the results.</p>

<p>As with implementation code, testing code should ideally be verbosely commented. This makes the code more readable and easily understandable. However, explaining and documenting our test cases or our implementations is something we may put off until later because it takes cycles away from other work, and the algorithms may be changed. In <a data-type="xref" href="ch06.html#ch06">Chapter 6</a>, we’ll explore how Copilot can help make these tasks almost effortless<a contenteditable="false" data-primary="" data-startref="xi_GitHubCopilottestingwith544" data-type="indexterm" id="id711"/><a contenteditable="false" data-primary="" data-startref="xi_testing544" data-type="indexterm" id="id712"/>.</p>
</div></section>
</div></section></div></div></body></html>