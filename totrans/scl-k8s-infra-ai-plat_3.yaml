- en: Chapter 4\. Model Deployment and Monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, you learned about model customization techniques,
    including fine-tuning and training, and about making training and evaluation repeatable.
    Once you’ve achieved the results you are looking for with your model, it’s time
    to deploy your model to production.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will prepare you for model deployment and serving by giving you
    an overview of the major technologies and techniques used with Kubernetes to deploy
    and monitor machine learning models. While we will focus on specific techniques
    relevant to large language models (LLMs) and generative AI, much of this chapter
    will also apply to traditional machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of LLM Serving
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Model serving* is the act of processing inference requests in real time, which
    requires deploying an already trained model to some location suitable for receiving
    these requests. At a high level, model serving involves packaging the model, deploying
    it on hardware accelerators like GPUs or CPUS, exposing APIs for users to query
    the model, and enabling metrics for monitoring and alerting. The components of
    a model-serving system include model-serving platforms,model-serving runtimes,
    and metric gathering and monitoring systems. Typically, an API gateway and load
    balancer to handle bursts of traffic for model queries is also included.'
  prefs: []
  type: TYPE_NORMAL
- en: The model-serving platform component retrieves the model from storage (such
    as Amazon S3 or a local persistent volume) and then performs various preprocessing
    tasks such as changing model formats or postprocessing steps such as gathering
    metrics. It incorporates a model-serving runtime in order to help it serve the
    model. It also exposes a REST or gRPC API so that the models can be interacted
    with by users while providing model access security through the gateway and load
    balancer.
  prefs: []
  type: TYPE_NORMAL
- en: The model-serving runtime component loads the model into the GPU or CPU memory,
    deserializes any incoming query or prompt from its over-the-wire representation,
    converts it into a format suitable for the model, and then executes the inference
    on the model to retrieve a response. This response is typically serialized into
    a JSON object or other format and sent back to the calling application.
  prefs: []
  type: TYPE_NORMAL
- en: The metrics and monitoring components typically aggregate the request metrics
    such as the request time, error codes, token count, tracing, and more, storing
    them to a metrics server like [Prometheus](https://prometheus.io). These metrics
    allow an MLOps practitioner to ensure the health and performance of the models
    in production and diagnose any issues that may come up via alerting.
  prefs: []
  type: TYPE_NORMAL
- en: Although the majority of development and compute time is spent on training and
    tuning a model, [nearly 90% of a model’s lifecycle is spent serving](https://oreil.ly/7rEbn),
    which is why optimizing serving can be pivotal to delivering business value from
    the model. In the next sections, you will learn about each of these essential
    components using Kubernetes-specific tools that will help you scale up your generative
    AI inference workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Using a Model-Serving Platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A model-serving platform is the core component of any inference system, managing
    model deployment and scaling according to the volume of incoming inference requests.
    There are currently many platforms available for serving models on Kubernetes.
    Their purpose is to simplify and scale the model deployment and inference serving
    processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to serve an LLM in a scalable fashion on Kubernetes, a serving platform
    should meet these requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: Support for different types of model architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extensible by adding new model architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for generating embeddings for different modalities, such as text or
    image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for multiple modalities in inference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for chaining inference across multiple models (*model composition*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A wide range of hardware accelerator support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration with standard Kubernetes APIs and tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robust support for different model artifact formats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for a wide range of storage technologies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration with API gateways
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ability to deploy models in an A/B or canary rollout fashion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide flexible integration options with pre- and postprocessing systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for automatically scaling inference infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration of model monitoring solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One of the most popular tools for deploying LLMs with Kubernetes is [KServe](https://oreil.ly/volxt).
    KServe is a controller for Kubernetes that enables Kubernetes to serve both predictive
    and generative AI models along with maintaining inference request pre- and postprocessing
    pipelines. Not only does it meet the previously mentioned requirements, but KServe
    also provides several benefits that have helped its widespread adoption in the
    enterprise:'
  prefs: []
  type: TYPE_NORMAL
- en: An active and thriving open source community
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for traffic routing and autoscaling, including scaling to zero
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for both batch and real-time inference workloads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for both predictive and generative AI inference with a standard protocol,
    the Open Inference Protocol
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Like other controllers, KServe is composed of a set of custom resources, which
    are extensions to the base Kubernetes API. One of these is the ServingRuntime.
    This is essentially a deployment template that defines the environment from which
    models will be served. KServe comes with a number of ServingRuntimes available
    out of the box, but others can be easily added to the system. Each one defines
    things like the container image to be used for the runtime and the model formats
    that the ServingRuntime supports, and can be further customized via environment
    variables set in the container. This allows users to easily add support for new
    model architectures.
  prefs: []
  type: TYPE_NORMAL
- en: At the core of KServe is the InferenceService. This is a custom resource definition
    where you define predictors, storage locations, model format, canaries for gradual
    deployment, deployment mode, and anything else required to serve your model. Models
    are typically initialized from cloud storage like Amazon S3 buckets, but it is
    also possible to use OCI-compliant containers as an alternative to cloud storage
    with KServe [“Modelcars”](https://oreil.ly/gDOnq).
  prefs: []
  type: TYPE_NORMAL
- en: To use this, an OCI-compliant container image must be created and then added
    to a container registry like Quay. When you deploy with KServe, you can then reference
    the repository holding the container. Because a Kubernetes cluster keeps a cache
    of downloaded container images, the model doesn’t need to be downloaded multiple
    times, which can reduce startup time while still reducing overall disk usage.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'KServe offers three deployment modes: `RawDeployment` mode uses a standard
    Kubernetes deployment and ingress gateway (an API gateway for routing inbound
    requests only); serverless mode uses [Knative](https://knative.dev) objects to
    enable serverless deployment; ModelMesh allows for multiple models to be deployed
    in a pod to scale smaller models with fewer compute resources.'
  prefs: []
  type: TYPE_NORMAL
- en: The serving runtime is used within an InferenceService, and the InferenceService
    is managed by the KServe Controller ([Figure 4-1](#ch04_figure_1_1738498450831008)).
    This ensures that the deployed application state matches the definition of the
    InferenceService, creating the deployment for each inference endpoint and enabling
    features like autoscaling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each endpoint is composed of three components:'
  prefs: []
  type: TYPE_NORMAL
- en: Predictor
  prefs: []
  type: TYPE_NORMAL
- en: This is the only required component of an endpoint. It consists of a model and
    model server that makes the model available at the endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer
  prefs: []
  type: TYPE_NORMAL
- en: This component allows users to define both pre- and postprocessing steps as
    needed to manage incoming request data and outgoing inference data.
  prefs: []
  type: TYPE_NORMAL
- en: Explainer
  prefs: []
  type: TYPE_NORMAL
- en: This enables an alternate workflow that provides both predictions and model
    explanations. KServe provides APIs so that users can write and configure their
    own explanation containers.
  prefs: []
  type: TYPE_NORMAL
- en: When a user calls a KServe endpoint with `:predict` or `:explain`, that request
    is routed to the three components. For either call, the transformer component
    is the first stop for the request. If `:predict` was called by the user, the request
    is then routed to the predictor. If `:explain` was called by the user, then the
    request is routed from the transformer to the explainer component, and then the
    explainer calls `:predict` on the predictor component ([Figure 4-1](#ch04_figure_1_1738498450831008)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/skia_0401.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-1\. The request flow for a user calling a KServe endpoint with the
    `:predict` or `:explain` calls
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While KServe has many features that make it a well-functioning platform for
    deploying machine learning models out of the box, LLMs often require some additional
    work.
  prefs: []
  type: TYPE_NORMAL
- en: Diving Into LLM-Serving Runtimes with vLLM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Because KServe allows you to define your own ServingRuntime resources, it is
    possible to use alternative model-serving runtimes with it. One such runtime is
    [vLLM](https://docs.vllm.ai), a serving system tailored for LLMs that aims to
    enhance inference efficiency and scalability. It addresses the challenges of deploying
    LLMs by optimizing memory usage and execution speed, making it suitable for real-time
    applications that require high throughput and low latency.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: KServe also includes an out-of-the-box LLM runtime, the [Hugging Face LLM Serving
    Runtime](https://oreil.ly/K3Bsx), which uses vLLM as its default backend.
  prefs: []
  type: TYPE_NORMAL
- en: vLLM provides a server built on FastAPI for online model serving that is compatible
    with the OpenAI API and also with popular machine learning frameworks like PyTorch,
    allowing for seamless integration with existing machine learning pipelines and
    facilitating the deployment of models trained with these frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: vLLM also supports dynamic batching, which groups multiple inference requests
    into a single batch to improve processing efficiency. This is particularly beneficial
    in high-traffic scenarios, where it can significantly increase throughput.
  prefs: []
  type: TYPE_NORMAL
- en: 'The keys to vLLM’s serving speed are a few core architectural features:'
  prefs: []
  type: TYPE_NORMAL
- en: Paged attention
  prefs: []
  type: TYPE_NORMAL
- en: This is an algorithm that allows the storage of large continuous key-value pairs
    in noncontiguous blocks of memory, optimizing memory use.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel execution
  prefs: []
  type: TYPE_NORMAL
- en: The architecture supports the parallel execution of model components by leveraging
    model parallelism and tensor slicing. This allows different parts of the model
    to be processed simultaneously across multiple hardware units, optimizing resource
    utilization and speeding up inference.
  prefs: []
  type: TYPE_NORMAL
- en: Tensor caching
  prefs: []
  type: TYPE_NORMAL
- en: vLLM has an in-memory tensor store, which caches frequently accessed tensors
    to avoid repeated computations. This significantly reduces the time needed for
    inference by providing fast access to necessary data.
  prefs: []
  type: TYPE_NORMAL
- en: While many model-serving runtimes can be used with KServe, vLLM is a strong
    contender for serving LLMs due to these features. Once your model is deployed,
    it’s imperative to be able to understand the model’s performance over time and
    to keep track of model families and versions that you have available to your system.
    In the next section, you will learn about how to monitor LLMs, what metrics to
    monitor, and how to use registries to keep track of your deployed models.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring and Keeping Track of Your Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whether you have one model or hundreds in production, it is essential to monitor
    their performance in real time and to keep track of which models you have in production.
    To monitor performance, you’ll have to know which metrics you should track. This
    is partially dependent on your use case and infrastructure, but there are also
    general metrics that you can track. Once you’ve built an understanding of metrics
    to track for deployed LLMs, we will move on to monitoring these metrics in KServe
    and then tracking your models in a registry.
  prefs: []
  type: TYPE_NORMAL
- en: LLM Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLM evaluation is a rapidly advancing field. After all, how do you tell if an
    LLM is doing what you want? Are you worried about *hallucinations*, or factually
    incorrect output? Or is machine creativity important for your use case? Or maybe
    you don’t care as much about content but want to make sure that your model is
    serving inference requests at an acceptable rate and that requests are not getting
    stuck in queues.
  prefs: []
  type: TYPE_NORMAL
- en: For task-based metrics, such as those measuring summarization or translation,
    there are countless metrics available to use. You should use caution, however,
    since these tasks are open-ended, and it is unlikely that there is a single metric
    that will give you an accurate view of your model’s performance. You will have
    to evaluate the metrics for your use case and pick a combination that accurately
    conveys your model’s performance on its specific task. A wide variety of summarization
    task-specific metrics can be found in the article [“LLM Evaluation for Text Summarization”](https://oreil.ly/qX9dx),
    published by Neptune.
  prefs: []
  type: TYPE_NORMAL
- en: Model-serving runtimes typically come with their own metrics that measure how
    well the server is handling requests at every stage of processing. vLLM, for instance,
    comes with a [large metrics class](https://oreil.ly/J57gs) that holds many different
    useful metrics for LLMs. Some especially important ones are `gauge_gpu_cache_usage`
    and `gauge_cpu_cache_usage`, which show how much of the key-value cache mentioned
    earlier in this chapter is being utilized, `num_requests_waiting`, which shows
    how many requests are waiting to be processed, and `num_requests_running`, which
    shows how many requests are being processed. All of these metrics are exposed
    by the `/metrics` endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: vLLM’s metrics, as well as those exposed by KServe, can be integrated into KServe
    and visualized in [Prometheus](https://prometheus.io). In KServe, all model-serving
    runtimes are able to export metrics in a Prometheus-compatible format.
  prefs: []
  type: TYPE_NORMAL
- en: Prediction Logging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prediction logging is important in traditional machine learning, and it remains
    important with LLMs. Sometimes this is also called *generation logging* since
    the result of a generation is logged. When combined with the input prompt, input
    and output tokens used, and other generation metrics, prediction logs become a
    powerful tool for auditing model usage and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: These can be stored in any existing log storage solution, and all-in-one solutions
    like [MLflow](https://oreil.ly/BRA7H) integrate prediction logging and viewing
    into their platform.
  prefs: []
  type: TYPE_NORMAL
- en: Production Model Registry
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [Chapter 3](ch03.html#ch03_making_training_repeatable_1738498450655759),
    we covered the overall importance of keeping a single, centralized model registry
    for the entire AI lifecycle, as well as why the preceding stages of the lifecycle
    need a registry. What we haven’t covered yet, though, is what a model registry
    brings to production.
  prefs: []
  type: TYPE_NORMAL
- en: When getting ready to deploy a model, it’s important to know which version of
    the model is ready for production and which version, if any, is currently deployed.
    This is information that you would store in the model registry, along with each
    model’s metadata, such as evaluation results and hyperparameters, which can help
    with the decision to deploy or with configuring the serving environment.
  prefs: []
  type: TYPE_NORMAL
- en: Once deployed, the model registry can still provide important benefits, mostly
    around monitoring and observability. A registry can help users easily find model
    artifacts to track performance metrics for specific deployed model versions, understand
    traffic patterns to those versions, get training and other details to diagnose
    issues found in production, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Like model registries, some groups are beginning to experiment with *prompt
    registries*. While these aren’t currently widely available, this will be an area
    of innovation to watch out for, as prompt registries could provide many of these
    same benefits to prompts themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Not only does a deployed model need metric monitoring, but it also needs safeguards
    and compliance built in to prevent abuse. This has become especially urgent with
    LLMs, due to how well they produce convincing natural language and due to the
    open interface with users they have compared to traditional APIs, creating a vast
    attack surface. This poses additional challenges for safely and responsibly serving
    LLMs.
  prefs: []
  type: TYPE_NORMAL
