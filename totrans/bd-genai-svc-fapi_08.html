<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 5. Achieving Concurrency in AI Workloads" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch05">
<h1><span class="label">Capitolo 5. </span>Raggiungere la concomitanza nei carichi di lavoro dell'IA</h1><div data-type="note"><p>Questo lavoro è stato tradotto utilizzando l'AI. Siamo lieti di ricevere il tuo feedback e i tuoi commenti: <a href="mailto:translation-feedback@oreilly.com">translation-feedback@oreilly.com</a></p></div>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id793">
<h1>Obiettivi del capitolo</h1>
<p><a data-primary="concurrency" data-type="indexterm" id="ix_ch05-asciidoc0"/>In questo capitolo imparerai a conoscere:</p>
<ul>
<li>
<p>Il ruolo e i vantaggi del multithreading, del multiprocessing e della programmazione asincrona per migliorare le prestazioni e la scalabilità delle applicazioni di intelligenza artificiale.</p>
</li>
<li>
<p>Il ruolo del pool di thread e del ciclo di eventi in un server FastAPI</p>
</li>
<li>
<p>Come evitare di bloccare il server durante l'elaborazione delle richieste</p>
</li>
<li>
<p>Utilizzare la programmazione asincrona per interagire con sistemi esterni come database, modelli di intelligenza artificiale e contenuti web costruendo un web scraper e un modulo RAG.</p>
</li>
<li>
<p>Le strategie di model-serving e le strategie di ottimizzazione della memoria di LLM per ridurre le operazioni bloccanti delimitate dalla memoria</p>
</li>
<li>
<p>Strategie per gestire compiti di inferenza AI di lunga durata</p>
</li>
</ul>
</div></aside>
<p>In questo capitolo imparerai a conoscere il ruolo e i vantaggi della programmazione asincrona per aumentare le prestazioni e la scalabilità dei tuoi servizi GenAI. Imparerai a gestire le interazioni simultanee con gli utenti e a interfacciarti con sistemi esterni come i database, a implementare il RAG e a leggere le pagine web per arricchire il contesto delimitato dai prompt del modello. Acquisirai le tecniche per gestire in modo efficace le operazioni<span class="keep-together">delimitate dall'I/O</span> e dalla CPU, soprattutto quando hai a che fare con servizi esterni o gestisci attività di inferenza di lunga durata.</p>
<p>Ci addentreremo anche nelle strategie per gestire in modo efficiente i compiti di inferenza dell'IA generativa di lunga durata, compreso l'uso del ciclo di eventi FastAPI per l'<span class="keep-together">esecuzione</span> dei compiti in background.</p>
<section data-pdf-bookmark="Optimizing GenAI Services for Multiple Users" data-type="sect1"><div class="sect1" id="id76">
<h1>Ottimizzare i servizi GenAI per più utenti</h1>
<p><a data-primary="concurrency" data-secondary="optimizing GenAI services for multiple users" data-type="indexterm" id="ix_ch05-asciidoc1"/>I carichi di lavoro dell'IA sono operazioni computazionalmente costose che possono impedire ai tuoi servizi GenAI di servire più richieste simultanee. Nella maggior parte degli scenari di produzione, più utenti utilizzeranno le tue applicazioni. Pertanto, i tuoi servizi dovranno servire le richieste <em>in modo simultaneo</em>in modo da poter eseguire più attività sovrapposte. Tuttavia, se ti interfacci con i modelli GenAI e con sistemi esterni come i database, il filesystem o internet, ci saranno operazioni che possono bloccare l'esecuzione di altre attività sul tuo server.<a data-primary="blocking operations" data-type="indexterm" id="id794"/>Le operazioni di lunga durata che<span class="keep-together">possono interrompere</span> il flusso di esecuzione del programma sono considerate <em>bloccanti</em>.</p>
<p>Queste operazioni di blocco possono essere di due tipi:</p>
<dl>
<dt>Ingresso/uscita (I/O) delimitato</dt>
<dd>
<p>Quando un processo deve attendere a causa di un'operazione di input/output di dati, che possono provenire da un utente, da un file, da un database, da una rete e così via.</p>
</dd>
<dt>Calcolo delimitato</dt>
<dd>
<p>Quando un processo deve attendere a causa di un'operazione ad alta intensità di calcolo sulla CPU o sulla GPU. I programmi delimitati dal calcolo spingono i core della CPU o della GPU al loro limite eseguendo calcoli intensivi, spesso bloccando l'esecuzione di altre attività.<sup><a data-type="noteref" href="ch05.html#id795" id="id795-marker" translate="no">1</a></sup>
Alcuni esempi sono l'elaborazione dei dati, l'inferenza o l'addestramento di modelli di intelligenza artificiale, il rendering di oggetti 3D, l'esecuzione di simulazioni e così via.</p>
</dd>
</dl>
<p>Hai a disposizione alcune strategie per servire più utenti:</p>
<dl>
<dt>Ottimizzazione del sistema</dt>
<dd>
<p>Per le attività legate all'I/O, come il recupero di dati da un database, l'utilizzo di file su disco, le richieste di rete o la lettura di pagine web.</p>
</dd>
<dt>Ottimizzazione del modello</dt>
<dd>
<p>Per compiti delimitati dalla memoria e dal calcolo, come il caricamento e l'inferenza del modello</p>
</dd>
<dt>Sistema di accodamento</dt>
<dd>
<p>Per gestire compiti di inferenza di lunga durata per evitare ritardi nelle risposte.</p>
</dd>
</dl>
<p class="less_space pagebreak-before">In questa sezione analizzeremo ogni strategia in modo più dettagliato. Per aiutarti a consolidare l'apprendimento, implementeremo anche diverse funzioni che sfruttano le strategie sopra citate:</p>
<ul>
<li>
<p>Costruisci uno <em>scraper di pagine web</em> per recuperare e analizzare in massa gli URL HTTP incollati nella chat, in modo da poter chiedere al tuo LLM informazioni sul contenuto delle pagine web.</p>
</li>
<li>
<p>Aggiungi un modulo <em>RAG (retrieval augmented generation</em> ) al tuo servizio con un database vettoriale self-hosted come <code translate="no">qdrant</code>, in modo da poter caricare e dialogare con i tuoi documenti tramite il tuo servizio LLM.</p>
</li>
<li>
<p>Aggiunta di un <em>sistema di generazione di immagini in batch</em>in modo da poter eseguire i carichi di lavoro di generazione di immagini come attività in background</p>
</li>
</ul>
<p>Prima di mostrarti come costruire le funzionalità sopra citate, dobbiamo approfondire l'argomento della <em>concorrenza</em> e del <em>parallelismo</em>, perché la comprensione di entrambi i concetti ti aiuterà a individuare le strategie corrette da utilizzare per i tuoi casi d'uso.</p>
<p><a data-primary="concurrency" data-secondary="defined" data-type="indexterm" id="id796"/>La<em>concomitanza</em> si riferisce alla capacità di un servizio di gestire più richieste o attività contemporaneamente, senza completarne una dopo l'altra. Durante le operazioni concomitanti, le tempistiche di più attività possono sovrapporsi e possono iniziare e terminare in momenti diversi.</p>
<p>In Python, puoi implementare la concorrenza con un singolo core della CPU passando da un compito all'altro su un singolo thread (tramite la programmazione asincrona) o tra diversi thread (tramite il multithreading).</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id797">
<h1>Slice del tempo</h1>
<p><a data-primary="multithreading" data-secondary="time slicing" data-type="indexterm" id="id798"/><a data-primary="time slicing" data-type="indexterm" id="id799"/>Il<em>time slice</em> è un meccanismo di schedulazione nella programmazione multithreading e asincrona in cui un processo alloca il tempo della CPU tra i compiti per dare l'illusione di un'esecuzione simultanea.</p>
<p><a data-primary="Global Interpreter Lock (GIL)" data-type="indexterm" id="id800"/><a data-primary="Python" data-secondary="Global Interpreter Lock (GIL)" data-tertiary="time slicing" data-type="indexterm" id="id801"/>In Python, il tempo della CPU può essere allocato a un solo task in qualsiasi momento grazie al <em>Global Interpreter Lock</em> (GIL) di Python. Il GIL di Python permette a un solo thread (cioè un flusso di esecuzione in un processo Python) di controllare l'interprete Python per l'esecuzione del codice. Ciò significa che solo un thread può trovarsi in uno stato di esecuzione in qualsiasi momento.<sup><a data-type="noteref" href="ch05.html#id802" id="id802-marker" translate="no">2</a></sup></p>
<p>Il GIL è stato originariamente implementato per semplificare lo sviluppo di Python, facilitare la gestione della memoria tra i thread in un processo Python e dare priorità alle prestazioni dei programmi a thread singolo. È stato inoltre aggiunto a Python per garantire <em>la sicurezza dei thread</em> all'interno del processo, prevenendo le condizioni di gara che potrebbero portare alla corruzione dei dati quando si lavora con risorse condivise tra i thread.</p>
<p>Tuttavia, l'aggiunta del GIL a Python significa anche che non è possibile una vera e propria esecuzione parallela di thread in un singolo processo Python. Quando un'attività è in attesa del completamento di un'operazione di I/O, la CPU può passare rapidamente a un'altra attività per evitare di bloccare altre operazioni. Le attività in pausa salvano il loro stato e possono riprendere quando le operazioni di I/O sono terminate.</p>
</div></aside>
<p>Con più core, puoi anche implementare un sottoinsieme della concorrenza chiamato <em>parallelismo</em>, in cui i compiti vengono suddivisi tra più lavoratori indipendenti (tramite il multiprocessing), ognuno dei quali esegue i compiti simultaneamente sulle proprie risorse isolate e su processi separati.</p>
<div data-type="note" epub:type="note"><h6>Nota</h6>
<p>Anche se si prevede di rimuovere presto il GIL da Python, al momento in cui scriviamo non è possibile che più thread lavorino simultaneamente attraverso i task. Pertanto, la concorrenza su un singolo core può dare un'illusione di parallelismo anche se c'è un solo processo che sta facendo tutto il lavoro. Il singolo processo può essere multitasking solo cambiando i thread attivi per ridurre al minimo i tempi di attesa delle operazioni di blocco dell'I/O.</p>
<p>Il vero parallelismo si può ottenere solo con più lavoratori (in multiprocessing).</p>
</div>
<p><a data-primary="concurrency" data-secondary="parallelism versus" data-type="indexterm" id="ix_ch05-asciidoc2"/><a data-primary="parallelism, concurrency versus" data-type="indexterm" id="ix_ch05-asciidoc3"/>Anche se la concorrenza e il parallelismo hanno molte somiglianze, non sono esattamente gli stessi concetti. La grande differenza tra loro è che la concorrenza può aiutarti a gestire più attività intersecando la loro esecuzione, il che è utile per le attività legate all'I/O. Il parallelismo, invece, prevede l'esecuzione di più attività simultaneamente, in genere su macchine multicore, il che è più utile per le attività legate alla CPU.</p>
<p>Puoi implementare la concorrenza utilizzando approcci come il threading o la programmazione asincrona (ad esempio, il time slice su una macchina single-core, in cui i compiti vengono interlacciati per dare l'impressione di un'esecuzione simultanea).</p>
<p>La<a data-type="xref" href="#concurrency_parallelism">Figura 5-1</a> mostra la relazione tra concurrency e parallelismo.</p>
<figure class="less_space pagebreak-before"><div class="figure" id="concurrency_parallelism">
<img alt="bgai 0501" src="assets/bgai_0501.png" width="346" height="313"/>
<h6><span class="label">Figura 5-1. </span>Concorrenza e parallelismo</h6>
</div></figure>
<p>Nella maggior parte dei sistemi scalabili, è possibile assistere sia alla concorrenza che al parallelismo.</p>
<p>In un sistema concorrente, vedrai il proprietario del ristorante prendere le ordinazioni e cucinare gli hamburger, occupandosi di volta in volta di ciascun compito e passando da un'attività all'altra in modo efficace e multitasking. In un sistema parallelo, vedrai diversi membri del personale prendere le ordinazioni e altri cucinare gli hamburger allo stesso tempo. In questo caso, diversi lavoratori si occupano di ciascun compito contemporaneamente.</p>
<p>Senza il multithreading o la programmazione asincrona in un processo a thread singolo, il processo deve aspettare che le operazioni bloccanti finiscano prima di poter avviare nuove attività. Senza il multiprocessing che implementa il parallelismo su più core, le operazioni computazionalmente costose possono bloccare l'applicazione dall'avvio di altre attività.</p>
<p>La<a data-type="xref" href="#concurrency_parallelism_timeline">Figura 5-2</a> mostra la distinzione tra esecuzione non concorrente, esecuzione concorrente senza parallelismo (singolo core) ed esecuzione concorrente con parallelismo (più core).</p>
<p>I tre modelli di esecuzione di Python mostrati nella <a data-type="xref" href="#concurrency_parallelism_timeline">Figura 5-2</a> sono i seguenti:</p>
<dl>
<dt>Nessuna concomitanza (sincrona)</dt>
<dd>
<p>Un singolo processo (su un solo core) esegue i compiti in modo sequenziale.</p>
</dd>
<dt>Concorrente e non parallelo</dt>
<dd>
<p>Più thread in un singolo processo (su un core) gestiscono i compiti in modo concorrente ma non in parallelo a causa del GIL di Python.</p>
</dd>
<dt>Concorrente e parallelo</dt>
<dd>
<p>Più processi su più core eseguono i compiti in parallelo, sfruttando al massimo i processori multicore per ottenere la massima efficienza.<a data-startref="ix_ch05-asciidoc3" data-type="indexterm" id="id803"/><a data-startref="ix_ch05-asciidoc2" data-type="indexterm" id="id804"/></p>
</dd>
</dl>
<figure><div class="figure" id="concurrency_parallelism_timeline">
<img alt="bgai 0502" src="assets/bgai_0502.png" width="994" height="1968"/>
<h6><span class="label">Figura 5-2. </span>Concorrenza con e senza parallelismo</h6>
</div></figure>
<p class="less_space pagebreak-before"><a data-primary="multiprocessing" data-type="indexterm" id="id805"/>Nel multiprocesso, ogni processo ha accesso al proprio spazio di memoria e alle proprie risorse per completare un'attività in modo isolato dagli altri processi. Questo isolamento può rendere i processi più stabili, in quanto se un processo si blocca non si ripercuoterà sugli altri, ma rende la comunicazione tra i processi più complessa rispetto ai thread, che condividono lo stesso spazio di memoria, come mostrato nella <a data-type="xref" href="#multiprocessing_resources">Figura 5-3</a>.</p>
<figure><div class="figure" id="multiprocessing_resources">
<img alt="bgai 0503" src="assets/bgai_0503.png" width="1441" height="535"/>
<h6><span class="label">Figura 5-3. </span>La condivisione delle risorse nel multithreading e nel multiprocessing</h6>
</div></figure>
<p>I carichi di lavoro distribuiti spesso utilizzano un processo di gestione che coordina l'esecuzione e la collaborazione di questi processi per evitare problemi come la corruzione dei dati e la duplicazione del lavoro. Un buon esempio di multiprocesso è quello di servire le richieste con un bilanciatore di carico che gestisce il traffico verso più container, ognuno dei quali esegue un'istanza della tua applicazione.</p>
<p>Sia il multithreading che la programmazione asincrona riducono i tempi di attesa delle attività di I/O perché il processore può svolgere altre attività in attesa dell'I/O. Tuttavia, non sono utili per le attività che richiedono calcoli pesanti, come l'inferenza dell'intelligenza artificiale, perché il processo è impegnato a calcolare alcuni risultati. Pertanto, per servire un modello GenAI self-hosted di grandi dimensioni a più utenti, è necessario scalare i servizi con il multiprocessing o utilizzare ottimizzazioni algoritmiche del modello (tramite server specializzati nell'inferenza del modello come vLLM).</p>
<p>Il tuo primo istinto quando lavori con modelli lenti potrebbe essere quello di adottare il parallelismo creando istanze multiple del tuo servizio FastAPI (multiprocessing) in una singola macchina per servire le richieste in parallelo.</p>
<p>Sfortunatamente, più worker in esecuzione in processi separati non avranno accesso a uno spazio di memoria condiviso. Di conseguenza, non puoi condividere artefatti come un modello GenAI caricato in memoria tra istanze separate della tua app in FastAPI. Purtroppo, sarà necessario caricare anche una nuova istanza del modello, il che consumerà significativamente le tue risorse hardware. Questo perché FastAPI è un server web generico che non ottimizza in modo nativo il servizio dei modelli GenAI.</p>
<p>La soluzione non è il parallelismo in sé, ma l'adozione della strategia del model-serving esterno, come illustrato nel <a data-type="xref" href="ch03.html#ch03">Capitolo 3</a>.</p>
<p>L'unico caso in cui puoi trattare i carichi di lavoro dell'inferenza dell'intelligenza artificiale come I/O-bound, invece che come compute-bound, è quando ti affidi alle API di provider di AI di terze parti (ad esempio, OpenAI API). In questo caso, stai scaricando i compiti legati al calcolo al provider del modello attraverso le richieste di rete.</p>
<p>Dal tuo lato, i carichi di lavoro dell'inferenza dell'intelligenza artificiale diventano vincolati all'I/O attraverso le richieste di rete, consentendo l'uso della concorrenza attraverso il time slice. Il fornitore di terze parti deve preoccuparsi di scalare i propri servizi per gestire le inferenze del modello - che sono delimitate dal calcolo - attraverso le proprie risorse hardware.</p>
<p>Puoi esternalizzare il servizio e l'inferenza di modelli GenAI più grandi, come un LLM, con server specializzati come vLLM, Ray Serve o NVIDIA Triton.</p>
<p>Più avanti in questo capitolo, spiegherò come questi server massimizzano l'efficienza delle operazioni delimitate dal calcolo durante l'inferenza del modello, riducendo al contempo l'ingombro in memoria del modello durante il processo di generazione dei dati.</p>
<p><a data-primary="concurrency" data-secondary="strategies" data-type="indexterm" id="ix_ch05-asciidoc4"/>Per aiutarti a digerire quanto discusso finora, dai un'occhiata alla tabella di confronto delle strategie di concorrenza nella <a data-type="xref" href="#concurrency_comparison">Tabella 5-1</a> per capire quando e perché usarle.</p>
<table class="striped" id="concurrency_comparison">
<caption><span class="label">Tabella 5-1. </span>Confronto tra le strategie di circolazione</caption>
<thead>
<tr>
<th>Strategia</th>
<th>Caratteristiche</th>
<th>Sfide</th>
<th>Casi d'uso</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Nessuna concomitanza (sincrona)</p></td>
<td>
<ul><li><p>Codice semplice, leggibile e di facile comprensione per il debug</p></li>
<li><p>Un singolo core e thread della CPU</p></li></ul></td>
<td><ul>
<li><p>Potenziali lunghi tempi di attesa a causa di operazioni di blocco dell'I/O o della CPU che bloccano l'esecuzione del processo.</p></li>
<li><p>Non può servire più utenti contemporaneamente</p></li></ul></td>
<td>
<ul>
<li><p>Applicazioni per utente singolo in cui gli utenti possono aspettare che le attività vengano completate</p></li>
<li><p>Servizi o applicazioni utilizzati di rado</p></li>
</ul></td>
</tr>
<tr>
<td><p>Async IO (asincrono)</p></td>
<td>
<ul>
<li><p>Un singolo core e thread della CPU</p></li>
<li><p>Il multitasking gestito da un ciclo di eventi all'interno del processo di Python</p></li><li><p>Thread-safe, in quanto il processo Python gestisce i compiti</p></li>
<li><p>Massimizza il tasso di utilizzo della CPU</p></li>
<li><p>Più veloce del multithreading e del multiprocessing per le attività di I/O</p></li>
</ul></td>
<td><ul><li><p>È più difficile da implementare nel codice e può rendere più difficile il debug.</p></li>
<li><p>Richiede librerie e dipendenze che utilizzano le funzionalità di Async IO</p></li>
<li><p>È facile commettere errori che bloccano il processo principale (e il ciclo degli eventi)</p></li></ul></td>
<td><p>Applicazioni con attività di I/O bloccanti</p></td>
</tr>
<tr>
<td><p>Multithreading</p></td>
<td>
<ul>
<li><p>Un singolo core della CPU ma più thread all'interno dello stesso processo</p></li>
<li><p>I thread condividono dati e risorse</p></li>
<li><p>Più semplice dell'IO asincrono da implementare nel codice</p></li>
<li><p>Multitasking su thread orchestrato dal sistema operativo</p></li></ul></td>
<td><ul><li><p>È difficile bloccare le risorse per ogni thread per evitare problemi di thread-safety che possono portare a bug non riproducibili e alla corruzione dei dati.</p></li>
<li><p>I thread possono bloccarsi l'un l'altro all'infinito (deadlock)</p></li>
<li><p>L'accesso concorrente alle risorse può causare risultati incoerenti (race conditions)</p></li>
<li><p>A un thread possono essere negate le risorse se monopolizza i thread (starvation)</p></li>
<li><p>La creazione e la distruzione di thread è computazionalmente costosa</p></li></ul></td>
<td><p>Applicazioni o servizi che hanno attività di I/O bloccanti</p></td>
</tr>
<tr>
<td><p>Multiprocesso</p></td>
<td>
<ul>
<li><p>Processi multipli in esecuzione su più core della CPU</p></li>
<li><p>A ogni processo viene assegnato un core della CPU e risorse isolate.</p></li>
<li><p>Il lavoro può essere distribuito tra i core della CPU e gestito da un processo di orchestrazione utilizzando strumenti come Celery</p></li></ul></td>
<td><ul>
<li><p>La condivisione di risorse hardware e di oggetti come un modello di AI di grandi dimensioni o di dati tra i processi può essere complessa e richiede meccanismi di comunicazione inter-processo (IPC) o una memoria condivisa dedicata.</p></li>
<li><p>Difficile mantenere sincronizzati più processi isolati</p></li>
<li><p>Creare e distruggere processi è computazionalmente costoso</p></li></ul></td>
<td><ul><li><p>Applicazioni o servizi che hanno compiti di calcolo delimitati e bloccanti</p></li>
<li><p>Attività di tipo "divide et impera" in cui l'elaborazione può essere eseguita in parti isolate.</p></li>
<li><p>Distribuire i carichi di lavoro o le richieste di elaborazione su più core della CPU</p></li></ul></td>
</tr>
</tbody>
</table>
<p>Ora che abbiamo esplorato varie strategie di concorrenza, continuiamo a migliorare i tuoi servizi con la programmazione asincrona per gestire in modo efficiente le operazioni legate all'I/O.<a data-startref="ix_ch05-asciidoc4" data-type="indexterm" id="id806"/>In seguito ci concentreremo sull'ottimizzazione delle attività legate al calcolo, in particolare l'inferenza di modelli tramite server specializzati.<a data-startref="ix_ch05-asciidoc1" data-type="indexterm" id="id807"/></p>
</div></section>
<section data-pdf-bookmark="Optimizing for I/O Tasks with Asynchronous Programming" data-type="sect1"><div class="sect1" id="id77">
<h1>Ottimizzazione delle attività di I/O con la programmazione asincrona</h1>
<p><a data-primary="asynchronous (async) I/O" data-type="indexterm" id="ix_ch05-asciidoc5"/><a data-primary="concurrency" data-secondary="optimizing for I/O tasks with asynchronous programming" data-type="indexterm" id="ix_ch05-asciidoc6"/>In questa sezione esploreremo l'uso della programmazione asincrona per evitare di bloccare il processo principale del server con attività delimitate dall'I/O durante i carichi di lavoro dell'intelligenza artificiale. Conoscerai anche il framework <code translate="no">asyncio</code> che permette di scrivere applicazioni asincrone in Python.</p>
<section class="less_space pagebreak-before" data-pdf-bookmark="Synchronous Versus Asynchronous (Async) Execution" data-type="sect2"><div class="sect2" id="id78">
<h2>Esecuzione sincrona contro esecuzione asincrona (Async)</h2>
<p><a data-primary="asynchronous (async) I/O" data-secondary="synchronous execution versus" data-type="indexterm" id="ix_ch05-asciidoc7"/><a data-primary="concurrency" data-secondary="optimizing for I/O tasks with asynchronous programming" data-tertiary="synchronous versus asynchronous execution" data-type="indexterm" id="ix_ch05-asciidoc8"/><a data-primary="synchronous execution, asynchronous execution versus" data-type="indexterm" id="ix_ch05-asciidoc9"/>Cosa si intende per applicazione asincrona? Per rispondere a questa domanda, confrontiamo i programmi sincroni e quelli asincroni.</p>
<p>Un'applicazione è considerata <em>sincrona</em>quando i compiti vengono eseguiti in ordine sequenziale e ogni compito attende il completamento del precedente prima di iniziare. Per le applicazioni che vengono eseguite di rado e che richiedono solo pochi secondi per essere elaborate, il codice sincrono raramente causa problemi e può rendere le implementazioni più veloci e semplici. Tuttavia, se hai bisogno della concorrenza e vuoi che l'efficienza dei tuoi servizi sia massimizzata su ogni core, i tuoi servizi devono lavorare in multitasking senza attendere il completamento di operazioni bloccanti. È qui che l'implementazione della concorrenza <em>asincrona</em> (async) può aiutarti.</p>
<p>Vediamo alcuni esempi di funzioni sincrone e asincrone per capire quanto un codice asincrono possa aumentare le prestazioni. In entrambi gli esempi, utilizzerò sleeping per simulare un'operazione di blocco dell'I/O, ma puoi immaginare altre operazioni di I/O eseguite in scenari reali.</p>
<p>L<a data-type="xref" href="#sync_execution">'esempio 5-1</a> mostra un esempio di codice sincrono che simula un'operazione di I/O bloccante con la funzione bloccante <code translate="no">time.sleep()</code>.</p>
<div data-type="example" id="sync_execution">
<h5><span class="label">Esempio 5-1. </span>Esecuzione sincrona</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="kn" translate="no">import</code><code translate="no"> </code><code class="nn" translate="no">time</code><code translate="no">
</code><code translate="no">
</code><code class="k" translate="no">def</code><code translate="no"> </code><code class="nf" translate="no">task</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">    </code><code class="nb" translate="no">print</code><code class="p" translate="no">(</code><code class="s2" translate="no">"</code><code class="s2" translate="no">Start of sync task</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">time</code><code class="o" translate="no">.</code><code class="n" translate="no">sleep</code><code class="p" translate="no">(</code><code class="mi" translate="no">5</code><code class="p" translate="no">)</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO1-1" id="co_achieving_concurrency_in_ai_workloads_CO1-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">    </code><code class="nb" translate="no">print</code><code class="p" translate="no">(</code><code class="s2" translate="no">"</code><code class="s2" translate="no">After 5 seconds of sleep</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">
</code><code class="n" translate="no">start</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">time</code><code class="o" translate="no">.</code><code class="n" translate="no">time</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code translate="no">
</code><code class="k" translate="no">for</code><code translate="no"> </code><code class="n" translate="no">_</code><code translate="no"> </code><code class="ow" translate="no">in</code><code translate="no"> </code><code class="nb" translate="no">range</code><code class="p" translate="no">(</code><code class="mi" translate="no">3</code><code class="p" translate="no">)</code><code class="p" translate="no">:</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO1-2" id="co_achieving_concurrency_in_ai_workloads_CO1-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">task</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code translate="no">
</code><code class="n" translate="no">duration</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">time</code><code class="o" translate="no">.</code><code class="n" translate="no">time</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code translate="no"> </code><code class="o" translate="no">-</code><code translate="no"> </code><code class="n" translate="no">start</code><code translate="no">
</code><code class="nb" translate="no">print</code><code class="p" translate="no">(</code><code class="sa" translate="no">f</code><code class="s2" translate="no">"</code><code class="se" translate="no">\n</code><code class="s2" translate="no">Process completed in: </code><code class="si" translate="no">{</code><code class="n" translate="no">duration</code><code class="si" translate="no">}</code><code class="s2" translate="no"> seconds</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code translate="no">
</code><code class="sd" translate="no">"""
Start of sync task
After 5 seconds of sleep
Start of sync task
After 5 seconds of sleep
Start of sync task
After 5 seconds of sleep

Process completed in: 15.014271020889282 seconds
"""</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO1-1" id="callout_achieving_concurrency_in_ai_workloads_CO1-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Usa <code translate="no">sleep()</code> per simulare un'operazione di blocco dell'I/O come l'invio di una richiesta di rete.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO1-2" id="callout_achieving_concurrency_in_ai_workloads_CO1-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a></dt>
<dd><p>Chiama il sito <code translate="no">task()</code> per tre volte, in sequenza. Il ciclo simula l'invio di più richieste di rete, una dopo l'altra.</p></dd>
</dl></div>
<p>La chiamata a <code translate="no">task()</code> per tre volte nell'<a data-type="xref" href="#sync_execution">Esempio 5-1</a> richiede 15 secondi perché Python aspetta che l'operazione bloccante <code translate="no">sleep()</code> venga completata.</p>
<p>Per sviluppare programmi asincroni in Python, puoi utilizzare il pacchetto <code translate="no">asyncio</code> che fa parte della libreria standard di Python 3.5 e versioni successive. Utilizzando <code translate="no">asyncio</code>, il codice asincrono è simile al codice sincrono sequenziale ma con l'aggiunta delle parole chiave <code translate="no">async</code> e <code translate="no">await</code> per eseguire operazioni di I/O non bloccanti.</p>
<p>L<a data-type="xref" href="#async_execution">'Esempio 5-2</a> mostra come puoi usare le parole chiave <code translate="no">async</code> e <code translate="no">await</code> con <code translate="no">asyncio</code> per eseguire l'<a data-type="xref" href="#sync_execution">Esempio 5-1</a> in modo asincrono.</p>
<div data-type="example" id="async_execution">
<h5><span class="label">Esempio 5-2. </span>Esecuzione asincrona</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="kn" translate="no">import</code><code translate="no"> </code><code class="nn" translate="no">time</code><code translate="no">
</code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="nn" translate="no">asyncio</code><code translate="no">
</code><code translate="no">
</code><code class="k" translate="no">async</code><code translate="no"> </code><code class="k" translate="no">def</code><code translate="no"> </code><code class="nf" translate="no">task</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code class="p" translate="no">:</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO2-1" id="co_achieving_concurrency_in_ai_workloads_CO2-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">    </code><code class="nb" translate="no">print</code><code class="p" translate="no">(</code><code class="s2" translate="no">"</code><code class="s2" translate="no">Start of async task</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">await</code><code translate="no"> </code><code class="n" translate="no">asyncio</code><code class="o" translate="no">.</code><code class="n" translate="no">sleep</code><code class="p" translate="no">(</code><code class="mi" translate="no">5</code><code class="p" translate="no">)</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO2-2" id="co_achieving_concurrency_in_ai_workloads_CO2-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">    </code><code class="nb" translate="no">print</code><code class="p" translate="no">(</code><code class="s2" translate="no">"</code><code class="s2" translate="no">Task resumed after 5 seconds</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">
</code><code class="k" translate="no">async</code><code translate="no"> </code><code class="k" translate="no">def</code><code translate="no"> </code><code class="nf" translate="no">spawn_tasks</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">await</code><code translate="no"> </code><code class="n" translate="no">asyncio</code><code class="o" translate="no">.</code><code class="n" translate="no">gather</code><code class="p" translate="no">(</code><code class="n" translate="no">task</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">task</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">task</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code class="p" translate="no">)</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO2-3" id="co_achieving_concurrency_in_ai_workloads_CO2-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">
</code><code class="n" translate="no">start</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">time</code><code class="o" translate="no">.</code><code class="n" translate="no">time</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code translate="no">
</code><code class="n" translate="no">asyncio</code><code class="o" translate="no">.</code><code class="n" translate="no">run</code><code class="p" translate="no">(</code><code class="n" translate="no">spawn_tasks</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code class="p" translate="no">)</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO2-4" id="co_achieving_concurrency_in_ai_workloads_CO2-4"><img alt="4" src="assets/4.png" width="12" height="12"/></a><code translate="no">
</code><code class="n" translate="no">duration</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">time</code><code class="o" translate="no">.</code><code class="n" translate="no">time</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code translate="no"> </code><code class="o" translate="no">-</code><code translate="no"> </code><code class="n" translate="no">start</code><code translate="no">
</code><code translate="no">
</code><code class="nb" translate="no">print</code><code class="p" translate="no">(</code><code class="sa" translate="no">f</code><code class="s2" translate="no">"</code><code class="se" translate="no">\n</code><code class="s2" translate="no">Process completed in: </code><code class="si" translate="no">{</code><code class="n" translate="no">duration</code><code class="si" translate="no">}</code><code class="s2" translate="no"> seconds</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code translate="no">
</code><code class="sd" translate="no">"""
Start of async task
Start of async task
Start of async task
Task resumed after 5 seconds
Task resumed after 5 seconds
Task resumed after 5 seconds

Process completed in: 5.0057971477508545 seconds </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO2-5" id="co_achieving_concurrency_in_ai_workloads_CO2-5"><img alt="5" src="assets/5.png" width="12" height="12"/></a><code class="sd" translate="no">
"""</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO2-1" id="callout_achieving_concurrency_in_ai_workloads_CO2-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Implementa una coroutine <code translate="no">task</code> che cede il controllo al ciclo degli eventi durante le operazioni bloccanti.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO2-2" id="callout_achieving_concurrency_in_ai_workloads_CO2-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a></dt>
<dd><p>Lo sleep non bloccante di cinque secondi segnala al ciclo degli eventi di eseguire un'altra attività durante l'attesa.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO2-3" id="callout_achieving_concurrency_in_ai_workloads_CO2-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a></dt>
<dd><p>Utilizza <code translate="no">asyncio.create_task</code> per generare istanze di task da concatenare (o riunire) ed eseguirle simultaneamente utilizzando <code translate="no">asyncio.gather</code>.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO2-4" id="callout_achieving_concurrency_in_ai_workloads_CO2-4"><img alt="4" src="assets/4.png" width="12" height="12"/></a></dt>
<dd><p>Crea un ciclo di eventi per programmare attività asincrone con il metodo <code translate="no">asyncio.run</code>.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO2-5" id="callout_achieving_concurrency_in_ai_workloads_CO2-5"><img alt="5" src="assets/5.png" width="12" height="12"/></a></dt>
<dd><p>Il tempo di esecuzione è 1/3 rispetto all'esempio sincrono, poiché questa volta il processo Python non è stato bloccato.</p></dd>
</dl></div>
<p>Dopo aver eseguito l'<a data-type="xref" href="#async_execution">Esempio 5-2</a>, noterai che la funzione <code translate="no">task()</code> è stata chiamata tre volte in modo concomitante. D'altra parte, il codice dell'<a data-type="xref" href="#sync_execution">Esempio 5-1</a> chiama la funzione <code translate="no">task()</code> tre volte in modo sequenziale. La funzione async è stata eseguita all'interno del ciclo di eventi di <code translate="no">asyncio</code>, che era responsabile dell'esecuzione del codice senza aspettare.</p>
<p>In qualsiasi codice asincrono, la parola chiave <code translate="no">await</code> segnala a Python le operazioni di I/O bloccanti in modo che vengano eseguite in modo <em>non bloccante</em> (cioè che possano essere eseguite senza bloccare il processo principale). Essendo a conoscenza delle operazioni bloccanti, Python può andare a fare qualcos'altro mentre aspetta che le operazioni bloccanti finiscano.</p>
<p>L<a data-type="xref" href="#await_keyword">'esempio 5-3</a> mostra come utilizzare le parole chiave <code translate="no">async</code> e <code translate="no">await</code> per dichiarare ed eseguire funzioni asincrone.</p>
<div data-type="example" id="await_keyword">
<h5><span class="label">Esempio 5-3. </span>Come utilizzare le parole chiave <code translate="no">async</code> e <code translate="no">await</code> </h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="kn" translate="no">import</code><code translate="no"> </code><code class="nn" translate="no">asyncio</code><code translate="no">
</code><code translate="no">
</code><code class="k" translate="no">async</code><code translate="no"> </code><code class="k" translate="no">def</code><code translate="no"> </code><code class="nf" translate="no">main</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">    </code><code class="nb" translate="no">print</code><code class="p" translate="no">(</code><code class="s2" translate="no">"</code><code class="s2" translate="no">Before sleeping</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">await</code><code translate="no"> </code><code class="n" translate="no">asyncio</code><code class="o" translate="no">.</code><code class="n" translate="no">sleep</code><code class="p" translate="no">(</code><code class="mi" translate="no">3</code><code class="p" translate="no">)</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO3-1" id="co_achieving_concurrency_in_ai_workloads_CO3-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">    </code><code class="nb" translate="no">print</code><code class="p" translate="no">(</code><code class="s2" translate="no">"</code><code class="s2" translate="no">After sleeping for 3 seconds</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">
</code><code class="n" translate="no">asyncio</code><code class="o" translate="no">.</code><code class="n" translate="no">run</code><code class="p" translate="no">(</code><code class="n" translate="no">main</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code class="p" translate="no">)</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO3-2" id="co_achieving_concurrency_in_ai_workloads_CO3-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">
</code><code class="sd" translate="no">"""
Before sleeping
After sleeping for 3 seconds </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO3-3" id="co_achieving_concurrency_in_ai_workloads_CO3-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a><code class="sd" translate="no">
"""</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO3-1" id="callout_achieving_concurrency_in_ai_workloads_CO3-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Simula un'operazione di I/O non bloccante<code translate="no">await</code>ing <code translate="no">asyncio.sleep()</code> in modo che Python possa fare altre cose durante l'attesa.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO3-2" id="callout_achieving_concurrency_in_ai_workloads_CO3-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a></dt>
<dd><p>Devi chiamare <code translate="no">main()</code> all'interno di <code translate="no">asyncio.run()</code> per eseguirla, poiché si tratta di una funzione asincrona, altrimenti non verrà eseguita e restituirà un oggetto <em>coroutine</em>. Tratterò le coroutine a breve.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO3-3" id="callout_achieving_concurrency_in_ai_workloads_CO3-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a></dt>
<dd><p>Se esegui il codice, la seconda istruzione verrà stampata 3 secondi dopo la prima. In questo caso, dato che non ci sono altre operazioni da eseguire oltre allo sleep, Python gira in idle fino al completamento dell'operazione di sleep.</p></dd>
</dl></div>
<p>Nell'<a data-type="xref" href="#await_keyword">Esempio 5-3</a>, ho usato lo sleep per simulare le operazioni di blocco dell'I/O, come le richieste di rete.</p>
<div data-type="caution"><h6>Attenzione</h6>
<p>Puoi usare la parola chiave <code translate="no">await</code> solo all'interno di una funzione dichiarata con <code translate="no">async def</code>. L'uso di <code translate="no">await</code> al di fuori di una funzione <code translate="no">async</code> solleverà un <code translate="no">SyntaxError</code> in Python. Un'altra insidia comune è l'uso di codice bloccante non asincrono all'interno di una funzione <code translate="no">async</code> che inavvertitamente impedirà a Python di svolgere altre attività durante l'attesa.</p>
</div>
<p>Ora hai capito che nei programmi asincroni, per evitare che il processo principale venga bloccato, Python passa da una funzione all'altra non appena si verifica un'operazione bloccante. Ora ti starai chiedendo:</p>
<ul>
<li>
<p>Come fa Python a sfruttare <code translate="no">asyncio</code> per mettere in pausa e riprendere le funzioni?</p>
</li>
<li>
<p>Qual è il meccanismo utilizzato da <code translate="no">asyncio</code> di Python per passare da una funzione all'altra senza dimenticare quelle sospese?</p>
</li>
<li>
<p>Come si possono mettere in pausa o riprendere le funzioni senza perdere il loro stato?</p>
</li>
</ul>
<p>Per rispondere alle domande di cui sopra, approfondiamo i meccanismi alla base di <code translate="no">asyncio</code>, poiché la comprensione delle risposte a queste domande ti aiuterà notevolmente a eseguire il debug del codice async nei tuoi servizi.</p>
<p><a data-primary="event loop" data-type="indexterm" id="id808"/>Il cuore di <code translate="no">asyncio</code> è costituito da un oggetto di prima classe chiamato <em>ciclo di eventi</em>, responsabile della gestione efficiente degli eventi di I/O, degli eventi di sistema e dei cambiamenti di contesto dell'applicazione.</p>
<p><a data-type="xref" href="#event_loop">La Figura 5-4</a> mostra come il ciclo di eventi di <code translate="no">asyncio</code> intraprende l'orchestrazione dei task in Python.</p>
<figure><div class="figure" id="event_loop">
<img alt="bgai 0504" src="assets/bgai_0504.png" width="1076" height="470"/>
<h6><span class="label">Figura 5-4. </span>Ciclo di eventi IO asincrono</h6>
</div></figure>
<p>Il ciclo degli eventi può essere paragonato a un ciclo <code translate="no">while True</code> che osserva gli eventi o i messaggi emessi dalle <em>funzioni coroutine</em> all'interno del processo Python e distribuisce gli eventi per passare da una funzione all'altra in attesa del completamento delle operazioni di blocco I/O. Questa orchestrazione permette alle altre funzioni di essere eseguite in modo asincrono senza interruzioni.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id809">
<h1>Funzioni Coroutine</h1>
<p><a data-primary="coroutine functions" data-type="indexterm" id="id810"/>Le funzioni coroutine sono funzioni specializzate che restituiscono il controllo al chiamante senza perdere il loro stato (cioè possono essere messe in pausa e riprese in un secondo momento). Il meccanismo per restituire il controllo al ciclo degli eventi si basa sulle coroutine.</p>
<p>La capacità di mettere in pausa e riprendere le operazioni senza perdere lo stato rende le coroutine simili alle funzioni dei generatori. In effetti, era possibile implementare le coroutine utilizzando i generatori prima di Python 3.5, quando non esisteva un supporto nativo per le funzioni coroutine. Come per i generatori, la semplice chiamata di una funzione coroutine non la eseguirà.</p>
<p>Per eseguire le coroutine, dovrai utilizzare <code translate="no">asyncio.run()</code> o attendere tramite la parola chiave <code translate="no">await</code>in Python 3.5 o versioni successive.</p>
<p>Oltre alle coroutine, il pacchetto <code translate="no">asyncio</code>implementa anche altre primitive di concorrenza come i <em>futures</em>, le <em>semafori</em> e i <em>lock</em>.<a data-startref="ix_ch05-asciidoc9" data-type="indexterm" id="id811"/><a data-startref="ix_ch05-asciidoc8" data-type="indexterm" id="id812"/><a data-startref="ix_ch05-asciidoc7" data-type="indexterm" id="id813"/></p>
</div></aside>
</div></section>
<section data-pdf-bookmark="Async Programming with Model Provider APIs" data-type="sect2"><div class="sect2" id="id79">
<h2>Programmazione asincrona con le API dei fornitori di modelli</h2>
<p><a data-primary="asynchronous (async) I/O" data-secondary="async programming with model provider APIs" data-type="indexterm" id="ix_ch05-asciidoc10"/><a data-primary="concurrency" data-secondary="optimizing for I/O tasks with asynchronous programming" data-tertiary="async programming with model provider APIs" data-type="indexterm" id="ix_ch05-asciidoc11"/><a data-primary="model providers" data-secondary="async programming with model provider APIs" data-type="indexterm" id="ix_ch05-asciidoc12"/>Tutti e tre gli esempi che ti ho mostrato finora sono considerati come esempi "Hello World" di programmazione asincrona. Ora, analizziamo uno scenario reale legato alla creazione di servizi GenAI in cui è necessario utilizzare le API di un fornitore di modelli, come OpenAI, Anthropic o Mistral, poiché potrebbe essere più costoso servire LLMs<span class="keep-together">da soli.</span></p>
<p>Inoltre, se sottoponi a stress test gli endpoint di generazione creati nel <a data-type="xref" href="ch03.html#ch03">Capitolo 3</a> inviando più richieste in un breve lasso di tempo, noterai lunghi tempi di attesa prima che ogni richiesta venga elaborata. Questo perché hai precaricato e ospitato il modello nello stesso processo Python e nello stesso core della CPU su cui gira il server. Quando invii la prima richiesta, l'intero server si blocca mentre il carico di lavoro dell'inferenza viene completato. Poiché durante l'inferenza la CPU lavora al massimo delle sue possibilità, il processo di inferenza/generazione è un'operazione delimitata dalla CPU. Tuttavia, non è necessario che lo sia.</p>
<p>Quando utilizzi l'API di un provider, non devi più preoccuparti dei carichi di lavoro dell'intelligenza artificiale legati alla CPU, poiché questi diventano legati all'I/O. Pertanto, ha senso sapere come sfruttare la programmazione asincrona per interagire simultaneamente con l'API del provider del modello.</p>
<p>La buona notizia è che i proprietari delle API spesso rilasciano <em>client</em> e<em>kit di sviluppo software</em> (SDK) sia sincroni che asincroni per ridurre il lavoro necessario per interagire con i loro endpoint.</p>
<div data-type="caution"><h6>Attenzione</h6>
<p>Se hai bisogno di fare richieste ad altri servizi esterni, di recuperare dati dai database o di ingerire contenuti dai file, aggiungerai al processo altre attività bloccanti di I/O. Queste attività bloccanti possono costringere il server a rimanere in attesa se non sfrutti la programmazione asincrona.</p>
<p>Tuttavia, qualsiasi codice sincrono può essere reso asincrono utilizzando un <a href="https://oreil.ly/hIDNI">processo o un esecutore di pool di thread</a> per evitare di eseguire l'attività all'interno del ciclo di eventi. Invece, si esegue l'attività asincrona su un processo o un thread separato per evitare di bloccare il ciclo di eventi.</p>
<p>Puoi anche verificare l'eventuale supporto asincrono controllando la documentazione della libreria o il codice sorgente alla ricerca di menzioni delle parole chiave <code translate="no">async</code> o <code translate="no">await</code>. Altrimenti, puoi provare a verificare se lo strumento può essere utilizzato all'interno di una funzione asincrona senza sollevare un <code translate="no">TypeError</code> quando usi <code translate="no">await</code> su di essa.</p>
<p>Se uno strumento, come ad esempio una libreria di database, ha solo un'implementazione sincrona, allora non puoi implementare l'asincronia con quello strumento. La soluzione sarà quella di passare lo strumento a un equivalente asincrono in modo da poterlo utilizzare con le<span class="keep-together">parole chiave</span><code translate="no">async</code> e <code translate="no">await</code>.</p>
</div>
<p>Nell'<a data-type="xref" href="#openai_clients">Esempio 5-4</a>, interagisci con l'API OpenAI GPT-3.5 tramite client OpenAI sia sincroni che asincroni per capire la differenza di prestazioni tra i due.</p>
<div data-type="note" epub:type="note"><h6>Nota</h6>
<p>È necessario installare la libreria <code translate="no">openai</code>:</p>
<pre data-code-language="bash" data-type="programlisting" translate="no">$<code class="w" translate="no"> </code>pip<code class="w" translate="no"> </code>install<code class="w" translate="no"> </code>openai<code class="w" translate="no"/></pre>
</div>
<div data-type="example" id="openai_clients">
<h5><span class="label">Esempio 5-4. </span>Confronto tra client OpenAI sincrono e asincrono</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="kn" translate="no">import</code> <code class="nn" translate="no">os</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">fastapi</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">FastAPI</code><code class="p" translate="no">,</code> <code class="n" translate="no">Body</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">openai</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">OpenAI</code><code class="p" translate="no">,</code> <code class="n" translate="no">AsyncOpenAI</code>

<code class="n" translate="no">app</code> <code class="o" translate="no">=</code> <code class="n" translate="no">FastAPI</code><code class="p" translate="no">()</code>

<code class="n" translate="no">sync_client</code> <code class="o" translate="no">=</code> <code class="n" translate="no">OpenAI</code><code class="p" translate="no">(</code><code class="n" translate="no">api_key</code><code class="o" translate="no">=</code><code class="n" translate="no">os</code><code class="o" translate="no">.</code><code class="n" translate="no">environ</code><code class="o" translate="no">.</code><code class="n" translate="no">get</code><code class="p" translate="no">(</code><code class="s2" translate="no">"OPENAI_API_KEY"</code><code class="p" translate="no">))</code>
<code class="n" translate="no">async_client</code> <code class="o" translate="no">=</code> <code class="n" translate="no">AsyncOpenAI</code><code class="p" translate="no">(</code><code class="n" translate="no">api_key</code><code class="o" translate="no">=</code><code class="n" translate="no">os</code><code class="o" translate="no">.</code><code class="n" translate="no">environ</code><code class="o" translate="no">.</code><code class="n" translate="no">get</code><code class="p" translate="no">(</code><code class="s2" translate="no">"OPENAI_API_KEY"</code><code class="p" translate="no">))</code>

<code class="nd" translate="no">@app</code><code class="o" translate="no">.</code><code class="n" translate="no">post</code><code class="p" translate="no">(</code><code class="s2" translate="no">"/sync"</code><code class="p" translate="no">)</code>
<code class="k" translate="no">def</code> <code class="nf" translate="no">sync_generate_text</code><code class="p" translate="no">(</code><code class="n" translate="no">prompt</code><code class="p" translate="no">:</code> <code class="nb" translate="no">str</code> <code class="o" translate="no">=</code> <code class="n" translate="no">Body</code><code class="p" translate="no">(</code><code class="o" translate="no">...</code><code class="p" translate="no">)):</code>
    <code class="n" translate="no">completion</code> <code class="o" translate="no">=</code> <code class="n" translate="no">sync_client</code><code class="o" translate="no">.</code><code class="n" translate="no">chat</code><code class="o" translate="no">.</code><code class="n" translate="no">completions</code><code class="o" translate="no">.</code><code class="n" translate="no">create</code><code class="p" translate="no">(</code>
        <code class="n" translate="no">messages</code><code class="o" translate="no">=</code><code class="p" translate="no">[</code>
            <code class="p" translate="no">{</code>
                <code class="s2" translate="no">"role"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"user"</code><code class="p" translate="no">,</code>
                <code class="s2" translate="no">"content"</code><code class="p" translate="no">:</code> <code class="n" translate="no">prompt</code><code class="p" translate="no">,</code>
            <code class="p" translate="no">}</code>
        <code class="p" translate="no">],</code>
        <code class="n" translate="no">model</code><code class="o" translate="no">=</code><code class="s2" translate="no">"gpt-3.5-turbo"</code><code class="p" translate="no">,</code>
    <code class="p" translate="no">)</code>
    <code class="k" translate="no">return</code> <code class="n" translate="no">completion</code><code class="o" translate="no">.</code><code class="n" translate="no">choices</code><code class="p" translate="no">[</code><code class="mi" translate="no">0</code><code class="p" translate="no">]</code><code class="o" translate="no">.</code><code class="n" translate="no">message</code><code class="o" translate="no">.</code><code class="n" translate="no">content</code>

<code class="nd" translate="no">@app</code><code class="o" translate="no">.</code><code class="n" translate="no">post</code><code class="p" translate="no">(</code><code class="s2" translate="no">"/async"</code><code class="p" translate="no">)</code>
<code class="k" translate="no">async</code> <code class="k" translate="no">def</code> <code class="nf" translate="no">async_generate_text</code><code class="p" translate="no">(</code><code class="n" translate="no">prompt</code><code class="p" translate="no">:</code> <code class="nb" translate="no">str</code> <code class="o" translate="no">=</code> <code class="n" translate="no">Body</code><code class="p" translate="no">(</code><code class="o" translate="no">...</code><code class="p" translate="no">)):</code>
    <code class="n" translate="no">completion</code> <code class="o" translate="no">=</code> <code class="k" translate="no">await</code> <code class="n" translate="no">async_client</code><code class="o" translate="no">.</code><code class="n" translate="no">chat</code><code class="o" translate="no">.</code><code class="n" translate="no">completions</code><code class="o" translate="no">.</code><code class="n" translate="no">create</code><code class="p" translate="no">(</code>
        <code class="n" translate="no">messages</code><code class="o" translate="no">=</code><code class="p" translate="no">[</code>
            <code class="p" translate="no">{</code>
                <code class="s2" translate="no">"role"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"user"</code><code class="p" translate="no">,</code>
                <code class="s2" translate="no">"content"</code><code class="p" translate="no">:</code> <code class="n" translate="no">prompt</code><code class="p" translate="no">,</code>
            <code class="p" translate="no">}</code>
        <code class="p" translate="no">],</code>
        <code class="n" translate="no">model</code><code class="o" translate="no">=</code><code class="s2" translate="no">"gpt-3.5-turbo"</code><code class="p" translate="no">,</code>
    <code class="p" translate="no">)</code>
    <code class="k" translate="no">return</code> <code class="n" translate="no">completion</code><code class="o" translate="no">.</code><code class="n" translate="no">choices</code><code class="p" translate="no">[</code><code class="mi" translate="no">0</code><code class="p" translate="no">]</code><code class="o" translate="no">.</code><code class="n" translate="no">message</code><code class="o" translate="no">.</code><code class="n" translate="no">content</code></pre></div>
<p>La differenza tra i client sync e async è che con la versione async,<span class="keep-together">FastAPI</span> può iniziare a elaborare gli input dell'utente in parallelo senza attendere la risposta dell'API OpenAI per l'input precedente.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id814">
<h1>Gestione dei limiti tariffari</h1>
<p><a data-primary="rate limiting" data-secondary="managing rate limits" data-type="indexterm" id="id815"/>Tieni presente che le richieste simultanee alle API esterne dovranno essere limitate per rimanere entro i limiti di velocità.</p>
<p><a data-primary="exponential backoff" data-type="indexterm" id="id816"/>Una strategia sensata è quella di implementare un <em>backoff esponenziale</em> in cui il tuo servizio ritarda il raggiungimento delle API esterne in base a una curva di ritardo esponenziale: più ricevi errori di limitazione della velocità dalle API esterne, più il tuo servizio aspetterà prima di riprendere le richieste.</p>
<p>Puoi richiedere di aumentare i limiti di attesa o chiedere agli utenti di aspettare prima di poter riprovare.</p>
<p>I pacchetti Python di terze parti, come <code translate="no">stamina</code>, possono aiutare a implementare una strategia di limitazione della velocità con API esterne.<sup><a data-type="noteref" href="ch05.html#id817" id="id817-marker" translate="no">3</a></sup></p>
</div></aside>
<p>Sfruttando il codice asincrono, puoi ottenere un enorme aumento del throughput e scalare un volume maggiore di richieste simultanee. Tuttavia, devi fare attenzione quando scrivi codice asincrono (async).</p>
<p>Ecco alcune insidie e problemi comuni che potresti incontrare con il codice async:</p>
<ul>
<li>
<p>La comprensione e il debug degli errori possono essere più complessi a causa del flusso di esecuzione non lineare dei task concorrenti.</p>
</li>
<li>
<p>Alcune librerie, come <code translate="no">aiohttp</code>, richiedono gestori di contesto asincroni nidificati per una corretta implementazione. Questo può creare confusione molto velocemente.</p>
</li>
<li>
<p>Mischiare codice asincrono e sincrono può annullare i vantaggi in termini di prestazioni, ad esempio se dimentichi di contrassegnare le funzioni con le parole chiave <code translate="no">async</code> e <code translate="no">await</code>.</p>
</li>
<li>
<p>Il mancato utilizzo di strumenti e librerie compatibili con l'asincronia può anche annullare i vantaggi in termini di prestazioni; ad esempio, utilizzare il pacchetto <code translate="no">requests</code> invece di <code translate="no">aiohttp</code> per effettuare chiamate API asincrone.</p>
</li>
<li>
<p>Dimenticare di attendere le coroutine all'interno di una funzione asincrona o attendere le non-coroutine può portare a un comportamento inaspettato. Tutte le parole chiave <code translate="no">async</code> devono essere seguite da un <code translate="no">await</code>.</p>
</li>
<li>
<p>Una gestione non corretta delle risorse (ad esempio, connessioni API/database aperte o buffer di file) può causare perdite di memoria che bloccano il computer. Puoi anche perdere memoria se non limiti il numero di operazioni concorrenti nel codice async.</p>
</li>
<li>
<p>Potresti anche imbatterti in problemi di concorrenza e di condizioni di gara in cui il principio di thread-safety viene violato, causando deadlock sulle risorse con conseguente<span class="keep-together">corruzione</span> dei dati.</p>
</li>
</ul>
<p>Questo elenco non è esaustivo e, come puoi vedere, ci sono diverse insidie nell'utilizzo della programmazione asincrona. Per questo motivo, ti consiglio di iniziare a scrivere programmi sincroni per capire il flusso e la logica di base del tuo codice, prima di affrontare le complessità della migrazione a un'implementazione asincrona.<a data-startref="ix_ch05-asciidoc12" data-type="indexterm" id="id818"/><a data-startref="ix_ch05-asciidoc11" data-type="indexterm" id="id819"/><a data-startref="ix_ch05-asciidoc10" data-type="indexterm" id="id820"/></p>
</div></section>
<section data-pdf-bookmark="Event Loop and Thread Pool in FastAPI" data-type="sect2"><div class="sect2" id="id80">
<h2>Ciclo di eventi e pool di thread in FastAPI</h2>
<p><a data-primary="concurrency" data-secondary="optimizing for I/O tasks with asynchronous programming" data-tertiary="event loop and thread pool in FastAPI" data-type="indexterm" id="ix_ch05-asciidoc13"/><a data-primary="event loop" data-type="indexterm" id="ix_ch05-asciidoc14"/><a data-primary="FastAPI (basics)" data-secondary="event loop and thread pool" data-type="indexterm" id="ix_ch05-asciidoc15"/><a data-primary="thread pool" data-secondary="in FastAPI" data-type="indexterm" id="ix_ch05-asciidoc16"/>FastAPI è in grado di gestire operazioni di blocco sia asincrone che sincrone, eseguendo i gestori di sincronizzazione nel suo <em>pool di thread</em> in modo che le operazioni di blocco non interrompano l'esecuzione dei compiti del <em>ciclo degli eventi</em>.</p>
<p><a data-primary="Asynchronous Server Gateway Interface (ASGI)" data-secondary="thread pool handling" data-type="indexterm" id="id821"/>Come ho detto nel <a data-type="xref" href="ch02.html#ch02">Capitolo 2</a>, FastAPI funziona con il framework web ASGI tramite Starlette. Se non fosse così, il server verrebbe eseguito in modo sincrono, quindi dovresti aspettare che ogni processo finisca prima di poter servire il successivo. Tuttavia, utilizzando ASGI, il server FastAPI supporta la concorrenza sia tramite il multithreading (tramite un pool di thread) che la programmazione asincrona (tramite un ciclo di eventi) per servire più richieste in parallelo, evitando che il processo principale del server venga bloccato.</p>
<p>FastAPI crea il pool di thread istanziando una collezione di thread all'avvio dell'applicazione per ridurre l'onere della creazione di thread in runtime.<sup><a data-type="noteref" href="ch05.html#id822" id="id822-marker" translate="no">4</a></sup>
In seguito, delega le attività in background e i carichi di lavoro sincroni al pool di thread per evitare che il ciclo degli eventi venga bloccato da operazioni bloccanti all'interno dei gestori sincroni. Il ciclo degli eventi è anche indicato come il thread principale del server FastAPI, responsabile dell'elaborazione delle richieste.</p>
<p>Come ho già detto, il ciclo degli eventi è il componente centrale di ogni applicazione costruita su <code translate="no">asyncio</code>, compresa FastAPI che implementa la concurrency. I cicli degli eventi eseguono attività asincrone e callback, tra cui l'esecuzione di operazioni di I/O di rete e l'esecuzione di sottoprocessi. In FastAPI, il ciclo degli eventi è anche responsabile dell'orchestrazione dell'elaborazione asincrona delle richieste.</p>
<p>Se possibile, dovresti eseguire i gestori nel ciclo degli eventi (tramite la programmazione asincrona) perché può essere ancora più efficiente che eseguirli nel pool di thread (tramite il multithreading). Questo perché ogni thread nel pool di thread deve acquisire il GIL prima di poter eseguire qualsiasi byte di codice e questo richiede un certo sforzo computazionale.</p>
<p>Immagina che più utenti simultanei utilizzino gli handler (endpoint) OpenAI GPT-3.5 sincroni e asincroni del tuo servizio FastAPI, come mostrato nell'<a data-type="xref" href="#openai_clients">Esempio 5-4</a>. FastAPI eseguirà le richieste dell'handler asincrono nel ciclo degli eventi, poiché tale handler utilizza un client OpenAI asincrono non bloccante.
D'altra parte, FastAPI deve delegare le richieste di handler sincroni al pool di thread per proteggere il loop di eventi dal blocco. Poiché delegare le richieste (ai thread) e passare da un thread all'altro in un pool di thread comporta un lavoro maggiore, le richieste sincrone termineranno più tardi rispetto alle loro controparti asincrone.</p>
<div data-type="note" epub:type="note"><h6>Nota</h6>
<p>Ricorda che tutto questo lavoro - l'elaborazione delle richieste di handler sincroni e asincroni - viene eseguito su un singolo core della CPU all'interno dello stesso processo FastAPI Python.</p>
<p>Questo per ridurre al minimo i tempi di inattività della CPU in attesa delle risposte dell'API OpenAI.</p>
</div>
<p>Le differenze di prestazioni sono mostrate nella <a data-type="xref" href="#multithreading_vs_async">Figura 5-5</a>.</p>
<figure><div class="figure" id="multithreading_vs_async">
<img alt="bgai 0505" src="assets/bgai_0505.png" width="1190" height="1340"/>
<h6><span class="label">Figura 5-5. </span>Come il multithreading e l'Async IO gestiscono le operazioni di blocco dell'I/O</h6>
</div></figure>
<p>La<a data-type="xref" href="#multithreading_vs_async">Figura 5-5</a> mostra che con i carichi di lavoro delimitati dall'I/O, le implementazioni asincrone sono più veloci e dovrebbero essere il metodo da preferire se hai bisogno di concorrenza. Tuttavia, FastAPI fa comunque un buon lavoro nel servire più richieste simultanee anche se deve lavorare con un client OpenAI sincrono. Invia semplicemente le chiamate API sincrone all'interno dei thread del pool di thread per implementare una forma di concorrenza per l'utente. Ecco perché la documentazione ufficiale di FastAPI ti dice di non preoccuparti troppo di dichiarare le tue funzioni handler come <code translate="no">async def</code> o <code translate="no">def</code>.</p>
<p>Tuttavia, tieni presente che quando dichiari i gestori con <code translate="no">async def</code>, FastAPI si fida di eseguire solo operazioni non bloccanti. Quando rompi questa fiducia ed esegui operazioni bloccanti all'interno delle rotte <code translate="no">async</code>, il ciclo dell'evento sarà bloccato e non potrà più continuare a eseguire attività fino a quando l'operazione bloccante<a data-startref="ix_ch05-asciidoc16" data-type="indexterm" id="id823"/><a data-startref="ix_ch05-asciidoc15" data-type="indexterm" id="id824"/><a data-startref="ix_ch05-asciidoc14" data-type="indexterm" id="id825"/><a data-startref="ix_ch05-asciidoc13" data-type="indexterm" id="id826"/> non sarà<span class="keep-together">terminata</span>.</p>
</div></section>
<section data-pdf-bookmark="Blocking the Main Server" data-type="sect2"><div class="sect2" id="id81">
<h2>Blocco del server principale</h2>
<p><a data-primary="concurrency" data-secondary="optimizing for I/O tasks with asynchronous programming" data-tertiary="blocking the main server" data-type="indexterm" id="ix_ch05-asciidoc17"/>Se usi la parola chiave <code translate="no">async</code> quando definisci le tue funzioni, assicurati di usare anche la parola chiave <code translate="no">await</code> da qualche parte all'interno della funzione e che nessuna delle dipendenze del pacchetto che usi all'interno della funzione sia sincrona.</p>
<p>Evita di dichiarare le funzioni dei gestori di rotte come <code translate="no">async</code> se la loro implementazione è sincrona. In caso contrario, le richieste ai gestori di rotte interessati bloccheranno il server principale dall'elaborazione di altre richieste mentre il server attende il completamento dell'operazione bloccante. Non importa se l'operazione bloccante è legata all'I/O o al calcolo. Pertanto, qualsiasi chiamata ai database o ai modelli di intelligenza artificiale può comunque causare il blocco se non stai attento.</p>
<p>Questo è un errore facile da commettere. Ad esempio, puoi usare una dipendenza sincrona all'interno di gestori che hai dichiarato asincroni, come mostrato nell'<a data-type="xref" href="#blocking_main_thread">Esempio 5-5</a>.</p>
<div data-type="example" id="blocking_main_thread">
<h5><span class="label">Esempio 5-5. </span>Implementazione errata dei gestori asincroni in FastAPI</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="kn" translate="no">import</code><code translate="no"> </code><code class="nn" translate="no">os</code><code translate="no">
</code><code class="kn" translate="no">from</code><code translate="no"> </code><code class="nn" translate="no">fastapi</code><code translate="no"> </code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="n" translate="no">FastAPI</code><code translate="no">
</code><code class="kn" translate="no">from</code><code translate="no"> </code><code class="nn" translate="no">openai</code><code translate="no"> </code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="n" translate="no">AsyncOpenAI</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">OpenAI</code><code translate="no">
</code><code translate="no">
</code><code class="n" translate="no">app</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">FastAPI</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">
</code><code class="nd" translate="no">@app</code><code class="o" translate="no">.</code><code class="n" translate="no">get</code><code class="p" translate="no">(</code><code class="s2" translate="no">"</code><code class="s2" translate="no">/block</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code translate="no">
</code><code class="k" translate="no">async</code><code translate="no"> </code><code class="k" translate="no">def</code><code translate="no"> </code><code class="nf" translate="no">block_server_controller</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">completion</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">sync_client</code><code class="o" translate="no">.</code><code class="n" translate="no">chat</code><code class="o" translate="no">.</code><code class="n" translate="no">completions</code><code class="o" translate="no">.</code><code class="n" translate="no">create</code><code class="p" translate="no">(</code><code class="o" translate="no">.</code><code class="o" translate="no">.</code><code class="o" translate="no">.</code><code class="p" translate="no">)</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO4-1" id="co_achieving_concurrency_in_ai_workloads_CO4-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">return</code><code translate="no"> </code><code class="n" translate="no">completion</code><code class="o" translate="no">.</code><code class="n" translate="no">choices</code><code class="p" translate="no">[</code><code class="mi" translate="no">0</code><code class="p" translate="no">]</code><code class="o" translate="no">.</code><code class="n" translate="no">message</code><code class="o" translate="no">.</code><code class="n" translate="no">content</code><code translate="no">
</code><code translate="no">
</code><code class="nd" translate="no">@app</code><code class="o" translate="no">.</code><code class="n" translate="no">get</code><code class="p" translate="no">(</code><code class="s2" translate="no">"</code><code class="s2" translate="no">/slow</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code translate="no">
</code><code class="k" translate="no">def</code><code translate="no"> </code><code class="nf" translate="no">slow_text_generator</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">completion</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">sync_client</code><code class="o" translate="no">.</code><code class="n" translate="no">chat</code><code class="o" translate="no">.</code><code class="n" translate="no">completions</code><code class="o" translate="no">.</code><code class="n" translate="no">create</code><code class="p" translate="no">(</code><code class="o" translate="no">.</code><code class="o" translate="no">.</code><code class="o" translate="no">.</code><code class="p" translate="no">)</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO4-2" id="co_achieving_concurrency_in_ai_workloads_CO4-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">return</code><code translate="no"> </code><code class="n" translate="no">completion</code><code class="o" translate="no">.</code><code class="n" translate="no">choices</code><code class="p" translate="no">[</code><code class="mi" translate="no">0</code><code class="p" translate="no">]</code><code class="o" translate="no">.</code><code class="n" translate="no">message</code><code class="o" translate="no">.</code><code class="n" translate="no">content</code><code translate="no">
</code><code translate="no">
</code><code class="nd" translate="no">@app</code><code class="o" translate="no">.</code><code class="n" translate="no">get</code><code class="p" translate="no">(</code><code class="s2" translate="no">"</code><code class="s2" translate="no">/fast</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code translate="no">
</code><code class="k" translate="no">async</code><code translate="no"> </code><code class="k" translate="no">def</code><code translate="no"> </code><code class="nf" translate="no">fast_text_generator</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">completion</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="k" translate="no">await</code><code translate="no"> </code><code class="n" translate="no">async_client</code><code class="o" translate="no">.</code><code class="n" translate="no">chat</code><code class="o" translate="no">.</code><code class="n" translate="no">completions</code><code class="o" translate="no">.</code><code class="n" translate="no">create</code><code class="p" translate="no">(</code><code class="o" translate="no">.</code><code class="o" translate="no">.</code><code class="o" translate="no">.</code><code class="p" translate="no">)</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO4-3" id="co_achieving_concurrency_in_ai_workloads_CO4-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">return</code><code translate="no"> </code><code class="n" translate="no">completion</code><code class="o" translate="no">.</code><code class="n" translate="no">choices</code><code class="p" translate="no">[</code><code class="mi" translate="no">0</code><code class="p" translate="no">]</code><code class="o" translate="no">.</code><code class="n" translate="no">message</code><code class="o" translate="no">.</code><code class="n" translate="no">content</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO4-1" id="callout_achieving_concurrency_in_ai_workloads_CO4-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Operazione di I/O bloccante per ottenere la risposta dell'API ChatGPT. Poiché il gestore della rotta è contrassegnato come asincrono, FastAPI si fida di noi per non eseguire operazioni bloccanti, ma poiché lo siamo, la richiesta bloccherà il ciclo degli eventi (thread principale del server). Altre richieste sono ora bloccate fino a quando la richiesta corrente non viene elaborata.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO4-2" id="callout_achieving_concurrency_in_ai_workloads_CO4-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a></dt>
<dd><p>Un semplice gestore di rotte sincrone con operazioni bloccanti che non sfrutta le funzionalità asincrone. Le richieste di sincronizzazione vengono affidate al pool di thread per essere eseguite in background, in modo da non bloccare il server principale.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO4-3" id="callout_achieving_concurrency_in_ai_workloads_CO4-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a></dt>
<dd><p>Un percorso asincrono non bloccante.</p></dd>
</dl></div>
<p>La richiesta non blocca il thread principale e non deve essere trasferita al pool di thread. Di conseguenza, il ciclo di eventi FastAPI può elaborare la richiesta molto più velocemente utilizzando il client OpenAI async.</p>
<p>Ora dovresti sentirti più a tuo agio nell'implementare nuove funzionalità nel tuo servizio FastAPI che richiedono l'esecuzione di compiti delimitati da I/O.</p>
<p>Per aiutarti a consolidare la comprensione dei concetti di I/O concurrency, nelle prossime sezioni costruirai diverse nuove funzionalità che utilizzano la concurrency nel tuo servizio FastAPI. Queste funzionalità includono:</p>
<dl>
<dt>Parla con il web</dt>
<dd>
<p>Costruisci e integra un modulo web scraper che ti permette di porre domande al tuo LLM self-hosted sul contenuto di un sito web fornendo un URL HTTP.</p>
</dd>
<dt>Parla con i documenti</dt>
<dd>
<p>Costruisci e integra un modulo RAG per elaborare i documenti in un database vettoriale. Un database vettoriale memorizza i dati in modo da supportare ricerche di similarità efficienti. Puoi quindi utilizzare la ricerca semantica, che comprende il significato delle query, per interagire con i documenti caricati utilizzando il tuo LLM.</p>
</dd>
</dl>
<p>Entrambi i progetti ti faranno fare esperienza pratica di interazione asincrona con sistemi esterni come siti web, database e filesystem.<a data-startref="ix_ch05-asciidoc17" data-type="indexterm" id="id827"/></p>
</div></section>
<section class="less_space pagebreak-before" data-pdf-bookmark="Project: Talk to the Web (Web Scraper)" data-type="sect2"><div class="sect2" id="id82">
<h2>Progetto: Parla con il web (Web Scraper)</h2>
<p><a data-primary="concurrency" data-secondary="optimizing for I/O tasks with asynchronous programming" data-tertiary="project: web scraper" data-type="indexterm" id="ix_ch05-asciidoc18"/><a data-primary="web scraper (async programming project)" data-type="indexterm" id="ix_ch05-asciidoc19"/>Le aziende spesso ospitano una serie di pagine web interne per manuali, processi e altra documentazione sotto forma di pagine HTML. Per le pagine più lunghe, i tuoi utenti potrebbero voler fornire degli URL quando pongono delle domande e aspettarsi che LLM recuperi e legga il contenuto. È qui che può essere utile avere un web scraper integrato.</p>
<p>Ci sono molti modi per creare un web scraper per il tuo LLM self-hosted. A seconda del tuo caso d'uso, puoi utilizzare una combinazione dei seguenti metodi:</p>
<ul>
<li>
<p>Recupera le pagine web come HTML e invia il contenuto HTML grezzo (o il testo interno) al tuo LLM per analizzare il contenuto nel formato desiderato.</p>
</li>
<li>
<p>Utilizza <em>framework di web scraping</em> come <code translate="no">BeautifulSoup</code> e <code translate="no">ScraPy</code> per analizzare il contenuto delle pagine web dopo averle recuperate.</p>
</li>
<li>
<p>Usa i <em>browser web headless</em> come Selenium e Microsoft Playwright per navigare dinamicamente tra i nodi delle pagine e analizzare i contenuti. I browser headless sono ottimi per la navigazione di applicazioni a pagina singola (SPA).</p>
</li>
</ul>
<div data-type="caution"><h6>Attenzione</h6>
<p>Tu o i tuoi utenti dovreste evitare gli strumenti di web scraping alimentati da LLM per scopi illegali. Assicurati di avere l'autorizzazione prima di estrarre contenuti dagli URL:</p>
<ul>
<li>
<p>Esamina le condizioni d'uso di ogni sito web, soprattutto se si parla di web scraping.</p>
</li>
<li>
<p>Usa le API quando è possibile.</p>
</li>
<li>
<p>Se non sei sicuro, chiedi direttamente il permesso ai proprietari dei siti web.</p>
</li>
</ul>
</div>
<p>Per questo mini-progetto, ci limiteremo a recuperare e inviare al nostro LLM il testo interno grezzo delle pagine HTML, poiché l'implementazione di uno scraper pronto per la produzione può diventare un libro a sé stante.</p>
<p>Il processo di costruzione di un semplice scraper asincrono è il seguente:</p>
<ol>
<li>
<p>Sviluppa una funzione per abbinare i modelli di URL usando la regex sui prompt degli utenti al LLM.</p>
</li>
<li>
<p>Se viene trovata, esegue un ciclo sull'elenco degli URL forniti e recupera le pagine in modo asincrono. Utilizzeremo una libreria HTTP asincrona chiamata <code translate="no">aiohttp</code> invece di <code translate="no">requests</code>poiché <code translate="no">requests</code> può effettuare solo richieste di rete sincrone.</p>
</li>
<li>
<p>Sviluppa una funzione di parsing per estrarre il contenuto testuale dall'HTML recuperato.</p>
</li>
<li>
<p>Il contenuto della pagina analizzata viene inviato al LLM insieme al prompt originale dell'utente.</p>
</li>
</ol>
<p>L<a data-type="xref" href="#web_scraper">'esempio 5-6</a> mostra come puoi implementare i passaggi sopra citati.</p>
<div data-type="note" epub:type="note"><h6>Nota</h6>
<p>Per eseguire questo esempio è necessario installare alcune dipendenze aggiuntive:</p>
<pre data-code-language="bash" data-type="programlisting" translate="no">$<code class="w" translate="no"> </code>pip<code class="w" translate="no"> </code>install<code class="w" translate="no"> </code>beautifulsoup<code class="w" translate="no"> </code>lxml<code class="w" translate="no"> </code>aiohttp<code class="w" translate="no"/></pre>
</div>
<div data-type="example" id="web_scraper">
<h5><span class="label">Esempio 5-6. </span>Creazione di uno scraper web asincrono</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="c1" translate="no"># scraper.py</code><code translate="no">
</code><code translate="no">
</code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="nn" translate="no">asyncio</code><code translate="no">
</code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="nn" translate="no">re</code><code translate="no">
</code><code translate="no">
</code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="nn" translate="no">aiohttp</code><code translate="no">
</code><code class="kn" translate="no">from</code><code translate="no"> </code><code class="nn" translate="no">bs4</code><code translate="no"> </code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="n" translate="no">BeautifulSoup</code><code translate="no">
</code><code class="kn" translate="no">from</code><code translate="no"> </code><code class="nn" translate="no">loguru</code><code translate="no"> </code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="n" translate="no">logger</code><code translate="no">
</code><code translate="no">
</code><code class="k" translate="no">def</code><code translate="no"> </code><code class="nf" translate="no">extract_urls</code><code class="p" translate="no">(</code><code class="n" translate="no">text</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="nb" translate="no">str</code><code class="p" translate="no">)</code><code translate="no"> </code><code class="o" translate="no">-</code><code class="o" translate="no">&gt;</code><code translate="no"> </code><code class="nb" translate="no">list</code><code class="p" translate="no">[</code><code class="nb" translate="no">str</code><code class="p" translate="no">]</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">url_pattern</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="sa" translate="no">r</code><code class="s2" translate="no">"</code><code class="s2" translate="no">(?P&lt;url&gt;https?:</code><code class="s2" translate="no">\</code><code class="s2" translate="no">/</code><code class="s2" translate="no">\</code><code class="s2" translate="no">/[^</code><code class="s2" translate="no">\</code><code class="s2" translate="no">s]+)</code><code class="s2" translate="no">"</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO5-1" id="co_achieving_concurrency_in_ai_workloads_CO5-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">urls</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">re</code><code class="o" translate="no">.</code><code class="n" translate="no">findall</code><code class="p" translate="no">(</code><code class="n" translate="no">url_pattern</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">text</code><code class="p" translate="no">)</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO5-2" id="co_achieving_concurrency_in_ai_workloads_CO5-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">return</code><code translate="no"> </code><code class="n" translate="no">urls</code><code translate="no">
</code><code translate="no">
</code><code translate="no">
</code><code class="k" translate="no">def</code><code translate="no"> </code><code class="nf" translate="no">parse_inner_text</code><code class="p" translate="no">(</code><code class="n" translate="no">html_string</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="nb" translate="no">str</code><code class="p" translate="no">)</code><code translate="no"> </code><code class="o" translate="no">-</code><code class="o" translate="no">&gt;</code><code translate="no"> </code><code class="nb" translate="no">str</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">soup</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">BeautifulSoup</code><code class="p" translate="no">(</code><code class="n" translate="no">html_string</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="s2" translate="no">"</code><code class="s2" translate="no">lxml</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">if</code><code translate="no"> </code><code class="n" translate="no">content</code><code translate="no"> </code><code class="o" translate="no">:=</code><code translate="no"> </code><code class="n" translate="no">soup</code><code class="o" translate="no">.</code><code class="n" translate="no">find</code><code class="p" translate="no">(</code><code class="s2" translate="no">"</code><code class="s2" translate="no">div</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="nb" translate="no">id</code><code class="o" translate="no">=</code><code class="s2" translate="no">"</code><code class="s2" translate="no">bodyContent</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code class="p" translate="no">:</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO5-3" id="co_achieving_concurrency_in_ai_workloads_CO5-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">        </code><code class="k" translate="no">return</code><code translate="no"> </code><code class="n" translate="no">content</code><code class="o" translate="no">.</code><code class="n" translate="no">get_text</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">logger</code><code class="o" translate="no">.</code><code class="n" translate="no">warning</code><code class="p" translate="no">(</code><code class="s2" translate="no">"</code><code class="s2" translate="no">Could not parse the HTML content</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">return</code><code translate="no"> </code><code class="s2" translate="no">"</code><code class="s2" translate="no">"</code><code translate="no">
</code><code translate="no">
</code><code translate="no">
</code><code class="k" translate="no">async</code><code translate="no"> </code><code class="k" translate="no">def</code><code translate="no"> </code><code class="nf" translate="no">fetch</code><code class="p" translate="no">(</code><code class="n" translate="no">session</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="n" translate="no">aiohttp</code><code class="o" translate="no">.</code><code class="n" translate="no">ClientSession</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">url</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="nb" translate="no">str</code><code class="p" translate="no">)</code><code translate="no"> </code><code class="o" translate="no">-</code><code class="o" translate="no">&gt;</code><code translate="no"> </code><code class="nb" translate="no">str</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">async</code><code translate="no"> </code><code class="k" translate="no">with</code><code translate="no"> </code><code class="n" translate="no">session</code><code class="o" translate="no">.</code><code class="n" translate="no">get</code><code class="p" translate="no">(</code><code class="n" translate="no">url</code><code class="p" translate="no">)</code><code translate="no"> </code><code class="k" translate="no">as</code><code translate="no"> </code><code class="n" translate="no">response</code><code class="p" translate="no">:</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO5-4" id="co_achieving_concurrency_in_ai_workloads_CO5-4"><img alt="4" src="assets/4.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">        </code><code class="n" translate="no">html_string</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="k" translate="no">await</code><code translate="no"> </code><code class="n" translate="no">response</code><code class="o" translate="no">.</code><code class="n" translate="no">text</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">        </code><code class="k" translate="no">return</code><code translate="no"> </code><code class="n" translate="no">parse_inner_text</code><code class="p" translate="no">(</code><code class="n" translate="no">html_string</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">
</code><code translate="no">
</code><code class="k" translate="no">async</code><code translate="no"> </code><code class="k" translate="no">def</code><code translate="no"> </code><code class="nf" translate="no">fetch_all</code><code class="p" translate="no">(</code><code class="n" translate="no">urls</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="nb" translate="no">list</code><code class="p" translate="no">[</code><code class="nb" translate="no">str</code><code class="p" translate="no">]</code><code class="p" translate="no">)</code><code translate="no"> </code><code class="o" translate="no">-</code><code class="o" translate="no">&gt;</code><code translate="no"> </code><code class="nb" translate="no">str</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">async</code><code translate="no"> </code><code class="k" translate="no">with</code><code translate="no"> </code><code class="n" translate="no">aiohttp</code><code class="o" translate="no">.</code><code class="n" translate="no">ClientSession</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code translate="no"> </code><code class="k" translate="no">as</code><code translate="no"> </code><code class="n" translate="no">session</code><code class="p" translate="no">:</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO5-5" id="co_achieving_concurrency_in_ai_workloads_CO5-5"><img alt="5" src="assets/5.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">        </code><code class="n" translate="no">results</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="k" translate="no">await</code><code translate="no"> </code><code class="n" translate="no">asyncio</code><code class="o" translate="no">.</code><code class="n" translate="no">gather</code><code class="p" translate="no">(</code><code translate="no">
</code><code translate="no">            </code><code class="o" translate="no">*</code><code class="p" translate="no">[</code><code class="n" translate="no">fetch</code><code class="p" translate="no">(</code><code class="n" translate="no">session</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">url</code><code class="p" translate="no">)</code><code translate="no"> </code><code class="k" translate="no">for</code><code translate="no"> </code><code class="n" translate="no">url</code><code translate="no"> </code><code class="ow" translate="no">in</code><code translate="no"> </code><code class="n" translate="no">urls</code><code class="p" translate="no">]</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">return_exceptions</code><code class="o" translate="no">=</code><code class="kc" translate="no">True</code><code translate="no">
</code><code translate="no">        </code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">success_results</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="p" translate="no">[</code><code class="n" translate="no">result</code><code translate="no"> </code><code class="k" translate="no">for</code><code translate="no"> </code><code class="n" translate="no">result</code><code translate="no"> </code><code class="ow" translate="no">in</code><code translate="no"> </code><code class="n" translate="no">results</code><code translate="no"> </code><code class="k" translate="no">if</code><code translate="no"> </code><code class="nb" translate="no">isinstance</code><code class="p" translate="no">(</code><code class="n" translate="no">result</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="nb" translate="no">str</code><code class="p" translate="no">)</code><code class="p" translate="no">]</code><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">if</code><code translate="no"> </code><code class="nb" translate="no">len</code><code class="p" translate="no">(</code><code class="n" translate="no">results</code><code class="p" translate="no">)</code><code translate="no"> </code><code class="o" translate="no">!=</code><code translate="no"> </code><code class="nb" translate="no">len</code><code class="p" translate="no">(</code><code class="n" translate="no">success_results</code><code class="p" translate="no">)</code><code class="p" translate="no">:</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO5-6" id="co_achieving_concurrency_in_ai_workloads_CO5-6"><img alt="6" src="assets/6.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">        </code><code class="n" translate="no">logger</code><code class="o" translate="no">.</code><code class="n" translate="no">warning</code><code class="p" translate="no">(</code><code class="s2" translate="no">"</code><code class="s2" translate="no">Some URLs could not be fetched</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">return</code><code translate="no"> </code><code class="s2" translate="no">"</code><code class="s2" translate="no"> </code><code class="s2" translate="no">"</code><code class="o" translate="no">.</code><code class="n" translate="no">join</code><code class="p" translate="no">(</code><code class="n" translate="no">success_results</code><code class="p" translate="no">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO5-1" id="callout_achieving_concurrency_in_ai_workloads_CO5-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Un semplice schema regex che raccoglie gli URL in un gruppo denominato <code translate="no">url</code>e che corrisponde ai protocolli <code translate="no">http</code> e <code translate="no">https</code>. Per semplicità, questo schema corrisponde a URL definiti in modo più generico e non convalida la struttura di un nome di dominio o di un percorso, né tiene conto delle stringhe di query o delle ancore in un URL.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO5-2" id="callout_achieving_concurrency_in_ai_workloads_CO5-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a></dt>
<dd><p>Trova tutte le corrispondenze non sovrapposte del modello regex nel testo.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO5-3" id="callout_achieving_concurrency_in_ai_workloads_CO5-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a></dt>
<dd><p>Utilizza il pacchetto <code translate="no">bs4</code> Beautiful Soup per analizzare la stringa HTML. Nelle pagine di Wikipedia, il contenuto dell'articolo è annidato all'interno di un contenitore <code translate="no">div</code> con <code translate="no">id="bodyContent"</code>, quindi la logica di parsing presuppone che vengano passati solo gli URL di Wikipedia. Puoi cambiare questa logica per altri URL o semplicemente utilizzare <code translate="no">soup.getText()</code>per prendere qualsiasi contenuto testuale annidato all'interno dell'HTML. Tuttavia, tieni presente che se analizzi l'HTML grezzo in questo modo, ci sarà molto rumore nel contenuto analizzato, il che può confondere LLM.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO5-4" id="callout_achieving_concurrency_in_ai_workloads_CO5-4"><img alt="4" src="assets/4.png" width="12" height="12"/></a></dt>
<dd><p>Data una sessione <code translate="no">aiohttp</code> e un URL, esegui una richiesta asincrona <code translate="no">get</code>. Crea un gestore di contesto asincrono <code translate="no">response</code> e <code translate="no">await</code> la risposta all'interno di questo gestore di contesto.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO5-5" id="callout_achieving_concurrency_in_ai_workloads_CO5-5"><img alt="5" src="assets/5.png" width="12" height="12"/></a></dt>
<dd><p>Dato un elenco di URL, crea un gestore di contesto asincrono della sessione client per eseguire in modo asincrono più chiamate di recupero. Poiché <code translate="no">fetch()</code> è una funzione coroutine (cioè utilizza la parola chiave <code translate="no">await</code> ),<code translate="no">fetch_all()</code> dovrà eseguire più coroutine <code translate="no">fetch()</code> all'interno di <code translate="no">asyncio.gather()</code> da programmare per l'esecuzione asincrona sul ciclo degli eventi.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO5-6" id="callout_achieving_concurrency_in_ai_workloads_CO5-6"><img alt="6" src="assets/6.png" width="12" height="12"/></a></dt>
<dd><p>Controlla che tutti gli URL siano stati recuperati con successo e, in caso contrario, emette un avviso.</p></dd>
</dl></div>
<p>Ora hai le funzioni di scraper di cui hai bisogno per implementare la funzione di web scraping nel tuo endpoint <code translate="no">/generate/text</code>.</p>
<p>Successivamente, aggiorna il gestore del testo per utilizzare le funzioni di scraper attraverso una dipendenza in modo asincrono, come mostrato nell'<a data-type="xref" href="#web_scraper_fastapi">Esempio 5-7</a>.</p>
<div data-type="example" id="web_scraper_fastapi">
<h5><span class="label">Esempio 5-7. </span>Iniettare la funzionalità di web scraper come dipendenza nel gestore FastAPI LLM</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="c1" translate="no"># dependencies.py</code><code translate="no">
</code><code translate="no">
</code><code class="kn" translate="no">from</code><code translate="no"> </code><code class="nn" translate="no">fastapi</code><code translate="no"> </code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="n" translate="no">Body</code><code translate="no">
</code><code class="kn" translate="no">from</code><code translate="no"> </code><code class="nn" translate="no">loguru</code><code translate="no"> </code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="n" translate="no">logger</code><code translate="no">
</code><code translate="no">
</code><code class="kn" translate="no">from</code><code translate="no"> </code><code class="nn" translate="no">schemas</code><code translate="no"> </code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="n" translate="no">TextModelRequest</code><code translate="no">
</code><code class="kn" translate="no">from</code><code translate="no"> </code><code class="nn" translate="no">scraper</code><code translate="no"> </code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="n" translate="no">extract_urls</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">fetch_all</code><code translate="no">
</code><code translate="no">
</code><code class="k" translate="no">async</code><code translate="no"> </code><code class="k" translate="no">def</code><code translate="no"> </code><code class="nf" translate="no">get_urls_content</code><code class="p" translate="no">(</code><code class="n" translate="no">body</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="n" translate="no">TextModelRequest</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">Body</code><code class="p" translate="no">(</code><code class="o" translate="no">.</code><code class="o" translate="no">.</code><code class="o" translate="no">.</code><code class="p" translate="no">)</code><code class="p" translate="no">)</code><code translate="no"> </code><code class="o" translate="no">-</code><code class="o" translate="no">&gt;</code><code translate="no"> </code><code class="nb" translate="no">str</code><code class="p" translate="no">:</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO6-1" id="co_achieving_concurrency_in_ai_workloads_CO6-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">urls</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">extract_urls</code><code class="p" translate="no">(</code><code class="n" translate="no">body</code><code class="o" translate="no">.</code><code class="n" translate="no">prompt</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">if</code><code translate="no"> </code><code class="n" translate="no">urls</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">        </code><code class="k" translate="no">try</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">            </code><code class="n" translate="no">urls_content</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="k" translate="no">await</code><code translate="no"> </code><code class="n" translate="no">fetch_all</code><code class="p" translate="no">(</code><code class="n" translate="no">urls</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">            </code><code class="k" translate="no">return</code><code translate="no"> </code><code class="n" translate="no">urls_content</code><code translate="no">
</code><code translate="no">        </code><code class="k" translate="no">except</code><code translate="no"> </code><code class="ne" translate="no">Exception</code><code translate="no"> </code><code class="k" translate="no">as</code><code translate="no"> </code><code class="n" translate="no">e</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">            </code><code class="n" translate="no">logger</code><code class="o" translate="no">.</code><code class="n" translate="no">warning</code><code class="p" translate="no">(</code><code class="sa" translate="no">f</code><code class="s2" translate="no">"</code><code class="s2" translate="no">Failed to fetch one or several URls - Error: </code><code class="si" translate="no">{</code><code class="n" translate="no">e</code><code class="si" translate="no">}</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">return</code><code translate="no"> </code><code class="s2" translate="no">"</code><code class="s2" translate="no">"</code><code translate="no">
</code><code translate="no">
</code><code class="c1" translate="no"># main.py</code><code translate="no">
</code><code translate="no">
</code><code class="kn" translate="no">from</code><code translate="no"> </code><code class="nn" translate="no">fastapi</code><code translate="no"> </code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="n" translate="no">Body</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">Depends</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">Request</code><code translate="no">
</code><code class="kn" translate="no">from</code><code translate="no"> </code><code class="nn" translate="no">dependencies</code><code translate="no"> </code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="n" translate="no">construct_prompt</code><code translate="no">
</code><code class="kn" translate="no">from</code><code translate="no"> </code><code class="nn" translate="no">schemas</code><code translate="no"> </code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="n" translate="no">TextModelResponse</code><code translate="no">
</code><code translate="no">
</code><code class="nd" translate="no">@app</code><code class="o" translate="no">.</code><code class="n" translate="no">post</code><code class="p" translate="no">(</code><code class="s2" translate="no">"</code><code class="s2" translate="no">/generate/text</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">response_model_exclude_defaults</code><code class="o" translate="no">=</code><code class="kc" translate="no">True</code><code class="p" translate="no">)</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO6-2" id="co_achieving_concurrency_in_ai_workloads_CO6-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a><code translate="no">
</code><code class="k" translate="no">async</code><code translate="no"> </code><code class="k" translate="no">def</code><code translate="no"> </code><code class="nf" translate="no">serve_text_to_text_controller</code><code class="p" translate="no">(</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">request</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="n" translate="no">Request</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">body</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="n" translate="no">TextModelRequest</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">Body</code><code class="p" translate="no">(</code><code class="o" translate="no">.</code><code class="o" translate="no">.</code><code class="o" translate="no">.</code><code class="p" translate="no">)</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">urls_content</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="nb" translate="no">str</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">Depends</code><code class="p" translate="no">(</code><code class="n" translate="no">get_urls_content</code><code class="p" translate="no">)</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO6-3" id="co_achieving_concurrency_in_ai_workloads_CO6-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a><code translate="no">
</code><code class="p" translate="no">)</code><code translate="no"> </code><code class="o" translate="no">-</code><code class="o" translate="no">&gt;</code><code translate="no"> </code><code class="n" translate="no">TextModelResponse</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">    </code><code class="o" translate="no">.</code><code class="o" translate="no">.</code><code class="o" translate="no">.</code><code translate="no"> </code><code class="c1" translate="no"># rest of controller logic</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">prompt</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">body</code><code class="o" translate="no">.</code><code class="n" translate="no">prompt</code><code translate="no"> </code><code class="o" translate="no">+</code><code translate="no"> </code><code class="s2" translate="no">"</code><code class="s2" translate="no"> </code><code class="s2" translate="no">"</code><code translate="no"> </code><code class="o" translate="no">+</code><code translate="no"> </code><code class="n" translate="no">urls_content</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">output</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">generate_text</code><code class="p" translate="no">(</code><code class="n" translate="no">models</code><code class="p" translate="no">[</code><code class="s2" translate="no">"</code><code class="s2" translate="no">text</code><code class="s2" translate="no">"</code><code class="p" translate="no">]</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">prompt</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">body</code><code class="o" translate="no">.</code><code class="n" translate="no">temperature</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">return</code><code translate="no"> </code><code class="n" translate="no">TextModelResponse</code><code class="p" translate="no">(</code><code class="n" translate="no">content</code><code class="o" translate="no">=</code><code class="n" translate="no">output</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">ip</code><code class="o" translate="no">=</code><code class="n" translate="no">request</code><code class="o" translate="no">.</code><code class="n" translate="no">client</code><code class="o" translate="no">.</code><code class="n" translate="no">host</code><code class="p" translate="no">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO6-1" id="callout_achieving_concurrency_in_ai_workloads_CO6-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Implementa una dipendenza <code translate="no">get_urls_content</code> FastAPI che ottiene un prompt dell'utente dal corpo della richiesta e trova tutti gli URL. Restituisce quindi il contenuto di tutti gli URL come una lunga stringa. La dipendenza ha una gestione delle eccezioni integrata per gestire eventuali errori di I/O restituendo una stringa vuota e registrando un avviso sul server.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO6-2" id="callout_achieving_concurrency_in_ai_workloads_CO6-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a></dt>
<dd><p>Quando utilizzi <code translate="no">aiohttp</code> all'interno di FastAPI, non devi gestire tu stesso il ciclo degli eventi perché FastAPI, in quanto framework asincrono, gestisce il ciclo degli eventi. Puoi definire il tuo endpoint come una funzione asincrona e utilizzare <code translate="no">aiohttp</code> per effettuare richieste HTTP asincrone all'interno del gestore o tramite una dipendenza come in questo esempio.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO6-3" id="callout_achieving_concurrency_in_ai_workloads_CO6-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a></dt>
<dd><p>Inietta i risultati della chiamata alla dipendenza <code translate="no">get_urls_content</code> nel gestore tramite la classe <code translate="no">Depends</code> di FastAPI. L'utilizzo di una dipendenza FastAPI ha permesso di mantenere la logica del controllore piccola, pulita e leggibile.</p></dd>
</dl></div>
<p>Ora esegui il client Streamlit nel browser e prova la tua nuova funzione. La<a data-type="xref" href="#llm_summary">Figura 5-6</a> mostra il mio esperimento.</p>
<figure><div class="figure" id="llm_summary">
<img alt="bgai 0506" src="assets/bgai_0506.png" width="704" height="408"/>
<h6><span class="label">Figura 5-6. </span>Richiesta al modello TinyLlama self-hosted di riassumere un articolo di Wikipedia</h6>
</div></figure>
<p>Congratulazioni! Hai imparato a costruire un semplice scraper web non bloccante per lavorare con il tuo LLM. In questo mini-progetto, hai sfruttato il pacchetto <code translate="no">re</code> per abbinare i modelli di URL nel prompt dell'utente e poi hai usato la libreria <code translate="no">aiohttp</code> per recuperare in modo asincrono più pagine contemporaneamente. Hai poi usato il pacchetto <code translate="no">BeautifulSoup</code> per analizzare il contenuto degli articoli di Wikipedia prendendo il contenuto testuale del contenitore <code translate="no">div</code> con l'ID di <code translate="no">bodyContent</code> all'interno della stringa HTML recuperata.
Per altri siti web o per le pagine interne dell'azienda, puoi sempre modificare la logica di parsing per ottenere un parsing appropriato. Infine, hai avvolto l'intera logica di scraping all'interno di una dipendenza FastAPI con gestione delle eccezioni integrata per sfruttare la dependency injection mentre aggiornavi il gestore del modello di testo.</p>
<p>Tieni presente che il tuo scraper non può gestire pagine complesse con layout dinamici renderizzati dal server. In questi casi, puoi aggiungere un browser headless al tuo web scraper per navigare nelle pagine dinamiche.</p>
<p>Inoltre, recuperare i contenuti di siti esterni sarà una sfida, poiché la maggior parte dei siti può implementare protezioni anti-scraping come il <em>blocco dell'IP</em> o i <em>CAPTCHA</em> come deterrenti comuni. Mantenere la <em>qualità</em> e la <em>coerenza</em> <em>dei dati</em> con i siti esterni è anche una sfida continua, poiché potrebbe essere necessario aggiornare regolarmente gli script di scraping per garantire un'estrazione accurata e affidabile.</p>
<p>Ora dovresti sentirti più a tuo agio nella costruzione di servizi alimentati da GenAI che devono interagire con il web facendo richieste di rete asincrone.</p>
<p>Successivamente, esamineremo altre interazioni asincrone di I/O, come quelle con i database e il filesystem, costruendo una funzione <em>talk to your documents</em>.</p>
<p>Questa funzionalità permette agli utenti di caricare documenti attraverso l'interfaccia Streamlit del tuo servizio. Il contenuto dei documenti caricati viene poi estratto, elaborato e salvato in un database. Successivamente, durante le interazioni dell'utente con il LLM, un sistema di recupero asincrono recupera contenuti semanticamente rilevanti dal database, che vengono poi utilizzati per aumentare il contesto fornito al LLM.</p>
<p>Questo processo è denominato RAG, che verrà sviluppato come modulo per il tuo LLM.<a data-startref="ix_ch05-asciidoc19" data-type="indexterm" id="id828"/><a data-startref="ix_ch05-asciidoc18" data-type="indexterm" id="id829"/></p>
</div></section>
<section data-pdf-bookmark="Project: Talk to Documents (RAG)" data-type="sect2"><div class="sect2" id="id83">
<h2>Progetto: Parla con i documenti (RAG)</h2>
<p><a data-primary="concurrency" data-secondary="optimizing for I/O tasks with asynchronous programming" data-tertiary="project: RAG module" data-type="indexterm" id="ix_ch05-asciidoc20"/><a data-primary="RAG (retrieval augmented generation) module" data-secondary="async programming project" data-type="indexterm" id="ix_ch05-asciidoc21"/>In questo progetto, inseriremo un modulo RAG nel tuo servizio GenAI per farti sperimentare l'interazione asincrona con sistemi esterni come un database e un filesystem.</p>
<p>Potresti essere curioso di sapere qual è lo scopo di un modulo RAG e la sua necessità. Il RAG è semplicemente una tecnica per aumentare il contesto dei prompt di LLM con fonti di dati personalizzate per attività ad alta intensità di conoscenza.<sup><a data-type="noteref" href="ch05.html#id830" id="id830-marker" translate="no">5</a></sup>
Si tratta di una tecnica efficace per fondare le risposte di LLM su fatti contenuti nei dati, senza dover ricorrere a una complessa e costosa messa a punto di LLM.</p>
<p>Le organizzazioni sono ansiose di implementare il RAG con il proprio LLM, poiché consente ai dipendenti di accedere alle enormi basi di conoscenza interne attraverso il LLM. Con il RAG, le aziende si aspettano che le basi di conoscenza interne, i sistemi e le procedure siano accessibili e prontamente disponibili a chiunque ne abbia bisogno per rispondere alle domande, proprio quando ne ha bisogno. Si prevede che questa accessibilità alle informazioni aziendali aumenti la produttività, riduca i costi e i tempi di ricerca delle informazioni e incrementi i profitti di qualsiasi azienda.</p>
<p><a data-primary="hallucinations" data-type="indexterm" id="id831"/>Tuttavia, le LLMs sono suscettibili di generare risposte che non aderiscono alle istruzioni fornite dall'utente. In altre parole, le LLMs possono <em>allucinare</em>risposte con informazioni o dati che non si basano su fatti o realtà.</p>
<p>Queste allucinazioni possono verificarsi a causa del fatto che il modello si basa su schemi presenti nei dati su cui è stato addestrato piuttosto che sull'accesso diretto a dati esterni, aggiornati e reali. Le LLM possono manifestare allucinazioni con risposte sicure ma errate o insensate, storie inventate o affermazioni prive di una base di verità.</p>
<p>Pertanto, per i compiti più complessi e ad alta intensità di conoscenza, vorrai che il tuo LLM acceda a fonti di conoscenza esterne per completare i compiti. Questo permette una maggiore coerenza fattuale e migliora l'affidabilità delle risposte generate.<a data-type="xref" href="#rag">La Figura 5-7</a> mostra il processo completo.</p>
<figure><div class="figure" id="rag">
<img alt="bgai 0507" src="assets/bgai_0507.png" width="861" height="768"/>
<h6><span class="label">Figura 5-7. </span>RAG</h6>
</div></figure>
<p>In questo progetto costruirai un semplice modulo RAG per il tuo servizio LLM in modo che gli utenti possano caricare e parlare dei loro documenti.</p>
<div data-type="note" epub:type="note"><h6>Nota</h6>
<p>C'è molto da sapere sul RAG, tanto da riempire diversi libri di testo e ogni giorno vengono pubblicati nuovi articoli su nuove tecniche e algoritmi.</p>
<p>Ti consiglio di consultare altre pubblicazioni su LLMs per conoscere il processo RAG e le tecniche RAG avanzate.</p>
</div>
<p>La pipeline di RAG consiste nelle seguenti fasi:</p>
<ol>
<li>
<p><em>Estrazione</em> di documenti da un filesystem per caricare il contenuto testuale in pezzi sulla memoria.</p>
</li>
<li>
<p><em>Trasformazione</em> dei contenuti testuali pulendoli, dividendoli e preparandoli per essere passati in un modello di embedding per produrre vettori di embedding che rappresentano il significato semantico di un brano.</p>
</li>
<li>
<p><em>Memorizzazione</em> dei vettori di incorporamento insieme ai metadati, come la fonte e il chunk di testo, in un archivio vettoriale come Qdrant.</p>
</li>
<li>
<p><em>Recupero</em> di vettori di incorporamento semanticamente rilevanti eseguendo una ricerca semantica sulla query dell'utente al LLM. I pezzi di testo originali - memorizzati come metadati dei vettori recuperati - vengono poi utilizzati per aumentare (cioè migliorare il contesto) il prompt iniziale fornito al LLM.</p>
</li>
<li>
<p><em>Generazione</em> della risposta del LLM che bypassa sia la query che i chunk recuperati (cioè il contesto) al LLM per ottenere una risposta.</p>
</li>
</ol>
<p>Puoi vedere la pipeline completa nella <a data-type="xref" href="#rag_pipeline">Figura 5-8</a>.</p>
<figure><div class="figure" id="rag_pipeline">
<img alt="bgai 0508" src="assets/bgai_0508.png" width="1411" height="210"/>
<h6><span class="label">Figura 5-8. </span>Pipeline RAG</h6>
</div></figure>
<p>Puoi prendere la pipeline mostrata nella <a data-type="xref" href="#rag_pipeline">Figura 5-8</a> e adattarla al tuo servizio esistente. La<a data-type="xref" href="#rag_module">Figura 5-9</a> mostra l'architettura di sistema di un servizio "parla con i tuoi documenti" abilitato con RAG.</p>
<figure><div class="figure" id="rag_module">
<img alt="bgai 0509" src="assets/bgai_0509.png" width="1292" height="792"/>
<h6><span class="label">Figura 5-9. </span>Parla con l'architettura del tuo sistema di documenti</h6>
</div></figure>
<p>La<a data-type="xref" href="#rag_module">Figura 5-9</a> illustra il modo in cui i documenti caricati dagli utenti tramite l'interfaccia Streamlit vengono memorizzati e poi recuperati per l'elaborazione e l'archiviazione nel database per essere successivamente recuperati per aumentare i prompt di LLM.</p>
<p>Il primo passo da compiere prima di implementare il sistema RAG della <a data-type="xref" href="#rag_module">Figura 5-9</a> è quello di includere la funzionalità di caricamento dei file sia nel client Streamlit che nell'API backend.</p>
<p>Utilizzando la classe <code translate="no">UploadFile</code> di FastAPI, puoi accettare documenti dagli utenti in pezzi e salvarli nel filesystem o in qualsiasi altra soluzione di archiviazione file, come ad esempio un archivio blob. L'elemento importante da notare è che questa operazione di I/O non è bloccante grazie alla programmazione asincrona, che la classe <code translate="no">UploadFile</code> di FastAPI supporta.</p>
<div data-type="tip"><h6>Suggerimento</h6>
<p><a data-primary="chunking" data-type="indexterm" id="id832"/>Poiché gli utenti possono caricare documenti di grandi dimensioni, la classe <code translate="no">UploadFile</code> di FastAPI supporta il <em>chunking</em>per memorizzare i documenti caricati, un pezzo alla volta.</p>
<p>In questo modo eviterai di intasare la memoria del tuo servizio. Dovrai anche proteggere il tuo servizio impedendo agli utenti di caricare documenti di dimensioni superiori a una certa soglia.</p>
</div>
<p>L<a data-type="xref" href="#upload_file">'esempio 5-8</a> mostra come implementare una funzionalità di caricamento asincrono di file.</p>
<div data-type="tip"><h6>Suggerimento</h6>
<p>Dovrai installare il pacchetto <code translate="no">aiofiles</code> per caricare i file in modo asincrono e <code translate="no">python-multipart</code> per ricevere i file caricati dai moduli HTML:</p>
<pre data-code-language="bash" data-type="programlisting" translate="no">$<code class="w" translate="no"> </code>pip<code class="w" translate="no"> </code>install<code class="w" translate="no"> </code>aiofiles<code class="w" translate="no"> </code>python-multipart<code class="w" translate="no"/></pre>
</div>
<div data-type="example" id="upload_file">
<h5><span class="label">Esempio 5-8. </span>Implementazione di un endpoint per il caricamento asincrono di file</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="c1" translate="no"># upload.py</code>

<code class="kn" translate="no">import</code> <code class="nn" translate="no">os</code>
<code class="kn" translate="no">import</code> <code class="nn" translate="no">aiofiles</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">aiofiles.os</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">makedirs</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">fastapi</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">UploadFile</code>

<code class="n" translate="no">DEFAULT_CHUNK_SIZE</code> <code class="o" translate="no">=</code> <code class="mi" translate="no">1024</code> <code class="o" translate="no">*</code> <code class="mi" translate="no">1024</code> <code class="o" translate="no">*</code> <code class="mi" translate="no">50</code>  <code class="c1" translate="no"># 50 megabytes</code>

<code class="k" translate="no">async</code> <code class="k" translate="no">def</code> <code class="nf" translate="no">save_file</code><code class="p" translate="no">(</code><code class="n" translate="no">file</code><code class="p" translate="no">:</code> <code class="n" translate="no">UploadFile</code><code class="p" translate="no">)</code> <code class="o" translate="no">-&gt;</code> <code class="nb" translate="no">str</code><code class="p" translate="no">:</code>
    <code class="k" translate="no">await</code> <code class="n" translate="no">makedirs</code><code class="p" translate="no">(</code><code class="s2" translate="no">"uploads"</code><code class="p" translate="no">,</code> <code class="n" translate="no">exist_ok</code><code class="o" translate="no">=</code><code class="kc" translate="no">True</code><code class="p" translate="no">)</code>
    <code class="n" translate="no">filepath</code> <code class="o" translate="no">=</code> <code class="n" translate="no">os</code><code class="o" translate="no">.</code><code class="n" translate="no">path</code><code class="o" translate="no">.</code><code class="n" translate="no">join</code><code class="p" translate="no">(</code><code class="s2" translate="no">"uploads"</code><code class="p" translate="no">,</code> <code class="n" translate="no">file</code><code class="o" translate="no">.</code><code class="n" translate="no">filename</code><code class="p" translate="no">)</code>
    <code class="k" translate="no">async</code> <code class="k" translate="no">with</code> <code class="n" translate="no">aiofiles</code><code class="o" translate="no">.</code><code class="n" translate="no">open</code><code class="p" translate="no">(</code><code class="n" translate="no">filepath</code><code class="p" translate="no">,</code> <code class="s2" translate="no">"wb"</code><code class="p" translate="no">)</code> <code class="k" translate="no">as</code> <code class="n" translate="no">f</code><code class="p" translate="no">:</code>
        <code class="k" translate="no">while</code> <code class="n" translate="no">chunk</code> <code class="o" translate="no">:=</code> <code class="k" translate="no">await</code> <code class="n" translate="no">file</code><code class="o" translate="no">.</code><code class="n" translate="no">read</code><code class="p" translate="no">(</code><code class="n" translate="no">DEFAULT_CHUNK_SIZE</code><code class="p" translate="no">):</code>
            <code class="k" translate="no">await</code> <code class="n" translate="no">f</code><code class="o" translate="no">.</code><code class="n" translate="no">write</code><code class="p" translate="no">(</code><code class="n" translate="no">chunk</code><code class="p" translate="no">)</code>
    <code class="k" translate="no">return</code> <code class="n" translate="no">filepath</code>

<code class="c1" translate="no"># main.py</code>

<code class="kn" translate="no">from</code> <code class="nn" translate="no">fastapi</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">FastAPI</code><code class="p" translate="no">,</code> <code class="n" translate="no">HTTPException</code><code class="p" translate="no">,</code> <code class="n" translate="no">status</code><code class="p" translate="no">,</code> <code class="n" translate="no">File</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">typing</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">Annotated</code>
<code class="kn" translate="no">from</code> <code class="nn" translate="no">upload</code> <code class="kn" translate="no">import</code> <code class="n" translate="no">save_file</code>

<code class="nd" translate="no">@app</code><code class="o" translate="no">.</code><code class="n" translate="no">post</code><code class="p" translate="no">(</code><code class="s2" translate="no">"/upload"</code><code class="p" translate="no">)</code>
<code class="k" translate="no">async</code> <code class="k" translate="no">def</code> <code class="nf" translate="no">file_upload_controller</code><code class="p" translate="no">(</code>
    <code class="n" translate="no">file</code><code class="p" translate="no">:</code> <code class="n" translate="no">Annotated</code><code class="p" translate="no">[</code><code class="n" translate="no">UploadFile</code><code class="p" translate="no">,</code> <code class="n" translate="no">File</code><code class="p" translate="no">(</code><code class="n" translate="no">description</code><code class="o" translate="no">=</code><code class="s2" translate="no">"Uploaded PDF documents"</code><code class="p" translate="no">)]</code>
<code class="p" translate="no">):</code>
    <code class="k" translate="no">if</code> <code class="n" translate="no">file</code><code class="o" translate="no">.</code><code class="n" translate="no">content_type</code> <code class="o" translate="no">!=</code> <code class="s2" translate="no">"application/pdf"</code><code class="p" translate="no">:</code>
        <code class="k" translate="no">raise</code> <code class="n" translate="no">HTTPException</code><code class="p" translate="no">(</code>
            <code class="n" translate="no">detail</code><code class="o" translate="no">=</code><code class="sa" translate="no">f</code><code class="s2" translate="no">"Only uploading PDF documents are supported"</code><code class="p" translate="no">,</code>
            <code class="n" translate="no">status_code</code><code class="o" translate="no">=</code><code class="n" translate="no">status</code><code class="o" translate="no">.</code><code class="n" translate="no">HTTP_400_BAD_REQUEST</code><code class="p" translate="no">,</code>
        <code class="p" translate="no">)</code>
    <code class="k" translate="no">try</code><code class="p" translate="no">:</code>
        <code class="k" translate="no">await</code> <code class="n" translate="no">save_file</code><code class="p" translate="no">(</code><code class="n" translate="no">file</code><code class="p" translate="no">)</code>
    <code class="k" translate="no">except</code> <code class="ne" translate="no">Exception</code> <code class="k" translate="no">as</code> <code class="n" translate="no">e</code><code class="p" translate="no">:</code>
        <code class="k" translate="no">raise</code> <code class="n" translate="no">HTTPException</code><code class="p" translate="no">(</code>
            <code class="n" translate="no">detail</code><code class="o" translate="no">=</code><code class="sa" translate="no">f</code><code class="s2" translate="no">"An error occurred while saving file - Error: </code><code class="si" translate="no">{</code><code class="n" translate="no">e</code><code class="si" translate="no">}</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code>
            <code class="n" translate="no">status_code</code><code class="o" translate="no">=</code><code class="n" translate="no">status</code><code class="o" translate="no">.</code><code class="n" translate="no">HTTP_500_INTERNAL_SERVER_ERROR</code><code class="p" translate="no">,</code>
        <code class="p" translate="no">)</code>
    <code class="k" translate="no">return</code> <code class="p" translate="no">{</code><code class="s2" translate="no">"filename"</code><code class="p" translate="no">:</code> <code class="n" translate="no">file</code><code class="o" translate="no">.</code><code class="n" translate="no">filename</code><code class="p" translate="no">,</code> <code class="s2" translate="no">"message"</code><code class="p" translate="no">:</code> <code class="s2" translate="no">"File uploaded successfully"</code><code class="p" translate="no">}</code>

<code class="c1" translate="no"># client.py</code>

<code class="kn" translate="no">import</code> <code class="nn" translate="no">requests</code>
<code class="kn" translate="no">import</code> <code class="nn" translate="no">streamlit</code> <code class="k" translate="no">as</code> <code class="nn" translate="no">st</code>

<code class="n" translate="no">st</code><code class="o" translate="no">.</code><code class="n" translate="no">write</code><code class="p" translate="no">(</code><code class="s2" translate="no">"Upload a file to FastAPI"</code><code class="p" translate="no">)</code>
<code class="n" translate="no">file</code> <code class="o" translate="no">=</code> <code class="n" translate="no">st</code><code class="o" translate="no">.</code><code class="n" translate="no">file_uploader</code><code class="p" translate="no">(</code><code class="s2" translate="no">"Choose a file"</code><code class="p" translate="no">,</code> <code class="nb" translate="no">type</code><code class="o" translate="no">=</code><code class="p" translate="no">[</code><code class="s2" translate="no">"pdf"</code><code class="p" translate="no">])</code>

<code class="k" translate="no">if</code> <code class="n" translate="no">st</code><code class="o" translate="no">.</code><code class="n" translate="no">button</code><code class="p" translate="no">(</code><code class="s2" translate="no">"Submit"</code><code class="p" translate="no">):</code>
    <code class="k" translate="no">if</code> <code class="n" translate="no">file</code> <code class="ow" translate="no">is</code> <code class="ow" translate="no">not</code> <code class="kc" translate="no">None</code><code class="p" translate="no">:</code>
        <code class="n" translate="no">files</code> <code class="o" translate="no">=</code> <code class="p" translate="no">{</code><code class="s2" translate="no">"file"</code><code class="p" translate="no">:</code> <code class="p" translate="no">(</code><code class="n" translate="no">file</code><code class="o" translate="no">.</code><code class="n" translate="no">name</code><code class="p" translate="no">,</code> <code class="n" translate="no">file</code><code class="p" translate="no">,</code> <code class="n" translate="no">file</code><code class="o" translate="no">.</code><code class="n" translate="no">type</code><code class="p" translate="no">)}</code>
        <code class="n" translate="no">response</code> <code class="o" translate="no">=</code> <code class="n" translate="no">requests</code><code class="o" translate="no">.</code><code class="n" translate="no">post</code><code class="p" translate="no">(</code><code class="s2" translate="no">"http://localhost:8000/upload"</code><code class="p" translate="no">,</code> <code class="n" translate="no">files</code><code class="o" translate="no">=</code><code class="n" translate="no">files</code><code class="p" translate="no">)</code>
        <code class="n" translate="no">st</code><code class="o" translate="no">.</code><code class="n" translate="no">write</code><code class="p" translate="no">(</code><code class="n" translate="no">response</code><code class="o" translate="no">.</code><code class="n" translate="no">text</code><code class="p" translate="no">)</code>
    <code class="k" translate="no">else</code><code class="p" translate="no">:</code>
        <code class="n" translate="no">st</code><code class="o" translate="no">.</code><code class="n" translate="no">write</code><code class="p" translate="no">(</code><code class="s2" translate="no">"No file uploaded."</code><code class="p" translate="no">)</code></pre></div>
<p>A questo punto dovresti essere in grado di caricare i file tramite l'interfaccia utente di Streamlit, come puoi vedere nella <a data-type="xref" href="#streamlit_upload">Figura 5-10</a>.</p>
<figure><div class="figure" id="streamlit_upload">
<img alt="bgai 0510" src="assets/bgai_0510.png" width="704" height="352"/>
<h6><span class="label">Figura 5-10. </span>Caricamento di file tramite Streamlit al servizio FastAPI</h6>
</div></figure>
<p>Con l'implementazione della funzionalità di upload, puoi ora dedicarti alla costruzione del modulo RAG. La<a data-type="xref" href="#rag_module_detailed">Figura 5-11</a> mostra la pipeline dettagliata, che apre il componente di trasformazione dei dati nella <a data-type="xref" href="#rag_module">Figura 5-9</a>.</p>
<figure><div class="figure" id="rag_module_detailed">
<img alt="bgai 0511" src="assets/bgai_0511.png" width="1417" height="1145"/>
<h6><span class="label">Figura 5-11. </span>Pipeline dettagliata di elaborazione dei dati RAG</h6>
</div></figure>
<p>Come puoi vedere nella <a data-type="xref" href="#rag_module_detailed">Figura 5-11</a>, devi recuperare in modo asincrono i file memorizzati dal disco rigido e farli passare attraverso una pipeline di trasformazione dei dati prima della memorizzazione tramite un client di database asincrono.</p>
<p><a data-primary="RAG (retrieval augmented generation) module" data-secondary="data transformation pipeline" data-type="indexterm" id="id833"/>La pipeline di trasformazione dei dati è composta dalle seguenti parti:</p>
<dl>
<dt>Estrattore</dt>
<dd>
<p>Estrarre il contenuto dei PDF e memorizzarlo in file di testo sul disco rigido.</p>
</dd>
<dt>Caricatore</dt>
<dd>
<p>Carica in modo asincrono un file di testo in memoria a pezzi.</p>
</dd>
<dt>Pulitore</dt>
<dd>
<p>Rimuove gli spazi bianchi o i caratteri di formattazione ridondanti dai pezzi di testo.</p>
</dd>
<dt>Incorporatore</dt>
<dd>
<p>Utilizza un modello di incorporamento pre-addestrato e auto-ospitato per convertire il testo in vettori di incorporamento.</p>
</dd>
</dl>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id834">
<h1>Incorporazioni</h1>
<p><a data-primary="embeddings" data-type="indexterm" id="id835"/>Gli embeddings sono vettori ad alta dimensione che catturano i significati semantici e le relazioni tra le parole e le frasi nei testi. I testi semanticamente simili hanno una distanza minore tra loro.</p>
<p>Utilizzando i modelli di embedding, è possibile codificare testi e immagini in rappresentazioni di embedding come punti all'interno di uno spazio vettoriale continuo. Questi punti di dati trasformati possono essere successivamente utilizzati per una miriade di applicazioni a valle, come il recupero di informazioni in RAG e compiti di clustering e classificazione.</p>
<p>Per ulteriori informazioni sui vettori di incorporamento, consulta il <a data-type="xref" href="ch03.html#ch03">Capitolo 3</a>.</p>
</div></aside>
<p>Una volta che gli utenti caricano i loro file PDF sul filesystem del tuo server tramite il processo mostrato nell'<a data-type="xref" href="#upload_file">Esempio 5-8</a>, puoi immediatamente convertirli in file di testo tramite la libreria <code translate="no">pypdf</code>. Poiché non esiste una libreria asincrona per caricare i file PDF binari, dovrai prima convertirli in file di testo.</p>
<p>L<a data-type="xref" href="#rag_extract">'esempio 5-9</a> mostra come caricare i PDF, estrarre ed elaborare il loro contenuto e poi archiviarli come file di testo.</p>
<div data-type="note" epub:type="note"><h6>Nota</h6>
<p>Dovrai installare diversi pacchetti per eseguire i prossimi esempi:</p>
<pre data-code-language="bash" data-type="programlisting" translate="no">$<code class="w" translate="no"> </code>pip<code class="w" translate="no"> </code>install<code class="w" translate="no"> </code>qdrant_client<code class="w" translate="no"> </code>aiofiles<code class="w" translate="no"> </code>pypdf<code class="w" translate="no"> </code>loguru<code class="w" translate="no"/></pre>
</div>
<div data-type="example" id="rag_extract">
<h5><span class="label">Esempio 5-9. </span>Estrattore PDF-testo RAG</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="c1" translate="no"># rag/extractor.py</code><code translate="no">
</code><code translate="no">
</code><code class="kn" translate="no">from</code><code translate="no"> </code><code class="nn" translate="no">pypdf</code><code translate="no"> </code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="n" translate="no">PdfReader</code><code translate="no">
</code><code translate="no">
</code><code class="k" translate="no">def</code><code translate="no"> </code><code class="nf" translate="no">pdf_text_extractor</code><code class="p" translate="no">(</code><code class="n" translate="no">filepath</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="nb" translate="no">str</code><code class="p" translate="no">)</code><code translate="no"> </code><code class="o" translate="no">-</code><code class="o" translate="no">&gt;</code><code translate="no"> </code><code class="kc" translate="no">None</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">content</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="s2" translate="no">"</code><code class="s2" translate="no">"</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">pdf_reader</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">PdfReader</code><code class="p" translate="no">(</code><code class="n" translate="no">filepath</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">strict</code><code class="o" translate="no">=</code><code class="kc" translate="no">True</code><code class="p" translate="no">)</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO7-1" id="co_achieving_concurrency_in_ai_workloads_CO7-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">for</code><code translate="no"> </code><code class="n" translate="no">page</code><code translate="no"> </code><code class="ow" translate="no">in</code><code translate="no"> </code><code class="n" translate="no">pdf_reader</code><code class="o" translate="no">.</code><code class="n" translate="no">pages</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">        </code><code class="n" translate="no">page_text</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">page</code><code class="o" translate="no">.</code><code class="n" translate="no">extract_text</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">        </code><code class="k" translate="no">if</code><code translate="no"> </code><code class="n" translate="no">page_text</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">            </code><code class="n" translate="no">content</code><code translate="no"> </code><code class="o" translate="no">+</code><code class="o" translate="no">=</code><code translate="no"> </code><code class="sa" translate="no">f</code><code class="s2" translate="no">"</code><code class="si" translate="no">{</code><code class="n" translate="no">page_text</code><code class="si" translate="no">}</code><code class="se" translate="no">\n</code><code class="se" translate="no">\n</code><code class="s2" translate="no">"</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO7-2" id="co_achieving_concurrency_in_ai_workloads_CO7-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">with</code><code translate="no"> </code><code class="nb" translate="no">open</code><code class="p" translate="no">(</code><code class="n" translate="no">filepath</code><code class="o" translate="no">.</code><code class="n" translate="no">replace</code><code class="p" translate="no">(</code><code class="s2" translate="no">"</code><code class="s2" translate="no">pdf</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="s2" translate="no">"</code><code class="s2" translate="no">txt</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="s2" translate="no">"</code><code class="s2" translate="no">w</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">encoding</code><code class="o" translate="no">=</code><code class="s2" translate="no">"</code><code class="s2" translate="no">utf-8</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code translate="no"> </code><code class="k" translate="no">as</code><code translate="no"> </code><code class="n" translate="no">file</code><code class="p" translate="no">:</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO7-3" id="co_achieving_concurrency_in_ai_workloads_CO7-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">        </code><code class="n" translate="no">file</code><code class="o" translate="no">.</code><code class="n" translate="no">write</code><code class="p" translate="no">(</code><code class="n" translate="no">content</code><code class="p" translate="no">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO7-1" id="callout_achieving_concurrency_in_ai_workloads_CO7-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Utilizza la libreria <code translate="no">pypdf</code> per aprire un puntatore di flusso a un file PDF con <code translate="no">strict=True</code>in modo che qualsiasi errore di lettura venga registrato sul terminale. Nota che non esiste un'implementazione asincrona della libreria <code translate="no">pypdf</code>, quindi la funzione viene dichiarata con la normale parola chiave <code translate="no">def</code>. È importante evitare di utilizzare questa funzione all'interno di una funzione asincrona per evitare di bloccare il ciclo di eventi che esegue il thread principale del server. Vedrai come le attività in background di FastAPI possono aiutare a risolvere questo problema.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO7-2" id="callout_achieving_concurrency_in_ai_workloads_CO7-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a></dt>
<dd><p>Esegue un loop su ogni pagina del documento PDF ed estrae e aggiunge tutti i contenuti testuali in una lunga stringa.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO7-3" id="callout_achieving_concurrency_in_ai_workloads_CO7-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a></dt>
<dd><p>Scrivi il contenuto del documento PDF in un file di testo per l'elaborazione a valle. Specifica <code translate="no">encoding="utf-8"</code> per evitare problemi su piattaforme come Windows.</p></dd>
</dl></div>
<p>L'estrattore di testo convertirà i file PDF in semplici file di testo che potremo riversare in memoria in pezzi utilizzando un caricatore di file asincrono.<a data-primary="Jina embedder model" data-type="indexterm" id="id836"/>Ogni pezzo può poi essere ripulito e incorporato in un vettore di incorporamento utilizzando un modello di incorporamento open source come <code translate="no">jinaai/jina-embeddings-v2-base-en</code>, disponibile per il download dall'<a href="https://oreil.ly/gI74r">hub</a> del <a href="https://oreil.ly/gI74r">modello Hugging Face</a>.</p>
<div data-type="note" epub:type="note"><h6>Nota</h6>
<p>Ho scelto l'embedder di base Jina perché corrisponde alle prestazioni del modello proprietario di OpenAI <code translate="no">text-embedding-ada-002</code>.</p>
</div>
<p>L<a data-type="xref" href="#rag_transform">'esempio 5-10</a> mostra l'implementazione della pipeline di trasformazione dei dati RAG che include le funzioni async di caricamento del testo, pulizia e incorporamento.</p>
<div data-type="example" id="rag_transform">
<h5><span class="label">Esempio 5-10. </span>Funzioni di trasformazione dei dati RAG</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="c1" translate="no"># rag/transform.py</code><code translate="no">
</code><code translate="no">
</code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="nn" translate="no">re</code><code translate="no">
</code><code class="kn" translate="no">from</code><code translate="no"> </code><code class="nn" translate="no">typing</code><code translate="no"> </code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="n" translate="no">Any</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">AsyncGenerator</code><code translate="no">
</code><code translate="no">
</code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="nn" translate="no">aiofiles</code><code translate="no">
</code><code class="kn" translate="no">from</code><code translate="no"> </code><code class="nn" translate="no">transformers</code><code translate="no"> </code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="n" translate="no">AutoModel</code><code translate="no">
</code><code translate="no">
</code><code class="n" translate="no">DEFAULT_CHUNK_SIZE</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="mi" translate="no">1024</code><code translate="no"> </code><code class="o" translate="no">*</code><code translate="no"> </code><code class="mi" translate="no">1024</code><code translate="no"> </code><code class="o" translate="no">*</code><code translate="no"> </code><code class="mi" translate="no">50</code><code translate="no">  </code><code class="c1" translate="no"># 50 megabytes</code><code translate="no">
</code><code translate="no">
</code><code class="n" translate="no">embedder</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">AutoModel</code><code class="o" translate="no">.</code><code class="n" translate="no">from_pretrained</code><code class="p" translate="no">(</code><code translate="no">
</code><code translate="no">    </code><code class="s2" translate="no">"</code><code class="s2" translate="no">jinaai/jina-embeddings-v2-base-en</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">trust_remote_code</code><code class="o" translate="no">=</code><code class="kc" translate="no">True</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO8-1" id="co_achieving_concurrency_in_ai_workloads_CO8-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a><code translate="no">
</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">
</code><code class="k" translate="no">async</code><code translate="no"> </code><code class="k" translate="no">def</code><code translate="no"> </code><code class="nf" translate="no">load</code><code class="p" translate="no">(</code><code class="n" translate="no">filepath</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="nb" translate="no">str</code><code class="p" translate="no">)</code><code translate="no"> </code><code class="o" translate="no">-</code><code class="o" translate="no">&gt;</code><code translate="no"> </code><code class="n" translate="no">AsyncGenerator</code><code class="p" translate="no">[</code><code class="nb" translate="no">str</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">Any</code><code class="p" translate="no">]</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">async</code><code translate="no"> </code><code class="k" translate="no">with</code><code translate="no"> </code><code class="n" translate="no">aiofiles</code><code class="o" translate="no">.</code><code class="n" translate="no">open</code><code class="p" translate="no">(</code><code class="n" translate="no">filepath</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="s2" translate="no">"</code><code class="s2" translate="no">r</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">encoding</code><code class="o" translate="no">=</code><code class="s2" translate="no">"</code><code class="s2" translate="no">utf-8</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code translate="no"> </code><code class="k" translate="no">as</code><code translate="no"> </code><code class="n" translate="no">f</code><code class="p" translate="no">:</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO8-2" id="co_achieving_concurrency_in_ai_workloads_CO8-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">        </code><code class="k" translate="no">while</code><code translate="no"> </code><code class="n" translate="no">chunk</code><code translate="no"> </code><code class="o" translate="no">:=</code><code translate="no"> </code><code class="k" translate="no">await</code><code translate="no"> </code><code class="n" translate="no">f</code><code class="o" translate="no">.</code><code class="n" translate="no">read</code><code class="p" translate="no">(</code><code class="n" translate="no">DEFAULT_CHUNK_SIZE</code><code class="p" translate="no">)</code><code class="p" translate="no">:</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO8-3" id="co_achieving_concurrency_in_ai_workloads_CO8-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">            </code><code class="k" translate="no">yield</code><code translate="no"> </code><code class="n" translate="no">chunk</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO8-4" id="co_achieving_concurrency_in_ai_workloads_CO8-4"><img alt="4" src="assets/4.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">
</code><code class="k" translate="no">def</code><code translate="no"> </code><code class="nf" translate="no">clean</code><code class="p" translate="no">(</code><code class="n" translate="no">text</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="nb" translate="no">str</code><code class="p" translate="no">)</code><code translate="no"> </code><code class="o" translate="no">-</code><code class="o" translate="no">&gt;</code><code translate="no"> </code><code class="nb" translate="no">str</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">t</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">text</code><code class="o" translate="no">.</code><code class="n" translate="no">replace</code><code class="p" translate="no">(</code><code class="s2" translate="no">"</code><code class="se" translate="no">\n</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="s2" translate="no">"</code><code class="s2" translate="no"> </code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">t</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">re</code><code class="o" translate="no">.</code><code class="n" translate="no">sub</code><code class="p" translate="no">(</code><code class="sa" translate="no">r</code><code class="s2" translate="no">"</code><code class="s2" translate="no">\</code><code class="s2" translate="no">s+</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="s2" translate="no">"</code><code class="s2" translate="no"> </code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">t</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">t</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">re</code><code class="o" translate="no">.</code><code class="n" translate="no">sub</code><code class="p" translate="no">(</code><code class="sa" translate="no">r</code><code class="s2" translate="no">"</code><code class="s2" translate="no">\</code><code class="s2" translate="no">. ,</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="s2" translate="no">"</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">t</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">t</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">t</code><code class="o" translate="no">.</code><code class="n" translate="no">replace</code><code class="p" translate="no">(</code><code class="s2" translate="no">"</code><code class="s2" translate="no">..</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="s2" translate="no">"</code><code class="s2" translate="no">.</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">t</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">t</code><code class="o" translate="no">.</code><code class="n" translate="no">replace</code><code class="p" translate="no">(</code><code class="s2" translate="no">"</code><code class="s2" translate="no">. .</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="s2" translate="no">"</code><code class="s2" translate="no">.</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">cleaned_text</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">t</code><code class="o" translate="no">.</code><code class="n" translate="no">replace</code><code class="p" translate="no">(</code><code class="s2" translate="no">"</code><code class="se" translate="no">\n</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="s2" translate="no">"</code><code class="s2" translate="no"> </code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code class="o" translate="no">.</code><code class="n" translate="no">strip</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">return</code><code translate="no"> </code><code class="n" translate="no">cleaned_text</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO8-5" id="co_achieving_concurrency_in_ai_workloads_CO8-5"><img alt="5" src="assets/5.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">
</code><code class="k" translate="no">def</code><code translate="no"> </code><code class="nf" translate="no">embed</code><code class="p" translate="no">(</code><code class="n" translate="no">text</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="nb" translate="no">str</code><code class="p" translate="no">)</code><code translate="no"> </code><code class="o" translate="no">-</code><code class="o" translate="no">&gt;</code><code translate="no"> </code><code class="nb" translate="no">list</code><code class="p" translate="no">[</code><code class="nb" translate="no">float</code><code class="p" translate="no">]</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">return</code><code translate="no"> </code><code class="n" translate="no">embedder</code><code class="o" translate="no">.</code><code class="n" translate="no">encode</code><code class="p" translate="no">(</code><code class="n" translate="no">text</code><code class="p" translate="no">)</code><code class="o" translate="no">.</code><code class="n" translate="no">tolist</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO8-6" id="co_achieving_concurrency_in_ai_workloads_CO8-6"><img alt="6" src="assets/6.png" width="12" height="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO8-1" id="callout_achieving_concurrency_in_ai_workloads_CO8-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Scarica e utilizza il modello open source <code translate="no">jina-embeddings-v2-base-en</code> per incorporare stringhe di testo in vettori di incorporamento. Imposta <code translate="no">trust_remote_code=True</code> per scaricare i pesi del modello e le configurazioni del tokenizer. Senza questo parametro impostato su <code translate="no">True</code>, i pesi del modello scaricato saranno inizializzati con valori casuali invece che con valori addestrati.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO8-2" id="callout_achieving_concurrency_in_ai_workloads_CO8-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a></dt>
<dd><p>Utilizza la libreria <code translate="no">aiofiles</code> per aprire una connessione asincrona a un file del filesystem.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO8-3" id="callout_achieving_concurrency_in_ai_workloads_CO8-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a></dt>
<dd><p>Carica il contenuto dei documenti di testo in pezzi per un'<span class="keep-together">operazione di</span> I/O efficiente dal punto di vista della memoria.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO8-4" id="callout_achieving_concurrency_in_ai_workloads_CO8-4"><img alt="4" src="assets/4.png" width="12" height="12"/></a></dt>
<dd><p>Invece di restituire un <code translate="no">chunk</code>, lo rende in modo che la funzione <code translate="no">load()</code> diventi un <em>generatore asincrono</em>. I generatori asincroni possono essere iterati con <code translate="no">async for loop</code>s in modo che le operazioni di blocco al loro interno possano essere <code translate="no">await</code>ed essere avviate/riprese dal ciclo di eventi. Sia i cicli asincroni <code translate="no">for</code> che i normali cicli <code translate="no">for</code> iterano in modo sequenziale sull'iterabile, ma i cicli asincroni <code translate="no">for</code> permettono di iterare su un iteratore asincrono.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO8-5" id="callout_achieving_concurrency_in_ai_workloads_CO8-5"><img alt="5" src="assets/5.png" width="12" height="12"/></a></dt>
<dd><p>Pulisci il testo eliminando gli spazi, le virgole, i punti e le interruzioni di riga.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO8-6" id="callout_achieving_concurrency_in_ai_workloads_CO8-6"><img alt="6" src="assets/6.png" width="12" height="12"/></a></dt>
<dd><p>Utilizza il modello di incorporazione Jina per convertire un brano di testo in un vettore di incorporazione.</p></dd>
</dl></div>
<p><a data-primary="vector database" data-type="indexterm" id="id837"/>Una volta che i dati sono stati elaborati in vettori di incorporamento, puoi memorizzarli nel <em>database dei vettori</em>.</p>
<p class="less_space pagebreak-before">A differenza delle alternative convenzionali come i database relazionali, un database vettoriale è progettato specificamente per gestire operazioni di archiviazione e recupero dei dati ottimizzate per la <em>ricerca semantica</em>, che produce risultati migliori rispetto alle ricerche per parole chiave che possono restituire risultati non ottimali o incompleti.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id838">
<h1>Eseguire una ricerca semantica</h1>
<p><a data-primary="cosine similarity" data-type="indexterm" id="id839"/><a data-primary="semantic search" data-type="indexterm" id="id840"/>La ricerca semantica utilizza un'operazione matematica chiamata <em>somiglianza del coseno</em>tra il vettore di incorporamento della query dell'utente e i vettori di incorporamento (cioè i documenti incorporati) memorizzati nel database per recuperare gli elementi più rilevanti.</p>
<p>Il calcolo della somiglianza del coseno esegue un calcolo normalizzato del prodotto del punto tra due vettori per restituire un numero compreso tra -1 e 1. Il calcolo della somiglianza del coseno è un calcolo normalizzato del prodotto del punto tra due vettori.<sup><a data-type="noteref" href="ch05.html#id841" id="id841-marker" translate="no">6</a></sup>
La normalizzazione garantisce che il risultato sia compreso tra -1 e 1.</p>
<p class="fix_tracking">
Un punteggio di 1 significa che i vettori sono allineati e condividono un significato semantico simile, mentre un punteggio di -1 implica che sono diametralmente opposti e significano significati opposti.</p>
<p>Un punteggio di 0 indica l'assenza di correlazione semantica tra i vettori, suggerendo che questi vettori possono essere esclusi dai risultati della ricerca.</p>
<p>I risultati restituiti dal database sono una raccolta di vettori di incorporamento ordinati che contengono metadati, tra cui il testo originale. Puoi iniettare questi frammenti di testo direttamente nei prompt degli utenti per arricchirli di un contesto rilevante prima di inoltrarli al LLM.</p>
<p>I risultati della ricerca semantica recuperati dal database sono ordinati in ordine decrescente in base ai punteggi di somiglianza.<a data-primary="context ranking" data-type="indexterm" id="id842"/>Questo ordinamento, definito come <em>classificazione del contesto</em>, è importante in quanto la ricerca ha dimostrato che le LLMs sono più sensibili alle parole che compaiono prima nel prompt rispetto a quelle successive.</p>
<p>Questo ha senso perché anche noi esseri umani abbiamo una migliore capacità di comprensione e attenzione, in un'unica sessione di lettura, all'inizio di documenti di grandi dimensioni piuttosto che a metà o alla fine. Ecco perché abbiamo sezioni come un "Executive Summary" nei report di grandi dimensioni per comunicare ai lettori il contenuto più importante del report.</p>
</div></aside>
<p class="less_space pagebreak-before">I seguenti esempi di codice richiedono l'esecuzione di un'istanza locale del database vettoriale <code translate="no">qdrant</code> sul tuo computer locale per il modulo RAG. L'impostazione di un database locale ti permetterà di fare esperienza pratica di lavoro asincrono con database vettoriali di livello di produzione. Per eseguire il database in un container, devi avere Docker installato sul tuo computer e poi estrarre ed eseguire il container del database vettoriale <code translate="no">qdrant</code>.<sup><a data-type="noteref" href="ch05.html#id843" id="id843-marker" translate="no">7</a></sup>
Se non hai familiarità con Docker, non preoccuparti: imparerai di più su Docker e la containerizzazione nel<a data-type="xref" href="ch12.html#ch12">Capitolo 12</a>.</p>
<pre data-code-language="bash" data-type="programlisting" translate="no"><code translate="no">$</code><code class="w" translate="no"> </code><code translate="no">docker</code><code class="w" translate="no"> </code><code translate="no">pull</code><code class="w" translate="no"> </code><code translate="no">qdrant/qdrant</code><code class="w" translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO9-1" id="co_achieving_concurrency_in_ai_workloads_CO9-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a><code class="w" translate="no">
</code><code translate="no">$</code><code class="w" translate="no"> </code><code translate="no">docker</code><code class="w" translate="no"> </code><code translate="no">run</code><code class="w" translate="no"> </code><code translate="no">-p</code><code class="w" translate="no"> </code><code class="m" translate="no">6333</code><code translate="no">:6333</code><code class="w" translate="no"> </code><code translate="no">-p</code><code class="w" translate="no"> </code><code class="m" translate="no">6334</code><code translate="no">:6334</code><code class="w" translate="no"> </code><code class="se" translate="no">\ </code><code class="w" translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO9-2" id="co_achieving_concurrency_in_ai_workloads_CO9-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a><code class="w" translate="no">
    </code><code translate="no">-v</code><code class="w" translate="no"> </code><code class="k" translate="no">$(</code><code class="nb" translate="no">pwd</code><code class="k" translate="no">)</code><code translate="no">/qdrant_storage:/qdrant/storage:z</code><code class="w" translate="no"> </code><code class="se" translate="no">\ </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO9-3" id="co_achieving_concurrency_in_ai_workloads_CO9-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a><code class="w" translate="no">
    </code><code translate="no">qdrant/qdrant</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO9-1" id="callout_achieving_concurrency_in_ai_workloads_CO9-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Scarica l'immagine del database vettoriale <code translate="no">qdrant</code> dal repository <code translate="no">qdrant</code> nel registro Docker.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO9-2" id="callout_achieving_concurrency_in_ai_workloads_CO9-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a></dt>
<dd><p>Esegui l'immagine <code translate="no">qdrant/qdrant</code>, quindi esponi e mappa le porte del container <code translate="no">6333</code> e <code translate="no">6334</code> sulle stesse porte del computer host.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO9-3" id="callout_achieving_concurrency_in_ai_workloads_CO9-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a></dt>
<dd><p>Monta il database <code translate="no">qdrant</code> sul filesystem del computer host nella directory principale del tuo progetto.</p></dd>
</dl>
<p>Poiché l'archiviazione e il recupero del database sono operazioni di I/O, dovresti utilizzare un client di database asincrono. Fortunatamente, <code translate="no">qdrant</code> fornisce un client di database asincrono con cui lavorare.</p>
<div data-type="tip"><h6>Suggerimento</h6>
<p>Puoi utilizzare altri fornitori di database vettoriali come Weaviate, Elastic, Milvus, Pinecone, Chroma o altri in sostituzione di Qdrant. Ognuno di essi ha una serie di caratteristiche e limitazioni da considerare per il tuo caso d'uso.</p>
<p>Se scegli un altro provider di database, assicurati che sia disponibile un client di database asincrono che puoi utilizzare.</p>
</div>
<p>Invece di scrivere diverse funzioni per memorizzare e recuperare i dati dal database, puoi usare il modello di repository menzionato nel <a data-type="xref" href="ch02.html#ch02">Capitolo 2</a>. Con il modello di repository, puoi astrarre le operazioni di basso livello di creazione, lettura, aggiornamento e cancellazione del database con dei valori predefiniti che corrispondono al tuo caso d'uso.</p>
<p class="less_space pagebreak-before">L<a data-type="xref" href="#rag_repository">'esempio 5-11</a> mostra l'implementazione del modello di repository per il database vettoriale Qdrant.</p>
<div data-type="example" id="rag_repository">
<h5><span class="label">Esempio 5-11. </span>Configurazione del client del database vettoriale utilizzando il modello del repository</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="c1" translate="no"># rag/repository.py</code><code translate="no">
</code><code translate="no">
</code><code class="kn" translate="no">from</code><code translate="no"> </code><code class="nn" translate="no">loguru</code><code translate="no"> </code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="n" translate="no">logger</code><code translate="no">
</code><code class="kn" translate="no">from</code><code translate="no"> </code><code class="nn" translate="no">qdrant_client</code><code translate="no"> </code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="n" translate="no">AsyncQdrantClient</code><code translate="no">
</code><code class="kn" translate="no">from</code><code translate="no"> </code><code class="nn" translate="no">qdrant_client</code><code class="nn" translate="no">.</code><code class="nn" translate="no">http</code><code translate="no"> </code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="n" translate="no">models</code><code translate="no">
</code><code class="kn" translate="no">from</code><code translate="no"> </code><code class="nn" translate="no">qdrant_client</code><code class="nn" translate="no">.</code><code class="nn" translate="no">http</code><code class="nn" translate="no">.</code><code class="nn" translate="no">models</code><code translate="no"> </code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="n" translate="no">ScoredPoint</code><code translate="no">
</code><code translate="no">
</code><code translate="no">
</code><code class="k" translate="no">class</code><code translate="no"> </code><code class="nc" translate="no">VectorRepository</code><code class="p" translate="no">:</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO10-1" id="co_achieving_concurrency_in_ai_workloads_CO10-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">def</code><code translate="no"> </code><code class="fm" translate="no">__init__</code><code class="p" translate="no">(</code><code class="bp" translate="no">self</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">host</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="nb" translate="no">str</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="s2" translate="no">"</code><code class="s2" translate="no">localhost</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">port</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="nb" translate="no">int</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="mi" translate="no">6333</code><code class="p" translate="no">)</code><code translate="no"> </code><code class="o" translate="no">-</code><code class="o" translate="no">&gt;</code><code translate="no"> </code><code class="kc" translate="no">None</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">        </code><code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">db_client</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">AsyncQdrantClient</code><code class="p" translate="no">(</code><code class="n" translate="no">host</code><code class="o" translate="no">=</code><code class="n" translate="no">host</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">port</code><code class="o" translate="no">=</code><code class="n" translate="no">port</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">async</code><code translate="no"> </code><code class="k" translate="no">def</code><code translate="no"> </code><code class="nf" translate="no">create_collection</code><code class="p" translate="no">(</code><code class="bp" translate="no">self</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">collection_name</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="nb" translate="no">str</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">size</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="nb" translate="no">int</code><code class="p" translate="no">)</code><code translate="no"> </code><code class="o" translate="no">-</code><code class="o" translate="no">&gt;</code><code translate="no"> </code><code class="nb" translate="no">bool</code><code class="p" translate="no">:</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO10-2" id="co_achieving_concurrency_in_ai_workloads_CO10-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">        </code><code class="n" translate="no">vectors_config</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">models</code><code class="o" translate="no">.</code><code class="n" translate="no">VectorParams</code><code class="p" translate="no">(</code><code translate="no">
</code><code translate="no">            </code><code class="n" translate="no">size</code><code class="o" translate="no">=</code><code class="n" translate="no">size</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">distance</code><code class="o" translate="no">=</code><code class="n" translate="no">models</code><code class="o" translate="no">.</code><code class="n" translate="no">Distance</code><code class="o" translate="no">.</code><code class="n" translate="no">COSINE</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO10-3" id="co_achieving_concurrency_in_ai_workloads_CO10-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">        </code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">        </code><code class="n" translate="no">response</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="k" translate="no">await</code><code translate="no"> </code><code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">db_client</code><code class="o" translate="no">.</code><code class="n" translate="no">get_collections</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">
</code><code translate="no">        </code><code class="n" translate="no">collection_exists</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="nb" translate="no">any</code><code class="p" translate="no">(</code><code translate="no">
</code><code translate="no">            </code><code class="n" translate="no">collection</code><code class="o" translate="no">.</code><code class="n" translate="no">name</code><code translate="no"> </code><code class="o" translate="no">==</code><code translate="no"> </code><code class="n" translate="no">collection_name</code><code translate="no">
</code><code translate="no">            </code><code class="k" translate="no">for</code><code translate="no"> </code><code class="n" translate="no">collection</code><code translate="no"> </code><code class="ow" translate="no">in</code><code translate="no"> </code><code class="n" translate="no">response</code><code class="o" translate="no">.</code><code class="n" translate="no">collections</code><code translate="no">
</code><code translate="no">        </code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">        </code><code class="k" translate="no">if</code><code translate="no"> </code><code class="n" translate="no">collection_exists</code><code class="p" translate="no">:</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO10-4" id="co_achieving_concurrency_in_ai_workloads_CO10-4"><img alt="4" src="assets/4.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">            </code><code class="n" translate="no">logger</code><code class="o" translate="no">.</code><code class="n" translate="no">debug</code><code class="p" translate="no">(</code><code translate="no">
</code><code translate="no">                </code><code class="sa" translate="no">f</code><code class="s2" translate="no">"</code><code class="s2" translate="no">Collection </code><code class="si" translate="no">{</code><code class="n" translate="no">collection_name</code><code class="si" translate="no">}</code><code class="s2" translate="no"> already exists - recreating it</code><code class="s2" translate="no">"</code><code translate="no">
</code><code translate="no">            </code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">            </code><code class="k" translate="no">await</code><code translate="no"> </code><code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">db_client</code><code class="o" translate="no">.</code><code class="n" translate="no">delete_collection</code><code class="p" translate="no">(</code><code class="n" translate="no">collection_name</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">            </code><code class="k" translate="no">return</code><code translate="no"> </code><code class="k" translate="no">await</code><code translate="no"> </code><code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">db_client</code><code class="o" translate="no">.</code><code class="n" translate="no">create_collection</code><code class="p" translate="no">(</code><code translate="no">
</code><code translate="no">                </code><code class="n" translate="no">collection_name</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">                </code><code class="n" translate="no">vectors_config</code><code class="o" translate="no">=</code><code class="n" translate="no">vectors_config</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">            </code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">
</code><code translate="no">        </code><code class="n" translate="no">logger</code><code class="o" translate="no">.</code><code class="n" translate="no">debug</code><code class="p" translate="no">(</code><code class="sa" translate="no">f</code><code class="s2" translate="no">"</code><code class="s2" translate="no">Creating collection </code><code class="si" translate="no">{</code><code class="n" translate="no">collection_name</code><code class="si" translate="no">}</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">        </code><code class="k" translate="no">return</code><code translate="no"> </code><code class="k" translate="no">await</code><code translate="no"> </code><code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">db_client</code><code class="o" translate="no">.</code><code class="n" translate="no">create_collection</code><code class="p" translate="no">(</code><code translate="no">
</code><code translate="no">            </code><code class="n" translate="no">collection_name</code><code class="o" translate="no">=</code><code class="n" translate="no">collection_name</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">            </code><code class="n" translate="no">vectors_config</code><code class="o" translate="no">=</code><code class="n" translate="no">models</code><code class="o" translate="no">.</code><code class="n" translate="no">VectorParams</code><code class="p" translate="no">(</code><code translate="no">
</code><code translate="no">                </code><code class="n" translate="no">size</code><code class="o" translate="no">=</code><code class="n" translate="no">size</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">distance</code><code class="o" translate="no">=</code><code class="n" translate="no">models</code><code class="o" translate="no">.</code><code class="n" translate="no">Distance</code><code class="o" translate="no">.</code><code class="n" translate="no">COSINE</code><code translate="no">
</code><code translate="no">            </code><code class="p" translate="no">)</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">        </code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">async</code><code translate="no"> </code><code class="k" translate="no">def</code><code translate="no"> </code><code class="nf" translate="no">delete_collection</code><code class="p" translate="no">(</code><code class="bp" translate="no">self</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">name</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="nb" translate="no">str</code><code class="p" translate="no">)</code><code translate="no"> </code><code class="o" translate="no">-</code><code class="o" translate="no">&gt;</code><code translate="no"> </code><code class="nb" translate="no">bool</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">        </code><code class="n" translate="no">logger</code><code class="o" translate="no">.</code><code class="n" translate="no">debug</code><code class="p" translate="no">(</code><code class="sa" translate="no">f</code><code class="s2" translate="no">"</code><code class="s2" translate="no">Deleting collection </code><code class="si" translate="no">{</code><code class="n" translate="no">name</code><code class="si" translate="no">}</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">        </code><code class="k" translate="no">return</code><code translate="no"> </code><code class="k" translate="no">await</code><code translate="no"> </code><code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">db_client</code><code class="o" translate="no">.</code><code class="n" translate="no">delete_collection</code><code class="p" translate="no">(</code><code class="n" translate="no">name</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">async</code><code translate="no"> </code><code class="k" translate="no">def</code><code translate="no"> </code><code class="nf" translate="no">create</code><code class="p" translate="no">(</code><code translate="no">
</code><code translate="no">        </code><code class="bp" translate="no">self</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">        </code><code class="n" translate="no">collection_name</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="nb" translate="no">str</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">        </code><code class="n" translate="no">embedding_vector</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="nb" translate="no">list</code><code class="p" translate="no">[</code><code class="nb" translate="no">float</code><code class="p" translate="no">]</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">        </code><code class="n" translate="no">original_text</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="nb" translate="no">str</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">        </code><code class="n" translate="no">source</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="nb" translate="no">str</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">    </code><code class="p" translate="no">)</code><code translate="no"> </code><code class="o" translate="no">-</code><code class="o" translate="no">&gt;</code><code translate="no"> </code><code class="kc" translate="no">None</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">        </code><code class="n" translate="no">response</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="k" translate="no">await</code><code translate="no"> </code><code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">db_client</code><code class="o" translate="no">.</code><code class="n" translate="no">count</code><code class="p" translate="no">(</code><code class="n" translate="no">collection_name</code><code class="o" translate="no">=</code><code class="n" translate="no">collection_name</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">        </code><code class="n" translate="no">logger</code><code class="o" translate="no">.</code><code class="n" translate="no">debug</code><code class="p" translate="no">(</code><code translate="no">
</code><code translate="no">            </code><code class="sa" translate="no">f</code><code class="s2" translate="no">"</code><code class="s2" translate="no">Creating a new vector with ID </code><code class="si" translate="no">{</code><code class="n" translate="no">response</code><code class="o" translate="no">.</code><code class="n" translate="no">count</code><code class="si" translate="no">}</code><code class="s2" translate="no"> </code><code class="s2" translate="no">"</code><code translate="no">
</code><code translate="no">            </code><code class="sa" translate="no">f</code><code class="s2" translate="no">"</code><code class="s2" translate="no">inside the </code><code class="si" translate="no">{</code><code class="n" translate="no">collection_name</code><code class="si" translate="no">}</code><code class="s2" translate="no">"</code><code translate="no">
</code><code translate="no">        </code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">        </code><code class="k" translate="no">await</code><code translate="no"> </code><code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">db_client</code><code class="o" translate="no">.</code><code class="n" translate="no">upsert</code><code class="p" translate="no">(</code><code translate="no">
</code><code translate="no">            </code><code class="n" translate="no">collection_name</code><code class="o" translate="no">=</code><code class="n" translate="no">collection_name</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">            </code><code class="n" translate="no">points</code><code class="o" translate="no">=</code><code class="p" translate="no">[</code><code translate="no">
</code><code translate="no">                </code><code class="n" translate="no">models</code><code class="o" translate="no">.</code><code class="n" translate="no">PointStruct</code><code class="p" translate="no">(</code><code translate="no">
</code><code translate="no">                    </code><code class="nb" translate="no">id</code><code class="o" translate="no">=</code><code class="n" translate="no">response</code><code class="o" translate="no">.</code><code class="n" translate="no">count</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">                    </code><code class="n" translate="no">vector</code><code class="o" translate="no">=</code><code class="n" translate="no">embedding_vector</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">                    </code><code class="n" translate="no">payload</code><code class="o" translate="no">=</code><code class="p" translate="no">{</code><code translate="no">
</code><code translate="no">                        </code><code class="s2" translate="no">"</code><code class="s2" translate="no">source</code><code class="s2" translate="no">"</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="n" translate="no">source</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">                        </code><code class="s2" translate="no">"</code><code class="s2" translate="no">original_text</code><code class="s2" translate="no">"</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="n" translate="no">original_text</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">                    </code><code class="p" translate="no">}</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">                </code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">            </code><code class="p" translate="no">]</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">        </code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">async</code><code translate="no"> </code><code class="k" translate="no">def</code><code translate="no"> </code><code class="nf" translate="no">search</code><code class="p" translate="no">(</code><code translate="no">
</code><code translate="no">        </code><code class="bp" translate="no">self</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">        </code><code class="n" translate="no">collection_name</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="nb" translate="no">str</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">        </code><code class="n" translate="no">query_vector</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="nb" translate="no">list</code><code class="p" translate="no">[</code><code class="nb" translate="no">float</code><code class="p" translate="no">]</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">        </code><code class="n" translate="no">retrieval_limit</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="nb" translate="no">int</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">        </code><code class="n" translate="no">score_threshold</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="nb" translate="no">float</code><code class="p" translate="no">,</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO10-5" id="co_achieving_concurrency_in_ai_workloads_CO10-5"><img alt="5" src="assets/5.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">    </code><code class="p" translate="no">)</code><code translate="no"> </code><code class="o" translate="no">-</code><code class="o" translate="no">&gt;</code><code translate="no"> </code><code class="nb" translate="no">list</code><code class="p" translate="no">[</code><code class="n" translate="no">ScoredPoint</code><code class="p" translate="no">]</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">        </code><code class="n" translate="no">logger</code><code class="o" translate="no">.</code><code class="n" translate="no">debug</code><code class="p" translate="no">(</code><code translate="no">
</code><code translate="no">            </code><code class="sa" translate="no">f</code><code class="s2" translate="no">"</code><code class="s2" translate="no">Searching for relevant items in the </code><code class="si" translate="no">{</code><code class="n" translate="no">collection_name</code><code class="si" translate="no">}</code><code class="s2" translate="no"> collection</code><code class="s2" translate="no">"</code><code translate="no">
</code><code translate="no">        </code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">        </code><code class="n" translate="no">response</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="k" translate="no">await</code><code translate="no"> </code><code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">db_client</code><code class="o" translate="no">.</code><code class="n" translate="no">query_points</code><code class="p" translate="no">(</code><code translate="no">
</code><code translate="no">            </code><code class="n" translate="no">collection_name</code><code class="o" translate="no">=</code><code class="n" translate="no">collection_name</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">            </code><code class="n" translate="no">query_vector</code><code class="o" translate="no">=</code><code class="n" translate="no">query_vector</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">            </code><code class="n" translate="no">limit</code><code class="o" translate="no">=</code><code class="n" translate="no">retrieval_limit</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">            </code><code class="n" translate="no">score_threshold</code><code class="o" translate="no">=</code><code class="n" translate="no">score_threshold</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">        </code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">        </code><code class="k" translate="no">return</code><code translate="no"> </code><code class="n" translate="no">response</code><code class="o" translate="no">.</code><code class="n" translate="no">points</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO10-1" id="callout_achieving_concurrency_in_ai_workloads_CO10-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Utilizza il pattern repository per interagire con il database vettoriale tramite un client asincrono. Normalmente, nel pattern repository implementerai i metodi <code translate="no">create</code>, <code translate="no">get</code>, <code translate="no">update</code> e <code translate="no">delete</code>. Ma per ora implementiamo i metodi <code translate="no">create_​col⁠lection</code>, <code translate="no">delete_collection</code>, <code translate="no">create</code> e <code translate="no">search</code>.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO10-2" id="callout_achieving_concurrency_in_ai_workloads_CO10-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a></dt>
<dd><p>I vettori devono essere memorizzati in una collezione. Una collezione è un insieme di punti nominati che puoi utilizzare durante una ricerca. Le collezioni sono simili alle tabelle di un database relazionale.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO10-3" id="callout_achieving_concurrency_in_ai_workloads_CO10-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a></dt>
<dd><p>Comunica al database che tutti i vettori di questa collezione devono essere confrontati tramite il calcolo della somiglianza del coseno che calcola le distanze tra i vettori.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO10-4" id="callout_achieving_concurrency_in_ai_workloads_CO10-4"><img alt="4" src="assets/4.png" width="12" height="12"/></a></dt>
<dd><p>Controlla se una collezione esiste prima di crearne una nuova, altrimenti crea nuovamente la collezione.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO10-5" id="callout_achieving_concurrency_in_ai_workloads_CO10-5"><img alt="5" src="assets/5.png" width="12" height="12"/></a></dt>
<dd><p>Imposta <code translate="no">retrieval_limit</code> e <code translate="no">score_threshold</code> per limitare il numero di elementi nei risultati della ricerca.</p></dd>
</dl></div>
<p>La classe <code translate="no">VectorRepository</code> dovrebbe ora rendere più semplice l'interazione con il database.</p>
<p>Quando si memorizzano le incorporazioni vettoriali, si memorizzano anche alcuni <em>metadati</em>, tra cui il nome del documento di origine, la posizione del testo all'interno della fonte e il testo originale estratto. I sistemi RAG si basano su questi metadati per aumentare i prompt di LLM e per mostrare le informazioni sulla fonte agli utenti.</p>
<div data-type="tip"><h6>Suggerimento</h6>
<p>Attualmente, la conversione del testo in vettori di incorporamento è un processo irreversibile. Pertanto, dovrai memorizzare il testo che ha creato l'incorporamento insieme al vettore di incorporamento come metadati.</p>
</div>
<p>Ora puoi estendere il sito <code translate="no">VectorRepository</code> e creare il sito <code translate="no">VectorService</code> che ti permette di concatenare la pipeline di elaborazione e archiviazione dei dati, come mostrato nell'<a data-type="xref" href="#rag_db_service">esempio 5-12</a>.</p>
<div data-type="example" id="rag_db_service">
<h5><span class="label">Esempio 5-12. </span>Servizio di database vettoriale</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="c1" translate="no"># rag/service.py</code><code translate="no">
</code><code translate="no">
</code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="nn" translate="no">os</code><code translate="no">
</code><code translate="no">
</code><code class="kn" translate="no">from</code><code translate="no"> </code><code class="nn" translate="no">loguru</code><code translate="no"> </code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="n" translate="no">logger</code><code translate="no">
</code><code class="kn" translate="no">from</code><code translate="no"> </code><code class="nn" translate="no">.</code><code class="nn" translate="no">repository</code><code translate="no"> </code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="n" translate="no">VectorRepository</code><code translate="no">
</code><code class="kn" translate="no">from</code><code translate="no"> </code><code class="nn" translate="no">.</code><code class="nn" translate="no">transform</code><code translate="no"> </code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="n" translate="no">clean</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">embed</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">load</code><code translate="no">
</code><code translate="no">
</code><code translate="no">
</code><code class="k" translate="no">class</code><code translate="no"> </code><code class="nc" translate="no">VectorService</code><code class="p" translate="no">(</code><code class="n" translate="no">VectorRepository</code><code class="p" translate="no">)</code><code class="p" translate="no">:</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO11-1" id="co_achieving_concurrency_in_ai_workloads_CO11-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">def</code><code translate="no"> </code><code class="fm" translate="no">__init__</code><code class="p" translate="no">(</code><code class="bp" translate="no">self</code><code class="p" translate="no">)</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">        </code><code class="nb" translate="no">super</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code class="o" translate="no">.</code><code class="fm" translate="no">__init__</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">async</code><code translate="no"> </code><code class="k" translate="no">def</code><code translate="no"> </code><code class="nf" translate="no">store_file_content_in_db</code><code class="p" translate="no">(</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO11-2" id="co_achieving_concurrency_in_ai_workloads_CO11-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">        </code><code class="bp" translate="no">self</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">        </code><code class="n" translate="no">filepath</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="nb" translate="no">str</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">        </code><code class="n" translate="no">chunk_size</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="nb" translate="no">int</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="mi" translate="no">512</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">        </code><code class="n" translate="no">collection_name</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="nb" translate="no">str</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="s2" translate="no">"</code><code class="s2" translate="no">knowledgebase</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">        </code><code class="n" translate="no">collection_size</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="nb" translate="no">int</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="mi" translate="no">768</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">    </code><code class="p" translate="no">)</code><code translate="no"> </code><code class="o" translate="no">-</code><code class="o" translate="no">&gt;</code><code translate="no"> </code><code class="kc" translate="no">None</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">        </code><code class="k" translate="no">await</code><code translate="no"> </code><code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">create_collection</code><code class="p" translate="no">(</code><code class="n" translate="no">collection_name</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">collection_size</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">        </code><code class="n" translate="no">logger</code><code class="o" translate="no">.</code><code class="n" translate="no">debug</code><code class="p" translate="no">(</code><code class="sa" translate="no">f</code><code class="s2" translate="no">"</code><code class="s2" translate="no">Inserting </code><code class="si" translate="no">{</code><code class="n" translate="no">filepath</code><code class="si" translate="no">}</code><code class="s2" translate="no"> content into database</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">        </code><code class="k" translate="no">async</code><code translate="no"> </code><code class="k" translate="no">for</code><code translate="no"> </code><code class="n" translate="no">chunk</code><code translate="no"> </code><code class="ow" translate="no">in</code><code translate="no"> </code><code class="n" translate="no">load</code><code class="p" translate="no">(</code><code class="n" translate="no">filepath</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">chunk_size</code><code class="p" translate="no">)</code><code class="p" translate="no">:</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO11-3" id="co_achieving_concurrency_in_ai_workloads_CO11-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">            </code><code class="n" translate="no">logger</code><code class="o" translate="no">.</code><code class="n" translate="no">debug</code><code class="p" translate="no">(</code><code class="sa" translate="no">f</code><code class="s2" translate="no">"</code><code class="s2" translate="no">Inserting </code><code class="s2" translate="no">'</code><code class="si" translate="no">{</code><code class="n" translate="no">chunk</code><code class="p" translate="no">[</code><code class="mi" translate="no">0</code><code class="p" translate="no">:</code><code class="mi" translate="no">20</code><code class="p" translate="no">]</code><code class="si" translate="no">}</code><code class="s2" translate="no">...</code><code class="s2" translate="no">'</code><code class="s2" translate="no"> into database</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">
</code><code translate="no">            </code><code class="n" translate="no">embedding_vector</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">embed</code><code class="p" translate="no">(</code><code class="n" translate="no">clean</code><code class="p" translate="no">(</code><code class="n" translate="no">chunk</code><code class="p" translate="no">)</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">            </code><code class="n" translate="no">filename</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">os</code><code class="o" translate="no">.</code><code class="n" translate="no">path</code><code class="o" translate="no">.</code><code class="n" translate="no">basename</code><code class="p" translate="no">(</code><code class="n" translate="no">filepath</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">            </code><code class="k" translate="no">await</code><code translate="no"> </code><code class="bp" translate="no">self</code><code class="o" translate="no">.</code><code class="n" translate="no">create</code><code class="p" translate="no">(</code><code translate="no">
</code><code translate="no">                </code><code class="n" translate="no">collection_name</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">embedding_vector</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">chunk</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">filename</code><code translate="no">
</code><code translate="no">            </code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">
</code><code translate="no">
</code><code class="n" translate="no">vector_service</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">VectorService</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO11-4" id="co_achieving_concurrency_in_ai_workloads_CO11-4"><img alt="4" src="assets/4.png" width="12" height="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO11-1" id="callout_achieving_concurrency_in_ai_workloads_CO11-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Crea la classe <code translate="no">VectorService</code> ereditando la classe <code translate="no">VectorRepository</code> in modo<span class="keep-together">da</span> poter utilizzare ed estendere i metodi comuni di funzionamento del database dell'<a data-type="xref" href="#rag_repository">Esempio 5-11</a>.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO11-2" id="callout_achieving_concurrency_in_ai_workloads_CO11-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a></dt>
<dd><p>Utilizza il metodo del servizio <code translate="no">store_file_content_in_db</code> per caricare, trasformare e archiviare in modo asincrono i documenti di testo grezzi nel database.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO11-3" id="callout_achieving_concurrency_in_ai_workloads_CO11-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a></dt>
<dd><p>Utilizza un generatore asincrono <code translate="no">load()</code> per caricare pezzi di testo da un file<span class="keep-together">in modo asincrono.</span></p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO11-4" id="callout_achieving_concurrency_in_ai_workloads_CO11-4"><img alt="4" src="assets/4.png" width="12" height="12"/></a></dt>
<dd><p>Crea un'istanza di <code translate="no">VectorService</code> da importare e utilizzare nell'<span class="keep-together">applicazione.</span></p></dd>
</dl></div>
<p>Il passo finale della pipeline di elaborazione e archiviazione dei dati RAG consiste nell'eseguire la logica di estrazione e archiviazione del testo all'interno di <code translate="no">file_upload_controller</code>come attività in background. L'implementazione è mostrata nell'<a data-type="xref" href="#rag_data_processor">Esempio 5-13</a> in modo che il gestore possa attivare entrambe le operazioni in background dopo aver risposto all'utente.</p>
<div data-type="example" id="rag_data_processor">
<h5><span class="label">Esempio 5-13. </span>Aggiornare il gestore di upload per elaborare e memorizzare il contenuto del file PDF nel database vettoriale</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="c1" translate="no"># main.py</code><code translate="no">
</code><code translate="no">
</code><code class="kn" translate="no">from</code><code translate="no"> </code><code class="nn" translate="no">fastapi</code><code translate="no"> </code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="p" translate="no">(</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">BackgroundTasks</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">FastAPI</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">File</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">UploadFile</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">status</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">HTTPException</code><code class="p" translate="no">,</code><code translate="no">
</code><code class="p" translate="no">)</code><code translate="no">
</code><code class="kn" translate="no">from</code><code translate="no"> </code><code class="nn" translate="no">typing</code><code translate="no"> </code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="n" translate="no">Annotated</code><code translate="no">
</code><code class="kn" translate="no">from</code><code translate="no"> </code><code class="nn" translate="no">rag</code><code translate="no"> </code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="n" translate="no">pdf_text_extractor</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">vector_service</code><code translate="no">
</code><code translate="no">
</code><code class="nd" translate="no">@app</code><code class="o" translate="no">.</code><code class="n" translate="no">post</code><code class="p" translate="no">(</code><code class="s2" translate="no">"</code><code class="s2" translate="no">/upload</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code translate="no">
</code><code class="k" translate="no">async</code><code translate="no"> </code><code class="k" translate="no">def</code><code translate="no"> </code><code class="nf" translate="no">file_upload_controller</code><code class="p" translate="no">(</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">file</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="n" translate="no">Annotated</code><code class="p" translate="no">[</code><code class="n" translate="no">UploadFile</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">File</code><code class="p" translate="no">(</code><code class="n" translate="no">description</code><code class="o" translate="no">=</code><code class="s2" translate="no">"</code><code class="s2" translate="no">A file read as UploadFile</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code class="p" translate="no">]</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">bg_text_processor</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="n" translate="no">BackgroundTasks</code><code class="p" translate="no">,</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO12-1" id="co_achieving_concurrency_in_ai_workloads_CO12-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a><code translate="no">
</code><code class="p" translate="no">)</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">    </code><code class="o" translate="no">.</code><code class="o" translate="no">.</code><code class="o" translate="no">.</code><code translate="no"> </code><code class="c1" translate="no"># Raise an HTTPException if data upload is not a PDF file</code><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">try</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">        </code><code class="n" translate="no">filepath</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="k" translate="no">await</code><code translate="no"> </code><code class="n" translate="no">save_file</code><code class="p" translate="no">(</code><code class="n" translate="no">file</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">        </code><code class="n" translate="no">bg_text_processor</code><code class="o" translate="no">.</code><code class="n" translate="no">add_task</code><code class="p" translate="no">(</code><code class="n" translate="no">pdf_text_extractor</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">filepath</code><code class="p" translate="no">)</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO12-2" id="co_achieving_concurrency_in_ai_workloads_CO12-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">        </code><code class="n" translate="no">bg_text_processor</code><code class="o" translate="no">.</code><code class="n" translate="no">add_task</code><code class="p" translate="no">(</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO12-3" id="co_achieving_concurrency_in_ai_workloads_CO12-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">            </code><code class="n" translate="no">vector_service</code><code class="o" translate="no">.</code><code class="n" translate="no">store_file_content_in_db</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">            </code><code class="n" translate="no">filepath</code><code class="o" translate="no">.</code><code class="n" translate="no">replace</code><code class="p" translate="no">(</code><code class="s2" translate="no">"</code><code class="s2" translate="no">pdf</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="s2" translate="no">"</code><code class="s2" translate="no">txt</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">            </code><code class="mi" translate="no">512</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">            </code><code class="s2" translate="no">"</code><code class="s2" translate="no">knowledgebase</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">            </code><code class="mi" translate="no">768</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">        </code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">except</code><code translate="no"> </code><code class="ne" translate="no">Exception</code><code translate="no"> </code><code class="k" translate="no">as</code><code translate="no"> </code><code class="n" translate="no">e</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">        </code><code class="k" translate="no">raise</code><code translate="no"> </code><code class="n" translate="no">HTTPException</code><code class="p" translate="no">(</code><code translate="no">
</code><code translate="no">            </code><code class="n" translate="no">detail</code><code class="o" translate="no">=</code><code class="sa" translate="no">f</code><code class="s2" translate="no">"</code><code class="s2" translate="no">An error occurred while saving file - Error: </code><code class="si" translate="no">{</code><code class="n" translate="no">e</code><code class="si" translate="no">}</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">            </code><code class="n" translate="no">status_code</code><code class="o" translate="no">=</code><code class="n" translate="no">status</code><code class="o" translate="no">.</code><code class="n" translate="no">HTTP_500_INTERNAL_SERVER_ERROR</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">        </code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">return</code><code translate="no"> </code><code class="p" translate="no">{</code><code class="s2" translate="no">"</code><code class="s2" translate="no">filename</code><code class="s2" translate="no">"</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="n" translate="no">file</code><code class="o" translate="no">.</code><code class="n" translate="no">filename</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="s2" translate="no">"</code><code class="s2" translate="no">message</code><code class="s2" translate="no">"</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="s2" translate="no">"</code><code class="s2" translate="no">File uploaded successfully</code><code class="s2" translate="no">"</code><code class="p" translate="no">}</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO12-1" id="callout_achieving_concurrency_in_ai_workloads_CO12-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Inietta la funzione dei compiti in background di FastAPI nel gestore per elaborare i caricamenti di file in background. I compiti in background di FastAPI saranno eseguiti in ordine subito dopo che il gestore avrà inviato una risposta al client.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO12-2" id="callout_achieving_concurrency_in_ai_workloads_CO12-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a></dt>
<dd><p>Esegue la funzione di estrazione del testo PDF in background dopo aver ricevuto una risposta dal client. Poiché <code translate="no">pdf_text_extractor</code> è una funzione sincrona, FastAPI eseguirà questa funzione su un thread separato all'interno del pool di thread per evitare di bloccare il ciclo di eventi.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO12-3" id="callout_achieving_concurrency_in_ai_workloads_CO12-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a></dt>
<dd><p>Esegui la funzione asincrona <code translate="no">vector_service.store_file_content_in_db</code> in background sul ciclo di eventi gestito da FastAPI non appena <code translate="no">pdf_text_extractor</code> ha terminato l'elaborazione. Imposta la funzione per caricare il contenuto del documento di testo in pezzi da 512 caratteri e memorizzarli nella collezione di vettori di <code translate="no">knowledgebase</code>, che accetta vettori di dimensioni 768.</p></dd>
</dl></div>
<p>Dopo aver costruito la pipeline di archiviazione dei dati RAG, ora puoi concentrarti sul sistema di ricerca e recupero, che ti permetterà di aumentare i prompt dell'utente all'LLM con le conoscenze del database. L'<a data-type="xref" href="#rag_generation">Esempio 5-14</a> integra le operazioni di ricerca e recupero RAG con il gestore LLM per aumentare i prompt LLM con un<span class="keep-together">contesto</span> aggiuntivo.</p>
<div data-type="example" id="rag_generation">
<h5><span class="label">Esempio 5-14. </span>Integrazione di RAG con l'endpoint che serve LLM</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="c1" translate="no"># dependencies.py</code><code translate="no">
</code><code translate="no">
</code><code class="kn" translate="no">from</code><code translate="no"> </code><code class="nn" translate="no">rag</code><code translate="no"> </code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="n" translate="no">vector_service</code><code translate="no">
</code><code class="kn" translate="no">from</code><code translate="no"> </code><code class="nn" translate="no">rag</code><code class="nn" translate="no">.</code><code class="nn" translate="no">transform</code><code translate="no"> </code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="n" translate="no">embed</code><code translate="no">
</code><code class="kn" translate="no">from</code><code translate="no"> </code><code class="nn" translate="no">schemas</code><code translate="no"> </code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="n" translate="no">TextModelRequest</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">TextModelResponse</code><code translate="no">
</code><code translate="no">
</code><code class="k" translate="no">async</code><code translate="no"> </code><code class="k" translate="no">def</code><code translate="no"> </code><code class="nf" translate="no">get_rag_content</code><code class="p" translate="no">(</code><code class="n" translate="no">body</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="n" translate="no">TextModelRequest</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">Body</code><code class="p" translate="no">(</code><code class="o" translate="no">.</code><code class="o" translate="no">.</code><code class="o" translate="no">.</code><code class="p" translate="no">)</code><code class="p" translate="no">)</code><code translate="no"> </code><code class="o" translate="no">-</code><code class="o" translate="no">&gt;</code><code translate="no"> </code><code class="nb" translate="no">str</code><code class="p" translate="no">:</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO13-1" id="co_achieving_concurrency_in_ai_workloads_CO13-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">rag_content</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="k" translate="no">await</code><code translate="no"> </code><code class="n" translate="no">vector_service</code><code class="o" translate="no">.</code><code class="n" translate="no">search</code><code class="p" translate="no">(</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO13-2" id="co_achieving_concurrency_in_ai_workloads_CO13-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">        </code><code class="s2" translate="no">"</code><code class="s2" translate="no">knowledgebase</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">embed</code><code class="p" translate="no">(</code><code class="n" translate="no">body</code><code class="o" translate="no">.</code><code class="n" translate="no">prompt</code><code class="p" translate="no">)</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="mi" translate="no">3</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="mf" translate="no">0.7</code><code translate="no">
</code><code translate="no">    </code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">rag_content_str</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="s2" translate="no">"</code><code class="se" translate="no">\n</code><code class="s2" translate="no">"</code><code class="o" translate="no">.</code><code class="n" translate="no">join</code><code class="p" translate="no">(</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO13-3" id="co_achieving_concurrency_in_ai_workloads_CO13-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">        </code><code class="p" translate="no">[</code><code class="n" translate="no">c</code><code class="o" translate="no">.</code><code class="n" translate="no">payload</code><code class="p" translate="no">[</code><code class="s2" translate="no">"</code><code class="s2" translate="no">original_text</code><code class="s2" translate="no">"</code><code class="p" translate="no">]</code><code translate="no"> </code><code class="k" translate="no">for</code><code translate="no"> </code><code class="n" translate="no">c</code><code translate="no"> </code><code class="ow" translate="no">in</code><code translate="no"> </code><code class="n" translate="no">rag_content</code><code class="p" translate="no">]</code><code translate="no">
</code><code translate="no">    </code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">return</code><code translate="no"> </code><code class="n" translate="no">rag_content_str</code><code translate="no">
</code><code translate="no">
</code><code translate="no">
</code><code class="c1" translate="no"># main.py</code><code translate="no">
</code><code translate="no">
</code><code class="o" translate="no">.</code><code class="o" translate="no">.</code><code class="o" translate="no">.</code><code translate="no"> </code><code class="c1" translate="no"># other imports</code><code translate="no">
</code><code class="kn" translate="no">from</code><code translate="no"> </code><code class="nn" translate="no">dependencies</code><code translate="no"> </code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="n" translate="no">get_rag_content</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">get_urls_content</code><code translate="no">
</code><code translate="no">
</code><code class="nd" translate="no">@app</code><code class="o" translate="no">.</code><code class="n" translate="no">post</code><code class="p" translate="no">(</code><code class="s2" translate="no">"</code><code class="s2" translate="no">/generate/text</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">response_model_exclude_defaults</code><code class="o" translate="no">=</code><code class="kc" translate="no">True</code><code class="p" translate="no">)</code><code translate="no">
</code><code class="k" translate="no">async</code><code translate="no"> </code><code class="k" translate="no">def</code><code translate="no"> </code><code class="nf" translate="no">serve_text_to_text_controller</code><code class="p" translate="no">(</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">request</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="n" translate="no">Request</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">body</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="n" translate="no">TextModelRequest</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">Body</code><code class="p" translate="no">(</code><code class="o" translate="no">.</code><code class="o" translate="no">.</code><code class="o" translate="no">.</code><code class="p" translate="no">)</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">urls_content</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="nb" translate="no">str</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">Depends</code><code class="p" translate="no">(</code><code class="n" translate="no">get_urls_content</code><code class="p" translate="no">)</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">rag_content</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="nb" translate="no">str</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">Depends</code><code class="p" translate="no">(</code><code class="n" translate="no">get_rag_content</code><code class="p" translate="no">)</code><code class="p" translate="no">,</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO13-4" id="co_achieving_concurrency_in_ai_workloads_CO13-4"><img alt="4" src="assets/4.png" width="12" height="12"/></a><code translate="no">
</code><code class="p" translate="no">)</code><code translate="no"> </code><code class="o" translate="no">-</code><code class="o" translate="no">&gt;</code><code translate="no"> </code><code class="n" translate="no">TextModelResponse</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">    </code><code class="o" translate="no">.</code><code class="o" translate="no">.</code><code class="o" translate="no">.</code><code translate="no"> </code><code class="c1" translate="no"># Raise HTTPException for invalid models</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">prompt</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">body</code><code class="o" translate="no">.</code><code class="n" translate="no">prompt</code><code translate="no"> </code><code class="o" translate="no">+</code><code translate="no"> </code><code class="s2" translate="no">"</code><code class="s2" translate="no"> </code><code class="s2" translate="no">"</code><code translate="no"> </code><code class="o" translate="no">+</code><code translate="no"> </code><code class="n" translate="no">urls_content</code><code translate="no"> </code><code class="o" translate="no">+</code><code translate="no"> </code><code class="n" translate="no">rag_content</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">output</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">generate_text</code><code class="p" translate="no">(</code><code class="n" translate="no">models</code><code class="p" translate="no">[</code><code class="s2" translate="no">"</code><code class="s2" translate="no">text</code><code class="s2" translate="no">"</code><code class="p" translate="no">]</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">prompt</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">body</code><code class="o" translate="no">.</code><code class="n" translate="no">temperature</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">return</code><code translate="no"> </code><code class="n" translate="no">TextModelResponse</code><code class="p" translate="no">(</code><code class="n" translate="no">content</code><code class="o" translate="no">=</code><code class="n" translate="no">output</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">ip</code><code class="o" translate="no">=</code><code class="n" translate="no">request</code><code class="o" translate="no">.</code><code class="n" translate="no">client</code><code class="o" translate="no">.</code><code class="n" translate="no">host</code><code class="p" translate="no">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO13-1" id="callout_achieving_concurrency_in_ai_workloads_CO13-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Crea la funzione di dipendenza <code translate="no">get_rag_content</code> da iniettare nel gestore del servizio LLM. Questa dipendenza ha accesso alla richiesta <code translate="no">body</code> e successivamente all'utente <code translate="no">prompt</code>.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO13-2" id="callout_achieving_concurrency_in_ai_workloads_CO13-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a></dt>
<dd><p>Utilizza <code translate="no">vector_service</code> per cercare nel database i contenuti rilevanti per l'utente <code translate="no">prompt</code>. Converti l'utente <code translate="no">prompt</code> in un embedding utilizzando la funzione <code translate="no">embed</code> quando passa alla funzione <code translate="no">vector_service.search</code>. Recupera i tre elementi più rilevanti solo se il loro punteggio di somiglianza coseno è superiore a <code translate="no">0.7</code> (o al 70%).</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO13-3" id="callout_achieving_concurrency_in_ai_workloads_CO13-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a></dt>
<dd><p>Unisce il carico di testo dei tre elementi più rilevanti recuperati come <code translate="no">rag_​con⁠tent_str</code> e lo restituisce.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO13-4" id="callout_achieving_concurrency_in_ai_workloads_CO13-4"><img alt="4" src="assets/4.png" width="12" height="12"/></a></dt>
<dd><p>Iniettare i risultati della funzione di dipendenza <code translate="no">get_rag_content</code> nel gestore LLM per aumentare il prompt finale del LLM con il contenuto del database vettoriale <code translate="no">knowledgebase</code>. Il gestore LLM può ora recuperare il contenuto delle pagine web e del database vettoriale RAG.</p></dd>
</dl></div>
<p>Se ora visiti il tuo browser e carichi un documento PDF, dovresti essere in grado di fare domande su di esso al tuo LLM. La<a data-type="xref" href="#rag_results">Figura 5-12</a> mostra il mio esperimento con il servizio caricando un campione di questo libro nella sua forma grezza e chiedendo al LLM di descrivere chi<span class="keep-together">sono.</span></p>
<div data-type="note" epub:type="note"><h6>Nota</h6>
<p>A seconda del modello e delle dimensioni degli input, potresti osservare un calo delle prestazioni o eccezioni come problemi di lunghezza dei token.</p>
</div>
<figure><div class="figure" id="rag_results">
<img alt="bgai 0512" src="assets/bgai_0512.png" width="974" height="870"/>
<h6><span class="label">Figura 5-12. </span>Sfruttare il RAG per fornire risposte alle query degli utenti</h6>
</div></figure>
<p>Congratulazioni, ora hai un sistema RAG perfettamente funzionante, grazie a modelli open source e a un database vettoriale.</p>
<p>Questo progetto più lungo è servito come esercitazione pratica per imparare i concetti relativi alla programmazione asincrona e alle operazioni di I/O con il filesystem e un database vettoriale costruendo un modulo RAG per il tuo sistema LLM. Nota che il sistema RAG che abbiamo appena costruito insieme ha ancora molte limitazioni:</p>
<ul>
<li>
<p>La suddivisione del testo può dividere le parole a metà con conseguente scarso recupero e<span class="keep-together">confusione in</span> LLM.</p>
</li>
<li>
<p>Il LLM può continuare a produrre allucinazioni e risultati incoerenti anche con i prompt aumentati.</p>
</li>
<li>
<p>Il sistema di ricerca e recupero può avere prestazioni scarse in alcuni casi.</p>
</li>
<li>
<p>I prompt aumentati possono superare la finestra contestuale di LLM.</p>
</li>
<li>
<p>Le informazioni recuperate dal database possono essere prive di fatti rilevanti a causa di una base di conoscenza obsoleta o incompleta, di query ambigue o di un algoritmo di recupero inadeguato.</p>
</li>
<li>
<p>Il contesto recuperato potrebbe non essere ordinato in base alla rilevanza della query dell'utente.</p>
</li>
</ul>
<p>Puoi lavorare per migliorare ulteriormente il modulo RAG implementando varie altre tecniche, che non tratterò in questo libro:</p>
<ul>
<li>
<p>Ottimizza le operazioni di divisione del testo, dimensionamento dei pezzi, pulizia e incorporazione.</p>
</li>
<li>
<p>Eseguire trasformazioni delle query utilizzando il LLM per aiutare il sistema di reperimento e di incremento tramite tecniche come la compressione del prompt, il concatenamento, la raffinazione, l'aggregazione e così via, per ridurre le allucinazioni e migliorare le prestazioni del LLM.</p>
</li>
<li>
<p>Riassumere o scomporre prompt aumentati di grandi dimensioni per alimentare il contesto nei modelli utilizzando un approccio a finestra scorrevole.</p>
</li>
<li>
<p>Migliorare gli algoritmi di recupero per gestire query ambigue e implementare meccanismi di fallback per i dati incompleti.</p>
</li>
<li>
<p>Migliorare le prestazioni di recupero con metodi come la <em>massima rilevanza marginale</em> (MMR) per arricchire il processo di incremento con documenti più diversi.</p>
</li>
<li>
<p>Implementare altre tecniche RAG avanzate come il reranking e il filtraggio del recupero, gli indici gerarchici del database, la fusione RAG, i pensieri aumentati dal recupero (RAT) e così via, per migliorare le prestazioni generali della generazione.<a data-startref="ix_ch05-asciidoc21" data-type="indexterm" id="id844"/><a data-startref="ix_ch05-asciidoc20" data-type="indexterm" id="id845"/></p>
</li>
</ul>
<p>Ti lascio approfondire queste tecniche e metterle in pratica da solo come esercizi aggiuntivi.<a data-startref="ix_ch05-asciidoc6" data-type="indexterm" id="id846"/><a data-startref="ix_ch05-asciidoc5" data-type="indexterm" id="id847"/></p>
<p>Nella prossima sezione, esamineremo altre tecniche per ottimizzare i tuoi servizi GenAI per evitare di bloccare il server con operazioni delimitate dal calcolo, come l'<span class="keep-together">inferenza</span> del modello.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="Optimizing Model Serving for Memory- and &#10;Compute-Bound AI &#10;Inference Tasks" data-type="sect1"><div class="sect1" id="id246">
<h1>Ottimizzare il servizio di modelli per<span class="keep-together">compiti di inferenza</span> AI<span class="keep-together">con limiti</span> di memoria e di<span class="keep-together">calcolo</span> </h1>
<p><a data-primary="concurrency" data-secondary="optimizing model serving for memory- and compute-bound AI inference tasks" data-type="indexterm" id="ix_ch05-asciidoc22"/><a data-primary="serving GenAI models" data-secondary="optimizing model serving for memory- and compute-bound AI inference tasks" data-type="indexterm" id="ix_ch05-asciidoc23"/>Finora abbiamo analizzato l'ottimizzazione delle operazioni del nostro servizio che sono delimitate dall'I/O. Hai imparato a sfruttare la programmazione asincrona per interagire con il web, i database e i file costruendo un web scraper e un modulo RAG.</p>
<p>Utilizzando strumenti e tecniche async, il tuo servizio è rimasto reattivo quando interagisce con il web, il filesystem e i database. Tuttavia, se il modello è in self-hosting, il passaggio a tecniche di programmazione async non eliminerà completamente i lunghi tempi di attesa, perché il collo di bottiglia saranno le operazioni di inferenza del modello.</p>
<section data-pdf-bookmark="Compute-Bound Operations" data-type="sect2"><div class="sect2" id="id247">
<h2>Operazioni delimitate dal calcolo</h2>
<p><a data-primary="concurrency" data-secondary="optimizing model serving for memory- and compute-bound AI inference tasks" data-tertiary="compute-bound operations" data-type="indexterm" id="id848"/><a data-primary="GPUs" data-secondary="compute-bound operations on" data-type="indexterm" id="id849"/>È possibile accelerare l'inferenza eseguendo i modelli su GPU per parallelizzare in modo massiccio i calcoli. Le moderne GPU hanno una potenza di calcolo sbalorditiva, misurata dal numero di operazioni<em>in virgola mobile</em> al secondo (FLOPS), e raggiungono i teraflop (NVIDIA A100) o i petaflop (NVIDIA H100) di calcolo. Tuttavia, nonostante la potenza e le capacità di parallelizzazione significative, i core delle moderne GPU sono spesso sottoutilizzati nei carichi di lavoro simultanei con modelli più grandi.</p>
<p>Quando i modelli sono auto-ospitati sulle GPU, i parametri del modello vengono caricati dal disco alla RAM (I/O bound) e poi spostati dalla RAM alla memoria ad alta larghezza di banda della GPU dalla CPU (memory bound). Una volta che i parametri del modello sono caricati sulla memoria della GPU, viene eseguita l'inferenza (compute bound).</p>
<p>Controintuitivamente, l'inferenza del modello per i modelli GenAI più grandi, come SDXL e LLMs, non è delimitata dall'I/O o dal calcolo, ma piuttosto dalla memoria. Ciò significa che ci vuole più tempo per caricare 1 MB di dati nei core di calcolo della GPU di quanto ne serva per elaborare 1 MB di dati. Inevitabilmente, per massimizzare la concorrenza del tuo servizio, dovrai <em>raggruppare</em> le richieste di inferenza e inserire il batch più grande possibile nella memoria a banda larga della GPU.</p>
<p>Pertanto, anche quando si utilizzano tecniche asincrone e GPU di ultima generazione, il server può essere bloccato in attesa che miliardi di parametri del modello vengano caricati nella memoria ad alta larghezza di banda della GPU durante ogni richiesta. Per evitare di bloccare il server, puoi disaccoppiare le operazioni di model-serving legate alla memoria dal server FastAPI esternalizzando il model-serving, come abbiamo accennato nel <a data-type="xref" href="ch03.html#ch03">Capitolo 3</a>.</p>
<p>Vediamo come delegare il servizio del modello a un altro processo.</p>
</div></section>
<section data-pdf-bookmark="Externalizing Model Serving" data-type="sect2"><div class="sect2" id="id84">
<h2>Modello esternalizzante al servizio</h2>
<p><a data-primary="concurrency" data-secondary="optimizing model serving for memory- and compute-bound AI inference tasks" data-tertiary="externalizing model serving" data-type="indexterm" id="ix_ch05-asciidoc24"/><a data-primary="external model serving" data-type="indexterm" id="ix_ch05-asciidoc25"/><a data-primary="serving GenAI models" data-secondary="external serving" data-type="indexterm" id="ix_ch05-asciidoc26"/>Quando esternalizzi i tuoi carichi di lavoro che prevedono l'uso di modelli, hai a disposizione diverse opzioni: puoi ospitare i modelli su un altro server FastAPI o utilizzare server specializzati nell'inferenza dei modelli.</p>
<p>I server di inferenza specializzati supportano solo una serie limitata di architetture di modelli GenAI. Tuttavia, se l'architettura del tuo modello è supportata, risparmierai molto tempo senza dover implementare da solo le ottimizzazioni per l'inferenza. Ad esempio, se hai bisogno di auto-ospitare LLMs, i framework LLM-serving possono eseguire per te diverse ottimizzazioni per l'inferenza, come l'elaborazione batch, il parallelismo tensoriale, la quantizzazione, la cache, lo streaming delle uscite, la gestione della memoria GPU, ecc.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id850">
<h1>Tecniche di ottimizzazione del modello</h1>
<p><a data-primary="model optimization" data-type="indexterm" id="id851"/>Ci sono diverse tecniche di compressione del modello che puoi provare per migliorare le prestazioni dell'inferenza:</p>
<ul>
<li>
<p><em>Quantizzazione</em> per comprimere i modelli</p>
</li>
<li>
<p><em>Pruning</em> per riassumere i parametri del modello</p>
</li>
<li>
<p><em>Distillazione</em> per creare modelli "studenti" significativamente più piccoli delle loro controparti "insegnanti".</p>
</li>
<li>
<p><em>Ottimizzazione di</em> piccoli modelli per specializzarli per il tuo caso d'uso</p>
</li>
</ul>
<p>Se utilizzi modelli basati su trasformatori, puoi ottimizzare ulteriormente l'inferenza del modello utilizzando le seguenti tecniche:</p>
<ul>
<li>
<p><em>Attenzione veloce</em> per ottimizzare i calcoli delle mappe di attenzione sulle GPU</p>
</li>
<li>
<p><em>KV caching</em> per sfruttare le tecniche di caching in-memory per velocizzare l'inferenza riutilizzando i risultati delle mappe di attenzione calcolate</p>
</li>
<li>
<p><em>Attenzione paginata</em> per ottimizzare l'uso della memoria cache KV dopo il calcolo dell'attenzione</p>
</li>
<li>
<p><em>Richiedi il batching</em>, compreso il batching semplice e continuo, per massimizzare i tassi di utilizzo della GPU</p>
</li>
</ul>
<p><a data-type="xref" href="ch10.html#ch10">Il Capitolo 10</a> approfondisce le tecniche di ottimizzazione dei modelli.</p>
</div></aside>
<p><a data-primary="vLLM" data-type="indexterm" id="ix_ch05-asciidoc27"/>Dato che in questo capitolo abbiamo lavorato principalmente con LLMs, ti mostrerò come integrare vLLM, un server LLM open source che può avviare per te un server FastAPI conforme alle specifiche API OpenAI. vLLM si integra perfettamente anche con le più diffuse architetture open source del modello Hugging Face, come GPT, Llama, Gemma, Mistral, Falcon, ecc.</p>
<div data-type="note" epub:type="note"><h6>Nota</h6>
<p>Al momento in cui scriviamo, altri server di hosting LLM che puoi utilizzare sono NVIDIA Triton Inference Server, Ray Serve, Hugging Face Inference e OpenLLM, tra gli altri.</p>
<p>Ci sono caratteristiche, vantaggi e svantaggi nell'utilizzo di ciascuno di essi, comprese le architetture dei modelli supportati. Ti consiglio di fare una ricerca su questi server prima di adottarli nei tuoi casi d'uso.</p>
</div>
<p>Puoi avviare il tuo server vLLM FastAPI con un solo comando, come mostrato nell'<a data-type="xref" href="#vllm">Esempio 5-15</a>. Per eseguire il codice dell'<a data-type="xref" href="#vllm">Esempio 5-15</a>, devi installare <code translate="no">vllm</code> utilizzando:</p>
<pre data-code-language="bash" data-type="programlisting" translate="no">$<code class="w" translate="no"> </code>pip<code class="w" translate="no"> </code>install<code class="w" translate="no"> </code>vllm<code class="w" translate="no"/></pre>
<div data-type="warning" epub:type="warning"><h6>Avvertenze</h6>
<p>Al momento in cui scriviamo, vLLM supporta solo le piattaforme Linux (inclusa WSL) con GPU compatibili con NVIDIA per eseguire le dipendenze del toolkit CUDA. Purtroppo, non è possibile installare vLLM su macchine Mac o Windows per effettuare test locali.</p>
<p>vLLM è stato progettato per i carichi di lavoro di inferenza di produzione su GPU NVIDIA in ambienti Linux, dove il server può delegare le richieste a più core di GPU tramite il <em>parallelismo tensoriale</em>. Supporta anche il calcolo distribuito quando si tratta di scalare i servizi al di là di una singola macchina grazie alla dipendenza da Ray Serve.</p>
<p>Consulta la documentazione di vLLM per maggiori dettagli sull'inferenza distribuita e sul servizio.</p>
</div>
<div data-type="example" id="vllm">
<h5><span class="label">Esempio 5-15. </span>Avvio del server vLLM FastAPI OpenAI API per TinyLama su una macchina Linux con 4 GPU NVIDIA T4 da 16 GB</h5>
<pre data-code-language="bash" data-type="programlisting" translate="no"><code translate="no">$</code><code class="w" translate="no"> </code><code translate="no">python</code><code class="w" translate="no"> </code><code translate="no">-m</code><code class="w" translate="no"> </code><code translate="no">vllm.entrypoints.openai.api_server</code><code class="w" translate="no"> </code><code class="se" translate="no">\ </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO14-1" id="co_achieving_concurrency_in_ai_workloads_CO14-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a><code class="w" translate="no">
</code><code translate="no">--model</code><code class="w" translate="no"> </code><code class="s2" translate="no">"TinyLlama/TinyLlama-1.1B-Chat-v1.0"</code><code class="w" translate="no"> </code><code class="se" translate="no">\
</code><code translate="no">--dtype</code><code class="w" translate="no"> </code><code translate="no">float16</code><code class="w" translate="no"> </code><code class="se" translate="no">\ </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO14-2" id="co_achieving_concurrency_in_ai_workloads_CO14-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a><code class="w" translate="no">
</code><code translate="no">--tensor-parallel-size</code><code class="w" translate="no"> </code><code class="m" translate="no">4</code><code class="w" translate="no"> </code><code class="se" translate="no">\ </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO14-3" id="co_achieving_concurrency_in_ai_workloads_CO14-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a><code class="w" translate="no">
</code><code translate="no">--api-key</code><code class="w" translate="no"> </code><code class="s2" translate="no">"your_secret_token"</code><code class="w" translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO14-4" id="co_achieving_concurrency_in_ai_workloads_CO14-4"><img alt="4" src="assets/4.png" width="12" height="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO14-1" id="callout_achieving_concurrency_in_ai_workloads_CO14-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Avvia un server API compatibile con OpenAI con FastAPI per servire il modello TinyLlama.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO14-2" id="callout_achieving_concurrency_in_ai_workloads_CO14-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a></dt>
<dd><p>Usa il tipo di dati di media precisione <code translate="no">float16</code>.<code translate="no">float16</code> è compatibile con l'hardware della GPU, mentre <code translate="no">bfloat16</code> è generalmente compatibile con l'hardware della CPU.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO14-3" id="callout_achieving_concurrency_in_ai_workloads_CO14-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a></dt>
<dd><p>Sfrutta la funzione di parallelismo tensoriale di vLLM per eseguire il server API su quattro GPU.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO14-4" id="callout_achieving_concurrency_in_ai_workloads_CO14-4"><img alt="4" src="assets/4.png" width="12" height="12"/></a></dt>
<dd><p>Imposta un token segreto per l'autenticazione di base per proteggere il server LLM. Questo è utile per la comunicazione sicura da macchina a macchina, ad esempio per comunicare direttamente con il tuo servizio FastAPI attuale.</p></dd>
</dl></div>
<p>Con il server vLLM FastAPI attivo e funzionante, ora puoi sostituire la logica di servizio del modello nel tuo servizio attuale con chiamate di rete al server vLLM. Fai riferimento all'<a data-type="xref" href="#vllm_fastapi_text_generation">Esempio 5-16</a> per i dettagli dell'implementazione.</p>
<div data-type="example" id="vllm_fastapi_text_generation">
<h5><span class="label">Esempio 5-16. </span>Sostituzione del model serving con chiamate API asincrone al nuovo server vLLM</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="c1" translate="no"># models.py</code><code translate="no">
</code><code translate="no">
</code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="nn" translate="no">os</code><code translate="no">
</code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="nn" translate="no">aiohttp</code><code translate="no">
</code><code class="kn" translate="no">from</code><code translate="no"> </code><code class="nn" translate="no">loguru</code><code translate="no"> </code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="n" translate="no">logger</code><code translate="no">
</code><code translate="no">
</code><code class="k" translate="no">async</code><code translate="no"> </code><code class="k" translate="no">def</code><code translate="no"> </code><code class="nf" translate="no">generate_text</code><code class="p" translate="no">(</code><code class="n" translate="no">prompt</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="nb" translate="no">str</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">temperature</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="nb" translate="no">float</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="mf" translate="no">0.7</code><code class="p" translate="no">)</code><code translate="no"> </code><code class="o" translate="no">-</code><code class="o" translate="no">&gt;</code><code translate="no"> </code><code class="nb" translate="no">str</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">system_prompt</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="s2" translate="no">"</code><code class="s2" translate="no">You are an AI assistant</code><code class="s2" translate="no">"</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">messages</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="p" translate="no">[</code><code translate="no">
</code><code translate="no">        </code><code class="p" translate="no">{</code><code class="s2" translate="no">"</code><code class="s2" translate="no">role</code><code class="s2" translate="no">"</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="s2" translate="no">"</code><code class="s2" translate="no">system</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="s2" translate="no">"</code><code class="s2" translate="no">content</code><code class="s2" translate="no">"</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="n" translate="no">system_prompt</code><code class="p" translate="no">}</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">        </code><code class="p" translate="no">{</code><code class="s2" translate="no">"</code><code class="s2" translate="no">role</code><code class="s2" translate="no">"</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="s2" translate="no">"</code><code class="s2" translate="no">user</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="s2" translate="no">"</code><code class="s2" translate="no">content</code><code class="s2" translate="no">"</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="n" translate="no">prompt</code><code class="p" translate="no">}</code><code class="p" translate="no">,</code><code translate="no">
</code><code translate="no">    </code><code class="p" translate="no">]</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">data</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="p" translate="no">{</code><code class="s2" translate="no">"</code><code class="s2" translate="no">temperature</code><code class="s2" translate="no">"</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="n" translate="no">temperature</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="s2" translate="no">"</code><code class="s2" translate="no">messages</code><code class="s2" translate="no">"</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="n" translate="no">messages</code><code class="p" translate="no">}</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">headers</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="p" translate="no">{</code><code class="s2" translate="no">"</code><code class="s2" translate="no">Authorization</code><code class="s2" translate="no">"</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="sa" translate="no">f</code><code class="s2" translate="no">"</code><code class="s2" translate="no">Bearer </code><code class="si" translate="no">{</code><code class="n" translate="no">os</code><code class="o" translate="no">.</code><code class="n" translate="no">environ</code><code class="o" translate="no">.</code><code class="n" translate="no">get</code><code class="p" translate="no">(</code><code class="s1" translate="no">'</code><code class="s1" translate="no">VLLM_API_KEY</code><code class="s1" translate="no">'</code><code class="p" translate="no">)</code><code class="si" translate="no">}</code><code class="s2" translate="no">"</code><code class="p" translate="no">}</code><code translate="no">
</code><code class="k" translate="no">try</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">   </code><code class="k" translate="no">async</code><code translate="no"> </code><code class="k" translate="no">with</code><code translate="no"> </code><code class="n" translate="no">aiohttp</code><code class="o" translate="no">.</code><code class="n" translate="no">ClientSession</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code translate="no"> </code><code class="k" translate="no">as</code><code translate="no"> </code><code class="n" translate="no">session</code><code class="p" translate="no">:</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO15-1" id="co_achieving_concurrency_in_ai_workloads_CO15-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">        </code><code class="n" translate="no">response</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="k" translate="no">await</code><code translate="no"> </code><code class="n" translate="no">session</code><code class="o" translate="no">.</code><code class="n" translate="no">post</code><code class="p" translate="no">(</code><code translate="no">
</code><code translate="no">            </code><code class="s2" translate="no">"</code><code class="s2" translate="no">http://localhost:8000/v1/chat</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">json</code><code class="o" translate="no">=</code><code class="n" translate="no">data</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">headers</code><code class="o" translate="no">=</code><code class="n" translate="no">headers</code><code translate="no">
</code><code translate="no">        </code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">        </code><code class="n" translate="no">predictions</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="k" translate="no">await</code><code translate="no"> </code><code class="n" translate="no">response</code><code class="o" translate="no">.</code><code class="n" translate="no">json</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code translate="no">
</code><code class="k" translate="no">except</code><code translate="no"> </code><code class="ne" translate="no">Exception</code><code translate="no"> </code><code class="k" translate="no">as</code><code translate="no"> </code><code class="n" translate="no">e</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">logger</code><code class="o" translate="no">.</code><code class="n" translate="no">error</code><code class="p" translate="no">(</code><code class="sa" translate="no">f</code><code class="s2" translate="no">"</code><code class="s2" translate="no">Failed to obtain predictions from vLLM - Error: </code><code class="si" translate="no">{</code><code class="n" translate="no">e</code><code class="si" translate="no">}</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">return</code><code translate="no"> </code><code class="p" translate="no">(</code><code translate="no">
</code><code translate="no">        </code><code class="s2" translate="no">"</code><code class="s2" translate="no">Failed to obtain predictions from vLLM - </code><code class="s2" translate="no">"</code><code translate="no">
</code><code translate="no">        </code><code class="s2" translate="no">"</code><code class="s2" translate="no">See server logs for more details</code><code class="s2" translate="no">"</code><code translate="no">
</code><code translate="no">    </code><code class="p" translate="no">)</code><code translate="no">
</code><code class="k" translate="no">try</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">output</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">predictions</code><code class="p" translate="no">[</code><code class="s2" translate="no">"</code><code class="s2" translate="no">choices</code><code class="s2" translate="no">"</code><code class="p" translate="no">]</code><code class="p" translate="no">[</code><code class="mi" translate="no">0</code><code class="p" translate="no">]</code><code class="p" translate="no">[</code><code class="s2" translate="no">"</code><code class="s2" translate="no">message</code><code class="s2" translate="no">"</code><code class="p" translate="no">]</code><code class="p" translate="no">[</code><code class="s2" translate="no">"</code><code class="s2" translate="no">content</code><code class="s2" translate="no">"</code><code class="p" translate="no">]</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO15-2" id="co_achieving_concurrency_in_ai_workloads_CO15-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">logger</code><code class="o" translate="no">.</code><code class="n" translate="no">debug</code><code class="p" translate="no">(</code><code class="sa" translate="no">f</code><code class="s2" translate="no">"</code><code class="s2" translate="no">Generated text: </code><code class="si" translate="no">{</code><code class="n" translate="no">output</code><code class="si" translate="no">}</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">return</code><code translate="no"> </code><code class="n" translate="no">output</code><code translate="no">
</code><code class="k" translate="no">except</code><code translate="no"> </code><code class="ne" translate="no">KeyError</code><code translate="no"> </code><code class="k" translate="no">as</code><code translate="no"> </code><code class="n" translate="no">e</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">logger</code><code class="o" translate="no">.</code><code class="n" translate="no">error</code><code class="p" translate="no">(</code><code class="sa" translate="no">f</code><code class="s2" translate="no">"</code><code class="s2" translate="no">Failed to parse predictions from vLLM - Error: </code><code class="si" translate="no">{</code><code class="n" translate="no">e</code><code class="si" translate="no">}</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">return</code><code translate="no"> </code><code class="p" translate="no">(</code><code translate="no">
</code><code translate="no">        </code><code class="s2" translate="no">"</code><code class="s2" translate="no">Failed to parse predictions from vLLM - </code><code class="s2" translate="no">"</code><code translate="no">
</code><code translate="no">        </code><code class="s2" translate="no">"</code><code class="s2" translate="no">See server logs for more details</code><code class="s2" translate="no">"</code><code translate="no">
</code><code translate="no">    </code><code class="p" translate="no">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO15-1" id="callout_achieving_concurrency_in_ai_workloads_CO15-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Usa <code translate="no">aiohttp</code> per creare una sessione asincrona per l'invio di richieste <code translate="no">POST</code> al server FastAPI di vLLM. Questa logica sostituisce la logica di inferenza della pipeline del modello Hugging Face sull'attuale server FastAPI.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO15-2" id="callout_achieving_concurrency_in_ai_workloads_CO15-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a></dt>
<dd><p>Poiché il server vLLM è compatibile con OpenAI, puoi accedere ai contenuti in uscita seguendo le specifiche API OpenAI.</p></dd>
</dl></div>
<p>Successivamente, rimuovi il codice relativo alla vita FastAPI in modo che il tuo servizio attuale non carichi il modello TinyLlama. Puoi ottenere questo risultato seguendo il codice riportato nell'<a data-type="xref" href="#vllm_fastapi_handler">Esempio 5-17</a>.</p>
<div data-type="example" id="vllm_fastapi_handler">
<h5><span class="label">Esempio 5-17. </span>Rimuovi la durata di vita di FastAPI e aggiorna il gestore della generazione del testo in modo che sia asincrono</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="c1" translate="no"># main.py</code><code translate="no">
</code><code translate="no">
</code><code class="kn" translate="no">from</code><code translate="no"> </code><code class="nn" translate="no">fastapi</code><code translate="no"> </code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="n" translate="no">FastAPI</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">Request</code><code translate="no">
</code><code class="kn" translate="no">from</code><code translate="no"> </code><code class="nn" translate="no">schemas</code><code translate="no"> </code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="n" translate="no">TextModelRequest</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">TextModelResponse</code><code translate="no">
</code><code class="kn" translate="no">from</code><code translate="no"> </code><code class="nn" translate="no">models</code><code translate="no"> </code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="n" translate="no">generate_text</code><code translate="no">
</code><code translate="no">
</code><code class="c1" translate="no"># Remove the asynccontextmanager to remove TinyLlama from FastAPI </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO16-1" id="co_achieving_concurrency_in_ai_workloads_CO16-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a><code translate="no">
</code><code class="c1" translate="no"># @asynccontextmanager</code><code translate="no">
</code><code class="c1" translate="no"># async def lifespan(app: FastAPI):</code><code translate="no">
</code><code class="c1" translate="no">#     models["text"] = load_text_model()</code><code translate="no">
</code><code class="c1" translate="no">#     yield</code><code translate="no">
</code><code class="c1" translate="no">#     models.clear()</code><code translate="no">
</code><code translate="no">
</code><code class="c1" translate="no"># Remove the `lifespan` argument from `FastAPI()`</code><code translate="no">
</code><code class="n" translate="no">app</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">FastAPI</code><code class="p" translate="no">(</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">
</code><code class="nd" translate="no">@app</code><code class="o" translate="no">.</code><code class="n" translate="no">post</code><code class="p" translate="no">(</code><code class="s2" translate="no">"</code><code class="s2" translate="no">/generate/text</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code translate="no">
</code><code class="k" translate="no">async</code><code translate="no"> </code><code class="k" translate="no">def</code><code translate="no"> </code><code class="nf" translate="no">serve_text_to_text_controller</code><code class="p" translate="no">(</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">request</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="n" translate="no">Request</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">body</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="n" translate="no">TextModelRequest</code><code translate="no">
</code><code class="p" translate="no">)</code><code translate="no"> </code><code class="o" translate="no">-</code><code class="o" translate="no">&gt;</code><code translate="no"> </code><code class="n" translate="no">TextModelResponse</code><code class="p" translate="no">:</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO16-2" id="co_achieving_concurrency_in_ai_workloads_CO16-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">    </code><code class="o" translate="no">.</code><code class="o" translate="no">.</code><code class="o" translate="no">.</code><code translate="no">  </code><code class="c1" translate="no"># controller logic</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">output</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="k" translate="no">await</code><code translate="no"> </code><code class="n" translate="no">generate_text</code><code class="p" translate="no">(</code><code class="n" translate="no">body</code><code class="o" translate="no">.</code><code class="n" translate="no">prompt</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">body</code><code class="o" translate="no">.</code><code class="n" translate="no">temperature</code><code class="p" translate="no">)</code><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">return</code><code translate="no"> </code><code class="n" translate="no">TextModelResponse</code><code class="p" translate="no">(</code><code class="n" translate="no">content</code><code class="o" translate="no">=</code><code class="n" translate="no">output</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">ip</code><code class="o" translate="no">=</code><code class="n" translate="no">request</code><code class="o" translate="no">.</code><code class="n" translate="no">client</code><code class="o" translate="no">.</code><code class="n" translate="no">host</code><code class="p" translate="no">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO16-1" id="callout_achieving_concurrency_in_ai_workloads_CO16-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Non è più necessario utilizzare FastAPI <code translate="no">lifespan</code> poiché il modello è ora servito da un server FastAPI vLLM esterno.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO16-2" id="callout_achieving_concurrency_in_ai_workloads_CO16-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a></dt>
<dd><p>Rendi <code translate="no">serve_text_to_text_controller</code> un gestore di rotte asincrone, in quanto ora esegue operazioni di I/O verso il server vLLM e non esegue più operazioni di inferenza del modello sincrone e delimitate da calcoli, in quanto la loro gestione è delegata al server vLLM.</p></dd>
</dl></div>
<p class="fix_tracking">
Congratulazioni, ora hai ottenuto la concurrency con i tuoi carichi di lavoro di inferenza AI. Hai implementato una forma di multiprocessing su una singola macchina spostando i tuoi carichi di lavoro di inferenza LLM su un altro server. Entrambi i server sono ora in esecuzione su core separati con il tuo server LLM che delega il lavoro a più core GPU, sfruttando il parallelismo. Questo significa che il tuo server principale è ora in grado di elaborare più richieste in entrata e di svolgere altre attività oltre all'elaborazione di un'operazione di inferenza LLM alla volta.</p>
<div data-type="tip"><h6>Suggerimento</h6>
<p>Tieni presente che tutte le concomitanze che hai ottenuto finora sono state limitate a una sola macchina.</p>
<p>Per supportare un numero maggiore di utenti contemporanei, potresti aver bisogno di più macchine con core CPU e GPU. A quel punto, framework di calcolo distribuito come Ray Serve e Kubernetes possono aiutarti a scalare e orchestrare i tuoi servizi oltre una singola macchina worker utilizzando il parallelismo.</p>
</div>
<p>Prima di integrare vLLM, si verificavano lunghi tempi di attesa tra una richiesta e l'altra perché il server principale era troppo impegnato nell'esecuzione delle operazioni di inferenza. Con vLLM, ora si ottiene una massiccia riduzione della latenza e un aumento del throughput del servizio LLM.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id852">
<h1>Latenza e throughput</h1>
<p><a data-primary="large language models (LLMs)" data-secondary="latency and throughput" data-type="indexterm" id="id853"/><a data-primary="latency" data-type="indexterm" id="id854"/>La<em>latenza</em>, nel contesto delle LLM, si riferisce al tempo che intercorre tra il momento in cui viene fatta una richiesta al modello e il momento in cui viene ricevuta la prima risposta.<a data-primary="throughput" data-type="indexterm" id="id855"/><em>Il throughput</em>, invece, è il numero di richieste che una LLM è in grado di elaborare in un determinato lasso di tempo e indica la capacità del server di gestire richieste concomitanti o sequenziali nel tempo. La latenza può essere misurata in <em>secondi di ritardo</em> e il throughput in <em>gettoni al minuto</em> (TPM).</p>
<p>Come sviluppatore, vorrai che il tuo servizio abbia la latenza più bassa e il throughput più alto possibile. Tuttavia, esiste un compromesso tra le dimensioni del modello e la qualità rispetto a queste due metriche. Normalmente, gli LLMs con un numero maggiore di parametri ottengono una qualità più elevata, ma anche una latenza maggiore e un throughput ridotto.</p>
<p>Attualmente sono in corso ricerche per utilizzare tecniche di compressione dei modelli come la<em>distillazione</em>, la <em>quantizzazione</em> e il <em>pruning</em> per ridurre le dimensioni dei modelli linguistici mantenendo alta la qualità, il throughput e la latenza nei servizi di inferenza AI.</p>
</div></aside>
<p>Oltre ai meccanismi di compressione del modello come la quantizzazione, vLLM utilizza altre tecniche di ottimizzazione come il batching continuo delle richieste, il partizionamento della cache (paged attention), la riduzione dell'ingombro della memoria della GPU tramite la condivisione della memoria e lo streaming delle uscite per ottenere una latenza ridotta e un throughput elevato.<a data-startref="ix_ch05-asciidoc27" data-type="indexterm" id="id856"/></p>
<p>Analizziamo più in dettaglio i meccanismi di batching delle richieste e di attenzione paginata per capire come ottimizzare ulteriormente l'inferenza LLM.</p>
<section class="less_space pagebreak-before" data-pdf-bookmark="Request batching and continuous batching" data-type="sect3"><div class="sect3" id="id85">
<h3>Richiesta di dosaggio e dosaggio continuo</h3>
<p><a data-primary="serving GenAI models" data-secondary="external serving" data-tertiary="request batching and continuous batching" data-type="indexterm" id="ix_ch05-asciidoc28"/>Come abbiamo discusso nel <a data-type="xref" href="ch03.html#ch03">Capitolo 3</a>, le LLMs producono la previsione del token successivo in modo autoregressivo, come si può vedere nella <a data-type="xref" href="#autoregressive_prediction5">Figura 5-13</a>.</p>
<figure><div class="figure" id="autoregressive_prediction5">
<img alt="bgai 0513" src="assets/bgai_0513.png" width="721" height="221"/>
<h6><span class="label">Figura 5-13. </span>Previsione autoregressiva</h6>
</div></figure>
<p>Ciò significa che le LLM devono eseguire diverse iterazioni di inferenza in un ciclo per produrre una risposta, e ogni iterazione produce un singolo token di uscita. La sequenza di input cresce man mano che il token di uscita di ogni iterazione viene aggiunto alla fine, e la nuova sequenza viene inoltrata al modello nella fase di iterazione successiva. Una volta che il modello genera un token di fine sequenza, il ciclo di generazione si interrompe. In sostanza, le LLM producono una sequenza di token di completamento, fermandosi solo dopo aver prodotto un token di stop o aver raggiunto la lunghezza massima della sequenza.</p>
<p>Il LLM deve calcolare diverse mappe di attenzione per ogni token della sequenza in modo da poter fare iterativamente le previsioni del token successivo.</p>
<p>Fortunatamente, le GPU possono parallelizzare i calcoli delle mappe di attenzione per ogni iterazione. Come hai imparato, queste mappe di attenzione catturano il significato e il contesto di ogni token all'interno della sequenza di input e sono costose da calcolare.<a data-primary="key-value (KV) caching" data-type="indexterm" id="id857"/>Per questo motivo, per ottimizzare l'inferenza, gli LLMs utilizzano il <em>caching</em> <em>chiave-valore</em> (KV) per memorizzare le mappe calcolate nella memoria della GPU.</p>
<div data-type="tip"><h6>Suggerimento</h6>
<p>La formula della mappa di attenzione calcola un <em>valore (V)</em> in base a una <em>query (Q)</em>e a una <em>chiave (K)</em>.</p>
<blockquote>
<p>Q = KV</p></blockquote>
<p>Questo calcolo deve essere eseguito per ogni token della sequenza, ma fortunatamente può essere vettorializzato utilizzando operazioni di moltiplicazione matriciale di grandi dimensioni su una GPU.</p>
</div>
<p>Tuttavia, immagazzinare i parametri nella memoria della GPU per riutilizzarli tra un'iterazione e l'altra può consumare enormi quantità di memoria della GPU. Ad esempio, un modello con 13B parametri consuma quasi 1 MB di stato per ogni token della sequenza, oltre a tutti i 13B parametri del modello. Questo significa che c'è un numero limitato di token che puoi immagazzinare in memoria per riutilizzarli.</p>
<p>Se utilizzi una GPU di fascia alta, come la A100 con 40 GB di RAM, puoi tenere in memoria solo 14 K token contemporaneamente, mentre il resto della memoria viene utilizzato per memorizzare 26 GB di parametri del modello. In breve, la memoria consumata dalla GPU scala con le dimensioni del modello di base più la lunghezza della sequenza di token.</p>
<p>A peggiorare le cose, se devi servire più utenti in contemporanea con richieste in batch, la memoria della tua GPU deve essere condivisa tra più inferenze LLM. Di conseguenza, hai meno memoria per memorizzare sequenze più lunghe e il tuo LLM è vincolato a una finestra di contesto più breve. D'altra parte, se vuoi mantenere una finestra di contesto ampia, non puoi gestire più utenti in contemporanea.
Ad esempio, una lunghezza di sequenza di 2048 significa che la dimensione del batch sarà limitata a 7 richieste simultanee (o 7 sequenze prompt). Realisticamente, questo è un limite delimitato e non lascia spazio alla memorizzazione di calcoli intermedi, che ridurranno ulteriormente i numeri sopra citati.</p>
<p>Ciò significa che gli LLMs non riescono a saturare completamente le risorse disponibili della GPU. Il motivo principale è che una parte significativa della banda di memoria della GPU viene consumata per caricare i parametri del modello invece di elaborare gli input.</p>
<p>Il primo passo per ridurre il carico dei tuoi servizi è quello di integrare i modelli più efficienti. Spesso i modelli più piccoli e compressi possono svolgere il lavoro che gli chiedi, con prestazioni simili alle loro controparti più grandi.</p>
<p><a data-primary="request batching" data-type="indexterm" id="id858"/>Un'altra soluzione adeguata al problema del sottoutilizzo della GPU è l'implementazione del<em>batching delle richieste</em>, in cui il modello elabora più input in gruppi, riducendo l'overhead del caricamento dei parametri del modello per ogni richiesta. In questo modo si utilizza in modo più efficiente la larghezza di banda della memoria del chip, ottenendo un maggiore utilizzo dei calcoli, un throughput più elevato e un'inferenza LLM meno costosa. I server di inferenza LLM come vLLM sfruttano il batching più l'attenzione veloce, la cache KV e i meccanismi di attenzione paginata per massimizzare il throughput.</p>
<p>Puoi vedere la differenza di latenza di risposta e di throughput con e senza il batching nella <a data-type="xref" href="#with_without_batching">Figura 5-14</a>.</p>
<figure><div class="figure" id="with_without_batching">
<img alt="bgai 0514" src="assets/bgai_0514.png" width="1408" height="554"/>
<h6><span class="label">Figura 5-14. </span>Latenza di risposta e throughput del server LLM con e senza batching</h6>
</div></figure>
<p>Esistono due modi per implementare il batching:</p>
<dl>
<dt>Dosaggio statico</dt>
<dd>
<p>La dimensione del lotto rimane costante.</p>
</dd>
<dt>Dosaggio dinamico o continuo</dt>
<dd>
<p>La dimensione del lotto viene determinata in base alla domanda.</p>
</dd>
</dl>
<p><a data-primary="static batching" data-type="indexterm" id="id859"/>Nel <em>batching statico</em>, aspettiamo che arrivi un numero predeterminato di richieste in entrata prima di raggrupparle ed elaborarle attraverso il modello. Tuttavia, poiché le richieste possono terminare in qualsiasi momento all'interno di un batch, stiamo effettivamente ritardando le risposte a ogni richiesta - aumentando la latenza - fino a quando l'intero batch non viene elaborato.</p>
<p>Rilasciare la risorsa GPU può essere complicato anche quando si elabora un batch e si aggiungono nuove richieste al batch che possono trovarsi in stati di completamento diversi. Di conseguenza, la GPU rimane sottoutilizzata perché le sequenze generate all'interno di un batch variano e non corrispondono alla lunghezza della sequenza più lunga di quel batch.</p>
<p>La<a data-type="xref" href="#static_batching">Figura 5-15</a> illustra il batching statico nel contesto dell'inferenza LLM.</p>
<figure><div class="figure" id="static_batching">
<img alt="bgai 0515" src="assets/bgai_0515.png" width="1179" height="605"/>
<h6><span class="label">Figura 5-15. </span>Batching statico con dimensione fissa del lotto</h6>
</div></figure>
<p>Nella <a data-type="xref" href="#static_batching">Figura 5-15</a> si notano i blocchi bianchi che rappresentano il tempo di calcolo della GPU sottoutilizzato. Solo una sequenza di input nel batch ha saturato la GPU per tutta la durata dell'elaborazione del batch.</p>
<p>Oltre ad aggiungere inutili tempi di attesa e a non saturare l'utilizzo della GPU, ciò che rende problematico il batching statico è che gli utenti di un servizio di chatbot alimentato da LLM non forniranno prompt di lunghezza fissa o non si aspetteranno output di lunghezza fissa. La variazione degli output generati potrebbe causare un massiccio sottoutilizzo delle GPU.</p>
<p>Una soluzione consiste nell'evitare di assumere sequenze di input o output fisse e nell'impostare invece dimensioni dinamiche del batch durante l'elaborazione di un batch.<a data-primary="continuous (dynamic) batching" data-type="indexterm" id="id860"/><a data-primary="dynamic (continuous) batching" data-type="indexterm" id="id861"/>Nel <em>batch</em> <em>dinamico</em> o <em>continuo</em>, la dimensione del batch può essere impostata in base alla lunghezza della sequenza di richieste in arrivo e alla risorsa GPU disponibile. Con questo approccio, le richieste di nuova generazione possono essere inserite in un batch sostituendo le richieste completate per ottenere un utilizzo della GPU più elevato rispetto al batch statico.</p>
<p>La<a data-type="xref" href="#dynamic_batching">Figura 5-16</a> mostra come il batching dinamico o continuo possa saturare completamente la risorsa GPU.</p>
<figure><div class="figure" id="dynamic_batching">
<img alt="bgai 0516" src="assets/bgai_0516.png" width="1179" height="605"/>
<h6><span class="label">Figura 5-16. </span>Dosaggio dinamico/continuo con dimensione variabile del lotto</h6>
</div></figure>
<p>Mentre i parametri del modello vengono caricati, le richieste possono continuare ad arrivare e il server di inferenza LLM le pianifica e le inserisce nel batch per massimizzare l'utilizzo della GPU. Questo approccio porta a un throughput più elevato e a una latenza ridotta.</p>
<p>Se stai costruendo un server di inferenza LLM, probabilmente vorrai inserire il meccanismo di batching continuo nel tuo server. Tuttavia, la buona notizia è che il server vLLM fornisce già il batching continuo con il suo server FastAPI, quindi non dovrai implementare tutto questo da solo. Inoltre, viene fornito con un'altra importante funzionalità di ottimizzazione della GPU, che lo distingue da altri framework di inferenza LLM alternativi: l'attenzione paginata.<a data-startref="ix_ch05-asciidoc28" data-type="indexterm" id="id862"/></p>
</div></section>
<section data-pdf-bookmark="Paged attention" data-type="sect3"><div class="sect3" id="id86">
<h3>Attenzione al pager</h3>
<p><a data-primary="paged attention" data-type="indexterm" id="ix_ch05-asciidoc29"/><a data-primary="serving GenAI models" data-secondary="external serving" data-tertiary="paged attention" data-type="indexterm" id="ix_ch05-asciidoc30"/>L'uso efficiente della memoria è una sfida cruciale per i sistemi che gestiscono un servizio ad alto rendimento, in particolare per gli LLMs. Per un'inferenza più veloce, i modelli odierni si affidano alle <em>cache KV</em> per memorizzare e riutilizzare le mappe di attenzione, che crescono esponenzialmente con l'aumentare della lunghezza delle sequenze di input.</p>
<p>L<em>'attenzione a pagine</em> è una soluzione innovativa progettata per ridurre al minimo la richiesta di memoria di queste cache KV, migliorando così l'efficienza della memoria degli LLMs e rendendoli più praticabili per l'uso su dispositivi con risorse limitate. Negli LLMs basati su trasformatori, i tensori delle chiavi e dei valori di attenzione vengono generati per ogni token di ingresso per catturare il contesto essenziale.
Invece di ricalcolare questi tensori a ogni passo, vengono salvati nella memoria della GPU sotto forma di cache KV, che funge da memoria del modello. Tuttavia, la cache KV può raggiungere dimensioni enormi, come 40 GB per un modello con 13B parametri, rappresentando una sfida significativa per l'archiviazione e l'accesso efficienti, in particolare su hardware con risorse limitate.</p>
<p>L'attenzione a pagine introduce un metodo che suddivide la cache KV in segmenti più piccoli e gestibili, chiamati <em>pagine</em>, ognuno dei quali contiene un vettore KV per un determinato numero di token. Grazie a questa segmentazione, l'attenzione a pagine può caricare e accedere in modo efficiente alla cache KV durante i calcoli dell'attenzione. Si può paragonare questa tecnica al modo in cui la memoria virtuale viene gestita dai sistemi operativi, in cui la disposizione logica dei dati è separata dalla loro memorizzazione fisica.
In sostanza, una tabella di blocchi mappa i blocchi logici in quelli fisici, consentendo l'allocazione dinamica della memoria quando vengono elaborati nuovi token. L'idea di base è quella di evitare la frammentazione della memoria sfruttando i blocchi logici (invece di quelli fisici) e di utilizzare una tabella di mappatura per accedere rapidamente ai dati archiviati in una memoria fisica paginata.</p>
<p>Puoi suddividere il meccanismo dell'attenzione paginata in diverse fasi:</p>
<dl>
<dt>Partizionare la cache KV</dt>
<dd>
<p>La cache è suddivisa in pagine di dimensioni fisse, ognuna delle quali contiene una parte delle coppie chiave-valore.</p>
</dd>
<dt>Creare la tabella di ricerca</dt>
<dd>
<p>Viene creata una tabella per mappare le chiavi di query alle pagine corrispondenti, facilitando l'assegnazione e il recupero rapido.</p>
</dd>
<dt>Caricamento selettivo</dt>
<dd>
<p>Durante l'inferenza vengono caricate solo le pagine necessarie per la sequenza di input corrente, riducendo l'ingombro della memoria.</p>
</dd>
<dt>Calcolo dell'attenzione</dt>
<dd>
<p>Il modello calcola l'attenzione utilizzando le coppie chiave-valore delle pagine caricate. Questo approccio mira a rendere gli LLMs più accessibili affrontando il collo di bottiglia della memoria, consentendo potenzialmente la loro distribuzione su una gamma più ampia di dispositivi.</p>
</dd>
</dl>
<p>Le fasi sopra descritte permettono al server vLLM di massimizzare l'efficienza dell'uso della memoria attraverso la mappatura dei blocchi di memoria fisica e logica in modo che la cache KV sia memorizzata e recuperata in modo efficiente durante la generazione.</p>
<p>In un <a href="https://oreil.ly/WgRfJ">post pubblicato sul blog di Anyscale.com</a>, gli autori hanno studiato e confrontato le prestazioni di vari framework di LLM durante l'inferenza, concludendo che i meccanismi di attenzione paginata e di batching continuo sono così potenti nell'ottimizzare l'uso della memoria della GPU che il server vLLM è stato in grado di ridurre le latenze di 4 volte e il throughput fino a 23 volte.</p>
<p class="less_space pagebreak-before">Nella prossima sezione, rivolgeremo la nostra attenzione ai carichi di lavoro GenAI che possono richiedere molto tempo per essere elaborati e che sono delimitati dal calcolo. Questo è principalmente il caso di modelli non LLM di grandi dimensioni come SDXL, dove l'esecuzione di inferenze in batch (come la generazione di immagini in batch) per più utenti può rivelarsi <a data-startref="ix_ch05-asciidoc30" data-type="indexterm" id="id863"/><a data-startref="ix_ch05-asciidoc29" data-type="indexterm" id="id864"/><a data-startref="ix_ch05-asciidoc26" data-type="indexterm" id="id865"/><a data-startref="ix_ch05-asciidoc25" data-type="indexterm" id="id866"/> <a data-startref="ix_ch05-asciidoc24" data-type="indexterm" id="id867"/> .<a data-startref="ix_ch05-asciidoc23" data-type="indexterm" id="id868"/><a data-startref="ix_ch05-asciidoc22" data-type="indexterm" id="id869"/></p>
</div></section>
</div></section>
</div></section>
<section data-pdf-bookmark="Managing Long-Running AI Inference Tasks" data-type="sect1"><div class="sect1" id="id87">
<h1>Gestione dei compiti di inferenza dell'intelligenza artificiale a lungo termine</h1>
<p><a data-primary="concurrency" data-secondary="managing long-running AI inference tasks" data-type="indexterm" id="ix_ch05-asciidoc31"/>Grazie alla possibilità di ospitare i modelli in un processo separato al di fuori del ciclo di eventi FastAPI, puoi rivolgere la tua attenzione alle operazioni bloccanti che richiedono molto tempo per essere completate.</p>
<p>Nella sezione precedente, hai sfruttato framework specializzati come vLLM per ospitare esternamente e ottimizzare i carichi di lavoro di inferenza dei tuoi LLMs. Tuttavia, potresti ancora imbatterti in modelli che possono richiedere molto tempo per generare risultati. Per evitare che i tuoi utenti attendano, dovresti gestire le attività che generano modelli e che richiedono molto tempo per essere completate.</p>
<p>Diversi modelli GenAI, come Stable Diffusion XL, possono richiedere diversi minuti, anche su una GPU, per produrre risultati. Nella maggior parte dei casi, puoi chiedere ai tuoi utenti di aspettare fino al completamento del processo di generazione. Ma se gli utenti utilizzano un singolo modello contemporaneamente, il server dovrà mettere in coda queste richieste. Quando i tuoi utenti lavorano con i modelli generativi, devono interagire con essi più volte per guidare il modello verso i risultati desiderati. Questo modello di utilizzo crea un grande arretrato di richieste e gli utenti alla fine della coda dovranno aspettare a lungo prima di vedere i risultati.</p>
<p>Se ci fosse un modo per gestire le attività di lunga durata senza far aspettare gli utenti, sarebbe perfetto. Fortunatamente, FastAPI fornisce un meccanismo per risolvere questo tipo di problemi.</p>
<p><a data-primary="background tasks (FastAPI mechanism)" data-type="indexterm" id="ix_ch05-asciidoc32"/>I <em>compiti in background</em> di FastAPI sono un meccanismo che puoi sfruttare per rispondere agli utenti mentre i tuoi modelli sono impegnati nell'elaborazione delle richieste. Questa funzione ti è stata presentata brevemente durante la costruzione del modulo RAG, dove un compito in background popolava un database vettoriale con il contenuto dei documenti PDF caricati.</p>
<p>Utilizzando i task in background, i tuoi utenti possono continuare a inviare richieste o a svolgere la loro giornata senza dover aspettare. Puoi salvare i risultati su disco o su un database per poterli recuperare in un secondo momento, oppure fornire un sistema di polling in modo che il client possa fare un ping per ricevere gli aggiornamenti mentre il modello elabora le richieste. Un'altra opzione è quella di creare una connessione live tra il client e il server in modo che l'interfaccia utente venga aggiornata con i risultati non appena questi sono disponibili. Tutte queste soluzioni sono realizzabili con i task in background di FastAPI.</p>
<p>L<a data-type="xref" href="#fastapi_background_tasks">'esempio 5-18</a> mostra come implementare i task in background per gestire le inferenze del modello a lungo termine.</p>
<div data-type="example" id="fastapi_background_tasks">
<h5><span class="label">Esempio 5-18. </span>Utilizzo di task in background per gestire l'inferenza del modello a lungo termine (ad esempio, la generazione di immagini in batch)</h5>
<pre data-code-language="python" data-type="programlisting" translate="no"><code class="c1" translate="no"># main.py</code><code translate="no">
</code><code translate="no">
</code><code class="kn" translate="no">from</code><code translate="no"> </code><code class="nn" translate="no">fastapi</code><code translate="no"> </code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="n" translate="no">BackgroundTasks</code><code translate="no">
</code><code class="kn" translate="no">import</code><code translate="no"> </code><code class="nn" translate="no">aiofiles</code><code translate="no">
</code><code translate="no">
</code><code class="o" translate="no">.</code><code class="o" translate="no">.</code><code class="o" translate="no">.</code><code translate="no">
</code><code translate="no">
</code><code class="k" translate="no">async</code><code translate="no"> </code><code class="k" translate="no">def</code><code translate="no"> </code><code class="nf" translate="no">batch_generate_image</code><code class="p" translate="no">(</code><code class="n" translate="no">prompt</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="nb" translate="no">str</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">count</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="nb" translate="no">int</code><code class="p" translate="no">)</code><code translate="no"> </code><code class="o" translate="no">-</code><code class="o" translate="no">&gt;</code><code translate="no"> </code><code class="kc" translate="no">None</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">images</code><code translate="no"> </code><code class="o" translate="no">=</code><code translate="no"> </code><code class="n" translate="no">generate_images</code><code class="p" translate="no">(</code><code class="n" translate="no">prompt</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">count</code><code class="p" translate="no">)</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO17-1" id="co_achieving_concurrency_in_ai_workloads_CO17-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">for</code><code translate="no"> </code><code class="n" translate="no">i</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">image</code><code translate="no"> </code><code class="ow" translate="no">in</code><code translate="no"> </code><code class="nb" translate="no">enumerate</code><code class="p" translate="no">(</code><code class="n" translate="no">images</code><code class="p" translate="no">)</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">        </code><code class="k" translate="no">async</code><code translate="no"> </code><code class="k" translate="no">with</code><code translate="no"> </code><code class="n" translate="no">aiofiles</code><code class="o" translate="no">.</code><code class="n" translate="no">open</code><code class="p" translate="no">(</code><code class="sa" translate="no">f</code><code class="s2" translate="no">"</code><code class="s2" translate="no">output_</code><code class="si" translate="no">{</code><code class="n" translate="no">i</code><code class="si" translate="no">}</code><code class="s2" translate="no">.png</code><code class="s2" translate="no">"</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">mode</code><code class="o" translate="no">=</code><code class="s1" translate="no">'</code><code class="s1" translate="no">wb</code><code class="s1" translate="no">'</code><code class="p" translate="no">)</code><code translate="no"> </code><code class="k" translate="no">as</code><code translate="no"> </code><code class="n" translate="no">f</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">            </code><code class="k" translate="no">await</code><code translate="no"> </code><code class="n" translate="no">f</code><code class="o" translate="no">.</code><code class="n" translate="no">write</code><code class="p" translate="no">(</code><code class="n" translate="no">image</code><code class="p" translate="no">)</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO17-2" id="co_achieving_concurrency_in_ai_workloads_CO17-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">
</code><code class="nd" translate="no">@app</code><code class="o" translate="no">.</code><code class="n" translate="no">get</code><code class="p" translate="no">(</code><code class="s2" translate="no">"</code><code class="s2" translate="no">/generate/image/background</code><code class="s2" translate="no">"</code><code class="p" translate="no">)</code><code translate="no">
</code><code class="k" translate="no">def</code><code translate="no"> </code><code class="nf" translate="no">serve_image_model_background_controller</code><code class="p" translate="no">(</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">background_tasks</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="n" translate="no">BackgroundTasks</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">prompt</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="nb" translate="no">str</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">count</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="nb" translate="no">int</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO17-3" id="co_achieving_concurrency_in_ai_workloads_CO17-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a><code translate="no">
</code><code class="p" translate="no">)</code><code class="p" translate="no">:</code><code translate="no">
</code><code translate="no">    </code><code class="n" translate="no">background_tasks</code><code class="o" translate="no">.</code><code class="n" translate="no">add_task</code><code class="p" translate="no">(</code><code class="n" translate="no">batch_generate_image</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">prompt</code><code class="p" translate="no">,</code><code translate="no"> </code><code class="n" translate="no">count</code><code class="p" translate="no">)</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO17-4" id="co_achieving_concurrency_in_ai_workloads_CO17-4"><img alt="4" src="assets/4.png" width="12" height="12"/></a><code translate="no">
</code><code translate="no">    </code><code class="k" translate="no">return</code><code translate="no"> </code><code class="p" translate="no">{</code><code class="s2" translate="no">"</code><code class="s2" translate="no">message</code><code class="s2" translate="no">"</code><code class="p" translate="no">:</code><code translate="no"> </code><code class="s2" translate="no">"</code><code class="s2" translate="no">Task is being processed in the background</code><code class="s2" translate="no">"</code><code class="p" translate="no">}</code><code translate="no"> </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO17-5" id="co_achieving_concurrency_in_ai_workloads_CO17-5"><img alt="5" src="assets/5.png" width="12" height="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO17-1" id="callout_achieving_concurrency_in_ai_workloads_CO17-1"><img alt="1" src="assets/1.png" width="12" height="12"/></a></dt>
<dd><p>Genera più immagini in un lotto utilizzando un'API esterna di model-serving come <a href="https://oreil.ly/NjlV4">Ray Serve</a>.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO17-2" id="callout_achieving_concurrency_in_ai_workloads_CO17-2"><img alt="2" src="assets/2.png" width="12" height="12"/></a></dt>
<dd><p>Esegui il loop delle immagini generate e salva ogni immagine su disco in modo asincrono utilizzando la libreria <code translate="no">aiofiles</code>. In produzione, puoi anche salvare le immagini di output su soluzioni di cloud storage da cui i clienti possono attingere direttamente.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO17-3" id="callout_achieving_concurrency_in_ai_workloads_CO17-3"><img alt="3" src="assets/3.png" width="12" height="12"/></a></dt>
<dd><p>Abilita il controller ad eseguire attività in background.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO17-4" id="callout_achieving_concurrency_in_ai_workloads_CO17-4"><img alt="4" src="assets/4.png" width="12" height="12"/></a></dt>
<dd><p>Passa la definizione della funzione <code translate="no">batch_generate_image</code> a un gestore di attività in background FastAPI con gli argomenti richiesti.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO17-5" id="callout_achieving_concurrency_in_ai_workloads_CO17-5"><img alt="5" src="assets/5.png" width="12" height="12"/></a></dt>
<dd><p>Restituisce un generico messaggio di successo al client prima di elaborare l'attività in background, in modo che l'utente non rimanga in attesa.</p></dd>
</dl></div>
<p>Nell'<a data-type="xref" href="#fastapi_background_tasks">Esempio 5-18</a>, stai permettendo a FastAPI di eseguire operazioni di inferenza in background (tramite un'API esterna del server del modello) in modo che il ciclo degli eventi rimanga sbloccato per elaborare altre richieste in arrivo. Puoi anche eseguire più attività in background, come la generazione di immagini in batch (in processi separati) e l'invio di e-mail di notifica. Queste attività vengono aggiunte a una coda ed elaborate in modo sequenziale senza bloccare l'utente. Puoi quindi memorizzare le immagini generate ed esporre un endpoint aggiuntivo che i clienti possono utilizzare per richiedere aggiornamenti sullo stato e recuperare i risultati dell'inferenza.</p>
<div data-type="warning" epub:type="warning"><h6>Avvertenze</h6>
<p>I task in background vengono eseguiti nello stesso ciclo di eventi, ma non forniscono un vero parallelismo, bensì solo una concomitanza.</p>
<p>Se esegui operazioni pesanti per la CPU, come l'inferenza dell'intelligenza artificiale, nei task<span class="keep-together">in background</span>, bloccherai il ciclo di eventi principale fino a quando tutti i task in background non saranno stati completati. Allo stesso modo, fai attenzione ai task<span class="keep-together">in background</span> asincroni: se non attendi le operazioni di I/O bloccanti, il task bloccherà il server principale dalla risposta ad altre richieste, anche se viene eseguito in background. FastAPI esegue i task in background non asincroni in un pool di thread interno.</p>
</div>
<p>Sebbene i task in background di FastAPI siano un ottimo strumento per gestire semplici lavori in batch, non sono scalabili e non sono in grado di gestire le eccezioni o i tentativi di risposta come gli strumenti specializzati. Altri framework per il servizio di ML come Ray Serve, BentoML e vLLM possono gestire meglio il servizio di modelli in scala fornendo funzioni come il batching delle richieste. Anche strumenti più sofisticati come Celery (un gestore di code), Redis (un database per la cache) e RabbitMQ (un broker di messaggi) possono essere utilizzati in combinazione per implementare una pipeline di inferenza più robusta e affidabile<a data-startref="ix_ch05-asciidoc32" data-type="indexterm" id="id870"/>.<a data-startref="ix_ch05-asciidoc31" data-type="indexterm" id="id871"/></p>
</div></section>
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="id88">
<h1>Sommario</h1>
<p>Questo capitolo ha esplorato gli aspetti complessi dell'applicazione della concomitanza nei sistemi di intelligenza artificiale.</p>
<p>Sei stato introdotto ai concetti di concorrenza e parallelismo, compresi i vari tipi di operazioni bloccanti che impediscono di servire simultaneamente gli utenti. Hai scoperto le tecniche di concorrenza come il multithreading, il multiprocessing e la programmazione asincrona, oltre alle loro differenze, somiglianze, vantaggi e svantaggi in vari casi d'uso.</p>
<p>Poi hai imparato a conoscere i pool di thread e i loop di eventi, in particolare in un ambiente server FastAPI, e hai compreso il loro ruolo nell'elaborazione delle richieste in modo concorrente. Questo ha comportato la comprensione di come e perché il server si può bloccare se non stai attento a come dichiari i gestori delle rotte.</p>
<p>In seguito, hai scoperto come implementare la programmazione asincrona per gestire le operazioni di blocco I/O. Attraverso esempi pratici, hai sviluppato una comprensione più profonda delle interazioni asincrone con i database e i contenuti web, costruendo sia un web scraper che un modulo RAG.</p>
<p>Inoltre, hai visto perché i modelli GenAI più grandi possono essere affamati di memoria e creare operazioni di blocco delimitate dalla memoria. In questo contesto, ti sono state presentate tecniche di ottimizzazione della memoria come il batching continuo e l'attenzione paginata nel servire LLMs per ridurre al minimo i colli di bottiglia legati alla memoria.</p>
<p>Infine, hai imparato a conoscere gli approcci per gestire i processi di inferenza dell'intelligenza artificiale di lunga durata, assicurandoti che il tuo servizio rimanga reattivo anche per operazioni prolungate.</p>
<p>Grazie alle conoscenze acquisite in questo capitolo, ora sei pronto ad applicare i principi della concorrenza ai tuoi servizi, creando applicazioni AI resilienti, scalabili e ad alte prestazioni.</p>
<p>La capacità di gestire più utenti contemporaneamente è un traguardo importante, ma ci sono ulteriori ottimizzazioni che puoi effettuare per migliorare ulteriormente l'esperienza utente dei tuoi servizi GenAI. Puoi fornire aggiornamenti in tempo reale tramite tecnologie di streaming per mostrare progressivamente agli utenti risultati quasi in tempo reale durante la generazione. Questo è particolarmente utile per gli LLMs che possono avere tempi di generazione più lunghi in scenari di conversazione.</p>
<p>Il prossimo capitolo esplorerà i carichi di lavoro in streaming dell'intelligenza artificiale, illustrando l'uso di tecnologie di comunicazione in tempo reale come gli eventi inviati dal server (SSE) e i WebSocket (WS). Imparerai la differenza tra queste tecnologie e come implementare lo streaming di modelli costruendo endpoint per interazioni in tempo reale da testo a testo, da testo a voce e da voce a testo.<a data-startref="ix_ch05-asciidoc0" data-type="indexterm" id="id872"/></p>
</div></section>
<section data-pdf-bookmark="Additional References" data-type="sect1"><div class="sect1" id="id442">
<h1>Riferimenti aggiuntivi</h1>
<ul>
<li>
<p>Kwon, W., et al. (2023).<a href="https://oreil.ly/PtCqL">"Efficient Memory Management for Large Language Model Serving with PagedAttention".</a> arXiv preprint arXiv:2309.06180.</p>
</li>
<li>
<p>Lewis, P., et al. (2022).<a href="https://oreil.ly/r5yVL">"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks".</a> arXiv preprint arXiv:2005.11401.</p>
</li>
</ul>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id795"><sup><a href="ch05.html#id795-marker">1</a></sup> Un core è una singola unità di elaborazione all'interno di una CPU o di una GPU che esegue le istruzioni. Le CPU e le GPU moderne dispongono di più core per eseguire i compiti simultaneamente.</p><p data-type="footnote" id="id802"><sup><a href="ch05.html#id802-marker">2</a></sup> Il multithreading nella maggior parte dei linguaggi è parallelo (eseguito su più core) e non concorrente. Python sta cambiando nelle prossime versioni per fare lo stesso (free-threading Python).</p><p data-type="footnote" id="id817"><sup><a href="ch05.html#id817-marker">3</a></sup> Puoi trovare un'implementazione personalizzata in <a href="https://oreil.ly/8E7GQ">OpenAI Cookbook su GitHub</a>.</p><p data-type="footnote" id="id822"><sup><a href="ch05.html#id822-marker">4</a></sup> Il costo dell'impostazione dei fili viene comunque sostenuto, ma viene fatto in anticipo per evitare di farlo al volo in seguito.</p><p data-type="footnote" id="id830"><sup><a href="ch05.html#id830-marker">5</a></sup> P. Lewis et al. (2022), <a href="https://oreil.ly/GCk08">"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",</a> arXiv preprint arXiv:2005.11401.</p><p data-type="footnote" id="id841"><sup><a href="ch05.html#id841-marker">6</a></sup> L'operazione del prodotto di punti moltiplica i componenti di due vettori e poi somma i risultati. Può essere utilizzata per calcolare il coseno dell'angolo tra i vettori per quantificare la loro somiglianza di direzione (cioè l'allineamento). I database vettoriali la utilizzano per eseguire ricerche semantiche sugli embedding dei documenti.</p><p data-type="footnote" id="id843"><sup><a href="ch05.html#id843-marker">7</a></sup> Consulta la <a href="https://oreil.ly/V4itQ">documentazione</a> di <a href="https://oreil.ly/V4itQ">Docker</a> per le istruzioni di installazione.</p></div></div></section></div></div></body></html>