<html><head></head><body><section data-pdf-bookmark="Chapter 5. Achieving Concurrency in AI Workloads" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch05">
<h1><span class="label">Chapter 5. </span>Achieving Concurrency in AI Workloads</h1>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id793">
<h1>Chapter Goals</h1>
<p><a data-primary="concurrency" data-type="indexterm" id="ix_ch05-asciidoc0"/>In this chapter, you will learn about:</p>

<ul>
<li>
<p>The role and benefits of multithreading, multiprocessing, and asynchronous programming in enhancing AI application performance and scalability</p>
</li>
<li>
<p>The role of the thread pool and event loop in a FastAPI server</p>
</li>
<li>
<p>How to avoid blocking the server when processing requests</p>
</li>
<li>
<p>Using asynchronous programming to interact with external systems such as databases, AI models, and web content by building a web scraper and a RAG module</p>
</li>
<li>
<p>The model-serving strategies and LLM memory-optimization strategies to reduce memory-bound blocking operations</p>
</li>
<li>
<p>Strategies for handling long-running AI inference tasks</p>
</li>
</ul>
</div></aside>

<p>In this chapter, you will learn more about the role and benefits of asynchronous programming in boosting the performance and scalability of your GenAI services.
As part of this, you’ll learn to manage concurrent user interactions and interface with external systems such as databases, implement RAG, and read web pages to enrich the context of model prompts.
You’ll acquire techniques for effectively dealing with 
<span class="keep-together">I/O-bound</span> and CPU-bound operations, especially when dealing with external services or handling long-running inference tasks.</p>

<p>We will also dive into strategies for efficiently handling long-running Generative AI inference tasks, including the use of FastAPI event loop for background tasks 
<span class="keep-together">execution.</span></p>






<section data-pdf-bookmark="Optimizing GenAI Services for Multiple Users" data-type="sect1"><div class="sect1" id="id76">
<h1>Optimizing GenAI Services for Multiple Users</h1>

<p><a data-primary="concurrency" data-secondary="optimizing GenAI services for multiple users" data-type="indexterm" id="ix_ch05-asciidoc1"/>AI workloads are computationally expensive operations that can inhibit your GenAI services from serving multiple simultaneous requests.
In most production scenarios, multiple users will be using your applications.
Therefore, your services will be expected to serve requests <em>concurrently</em>
such that multiple overlapping tasks can be executed.
However, if you’re interfacing with GenAI models and external systems such as databases, the filesystem, or the internet, there will be operations that can block other tasks from executing on your server.
<a data-primary="blocking operations" data-type="indexterm" id="id794"/>Long-running operations that 
<span class="keep-together">can halt</span> the program execution flow are considered <em>blocking</em>.</p>

<p>These blocking operations can be twofold:</p>
<dl>
<dt>Input/output (I/O) bound</dt>
<dd>
<p>Where a process has to wait because of data input/output operation, which can come from a user, file, database, network, etc.
Examples include reading or writing a file to a disk, making network requests and API calls, sending or receiving data from databases, or waiting for user input.</p>
</dd>
<dt>Compute bound</dt>
<dd>
<p>Where a process has to wait because of a compute-intensive operation on CPU or GPU.
Compute-bound programs push the CPU or GPU cores to their limit by performing intensive computations, often blocking them from performing other tasks.<sup><a data-type="noteref" href="ch05.html#id795" id="id795-marker">1</a></sup>
Examples include data processing, AI model inference or training, 3D object rendering, running simulations, etc.</p>
</dd>
</dl>

<p>You have a few strategies to serve multiple users:</p>
<dl>
<dt>System optimization</dt>
<dd>
<p>For I/O-bound tasks like fetching data from a database, working with files on disk, making network requests, or reading web pages</p>
</dd>
<dt>Model optimization</dt>
<dd>
<p>For memory- and compute-bound tasks such as model loading and inference</p>
</dd>
<dt>Queuing system</dt>
<dd>
<p>For handling long-running inference tasks to avoid delays in responding</p>
</dd>
</dl>

<p class="less_space pagebreak-before">In this section, we will look at each strategy in more detail.
To help solidify your learning, we will also implement several features together that leverage the aforementioned strategies:</p>

<ul>
<li>
<p>Building a <em>web page scraper</em> for bulk fetching and parsing of HTTP URLs pasted in the chat, so that you can ask your LLM about the content of web pages</p>
</li>
<li>
<p>Adding a <em>retrieval augmented generation</em> (RAG) module to your service with a self-hosted vector database such as <code>qdrant</code> so that you can upload and talk to your documents via your LLM service</p>
</li>
<li>
<p>Adding a <em>batch image generation system</em>
so that you can run image generation workloads as background tasks</p>
</li>
</ul>

<p>Before I can show you how to build the aforementioned features, we should dive deeper into the topic of <em>concurrency</em> and <em>parallelism</em> as understanding both concepts will help you identify the correct strategies to use for your own use cases.</p>

<p><a data-primary="concurrency" data-secondary="defined" data-type="indexterm" id="id796"/><em>Concurrency</em> refers to the ability of a service in handling multiple requests or tasks at the same time, without completing one after another.
During concurrent operations, the timeline of multiple tasks can overlap and may start and end at different times.</p>

<p>In Python, you can implement concurrency with a single CPU core by switching between tasks on a single thread (via asynchronous programming) or across different threads (via multithreading).</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id797">
<h1>Time Slicing</h1>
<p><a data-primary="multithreading" data-secondary="time slicing" data-type="indexterm" id="id798"/><a data-primary="time slicing" data-type="indexterm" id="id799"/><em>Time slicing</em> is a scheduling mechanism in multithreading and asynchronous programming where a process allocates CPU time between tasks to give the illusion of concurrent execution.</p>

<p><a data-primary="Global Interpreter Lock (GIL)" data-type="indexterm" id="id800"/><a data-primary="Python" data-secondary="Global Interpreter Lock (GIL)" data-tertiary="time slicing" data-type="indexterm" id="id801"/>In Python, the CPU time can be allocated to only one task at any moment because of Python’s <em>Global Interpreter Lock</em> (GIL).
Python’s GIL allows only one thread (i.e., execution flow in a Python process) to control the Python interpreter for executing code.
This means that only one thread can be in a state of execution at any point in time.<sup><a data-type="noteref" href="ch05.html#id802" id="id802-marker">2</a></sup></p>

<p>The GIL was originally implemented to simplify Python’s development, ease memory management between threads in a Python process, and prioritize the performance of single-threaded programs.
It was also added to Python to ensure <em>thread-safety</em> within the process, preventing race conditions that could lead to data corruption when working with shared resources between threads.</p>

<p>However, the addition of the GIL to Python also means that true parallel execution of threads in a single Python process is not possible.
When a task is waiting for an I/O operation to finish, the CPU can quickly switch to another task to avoid blocking other operations.
The paused tasks save their state and can resume once the I/O operations are done.</p>
</div></aside>

<p>With multiple cores, you can also implement a subset of concurrency called <em>parallelism</em>
where tasks are split among several independent workers (via multiprocessing), with each executing tasks simultaneously on their own isolated resources and separate processes.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Although there are plans to remove the GIL from Python soon, at the time of this writing it is not possible for multiple threads to simultaneously work through tasks.
Therefore, concurrency on a single core can give an illusion of parallelism even though there is one process doing all the work.
The single process can only multitask by switching active threads to minimize waiting times of I/O blocking operations.</p>

<p>You can only achieve true parallelism with multiple workers (in multiprocessing).</p>
</div>

<p><a data-primary="concurrency" data-secondary="parallelism versus" data-type="indexterm" id="ix_ch05-asciidoc2"/><a data-primary="parallelism, concurrency versus" data-type="indexterm" id="ix_ch05-asciidoc3"/>Even though concurrency and parallelism have many similarities, they aren’t exactly the same concepts.
The big difference between them is that concurrency can help you manage multiple tasks by interleaving their execution, which is useful for I/O-bound tasks.
Parallelism, on the other hand, involves executing multiple tasks simultaneously, typically on multicore machines, which is more useful for CPU-bound tasks.</p>

<p>You can implement concurrency using approaches like threading or asynchronous programming (i.e., time-slicing on a single-core machine, where tasks are interleaved to give the appearance of simultaneous execution).</p>

<p><a data-type="xref" href="#concurrency_parallelism">Figure 5-1</a> shows the relationship between concurrency and parallelism.</p>

<figure class="less_space pagebreak-before"><div class="figure" id="concurrency_parallelism">
<img alt="bgai 0501" src="assets/bgai_0501.png"/>
<h6><span class="label">Figure 5-1. </span>Concurrency and parallelism</h6>
</div></figure>

<p>In most scalable systems, you can witness both concurrency and parallelism.</p>

<p>Imagine that you’re visiting a fast-food restaurant and placing an order.
In a concurrent system, you’ll see the restaurant owner taking orders while cooking burgers, attending to each task time to time, and effectively multitasking by switching between tasks.
In a parallel system, you’ll see multiple staff members taking orders and a few others cooking the burgers at the same time.
Here different workers handle each task simultaneously.</p>

<p>Without any multithreading or asynchronous programming in a single-threaded process, the process has to wait for blocking operations to finish before it can start new tasks.
Without multiprocessing implementing parallelism on multiple cores, computationally expensive operations can block the application from starting other tasks.</p>

<p><a data-type="xref" href="#concurrency_parallelism_timeline">Figure 5-2</a> shows the distinctions between nonconcurrent execution, concurrent execution without parallelism (single core), and concurrent execution with parallelism (multiple cores).</p>

<p>The three Python execution models shown in <a data-type="xref" href="#concurrency_parallelism_timeline">Figure 5-2</a> are as follows:</p>
<dl>
<dt>No concurrency (synchronous)</dt>
<dd>
<p>A single process (on one core) executes tasks sequentially.</p>
</dd>
<dt>Concurrent and non-parallel</dt>
<dd>
<p>Multiple threads in a single process (on a core) handle tasks concurrently but not in parallel due to Python’s GIL.</p>
</dd>
<dt>Concurrent and parallel</dt>
<dd>
<p>Multiple processes on multiple cores perform the tasks in parallel, making the most of multicore processors for maximum efficiency.<a data-startref="ix_ch05-asciidoc3" data-type="indexterm" id="id803"/><a data-startref="ix_ch05-asciidoc2" data-type="indexterm" id="id804"/></p>
</dd>
</dl>

<figure><div class="figure" id="concurrency_parallelism_timeline">
<img alt="bgai 0502" src="assets/bgai_0502.png"/>
<h6><span class="label">Figure 5-2. </span>Concurrency with and without parallelism</h6>
</div></figure>

<p class="less_space pagebreak-before"><a data-primary="multiprocessing" data-type="indexterm" id="id805"/>In multiprocessing, each process has access to its own memory space and resources to complete a task in isolation from other processes.
This isolation can make processes more stable—since if a process crashes, it won’t affect others—but makes inter-process communication more complex compared to threads, which share the same memory space, as shown in <a data-type="xref" href="#multiprocessing_resources">Figure 5-3</a>.</p>

<figure><div class="figure" id="multiprocessing_resources">
<img alt="bgai 0503" src="assets/bgai_0503.png"/>
<h6><span class="label">Figure 5-3. </span>Resource sharing in multithreading and multiprocessing</h6>
</div></figure>

<p>Distributed workloads often use a managing process that coordinates the execution and collaboration of these processes to avoid issues such as data corruption and duplicating work.
A good example of multiprocessing is when you serve requests with a load balancer managing traffic to multiple containers, each running an instance of your application.</p>

<p>Both multithreading and asynchronous programming reduce wait time in I/O tasks because the processor can do other work while waiting for I/O. However, they don’t help with tasks that require heavy computation, like AI inference, because the process is busy with computing some results.
Therefore, to serve a large self-hosted GenAI model to multiple users, you should either scale services with multiprocessing or use algorithmic model optimizations (via specialized model inference servers like vLLM).</p>

<p>Your first instinct when working with slow models may be to adopt parallelism by creating multiple instances of your FastAPI service (multiprocessing) in a single machine to serve requests in parallel.</p>

<p>Unfortunately, multiple workers running in separate processes will not have access to a shared memory space.
As a result, you can’t share artifacts—​like a GenAI model—​loaded in memory between separate instances of your app in FastAPI.
Sadly, a new instance of your model will also need to be loaded, which will significantly eat up your hardware resources.
This is because FastAPI is a general-purpose web server that doesn’t natively optimize serving GenAI models.</p>

<p>The solution is not parallelism on its own, but to adopt the external model-serving strategy, as discussed in <a data-type="xref" href="ch03.html#ch03">Chapter 3</a>.</p>

<p>The only instance where you can treat AI inference workloads as I/O-bound, instead of compute-bound, is when you’re relying on third-party AI provider APIs (e.g., OpenAI API). In this case, you’re offloading the compute-bound tasks to the model provider through network requests.</p>

<p>On your side, the AI inference workloads become I/O-bound through network requests, allowing for the use of concurrency through time slicing.
The third-party provider has to worry about scaling their services to handle model inferences—​that are compute-bound—​across their hardware resources.</p>

<p>You can externalize the serving and inference of larger GenAI models such as an LLM, with specialized servers like vLLM, Ray Serve, or NVIDIA Triton.</p>

<p>Later in this chapter, I will detail how these servers maximize inference efficiency of compute-bound operations during model inference while minimizing the model’s memory footprint during the data generation process.</p>

<p><a data-primary="concurrency" data-secondary="strategies" data-type="indexterm" id="ix_ch05-asciidoc4"/>To help you digest what was discussed so far, have a look at the comparison table of concurrency strategies in <a data-type="xref" href="#concurrency_comparison">Table 5-1</a> to understand when and why to use each.</p>
<table class="striped" id="concurrency_comparison">
<caption><span class="label">Table 5-1. </span>Comparison of concurrency strategies</caption>
<thead>
<tr>
<th>Strategy</th>
<th>Features</th>
<th>Challenges</th>
<th>Use cases</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>No concurrency (synchronous)</p></td>
<td>
<ul><li><p>Simple, readable, easy-to-understand code to debug</p></li>
<li><p>A single CPU core and thread</p></li></ul></td>
<td><ul>
<li><p>Potential long waiting times depending on I/O or CPU blocking operations halting the process execution</p></li>
<li><p>Can’t serve multiple users simultaneously</p></li></ul></td>
<td>
<ul>
<li><p>Single user applications where users can wait for tasks to finish</p></li>
<li><p>Infrequently used services or applications</p></li>
</ul></td>
</tr>
<tr>
<td><p>Async IO (asynchronous)</p></td>
<td>
<ul>
<li><p>A single CPU core and thread</p></li>
<li><p>Multitasking managed by an event loop within the Python process</p></li><li><p>Thread-safe as the Python process manages tasks</p></li>
<li><p>Maximizes the CPU utilization rate</p></li>
<li><p>Faster than multithreading and multiprocessing for I/O tasks</p></li>
</ul></td>
<td><ul><li><p>Harder to implement in code and can make debugging harder</p></li>
<li><p>Requires libraries and dependencies that use Async IO features</p></li>
<li><p>Easy to make mistakes that block the main process (and event loop)</p></li></ul></td>
<td><p>Applications that have blocking I/O tasks</p></td>
</tr>
<tr>
<td><p>Multithreading</p></td>
<td>
<ul>
<li><p>A single CPU core but multiple threads within the same process</p></li>
<li><p>Threads share data and resources</p></li>
<li><p>Simpler than Async IO to implement in code</p></li>
<li><p>Multitasking across threads orchestrated by the OS</p></li></ul></td>
<td><ul><li><p>Difficult to lock resources for each thread
to avoid thread-safety issues that can lead to nonreproducible bugs and data corruption</p></li>
<li><p>Threads can block each other indefinitely (deadlocks)</p></li>
<li><p>Concurrent access to resources can cause inconsistent results (race conditions)</p></li>
<li><p>A thread can be denied resources by monopolizing threads (starvation)</p></li>
<li><p>Creating and destroying threads is computationally expensive</p></li></ul></td>
<td><p>Applications or services that have blocking I/O tasks</p></td>
</tr>
<tr>
<td><p>Multiprocessing</p></td>
<td>
<ul>
<li><p>Multiple processes running on several CPU cores</p></li>
<li><p>Each process is allocated a CPU core and isolated resources</p></li>
<li><p>Work can be distributed across CPU cores and managed by an orchestrator process using tools like Celery</p></li></ul></td>
<td><ul>
<li><p>Sharing hardware resources and objects like a large AI model or data between processes can be complex
and requires inter-process communication (IPC) mechanisms or a dedicated shared memory</p></li>
<li><p>Difficult to keep multiple isolated processes in sync</p></li>
<li><p>Creating and destroying processes is computationally expensive</p></li></ul></td>
<td><ul><li><p>Applications or services that have blocking compute-bound tasks</p></li>
<li><p>Divide-and-conquer type of tasks where processing can be done in isolated chunks</p></li>
<li><p>Distributing workloads or processing requests across multiple CPU cores</p></li></ul></td>
</tr>
</tbody>
</table>

<p>Now that we’ve explored various concurrency strategies, let’s continue by enhancing your services with asynchronous programming to efficiently manage I/O-bound operations.<a data-startref="ix_ch05-asciidoc4" data-type="indexterm" id="id806"/>
Later we’ll focus on optimizing compute-bound tasks, specifically model inference via specialized servers.<a data-startref="ix_ch05-asciidoc1" data-type="indexterm" id="id807"/></p>
</div></section>






<section data-pdf-bookmark="Optimizing for I/O Tasks with Asynchronous Programming" data-type="sect1"><div class="sect1" id="id77">
<h1>Optimizing for I/O Tasks with Asynchronous Programming</h1>

<p><a data-primary="asynchronous (async) I/O" data-type="indexterm" id="ix_ch05-asciidoc5"/><a data-primary="concurrency" data-secondary="optimizing for I/O tasks with asynchronous programming" data-type="indexterm" id="ix_ch05-asciidoc6"/>In this section, we’ll explore the use of asynchronous programming to prevent blocking the main server process with I/O-bound tasks during AI workloads.
You’ll also learn about the <code>asyncio</code> framework that enables writing asynchronous applications in Python.</p>








<section class="less_space pagebreak-before" data-pdf-bookmark="Synchronous Versus Asynchronous (Async) Execution" data-type="sect2"><div class="sect2" id="id78">
<h2>Synchronous Versus Asynchronous (Async) Execution</h2>

<p><a data-primary="asynchronous (async) I/O" data-secondary="synchronous execution versus" data-type="indexterm" id="ix_ch05-asciidoc7"/><a data-primary="concurrency" data-secondary="optimizing for I/O tasks with asynchronous programming" data-tertiary="synchronous versus asynchronous execution" data-type="indexterm" id="ix_ch05-asciidoc8"/><a data-primary="synchronous execution, asynchronous execution versus" data-type="indexterm" id="ix_ch05-asciidoc9"/>What is considered an asynchronous application?
To answer the question, let’s compare both synchronous and asynchronous programs.</p>

<p>An application is considered <em>synchronous</em>
when tasks are performed in a sequential order with each task waiting for the previous one to complete before starting. For applications that run infrequently and take only a few seconds to process, synchronous code rarely causes a problem and can make implementations faster and easier.
However, if you need concurrency and want the efficiency of your services to be maximized on every core, your services should multitask without waiting for blocking operations to complete.
That’s where implementing <em>asynchronous</em> (async) concurrency can help.</p>

<p>Let’s look at a few examples of synchronous and async functions to understand how much of a performance boost an async code can give you.
In both examples, I will use sleeping to simulate I/O blocking operation, but you can imagine other I/O tasks being performed in real-world scenarios.</p>

<p><a data-type="xref" href="#sync_execution">Example 5-1</a> shows an example of a synchronous code that simulates an I/O blocking operation with the blocking <code>time.sleep()</code> function.</p>
<div data-type="example" id="sync_execution">
<h5><span class="label">Example 5-1. </span>Synchronous execution</h5>

<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">time</code>

<code class="k">def</code> <code class="nf">task</code><code class="p">(</code><code class="p">)</code><code class="p">:</code>
    <code class="nb">print</code><code class="p">(</code><code class="s2">"</code><code class="s2">Start of sync task</code><code class="s2">"</code><code class="p">)</code>
    <code class="n">time</code><code class="o">.</code><code class="n">sleep</code><code class="p">(</code><code class="mi">5</code><code class="p">)</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO1-1" id="co_achieving_concurrency_in_ai_workloads_CO1-1"><img alt="1" src="assets/1.png"/></a>
    <code class="nb">print</code><code class="p">(</code><code class="s2">"</code><code class="s2">After 5 seconds of sleep</code><code class="s2">"</code><code class="p">)</code>

<code class="n">start</code> <code class="o">=</code> <code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">(</code><code class="p">)</code>
<code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">3</code><code class="p">)</code><code class="p">:</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO1-2" id="co_achieving_concurrency_in_ai_workloads_CO1-2"><img alt="2" src="assets/2.png"/></a>
    <code class="n">task</code><code class="p">(</code><code class="p">)</code>
<code class="n">duration</code> <code class="o">=</code> <code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">(</code><code class="p">)</code> <code class="o">-</code> <code class="n">start</code>
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="se">\n</code><code class="s2">Process completed in: </code><code class="si">{</code><code class="n">duration</code><code class="si">}</code><code class="s2"> seconds</code><code class="s2">"</code><code class="p">)</code>
<code class="sd">"""
Start of sync task
After 5 seconds of sleep
Start of sync task
After 5 seconds of sleep
Start of sync task
After 5 seconds of sleep

Process completed in: 15.014271020889282 seconds
"""</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO1-1" id="callout_achieving_concurrency_in_ai_workloads_CO1-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Use <code>sleep()</code> to simulate an I/O blocking operation such as sending a network request.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO1-2" id="callout_achieving_concurrency_in_ai_workloads_CO1-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Call the <code>task()</code> three times, sequentially.
The loop simulates sending multiple network requests, one after another.</p></dd>
</dl></div>

<p>Calling <code>task()</code> three times in <a data-type="xref" href="#sync_execution">Example 5-1</a> takes 15 seconds to complete as Python waits for the blocking operation <code>sleep()</code> to complete.</p>

<p>To develop async programs in Python, you can use the <code>asyncio</code> package as part of the standard library of Python 3.5 and later versions.
Using <code>asyncio</code>, asynchronous code looks similar to sequential synchronous code but with additions of <code>async</code> and <code>await</code> keywords to perform nonblocking I/O operations.</p>

<p><a data-type="xref" href="#async_execution">Example 5-2</a> shows how you can use <code>async</code> and <code>await</code> keywords with <code>asyncio</code> to run <a data-type="xref" href="#sync_execution">Example 5-1</a> asynchronously.</p>
<div data-type="example" id="async_execution">
<h5><span class="label">Example 5-2. </span>Asynchronous execution</h5>

<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">time</code>
<code class="kn">import</code> <code class="nn">asyncio</code>

<code class="k">async</code> <code class="k">def</code> <code class="nf">task</code><code class="p">(</code><code class="p">)</code><code class="p">:</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO2-1" id="co_achieving_concurrency_in_ai_workloads_CO2-1"><img alt="1" src="assets/1.png"/></a>
    <code class="nb">print</code><code class="p">(</code><code class="s2">"</code><code class="s2">Start of async task</code><code class="s2">"</code><code class="p">)</code>
    <code class="k">await</code> <code class="n">asyncio</code><code class="o">.</code><code class="n">sleep</code><code class="p">(</code><code class="mi">5</code><code class="p">)</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO2-2" id="co_achieving_concurrency_in_ai_workloads_CO2-2"><img alt="2" src="assets/2.png"/></a>
    <code class="nb">print</code><code class="p">(</code><code class="s2">"</code><code class="s2">Task resumed after 5 seconds</code><code class="s2">"</code><code class="p">)</code>

<code class="k">async</code> <code class="k">def</code> <code class="nf">spawn_tasks</code><code class="p">(</code><code class="p">)</code><code class="p">:</code>
    <code class="k">await</code> <code class="n">asyncio</code><code class="o">.</code><code class="n">gather</code><code class="p">(</code><code class="n">task</code><code class="p">(</code><code class="p">)</code><code class="p">,</code> <code class="n">task</code><code class="p">(</code><code class="p">)</code><code class="p">,</code> <code class="n">task</code><code class="p">(</code><code class="p">)</code><code class="p">)</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO2-3" id="co_achieving_concurrency_in_ai_workloads_CO2-3"><img alt="3" src="assets/3.png"/></a>

<code class="n">start</code> <code class="o">=</code> <code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">(</code><code class="p">)</code>
<code class="n">asyncio</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">spawn_tasks</code><code class="p">(</code><code class="p">)</code><code class="p">)</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO2-4" id="co_achieving_concurrency_in_ai_workloads_CO2-4"><img alt="4" src="assets/4.png"/></a>
<code class="n">duration</code> <code class="o">=</code> <code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">(</code><code class="p">)</code> <code class="o">-</code> <code class="n">start</code>

<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="se">\n</code><code class="s2">Process completed in: </code><code class="si">{</code><code class="n">duration</code><code class="si">}</code><code class="s2"> seconds</code><code class="s2">"</code><code class="p">)</code>
<code class="sd">"""
Start of async task
Start of async task
Start of async task
Task resumed after 5 seconds
Task resumed after 5 seconds
Task resumed after 5 seconds

Process completed in: 5.0057971477508545 seconds </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO2-5" id="co_achieving_concurrency_in_ai_workloads_CO2-5"><img alt="5" src="assets/5.png"/></a><code class="sd">
"""</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO2-1" id="callout_achieving_concurrency_in_ai_workloads_CO2-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Implement a <code>task</code> coroutine that cedes control to the event loop on blocking operations.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO2-2" id="callout_achieving_concurrency_in_ai_workloads_CO2-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>The nonblocking five-second sleep signals to the event loop to run another task while waiting.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO2-3" id="callout_achieving_concurrency_in_ai_workloads_CO2-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Use <code>asyncio.create_task</code> to spawn task instances to chain (or gather) and run them concurrently using <code>asyncio.gather</code>.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO2-4" id="callout_achieving_concurrency_in_ai_workloads_CO2-4"><img alt="4" src="assets/4.png"/></a></dt>
<dd><p>Create an event loop to schedule async tasks with via the <code>asyncio.run</code> method.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO2-5" id="callout_achieving_concurrency_in_ai_workloads_CO2-5"><img alt="5" src="assets/5.png"/></a></dt>
<dd><p>Execution time is 1/3 of the synchronous example since the Python process wasn’t blocked this time.</p></dd>
</dl></div>

<p>After running <a data-type="xref" href="#async_execution">Example 5-2</a>, you will notice that the <code>task()</code> function was concurrently called three times.
On the other hand, the code in <a data-type="xref" href="#sync_execution">Example 5-1</a> calls the <code>task()</code> function three times sequentially.
The async function ran inside the <code>asyncio</code>’s event loop, which was responsible for executing the code without waiting.</p>

<p>In any async code, the <code>await</code> keyword flags the I/O blocking operations to Python so that they’re executed in a <em>nonblocking</em> manner (i.e., they can run without blocking the main process).
By being made aware of blocking operations, Python can go and do something else while waiting for blocking operations to finish.</p>

<p><a data-type="xref" href="#await_keyword">Example 5-3</a> shows how to use the <code>async</code> and <code>await</code> keywords to declare and run async functions.</p>
<div data-type="example" id="await_keyword">
<h5><span class="label">Example 5-3. </span>How to use <code>async</code> and <code>await</code> keywords</h5>

<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">asyncio</code>

<code class="k">async</code> <code class="k">def</code> <code class="nf">main</code><code class="p">(</code><code class="p">)</code><code class="p">:</code>
    <code class="nb">print</code><code class="p">(</code><code class="s2">"</code><code class="s2">Before sleeping</code><code class="s2">"</code><code class="p">)</code>
    <code class="k">await</code> <code class="n">asyncio</code><code class="o">.</code><code class="n">sleep</code><code class="p">(</code><code class="mi">3</code><code class="p">)</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO3-1" id="co_achieving_concurrency_in_ai_workloads_CO3-1"><img alt="1" src="assets/1.png"/></a>
    <code class="nb">print</code><code class="p">(</code><code class="s2">"</code><code class="s2">After sleeping for 3 seconds</code><code class="s2">"</code><code class="p">)</code>

<code class="n">asyncio</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">main</code><code class="p">(</code><code class="p">)</code><code class="p">)</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO3-2" id="co_achieving_concurrency_in_ai_workloads_CO3-2"><img alt="2" src="assets/2.png"/></a>

<code class="sd">"""
Before sleeping
After sleeping for 3 seconds </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO3-3" id="co_achieving_concurrency_in_ai_workloads_CO3-3"><img alt="3" src="assets/3.png"/></a><code class="sd">
"""</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO3-1" id="callout_achieving_concurrency_in_ai_workloads_CO3-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Simulate a nonblocking I/O operation by
<code>await</code>ing the <code>asyncio.sleep()</code> so that Python can go and do other things while waiting.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO3-2" id="callout_achieving_concurrency_in_ai_workloads_CO3-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>You need to call <code>main()</code> inside the <code>asyncio.run()</code> to execute it as it’s an async function.
Otherwise, it will not be executed and returns a <em>coroutine</em> object instead.
I will cover coroutines shortly.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO3-3" id="callout_achieving_concurrency_in_ai_workloads_CO3-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>If you run the code, the second statement will be printed 3 seconds after the first one.
In this instance, as there are no other operations to run beyond sleeping, Python runs in idle until the sleep operation is completed.</p></dd>
</dl></div>

<p>In <a data-type="xref" href="#await_keyword">Example 5-3</a>, I used sleeping as a way to simulate I/O blocking operations such as making network requests.</p>
<div data-type="caution"><h6>Caution</h6>
<p>You can only use the <code>await</code> keyword inside a function declared with <code>async def</code>.
Using <code>await</code> outside of an <code>async</code> function will raise a <code>SyntaxError</code> in Python.
Another common pitfall is using blocking code that’s not asynchronous within an <code>async</code> function that will inadvertently prevent Python from doing other tasks while waiting.</p>
</div>

<p>So, you now understand that in async programs, to keep the main process from being blocked, Python switches between functions as soon as it hits a blocking operation.
You now may be wondering:</p>

<ul>
<li>
<p>How does Python leverage <code>asyncio</code> to pause and resume functions?</p>
</li>
<li>
<p>What is the mechanism that Python’s <code>asyncio</code> uses to move from one function to another without forgetting about those that are suspended?</p>
</li>
<li>
<p>How can functions be paused or resumed without losing their state?</p>
</li>
</ul>

<p>To answer the aforementioned questions, let’s dive deeper into the underlying mechanisms within <code>asyncio</code>,
as understanding the answers to these questions will help you significantly to debug async code in your services.</p>

<p><a data-primary="event loop" data-type="indexterm" id="id808"/>At the heart of <code>asyncio</code> lies a first-class object called an <em>event loop</em>, responsible for efficient handling of I/O events, system events, and application context changes.</p>

<p><a data-type="xref" href="#event_loop">Figure 5-4</a> shows how the <code>asyncio</code> event loop undertakes task orchestration in Python.</p>

<figure><div class="figure" id="event_loop">
<img alt="bgai 0504" src="assets/bgai_0504.png"/>
<h6><span class="label">Figure 5-4. </span>Async IO event loop</h6>
</div></figure>

<p>The event loop can be compared to a <code>while True</code> loop that watches for events or messages emitted by <em>coroutine functions</em> within the Python process and dispatches events to switch between functions while waiting for I/O blocking operations to complete.
This orchestration allows other functions to execute asynchronously without interruption.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id809">
<h1>Coroutine Functions</h1>
<p><a data-primary="coroutine functions" data-type="indexterm" id="id810"/>Coroutine functions are specialized functions that yield control back to the caller without losing their state (i.e., they can be paused and resumed at a later time).
The mechanism for yielding back control to the event loop relies on coroutines.</p>

<p>The ability to pause and resume operations without losing state makes coroutines similar to generator functions.
In fact, you could implement coroutines using generators prior to Python 3.5 when there was no native support for coroutine functions.
Similar to generators, simply calling a coroutine function will not execute it.</p>

<p>To run coroutines, you will need to use <code>asyncio.run()</code> or await them via the <code>await</code>
keyword in Python 3.5 or later versions.</p>

<p>Aside from coroutines, the <code>asyncio</code>
package also implements other concurrency primitives such as <em>futures</em>, <em>semaphores</em>, and <em>locks</em>.<a data-startref="ix_ch05-asciidoc9" data-type="indexterm" id="id811"/><a data-startref="ix_ch05-asciidoc8" data-type="indexterm" id="id812"/><a data-startref="ix_ch05-asciidoc7" data-type="indexterm" id="id813"/></p>
</div></aside>
</div></section>








<section data-pdf-bookmark="Async Programming with Model Provider APIs" data-type="sect2"><div class="sect2" id="id79">
<h2>Async Programming with Model Provider APIs</h2>

<p><a data-primary="asynchronous (async) I/O" data-secondary="async programming with model provider APIs" data-type="indexterm" id="ix_ch05-asciidoc10"/><a data-primary="concurrency" data-secondary="optimizing for I/O tasks with asynchronous programming" data-tertiary="async programming with model provider APIs" data-type="indexterm" id="ix_ch05-asciidoc11"/><a data-primary="model providers" data-secondary="async programming with model provider APIs" data-type="indexterm" id="ix_ch05-asciidoc12"/>All three examples I’ve shown you so far are considered to be the “Hello World” examples of async programming.
Now, let’s look at a real-world scenario related to building GenAI services where you need to use a model provider’s API—such as OpenAI, Anthropic, or Mistral—since it may be more expensive to serve LLMs 
<span class="keep-together">yourself.</span></p>

<p>Additionally, if you stress test the generation endpoints you created in <a data-type="xref" href="ch03.html#ch03">Chapter 3</a> by sending multiple requests in a short timeframe, you will notice long waiting times before each request is processed.
This is because you were preloading and hosting the model in the same Python process and CPU core that the server is running on.
When you send the first request, the whole server becomes blocked while the inference workload is complete.
Since during inference the CPU is working as hard as it can, the inference/generation process is a CPU-bound blocking operation.
However, it doesn’t have to be.</p>

<p>When you use a provider’s API, you no longer have CPU-bound AI workloads to worry about since they become I/O-bound for you, and you offload the CPU-bound workloads to the provider.
Therefore, it makes sense to know how to leverage async programming to concurrently interact with the model provider’s API.</p>

<p>The good news is API owners will often release both synchronous and asynchronous <em>clients</em> and
<em>software development kits</em> (SDKs) to reduce the work needed to interact with their endpoints.</p>
<div data-type="caution"><h6>Caution</h6>
<p>If you need to make requests to other external services, fetch some data from databases, or ingest content from files, you will add other I/O blocking tasks to the process.
These blocking tasks can force the server to keep waiting if you don’t leverage asynchronous programming.</p>

<p>However, any synchronous code can be made async using a <a href="https://oreil.ly/hIDNI">process or thread pool executor</a> to avoid running the task within the event loop.
Instead, you run the asynchronous task on a separate process or thread to prevent blocking the event loop.</p>

<p>You can also verify any async support by checking library documentation or source code for mentions of <code>async</code> or <code>await</code> keywords.
Otherwise, you can try testing whether the tool can be used within an async function without raising a <code>TypeError</code> when you use <code>await</code> on it.</p>

<p>If a tool, such as a database library, only has a synchronous implementation, then you can’t implement asynchronicity with that tool.
The solution will be to switch the tool to an asynchronous equivalent so that can you can use them with the
<code>async</code> and <code>await</code> 
<span class="keep-together">keywords.</span></p>
</div>

<p>In <a data-type="xref" href="#openai_clients">Example 5-4</a>, you will interact with OpenAI GPT-3.5 API via both synchronous and asynchronous OpenAI clients to understand the performance difference between the two.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>You will need to install the <code>openai</code> library:</p>

<pre data-code-language="bash" data-type="programlisting">$ pip install openai<code class="w"/></pre>
</div>
<div data-type="example" id="openai_clients">
<h5><span class="label">Example 5-4. </span>Comparing synchronous and asynchronous OpenAI clients</h5>

<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">os</code>
<code class="kn">from</code> <code class="nn">fastapi</code> <code class="kn">import</code> <code class="n">FastAPI</code><code class="p">,</code> <code class="n">Body</code>
<code class="kn">from</code> <code class="nn">openai</code> <code class="kn">import</code> <code class="n">OpenAI</code><code class="p">,</code> <code class="n">AsyncOpenAI</code>

<code class="n">app</code> <code class="o">=</code> <code class="n">FastAPI</code><code class="p">()</code>

<code class="n">sync_client</code> <code class="o">=</code> <code class="n">OpenAI</code><code class="p">(</code><code class="n">api_key</code><code class="o">=</code><code class="n">os</code><code class="o">.</code><code class="n">environ</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"OPENAI_API_KEY"</code><code class="p">))</code>
<code class="n">async_client</code> <code class="o">=</code> <code class="n">AsyncOpenAI</code><code class="p">(</code><code class="n">api_key</code><code class="o">=</code><code class="n">os</code><code class="o">.</code><code class="n">environ</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"OPENAI_API_KEY"</code><code class="p">))</code>

<code class="nd">@app</code><code class="o">.</code><code class="n">post</code><code class="p">(</code><code class="s2">"/sync"</code><code class="p">)</code>
<code class="k">def</code> <code class="nf">sync_generate_text</code><code class="p">(</code><code class="n">prompt</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="n">Body</code><code class="p">(</code><code class="o">...</code><code class="p">)):</code>
    <code class="n">completion</code> <code class="o">=</code> <code class="n">sync_client</code><code class="o">.</code><code class="n">chat</code><code class="o">.</code><code class="n">completions</code><code class="o">.</code><code class="n">create</code><code class="p">(</code>
        <code class="n">messages</code><code class="o">=</code><code class="p">[</code>
            <code class="p">{</code>
                <code class="s2">"role"</code><code class="p">:</code> <code class="s2">"user"</code><code class="p">,</code>
                <code class="s2">"content"</code><code class="p">:</code> <code class="n">prompt</code><code class="p">,</code>
            <code class="p">}</code>
        <code class="p">],</code>
        <code class="n">model</code><code class="o">=</code><code class="s2">"gpt-3.5-turbo"</code><code class="p">,</code>
    <code class="p">)</code>
    <code class="k">return</code> <code class="n">completion</code><code class="o">.</code><code class="n">choices</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">message</code><code class="o">.</code><code class="n">content</code>

<code class="nd">@app</code><code class="o">.</code><code class="n">post</code><code class="p">(</code><code class="s2">"/async"</code><code class="p">)</code>
<code class="k">async</code> <code class="k">def</code> <code class="nf">async_generate_text</code><code class="p">(</code><code class="n">prompt</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="n">Body</code><code class="p">(</code><code class="o">...</code><code class="p">)):</code>
    <code class="n">completion</code> <code class="o">=</code> <code class="k">await</code> <code class="n">async_client</code><code class="o">.</code><code class="n">chat</code><code class="o">.</code><code class="n">completions</code><code class="o">.</code><code class="n">create</code><code class="p">(</code>
        <code class="n">messages</code><code class="o">=</code><code class="p">[</code>
            <code class="p">{</code>
                <code class="s2">"role"</code><code class="p">:</code> <code class="s2">"user"</code><code class="p">,</code>
                <code class="s2">"content"</code><code class="p">:</code> <code class="n">prompt</code><code class="p">,</code>
            <code class="p">}</code>
        <code class="p">],</code>
        <code class="n">model</code><code class="o">=</code><code class="s2">"gpt-3.5-turbo"</code><code class="p">,</code>
    <code class="p">)</code>
    <code class="k">return</code> <code class="n">completion</code><code class="o">.</code><code class="n">choices</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">message</code><code class="o">.</code><code class="n">content</code></pre></div>

<p>The difference between the sync and async clients is that with the async version, 
<span class="keep-together">FastAPI</span> can start processing user inputs in parallel without waiting for a response from the OpenAI API for the previous user input.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id814">
<h1>Managing Rate Limits</h1>
<p><a data-primary="rate limiting" data-secondary="managing rate limits" data-type="indexterm" id="id815"/>Be mindful that concurrent requests to external APIs will need to be throttled to stay within rate limits.</p>

<p><a data-primary="exponential backoff" data-type="indexterm" id="id816"/>A sensible strategy is to implement an <em>exponential backoff</em> where your service delays hitting external APIs based on an exponential delay curve.
The more you receive rate-limiting errors from external APIs, the longer your service will wait before resuming requests.</p>

<p>You can either apply to increase your wait limits or ask users to wait before they can try again.</p>

<p>Third-party Python packages such as <code>stamina</code> can help implement a rate-limiting strategy with external APIs.<sup><a data-type="noteref" href="ch05.html#id817" id="id817-marker">3</a></sup></p>
</div></aside>

<p>By leveraging asynchronous code, you can get a massive boost in throughput and scale to a larger volume of concurrent requests.
However, you must be careful when writing asynchronous (async) code.</p>

<p>Here are some common pitfalls and problems you might face with async code:</p>

<ul>
<li>
<p>Understanding and debugging errors can be more complex due to the nonlinear execution flow of concurrent tasks.</p>
</li>
<li>
<p>Some libraries, like <code>aiohttp</code>, require nested async context managers for proper implementation.
This can get confusing pretty fast.</p>
</li>
<li>
<p>Mixing asynchronous and synchronous code can negate any performance benefits, such as if you forget to mark functions with the <code>async</code> and <code>await</code> keywords.</p>
</li>
<li>
<p>Not using async-compatible tools and libraries can also cancel out any performance benefits; for example, using the <code>requests</code> package instead of <code>aiohttp</code> for making async API calls.</p>
</li>
<li>
<p>Forgetting to await coroutines within any async function or awaiting non-coroutines can lead to unexpected behavior.
All <code>async</code> keywords must be followed by an <code>await</code>.</p>
</li>
<li>
<p>Improperly managing resources (e.g., open API/database connections or file buffers) can cause memory leaks that freeze your computer.
You can also leak memory if you don’t limit the number of concurrent operations in async code.</p>
</li>
<li>
<p>You might also run into concurrency and race condition issues where the thread-safety principle is violated, causing deadlocks on resources leading to data 
<span class="keep-together">corruption.</span></p>
</li>
</ul>

<p>This list is not exhaustive, and as you can see, there are several pitfalls to using asynchronous programming.
Therefore, I recommend starting with writing synchronous programs first, to understand the basic flow and logic of your code, before dealing with the complexities of migrating to an async implementation.<a data-startref="ix_ch05-asciidoc12" data-type="indexterm" id="id818"/><a data-startref="ix_ch05-asciidoc11" data-type="indexterm" id="id819"/><a data-startref="ix_ch05-asciidoc10" data-type="indexterm" id="id820"/></p>
</div></section>








<section data-pdf-bookmark="Event Loop and Thread Pool in FastAPI" data-type="sect2"><div class="sect2" id="id80">
<h2>Event Loop and Thread Pool in FastAPI</h2>

<p><a data-primary="concurrency" data-secondary="optimizing for I/O tasks with asynchronous programming" data-tertiary="event loop and thread pool in FastAPI" data-type="indexterm" id="ix_ch05-asciidoc13"/><a data-primary="event loop" data-type="indexterm" id="ix_ch05-asciidoc14"/><a data-primary="FastAPI (basics)" data-secondary="event loop and thread pool" data-type="indexterm" id="ix_ch05-asciidoc15"/><a data-primary="thread pool" data-secondary="in FastAPI" data-type="indexterm" id="ix_ch05-asciidoc16"/>Under the hood, FastAPI can handle both async and sync blocking operations.
It does this by running sync handlers in its <em>thread pool</em> so that blocking operations don’t stop the <em>event loop</em> from executing tasks.</p>

<p><a data-primary="Asynchronous Server Gateway Interface (ASGI)" data-secondary="thread pool handling" data-type="indexterm" id="id821"/>As I mentioned in <a data-type="xref" href="ch02.html#ch02">Chapter 2</a>, FastAPI runs on the ASGI web framework via Starlette.
If it didn’t, the server would effectively run synchronously, so you would have to wait for each process to finish before it could serve the next.
However, using ASGI, the FastAPI server supports concurrency via both multithreading (via a thread pool) and asynchronous programming (via an event loop) to serve multiple requests in parallel, while keeping the main server process from being blocked.</p>

<p>FastAPI sets up the thread pool by instantiating a collection of threads at application startup to reduce the runtime burden of thread creation.<sup><a data-type="noteref" href="ch05.html#id822" id="id822-marker">4</a></sup>
It then delegates background tasks and synchronous workloads to the thread pool to prevent the event loop from being blocked by any blocking operations inside the synchronous handlers.
The event loop is also referred to as the main FastAPI server thread that is responsible for orchestrating the processing of requests.</p>

<p>As I mentioned, the event loop is the core component of every application built on top of <code>asyncio</code>, including FastAPI that implements concurrency.
Event loops run asynchronous tasks and callbacks, including performing network I/O operations, and running subprocesses.
In FastAPI, the event loop is also responsible for orchestrating the asynchronous processing of requests.</p>

<p>If possible, you should run handlers on the event loop (via asynchronous programming) as it can be even more efficient than running them on the thread pool (via multithreading).
This is because each thread in the thread pool has to acquire the GIL before it can execute any code bytes, and that requires some computational effort.</p>

<p>Imagine if multiple concurrent users were using both the synchronous and asynchronous OpenAI GPT-3.5 handlers (endpoints) of your FastAPI service, as shown in <a data-type="xref" href="#openai_clients">Example 5-4</a>.
FastAPI will run the async handler requests on the event loop since that handler uses a nonblocking async OpenAI client.
On the other hand, FastAPI has to delegate the synchronous handler requests to the thread pool to protect the event loop from blocking.
Since delegating requests (to threads) and switching between threads in a thread pool is more work, the synchronous requests will finish later than their async counterparts.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Remember that all of this work—processing both synchronous and async handler requests—is running on a single CPU core within the same FastAPI Python process.</p>

<p>This is so that the CPU idle time is minimized while waiting for responses from OpenAI API.</p>
</div>

<p>The differences in performance are shown in <a data-type="xref" href="#multithreading_vs_async">Figure 5-5</a>.</p>

<figure><div class="figure" id="multithreading_vs_async">
<img alt="bgai 0505" src="assets/bgai_0505.png"/>
<h6><span class="label">Figure 5-5. </span>How multithreading and Async IO handle I/O blocking operations</h6>
</div></figure>

<p><a data-type="xref" href="#multithreading_vs_async">Figure 5-5</a> shows that with I/O-bound workloads, async implementations are faster and should be your preferred method if you need concurrency.
However, FastAPI does still do a solid job of serving multiple concurrent requests even if it has to work with a synchronous OpenAI client.
It simply sends the synchronous API calls within threads of the thread pool to implement some form of concurrency for you.
That’s why the FastAPI official documentation tells you to not worry too much about declaring your handler functions as <code>async def</code> or <code>def</code>.</p>

<p>However, keep in mind that when you declare handlers with <code>async def</code>, FastAPI trusts you with performing only nonblocking operations.
When you break that trust and execute blocking operations inside <code>async</code> routes, the event loop will be blocked and can no longer continue with executing tasks until the blocking<a data-startref="ix_ch05-asciidoc16" data-type="indexterm" id="id823"/><a data-startref="ix_ch05-asciidoc15" data-type="indexterm" id="id824"/><a data-startref="ix_ch05-asciidoc14" data-type="indexterm" id="id825"/><a data-startref="ix_ch05-asciidoc13" data-type="indexterm" id="id826"/> operation is 
<span class="keep-together">finished.</span></p>
</div></section>








<section data-pdf-bookmark="Blocking the Main Server" data-type="sect2"><div class="sect2" id="id81">
<h2>Blocking the Main Server</h2>

<p><a data-primary="concurrency" data-secondary="optimizing for I/O tasks with asynchronous programming" data-tertiary="blocking the main server" data-type="indexterm" id="ix_ch05-asciidoc17"/>If you’re using the <code>async</code> keyword when defining your functions, make sure you’re also using the <code>await</code> keyword somewhere inside your function and that none of the package dependencies you use inside the function are synchronous.</p>

<p>Avoid declaring route handler functions as <code>async</code> if their implementation is synchronous.
Otherwise, requests to the affected route handlers will block the main server from processing other requests while the server is waiting for the blocking operation to complete.
It won’t matter if the blocking operation is I/O-bound or compute-bound.
Therefore, any calls to databases or AI models can still cause the blockage if you’re not careful.</p>

<p>This is an easy mistake to make.
For instance, you may use a synchronous dependency inside handlers you’ve declared as async, as shown in <a data-type="xref" href="#blocking_main_thread">Example 5-5</a>.</p>
<div data-type="example" id="blocking_main_thread">
<h5><span class="label">Example 5-5. </span>Incorrect implementation of asynchronous handlers in FastAPI</h5>

<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">os</code>
<code class="kn">from</code> <code class="nn">fastapi</code> <code class="kn">import</code> <code class="n">FastAPI</code>
<code class="kn">from</code> <code class="nn">openai</code> <code class="kn">import</code> <code class="n">AsyncOpenAI</code><code class="p">,</code> <code class="n">OpenAI</code>

<code class="n">app</code> <code class="o">=</code> <code class="n">FastAPI</code><code class="p">(</code><code class="p">)</code>

<code class="nd">@app</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"</code><code class="s2">/block</code><code class="s2">"</code><code class="p">)</code>
<code class="k">async</code> <code class="k">def</code> <code class="nf">block_server_controller</code><code class="p">(</code><code class="p">)</code><code class="p">:</code>
    <code class="n">completion</code> <code class="o">=</code> <code class="n">sync_client</code><code class="o">.</code><code class="n">chat</code><code class="o">.</code><code class="n">completions</code><code class="o">.</code><code class="n">create</code><code class="p">(</code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code class="p">)</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO4-1" id="co_achieving_concurrency_in_ai_workloads_CO4-1"><img alt="1" src="assets/1.png"/></a>
    <code class="k">return</code> <code class="n">completion</code><code class="o">.</code><code class="n">choices</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">message</code><code class="o">.</code><code class="n">content</code>

<code class="nd">@app</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"</code><code class="s2">/slow</code><code class="s2">"</code><code class="p">)</code>
<code class="k">def</code> <code class="nf">slow_text_generator</code><code class="p">(</code><code class="p">)</code><code class="p">:</code>
    <code class="n">completion</code> <code class="o">=</code> <code class="n">sync_client</code><code class="o">.</code><code class="n">chat</code><code class="o">.</code><code class="n">completions</code><code class="o">.</code><code class="n">create</code><code class="p">(</code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code class="p">)</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO4-2" id="co_achieving_concurrency_in_ai_workloads_CO4-2"><img alt="2" src="assets/2.png"/></a>
    <code class="k">return</code> <code class="n">completion</code><code class="o">.</code><code class="n">choices</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">message</code><code class="o">.</code><code class="n">content</code>

<code class="nd">@app</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"</code><code class="s2">/fast</code><code class="s2">"</code><code class="p">)</code>
<code class="k">async</code> <code class="k">def</code> <code class="nf">fast_text_generator</code><code class="p">(</code><code class="p">)</code><code class="p">:</code>
    <code class="n">completion</code> <code class="o">=</code> <code class="k">await</code> <code class="n">async_client</code><code class="o">.</code><code class="n">chat</code><code class="o">.</code><code class="n">completions</code><code class="o">.</code><code class="n">create</code><code class="p">(</code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code class="p">)</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO4-3" id="co_achieving_concurrency_in_ai_workloads_CO4-3"><img alt="3" src="assets/3.png"/></a>
    <code class="k">return</code> <code class="n">completion</code><code class="o">.</code><code class="n">choices</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">message</code><code class="o">.</code><code class="n">content</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO4-1" id="callout_achieving_concurrency_in_ai_workloads_CO4-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>I/O blocking operation to get ChatGPT API response.
Because the route handler is marked async, FastAPI trusts us to not run blocking operations, but as we are, the request will block the event loop (main server thread).
Other requests are now blocked until the current request is processed.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO4-2" id="callout_achieving_concurrency_in_ai_workloads_CO4-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>A simple synchronous route handler with blocking operation that doesn’t leverage asynchronous features.
Sync requests are handed off to the thread pool to run in the background so that the main server is not blocked.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO4-3" id="callout_achieving_concurrency_in_ai_workloads_CO4-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>An asynchronous route that is nonblocking.</p></dd>
</dl></div>

<p>The request won’t block the main thread and doesn’t need to be handed off to the thread pool.
As a result, the FastAPI event loop can process the request much faster using the async OpenAI client.</p>

<p>You now should feel more comfortable implementing new features in your FastAPI service that require performing I/O-bound tasks.</p>

<p>To help solidify your understanding of the I/O concurrency concepts, in the next few sections you will build several new features using concurrency into your FastAPI service. These features include:</p>
<dl>
<dt>Talk to the web</dt>
<dd>
<p>Build and integrate a web scraper module that allows you to ask questions to your self-hosted LLM about the content of a website by providing an HTTP URL.</p>
</dd>
<dt>Talk to documents</dt>
<dd>
<p>Build and integrate a RAG module to process documents into a vector database.
A vector database stores data in a way that supports efficient similarity searches.
You can then use semantic search, which understands the meaning of queries, to interact with uploaded documents using your LLM.</p>
</dd>
</dl>

<p>Both projects will give you a hands-on experience interacting asynchronously with external systems such as websites, a database, and a filesystem.<a data-startref="ix_ch05-asciidoc17" data-type="indexterm" id="id827"/></p>
</div></section>








<section class="less_space pagebreak-before" data-pdf-bookmark="Project: Talk to the Web (Web Scraper)" data-type="sect2"><div class="sect2" id="id82">
<h2>Project: Talk to the Web (Web Scraper)</h2>

<p><a data-primary="concurrency" data-secondary="optimizing for I/O tasks with asynchronous programming" data-tertiary="project: web scraper" data-type="indexterm" id="ix_ch05-asciidoc18"/><a data-primary="web scraper (async programming project)" data-type="indexterm" id="ix_ch05-asciidoc19"/>Companies often host a series of internal web pages for manuals, processes, and other documentation as HTML pages.
For longer pages, your users may want to provide URLs when asking questions and expect your LLM to fetch and read the content.
This is where having a built-in web scraper can come in handy.</p>

<p>There are many ways to build a web scraper for your self-hosted LLM.
Depending on your use case, you can use a combination of the following methods:</p>

<ul>
<li>
<p>Fetch web pages as HTML and feed the raw HTML (or inner text content) to your LLM to parse the content into your desired format.</p>
</li>
<li>
<p>Use <em>web scraping frameworks</em> such as <code>BeautifulSoup</code> and <code>ScraPy</code> to parse the content of web pages after fetching.</p>
</li>
<li>
<p>Use <em>headless web browsers</em> such as
Selenium and Microsoft Playwright to dynamically navigate nodes in pages and parse content.
Headless browsers are great for navigation single-page applications (SPAs).</p>
</li>
</ul>
<div data-type="caution"><h6>Caution</h6>
<p>You or your users should avoid LLM-powered web scraping tools for illegal purposes.
Make sure you have permission before extracting content from URLs:</p>

<ul>
<li>
<p>Review each website’s terms of use, especially if there is a mention of web scraping.</p>
</li>
<li>
<p>Use APIs when possible.</p>
</li>
<li>
<p>Ask website owners for permission directly if unsure.</p>
</li>
</ul>
</div>

<p>For this mini-project, we will only fetch and feed raw inner text of HTML pages to our LLM since implementing a production-ready scraper can become a book of its own.</p>

<p>The process for building a simple asynchronous scraper is as follows:</p>
<ol>
<li>
<p>Develop a function to match URL patterns using regex on user prompts to the LLM.</p>
</li>
<li>
<p>If found, loop over the list of provided URLs and asynchronously fetch the pages.
We will use an asynchronous HTTP library called <code>aiohttp</code> instead of the <code>requests</code>
since <code>requests</code> can only make synchronous network requests.</p>
</li>
<li>
<p>Develop a parsing function to extract the textual content from fetched HTML.</p>
</li>
<li>
<p>Feed the parsed page content to the LLM alongside the original user prompt.</p>
</li>

</ol>

<p><a data-type="xref" href="#web_scraper">Example 5-6</a> demonstrates how you can implement the aforementioned steps.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>You will need to install a few additional dependencies to run this example:</p>

<pre data-code-language="bash" data-type="programlisting">$ pip install beautifulsoup lxml aiohttp<code class="w"/></pre>
</div>
<div data-type="example" id="web_scraper">
<h5><span class="label">Example 5-6. </span>Building an asynchronous web scraper</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># scraper.py</code>

<code class="kn">import</code> <code class="nn">asyncio</code>
<code class="kn">import</code> <code class="nn">re</code>

<code class="kn">import</code> <code class="nn">aiohttp</code>
<code class="kn">from</code> <code class="nn">bs4</code> <code class="kn">import</code> <code class="n">BeautifulSoup</code>
<code class="kn">from</code> <code class="nn">loguru</code> <code class="kn">import</code> <code class="n">logger</code>

<code class="k">def</code> <code class="nf">extract_urls</code><code class="p">(</code><code class="n">text</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="nb">list</code><code class="p">[</code><code class="nb">str</code><code class="p">]</code><code class="p">:</code>
    <code class="n">url_pattern</code> <code class="o">=</code> <code class="sa">r</code><code class="s2">"</code><code class="s2">(?P&lt;url&gt;https?:</code><code class="s2">\</code><code class="s2">/</code><code class="s2">\</code><code class="s2">/[^</code><code class="s2">\</code><code class="s2">s]+)</code><code class="s2">"</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO5-1" id="co_achieving_concurrency_in_ai_workloads_CO5-1"><img alt="1" src="assets/1.png"/></a>
    <code class="n">urls</code> <code class="o">=</code> <code class="n">re</code><code class="o">.</code><code class="n">findall</code><code class="p">(</code><code class="n">url_pattern</code><code class="p">,</code> <code class="n">text</code><code class="p">)</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO5-2" id="co_achieving_concurrency_in_ai_workloads_CO5-2"><img alt="2" src="assets/2.png"/></a>
    <code class="k">return</code> <code class="n">urls</code>


<code class="k">def</code> <code class="nf">parse_inner_text</code><code class="p">(</code><code class="n">html_string</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="nb">str</code><code class="p">:</code>
    <code class="n">soup</code> <code class="o">=</code> <code class="n">BeautifulSoup</code><code class="p">(</code><code class="n">html_string</code><code class="p">,</code> <code class="s2">"</code><code class="s2">lxml</code><code class="s2">"</code><code class="p">)</code>
    <code class="k">if</code> <code class="n">content</code> <code class="o">:=</code> <code class="n">soup</code><code class="o">.</code><code class="n">find</code><code class="p">(</code><code class="s2">"</code><code class="s2">div</code><code class="s2">"</code><code class="p">,</code> <code class="nb">id</code><code class="o">=</code><code class="s2">"</code><code class="s2">bodyContent</code><code class="s2">"</code><code class="p">)</code><code class="p">:</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO5-3" id="co_achieving_concurrency_in_ai_workloads_CO5-3"><img alt="3" src="assets/3.png"/></a>
        <code class="k">return</code> <code class="n">content</code><code class="o">.</code><code class="n">get_text</code><code class="p">(</code><code class="p">)</code>
    <code class="n">logger</code><code class="o">.</code><code class="n">warning</code><code class="p">(</code><code class="s2">"</code><code class="s2">Could not parse the HTML content</code><code class="s2">"</code><code class="p">)</code>
    <code class="k">return</code> <code class="s2">"</code><code class="s2">"</code>


<code class="k">async</code> <code class="k">def</code> <code class="nf">fetch</code><code class="p">(</code><code class="n">session</code><code class="p">:</code> <code class="n">aiohttp</code><code class="o">.</code><code class="n">ClientSession</code><code class="p">,</code> <code class="n">url</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="nb">str</code><code class="p">:</code>
    <code class="k">async</code> <code class="k">with</code> <code class="n">session</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">url</code><code class="p">)</code> <code class="k">as</code> <code class="n">response</code><code class="p">:</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO5-4" id="co_achieving_concurrency_in_ai_workloads_CO5-4"><img alt="4" src="assets/4.png"/></a>
        <code class="n">html_string</code> <code class="o">=</code> <code class="k">await</code> <code class="n">response</code><code class="o">.</code><code class="n">text</code><code class="p">(</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">parse_inner_text</code><code class="p">(</code><code class="n">html_string</code><code class="p">)</code>


<code class="k">async</code> <code class="k">def</code> <code class="nf">fetch_all</code><code class="p">(</code><code class="n">urls</code><code class="p">:</code> <code class="nb">list</code><code class="p">[</code><code class="nb">str</code><code class="p">]</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="nb">str</code><code class="p">:</code>
    <code class="k">async</code> <code class="k">with</code> <code class="n">aiohttp</code><code class="o">.</code><code class="n">ClientSession</code><code class="p">(</code><code class="p">)</code> <code class="k">as</code> <code class="n">session</code><code class="p">:</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO5-5" id="co_achieving_concurrency_in_ai_workloads_CO5-5"><img alt="5" src="assets/5.png"/></a>
        <code class="n">results</code> <code class="o">=</code> <code class="k">await</code> <code class="n">asyncio</code><code class="o">.</code><code class="n">gather</code><code class="p">(</code>
            <code class="o">*</code><code class="p">[</code><code class="n">fetch</code><code class="p">(</code><code class="n">session</code><code class="p">,</code> <code class="n">url</code><code class="p">)</code> <code class="k">for</code> <code class="n">url</code> <code class="ow">in</code> <code class="n">urls</code><code class="p">]</code><code class="p">,</code> <code class="n">return_exceptions</code><code class="o">=</code><code class="kc">True</code>
        <code class="p">)</code>
    <code class="n">success_results</code> <code class="o">=</code> <code class="p">[</code><code class="n">result</code> <code class="k">for</code> <code class="n">result</code> <code class="ow">in</code> <code class="n">results</code> <code class="k">if</code> <code class="nb">isinstance</code><code class="p">(</code><code class="n">result</code><code class="p">,</code> <code class="nb">str</code><code class="p">)</code><code class="p">]</code>
    <code class="k">if</code> <code class="nb">len</code><code class="p">(</code><code class="n">results</code><code class="p">)</code> <code class="o">!=</code> <code class="nb">len</code><code class="p">(</code><code class="n">success_results</code><code class="p">)</code><code class="p">:</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO5-6" id="co_achieving_concurrency_in_ai_workloads_CO5-6"><img alt="6" src="assets/6.png"/></a>
        <code class="n">logger</code><code class="o">.</code><code class="n">warning</code><code class="p">(</code><code class="s2">"</code><code class="s2">Some URLs could not be fetched</code><code class="s2">"</code><code class="p">)</code>
    <code class="k">return</code> <code class="s2">"</code> <code class="s2">"</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">success_results</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO5-1" id="callout_achieving_concurrency_in_ai_workloads_CO5-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>A simple regex pattern that captures the URLs into a named group called <code>url</code>
and matches both <code>http</code> and <code>https</code> protocols.
For simplicity, this pattern matches more loosely defined URLs and doesn’t validate the structure of a domain name or path, nor does it account for query strings or anchors in a URL.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO5-2" id="callout_achieving_concurrency_in_ai_workloads_CO5-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Find all nonoverlapping matches of the regex pattern in the text.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO5-3" id="callout_achieving_concurrency_in_ai_workloads_CO5-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Use the <code>bs4</code> Beautiful Soup package to parse the HTML string.
In Wikipedia pages, the article content is nested within a <code>div</code> container with the <code>id="bodyContent"</code>, so the parsing logic assumes only Wikipedia URLs will be passed in.
You can change this logic for other URLs or just use <code>soup.getText()</code>
to grab any text content nested within the HTML.
However, bear in mind that there will be lots of noise in the parsed content if you parse the raw HTML like that, which can confuse the LLM.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO5-4" id="callout_achieving_concurrency_in_ai_workloads_CO5-4"><img alt="4" src="assets/4.png"/></a></dt>
<dd><p>Given an <code>aiohttp</code> session and a URL, perform an asynchronous <code>get</code> request.
Create a <code>response</code> async context manager and <code>await</code> the response within this context manager.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO5-5" id="callout_achieving_concurrency_in_ai_workloads_CO5-5"><img alt="5" src="assets/5.png"/></a></dt>
<dd><p>Given a list of URLs, create a client session async context manager to asynchronously perform multiple fetch calls.
Since <code>fetch()</code> is a coroutine function (i.e., it uses the <code>await</code> keyword),
<code>fetch_all()</code> will need to run multiple <code>fetch()</code> coroutines inside the <code>asyncio.gather()</code> to be scheduled for asynchronous execution on the event loop.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO5-6" id="callout_achieving_concurrency_in_ai_workloads_CO5-6"><img alt="6" src="assets/6.png"/></a></dt>
<dd><p>Check that all URLs have been fetched successfully and, if not, raise a warning.</p></dd>
</dl></div>

<p>You now have the utility scraper functions you need to implement the web scraping feature in your <code>/generate/text</code> endpoint.</p>

<p>Next, upgrade the text-to-text handler to use the scraper functions via a dependency in an asynchronous manner, as shown in <a data-type="xref" href="#web_scraper_fastapi">Example 5-7</a>.</p>
<div data-type="example" id="web_scraper_fastapi">
<h5><span class="label">Example 5-7. </span>Injecting web scraper functionality as a dependency into the FastAPI LLM handler</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># dependencies.py</code>

<code class="kn">from</code> <code class="nn">fastapi</code> <code class="kn">import</code> <code class="n">Body</code>
<code class="kn">from</code> <code class="nn">loguru</code> <code class="kn">import</code> <code class="n">logger</code>

<code class="kn">from</code> <code class="nn">schemas</code> <code class="kn">import</code> <code class="n">TextModelRequest</code>
<code class="kn">from</code> <code class="nn">scraper</code> <code class="kn">import</code> <code class="n">extract_urls</code><code class="p">,</code> <code class="n">fetch_all</code>

<code class="k">async</code> <code class="k">def</code> <code class="nf">get_urls_content</code><code class="p">(</code><code class="n">body</code><code class="p">:</code> <code class="n">TextModelRequest</code> <code class="o">=</code> <code class="n">Body</code><code class="p">(</code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code class="p">)</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="nb">str</code><code class="p">:</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO6-1" id="co_achieving_concurrency_in_ai_workloads_CO6-1"><img alt="1" src="assets/1.png"/></a>
    <code class="n">urls</code> <code class="o">=</code> <code class="n">extract_urls</code><code class="p">(</code><code class="n">body</code><code class="o">.</code><code class="n">prompt</code><code class="p">)</code>
    <code class="k">if</code> <code class="n">urls</code><code class="p">:</code>
        <code class="k">try</code><code class="p">:</code>
            <code class="n">urls_content</code> <code class="o">=</code> <code class="k">await</code> <code class="n">fetch_all</code><code class="p">(</code><code class="n">urls</code><code class="p">)</code>
            <code class="k">return</code> <code class="n">urls_content</code>
        <code class="k">except</code> <code class="ne">Exception</code> <code class="k">as</code> <code class="n">e</code><code class="p">:</code>
            <code class="n">logger</code><code class="o">.</code><code class="n">warning</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="s2">Failed to fetch one or several URls - Error: </code><code class="si">{</code><code class="n">e</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>
    <code class="k">return</code> <code class="s2">"</code><code class="s2">"</code>

<code class="c1"># main.py</code>

<code class="kn">from</code> <code class="nn">fastapi</code> <code class="kn">import</code> <code class="n">Body</code><code class="p">,</code> <code class="n">Depends</code><code class="p">,</code> <code class="n">Request</code>
<code class="kn">from</code> <code class="nn">dependencies</code> <code class="kn">import</code> <code class="n">construct_prompt</code>
<code class="kn">from</code> <code class="nn">schemas</code> <code class="kn">import</code> <code class="n">TextModelResponse</code>

<code class="nd">@app</code><code class="o">.</code><code class="n">post</code><code class="p">(</code><code class="s2">"</code><code class="s2">/generate/text</code><code class="s2">"</code><code class="p">,</code> <code class="n">response_model_exclude_defaults</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO6-2" id="co_achieving_concurrency_in_ai_workloads_CO6-2"><img alt="2" src="assets/2.png"/></a>
<code class="k">async</code> <code class="k">def</code> <code class="nf">serve_text_to_text_controller</code><code class="p">(</code>
    <code class="n">request</code><code class="p">:</code> <code class="n">Request</code><code class="p">,</code>
    <code class="n">body</code><code class="p">:</code> <code class="n">TextModelRequest</code> <code class="o">=</code> <code class="n">Body</code><code class="p">(</code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code class="p">)</code><code class="p">,</code>
    <code class="n">urls_content</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="n">Depends</code><code class="p">(</code><code class="n">get_urls_content</code><code class="p">)</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO6-3" id="co_achieving_concurrency_in_ai_workloads_CO6-3"><img alt="3" src="assets/3.png"/></a>
<code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="n">TextModelResponse</code><code class="p">:</code>
    <code class="o">.</code><code class="o">.</code><code class="o">.</code> <code class="c1"># rest of controller logic</code>
    <code class="n">prompt</code> <code class="o">=</code> <code class="n">body</code><code class="o">.</code><code class="n">prompt</code> <code class="o">+</code> <code class="s2">"</code> <code class="s2">"</code> <code class="o">+</code> <code class="n">urls_content</code>
    <code class="n">output</code> <code class="o">=</code> <code class="n">generate_text</code><code class="p">(</code><code class="n">models</code><code class="p">[</code><code class="s2">"</code><code class="s2">text</code><code class="s2">"</code><code class="p">]</code><code class="p">,</code> <code class="n">prompt</code><code class="p">,</code> <code class="n">body</code><code class="o">.</code><code class="n">temperature</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">TextModelResponse</code><code class="p">(</code><code class="n">content</code><code class="o">=</code><code class="n">output</code><code class="p">,</code> <code class="n">ip</code><code class="o">=</code><code class="n">request</code><code class="o">.</code><code class="n">client</code><code class="o">.</code><code class="n">host</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO6-1" id="callout_achieving_concurrency_in_ai_workloads_CO6-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Implement a <code>get_urls_content</code> FastAPI dependency that gets a user prompt from the request body and finds all URLs.
It then returns the content of all URLs as a long string.
The dependency has exception handling built in to handle any I/O errors by returning an empty string and logging a warning on the server.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO6-2" id="callout_achieving_concurrency_in_ai_workloads_CO6-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>When using <code>aiohttp</code> inside FastAPI, you don’t need to manage the event loop yourself because FastAPI, as an asynchronous framework, handles the event loop.
You can define your endpoint as an async function and use <code>aiohttp</code> to make asynchronous HTTP requests within the handler or via a dependency like in this example.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO6-3" id="callout_achieving_concurrency_in_ai_workloads_CO6-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Inject the results of the <code>get_urls_content</code> dependency call to the handler via the FastAPI’s <code>Depends</code> class.
Using a FastAPI dependency here kept the controller logic small, clean, and readable.</p></dd>
</dl></div>

<p>Now, run the Streamlit client in the browser and try your shiny new feature.
<a data-type="xref" href="#llm_summary">Figure 5-6</a> shows my experiment.</p>

<figure><div class="figure" id="llm_summary">
<img alt="bgai 0506" src="assets/bgai_0506.png"/>
<h6><span class="label">Figure 5-6. </span>Asking the self-hosted TinyLlama model to summarize a Wikipedia article</h6>
</div></figure>

<p>Congratulations!
You’ve learned how to build a simple nonblocking web scraper to work with your own LLM.
In this mini-project, you leveraged <code>re</code> package to match URL patterns in the user prompt and then used the <code>aiohttp</code> library to asynchronously fetch multiple pages concurrently.
You then used the <code>BeautifulSoup</code> package to parse the content of Wikipedia articles by grabbing the text content of the <code>div</code> container with the ID of <code>bodyContent</code> within the fetched HTML string.
For other websites or internal company web pages, you can always alter the parsing logic for appropriate parsing.
Finally, you wrapped the whole scraping logic inside a FastAPI dependency with exception handling built-in to make use of dependency injection while upgrading the text model-serving handler.</p>

<p>Bear in mind that your scraper can’t handle complex pages with dynamic layouts that are server-rendered.
In such cases, you can add a headless browser to your web scraper for navigating dynamic pages.</p>

<p>Additionally, fetching content of external sites will be challenging since most sites may implement anti-scraping protections such as <em>IP blocking</em> or <em>CAPTCHAs</em> as common deterrents.
Maintaining <em>data quality</em> and <em>consistency</em>
with external websites is also an ongoing challenge as you may need to update your scraping scripts regularly to ensure accurate and reliable extraction.</p>

<p>You should now feel more comfortable building GenAI-powered services that need to interact with the web via making asynchronous network requests.</p>

<p>Next, we will look at other I/O asynchronous interactions such as those with databases and the filesystem by building a <em>talk to your documents</em> feature.</p>

<p>This functionality allows users to upload documents through the Streamlit interface to your service.
The content of the uploaded documents is then extracted, processed, and saved in a database.
Subsequently, during user interactions with the LLM, an asynchronous retrieval system retrieves semantically relevant content from the database, which is then used to augment the context provided to the LLM.</p>

<p>This process is referred to as RAG,
which we will build as a module for your LLM next.<a data-startref="ix_ch05-asciidoc19" data-type="indexterm" id="id828"/><a data-startref="ix_ch05-asciidoc18" data-type="indexterm" id="id829"/></p>
</div></section>








<section data-pdf-bookmark="Project: Talk to Documents (RAG)" data-type="sect2"><div class="sect2" id="id83">
<h2>Project: Talk to Documents (RAG)</h2>

<p><a data-primary="concurrency" data-secondary="optimizing for I/O tasks with asynchronous programming" data-tertiary="project: RAG module" data-type="indexterm" id="ix_ch05-asciidoc20"/><a data-primary="RAG (retrieval augmented generation) module" data-secondary="async programming project" data-type="indexterm" id="ix_ch05-asciidoc21"/>In this project, we will build a RAG module into your GenAI service to give you a hands-on experience interacting asynchronously with external systems such as a database and a filesystem.</p>

<p>You might be curious about the purpose of a RAG module and its necessity. RAG is simply a technique for augmenting the context of LLM prompts with custom data sources for knowledge-intensive tasks.<sup><a data-type="noteref" href="ch05.html#id830" id="id830-marker">5</a></sup>
It is an effective technique to ground LLM responses in facts contained within data without the need for complex and expensive LLM fine-tuning.</p>

<p>Organizations are eager to implement RAG with their own LLMs since it allows their employees to engage with their massive internal knowledge bases via the LLM.
With RAG, businesses expect that the internal knowledge bases, systems, and procedures will be made accessible and readily available to anyone who needs them to answer questions, just when they need it.
This accessibility to the company’s body of information is expected to enhance productivity, cut costs and time looking for information, and boost profits for any business.</p>

<p><a data-primary="hallucinations" data-type="indexterm" id="id831"/>However, LLMs are susceptible to generating responses that don’t adhere to the instructions given by the user.
In other words, the LLM can <em>hallucinate</em>
responses with information or data that is not based on facts or reality.</p>

<p>These hallucinations can occur due to the model’s reliance on patterns in the data it was trained on rather than direct access to external, up-to-date, and factual data.
LLMs can manifest hallucinations with confidently presented yet incorrect or nonsensical answers, fabricated stories, or claims without a basis in truth.</p>

<p>Therefore, for more complex and knowledge-intensive tasks, you will want your LLM to access external knowledge sources to complete tasks.
This enables more factual consistency and improves the reliability of the generated responses.
<a data-type="xref" href="#rag">Figure 5-7</a> shows the full process.</p>

<figure><div class="figure" id="rag">
<img alt="bgai 0507" src="assets/bgai_0507.png"/>
<h6><span class="label">Figure 5-7. </span>RAG</h6>
</div></figure>

<p>In this project, you will build a simple RAG module for your LLM service such that users can upload and talk to their documents.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>There is a lot to know about RAG.
It’s enough to fill several textbooks with new papers being published every day for new techniques and algorithms.</p>

<p>I recommend checking out other publications on LLMs to learn about the RAG process and advanced RAG techniques.</p>
</div>

<p>The pipeline for RAG consists of the following stages:</p>
<ol>
<li>
<p><em>Extraction</em> of documents from a filesystem to load the textual content in chunks onto memory.</p>
</li>
<li>
<p><em>Transformation</em> of the textual content by cleaning, splitting, and preparing them to be passed into an embedding model to produce embedding vectors that represent a chunk’s semantic meaning.</p>
</li>
<li>
<p><em>Storage</em> of embedding vectors alongside metadata, such as the source and text chunk, in a vector store such as Qdrant.</p>
</li>
<li>
<p><em>Retrieval</em> of semantically relevant embedding vectors by performing a semantic search on the user’s query to the LLM.
The original text chunks—​stored as metadata of the retrieved vectors—​are then used to augment (i.e., enhance the context within) the initial prompt provided to the LLM.</p>
</li>
<li>
<p><em>Generation</em> of LLM response bypassing both the query and retrieved chunks (i.e., context) to the LLM for getting a response.</p>
</li>

</ol>

<p>You can see the full pipeline in <a data-type="xref" href="#rag_pipeline">Figure 5-8</a>.</p>

<figure><div class="figure" id="rag_pipeline">
<img alt="bgai 0508" src="assets/bgai_0508.png"/>
<h6><span class="label">Figure 5-8. </span>RAG pipeline</h6>
</div></figure>

<p>You can take the pipeline shown in <a data-type="xref" href="#rag_pipeline">Figure 5-8</a> and build it to your existing service.
<a data-type="xref" href="#rag_module">Figure 5-9</a> shows the system architecture of a “talk to your documents” service enabled with RAG.</p>

<figure><div class="figure" id="rag_module">
<img alt="bgai 0509" src="assets/bgai_0509.png"/>
<h6><span class="label">Figure 5-9. </span>Talk to your documents system architecture</h6>
</div></figure>

<p><a data-type="xref" href="#rag_module">Figure 5-9</a> outlines how the documents uploaded by users via the Streamlit interface are stored and then fetched for processing and storage into the database for later retrieval to augment the LLM prompts.</p>

<p>The first step before implementing the RAG system in <a data-type="xref" href="#rag_module">Figure 5-9</a> is to include a file upload functionality in both the Streamlit client and your backend API.</p>

<p>Using FastAPI’s <code>UploadFile</code> class, you can accept documents from users in chunks and save them into the filesystem or any other file storage solution such as a blob storage.
The important item to note here is that this I/O operation is nonblocking through asynchronous programming, which FastAPI’s <code>UploadFile</code> class supports.</p>
<div data-type="tip"><h6>Tip</h6>
<p><a data-primary="chunking" data-type="indexterm" id="id832"/>Since users may upload large documents, FastAPI’s <code>UploadFile</code> class supports <em>chunking</em>
to store the uploaded documents, one piece at a time.</p>

<p>This will prevent your service’s memory from being clogged up.
You will also want to protect your service by disallowing users from uploading documents above a certain size.</p>
</div>

<p><a data-type="xref" href="#upload_file">Example 5-8</a> shows how to implement an asynchronous file upload functionality.</p>
<div data-type="tip"><h6>Tip</h6>
<p>You will need to install <code>aiofiles</code> package to asynchronously upload files alongside <code>python-multipart</code> to receive uploaded files from HTML forms:</p>

<pre data-code-language="bash" data-type="programlisting">$ pip install aiofiles python-multipart<code class="w"/></pre>
</div>
<div data-type="example" id="upload_file">
<h5><span class="label">Example 5-8. </span>Implementing an asynchronous file upload endpoint</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># upload.py</code>

<code class="kn">import</code> <code class="nn">os</code>
<code class="kn">import</code> <code class="nn">aiofiles</code>
<code class="kn">from</code> <code class="nn">aiofiles.os</code> <code class="kn">import</code> <code class="n">makedirs</code>
<code class="kn">from</code> <code class="nn">fastapi</code> <code class="kn">import</code> <code class="n">UploadFile</code>

<code class="n">DEFAULT_CHUNK_SIZE</code> <code class="o">=</code> <code class="mi">1024</code> <code class="o">*</code> <code class="mi">1024</code> <code class="o">*</code> <code class="mi">50</code>  <code class="c1"># 50 megabytes</code>

<code class="k">async</code> <code class="k">def</code> <code class="nf">save_file</code><code class="p">(</code><code class="n">file</code><code class="p">:</code> <code class="n">UploadFile</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">str</code><code class="p">:</code>
    <code class="k">await</code> <code class="n">makedirs</code><code class="p">(</code><code class="s2">"uploads"</code><code class="p">,</code> <code class="n">exist_ok</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
    <code class="n">filepath</code> <code class="o">=</code> <code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="s2">"uploads"</code><code class="p">,</code> <code class="n">file</code><code class="o">.</code><code class="n">filename</code><code class="p">)</code>
    <code class="k">async</code> <code class="k">with</code> <code class="n">aiofiles</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">filepath</code><code class="p">,</code> <code class="s2">"wb"</code><code class="p">)</code> <code class="k">as</code> <code class="n">f</code><code class="p">:</code>
        <code class="k">while</code> <code class="n">chunk</code> <code class="o">:=</code> <code class="k">await</code> <code class="n">file</code><code class="o">.</code><code class="n">read</code><code class="p">(</code><code class="n">DEFAULT_CHUNK_SIZE</code><code class="p">):</code>
            <code class="k">await</code> <code class="n">f</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="n">chunk</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">filepath</code>

<code class="c1"># main.py</code>

<code class="kn">from</code> <code class="nn">fastapi</code> <code class="kn">import</code> <code class="n">FastAPI</code><code class="p">,</code> <code class="n">HTTPException</code><code class="p">,</code> <code class="n">status</code><code class="p">,</code> <code class="n">File</code>
<code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">Annotated</code>
<code class="kn">from</code> <code class="nn">upload</code> <code class="kn">import</code> <code class="n">save_file</code>

<code class="nd">@app</code><code class="o">.</code><code class="n">post</code><code class="p">(</code><code class="s2">"/upload"</code><code class="p">)</code>
<code class="k">async</code> <code class="k">def</code> <code class="nf">file_upload_controller</code><code class="p">(</code>
    <code class="n">file</code><code class="p">:</code> <code class="n">Annotated</code><code class="p">[</code><code class="n">UploadFile</code><code class="p">,</code> <code class="n">File</code><code class="p">(</code><code class="n">description</code><code class="o">=</code><code class="s2">"Uploaded PDF documents"</code><code class="p">)]</code>
<code class="p">):</code>
    <code class="k">if</code> <code class="n">file</code><code class="o">.</code><code class="n">content_type</code> <code class="o">!=</code> <code class="s2">"application/pdf"</code><code class="p">:</code>
        <code class="k">raise</code> <code class="n">HTTPException</code><code class="p">(</code>
            <code class="n">detail</code><code class="o">=</code><code class="sa">f</code><code class="s2">"Only uploading PDF documents are supported"</code><code class="p">,</code>
            <code class="n">status_code</code><code class="o">=</code><code class="n">status</code><code class="o">.</code><code class="n">HTTP_400_BAD_REQUEST</code><code class="p">,</code>
        <code class="p">)</code>
    <code class="k">try</code><code class="p">:</code>
        <code class="k">await</code> <code class="n">save_file</code><code class="p">(</code><code class="n">file</code><code class="p">)</code>
    <code class="k">except</code> <code class="ne">Exception</code> <code class="k">as</code> <code class="n">e</code><code class="p">:</code>
        <code class="k">raise</code> <code class="n">HTTPException</code><code class="p">(</code>
            <code class="n">detail</code><code class="o">=</code><code class="sa">f</code><code class="s2">"An error occurred while saving file - Error: </code><code class="si">{</code><code class="n">e</code><code class="si">}</code><code class="s2">"</code><code class="p">,</code>
            <code class="n">status_code</code><code class="o">=</code><code class="n">status</code><code class="o">.</code><code class="n">HTTP_500_INTERNAL_SERVER_ERROR</code><code class="p">,</code>
        <code class="p">)</code>
    <code class="k">return</code> <code class="p">{</code><code class="s2">"filename"</code><code class="p">:</code> <code class="n">file</code><code class="o">.</code><code class="n">filename</code><code class="p">,</code> <code class="s2">"message"</code><code class="p">:</code> <code class="s2">"File uploaded successfully"</code><code class="p">}</code>

<code class="c1"># client.py</code>

<code class="kn">import</code> <code class="nn">requests</code>
<code class="kn">import</code> <code class="nn">streamlit</code> <code class="k">as</code> <code class="nn">st</code>

<code class="n">st</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="s2">"Upload a file to FastAPI"</code><code class="p">)</code>
<code class="n">file</code> <code class="o">=</code> <code class="n">st</code><code class="o">.</code><code class="n">file_uploader</code><code class="p">(</code><code class="s2">"Choose a file"</code><code class="p">,</code> <code class="nb">type</code><code class="o">=</code><code class="p">[</code><code class="s2">"pdf"</code><code class="p">])</code>

<code class="k">if</code> <code class="n">st</code><code class="o">.</code><code class="n">button</code><code class="p">(</code><code class="s2">"Submit"</code><code class="p">):</code>
    <code class="k">if</code> <code class="n">file</code> <code class="ow">is</code> <code class="ow">not</code> <code class="kc">None</code><code class="p">:</code>
        <code class="n">files</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"file"</code><code class="p">:</code> <code class="p">(</code><code class="n">file</code><code class="o">.</code><code class="n">name</code><code class="p">,</code> <code class="n">file</code><code class="p">,</code> <code class="n">file</code><code class="o">.</code><code class="n">type</code><code class="p">)}</code>
        <code class="n">response</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">post</code><code class="p">(</code><code class="s2">"http://localhost:8000/upload"</code><code class="p">,</code> <code class="n">files</code><code class="o">=</code><code class="n">files</code><code class="p">)</code>
        <code class="n">st</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="n">response</code><code class="o">.</code><code class="n">text</code><code class="p">)</code>
    <code class="k">else</code><code class="p">:</code>
        <code class="n">st</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="s2">"No file uploaded."</code><code class="p">)</code></pre></div>

<p>You should now be able to upload files via the Streamlit UI, as you can see in <a data-type="xref" href="#streamlit_upload">Figure 5-10</a>.</p>

<figure><div class="figure" id="streamlit_upload">
<img alt="bgai 0510" src="assets/bgai_0510.png"/>
<h6><span class="label">Figure 5-10. </span>Uploading files via Streamlit to the FastAPI service</h6>
</div></figure>

<p>With upload functionality implemented, you can now turn your attention to building the RAG module.
<a data-type="xref" href="#rag_module_detailed">Figure 5-11</a> shows the detailed pipeline, which opens up the data transformation component in <a data-type="xref" href="#rag_module">Figure 5-9</a>.</p>

<figure><div class="figure" id="rag_module_detailed">
<img alt="bgai 0511" src="assets/bgai_0511.png"/>
<h6><span class="label">Figure 5-11. </span>Detailed RAG data processing pipeline</h6>
</div></figure>

<p>As you can see in <a data-type="xref" href="#rag_module_detailed">Figure 5-11</a>, you need to asynchronously fetch the stored files from the hard disk and pass them through a data transformation pipeline prior to storage via an asynchronous database client.</p>

<p><a data-primary="RAG (retrieval augmented generation) module" data-secondary="data transformation pipeline" data-type="indexterm" id="id833"/>The data transformation pipeline consists of the following parts:</p>
<dl>
<dt>Extractor</dt>
<dd>
<p>Extract content of PDFs and store in text files back onto the hard disk.</p>
</dd>
<dt>Loader</dt>
<dd>
<p>Asynchronously load a text file into memory in chunks.</p>
</dd>
<dt>Cleaner</dt>
<dd>
<p>Remove any redundant whitespace or formatting characters from text chunks.</p>
</dd>
<dt>Embedder</dt>
<dd>
<p>Use a pretrained and self-hosted embedding model to convert text into embedding vectors.</p>
</dd>
</dl>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id834">
<h1>Embeddings</h1>
<p><a data-primary="embeddings" data-type="indexterm" id="id835"/>Embeddings are high-dimensional vectors that capture semantic meanings and relationships between words and phrases in texts.
Semantically similar texts have a smaller distance between them.</p>

<p>Using embedding models, you can encode text and images into embedding representations as points within a continuous vector space.
These transformed data points can subsequently be used for a myriad of downstream applications, such as information retrieval in RAG as well as clustering and classification tasks.</p>

<p>Please refer to <a data-type="xref" href="ch03.html#ch03">Chapter 3</a> for more information on embedding vectors.</p>
</div></aside>

<p>Once users upload their PDF files onto your server’s filesystem via the process shown in <a data-type="xref" href="#upload_file">Example 5-8</a>, you can immediately convert them into text files via the <code>pypdf</code> library.
Since there is no asynchronous library for loading binary PDF files, you will want to convert them into text files first.</p>

<p><a data-type="xref" href="#rag_extract">Example 5-9</a> shows how to load PDFs, extract and process their content, and then store them as text files.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>You will need to install several packages to run the upcoming examples:</p>

<pre data-code-language="bash" data-type="programlisting">$ pip install qdrant_client aiofiles pypdf loguru<code class="w"/></pre>
</div>
<div data-type="example" id="rag_extract">
<h5><span class="label">Example 5-9. </span>RAG PDF-to-text extractor</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># rag/extractor.py</code>

<code class="kn">from</code> <code class="nn">pypdf</code> <code class="kn">import</code> <code class="n">PdfReader</code>

<code class="k">def</code> <code class="nf">pdf_text_extractor</code><code class="p">(</code><code class="n">filepath</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="kc">None</code><code class="p">:</code>
    <code class="n">content</code> <code class="o">=</code> <code class="s2">"</code><code class="s2">"</code>
    <code class="n">pdf_reader</code> <code class="o">=</code> <code class="n">PdfReader</code><code class="p">(</code><code class="n">filepath</code><code class="p">,</code> <code class="n">strict</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO7-1" id="co_achieving_concurrency_in_ai_workloads_CO7-1"><img alt="1" src="assets/1.png"/></a>
    <code class="k">for</code> <code class="n">page</code> <code class="ow">in</code> <code class="n">pdf_reader</code><code class="o">.</code><code class="n">pages</code><code class="p">:</code>
        <code class="n">page_text</code> <code class="o">=</code> <code class="n">page</code><code class="o">.</code><code class="n">extract_text</code><code class="p">(</code><code class="p">)</code>
        <code class="k">if</code> <code class="n">page_text</code><code class="p">:</code>
            <code class="n">content</code> <code class="o">+</code><code class="o">=</code> <code class="sa">f</code><code class="s2">"</code><code class="si">{</code><code class="n">page_text</code><code class="si">}</code><code class="se">\n</code><code class="se">\n</code><code class="s2">"</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO7-2" id="co_achieving_concurrency_in_ai_workloads_CO7-2"><img alt="2" src="assets/2.png"/></a>
    <code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="n">filepath</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="s2">"</code><code class="s2">pdf</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="s2">txt</code><code class="s2">"</code><code class="p">)</code><code class="p">,</code> <code class="s2">"</code><code class="s2">w</code><code class="s2">"</code><code class="p">,</code> <code class="n">encoding</code><code class="o">=</code><code class="s2">"</code><code class="s2">utf-8</code><code class="s2">"</code><code class="p">)</code> <code class="k">as</code> <code class="n">file</code><code class="p">:</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO7-3" id="co_achieving_concurrency_in_ai_workloads_CO7-3"><img alt="3" src="assets/3.png"/></a>
        <code class="n">file</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="n">content</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO7-1" id="callout_achieving_concurrency_in_ai_workloads_CO7-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Use the <code>pypdf</code> library to open a stream pointer to a PDF file with <code>strict=True</code>
so that any read errors are logged to the terminal.
Note that there is no asynchronous implementation of the <code>pypdf</code> library, so the function is declared with a normal <code>def</code> keyword.
It is important to avoid using this function within an asynchronous function to avoid blocking the event loop that runs the main server thread.
You will see how FastAPI background tasks can help solve this problem.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO7-2" id="callout_achieving_concurrency_in_ai_workloads_CO7-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Loop over every page in the PDF document, and extract and append all text content into a long string.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO7-3" id="callout_achieving_concurrency_in_ai_workloads_CO7-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Write the content of the PDF document into a text file for downstream processing.
Specify <code>encoding="utf-8"</code> to avoid problems on platforms like Windows.</p></dd>
</dl></div>

<p>The text extractor will convert the PDF files into simple text files that we can stream into memory in chunks using an asynchronous file loader.
<a data-primary="Jina embedder model" data-type="indexterm" id="id836"/>Each chunk can then be cleaned and embedded into an embedding vector using an open source embedding model such as <code>jinaai/jina-embeddings-v2-base-en</code>, available to download from the <a href="https://oreil.ly/gI74r">Hugging Face model hub</a>.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>I selected the Jina base embedder since it matches the performance of OpenAI’s proprietary <code>text-embedding-ada-002</code> model.</p>
</div>

<p><a data-type="xref" href="#rag_transform">Example 5-10</a> shows the implementation of the RAG data transformation pipeline including the async text loader, cleaner, and embedding functions.</p>
<div data-type="example" id="rag_transform">
<h5><span class="label">Example 5-10. </span>RAG data transformation functions</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># rag/transform.py</code>

<code class="kn">import</code> <code class="nn">re</code>
<code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">Any</code><code class="p">,</code> <code class="n">AsyncGenerator</code>

<code class="kn">import</code> <code class="nn">aiofiles</code>
<code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">AutoModel</code>

<code class="n">DEFAULT_CHUNK_SIZE</code> <code class="o">=</code> <code class="mi">1024</code> <code class="o">*</code> <code class="mi">1024</code> <code class="o">*</code> <code class="mi">50</code>  <code class="c1"># 50 megabytes</code>

<code class="n">embedder</code> <code class="o">=</code> <code class="n">AutoModel</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code>
    <code class="s2">"</code><code class="s2">jinaai/jina-embeddings-v2-base-en</code><code class="s2">"</code><code class="p">,</code> <code class="n">trust_remote_code</code><code class="o">=</code><code class="kc">True</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO8-1" id="co_achieving_concurrency_in_ai_workloads_CO8-1"><img alt="1" src="assets/1.png"/></a>
<code class="p">)</code>

<code class="k">async</code> <code class="k">def</code> <code class="nf">load</code><code class="p">(</code><code class="n">filepath</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="n">AsyncGenerator</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="n">Any</code><code class="p">]</code><code class="p">:</code>
    <code class="k">async</code> <code class="k">with</code> <code class="n">aiofiles</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">filepath</code><code class="p">,</code> <code class="s2">"</code><code class="s2">r</code><code class="s2">"</code><code class="p">,</code> <code class="n">encoding</code><code class="o">=</code><code class="s2">"</code><code class="s2">utf-8</code><code class="s2">"</code><code class="p">)</code> <code class="k">as</code> <code class="n">f</code><code class="p">:</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO8-2" id="co_achieving_concurrency_in_ai_workloads_CO8-2"><img alt="2" src="assets/2.png"/></a>
        <code class="k">while</code> <code class="n">chunk</code> <code class="o">:=</code> <code class="k">await</code> <code class="n">f</code><code class="o">.</code><code class="n">read</code><code class="p">(</code><code class="n">DEFAULT_CHUNK_SIZE</code><code class="p">)</code><code class="p">:</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO8-3" id="co_achieving_concurrency_in_ai_workloads_CO8-3"><img alt="3" src="assets/3.png"/></a>
            <code class="k">yield</code> <code class="n">chunk</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO8-4" id="co_achieving_concurrency_in_ai_workloads_CO8-4"><img alt="4" src="assets/4.png"/></a>

<code class="k">def</code> <code class="nf">clean</code><code class="p">(</code><code class="n">text</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="nb">str</code><code class="p">:</code>
    <code class="n">t</code> <code class="o">=</code> <code class="n">text</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="s2">"</code><code class="se">\n</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code> <code class="s2">"</code><code class="p">)</code>
    <code class="n">t</code> <code class="o">=</code> <code class="n">re</code><code class="o">.</code><code class="n">sub</code><code class="p">(</code><code class="sa">r</code><code class="s2">"</code><code class="s2">\</code><code class="s2">s+</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code> <code class="s2">"</code><code class="p">,</code> <code class="n">t</code><code class="p">)</code>
    <code class="n">t</code> <code class="o">=</code> <code class="n">re</code><code class="o">.</code><code class="n">sub</code><code class="p">(</code><code class="sa">r</code><code class="s2">"</code><code class="s2">\</code><code class="s2">. ,</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="s2">"</code><code class="p">,</code> <code class="n">t</code><code class="p">)</code>
    <code class="n">t</code> <code class="o">=</code> <code class="n">t</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="s2">"</code><code class="s2">..</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="s2">.</code><code class="s2">"</code><code class="p">)</code>
    <code class="n">t</code> <code class="o">=</code> <code class="n">t</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="s2">"</code><code class="s2">. .</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="s2">.</code><code class="s2">"</code><code class="p">)</code>
    <code class="n">cleaned_text</code> <code class="o">=</code> <code class="n">t</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="s2">"</code><code class="se">\n</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code> <code class="s2">"</code><code class="p">)</code><code class="o">.</code><code class="n">strip</code><code class="p">(</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">cleaned_text</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO8-5" id="co_achieving_concurrency_in_ai_workloads_CO8-5"><img alt="5" src="assets/5.png"/></a>

<code class="k">def</code> <code class="nf">embed</code><code class="p">(</code><code class="n">text</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="nb">list</code><code class="p">[</code><code class="nb">float</code><code class="p">]</code><code class="p">:</code>
    <code class="k">return</code> <code class="n">embedder</code><code class="o">.</code><code class="n">encode</code><code class="p">(</code><code class="n">text</code><code class="p">)</code><code class="o">.</code><code class="n">tolist</code><code class="p">(</code><code class="p">)</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO8-6" id="co_achieving_concurrency_in_ai_workloads_CO8-6"><img alt="6" src="assets/6.png"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO8-1" id="callout_achieving_concurrency_in_ai_workloads_CO8-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Download and use the open source <code>jina-embeddings-v2-base-en</code> model to embed text strings into embedding vectors.
Set <code>trust_remote_code=True</code> to download model weights and tokenizer configurations.
Without this parameter set to <code>True</code>, the downloaded model weights will be initialized with random values instead of trained values.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO8-2" id="callout_achieving_concurrency_in_ai_workloads_CO8-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Use the <code>aiofiles</code> library to open an asynchronous connections to a file on the filesystem.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO8-3" id="callout_achieving_concurrency_in_ai_workloads_CO8-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Load the content of text documents in chunks for memory-efficient I/O 
<span class="keep-together">operation.</span></p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO8-4" id="callout_achieving_concurrency_in_ai_workloads_CO8-4"><img alt="4" src="assets/4.png"/></a></dt>
<dd><p>Instead of returning a <code>chunk</code>, yield it so that the <code>load()</code> function becomes an <em>asynchronous generator</em>.
Asynchronous generators can be iterated with <code>async for loop</code>s so that blocking operations within them can be <code>await</code>ed to let the event loop start/resume other tasks.
Both async <code>for</code> loops and normal <code>for</code> loops, iterate sequentially over the iterable but async <code>for</code> loops allow for iteration over an async iterator.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO8-5" id="callout_achieving_concurrency_in_ai_workloads_CO8-5"><img alt="5" src="assets/5.png"/></a></dt>
<dd><p>Clean the text by removing any extra spaces, commas, dots, and line breaks.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO8-6" id="callout_achieving_concurrency_in_ai_workloads_CO8-6"><img alt="6" src="assets/6.png"/></a></dt>
<dd><p>Use the Jina embedding model to convert a text chunk to an embedding vector.</p></dd>
</dl></div>

<p><a data-primary="vector database" data-type="indexterm" id="id837"/>Once the data is processed into embedding vectors, you can store them into the <em>vector database</em>.</p>

<p class="less_space pagebreak-before">Unlike conventional alternatives such as relational databases, a vector database is specifically designed for handling data storage and retrieval operations optimized for <em>semantic searching</em>, which yields better results compared to keyword searches that can return suboptimal or incomplete results.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id838">
<h1>Performing a Semantic Search</h1>
<p><a data-primary="cosine similarity" data-type="indexterm" id="id839"/><a data-primary="semantic search" data-type="indexterm" id="id840"/>Semantic search uses a mathematical operation called <em>cosine similarity</em>
between the user query’s embedding vector and the embedding vectors (i.e., embedded documents) stored in the database to retrieve the most relevant items.</p>

<p>Cosine similarity calculation performs a normalized dot product computation between two vectors to return a number between –1 and 1.<sup><a data-type="noteref" href="ch05.html#id841" id="id841-marker">6</a></sup>
The normalization ensures the result is between –1 and 1.</p>
<p class="fix_tracking">
A score of 1 means the vectors are aligned and share a similar semantic meaning, while a score of –1 implies they’re diametrically opposed, signifying opposite meanings.</p>

<p>A score of 0 indicates no semantic correlation between the vectors, suggesting that these vectors can be excluded from search results.</p>

<p>The returned results from the database are a collection of ordered embedding vectors that contain metadata including the original text.
You can inject these text snippets directly into user prompts to augment them with relevant context before forwarding to the LLM.</p>

<p>Semantic search results retrieved from the database are sorted in descending order by similarity scores.
<a data-primary="context ranking" data-type="indexterm" id="id842"/>This ordering, referred to as <em>context ranking</em>, is important since research has shown that LLMs are more sensitive to the words that appear earlier in the prompt than later.</p>

<p>This makes sense since we humans also have better comprehension and attention, in a single reading session, earlier within large documents rather than in the middle or the end.
That’s why we have sections such as an “Executive Summary” in large reports to communicate the most important content of the report to readers.</p>
</div></aside>

<p class="less_space pagebreak-before">The following code examples require you to run a local instance of the <code>qdrant</code> vector database on your local machine for the RAG module.
Having a local database setup will give you the hands-on experience of working asynchronously with production-grade vector databases.
To run the database in a container, you should have Docker installed on your machine and then pull and run the <code>qdrant</code> vector database container.<sup><a data-type="noteref" href="ch05.html#id843" id="id843-marker">7</a></sup>
If you aren’t familiar with Docker, don’t worry.
You will learn more about Docker and containerization in
<a data-type="xref" href="ch12.html#ch12">Chapter 12</a>.</p>

<pre data-code-language="bash" data-type="programlisting"><code>$</code> <code>docker</code> <code>pull</code> <code>qdrant/qdrant</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO9-1" id="co_achieving_concurrency_in_ai_workloads_CO9-1"><img alt="1" src="assets/1.png"/></a>
<code>$</code> <code>docker</code> <code>run</code> <code>-p</code> <code class="m">6333</code><code>:6333</code> <code>-p</code> <code class="m">6334</code><code>:6334</code> <code class="se">\ </code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO9-2" id="co_achieving_concurrency_in_ai_workloads_CO9-2"><img alt="2" src="assets/2.png"/></a>
    <code>-v</code> <code class="k">$(</code><code class="nb">pwd</code><code class="k">)</code><code>/qdrant_storage:/qdrant/storage:z</code> <code class="se">\ </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO9-3" id="co_achieving_concurrency_in_ai_workloads_CO9-3"><img alt="3" src="assets/3.png"/></a>
    <code>qdrant/qdrant</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO9-1" id="callout_achieving_concurrency_in_ai_workloads_CO9-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Download the <code>qdrant</code> vector database image from the <code>qdrant</code> repository in the Docker registry.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO9-2" id="callout_achieving_concurrency_in_ai_workloads_CO9-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Run the <code>qdrant/qdrant</code> image, and then expose and map container ports <code>6333</code> and <code>6334</code> to the same ports on the host machine.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO9-3" id="callout_achieving_concurrency_in_ai_workloads_CO9-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Mount the <code>qdrant</code> database storage to the host machine filesystem at your project’s root directory.</p></dd>
</dl>

<p>Since database storage and retrieval are I/O operations, you should use an asynchronous database client.
Thankfully, <code>qdrant</code> provides an asynchronous database client to work with.</p>
<div data-type="tip"><h6>Tip</h6>
<p>You can use other vector database providers such as
Weaviate, Elastic, Milvus, Pinecone, Chroma, or others in replacement of Qdrant.
Each has a set of features and limitations to consider for your own use case.</p>

<p>If you’re picking another database provider, make sure there is an asynchronous database client available that you can use.</p>
</div>

<p>Instead of writing several functions to store and retrieve data from the database, you can use the repository pattern mentioned in <a data-type="xref" href="ch02.html#ch02">Chapter 2</a>.
With the repository pattern, you can abstract low-level create, read, update, and delete database operations with defaults that match your use case.</p>

<p class="less_space pagebreak-before"><a data-type="xref" href="#rag_repository">Example 5-11</a> shows the repository pattern implementation for the Qdrant vector database.</p>
<div data-type="example" id="rag_repository">
<h5><span class="label">Example 5-11. </span>Vector database client setup using the repository pattern</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># rag/repository.py</code>

<code class="kn">from</code> <code class="nn">loguru</code> <code class="kn">import</code> <code class="n">logger</code>
<code class="kn">from</code> <code class="nn">qdrant_client</code> <code class="kn">import</code> <code class="n">AsyncQdrantClient</code>
<code class="kn">from</code> <code class="nn">qdrant_client</code><code class="nn">.</code><code class="nn">http</code> <code class="kn">import</code> <code class="n">models</code>
<code class="kn">from</code> <code class="nn">qdrant_client</code><code class="nn">.</code><code class="nn">http</code><code class="nn">.</code><code class="nn">models</code> <code class="kn">import</code> <code class="n">ScoredPoint</code>


<code class="k">class</code> <code class="nc">VectorRepository</code><code class="p">:</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO10-1" id="co_achieving_concurrency_in_ai_workloads_CO10-1"><img alt="1" src="assets/1.png"/></a>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">host</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="s2">"</code><code class="s2">localhost</code><code class="s2">"</code><code class="p">,</code> <code class="n">port</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">6333</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="kc">None</code><code class="p">:</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">db_client</code> <code class="o">=</code> <code class="n">AsyncQdrantClient</code><code class="p">(</code><code class="n">host</code><code class="o">=</code><code class="n">host</code><code class="p">,</code> <code class="n">port</code><code class="o">=</code><code class="n">port</code><code class="p">)</code>

    <code class="k">async</code> <code class="k">def</code> <code class="nf">create_collection</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">collection_name</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code> <code class="n">size</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="nb">bool</code><code class="p">:</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO10-2" id="co_achieving_concurrency_in_ai_workloads_CO10-2"><img alt="2" src="assets/2.png"/></a>
        <code class="n">vectors_config</code> <code class="o">=</code> <code class="n">models</code><code class="o">.</code><code class="n">VectorParams</code><code class="p">(</code>
            <code class="n">size</code><code class="o">=</code><code class="n">size</code><code class="p">,</code> <code class="n">distance</code><code class="o">=</code><code class="n">models</code><code class="o">.</code><code class="n">Distance</code><code class="o">.</code><code class="n">COSINE</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO10-3" id="co_achieving_concurrency_in_ai_workloads_CO10-3"><img alt="3" src="assets/3.png"/></a>
        <code class="p">)</code>
        <code class="n">response</code> <code class="o">=</code> <code class="k">await</code> <code class="bp">self</code><code class="o">.</code><code class="n">db_client</code><code class="o">.</code><code class="n">get_collections</code><code class="p">(</code><code class="p">)</code>

        <code class="n">collection_exists</code> <code class="o">=</code> <code class="nb">any</code><code class="p">(</code>
            <code class="n">collection</code><code class="o">.</code><code class="n">name</code> <code class="o">==</code> <code class="n">collection_name</code>
            <code class="k">for</code> <code class="n">collection</code> <code class="ow">in</code> <code class="n">response</code><code class="o">.</code><code class="n">collections</code>
        <code class="p">)</code>
        <code class="k">if</code> <code class="n">collection_exists</code><code class="p">:</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO10-4" id="co_achieving_concurrency_in_ai_workloads_CO10-4"><img alt="4" src="assets/4.png"/></a>
            <code class="n">logger</code><code class="o">.</code><code class="n">debug</code><code class="p">(</code>
                <code class="sa">f</code><code class="s2">"</code><code class="s2">Collection </code><code class="si">{</code><code class="n">collection_name</code><code class="si">}</code><code class="s2"> already exists - recreating it</code><code class="s2">"</code>
            <code class="p">)</code>
            <code class="k">await</code> <code class="bp">self</code><code class="o">.</code><code class="n">db_client</code><code class="o">.</code><code class="n">delete_collection</code><code class="p">(</code><code class="n">collection_name</code><code class="p">)</code>
            <code class="k">return</code> <code class="k">await</code> <code class="bp">self</code><code class="o">.</code><code class="n">db_client</code><code class="o">.</code><code class="n">create_collection</code><code class="p">(</code>
                <code class="n">collection_name</code><code class="p">,</code>
                <code class="n">vectors_config</code><code class="o">=</code><code class="n">vectors_config</code><code class="p">,</code>
            <code class="p">)</code>

        <code class="n">logger</code><code class="o">.</code><code class="n">debug</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="s2">Creating collection </code><code class="si">{</code><code class="n">collection_name</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>
        <code class="k">return</code> <code class="k">await</code> <code class="bp">self</code><code class="o">.</code><code class="n">db_client</code><code class="o">.</code><code class="n">create_collection</code><code class="p">(</code>
            <code class="n">collection_name</code><code class="o">=</code><code class="n">collection_name</code><code class="p">,</code>
            <code class="n">vectors_config</code><code class="o">=</code><code class="n">models</code><code class="o">.</code><code class="n">VectorParams</code><code class="p">(</code>
                <code class="n">size</code><code class="o">=</code><code class="n">size</code><code class="p">,</code> <code class="n">distance</code><code class="o">=</code><code class="n">models</code><code class="o">.</code><code class="n">Distance</code><code class="o">.</code><code class="n">COSINE</code>
            <code class="p">)</code><code class="p">,</code>
        <code class="p">)</code>

    <code class="k">async</code> <code class="k">def</code> <code class="nf">delete_collection</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">name</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="nb">bool</code><code class="p">:</code>
        <code class="n">logger</code><code class="o">.</code><code class="n">debug</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="s2">Deleting collection </code><code class="si">{</code><code class="n">name</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>
        <code class="k">return</code> <code class="k">await</code> <code class="bp">self</code><code class="o">.</code><code class="n">db_client</code><code class="o">.</code><code class="n">delete_collection</code><code class="p">(</code><code class="n">name</code><code class="p">)</code>

    <code class="k">async</code> <code class="k">def</code> <code class="nf">create</code><code class="p">(</code>
        <code class="bp">self</code><code class="p">,</code>
        <code class="n">collection_name</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code>
        <code class="n">embedding_vector</code><code class="p">:</code> <code class="nb">list</code><code class="p">[</code><code class="nb">float</code><code class="p">]</code><code class="p">,</code>
        <code class="n">original_text</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code>
        <code class="n">source</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code>
    <code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="kc">None</code><code class="p">:</code>
        <code class="n">response</code> <code class="o">=</code> <code class="k">await</code> <code class="bp">self</code><code class="o">.</code><code class="n">db_client</code><code class="o">.</code><code class="n">count</code><code class="p">(</code><code class="n">collection_name</code><code class="o">=</code><code class="n">collection_name</code><code class="p">)</code>
        <code class="n">logger</code><code class="o">.</code><code class="n">debug</code><code class="p">(</code>
            <code class="sa">f</code><code class="s2">"</code><code class="s2">Creating a new vector with ID </code><code class="si">{</code><code class="n">response</code><code class="o">.</code><code class="n">count</code><code class="si">}</code> <code class="s2">"</code>
            <code class="sa">f</code><code class="s2">"</code><code class="s2">inside the </code><code class="si">{</code><code class="n">collection_name</code><code class="si">}</code><code class="s2">"</code>
        <code class="p">)</code>
        <code class="k">await</code> <code class="bp">self</code><code class="o">.</code><code class="n">db_client</code><code class="o">.</code><code class="n">upsert</code><code class="p">(</code>
            <code class="n">collection_name</code><code class="o">=</code><code class="n">collection_name</code><code class="p">,</code>
            <code class="n">points</code><code class="o">=</code><code class="p">[</code>
                <code class="n">models</code><code class="o">.</code><code class="n">PointStruct</code><code class="p">(</code>
                    <code class="nb">id</code><code class="o">=</code><code class="n">response</code><code class="o">.</code><code class="n">count</code><code class="p">,</code>
                    <code class="n">vector</code><code class="o">=</code><code class="n">embedding_vector</code><code class="p">,</code>
                    <code class="n">payload</code><code class="o">=</code><code class="p">{</code>
                        <code class="s2">"</code><code class="s2">source</code><code class="s2">"</code><code class="p">:</code> <code class="n">source</code><code class="p">,</code>
                        <code class="s2">"</code><code class="s2">original_text</code><code class="s2">"</code><code class="p">:</code> <code class="n">original_text</code><code class="p">,</code>
                    <code class="p">}</code><code class="p">,</code>
                <code class="p">)</code>
            <code class="p">]</code><code class="p">,</code>
        <code class="p">)</code>

    <code class="k">async</code> <code class="k">def</code> <code class="nf">search</code><code class="p">(</code>
        <code class="bp">self</code><code class="p">,</code>
        <code class="n">collection_name</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code>
        <code class="n">query_vector</code><code class="p">:</code> <code class="nb">list</code><code class="p">[</code><code class="nb">float</code><code class="p">]</code><code class="p">,</code>
        <code class="n">retrieval_limit</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code>
        <code class="n">score_threshold</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO10-5" id="co_achieving_concurrency_in_ai_workloads_CO10-5"><img alt="5" src="assets/5.png"/></a>
    <code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="nb">list</code><code class="p">[</code><code class="n">ScoredPoint</code><code class="p">]</code><code class="p">:</code>
        <code class="n">logger</code><code class="o">.</code><code class="n">debug</code><code class="p">(</code>
            <code class="sa">f</code><code class="s2">"</code><code class="s2">Searching for relevant items in the </code><code class="si">{</code><code class="n">collection_name</code><code class="si">}</code><code class="s2"> collection</code><code class="s2">"</code>
        <code class="p">)</code>
        <code class="n">response</code> <code class="o">=</code> <code class="k">await</code> <code class="bp">self</code><code class="o">.</code><code class="n">db_client</code><code class="o">.</code><code class="n">query_points</code><code class="p">(</code>
            <code class="n">collection_name</code><code class="o">=</code><code class="n">collection_name</code><code class="p">,</code>
            <code class="n">query_vector</code><code class="o">=</code><code class="n">query_vector</code><code class="p">,</code>
            <code class="n">limit</code><code class="o">=</code><code class="n">retrieval_limit</code><code class="p">,</code>
            <code class="n">score_threshold</code><code class="o">=</code><code class="n">score_threshold</code><code class="p">,</code>
        <code class="p">)</code>
        <code class="k">return</code> <code class="n">response</code><code class="o">.</code><code class="n">points</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO10-1" id="callout_achieving_concurrency_in_ai_workloads_CO10-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Use the repository pattern to interact with the vector database via an asynchronous client.
Normally, in the repository pattern you will implement the <code>create</code>, <code>get</code>, <code>update</code>, and <code>delete</code> methods.
But for now let’s implement the <code>create_​col⁠lection</code>, <code>delete_collection</code>, <code>create</code>, and <code>search</code> methods.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO10-2" id="callout_achieving_concurrency_in_ai_workloads_CO10-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Vectors need to be stored in a collection.
A collection is a named set of points that you can use during a search.
Collections are similar to tables in a relational database.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO10-3" id="callout_achieving_concurrency_in_ai_workloads_CO10-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Let the database know that any vectors in this collection should be compared via the cosine similarity calculation that calculates distances between vectors.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO10-4" id="callout_achieving_concurrency_in_ai_workloads_CO10-4"><img alt="4" src="assets/4.png"/></a></dt>
<dd><p>Check whether a collection exists before creating a new one.
Otherwise, re-create the collection.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO10-5" id="callout_achieving_concurrency_in_ai_workloads_CO10-5"><img alt="5" src="assets/5.png"/></a></dt>
<dd><p>Set the <code>retrieval_limit</code> and <code>score_threshold</code> to limit the number of items in the search results.</p></dd>
</dl></div>

<p>The <code>VectorRepository</code> class should now make it easier to interact with the database.</p>

<p>When storing vector embeddings, you will also store some <em>metadata</em>
including the name of the source document, the location of the text within source, and the original extracted text.
RAG systems rely on this metadata to augment the LLM prompts and to show source information to the users.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Currently, converting text to embedding vectors is an irreversible process.
Therefore, you will need to store the text that created the embedding with the embedding vector as metadata.</p>
</div>

<p>You can now extend the <code>VectorRepository</code> and create the <code>VectorService</code> that allow you to chain together the data processing and storage pipeline, as shown in
<a data-type="xref" href="#rag_db_service">Example 5-12</a>.</p>
<div data-type="example" id="rag_db_service">
<h5><span class="label">Example 5-12. </span>Vector database service</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># rag/service.py</code>

<code class="kn">import</code> <code class="nn">os</code>

<code class="kn">from</code> <code class="nn">loguru</code> <code class="kn">import</code> <code class="n">logger</code>
<code class="kn">from</code> <code class="nn">.</code><code class="nn">repository</code> <code class="kn">import</code> <code class="n">VectorRepository</code>
<code class="kn">from</code> <code class="nn">.</code><code class="nn">transform</code> <code class="kn">import</code> <code class="n">clean</code><code class="p">,</code> <code class="n">embed</code><code class="p">,</code> <code class="n">load</code>


<code class="k">class</code> <code class="nc">VectorService</code><code class="p">(</code><code class="n">VectorRepository</code><code class="p">)</code><code class="p">:</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO11-1" id="co_achieving_concurrency_in_ai_workloads_CO11-1"><img alt="1" src="assets/1.png"/></a>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code><code class="p">:</code>
        <code class="nb">super</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">(</code><code class="p">)</code>

    <code class="k">async</code> <code class="k">def</code> <code class="nf">store_file_content_in_db</code><code class="p">(</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO11-2" id="co_achieving_concurrency_in_ai_workloads_CO11-2"><img alt="2" src="assets/2.png"/></a>
        <code class="bp">self</code><code class="p">,</code>
        <code class="n">filepath</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code>
        <code class="n">chunk_size</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">512</code><code class="p">,</code>
        <code class="n">collection_name</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="s2">"</code><code class="s2">knowledgebase</code><code class="s2">"</code><code class="p">,</code>
        <code class="n">collection_size</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">768</code><code class="p">,</code>
    <code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="kc">None</code><code class="p">:</code>
        <code class="k">await</code> <code class="bp">self</code><code class="o">.</code><code class="n">create_collection</code><code class="p">(</code><code class="n">collection_name</code><code class="p">,</code> <code class="n">collection_size</code><code class="p">)</code>
        <code class="n">logger</code><code class="o">.</code><code class="n">debug</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="s2">Inserting </code><code class="si">{</code><code class="n">filepath</code><code class="si">}</code><code class="s2"> content into database</code><code class="s2">"</code><code class="p">)</code>
        <code class="k">async</code> <code class="k">for</code> <code class="n">chunk</code> <code class="ow">in</code> <code class="n">load</code><code class="p">(</code><code class="n">filepath</code><code class="p">,</code> <code class="n">chunk_size</code><code class="p">)</code><code class="p">:</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO11-3" id="co_achieving_concurrency_in_ai_workloads_CO11-3"><img alt="3" src="assets/3.png"/></a>
            <code class="n">logger</code><code class="o">.</code><code class="n">debug</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="s2">Inserting </code><code class="s2">'</code><code class="si">{</code><code class="n">chunk</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="mi">20</code><code class="p">]</code><code class="si">}</code><code class="s2">...</code><code class="s2">'</code><code class="s2"> into database</code><code class="s2">"</code><code class="p">)</code>

            <code class="n">embedding_vector</code> <code class="o">=</code> <code class="n">embed</code><code class="p">(</code><code class="n">clean</code><code class="p">(</code><code class="n">chunk</code><code class="p">)</code><code class="p">)</code>
            <code class="n">filename</code> <code class="o">=</code> <code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">basename</code><code class="p">(</code><code class="n">filepath</code><code class="p">)</code>
            <code class="k">await</code> <code class="bp">self</code><code class="o">.</code><code class="n">create</code><code class="p">(</code>
                <code class="n">collection_name</code><code class="p">,</code> <code class="n">embedding_vector</code><code class="p">,</code> <code class="n">chunk</code><code class="p">,</code> <code class="n">filename</code>
            <code class="p">)</code>


<code class="n">vector_service</code> <code class="o">=</code> <code class="n">VectorService</code><code class="p">(</code><code class="p">)</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO11-4" id="co_achieving_concurrency_in_ai_workloads_CO11-4"><img alt="4" src="assets/4.png"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO11-1" id="callout_achieving_concurrency_in_ai_workloads_CO11-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Create the <code>VectorService</code> class by inheriting the <code>VectorRepository</code> class so 
<span class="keep-together">that you</span> can use and extend common database operation methods from <a data-type="xref" href="#rag_repository">Example 5-11</a>.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO11-2" id="callout_achieving_concurrency_in_ai_workloads_CO11-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Use the <code>store_file_content_in_db</code> service method to asynchronously load, transform, and store raw text documents into the database in chunks.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO11-3" id="callout_achieving_concurrency_in_ai_workloads_CO11-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Use an asynchronous generator <code>load()</code> to load text chunks from a file 
<span class="keep-together">asynchronously.</span></p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO11-4" id="callout_achieving_concurrency_in_ai_workloads_CO11-4"><img alt="4" src="assets/4.png"/></a></dt>
<dd><p>Create an instance of the <code>VectorService</code> to import and use across the 
<span class="keep-together">application.</span></p></dd>
</dl></div>

<p>The final step in the RAG data processing and storage pipeline is to run the text extraction and storage logic within the <code>file_upload_controller</code>
as background tasks.
The implementation is shown in <a data-type="xref" href="#rag_data_processor">Example 5-13</a> so that the handler can trigger both operations in the background after responding to the user.</p>
<div data-type="example" id="rag_data_processor">
<h5><span class="label">Example 5-13. </span>Update the upload handler to process and store PDF file content in the vector database</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># main.py</code>

<code class="kn">from</code> <code class="nn">fastapi</code> <code class="kn">import</code> <code class="p">(</code>
    <code class="n">BackgroundTasks</code><code class="p">,</code>
    <code class="n">FastAPI</code><code class="p">,</code>
    <code class="n">File</code><code class="p">,</code>
    <code class="n">UploadFile</code><code class="p">,</code>
    <code class="n">status</code><code class="p">,</code>
    <code class="n">HTTPException</code><code class="p">,</code>
<code class="p">)</code>
<code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">Annotated</code>
<code class="kn">from</code> <code class="nn">rag</code> <code class="kn">import</code> <code class="n">pdf_text_extractor</code><code class="p">,</code> <code class="n">vector_service</code>

<code class="nd">@app</code><code class="o">.</code><code class="n">post</code><code class="p">(</code><code class="s2">"</code><code class="s2">/upload</code><code class="s2">"</code><code class="p">)</code>
<code class="k">async</code> <code class="k">def</code> <code class="nf">file_upload_controller</code><code class="p">(</code>
    <code class="n">file</code><code class="p">:</code> <code class="n">Annotated</code><code class="p">[</code><code class="n">UploadFile</code><code class="p">,</code> <code class="n">File</code><code class="p">(</code><code class="n">description</code><code class="o">=</code><code class="s2">"</code><code class="s2">A file read as UploadFile</code><code class="s2">"</code><code class="p">)</code><code class="p">]</code><code class="p">,</code>
    <code class="n">bg_text_processor</code><code class="p">:</code> <code class="n">BackgroundTasks</code><code class="p">,</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO12-1" id="co_achieving_concurrency_in_ai_workloads_CO12-1"><img alt="1" src="assets/1.png"/></a>
<code class="p">)</code><code class="p">:</code>
    <code class="o">.</code><code class="o">.</code><code class="o">.</code> <code class="c1"># Raise an HTTPException if data upload is not a PDF file</code>
    <code class="k">try</code><code class="p">:</code>
        <code class="n">filepath</code> <code class="o">=</code> <code class="k">await</code> <code class="n">save_file</code><code class="p">(</code><code class="n">file</code><code class="p">)</code>
        <code class="n">bg_text_processor</code><code class="o">.</code><code class="n">add_task</code><code class="p">(</code><code class="n">pdf_text_extractor</code><code class="p">,</code> <code class="n">filepath</code><code class="p">)</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO12-2" id="co_achieving_concurrency_in_ai_workloads_CO12-2"><img alt="2" src="assets/2.png"/></a>
        <code class="n">bg_text_processor</code><code class="o">.</code><code class="n">add_task</code><code class="p">(</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO12-3" id="co_achieving_concurrency_in_ai_workloads_CO12-3"><img alt="3" src="assets/3.png"/></a>
            <code class="n">vector_service</code><code class="o">.</code><code class="n">store_file_content_in_db</code><code class="p">,</code>
            <code class="n">filepath</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="s2">"</code><code class="s2">pdf</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="s2">txt</code><code class="s2">"</code><code class="p">)</code><code class="p">,</code>
            <code class="mi">512</code><code class="p">,</code>
            <code class="s2">"</code><code class="s2">knowledgebase</code><code class="s2">"</code><code class="p">,</code>
            <code class="mi">768</code><code class="p">,</code>
        <code class="p">)</code>

    <code class="k">except</code> <code class="ne">Exception</code> <code class="k">as</code> <code class="n">e</code><code class="p">:</code>
        <code class="k">raise</code> <code class="n">HTTPException</code><code class="p">(</code>
            <code class="n">detail</code><code class="o">=</code><code class="sa">f</code><code class="s2">"</code><code class="s2">An error occurred while saving file - Error: </code><code class="si">{</code><code class="n">e</code><code class="si">}</code><code class="s2">"</code><code class="p">,</code>
            <code class="n">status_code</code><code class="o">=</code><code class="n">status</code><code class="o">.</code><code class="n">HTTP_500_INTERNAL_SERVER_ERROR</code><code class="p">,</code>
        <code class="p">)</code>
    <code class="k">return</code> <code class="p">{</code><code class="s2">"</code><code class="s2">filename</code><code class="s2">"</code><code class="p">:</code> <code class="n">file</code><code class="o">.</code><code class="n">filename</code><code class="p">,</code> <code class="s2">"</code><code class="s2">message</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">File uploaded successfully</code><code class="s2">"</code><code class="p">}</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO12-1" id="callout_achieving_concurrency_in_ai_workloads_CO12-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Inject the FastAPI background tasks feature into the handler for processing file uploads in the background.
FastAPI background tasks will be executed in order shortly after the handler sends a response to the client.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO12-2" id="callout_achieving_concurrency_in_ai_workloads_CO12-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Run the PDF text-extraction function in the background after retuning a response to the client.
Since the <code>pdf_text_extractor</code> is a synchronous function, FastAPI will run this function on a separate thread within the thread pool to avoid blocking the event loop.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO12-3" id="callout_achieving_concurrency_in_ai_workloads_CO12-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Run the <code>vector_service.store_file_content_in_db</code> asynchronous function in the background on the FastAPI managed event loop as soon as the <code>pdf_text_extractor</code> has finished processing.
Set the function to load content of the text document in chunks of 512 characters and store them in the <code>knowledgebase</code> vector collection, which accepts vectors of size 768.</p></dd>
</dl></div>

<p>After building the RAG data storage pipeline, you can now focus on the search-and-retrieval system, which will allow you to augment the user prompts to the LLM, with knowledge from the database.
<a data-type="xref" href="#rag_generation">Example 5-14</a> integrates the RAG search-and-retrieval operations with the LLM handler to augment the LLM prompts with additional 
<span class="keep-together">context.</span></p>
<div data-type="example" id="rag_generation">
<h5><span class="label">Example 5-14. </span>RAG integration with the LLM-serving endpoint</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># dependencies.py</code>

<code class="kn">from</code> <code class="nn">rag</code> <code class="kn">import</code> <code class="n">vector_service</code>
<code class="kn">from</code> <code class="nn">rag</code><code class="nn">.</code><code class="nn">transform</code> <code class="kn">import</code> <code class="n">embed</code>
<code class="kn">from</code> <code class="nn">schemas</code> <code class="kn">import</code> <code class="n">TextModelRequest</code><code class="p">,</code> <code class="n">TextModelResponse</code>

<code class="k">async</code> <code class="k">def</code> <code class="nf">get_rag_content</code><code class="p">(</code><code class="n">body</code><code class="p">:</code> <code class="n">TextModelRequest</code> <code class="o">=</code> <code class="n">Body</code><code class="p">(</code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code class="p">)</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="nb">str</code><code class="p">:</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO13-1" id="co_achieving_concurrency_in_ai_workloads_CO13-1"><img alt="1" src="assets/1.png"/></a>
    <code class="n">rag_content</code> <code class="o">=</code> <code class="k">await</code> <code class="n">vector_service</code><code class="o">.</code><code class="n">search</code><code class="p">(</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO13-2" id="co_achieving_concurrency_in_ai_workloads_CO13-2"><img alt="2" src="assets/2.png"/></a>
        <code class="s2">"</code><code class="s2">knowledgebase</code><code class="s2">"</code><code class="p">,</code> <code class="n">embed</code><code class="p">(</code><code class="n">body</code><code class="o">.</code><code class="n">prompt</code><code class="p">)</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mf">0.7</code>
    <code class="p">)</code>
    <code class="n">rag_content_str</code> <code class="o">=</code> <code class="s2">"</code><code class="se">\n</code><code class="s2">"</code><code class="o">.</code><code class="n">join</code><code class="p">(</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO13-3" id="co_achieving_concurrency_in_ai_workloads_CO13-3"><img alt="3" src="assets/3.png"/></a>
        <code class="p">[</code><code class="n">c</code><code class="o">.</code><code class="n">payload</code><code class="p">[</code><code class="s2">"</code><code class="s2">original_text</code><code class="s2">"</code><code class="p">]</code> <code class="k">for</code> <code class="n">c</code> <code class="ow">in</code> <code class="n">rag_content</code><code class="p">]</code>
    <code class="p">)</code>

    <code class="k">return</code> <code class="n">rag_content_str</code>


<code class="c1"># main.py</code>

<code class="o">.</code><code class="o">.</code><code class="o">.</code> <code class="c1"># other imports</code>
<code class="kn">from</code> <code class="nn">dependencies</code> <code class="kn">import</code> <code class="n">get_rag_content</code><code class="p">,</code> <code class="n">get_urls_content</code>

<code class="nd">@app</code><code class="o">.</code><code class="n">post</code><code class="p">(</code><code class="s2">"</code><code class="s2">/generate/text</code><code class="s2">"</code><code class="p">,</code> <code class="n">response_model_exclude_defaults</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="k">async</code> <code class="k">def</code> <code class="nf">serve_text_to_text_controller</code><code class="p">(</code>
    <code class="n">request</code><code class="p">:</code> <code class="n">Request</code><code class="p">,</code>
    <code class="n">body</code><code class="p">:</code> <code class="n">TextModelRequest</code> <code class="o">=</code> <code class="n">Body</code><code class="p">(</code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code class="p">)</code><code class="p">,</code>
    <code class="n">urls_content</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="n">Depends</code><code class="p">(</code><code class="n">get_urls_content</code><code class="p">)</code><code class="p">,</code>
    <code class="n">rag_content</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="n">Depends</code><code class="p">(</code><code class="n">get_rag_content</code><code class="p">)</code><code class="p">,</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO13-4" id="co_achieving_concurrency_in_ai_workloads_CO13-4"><img alt="4" src="assets/4.png"/></a>
<code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="n">TextModelResponse</code><code class="p">:</code>
    <code class="o">.</code><code class="o">.</code><code class="o">.</code> <code class="c1"># Raise HTTPException for invalid models</code>
    <code class="n">prompt</code> <code class="o">=</code> <code class="n">body</code><code class="o">.</code><code class="n">prompt</code> <code class="o">+</code> <code class="s2">"</code> <code class="s2">"</code> <code class="o">+</code> <code class="n">urls_content</code> <code class="o">+</code> <code class="n">rag_content</code>
    <code class="n">output</code> <code class="o">=</code> <code class="n">generate_text</code><code class="p">(</code><code class="n">models</code><code class="p">[</code><code class="s2">"</code><code class="s2">text</code><code class="s2">"</code><code class="p">]</code><code class="p">,</code> <code class="n">prompt</code><code class="p">,</code> <code class="n">body</code><code class="o">.</code><code class="n">temperature</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">TextModelResponse</code><code class="p">(</code><code class="n">content</code><code class="o">=</code><code class="n">output</code><code class="p">,</code> <code class="n">ip</code><code class="o">=</code><code class="n">request</code><code class="o">.</code><code class="n">client</code><code class="o">.</code><code class="n">host</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO13-1" id="callout_achieving_concurrency_in_ai_workloads_CO13-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Create the <code>get_rag_content</code> dependency function for injection into the LLM-serving handler.
This dependency has access to the request <code>body</code> and subsequently the user <code>prompt</code>.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO13-2" id="callout_achieving_concurrency_in_ai_workloads_CO13-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Use the <code>vector_service</code> to search the database for content relevant to the user <code>prompt</code>.
Convert the user <code>prompt</code> to an embedding using the <code>embed</code> function when passing to the <code>vector_service.search</code> function.
Only retrieve the three most relevant items if their cosine similarity score is above <code>0.7</code> (or 70%).</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO13-3" id="callout_achieving_concurrency_in_ai_workloads_CO13-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Merge the text payload of the top three most relevant retrieved items as <code>rag_​con⁠tent_str</code> and return it.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO13-4" id="callout_achieving_concurrency_in_ai_workloads_CO13-4"><img alt="4" src="assets/4.png"/></a></dt>
<dd><p>Inject the results of the <code>get_rag_content</code> dependency function into the LLM handler to augment the final prompt to the LLM with content from the vector database <code>knowledgebase</code>.
The LLM handler can now fetch content of web pages and the RAG vector database.</p></dd>
</dl></div>

<p>If you now visit your browser and upload a PDF document, you should be able to ask questions about it to your LLM.
<a data-type="xref" href="#rag_results">Figure 5-12</a> shows my experiment with the service by uploading a sample of this book in its raw form and asking the LLM to describe who 
<span class="keep-together">I am.</span></p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Depending on the model and size of the inputs, you may observe performance degradations or exceptions like token length limit issues.</p>
</div>

<figure><div class="figure" id="rag_results">
<img alt="bgai 0512" src="assets/bgai_0512.png"/>
<h6><span class="label">Figure 5-12. </span>Leveraging RAG to provide answers in response to user queries</h6>
</div></figure>

<p>Congratulations!
You now have a fully working RAG system enabled by open source models and a vector database.</p>

<p>This longer project served as a hands-on tutorial for learning concepts related to asynchronous programming and I/O operations with the filesystem and a vector database by building a RAG module for your LLM system.
Note that the RAG system we just built together still has many limitations:</p>

<ul>
<li>
<p>Text splitting may split words in half leading to poor retrieval and LLM 
<span class="keep-together">confusion.</span></p>
</li>
<li>
<p>The LLM may still produce hallucinations and inconsistent outputs even with the augmented prompts.</p>
</li>
<li>
<p>The search-and-retrieval system may perform poorly in certain instances.</p>
</li>
<li>
<p>The augmented prompts may exceed the LLM context window.</p>
</li>
<li>
<p>The retrieved information from the database may lack the relevant facts due to an outdated or incomplete knowledge base, ambiguous queries, or poor retrieval algorithm.</p>
</li>
<li>
<p>The retrieved context may not be ordered based on relevance to the user query.</p>
</li>
</ul>

<p>You can work on improving the RAG module further by implementing various other techniques, which I will not cover in this book:</p>

<ul>
<li>
<p>Optimize text splitting, chunk sizing, cleaning and embedding operations.</p>
</li>
<li>
<p>Perform query transformations using the LLM to aid the retrieval and augmentation system via techniques such as prompt compression, chaining, refining, and aggregating, etc.,
to reduce hallucinations and improve LLM performance.</p>
</li>
<li>
<p>Summarize or break down large augmented prompts to feed the context into the models using a sliding window approach.</p>
</li>
<li>
<p>Enhance retrieval algorithms to handle ambiguous queries and implement fallback mechanisms for incomplete data.</p>
</li>
<li>
<p>Enhance the retrieval performance with methods such as <em>maximal marginal relevance</em> (MMR) to enrich the augmentation process with more diverse documents.</p>
</li>
<li>
<p>Implement other advanced RAG techniques like retrieval reranking and filtering, hierarchical database indices, RAG fusion, retrieval augmented thoughts (RAT), etc., to improve the overall generation performance.<a data-startref="ix_ch05-asciidoc21" data-type="indexterm" id="id844"/><a data-startref="ix_ch05-asciidoc20" data-type="indexterm" id="id845"/></p>
</li>
</ul>

<p>I’ll let you research these techniques in more detail and implement them as additional exercises on your own.<a data-startref="ix_ch05-asciidoc6" data-type="indexterm" id="id846"/><a data-startref="ix_ch05-asciidoc5" data-type="indexterm" id="id847"/></p>

<p>In the next section, well review other techniques for optimizing your GenAI services to avoid blocking the server with compute-bound operations such as model 
<span class="keep-together">inference.</span></p>
</div></section>
</div></section>






<section data-pdf-bookmark="Optimizing Model Serving for Memory- and &#10;Compute-Bound AI &#10;Inference Tasks" data-type="sect1"><div class="sect1" id="id246">
<h1>Optimizing Model Serving for Memory- and 
<span class="keep-together">Compute-Bound</span> AI 
<span class="keep-together">Inference Tasks</span></h1>

<p><a data-primary="concurrency" data-secondary="optimizing model serving for memory- and compute-bound AI inference tasks" data-type="indexterm" id="ix_ch05-asciidoc22"/><a data-primary="serving GenAI models" data-secondary="optimizing model serving for memory- and compute-bound AI inference tasks" data-type="indexterm" id="ix_ch05-asciidoc23"/>So far, we’ve looked at optimizing the operations of our service that are I/O bound.
You learned to leverage asynchronous programming to interact with the web, databases, and files by building a web scraper and a RAG module.</p>

<p>Using async tools and techniques, your service remained responsive when interacting with the web, the filesystem, and databases.
However, if you’re self-hosting the model, switching to async programming techniques won’t fully eliminate the long waiting times.
This is because the bottleneck will be model inference operations.</p>








<section data-pdf-bookmark="Compute-Bound Operations" data-type="sect2"><div class="sect2" id="id247">
<h2>Compute-Bound Operations</h2>

<p><a data-primary="concurrency" data-secondary="optimizing model serving for memory- and compute-bound AI inference tasks" data-tertiary="compute-bound operations" data-type="indexterm" id="id848"/><a data-primary="GPUs" data-secondary="compute-bound operations on" data-type="indexterm" id="id849"/>You can speed up the inference by running models on GPUs to massively parallelize computations.
Modern GPUs have staggering compute power measured by the number of
<em>floating-point</em> operations per second (FLOPS),
with modern GPUs reaching teraflops (NVIDIA A100) or petaflops (NVIDIA H100) of compute.
However, despite their significant power and parallelization capabilities, modern GPU cores are often underutilized under concurrent workloads with larger models.</p>

<p>When self-hosting models on GPUs, model parameters are loaded from disk to RAM (I/O bound) and then moved from RAM to the GPU high-bandwidth memory by the CPU (memory bound).
Once model parameters are loaded on the GPU memory, inference is performed (compute bound).</p>

<p>Counterintuitively, model inference for larger GenAI models such as SDXL and LLMs is not I/O- or compute-bound, but rather memory-bound.
This means it takes more time to load 1 MB of data into GPU’s compute cores than it takes for those compute cores to process 1 MB of data.
Inevitably, to maximize the concurrency of your service, you will need to <em>batch</em> the inference requests and fit the largest batch size you can into the GPU high-bandwidth memory.</p>

<p>Therefore, even when using async techniques and latest GPUs, your server can be blocked waiting for billions of model parameters to be loaded to the GPU high-bandwidth memory during each request.
To avoid blocking the server, you can decouple the memory-bound model-serving operations from your FastAPI server by externalizing model serving, as we touched upon in <a data-type="xref" href="ch03.html#ch03">Chapter 3</a>.</p>

<p>Let’s see how to delegate model serving to another process.</p>
</div></section>








<section data-pdf-bookmark="Externalizing Model Serving" data-type="sect2"><div class="sect2" id="id84">
<h2>Externalizing Model Serving</h2>

<p><a data-primary="concurrency" data-secondary="optimizing model serving for memory- and compute-bound AI inference tasks" data-tertiary="externalizing model serving" data-type="indexterm" id="ix_ch05-asciidoc24"/><a data-primary="external model serving" data-type="indexterm" id="ix_ch05-asciidoc25"/><a data-primary="serving GenAI models" data-secondary="external serving" data-type="indexterm" id="ix_ch05-asciidoc26"/>You have several options available to you when externalizing your model-serving workloads.
You can either host models on another FastAPI server or use specialized model inference servers.</p>

<p>Specialized inference servers support only a limited set of GenAI model architectures.
However, if your model architecture is supported, you will save a lot of time not having to implement inference optimizations yourself.
For instance, if you need to self-host LLMs, LLM-serving frameworks can perform several inference optimizations for you such as batch processing, tensor parallelism, quantization, caching, streaming outputs, GPU memory management, etc.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id850">
<h1>Model Optimization Techniques</h1>
<p><a data-primary="model optimization" data-type="indexterm" id="id851"/>You have several model compression techniques you can try to improve inference performance:</p>

<ul>
<li>
<p><em>Quantization</em> to compress models</p>
</li>
<li>
<p><em>Pruning</em> to summarize model parameters</p>
</li>
<li>
<p><em>Distillation</em> to create “student” models significantly smaller than their “teacher” counterparts</p>
</li>
<li>
<p><em>Fine-tuning</em> small models to specialize them for your use case</p>
</li>
</ul>

<p>If you’re serving transformer-based models, you can further optimize model inference using the following techniques:</p>

<ul>
<li>
<p><em>Fast attention</em> to optimize attention map calculations on GPUs</p>
</li>
<li>
<p><em>KV caching</em> to leverage in-memory caching techniques to speed up inference by reusing the results of computed attention maps</p>
</li>
<li>
<p><em>Paged attention</em> to optimize KV cache memory usage after attention computation</p>
</li>
<li>
<p><em>Request batching</em>, including simple and continuous batching, to maximize GPU utilization rates</p>
</li>
</ul>

<p><a data-type="xref" href="ch10.html#ch10">Chapter 10</a> will delve into more details of model optimization techniques.</p>
</div></aside>

<p><a data-primary="vLLM" data-type="indexterm" id="ix_ch05-asciidoc27"/>Since we’ve been mostly working with LLMs in this chapter, I will show you how to integrate vLLM, an open source LLM server that can start a FastAPI server for you matching the OpenAI API specification.
vLLM also has seamless integration with popular open source Hugging Face model architectures including GPT, Llama, Gemma, Mistral, Falcon, etc.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>At the time of writing, other LLM hosting servers you can use include NVIDIA Triton Inference Server, Ray Serve, Hugging Face Inference, and OpenLLM, among others.</p>

<p>There are features, benefits, and drawbacks to using each including the supported model architectures.
I recommend researching these servers prior to adopting them in your own use cases.</p>
</div>

<p>You can start your own vLLM FastAPI server via a single command, as shown in <a data-type="xref" href="#vllm">Example 5-15</a>.
To run the code in <a data-type="xref" href="#vllm">Example 5-15</a>, you will need to install <code>vllm</code> using:</p>

<pre data-code-language="bash" data-type="programlisting">$ pip install vllm<code class="w"/></pre>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>At the time of writing, vLLM only supports Linux platforms (including WSL) with NVIDIA-compatible GPUs to run CUDA toolkit dependencies.
Unfortunately, you can’t install vLLM on Mac or Windows machines for local testing.</p>

<p>vLLM is designed for production inference workloads on NVIDIA GPUs in Linux environments where the server can delegate requests to multiple GPU cores via <em>tensor parallelism</em>.
It does also support distributed computing when scaling services beyond a single machine via its Ray Serve dependency.</p>

<p>Please consult vLLM documentation for more details related to distributed inference and serving.</p>
</div>
<div data-type="example" id="vllm">
<h5><span class="label">Example 5-15. </span>Starting the vLLM FastAPI OpenAI API server for TinyLlama on a Linux machine with 4x 16 GB NVIDIA T4 GPUs</h5>

<pre data-code-language="bash" data-type="programlisting"><code>$</code> <code>python</code> <code>-m</code> <code>vllm.entrypoints.openai.api_server</code> <code class="se">\ </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO14-1" id="co_achieving_concurrency_in_ai_workloads_CO14-1"><img alt="1" src="assets/1.png"/></a>
<code>--model</code> <code class="s2">"TinyLlama/TinyLlama-1.1B-Chat-v1.0"</code> <code class="se">\
</code><code>--dtype</code> <code>float16</code> <code class="se">\ </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO14-2" id="co_achieving_concurrency_in_ai_workloads_CO14-2"><img alt="2" src="assets/2.png"/></a>
<code>--tensor-parallel-size</code> <code class="m">4</code> <code class="se">\ </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO14-3" id="co_achieving_concurrency_in_ai_workloads_CO14-3"><img alt="3" src="assets/3.png"/></a>
<code>--api-key</code> <code class="s2">"your_secret_token"</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO14-4" id="co_achieving_concurrency_in_ai_workloads_CO14-4"><img alt="4" src="assets/4.png"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO14-1" id="callout_achieving_concurrency_in_ai_workloads_CO14-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Start an OpenAI-compatible API server with FastAPI to serve the TinyLlama model.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO14-2" id="callout_achieving_concurrency_in_ai_workloads_CO14-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Use the <code>float16</code> medium precision data type.
<code>float16</code> is compatible with GPU hardware, whereas <code>bfloat16</code> is generally compatible with CPU hardware.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO14-3" id="callout_achieving_concurrency_in_ai_workloads_CO14-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Leverage vLLM tensor parallelism feature to run the API server on four GPUs.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO14-4" id="callout_achieving_concurrency_in_ai_workloads_CO14-4"><img alt="4" src="assets/4.png"/></a></dt>
<dd><p>Set a secret token for basic authentication to secure the LLM server.
This is useful for secure machine-to-machine communication, for instance, to directly communicate with your current FastAPI service.</p></dd>
</dl></div>

<p>With the vLLM FastAPI server up and running, you can now replace the model-serving logic in your current service with network calls to the vLLM server.
Refer to <a data-type="xref" href="#vllm_fastapi_text_generation">Example 5-16</a> for implementation details.</p>
<div data-type="example" id="vllm_fastapi_text_generation">
<h5><span class="label">Example 5-16. </span>Replace model serving with asynchronous API calls to the new vLLM server</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># models.py</code>

<code class="kn">import</code> <code class="nn">os</code>
<code class="kn">import</code> <code class="nn">aiohttp</code>
<code class="kn">from</code> <code class="nn">loguru</code> <code class="kn">import</code> <code class="n">logger</code>

<code class="k">async</code> <code class="k">def</code> <code class="nf">generate_text</code><code class="p">(</code><code class="n">prompt</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code> <code class="n">temperature</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="mf">0.7</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="nb">str</code><code class="p">:</code>
    <code class="n">system_prompt</code> <code class="o">=</code> <code class="s2">"</code><code class="s2">You are an AI assistant</code><code class="s2">"</code>
    <code class="n">messages</code> <code class="o">=</code> <code class="p">[</code>
        <code class="p">{</code><code class="s2">"</code><code class="s2">role</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">system</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="s2">content</code><code class="s2">"</code><code class="p">:</code> <code class="n">system_prompt</code><code class="p">}</code><code class="p">,</code>
        <code class="p">{</code><code class="s2">"</code><code class="s2">role</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">user</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"</code><code class="s2">content</code><code class="s2">"</code><code class="p">:</code> <code class="n">prompt</code><code class="p">}</code><code class="p">,</code>
    <code class="p">]</code>
    <code class="n">data</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"</code><code class="s2">temperature</code><code class="s2">"</code><code class="p">:</code> <code class="n">temperature</code><code class="p">,</code> <code class="s2">"</code><code class="s2">messages</code><code class="s2">"</code><code class="p">:</code> <code class="n">messages</code><code class="p">}</code>
    <code class="n">headers</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"</code><code class="s2">Authorization</code><code class="s2">"</code><code class="p">:</code> <code class="sa">f</code><code class="s2">"</code><code class="s2">Bearer </code><code class="si">{</code><code class="n">os</code><code class="o">.</code><code class="n">environ</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s1">'</code><code class="s1">VLLM_API_KEY</code><code class="s1">'</code><code class="p">)</code><code class="si">}</code><code class="s2">"</code><code class="p">}</code>
<code class="k">try</code><code class="p">:</code>
   <code class="k">async</code> <code class="k">with</code> <code class="n">aiohttp</code><code class="o">.</code><code class="n">ClientSession</code><code class="p">(</code><code class="p">)</code> <code class="k">as</code> <code class="n">session</code><code class="p">:</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO15-1" id="co_achieving_concurrency_in_ai_workloads_CO15-1"><img alt="1" src="assets/1.png"/></a>
        <code class="n">response</code> <code class="o">=</code> <code class="k">await</code> <code class="n">session</code><code class="o">.</code><code class="n">post</code><code class="p">(</code>
            <code class="s2">"</code><code class="s2">http://localhost:8000/v1/chat</code><code class="s2">"</code><code class="p">,</code> <code class="n">json</code><code class="o">=</code><code class="n">data</code><code class="p">,</code> <code class="n">headers</code><code class="o">=</code><code class="n">headers</code>
        <code class="p">)</code>
        <code class="n">predictions</code> <code class="o">=</code> <code class="k">await</code> <code class="n">response</code><code class="o">.</code><code class="n">json</code><code class="p">(</code><code class="p">)</code>
<code class="k">except</code> <code class="ne">Exception</code> <code class="k">as</code> <code class="n">e</code><code class="p">:</code>
    <code class="n">logger</code><code class="o">.</code><code class="n">error</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="s2">Failed to obtain predictions from vLLM - Error: </code><code class="si">{</code><code class="n">e</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>
    <code class="k">return</code> <code class="p">(</code>
        <code class="s2">"</code><code class="s2">Failed to obtain predictions from vLLM - </code><code class="s2">"</code>
        <code class="s2">"</code><code class="s2">See server logs for more details</code><code class="s2">"</code>
    <code class="p">)</code>
<code class="k">try</code><code class="p">:</code>
    <code class="n">output</code> <code class="o">=</code> <code class="n">predictions</code><code class="p">[</code><code class="s2">"</code><code class="s2">choices</code><code class="s2">"</code><code class="p">]</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="p">[</code><code class="s2">"</code><code class="s2">message</code><code class="s2">"</code><code class="p">]</code><code class="p">[</code><code class="s2">"</code><code class="s2">content</code><code class="s2">"</code><code class="p">]</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO15-2" id="co_achieving_concurrency_in_ai_workloads_CO15-2"><img alt="2" src="assets/2.png"/></a>
    <code class="n">logger</code><code class="o">.</code><code class="n">debug</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="s2">Generated text: </code><code class="si">{</code><code class="n">output</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">output</code>
<code class="k">except</code> <code class="ne">KeyError</code> <code class="k">as</code> <code class="n">e</code><code class="p">:</code>
    <code class="n">logger</code><code class="o">.</code><code class="n">error</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="s2">Failed to parse predictions from vLLM - Error: </code><code class="si">{</code><code class="n">e</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>
    <code class="k">return</code> <code class="p">(</code>
        <code class="s2">"</code><code class="s2">Failed to parse predictions from vLLM - </code><code class="s2">"</code>
        <code class="s2">"</code><code class="s2">See server logs for more details</code><code class="s2">"</code>
    <code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO15-1" id="callout_achieving_concurrency_in_ai_workloads_CO15-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Use <code>aiohttp</code> to create an asynchronous session for sending <code>POST</code> requests to the vLLM FastAPI server.
This logic replaces the Hugging Face model pipeline inference logic on the current FastAPI server.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO15-2" id="callout_achieving_concurrency_in_ai_workloads_CO15-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Since the vLLM server is OpenAI compatible, you can access the output content by following the OpenAI API specification.</p></dd>
</dl></div>

<p>Next, remove the code related to the FastAPI lifespan so that your current service won’t load the TinyLlama model.
You can achieve this by following the code in <a data-type="xref" href="#vllm_fastapi_handler">Example 5-17</a>.</p>
<div data-type="example" id="vllm_fastapi_handler">
<h5><span class="label">Example 5-17. </span>Remove the FastAPI lifespan and update the text generation handler to be asynchronous</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># main.py</code>

<code class="kn">from</code> <code class="nn">fastapi</code> <code class="kn">import</code> <code class="n">FastAPI</code><code class="p">,</code> <code class="n">Request</code>
<code class="kn">from</code> <code class="nn">schemas</code> <code class="kn">import</code> <code class="n">TextModelRequest</code><code class="p">,</code> <code class="n">TextModelResponse</code>
<code class="kn">from</code> <code class="nn">models</code> <code class="kn">import</code> <code class="n">generate_text</code>

<code class="c1"># Remove the asynccontextmanager to remove TinyLlama from FastAPI </code><a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO16-1" id="co_achieving_concurrency_in_ai_workloads_CO16-1"><img alt="1" src="assets/1.png"/></a>
<code class="c1"># @asynccontextmanager</code>
<code class="c1"># async def lifespan(app: FastAPI):</code>
<code class="c1">#     models["text"] = load_text_model()</code>
<code class="c1">#     yield</code>
<code class="c1">#     models.clear()</code>

<code class="c1"># Remove the `lifespan` argument from `FastAPI()`</code>
<code class="n">app</code> <code class="o">=</code> <code class="n">FastAPI</code><code class="p">(</code><code class="p">)</code>

<code class="nd">@app</code><code class="o">.</code><code class="n">post</code><code class="p">(</code><code class="s2">"</code><code class="s2">/generate/text</code><code class="s2">"</code><code class="p">)</code>
<code class="k">async</code> <code class="k">def</code> <code class="nf">serve_text_to_text_controller</code><code class="p">(</code>
    <code class="n">request</code><code class="p">:</code> <code class="n">Request</code><code class="p">,</code> <code class="n">body</code><code class="p">:</code> <code class="n">TextModelRequest</code>
<code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="n">TextModelResponse</code><code class="p">:</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO16-2" id="co_achieving_concurrency_in_ai_workloads_CO16-2"><img alt="2" src="assets/2.png"/></a>
    <code class="o">.</code><code class="o">.</code><code class="o">.</code>  <code class="c1"># controller logic</code>
    <code class="n">output</code> <code class="o">=</code> <code class="k">await</code> <code class="n">generate_text</code><code class="p">(</code><code class="n">body</code><code class="o">.</code><code class="n">prompt</code><code class="p">,</code> <code class="n">body</code><code class="o">.</code><code class="n">temperature</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">TextModelResponse</code><code class="p">(</code><code class="n">content</code><code class="o">=</code><code class="n">output</code><code class="p">,</code> <code class="n">ip</code><code class="o">=</code><code class="n">request</code><code class="o">.</code><code class="n">client</code><code class="o">.</code><code class="n">host</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO16-1" id="callout_achieving_concurrency_in_ai_workloads_CO16-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>There is no need to use FastAPI <code>lifespan</code> anymore since the model is now served by an external vLLM FastAPI server.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO16-2" id="callout_achieving_concurrency_in_ai_workloads_CO16-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Make <code>serve_text_to_text_controller</code> an async route handler as it is now performing I/O operations to the vLLM server.
It is no longer running synchronous compute-bound model inference operations as those are delegated to the vLLM server to manage.</p></dd>
</dl></div>
<p class="fix_tracking">
Congratulations, you’ve now achieved concurrency with your AI inference workloads.
You implemented a form of multiprocessing on a single machine by moving your LLM inference workloads to another server.
Both servers are now running on separate cores with your LLM server delegating work to multiple GPU cores, leveraging parallelism.
This means your main server is now able to process multiple incoming requests and do other tasks than processing one LLM inference operation at a time.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Bear in mind that any concurrency you’ve achieved so far has been limited to a single machine.</p>

<p>To support more concurrent users, you may need more machines with CPU and GPU cores.
At that point, distributed computing frameworks like Ray Serve
and Kubernetes can help to scale and orchestrate your services beyond a single worker machine using parallelism.</p>
</div>

<p>Before integrating vLLM, you would experience long waiting times between requests because your main server was too busy running inference operations.
With vLLM, there is now a massive reduction in latency and increase in throughput of your LLM service.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id852">
<h1>Latency and Throughput</h1>
<p><a data-primary="large language models (LLMs)" data-secondary="latency and throughput" data-type="indexterm" id="id853"/><a data-primary="latency" data-type="indexterm" id="id854"/><em>Latency</em> in the context of LLMs refers to the time taken from when a request is made to the model until the first response is received.
It’s a measure of the delay experienced during the processing of a single request.
<a data-primary="throughput" data-type="indexterm" id="id855"/><em>Throughput</em>, on the other hand, is the number of requests that an LLM can process within a given time frame and indicates the server’s capacity to handle concurrent or sequential requests over time.
Latency can be measured in <em>delay seconds</em> and throughput in <em>tokens per minute</em> (TPM).</p>

<p>As a developer, you will want your service to have the lowest latency and the highest throughput possible.
However, there is a trade-off between the model size and quality versus these two metrics.
Normally, LLMs with larger number of parameters achieve higher quality but also increased latency and reduced throughput.</p>

<p>Research is currently underway to use model compression techniques such as
<em>distillation</em>, <em>quantization</em>, and <em>pruning</em> to keep language models small while maintaining high quality, throughput, and small latency in AI inference services.</p>
</div></aside>

<p>In addition to model compression mechanisms like quantization, vLLM uses other optimization techniques including continuous request batching,
cache partitioning (paged attention), reduced GPU memory footprint via memory sharing,
and streaming outputs to achieve smaller latency and high throughput.<a data-startref="ix_ch05-asciidoc27" data-type="indexterm" id="id856"/></p>

<p>Let’s look at both the request batching and paged attention mechanisms in more detail to understand how to further optimize LLM inference.</p>










<section class="less_space pagebreak-before" data-pdf-bookmark="Request batching and continuous batching" data-type="sect3"><div class="sect3" id="id85">
<h3>Request batching and continuous batching</h3>

<p><a data-primary="serving GenAI models" data-secondary="external serving" data-tertiary="request batching and continuous batching" data-type="indexterm" id="ix_ch05-asciidoc28"/>As we discussed in <a data-type="xref" href="ch03.html#ch03">Chapter 3</a>, LLMs produce the next token prediction in an autoregressive manner, as you can see in <a data-type="xref" href="#autoregressive_prediction5">Figure 5-13</a>.</p>

<figure><div class="figure" id="autoregressive_prediction5">
<img alt="bgai 0513" src="assets/bgai_0513.png"/>
<h6><span class="label">Figure 5-13. </span>Autoregressive prediction</h6>
</div></figure>

<p>This means the LLMs must perform several inference iterations in a loop to produce a response, and each iteration produces a single output token.
The input sequence grows as each iteration’s output token is appended to the end, and the new sequence is forwarded to the model in the next iteration step.
Once the model generates an end-of-sequence token, the generation loop stops.
Essentially, the LLM produces a sequence of completion tokens, stopping only after producing a stop token or reaching a maximum sequence length.</p>

<p>The LLM must calculate several attention maps for each token in the sequence so that it can iteratively make the next token predictions.</p>

<p>Fortunately, GPUs can parallelize the attention map calculations for each iteration.
As you learned, these attention maps are capturing the meaning and context of each token within the input sequence and are expensive to calculate.
<a data-primary="key-value (KV) caching" data-type="indexterm" id="id857"/>Therefore, to optimize inference, LLMs use <em>key-value</em> (KV) <em>caching</em> to store calculated maps in the GPU memory.</p>
<div data-type="tip"><h6>Tip</h6>
<p>The attention map formula computes a <em>value (V)</em> based on a given <em>query (Q)</em>
and a <em>key (K)</em>.</p>
<blockquote>
<p>Q = KV</p></blockquote>

<p>This calculation has to be done for each token in the sequence but luckily can be vectorized using large matrix multiplication operations on a GPU.</p>
</div>

<p>However, storing parameters on the GPU memory for reuse between iterations can consume huge chunks of GPU memory.
For instance, a 13B-parameter model consumes nearly 1 MB of state for each token in a sequence on top of all those 13B model parameters.
This means there is a limited number of tokens you can store in memory for reuse.</p>

<p>If you’re using a higher-end GPU, such as the A100 with 40 GB RAM, you can only hold 14 K tokens in memory at once, while the rest of the memory is used up for storing 26 GB of model parameters.
In short, the GPU memory consumed scales with the base model size plus the length of the token sequence.</p>

<p>To make matters worse, if you need to serve multiple users concurrently by batching requests, your GPU memory has to be shared between multiple LLM inferences.
As a result, you have less memory to store longer sequences, and your LLM is constrained to a shorter context window.
On the other hand, if you want to maintain a large context window, then you can’t handle more concurrent users.
As an example, a sequence length of 2048 means that your batch size will be limited to 7 concurrent requests (or 7 prompt sequences).
Realistically, this is an upper-bound limit and doesn’t leave room for storing intermediate computations, which will reduce the aforementioned numbers even further.</p>

<p>What this all means is that LLMs are failing to fully saturate the GPU’s available resources.
The primary reason is that a significant portion of the GPU’s memory bandwidth is consumed in loading the model parameters instead of processing inputs.</p>

<p>The first step to reduce the load on your services is to integrate the most efficient models.
Often, smaller and more compressed models could do the job you’re asking of them, with a similar performance to their larger counterparts.</p>

<p><a data-primary="request batching" data-type="indexterm" id="id858"/>Another suitable solution to the GPU underutilization problem is to implement
<em>request batching</em>
where the model processes multiple inputs in groups, reducing the overhead of loading model parameters for each request.
This is more efficient in using the chip’s memory bandwidth, leading to higher compute utilization, higher throughput, and less expensive LLM inference.
LLM inference servers like vLLM take advantage of batching plus fast attention, KV caching, and paged attention mechanisms to maximize throughput.</p>

<p>You can see the difference of response latency and throughput with and without batching in <a data-type="xref" href="#with_without_batching">Figure 5-14</a>.</p>

<figure><div class="figure" id="with_without_batching">
<img alt="bgai 0514" src="assets/bgai_0514.png"/>
<h6><span class="label">Figure 5-14. </span>LLM server response latency and throughput with and without batching</h6>
</div></figure>

<p>There are two ways to implement batching:</p>
<dl>
<dt>Static batching</dt>
<dd>
<p>The size of the batch remains constant.</p>
</dd>
<dt>Dynamic or continuous batching</dt>
<dd>
<p>The size of batch is determined based on demand.</p>
</dd>
</dl>

<p><a data-primary="static batching" data-type="indexterm" id="id859"/>In <em>static batching</em>, we wait for a predetermined number of incoming requests to arrive before we batch and process them through the model.
However, since requests can finish at any time in a batch, we’re effectively delaying responses to every request—​and increasing latency—​until the whole batch is processed.</p>

<p>Releasing the GPU resource can also be tricky when processing a batch and adding new requests to the batch that may be at different completion states.
As a result, the GPU remains underutilized as the generated sequences within a batch vary and don’t match the length of the longest sequence in that batch.</p>

<p><a data-type="xref" href="#static_batching">Figure 5-15</a> illustrates static batching in the context of LLM inference.</p>

<figure><div class="figure" id="static_batching">
<img alt="bgai 0515" src="assets/bgai_0515.png"/>
<h6><span class="label">Figure 5-15. </span>Static batching with fixed batch size</h6>
</div></figure>

<p>In <a data-type="xref" href="#static_batching">Figure 5-15</a> you will notice the white blocks representing underutilized GPU computation time.
Only one input sequence in the batch saturated the GPU across the batch’s processing timeline.</p>

<p>Aside from adding unnecessary waiting times and not saturating the GPU utilization, what makes static batching problematic is that users of an LLM-powered chatbot service won’t be providing fixed-length prompts or expect fixed-length outputs.
The variance in generation outputs could cause massive underutilization of GPUs.</p>

<p>A solution is to avoid assuming fixed input or output sequences and instead set dynamic batch sizes during the processing of a batch.
<a data-primary="continuous (dynamic) batching" data-type="indexterm" id="id860"/><a data-primary="dynamic (continuous) batching" data-type="indexterm" id="id861"/>In <em>dynamic</em> or <em>continuous batching</em>, the size of batch can be set based on the incoming request sequence length and the available GPU resource.
With this approach, new generation requests can be inserted in a batch by replacing completed requests to yield higher GPU utilization than static batching.</p>

<p><a data-type="xref" href="#dynamic_batching">Figure 5-16</a> shows how dynamic or continuous batching can fully saturate the GPU resource.</p>

<figure><div class="figure" id="dynamic_batching">
<img alt="bgai 0516" src="assets/bgai_0516.png"/>
<h6><span class="label">Figure 5-16. </span>Dynamic/continuous batching with variable batch size</h6>
</div></figure>

<p>While the model parameters are loaded, requests can keep flowing in, and the LLM inference server schedules and insert them into the batch to maximize GPU usage.
This approach leads to higher throughput and reduced latency.</p>

<p>If you’re building a LLM inference server, you will probably want to bake in the continuous batching mechanism into your server.
However, the good news is that the vLLM server already provides continuous batching out of the box with its FastAPI server, so you don’t have to implement all of that yourself.
Additionally, it also ships with another important GPU optimization feature, which sets it apart from other alternative LLM inference frameworks: paged attention.<a data-startref="ix_ch05-asciidoc28" data-type="indexterm" id="id862"/></p>
</div></section>










<section data-pdf-bookmark="Paged attention" data-type="sect3"><div class="sect3" id="id86">
<h3>Paged attention</h3>

<p><a data-primary="paged attention" data-type="indexterm" id="ix_ch05-asciidoc29"/><a data-primary="serving GenAI models" data-secondary="external serving" data-tertiary="paged attention" data-type="indexterm" id="ix_ch05-asciidoc30"/>Efficient memory usage is a critical challenge for systems that handle high-throughput serving, particularly for LLMs.
For faster inference, today’s models rely on <em>KV caches</em> to store and reuse attention maps, which grow exponentially as input sequence lengths increase.</p>

<p><em>Paged attention</em> is a novel solution designed to minimize the memory demands of these KV caches, subsequently enhancing the memory efficiency of LLMs and making them more viable for use on devices with limited resources.
In transformer-based LLMs, attention key and value tensors are generated for each input token to capture essential context.
Instead of recalculating these tensors at every step, they’re saved in the GPU memory as a KV cache, which serves as the model’s memory.
However, the KV cache can grow to enormous sizes, such as 40 GB for a model with 13B parameters, posing a significant challenge for efficient storage and access, particularly on hardware with constrained resources.</p>

<p>Paged attention introduces a method that breaks down the KV cache into smaller, more manageable segments called <em>pages</em>, each holding a KV vector for a set number of tokens.
With this segmentation, paged attention can efficiently load and access KV caches during the attention computations.
You can compare this technique to how the virtual memory is managed by operating systems, where the logical arrangement of data is separated from its physical storage.
Essentially, a block table maps the logical blocks to physical ones, allowing for dynamic allocation of memory as new tokens are processed.
The core idea is to avoid memory fragmentation by leveraging logical blocks (instead of physical ones) and use a mapping table to quickly access data stored in a paged physical memory.</p>

<p>You can break down the paged attention mechanism into several steps:</p>
<dl>
<dt>Partitioning the KV cache</dt>
<dd>
<p>The cache is split into fixed-size pages, with each containing a portion of the key-value pairs.</p>
</dd>
<dt>Building the lookup table</dt>
<dd>
<p>A table is created to map query keys to their corresponding pages, facilitating quick allocation and retrieval.</p>
</dd>
<dt>Selective loading</dt>
<dd>
<p>Only the necessary pages for the current input sequence are loaded during inference, reducing the memory footprint.</p>
</dd>
<dt>Attention computation</dt>
<dd>
<p>The model computes attention using the key-value pairs from the loaded pages.
This approach aims to make LLMs more accessible by addressing the memory bottleneck, potentially enabling their deployment on a wider range of devices.</p>
</dd>
</dl>

<p>The aforementioned steps enable the vLLM server to maximize memory usage efficiency through the mapping of physical and logical memory blocks so that the KV cache is efficiently stored and retrieved during generation.</p>

<p>In a <a href="https://oreil.ly/WgRfJ">blog post published on Anyscale.com</a>, the authors have researched and compared the performance of various LLM-serving frameworks during inference.
The authors concluded that leveraging both paged attention and continuous batching mechanisms are so powerful in optimizing GPU memory usage that the vLLM server was able to reduce latencies by 4 times and throughput by up to 23 times.</p>

<p class="less_space pagebreak-before">In the next section, we will turn our attention to GenAI workloads that can take a long time to process and are compute-bound.
This is mostly the case with large non-LLM models such as SDXL where performing batch inferences (such as batch image generation) for multiple users may prove <a data-startref="ix_ch05-asciidoc30" data-type="indexterm" id="id863"/><a data-startref="ix_ch05-asciidoc29" data-type="indexterm" id="id864"/>challenging<a data-startref="ix_ch05-asciidoc26" data-type="indexterm" id="id865"/><a data-startref="ix_ch05-asciidoc25" data-type="indexterm" id="id866"/><a data-startref="ix_ch05-asciidoc24" data-type="indexterm" id="id867"/>.<a data-startref="ix_ch05-asciidoc23" data-type="indexterm" id="id868"/><a data-startref="ix_ch05-asciidoc22" data-type="indexterm" id="id869"/></p>
</div></section>
</div></section>
</div></section>






<section data-pdf-bookmark="Managing Long-Running AI Inference Tasks" data-type="sect1"><div class="sect1" id="id87">
<h1>Managing Long-Running AI Inference Tasks</h1>

<p><a data-primary="concurrency" data-secondary="managing long-running AI inference tasks" data-type="indexterm" id="ix_ch05-asciidoc31"/>With the ability to host models in a separate process outside the FastAPI event loop, you can turn your attention to blocking operations that take a long time to complete.</p>

<p>In the previous section, you leveraged specialized frameworks such as vLLM to externally host and optimize the inference workloads of your LLMs.
However, you may still run into models that can take significant time to generate results.
To prevent your users from waiting, you should manage tasks that generate models and take a long time to complete.</p>

<p>Several GenAI models such as Stable Diffusion XL may take several minutes, even on a GPU, to produce results.
In most cases, you can ask your users to wait until the generation process is complete.
But if users are using a single model simultaneously, the server will have to queue these requests.
When your users work with generative models, they need to interact with it several times to guide the model to the results they want.
This usage pattern creates a large backlog of requests, and users at the end of the queue will have to wait a long time before they see any results.</p>

<p>If there was a way to handle long-running tasks without making the users wait, that would be perfect.
Luckily, FastAPI provides a mechanism for solving these kinds of problems.</p>

<p><a data-primary="background tasks (FastAPI mechanism)" data-type="indexterm" id="ix_ch05-asciidoc32"/>FastAPI’s <em>background tasks</em> is a mechanism you can leverage to respond to users while your models are busy processing the request.
You’ve been briefly introduced to this feature while building the RAG module where a background task was populating a vector database with the content of the uploaded PDF documents.</p>

<p>Using background tasks, your users can continue sending requests or carry on with their day without having to wait.
You can either save the results to disk or a database for later retrieval or provide a polling system so that their client can ping for updates as the model processes the requests.
Another option is to create a live connection between the client and the server so that their UI is updated with the results as soon as it becomes available.
All these solutions are doable with FastAPI’s background tasks.</p>

<p><a data-type="xref" href="#fastapi_background_tasks">Example 5-18</a> shows how to implement background tasks to handle long-running model inferences.</p>
<div data-type="example" id="fastapi_background_tasks">
<h5><span class="label">Example 5-18. </span>Using background tasks to handle long-running model inference (e.g., batch generating images)</h5>

<pre data-code-language="python" data-type="programlisting"><code class="c1"># main.py</code>

<code class="kn">from</code> <code class="nn">fastapi</code> <code class="kn">import</code> <code class="n">BackgroundTasks</code>
<code class="kn">import</code> <code class="nn">aiofiles</code>

<code class="o">.</code><code class="o">.</code><code class="o">.</code>

<code class="k">async</code> <code class="k">def</code> <code class="nf">batch_generate_image</code><code class="p">(</code><code class="n">prompt</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code> <code class="n">count</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-</code><code class="o">&gt;</code> <code class="kc">None</code><code class="p">:</code>
    <code class="n">images</code> <code class="o">=</code> <code class="n">generate_images</code><code class="p">(</code><code class="n">prompt</code><code class="p">,</code> <code class="n">count</code><code class="p">)</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO17-1" id="co_achieving_concurrency_in_ai_workloads_CO17-1"><img alt="1" src="assets/1.png"/></a>
    <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">image</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">images</code><code class="p">)</code><code class="p">:</code>
        <code class="k">async</code> <code class="k">with</code> <code class="n">aiofiles</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="s2">output_</code><code class="si">{</code><code class="n">i</code><code class="si">}</code><code class="s2">.png</code><code class="s2">"</code><code class="p">,</code> <code class="n">mode</code><code class="o">=</code><code class="s1">'</code><code class="s1">wb</code><code class="s1">'</code><code class="p">)</code> <code class="k">as</code> <code class="n">f</code><code class="p">:</code>
            <code class="k">await</code> <code class="n">f</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="n">image</code><code class="p">)</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO17-2" id="co_achieving_concurrency_in_ai_workloads_CO17-2"><img alt="2" src="assets/2.png"/></a>

<code class="nd">@app</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"</code><code class="s2">/generate/image/background</code><code class="s2">"</code><code class="p">)</code>
<code class="k">def</code> <code class="nf">serve_image_model_background_controller</code><code class="p">(</code>
    <code class="n">background_tasks</code><code class="p">:</code> <code class="n">BackgroundTasks</code><code class="p">,</code> <code class="n">prompt</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code> <code class="n">count</code><code class="p">:</code> <code class="nb">int</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO17-3" id="co_achieving_concurrency_in_ai_workloads_CO17-3"><img alt="3" src="assets/3.png"/></a>
<code class="p">)</code><code class="p">:</code>
    <code class="n">background_tasks</code><code class="o">.</code><code class="n">add_task</code><code class="p">(</code><code class="n">batch_generate_image</code><code class="p">,</code> <code class="n">prompt</code><code class="p">,</code> <code class="n">count</code><code class="p">)</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO17-4" id="co_achieving_concurrency_in_ai_workloads_CO17-4"><img alt="4" src="assets/4.png"/></a>
    <code class="k">return</code> <code class="p">{</code><code class="s2">"</code><code class="s2">message</code><code class="s2">"</code><code class="p">:</code> <code class="s2">"</code><code class="s2">Task is being processed in the background</code><code class="s2">"</code><code class="p">}</code> <a class="co" href="#callout_achieving_concurrency_in_ai_workloads_CO17-5" id="co_achieving_concurrency_in_ai_workloads_CO17-5"><img alt="5" src="assets/5.png"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO17-1" id="callout_achieving_concurrency_in_ai_workloads_CO17-1"><img alt="1" src="assets/1.png"/></a></dt>
<dd><p>Generate multiple images in a batch using an external model-serving API like <a href="https://oreil.ly/NjlV4">Ray Serve</a>.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO17-2" id="callout_achieving_concurrency_in_ai_workloads_CO17-2"><img alt="2" src="assets/2.png"/></a></dt>
<dd><p>Loop over the generated images and asynchronously save each to disk using the <code>aiofiles</code> library.
In production, you can also save output images to cloud storage solutions that clients can directly fetch from.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO17-3" id="callout_achieving_concurrency_in_ai_workloads_CO17-3"><img alt="3" src="assets/3.png"/></a></dt>
<dd><p>Enable the controller to perform background tasks.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO17-4" id="callout_achieving_concurrency_in_ai_workloads_CO17-4"><img alt="4" src="assets/4.png"/></a></dt>
<dd><p>Pass the <code>batch_generate_image</code> function definition to a FastAPI background tasks handler with the required arguments.</p></dd>
<dt><a class="co" href="#co_achieving_concurrency_in_ai_workloads_CO17-5" id="callout_achieving_concurrency_in_ai_workloads_CO17-5"><img alt="5" src="assets/5.png"/></a></dt>
<dd><p>Return a generic success message to the client before processing the background task so that the user is not kept waiting.</p></dd>
</dl></div>

<p>In <a data-type="xref" href="#fastapi_background_tasks">Example 5-18</a>, you’re allowing FastAPI to run inference operations in the background (via an external model server API) such that the event loop remains unblocked to process other incoming requests.
You can even run multiple tasks in the background, such as generating images in batches (in separate processes) and sending notification emails.
These tasks are added to a queue and processed sequentially without blocking the user. You can then store the generated images and expose an additional endpoint that clients can use to poll for status updates and to retrieve the inference results.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Background tasks run in the same event loop.
They won’t provide true parallelism; they only provide concurrency.</p>

<p>If you run heavy CPU-bound operations like AI inference in 
<span class="keep-together">background</span> tasks, it’ll block the main event loop until all background tasks are completed.
Similarly, be careful with async 
<span class="keep-together">background</span> tasks.
If you don’t await the blocking I/O operations, the task will block the main server from responding to other requests, even if it runs in the background.
FastAPI runs nonasync background tasks in an internal thread pool.</p>
</div>

<p>While FastAPI’s background tasks are a wonderful tool for handling simple batch jobs, it doesn’t scale and can’t handle exceptions or retries as well as specialized tools.
Other ML-serving frameworks like Ray Serve, BentoML, and vLLM may handle model serving better at scale by providing features such as request batching.
More sophisticated tools like Celery (a queue manager), Redis (a caching database), and RabbitMQ (a message broker) can also be used in combination to implement a more robust and reliable inference pipeline<a data-startref="ix_ch05-asciidoc32" data-type="indexterm" id="id870"/>.<a data-startref="ix_ch05-asciidoc31" data-type="indexterm" id="id871"/></p>
</div></section>






<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="id88">
<h1>Summary</h1>

<p>This chapter explored the complex aspects of applying concurrency in AI systems.</p>

<p>You were introduced to concurrency and parallelism concepts, including several types of blocking operations that prevent you from simultaneously serving users.
You discovered concurrency techniques such as multithreading, multiprocessing, and asynchronous programming alongside their differences, similarities, benefits, and drawbacks in various use cases.</p>

<p>Next, you learned about thread pools and event loops, particularly in a FastAPI server environment, and understood their roles in processing requests concurrently.
This involved understanding how and why the server can be blocked if you’re not careful how you declare your route handlers.</p>

<p>Later, you discovered how to implement asynchronous programming to manage I/O blocking operations.
Through hands-on examples, you developed a deeper understanding of asynchronous interactions with databases and the web content, constructing both a web scraper and a RAG module.</p>

<p>Furthermore, you saw why larger GenAI models can be memory hungry and create memory-bound blocking operations.
As part of this, you were introduced to memory optimization techniques such as continuous batching and paged attention in serving LLMs to minimize memory-related bottlenecks.</p>

<p>Finally, you learned about approaches for handling long-running AI inference processes, ensuring your service remains responsive over prolonged operations.</p>

<p>With your knowledge from this chapter, you’re now prepared to apply concurrency principles to your own services, crafting resilient, scalable, and high-performing AI applications.</p>

<p>The ability to handle multiple users simultaneously is a significant milestone.
But there are additional optimizations you can perform to improve the user experience of your GenAI services even further.
You can provide real-time updates via streaming technologies to progressively show near real-time results to users during generation.
This is particularly useful for LLMs that may have longer generation times in conversation scenarios.</p>

<p>The upcoming chapter will explore AI streaming workloads, detailing the use of real-time communication technologies like server-sent events (SSE) and WebSocket (WS).
You will learn the difference between these technologies and how to implement model streaming by building endpoints for real-time text-to-text, text-to-speech, and speech-to-text interactions.<a data-startref="ix_ch05-asciidoc0" data-type="indexterm" id="id872"/></p>
</div></section>






<section data-pdf-bookmark="Additional References" data-type="sect1"><div class="sect1" id="id442">
<h1>Additional References</h1>

<ul>
<li>
<p>Kwon, W., et al. (2023).
<a href="https://oreil.ly/PtCqL">“Efficient Memory Management for Large Language Model Serving with PagedAttention”</a>.
arXiv preprint arXiv:2309.06180.</p>
</li>
<li>
<p>Lewis, P., et al. (2022).
<a href="https://oreil.ly/r5yVL">“Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks”</a>.
arXiv preprint arXiv:2005.11401.</p>
</li>
</ul>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id795"><sup><a href="ch05.html#id795-marker">1</a></sup> A core is an individual processing unit within a CPU or GPU that executes instructions. Modern CPUs and GPUs have multiple cores to perform tasks simultaneously.</p><p data-type="footnote" id="id802"><sup><a href="ch05.html#id802-marker">2</a></sup> Multithreading in most languages is parallel (running on multiple cores) and not concurrent. Python is changing over the next coming versions to do the same (free-threaded Python).</p><p data-type="footnote" id="id817"><sup><a href="ch05.html#id817-marker">3</a></sup> You can also find a custom implementation in <a href="https://oreil.ly/8E7GQ">OpenAI Cookbook on GitHub</a>.</p><p data-type="footnote" id="id822"><sup><a href="ch05.html#id822-marker">4</a></sup> The cost of setting up the threads is still incurred; it’s just done early to avoid doing it on the fly later.</p><p data-type="footnote" id="id830"><sup><a href="ch05.html#id830-marker">5</a></sup> P. Lewis et al. (2022), <a href="https://oreil.ly/GCk08">“Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks”</a>, arXiv preprint arXiv:2005.11401.</p><p data-type="footnote" id="id841"><sup><a href="ch05.html#id841-marker">6</a></sup> A dot product operation multiplies components of two vectors and then sums the results. It can be used to calculate the cosine of the angle between the vectors to quantify their similarity in direction (i.e., alignment). Vector databases use it to perform semantic search on document embeddings.</p><p data-type="footnote" id="id843"><sup><a href="ch05.html#id843-marker">7</a></sup> Refer to the <a href="https://oreil.ly/V4itQ">Docker documentation</a> for installation instructions.</p></div></div></section></body></html>