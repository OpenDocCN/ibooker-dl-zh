["```py\nnew_weight = weight - lr * weight.grad\n```", "```py\ndls = get_data(URLs.IMAGENETTE_160, 160, 128)\n```", "```py\ndef get_learner(**kwargs):\n    return cnn_learner(dls, resnet34, pretrained=False,\n                    metrics=accuracy, **kwargs).to_fp16()\n```", "```py\nlearn = get_learner()\nlearn.fit_one_cycle(3, 0.003)\n```", "```py\nlearn = get_learner(opt_func=SGD)\n```", "```py\nlearn.lr_find()\n```", "```py\n(0.017378008365631102, 3.019951861915615e-07)\n```", "```py\nlearn.fit_one_cycle(3, 0.03, moms=(0,0,0))\n```", "```py\ndef zero_grad(self):\n    for p,*_ in self.all_params():\n        p.grad.detach_()\n        p.grad.zero_()\n\ndef step(self):\n    for p,pg,state,hyper in self.all_params():\n        for cb in self.cbs:\n            state = _update(state, cb(p, **{**state, **hyper}))\n        self.state[p] = state\n```", "```py\ndef sgd_cb(p, lr, **kwargs): p.data.add_(-lr, p.grad.data)\n```", "```py\nopt_func = partial(Optimizer, cbs=[sgd_cb])\n```", "```py\nlearn = get_learner(opt_func=opt_func)\nlearn.fit(3, 0.03)\n```", "```py\nweight.avg = beta * weight.avg + (1-beta) * weight.grad\nnew_weight = weight - lr * weight.avg\n```", "```py\ndef average_grad(p, mom, grad_avg=None, **kwargs):\n    if grad_avg is None: grad_avg = torch.zeros_like(p.grad.data)\n    return {'grad_avg': grad_avg*mom + p.grad.data}\n```", "```py\ndef momentum_step(p, lr, grad_avg, **kwargs): p.data.add_(-lr, grad_avg)\n```", "```py\nopt_func = partial(Optimizer, cbs=[average_grad,momentum_step], mom=0.9)\n```", "```py\nlearn = get_learner(opt_func=opt_func)\nlearn.fit_one_cycle(3, 0.03)\n```", "```py\nlearn.recorder.plot_sched()\n```", "```py\nw.square_avg = alpha * w.square_avg + (1-alpha) * (w.grad ** 2)\nnew_w = w - lr * w.grad / math.sqrt(w.square_avg + eps)\n```", "```py\ndef average_sqr_grad(p, sqr_mom, sqr_avg=None, **kwargs):\n    if sqr_avg is None: sqr_avg = torch.zeros_like(p.grad.data)\n    return {'sqr_avg': sqr_avg*sqr_mom + p.grad.data**2}\n```", "```py\ndef rms_prop_step(p, lr, sqr_avg, eps, grad_avg=None, **kwargs):\n    denom = sqr_avg.sqrt().add_(eps)\n    p.data.addcdiv_(-lr, p.grad, denom)\n\nopt_func = partial(Optimizer, cbs=[average_sqr_grad,rms_prop_step],\n                   sqr_mom=0.99, eps=1e-7)\n```", "```py\nlearn = get_learner(opt_func=opt_func)\nlearn.fit_one_cycle(3, 0.003)\n```", "```py\nw.avg = beta * w.avg + (1-beta) * w.grad\nunbias_avg = w.avg / (1 - (beta**(i+1)))\n```", "```py\nw.avg = beta1 * w.avg + (1-beta1) * w.grad\nunbias_avg = w.avg / (1 - (beta1**(i+1)))\nw.sqr_avg = beta2 * w.sqr_avg + (1-beta2) * (w.grad ** 2)\nnew_w = w - lr * unbias_avg / sqrt(w.sqr_avg + eps)\n```", "```py\nnew_weight = weight - lr*weight.grad - lr*wd*weight\n```", "```py\nweight.grad += wd*weight\n```", "```py\nfor xb,yb in dl:\n    loss = loss_func(model(xb), yb)\n    loss.backward()\n    opt.step()\n    opt.zero_grad()\n```", "```py\ntry:\n    self._split(b);                                  self('begin_batch')\n    self.pred = self.model(*self.xb);                self('after_pred')\n    self.loss = self.loss_func(self.pred, *self.yb); self('after_loss')\n    if not self.training: return\n    self.loss.backward();                            self('after_backward')\n    self.opt.step();                                 self('after_step')\n    self.opt.zero_grad()\nexcept CancelBatchException:                         self('after_cancel_batch')\nfinally:                                             self('after_batch')\n```", "```py\nclass ModelResetter(Callback):\n    def begin_train(self):    self.model.reset()\n    def begin_validate(self): self.model.reset()\n```", "```py\nclass RNNRegularizer(Callback):\n    def __init__(self, alpha=0., beta=0.): self.alpha,self.beta = alpha,beta\n\n    def after_pred(self):\n        self.raw_out,self.out = self.pred[1],self.pred[2]\n        self.learn.pred = self.pred[0]\n\n    def after_loss(self):\n        if not self.training: return\n        if self.alpha != 0.:\n            self.learn.loss += self.alpha * self.out[-1].float().pow(2).mean()\n        if self.beta != 0.:\n            h = self.raw_out[-1]\n            if len(h)>1:\n                self.learn.loss += self.beta * (h[:,1:] - h[:,:-1]\n                                               ).float().pow(2).mean()\n```", "```py\nclass TerminateOnNaNCallback(Callback):\n    run_before=Recorder\n    def after_batch(self):\n        if torch.isinf(self.loss) or torch.isnan(self.loss):\n            raise CancelFitException\n```"]