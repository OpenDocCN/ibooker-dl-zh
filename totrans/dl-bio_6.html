<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 6. Learning Spatial Organization Patterns Within Cells"><div class="chapter" id="learning-spatial-organization-patterns-within-cells">
 <h1><span class="label">Chapter 6. </span>Learning Spatial Organization <span class="keep-together">Patterns Within Cells</span></h1>
 <p><a contenteditable="false" data-primary="cells, spatial organization patterns within" data-type="indexterm" id="ch06_localization.html0"/>In this chapter, we shift focus from classifying high-level cell states—such as distinguishing cancerous from healthy tissue—to something more low level and foundational: understanding the <em>spatial organization inside individual cells</em>. Specifically, we’ll train a deep learning model to analyze microscopy images and learn where exactly in the cell different proteins are located, a task known as <em>protein localization</em>.
</p>
 <p>Protein localization plays a crucial role in cell biology. A protein’s position within the cell—for example, whether it’s in the nucleus or the mitochondria—often determines its function. Mislocalization of proteins is implicated in many diseases, even when the protein’s structure is normal (i.e., not mutated or altered). Thanks to modern fluorescence microscopy, we can observe a protein’s location in a cell directly, but the resulting images are often high dimensional, noisy, and hard to interpret at scale.
</p>
 <p>Unlike earlier chapters, the goal here isn’t to strictly optimize a metric like accuracy, recall, or precision on a specific classification or regression task. Instead, we’ll train a model to learn a <em>latent representation</em> of protein localization directly from raw microscopy images. You can think of a <em>latent space</em> as the model’s internal map—a compressed representation where proteins with similar localization patterns are grouped together, even without explicit labels. This approach falls under <em>representation learning</em>: the goal is to uncover meaningful structure in the data that reflects biological patterns.
</p>
 <p>Why focus on representation learning instead of just training a classifier for a specific task of interest? In many biological settings, and especially in protein localization, we don’t have clean, comprehensive labels. Instead of forcing the model to solve a narrow, predefined task, we want it to learn a <em>rich internal representation</em> of the data that captures spatial patterns and similarities between proteins. These representations can be reused for clustering, visualization, identifying unknown cellular compartments, or understanding how protein localization changes across cell types or conditions. This is analogous to how large language models learn general-purpose representations of words or sentences, which can then be reused for many downstream tasks.
</p>
 <p>Our approach to modeling protein localization in this chapter is based on <em>cytoself</em>, a self-supervised deep learning method published in <em>Nature Methods</em> in 2022.<sup><a data-type="noteref" id="id943-marker" href="ch06.html#id943">1</a></sup> The model combines image reconstruction (rebuilding the microscopy input image) with a secondary task—predicting the protein’s identity—to learn a rich and interpretable embedding space that reflects biological localization patterns.
 </p>
 <p>Unlike in previous chapters, the model’s primary output isn’t a classification label or regression score. <a contenteditable="false" data-primary="embeddings" data-secondary="defined" data-type="indexterm" id="id944"/>Instead, for a given microscopy image of a fluorescently tagged protein, it produces an <em>embedding</em>—a position in the latent space—that captures the spatial characteristics of the protein’s localization. These embeddings can then be visualized, clustered, or compared to known annotations.
 </p>

 <p>This is the most advanced chapter in the book. You’ll work with a large, real-world microscopy dataset and implement a custom vector-quantized variational autoencoder (VQ-VAE) from scratch. The chapter takes you deep into self-supervised learning, large-scale image processing, and the spatial organization of cells—and reproduces core results from a recent deep learning biology paper.</p>

 <div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>More than in any previous chapter, we strongly recommend keeping the companion Colab notebook open as you read. You may need to scale down the model to fit within memory limits, but actively running the code will solidify your understanding and give you room to explore.</p> 
<p>To run the full-scale model, we recommend using a powerful GPU such as an A100. These are available through platforms like Google Colab Pro, Kaggle Notebooks (with upgrades), Google Cloud Platform (GCP), or AWS EC2.</p>
 </div>
 <section data-type="sect1" data-pdf-bookmark="Biology Primer"><div class="sect1" id="biology-primer_66266475">
  <h1>Biology Primer</h1>
 <p><a contenteditable="false" data-primary="cells, spatial organization patterns within" data-secondary="biology primer" data-type="indexterm" id="ch06_localization.html1"/>The cell was first observed in 1665 by the British scientist <a href="https://oreil.ly/6JmDd">Robert Hooke</a>, who used a microscope to describe its structure in cork tissue. Since then, microscopy has become one of biology’s most essential tools. Modern microscopes allow researchers to visualize living cells in astonishing detail—and increasingly, to capture this data at massive scale.
  </p>
 <p>Microscopy is now central to many areas of biomedical research. For example, pharmaceutical companies routinely screen the effects of drug candidates by imaging thousands of treated cells and then use machine learning models to assess cellular responses in an automated way. Does a cell look alive or dead? Does its observable structure change in response to a particular drug compound? Do the cells divide more or less rapidly as a result of a treatment?
  </p>
 <p>Despite its power to capture biological detail, microscopy data can be challenging to analyze. Cells vary naturally in size, shape, and appearance, and the imaging and sample preparation process itself introduces noise and artifacts. Furthermore, unlike genomes, which can to some extent be compared to a universal “reference” genome, cells don’t come with a “baseline cell” or a standardized coordinate system, which can make analysis difficult. And microscopy can challenge computational resources: high-resolution microscopy produces large volumes of data that can strain both memory and compute.
  </p>
 <p>Traditionally, microscopy image analysis relied on manually defined, hand-engineered features. For example, in drug screening experiments, scientists might measure whether a compound causes cells to shrink, swell, or change shape—signs that the drug is affecting cell health or behavior. To do so, researchers would extract properties such as cell size, shape, brightness, or texture using classical image processing techniques, including thresholding, edge detection, and morphological operations (which manipulate shapes in the image to clean up noise, fill gaps, or separate touching cells). These features were then fed into relatively simple models such as logistic regression or decision trees to predict cellular outcomes.
  </p>
  <div data-type="note" epub:type="note"><h6>Note</h6>
  <p>This earlier approach to microscopy image analysis required domain expertise to decide which features mattered, and they often missed subtle patterns not obvious to the human eye. Deep learning changed this: modern convolutional neural networks (CNNs) can learn to extract meaningful features directly from raw pixels, capturing complex visual cues without relying on handcrafted rules.
   </p>
  </div>

  <p>These advances in image analysis have opened the door to exploring deeper biological questions—including how proteins are spatially organized within cells.</p>

  <section data-type="sect2" data-pdf-bookmark="Spatial Organization Within the Cell"><div class="sect2" id="spatial-organization-within-the-cell">
   <h2>Spatial Organization Within the Cell</h2>
  <p><a contenteditable="false" data-primary="cells, spatial organization patterns within" data-secondary="biology primer" data-tertiary="spatial organization within the cell" data-type="indexterm" id="ch06_localization.html2"/>Cells aren’t just sacks of molecules; they’re intricately organized. Subcellular compartments evolved billions of years ago and represent one of the most fundamental principles of biology: <em>specialization through spatial organization</em>. Instead of letting everything float freely in an undifferentiated soup, cells developed internal structures that separate and coordinate different functions.
   </p>
  <p>All three domains of life—bacteria, archaea, and eukaryotes—show signs of this spatial complexity. In eukaryotic cells (the kinds of cells found in humans, plants, fungi, and more), this organization is especially pronounced. These cells contain membrane-bound compartments called <em>organelles</em>, each with a specialized job, as shown in <a data-type="xref" href="#organelle-image">Figure 6-1</a>.
   </p>
   <figure><div id="organelle-image" class="figure">
    <img alt="" src="assets/dlfb_0601.png" width="600" height="449"/>
    <h6><span class="label">Figure 6-1. </span>A visual representation of the complex organization within a eukaryotic cell. The different organelles compartmentalize the cell into regions where different specialized functions are performed (illustration from the National Institutes of Health).
    </h6>
   </div></figure>
  <p>We won’t dive into full organelle flashcards here (no need to relive high school biology trauma), but here’s a quick recap of the structures that will be most relevant in this chapter:
   </p>
   <dl>
  <dt>Plasma membrane</dt>
  <dd><p>A lipid bilayer that wraps the cell, protecting its internal environment and controlling what enters and exits.</p></dd>
  <dt>Nucleus</dt>
  <dd><p>The command center of the cell. It’s enclosed in its own membrane and houses the DNA—the genetic blueprint.</p></dd>
  <dt>Cytoplasm</dt>
  <dd><p>The gel-like substance that fills the cell. This is where most cellular activity happens, and it’s home to many other organelles.</p></dd>
  <dt>Mitochondria</dt>
  <dd><p>Often called the “powerhouses” of the cell. They generate energy in the form of ATP. Evolutionarily, they were once free-living bacteria that formed a mutually beneficial partnership with early cells.</p></dd>
  <dt>Endoplasmic reticulum (ER)</dt>
  <dd><p>Comes in two flavors. The <em>rough ER</em> is studded with ribosomes (protein-making factories) and helps synthesize proteins. The <em>smooth ER</em> handles lipids and detoxification.</p></dd>
  <dt>Golgi apparatus</dt>
  <dd><p>The cell’s shipping center. It modifies, packages, and routes proteins and lipids to their final destinations.</p></dd>
   </dl>
  <p>With that background on cellular compartments in place, let’s turn to protein localization.<a contenteditable="false" data-primary="" data-startref="ch06_localization.html2" data-type="indexterm" id="id945"/>
   </p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Protein Localization"><div class="sect2" id="protein-localization">
   <h2>Protein Localization</h2>
  <p><a contenteditable="false" data-primary="cells, spatial organization patterns within" data-secondary="biology primer" data-tertiary="protein localization" data-type="indexterm" id="ch06_localization.html3"/><a contenteditable="false" data-primary="protein localization" data-type="indexterm" id="ch06_localization.html4"/>Every protein begins its life in the same way: it’s transcribed from a gene into messenger RNA (mRNA) in the nucleus, and then it is translated into a chain of amino acids by ribosomes in the cytoplasm. But from there, things get much more spatially complex.
   </p>
  <p>Once a protein is made, it doesn’t just float around randomly. It gets shipped to a specific destination inside the cell, guided by a sort of molecular postal code. Some proteins are sent to the nucleus, others to the mitochondria, the cell membrane, or the ER. This process is known as <em>protein localization</em>, and it’s critical for proper cellular function.
   </p>
  <p>A protein’s location is often just as important as its molecular structure. Even a perfectly formed protein can’t function if it’s in the wrong place. For example, the protein DNA polymerase needs to be inside the nucleus to carry out its function of replicating DNA. If it ends up in the cytoplasm, it’s effectively useless. On the other hand, some proteins aren’t confined to a specific location. For instance, the protein actin is found throughout the cell, where it helps maintain its shape and enable movement.
   </p>
  <p>Mislocalization—when a protein ends up in the wrong compartment—is a key driver of disease. For example, in amyotrophic lateral sclerosis (ALS), the protein TDP-43 accumulates in the cytoplasm of neurons, even though its normal location is in the nucleus.<sup><a data-type="noteref" id="id946-marker" href="ch06.html#id946">2</a></sup> The protein isn’t mutated, misfolded, or present in abnormal quantities—it’s simply in the wrong place. And that alone is enough to disrupt cellular function and trigger disease.
   </p>
  <p>Another example is the tumor suppressor BRCA1, which normally helps repair DNA in the nucleus. In some breast and ovarian cancers, BRCA1 is mislocalized to the cytoplasm, where it can no longer perform its repair function, even though the protein itself may be entirely structurally normal.<sup><a data-type="noteref" id="id947-marker" href="ch06.html#id947">3</a></sup>
   </p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Understanding Protein Localization"><div class="sect2" id="understanding-protein-localization">
   <h2>Understanding Protein Localization</h2>
  <p>Once a protein is made, how does it end up in the right part of the cell? The answer lies in short amino acid sequences, which are like molecular zip codes embedded within the protein. These address tags are recognized by the cell’s transport machinery and guide proteins to their proper destinations: the nucleus, mitochondria, plasma membrane, and so on.
   </p>
   <div data-type="warning" epub:type="warning"><h6>Warning</h6>
   <p>The rules of this system are still only partially understood. Many proteins localize to multiple compartments. Some are misdirected under stress or disease conditions. And some just seem to defy categorization.
    </p>
   <p>Even in healthy cells, we don’t yet have a complete map of where all human proteins go—or how those localizations change across cell types, developmental stages, or environmental conditions.
    </p>
   </div>
  <p>Mapping the protein localization landscape in detail is one of the big open challenges in cell biology. Understanding protein localization at scale could:
   </p>
   <dl>
   <dt>Reveal new, previously unknown subcellular compartments</dt>
    <dd><p>It may be surprising, but we’re still discovering fundamental structures inside cells. In recent years, researchers have identified the <em>exclusome</em> (a cytoplasmic DNA compartment in mammalian cells), <em>paraspeckles</em> and <em>nuclear speckles</em> (membraneless nuclear bodies involved in RNA processing), and even new entire organelles. The most recent example is the <em>nitroplast</em>, a nitrogen-fixing organelle discovered in marine algae as recently as 2024.<sup><a data-type="noteref" id="id948-marker" href="ch06.html#id948">4</a></sup> These findings show how much more there is to uncover about the cell’s internal architecture.</p></dd>
    <dt>Help assign functions to poorly annotated proteins</dt>
    <dd><p>If a protein consistently localizes to mitochondria, you could hypothesize that it plays a role in energy metabolism or apoptosis (programmed cell death), which are key functions of the mitochondria. For example, the cytoself model we study in this chapter grouped several previously uncharacterized proteins with known mitochondrial proteins, leading researchers to propose their involvement in oxidative phosphorylation—the process by which cells generate ATP within the mitochondrial matrix.</p></dd>
    <dt>Detect early cellular changes that mark disease</dt>
    <dd><p>Shifts in protein localization can serve as early warning signs of various diseases. For example, we previously mentioned that in ALS, the protein TDP-43 moves from the nucleus to the cytoplasm, but remarkably, this change has been observed in presymptomatic individuals carrying disease-linked mutations.<sup><a data-type="noteref" id="id949-marker" href="ch06.html#id949">5</a></sup> More broadly, large-scale profiling of localization patterns could help detect early cellular dysfunction across a wide range of conditions.</p></dd>
    <dt>Therapies that correct protein mislocalization</dt>
    <dd><p>In diseases where a protein is physically functional but simply ends up in the wrong place, one therapeutic strategy is to restore its proper localization using engineered localization signals. For example, researchers have used nuclear localization signals to redirect tumor suppressors like p53 or BRCA1 back to the nucleus, where they can resume their normal function.</p></dd>
    <dt>Better therapeutic targeting</dt>
    <dd><p>Another approach is to guide drugs, proteins, or nanoparticles to specific subcellular compartments, such as the lysosome or mitochondria, to maximize their effectiveness and minimize side effects. This strategy is used in emerging nanomedicine platforms.<sup><a data-type="noteref" id="id950-marker" href="ch06.html#id950">6</a></sup></p></dd>
   </dl>
  <p>Imagine being able to say not just <em>what</em> a protein does but <em>where</em> it does it—and how its journey changes as a cell divides, differentiates, or begins to break down in disease.
   </p>
  <p>Ideally, that gives you a sense of how exciting the protein localization space is. Now let’s dive into how machine learning can help us explore it<a contenteditable="false" data-primary="" data-startref="ch06_localization.html4" data-type="indexterm" id="id951"/><a contenteditable="false" data-primary="" data-startref="ch06_localization.html3" data-type="indexterm" id="id952"/>.<a contenteditable="false" data-primary="" data-startref="ch06_localization.html1" data-type="indexterm" id="id953"/>
   </p>
  </div></section>
 </div></section>
 <section data-type="sect1" data-pdf-bookmark="Machine Learning Primer"><div class="sect1" id="machine-learning-primer_43560611">
  <h1>Machine Learning Primer</h1>
 <p><a contenteditable="false" data-primary="cells, spatial organization patterns within" data-secondary="machine learning primer" data-type="indexterm" id="ch06_localization.html5"/><a contenteditable="false" data-primary="machine learning" data-secondary="for learning spatial organization patterns within cells" data-type="indexterm" id="ch06_localization.html6"/>The model we’ll be building in this chapter—the cytoself from the Kobayashi paper mentioned earlier—is based on a type of neural network called a <em>vector-quantized variational autoencoder (VQ-VAE)</em>. If you haven’t seen this type of model before, don’t worry: we’ll walk through the key ideas that lead up to this model so you understand not just <em>what</em> we’re building but <em>why</em> it works.
  </p>
 <p><a contenteditable="false" data-primary="vector-quantized variational autoencoders (VQ-VAEs)" data-secondary="reasons to use" data-type="indexterm" id="id954"/>Why use a VQ-VAE? Unlike most models, which map images into a continuous feature space, a <a contenteditable="false" data-primary="codebook" data-secondary="defined" data-type="indexterm" id="id955"/>VQ-VAE forces the model to describe each image using a limited set of learned visual patterns, called a <em>codebook</em>. You can think of this like a tile set or a visual vocabulary, where each pattern gets reused across many inputs. This encourages the model to represent each protein image using discrete building blocks, making it easier to group proteins by similar localization and uncover shared visual motifs. In other words, instead of inventing new coordinates for every input, the model says, “This protein looks like a mix of tile #7 and tile #241.”
  </p>
 <p>This kind of representation is especially useful in biology, where we’re trying to <em>discover</em>
   structure in the data, not just reconstruct it. The discreteness also makes the model more interpretable and allows downstream tools (like clustering or dimensionality reduction) to work more effectively.
  </p>
  <section data-type="sect2" data-pdf-bookmark="Autoencoders (AEs)"><div class="sect2" id="autoencoders-aes">
   <h2>Autoencoders (AEs)</h2>
   <p><a contenteditable="false" data-primary="autoencoders (AEs)" data-type="indexterm" id="ch06_localization.html7"/><a contenteditable="false" data-primary="cells, spatial organization patterns within" data-secondary="machine learning primer" data-tertiary="autoencoders" data-type="indexterm" id="ch06_localization.html8"/><a contenteditable="false" data-primary="machine learning" data-secondary="for learning spatial organization patterns within cells" data-tertiary="autoencoders" data-type="indexterm" id="ch06_localization.html9"/>Let’s start by unpacking the last and arguably most important part of the VQ-VAE acronym: the AE, <em>autoencoder</em>.</p>
   <p>An autoencoder is a type of neural network trained to reconstruct its input. It does this in two steps:</p>
   <ol class="simple">
    <li><p>An <em>encoder</em> compresses the input into a lower-dimensional representation.</p></li>
    <li><p>A <em>decoder</em> then attempts to reconstruct the original input from that compressed version.</p></li>
   </ol>
  <p>In the simplest case, both the encoder and decoder might consist of fully connected linear layers. The internal representation, known as the bottleneck, typically has fewer neurons than the input, forcing the model to compress the data, as illustrated in <a data-type="xref" href="#autoencoder-diagram">Figure 6-2</a> (diagram taken from “Introduction to Autoencoders”).<sup><a data-type="noteref" id="id956-marker" href="ch06.html#id956">7</a></sup>
    
   </p>
   <figure><div id="autoencoder-diagram" class="figure">
    <img alt="" src="assets/dlfb_0602.png" width="600" height="576"/>
    <h6><span class="label">Figure 6-2. </span>An autoencoder learns a compressed internal representation of its input by forcing it through a low-dimensional “bottleneck.” The network is trained to reconstruct the original input as closely as possible from this bottleneck representation.
    </h6>
   </div></figure>
  <p>The bottleneck forces the model to distill the most important patterns in the data, while discarding irrelevant details. This is a form of dimensionality reduction, similar in spirit to techniques like principal component analysis (PCA). But unlike PCA, autoencoders can learn nonlinear transformations and scale to large, complex datasets, making them especially powerful for data like microscopy images.
   </p>
  <p>One important detail: in a standard autoencoder, the internal representation—the activations of the neurons in the bottleneck layer—are <em>continuous</em>. That means each neuron in the bottleneck can take on any real number (like 1.27, –3.14, etc.), so the representation of each input lives in a continuous space. This gives the model a lot of flexibility, but it can also make the latent space less structured and harder to interpret. Two inputs that look quite similar might map to distant points in the latent space, and it can be difficult to understand what each dimension represents.
   </p>
  <p>Later, we’ll see how VQ-VAEs address this by introducing a fixed set of allowed <em>codes</em>. Instead of letting the encoder output arbitrary values, it’s forced to pick from a dictionary of discrete latent vectors—a design that makes the representation more structured, compressible, and interpretable. Of course, this comes with a trade-off: the model gives up some of the expressivity of continuous representations in exchange for a more constrained and interpretable latent space.
   </p>
   <div data-type="tip"><h6>Tip</h6>
   <p>Cytoself uses a ResNet-style CNN as its encoder. If you’d like a refresher on CNNs and ResNets, see the previous chapter’s explanation of these topics.<a contenteditable="false" data-primary="" data-startref="ch06_localization.html9" data-type="indexterm" id="id957"/><a contenteditable="false" data-primary="" data-startref="ch06_localization.html8" data-type="indexterm" id="id958"/><a contenteditable="false" data-primary="" data-startref="ch06_localization.html7" data-type="indexterm" id="id959"/>
    </p>
   </div>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Variational Autoencoders (VAEs)"><div class="sect2" id="variational-autoencoders-vaes">
   <h2>Variational Autoencoders (VAEs)</h2>
  <p><a contenteditable="false" data-primary="autoencoders (AEs)" data-secondary="VAEs" data-type="indexterm" id="ch06_localization.html10"/><a contenteditable="false" data-primary="cells, spatial organization patterns within" data-secondary="machine learning primer" data-tertiary="VAEs" data-type="indexterm" id="ch06_localization.html11"/><a contenteditable="false" data-primary="machine learning" data-secondary="for learning spatial organization patterns within cells" data-tertiary="VAEs" data-type="indexterm" id="ch06_localization.html12"/><a contenteditable="false" data-primary="variational autoencoders (VAEs)" data-type="indexterm" id="ch06_localization.html13"/>Now that we’ve introduced regular autoencoders, let’s look at a variation that adds a twist—and opens the door to powerful generative capabilities. The <em>V</em> in <em>VAE</em> stands for <em>variational</em>. In a <em>variational autoencoder</em>, we no longer compress each input (like a microscopy image) into a single point in the latent space. Instead, the model learns to represent each input as a <em>probability distribution</em> over the latent space. That’s a bit of a mouthful, so let’s break it down step-by-step.
   </p>
  <p>In a standard autoencoder:
   </p>
   <ul class="simple">
    <li><p>The encoder takes an input and produces a fixed set of numbers: one activation for each neuron in the bottleneck layer we described earlier.</p></li>
    <li><p>These numbers describe a single point in latent space.</p></li>
    <li><p>The decoder then uses that point to reconstruct the input.</p></li>
   </ul>
  <p>In a variational autoencoder:
   </p>
   <ul class="simple">
    <li><p>The encoder outputs two numbers per latent dimension: a <em>mean</em> and a <em>standard deviation</em>.</p></li>
    <li><p>These define a normal distribution—a bell curve—for each coordinate in the latent space.</p></li>
    <li><p>Instead of feeding a fixed number to the decoder, the model samples a value from each distribution.</p></li>
   </ul>
  <p>To make this concrete:
   </p>
   <ul class="simple">
    <li><p>In a standard autoencoder, a given input might be compressed to a single point in the latent space, for example, <code>[1.3, -0.7, 1.9]</code> for a bottleneck layer with three neurons. This exact vector is passed <em>directly</em> to the decoder to reconstruct the input.</p></li>
    <li><p>In a variational autoencoder, the encoder instead outputs two separate vectors:</p>
     <ul>
      <li><p>One for the means (e.g., <code>[1.3, -0.7, 1.9]</code>)</p></li>      
      <li><p>And one for the standard deviations (e.g., <code>[0.2, 0.5, 0.1]</code>)</p></li>     
    </ul>
    </li>
   </ul>
  <p>This is often done using a shared hidden layer followed by two parallel linear layers: one predicts the means, the other the standard deviations. So, in this example, the encoder outputs six values in total: three means and three standard deviations.
   </p>
  <p>Together, these describe a 3D Gaussian distribution: not a single point, but a cloud of likely values. During training, the model randomly samples a vector from this distribution; for example:
   </p>
   <ul class="simple">
  <li><p>Something like <code>1.1</code> from a distribution centered at <code>1.3 ± 0.2</code></p></li>
  <li><p> <code>-0.5</code> from <code>-0.7 ± 0.5</code></p></li>
  <li><p> <code>1.7</code> from <code>1.9 ± 0.1</code></p></li>
   </ul>
  <p>The sampled vector—in this case, <code>[1.1, -0.5, 1.7]</code>—is what actually gets passed to the decoder for reconstruction.
   </p>
   <section data-type="sect3" data-pdf-bookmark="Why add randomness?"><div class="sect3" id="why-add-randomness">
    <h3>Why add randomness?</h3>
   <p><a contenteditable="false" data-primary="variational autoencoders (VAEs)" data-secondary="benefits of introducing randomness" data-type="indexterm" id="id960"/>At first, introducing randomness might sound like unnecessary fuzziness: why not just stick with a fixed encoding? But this design choice has several powerful benefits:
    </p>
    <dl>
      <dt>It smooths the latent space.</dt>
     <dd>
      <p>Because the model samples slightly different encodings for the same input, it learns to decode nearby points into similar outputs. This forces the latent space to be smooth, continuous, and meaningful—with small movements in the space leading to small, realistic changes in the output.
      </p></dd>
    <dt>It groups similar inputs together.</dt>
     <dd>
      <p>Inputs that are alike produce similar distributions, so their samples overlap in the latent space. This naturally pulls similar data points closer together, helping the model learn structure in the dataset.
      </p></dd>
    <dt>It prevents overfitting.</dt>
     <dd>
      <p>By introducing controlled randomness during training, the model can’t just memorize exact input-output pairs. It has to learn patterns that hold across perturbations.
      </p></dd>
    <dt>It enables generation.</dt>
     <dd>
      <p>Once trained, the model can generate new, realistic-looking outputs by simply sampling from the latent space, even in regions that weren’t seen during training. This makes VAEs useful not just for reconstruction, but for creative or exploratory tasks.
      </p></dd>
    </dl>
</div></section>
    <section data-type="sect3" data-pdf-bookmark="Continuous latent space"><div class="sect3" id="cls">
    <h3>Continuous latent space</h3>
<p><a contenteditable="false" data-primary="latent space" data-secondary="VAEs and" data-type="indexterm" id="ch06_localization.html14"/><a contenteditable="false" data-primary="variational autoencoders (VAEs)" data-secondary="continuous latent space" data-type="indexterm" id="ch06_localization.html15"/>We can visualize this difference in how latent spaces are structured in VAEs versus standard autoencoders using the diagram in <a data-type="xref" href="#latent-space-structure">Figure 6-3</a> (based on a figure by Saul Dobilas).<sup><a data-type="noteref" id="id961-marker" href="ch06.html#id961">8</a></sup>
  
    </p>

    <figure><div id="latent-space-structure" class="figure">
     <img alt="" src="assets/dlfb_0603.png" width="600" height="506"/>
     <h6><span class="label">Figure 6-3. </span>An intuitive way to think about regularized continuous latent spaces. In a standard autoencoder, points are mapped discretely and may not generalize meaningfully. In a VAE, points are sampled from smooth distributions, enabling meaningful interpolation and generative sampling.
     </h6>
    </div></figure>
   <p>This diagram highlights how VAEs encourage a smooth and continuous latent space, enabling interpolation and generation, a key distinction from regular autoencoders.
    </p>
    <div data-type="note" epub:type="note"><h6>Note</h6>
    <p>You might wonder: can’t you use a regular autoencoder for grouping similar inputs together and for generative sampling?
     </p>
    <p>In theory, yes; similar inputs often <em>do</em> end up close together in the latent space, and you <em>can</em> try sampling from it. But there’s no guarantee that the space will be smooth, continuous, or meaningful. Some regions may decode into nonsense, and small changes in latent space might lead to big, unpredictable jumps in the output.
     </p>
    <p>VAEs solve this by explicitly <em>shaping the latent space</em>. They use probability distributions and regularization to encourage the model to use the space in a more structured and consistent way.<a contenteditable="false" data-primary="" data-startref="ch06_localization.html15" data-type="indexterm" id="id962"/><a contenteditable="false" data-primary="" data-startref="ch06_localization.html14" data-type="indexterm" id="id963"/>
     </p>
    </div>
   <p>There’s just one more concept to cover before we can understand this chapter’s architecture: <em>vector quantization</em><a contenteditable="false" data-primary="" data-startref="ch06_localization.html13" data-type="indexterm" id="id964"/><a contenteditable="false" data-primary="" data-startref="ch06_localization.html12" data-type="indexterm" id="id965"/><a contenteditable="false" data-primary="" data-startref="ch06_localization.html11" data-type="indexterm" id="id966"/><a contenteditable="false" data-primary="" data-startref="ch06_localization.html10" data-type="indexterm" id="id967"/>.
    </p>
   </div></section>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Vector-Quantized Variational Autoencoders (VQ-VAEs)"><div class="sect2" id="vector-quantized-variational-autoencoders-vq-vaes">
   <h2>Vector-Quantized Variational Autoencoders (VQ-VAEs)</h2>
  <p><a contenteditable="false" data-primary="cells, spatial organization patterns within" data-secondary="machine learning primer" data-tertiary="VQ-VAEs" data-type="indexterm" id="ch06_localization.html16"/><a contenteditable="false" data-primary="machine learning" data-secondary="for learning spatial organization patterns within cells" data-tertiary="VQ-VAEs" data-type="indexterm" id="ch06_localization.html17"/><a contenteditable="false" data-primary="variational autoencoders (VAEs)" data-secondary="VQ-VAEs" data-type="indexterm" id="ch06_localization.html18"/><a contenteditable="false" data-primary="vector-quantized variational autoencoders (VQ-VAEs)" data-secondary="basics" data-type="indexterm" id="ch06_localization.html19"/>The <em>VQ</em> in <em>VQ-VAE</em> stands for <em>vector quantization</em>, a classical technique borrowed from signal processing and data compression. <a contenteditable="false" data-primary="codebook" data-secondary="defined" data-type="indexterm" id="id968"/>At its core, vector quantization means taking a continuous input, such as a floating-point vector, and snapping it to the nearest match from a fixed set of allowed vectors. This set is called a <em>codebook</em>.
   </p>
  <p>To make this concrete:
   </p>
   <ul class="simple">
    <li><p>Imagine our codebook contains just two vectors: <code>[2, 0.5]</code> and <code>[1, -3]</code>.</p></li>
    <li><p>Now suppose the encoder outputs <code>[1.8, 0.3]</code>.</p></li>
    <li><p>Instead of passing this continuous vector to the decoder, the VQ-VAE finds the nearest codebook entry—in this case, <code>[2, 0.5]</code> —and <em>replaces</em> the encoder output with that vector.</p></li>
   </ul>
  <p>This “snapping” process is called <em>quantization</em>. You can think of it like rounding a continuous input to the closest available option. The decoder never sees the raw encoder output, only the snapped codebook vector.
   </p>
  <p>This makes VQ-VAEs different from variational autoencoders or standard autoencoders: rather than learning a continuous latent space, the model learns a <em>discrete vocabulary of embeddings</em> and uses that to represent everything it sees. By compressing the encoder output to a finite set of discrete vectors, the model encourages robustness, interpretability, and reusability in its representations, all of which are particularly helpful for downstream tasks like clustering or biological discovery.
   </p>
   <section data-type="sect3" data-pdf-bookmark="Where does the codebook come from?"><div class="sect3" id="where-does-the-codebook-come-from">
    <h3>Where does the codebook come from?</h3>
   <p><a contenteditable="false" data-primary="codebook" data-secondary="origins" data-type="indexterm" id="id969"/>The codebook in a VQ-VAE is
     <em>learned during training</em>. Just like the weights in the encoder and decoder, the vectors in the codebook start out random and are gradually refined through backpropagation. Over time, the vectors adapt to represent recurring, meaningful patterns in the data, so the model gets better at snapping encoder outputs to useful representations.
    </p>
  </div></section>
    <section data-type="sect3" data-pdf-bookmark="How large should the codebook be?"><div class="sect3" id="how-large">
    <h3>How large should the codebook be?</h3>

    <p><a contenteditable="false" data-primary="codebook" data-secondary="size distinctions" data-type="indexterm" id="id970"/>There’s no one-size-fits-all answer: the optimal number of vectors depends on your data and goals. A <em>larger codebook</em> (e.g., 1,024+ entries) allows finer distinctions between inputs. A <em>smaller codebook</em> (e.g., 64–128) forces the model to reuse patterns more often, which can help with generalization and interpretability.
     </p>
    <p>In biological imaging tasks like protein localization, codebook sizes typically range from 128 to 512, depending on the number of proteins, the resolution of localization patterns, and the desired balance between expressiveness and interpretability. Cytoself, for instance, uses two codebooks of 2,048 entries, each made up of 64D vectors—one for the global representation and one for the local—giving the model rich capacity to represent complex spatial patterns.<a contenteditable="false" data-primary="" data-startref="ch06_localization.html19" data-type="indexterm" id="id971"/><a contenteditable="false" data-primary="" data-startref="ch06_localization.html18" data-type="indexterm" id="id972"/><a contenteditable="false" data-primary="" data-startref="ch06_localization.html17" data-type="indexterm" id="id973"/><a contenteditable="false" data-primary="" data-startref="ch06_localization.html16" data-type="indexterm" id="id974"/>
     </p>
  
   </div></section>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Dissecting a VQ-VAE Diagram"><div class="sect2" id="dissecting-a-vq-vae-diagram">
   <h2>Dissecting a VQ-VAE Diagram</h2>
  <p><a contenteditable="false" data-primary="cells, spatial organization patterns within" data-secondary="machine learning primer" data-tertiary="dissecting a VQ-VAE diagram" data-type="indexterm" id="ch06_localization.html20"/><a contenteditable="false" data-primary="machine learning" data-secondary="for learning spatial organization patterns within cells" data-tertiary="dissecting a VQ-VAE diagram" data-type="indexterm" id="ch06_localization.html21"/><a contenteditable="false" data-primary="vector-quantized variational autoencoders (VQ-VAEs)" data-secondary="dissecting a VQ-VAE diagram" data-type="indexterm" id="ch06_localization.html22"/>Now that we’ve covered the key building blocks—autoencoders, variational inference, and vector quantization—we’re ready to interpret the original VQ-VAE diagram, shown in <a data-type="xref" href="#vqvae-original">Figure 6-4</a>.<sup><a data-type="noteref" id="id975-marker" href="ch06.html#id975">9</a></sup>
   </p>
   <figure><div id="vqvae-original" class="figure">
    <img alt="" src="assets/dlfb_0604.png" width="600" height="279"/>
    <h6><span class="label">Figure 6-4. </span>Illustration of the major components in a VQ-VAE, based on van den Oord et al. (2017).
    </h6>
   </div></figure>
  <p>Here’s a step-by-step breakdown of what’s happening in the above VQ-VAE diagram:
   </p>
   <dl>
    <dt>1. Input image (dog photo on the far left)</dt>
    <dd><p>The process begins with a raw input image; here, the input is a photo of a dog. In our case, this would be a microscopy image showing protein localization. <a contenteditable="false" data-primary="latent representation" data-type="indexterm" id="id976"/>The image is passed into a CNN encoder, which extracts meaningful features and transforms the image into a compressed latent representation.</p>
     <p>This representation is shown as the <em>cube labeled <code>z<sub>e</sub>(x)</code></em> (the variable <code>z</code> is often used to denote embeddings). It’s a 3D tensor because CNNs process images into pseudoimages called feature maps, which highlight specific parts of the input. This cube holds the distilled, continuous version of the input, and it’s what will be quantized next using the codebook.
     </p></dd>
    <dt>2. Quantization step (dot to the right of the left cube)</dt>
    <dd><p>Each vector in the cube <code>z<sub>e</sub>(x)</code>—the continuous encoder output—is passed to the <em>vector quantization step</em>. This step compares each D-dimensional vector (D = 64 for cytoself) in the grid to every entry in a learned <em>codebook</em> (<code>e<sub>1</sub></code>, <code>e<sub>2</sub></code>, …, <code>e<sub>k</sub></code>), shown at the top of the diagram.</p>
     <p>The model then <em>replaces</em> that vector with the nearest codebook vector, snapping it to the closest match. This creates a new tensor called <code>z<sub>q</sub>(x)</code>, the quantized version of the encoder output, where every original vector has been replaced by a discrete one from the codebook.</p>
     </dd>
     <dt>3. Decoder (cube on the right)</dt>
    <dd><p>Once quantized, the decoder input <code>z<sub>q</sub>(x)</code> (right cube) is passed into the <em>decoder</em> (“CNN” arrow), which is another CNN. Its job is to take the snapped, discrete representation and <em>reconstruct the original image</em> as closely as possible. In this diagram, the decoder output is shown as a second image of the dog, visually similar to the input. For us in this chapter, this would be a reconstructed microscopy image.</p>
     <p>The closer the reconstruction is to the original (i.e., the lower the reconstruction loss), the better the model has learned to encode meaningful information in its quantized representation.</p>
     </dd>
     <dt>4. Codebook embedding space (far right)</dt>
    <dd><p>The panel on the far right shows a <em>zoomed-in view of the embedding space</em>, the set of vectors in the codebook. The central dot (with border and arrow) represents the encoder’s original output vector, <code>z<sub>e</sub>(x)</code>, which sits somewhere in continuous space. The model snaps it to the closest codebook vector, here shown as <code>e<sub>2</sub></code>. That snapped value becomes the quantized output <code>z<sub>q</sub>(x)</code>.</p>
     <p>The arrow indicates the <em>training signal</em>: during learning, the encoder is nudged to produce outputs that are closer to the codebook entries they get snapped to. This improves the efficiency of quantization over time and ensures that codebook entries are actually used.</p></dd>
  </dl>
  <p>This is starting to get into VQ-VAE training strategy, so let’s now cover this topic explicitly.<a contenteditable="false" data-primary="" data-startref="ch06_localization.html22" data-type="indexterm" id="id977"/><a contenteditable="false" data-primary="" data-startref="ch06_localization.html21" data-type="indexterm" id="id978"/><a contenteditable="false" data-primary="" data-startref="ch06_localization.html20" data-type="indexterm" id="id979"/>
   </p>
  </div></section>
  <section data-type="sect2" class="pagebreak-before less_space" data-pdf-bookmark="Training a VQ-VAE"><div class="sect2" id="training-a-vq-vae">
   <h2>Training a VQ-VAE</h2>
  <p><a contenteditable="false" data-primary="cells, spatial organization patterns within" data-secondary="machine learning primer" data-tertiary="training a VQ-VAE" data-type="indexterm" id="ch06_localization.html23"/><a contenteditable="false" data-primary="machine learning" data-secondary="for learning spatial organization patterns within cells" data-tertiary="training a VQ-VAE" data-type="indexterm" id="ch06_localization.html24"/><a contenteditable="false" data-primary="training" data-secondary="for learning spatial organization within cells" data-tertiary="VQ-VAE" data-type="indexterm" id="ch06_localization.html25"/><a contenteditable="false" data-primary="vector-quantized variational autoencoders (VQ-VAEs)" data-secondary="training a VQ-VAE" data-type="indexterm" id="ch06_localization.html26"/>Now that we’ve broadly seen how a VQ-VAE is structured, let’s look at how it’s trained.
   </p>
  <p>Training a VQ-VAE involves optimizing more than just the <em>reconstruction loss</em>, that is, how closely the output image matches the original input. The model also needs to <em>refine the codebook vectors</em> over time and ensure that the encoder learns representations that snap cleanly to those vectors. This creates an interesting balance between three components:
   </p>
   <dl>
    <dt>The quality of the encoder’s representations</dt>
    <dd><p>How well does the encoder capture the meaningful signals in the input data?</p></dd>
    <dt>The usefulness and coverage of the codebook</dt>
    <dd><p>Does the codebook provide enough diversity to represent a wide range of inputs, and are all entries being used effectively?
     </p></dd>
    <dt>The decoder’s ability to reconstruct from quantized codes</dt>
    <dd><p>How accurately can the decoder recover the original input using only the compressed, discrete representations?
     </p></dd>
  </dl>
  <p>There are a few unique challenges in training VQ-VAEs to be aware of:
   </p>
   <dl class="pagebreak-after">
    <dt>Quantization error</dt>
    <dd><p>If the nearest codebook vector is a poor match, some detail is lost during quantization, and the reconstruction suffers.
     </p>
      <p>The key is to make sure your codebook is large enough and your encoder is flexible enough to produce embeddings that land near useful entries.</p></dd>
    <dt>Codebook collapse</dt>
    <dd><p>If only a few codebook vectors are used repeatedly while others are ignored, the model wastes capacity. Preventing this requires some extra care during training:
     </p>
     <dl>
      <dt>Commitment loss</dt>
      <dd>
       <p>This loss term encourages the encoder to commit to a chosen codebook vector by penalizing large differences between the encoder output and its nearest codebook entry. It keeps the encoder from drifting too far away from the codebook.
       </p>
      </dd>
      <dt>Entropy penalty</dt>
      <dd>
       <p>This encourages the model to use a wider range of codebook entries more evenly, increasing the diversity of representations and reducing collapse.
       </p>
      </dd>
    </dl>
    </dd>
  </dl>
  <p>Despite these challenges, the VQ-VAE is a powerful architecture, especially when you want a model that produces discrete, interpretable representations while still learning directly from data.
   </p>
  <div data-type="note" epub:type="note"><h6>Note</h6>
    <p>If this feels a bit complex, you’re not alone—engaging with modern deep learning models can be challenging at first. But hands-on practice is the best way to build intuition and fluency. In the next section, you’ll start building and training a VQ-VAE yourself, applying everything we’ve covered so far<a contenteditable="false" data-primary="" data-startref="ch06_localization.html26" data-type="indexterm" id="id980"/><a contenteditable="false" data-primary="" data-startref="ch06_localization.html25" data-type="indexterm" id="id981"/><a contenteditable="false" data-primary="" data-startref="ch06_localization.html24" data-type="indexterm" id="id982"/><a contenteditable="false" data-primary="" data-startref="ch06_localization.html23" data-type="indexterm" id="id983"/>.<a contenteditable="false" data-primary="" data-startref="ch06_localization.html6" data-type="indexterm" id="id984"/><a contenteditable="false" data-primary="" data-startref="ch06_localization.html5" data-type="indexterm" id="id985"/></p>
  </div>
  </div></section>
 </div></section>
 <section data-type="sect1" data-pdf-bookmark="Constructing the Dataset"><div class="sect1" id="constructing-the-dataset">
  <h1>Constructing the Dataset</h1>
 <p><a contenteditable="false" data-primary="cells, spatial organization patterns within" data-secondary="constructing the dataset" data-type="indexterm" id="ch06_localization.html27"/>All models need good data. In this section, we’ll take a closer look at how to prepare a cell imaging dataset to learn about protein localization.
  </p>
  <section data-type="sect2" data-pdf-bookmark="Data Requirements"><div class="sect2" id="data-requirements">
   <h2>Data Requirements</h2>
  <p><a contenteditable="false" data-primary="cells, spatial organization patterns within" data-secondary="constructing the dataset" data-tertiary="data requirements" data-type="indexterm" id="ch06_localization.html28"/>A high-quality dataset is crucial for successfully applying deep learning. Fortunately, there are several excellent protein localization resources available. One of the best is <a href="https://oreil.ly/km_v9">OpenCell</a>, which provides standardized, high-resolution images of human proteins inside cells, such as that shown in <a data-type="xref" href="#actb-opencell-screenshot">Figure 6-5</a>.</p>
  <p>Because all images were taken using a single, consistent imaging pipeline, they are particularly well suited for machine learning. This consistency ensures that the model focuses on learning biological variation in protein localization, rather than being distracted by irrelevant differences in imaging conditions or processing pipelines.
   </p>
   <!-- ASHLEY: the next div + p moved to precede figure#actb-opencell-screenshot
   to fill in a big whitespace gap -->
   <div data-type="note" epub:type="note"><h6>Note</h6>
   <p>Want to explore the dataset yourself? Head to <!--Do not shorten link--><a href="https://opencell.sf.czbiohub.org">OpenCell</a>, which has a beautifully designed interface that makes browsing the data fast and intuitive.
    </p>
   </div>
  <p>The model we’ll build in this chapter relies entirely on imaging data; it doesn’t use any labels or annotations during training. This is a major strength: it learns localization patterns directly from raw microscopy images, without any manual supervision. While OpenCell does provide localization annotations, we’ll only use them at the end to sanity-check our results. Overreliance on curated labels can introduce bias and limit scalability, as manual annotation is both expensive and subjective. This is where self-supervised learning shines—it enables models to uncover meaningful patterns without needing predefined labels.
   </p>
   <figure><div id="actb-opencell-screenshot" class="figure">
    <img alt="" src="assets/dlfb_0605.png" width="600" height="661"/>
    <h6><span class="label">Figure 6-5. </span>Example entry for the protein ACTB (beta-actin) from the OpenCell database. Each protein has a detailed summary page that includes its identifiers (e.g., UniProt ID), sequence information, expression levels (how many copies of the gene and protein are present in the cell), and subcellular localization annotations. On the right, you can see a fluorescence microscopy image showing ACTB localization (gray) alongside nuclear staining (blue). These multichannel images, together with protein-specific metadata, form the basis of the dataset used in this chapter.
    </h6>
   </div></figure>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Sourcing the Data"><div class="sect2" id="sourcing-the-data">
   <h2>Sourcing the Data</h2>
  <p><a contenteditable="false" data-primary="cells, spatial organization patterns within" data-secondary="constructing the dataset" data-tertiary="sourcing the data" data-type="indexterm" id="ch06_localization.html29"/>Preparing images for deep learning can be a challenge in itself, especially when it comes to format, consistency, and scale. We saw this previously in <a data-type="xref" data-xrefstyle="chap-num" href="ch05.html#detecting-skin-cancer-in-medical-images">Chapter 5</a>, where careful preprocessing was essential. Fortunately, the cytoself authors have released a preprocessed version of the dataset they used in their paper, so we don’t have to start from scratch here.
   </p>
  <p>The dataset contains 1,311 fluorescently tagged proteins, each imaged in roughly 18 different fields of view. From each field of view, about 45 individual crops are extracted, each typically containing around three cells. This yields roughly 800 image crops per protein, totaling more than 1,048,800 images in the full dataset.
Each image includes two channels:
   </p>
   <ul class="simple">
    <li><p>The green <em>protein channel</em>, showing the location of the fluorescently tagged protein of interest. This is the only channel used for training in this chapter.</p></li>
    <li><p>A <em>nuclear stain</em> using Hoechst, a blue fluorescent dye that binds strongly to DNA. This highlights the nuclei of the cells and provides spatial context, for example, helping to identify whether a protein is nuclear, cytoplasmic, or membrane bound.</p></li>
   </ul>
  <p>From the nuclear stain, a third representation is computed: the <em>distance-to-nucleus map</em>. This is not a separate imaging channel, but a spatial map derived from the Hoechst signal that gives the model more information about relative positioning:
   </p>
   <ul class="simple">
    <li><p>Pixels inside the nucleus are assigned positive values, representing the shortest distance to the nuclear boundary.</p></li>
    <li><p>Pixels outside the nucleus are assigned negative values, also based on shortest distance to the nuclear edge.</p></li>
   </ul>
  <p>To simplify the task, we will use only the tagged protein channel for model training. The other channel (Hoechst) and the derived distance map provide useful spatial context but are not required for this self-supervised learning setup.
   </p>
  <p>In addition to the image data, the dataset includes:
   </p>
   <ul>
    <li><p>Protein annotations: Such as gene names and unique IDs</p></li>
    <li><p>Curated localization labels: Manually assigned organelle or compartment <span class="keep-together">categories</span></p></li>
    </ul>
  <p class="pagebreak-before">These curated labels are <em>not</em> used during training. Instead, they’re only used afterward to evaluate whether the model has learned biologically meaningful representations on its own.
   </p>
   <div data-type="warning" epub:type="warning"><h6>Warning</h6>
   <p>This dataset is significantly larger than those used in previous chapters, introducing new challenges in terms of memory usage, data loading speed, and training efficiency.
    </p>
   <p>Make sure you have enough disk space and access to a capable GPU. For example, use an A100 available through Colab Pro+, or be prepared to scale down the model or batch size to make training feasible.<a contenteditable="false" data-primary="" data-startref="ch06_localization.html29" data-type="indexterm" id="id986"/>
    </p>
   </div>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Getting a Glimpse of the Dataset"><div class="sect2" id="getting-a-glimpse-of-the-dataset">
   <h2>Getting a Glimpse of the Dataset</h2>
  <p><a contenteditable="false" data-primary="cells, spatial organization patterns within" data-secondary="constructing the dataset" data-tertiary="getting a glimpse of the dataset" data-type="indexterm" id="ch06_localization.html30"/>For more complex datasets, it’s common to wrap the data loading and preprocessing logic in a custom <code>Dataset</code> class, as we have done in earlier chapters in this book. This helps make the data easier to explore and use in training.
   </p>
  <p>Let’s first use the utility function <code>get_dataset</code> to start inspecting a few of the image frames in the dataset; see
    <a data-type="xref" href="#random-frames-plot">Figure 6-6</a>. Sampling a handful of frames quickly reveals just how much variation there is in patterns and intensity across the dataset:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">jax</code>
<code class="kn">from</code> <code class="nn">dlfb.localization.dataset.utils</code> <code class="kn">import</code> <code class="n">get_dataset</code>
<code class="kn">from</code> <code class="nn">dlfb.utils.context</code> <code class="kn">import</code> <code class="n">assets</code>

<code class="n">rng</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">PRNGKey</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
<code class="n">rng</code><code class="p">,</code> <code class="n">rng_frames</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>

<code class="n">dataset</code> <code class="o">=</code> <code class="n">get_dataset</code><code class="p">(</code><code class="n">data_path</code><code class="o">=</code><code class="n">assets</code><code class="p">(</code><code class="s2">"localization/datasets"</code><code class="p">))</code>
<code class="n">n_frames</code> <code class="o">=</code> <code class="mi">16</code>
<code class="n">dataset</code><code class="o">.</code><code class="n">plot_random_frames</code><code class="p">(</code><code class="n">n</code><code class="o">=</code><code class="n">n_frames</code><code class="p">,</code> <code class="n">rng</code><code class="o">=</code><code class="n">rng_frames</code><code class="p">);</code>
</pre>
      
 <!-- ASHLEY: moved the following p + p to precede figure#random-frames-plot -->
  <p>This diversity is expected: each image corresponds to a different tagged protein, and many proteins aren’t restricted to a single cellular compartment. Instead, they localize to multiple regions of the cell, often with varying abundance. This adds another layer of complexity to the learning task.</p>
  <p>Interestingly, this heterogeneity isn’t just seen across different proteins. It also appears <em>within the same protein</em>, across different cells. Even when the protein and the imaging setup are held constant, localization patterns can vary from cell to cell.</p>
    
     <figure><div id="random-frames-plot" class="figure">
      <img alt="" src="assets/dlfb_0606.png" width="600" height="600"/>
      <h6><span class="label">Figure 6-6. </span>Plot of a random subset of frames. The protein symbols and their primary localization(s) are given for each frame (number indicates additional secondary localizations measured).
      </h6>
     </div></figure>
    
  <p>To illustrate this, we’ll load a dataset containing all image frames for a single protein, ACTB (beta-actin), and plot a few random samples. In
    <a data-type="xref" href="#sele-prot-random-frames-plot">Figure 6-7</a>, you’ll see that even for this single protein, there is substantial variation in signal strength, shape, and localization simply due to the nature of biological variability—remember that all frames were captured under identical experimental conditions.
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">selected_protein</code> <code class="o">=</code> <code class="s2">"ACTB"</code>
<code class="n">dataset</code><code class="o">.</code><code class="n">plot_random_frames</code><code class="p">(</code>
  <code class="n">n</code><code class="o">=</code><code class="n">n_frames</code><code class="p">,</code> <code class="n">with_labels</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code> <code class="n">rng</code><code class="o">=</code><code class="n">rng_frames</code><code class="p">,</code> <code class="n">gene_symbols</code><code class="o">=</code><code class="p">[</code><code class="n">selected_protein</code><code class="p">]</code>
<code class="p">);</code>
</pre>
      
     
    
    
     <figure class="pagebreak-after"><div id="sele-prot-random-frames-plot" class="figure">
      <img alt="" src="assets/dlfb_0607.png" width="600" height="600"/>
      <h6><span class="label">Figure 6-7. </span>Random subset of 16 image frames showing the localization of the protein ACTB (beta-actin), a cytoskeletal protein annotated as localizing to the <em>membrane</em>, <em>cytoskeleton</em>, and <em>cytoplasm</em>. Even though these frames all represent the same protein under identical experimental conditions, they display considerable variation in shape, brightness, and localization pattern, highlighting the inherent biological variability across cells.
      </h6>
     </div></figure>
    
   
  <p>Let’s now take a closer look at just a single frame in <a data-type="xref" href="#sele-prot-closeup-plot">Figure 6-8</a>, which will give us a better sense of what the model will be processing during training. The figure is deliberately shown large so that you can see the individual pixels clearly. Try to think of the image not just as a picture, but as a matrix of numbers: white pixels have values close to 255, black pixels are near 0, and all the shades of gray are values in between:
   </p>
     
       <pre data-type="programlisting" data-code-language="python"><code class="n">dataset</code><code class="o">.</code><code class="n">plot_random_frames</code><code class="p">(</code><code class="n">n</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">with_labels</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code> <code class="n">rng</code><code class="o">=</code><code class="n">rng_frames</code><code class="p">);</code>
</pre>
      
     
    
    
     <figure><div id="sele-prot-closeup-plot" class="figure">
      <img alt="" src="assets/dlfb_0608.png" width="600" height="600"/>
      <h6><span class="label">Figure 6-8. </span>Close-up view of a single image frame for the protein ACTB. The pixelated appearance reflects the raw input that the model will see: a 2D matrix of intensity values. Bright regions correspond to high fluorescence signal (values near 255), and dark regions correspond to low signal (values near 0). This is the actual numerical input used to learn spatial localization patterns.  Despite the images looking noisy or unclear to the human eye, the model must learn to extract consistent patterns from them.<a contenteditable="false" data-primary="" data-startref="ch06_localization.html30" data-type="indexterm" id="id987"/>
      </h6>
     </div></figure>
    
   
  </div></section>
  <section data-type="sect2" class="pagebreak-before less_space" data-pdf-bookmark="Implementing a DatasetBuilder Class"><div class="sect2" id="implementing-a-datasetbuilder-class">
   <h2>Implementing a DatasetBuilder Class</h2>
  <p><a contenteditable="false" data-primary="cells, spatial organization patterns within" data-secondary="constructing the dataset" data-tertiary="implementing a DatasetBuilder class" data-type="indexterm" id="ch06_localization.html31"/><a contenteditable="false" data-primary="DatasetBuilder class" data-type="indexterm" id="ch06_localization.html32"/>Here, we’ve implemented a helper class called <code>DatasetBuilder</code>, which handles dataset setup and caching behind the scenes:
   </p>
   
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">DatasetBuilder</code><code class="p">:</code>
  <code class="sd">"""Builds a dataset with splits for learning."""</code>

  <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">data_path</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code> <code class="n">force_recreate</code><code class="p">:</code> <code class="nb">bool</code> <code class="o">=</code> <code class="kc">False</code><code class="p">):</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">images</code> <code class="o">=</code> <code class="n">ImageLoader</code><code class="p">(</code><code class="n">data_path</code><code class="p">)</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="n">force_recreate</code><code class="o">=</code><code class="n">force_recreate</code><code class="p">)</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">labels</code> <code class="o">=</code> <code class="n">LabelLoader</code><code class="p">(</code><code class="n">data_path</code><code class="p">)</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="n">force_recreate</code><code class="o">=</code><code class="n">force_recreate</code><code class="p">)</code>

  <code class="k">def</code> <code class="nf">build</code><code class="p">(</code>
    <code class="bp">self</code><code class="p">,</code>
    <code class="n">rng</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code>
    <code class="n">splits</code><code class="p">:</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="nb">float</code><code class="p">],</code>
    <code class="n">exclusive_by</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="s2">"fov_id"</code><code class="p">,</code>
    <code class="n">n_proteins</code><code class="p">:</code> <code class="nb">int</code> <code class="o">|</code> <code class="kc">None</code> <code class="o">=</code> <code class="kc">None</code><code class="p">,</code>
    <code class="n">max_frames</code><code class="p">:</code> <code class="nb">int</code> <code class="o">|</code> <code class="kc">None</code> <code class="o">=</code> <code class="kc">None</code><code class="p">,</code>
  <code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="n">Dataset</code><code class="p">]:</code>
    <code class="sd">"""Retrieve a dataset of proteins split into learning sets."""</code>
    <code class="n">validate_splits</code><code class="p">(</code><code class="n">splits</code><code class="p">)</code>

    <code class="k">if</code> <code class="ow">not</code> <code class="n">n_proteins</code><code class="p">:</code>
      <code class="n">n_proteins</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">labels</code><code class="o">.</code><code class="n">get_n_proteins</code><code class="p">()</code>

    <code class="c1"># Sample frames from chosen proteins.</code>
    <code class="n">rng</code><code class="p">,</code> <code class="n">rng_proteins</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="n">num</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
    <code class="n">frames</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">labels</code><code class="o">.</code><code class="n">get_frames_of_random_proteins</code><code class="p">(</code><code class="n">rng_proteins</code><code class="p">,</code> <code class="n">n_proteins</code><code class="p">)</code>

    <code class="n">n_frames</code> <code class="o">=</code> <code class="n">frames</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>
    <code class="k">if</code> <code class="n">max_frames</code> <code class="ow">is</code> <code class="ow">not</code> <code class="kc">None</code> <code class="ow">and</code> <code class="n">n_frames</code> <code class="o">&gt;</code> <code class="n">max_frames</code><code class="p">:</code>
      <code class="c1"># Limit number of frames used.</code>
      <code class="n">frames</code> <code class="o">=</code> <code class="n">frames</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="n">max_frames</code><code class="p">)</code>
      <code class="n">n_frames</code> <code class="o">=</code> <code class="n">max_frames</code>

    <code class="c1"># Get random entities to exclusively be assigned across splits</code>
    <code class="n">rng</code><code class="p">,</code> <code class="n">rng_perm</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>
    <code class="n">set_ids</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">frames</code><code class="p">[</code><code class="n">exclusive_by</code><code class="p">]</code><code class="o">.</code><code class="n">to_numpy</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">int32</code><code class="p">))</code>
    <code class="n">shuffled_set_ids</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">permutation</code><code class="p">(</code><code class="n">rng_perm</code><code class="p">,</code> <code class="n">jnp</code><code class="o">.</code><code class="n">unique</code><code class="p">(</code><code class="n">set_ids</code><code class="p">))</code>

    <code class="c1"># Assign consecutive ids to proteins across all frames</code>
    <code class="n">frame_ids</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">frames</code><code class="p">[</code><code class="s2">"frame_id"</code><code class="p">]</code><code class="o">.</code><code class="n">to_numpy</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">int32</code><code class="p">))</code>
    <code class="n">lookup_with_protein_encoding</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">_encode_proteins_across_frames</code><code class="p">(</code>
      <code class="bp">self</code><code class="o">.</code><code class="n">labels</code><code class="o">.</code><code class="n">lookup</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="n">frame_ids</code><code class="o">.</code><code class="n">tolist</code><code class="p">()]</code>
    <code class="p">)</code>

    <code class="c1"># Assemble the dataset by splits considering exclusive sets</code>
    <code class="n">dataset_splits</code><code class="p">,</code> <code class="n">start</code> <code class="o">=</code> <code class="p">{},</code> <code class="mi">0</code>
    <code class="k">for</code> <code class="n">name</code><code class="p">,</code> <code class="n">size</code> <code class="ow">in</code> <code class="bp">self</code><code class="o">.</code><code class="n">_get_split_sizes</code><code class="p">(</code>
      <code class="n">splits</code><code class="p">,</code> <code class="n">n_sets</code><code class="o">=</code><code class="nb">len</code><code class="p">(</code><code class="n">shuffled_set_ids</code><code class="p">)</code>
    <code class="p">):</code>
      <code class="n">mask</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">isin</code><code class="p">(</code><code class="n">set_ids</code><code class="p">,</code> <code class="n">shuffled_set_ids</code><code class="p">[</code><code class="n">start</code> <code class="p">:</code> <code class="p">(</code><code class="n">start</code> <code class="o">+</code> <code class="n">size</code><code class="p">)])</code>
      <code class="n">dataset_splits</code><code class="p">[</code><code class="n">name</code><code class="p">]</code> <code class="o">=</code> <code class="n">Dataset</code><code class="p">(</code>
        <code class="n">images</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">images</code><code class="p">,</code>
        <code class="n">labels</code><code class="o">=</code><code class="n">Labels</code><code class="p">(</code>
          <code class="n">lookup</code><code class="o">=</code><code class="n">lookup_with_protein_encoding</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code>
            <code class="n">frame_ids</code><code class="p">[</code><code class="n">mask</code><code class="p">]</code><code class="o">.</code><code class="n">tolist</code><code class="p">()</code>
          <code class="p">]</code><code class="o">.</code><code class="n">reset_index</code><code class="p">(</code><code class="n">drop</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
        <code class="p">),</code>
      <code class="p">)</code>
      <code class="n">start</code> <code class="o">+=</code> <code class="n">size</code>

    <code class="k">return</code> <code class="n">dataset_splits</code>

  <code class="k">def</code> <code class="nf">_get_split_sizes</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">splits</code><code class="p">,</code> <code class="n">n_sets</code><code class="p">):</code>
    <code class="sd">"""Convert split fractional sizes to absolute counts."""</code>
    <code class="n">names</code> <code class="o">=</code> <code class="nb">list</code><code class="p">(</code><code class="n">splits</code><code class="o">.</code><code class="n">keys</code><code class="p">())</code>
    <code class="n">sizes</code> <code class="o">=</code> <code class="p">[</code><code class="nb">int</code><code class="p">(</code><code class="n">n_sets</code> <code class="o">*</code> <code class="n">splits</code><code class="p">[</code><code class="n">name</code><code class="p">])</code> <code class="k">for</code> <code class="n">name</code> <code class="ow">in</code> <code class="n">names</code><code class="p">[:</code><code class="o">-</code><code class="mi">1</code><code class="p">]]</code>
    <code class="n">sizes</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">n_sets</code> <code class="o">-</code> <code class="nb">sum</code><code class="p">(</code><code class="n">sizes</code><code class="p">))</code>  <code class="c1"># Ensure total adds up</code>
    <code class="k">for</code> <code class="n">name</code><code class="p">,</code> <code class="n">size</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">names</code><code class="p">,</code> <code class="n">sizes</code><code class="p">):</code>
      <code class="k">yield</code> <code class="n">name</code><code class="p">,</code> <code class="n">size</code>

  <code class="k">def</code> <code class="nf">_encode_proteins_across_frames</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">lookup</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">:</code>
    <code class="sd">"""Encode protein labels across dataset to consecutive integers."""</code>
    <code class="n">protein_ids_in_frames</code> <code class="o">=</code> <code class="n">lookup</code><code class="p">[</code><code class="s2">"protein_id"</code><code class="p">]</code><code class="o">.</code><code class="n">to_list</code><code class="p">()</code>
    <code class="n">unique_protein_ids</code> <code class="o">=</code> <code class="nb">sorted</code><code class="p">(</code><code class="nb">set</code><code class="p">(</code><code class="n">protein_ids_in_frames</code><code class="p">))</code>
    <code class="n">mapping</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code>
      <code class="p">[</code>
        <code class="p">{</code><code class="s2">"protein_id"</code><code class="p">:</code> <code class="n">id_</code><code class="p">,</code> <code class="s2">"code"</code><code class="p">:</code> <code class="n">idx</code><code class="p">}</code>
        <code class="k">for</code> <code class="n">idx</code><code class="p">,</code> <code class="n">id_</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">unique_protein_ids</code><code class="p">)</code>
      <code class="p">]</code>
    <code class="p">)</code>
    <code class="k">return</code> <code class="n">lookup</code><code class="o">.</code><code class="n">merge</code><code class="p">(</code><code class="n">mapping</code><code class="p">,</code> <code class="n">how</code><code class="o">=</code><code class="s2">"left"</code><code class="p">,</code> <code class="n">on</code><code class="o">=</code><code class="s2">"protein_id"</code><code class="p">)</code><code class="o">.</code><code class="n">set_index</code><code class="p">(</code>
      <code class="s2">"frame_id"</code><code class="p">,</code> <code class="n">drop</code><code class="o">=</code><code class="kc">False</code>
    <code class="p">)</code>
</pre>
      
     
    
   
  <p>During initialization, we provide the path to where the data is stored. This allows the <code>DatasetBuilder</code> to create linked instances of <code>Images</code> and <code>Labels</code>. You can also see that the <code>DatasetBuilder</code> has a single public method, <code>.build</code>, which returns a <code>dict[str, Dataset]</code> where every key is a dataset split and the value corresponds to a dataset. This method gives you the option to subset the dataset to a limited number of randomly selected proteins (<code>n_proteins</code>) and/or a maximum number of frames (<code>max_frames</code>).
   </p>
  <p>It also splits the dataset into different sets for the learning stage. The sets can be provided with the <code>splits</code> parameter where the names and fractional sizes can be requested. Finally, it is possible to request with the <code>exclusive_by</code> parameter that across splits, fields of view or proteins are never shared (with <code>fov_id</code> or <code>protein_id</code>, respectively). The former ensures that during learning, we do not inadvertently bleed information between splits, as two frames could otherwise capture an overlapping zone in a field of view. Not having the same proteins in the training and evaluation sets ensures that more general representations are learned, as was done in the original paper. Here we use <code>fov_id</code>, as we will restrict our dataset to a smaller number of proteins for faster training. Implementation-wise, we first randomly split the dataset by unique fields of view, and then, as we build the splits, we mask frames not to appear across splits. You will also have noticed that we are encoding the original protein IDs to consecutive integers, which, as you will see later, is required by the loss function during the training loop.
   </p>

   <section data-type="sect3" data-pdf-bookmark="Building a first dataset instance"><div class="sect3" id="bfdi">
    <h3>Building a first dataset instance</h3>
  <p>We’ll now look at using the <code>DatasetBuilder</code> to create instances of <code>Dataset</code> splits:
   </p> 
      
       <pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">dlfb.localization.dataset</code> <code class="kn">import</code> <code class="n">Dataset</code>
<code class="kn">from</code> <code class="nn">dlfb.localization.dataset.builder</code> <code class="kn">import</code> <code class="n">DatasetBuilder</code>
<code class="kn">from</code> <code class="nn">dlfb.utils.context</code> <code class="kn">import</code> <code class="n">assets</code>

<code class="n">builder</code> <code class="o">=</code> <code class="n">DatasetBuilder</code><code class="p">(</code><code class="n">data_path</code><code class="o">=</code><code class="n">assets</code><code class="p">(</code><code class="s2">"localization/datasets"</code><code class="p">))</code>

<code class="n">rng</code><code class="p">,</code> <code class="n">rng_dataset</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>
<code class="n">dataset</code><code class="p">:</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="n">Dataset</code><code class="p">]</code> <code class="o">=</code> <code class="n">builder</code><code class="o">.</code><code class="n">build</code><code class="p">(</code>
  <code class="n">rng</code><code class="o">=</code><code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">PRNGKey</code><code class="p">(</code><code class="mi">42</code><code class="p">),</code>
  <code class="n">splits</code><code class="o">=</code><code class="p">{</code><code class="s2">"train"</code><code class="p">:</code> <code class="mf">0.80</code><code class="p">,</code> <code class="s2">"valid"</code><code class="p">:</code> <code class="mf">0.10</code><code class="p">,</code> <code class="s2">"test"</code><code class="p">:</code> <code class="mf">0.10</code><code class="p">},</code>
  <code class="n">exclusive_by</code><code class="o">=</code><code class="s2">"fov_id"</code><code class="p">,</code>
  <code class="n">n_proteins</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code>
<code class="p">)</code>
</pre>
        
  <p>We now have a dataset. Having a dataset builder provides flexibility that makes it easy to explore, debug, and prototype with small slices of the dataset before scaling up to the full training set.
   </p>
 </div></section>

   <section data-type="sect3" data-pdf-bookmark="Accessing the dataset internals"><div class="sect3" id="accessing-the-dataset-internals">
    <h3>Accessing the dataset internals</h3>
   <p>Behind the scenes, the
     <code>Dataset</code> class contains two main components:
     <code>Images</code> and
     <code>Labels</code>. These handle the raw microscopy image data and the metadata annotations, respectively.
    </p>
   <p>You don’t need to dive into their implementations to follow along in this chapter, but if you’re curious, the full source code is available. The lower-level methods let you load specific proteins, subsample data, or query for localization annotations—helpful tools for deeper biological exploration or interactive experimentation.
    </p>
   <p>If you read the earlier chapters, this modular data structure should feel familiar. We assemble data from different blocks, use memory mapping for large arrays, and wrap everything into Python classes for convenience and speed. There are of course a lot of improvements that you can make to allow for even faster access to larger datasets, and <a href="https://oreil.ly/m29ho">TensorStore</a> certainly ticks a lot of boxes here.
    </p>
   <p>Now that we’ve examined how the data is structured and loaded, let’s move on to training our first model<a contenteditable="false" data-primary="" data-startref="ch06_localization.html32" data-type="indexterm" id="id988"/><a contenteditable="false" data-primary="" data-startref="ch06_localization.html31" data-type="indexterm" id="id989"/>.<a contenteditable="false" data-primary="" data-startref="ch06_localization.html27" data-type="indexterm" id="id990"/>
    </p>
   </div></section>
  </div></section>
 </div></section>
 <section data-type="sect1" data-pdf-bookmark="Building a Prototype Model"><div class="sect1" id="building-a-prototype-model">
  <h1>Building a Prototype Model</h1>
 <p><a contenteditable="false" data-primary="cells, spatial organization patterns within" data-secondary="building a prototype model" data-type="indexterm" id="ch06_localization.html33"/>In this section, we’ll build a simplified version of the cytoself model we introduced earlier. The aim is to distill the core architectural ideas into a somewhat compressed prototype that is easy to understand, train, and modify. The code implementation here is adapted in part from the official <a href="https://oreil.ly/KF0A0">Haiku VQ-VAE repository</a> and the <a href="https://oreil.ly/z-ZXk">VQ-VAE Flax implementation</a> from Arnaud Aillaud.
  </p>
 <p>We intentionally omit some of the more complex features from the original work, such as split quantization, hierarchical vector quantization, and multiresolution training, in order to keep the code accessible and focused on the key mechanisms. This also makes it easier to tinker and explore your own architectural experiments. Check out the final section of this chapter for more information on possible extensions to the model.
  </p>
  <section data-type="sect2" data-pdf-bookmark="Defining the LocalizationModel"><div class="sect2" id="defining-the-localizationmodel">
   <h2>Defining the LocalizationModel</h2>
  <p><a contenteditable="false" data-primary="cells, spatial organization patterns within" data-secondary="building a prototype model" data-tertiary="defining the LocalizationModel" data-type="indexterm" id="ch06_localization.html34"/>The following code defines the core model we’ll use throughout this chapter:
   </p>
   
       <pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">LocalizationModel</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
  <code class="sd">"""VQ-VAE model with a fully connected output head."""</code>

  <code class="n">embedding_dim</code><code class="p">:</code> <code class="nb">int</code>
  <code class="n">num_embeddings</code><code class="p">:</code> <code class="nb">int</code>
  <code class="n">commitment_cost</code><code class="p">:</code> <code class="nb">float</code>
  <code class="n">num_classes</code><code class="p">:</code> <code class="nb">int</code> <code class="o">|</code> <code class="kc">None</code>
  <code class="n">dropout_rate</code><code class="p">:</code> <code class="nb">float</code>
  <code class="n">classification_head_layers</code><code class="p">:</code> <code class="nb">int</code>

  <code class="k">def</code> <code class="nf">setup</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
    <code class="sd">"""Builds the encoder, decoder, quantizer, and output head."""</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">encoder</code> <code class="o">=</code> <code class="n">Encoder</code><code class="p">(</code><code class="n">latent_dim</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">embedding_dim</code><code class="p">)</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">vector_quantizer</code> <code class="o">=</code> <code class="n">VectorQuantizer</code><code class="p">(</code>
      <code class="n">num_embeddings</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">num_embeddings</code><code class="p">,</code>
      <code class="n">embedding_dim</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">embedding_dim</code><code class="p">,</code>
      <code class="n">commitment_cost</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">commitment_cost</code><code class="p">,</code>
    <code class="p">)</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">decoder</code> <code class="o">=</code> <code class="n">Decoder</code><code class="p">(</code><code class="n">latent_dim</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">embedding_dim</code><code class="p">)</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">classification_head</code> <code class="o">=</code> <code class="n">ClassificationHead</code><code class="p">(</code>
      <code class="n">num_classes</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">num_classes</code><code class="p">,</code>
      <code class="n">dropout_rate</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">dropout_rate</code><code class="p">,</code>
      <code class="n">layers</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">classification_head_layers</code><code class="p">,</code>
    <code class="p">)</code>

  <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code> <code class="n">is_training</code><code class="p">:</code> <code class="nb">bool</code><code class="p">):</code>
    <code class="sd">"""Runs a forward pass."""</code>
    <code class="n">ze</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">encoder</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">zq</code><code class="p">,</code> <code class="n">perplexity</code><code class="p">,</code> <code class="n">codebook_loss</code><code class="p">,</code> <code class="n">commitment_loss</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">vector_quantizer</code><code class="p">(</code><code class="n">ze</code><code class="p">)</code>
    <code class="n">decoded</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">decoder</code><code class="p">(</code><code class="n">zq</code><code class="p">)</code>
    <code class="n">logits</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">classification_head</code><code class="p">(</code>
      <code class="n">zq</code><code class="o">.</code><code class="n">reshape</code><code class="p">((</code><code class="n">zq</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="o">-</code><code class="mi">1</code><code class="p">)),</code> <code class="n">is_training</code>
    <code class="p">)</code>
    <code class="k">return</code> <code class="n">decoded</code><code class="p">,</code> <code class="n">perplexity</code><code class="p">,</code> <code class="n">codebook_loss</code><code class="p">,</code> <code class="n">commitment_loss</code><code class="p">,</code> <code class="n">logits</code>

  <code class="k">def</code> <code class="nf">create_train_state</code><code class="p">(</code>
    <code class="bp">self</code><code class="p">,</code> <code class="n">rng</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code> <code class="n">dummy_input</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code> <code class="n">tx</code>
  <code class="p">)</code> <code class="o">-&gt;</code> <code class="n">TrainState</code><code class="p">:</code>
    <code class="sd">"""Initializes training state."""</code>
    <code class="n">rng</code><code class="p">,</code> <code class="n">rng_init</code><code class="p">,</code> <code class="n">rng_dropout</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>
    <code class="n">variables</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">init</code><code class="p">(</code><code class="n">rng_init</code><code class="p">,</code> <code class="n">dummy_input</code><code class="p">,</code> <code class="n">is_training</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">TrainState</code><code class="o">.</code><code class="n">create</code><code class="p">(</code>
      <code class="n">apply_fn</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">apply</code><code class="p">,</code> <code class="n">params</code><code class="o">=</code><code class="n">variables</code><code class="p">[</code><code class="s2">"params"</code><code class="p">],</code> <code class="n">tx</code><code class="o">=</code><code class="n">tx</code><code class="p">,</code> <code class="n">key</code><code class="o">=</code><code class="n">rng_dropout</code>
    <code class="p">)</code>

  <code class="k">def</code> <code class="nf">get_encoding_indices</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">:</code>
    <code class="sd">"""Returns nearest codebook indices for input."""</code>
    <code class="n">ze</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">encoder</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">encoding_indices</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">vector_quantizer</code><code class="o">.</code><code class="n">get_closest_codebook_indices</code><code class="p">(</code><code class="n">ze</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">encoding_indices</code>
</pre>
  
  <p>This model is defined with the main components we already introduced:
   </p>
   <dl>
    <dt>Encoder</dt>
    <dd><p>Maps the input image into a latent space
     </p></dd>
    <dt>Vector quantizer</dt>
    <dd><p>Discretizes this latent space using a learned codebook
     </p></dd>
    <dt>Decoder</dt>
    <dd><p>Reconstructs the input image from the quantized representation
     </p></dd>
  </dl>
  <p>These are all wired together in the model’s <code>setup</code> method. You’ll also notice an additional component, the <code>ClassificationHead</code>, which we’ll come back to later. For now, just know that it’s used to mitigate the previously mentioned codebook collapse, a failure mode where the model uses only a small fraction of the codebook entries, reducing the representational power of the latent space.
   </p>
  <p>In the following sections, we’ll walk through each part of the model, starting with the <code>Encoder</code>, which is responsible for extracting latent features from the input microscopy images.<a contenteditable="false" data-primary="" data-startref="ch06_localization.html34" data-type="indexterm" id="id991"/>
   </p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="The Encoder: Processing Input Images"><div class="sect2" id="the-encoder-processing-input-images">
   <h2>The Encoder: Processing Input Images</h2>
  <p><a contenteditable="false" data-primary="cells, spatial organization patterns within" data-secondary="building a prototype model" data-tertiary="encoder for processing input images" data-type="indexterm" id="ch06_localization.html35"/><a contenteditable="false" data-primary="latent representation" data-type="indexterm" id="ch06_localization.html36"/>The <code>Encoder</code> is the first part of our model and is responsible for converting raw input images into a continuous latent representation. Its main job is to process the input into a form that the <code>VectorQuantizer</code> can work with—a spatial feature map with rich, expressive features per pixel. This latent representation will later be discretized by the <code>VectorQuantizer</code>, so it needs to be of the right shape and dimensionality.
   </p>
  <p>The encoder is built from three convolutional layers followed by two residual blocks:
   </p>
   <ul class="simple">
    <li><p>The first two conv layers downsample the image by a factor of 4 overall (each has a stride length of 2), reducing spatial resolution while increasing feature dimensionality.</p></li>
    <li><p>The third conv layer preserves spatial resolution but deepens the feature map.</p></li>
    <li><p>The two <code>ResnetBlock</code>s further refine the features with normalization, nonlinearity, and skip connections.</p></li>
   </ul>
  <p>Together, this forms the pipeline that turns a 100 × 100 grayscale image into a smaller grid of <code>latent_dim</code>-dimensional feature vectors, ready for quantization:
   </p>
   
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">Encoder</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
  <code class="sd">"""Convolutional encoder producing latent feature maps."""</code>

  <code class="n">latent_dim</code><code class="p">:</code> <code class="nb">int</code>

  <code class="k">def</code> <code class="nf">setup</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
    <code class="sd">"""Initializes convolutional and residual layers."""</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">conv1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Conv</code><code class="p">(</code>
      <code class="bp">self</code><code class="o">.</code><code class="n">latent_dim</code> <code class="o">//</code> <code class="mi">2</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="p">(</code><code class="mi">4</code><code class="p">,</code> <code class="mi">4</code><code class="p">),</code> <code class="n">strides</code><code class="o">=</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">2</code><code class="p">),</code> <code class="n">padding</code><code class="o">=</code><code class="mi">1</code>
    <code class="p">)</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">conv2</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Conv</code><code class="p">(</code>
      <code class="bp">self</code><code class="o">.</code><code class="n">latent_dim</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="p">(</code><code class="mi">4</code><code class="p">,</code> <code class="mi">4</code><code class="p">),</code> <code class="n">strides</code><code class="o">=</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">2</code><code class="p">),</code> <code class="n">padding</code><code class="o">=</code><code class="mi">1</code>
    <code class="p">)</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">conv3</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Conv</code><code class="p">(</code>
      <code class="bp">self</code><code class="o">.</code><code class="n">latent_dim</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="p">(</code><code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">),</code> <code class="n">strides</code><code class="o">=</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code> <code class="n">padding</code><code class="o">=</code><code class="mi">1</code>
    <code class="p">)</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">res_block1</code> <code class="o">=</code> <code class="n">ResnetBlock</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">latent_dim</code><code class="p">)</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">res_block2</code> <code class="o">=</code> <code class="n">ResnetBlock</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">latent_dim</code><code class="p">)</code>

  <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
    <code class="sd">"""Forward pass applying convolution and residual blocks to input."""</code>
    <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">conv1</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">conv2</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">conv3</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">res_block1</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">res_block2</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">x</code>


<code class="k">class</code> <code class="nc">ResnetBlock</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
  <code class="sd">"""Residual convolutional block with GroupNorm and Swish activation."""</code>

  <code class="n">latent_dim</code><code class="p">:</code> <code class="nb">int</code>

  <code class="k">def</code> <code class="nf">setup</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
    <code class="sd">"""Initializes normalization and convolutional layers."""</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">norm1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">GroupNorm</code><code class="p">()</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">conv1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Conv</code><code class="p">(</code>
      <code class="bp">self</code><code class="o">.</code><code class="n">latent_dim</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="p">(</code><code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">),</code> <code class="n">strides</code><code class="o">=</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code> <code class="n">padding</code><code class="o">=</code><code class="mi">1</code>
    <code class="p">)</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">norm2</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">GroupNorm</code><code class="p">()</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">conv2</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Conv</code><code class="p">(</code>
      <code class="bp">self</code><code class="o">.</code><code class="n">latent_dim</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="p">(</code><code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">),</code> <code class="n">strides</code><code class="o">=</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code> <code class="n">padding</code><code class="o">=</code><code class="mi">1</code>
    <code class="p">)</code>

  <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
    <code class="sd">"""Applies two conv layers with Swish activation and skip connection."""</code>
    <code class="n">h</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">swish</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">norm1</code><code class="p">(</code><code class="n">x</code><code class="p">))</code>
    <code class="n">h</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">conv1</code><code class="p">(</code><code class="n">h</code><code class="p">)</code>
    <code class="n">h</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">swish</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">norm2</code><code class="p">(</code><code class="n">h</code><code class="p">))</code>
    <code class="n">h</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">conv2</code><code class="p">(</code><code class="n">h</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">x</code> <code class="o">+</code> <code class="n">h</code>
</pre>
      
     
    
   
  <p>As you can see, the only parameter we are passing into the <code>Encoder</code> is <code>latent_dim</code>. This controls how many channels the network will output; in other words, how expressive each spatial location in the latent space is. In the context of our full model, this is set to <code>embedding_dim</code>, which ensures that the output from the encoder matches the dimensionality expected by the <code>VectorQuantizer</code>.
   </p>
  <p>In the context of a <code>LocalizationModel</code> model instantiation, it is set to <code>self.embedding_dim</code>, since the <code>Encoder</code> has to encode raw input images into a form that is compatible with the quantization step afterwards; in other words, it needs to have a compatible shape. In the context of the VQ-VAE, this is directly related to the quantized latent space dimensions, as it will need to return encoded images that are shaped correctly for the quantization process.
   </p>
  <p>The <code>ResnetBlock</code> uses <em>group normalization</em> and the <em>swish</em> activation function, a choice inspired by more recent diffusion models (like <em>stable diffusion</em>). As a quick reminder, the key idea with residual blocks is that the output is refined by residual learning: instead of trying to learn a full transformation of the input from scratch, the network learns a correction on top of the input. This typically helps with gradient flow and generalization.
   </p>
  <p>At the end of the encoder, we are left with a spatial feature map with shape <code>[batch_size, height, width, latent_dim]</code>, which will then be passed into the <code>VectorQuantizer</code>. This completes the encoding stage of the VQ-VAE architecture.
   </p>
   <div data-type="note" epub:type="note"><h6>Note</h6>
   <p>You can change up various parts of this encoder—convolution kernels, number of layers, activation functions, stride length, and so on. For simplicity, we don’t expose them as parameters to
     <code>Encoder</code> here, but these are all hyperparameters that you can try tuning to optimize performance.<a contenteditable="false" data-primary="" data-startref="ch06_localization.html36" data-type="indexterm" id="id992"/><a contenteditable="false" data-primary="" data-startref="ch06_localization.html35" data-type="indexterm" id="id993"/>
    </p>
   </div>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="The VectorQuantizer: Discretizing the Embeddings"><div class="sect2" id="the-vectorquantizer-discretizing-the-embeddings">
   <h2>The VectorQuantizer: Discretizing the Embeddings</h2>
  <p><a contenteditable="false" data-primary="cells, spatial organization patterns within" data-secondary="building a prototype model" data-tertiary="VectorQuantizer for discretizing embeddings" data-type="indexterm" id="ch06_localization.html37"/><a contenteditable="false" data-primary="VectorQuantizer class" data-type="indexterm" id="ch06_localization.html38"/>The <code>VectorQuantizer</code> is the heart of a VQ-VAE—it’s where we turn continuous embeddings into discrete codes. This step forces the model to commit to a limited vocabulary of learned feature vectors, improving compression and encouraging meaningful, reusable representations.
   </p>
  <p>Let’s break down the key components of the <code>VectorQuantizer</code> module:
   </p>
   
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">VectorQuantizer</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
  <code class="sd">"""Vector quantization module for VQ-VAE."""</code>

  <code class="n">num_embeddings</code><code class="p">:</code> <code class="nb">int</code>
  <code class="n">embedding_dim</code><code class="p">:</code> <code class="nb">int</code>
  <code class="n">commitment_cost</code><code class="p">:</code> <code class="nb">float</code>

  <code class="k">def</code> <code class="nf">setup</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
    <code class="sd">"""Initializes the codebook as trainable parameters."""</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">codebook</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">param</code><code class="p">(</code>
      <code class="s2">"codebook"</code><code class="p">,</code>
      <code class="n">nn</code><code class="o">.</code><code class="n">initializers</code><code class="o">.</code><code class="n">lecun_uniform</code><code class="p">(),</code>
      <code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">embedding_dim</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">num_embeddings</code><code class="p">),</code>
    <code class="p">)</code>

  <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">inputs</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">):</code>
    <code class="sd">"""Applies quantization and returns outputs with losses and perplexity."""</code>
    <code class="n">quantized</code><code class="p">,</code> <code class="n">encoding_indices</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">quantize</code><code class="p">(</code><code class="n">inputs</code><code class="p">)</code>
    <code class="n">codebook_loss</code><code class="p">,</code> <code class="n">commitment_loss</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">compute_losses</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="n">quantized</code><code class="p">)</code>
    <code class="n">perplexity</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">calculate_perplexity</code><code class="p">(</code><code class="n">encoding_indices</code><code class="p">)</code>
    <code class="n">ste</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">get_straight_through_estimator</code><code class="p">(</code><code class="n">quantized</code><code class="p">,</code> <code class="n">inputs</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">ste</code><code class="p">,</code> <code class="n">perplexity</code><code class="p">,</code> <code class="n">codebook_loss</code><code class="p">,</code> <code class="n">commitment_loss</code>

  <code class="k">def</code> <code class="nf">quantize</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">inputs</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">):</code>
    <code class="sd">"""Snaps inputs to nearest codebook entries."""</code>
    <code class="n">encoding_indices</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">get_closest_codebook_indices</code><code class="p">(</code><code class="n">inputs</code><code class="p">)</code>
    <code class="n">flat_quantized</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">take</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">codebook</code><code class="p">,</code> <code class="n">encoding_indices</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">swapaxes</code><code class="p">(</code>
      <code class="mi">1</code><code class="p">,</code> <code class="mi">0</code>
    <code class="p">)</code>
    <code class="n">quantized</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">flat_quantized</code><code class="p">,</code> <code class="n">inputs</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">quantized</code><code class="p">,</code> <code class="n">encoding_indices</code>

  <code class="k">def</code> <code class="nf">get_closest_codebook_indices</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">inputs</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">:</code>
    <code class="sd">"""Returns indices of closest codebook vectors."""</code>
    <code class="n">distances</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">calculate_distances</code><code class="p">(</code><code class="n">inputs</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">jnp</code><code class="o">.</code><code class="n">argmin</code><code class="p">(</code><code class="n">distances</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>

  <code class="k">def</code> <code class="nf">calculate_distances</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">inputs</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">:</code>
    <code class="sd">"""Computes Euclidean distances between inputs and codebook vectors."""</code>
    <code class="n">flat_inputs</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">embedding_dim</code><code class="p">))</code>
    <code class="n">distances</code> <code class="o">=</code> <code class="p">(</code>
      <code class="n">jnp</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">jnp</code><code class="o">.</code><code class="n">square</code><code class="p">(</code><code class="n">flat_inputs</code><code class="p">),</code> <code class="mi">1</code><code class="p">,</code> <code class="n">keepdims</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
      <code class="o">-</code> <code class="mi">2</code> <code class="o">*</code> <code class="n">jnp</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">flat_inputs</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">codebook</code><code class="p">)</code>
      <code class="o">+</code> <code class="n">jnp</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">jnp</code><code class="o">.</code><code class="n">square</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">codebook</code><code class="p">),</code> <code class="mi">0</code><code class="p">,</code> <code class="n">keepdims</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
    <code class="p">)</code>
    <code class="k">return</code> <code class="n">distances</code>

  <code class="k">def</code> <code class="nf">compute_losses</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">inputs</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code> <code class="n">quantized</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">):</code>
    <code class="sd">"""Computes codebook and commitment losses."""</code>
    <code class="n">codebook_loss</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">jnp</code><code class="o">.</code><code class="n">square</code><code class="p">(</code><code class="n">quantized</code> <code class="o">-</code> <code class="n">lax</code><code class="o">.</code><code class="n">stop_gradient</code><code class="p">(</code><code class="n">inputs</code><code class="p">)))</code>
    <code class="n">commitment_loss</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">commitment_cost</code> <code class="o">*</code> <code class="n">jnp</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code>
      <code class="n">jnp</code><code class="o">.</code><code class="n">square</code><code class="p">(</code><code class="n">lax</code><code class="o">.</code><code class="n">stop_gradient</code><code class="p">(</code><code class="n">quantized</code><code class="p">)</code> <code class="o">-</code> <code class="n">inputs</code><code class="p">)</code>
    <code class="p">)</code>
    <code class="k">return</code> <code class="n">codebook_loss</code><code class="p">,</code> <code class="n">commitment_loss</code>

  <code class="k">def</code> <code class="nf">calculate_perplexity</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">encoding_indices</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">:</code>
    <code class="sd">"""Computes codebook usage perplexity."""</code>
    <code class="n">encodings</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">one_hot</code><code class="p">(</code>
      <code class="n">encoding_indices</code><code class="p">,</code>
      <code class="bp">self</code><code class="o">.</code><code class="n">num_embeddings</code><code class="p">,</code>
    <code class="p">)</code>
    <code class="n">avg_probs</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">encodings</code><code class="p">,</code> <code class="mi">0</code><code class="p">)</code>
    <code class="n">perplexity</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">exp</code><code class="p">(</code><code class="o">-</code><code class="n">jnp</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">avg_probs</code> <code class="o">*</code> <code class="n">jnp</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="n">avg_probs</code> <code class="o">+</code> <code class="mf">1e-10</code><code class="p">)))</code>
    <code class="k">return</code> <code class="n">perplexity</code>

  <code class="nd">@staticmethod</code>
  <code class="k">def</code> <code class="nf">get_straight_through_estimator</code><code class="p">(</code>
    <code class="n">quantized</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code> <code class="n">inputs</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code>
  <code class="p">)</code> <code class="o">-&gt;</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">:</code>
    <code class="sd">"""Applies straight-through estimator to pass gradients through</code>
<code class="sd">    quantization.</code>
<code class="sd">    """</code>

    <code class="n">ste</code> <code class="o">=</code> <code class="n">inputs</code> <code class="o">+</code> <code class="n">lax</code><code class="o">.</code><code class="n">stop_gradient</code><code class="p">(</code><code class="n">quantized</code> <code class="o">-</code> <code class="n">inputs</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">ste</code>
</pre>
      
     
    
   
  <p>Following are the key parts of the <code>VectorQuantizer</code> class:
   </p>
   <dl>
    <dt>Codebook</dt>
    <dd><p>A learnable matrix of shape <code>(embedding_dim, num_embeddings)</code>, initialized with <code>lecun_uniform</code>. Each column is a codebook vector—essentially, a prototype that the model can match against. This matrix is what defines the discrete latent space.
     </p></dd>
    <dt class="less_space pagebreak-before">Quantization</dt>
    <dd><p>The <code>quantize()</code> function replaces each encoded input vector with the nearest codebook vector, based on Euclidean distance. This forces the model to express its understanding of each input frame using a fixed vocabulary of learned visual patterns.
     </p></dd>
    <dt>Losses</dt>
    <dd>
     <dl>
      <dt><code>codebook_loss</code></dt>
      <dd><p>Encourages codebook vectors to move toward the encoder output. This updates the codebook.</p></dd>
      <dt><code>commitment_loss</code></dt>
      <dd><p>Encourages the encoder output to commit to a chosen codebook vector rather than fluctuate. This updates the encoder. The two losses are combined to maintain a balance between encoder stability and codebook usage.</p></dd>
    </dl>
  </dd>
    <dt>Straight-through estimator (STE)</dt>
    <dd><p>Quantization is nondifferentiable: you can’t backpropagate through a hard lookup operation. The STE solves this by copying the quantized vector for the forward pass, but passing gradients as if the quantization didn’t happen. It’s a standard trick that allows training to proceed via approximate gradients.
     </p></dd>
    <dt>Perplexity</dt>
    <dd><p>A metric that tells us how many codebook vectors the model is using. High perplexity (close to <code>num_embeddings</code>) means the model is spreading its attention across many entries. Low perplexity means collapse—only a few vectors are being used, which limits the model’s capacity.
     </p></dd>
  </dl>
  <p>You can see that the parameters for this module are <code>num_embeddings</code>, <code>embedding_dim</code>, and <code>commitment_cost</code>, which define the structure of the discrete latent space:
   </p>
   <dl>
    <dt>Number of embeddings</dt>
    <dd><p>Determines how many discrete vectors the model has to choose from. A higher number allows for more fine-grained and diverse representations.
     </p></dd>
    <dt>Embedding dimension (also called latent dimension)</dt>
    <dd><p>Defines the richness of each vector. For example, if it’s 64, each codebook entry has 64 values. Higher-dimensional embeddings can capture more nuanced patterns, but they require more data and computation to train effectively.
     </p></dd>
    <dt>Commitment cost</dt>
    <dd><p>A weighting factor used in the loss function to penalize encoder outputs that deviate from their selected codebook vectors. If this value is too low, the encoder may ignore the codebook. If it’s too high, the encoder may be overly constrained and learn less expressive representations.
     </p></dd>
  </dl>
  <p class="pagebreak-before">Here is the main work the <code>VectorQuantizer</code> does during the forward pass, via its <code>__call__</code> method:
   </p>
   
    
     
      
       <pre data-type="programlisting" data-code-language="python">  <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">inputs</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">):</code>
    <code class="sd">"""Applies quantization and returns outputs with losses and perplexity."""</code>
    <code class="n">quantized</code><code class="p">,</code> <code class="n">encoding_indices</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">quantize</code><code class="p">(</code><code class="n">inputs</code><code class="p">)</code>
    <code class="n">codebook_loss</code><code class="p">,</code> <code class="n">commitment_loss</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">compute_losses</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="n">quantized</code><code class="p">)</code>
    <code class="n">perplexity</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">calculate_perplexity</code><code class="p">(</code><code class="n">encoding_indices</code><code class="p">)</code>
    <code class="n">ste</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">get_straight_through_estimator</code><code class="p">(</code><code class="n">quantized</code><code class="p">,</code> <code class="n">inputs</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">ste</code><code class="p">,</code> <code class="n">perplexity</code><code class="p">,</code> <code class="n">codebook_loss</code><code class="p">,</code> <code class="n">commitment_loss</code>
</pre>
      
     
    
   
  <p>This function performs four key operations:
   </p>
   <ul>
    <li><p>Quantization: It quantizes the input by replacing each encoded vector with the nearest entry from the codebook.
     </p></li>
    <li><p>Loss computation: It calculates two loss terms: one to pull codebook entries toward encoder outputs (<code>codebook_loss</code>) and another to encourage the encoder to stay near a codebook entry (<code>commitment_loss</code>).
     </p></li>
    <li><p>Perplexity: It calculates the codebook perplexity to assess how well the model is utilizing the full range of codebook entries.
     </p></li>
    <li><p>STE: It enables backpropagation through the nondifferentiable quantization step.
     </p></li>
    </ul>
  <p>Let’s walk through the quantization step in more detail:
   </p>
   
       <pre data-type="programlisting" data-code-language="python">  <code class="k">def</code> <code class="nf">quantize</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">inputs</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">):</code>
    <code class="sd">"""Snaps inputs to nearest codebook entries."""</code>
    <code class="n">encoding_indices</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">get_closest_codebook_indices</code><code class="p">(</code><code class="n">inputs</code><code class="p">)</code>
    <code class="n">flat_quantized</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">take</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">codebook</code><code class="p">,</code> <code class="n">encoding_indices</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">swapaxes</code><code class="p">(</code>
      <code class="mi">1</code><code class="p">,</code> <code class="mi">0</code>
    <code class="p">)</code>
    <code class="n">quantized</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">flat_quantized</code><code class="p">,</code> <code class="n">inputs</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">quantized</code><code class="p">,</code> <code class="n">encoding_indices</code>


  <code class="k">def</code> <code class="nf">calculate_distances</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">inputs</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">:</code>
    <code class="sd">"""Computes Euclidean distances between inputs and codebook vectors."""</code>
    <code class="n">flat_inputs</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">embedding_dim</code><code class="p">))</code>
    <code class="n">distances</code> <code class="o">=</code> <code class="p">(</code>
      <code class="n">jnp</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">jnp</code><code class="o">.</code><code class="n">square</code><code class="p">(</code><code class="n">flat_inputs</code><code class="p">),</code> <code class="mi">1</code><code class="p">,</code> <code class="n">keepdims</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
      <code class="o">-</code> <code class="mi">2</code> <code class="o">*</code> <code class="n">jnp</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">flat_inputs</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">codebook</code><code class="p">)</code>
      <code class="o">+</code> <code class="n">jnp</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">jnp</code><code class="o">.</code><code class="n">square</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">codebook</code><code class="p">),</code> <code class="mi">0</code><code class="p">,</code> <code class="n">keepdims</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
    <code class="p">)</code>
    <code class="k">return</code> <code class="n">distances</code>
</pre>
      
     
    
   
  <p class="pagebreak-before">In <code>quantize</code>, the goal is to replace each input vector with its nearest neighbor in the codebook. This is done in a few steps:
   </p>
   <ul>
    <li><p>Flatten the input so that we can process all spatial positions as a list of vectors.</p></li>
    <li><p>Compute distances between each input vector and all codebook vectors using Euclidean distance.</p></li>
    <li><p>Find the nearest codebook vector for each input location (<code>argmin</code> over distances).</p></li>
    <li><p>Gather the corresponding codebook entries using the indices.</p></li>
    <li><p>Reshape the quantized result back to the original input shape.</p></li>
   </ul>
  <p>The distances are computed using <code>calculate_distances</code>, which implements the squared Euclidean distance between a flattened input vector <code>x</code> and codebook vector <code>y</code>. This is based on the identity:
   </p>
   <div data-type="equation">
    <math display="block">
  <mrow>
    <msup><mrow><mo>∥</mo><mi>x</mi><mo>-</mo><mi>y</mi><mo>∥</mo></mrow> <mn>2</mn> </msup>
    <mo>=</mo>
    <msup><mrow><mo>∥</mo><mi>x</mi><mo>∥</mo></mrow> <mn>2</mn> </msup>
    <mo>-</mo>
    <mn>2</mn>
    <mrow>
      <mo>〈</mo>
      <mi>x</mi>
      <mo lspace="0%" rspace="0%">,</mo>
      <mi>y</mi>
      <mo>〉</mo>
    </mrow>
    <mo>+</mo>
    <msup><mrow><mo>∥</mo><mi>y</mi><mo>∥</mo></mrow> <mn>2</mn> </msup>
  </mrow>
</math>
   </div>
  <p>This formulation efficiently computes the distances using matrix operations.
   </p>
  <p>In summary, during the forward pass, the <code>VectorQuantizer</code> finds the best-matching codebook vector for each encoded input, replaces it, and enables gradients to flow using the STE. The result is a discretized latent representation that is both more structured and interpretable.
   </p>
   <section data-type="sect3" data-pdf-bookmark="Calculating VQ-VAE–specific losses"><div class="sect3" id="calculating-vq-vae-specific-losses">
    <h3>Calculating VQ-VAE–specific losses</h3>
   <p><a contenteditable="false" data-primary="VectorQuantizer class" data-secondary="calculating VQ–VAE–specific losses" data-type="indexterm" id="id994"/>With the quantized embeddings in hand, most of the hard work is done. But we still need to evaluate how well the original inputs are captured. In other words, we need to calculate the
     <em>VQ-VAE–specific</em> losses:
    </p>
   
<pre data-type="programlisting" data-code-language="python" class="pagebreak-after">  <code class="k">def</code> <code class="nf">compute_losses</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">inputs</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code> <code class="n">quantized</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">):</code>
    <code class="sd">"""Computes codebook and commitment losses."""</code>
    <code class="n">codebook_loss</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">jnp</code><code class="o">.</code><code class="n">square</code><code class="p">(</code><code class="n">quantized</code> <code class="o">-</code> <code class="n">lax</code><code class="o">.</code><code class="n">stop_gradient</code><code class="p">(</code><code class="n">inputs</code><code class="p">)))</code>
    <code class="n">commitment_loss</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">commitment_cost</code> <code class="o">*</code> <code class="n">jnp</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code>
      <code class="n">jnp</code><code class="o">.</code><code class="n">square</code><code class="p">(</code><code class="n">lax</code><code class="o">.</code><code class="n">stop_gradient</code><code class="p">(</code><code class="n">quantized</code><code class="p">)</code> <code class="o">-</code> <code class="n">inputs</code><code class="p">)</code>
    <code class="p">)</code>
    <code class="k">return</code> <code class="n">codebook_loss</code><code class="p">,</code> <code class="n">commitment_loss</code>
</pre>

   <p>There are two components here:
    </p>
    <dl>
      <dt>Codebook loss</dt>
     <dd><p>This term measures how far the quantized vectors are from the original encoder outputs. We want this difference to be small. Ideally, the quantized version should be nearly identical to what the encoder originally produced. The key detail is that <code>inputs</code> are wrapped in <code>lax.stop_gradient</code>. This prevents gradients from flowing back into the encoder so that only the codebook is updated to better match the encoder output.</p></dd>
    <dt>Commitment loss</dt>
     <dd><p>This encourages the encoder to produce outputs that are close to some entry in the codebook. It helps avoid drifting too far from quantized values.</p>
    <p>Here, <code>quantized</code> is wrapped in <code>lax.stop_gradient</code>. The gradient flows only to the encoder and not the codebook, encouraging it to “commit” to one of the existing codebook vectors. The <code>self.commitment_cost</code> parameter scales this loss to control how strongly the encoder is pulled toward existing codebook entries.</p></dd>
    </dl>
   <p>These two losses play complementary roles: one pulls the codebook toward the encoder outputs and the other pulls the encoder toward the codebook. Together, they ensure that both parts of the model co-adapt and stabilize over time, leading to a high-quality quantized latent space.
    </p>
   </div></section>
   <section data-type="sect3" data-pdf-bookmark="Using perplexity to measure codebook use"><div class="sect3" id="perplexity-measures-codebook-use">
    <h3>Using perplexity to measure codebook use</h3>
   <p><a contenteditable="false" data-primary="codebook" data-secondary="measuring use with perplexity" data-type="indexterm" id="ch06_localization.html39"/><a contenteditable="false" data-primary="perplexity" data-secondary="measuring codebook use with" data-type="indexterm" id="ch06_localization.html40"/><a contenteditable="false" data-primary="VectorQuantizer class" data-secondary="using perplexity to measure codebook use" data-type="indexterm" id="ch06_localization.html41"/>After quantization, we want to understand how well the model is using its codebook. Is it relying on just a few entries, or is it spreading its attention across many? That’s what the
     <em>perplexity</em> metric captures:
    </p>
    
     
      
       
        <pre data-type="programlisting" data-code-language="python">  <code class="k">def</code> <code class="nf">calculate_perplexity</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">encoding_indices</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">:</code>
    <code class="sd">"""Computes codebook usage perplexity."""</code>
    <code class="n">encodings</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">one_hot</code><code class="p">(</code>
      <code class="n">encoding_indices</code><code class="p">,</code>
      <code class="bp">self</code><code class="o">.</code><code class="n">num_embeddings</code><code class="p">,</code>
    <code class="p">)</code>
    <code class="n">avg_probs</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">encodings</code><code class="p">,</code> <code class="mi">0</code><code class="p">)</code>
    <code class="n">perplexity</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">exp</code><code class="p">(</code><code class="o">-</code><code class="n">jnp</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">avg_probs</code> <code class="o">*</code> <code class="n">jnp</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="n">avg_probs</code> <code class="o">+</code> <code class="mf">1e-10</code><code class="p">)))</code>
    <code class="k">return</code> <code class="n">perplexity</code>
</pre>
       
      
     
    
   <p>Let’s break this down:
    </p>
    <ul>
     <li><p>The <code>encoding_indices</code> gives the index of the selected codebook entry for each input.</p></li>
     <li><p>We one-hot encode these indices to count how often each codebook vector is used.</p></li>
     <li><p>Taking the average of this one-hot matrix gives a frequency distribution over the codebook entries.</p></li>
     <li><p>We then compute the entropy of this distribution, and we exponentiate it to get the perplexity.</p></li>
    </ul>
   <p>The result is a number between 1 and
     <code>num_embeddings</code>, indicating how many codebook entries are effectively in use.
    </p>
    <aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id995">
      <h1>Different Meanings of Perplexity</h1>
    <p><a contenteditable="false" data-primary="perplexity" data-secondary="different meanings of" data-type="indexterm" id="id996"/>The term <em>perplexity</em> is used across different areas of machine learning, where it reflects the entropy of a probability distribution.
     </p>
    <p><em>Entropy</em> is a measure of uncertainty or unpredictability. A distribution with <em>high entropy</em> spreads its weight relatively evenly across many options, meaning there’s more uncertainty about which option will be chosen. In contrast, <em>low entropy</em> means the distribution is concentrated on a few options, indicating higher certainty.
     </p>
    <p>In a VQ-VAE, high entropy in codebook usage (and thus high perplexity) is desirable. It means the model is using many different codebook vectors rather than relying on just a few.
     </p>
    <p>In language modeling, perplexity is also used, but with a slightly different interpretation: lower perplexity is better. It is computed over the predicted next token (or a masked token), and lower values indicate that the model assigns higher probability to the correct word, meaning it’s more confident and accurate in its predictions.<a contenteditable="false" data-primary="" data-startref="ch06_localization.html41" data-type="indexterm" id="id997"/><a contenteditable="false" data-primary="" data-startref="ch06_localization.html40" data-type="indexterm" id="id998"/><a contenteditable="false" data-primary="" data-startref="ch06_localization.html39" data-type="indexterm" id="id999"/>
     </p>
    </div></aside>
   </div></section>
   <section data-type="sect3" data-pdf-bookmark="Using the straight-through estimator"><div class="sect3" id="straight-through-estimator-ste">
    <h3>Using the straight-through estimator</h3>
   <p><a contenteditable="false" data-primary="straight-through estimator (STE)" data-type="indexterm" id="id1000"/><a contenteditable="false" data-primary="VectorQuantizer class" data-secondary="using the straight-through estimator" data-type="indexterm" id="id1001"/>The final operation in the
     <code>__call__</code> method of the
     <code>VectorQuantizer</code> is the computation of the STE:
    </p>
    
     
      
       
        <pre data-type="programlisting" data-code-language="python">  <code class="nd">@staticmethod</code>
  <code class="k">def</code> <code class="nf">get_straight_through_estimator</code><code class="p">(</code>
    <code class="n">quantized</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code> <code class="n">inputs</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code>
  <code class="p">)</code> <code class="o">-&gt;</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">:</code>
    <code class="sd">"""Applies straight-through estimator to pass gradients through</code>
<code class="sd">    quantization.</code>
<code class="sd">    """</code>
    <code class="n">ste</code> <code class="o">=</code> <code class="n">inputs</code> <code class="o">+</code> <code class="n">lax</code><code class="o">.</code><code class="n">stop_gradient</code><code class="p">(</code><code class="n">quantized</code> <code class="o">-</code> <code class="n">inputs</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">ste</code>
</pre>
    
    
   <p class="pagebreak-before">In a typical neural network, gradients are computed via backpropagation through a series of differentiable operations. However, <em>quantization isn’t differentiable</em>. It involves snapping continuous values to the nearest discrete codebook entry, and you can’t take a gradient through that. This poses a challenge for
gradient-based optimization.
    </p>
   <p>To solve this, we use the STE trick. It works like this:
    </p>
    <dl>
    <dt>Forward pass</dt>
     <dd><p>The output of this expression is quantized, so the decoder receives the discrete codebook entries.</p></dd>
    <dt>Backward pass</dt>
     <dd><p>Because <code>quantized - inputs</code> is wrapped with <code>stop_gradient</code>, it <em>has no effect on the gradients</em>. During backpropagation, only <code>inputs</code> contributes to the gradient. In other words, the gradient of the STE output with respect to the encoder input is treated as if the quantization step never happened.</p></dd>
    </dl>
   <p>This means that while the model behaves as if quantized during inference and reconstruction, the encoder receives useful gradients during training—treating quantization as if it were an identity function. Using the STE is essential for training models with discrete bottlenecks, like VQ-VAEs. It allows us to maintain the representational advantages of a discrete latent space, while still optimizing with gradient descent, the foundation of modern deep learning.
    </p>
    <aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1002">
      <h1>JAX’s stop_gradient</h1>
    <p><a contenteditable="false" data-primary="JAX" data-secondary="stop_gradient function" data-type="indexterm" id="id1003"/><a contenteditable="false" data-primary="stop_gradient (JAX function)" data-type="indexterm" id="id1004"/>JAX’s <code>stop_gradient</code> is a function that blocks the flow of gradients during backpropagation. It acts like an identity operation during the forward pass, but during the backward pass, it tells the model: “Don’t compute gradients through this expression.”</p>
    <p>You’ll see <code>stop_gradient</code> used in two main situations:
     </p>
     <ul class="simple">
      <li><p><em>To handle nondifferentiable operations</em>, such as vector quantization, where we want to bypass the operation during gradient computation while still using it in the forward pass.</p></li>      
      <li><p><em>To freeze part of a model</em> during fine-tuning; for example, running a pretrained backbone without updating its weights. Wrapping the backbone’s output in <code>stop_gradient</code> ensures that gradients only flow into newly added layers, like classification heads.</p></li>     
    </ul>
    <p>In earlier chapters, we froze parameters using optimizer-level tricks like <code>optax.multi_transform</code> and <code>set_to_zero</code>. These prevent <em>parameter updates</em>, but they still allow <em>gradients to be computed</em> and flow through the model.
     </p>
    <p>In contrast, <code>stop_gradient</code> blocks gradient flow entirely. It’s like cutting the gradient circuit at a specific point in the computation. Both approaches are valid and often interchangeable. Your choice just depends on the level of control you need. If you want to avoid updates but still propagate gradients (e.g., for analysis, visualization, or mechanisms that depend on gradient information), use the optimizer route. If you want to shut down all gradients past a point, use <code>stop_gradient</code>. It’s the more “nuclear” option.
     </p>
    </div></aside>
   <p>This covers the core of the VQ-VAE and the most complex aspect of this chapter. All that’s left now is the decoder—the final piece that turns the quantized latent codes back into a reconstructed image.<a contenteditable="false" data-primary="" data-startref="ch06_localization.html38" data-type="indexterm" id="id1005"/><a contenteditable="false" data-primary="" data-startref="ch06_localization.html37" data-type="indexterm" id="id1006"/>
    </p>
   </div></section>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Decoder: Decoding the Discretized Embeddings Back to Images"><div class="sect2" id="decoder-decoding-the-discretized-embeddings-back-to-images">
   <h2>Decoder: Decoding the Discretized Embeddings Back to Images</h2>
  <p><a contenteditable="false" data-primary="cells, spatial organization patterns within" data-secondary="building a prototype model" data-tertiary="Decoder stage" data-type="indexterm" id="ch06_localization.html42"/>The <code>Decoder</code> is the final stage of our VQ-VAE model. Conceptually, it mirrors the <code>Encoder</code>, but instead of compressing the image, it transforms the quantized latent representation back into the original image space. Its job is to turn the discrete, low-resolution feature map back into a full-resolution grayscale image.
   </p>
  <p>Here’s the code:
   </p>
     
       <pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">Decoder</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
  <code class="sd">"""Decoder module for reconstructing input from quantized representations."""</code>

  <code class="n">latent_dim</code><code class="p">:</code> <code class="nb">int</code>

  <code class="k">def</code> <code class="nf">setup</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="kc">None</code><code class="p">:</code>
    <code class="sd">"""Initializes residual blocks and upsampling layers."""</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">res_block1</code> <code class="o">=</code> <code class="n">ResnetBlock</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">latent_dim</code><code class="p">)</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">res_block2</code> <code class="o">=</code> <code class="n">ResnetBlock</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">latent_dim</code><code class="p">)</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">upsample1</code> <code class="o">=</code> <code class="n">Upsample</code><code class="p">(</code><code class="n">latent_dim</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">latent_dim</code> <code class="o">//</code> <code class="mi">2</code><code class="p">,</code> <code class="n">upfactor</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">upsample2</code> <code class="o">=</code> <code class="n">Upsample</code><code class="p">(</code><code class="n">latent_dim</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">upfactor</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>

  <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">:</code>
    <code class="sd">"""Applies the decoder to input and returns the reconstructed output."""</code>
    <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">res_block1</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">res_block2</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">upsample1</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">upsample2</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">x</code>


<code class="k">class</code> <code class="nc">Upsample</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
  <code class="sd">"""Upsampling block using bilinear interpolation followed by convolution."""</code>

  <code class="n">latent_dim</code><code class="p">:</code> <code class="nb">int</code>
  <code class="n">upfactor</code><code class="p">:</code> <code class="nb">int</code>

  <code class="k">def</code> <code class="nf">setup</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="kc">None</code><code class="p">:</code>
    <code class="sd">"""Initializes the convolutional layer for post-interpolation refinement."""</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">conv</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Conv</code><code class="p">(</code>
      <code class="bp">self</code><code class="o">.</code><code class="n">latent_dim</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="p">(</code><code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">),</code> <code class="n">strides</code><code class="o">=</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code> <code class="n">padding</code><code class="o">=</code><code class="mi">1</code>
    <code class="p">)</code>

  <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">:</code>
    <code class="sd">"""Upsamples input using bilinear interpolation and applies convolution."""</code>
    <code class="n">batch</code><code class="p">,</code> <code class="n">height</code><code class="p">,</code> <code class="n">width</code><code class="p">,</code> <code class="n">channels</code> <code class="o">=</code> <code class="n">x</code><code class="o">.</code><code class="n">shape</code>
    <code class="n">hidden_states</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">image</code><code class="o">.</code><code class="n">resize</code><code class="p">(</code>
      <code class="n">x</code><code class="p">,</code>
      <code class="n">shape</code><code class="o">=</code><code class="p">(</code>
        <code class="n">batch</code><code class="p">,</code>
        <code class="n">height</code> <code class="o">*</code> <code class="bp">self</code><code class="o">.</code><code class="n">upfactor</code><code class="p">,</code>
        <code class="n">width</code> <code class="o">*</code> <code class="bp">self</code><code class="o">.</code><code class="n">upfactor</code><code class="p">,</code>
        <code class="n">channels</code><code class="p">,</code>
      <code class="p">),</code>
      <code class="n">method</code><code class="o">=</code><code class="s2">"bilinear"</code><code class="p">,</code>
    <code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">conv</code><code class="p">(</code><code class="n">hidden_states</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">x</code>
</pre>
      
     
    
   
  <p>And here is the structure:
   </p>
   <ul class="simple">
    <li><p>Two <code>ResnetBlock</code>s refine the latent representation.</p></li>
    <li><p>Two <code>Upsample</code> layers then increase the spatial resolution step-by-step.</p></li>
    <li><p>The final output has shape <code>[batch_size, height, width, 1]</code>, a single-channel image.</p></li>
   </ul>
  <p>The <code>Upsample</code> module works like this:
   </p>
   <ul class="simple">
    <li><p>It uses bilinear interpolation to resize the feature maps to a larger spatial size (e.g., doubling width and height).</p></li>
    <li><p>Then, it applies a 3 × 3 convolution to learn a transformation of the upsampled features.</p></li>
   </ul>
  <p>As a reminder, <em>bilinear interpolation</em> resizes images by estimating new pixel values based on the four nearest neighbors. It performs linear interpolation twice: first along one axis (e.g., left to right) and then along the other (top to bottom). For example, when upsampling a 2 × 2 image to 3 × 3, the new center pixel is computed as a weighted average of the four corner values, creating smooth transitions. This avoids the blocky appearance of nearest-neighbor resizing, which simply assigns each new pixel the value of the single closest original pixel, leading to sharp, jagged edges.
   </p>
   <aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1007">
    <h1>Avoiding the Checkerboard Artifact</h1>
   <p><a contenteditable="false" data-primary="checkerboard artifact" data-type="indexterm" id="id1008"/>We use
     upsampling followed by convolution instead of transposed convolutions to avoid the <!--Do not shorten link--><a href="https://distill.pub/2016/deconv-checkerboard">checkerboard artifact</a>, a common issue that occurs when transposed convolutions produce uneven overlapping in the output pixels.
    </p>
   <p><a contenteditable="false" data-primary="deconvolution" data-type="indexterm" id="id1009"/><a contenteditable="false" data-primary="transposed convolution" data-type="indexterm" id="id1010"/>A
     <em>transposed convolution</em> (sometimes called a
     <em>deconvolution</em>) is an operation that increases spatial resolution. It is like the reverse of a regular convolution: it “spreads” input pixels into a larger grid, often used in decoders or generative models. However, due to the way it places values into the output grid, it can lead to artifacts if it is not very carefully designed.
    </p>
   <p>Instead, we explicitly upsample the feature map using bilinear interpolation (which increases spatial size smoothly), and then we apply a regular convolution to let the model learn features at the new resolution. This approach tends to produce more stable and visually coherent outputs, which is especially important in sensitive image reconstruction tasks like ours.<a contenteditable="false" data-primary="" data-startref="ch06_localization.html42" data-type="indexterm" id="id1011"/>
    </p>
   </div></aside>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="ClassificationHead: A Simple but Crucial Module"><div class="sect2" id="classificationhead-a-simple-but-crucial-module">
   <h2>ClassificationHead: A Simple but Crucial Module</h2>
  <p><a contenteditable="false" data-primary="cells, spatial organization patterns within" data-secondary="building a prototype model" data-tertiary="ClassificationHead module" data-type="indexterm" id="ch06_localization.html43"/><a contenteditable="false" data-primary="ClassificationHead module" data-type="indexterm" id="ch06_localization.html44"/>There is one final component in our model that we need to discuss: the <code>ClassificationHead</code>. This module performs a seemingly simple task—predicting protein IDs—but it turns out to be one of the most important parts of the architecture. The original Kobayashi paper discovered that this model block, which they called <em>FcBlock</em>, was actually crucial in guiding the model to learn general protein localization patterns.
   </p>
  <p>In essence, the <code>ClassificationHead</code> takes the quantized embeddings and tries to classify which protein the input microscopy image contained. This is implemented as a small, fully connected network (hence, “Fc”), with one or two dense layers, ReLU activations, and dropout:
   </p>
      
       <pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">ClassificationHead</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
  <code class="sd">"""Fully connected MLP head with optional dropout."""</code>

  <code class="n">num_classes</code><code class="p">:</code> <code class="nb">int</code>
  <code class="n">dropout_rate</code><code class="p">:</code> <code class="nb">float</code>
  <code class="n">layers</code><code class="p">:</code> <code class="nb">int</code>

  <code class="nd">@nn</code><code class="o">.</code><code class="n">compact</code>
  <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code> <code class="n">is_training</code><code class="p">:</code> <code class="nb">bool</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">:</code>
    <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">layers</code> <code class="o">-</code> <code class="mi">1</code><code class="p">):</code>
      <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="n">features</code><code class="o">=</code><code class="mi">1000</code><code class="p">)(</code><code class="n">x</code><code class="p">)</code>
      <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
      <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="n">rate</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">dropout_rate</code><code class="p">)(</code><code class="n">x</code><code class="p">,</code> <code class="n">deterministic</code><code class="o">=</code><code class="ow">not</code> <code class="n">is_training</code><code class="p">)</code>

    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="n">features</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">num_classes</code><code class="p">)(</code><code class="n">x</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">x</code>
</pre>
      
     
    
   
  <p>The parameters include:</p>
   <ul>
    <li><p><code>num_classes</code>: The number of protein IDs in the dataset</p></li>
    <li><p><code>dropout_rate</code>: Helps regularize the model and prevent overfitting</p></li>
    <li><p><code>layers</code>: Whether to use a one- or two-layer classifier</p></li>
   </ul>
  <p>Despite its simplicity, this block had the largest impact on performance in the original cytoself paper. It acts as a form of auxiliary task: the model is explicitly asked to predict the protein identity from its embedding; a task that is possible only if the embeddings encode useful spatial features.
   </p>

  
  <p>This changes the role of the embeddings: instead of only trying to minimize reconstruction loss, the model is now encouraged to organize the latent space in a way that helps with protein discrimination. This helps prevent codebook collapse and leads to much better localization-specific features.
   </p>

   <div data-type="note" epub:type="note"><h6>Note</h6>
    <p>Interestingly, this auxiliary protein classification task is hard for the model—and that makes intuitive sense. Many proteins are part of the same complex and share the same localization, making their image frames visually indistinguishable. But that’s the point: by attempting this difficult task, the model is pushed to extract subtle, generalizable cues related to localization, even if it doesn’t achieve perfect classification.</p>
  </div>

  <p>Later in the chapter, you’ll see the difference when this component is removed. It also demonstrates a broader lesson: <em>adding the right auxiliary task can transform a model’s ability to learn</em>. You can control how strongly this auxiliary task influences training via the <code>classification_weight</code> parameter.<a contenteditable="false" data-primary="" data-startref="ch06_localization.html44" data-type="indexterm" id="id1012"/><a contenteditable="false" data-primary="" data-startref="ch06_localization.html43" data-type="indexterm" id="id1013"/>
   </p>
  <p>We now have a model. Let’s train it.
   </p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Setting Up Model Training"><div class="sect2" id="setting-up-model-training">
   <h2>Setting Up Model Training</h2>
  <p><a contenteditable="false" data-primary="cells, spatial organization patterns within" data-secondary="building a prototype model" data-tertiary="setting up model training" data-type="indexterm" id="ch06_localization.html45"/><a contenteditable="false" data-primary="training" data-secondary="for learning spatial organization within cells" data-tertiary="prototype model" data-type="indexterm" id="ch06_localization.html46"/>We will now train the <code>LocalizationModel</code> model we’ve built. To begin, we’ll use a smaller number of image frames to allow for faster iteration and debugging.
   </p>
  <p class="pagebreak-after">The main training loop is defined in the <code>train</code> function. It sets up the training state, splits the data into batches, and iterates over the dataset for a number of epochs:
   </p>
   
    
     
      
       <pre data-type="programlisting" data-code-language="python" class="less_space"><code class="nd">@restorable</code>
<code class="k">def</code> <code class="nf">train</code><code class="p">(</code>
  <code class="n">state</code><code class="p">:</code> <code class="n">TrainState</code><code class="p">,</code>
  <code class="n">rng</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code>
  <code class="n">dataset_splits</code><code class="p">:</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="n">Dataset</code><code class="p">],</code>
  <code class="n">num_epochs</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code>
  <code class="n">batch_size</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code>
  <code class="n">classification_weight</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code>
  <code class="n">eval_every</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">10</code><code class="p">,</code>
<code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">tuple</code><code class="p">[</code><code class="n">TrainState</code><code class="p">,</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="nb">list</code><code class="p">[</code><code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="nb">float</code><code class="p">]]]]]:</code>
  <code class="sd">"""Train the VQ-VAE model with optional classification."""</code>
  <code class="c1"># Setup metrics logging</code>
  <code class="n">metrics</code> <code class="o">=</code> <code class="n">MetricsLogger</code><code class="p">()</code>

  <code class="n">epochs</code> <code class="o">=</code> <code class="n">tqdm</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="n">num_epochs</code><code class="p">))</code>
  <code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="n">epochs</code><code class="p">:</code>
    <code class="n">epochs</code><code class="o">.</code><code class="n">set_description</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Epoch </code><code class="si">{</code><code class="n">epoch</code> <code class="o">+</code> <code class="mi">1</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>
    <code class="n">rng</code><code class="p">,</code> <code class="n">rng_batch</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>

    <code class="c1"># Perform a training step on a batch of train data and log metrics.</code>
    <code class="k">for</code> <code class="n">batch</code> <code class="ow">in</code> <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">get_batches</code><code class="p">(</code>
      <code class="n">rng_batch</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="n">batch_size</code>
    <code class="p">):</code>
      <code class="n">rng</code><code class="p">,</code> <code class="n">rng_dropout</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>
      <code class="n">state</code><code class="p">,</code> <code class="n">batch_metrics</code> <code class="o">=</code> <code class="n">train_step</code><code class="p">(</code>
        <code class="n">state</code><code class="p">,</code> <code class="n">batch</code><code class="p">,</code> <code class="n">rng_dropout</code><code class="p">,</code> <code class="n">classification_weight</code>
      <code class="p">)</code>
      <code class="n">metrics</code><code class="o">.</code><code class="n">log_step</code><code class="p">(</code><code class="n">split</code><code class="o">=</code><code class="s2">"train"</code><code class="p">,</code> <code class="o">**</code><code class="n">batch_metrics</code><code class="p">)</code>

    <code class="c1"># Evaluate on the validation split</code>
    <code class="k">if</code> <code class="n">epoch</code> <code class="o">%</code> <code class="n">eval_every</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>
      <code class="n">rng</code><code class="p">,</code> <code class="n">rng_batch</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>
      <code class="k">for</code> <code class="n">batch</code> <code class="ow">in</code> <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"valid"</code><code class="p">]</code><code class="o">.</code><code class="n">get_batches</code><code class="p">(</code>
        <code class="n">rng_batch</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="n">batch_size</code>
      <code class="p">):</code>
        <code class="n">batch_metrics</code> <code class="o">=</code> <code class="n">eval_step</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">batch</code><code class="p">,</code> <code class="n">classification_weight</code><code class="p">)</code>
        <code class="n">metrics</code><code class="o">.</code><code class="n">log_step</code><code class="p">(</code><code class="n">split</code><code class="o">=</code><code class="s2">"valid"</code><code class="p">,</code> <code class="o">**</code><code class="n">batch_metrics</code><code class="p">)</code>

    <code class="n">metrics</code><code class="o">.</code><code class="n">flush</code><code class="p">(</code><code class="n">epoch</code><code class="o">=</code><code class="n">epoch</code><code class="p">)</code>
    <code class="n">epochs</code><code class="o">.</code><code class="n">set_postfix_str</code><code class="p">(</code><code class="n">metrics</code><code class="o">.</code><code class="n">latest</code><code class="p">([</code><code class="s2">"total_loss"</code><code class="p">]))</code>

  <code class="k">return</code> <code class="n">state</code><code class="p">,</code> <code class="n">metrics</code><code class="o">.</code><code class="n">export</code><code class="p">()</code>
</pre>
      
  <p class="pagebreak-after">The first thing to notice is that training proceeds in epochs, which form the main loop. As a reminder, an <em>epoch</em> is a full pass through the entire training set; every training example is seen once. Before we enter the loop for the first time, we initialize the training state so that we have a starting point. Then, the training begins with the first epoch.
   </p>
  <p>Before diving into the training logic, let’s briefly look at how the dataset is fed into the model. A key part of this is the <code>Dataset.get_batches</code> method, which handles how image examples are served during training:
   </p>
   
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">get_batches</code><code class="p">(</code>
    <code class="bp">self</code><code class="p">,</code>
    <code class="n">rng</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code>
    <code class="n">batch_size</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code>
  <code class="p">):</code>
    <code class="sd">"""Yields batches of image and label data for training or evaluation."""</code>
    <code class="n">frame_ids</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">labels</code><code class="o">.</code><code class="n">get_frame_ids</code><code class="p">()</code>

    <code class="n">n_frames</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">frame_ids</code><code class="p">)</code>
    <code class="n">batches_per_epoch</code> <code class="o">=</code> <code class="n">n_frames</code> <code class="o">//</code> <code class="n">batch_size</code>

    <code class="c1"># Shuffle data.</code>
    <code class="n">_</code><code class="p">,</code> <code class="n">rng_perm</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="n">num</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
    <code class="n">shuffled_idx</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">permutation</code><code class="p">(</code><code class="n">rng_perm</code><code class="p">,</code> <code class="n">n_frames</code><code class="p">)</code>

    <code class="c1"># The model has a softmax layer and expects consecutive integers.</code>
    <code class="n">all_labels</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">labels</code><code class="o">.</code><code class="n">lookup</code><code class="p">[[</code><code class="s2">"frame_id"</code><code class="p">,</code> <code class="s2">"code"</code><code class="p">]]</code><code class="o">.</code><code class="n">set_index</code><code class="p">(</code><code class="s2">"frame_id"</code><code class="p">)</code>

    <code class="k">for</code> <code class="n">idx_set</code> <code class="ow">in</code> <code class="n">shuffled_idx</code><code class="p">[:</code> <code class="n">batches_per_epoch</code> <code class="o">*</code> <code class="n">batch_size</code><code class="p">]</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code>
      <code class="p">(</code><code class="n">batches_per_epoch</code><code class="p">,</code> <code class="n">batch_size</code><code class="p">)</code>
    <code class="p">):</code>
      <code class="n">frame_id_set</code> <code class="o">=</code> <code class="n">frame_ids</code><code class="p">[</code><code class="n">idx_set</code><code class="p">]</code>
      <code class="k">yield</code> <code class="p">{</code>
        <code class="s2">"frame_ids"</code><code class="p">:</code> <code class="n">frame_id_set</code><code class="p">,</code>
        <code class="s2">"images"</code><code class="p">:</code> <code class="bp">self</code><code class="o">.</code><code class="n">images</code><code class="o">.</code><code class="n">frames</code><code class="p">[</code><code class="n">frame_id_set</code><code class="p">],</code>
        <code class="s2">"labels"</code><code class="p">:</code> <code class="n">all_labels</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">frame_id_set</code><code class="p">][</code><code class="s2">"code"</code><code class="p">]</code><code class="o">.</code><code class="n">to_numpy</code><code class="p">(</code><code class="n">dtype</code><code class="o">=</code><code class="nb">int</code><code class="p">),</code>
      <code class="p">}</code>
</pre>
      
   
  <p>You can see that we first select either the <em>training</em> or the <em>test</em> set of image frames, split them into batches of a preset size, and shuffle their indices. The protein labels for each frame are then encoded as integers so that they can be used with the <code>optax.softmax_cross_entropy_with_integer_labels</code> loss function. Finally, each batch yields the image data, the integer-encoded protein labels, and the corresponding frame IDs (which can be useful for analysis or visualization).
   </p>
  <p>Once the data is batched, it’s passed into two key functions: <code>train_step</code> for updating the model and <code>eval_step</code> for monitoring performance. Let’s take a closer look at <code>train_step</code>:
   </p>
   
       <pre data-type="programlisting" data-code-language="python"><code class="nd">@jax</code><code class="o">.</code><code class="n">jit</code>
<code class="k">def</code> <code class="nf">train_step</code><code class="p">(</code>
  <code class="n">state</code><code class="p">:</code> <code class="n">TrainState</code><code class="p">,</code>
  <code class="n">batch</code><code class="p">:</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">],</code>
  <code class="n">rng_dropout</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code>
  <code class="n">classification_weight</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code>
<code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">tuple</code><code class="p">[</code><code class="n">TrainState</code><code class="p">,</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="nb">float</code><code class="p">]]:</code>
  <code class="sd">"""Train for a single step."""</code>

  <code class="k">def</code> <code class="nf">calculate_loss</code><code class="p">(</code><code class="n">params</code><code class="p">:</code> <code class="nb">dict</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">tuple</code><code class="p">[</code><code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="nb">float</code><code class="p">]]:</code>
    <code class="sd">"""Forward pass and loss computation."""</code>
    <code class="p">(</code>
      <code class="n">x_recon</code><code class="p">,</code>
      <code class="n">perplexity</code><code class="p">,</code>
      <code class="n">codebook_loss</code><code class="p">,</code>
      <code class="n">commitment_loss</code><code class="p">,</code>
      <code class="n">logits</code><code class="p">,</code>
    <code class="p">)</code> <code class="o">=</code> <code class="n">state</code><code class="o">.</code><code class="n">apply_fn</code><code class="p">(</code>
      <code class="p">{</code><code class="s2">"params"</code><code class="p">:</code> <code class="n">params</code><code class="p">},</code>
      <code class="n">batch</code><code class="p">[</code><code class="s2">"images"</code><code class="p">],</code>
      <code class="n">is_training</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
      <code class="n">rngs</code><code class="o">=</code><code class="p">{</code><code class="s2">"dropout"</code><code class="p">:</code> <code class="n">rng_dropout</code><code class="p">},</code>
    <code class="p">)</code>

    <code class="n">loss_components</code> <code class="o">=</code> <code class="p">{</code>
      <code class="s2">"recon_loss"</code><code class="p">:</code> <code class="n">optax</code><code class="o">.</code><code class="n">squared_error</code><code class="p">(</code>
        <code class="n">predictions</code><code class="o">=</code><code class="n">x_recon</code><code class="p">,</code> <code class="n">targets</code><code class="o">=</code><code class="n">batch</code><code class="p">[</code><code class="s2">"images"</code><code class="p">]</code>
      <code class="p">)</code><code class="o">.</code><code class="n">mean</code><code class="p">(),</code>
      <code class="s2">"codebook_loss"</code><code class="p">:</code> <code class="n">codebook_loss</code><code class="p">,</code>
      <code class="s2">"commitment_loss"</code><code class="p">:</code> <code class="n">commitment_loss</code><code class="p">,</code>
      <code class="s2">"classification_loss"</code><code class="p">:</code> <code class="n">classification_weight</code>
      <code class="o">*</code> <code class="n">optax</code><code class="o">.</code><code class="n">softmax_cross_entropy_with_integer_labels</code><code class="p">(</code>
        <code class="n">logits</code><code class="o">=</code><code class="n">logits</code><code class="p">,</code> <code class="n">labels</code><code class="o">=</code><code class="n">batch</code><code class="p">[</code><code class="s2">"labels"</code><code class="p">]</code>
      <code class="p">)</code><code class="o">.</code><code class="n">mean</code><code class="p">(),</code>
    <code class="p">}</code>

    <code class="n">metrics</code> <code class="o">=</code> <code class="p">{</code>
      <code class="s2">"total_loss"</code><code class="p">:</code> <code class="n">sum_loss_components</code><code class="p">(</code><code class="o">**</code><code class="n">loss_components</code><code class="p">),</code>
      <code class="s2">"perplexity"</code><code class="p">:</code> <code class="n">perplexity</code><code class="p">,</code>
      <code class="s2">"accuracy"</code><code class="p">:</code> <code class="n">accuracy_score</code><code class="p">(</code><code class="n">batch</code><code class="p">[</code><code class="s2">"labels"</code><code class="p">],</code> <code class="n">y_pred</code><code class="o">=</code><code class="n">logits</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">)),</code>
      <code class="o">**</code><code class="n">loss_components</code><code class="p">,</code>
    <code class="p">}</code>
    <code class="k">return</code> <code class="n">metrics</code><code class="p">[</code><code class="s2">"total_loss"</code><code class="p">],</code> <code class="n">metrics</code>

  <code class="c1"># Compute gradients and apply update.</code>
  <code class="n">grad_fn</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">value_and_grad</code><code class="p">(</code><code class="n">calculate_loss</code><code class="p">,</code> <code class="n">has_aux</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
  <code class="p">(</code><code class="n">_</code><code class="p">,</code> <code class="n">metrics</code><code class="p">),</code> <code class="n">grads</code> <code class="o">=</code> <code class="n">grad_fn</code><code class="p">(</code><code class="n">state</code><code class="o">.</code><code class="n">params</code><code class="p">)</code>
  <code class="n">state</code> <code class="o">=</code> <code class="n">state</code><code class="o">.</code><code class="n">apply_gradients</code><code class="p">(</code><code class="n">grads</code><code class="o">=</code><code class="n">grads</code><code class="p">)</code>
  <code class="k">return</code> <code class="n">state</code><code class="p">,</code> <code class="n">metrics</code>
</pre>
      
  <p>Within the <code>train_step</code> function, the inner <code>calculate_loss</code> defines how the model’s loss is computed. This function is the core of the training step. It determines how well the model is performing and guides the weight updates to minimize the loss.
   </p>
  <p>First, we call <code>state.apply_fn</code>, which runs the <code>__call__</code> method of the <code>LocalizationModel</code> model. This returns the reconstruction (<code>x_recon</code>), the codebook-related losses (<code>codebook_loss</code> and <code>commitment_loss</code>), the classification logits, and the <code>perplexity</code> of the quantization.
   </p>
  <p class="pagebreak-before">We then compute two additional losses:
   </p>
   <ul>
    <li><p><code>recon_loss</code>: How different the reconstructed image is from the original, using squared error</p></li>
    <li><p><code>classification_loss</code>: How well the model predicts the protein ID from the image embedding, using cross-entropy</p></li>
   </ul>
  <p>These losses are assembled into <code>loss_components</code>, and then they are combined into a <code>total_loss</code> that drives training. Notably, the classification loss is multiplied by a <code>classification_weight</code>, allowing us to control how much it contributes to learning. Setting it to zero <em>ablates</em> (removes) the classification task, something we will test later in the chapter.
   </p>
  <p>Finally, we calculate evaluation metrics to track how training is progressing. These include:
   </p>
   <ul>
    <li><p><code>perplexity</code>: How effectively the model is using the codebook</p></li>
    <li><p><code>accuracy</code>: How effectively the model is predicting the protein IDs</p></li>
   </ul>
  <p>All of this is used to compute gradients via <code>jax.value_and_grad</code> and then update the model with <code>state.apply_gradients</code>. This design cleanly separates different objectives (reconstruction, quantization, classification) and lets you experiment with different trade-offs by adjusting loss weights.
   </p>
  <p>After each epoch, we store the metrics collected across batches. These include reconstruction loss, codebook and commitment losses, classification accuracy, and perplexity. The metrics are averaged across batches to give a summary per epoch, allowing us to monitor model progress and convergence.
   </p>
  <p>The <code>eval_step</code> is essentially the same as the <code>train_step</code>, with one key difference: it does not update the model weights. Instead, it runs the model in inference mode and is used to assess how well the current model performs on a held-out test set. This gives us an unbiased signal of generalization performance.
   </p>
  <p>We’ve now covered the model, dataset, and training logic. It’s finally time to give it a spin and see what it can learn<a contenteditable="false" data-primary="" data-startref="ch06_localization.html46" data-type="indexterm" id="id1014"/>.<a contenteditable="false" data-primary="" data-startref="ch06_localization.html45" data-type="indexterm" id="id1015"/><a contenteditable="false" data-primary="" data-startref="ch06_localization.html33" data-type="indexterm" id="id1016"/>
   </p>
  </div></section>
 </div></section>
 <section data-type="sect1" data-pdf-bookmark="Training with a Small Image Set"><div class="sect1" id="training-with-a-small-image-set">
  <h1>Training with a Small Image Set</h1>
 <p><a contenteditable="false" data-primary="cells, spatial organization patterns within" data-secondary="training with a small image set" data-type="indexterm" id="ch06_localization.html47"/><a contenteditable="false" data-primary="training" data-secondary="for learning spatial organization within cells" data-tertiary="small image set" data-type="indexterm" id="ch06_localization.html48"/>Let’s see the model in action. We’ll start by training it on a small subset of the data: 50 proteins, split into 80% training data, 10% validation data, and 10% test data, using a fixed random seed for reproducibility.
  </p>
 <p>We define our model architecture by setting the <code>embedding_dim</code>, <code>num_embeddings</code>, <code>commitment_cost</code>, <code>dropout_rate</code>,
   and <code>classification_head_layers</code>. Then, we <span class="keep-together">specify</span> the training parameters: number of <code>epochs</code>, <code>batch_size</code>, <code>learning_rate</code>, and <code>classification_weight</code>.
  </p>
 
     
      <pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">dlfb.localization.dataset.utils</code> <code class="kn">import</code> <code class="n">count_unique_proteins</code>
<code class="kn">from</code> <code class="nn">dlfb.localization.model</code> <code class="kn">import</code> <code class="n">LocalizationModel</code>
<code class="kn">from</code> <code class="nn">dlfb.localization.train</code> <code class="kn">import</code> <code class="n">train</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">LocalizationModel</code><code class="p">(</code>
  <code class="n">num_classes</code><code class="o">=</code><code class="n">count_unique_proteins</code><code class="p">(</code><code class="n">dataset_splits</code><code class="p">),</code>
  <code class="n">embedding_dim</code><code class="o">=</code><code class="mi">64</code><code class="p">,</code>
  <code class="n">num_embeddings</code><code class="o">=</code><code class="mi">512</code><code class="p">,</code>
  <code class="n">commitment_cost</code><code class="o">=</code><code class="mf">0.25</code><code class="p">,</code>
  <code class="n">dropout_rate</code><code class="o">=</code><code class="mf">0.45</code><code class="p">,</code>
  <code class="n">classification_head_layers</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code>
<code class="p">)</code>
</pre>
     
    
   
  
 <p>Now we can start training:</p>
  
   
    
     
      <pre data-type="programlisting" data-code-language="python"><code class="n">rng</code><code class="p">,</code> <code class="n">rng_init</code><code class="p">,</code> <code class="n">rng_train</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>

<code class="n">state</code><code class="p">,</code> <code class="n">metrics</code> <code class="o">=</code> <code class="n">train</code><code class="p">(</code>
  <code class="n">state</code><code class="o">=</code><code class="n">model</code><code class="o">.</code><code class="n">create_train_state</code><code class="p">(</code>
    <code class="n">rng</code><code class="o">=</code><code class="n">rng_init</code><code class="p">,</code>
    <code class="n">dummy_input</code><code class="o">=</code><code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">get_dummy_input</code><code class="p">(),</code>
    <code class="n">tx</code><code class="o">=</code><code class="n">optax</code><code class="o">.</code><code class="n">adam</code><code class="p">(</code><code class="mf">0.001</code><code class="p">),</code>
  <code class="p">),</code>
  <code class="n">rng</code><code class="o">=</code><code class="n">rng_train</code><code class="p">,</code>
  <code class="n">dataset_splits</code><code class="o">=</code><code class="n">dataset_splits</code><code class="p">,</code>
  <code class="n">num_epochs</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code>
  <code class="n">batch_size</code><code class="o">=</code><code class="mi">256</code><code class="p">,</code>
  <code class="n">classification_weight</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code>
  <code class="n">eval_every</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code>
  <code class="n">store_path</code><code class="o">=</code><code class="n">assets</code><code class="p">(</code><code class="s2">"localization/models/small"</code><code class="p">),</code>
<code class="p">)</code>
</pre>
     
    
   
  
 <p>After training, we now have a <code>LocalizationModel</code>
   model that has learned to compress, quantize, and reconstruct protein localization patterns, while also performing auxiliary classification. But how well has it actually learned? Let’s find out. We’ll start by visually inspecting its reconstructions.
  </p>
  <section data-type="sect2" data-pdf-bookmark="Inspecting Image Reconstruction"><div class="sect2" id="inspecting-image-reconstruction">
   <h2>Inspecting Image Reconstruction</h2>
  <p><a contenteditable="false" data-primary="cells, spatial organization patterns within" data-secondary="training with a small image set" data-tertiary="inspecting image reconstruction" data-type="indexterm" id="id1017"/>Before diving deeper, let’s do a quick sanity check: can the <code>LocalizationModel</code> model we just trained reconstruct the input images at all? If it has learned any meaningful representation of the input data, its reconstructions should at least vaguely resemble the original frames. We will evaluate this on the validation set of the dataset, that is, on frames that were never seen during training.
   </p>
  <p>Indeed, the model captures some of the structural features present in the inputs (see
    <a data-type="xref" href="#recon-plot">Figure 6-9</a>). The reconstructions are far from perfect—blurry and low resolution—but that’s OK. Remember, our goal isn’t to generate photorealistic images, it’s to learn discrete representations that encode spatial localization patterns. Reconstruction is just a training objective to help guide that process.
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">dlfb.localization.inspect.reconstruction</code> <code class="kn">import</code> <code class="n">show_reconstruction</code>

<code class="n">show_reconstruction</code><code class="p">(</code><code class="n">dataset</code><code class="p">[</code><code class="s2">"valid"</code><code class="p">],</code> <code class="n">state</code><code class="p">,</code> <code class="n">n</code><code class="o">=</code><code class="mi">8</code><code class="p">,</code> <code class="n">rng</code><code class="o">=</code><code class="n">rng_frames</code><code class="p">);</code>
</pre>
      
     
    
    
     <figure class="pagebreak-after"><div id="recon-plot" class="figure">
      <img alt="" src="assets/dlfb_0609.png" width="600" height="606"/>
      <h6><span class="label">Figure 6-9. </span>Reconstructed images of random proteins from the small-scale model. Each pair shows an input image (left) and its reconstruction (right). While blurry, the reconstructions often capture core structural features—a sign that the model is learning to encode localization-relevant information. Protein is indicated per panel (number indicates additional secondary localizations measured).
      </h6>
     </div></figure>
    
   
  </div></section>
  <section data-type="sect2" class="less_space" data-pdf-bookmark="Examining Evaluation Metrics Over Epochs"><div class="sect2" id="evaluation-metrics-over-epochs">
   <h2>Examining Evaluation Metrics Over Epochs</h2>
  <p><a contenteditable="false" data-primary="cells, spatial organization patterns within" data-secondary="training with a small image set" data-tertiary="examining evaluation metrics over epochs" data-type="indexterm" id="ch06_localization.html49"/>Let’s now look more closely at how training progressed by examining the loss curves (see
    <a data-type="xref" href="#losses-plot">Figure 6-10</a>). The left panel shows the four individual loss components used during training, while the right panel displays total training and test loss across epochs.
   </p>     
      
       <pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">dlfb.localization.inspect.metrics</code> <code class="kn">import</code> <code class="n">plot_losses</code>

<code class="n">plot_losses</code><code class="p">(</code><code class="n">metrics</code><code class="p">);</code>
</pre>
   
     <figure><div id="losses-plot" class="figure">
      <img alt="" src="assets/dlfb_0610.png" width="600" height="282"/>
      <h6><span class="label">Figure 6-10. </span>Training dynamics represented by individual loss components on the training set (left) and total training versus validation loss over epochs (right). All loss components decrease over time. While training loss continues to improve steadily, validation loss plateaus early, suggesting the model generalizes reasonably well but gains from further training may be limited.</h6>
     </div></figure>
    
   
  <p>As expected, all loss components steadily decrease over time—especially the classification loss, which dominates the total due to its larger magnitude. This makes sense: distinguishing between 50 proteins based solely on their localization patterns is a challenging task. Remember that we can always adjust the classification loss’s relative weight using the <code>classification_weight</code> parameter later.
   </p>
  <p class="pagebreak-after">Meanwhile, validation loss closely tracks training loss throughout, with only a slight and stable gap between them. There’s no clear sign of overfitting, which is encouraging given the small dataset.
   </p>
  <p>Next, let’s inspect how the codebook is being used—by looking at the evolution of its <em>perplexity</em> over time. In VQ-VAEs, perplexity measures how many codebook entries are effectively being used during vector quantization. If the model relies on just a handful of embeddings (e.g., 10 out of 512), the perplexity will be low. If it spreads usage more evenly across many entries, the perplexity rises, approaching the total number of codebook vectors available. In <a data-type="xref" href="#plot-perplexity">Figure 6-11</a> we see the evolution of perplexity:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">dlfb.localization.inspect.metrics</code> <code class="kn">import</code> <code class="n">plot_perplexity</code>

<code class="n">plot_perplexity</code><code class="p">(</code><code class="n">metrics</code><code class="p">);</code>
</pre>
      
     
    
    
     <figure><div id="plot-perplexity" class="figure">
      <img alt="" src="assets/dlfb_0611.png" width="600" height="500"/>
      <h6><span class="label">Figure 6-11. </span>Perplexity increases over epochs on both the training and validation sets. A rising perplexity indicates that the model is using more of the available codebook entries, rather than collapsing onto a small subset. The close alignment between training and validation perplexity suggests that this richer, more diverse representation generalizes well beyond the training data.
      </h6>
     </div></figure>
    
   
  <p class="pagebreak-after">As you can see in
    <a data-type="xref" href="#plot-perplexity">Figure 6-11</a>, perplexity steadily increases across epochs for both the training and validation sets. This is a strong signal that the model is learning to encode diverse spatial patterns using a broad vocabulary of codebook entries. Instead of relying on a narrow set of common features, it’s finding more nuanced ways to represent the variation in protein localization—effectively condensing complex microscopy images into compact, expressive codes.<a contenteditable="false" data-primary="" data-startref="ch06_localization.html49" data-type="indexterm" id="id1018"/>
   </p>
  </div></section>
  <section data-type="sect2" class="less_space" data-pdf-bookmark="Training a Model Without a Classification Task"><div class="sect2" id="model-without-classification-task">
   <h2>Training a Model Without a Classification Task</h2>
  <p><a contenteditable="false" data-primary="cells, spatial organization patterns within" data-secondary="training with a small image set" data-tertiary="training a model without a classification task" data-type="indexterm" id="ch06_localization.html51"/>Next, we’ll train the exact same model on the same data, but this time with the classification weight set to 0. This simulates removing the auxiliary protein ID classification task, in other words, disabling the <code>ClassificationHead</code>. Note that we are using the same random seed to provide the closest possible comparison. This lets us see how much that component helps guide the model’s learning.
   </p>
   
   <div data-type="tip"><h6>Tip</h6>
   <p>As a general design principle, it’s helpful to structure your code so that model components can be ablated (i.e., effectively removed) by setting their weight to zero via a config or flag—rather than rewriting or commenting out parts of the architecture. This makes it much easier to test hypotheses, compare model variants, and run controlled experiments. It’s a simple practice that promotes modular, reproducible research.</p>
   </div> 
     
   <p>Let’s give it a go:</p>

       <pre data-type="programlisting" data-code-language="python"><code class="n">state_alt</code><code class="p">,</code> <code class="n">metrics_alt</code> <code class="o">=</code> <code class="n">train</code><code class="p">(</code>
  <code class="n">state</code><code class="o">=</code><code class="n">model</code><code class="o">.</code><code class="n">create_train_state</code><code class="p">(</code>
    <code class="n">rng</code><code class="o">=</code><code class="n">rng_init</code><code class="p">,</code>
    <code class="n">dummy_input</code><code class="o">=</code><code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">get_dummy_input</code><code class="p">(),</code>
    <code class="n">tx</code><code class="o">=</code><code class="n">optax</code><code class="o">.</code><code class="n">adam</code><code class="p">(</code><code class="mf">0.001</code><code class="p">),</code>
  <code class="p">),</code>
  <code class="n">rng</code><code class="o">=</code><code class="n">rng_train</code><code class="p">,</code>
  <code class="n">dataset_splits</code><code class="o">=</code><code class="n">dataset_splits</code><code class="p">,</code>
  <code class="n">num_epochs</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code>
  <code class="n">batch_size</code><code class="o">=</code><code class="mi">256</code><code class="p">,</code>
  <code class="n">classification_weight</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code>  <code class="c1"># i.e. the protein id are ignored</code>
  <code class="n">eval_every</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code>
  <code class="n">store_path</code><code class="o">=</code><code class="n">assets</code><code class="p">(</code><code class="s2">"localization/models/small_alt"</code><code class="p">),</code>
<code class="p">)</code>
</pre>
      
     
    
   
  <p>This time around, the model performs much worse, and its perplexity collapses (down to ~30, previously ~350), as shown in
    <a data-type="xref" href="#alt-plot-perplexity">Figure 6-12</a>.
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">plot_perplexity</code><code class="p">(</code><code class="n">metrics_alt</code><code class="p">);</code>
</pre>
      
     
    
    
     <figure><div id="alt-plot-perplexity" class="figure">
      <img alt="" src="assets/dlfb_0612.png" width="600" height="501"/>
      <h6><span class="label">Figure 6-12. </span>Perplexity over epochs collapses without the <code>ClassificationHead</code> in the model architecture, suggesting that the auxiliary protein identification task plays a critical role in encouraging a diverse and informative representation.
      </h6>
     </div></figure>
    
   
  <p>Perhaps counterintuitively, this model’s reconstructed microscopy images actually look a bit <em>better</em> in <a data-type="xref" href="#alt-recon-plot">Figure 6-13</a>.
   </p>
  

      
       <pre data-type="programlisting" data-code-language="python"><code class="n">show_reconstruction</code><code class="p">(</code><code class="n">dataset</code><code class="p">[</code><code class="s2">"valid"</code><code class="p">],</code> <code class="n">state_alt</code><code class="p">,</code> <code class="n">n</code><code class="o">=</code><code class="mi">8</code><code class="p">,</code> <code class="n">rng</code><code class="o">=</code><code class="n">rng_frames</code><code class="p">);</code>
</pre>
      

  <!-- ASHLEY: moved the next p*4 to precede figure#alt-recon-plot -->

  <p>At first glance, this might seem like an improvement, but it actually reveals a critical trade-off in the model’s objectives.
   </p>
  <p>Without the auxiliary classification task (i.e., without the <code>ClassificationHead</code>), the model can focus more of its efforts on minimizing the reconstruction loss. This encourages it to memorize the input data as precisely as possible, often by collapsing to a small number of frequently used codebook entries. That’s why the reconstructions look sharper: the model has overfit to pixel-level detail rather than learning generalizable representations.
   </p>
  <p>But VQ-VAEs aren’t just about pretty reconstructions. They’re about learning discrete, structured representations of the input. With the classification task enabled, the model is forced to organize its internal representations in a way that’s useful for predicting protein identity. This encourages it to capture high-level biological features, such as localization patterns, at the cost of slightly blurrier reconstructions.
   </p>
  <p>This same observation was highlighted in the original cytoself paper: adding classification-like objectives improves the quality of the latent space by pushing the model to encode meaningful, discriminative features. In fact, cytoself showed that models trained only to reconstruct images were far less effective at clustering localizations or identifying complexes, even when their reconstructions looked visually fine.
   </p>
    
     <figure><div id="alt-recon-plot" class="figure">
      <img alt="" src="assets/dlfb_0613.png" width="600" height="606"/>
      <h6><span class="label">Figure 6-13. </span>Reconstructed images appear slightly more faithful to the input when the <code>ClassificationHead</code> is removed—likely because the model can focus its efforts on the reconstruction task, resulting in less blurring.
      </h6>
     </div></figure>
    
   <aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1019">
    <h1>How Auxiliary Tasks Can Help</h1>
   <p><a contenteditable="false" data-primary="auxiliary tasks" data-type="indexterm" id="id1020"/>An
     <em>auxiliary task</em> is a secondary prediction task that helps the model learn better internal representations, and they can be very helpful to machine learning models. Adding a related task guides the model to focus on meaningful structure in the data. You implement this by adding additional heads to your model and literally summing their loss values during training to get the total loss. You can control how much each task contributes by adjusting the loss weights.
    </p>
   <p>For example, for the chapter on skin cancer, you could implement such auxiliary tasks as predicting
     <em>tissue type</em>,
     <em>cell density</em>, or even
     <em>tumor grade</em>. One popular approach in the cancer imaging space is to add a segmentation task (pixel-wise labeling), such as detecting cell nuclei or highlighting tumor regions, to help the model learn more localized, interpretable features.
    </p>
   <p>These auxiliary heads can be safely discarded after training, or they can be kept if they’re useful. Even when thrown away, they shape the learned representations in ways that often improve generalization, robustness, and interpretability<a contenteditable="false" data-primary="" data-startref="ch06_localization.html51" data-type="indexterm" id="id1021"/>.<a contenteditable="false" data-primary="" data-startref="ch06_localization.html48" data-type="indexterm" id="id1022"/><a contenteditable="false" data-primary="" data-startref="ch06_localization.html47" data-type="indexterm" id="id1023"/>
    </p>
   </div></aside>
  </div></section>
 </div></section>
 <section data-type="sect1" data-pdf-bookmark="Understanding the Model"><div class="sect1" id="understanding-the-model">
  <h1>Understanding the Model</h1>
 <p><a contenteditable="false" data-primary="cells, spatial organization patterns within" data-secondary="understanding the model" data-type="indexterm" id="ch06_localization.html53"/>What has the model actually learned about spatial organization within cells? To answer this question, we need to take a closer look at the model’s <em>latent space</em>: the internal representation it builds from the input images. In particular, since our model uses a codebook, we can analyze <em>which</em>
   entries get used and <em>how often</em>
   for each protein. This gives us a kind of <em>feature spectrum</em> or a summary of how each protein maps onto the learned visual vocabulary.
  </p>
  <section data-type="sect2" data-pdf-bookmark="Understanding Localization Clustering"><div class="sect2" id="clustering-of-localization">
   <h2>Understanding Localization Clustering</h2>
  <p><a contenteditable="false" data-primary="cells, spatial organization patterns within" data-secondary="understanding the model" data-tertiary="understanding location clustering" data-type="indexterm" id="ch06_localization.html54"/><a contenteditable="false" data-primary="location clustering" data-type="indexterm" id="ch06_localization.html55"/><a contenteditable="false" data-primary="uniform manifold approximation and projection (UMAP)" data-type="indexterm" id="ch06_localization.html56"/>A natural question is whether the model can distinguish between different subcellular compartments. One way to test this is to apply the uniform manifold approximation and projection (UMAP) dimensionality reduction technique to the learned embeddings. UMAP projects high-dimensional data into two dimensions while preserving local structure, making it easier to visualize complex relationships.
   </p>
   <div data-type="tip"><h6>Tip</h6>
   <p>There are other dimensionality reduction techniques, such as <em>t-SNE</em> and <em>PCA</em>, but <em>UMAP</em> is especially well suited for this task. Unlike PCA, it can capture complex nonlinear relationships. UMAP is similar to t-SNE in many ways, but with a key advantage: it tends to preserve both local <em>and</em> global structure more effectively. This makes it especially useful for visualizing patterns across diverse datasets like this one.
    </p>
   <p>UMAP has become a go-to tool in fields like proteomics and genomics—not just for its performance, but also because it works well out of the box, with minimal hyperparameter tuning.
    </p>
   </div>
  <p>If the model has learned meaningful spatial features, we’d expect image frames from similar cellular compartments—like the mitochondria, nucleus, or ER—to cluster together in the UMAP space. To visualize this, we need to extract the model’s internal representation of each image.
   </p>
  <p>The function <code>get_frame_encoding_index_histogram</code> does this by calculating a codebook usage histogram for each frame:
   </p>

      
       <pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">get_frame_embeddings</code><code class="p">(</code>
  <code class="n">state</code><code class="p">:</code> <code class="n">TrainState</code><code class="p">,</code>
  <code class="n">dataset_split</code><code class="p">:</code> <code class="n">Dataset</code><code class="p">,</code>
  <code class="n">batch_size</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">256</code><code class="p">,</code>
<code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">ndarray</code><code class="p">]:</code>
  <code class="sd">"""Returns per-frame histograms of codebook encoding indices."""</code>
  <code class="n">num_embeddings</code> <code class="o">=</code> <code class="n">get_num_embeddings</code><code class="p">(</code><code class="n">state</code><code class="p">)</code>
  <code class="n">frame_ids</code><code class="p">,</code> <code class="n">frame_histograms</code> <code class="o">=</code> <code class="p">[],</code> <code class="p">[]</code>

  <code class="n">rng</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">PRNGKey</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
  <code class="k">for</code> <code class="n">batch</code> <code class="ow">in</code> <code class="n">dataset_split</code><code class="o">.</code><code class="n">get_batches</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="n">batch_size</code><code class="p">):</code>
    <code class="n">frame_ids</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">batch</code><code class="p">[</code><code class="s2">"frame_ids"</code><code class="p">])</code>
    <code class="n">encoding_indices</code> <code class="o">=</code> <code class="n">pluck_encodings</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">batch</code><code class="p">)</code>

    <code class="c1"># Reshape and count codebook usage per frame.</code>
    <code class="n">frame_histograms</code><code class="o">.</code><code class="n">append</code><code class="p">(</code>
      <code class="n">np</code><code class="o">.</code><code class="n">apply_along_axis</code><code class="p">(</code>
        <code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">histogram</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">bins</code><code class="o">=</code><code class="n">np</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="n">num_embeddings</code> <code class="o">+</code> <code class="mf">0.5</code><code class="p">))[</code><code class="mi">0</code><code class="p">],</code>
        <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code>
        <code class="n">arr</code><code class="o">=</code><code class="n">jnp</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">encoding_indices</code><code class="p">,</code> <code class="p">(</code><code class="n">batch_size</code><code class="p">,</code> <code class="o">-</code><code class="mi">1</code><code class="p">)),</code>
      <code class="p">)</code>
    <code class="p">)</code>

  <code class="k">return</code> <code class="p">{</code>
    <code class="s2">"frame_ids"</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">concatenate</code><code class="p">(</code><code class="n">frame_ids</code><code class="p">),</code>
    <code class="s2">"frame_histograms"</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">concatenate</code><code class="p">(</code><code class="n">frame_histograms</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">),</code>
  <code class="p">}</code>
</pre>
      
     
 
  <p>For each batch of images, we use the model to obtain the encoding indices: the discrete codebook entries selected for each spatial patch in each frame. These are returned by the <code>pluck_encodings</code> helper function. Each frame’s output is then flattened and passed to <code>np.histogram</code>, which counts how often each codebook entry is used.
   </p>
  <p>The result is a histogram vector per frame—one value per codebook entry—describing how frequently that entry was activated. These vectors can be thought of as discrete fingerprints of each image, which we can then reduce to 2D with UMAP for visualization.
   </p>
   <div data-type="note" epub:type="note"><h6>Note</h6>
   <p>Why proceed in batches? Batching during the embedding extraction—just like during training—helps manage memory and computation. It ensures that we don’t load the entire dataset into memory at once. Note that the batch size used here doesn’t have to match the training batch size.
    </p>
   </div>
  <p>We’ll generate projections for two versions of our model: one with the <code>ClassificationHead</code> enabled and one without. By plotting both side-by-side, we can observe how the presence of the <code>ClassificationHead</code> affects the structure of the learned embedding space. We continue using the validation dataset to evaluate our model. The resulting visualization is shown in <a data-type="xref" href="#projections-plot">Figure 6-14</a>:
   </p>

      
       <pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">dlfb.localization.inspect.embeddings.clustering</code> <code class="kn">import</code> <code class="p">(</code>
  <code class="n">calculate_projection</code><code class="p">,</code>
  <code class="n">plot_projection</code><code class="p">,</code>
<code class="p">)</code>
<code class="kn">from</code> <code class="nn">dlfb.localization.inspect.embeddings.utils</code> <code class="kn">import</code> <code class="n">get_frame_embeddings</code>

<code class="n">frame_embeddings</code> <code class="o">=</code> <code class="p">{}</code>
<code class="k">for</code> <code class="n">name</code><code class="p">,</code> <code class="n">s</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">([</code><code class="s2">"no_head"</code><code class="p">,</code> <code class="s2">"with_head"</code><code class="p">],</code> <code class="p">[</code><code class="n">state_alt</code><code class="p">,</code> <code class="n">state</code><code class="p">]):</code>
  <code class="n">frame_embeddings</code><code class="p">[</code><code class="n">name</code><code class="p">]</code> <code class="o">=</code> <code class="n">get_frame_embeddings</code><code class="p">(</code><code class="n">s</code><code class="p">,</code> <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"valid"</code><code class="p">])</code>

<code class="n">projection</code> <code class="o">=</code> <code class="n">calculate_projection</code><code class="p">(</code><code class="n">frame_embeddings</code><code class="p">)</code>
<code class="n">plot_projection</code><code class="p">(</code>
  <code class="n">projection</code><code class="p">,</code>
  <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"valid"</code><code class="p">],</code>
  <code class="n">titles</code><code class="o">=</code><code class="p">[</code><code class="s2">"No ClassificationHead"</code><code class="p">,</code> <code class="s2">"With ClassificationHead"</code><code class="p">],</code>
<code class="p">);</code>
</pre>
      

     <figure><div id="projections-plot" class="figure">
      <img alt="" src="assets/dlfb_0614.png" width="600" height="291"/>
      <h6><span class="label">Figure 6-14. </span>UMAP projections of the model with (left) and without (right) the <code>ClassificationHead</code>. The model on the left learns clearer, more meaningful structure—with frames from the same subcellular compartment (e.g., vesicles, chromatin, nucleolus) forming tighter clusters. Without the <code>ClassificationHead</code>, the structure is less distinct, and compartments are harder to separate. Around 1% of frames with a single predominant localization are highlighted with larger markers to intuitively annotate clusters.
      </h6>
     </div></figure>
    
   
<p class="pagebreak-before">You can see that, once the embeddings are labeled with ground-truth localization categories, the model has clearly learned to distinguish different subcellular compartments based on raw pixel patterns. In the UMAP plot on the left (with the <code>ClassificationHead</code>), compartments like chromatin, mitochondria, and nucleolus form fairly tight, distinct clusters—indicating that the model has learned consistent visual features for these structures. Vesicles sometimes form a separate group but often show overlap with ER and cytoplasmic frames, likely reflecting their more diffuse and variable appearance in the raw images.</p>
<p>Removing the <code>ClassificationHead</code> has a clear visual impact. In the righthand UMAP, the model still produces structured embeddings, but the clusters are less distinct. Localizations such as cytoplasm, ER, and vesicles are more intermixed, suggesting that without the auxiliary classification task, the model learns a weaker or more entangled representation of subcellular identity. This comparison highlights how architectural choices—even auxiliary objectives—can meaningfully shape the structure of learned representations.</p>
   <div data-type="tip"><h6>Tip</h6>
   <p>Since we want to compare how the presence or absence of the
     <code>ClassificationHead</code> affects the learned embeddings, it’s important that both UMAP visualizations are directly comparable. By default, techniques like UMAP involve some randomness in initialization and optimization, which can lead to inconsistent visual layouts across runs. To address this, we use an
     <em>aligned UMAP</em>, a variant of UMAP that tracks shared data points across embeddings to ensure consistent alignment and make visual comparisons meaningful.
    </p>
   </div>
  <p>While these UMAP plots are useful for qualitative interpretation, it’s also possible to quantify clustering quality more formally; for example, by comparing the average distances between embeddings from the <em>same</em> localization versus those from <em>different</em> localizations. If the model has learned a meaningful embedding space, we’d expect intra-class distances (within a localization category) to be small, and inter-class distances to be larger. This basic intuition underpins several clustering quality metrics and was used by Kobayashi et al. to evaluate their model quantitatively. In this chapter, we’ll focus on visual exploration, but you can always extend the code to include such metrics if you would like to.<a contenteditable="false" data-primary="" data-startref="ch06_localization.html56" data-type="indexterm" id="id1024"/><a contenteditable="false" data-primary="" data-startref="ch06_localization.html55" data-type="indexterm" id="id1025"/><a contenteditable="false" data-primary="" data-startref="ch06_localization.html54" data-type="indexterm" id="id1026"/>
   </p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Inspecting Feature Spectrums"><div class="sect2" id="inspecting-feature-spectrums">
   <h2>Inspecting Feature Spectrums</h2>
  <p><a contenteditable="false" data-primary="cells, spatial organization patterns within" data-secondary="understanding the model" data-tertiary="inspecting feature spectrums" data-type="indexterm" id="ch06_localization.html57"/><a contenteditable="false" data-primary="feature spectrums" data-type="indexterm" id="ch06_localization.html58"/>To better understand what our model has learned, we can analyze how it uses its codebook across different proteins. Remember, each image is encoded using entries from a learned set of code vectors (the codebook). Over time, some entries may become specialized, for instance, consistently firing for mitochondrial proteins, or for nuclear-localized ones.
   </p>
  <p>A <em>feature spectrum</em> is a simple but powerful way to summarize this behavior. For each protein, we:
   </p>
   <ul class="simple">
    <li><p>Collect all its image frames.</p></li>
    <li><p>Count how often each codebook entry is used across those frames.</p></li>
    <li><p>Aggregate these counts into a histogram. This is the protein’s feature spectrum.</p></li>
   </ul>
  <p>Each feature spectrum essentially acts like a “fingerprint” of how the model encodes that protein’s spatial patterns.
   </p>
  <p>To explore how specialized the codebook has become, we compare these spectra across proteins. If certain codebook entries are consistently used for similar proteins (e.g., those localized to the ER), we expect those spectra to be correlated.
   </p>
  <p>The following function computes a Pearson correlation matrix across all protein spectra and then applies hierarchical clustering to group similar patterns. This reveals which sets of codebook entries (features) tend to be co-used and may correspond to broader localization categories:
   </p>
    
       <pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">cluster_feature_spectrums</code><code class="p">(</code>
  <code class="n">protein_histograms</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">ndarray</code><code class="p">,</code> <code class="n">n_clusters</code><code class="p">:</code> <code class="nb">int</code>
<code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">tuple</code><code class="p">[</code><code class="n">np</code><code class="o">.</code><code class="n">ndarray</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">ndarray</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">ndarray</code><code class="p">]:</code>
  <code class="sd">"""Cluster proteins based on similarity in codebook usage patterns."""</code>
  <code class="n">corr_idx_idx</code> <code class="o">=</code> <code class="n">np_pearson_cor</code><code class="p">(</code><code class="n">protein_histograms</code><code class="p">,</code> <code class="n">protein_histograms</code><code class="p">)</code>
  <code class="n">tree</code> <code class="o">=</code> <code class="n">linkage</code><code class="p">(</code>
    <code class="n">corr_idx_idx</code><code class="p">,</code>
    <code class="n">method</code><code class="o">=</code><code class="s2">"average"</code><code class="p">,</code>
    <code class="n">metric</code><code class="o">=</code><code class="s2">"euclidean"</code><code class="p">,</code>
    <code class="n">optimal_ordering</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
  <code class="p">)</code>
  <code class="n">encoding_clusters</code> <code class="o">=</code> <code class="n">fcluster</code><code class="p">(</code><code class="n">tree</code><code class="p">,</code> <code class="n">n_clusters</code><code class="p">,</code> <code class="n">criterion</code><code class="o">=</code><code class="s2">"maxclust"</code><code class="p">)</code>
  <code class="k">return</code> <code class="n">corr_idx_idx</code><code class="p">,</code> <code class="n">tree</code><code class="p">,</code> <code class="n">encoding_clusters</code>
</pre>
      
   
   
  <p>We first compute how correlated the spectra are across all proteins using Pearson correlation. Then, we build a tree (dendrogram) from that correlation matrix and cluster it to find groups of related proteins based on how similarly they use the codebook. These clusters often reflect shared localization patterns or functional relationships.
   </p>
  <p>The resulting heatmap in <a data-type="xref" href="#corr-heatmap">Figure 6-15</a> gives us a bird’s-eye view of which parts of the codebook are used together and by which proteins:
   </p>
  
      
       <pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">dlfb.localization.inspect.embeddings.feature_spectrum</code> <code class="kn">import</code> <code class="p">(</code>
  <code class="n">plot_encoding_corr_heatmap</code><code class="p">,</code>
<code class="p">)</code>
<code class="kn">from</code> <code class="nn">dlfb.localization.inspect.embeddings.utils</code> <code class="kn">import</code> <code class="n">aggregate_proteins</code>

<code class="n">protein_ids</code><code class="p">,</code> <code class="n">protein_histograms</code> <code class="o">=</code> <code class="n">aggregate_proteins</code><code class="p">(</code>
  <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"valid"</code><code class="p">],</code> <code class="o">**</code><code class="n">frame_embeddings</code><code class="p">[</code><code class="s2">"with_head"</code><code class="p">]</code>
<code class="p">)</code>
<code class="n">corr_idx_idx</code><code class="p">,</code> <code class="n">tree</code><code class="p">,</code> <code class="n">encoding_clusters</code> <code class="o">=</code> <code class="n">cluster_feature_spectrums</code><code class="p">(</code>
  <code class="n">protein_histograms</code><code class="p">,</code> <code class="n">n_clusters</code><code class="o">=</code><code class="mi">8</code>
<code class="p">)</code>
<code class="n">plot_encoding_corr_heatmap</code><code class="p">(</code><code class="n">corr_idx_idx</code><code class="p">,</code> <code class="n">tree</code><code class="p">,</code> <code class="n">encoding_clusters</code><code class="p">);</code>
</pre>
      
     

     <figure><div id="corr-heatmap" class="figure">
      <img alt="" src="assets/dlfb_0615.png" width="600" height="616"/>
      <h6><span class="label">Figure 6-15. </span>Correlation heatmap between codebook entries, based on how often they co-occur across proteins. Each axis shows the index of a vector quantization (vq) codebook entry (not all 512 indices are written out due to space constraints). The shading indicates the Pearson correlation between code usage patterns across proteins: lighter denotes strong positive correlation, and darker strong negative correlation. The dendrogram here clusters codebook entries into groups that represent shared spatial features learned by the model.
      </h6>
     </div></figure>
    
   
  <p>We can now take a deep dive into what the model’s feature spectra actually represent. Recall that for each protein, we computed a histogram of how often each codebook entry is used—essentially, a fingerprint of localization patterns learned by the model. Then, we computed the Pearson correlation between these fingerprints across proteins to see which codebook entries tend to be used together.
   </p>
  <p>The resulting plot in <a data-type="xref" href="#corr-heatmap">Figure 6-15</a> shows a correlation heatmap between all 512 codebook entries, with the color scale ranging from –1 (strong anticorrelation, darker) to +1 (strong correlation, lighter). Codebook entries that are frequently used together—for example, to describe a specific organelle—appear as bright blocks.
   </p>
  <p>The dendrogram at the top reflects a hierarchical clustering of these correlations. Based on this structure, the clustering procedure has grouped the codebook entries into eight broader encoding clusters, each of which represents a recurring spatial motif captured by the model. The color bar just below the dendrogram indicates these groupings.
   </p>
  <p>Here’s what this heatmap tells us:
   </p>
   <dl>
    <dt>Diagonal blocks</dt>
    <dd><p>The visible lighter blocks along the diagonal show groups of codebook entries that are highly correlated, suggesting that they work together to represent similar spatial features across proteins.
     </p></dd>
    <dt>Off-diagonal near-zero correlations</dt>
    <dd><p>The lack of strong off-diagonal structure implies that these clusters are relatively distinct. The model has learned specialized regions of the codebook for different spatial contexts.
     </p></dd>
    <dt>Unsupervised insight</dt>
    <dd><p>Crucially, this entire structure emerged without any localization labels. It reflects how the model has self-organized its representation space purely from image similarity.
     </p></dd>
  </dl>
  <p>This is a powerful window into the internal structure of the learned representations, revealing that even individual quantized codes fall into larger functional groups that track meaningful biological variation.
   </p>
  <p class="pagebreak-before">To take this analysis a step further, we can examine how these codebook entries are used across known subcellular compartments. If the model has learned meaningful localization features, we should see that certain codebook vectors are enriched for specific compartments or structures like chromatin, vesicles, or ER.
   </p>
  <p>This is shown in <a data-type="xref" href="#stacked-histograms">Figure 6-16</a>, where we average the feature spectra across all proteins within each localization class. Even though the model was never told anything about localization labels during training, we can now see clear signatures that align with known biology:
   </p>
  
    
       <pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">dlfb.localization.inspect.embeddings.feature_spectrum</code> <code class="kn">import</code> <code class="p">(</code>
  <code class="n">plot_stacked_histrograms</code><code class="p">,</code>
<code class="p">)</code>
<code class="kn">from</code> <code class="nn">dlfb.localization.inspect.embeddings.utils</code> <code class="kn">import</code> <code class="n">aggregate_localizations</code>

<code class="n">localizations</code><code class="p">,</code> <code class="n">localization_histograms</code> <code class="o">=</code> <code class="n">aggregate_localizations</code><code class="p">(</code>
  <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"valid"</code><code class="p">],</code> <code class="n">protein_ids</code><code class="p">,</code> <code class="n">protein_histograms</code>
<code class="p">)</code>
<code class="n">plot_stacked_histrograms</code><code class="p">(</code>
  <code class="n">localizations</code><code class="p">,</code> <code class="n">localization_histograms</code><code class="p">,</code> <code class="n">tree</code><code class="p">,</code> <code class="n">encoding_clusters</code>
<code class="p">);</code>
</pre>
      
  <!-- ASHLEY: the next p + p moved to precede figure#stacked-histograms -->

  <p>Although this is our initial, relatively simple model trained on a small subset of proteins, we already see some evidence that it is learning meaningful biological structure. Each row in the figure represents the average codebook usage (feature spectrum) for a given localization class. For instance, <code>nucleolus</code> (first row) shows peaks mostly within group VIII, suggesting that only a few specific codebook entries are consistently used to represent that compartment. This kind of narrow, high-signal spectrum is typical for highly structured and easily distinguishable compartments.
   </p>
  <p>In contrast, <code>mitochondria</code> (third row), <code>er</code> (fourth row), and especially <code>cytoplasm</code> (fifth row) display broader, more distributed activation across various groups—perhaps reflecting their more heterogeneous or variable visual features, or simply lacking in learned separation due to our limited dataset. Still, each compartment has its own distinctive fingerprint: for instance, <code>mitochondria</code> shows much higher peaks in group VI. Interestingly, while <code>nucleoplasm</code> is spatially adjacent to <code>nucleolus</code> in the cell, it exhibits a markedly different activation pattern—whereas <code>nucleoplasm</code> appears much more similar to <code>chromatin</code>, possibly due to less structural separation or shared staining characteristics.
   </p>   
    
     <figure class="pagebreak-after"><div id="stacked-histograms" class="figure">
      <img alt="" src="assets/dlfb_0616.png" width="600" height="560"/>
      <h6><span class="label">Figure 6-16. </span>Stacked histograms of codebook usage for different protein localization classes. Each row shows the average feature spectrum for a specific subcellular compartment (e.g., ER, nucleoplasm). Vertical groupings (I–VIII) reflect clusters of related codebook entries, as defined in the correlation heatmap shown previously. Despite never being trained with localization labels, the model has learned to associate certain codebook vectors with specific biological structures, revealing meaningful and interpretable signatures.
      </h6>
     </div></figure>
     
   <aside data-type="sidebar" epub:type="sidebar" class="less_space"><div class="sidebar" id="id1027">
  <h5>Looking at Individual Proteins</h5>
  <p>Much of our analysis so far has focused on aggregate behavior—examining feature spectra, UMAP projections, and clustering across many proteins. While this provides a useful global picture, it can obscure how the model treats individual examples.</p>
  <p>A valuable complementary approach is to pick a <em>single protein</em> and investigate it more closely:</p>
  <ul>
    <li><p>Review its raw input images across training frames.</p></li>
    <li><p>Visualize which codebook vectors are most active (its feature spectrum).</p></li>
    <li><p>Find its most similar neighbors using cosine similarity between embeddings.</p></li>
    <li><p>Inspect the classification output. If it’s misclassified, are the top predicted alternatives biologically reasonable?</p></li>
  </ul>
  <p>This kind of deep-dive analysis grounds abstract metrics in biological intuition and often reveals surprising model behavior. As an exercise, try applying this to proteins with contrasting properties—disease-associated versus not, structured versus disordered, or well-annotated versus unknown.</p>
</div></aside>
  <p>We’ve only scratched the surface using a small dataset and a simple architecture. In the next section, we’ll scale up both the model and the data to see how far this approach can go<a contenteditable="false" data-primary="" data-startref="ch06_localization.html58" data-type="indexterm" id="id1028"/><a contenteditable="false" data-primary="" data-startref="ch06_localization.html57" data-type="indexterm" id="id1029"/>.<a contenteditable="false" data-primary="" data-startref="ch06_localization.html53" data-type="indexterm" id="id1030"/>
   </p>
  </div></section>
 </div></section>
 <section data-type="sect1" data-pdf-bookmark="Improving the Model"><div class="sect1" id="improving-the-model_19062731">
  <h1>Improving the Model</h1>
 <p><a contenteditable="false" data-primary="cells, spatial organization patterns within" data-secondary="improving the model" data-type="indexterm" id="ch06_localization.html59"/>We’ve seen that even a relatively simple model with a small dataset of 50 proteins can learn meaningful representations of protein localization. The next step is to scale up the dataset to push performance further.
  </p>
  <section data-type="sect2" data-pdf-bookmark="Scaling Up the Data"><div class="sect2" id="scaling-up-the-data">
   <h2>Scaling Up the Data</h2>
  <p><a contenteditable="false" data-primary="cells, spatial organization patterns within" data-secondary="improving the model" data-tertiary="scaling up the data" data-type="indexterm" id="ch06_localization.html60"/>We now increase the number of proteins and their corresponding imaging frames significantly (from 50 to 500). This presents new challenges, especially in terms of training time and resource management. To make this process more manageable, we’ll save the final training state so that it can be reloaded for further inspection and evaluation:
   </p>   
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">dataset_splits</code> <code class="o">=</code> <code class="n">DatasetBuilder</code><code class="p">(</code>
  <code class="n">data_path</code><code class="o">=</code><code class="n">assets</code><code class="p">(</code><code class="s2">"localization/datasets"</code><code class="p">)</code>
<code class="p">)</code><code class="o">.</code><code class="n">build</code><code class="p">(</code>
  <code class="n">rng</code><code class="o">=</code><code class="n">rng_dataset</code><code class="p">,</code>
  <code class="n">splits</code><code class="o">=</code><code class="p">{</code><code class="s2">"train"</code><code class="p">:</code> <code class="mf">0.80</code><code class="p">,</code> <code class="s2">"valid"</code><code class="p">:</code> <code class="mf">0.10</code><code class="p">,</code> <code class="s2">"test"</code><code class="p">:</code> <code class="mf">0.10</code><code class="p">},</code>
  <code class="n">n_proteins</code><code class="o">=</code><code class="mi">500</code><code class="p">,</code>  <code class="c1"># a larger number of proteins</code>
<code class="p">)</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">LocalizationModel</code><code class="p">(</code>
  <code class="n">num_classes</code><code class="o">=</code><code class="n">count_unique_proteins</code><code class="p">(</code><code class="n">dataset_splits</code><code class="p">),</code>
  <code class="n">embedding_dim</code><code class="o">=</code><code class="mi">64</code><code class="p">,</code>
  <code class="n">num_embeddings</code><code class="o">=</code><code class="mi">512</code><code class="p">,</code>
  <code class="n">commitment_cost</code><code class="o">=</code><code class="mf">0.25</code><code class="p">,</code>
  <code class="n">dropout_rate</code><code class="o">=</code><code class="mf">0.45</code><code class="p">,</code>
  <code class="n">classification_head_layers</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code>
<code class="p">)</code>

<code class="n">state</code><code class="p">,</code> <code class="n">metrics</code> <code class="o">=</code> <code class="n">train</code><code class="p">(</code>
  <code class="n">state</code><code class="o">=</code><code class="n">model</code><code class="o">.</code><code class="n">create_train_state</code><code class="p">(</code>
    <code class="n">rng</code><code class="o">=</code><code class="n">rng_init</code><code class="p">,</code>
    <code class="n">dummy_input</code><code class="o">=</code><code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">get_dummy_input</code><code class="p">(),</code>
    <code class="n">tx</code><code class="o">=</code><code class="n">optax</code><code class="o">.</code><code class="n">adam</code><code class="p">(</code><code class="mf">0.001</code><code class="p">),</code>
  <code class="p">),</code>
  <code class="n">rng</code><code class="o">=</code><code class="n">rng_train</code><code class="p">,</code>
  <code class="n">dataset_splits</code><code class="o">=</code><code class="n">dataset_splits</code><code class="p">,</code>
  <code class="n">num_epochs</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code>
  <code class="n">batch_size</code><code class="o">=</code><code class="mi">256</code><code class="p">,</code>
  <code class="n">classification_weight</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code>
  <code class="n">eval_every</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code>
  <code class="n">store_path</code><code class="o">=</code><code class="n">assets</code><code class="p">(</code><code class="s2">"localization/models/large"</code><code class="p">),</code>
<code class="p">)</code>
</pre>
      
     
    
   
  <p>This model takes considerably longer to train, but the payoff is a more expressive and well-structured representation space.
    <a data-type="xref" href="#large-stacked-histograms">Figure 6-17</a> shows the aggregated codebook usage (feature spectra) across different localization classes, showing clearer <span class="keep-together">localization</span> signatures across a broader range of proteins (this plot also includes an expanded number of location classes as rows):
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">dlfb.localization.inspect.embeddings.feature_spectrum</code> <code class="kn">import</code> <code class="p">(</code>
  <code class="n">plot_stacked_histrograms</code><code class="p">,</code>
<code class="p">)</code>
<code class="kn">from</code> <code class="nn">dlfb.localization.inspect.embeddings.utils</code> <code class="kn">import</code> <code class="p">(</code>
  <code class="n">aggregate_localizations</code><code class="p">,</code>
  <code class="n">aggregate_proteins</code><code class="p">,</code>
  <code class="n">cluster_feature_spectrums</code><code class="p">,</code>
  <code class="n">get_frame_embeddings</code><code class="p">,</code>
<code class="p">)</code>

<code class="n">frame_embeddings</code> <code class="o">=</code> <code class="n">get_frame_embeddings</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"valid"</code><code class="p">])</code>
<code class="n">protein_ids</code><code class="p">,</code> <code class="n">protein_histograms</code> <code class="o">=</code> <code class="n">aggregate_proteins</code><code class="p">(</code>
  <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"valid"</code><code class="p">],</code> <code class="o">**</code><code class="n">frame_embeddings</code>
<code class="p">)</code>
<code class="n">_</code><code class="p">,</code> <code class="n">tree</code><code class="p">,</code> <code class="n">encoding_clusters</code> <code class="o">=</code> <code class="n">cluster_feature_spectrums</code><code class="p">(</code>
  <code class="n">protein_histograms</code><code class="p">,</code> <code class="n">n_clusters</code><code class="o">=</code><code class="mi">12</code>
<code class="p">)</code>
<code class="n">localizations</code><code class="p">,</code> <code class="n">localization_histograms</code> <code class="o">=</code> <code class="n">aggregate_localizations</code><code class="p">(</code>
  <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"valid"</code><code class="p">],</code> <code class="n">protein_ids</code><code class="p">,</code> <code class="n">protein_histograms</code>
<code class="p">)</code>
<code class="n">plot_stacked_histrograms</code><code class="p">(</code>
  <code class="n">localizations</code><code class="p">,</code> <code class="n">localization_histograms</code><code class="p">,</code> <code class="n">tree</code><code class="p">,</code> <code class="n">encoding_clusters</code>
<code class="p">);</code>
</pre>
      
     
    
    
     <figure><div id="large-stacked-histograms" class="figure">
      <img alt="" src="assets/dlfb_0617.png" width="600" height="560"/>
      <h6><span class="label">Figure 6-17. </span>Histograms of embedding codebook usage (feature spectra) for different subcellular localizations. Each row shows the average distribution of codebook features across all image frames annotated with a given localization (e.g., chromatin, vesicles, nuclear membrane). Distinct localization categories exhibit unique spectral “signatures”—for instance, nuclear membrane and chromatin show sharp, narrow peaks. This representation provides a quantitative “signature” of protein localization and allows predictions for unannotated proteins based on their similarity to known spectra.
      </h6>
     </div></figure>
    
   
  <p class="pagebreak-before">Now we’re cooking; we see more clearly differentiated spectral signatures compared to the earlier, smaller model. Many compartments display sharp, concentrated peaks, indicating that the model consistently relies on a small set of highly specific codebook vectors for these localizations. The rows—each representing a different subcellular compartment—now show more distinct and characteristic patterns. For example, comparing the “before” (<a data-type="xref" href="#stacked-histograms">Figure 6-16</a>)  and “after” (<a data-type="xref" href="#large-stacked-histograms">Figure 6-17</a>) spectra for <code>mitochondria</code> and <code>er</code>, we see that in the “after” state, the signatures are less diffuse and more sharply defined, suggesting that the model has learned to focus more precisely on key features for these compartments.
   </p>
  
  <p>We could speculate that compartments showing broader activation across several codebook regions, such as the <code>er</code> and <code>cytoplasm</code>, do so because of their inherently more variable or sprawling visual structure. These compartments tend to span large regions of the cell and appear with more morphological diversity. For instance, the ER forms a network that spans the cell, while cytoplasmic proteins can exhibit diffuse or context-dependent patterns. These differences likely lead the model to spread their representations across a wider set of features.
   </p>
   <div data-type="tip"><h6>Tip</h6>
   <p>These improvements to the spectra came purely from scaling up the dataset; no changes were made to the model architecture. Try exploring the model side of things next: increase capacity by adjusting <code>num_embeddings</code>, <code>embedding_dim</code>, or <span class="keep-together"><code>classification_</code></span><code>head_layers</code>, or experiment with your own architecture changes. There’s a lot of room to get creative here.
    </p>
   </div>
  <p>Next, we can replot a UMAP projection to visualize the model’s learned representation space (see <a data-type="xref" href="#large-projections-plot">Figure 6-18</a>):
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">dlfb.localization.inspect.embeddings.clustering</code> <code class="kn">import</code> <code class="p">(</code>
  <code class="n">calculate_projection</code><code class="p">,</code>
  <code class="n">plot_projection</code><code class="p">,</code>
<code class="p">)</code>

<code class="n">projection</code> <code class="o">=</code> <code class="n">calculate_projection</code><code class="p">(</code><code class="n">frame_embeddings</code><code class="p">)</code>
<code class="n">plot_projection</code><code class="p">(</code>
  <code class="n">projection</code><code class="p">,</code>
  <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"valid"</code><code class="p">],</code>
  <code class="n">subset_mode</code><code class="o">=</code><code class="s2">"single"</code><code class="p">,</code>  <code class="c1"># Only show frames with single localization</code>
  <code class="n">titles</code><code class="o">=</code><code class="p">[</code><code class="s2">"Localization UMAP Projection"</code><code class="p">],</code>
<code class="p">);</code>
</pre>
      
     
    
    
     <figure><div id="large-projections-plot" class="figure">
      <img alt="" src="assets/dlfb_0618.png" width="600" height="597"/>
      <h6><span class="label">Figure 6-18. </span>UMAP projection of embeddings produced by the larger model and dataset. The tight clustering of similar labels (e.g., nucleolus, ER, chromatin) shows that the model has learned to distinguish complex spatial patterns—without using labels <span class="keep-together">during training</span>.
      </h6>
     </div></figure>
    
   
  <p>This projection offers a compelling view into how well the larger model has learned to distinguish subcellular compartments. Each point represents an individual image frame, colored by its known localization. But remember, the model was trained in a self-supervised manner and never had access to these labels.
   </p>
  <p>First, we see more points in the plot because the dataset is larger. We also observe that the separation between groups is now more distinct compared to earlier runs. Compartments such as the <code>cytoplasm</code>, <code>vesicles</code>, and <code>nucleoplasm</code> now form well-defined groups and are less intermixed, suggesting that the model has learned highly consistent visual signatures for these localizations. In some cases—such as <code>vesicles</code>—the model even appears to discover distinct subcategories, which are cleanly separated in the embedding space. Previously diffuse or heterogeneous categories like <code>cytoplasm</code> now exhibit more internal structure, with clearer spatial separation.
   </p>
   <aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id1031">
    <h1>Explore Rich Embeddings</h1>
   <p><a contenteditable="false" data-primary="embeddings" data-secondary="rich" data-type="indexterm" id="id1032"/><a contenteditable="false" data-primary="rich embeddings" data-type="indexterm" id="id1033"/>This kind of embedding space is rich with both biological and technical signals. Here are a few ideas for exploring it further:
    </p>
    <ul class="simple">
     <li><p>If certain localization classes (like <code>vesicles</code>, <code>chromatin</code>, <code>nucleoplasm</code>) form multiple separate clusters, inspect representative examples from each. Do the clusters correspond to specific cell cycle stages, subtypes, or imaging artifacts? Is there any sort of visible pattern that distinguishes them?</p></li>
     <li><p>Examine <em>outliers</em>, which are individual frames that sit far from any cluster. These might reflect mislabeled data, rare biological phenomena, or technical noise.</p></li>
     <li><p>Leverage known protein complexes. Do proteins that physically interact also lie close together in embedding space? Could you rank candidate complexes using dot products or distances between embeddings, potentially discovering novel interactions? (This is broadly similar to what was done in the cytoself paper.)</p></li>
    </ul>
   <p>It’s also worth checking how well the test set embeddings separate, to further confirm that the model has learned a generalizable representation.</p> 
   <p>Embedding spaces like these are not just for visualization. They offer a powerful tool for downstream discovery.<a contenteditable="false" data-primary="" data-startref="ch06_localization.html60" data-type="indexterm" id="id1034"/>
    </p>
   </div></aside>
  </div></section>

 <section data-type="sect2" data-pdf-bookmark="Going Further"><div class="sect2" id="going-further">
  <h2>Going Further</h2>
 <p>This is as far as we’ll go in this chapter, but there’s much more you can explore from here. The original cytoself paper introduces several architectural innovations that improve both performance and biological insight:
  </p>
  <dl>
    <dt>Split quantization</dt>
   <dd>
    <p>Instead of quantizing full feature vectors, cytoself divides them into smaller chunks and quantizes each independently. This improves codebook usage and the richness of the learned representations.
    </p>
  </dd>
  <dt>Global and local representations</dt>
   <dd>
    <p>The model processes each image at two spatial scales:
    </p>
    <ul>
     <li><p>A <em>coarse</em> (4 × 4 × 576) representation that captures high-level localization patterns.</p></li>
     <li><p>A <em>fine</em> (25 × 25 × 64) representation that preserves detailed spatial features. Each is quantized using a separate codebook.</p></li>
    </ul>
  </dd>
  </dl>
 <p>These additions enabled cytoself to achieve several impressive results, all in an unsupervised setting:
  </p>
  <dl>
    <dt>Predicting localization of previously uncharacterized proteins</dt>
   <dd>
    <p>For instance, it correctly inferred that FAM241A localized to the endoplasmic reticulum—a prediction that was later confirmed through colocalization experiments and mass spectrometry.
    </p>
  </dd>
  <dt>Distinguishing subtle subcellular differences</dt>
   <dd>
    <p>The model learned to separate visually similar compartments like lysosomes and endosomes. Though hard to distinguish by eye, these compartments differ in function: lysosomes degrade cellular material, while endosomes serve as transport vesicles en route to lysosomes.
    </p>
  </dd>
  <dt>Discovering protein complexes</dt>
   <dd>
    <p>Perhaps most impressively, cytoself grouped proteins belonging to the same complexes, such as ribosomes or the proteasome, purely from embedding similarity. By comparing protein-level embeddings, the authors showed that known complex members cluster tightly, even without any labels or prior knowledge. In some cases, this outperformed previous supervised methods, and it even hinted at previously uncharacterized complex members.
    </p>
  </dd>
  </dl>
 <p>If you’re interested in learning more, we encourage you to read the original paper and explore the <a href="https://oreil.ly/eX8XH">OpenCell resource</a>, which hosts the data and interactive tools used in the study. The paper also outlines exciting future directions like 3D imaging, label-free microscopy, and cross-species generalization.
  </p>
  </div></section>
 </div></section>
 <section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="summary_122248091">
  <h1>Summary</h1>
 <p>In this chapter, we built a self-supervised deep learning model to learn spatial organization within human cells, all without relying on any manual annotations. This kind of approach is especially powerful for large-scale biological imaging, where manual curation not only is expensive and time-consuming but also can introduce human bias. You tackled a real-world challenge involving complex data, biological nuance, and custom neural architectures, all while drawing on key ideas from across the book.<a contenteditable="false" data-primary="" data-startref="ch06_localization.html0" data-type="indexterm" id="id1035"/>
  </p>
 <p>This was also the final end-to-end project chapter
   of the book. If you made it this far, congratulations. We hope this book helped you build intuition, fluency, and confidence in applying deep learning to biology. Whether you’re exploring new datasets, designing new models, or testing the boundaries of what today’s models can (and can’t) do, we’re excited to see where you go next.
  </p>
 <p>Happy modeling—and keep exploring!
  </p>
 </div></section>
<div data-type="footnotes"><p data-type="footnote" id="id943"><sup><a href="ch06.html#id943-marker">1</a></sup> Kobayashi, H., Cheveralls, K. C., Leonetti, M. D., &amp; Royer, L. A. (2022). Self-supervised deep learning encodes high-resolution features of protein subcellular localization. <em>Nature Methods</em>, 19(8), 995–1003. https://doi.org/10.1038/s41592-022-01541-z</p><p data-type="footnote" id="id946"><sup><a href="ch06.html#id946-marker">2</a></sup> Suk, T. R. (2020). The Role of TDP-43 Mislocalization in Amyotrophic Lateral Sclerosis. <em>Molecular Neurode‐Generation</em>, 15(1), 45.</p><p data-type="footnote" id="id947"><sup><a href="ch06.html#id947-marker">3</a></sup> Rodriguez, J. A., Au, W. W., &amp; Henderson, B. R. (2003). Cytoplasmic mislocalization of BRCA1 caused by cancer-associated mutations in the BRCT domain. <em>Experimental Cell Research</em>, 293(1), 14–21. https://doi.org/10.1016/j.yexcr.2003.09.027</p><p data-type="footnote" id="id948"><sup><a href="ch06.html#id948-marker">4</a></sup> Coale, T. H. et al. (2024). Nitrogen-fixing organelle in a marine alga. <em>Science</em>, 384(6692), 217–222. https://doi.org/10.1126/science.adk1075</p><p data-type="footnote" id="id949"><sup><a href="ch06.html#id949-marker">5</a></sup> Paré, B., et al. (2015). <a href="https://doi.org/10.1186/s40478-014-0181-z">Early detection of structural abnormalities and cytoplasmic accumulation of TDP-43 in tissue-engineered skins derived from ALS patients</a>. <em>Acta Neuropathologica Communications</em>, 3(1).</p><p data-type="footnote" id="id950"><sup><a href="ch06.html#id950-marker">6</a></sup> Ye, D., et al. (2023). Recent Advances in Nanomedicine Design Strategies for Targeting Subcellular Structures,. <em>iScience</em>, 28(1), 111597.</p><p data-type="footnote" id="id956"><sup><a href="ch06.html#id956-marker">7</a></sup> Jordan, J. (2018, March 19). <a href="https://oreil.ly/iLKwc"><em>Introduction to autoencoders</em></a>. Jeremy Jordan.</p><p data-type="footnote" id="id961"><sup><a href="ch06.html#id961-marker">8</a></sup> Dobilas, S. (2025, January 22). <a href="https://oreil.ly/0SB-Q">VAE: Variational Autoencoders – How to employ neural networks to generate new images</a>. <em>Towards Data Science</em>.</p><p data-type="footnote" id="id975"><sup><a href="ch06.html#id975-marker">9</a></sup> Van Den Oord, Aaron, Oriol Vinyals, and Koray Kavukcuoglu. 2017. <a href="https://oreil.ly/D-Rst">“Neural Discrete Representation Learning”</a>. arXiv.Org. November 2, 2017.</p></div></div></section></div></div></body></html>