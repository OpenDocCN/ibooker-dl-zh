- en: appendix B Reinforcement learning with human feedback
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement learning with human feedback (RLHF) is a variation of traditional
    reinforcement learning (RL), which typically involves solving the k-armed bandit
    problem. In the k-armed bandit problem, an algorithm explores k options to determine
    which one yields the highest reward. However, RLHF takes a different approach.
    Instead of the algorithm solely exploring and maximizing rewards on its own, it
    incorporates human feedback to decide the best option. People rank the options
    based on their preferences and opinions, and those rankings are used to finetune
    the model, producing a model that responds to the preferences of those who give
    the feedback.
  prefs: []
  type: TYPE_NORMAL
- en: In listing B.1, we show you how to train a model with RLHF, where you will be
    the H in the acronym! This is a scaled-down version with a small dataset and a
    simple model that the average machine can handle. Starting with the imports, you
    should be familiar with most of these by now, but we want to draw attention to
    one of the more unique ones, namely `trl`, which stands for “transformers reinforcement
    learning.” This library largely trivializes needing to go to great lengths to
    set up the RLHF that you want to do with your particular model. It also integrates
    very well with the Hugging Face ecosystem, including Accelerate and PEFT (Parameter-Efficient
    Fine-Tuning) if you want to RLHF LoRAs for different tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Listing B.1 Example RLHF training
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’re going to pull a dataset to train on. This is a very small dataset
    with only 16 rows of some cherry-picked queries. We won’t be able to really tune
    in any model off of such a small dataset, but we aren’t too concerned; we’re really
    just going through the motions right now to get a feel for how to do RLHF:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll load in our model. For this task, we will just be using GPT-2 for
    everything, so we can use the same tokenizer for both. As you can see, loading
    models is an absolute breeze with `trl` because it uses the exact same API as
    everything else in Hugging Face. As a note, GPT-2 doesn’t have a `pad_token,`
    so we’ll give it one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'For this task, we will be using proximal policy optimization (PPO), which is
    a very popular optimization algorithm for reinforcement learning tasks. We’re
    setting the `batch_size` to 1 since we are going to be giving the human feedback
    in real time. We’ll also define some parameters for text generation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we are ready to train our model! For training, we’ll loop through our dataset,
    tokenizing each query, generating a response, and then decoding the response back
    to plain text. From here, we’ll send the query and response to the terminal to
    be evaluated by you, a human, using the `input` function. You can respond to the
    prompt with an integer to give it a reward. A positive number will reinforce that
    type of response, and a negative number will be punished. Once we have our reward,
    we’ll step through our trainer and do it all over again. Lastly, we’ll save our
    model when we are done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Gets response from model'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Gets reward score from the user'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Runs PPO step'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Saves model'
  prefs: []
  type: TYPE_NORMAL
- en: While this works for demonstration purposes, this isn’t how you’ll run RLHF
    for production workloads. Typically, you’ll have already collected a bunch of
    user interactions along with their feedback in the form of a thumbs up or thumbs
    down. Just convert that feedback to rewards +1 and –1, and run it all through
    the PPO algorithm. Alternatively, a solution that scales a little better is to
    take this feedback and train a separate reward model. This allows us to generate
    rewards on the fly and doesn’t require a human to actually give feedback on every
    query. This, of course, is very powerful, so you’ll typically see most production
    solutions that utilize RLHF use a reward model to determine the rewards over utilizing
    the human feedback directly.
  prefs: []
  type: TYPE_NORMAL
- en: If this example piques your interest, we highly recommend checking out other
    examples and docs for the trl library, which you can find at [https://github.com/huggingface/trl](https://github.com/huggingface/trl).
    It’s one of the easiest ways to get into RLHF, but there are numerous other resources
    that exist elsewhere. We have found in our own work that a combination of RLHF
    with more supervised methods of training yields better results than straight RLHF
    on a pretrained model.
  prefs: []
  type: TYPE_NORMAL
