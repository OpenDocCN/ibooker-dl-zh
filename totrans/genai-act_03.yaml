- en: '3 Working through an API: Generating text'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 通过API工作：生成文本
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Generative AI models and their categorization based on specific applications
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据特定应用对生成式AI模型及其分类
- en: The process of listing available models, understanding their capabilities, and
    choosing the appropriate ones
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列出可用的模型、了解它们的功能以及选择合适的模型的过程
- en: The completion API and chat completion API offered by OpenAI
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI提供的完成API和聊天完成API
- en: Advanced options for completion and chat completion APIs that help us steer
    the model and hence control the generation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完成API和聊天完成API的高级选项，帮助我们引导模型并因此控制生成
- en: The importance of managing tokens in a conversation for improved user experience
    and cost-effectiveness
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在对话中管理令牌以改善用户体验和成本效益的重要性
- en: 'We have seen that large language models (LLMs) provide a powerful suite of
    machine learning tools specifically designed to enhance natural language understanding
    and generation. OpenAI features two notable APIs: the completion and the chat
    completion APIs. These APIs, unique in their dynamic and effective text-generation
    capabilities, resemble human output. In addition, they offer developers exclusive
    opportunities to craft various applications, from chatbots to writing assistants.
    OpenAI was the first to introduce the pattern of completion and chat completion
    APIs, which now embody almost all implementations, especially when companies want
    to build generative-AI-powered tools and products.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，大型语言模型（LLMs）提供了一套强大的机器学习工具，这些工具专门设计用于增强自然语言理解和生成。OpenAI提供了两个显著的API：完成API和聊天完成API。这些API以其动态和有效的文本生成能力而独树一帜，其输出类似于人类。此外，它们还为开发者提供了独特的机遇，可以构建从聊天机器人到写作助手的各种应用。OpenAI是第一个引入完成API和聊天完成API模式的公司，现在这一模式几乎包含了所有实现，尤其是在公司想要构建由生成式AI驱动的工具和产品时。
- en: The completion API by OpenAI is an advanced tool that generates contextually
    appropriate and coherent text to complete user prompts. Conversely, the chat completion
    API was designed to emulate an interaction with a machine learning model, preserving
    the context of a conversation across multiple exchanges, which makes it suitable
    for interactive applications.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的完成API是一个高级工具，它生成与上下文相关且连贯的文本以完成用户提示。相反，聊天完成API被设计成模拟与机器学习模型的交互，保持对话在多次交流中的上下文，这使得它适合交互式应用。
- en: Chapter 3 establishes the groundwork for scaling enterprises. These APIs can
    significantly accelerate the development of intelligent applications, thereby
    reducing the time to value. We’ll mostly use OpenAI and Azure OpenAI as illustrative
    examples, often interchangeably. The code models remain consistent, and the APIs
    are largely similar. Many enterprises may gravitate toward Azure OpenAI because
    of the control it offers, while others might favor OpenAI. It is important to
    note that we assume here that an Azure OpenAI instance has already been deployed
    as part of your Azure subscription, and we will be referencing it in the context
    of our examples.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 第3章为企业的扩展奠定了基础。这些API可以显著加速智能应用的开发，从而缩短价值实现的时间。我们将主要使用OpenAI和Azure OpenAI作为示例，经常互换使用。代码模型保持一致，API也大体相似。许多企业可能会因为Azure
    OpenAI提供的控制而倾向于它，而其他人可能会更偏好OpenAI。重要的是要注意，我们在这里假设Azure OpenAI实例已经作为您Azure订阅的一部分部署，我们将在示例的上下文中引用它。
- en: This chapter outlines the basics of the completion and the chat completion APIs,
    including how they differ and when to use each. We will see how to implement them
    in an application and how we can steer the model generation and its randomness.
    We’ll also see how to manage tokens, which are key operation considerations when
    deploying to production. These are the fundamental aspects required to build on
    for a mission-critical application. But first, let’s start by understanding the
    different model categories and their advantages.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章概述了完成API和聊天完成API的基本知识，包括它们之间的区别以及何时使用每个API。我们将看到如何在应用中实现它们，以及我们如何引导模型生成及其随机性。我们还将看到如何管理令牌，这在部署到生产环境时是关键的操作考虑因素。这些都是构建关键任务应用所需的基本方面。但在开始之前，让我们先了解不同的模型类别及其优势。
- en: 3.1 Model categories
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 模型类别
- en: Generative AI models can be classified into various categories based on their
    specific applications, and each category includes different types of models. We
    start our discussion by understanding the different classifications of models
    within generative AI. This understanding will help us identify the range of models
    available and choose the most appropriate one for a given situation.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 根据其特定的应用，生成式人工智能模型可以划分为各种类别，每个类别都包含不同类型的模型。我们首先通过了解生成式人工智能中模型的分类来开始我们的讨论。这种理解将帮助我们确定可用的模型范围，并选择在特定情况下最合适的一个。
- en: The availability of different types and models may vary, depending on the API
    in use. For example, Azure OpenAI and OpenAI provide different versions of LLMs.
    Some versions might be phased out, some could be limited, and others could be
    exclusive to a certain organization.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 不同类型和模型的可用性可能因所使用的 API 而异。例如，Azure OpenAI 和 OpenAI 提供了不同版本的 LLM。一些版本可能会被淘汰，一些可能会受限，而另一些可能仅限于特定组织。
- en: Different models have unique features and capabilities, directly affecting their
    cost and computational requirements. Thus, choosing the right model for each use
    case is critical. In conventional computer science, the idea that bigger is better
    has often been applied to memory, storage, CPUs, or bandwidth. However, in the
    case of LLMs, this principle is not always applicable. OpenAI provides a host
    of models categorized, as shown in table 3.1\. Note that these are the same for
    both OpenAI and Azure OpenAI, as the underlying models are identical.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的模型具有独特的特性和能力，这直接影响了它们的成本和计算需求。因此，为每个用例选择正确的模型至关重要。在传统的计算机科学中，更大的就是更好的这一观念常常应用于内存、存储、CPU
    或带宽。然而，在大型语言模型（LLM）的情况下，这一原则并不总是适用。OpenAI 提供了一系列分类的模型，如表 3.1 所示。请注意，这些模型在 OpenAI
    和 Azure OpenAI 中都是相同的，因为底层模型是相同的。
- en: Table 3.1 OpenAI model categories
  id: totrans-15
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 3.1 OpenAI 模型类别
- en: '| Model category | Description |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 模型类别 | 描述 |'
- en: '| --- | --- |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| GPT-4  | The newest and most powerful version is a set of multimodal models.
    GPT-4 is trained on a larger dataset with more parameters, making it even more
    capable. It can perform tasks that are out of reach for the previous models. There
    are various models in the GPT-4 family—GPT-4.0, GPT-4 Turbo, and the latest GPT-4o
    (omni), a multimodal model and the most powerful in the family at the time of
    publication.  |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4  | 最新且最强大的版本是一组多模态模型。GPT-4 在更大的数据集和更多参数上进行了训练，使其能力进一步增强。它可以执行之前模型无法完成的任务。GPT-4
    系列中包含各种模型——GPT-4.0、GPT-4 Turbo 和最新的 GPT-4o（全能），这是在发布时该系列中最强大的多模态模型。  |'
- en: '| GPT-3.5  | A set of models that improve on GPT-3 and can understand and generate
    natural language or code. When unsure, these should be the default models for
    most enterprises.  |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5  | 一组在 GPT-3 的基础上进行改进的模型，能够理解和生成自然语言或代码。在不确定的情况下，这些应该是大多数企业的默认模型。  |'
- en: '| DALL.E  | A model that can generate images when given a prompt  |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| DALL.E  | 一种在给定提示时能够生成图像的模型  |'
- en: '| Whisper  | A model that is used for speech-to-text, converting audio into
    text  |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| Whisper  | 一种用于语音转文本的模型，将音频转换为文本  |'
- en: '| Embeddings  | A set of models to convert text into its numerical form  |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| Embeddings  | 一组将文本转换为数值形式的模型  |'
- en: '| GPT-3 (Legacy)  | A set of models that can generate and understand natural
    language. These were the original set of models that are now considered legacy.
    In most cases, we would want to start with one of the newer models, 3.5 or 4.0,
    which derive from GPT-3\.  |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3 (Legacy)  | 一组能够生成和理解自然语言的模型。这些是现在被认为是遗留的原始模型集合。在大多数情况下，我们希望从较新的模型开始，例如
    3.5 或 4.0，这些模型源自 GPT-3。  |'
- en: Each model category contains variations that are further distinguished by certain
    features such as token size. As discussed in the previous chapter, token size
    determines a model’s context window, which defines the amount of input and output
    it can process. For instance, the original GPT-3 models had a maximum token size
    of 2K. GPT-3.5 Turbo, a subset of models within the GPT-3.5 category, has two
    versions—one with a token size of 4K and another with a token size of 16K. These
    are double and quadruple the token size of the original GPT-3 models. Table 3.2
    outlines the more popular models and their capabilities.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.2 Model descriptions and capabilities
  id: totrans-25
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Model | Capabilities |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
- en: '| Ada (legacy)  | Simple classification, parsing, and formatting of text. This
    model is part of the GPT-3 legacy.  |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
- en: '| Babbage (legacy)  | Semantic search ranking, medium complex classification.
    This model is part of the GPT-3 legacy.  |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
- en: '| Curie (legacy)  | Answering questions, highly complex classification. This
    model is part of the GPT-3 legacy.  |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
- en: '| Davinci (legacy)  | Summarization, generating creative content. This model
    is part of the GPT-3 legacy.  |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
- en: '| Cushman-Codex (legacy)  | A descendant of the GPT-3 series, trained in natural
    language and billions of lines of code. It is the most capable in Python and proficient
    in over a dozen other programming languages.  |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
- en: '| Davinci-Codex  | A more capable model of Cushman-codex  |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
- en: '| GPT3.5-Turbo  | The most capable GPT-3.5 model optimized for chat use cases
    is 90% cheaper and more effective than GPT-3 Davinci.  |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
- en: '| GPT-4, GPT-4 Turbo  | More capable than any GPT-3.5 model. It is able to
    do more complex tasks and is optimized for chat models.  |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
- en: '| GPT-4o  | The latest GPT-4o model is more capable than the GPT-4 and GPT-4
    Turbo, but it is also twice as fast and 50% cheaper.  |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
- en: '| text-embedding-ada-002, text-embedding-ada-003  | This new embedding model
    replaces five separate models for text search, similarity, and code search, outperforming
    them at most tasks; furthermore, it is 99.8% cheaper.  |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
- en: Note that the mentioned legacy models are still available and work as intended.
    However, the newer models are better, having more mindshare and longer support.
    Most should start with GPT-3.5 Turbo as the default model and use GPT-4 on a case-by-case
    basis. Sometimes, even a smaller, older model, such as the GPT-3 Curie, is good.
    This provides the right balance between the model’s capability, cost, and overall
    performance.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: In the early days of generative AI, all the models were available only to some.
    These will vary by company, region, and in the case of Azure, your subscription
    type, among other things. We have to list the models and their capabilities that
    are available for us to use. However, before listing models, let us see the dependencies
    required to get things working.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 Dependencies
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we call out the run time dependencies and configurations needed
    at a high level. To get things working, we need at least the following items:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '*Development IDE*—We use Visual Studio Code for our examples, but you can use
    anything you are comfortable with.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Python*—We use v3.11.3 in this book, but you can use any version as long as
    it is v3.7.1 or later. The installation instructions are available at [https://www.python.org/](https://www.python.org/)
    if you need to install Python.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*OpenAI Python libraries**—*We use Python libraries for most of the code and
    the demos. The OpenAI Python library can be a simple installation in conda, using
    `conda` `install` `-c` `conda-forge` `openai`. If you are using pip, use `pip`
    `install --upgrade openai`. There are also software development kits (SDKs) for
    specific languages if you prefer to use those instead of Python packages.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Azure Subscription or OpenAI API access**—*We use OpenAI’s endpoint and the
    Azure OpenAI (AOAI) endpoint interchangeably; in most cases, either option will
    work. Given the emphasis on enterprises for this book, we tend to lean toward
    using the Azure OpenAI service:'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To use the library with Azure endpoints, we need the `api_key`.
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We also need to set the `api_type`, `api_base`, and `api_version` properties.
    The `api_type` must be set to `azure`, the `api_base` points to the endpoint that
    we deploy, and the corresponding version of the API is specified via `api_version`.
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure OpenAI uses '`engine'` as the parameter to specify the model’s name. When
    deploying the model in your Azure subscription, this name needs to be set to your
    chosen name. For example, figure 3.1 is a screenshot of the deployments in one
    subscription. OpenAI, however, uses the parameter `model` to specify the model’s
    name. These model names are standard as they release them. You can find more details
    on Azure OpenAI and OpenAI at [https://mng.bz/yoYd](https://mng.bz/yoYd) and [https://platform.openai.com/docs/](https://platform.openai.com/docs/).
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Note  The GitHub code repository accompanying the book ([https://bit.ly/GenAIBook](https://bit.ly/GenAIBook))
    has the details of the code, including dependencies and instructions.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Hardcoding the endpoint and key is not an advisable practice. There are multiple
    methods to accomplish this task, one of which includes using environment variables.
    We demonstrate this method in the steps that follow. Other alternatives could
    be fetching them from secret stores or environment files. For the sake of simplicity,
    we will stick to environment variables in this guide. However, you are encouraged
    to adhere to your enterprise’s best practices and recommendations. Setting up
    the environment variables can be achieved through the following commands.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: For Windows, these are
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note  You may need to restart your terminal to read the new variables.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: On Linux/Mac, we have
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Bash uses
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note  In this book, we will use conda, an open source package manager, to manage
    our specific runtime versions and dependencies. Technically, using a package manager
    like conda is not mandatory, but it is extremely beneficial for isolating and
    troubleshooting problems and is highly recommended. We won’t delve into the specifics
    of installing conda in this context; for detailed, step-by-step instructions on
    how to install it, please refer to the official documentation at [https://docs.conda.io/](https://docs.conda.io/).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在本书中，我们将使用conda，一个开源的包管理器，来管理我们的特定运行时版本和依赖项。技术上，使用像conda这样的包管理器不是强制性的，但它对于隔离和解决问题非常有帮助，并且强烈推荐。在此上下文中，我们不会深入介绍如何安装conda；有关如何安装的详细、分步说明，请参阅官方文档[https://docs.conda.io/](https://docs.conda.io/)。
- en: 'First, let us create a new conda environment and install the required OpenAI
    Python library:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建一个新的conda环境并安装所需的OpenAI Python库：
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now that we have our dependencies installed, let’s connect to the Azure OpenAI
    endpoint and get details of the available models.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经安装了依赖项，让我们连接到Azure OpenAI端点并获取可用模型的详细信息。
- en: 3.1.2 Listing models
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.2 列出模型
- en: As we outlined earlier, each organization may have different models for use.
    We’ll start by understanding what models we have access to; we’ll use the APIs
    to help us set up the basic environment and get it running. Then, I’ll show you
    how to do this using the Azure OpenAI Python SDK and outline the differences when
    using the OpenAI API.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前概述的，每个组织可能都有不同的模型可供使用。我们将首先了解我们有哪些模型可以使用；我们将使用API来帮助我们设置基本环境并使其运行。然后，我将向您展示如何使用Azure
    OpenAI Python SDK来完成这项工作，并概述使用OpenAI API时的差异。
- en: As the next listing shows, we connect to the Azure OpenAI endpoint, get a list
    of all the models available, iterate over those, and print out the details of
    each model to the console.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，我们连接到Azure OpenAI端点，获取所有可用模型的列表，遍历这些模型，并将每个模型的详细信息打印到控制台。
- en: Listing 3.1 Listing Azure OpenAI models available
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.1 列出可用的Azure OpenAI模型
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#1 Required for Azure OpenAI endpoints'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 必须用于Azure OpenAI端点'
- en: '#2 This is the environment variable pointing to the endpoint published via
    the Azure portal.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 这是指向通过Azure门户发布的端点的环境变量。'
- en: '#3 Choose the API version we want to use from the multiple options.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 从多个选项中选择我们想要使用的API版本。'
- en: '#4 This is the environment variable with the API key.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 这是包含API密钥的环境变量。'
- en: Running this code will present us with a list of available models. The following
    listing shows an example of the models available; the exact list may be different
    for you.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码将向我们展示可用模型的列表。以下列表显示了可用模型的示例；您的确切列表可能会有所不同。
- en: Listing 3.2 Listing Azure OpenAI models’ output
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.2 列出Azure OpenAI模型的输出
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Each model is characterized by its distinct capabilities, suggesting the use
    cases for which it is tailored—specifically for chat completions, completions
    (which are regular text completions), embeddings, and fine-tuning. For example,
    a chat completion model would be the ideal selection in a situation where conversational
    engagement is required, like a chat-based interaction that requires significant
    dialogue exchange. Conversely, a completion model would be the most suitable for
    text generation. We can view the OpenAI base models with Azure AI Studio in figure
    3.1.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型都有其独特的功能，这表明了它针对的使用场景——特别是用于聊天补全、补全（即常规文本补全）、嵌入和微调。例如，在需要大量对话交流的基于聊天的交互中，聊天补全模型将是理想的选择。相反，补全模型将最适合文本生成。我们可以在图3.1中查看Azure
    AI Studio中的OpenAI基础模型。
- en: '![figure](../Images/CH03_F01_Bahree.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F01_Bahree.png)'
- en: Figure 3.1 Base model listed
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.1 列出的基础模型
- en: This feature is part of Azure AI Studio, which you can access when logging into
    your Azure subscription and accessing your Azure OpenAI deployment. You can also
    access it directly via the portal at [https://oai.azure.com/portal](https://oai.azure.com/portal).
    Now that we know which model to use, let’s generate some text. We’ll use the completion
    API and a model that supports completions.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 此功能是Azure AI Studio的一部分，您可以在登录Azure订阅并访问您的Azure OpenAI部署时访问它。您也可以直接通过门户[https://oai.azure.com/portal](https://oai.azure.com/portal)访问它。现在我们已经知道了要使用哪个模型，让我们生成一些文本。我们将使用补全API和一个支持补全的模型。
- en: 3.2 Completion API
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 补全API
- en: The completion API is a sophisticated tool that generates text to complete prompts
    provided by the user. It forms the backbone of the OpenAI API and offers a simple
    yet robust and flexible API. It is designed to produce text that is coherent and
    contextually fitting for the given prompt.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 完成API是一个复杂的工具，用于生成文本以完成用户提供的提示。它是OpenAI API的核心，提供了一个简单但强大且灵活的API。它旨在生成与给定提示一致且上下文合适的文本。
- en: Many generation examples that are not chat-type constructs use the completion
    API. We must use the completion API to generate text that is not a chat-style
    conversation. Some of the benefits of completion API are
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 许多非聊天型结构的生成示例都使用完成API。我们必须使用完成API来生成非聊天风格的文本。完成API的一些好处包括
- en: '*Contextual understanding*—The completion API can understand the context of
    the prompt and generate relevant text.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*上下文理解*—完成API可以理解提示的上下文并生成相关文本。'
- en: '*Versatility*—It can be used in various applications, from creating content
    to answering questions, which makes it a valuable tool for multiple applications.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多功能性*—它可以用于各种应用，从创建内容到回答问题，使其成为多种应用的宝贵工具。'
- en: '*Multiple language understanding*—The completion API can understand and generate
    content in several languages, which makes it a global resource.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多语言理解*—完成API能够理解和生成多种语言的内容，使其成为全球资源。'
- en: '*Easy implementation*—The completion API is straightforward, which makes it
    accessible to developers of various skill levels.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*易于实现*—完成API简单直接，这使得它对各种技能水平的开发者都易于访问。'
- en: 'The API’s structure is quite simple, as shown in the following snippet. The
    input (prompt) and the output (completion) are in text format. The API response
    is a JSON object from which the generated text can be extracted using the text
    key. This response is called text completion. The completion strives to adhere
    to the instructions and context provided in the prompt and is one of the potential
    outputs:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: API的结构非常简单，如下面的代码片段所示。输入（提示）和输出（完成）都是文本格式。API响应是一个JSON对象，可以使用text键从中提取生成的文本。这个响应被称为文本完成。完成试图遵循提示和上下文中提供的内容，并且是潜在输出之一：
- en: '[PRE6]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We start with an instruction, which is the prompt that specifies what we aim
    to generate. In our example, the instruction asks the model to generate a few
    bullets outlining why pets are awesome. The completion API has numerous parameters,
    but the most essential ones are detailed in table 3.3\. We discussed many other
    parameters earlier in this chapter and the book (e.g., the prompt, tokens, and
    temperatures). The stop sequences, however, are a new concept. We can employ these
    sequences to make the model cease generating tokens at a certain point, such as
    at the end of a sentence or a list.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个指令开始，即指定我们想要生成的提示。在我们的例子中，指令要求模型生成几个要点，概述为什么宠物很棒。完成API有许多参数，但最重要的参数在表3.3中详细说明。我们在本章和书中讨论了许多其他参数（例如，提示、标记和温度）。然而，停止序列是一个新概念。我们可以使用这些序列使模型在某个特定点停止生成标记，例如在句子的末尾或列表的末尾。
- en: Table 3.3 Completion API
  id: totrans-88
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.3 完成API
- en: '| Parameter | Type | Default value | Description |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 默认值 | 描述 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| `prompt`  | String or array  | `<\&#124;endoftext\&#124;>`  | A string or
    an array of strings is the prompt used to generate these completions.  |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| `prompt`  | 字符串或数组  | `<\&#124;endoftext\&#124;>`  | 一个字符串或字符串数组是用于生成这些完成的提示。  |'
- en: '| `max_tokens`  | Integer  | 16  | This is the maximum number of tokens to
    generate in the completion, including the prompt. The `max_tokens` must not exceed
    the model’s context length.  |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| `max_tokens`  | 整数  | 16  | 这是在完成中生成的最大标记数，包括提示。`max_tokens`不能超过模型的上下文长度。  |'
- en: '| `temperature`  | Number (float)  | 1  | This ranges between 0 and 2\. Higher
    values mean the model takes more risks and gets more creative.  |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| `temperature`  | 数字（浮点数）  | 1  | 这个值介于0和2之间。更高的值意味着模型承担更多风险并更具创造性。  |'
- en: '| `stop`  | String or array  | Null  | This can be up to four sequences where
    the API stops generating further tokens. The returned text will not contain the
    stop sequence.  |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| `stop`  | 字符串或数组  | Null  | 这可以是多达四个序列，API会在这些序列处停止生成更多标记。返回的文本将不包含停止序列。  |'
- en: '| `n`  | Integer  | 1 (optional)  | This defines the number of completions
    to generate for each prompt. This generates many completions and can quickly consume
    the token limit; we should have a reasonable setting for `max_tokens` and stop
    managing cost.  |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
- en: '| `stream`  | Boolean  | False (optional)  | This is a flag controlling whether
    to stream back partial progress as tokens are generated. If set, the stream is
    terminated by a data `[DONE]`message.  |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
- en: '| `best_of`  | Integer  | 1 (optional)  | This generates `best_of` completions
    server-side and returns the best completion. This parameter cannot be used with
    gpt-35-turbo.  |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
- en: '| `top_p`  | Number (float)  | 1 (optional)  | This controls randomness using
    a technique called nucleus sampling, an alternative to the `temperature` setting
    with a value ranging between 0 and 1\.  |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
- en: '| `logit_bias`  | Map  | Null (optional)  | This defines the likelihood of
    specified tokens appearing in the completion. It uses a mapping of tokens to a
    bias value (–100 of a ban to 100 of exclusive selection).  |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
- en: '| `user`  | String  | Null (optional)  | This parameter is a unique ID representing
    the end-user; it can help debug, monitor, and detect abuse.  |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
- en: '| `logprobs`  | Integer  | Null (optional)  | This is an optional array of
    log probabilities representing the alternate tokens and their likelihood considered
    for completion. This parameter cannot be used with gpt-35-turbo.  |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
- en: '| `suffix`  | String  | Null (optional)  | This parameter can be a string of
    up to 40 characters added as a suffix to the generated text.  |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
- en: '| `echo`  | Boolean  | False (optional)  | This determines whether the prompt
    is included in the completion. This is useful for use cases that need to capture
    the prompts and for debugging purposes. It cannot be used with gpt-35-turbo.  |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
- en: '| `presence_ penalty`  | Number (float)  | 0 (optional)  | This parameter steers
    the model’s tendency and helps outline its behavior to introduce new topics or
    ideas into the generated text. It ranges from 0.0 to 1.0\.  |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
- en: '| `frequency_ penalty`  | Number (float)  | 0 (optional)  | This is another
    parameter that helps steer the model and improve the generation results. It controls
    the level of common or uncommon words in the generated text and can be set to
    a value from 0.0 to 1.0\.  |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
- en: '| `function_ call`  |  |  | This controls how the model responds to functions
    when function calling is desired. It only works with 0613 or newer versions of
    the OpenAI models.  |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
- en: '| `functions`  |  |  | This is a list of functions that the model may use.  |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
- en: Note that the table only lists the most used parameters. It helps us understand
    some of the flows and concepts. Some parameters, such as functions, have more
    advanced uses, which will be covered in later chapters on prompt engineering.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: We stick with the pets theme and use the model to help us suggest names for
    a pet salon business. We ask for three names, and the instructions also outline
    some of the important characteristics to use. These aspects of the instructions
    help us steer the model toward some desired attributes. Please refer to the API
    documentation for a full list of parameters. Let’s call the completion API and
    walk through it.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.3 Calling the completion API
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '#1 Completion API call for generating text'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Specifies the model to use; note that this name will change based on what
    you set in the deployment'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Prompt'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Model configurations'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Extracts the generated text from the response'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! We used the API for our first text generation. Because of the
    nondeterministic nature of AI, especially generative AI, the output you will see
    when running this differs from
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The output is as follows.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Prompt.png)**Suggest three names for a new pet salon business.
    The generated name ideas should evoke positive emotions and the following key
    features: professional, friendly, personalized service.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**1\. Pawsitively Professional Pet Salon'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2\. Fur & Feathers Friendly Pet Parlor
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 3\. Happy Tails Personalized Pet Pampering
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note  LLMs and most other generative AI models are nondeterministic, meaning
    that identical inputs could give different outputs. Changing the temperature setting
    to zero can make the outputs more deterministic, but a small amount of variability
    may remain.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Expanding completions
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s see what a complete response from the API looks like and walk through
    that structure. The following listing shows the full response from the API. The
    `choices` field is among the most interesting, given that it has the completion
    text. The choices property is an array, where each item has an `index`, the reason
    the generation finished (`finish_reason`), and the generated text (via the `text`
    property).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.4 API response from a completion API
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 Array of completion data'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Response creation datetime stamp'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Unique ID of the response'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Model ID used to generate the response'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Count of tokens used in this request'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.4 shows the remaining properties. The usage property outlines the tokens
    used (`total_tokens`), including the prompt and response tokens. Because we pay
    per token, it is important to structure the prompt for aspects—first, to return
    only what is needed, minimizing token usage, and second, to limit the number of
    tokens generated in the first place.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.4 Completion response properties
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Property | Description |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
- en: '| `choices`  | An array that can contain one or more completions data  |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
- en: '| `created`  | UNIX date-time stamp when the response was created  |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
- en: '| `id`  | A unique identifier of the response is useful when we need to track
    responses  |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
- en: '| `model`  | Represents the model that was used for the generation  |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
- en: '| `object`  | Outlines the data type of the response (e.g., in this case, it
    is a `text_completion`, outlining a completion API)  |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
- en: '| `usage`  | Counts the number of tokens used by this request  |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
- en: A property called `logprobs` specifies the number of log probabilities to generate
    for each token in the response. The log probabilities are useful for generating
    more diverse and interesting responses. It returns the log probabilities of the
    top *n* tokens for each token in the response. The log probabilities are returned
    as an array of arrays, where each subarray corresponds to a token in the response
    and contains the log probabilities of the top *n* tokens for that token.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Azure content safety filter
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sometimes, the API returns a `null` response, as shown in listing 3.5\. When
    this happens, we should check the value of the `finish_reason` field. If its value
    is set to `content_filter`, the content filtering system that works alongside
    models has been triggered. The `finish_reason` field indicates why the API returned
    the output it did, and every response will include this field. This topic will
    be covered in more detail later in the chapter.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: The filtering system uses specific categories to identify and act on potentially
    harmful content as part of both the input prompts and generated completions. The
    application that uses these APIs must handle this situation and retry after the
    appropriate back-off period. The content safety filter and ethical AI will be
    covered in more detail in chapter 13.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.5 Output showing `null` response
  id: totrans-148
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#1 null response'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Content filter is the reason the response finished.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3 Multiple completions
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We might want multiple completions for a few reasons. Sometimes, we need to
    generate multiple message choices for the same prompt. At other times, the API
    is throttled for capacity reasons, and we might want to get more from the same
    API call instead of being rate limited. The completions API can return multiple
    responses; this is done by setting the `n` parameter to more than the default
    value of 1\. For example, we can add this parameter to the completion call:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: When we run this updated code, we get the response shown in listing 3.6\. The
    property choices are an array, and we have three items, with the index starting
    at a base zero. Each has the generated text for us to use. Depending on the use
    case, this is helpful when picking multiple completions.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.6 Output showing multiple responses
  id: totrans-156
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Another similar but more powerful parameter is the `best_of` parameter. Like
    the `n` parameter, it generates multiple completions, allowing the option to pick
    the best. The `best_of` is the completion with the highest log probability per
    token. We cannot stream results when using this option. However, it can be combined
    with the `n` parameters, with `best_of` needs greater than `n`.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the following listing, if we set `n` to 5, we get five completions
    as expected; for brevity, we do not show all five of the completions here, but
    note that this call uses 184 tokens.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.7 Output showing multiple responses
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'If we run a similar call using the `best_of` parameter, do not specify the
    `n` parameter:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: When we run this code, we get only one completion, as shown in listing 3.8;
    however, we are using a similar number of tokens as earlier (171 versus 184).
    This is because the service generates five completions on the server side and
    returns the best one. The API uses the log probability per token to pick the best
    option. The higher the log probability, the more confident the model is about
    its prediction.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.8 Output generation with `best_of` five completions
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The one parameter that influences many of the responses is the temperature setting.
    Let’s see how this changes the output.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.4 Controlling randomness
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As discussed in the previous chapter, the `temperature` setting influences the
    randomness of the generated output. A lower temperature produces more repetitive
    and deterministic responses, while a higher temperature produces more innovative
    responses. Fundamentally, there isn’t a right setting—it all comes down to the
    use cases.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: For enterprises, a more creative output would be when there is interest in diverse
    output and creating text for use cases such as content generation for marketing,
    stories, poems, lyrics, jokes, etc. These are things that usually require creativity.
    However, enterprises need more reliable and precise answers for use cases, such
    as document automation for invoice generation, proposals, code generation, etc.
    These settings are applicable per API call, so combining different temperature
    levels in the same workflow is possible.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: As demonstrated in previous examples, we recommend a temperature setting of
    0.8 for creative responses. Conversely, a setting of 0.2 is suggested for more
    predictable responses. Using an example, let us examine how these settings alter
    the output and observe the variations between multiple calls.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'When the temperature was set to 0.8, we received the following responses from
    three consecutive calls. The output changes as expected, offering suggestions
    like those seen throughout this chapter. It is important to note that we do not
    need to make three separate API calls. We can set the `n` parameter to 3 in a
    single API call to generate multiple responses. Here is what our API call looks
    like:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The following listing shows the creative generation for the three responses.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.9 Completions output with the temperature at 0.8
  id: totrans-175
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '#1 First response: get blocked by the content filter'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Second of three responses'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Final response with very different generated text'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Let’s change the setting to make this more deterministic and run it again. Note
    that the only change in the API call is `temperature=0.2`. The output is predictable
    and deterministic, with very similar text generated between the three responses.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.10 Completions output with the temperature at 0.2
  id: totrans-181
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '#1 One of three responses'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Two of three responses; very similar generated text'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The final response with very similar generated text'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: The temperature value goes up to 2, but it is not recommended to go that high,
    as the model starts hallucinating more and creating nonsensical text. If we want
    more creativity, we usually want it to be at 0.8 and, at most, 1.2\. Let us see
    an example when the temperature is changed to 1.8\. In this example, we did not
    even get the third generation, as we hit the token limit and stopped the generation.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.11 Completions output with the temperature at 1.8
  id: totrans-187
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '#1 One of three responses with names that aren’t very clear'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Second and third of three responses, with nonsensical names'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.5 Controlling randomness using top_p
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An alternative to the `temperature` parameter for managing randomness is the
    `top_p` parameter. It has the same affect on the generation as the temperature
    parameter, but it uses a different technique called *nucleus sampling*. Essentially,
    nucleus sampling allows only the tokens with a probability equal to or less than
    the value of `top_p` to be considered as part of the generation.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Nucleus sampling creates texts by picking words from a small group of the most
    likely ones with the highest cumulative probability. The `top_p` value decides
    how small this group is based on the total chance for the words to appear in it.
    The group size can change depending on the next word’s chance. Nucleus sampling
    can help avoid repetition and generate more varied and clearer texts than other
    methods.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we have the `top_p` value set to 0.9, only the tokens that make
    up 90% of the probability distribution will be sampled for the generation of text.
    This allows us to avoid the last 10%, which are often quite random and diverse
    and end up as nonsensical hallucinations.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: A lower value of `top_p` makes the model more consistent and less creative as
    it chooses fewer tokens to generate. Conversely, a higher value makes the generation
    more creative and diverse, as it has a larger set of tokens to operate. The larger
    value also makes it prone to more errors and randomness. The exact value of `top_p`
    depends on the use case; in most cases, the ideal value for `top_p` ranges between
    0.7 and 0.95\. We should change either the temperature attribute or `top_p`, but
    not both. Table 3.5 outlines the relationship between the two.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.5 Relationship between temperature and `top_p`
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Temperature | top_p | Effect |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
- en: '| Low  | Low  | Generates predictable text that closely follows common language
    patterns  |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
- en: '| Low  | High  | Generates predictable text, but with occasional less common
    words or phrases  |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
- en: '| High  | Low  | Generates text that is often coherent but with creative and
    unexpected word usage  |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
- en: '| High  | High  | Generates highly diverse and unpredictable text with various
    word choices and ideas; has very creative and diverse output, but may contain
    many errors  |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
- en: Let us look at some of the advanced API options for specific scenarios.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Advanced completion API options
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have examined the basic constructs of the completion API and understand
    how they work, we need to consider more advanced aspects of the completion API.
    Many of these might not seem as complex, but they add many more responsibilities
    to the system architecture, complicating overall implementation.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 Streaming completions
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The completions API allows streaming responses, offering immediate access to
    information as soon as it is ready rather than waiting for a full response. For
    enterprises, streaming can be important in some cases where real-time content
    generation with lower latency is key. This feature can enhance user experiences
    by processing incoming responses promptly.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: To enable streaming from the API’s standpoint, modify the `stream` parameter
    to `true`. By default, this optional parameter is set to `false`.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Streaming employs server-sent events (SSE), which require a client-side implementation.
    SSE is a standard protocol allowing servers to continue transmitting data to clients
    after establishing the initial connection. It is a long-term, one-way connection
    from server to client. SSE offers advantages such as low latency, reduced bandwidth
    consumption, and an uncomplicated configuration setup.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.12 demonstrates how our example can be adjusted to utilize streaming.
    Although the API modification is straightforward, the description and requested
    multiple generations were adjusted (using the `n` property). This allows us to
    generate more text artificially, making it easier to observe the streaming generation.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.12 Streaming completion
  id: totrans-211
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '#1 Tweaked the prompt slightly to add descriptions'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '#2 We need to handle the streaming response on the client side.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Enables streaming'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '#4 We need to loop through the array and handle multiple generations.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: When managing a streaming call, we must pay extra attention to the `finish_reason`
    property. As messages are streamed, each appears as a standard completion, with
    the text representing the newly generated token. In these instances, the `finish_reason`
    remains null. However, the final message differs; its `finish_reason` could be
    either `stop` or `length`, depending on what triggered it.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.13 Streaming finish reason
  id: totrans-218
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '3.3.2 Influencing token probabilities: logit_bias'
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `logit_bias` parameter is one way we can influence output completion. In
    the API, this parameter allows us to manipulate the probability of certain tokens,
    which can be words or phrases, that the model generates in its responses. It is
    called `logit_ bias` because it directly affects the log odds, or logits, that
    the model calculates for each potential token during the generation process. The
    bias values are added to these log-odds before converting them to probabilities,
    altering the final distribution of tokens the model can pick from.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: The importance of this feature lies in its ability to steer the model’s output.
    Say we are creating a chatbot and want it to avoid certain words or phrases. We
    can use `logit_bias` to decrease the likelihood of those tokens being chosen by
    the model. In contrast, if there are certain words or phrases we want the model
    to favor, we could use `logit_bias` to increase their likelihood. The range of
    this parameter is from –100 to 100, and it operates on tokens for the word. Setting
    a token to –100 effectively bans it from the generation, whereas setting it to
    100 makes it exclusive.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: To use `logit_bias`, we provide a dictionary where the keys are the tokens,
    and the val-ues are the biases that need to be applied to those tokens. To get
    the token, we use the `tiktoken` library. Once you have the appropriate token,
    you can assign a positive bias to make it more likely to appear or a negative
    bias to make it less likely, as shown in figure 3.2\. The blocks show the degree
    of probability that different tokens can be at different probabilities of banning
    or exclusive generation. Smaller changes to the tokens’ value increase or decrease
    the probability of these tokens in the generated output.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F02_Bahree.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 The `logit_bias` parameter
  id: totrans-225
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Let’s use an example to see how we can make this work. For our pet salon name,
    we do not want to use the words “purr,” “purrs,” or “meow.” The first thing we
    want to do is create the tokens for these words. We also want to add words with
    a preceding space and capitalize them as spaces. Capital letters are all different
    tokens. So “Meow” and “Meow” (with a space) and “meow” (again with a space) might
    read the same to us, but when it comes to tokens, these words are all different.
    The output shows us the tokens for the corresponding word:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Now that we have the tokens, we can add them to the completion call. Note that
    we assign each token a bias of –100, steering the model away from these words.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.14 `logit_bias` implementation
  id: totrans-229
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '#1 Dictionary containing the tokens and the corresponding bias values to steer
    the model on these specific tokens'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: We do not have any words we want to avoid when we run this code.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.15 Output of `logit_bias` generation
  id: totrans-233
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We can do the opposite and positively bias tokens too. Say we want to overemphasize
    and steer the model toward the word “Furry.” We can use the `tiktoken` library
    we saw earlier and find that the tokens for “Furry” are `[37, 16682]`. We can
    update the previous API call with this and, in this case, a positive bias of 5.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 3.16 `logit_bias`: Positive implementation'
  id: totrans-236
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: When we run this code, we get the output shown in the following listing. As
    we can see, there is a much stronger emphasis on “Furry” in our generation. The
    completions also take longer, as the model competes with the bias when generating
    certain tokens.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 3.17 Output `logit_bias`: Positive implementation'
  id: totrans-239
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The `logit_bias` feature should be used carefully; it is a powerful tool for
    guiding the model’s output. However, excessive or inappropriate use can lead to
    nonsensical, overly repetitive, or biased output in unexpected ways.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.3 Presence and frequency penalties
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have two additional parameters in the API, called *presence* and *frequency*
    penalties, that help steer the language model’s output by controlling the generation’s
    repetition. These two parameters influence the likelihood of words (technically
    a sequence of tokens) reappearing in a completion. A higher presence penalty encourages
    the model to focus on the prompt and avoid using tokens that already appear there.
    In contrast, a higher frequency penalty discourages the model from repeating itself.
    Let’s take a look at both in a little more detail.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Presence penalty parameter
  id: totrans-244
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The presence penalty parameter affects how often the same token appears in the
    output. This is achievable by using the presence penalty as a value subtracted
    from the probability of a token each time it is generated. This means that the
    more a token is used, the less likely it is to be used again. This helps make
    the model use more varied tokens in the generation and explore new topics. The
    value of this parameter can range from 0 to 2.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: The default value is 0, meaning the model does not care if a token is repeated.
    A high presence penalty (1.0) makes the model less likely to use the same token
    again, and a higher value makes the model introduce new topics in the output.
    A low presence penalty (0) makes the model stick to the existing topics in the
    text. Each time a token is generated, the parameter value is subtracted from the
    log probability of that token.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: We can improve the quality of the generation by preventing the same text from
    being repeated multiple times, helping control the flow, and making the output
    more engaging. Now let’s look at the frequency penalty parameter.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Frequency penalty parameter
  id: totrans-248
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This parameter controls how much the model avoids repeating itself in the output.
    The higher the frequency penalty (1.0), the more the model tries to use different
    words and phrases, which results in a more diverse generation. The lower the frequency
    penalty (0.0), the more the model can repeat the same words and phrases and the
    more predictable the output. This differs from the presence penalty, which encourages
    the model to use new words and phrases. The frequency penalty adds to the log
    probability of a token each time it appears in the output.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: The best values for both parameters depend on what you want to achieve with
    the output. Usually, choosing values between 0.1 and 1.0 would be best, which
    noticeably affects the output. If you want a stronger effect, you can increase
    the values up to 2.0, but this might reduce the output quality.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Note that tuning these parameters requires some trial and error to get the desired
    results, as the model’s output is also influenced by many other factors, including
    the prompt you provide and other fine-tuning parameters. Figure 3.3\. shows the
    correlation for both the presence and frequency penalty parameters.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F03_Bahree.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 Penalty presence parameter
  id: totrans-253
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 3.3.4 Log probabilities
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When an LLM generates a token, it assigns a probability to the next considered
    token and uses various techniques to pick the token used in the completion from
    these options. The `logprobs` property of the completion API exposes the natural
    logarithm for these probabilities at each step.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: This is an integer (max value of 5) that shows the alternate tokens considered
    for each token included in the completion. If this value is set to 3, the API
    will return a list of the three most likely tokens for each selected token in
    the generation. Note that the API always returns the `logprobs` of the sampled
    token, so in the response, we might end up with `logprobs + 1` element in the
    array.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Fundamentally, we use this approach to help debug and improve the prompts. If
    the model isn’t generating text we like, we can use this to see what other words
    (technically tokens) the model considered. This allows us to tune some other settings
    to steer the model. Conversely, we can use the same thing to control randomness
    in the model generation and make the output more deterministic. Finally, we can
    also use this to understand how confident the model is. If the probabilities are
    the same for several different words, this means that the model is not certain
    what word comes next.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we want to get a name for a white dog; we can call the completion API.
    In this example, we get the name Cotton, which isn’t bad:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'If we want to see what other tokens were considered for the name, we can add
    the `logprobs` properties:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'As seen in the completion output in the following listing, the model considered
    the following tokens: Casper, Coco, and Snow.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.18 Output log probabilities
  id: totrans-263
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: As a reminder, we should use this property judiciously and only when required.
    Not only does it increase the number of tokens generated and, hence, the cost
    of the API call, but it also takes time and adds time to the API call, thereby
    increasing overall latency.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the completion API for text generation, let’s see how
    we can use the chat completion API.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Chat completion API
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The chat completion API has been designed to facilitate interactive and dynamic
    conversations. It is an evolution of the completion API, providing users with
    a more conversational and engaging experience. With this API, developers can create
    applications that have a dialogue with users, making it ideal for creating chatbots,
    writing assistants, and more.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: The key benefits that the chat completion API provides over the completion API
    are
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '*Enhanced interactivity*—The chat completion API allows for a more dynamic
    and interactive conversation with the user, making the user experience more engaging
    and natural.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Contextual understanding*—The API maintains the context of the conversation,
    ensuring that the responses are relevant and coherent.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Multiturn conversation*—Unlike the completion API, which is more suited for
    single-turn tasks, the multiturn conversation API allows developers to simulate
    conversations with multiple exchanges.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cost-effective*—Completion API uses GPT-3.5 Turbo or GPT-4 models, which perform
    at a similar capability as text-davinci-003 but at 10% of the price per token,
    making it a more economical choice for developers.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At a high level, using the chat completion API is similar to the completion
    API. The API takes a series of messages as input, forming the basis of the interaction
    with the model. The ordering of the messages is important, as it outlines the
    turn-by-turn interaction.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'Each message has two properties: role and content. The role parameter has the
    following three options: `system`, `user`, or `assistant`. The content contains
    the message’s text from the role. Table 3.6 outlines the details of each role
    and its purpose.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.6 Chat completion API role description
  id: totrans-276
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Role parameter | Description |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
- en: '| `system`  | The `system` role is typically used to set the assistant’s behavior
    and provide the model with high-level instructions that guide the behavior throughout
    the conversation. This is where we can describe the assistant’s personality and
    tell it what it should and should not answer, as well as how to format responses.
    While there is no token limit, it is included with every API call and is part
    of the overall token limit.  |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
- en: '| `user`  | This represents the user’s input in the conversation; these messages
    contain the instructions or queries from the user that the assistant responds
    to.  |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
- en: '| `assistant`  | This represents the assistant’s prior messages in the conversation.
    Think of this as the ongoing memory that helps the model and provides the conversation
    context as it proceeds, turn by turn.  |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
- en: Listing 3.19 shows the chat completion API. As we called out earlier, the order
    of the messages in the array matters, as it represents the flow of the conversation.
    Usually, the conversation starts with a `system` message that sets the assistant’s
    behavior, followed by alternating `user` and `assistant` messages as the conversation
    proceeds turn by turn. The assistant’s replies are generated based on the conversation
    history.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.19 Chat completion API
  id: totrans-283
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '#1 Chat complete API call'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Different models needed (Turbo) compared to completion API'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '#3 List of messages that form the heart of the API'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '#4 These parameters are the same as for the completion API.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: We need to update the engine parameter to use one of the chat-compatible models.
    As shown earlier in this chapter, not all models support the chat style, and we
    need to pick the models with the `chat_completion` capability (GPT-3.5 Turbo,
    GPT-4, GPT-4 Turbo). All the other parameters are the same as the completion API
    that we covered earlier in this chapter, and we will not get into those details
    again.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: 'Note  The following parameters are unavailable with the new GPT-35 Turbo and
    GPT-4 models: `logprobs`, `best_of`, and `echo`. Trying to set any of these parameters
    will throw an exception.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: The output of the previous example is shown in the next listing. The user started
    with “Hello, World!”, and the system responded, asking how to help us with the
    assistant message. The question about dog details is the next dialogue turn.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.20 Chat completion API output
  id: totrans-292
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 3.4.1 System role
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The system role (some also call it the system message) is included at the beginning
    of the message array. This message provides the initial instructions for the model,
    and we can provide various pieces of information in the system role, including
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: A brief description of the assistant
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Personality traits of the assistant
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rules and instructions you want the assistant to follow
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional information needed for the model (e.g., relevant questions from an
    FAQ)
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We customize the system role and include basic instructions for the use case.
    From an API perspective, even though the system role is optional, it is highly
    recommended that you make this intentional to get the best results. For example,
    if we expand on the previous example of chatting for pets and pet salons, we can
    instruct the model to only reply in rhyme.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.21 Chat completion system message example
  id: totrans-301
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '#1 Instructs to answer in rhyme'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: In the example, we can have a conversation as expected, which can vary topics
    in turns, but all the answers rhyme.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: When we want to give the model additional data as context for the conversation,
    this is called grounding the data. If there is a small amount of data, this can
    be part of the `system` role, as shown in the next listing. However, if there
    is a large amount of data, we should use embeddings and retrieve the most relevant
    information using a semantic search (e.g., Azure cognitive search).
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.22 Grounding system message example
  id: totrans-306
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 3.4.2 Finish reason
  id: totrans-308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Every chat completion API response has a finish reason encoded in the `finish_
    reason` field. Tracking is important in this case, as it helps us understand why
    the API returned the response it did. This can be useful for debugging and improving
    the application. For example, if you receive an incomplete response due to the
    `length` finish reason, you may want to adjust the `max_tokens` parameter to generate
    more complete responses. The possible values for `finish_reason` are
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '`stop`—The API finished generating and either returned a complete message or
    a message terminated by one of the stop sequences provided using the stop parameter.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`length—`The API stopped the model output due to the `max_tokens` parameter
    or token limit.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function_call—`The model decided to call a function.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`content_filter—`Some of the completion was filtered due to harmful content.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3.4.3 Chat completion API for nonchat scenarios
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OpenAI’s chat completion can be used for nonchat scenarios. The API is quite
    similar and designed to be a flexible tool that can be adapted to various use
    cases, not just conversations. In most cases, the recommended path uses the chat
    completion API as if it were the completion API. The main reason is that the newer
    models (Chat 3.5-Turbo and GPT-4) are much more efficient, cheaper, and powerful
    than the earlier models. The completion use cases we have seen, such as analyzing
    and generating text and answering questions from a knowledge base, would all still
    work with the chat completion API.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the chat completion API nonchat scenarios usually involves structuring
    the conversation with a series of messages and a system message to set the assistant’s
    behavior. For example, as shown in the following listing, the system message sets
    the role of the assistant, and the user message provides the task.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.23 Chat completion as a completion API example
  id: totrans-317
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We can also use a series of user messages to provide more context or accomplish
    more complex tasks, as shown in the next listing. In this example, the first user
    message sets up the task, and the second user message provides more specific details.
    The assistant generates a response that attempts to complete the task in the user
    messages.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.24 Chat completion as a completion API example
  id: totrans-320
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 3.4.4 Managing conversation
  id: totrans-322
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our examples keep running, but the conversation will hit the model’s token limit
    as it continues. With each turn of the conversation (i.e., the question asked
    and the answer received), the list of messages grows. As a reminder, the token
    limit for GPT-35 Turbo is 4K tokens, and for GPT-4 and GPT-4 32K, it is 8K and
    32K, respectively; these include the total count from the message list sent and
    the model response. We get an exception if the total count exceeds the relevant
    model limit.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: No out-of-the-box option can track this token count for us and ensure it falls
    within the token limit. As part of the enterprise app design, we need to track
    the token count and only send a prompt that falls within the limit.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: 'Many enterprises are in the process of implementing an enterprise version of
    ChatGPT using the chat API. Here are some of the best practices that can help
    enterprises manage these conversations. Remember, the best way to get your desired
    output involves iterative testing and refining your instructions:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '*Setting the behavior with system message*—You should use the system message
    at the start of the conversation to guide the model’s behavior and for enterprises
    to tune to reflect their brand or IP.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Providing explicit instructions*—If the model is not generating your desired
    output, make your instructions more explicit. Think about it at the same level
    as if you were telling a toddler what not to do.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Breaking down complex tasks*—If you have a complex task, break it down into
    several simpler tasks, and send them as separate user messages. You often need
    to show, not explain it. This is called Chain of Thought (CoT), and it will be
    covered in more detail in chapter 6\.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Experimentation*—Feel free to experiment with the parameters to get the desired
    output. A higher temperature value (e.g., 0.8) makes the generation more random,
    while a lower value (e.g., 0.2) makes it more deterministic. You can also use
    the maximum token value to limit response length.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Managing tokens*—Be aware of the total number of tokens in a conversation,
    as input and output tokens count toward the total. You must truncate, omit, or
    shorten your text if a conversation has too many tokens to fit within the model''s
    maximum limit.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Handling sensitive content*—If you’re dealing with potentially unsafe content,
    you should look at Azure OpenAI’s Responsible AI guidelines ([https://mng.bz/pxVK](https://mng.bz/pxVK)).
    However, if you are using OpenAI’s API, then OpenAI’s moderation guide is helpful
    ([https://mng.bz/OmEw](https://mng.bz/OmEw)) for adding a moderation layer to
    the outputs of the chat API.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracking tokens
  id: totrans-332
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As outlined earlier, keeping track of tokens when using the conversational
    API is key. Not only will the experience suffer if we go over the total token
    size, but the total number of tokens in an API also has a direct effect on latency
    and on how long the call takes. Finally, the more tokens we use, the more we pay.
    Here are some ways you can manage tokens:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '*Count tokens**.* Use the `tiktoken` library, which allows us to count how
    many tokens are in a string without making an API call.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Limit response length**.* When making an API call, use the `max_tokens` property
    to limit the length of the model’s responses.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Truncate long conversations**.* If a conversation has too many tokens to fit
    within the model’s maximum limit, we must truncate, omit, or shorten our text.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Limit the number of turns**.* Limiting the number of turns in the conversation
    is a good way to truncate or shorten the text. This also helps steer the model
    better when the conversation gets longer and tends to start hallucinating.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Check the* `usage`*field in the API response**.* After making an API call,
    we can check the usage field in the API response to see the total number of tokens
    used. This is ongoing and includes both input and output tokens. It is a good
    way to keep track of tokens and show them to the user via some UX.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Reduce temperature**.* Reducing the temperature parameter can make the model''s
    outputs more focused and concise, which can help reduce the number of tokens used
    in the response.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Say we want to build a chat application for our pet salon and allow customers
    to ask us questions about pets, grooming, and their needs. We can build a console
    chat application, as shown in listing 3.25\. It also shows us a possible way to
    track and manage tokens. In this example, we have a function `num_tokens_from_messages`
    which, as the name suggests, is used to calculate the number of tokens in a conversation.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: As the conversation grows turn by turn, we calculate the number of tokens used,
    and once it reaches the model limit, the old messages are removed from the conversation.
    Note that we start at index 1\. This ensures we always preserve the system message
    at index 0 and only remove user/assistant messages.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.25 `ConsoleChatApp:` Token management
  id: totrans-342
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '#1 Sets up the OpenAI environment and configuration details'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Sets up the system message for the chat'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Function to count the total tokens from all the messages in the conversation'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Uses the tiktoken library to count tokens'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Loops through the messages'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Captures the user input'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '#7 When the total tokens exceed the token limit, we remove the second token.
    The first token is the system token, which we always want.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Chat completion API call'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: Chat completion vs. completion API
  id: totrans-352
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Both chat completion and completion APIs are designed to generate human-like
    text and are used in different contexts. The completion API is designed for single-turn
    tasks, providing completion to a prompt provided by the user. It is most suited
    for tasks where only a single response is required.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, the chat completion API is designed for multiturn conversations,
    maintaining the context of the conversation over multiple exchanges. This makes
    it more suitable for interactive applications such as chatbots. The chat completion
    API is a new dedicated API for interacting with the GPT-35-Turbo and GPT-4 models
    and is the preferred method. The chat completion API is geared more toward chatbots,
    and using the different roles (`system`, `user`, and `assistant`), we can get
    the memory of previous messages and organize few-shot examples.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.5 Best practices for managing tokens
  id: totrans-355
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For LLMs, tokens are the new currency. As most enterprises go beyond kicking
    tires to business-critical use cases, managing tokens would become a priority
    for computations, cost, and overall experience. From an enterprise application
    perspective, here are some of the considerations for managing tokens:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '*Concise prompts*—Where possible, using concise prompts and limiting the maximum
    number of tokens will reduce the token’s usage, making it more cost-effective.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Stop sequences*—Use stop sequences to stop the generations to avoid generating
    unnecessary tokens.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Counting tokens*—We can count tokens using the `tiktoken` library as outlined
    earlier and avoid making the API calls do the same.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Smaller models*—Generally speaking, in computing, bigger and newer hardware
    and software are considered faster, cheaper, and better; however, this isn’t necessarily
    the case for LLMs. Where possible, consider using smaller models such as GPT-3.5
    Turbo first, and when they might not be a good fit, consider going to the next
    one. Smaller models are less compute intensive and, hence, are more economical.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Use caching*—For prompts that are either quite static or frequently repeated,
    implementing a caching strategy would help save tokens and avoid making API calls
    repeatedly. For more complex scenarios, look to cache the embeddings using a vector
    search and store, such as Azure Cognitive Search, Pinecone, etc. The last chapter
    covered an introduction to embeddings, and we will get more details on embeddings
    and searching later in chapters 7 and 8 when we cover RAG and chatting with your
    data.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3.4.6 Additional LLM providers
  id: totrans-362
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Additional vendors also now have LLMs to use for enterprises. These are either
    available via APIs or, in some cases, as model weights that enterprises can self-host.
    Table 3.7 outlines some of the more famous ones available at the time of publication.
    Please note that some restrictions are in place from a commercial-licensing perspective.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.7 Other LLM providers
  id: totrans-364
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Models | Descriptions |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
- en: '| Llama 2  | Meta released Llama 2, an open source LLM, which comes in three
    sizes (7 billion, 13 billion, and 70 billion parameters) and is free for research
    and commercial purposes. Companies can access this through cloud options such
    as Azure AI’s model catalog, Hugging Face, or AWS. Enterprises that want to host
    it using their own compute and GPUs can request access from Meta via [https://ai.meta.com/llama/](https://ai.meta.com/llama/).  |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
- en: '| PaLM  | PaLM is a 13 billion-parameter model from Google that is part of
    their generative AI for developer products. The model can perform text summarization,
    dialogue generation, and natural language inference tasks. At the time of publication,
    there was a waitlist for an API key; details are available at [https://developers.generativeai.google/](https://developers.generativeai.google/).  |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
- en: '| BLOOM  | Bloom is a 223-billion parameter, open source multilingual model
    that can understand and generate text in over 100 languages by collaborating with
    over 1,000 researchers across more than 250 institutions. It is available via
    Hugging Face for deployment. More details are available at [https://huggingface.co/bigscience/bloom](https://huggingface.co/bigscience/bloom).  |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
- en: '| Claude  | Claude is a 12-billion parameter developed by Anthropic. It is
    accessible through a playground interface and API in its developer console for
    development and evaluation purposes only. At publication, for production use,
    enterprises must contact Claude for commercial discussions. More details can be
    found at [https://mng.bz/YVqz](https://mng.bz/YVqz).  |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
- en: '| Gemini  | Google recently released a new LLM called Gemini, a successor to
    PaLM 2 and optimized for different sizes: ultra, pro, and nano. It is designed
    to be more powerful than its predecessor and can be used to generate new content.
    Google claims it to be their most capable AI model yet. More details can be found
    at [https://mng.bz/GNxD](https://mng.bz/GNxD).  |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
- en: Interestingly, all these vendors follow a similar approach to the concepts and
    APIs established by OpenAI. For example, as outlined by their documents, the PaLM
    model from Google’s completion API equivalent is presented in the next listing.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.26 PaLM-generated text API signature
  id: totrans-373
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: While these options exist, and some are from reputable and leading technology
    companies, for most enterprises, Azure OpenAI and OpenAI are the most mature,
    with the most enterprise controls and support needed. The next chapter will deal
    with images, and we will learn how to move from text to images and generate in
    that modality.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-376
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GenAI models are classified into various categories, depending on the type.
    Each model has additional capabilities and characteristics. Choosing the right
    model for the use case at hand is important. And unlike computer science, in our
    case, the biggest model isn’t necessarily better.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The completion API is a sophisticated tool that generates text, which can be
    used to complete prompts provided by the user and forms the backbone of the text
    generation paradigm.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The completion API is relatively easy to use with only a few key parameters,
    such as the prompt, number of tokens to generate, temperature parameter that helps
    steer the model, and number of completions to generate.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The API exposes many advanced options for steering models and controlling randomness
    and generated text, such as `logit_bias`, presence penalty, and frequency penalty.
    All these work in tandem and help generate better output.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When using Azure OpenAI, the content safety filter can help filter specific
    categories to identify and act on potentially harmful content as part of both
    the input prompts and generated completions.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The chat completion API builds on the completion API, going from one set of
    instructions and APIs to a dialogue with the user in a turn-by-turn interaction.
    The chat completion consists of multiple systems, user, and assistance roles.
    The conversation starts with a `system` message that sets the assistant's behavior,
    followed by alternating `user` and `assistant` messages as the conversation proceeds
    turn by turn.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The system role is included at the beginning of the message array. It provides
    the initial instructions for the model, including personality traits, instructions
    and rules for the assistant to follow, and additional information we want to provide
    as context for the model; this additional information is called grounding the
    data.
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each completion and chat completion API response has a finish reason, which
    helps us understand why the API returned the response it did. This can be useful
    for debugging and improving the application.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The language learning models all have a finite context window and are quite
    expensive. Managing tokens becomes important for us to be able to run things at
    a reasonable cost and within the API allowance. This also helps us manage tokens
    in conversations for improved user experience and cost-effectiveness.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to Azure OpenAI and OpenAI, there are other LLM providers, such
    as Meta’s Llama 2, Google’s Gemini and PaLM, Bloom by BigScience, and Anthropic’s
    Claude. Their offerings are similar and follow the completions and chat completions
    paradigm, including similar APIs.
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
