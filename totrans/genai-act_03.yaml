- en: '3 Working through an API: Generating text'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generative AI models and their categorization based on specific applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The process of listing available models, understanding their capabilities, and
    choosing the appropriate ones
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The completion API and chat completion API offered by OpenAI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced options for completion and chat completion APIs that help us steer
    the model and hence control the generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The importance of managing tokens in a conversation for improved user experience
    and cost-effectiveness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We have seen that large language models (LLMs) provide a powerful suite of
    machine learning tools specifically designed to enhance natural language understanding
    and generation. OpenAI features two notable APIs: the completion and the chat
    completion APIs. These APIs, unique in their dynamic and effective text-generation
    capabilities, resemble human output. In addition, they offer developers exclusive
    opportunities to craft various applications, from chatbots to writing assistants.
    OpenAI was the first to introduce the pattern of completion and chat completion
    APIs, which now embody almost all implementations, especially when companies want
    to build generative-AI-powered tools and products.'
  prefs: []
  type: TYPE_NORMAL
- en: The completion API by OpenAI is an advanced tool that generates contextually
    appropriate and coherent text to complete user prompts. Conversely, the chat completion
    API was designed to emulate an interaction with a machine learning model, preserving
    the context of a conversation across multiple exchanges, which makes it suitable
    for interactive applications.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 3 establishes the groundwork for scaling enterprises. These APIs can
    significantly accelerate the development of intelligent applications, thereby
    reducing the time to value. We’ll mostly use OpenAI and Azure OpenAI as illustrative
    examples, often interchangeably. The code models remain consistent, and the APIs
    are largely similar. Many enterprises may gravitate toward Azure OpenAI because
    of the control it offers, while others might favor OpenAI. It is important to
    note that we assume here that an Azure OpenAI instance has already been deployed
    as part of your Azure subscription, and we will be referencing it in the context
    of our examples.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter outlines the basics of the completion and the chat completion APIs,
    including how they differ and when to use each. We will see how to implement them
    in an application and how we can steer the model generation and its randomness.
    We’ll also see how to manage tokens, which are key operation considerations when
    deploying to production. These are the fundamental aspects required to build on
    for a mission-critical application. But first, let’s start by understanding the
    different model categories and their advantages.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Model categories
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative AI models can be classified into various categories based on their
    specific applications, and each category includes different types of models. We
    start our discussion by understanding the different classifications of models
    within generative AI. This understanding will help us identify the range of models
    available and choose the most appropriate one for a given situation.
  prefs: []
  type: TYPE_NORMAL
- en: The availability of different types and models may vary, depending on the API
    in use. For example, Azure OpenAI and OpenAI provide different versions of LLMs.
    Some versions might be phased out, some could be limited, and others could be
    exclusive to a certain organization.
  prefs: []
  type: TYPE_NORMAL
- en: Different models have unique features and capabilities, directly affecting their
    cost and computational requirements. Thus, choosing the right model for each use
    case is critical. In conventional computer science, the idea that bigger is better
    has often been applied to memory, storage, CPUs, or bandwidth. However, in the
    case of LLMs, this principle is not always applicable. OpenAI provides a host
    of models categorized, as shown in table 3.1\. Note that these are the same for
    both OpenAI and Azure OpenAI, as the underlying models are identical.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.1 OpenAI model categories
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Model category | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4  | The newest and most powerful version is a set of multimodal models.
    GPT-4 is trained on a larger dataset with more parameters, making it even more
    capable. It can perform tasks that are out of reach for the previous models. There
    are various models in the GPT-4 family—GPT-4.0, GPT-4 Turbo, and the latest GPT-4o
    (omni), a multimodal model and the most powerful in the family at the time of
    publication.  |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5  | A set of models that improve on GPT-3 and can understand and generate
    natural language or code. When unsure, these should be the default models for
    most enterprises.  |'
  prefs: []
  type: TYPE_TB
- en: '| DALL.E  | A model that can generate images when given a prompt  |'
  prefs: []
  type: TYPE_TB
- en: '| Whisper  | A model that is used for speech-to-text, converting audio into
    text  |'
  prefs: []
  type: TYPE_TB
- en: '| Embeddings  | A set of models to convert text into its numerical form  |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3 (Legacy)  | A set of models that can generate and understand natural
    language. These were the original set of models that are now considered legacy.
    In most cases, we would want to start with one of the newer models, 3.5 or 4.0,
    which derive from GPT-3\.  |'
  prefs: []
  type: TYPE_TB
- en: Each model category contains variations that are further distinguished by certain
    features such as token size. As discussed in the previous chapter, token size
    determines a model’s context window, which defines the amount of input and output
    it can process. For instance, the original GPT-3 models had a maximum token size
    of 2K. GPT-3.5 Turbo, a subset of models within the GPT-3.5 category, has two
    versions—one with a token size of 4K and another with a token size of 16K. These
    are double and quadruple the token size of the original GPT-3 models. Table 3.2
    outlines the more popular models and their capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.2 Model descriptions and capabilities
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Model | Capabilities |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Ada (legacy)  | Simple classification, parsing, and formatting of text. This
    model is part of the GPT-3 legacy.  |'
  prefs: []
  type: TYPE_TB
- en: '| Babbage (legacy)  | Semantic search ranking, medium complex classification.
    This model is part of the GPT-3 legacy.  |'
  prefs: []
  type: TYPE_TB
- en: '| Curie (legacy)  | Answering questions, highly complex classification. This
    model is part of the GPT-3 legacy.  |'
  prefs: []
  type: TYPE_TB
- en: '| Davinci (legacy)  | Summarization, generating creative content. This model
    is part of the GPT-3 legacy.  |'
  prefs: []
  type: TYPE_TB
- en: '| Cushman-Codex (legacy)  | A descendant of the GPT-3 series, trained in natural
    language and billions of lines of code. It is the most capable in Python and proficient
    in over a dozen other programming languages.  |'
  prefs: []
  type: TYPE_TB
- en: '| Davinci-Codex  | A more capable model of Cushman-codex  |'
  prefs: []
  type: TYPE_TB
- en: '| GPT3.5-Turbo  | The most capable GPT-3.5 model optimized for chat use cases
    is 90% cheaper and more effective than GPT-3 Davinci.  |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4, GPT-4 Turbo  | More capable than any GPT-3.5 model. It is able to
    do more complex tasks and is optimized for chat models.  |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4o  | The latest GPT-4o model is more capable than the GPT-4 and GPT-4
    Turbo, but it is also twice as fast and 50% cheaper.  |'
  prefs: []
  type: TYPE_TB
- en: '| text-embedding-ada-002, text-embedding-ada-003  | This new embedding model
    replaces five separate models for text search, similarity, and code search, outperforming
    them at most tasks; furthermore, it is 99.8% cheaper.  |'
  prefs: []
  type: TYPE_TB
- en: Note that the mentioned legacy models are still available and work as intended.
    However, the newer models are better, having more mindshare and longer support.
    Most should start with GPT-3.5 Turbo as the default model and use GPT-4 on a case-by-case
    basis. Sometimes, even a smaller, older model, such as the GPT-3 Curie, is good.
    This provides the right balance between the model’s capability, cost, and overall
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: In the early days of generative AI, all the models were available only to some.
    These will vary by company, region, and in the case of Azure, your subscription
    type, among other things. We have to list the models and their capabilities that
    are available for us to use. However, before listing models, let us see the dependencies
    required to get things working.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 Dependencies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we call out the run time dependencies and configurations needed
    at a high level. To get things working, we need at least the following items:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Development IDE*—We use Visual Studio Code for our examples, but you can use
    anything you are comfortable with.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Python*—We use v3.11.3 in this book, but you can use any version as long as
    it is v3.7.1 or later. The installation instructions are available at [https://www.python.org/](https://www.python.org/)
    if you need to install Python.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*OpenAI Python libraries**—*We use Python libraries for most of the code and
    the demos. The OpenAI Python library can be a simple installation in conda, using
    `conda` `install` `-c` `conda-forge` `openai`. If you are using pip, use `pip`
    `install --upgrade openai`. There are also software development kits (SDKs) for
    specific languages if you prefer to use those instead of Python packages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Azure Subscription or OpenAI API access**—*We use OpenAI’s endpoint and the
    Azure OpenAI (AOAI) endpoint interchangeably; in most cases, either option will
    work. Given the emphasis on enterprises for this book, we tend to lean toward
    using the Azure OpenAI service:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To use the library with Azure endpoints, we need the `api_key`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We also need to set the `api_type`, `api_base`, and `api_version` properties.
    The `api_type` must be set to `azure`, the `api_base` points to the endpoint that
    we deploy, and the corresponding version of the API is specified via `api_version`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure OpenAI uses '`engine'` as the parameter to specify the model’s name. When
    deploying the model in your Azure subscription, this name needs to be set to your
    chosen name. For example, figure 3.1 is a screenshot of the deployments in one
    subscription. OpenAI, however, uses the parameter `model` to specify the model’s
    name. These model names are standard as they release them. You can find more details
    on Azure OpenAI and OpenAI at [https://mng.bz/yoYd](https://mng.bz/yoYd) and [https://platform.openai.com/docs/](https://platform.openai.com/docs/).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Note  The GitHub code repository accompanying the book ([https://bit.ly/GenAIBook](https://bit.ly/GenAIBook))
    has the details of the code, including dependencies and instructions.
  prefs: []
  type: TYPE_NORMAL
- en: Hardcoding the endpoint and key is not an advisable practice. There are multiple
    methods to accomplish this task, one of which includes using environment variables.
    We demonstrate this method in the steps that follow. Other alternatives could
    be fetching them from secret stores or environment files. For the sake of simplicity,
    we will stick to environment variables in this guide. However, you are encouraged
    to adhere to your enterprise’s best practices and recommendations. Setting up
    the environment variables can be achieved through the following commands.
  prefs: []
  type: TYPE_NORMAL
- en: For Windows, these are
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note  You may need to restart your terminal to read the new variables.
  prefs: []
  type: TYPE_NORMAL
- en: On Linux/Mac, we have
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Bash uses
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note  In this book, we will use conda, an open source package manager, to manage
    our specific runtime versions and dependencies. Technically, using a package manager
    like conda is not mandatory, but it is extremely beneficial for isolating and
    troubleshooting problems and is highly recommended. We won’t delve into the specifics
    of installing conda in this context; for detailed, step-by-step instructions on
    how to install it, please refer to the official documentation at [https://docs.conda.io/](https://docs.conda.io/).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let us create a new conda environment and install the required OpenAI
    Python library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have our dependencies installed, let’s connect to the Azure OpenAI
    endpoint and get details of the available models.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Listing models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we outlined earlier, each organization may have different models for use.
    We’ll start by understanding what models we have access to; we’ll use the APIs
    to help us set up the basic environment and get it running. Then, I’ll show you
    how to do this using the Azure OpenAI Python SDK and outline the differences when
    using the OpenAI API.
  prefs: []
  type: TYPE_NORMAL
- en: As the next listing shows, we connect to the Azure OpenAI endpoint, get a list
    of all the models available, iterate over those, and print out the details of
    each model to the console.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.1 Listing Azure OpenAI models available
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Required for Azure OpenAI endpoints'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 This is the environment variable pointing to the endpoint published via
    the Azure portal.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Choose the API version we want to use from the multiple options.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 This is the environment variable with the API key.'
  prefs: []
  type: TYPE_NORMAL
- en: Running this code will present us with a list of available models. The following
    listing shows an example of the models available; the exact list may be different
    for you.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.2 Listing Azure OpenAI models’ output
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Each model is characterized by its distinct capabilities, suggesting the use
    cases for which it is tailored—specifically for chat completions, completions
    (which are regular text completions), embeddings, and fine-tuning. For example,
    a chat completion model would be the ideal selection in a situation where conversational
    engagement is required, like a chat-based interaction that requires significant
    dialogue exchange. Conversely, a completion model would be the most suitable for
    text generation. We can view the OpenAI base models with Azure AI Studio in figure
    3.1.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F01_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 Base model listed
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This feature is part of Azure AI Studio, which you can access when logging into
    your Azure subscription and accessing your Azure OpenAI deployment. You can also
    access it directly via the portal at [https://oai.azure.com/portal](https://oai.azure.com/portal).
    Now that we know which model to use, let’s generate some text. We’ll use the completion
    API and a model that supports completions.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Completion API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The completion API is a sophisticated tool that generates text to complete prompts
    provided by the user. It forms the backbone of the OpenAI API and offers a simple
    yet robust and flexible API. It is designed to produce text that is coherent and
    contextually fitting for the given prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Many generation examples that are not chat-type constructs use the completion
    API. We must use the completion API to generate text that is not a chat-style
    conversation. Some of the benefits of completion API are
  prefs: []
  type: TYPE_NORMAL
- en: '*Contextual understanding*—The completion API can understand the context of
    the prompt and generate relevant text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Versatility*—It can be used in various applications, from creating content
    to answering questions, which makes it a valuable tool for multiple applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Multiple language understanding*—The completion API can understand and generate
    content in several languages, which makes it a global resource.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Easy implementation*—The completion API is straightforward, which makes it
    accessible to developers of various skill levels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The API’s structure is quite simple, as shown in the following snippet. The
    input (prompt) and the output (completion) are in text format. The API response
    is a JSON object from which the generated text can be extracted using the text
    key. This response is called text completion. The completion strives to adhere
    to the instructions and context provided in the prompt and is one of the potential
    outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We start with an instruction, which is the prompt that specifies what we aim
    to generate. In our example, the instruction asks the model to generate a few
    bullets outlining why pets are awesome. The completion API has numerous parameters,
    but the most essential ones are detailed in table 3.3\. We discussed many other
    parameters earlier in this chapter and the book (e.g., the prompt, tokens, and
    temperatures). The stop sequences, however, are a new concept. We can employ these
    sequences to make the model cease generating tokens at a certain point, such as
    at the end of a sentence or a list.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.3 Completion API
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Parameter | Type | Default value | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `prompt`  | String or array  | `<\&#124;endoftext\&#124;>`  | A string or
    an array of strings is the prompt used to generate these completions.  |'
  prefs: []
  type: TYPE_TB
- en: '| `max_tokens`  | Integer  | 16  | This is the maximum number of tokens to
    generate in the completion, including the prompt. The `max_tokens` must not exceed
    the model’s context length.  |'
  prefs: []
  type: TYPE_TB
- en: '| `temperature`  | Number (float)  | 1  | This ranges between 0 and 2\. Higher
    values mean the model takes more risks and gets more creative.  |'
  prefs: []
  type: TYPE_TB
- en: '| `stop`  | String or array  | Null  | This can be up to four sequences where
    the API stops generating further tokens. The returned text will not contain the
    stop sequence.  |'
  prefs: []
  type: TYPE_TB
- en: '| `n`  | Integer  | 1 (optional)  | This defines the number of completions
    to generate for each prompt. This generates many completions and can quickly consume
    the token limit; we should have a reasonable setting for `max_tokens` and stop
    managing cost.  |'
  prefs: []
  type: TYPE_TB
- en: '| `stream`  | Boolean  | False (optional)  | This is a flag controlling whether
    to stream back partial progress as tokens are generated. If set, the stream is
    terminated by a data `[DONE]`message.  |'
  prefs: []
  type: TYPE_TB
- en: '| `best_of`  | Integer  | 1 (optional)  | This generates `best_of` completions
    server-side and returns the best completion. This parameter cannot be used with
    gpt-35-turbo.  |'
  prefs: []
  type: TYPE_TB
- en: '| `top_p`  | Number (float)  | 1 (optional)  | This controls randomness using
    a technique called nucleus sampling, an alternative to the `temperature` setting
    with a value ranging between 0 and 1\.  |'
  prefs: []
  type: TYPE_TB
- en: '| `logit_bias`  | Map  | Null (optional)  | This defines the likelihood of
    specified tokens appearing in the completion. It uses a mapping of tokens to a
    bias value (–100 of a ban to 100 of exclusive selection).  |'
  prefs: []
  type: TYPE_TB
- en: '| `user`  | String  | Null (optional)  | This parameter is a unique ID representing
    the end-user; it can help debug, monitor, and detect abuse.  |'
  prefs: []
  type: TYPE_TB
- en: '| `logprobs`  | Integer  | Null (optional)  | This is an optional array of
    log probabilities representing the alternate tokens and their likelihood considered
    for completion. This parameter cannot be used with gpt-35-turbo.  |'
  prefs: []
  type: TYPE_TB
- en: '| `suffix`  | String  | Null (optional)  | This parameter can be a string of
    up to 40 characters added as a suffix to the generated text.  |'
  prefs: []
  type: TYPE_TB
- en: '| `echo`  | Boolean  | False (optional)  | This determines whether the prompt
    is included in the completion. This is useful for use cases that need to capture
    the prompts and for debugging purposes. It cannot be used with gpt-35-turbo.  |'
  prefs: []
  type: TYPE_TB
- en: '| `presence_ penalty`  | Number (float)  | 0 (optional)  | This parameter steers
    the model’s tendency and helps outline its behavior to introduce new topics or
    ideas into the generated text. It ranges from 0.0 to 1.0\.  |'
  prefs: []
  type: TYPE_TB
- en: '| `frequency_ penalty`  | Number (float)  | 0 (optional)  | This is another
    parameter that helps steer the model and improve the generation results. It controls
    the level of common or uncommon words in the generated text and can be set to
    a value from 0.0 to 1.0\.  |'
  prefs: []
  type: TYPE_TB
- en: '| `function_ call`  |  |  | This controls how the model responds to functions
    when function calling is desired. It only works with 0613 or newer versions of
    the OpenAI models.  |'
  prefs: []
  type: TYPE_TB
- en: '| `functions`  |  |  | This is a list of functions that the model may use.  |'
  prefs: []
  type: TYPE_TB
- en: Note that the table only lists the most used parameters. It helps us understand
    some of the flows and concepts. Some parameters, such as functions, have more
    advanced uses, which will be covered in later chapters on prompt engineering.
  prefs: []
  type: TYPE_NORMAL
- en: We stick with the pets theme and use the model to help us suggest names for
    a pet salon business. We ask for three names, and the instructions also outline
    some of the important characteristics to use. These aspects of the instructions
    help us steer the model toward some desired attributes. Please refer to the API
    documentation for a full list of parameters. Let’s call the completion API and
    walk through it.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.3 Calling the completion API
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Completion API call for generating text'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Specifies the model to use; note that this name will change based on what
    you set in the deployment'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Prompt'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Model configurations'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Extracts the generated text from the response'
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! We used the API for our first text generation. Because of the
    nondeterministic nature of AI, especially generative AI, the output you will see
    when running this differs from
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Prompt.png)**Suggest three names for a new pet salon business.
    The generated name ideas should evoke positive emotions and the following key
    features: professional, friendly, personalized service.'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**1\. Pawsitively Professional Pet Salon'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2\. Fur & Feathers Friendly Pet Parlor
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 3\. Happy Tails Personalized Pet Pampering
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note  LLMs and most other generative AI models are nondeterministic, meaning
    that identical inputs could give different outputs. Changing the temperature setting
    to zero can make the outputs more deterministic, but a small amount of variability
    may remain.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Expanding completions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s see what a complete response from the API looks like and walk through
    that structure. The following listing shows the full response from the API. The
    `choices` field is among the most interesting, given that it has the completion
    text. The choices property is an array, where each item has an `index`, the reason
    the generation finished (`finish_reason`), and the generated text (via the `text`
    property).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.4 API response from a completion API
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Array of completion data'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Response creation datetime stamp'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Unique ID of the response'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Model ID used to generate the response'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Count of tokens used in this request'
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.4 shows the remaining properties. The usage property outlines the tokens
    used (`total_tokens`), including the prompt and response tokens. Because we pay
    per token, it is important to structure the prompt for aspects—first, to return
    only what is needed, minimizing token usage, and second, to limit the number of
    tokens generated in the first place.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.4 Completion response properties
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Property | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `choices`  | An array that can contain one or more completions data  |'
  prefs: []
  type: TYPE_TB
- en: '| `created`  | UNIX date-time stamp when the response was created  |'
  prefs: []
  type: TYPE_TB
- en: '| `id`  | A unique identifier of the response is useful when we need to track
    responses  |'
  prefs: []
  type: TYPE_TB
- en: '| `model`  | Represents the model that was used for the generation  |'
  prefs: []
  type: TYPE_TB
- en: '| `object`  | Outlines the data type of the response (e.g., in this case, it
    is a `text_completion`, outlining a completion API)  |'
  prefs: []
  type: TYPE_TB
- en: '| `usage`  | Counts the number of tokens used by this request  |'
  prefs: []
  type: TYPE_TB
- en: A property called `logprobs` specifies the number of log probabilities to generate
    for each token in the response. The log probabilities are useful for generating
    more diverse and interesting responses. It returns the log probabilities of the
    top *n* tokens for each token in the response. The log probabilities are returned
    as an array of arrays, where each subarray corresponds to a token in the response
    and contains the log probabilities of the top *n* tokens for that token.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Azure content safety filter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sometimes, the API returns a `null` response, as shown in listing 3.5\. When
    this happens, we should check the value of the `finish_reason` field. If its value
    is set to `content_filter`, the content filtering system that works alongside
    models has been triggered. The `finish_reason` field indicates why the API returned
    the output it did, and every response will include this field. This topic will
    be covered in more detail later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The filtering system uses specific categories to identify and act on potentially
    harmful content as part of both the input prompts and generated completions. The
    application that uses these APIs must handle this situation and retry after the
    appropriate back-off period. The content safety filter and ethical AI will be
    covered in more detail in chapter 13.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.5 Output showing `null` response
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '#1 null response'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Content filter is the reason the response finished.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3 Multiple completions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We might want multiple completions for a few reasons. Sometimes, we need to
    generate multiple message choices for the same prompt. At other times, the API
    is throttled for capacity reasons, and we might want to get more from the same
    API call instead of being rate limited. The completions API can return multiple
    responses; this is done by setting the `n` parameter to more than the default
    value of 1\. For example, we can add this parameter to the completion call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: When we run this updated code, we get the response shown in listing 3.6\. The
    property choices are an array, and we have three items, with the index starting
    at a base zero. Each has the generated text for us to use. Depending on the use
    case, this is helpful when picking multiple completions.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.6 Output showing multiple responses
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Another similar but more powerful parameter is the `best_of` parameter. Like
    the `n` parameter, it generates multiple completions, allowing the option to pick
    the best. The `best_of` is the completion with the highest log probability per
    token. We cannot stream results when using this option. However, it can be combined
    with the `n` parameters, with `best_of` needs greater than `n`.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the following listing, if we set `n` to 5, we get five completions
    as expected; for brevity, we do not show all five of the completions here, but
    note that this call uses 184 tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.7 Output showing multiple responses
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run a similar call using the `best_of` parameter, do not specify the
    `n` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: When we run this code, we get only one completion, as shown in listing 3.8;
    however, we are using a similar number of tokens as earlier (171 versus 184).
    This is because the service generates five completions on the server side and
    returns the best one. The API uses the log probability per token to pick the best
    option. The higher the log probability, the more confident the model is about
    its prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.8 Output generation with `best_of` five completions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The one parameter that influences many of the responses is the temperature setting.
    Let’s see how this changes the output.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.4 Controlling randomness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As discussed in the previous chapter, the `temperature` setting influences the
    randomness of the generated output. A lower temperature produces more repetitive
    and deterministic responses, while a higher temperature produces more innovative
    responses. Fundamentally, there isn’t a right setting—it all comes down to the
    use cases.
  prefs: []
  type: TYPE_NORMAL
- en: For enterprises, a more creative output would be when there is interest in diverse
    output and creating text for use cases such as content generation for marketing,
    stories, poems, lyrics, jokes, etc. These are things that usually require creativity.
    However, enterprises need more reliable and precise answers for use cases, such
    as document automation for invoice generation, proposals, code generation, etc.
    These settings are applicable per API call, so combining different temperature
    levels in the same workflow is possible.
  prefs: []
  type: TYPE_NORMAL
- en: As demonstrated in previous examples, we recommend a temperature setting of
    0.8 for creative responses. Conversely, a setting of 0.2 is suggested for more
    predictable responses. Using an example, let us examine how these settings alter
    the output and observe the variations between multiple calls.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the temperature was set to 0.8, we received the following responses from
    three consecutive calls. The output changes as expected, offering suggestions
    like those seen throughout this chapter. It is important to note that we do not
    need to make three separate API calls. We can set the `n` parameter to 3 in a
    single API call to generate multiple responses. Here is what our API call looks
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The following listing shows the creative generation for the three responses.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.9 Completions output with the temperature at 0.8
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '#1 First response: get blocked by the content filter'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Second of three responses'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Final response with very different generated text'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s change the setting to make this more deterministic and run it again. Note
    that the only change in the API call is `temperature=0.2`. The output is predictable
    and deterministic, with very similar text generated between the three responses.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.10 Completions output with the temperature at 0.2
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '#1 One of three responses'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Two of three responses; very similar generated text'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The final response with very similar generated text'
  prefs: []
  type: TYPE_NORMAL
- en: The temperature value goes up to 2, but it is not recommended to go that high,
    as the model starts hallucinating more and creating nonsensical text. If we want
    more creativity, we usually want it to be at 0.8 and, at most, 1.2\. Let us see
    an example when the temperature is changed to 1.8\. In this example, we did not
    even get the third generation, as we hit the token limit and stopped the generation.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.11 Completions output with the temperature at 1.8
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '#1 One of three responses with names that aren’t very clear'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Second and third of three responses, with nonsensical names'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.5 Controlling randomness using top_p
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An alternative to the `temperature` parameter for managing randomness is the
    `top_p` parameter. It has the same affect on the generation as the temperature
    parameter, but it uses a different technique called *nucleus sampling*. Essentially,
    nucleus sampling allows only the tokens with a probability equal to or less than
    the value of `top_p` to be considered as part of the generation.
  prefs: []
  type: TYPE_NORMAL
- en: Nucleus sampling creates texts by picking words from a small group of the most
    likely ones with the highest cumulative probability. The `top_p` value decides
    how small this group is based on the total chance for the words to appear in it.
    The group size can change depending on the next word’s chance. Nucleus sampling
    can help avoid repetition and generate more varied and clearer texts than other
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we have the `top_p` value set to 0.9, only the tokens that make
    up 90% of the probability distribution will be sampled for the generation of text.
    This allows us to avoid the last 10%, which are often quite random and diverse
    and end up as nonsensical hallucinations.
  prefs: []
  type: TYPE_NORMAL
- en: A lower value of `top_p` makes the model more consistent and less creative as
    it chooses fewer tokens to generate. Conversely, a higher value makes the generation
    more creative and diverse, as it has a larger set of tokens to operate. The larger
    value also makes it prone to more errors and randomness. The exact value of `top_p`
    depends on the use case; in most cases, the ideal value for `top_p` ranges between
    0.7 and 0.95\. We should change either the temperature attribute or `top_p`, but
    not both. Table 3.5 outlines the relationship between the two.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.5 Relationship between temperature and `top_p`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Temperature | top_p | Effect |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Low  | Low  | Generates predictable text that closely follows common language
    patterns  |'
  prefs: []
  type: TYPE_TB
- en: '| Low  | High  | Generates predictable text, but with occasional less common
    words or phrases  |'
  prefs: []
  type: TYPE_TB
- en: '| High  | Low  | Generates text that is often coherent but with creative and
    unexpected word usage  |'
  prefs: []
  type: TYPE_TB
- en: '| High  | High  | Generates highly diverse and unpredictable text with various
    word choices and ideas; has very creative and diverse output, but may contain
    many errors  |'
  prefs: []
  type: TYPE_TB
- en: Let us look at some of the advanced API options for specific scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Advanced completion API options
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have examined the basic constructs of the completion API and understand
    how they work, we need to consider more advanced aspects of the completion API.
    Many of these might not seem as complex, but they add many more responsibilities
    to the system architecture, complicating overall implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 Streaming completions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The completions API allows streaming responses, offering immediate access to
    information as soon as it is ready rather than waiting for a full response. For
    enterprises, streaming can be important in some cases where real-time content
    generation with lower latency is key. This feature can enhance user experiences
    by processing incoming responses promptly.
  prefs: []
  type: TYPE_NORMAL
- en: To enable streaming from the API’s standpoint, modify the `stream` parameter
    to `true`. By default, this optional parameter is set to `false`.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming employs server-sent events (SSE), which require a client-side implementation.
    SSE is a standard protocol allowing servers to continue transmitting data to clients
    after establishing the initial connection. It is a long-term, one-way connection
    from server to client. SSE offers advantages such as low latency, reduced bandwidth
    consumption, and an uncomplicated configuration setup.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.12 demonstrates how our example can be adjusted to utilize streaming.
    Although the API modification is straightforward, the description and requested
    multiple generations were adjusted (using the `n` property). This allows us to
    generate more text artificially, making it easier to observe the streaming generation.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.12 Streaming completion
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Tweaked the prompt slightly to add descriptions'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 We need to handle the streaming response on the client side.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Enables streaming'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 We need to loop through the array and handle multiple generations.'
  prefs: []
  type: TYPE_NORMAL
- en: When managing a streaming call, we must pay extra attention to the `finish_reason`
    property. As messages are streamed, each appears as a standard completion, with
    the text representing the newly generated token. In these instances, the `finish_reason`
    remains null. However, the final message differs; its `finish_reason` could be
    either `stop` or `length`, depending on what triggered it.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.13 Streaming finish reason
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '3.3.2 Influencing token probabilities: logit_bias'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `logit_bias` parameter is one way we can influence output completion. In
    the API, this parameter allows us to manipulate the probability of certain tokens,
    which can be words or phrases, that the model generates in its responses. It is
    called `logit_ bias` because it directly affects the log odds, or logits, that
    the model calculates for each potential token during the generation process. The
    bias values are added to these log-odds before converting them to probabilities,
    altering the final distribution of tokens the model can pick from.
  prefs: []
  type: TYPE_NORMAL
- en: The importance of this feature lies in its ability to steer the model’s output.
    Say we are creating a chatbot and want it to avoid certain words or phrases. We
    can use `logit_bias` to decrease the likelihood of those tokens being chosen by
    the model. In contrast, if there are certain words or phrases we want the model
    to favor, we could use `logit_bias` to increase their likelihood. The range of
    this parameter is from –100 to 100, and it operates on tokens for the word. Setting
    a token to –100 effectively bans it from the generation, whereas setting it to
    100 makes it exclusive.
  prefs: []
  type: TYPE_NORMAL
- en: To use `logit_bias`, we provide a dictionary where the keys are the tokens,
    and the val-ues are the biases that need to be applied to those tokens. To get
    the token, we use the `tiktoken` library. Once you have the appropriate token,
    you can assign a positive bias to make it more likely to appear or a negative
    bias to make it less likely, as shown in figure 3.2\. The blocks show the degree
    of probability that different tokens can be at different probabilities of banning
    or exclusive generation. Smaller changes to the tokens’ value increase or decrease
    the probability of these tokens in the generated output.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F02_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 The `logit_bias` parameter
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Let’s use an example to see how we can make this work. For our pet salon name,
    we do not want to use the words “purr,” “purrs,” or “meow.” The first thing we
    want to do is create the tokens for these words. We also want to add words with
    a preceding space and capitalize them as spaces. Capital letters are all different
    tokens. So “Meow” and “Meow” (with a space) and “meow” (again with a space) might
    read the same to us, but when it comes to tokens, these words are all different.
    The output shows us the tokens for the corresponding word:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have the tokens, we can add them to the completion call. Note that
    we assign each token a bias of –100, steering the model away from these words.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.14 `logit_bias` implementation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Dictionary containing the tokens and the corresponding bias values to steer
    the model on these specific tokens'
  prefs: []
  type: TYPE_NORMAL
- en: We do not have any words we want to avoid when we run this code.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.15 Output of `logit_bias` generation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We can do the opposite and positively bias tokens too. Say we want to overemphasize
    and steer the model toward the word “Furry.” We can use the `tiktoken` library
    we saw earlier and find that the tokens for “Furry” are `[37, 16682]`. We can
    update the previous API call with this and, in this case, a positive bias of 5.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 3.16 `logit_bias`: Positive implementation'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: When we run this code, we get the output shown in the following listing. As
    we can see, there is a much stronger emphasis on “Furry” in our generation. The
    completions also take longer, as the model competes with the bias when generating
    certain tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 3.17 Output `logit_bias`: Positive implementation'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The `logit_bias` feature should be used carefully; it is a powerful tool for
    guiding the model’s output. However, excessive or inappropriate use can lead to
    nonsensical, overly repetitive, or biased output in unexpected ways.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.3 Presence and frequency penalties
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have two additional parameters in the API, called *presence* and *frequency*
    penalties, that help steer the language model’s output by controlling the generation’s
    repetition. These two parameters influence the likelihood of words (technically
    a sequence of tokens) reappearing in a completion. A higher presence penalty encourages
    the model to focus on the prompt and avoid using tokens that already appear there.
    In contrast, a higher frequency penalty discourages the model from repeating itself.
    Let’s take a look at both in a little more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Presence penalty parameter
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The presence penalty parameter affects how often the same token appears in the
    output. This is achievable by using the presence penalty as a value subtracted
    from the probability of a token each time it is generated. This means that the
    more a token is used, the less likely it is to be used again. This helps make
    the model use more varied tokens in the generation and explore new topics. The
    value of this parameter can range from 0 to 2.
  prefs: []
  type: TYPE_NORMAL
- en: The default value is 0, meaning the model does not care if a token is repeated.
    A high presence penalty (1.0) makes the model less likely to use the same token
    again, and a higher value makes the model introduce new topics in the output.
    A low presence penalty (0) makes the model stick to the existing topics in the
    text. Each time a token is generated, the parameter value is subtracted from the
    log probability of that token.
  prefs: []
  type: TYPE_NORMAL
- en: We can improve the quality of the generation by preventing the same text from
    being repeated multiple times, helping control the flow, and making the output
    more engaging. Now let’s look at the frequency penalty parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Frequency penalty parameter
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This parameter controls how much the model avoids repeating itself in the output.
    The higher the frequency penalty (1.0), the more the model tries to use different
    words and phrases, which results in a more diverse generation. The lower the frequency
    penalty (0.0), the more the model can repeat the same words and phrases and the
    more predictable the output. This differs from the presence penalty, which encourages
    the model to use new words and phrases. The frequency penalty adds to the log
    probability of a token each time it appears in the output.
  prefs: []
  type: TYPE_NORMAL
- en: The best values for both parameters depend on what you want to achieve with
    the output. Usually, choosing values between 0.1 and 1.0 would be best, which
    noticeably affects the output. If you want a stronger effect, you can increase
    the values up to 2.0, but this might reduce the output quality.
  prefs: []
  type: TYPE_NORMAL
- en: Note that tuning these parameters requires some trial and error to get the desired
    results, as the model’s output is also influenced by many other factors, including
    the prompt you provide and other fine-tuning parameters. Figure 3.3\. shows the
    correlation for both the presence and frequency penalty parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F03_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 Penalty presence parameter
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 3.3.4 Log probabilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When an LLM generates a token, it assigns a probability to the next considered
    token and uses various techniques to pick the token used in the completion from
    these options. The `logprobs` property of the completion API exposes the natural
    logarithm for these probabilities at each step.
  prefs: []
  type: TYPE_NORMAL
- en: This is an integer (max value of 5) that shows the alternate tokens considered
    for each token included in the completion. If this value is set to 3, the API
    will return a list of the three most likely tokens for each selected token in
    the generation. Note that the API always returns the `logprobs` of the sampled
    token, so in the response, we might end up with `logprobs + 1` element in the
    array.
  prefs: []
  type: TYPE_NORMAL
- en: Fundamentally, we use this approach to help debug and improve the prompts. If
    the model isn’t generating text we like, we can use this to see what other words
    (technically tokens) the model considered. This allows us to tune some other settings
    to steer the model. Conversely, we can use the same thing to control randomness
    in the model generation and make the output more deterministic. Finally, we can
    also use this to understand how confident the model is. If the probabilities are
    the same for several different words, this means that the model is not certain
    what word comes next.
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we want to get a name for a white dog; we can call the completion API.
    In this example, we get the name Cotton, which isn’t bad:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to see what other tokens were considered for the name, we can add
    the `logprobs` properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'As seen in the completion output in the following listing, the model considered
    the following tokens: Casper, Coco, and Snow.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.18 Output log probabilities
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: As a reminder, we should use this property judiciously and only when required.
    Not only does it increase the number of tokens generated and, hence, the cost
    of the API call, but it also takes time and adds time to the API call, thereby
    increasing overall latency.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the completion API for text generation, let’s see how
    we can use the chat completion API.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Chat completion API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The chat completion API has been designed to facilitate interactive and dynamic
    conversations. It is an evolution of the completion API, providing users with
    a more conversational and engaging experience. With this API, developers can create
    applications that have a dialogue with users, making it ideal for creating chatbots,
    writing assistants, and more.
  prefs: []
  type: TYPE_NORMAL
- en: The key benefits that the chat completion API provides over the completion API
    are
  prefs: []
  type: TYPE_NORMAL
- en: '*Enhanced interactivity*—The chat completion API allows for a more dynamic
    and interactive conversation with the user, making the user experience more engaging
    and natural.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Contextual understanding*—The API maintains the context of the conversation,
    ensuring that the responses are relevant and coherent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Multiturn conversation*—Unlike the completion API, which is more suited for
    single-turn tasks, the multiturn conversation API allows developers to simulate
    conversations with multiple exchanges.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cost-effective*—Completion API uses GPT-3.5 Turbo or GPT-4 models, which perform
    at a similar capability as text-davinci-003 but at 10% of the price per token,
    making it a more economical choice for developers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At a high level, using the chat completion API is similar to the completion
    API. The API takes a series of messages as input, forming the basis of the interaction
    with the model. The ordering of the messages is important, as it outlines the
    turn-by-turn interaction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each message has two properties: role and content. The role parameter has the
    following three options: `system`, `user`, or `assistant`. The content contains
    the message’s text from the role. Table 3.6 outlines the details of each role
    and its purpose.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.6 Chat completion API role description
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Role parameter | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `system`  | The `system` role is typically used to set the assistant’s behavior
    and provide the model with high-level instructions that guide the behavior throughout
    the conversation. This is where we can describe the assistant’s personality and
    tell it what it should and should not answer, as well as how to format responses.
    While there is no token limit, it is included with every API call and is part
    of the overall token limit.  |'
  prefs: []
  type: TYPE_TB
- en: '| `user`  | This represents the user’s input in the conversation; these messages
    contain the instructions or queries from the user that the assistant responds
    to.  |'
  prefs: []
  type: TYPE_TB
- en: '| `assistant`  | This represents the assistant’s prior messages in the conversation.
    Think of this as the ongoing memory that helps the model and provides the conversation
    context as it proceeds, turn by turn.  |'
  prefs: []
  type: TYPE_TB
- en: Listing 3.19 shows the chat completion API. As we called out earlier, the order
    of the messages in the array matters, as it represents the flow of the conversation.
    Usually, the conversation starts with a `system` message that sets the assistant’s
    behavior, followed by alternating `user` and `assistant` messages as the conversation
    proceeds turn by turn. The assistant’s replies are generated based on the conversation
    history.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.19 Chat completion API
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Chat complete API call'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Different models needed (Turbo) compared to completion API'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 List of messages that form the heart of the API'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 These parameters are the same as for the completion API.'
  prefs: []
  type: TYPE_NORMAL
- en: We need to update the engine parameter to use one of the chat-compatible models.
    As shown earlier in this chapter, not all models support the chat style, and we
    need to pick the models with the `chat_completion` capability (GPT-3.5 Turbo,
    GPT-4, GPT-4 Turbo). All the other parameters are the same as the completion API
    that we covered earlier in this chapter, and we will not get into those details
    again.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note  The following parameters are unavailable with the new GPT-35 Turbo and
    GPT-4 models: `logprobs`, `best_of`, and `echo`. Trying to set any of these parameters
    will throw an exception.'
  prefs: []
  type: TYPE_NORMAL
- en: The output of the previous example is shown in the next listing. The user started
    with “Hello, World!”, and the system responded, asking how to help us with the
    assistant message. The question about dog details is the next dialogue turn.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.20 Chat completion API output
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 3.4.1 System role
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The system role (some also call it the system message) is included at the beginning
    of the message array. This message provides the initial instructions for the model,
    and we can provide various pieces of information in the system role, including
  prefs: []
  type: TYPE_NORMAL
- en: A brief description of the assistant
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Personality traits of the assistant
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rules and instructions you want the assistant to follow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional information needed for the model (e.g., relevant questions from an
    FAQ)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We customize the system role and include basic instructions for the use case.
    From an API perspective, even though the system role is optional, it is highly
    recommended that you make this intentional to get the best results. For example,
    if we expand on the previous example of chatting for pets and pet salons, we can
    instruct the model to only reply in rhyme.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.21 Chat completion system message example
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Instructs to answer in rhyme'
  prefs: []
  type: TYPE_NORMAL
- en: In the example, we can have a conversation as expected, which can vary topics
    in turns, but all the answers rhyme.
  prefs: []
  type: TYPE_NORMAL
- en: When we want to give the model additional data as context for the conversation,
    this is called grounding the data. If there is a small amount of data, this can
    be part of the `system` role, as shown in the next listing. However, if there
    is a large amount of data, we should use embeddings and retrieve the most relevant
    information using a semantic search (e.g., Azure cognitive search).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.22 Grounding system message example
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 3.4.2 Finish reason
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Every chat completion API response has a finish reason encoded in the `finish_
    reason` field. Tracking is important in this case, as it helps us understand why
    the API returned the response it did. This can be useful for debugging and improving
    the application. For example, if you receive an incomplete response due to the
    `length` finish reason, you may want to adjust the `max_tokens` parameter to generate
    more complete responses. The possible values for `finish_reason` are
  prefs: []
  type: TYPE_NORMAL
- en: '`stop`—The API finished generating and either returned a complete message or
    a message terminated by one of the stop sequences provided using the stop parameter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`length—`The API stopped the model output due to the `max_tokens` parameter
    or token limit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function_call—`The model decided to call a function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`content_filter—`Some of the completion was filtered due to harmful content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3.4.3 Chat completion API for nonchat scenarios
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OpenAI’s chat completion can be used for nonchat scenarios. The API is quite
    similar and designed to be a flexible tool that can be adapted to various use
    cases, not just conversations. In most cases, the recommended path uses the chat
    completion API as if it were the completion API. The main reason is that the newer
    models (Chat 3.5-Turbo and GPT-4) are much more efficient, cheaper, and powerful
    than the earlier models. The completion use cases we have seen, such as analyzing
    and generating text and answering questions from a knowledge base, would all still
    work with the chat completion API.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the chat completion API nonchat scenarios usually involves structuring
    the conversation with a series of messages and a system message to set the assistant’s
    behavior. For example, as shown in the following listing, the system message sets
    the role of the assistant, and the user message provides the task.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.23 Chat completion as a completion API example
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: We can also use a series of user messages to provide more context or accomplish
    more complex tasks, as shown in the next listing. In this example, the first user
    message sets up the task, and the second user message provides more specific details.
    The assistant generates a response that attempts to complete the task in the user
    messages.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.24 Chat completion as a completion API example
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 3.4.4 Managing conversation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our examples keep running, but the conversation will hit the model’s token limit
    as it continues. With each turn of the conversation (i.e., the question asked
    and the answer received), the list of messages grows. As a reminder, the token
    limit for GPT-35 Turbo is 4K tokens, and for GPT-4 and GPT-4 32K, it is 8K and
    32K, respectively; these include the total count from the message list sent and
    the model response. We get an exception if the total count exceeds the relevant
    model limit.
  prefs: []
  type: TYPE_NORMAL
- en: No out-of-the-box option can track this token count for us and ensure it falls
    within the token limit. As part of the enterprise app design, we need to track
    the token count and only send a prompt that falls within the limit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many enterprises are in the process of implementing an enterprise version of
    ChatGPT using the chat API. Here are some of the best practices that can help
    enterprises manage these conversations. Remember, the best way to get your desired
    output involves iterative testing and refining your instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Setting the behavior with system message*—You should use the system message
    at the start of the conversation to guide the model’s behavior and for enterprises
    to tune to reflect their brand or IP.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Providing explicit instructions*—If the model is not generating your desired
    output, make your instructions more explicit. Think about it at the same level
    as if you were telling a toddler what not to do.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Breaking down complex tasks*—If you have a complex task, break it down into
    several simpler tasks, and send them as separate user messages. You often need
    to show, not explain it. This is called Chain of Thought (CoT), and it will be
    covered in more detail in chapter 6\.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Experimentation*—Feel free to experiment with the parameters to get the desired
    output. A higher temperature value (e.g., 0.8) makes the generation more random,
    while a lower value (e.g., 0.2) makes it more deterministic. You can also use
    the maximum token value to limit response length.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Managing tokens*—Be aware of the total number of tokens in a conversation,
    as input and output tokens count toward the total. You must truncate, omit, or
    shorten your text if a conversation has too many tokens to fit within the model''s
    maximum limit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Handling sensitive content*—If you’re dealing with potentially unsafe content,
    you should look at Azure OpenAI’s Responsible AI guidelines ([https://mng.bz/pxVK](https://mng.bz/pxVK)).
    However, if you are using OpenAI’s API, then OpenAI’s moderation guide is helpful
    ([https://mng.bz/OmEw](https://mng.bz/OmEw)) for adding a moderation layer to
    the outputs of the chat API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracking tokens
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As outlined earlier, keeping track of tokens when using the conversational
    API is key. Not only will the experience suffer if we go over the total token
    size, but the total number of tokens in an API also has a direct effect on latency
    and on how long the call takes. Finally, the more tokens we use, the more we pay.
    Here are some ways you can manage tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Count tokens**.* Use the `tiktoken` library, which allows us to count how
    many tokens are in a string without making an API call.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Limit response length**.* When making an API call, use the `max_tokens` property
    to limit the length of the model’s responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Truncate long conversations**.* If a conversation has too many tokens to fit
    within the model’s maximum limit, we must truncate, omit, or shorten our text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Limit the number of turns**.* Limiting the number of turns in the conversation
    is a good way to truncate or shorten the text. This also helps steer the model
    better when the conversation gets longer and tends to start hallucinating.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Check the* `usage`*field in the API response**.* After making an API call,
    we can check the usage field in the API response to see the total number of tokens
    used. This is ongoing and includes both input and output tokens. It is a good
    way to keep track of tokens and show them to the user via some UX.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Reduce temperature**.* Reducing the temperature parameter can make the model''s
    outputs more focused and concise, which can help reduce the number of tokens used
    in the response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Say we want to build a chat application for our pet salon and allow customers
    to ask us questions about pets, grooming, and their needs. We can build a console
    chat application, as shown in listing 3.25\. It also shows us a possible way to
    track and manage tokens. In this example, we have a function `num_tokens_from_messages`
    which, as the name suggests, is used to calculate the number of tokens in a conversation.
  prefs: []
  type: TYPE_NORMAL
- en: As the conversation grows turn by turn, we calculate the number of tokens used,
    and once it reaches the model limit, the old messages are removed from the conversation.
    Note that we start at index 1\. This ensures we always preserve the system message
    at index 0 and only remove user/assistant messages.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.25 `ConsoleChatApp:` Token management
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Sets up the OpenAI environment and configuration details'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Sets up the system message for the chat'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Function to count the total tokens from all the messages in the conversation'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Uses the tiktoken library to count tokens'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Loops through the messages'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Captures the user input'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 When the total tokens exceed the token limit, we remove the second token.
    The first token is the system token, which we always want.'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Chat completion API call'
  prefs: []
  type: TYPE_NORMAL
- en: Chat completion vs. completion API
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Both chat completion and completion APIs are designed to generate human-like
    text and are used in different contexts. The completion API is designed for single-turn
    tasks, providing completion to a prompt provided by the user. It is most suited
    for tasks where only a single response is required.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, the chat completion API is designed for multiturn conversations,
    maintaining the context of the conversation over multiple exchanges. This makes
    it more suitable for interactive applications such as chatbots. The chat completion
    API is a new dedicated API for interacting with the GPT-35-Turbo and GPT-4 models
    and is the preferred method. The chat completion API is geared more toward chatbots,
    and using the different roles (`system`, `user`, and `assistant`), we can get
    the memory of previous messages and organize few-shot examples.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.5 Best practices for managing tokens
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For LLMs, tokens are the new currency. As most enterprises go beyond kicking
    tires to business-critical use cases, managing tokens would become a priority
    for computations, cost, and overall experience. From an enterprise application
    perspective, here are some of the considerations for managing tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Concise prompts*—Where possible, using concise prompts and limiting the maximum
    number of tokens will reduce the token’s usage, making it more cost-effective.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Stop sequences*—Use stop sequences to stop the generations to avoid generating
    unnecessary tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Counting tokens*—We can count tokens using the `tiktoken` library as outlined
    earlier and avoid making the API calls do the same.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Smaller models*—Generally speaking, in computing, bigger and newer hardware
    and software are considered faster, cheaper, and better; however, this isn’t necessarily
    the case for LLMs. Where possible, consider using smaller models such as GPT-3.5
    Turbo first, and when they might not be a good fit, consider going to the next
    one. Smaller models are less compute intensive and, hence, are more economical.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Use caching*—For prompts that are either quite static or frequently repeated,
    implementing a caching strategy would help save tokens and avoid making API calls
    repeatedly. For more complex scenarios, look to cache the embeddings using a vector
    search and store, such as Azure Cognitive Search, Pinecone, etc. The last chapter
    covered an introduction to embeddings, and we will get more details on embeddings
    and searching later in chapters 7 and 8 when we cover RAG and chatting with your
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3.4.6 Additional LLM providers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Additional vendors also now have LLMs to use for enterprises. These are either
    available via APIs or, in some cases, as model weights that enterprises can self-host.
    Table 3.7 outlines some of the more famous ones available at the time of publication.
    Please note that some restrictions are in place from a commercial-licensing perspective.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.7 Other LLM providers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Models | Descriptions |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Llama 2  | Meta released Llama 2, an open source LLM, which comes in three
    sizes (7 billion, 13 billion, and 70 billion parameters) and is free for research
    and commercial purposes. Companies can access this through cloud options such
    as Azure AI’s model catalog, Hugging Face, or AWS. Enterprises that want to host
    it using their own compute and GPUs can request access from Meta via [https://ai.meta.com/llama/](https://ai.meta.com/llama/).  |'
  prefs: []
  type: TYPE_TB
- en: '| PaLM  | PaLM is a 13 billion-parameter model from Google that is part of
    their generative AI for developer products. The model can perform text summarization,
    dialogue generation, and natural language inference tasks. At the time of publication,
    there was a waitlist for an API key; details are available at [https://developers.generativeai.google/](https://developers.generativeai.google/).  |'
  prefs: []
  type: TYPE_TB
- en: '| BLOOM  | Bloom is a 223-billion parameter, open source multilingual model
    that can understand and generate text in over 100 languages by collaborating with
    over 1,000 researchers across more than 250 institutions. It is available via
    Hugging Face for deployment. More details are available at [https://huggingface.co/bigscience/bloom](https://huggingface.co/bigscience/bloom).  |'
  prefs: []
  type: TYPE_TB
- en: '| Claude  | Claude is a 12-billion parameter developed by Anthropic. It is
    accessible through a playground interface and API in its developer console for
    development and evaluation purposes only. At publication, for production use,
    enterprises must contact Claude for commercial discussions. More details can be
    found at [https://mng.bz/YVqz](https://mng.bz/YVqz).  |'
  prefs: []
  type: TYPE_TB
- en: '| Gemini  | Google recently released a new LLM called Gemini, a successor to
    PaLM 2 and optimized for different sizes: ultra, pro, and nano. It is designed
    to be more powerful than its predecessor and can be used to generate new content.
    Google claims it to be their most capable AI model yet. More details can be found
    at [https://mng.bz/GNxD](https://mng.bz/GNxD).  |'
  prefs: []
  type: TYPE_TB
- en: Interestingly, all these vendors follow a similar approach to the concepts and
    APIs established by OpenAI. For example, as outlined by their documents, the PaLM
    model from Google’s completion API equivalent is presented in the next listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.26 PaLM-generated text API signature
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: While these options exist, and some are from reputable and leading technology
    companies, for most enterprises, Azure OpenAI and OpenAI are the most mature,
    with the most enterprise controls and support needed. The next chapter will deal
    with images, and we will learn how to move from text to images and generate in
    that modality.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GenAI models are classified into various categories, depending on the type.
    Each model has additional capabilities and characteristics. Choosing the right
    model for the use case at hand is important. And unlike computer science, in our
    case, the biggest model isn’t necessarily better.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The completion API is a sophisticated tool that generates text, which can be
    used to complete prompts provided by the user and forms the backbone of the text
    generation paradigm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The completion API is relatively easy to use with only a few key parameters,
    such as the prompt, number of tokens to generate, temperature parameter that helps
    steer the model, and number of completions to generate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The API exposes many advanced options for steering models and controlling randomness
    and generated text, such as `logit_bias`, presence penalty, and frequency penalty.
    All these work in tandem and help generate better output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When using Azure OpenAI, the content safety filter can help filter specific
    categories to identify and act on potentially harmful content as part of both
    the input prompts and generated completions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The chat completion API builds on the completion API, going from one set of
    instructions and APIs to a dialogue with the user in a turn-by-turn interaction.
    The chat completion consists of multiple systems, user, and assistance roles.
    The conversation starts with a `system` message that sets the assistant's behavior,
    followed by alternating `user` and `assistant` messages as the conversation proceeds
    turn by turn.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The system role is included at the beginning of the message array. It provides
    the initial instructions for the model, including personality traits, instructions
    and rules for the assistant to follow, and additional information we want to provide
    as context for the model; this additional information is called grounding the
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each completion and chat completion API response has a finish reason, which
    helps us understand why the API returned the response it did. This can be useful
    for debugging and improving the application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The language learning models all have a finite context window and are quite
    expensive. Managing tokens becomes important for us to be able to run things at
    a reasonable cost and within the API allowance. This also helps us manage tokens
    in conversations for improved user experience and cost-effectiveness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to Azure OpenAI and OpenAI, there are other LLM providers, such
    as Meta’s Llama 2, Google’s Gemini and PaLM, Bloom by BigScience, and Anthropic’s
    Claude. Their offerings are similar and follow the completions and chat completions
    paradigm, including similar APIs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
